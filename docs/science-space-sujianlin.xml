<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>科学空间|Scientific Spaces</title>
<link>https://kexue.fm/</link>

<item>
<title>利用“熄火保护 + 通断器”实现燃气灶智能关火</title>
<link>https://spaces.ac.cn/archives/10394</link>
<guid>https://spaces.ac.cn/archives/10394</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文分享了一种将普通燃气灶智能化的方案，通过利用熄火保护装置与通断器，接入米家系统，实现智能关火功能。

1. 燃气灶智能化方向：检测开关火状态，实现与抽油烟机等设备的联动；实现智能关火，包括定时关火和接入米家等智能家居系统。
   
2. 市场现状：功能带有智能化的燃气灶选择较少且价格偏高，更换不划算。

3. 解决方案：为普通燃气灶提供魔改方案，利用自带的熄火保护装置，通过通断器接入米家，实现智能关火功能。 

4. 功能提升：用户可以通过米家实现语音关火与远程关火，提升使用便利性与安全性。 <div>
<p>燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气灶选择并不多，并且相比普通燃气灶贵不少，单纯为了这两点功能而换一个新燃气灶并不划算，所以就出现了一些将普通燃气灶智能化的的魔改方案。</p><p><a href="https://kexue.fm/usr/uploads/2024/09/2699197935.png" target="_blank" title="点击查看原图"><img alt="接入方案示意图.png" src="https://kexue.fm/usr/uploads/2024/09/2699197935.png" width="300" /></a></p><p>本文主要分享基于燃气灶自带的熄火保护装置，利用通断器将燃气灶接入米家，实现智能关火功能。</p><p class="more"><a href="https://spaces.ac.cn/archives/10394" title="利用“熄火保护 + 通断器”实现燃气灶智能关火">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
</item>
<item>
<title>Softmax后传：寻找Top-K的光滑近似</title>
<link>https://spaces.ac.cn/archives/10373</link>
<guid>https://spaces.ac.cn/archives/10373</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文探讨了Top-k算子的光滑近似，定义了Top-k算子，并介绍其在向量中的运用。

要点总结：
1. Softmax函数是$\max$算子的光滑近似，通过指数归一化实现将向量转化为概率分布。
2. 介绍了温度参数的作用，这可以调节Softmax与$\text{argmax}$的接近程度。
3. 侧重探讨一般的Top-$k$算子的光滑近似。
4. 定义了Top-$k$算子$\mathcal{T}_k$，用于提取向量中最大的$k$个元素的下标集合。
5. 通过示例（如$\mathcal{T}_2([3,2,1,4]) = [1,0,0,1]$），展示了算子的具体输出，即Multi-Hot向量的形成。 <div>
<p>Softmax，顾名思义是“soft的max”，是$\max$算子（准确来说是$\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\boldsymbol{x}\in\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在<a href="https://kexue.fm/archives/10145" target="_blank">《通向概率分布之路：盘点Softmax及其替代品》</a>也介绍过其他一些能实现相同效果的方案。</p><p>我们知道，最大值通常又称Top-1，它的光滑近似方案看起来已经相当成熟，那读者有没有思考过，一般的Top-$k$的光滑近似又是怎么样的呢？下面让我们一起来探讨一下这个问题。</p><h2>问题描述</h2><p>设向量$\boldsymbol{x}=(x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$，简单起见我们假设它们两两不相等，即$i\neq j \Leftrightarrow x_i\neq x_j$。记$\Omega_k(\boldsymbol{x})$为$\boldsymbol{x}$最大的$k$个分量的下标集合，即$|\Omega_k(\boldsymbol{x})|=k$以及$\forall i\in \Omega_k(\boldsymbol{x}), j \not\in \Omega_k(\boldsymbol{x})\Rightarrow x_i > x_j$。我们定义Top-$k$算子$\mathcal{T}_k$为$\mathbb{R}^n\mapsto\{0,1\}^n$的映射：<br />
\begin{equation}<br />
[\mathcal{T}_k(\boldsymbol{x})]_i = \left\{\begin{aligned}1,\,\, i\in \Omega_k(\boldsymbol{x}) \\ 0,\,\, i \not\in \Omega_k(\boldsymbol{x})\end{aligned}\right.<br />
\end{equation}<br />
说白了，如果$x_i$属于最大的$k$个元素之一，那么对应的位置变成1，否则变成0，最终结果是一个Multi-Hot向量，比如$\mathcal{T}_2([3,2,1,4]) = [1,0,0,1]$。</p><p class="more"><a href="https://spaces.ac.cn/archives/10373" title="Softmax后传：寻找Top-K的光滑近似">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>低秩近似之路（一）：伪逆</title>
<link>https://spaces.ac.cn/archives/10366</link>
<guid>https://spaces.ac.cn/archives/10366</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文介绍了矩阵低秩近似中的伪逆概念，旨在帮助读者系统理解该主题，尤其在当前基于低秩近似的技术应用日益增长的背景下，强调了伪逆作为推广逆矩阵的重要性。

<br><br>
总结:
1. 低秩近似是一个熟悉但复杂的概念，广泛应用于现代技术，如LoRA等微调方法。
2. 伪逆（Pseudo Inverse）是低秩近似中的一个重要概念，代表了不可逆矩阵的“广义逆”。
3. 当前很多研究和应用中涉及到伪逆，为理解低秩近似提供了基础。
4. 本文旨在系统梳理与伪逆相关的理论内容，提高对低秩近似的整体认识。 <div>
<p>可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中时常能看到一些不熟悉但又让我们叹为观止的新技巧，这就导致了一种似懂非懂的陌生感。</p><p>因此，在这个系列文章中，笔者将试图系统梳理一下矩阵低秩近似相关的理论内容，以补全对低秩近似的了解。而在第一篇文章中，我们主要介绍低秩近似系列中相对简单的一个概念——伪逆。</p><h2>优化视角</h2><p>伪逆（Pseudo Inverse），也称“广义逆（Generalized Inverse）”，顾名思义就是“广义的逆矩阵”，它实际上是“逆矩阵”的概念对于不可逆矩阵的推广。</p><p class="more"><a href="https://spaces.ac.cn/archives/10366" title="低秩近似之路（一）：伪逆">[...]</a></p>
]]></content:encoded>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
</item>
<item>
<title>“闭门造车”之多模态思路浅谈（三）：位置编码</title>
<link>https://spaces.ac.cn/archives/10352</link>
<guid>https://spaces.ac.cn/archives/10352</guid>
<content:encoded><![CDATA[
<div> 摘要:  
多模态LLM在位置编码方面缺乏共识，而RoPE系列方法为多维序列提供了新的思路。

要点总结：  
1. 多模态LLM与文本LLM的主要差异在于方法论尚未成熟。  
2. 提到之前提出的“RoPE-Tie”方案，但存在细节不足的问题。  
3. 回顾当前多模态模型位置编码的现状，强调缺乏共识。  
4. 主流位置编码方法为RoPE，适用于1D序列。  
5. RoPE可以扩展到2D及3D序列，分别称为RoPE-2D和RoPE-3D，用于图像和视频等数据。 <div>
<p>在前面的文章中，我们曾表达过这样的观点：多模态LLM相比纯文本LLM的主要差异在于，前者甚至还没有形成一个公认为标准的方法论。这里的方法论，不仅包括之前讨论的生成和训练策略，还包括一些基础架构的设计，比如本文要谈的“多模态位置编码”。</p><p>对于这个主题，我们之前在<a href="https://kexue.fm/archives/10040" target="_blank">《Transformer升级之路：17、多模态位置编码的简单思考》</a>就已经讨论过一遍，并且提出了一个方案（RoPE-Tie）。然而，当时笔者对这个问题的思考仅处于起步阶段，存在细节考虑不周全、认识不够到位等问题，所以站在现在的角度回看，当时所提的方案与完美答案还有明显的距离。</p><p>因此，本文我们将自上而下地再次梳理这个问题，并且给出一个自认为更加理想的结果。</p><h2>多模位置</h2><p>多模态模型居然连位置编码都没有形成共识，这一点可能会让很多读者意外，但事实上确实如此。对于文本LLM，目前主流的位置编码是<a href="https://kexue.fm/archives/8265" target="_blank">RoPE</a>（RoPE就不展开介绍了，假设读者已经熟知），更准确来说是RoPE-1D，因为原始设计只适用于1D序列。后来我们推导了<a href="https://kexue.fm/archives/8397" target="_blank">RoPE-2D</a>，这可以用于图像等2D序列，按照RoPE-2D的思路我们可以平行地推广到RoPE-3D，用于视频等3D序列。</p><p class="more"><a href="https://spaces.ac.cn/archives/10352" title="“闭门造车”之多模态思路浅谈（三）：位置编码">[...]</a></p>
]]></content:encoded>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
</item>
<item>
<title>Decoder-only的LLM为什么需要位置编码？</title>
<link>https://spaces.ac.cn/archives/10347</link>
<guid>https://spaces.ac.cn/archives/10347</guid>
<content:encoded><![CDATA[
<div> 摘要：本文探讨主流Decoder-only LLM为何仍使用位置编码，尽管已有研究表明Causal Attention可在无附加位置编码情况下取得效果。通过三个角度分析其必要性、实现方式及不足之处。

<br><br>
总结要点：
1. 位置编码在Attention中的作用是帮助模型理解序列中元素的相对位置，以提高模型对时序信息的捕捉能力。
2. NoPE的Causal Attention通过依赖上下文信息和自回归特性实现位置编码，能在一定程度上克服传统位置编码的需求。
3. 不过，NoPE在处理长序列时仍有不足之处，可能无法充分捕捉远程依赖关系，因此主流LLM选择加入RoPE、ALIBI等位置编码方法，以增强模型的表达力和效果。 <div>
<p>众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在<a href="https://kexue.fm/archives/9529" target="_blank">《为什么现在的LLM都是Decoder-only的架构？》</a>也有过相关讨论），而对于Causal Attention，已经有不少工作表明它不需要额外的位置编码（简称NoPE）就可以取得非平凡的结果。然而，事实是主流的Decoder-only LLM都还是加上了额外的位置编码，比如RoPE、ALIBI等。</p><p>那么问题就来了：明明说了不加位置编码也可以，为什么主流的LLM反而都加上了呢？不是说“多一事不如少一事”吗？这篇文章我们从三个角度给出笔者的看法：</p><blockquote><p>1、位置编码对于Attention的作用是什么？</p><p>2、NoPE的Causal Attention是怎么实现位置编码的？</p><p>3、NoPE实现的位置编码有什么不足？</p></blockquote><p></p><p class="more"><a href="https://spaces.ac.cn/archives/10347" title="Decoder-only的LLM为什么需要位置编码？">[...]</a></p>
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>近乎完美地解决MathJax与Marked的冲突</title>
<link>https://spaces.ac.cn/archives/10332</link>
<guid>https://spaces.ac.cn/archives/10332</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文探讨了MathJax与Markdown的兼容性问题，解决MathJax与Marked冲突的方案。

<br><br>

1. 引入MathJax用于解析LaTeX公式，但引发兼容性问题。
2. 上一篇文章处理了MathJax与谷歌翻译、延时加载的兼容性问题。
3. Markdown是一种轻量级标记语言，广泛用于文档编写。
4. Markdown在浏览器中不可直接使用，需要转换为HTML。
5. Cool Papers的Kimi功能基于Markdown语法，实现文档输出。
6. 本文重点解决MathJax在与Marked工具结合时的冲突，以提高文档的展示效果。 <div>
<p>在<a href="https://kexue.fm/archives/10320" target="_blank">《让MathJax更好地兼容谷歌翻译和延时加载》</a>我们提到<a href="https://papers.cool/" target="_blank">Cool Papers</a>加入了MathJax来解析LaTeX公式，不过万万没想到引发了诸多兼容性问题，虽然部分问题纯粹是笔者的强迫症作祟，但一个尽可能完美的解决方案终究是让人赏心悦目的，所以还是愿意在上面花一点心思。</p><p>上一篇文章我们已经解决了MathJax与谷歌翻译、延时加载的兼容性，这篇文章我们则来解决MathJax与Marked的冲突。</p><h2>问题简述</h2><p>Markdown是一种轻量级标记语言，允许人们使用易读易写的纯文本格式编写文档，可谓是目前最流行的写作语法之一，Cool Papers中的[Kimi]功能，基本上也是按照Markdown语法输出。然而。Markdown并不是直接面向浏览器的语言，面向浏览器的语言叫做HTML，所以在展示给用户之前，有一个Markdown转HTML的过程（渲染）。</p><p class="more"><a href="https://spaces.ac.cn/archives/10332" title="近乎完美地解决MathJax与Marked的冲突">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
</item>
<item>
<title>让MathJax更好地兼容谷歌翻译和延时加载</title>
<link>https://spaces.ac.cn/archives/10320</link>
<guid>https://spaces.ac.cn/archives/10320</guid>
<content:encoded><![CDATA[
<div> 摘要: 这篇文章介绍了Cool Papers成功渲染数学公式的解决方案，提升了用户阅读体验，并解决了MathJax与谷歌翻译及延时加载的兼容性问题。

要点:  
1. 读者需求：用户希望渲染数学公式，提升阅读体验。  
2. 先前问题：MathJax与谷歌翻译及延时加载不兼容，导致无法渲染。  
3. 解决过程：经过反复查阅和调试，成功解决了兼容性问题。  
4. 当前状态：Cool Papers现已支持数学公式渲染，为偏数学的论文提供更好的展示。 <div>
<p>很早之前，就有读者提出希望把<a href="https://papers.cool/" target="_blank">Cool Papers</a>上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一堆乱码，确实会比较影响阅读体验。然而，之前的测试显示，负责渲染公式的MathJax跟谷歌翻译和延时加载都不大兼容，所以尽管需求存在已久，但笔者一直没有把它加上去。</p><p>不过好消息是，经过反复查阅和调试，这两天笔者总算把兼容性问题解决了，所以现在大家看到的Cool Papers已经能够渲染数学公式了。这篇文章总结一下解决方案，供大家参考。</p><p><a href="https://kexue.fm/usr/uploads/2024/08/3020393852.png" target="_blank" title="点击查看原图"><img alt="摘要带有公式的论文.png" src="https://kexue.fm/usr/uploads/2024/08/3020393852.png" width="600" /></a></p><p class="more"><a href="https://spaces.ac.cn/archives/10320" title="让MathJax更好地兼容谷歌翻译和延时加载">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
</item>
<item>
<title>“Cool Papers + 站内搜索”的一些新尝试</title>
<link>https://spaces.ac.cn/archives/10311</link>
<guid>https://spaces.ac.cn/archives/10311</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文介绍了Cool Papers新增的站内搜索系统，通过算法提升搜索效率，帮助用户快速找到所需论文，强调了关键词提炼和全文检索引擎的重要性。

要点总结：
1. 本文介绍了Cool Papers的站内搜索系统。
2. 该系统旨在帮助用户快速查找论文。
3. 高效检索需要技巧，如准确提炼关键词。
4. 算法的应用简化了繁琐的人工操作。
5. 相关技术为全文检索引擎。
6. 搜索算法基于关键词匹配，利用相似度指标提升结果精确度。 <div>
<p>在<a href="https://kexue.fm/archives/10088" target="_blank">《Cool Papers更新：简单搭建了一个站内检索系统》</a>这篇文章中，我们介绍了<a href="https://papers.cool/" target="_blank">Cool Papers</a>新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何高效地检索到对自己有价值的结果，并不是一件简单的事情，这里边往往需要一些技巧，比如精准提炼关键词。</p><p>这时候算法的价值就体现出来了，有些步骤人工来做会比较繁琐，但用算法来却很简单。所以接下来，我们将介绍几点通过算法来提高Cool Papers的搜索和筛选论文效率的新尝试。</p><h2>相关论文</h2><p>站内搜索背后的技术是全文检索引擎（Full-text Search Engine），简单来说，这就是一个基于关键词匹配的搜索算法，其相似度指标是<a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank">BM25</a>。</p><p class="more"><a href="https://spaces.ac.cn/archives/10311" title="“Cool Papers + 站内搜索”的一些新尝试">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
</item>
<item>
<title>通向最优分布之路：概率空间的最小化</title>
<link>https://spaces.ac.cn/archives/10289</link>
<guid>https://spaces.ac.cn/archives/10289</guid>
<content:encoded><![CDATA[
<div> 摘要: 本文探讨了概率空间中的优化问题，强调了该领域与无约束优化的区别及其复杂性，并提出了一些新的分析和计算方法，以确保优化结果符合概率分布的特性。  

1. 无约束优化通常通过求导和梯度下降来确定函数的最小值。  
2. 概率空间中的优化任务更为复杂，因为目标函数的输入是概率分布。  
3. 直接求梯度零点或进行梯度下降可能导致非概率分布的结果。  
4. 因此，需要新的分析和计算方法，确保优化结果符合概率分布特性。  
5. 作者通过系统学习相关内容，总结出解决概率分布优化问题的方法，供读者参考。   <div>
<p>当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。</p><p>以上这些都是无约束优化的基础结果，相信不少读者都有所了解。然而，本文的主题是概率空间中的优化，即目标函数的输入是一个概率分布，这类目标的优化更为复杂，因为它的搜索空间不再是无约束的，如果我们依旧去求解梯度零点或者执行梯度下降，所得结果未必能保证是一个概率分布。因此，我们需要寻找一种新的分析和计算方法，以确保优化结果能够符合概率分布的特性。</p><p>对此，笔者一直以来也感到颇为头疼，所以近来决定”痛定思痛“，针对概率分布的优化问题系统学习了一番，最后将学习所得整理在此，供大家参考。</p><p class="more"><a href="https://spaces.ac.cn/archives/10289" title="通向最优分布之路：概率空间的最小化">[...]</a></p>
]]></content:encoded>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
</item>
<item>
<title>对齐全量微调！这是我看过最精彩的LoRA改进（二）</title>
<link>https://spaces.ac.cn/archives/10266</link>
<guid>https://spaces.ac.cn/archives/10266</guid>
<content:encoded><![CDATA[
<div> 摘要:  
本文介绍了LoRA-GA和LoRA-Pro两种LoRA变体，前者通过梯度SVD改进初始化以对齐全量微调的第一步更新，后者则对齐每一步梯度，实现整条优化轨迹的对齐，彼此互为补充。

<br><br>  
总结:  
1. 引入LoRA-GA，用梯度SVD改进LoRA初始化，以对齐全量微调的第一步更新。
2. 读者提出的疑问关注后续梯度$W_2, W_3,\cdots$的对齐。
3. LoRA-Pro的出现正好回应了这一问题，它关注每一步梯度的对齐。
4. LoRA-GA和LoRA-Pro互补，LoRA-GA优化第一步，LoRA-Pro优化整个优化轨迹。  
5. 本文延续了上文的记号和内容，不再重复细节，重点讨论LoRA变体的改进及其理论依据。 <div>
<p>前两周笔者写了<a href="https://kexue.fm/archives/10226" target="_blank">《对齐全量微调！这是我看过最精彩的LoRA（一）》</a>（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，这样做也只能尽量对齐第一步更新后的$W_1$，所以当时就有读者提出了“后面的$W_2,W_3,\cdots$不管了吗？”的疑问，当时笔者也没想太深入，就单纯觉得对齐了第一步后，后面的优化也会严格一条较优的轨迹走。</p><p>有趣的是，LoRA-GA才出来没多久，arXiv上就新出了<a href="https://arxiv.org/abs/2407.18242" target="_blank">《LoRA-Pro: Are Low-Rank Adapters Properly Optimized?》</a>，其所提的LoRA-Pro正好能回答这个问题！LoRA-Pro同样是想着对齐全量微调，但它对齐的是每一步梯度，从而对齐整条优化轨迹，这正好是跟LoRA-GA互补的改进点。</p><h2>对齐全量</h2><p>本文接着上一篇文章的记号和内容进行讲述，所以这里仅对上一节的内容做一个简单回顾，不再详细重复介绍。LoRA的参数化方式是<br />
\begin{equation}W = (W_0 - A_0 B_0) + AB\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10266" title="对齐全量微调！这是我看过最精彩的LoRA改进（二）">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
</item>

</channel>
</rss>