<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Microsoft Research</title>
<link>https://www.microsoft.com/en-us/research/</link>

<item>
<title>Intern Insights: Vaishnavi Ranganathan with Angela Busheska</title>
<link>https://www.microsoft.com/en-us/research/podcast/intern-insights-vaishnavi-ranganathan-with-angela-busheska/</link>
<guid>https://www.microsoft.com/en-us/research/podcast/intern-insights-vaishnavi-ranganathan-with-angela-busheska/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-image size-full"><img alt="Outline illustrations of Angela Busheska, an undergraduate engineering student at Lafayette College and Vaishnavi Ranganathan, a Senior Researcher at Microsoft." class="wp-image-1097145" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Angela-and-Vaishnavi_Intern_Insights_Hero_Feature_No_Text_1400x788-1.jpg" width="1401" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	
</div>



<p>Every year, interns from academic institutions around the world apply and grow their knowledge as members of the research community at Microsoft. In this Microsoft Research Podcast series, these students join their internship supervisors to share their experience working alongside some of the leading researchers in their respective fields.&nbsp;</p>



<p>In this episode, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://angelabusheska.com/" rel="noreferrer noopener" target="_blank">Angela Busheska</a>, an undergraduate engineering student at Lafayette College, talks to Senior Researcher <a href="https://www.microsoft.com/en-us/research/people/vnattar/?msockid=35739e94ab6c69d41b738b93aa076831">Vaishnavi Ranganathan</a> about her work on TerraTrace, a platform that brings together statistics and large language models to track land use over time for agricultural and forestry applications. Busheska discusses the personal loss that drew her to climate activism, the chain of events that led to a memorable face-to-face meeting with Microsoft’s chief sustainability officer, and her advice for going after the internship you want and making the experience count.&nbsp;</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-1 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<figure class="wp-block-image size-full"><img alt="Angela Busheska standing to the left of the Microsoft sign on the Microsoft campus in Redmond, Washington." class="wp-image-1096734" height="900" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/AB-Seattle.jpeg" width="1200" /><figcaption class="wp-element-caption">Angela Busheska, pictured on the Microsoft campus in Redmond, Washington, was a part of the Microsoft Research Undergraduate Research Intern Program. During her time in the internship program, she helped develop a platform for tracking land use across time for agricultural and forestry applications. </figcaption></figure>
</div>



<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<figure class="wp-block-image size-full"><img alt="Angela Busheska and Melanie Nakagawa standing in front of a fence " class="wp-image-1096740" height="1200" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/10/Bloomberg-Green-Festival.jpg" width="900" /><figcaption class="wp-element-caption">During her internship, Busheska met with Microsoft Chief Sustainability Officer Melanie Nakagawa at the Bloomberg Green Festival in Seattle and spoke with the Microsoft executive about her sustainability work.&nbsp;</figcaption></figure>
</div>
</div>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity" />



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/Angelaangie-ai/terra_trace/tree/main" rel="noreferrer noopener" target="_blank">TerraTrace<span class="sr-only"> (opens in new tab)</span></a><br />GitHub repo</li>



<li><a href="https://www.microsoft.com/en-us/research/project/project-farmvibes/">Project FarmVibes</a><br />Project homepage</li>



<li><a href="https://www.microsoft.com/en-us/research/project/foodvibes/">Project FoodVibes</a><br />Project homepage</li>
</ul>



<div class="wp-block-spacer" style="height: 20px;"></div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="black" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less">
			<div>
				<span>
					

<h3 class="wp-block-heading" id="transcript-1">Transcript</h3>



<p>[TEASER]&nbsp;</p>



<p>[MUSIC PLAYS UNDER DIALOGUE]&nbsp;</p>



<p><strong>ANGELA BUSHESKA: </strong>Being in New York while working with people from Seattle and Brazil allowed me to have a broad range of people that I had a chance to meet. So, like, the New York office, it is very specifically focused on economics and social aspects that I&#8217;m not an expert on, but having a lunch with these people every single day, I had a chance to learn a lot.&nbsp;Being in Seattle and meeting with interns and researchers in Seattle, I had a chance to learn how other projects came to life. With <em>our</em> meetings, I realized how FarmVibes and FoodVibes were once just, like, a small idea and now are these huge projects. So having a chance to understand the history of all the things around me was a great way to see how <em>I</em> am able to build something for the future.</p>



<p>[TEASER ENDS]</p>



<p>[MUSIC FADES]</p>



<p><strong>VAISHNAVI RANGANATHAN:</strong> Hey, everyone, welcome to <em>Intern Insights</em>, a Microsoft Research Podcast featuring some of the brilliant students who are contributing to research and advances at Microsoft as a part of the renowned internship program at Microsoft Research. I&#8217;m Dr. Vaishnavi Ranganathan, a senior researcher at Microsoft Research with a passion for leveraging sensing and wireless technology to help address global challenges in health, environment, and sustainability. Today, I&#8217;m speaking with my intern, Angela Busheska, about her work this summer and her experience as a Microsoft Research intern. Angela, welcome to the podcast. So I&#8217;ve had the pleasure of getting to know you this summer not only in the capacity as a student and researcher but also as a person. So to start with, why don&#8217;t you introduce yourself. Tell the audience a little bit about <em>yourself</em>—where you&#8217;re from, your academic background, what excites you in research, and most importantly, outside studies, what do you enjoy doing?</p>



				</span>
				<span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1">
					



<p><strong>ANGELA BUSHESKA: </strong>Yeah, absolutely. Thank you so much for having me. Super excited to be here and super excited to have spent three months—time flies!— as a researcher along with the project. So my name is Angela. I am originally from North Macedonia, a very small country next to Greece in the Balkans. And I spent my first, like, 16 years thinking that I would become a mathematician, really focused on math Olympiads. And then I moved to the capital city of my country, which is Skopje, North Macedonia, and that year, in 2019, we had the greatest pollution. We were, like, on the top of every single list in terms of pollution, but honestly, I didn&#8217;t care. I was like, I&#8217;m going to do, like, math Olympiads, win every medal. I moved in with my aunt, who was living very close to the city center, and she had a lot of cardiovascular difficulties. And then this air pollution clogged her blood, and she passed away. And that was the moment when I saw like, I love math, but this air pollution took my aunt, so I <em>have</em> to do something. So I made the hard decision that junior year going into senior year to cut my participation in math Olympiads after seven great years there and focus on the climate. I really just started to understand where is the pollution coming from in my area. I started to realize that fast fashion had a huge influence, and then that&#8217;s why I started, like, getting into this field, getting into research, started a climate tech nonprofit called EnRoute, where our mission is to debunk the fast fashion supply chain and let people but also corporations know what is behind their clothes. So we, kind of, help both sides. We help people with their shopping choices, but we help a lot of corporations and policy to understand either what&#8217;s behind their supply chain and change it or how to better communicate their supply chain because there&#8217;s a lot of greenwashing that is happening in this space. So it really started as a rag-tag group of teenagers, and this is my fifth year working at it. So I really had a passion for both understanding the research side of it and the nonprofit, like, getting people to learn more about it. So I think that marked really my presence. And then coming into the US in 2021, I started to study both electrical engineering and computer science just because I love this intersection of building and doing stuff in practice and a way to apply my math knowledge into real life.</p>



<p><strong>RANGANATHAN: </strong>Wow, so early in life you have such amazing goals. That&#8217;s impressive. So, Angela, we connected after Madeleine Daepp, a researcher who was involved in the intern selection process, introduced your work and your passion for sustainability to me. I saw your passion for this area and, you know, your enthusiasm in the first conversation—we were already working on ideas, right! I knew I had to work with you. But for you, there are many other internship opportunities. I would love to know what excited you the most about Microsoft Research.</p>



<p><strong>BUSHESKA: </strong>Absolutely. Yeah. So as I said, like, I started getting into this area of sustainability by myself. I really didn&#8217;t have a structured way to learn. So when I heard that there is an opportunity from people who have, like, previously worked in this area, are doctors in this area, to learn along them, that was definitely one. But also throughout my life, I had like entrepreneurship stints. I was selling lemonade when I was [LAUGHS] 6 years old; started a nonprofit. And this was a great opportunity for me to start a zero-to-one project while being an intern, which is, I would say, a once-in-a-lifetime chance. We had a chance to define the problem, see solution, try things. I was not part of just one project or building one feature. I had a chance to drive it along you and the team. So that was, I think, the main reason why.</p>



<p><strong>RANGANATHAN:</strong> I&#8217;m glad we were able to attract your attention here. [LAUGHS] Before we get into specifics on, you know, exactly what we did this summer, I&#8217;d like for us to talk a bit about your internship setup, right. I think it allowed for some really unique experiences, and, you know, I would love to have you share your internship highlights. I think there is one in particular that I&#8217;m thinking about, which I really hope you&#8217;ll share.</p>



<p><strong>BUSHESKA: </strong>Absolutely. I would say this was an internship like no other experience I&#8217;ve had in the past. So just for context, I was interning in New York while part of the team being in Brazil, part of the team—you and the other researchers—being in Redmond, a couple of other researchers, like, being technically in New York. So I had a chance to interact with all of these different time zones. Also for an additional context, I was a part of undergrad research internship group where they brought us to Seattle from New York—and also there are other interns from Boston—to spend like a week and learn more about Microsoft, Microsoft Research, the leadership. So that was an incredible opportunity. Also, I had a great opportunity prior to that to come to the team meeting to see farms in real life, interact with farmers, interact with the team in person, which was also another great opportunity. And I&#8217;ll come to the one that you&#8217;re referring [to] here [LAUGHS]. So the second time when I was, like, visiting Seattle, I remember a week prior, it was Friday, we were finishing up some stuff. You actually pointed out that I should definitely message some of the leadership people to interact with them and hear their insights. And it was EST time zone, somewhere around 8 when I left the office, and I shoot an email to the chief sustainability officer, Melanie Nakagawa. I was like, “Hey, my name is Angela. I&#8217;m a research intern. I have worked in, like, fast fashion in the past. Now I&#8217;m working with Vaishnavi and the team on this food project. I’d really love to meet you next week. [LAUGHS] I&#8217;m coming for like three days.” I sent that email. I was like, there&#8217;s no way I&#8217;m getting response from this. And then on Sunday, I traveled to Seattle. And I remember during the first day, assistant of the chief sustainability officer says, like, there is this Bloomberg Green Festival that is happening in Seattle on Thursday, and we would love for you to come and meet Melanie. It was, like, I don&#8217;t know. It was like real life or not. I really couldn&#8217;t … it took a lot of time to process. And after that, one thing additional that was a barrier was how to get into this festival because this festival had a ticket of like $300, $400. And I think that by that time everything was sold out. So I was like, I would love to, but I cannot just get into the Bloomberg Green Festival without a badge. So then I realized there is this sustainability community at Microsoft that is like 8,000 employees or something, and they got badges previously for the festival, and everyone who is working there can just get the basic badge just to get into. So I emailed a couple of people from there; they set [me up] with a badge. Huge props also to the undergraduate research team who extended my stay for a day because I was supposed to leave for New York the other day. So a couple of miracles happened there. I had a chance to go to the Bloomberg Green Festival, had a chance to meet with Melanie and her assistant, Tyler, who was working also in this area of sustainability in her presence on, like, these events. It really goes to say how an undergrad intern can, like, meet with leadership and learn from the leadership to the place and the opportunities we had during this internship. Thank you so much Melanie and Tyler for taking out the time at the Bloomberg Green Festival. I know it was a super packed day for both of you, but thank you so much for taking the time.</p>



<p><strong>RANGANATHAN: </strong>Thank you for sharing that. I want to point out something. It&#8217;s amazing how the leadership makes time and they really value every person&#8217;s work, and that&#8217;s an internship which is accounted for now, right, and you know your work is meaningful to the company. I think Microsoft is so big that people don&#8217;t realize there&#8217;s this community of 8,000 people in sustainability often, so it&#8217;s amazing. It blows my mind every single time, and kudos to you for following up on my late-night comment. [LAUGHS] Um, Angela, so this internship falls under larger efforts within Microsoft Research to establish a sustainable agriculture and food supply chain, a project that we know as FoodVibes. In your work this summer, you focus specifically on enabling technology to meet the new European Union&#8217;s regulation around deforestation-free products. Could you tell us, in your words, about these regulations and how you think the work can aid in their implementation?</p>



<p><strong>BUSHESKA: </strong>Absolutely, yeah. So the European Union’s regulation on deforestation-free products, also known as EUDR, is a legislation that will prevent food linked to deforestation to enter European Union borders<a href="#_ftn1" id="_ftnref1">[1]</a>. But how will the border officials know if something is coming from a deforested area or not? On another side, it&#8217;s also the farmers from this area who don&#8217;t know what exactly is a deforested area, especially the ones who moved after maybe an area has been deforested. So there are a lot of questions around there, and I think that getting into and understanding the problem was a huge part to continue after that in building the solution.</p>



<p><strong>RANGANATHAN: </strong>Yeah, and I believe that having a large team here really helped because we all often had discussions around many of these areas and we really valued your inputs and how you participated in these discussions, right. So through your internship, we&#8217;ve seen that there are a variety of tools, you know, and vegetation metrics at our disposal to help determine what exists on a plot of land because that&#8217;s what we want to identify. There&#8217;s, like, satellite imagery. We get the normalized difference vegetation index, and then there’s the USDA&#8217;s cropland data layer, which exists purely for the US. But, you know, we are getting an incomplete picture here, at least as it pertains to this particular use case. How do you think TerraTrace brings these pieces together, and what are some of the specific challenges or motivations of the core of your work?</p>



<p><strong>BUSHESKA: </strong>Absolutely. Yeah, I think, like, a couple of the first weeks, we tried these great machine learning models, and one thing we realized is that when we have, like, orchards, and when we had pine trees, and then when we had, like, regular forest, all of them are classified the same, which was a really great light saying, like, yeah, these are great. These are models that are trained by millions of data. They still don&#8217;t work as we want to. And also, I think that in an era where everyone is, like, training big great models, we kind of decided to take another route and say like, models are great. What if we go step by step and maybe we don&#8217;t need a full model round. Maybe we can go step by step to a certain point to try to understand deeply. And I think I&#8217;m really grateful that we took this direction because we had a chance to understand at a granular level what is actually happening. Here we had, like, we were lucky at the beginning to work with farms in Washington. We had, like, an actual farmer who was in the team who we have, like, pictures and we had all the data. So that was a great ground truth for us to understand, OK, this is how the vegetation looks on a farm. This is how the vegetation looks on a forest across years. So we had these snapshots and then from then on, we just moved along to say, well, if we know about this, what&#8217;s happening in Washington, how we can scale now to other places in the US. We looked deeply into California just because it has a lot of, like, agricultural diversity. And I think that this step-by-step level brought us to create TerraTrace. TerraTrace is a platform where one can enter, like, the coordinates of a farm and see what has been happening on a given piece of land across time. Now, TerraTrace combines a couple of different things. It combines mathematical results, combines LLMs, and combines just, like, the basic information of risk. And I think this is good just because it&#8217;s not a heavy computational platform that we have. And if we need to use it on, like, a couple of years from now on a farm, we technically can. If we need to use it, like, in a legal office, we can. Just because it doesn&#8217;t depend a lot on data and it&#8217;s, like, easily portable, which is another plus, and differentiator compared to what exists outside.</p>



<p><strong>RANGANATHAN: </strong>Do you want to share a little bit about the signature curves that you identified?</p>



<p><strong>BUSHESKA: </strong>Absolutely. So when I just, like, realized … there is a metric called NDVI (Normalized Difference Vegetation Index), which is basically measuring vegetation. It is a time series of vegetation. What we did with NDVI is we tried to, like, measure it across time on different places. And I think in coffee, it had the greatest impact. Just because there was a coffee in Vietnam, coffee in Honduras, completely two different places. And then when we analyzed and saw the shapes, we were like, wow, they are completely two different places; however, they still follow the same trend and that was a great calling to say, like, this is a greater metric. Maybe if, like, vision computer models failed, this is something that is, like, very rudimentary. You wouldn&#8217;t expect that just a very simple index would calculate that, but we saw it working for a lot of different places, and I&#8217;m particularly grateful for the places that were not together at the same place.</p>



<p><strong>RANGANATHAN: </strong>Yeah, and I think, like, looking at this, like, temporal, over-time picture is the unique aspect of what you built and that was what unlocked it for us, right. So you hinted at this. In developing the TerraTrace platform, you ended up combining large language models, or LLMs, and statistics. What did you like about this approach? What did each bring to the table? How did it contribute to your ultimate goal? Could you share a bit about?</p>



<p><strong>BUSHESKA: </strong>As I said, like, at the beginning, I&#8217;m a huge math person, and I truly believe that math has more power than we credit today for. So one thing that I started building, when I started building, I was just, how can we get as much information as possible without LLMs and then give it to the LLMs? That was kind of the approach that I started. So we have this, as you mentioned previously, crop data layer that was the base truth. So I could always go back and compare to see if it has, like, if we are doing the same thing. Then we had the signature curves that we could compare to base signature curves to understand, OK, it&#8217;s coffee or it&#8217;s a farm. And now when we were having a farm, we had a problem because corn looked the same as wheat. Multiple crops look the same. So I needed something more to differentiate what exactly is there. So by math, I managed to understand like the growth rate, the fallout rate, what is the percentage when the curve is up, how much percentage is down so every insight that we could get by just basic statistics. And after that, all this combined knowledge was given to the LLM to, kind of, just confirm that the math is right and give us, OK, greater insight into, well, your math is good, and after that, based on your math and everything the LLM knows, well, there is this probability that it&#8217;s corn and after that we came to the crop data layer to say, yep, it&#8217;s corn. Referring to corn just because this was our example demo. Also, LLMs are changing very rapidly today, so like GPT-4, GPT-4 Turbo, expecting a GPT-5, so constant development needs to be done, and I think that if something is, like, changed and doesn&#8217;t work in the platform, one thing that will always work is the statistics. So having something to always refer to was a very interesting process.</p>



<p><strong>RANGANATHAN: </strong>Yeah, I really appreciate, you know, how we had the focus on, hey, let&#8217;s do what we can with math first because, after all, this is a sustainability project, right?</p>



<p><strong>BUSHESKA: </strong>Exactly.</p>



<p><strong>RANGANATHAN:</strong> So LLMs were great at summarizing this, taking all the data and, like, giving you the outputs. I think that was a very interesting approach, too. So there are several existing pieces of work we&#8217;ve seen in this area. We&#8217;ve relied on a lot of literature in this specific application, right, that ended up havingauthors who were affiliated with Microsoft. Could you share a little bit about those works that we drew on and how we leveraged the Microsoft connections?</p>



<p><strong>BUSHESKA: </strong>Absolutely. One thing that was something I was not expecting was reading the papers and then hopping on a call with the authors. So one thing that I would credit is first we understood … there was this, like, GeoLLM group. They actually had a presentation in our weekly meeting for the group. So it was, like, me just staying in the presentation and trying to learn. And the other day, they send out like, this is a research paper; take a look at it. And after, like, we realized that the majority of the authors were in the same building as you actually, we had a great chance to, like, meet, like, with them and understand how …</p>



<p><strong>RANGANATHAN: </strong>This is SatCLIP, right?</p>



<p><strong>BUSHESKA: </strong>… <a href="https://www.microsoft.com/en-us/research/publication/satclip-global-general-purpose-location-embeddings-with-satellite-imagery/?msockid=35739e94ab6c69d41b738b93aa076831">SatCLIP<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2404.15500v1" rel="noreferrer noopener" target="_blank">GeoLLM<span class="sr-only"> (opens in new tab)</span></a>, both of them. Trying to understand, like, how they have built it, what they have built, why, how they are planning to continue, how I am able to use it. And, actually, because when you&#8217;re reading a paper and you&#8217;re reading GitHub repo, it&#8217;s one thing. [LAUGHTER] When you&#8217;re speaking with the author and seeing, like, this is why we&#8217;ve built it, here&#8217;s the limitations, be careful how you&#8217;re using it, it&#8217;s completely another thing. So I am really grateful for this situation. But also, not only, like, inside of Microsoft, we also had a chance to work with a lot of other folks. For example, we read a lot of papers from research groups in universities. And because our interns came from these universities, it was, like, great to understand from them also what they have built.</p>



<p><strong>RANGANATHAN:</strong> Nice, and the other interns whom you worked with, which we&#8217;ll come to. So for this specific work, we were motivated by the use case. What were some of your findings, and also how are you envisioning this work as a foundation for other supply-chain or even, you know, broader sustainability scenarios and even beyond those fields that you can think of?</p>



<p><strong>BUSHESKA:</strong> Yeah, so one thing that we, as a conclusion, came out from here is that signature curves are not just by look. When we tested that with a lot of different other signature curves, we realized, OK, this is true because in most of the cases, they worked. Obviously we had some failures, like citrus signature curves didn&#8217;t work because of other reasons that happened on the farm. But for most of them, we got same results on what&#8217;s happened with the signature curves with what happens actually on the farm, which is a great way to further this exploration here. Another thing that we realize from this project is now the group previously has worked on trying to understand supply chain, like having the tracking part and having this code to track along the journey, but we were missing the part on what happened on the land. And now that we have this additional way to understand what happened on the land, I think it is a full system of starting this is what happened on the land, this is what happened across the road, and this is where it is now, so that not only EUDR, but even, like, customers in the future will know where their food is coming from, which is that total part of sustainability that we want to get to. Because I think that at this point, we know the challenges, we know the climate change challenges that we are having, and the more information we have, the better decisions we and policymakers would be able to make.</p>



<p><strong>RANGANATHAN:</strong> And I think we realized this and you can chime in, Angela, but I feel that the traceability is a vehicle for data. What you build is a means to take all that data and make it meaningful to people, right. Like what does NDVI mean? I have no idea as a layperson, so you can actually get that information out. There were also a few other use cases that you gleaned out of your data input, right. Like wildfire was one of them. Would you like to share a little bit about that?</p>



<p><strong>BUSHESKA:</strong> Yes, so we were looking specifically about California, specifically about 2020, which was a season of very, very bad wildfires. And we could see by measuring this vegetation how, like, wildfires affected these regions. Like, you could see how vegetation was dropping very quickly from a great number of one to just, like, flat zero just because everything was there burned. So seeing firsthand on these curves and after that linking up to a lot of other background wildfire data was a good check-in. Like, you could see that it&#8217;s not a person deforestation; it&#8217;s a wildfire deforestation. And also it is really helpful for risk estimation as we go further along. We could see, like, this past year climate change effects are there. It&#8217;s not just like this fancy term in the future. It is there. So I think that having a chance to see in the past and having these models, it&#8217;s also giving us a preview for the future. Because ultimately we want this greater food yield so that more and more people can enjoy healthy food. So having a chance to predict the risk would help us to save more food and, yeah, care better about our planet and our people.</p>



<p><strong>RANGANATHAN:</strong> I think there&#8217;s a lot of potential. The more information you have, the more applications in sustainability you can build. This is merely, like, a footstool to launch it off, right. You’ve said one of the things you&#8217;ll miss most from your internship is the density of smart people in the building and in the company. Over the course of your internship, what did you learn? How did your actual experience compare to, like, expectations you had for it?</p>



<p><strong>BUSHESKA:</strong> I think it exceeded every single expectation that I had because I would say when you mentioned the density of smart people, it was really incredible to have—and I think that this … maybe my very interesting connection of being in New York while working with people from Seattle and Brazil allowed me to have a broad range of people that I had, like, had a chance to meet. So, like, the New York office, it is very specifically focused on economics and social aspects that I&#8217;m not an expert on, but having a lunch with these people every single day, I had a chance to learn a lot about how, like, human data is provisioned. They were building a lot of things for, like, Microsoft&#8217;s new projects and new products that are, like, the Copilot and the computers, so I had a chance to hear about those perspectives. Being in Seattle and meeting with interns and researchers in Seattle, I had a chance to learn not only about the farms perspective but also how other projects came to life. With our meetings, I realized how FarmVibes and FoodVibes were once just, like, a small idea and now are these huge projects. So having a chance to understand the history of all the things around me was a great way to see how <em>I</em> am able to build something for the future. And also maybe another thing is the collaboration that happens. As I said, I was, like, surrounded by a lot of socioeconomic people—that I am not really an expert in—but they were really great with providing me advice from an area I would never think of. So when I was speaking, like, from farms to someone who is in economics, they would raise another point on how this can hurt or help economy or to look at it from another side, which is all great perspectives to have when you&#8217;re creating a research from scratch. That&#8217;s why I really, like, referred to entrepreneurship previously because I feel like even though was an intern, I really had a chance to shape the project in a way that I would get all these insights from people and then obviously decide how to go further.</p>



<p><strong>RANGANATHAN:</strong> Yeah, and I want to add summer with interns is the highest energy level at MSR (Microsoft Research) and the best time of my year at Microsoft. So having successfully completed your internship, what advice do you have for our audience, you know, when it comes to applying for an internship like this in industry and then getting the most out of that experience?</p>



<p><strong>BUSHESKA:</strong> I would say one of the things that really maybe helped me through the process is, like, working obviously, in an area that I was really passionate about. But another thing, and especially this is coming specifically for the marginalized communities in tech—although I would say MSR is doing a great job in having this balance. But I&#8217;m coming from a liberal arts college where we have, like, two girls that are studying electrical engineering—so having this disbalance can in some ways, like, I don’t know, put you, “I&#8217;m not good enough” and all this imposter syndrome. So maybe one thing is definitely, like, reach out to people. And something that I really learned heavily from you is reach out. There is nothing you can lose. Most of the time, it is just … getting a no is maybe the hardest answer that you will get. So having a chance to ask is always a good idea. Obviously, don&#8217;t go to, like, every single executive to ask questions, even though it paid out—it paid out [LAUGHTER]—and just, like, ask for help from the people around you. Ask to learn. I realized, like, researchers <em>really</em> want to speak about what they have worked on previously. So whenever you get a chance to ask them about their past project, they&#8217;re super eager to tell you about like their grad days and how they have become what they are now. So that was also great. And in terms of applying, I think I would … when I applied for this, I was like, there&#8217;s no chance I will be able to get in. But for me, as an introvert turned into an extrovert—fun fact, I was a big introvert in the past who turned into an extrovert just because of my work—it&#8217;s like if you don&#8217;t apply, there&#8217;s a 100 percent chance that you&#8217;re not going to get in. So, like, when I interviewed, after my first interview, I was like, yeah, there is, like, no chance. Because, like, I presented the projects, but I was like, there are so many other students who I know that are working in these areas and then it happened and then I&#8217;m here recording this podcast. I remember it was Thanksgiving break when I applied for this internship. If I was like, no—if I self-rejected myself previously, yeah, this would never have happened. So I would say for all interns out there, just, like, don&#8217;t self-reject yourself. Let them reject you. [LAUGHTER] It&#8217;s my really advice.</p>



<p><strong>RANGANATHAN:</strong> And, yeah, have more confidence. I&#8217;m glad you applied and you joined us because this was a highlight of the summer for us, this was, right. So your work with Microsoft Research is one of your several projects. You&#8217;ve been involved with many other things, right. What else do you have going on? How does it contribute to the impact you&#8217;re looking to have at this point in your life?</p>



<p><strong>BUSHESKA:</strong> Yeah. So one thing that I realized in the past four years is that I started … so my, like, journey was very, I would say, interesting and not predictable just because when I, like, graduated from high school, I got trapped by all the COVID pandemic. I couldn&#8217;t fly into the US just because of all the visa regulations. It was like, OK, a gap year at home, which if you asked any of my friends before, I was never the person who would take a gap year. So staying that year really, like, without a school or official job was the chance for me to just look at and start my nonprofit, EnRoute. From then on, I worked with policy. We worked at the UN. We even went to Hollywood in March to speak about, like, fast fashion. So I realized there is no one way to make an impact. So I was really glad to join research, to join policy, and speak wherever I can. I think now I&#8217;m getting back into my senior year of college. Time flies. And something that I am, like, looking definitely into is how to continue my research in this area, where I most probably would love to be through grad school. Having a chance also to push further this project just because I think that speaking out and sharing research is one of the, like, greatest things we can do, like referring to all the other times when I was speaking on conferences. It will be magical for someone to respond with positive and negative feedback and maybe even can create a collaboration for us to move forward. So that is definitely something that I&#8217;m looking forward to. So definitely this area is something that I got in by mistake. I got in really just by … because I had that great time in my senior year to spend more time research, but it definitely something that marked my life at this point. And I think that, like, whatever route I pursue will be to fight climate change in any way or another.</p>



<p><strong>RANGANATHAN:</strong> I want to point out we are also planning to open source this and you hope to work with people, you know, to get it in the hands of people who need it and could benefit from it most, right.</p>



<p><strong>BUSHESKA:</strong> Absolutely.</p>



<p><strong>RANGANATHAN: </strong>So that&#8217;s a great direction. I think every year our undergraduate researchers set a new bar in my life expectations. And I thank you for that. This year is another bump up. So, yeah, you are a senior, Angela. What&#8217;s next for you, and what&#8217;s next for your research and work?</p>



<p><strong>BUSHESKA:</strong> Yeah, so this will be a fall really spent with a lot of applications because of, like, graduate school. Definitely something that I&#8217;m looking forward, especially graduate school in this area. So we kind of spent a lot of time living in an era of GPT wrappers, and I think when there is a product out there that is really based on research, it can make a tremendous impact. As I said, I am really a person who loves to experiment and do things. So something out there is definitely either, like, having a research that is spun out of this or having perhaps a startup down the line that is based in this area and helping millions of people around the world is definitely something I&#8217;m looking forward. Because I saw the impact when you&#8217;re helping other people, and I think that is the best thing that one person can do. So I was really excited to do it with all of you this summer and obviously continuing on to open source this project obviously for feedback but also for real use. And if we can help with something, that will be amazing. Hopefully getting these out to conferences around the world. And, yeah, see where it can take us.</p>



<p><strong>RANGANATHAN:</strong> Yeah, I have one last question for you. You had some interesting intern mentors, myself included, and then Bruno Silva, who sits in Brazil, Peder Olsen, and Solon Barocas. Do you have anything to say to all of them? [LAUGHS]</p>



<p><strong>BUSHESKA:</strong> Yeah, I would say a big thank-you for keeping up with my questions. As all the listeners can probably see, I&#8217;m a person who really speaks a lot, even more than needed sometimes. So I would say I had a great balance because I was in a time zone that was between Brazil and Redmond. So I would spend a lot of my mornings, especially in the beginning—huge shoutout to Bruno for dealing with all of my technical questions [LAUGHS]. It was my first time setting up a lot of research clusters. We should not forget also the people at Microsoft who are dealing with all of the GPUs and stuff because I think that managing all of these researchers for all the people out there is a hard job. So having a chance to communicate and also understand. Even though I study electrical engineering, I never had a chance to deeply understand how they&#8217;re used in real life. Speaking also with people from all different walks of life because Peder, my other mentor, was someone who has been really experienced in this area, so he would reference projects from I don&#8217;t know how many years, and it was, like, so fun to see the history, to see his previous experiences come to life. You don&#8217;t every single day get to have like three or four mentors from all different areas and aspects. So I&#8217;m really grateful that I had a chance to talk and also get called when things were not right so we can make them better and more correct for the people that will use this project.</p>



<p><strong>RANGANATHAN:</strong> I&#8217;ll tell them you said thank you. So, Angela, I think we are coming to a close here. Thank you so much for taking the time to share your experience with us. It&#8217;s been a pleasure to work with you, and I really look forward to seeing what the future holds for you and what amazing things you&#8217;re going to achieve. So thank you. Closing out.</p>



<p>[MUSIC]&nbsp;</p>



<p><strong>BUSHESKA:</strong> Thank you so much. Really appreciate to have this opportunity. And for everyone that is, like, working in this area, for every future intern that is doubting themselves, feel free to find me out there and, like, ask. Always happy to help people, always happy to chat sustainability with people. So thank you so much.</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button">
				Show more			</button>
		</div>
	</div>
</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p><a href="#_ftnref1" id="_ftn1">[1]<span class="sr-only"> (opens in new tab)</span></a> For more information, see <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://environment.ec.europa.eu/topics/forests/deforestation/regulation-deforestation-free-products_en" rel="noreferrer noopener" target="_blank">&#8220;Regulation on Deforestation-free products&#8221; on the European Commission website<span class="sr-only"> (opens in new tab)</span></a>.</p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/intern-insights-vaishnavi-ranganathan-with-angela-busheska/">Intern Insights: Vaishnavi Ranganathan with Angela Busheska</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 15:14:56 +0000</pubDate>
<pubDate>Thu, 24 Oct 2024 15:14:56 +0000</pubDate>
</item>
<item>
<title>Research Focus: Week of October 7, 2024</title>
<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-7-2024/</link>
<guid>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-7-2024/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-pullquote"><blockquote><p>Welcome to Research Focus, a series of blog posts that highlights notable publications, events, code/datasets, new hires and other milestones from across the research community at Microsoft.</p></blockquote></figure>



<figure class="wp-block-image aligncenter size-full"><img alt="Research Focus | October 7, 2024" class="wp-image-1089057" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF51-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Securely Training Decision Trees Efficiently</h2>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/securely-training-decision-trees-efficiently/" rel="noreferrer noopener" target="_blank">Securely Training Decision Trees Efficiently</a> that will appear at ACM CCS 2024, researchers from Microsoft significantly reduce the communication complexity of secure decision tree training. Decision trees are an important class of supervised learning algorithms. In this approach, a classification or regression tree is built based on a set of features or attributes present in the training dataset. As with many learning algorithms, the accuracy of decision trees can be greatly improved with larger volumes of data. However, this can be a challenge, since data may come from multiple independent sources and require attention to data privacy concerns. In this case, the use of a privacy-enhancing technology, such as secure multi-party computation (MPC), can help protect the underlying training data. &nbsp;</p>



<p>When the number of elements in the dataset is 𝑁, the number of attributes is 𝑚 and the height of the tree to be built is ℎ, the researchers construct a protocol with communication complexity O(𝑚𝑁 log 𝑁 + ℎ𝑚𝑁 + ℎ𝑁 log 𝑁 ), thereby achieving an improvement of ≈ min(ℎ, 𝑚, log 𝑁 ) over the previous state of the art. The essential feature is an improved protocol to regroup sorted private elements further into additional groups (according to a flag vector) while maintaining their relative ordering. Implementing this protocol in the MP-SPDZ framework shows that it requires 10× lesser communication and is 9× faster than existing approaches.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-1 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--a28a27984e036278ebc05191119983a1"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/securely-training-decision-trees-efficiently/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Multi-label audio classification with a noisy zero-shot teacher</h2>



<p>Improving the real-world accuracy of audio content detection (ACD) is an important problem for streaming platforms, operating systems and playback devices. It’s similar to audio tagging, i.e., labeling sounds present in a given audio segment of several seconds length or longer. However, ACD may consist of a small number of higher-level labels or super-classes, e.g. speech, music, traffic, machines, animals, etc., where each label can include a multitude of specific sounds.</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/multi-label-audio-classification-with-a-noisy-zero-shot-teacher/" rel="noreferrer noopener" target="_blank">Multi-label audio classification with a noisy zero-shot teacher</a>, researchers from Microsoft propose a novel training scheme using self-label correction and data augmentation methods to deal with noisy labels and improve real-world accuracy on a polyphonic audio content detection task. The augmentation method reduces label noise by mixing multiple audio clips and joining their labels, while being compatible with multiple active labels. The researchers show that performance can be improved by a self-label correction method using the same pretrained model. They also show that it is feasible to use a strong zero-shot model such as CLAP to generate labels for unlabeled data and improve the results using the proposed training and label enhancement methods. The resulting model performs similar to CLAP while providing an efficient mobile device friendly architecture which can be quickly adapted to unlabeled sound classes. </p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-2 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--29b4c779811ee36404edbdf8b92745f4"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/multi-label-audio-classification-with-a-noisy-zero-shot-teacher/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Tabularis Revilio: Converting Text to Tables</h2>



<p>Tables are commonly used to store and present data. These tables are often moved as free-form text when copied from documents and applications without proper tabular support like PDF documents, web pages, or images. Users are dependent on manual effort or programming abilities to parse this free-form text back into structured tables.</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/tabularis-revilio-converting-text-to-tables/" rel="noreferrer noopener" target="_blank">Tabularis Revilio: Converting Text to Tables</a>, researchers from Microsoft present a novel neurosymbolic system for reconstructing tables when their column boundaries have been lost. Revilio addresses this task by detecting headers, generating an initial table sketch using a large language model (LLM), and using that sketch as a guiding representation during an enumerate-and-test strategy that evaluates syntactic and semantic table structures. Revilio was evaluated on a diverse set of datasets, demonstrating significant improvements over existing table parsing methods. Revilio outperforms traditional techniques in both accuracy and scalability, handling large tables with over 100,000 rows. The researchers’ experiments using publicly available datasets show an increase in reconstruction accuracy by 5.8–11.3% over both neural and symbolic baseline state-of-the-art systems.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-3 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--902a546be15a65958650e2c444046582"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/tabularis-revilio-converting-text-to-tables/">Read the paper</a></div>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Event Series</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" target="_blank">
					<img alt="various abstract 3D shapes on a light blue background" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/01/MRF-24_WebImage_1400x788.png" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum</h2>
				
								<p class="large">Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a class="btn btn-brand glyph-append glyph-append-chevron-right" href="https://researchforum.microsoft.com/?OCID=msr_researchforum_MCR_Blog_Promo" target="_blank">
							Register for series						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span class="sr-only" id="label-external-link">Opens in a new tab</span>	</div><!--/.msr-promo-->
	</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Confidential Container Groups: Implementing Confidential Computing on Azure Container Instances</h2>



<p>Container-based technologies empower cloud tenants to develop highly portable software and deploy services in the cloud at a rapid pace. Cloud privacy, meanwhile, is important as a large number of container deployments operate on privacy-sensitive data, but challenging due to the increasing frequency and sophistication of attacks. State-of-the-art confidential container-based designs leverage process-based trusted execution environments (TEEs), but face security and compatibility issues that limit their practical deployment.</p>



<p>In a recent article in <em>Communications of the ACM</em>: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cacm.acm.org/practice/confidential-container-groups/" rel="noreferrer noopener" target="_blank">Confidential Container Groups: Implementing Confidential Computing on Azure Container Instances<span class="sr-only"> (opens in new tab)</span></a>, researchers from Microsoft with external colleagues present the Parma architecture, which provides lift-and-shift deployment of unmodified containers while providing strong security protection against a powerful attacker who controls the untrusted host and hypervisor. Parma leverages VM-level isolation to execute a container group within a unique VM-based TEE. Besides container integrity and user data confidentiality and integrity, Parma also offers container attestation and execution integrity based on an attested execution policy. This policy, which is specified by the customer, delimits the actions that the cloud service provider is allowed to take on their behalf when managing the container group. </p>



<p>The result is that customers receive the security protections of TEEs for their container workloads with minimal costs to perfromance. To learn more, check out <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://learn.microsoft.com/en-us/azure/container-instances/container-instances-confidential-overview" rel="noreferrer noopener" target="_blank">Confidential Containers on Azure Container Instances<span class="sr-only"> (opens in new tab)</span></a>, which is based on Microsoft’s Parma architecture.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--7569fbe7e93f6805e979617de8526d87"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/confidential-container-groups/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-67f03dda6c961ae118dd159cd334a203" id="new-research">NEW VIDEO SERIES</h2>



<h2 class="wp-block-heading" id="heading">AI for Business Transformation with Peter Lee and Vijay Mital</h2>



<p>Generative AI is changing how businesses operate and how stakeholders talk to each other. The building blocks for large scale AI transformation are now in place, but we are only beginning to imagine how it will unfold. Learn what Microsoft research leaders discovered from some early AI innovation in healthcare, and how businesses can prepare for what’s ahead.</p>



<p>In this new three-part video series, Microsoft Research President Peter Lee and Corporate Vice President Vijay Mital discuss how Microsoft is helping businesses navigate this transformation, along with the critical role of data and how emerging multimodal AI models could turbocharge business innovation.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--b294bcefe3f8699fd86cc98f9120f747"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/video/ai-for-business-transformation-lessons-from-healthcare/" rel="noreferrer noopener" target="_blank">Watch the video</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-october-7-2024/">Research Focus: Week of October 7, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 16:00:00 +0000</pubDate>
<pubDate>Wed, 09 Oct 2024 16:00:00 +0000</pubDate>
</item>
<item>
<title>Data Formulator: Exploring how AI can help analysts create rich data visualizations</title>
<link>https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/</link>
<guid>https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-image size-full"><img alt="white outline icons (representing AI and human computer interaction) on a blue to purple to pink gradient background." class="wp-image-1087785" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Data-Formulator-2-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<p>Transforming raw data into meaningful visuals, such as charts, is key to uncovering hidden trends and valuable insights, but even with advances in AI-powered tools, this process remains complex. Integrating AI into the iterative nature of the data visualization process is particularly challenging, as data analysts often struggle to describe complicated tasks in a single text prompt while lacking the direct control of traditional tools. This highlights the need for smarter, more intuitive solutions that combine AI’s precision with the flexibility of hands-on methods.</p>



<div class="annotations ">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Publication</span>
			<a class="annotations__link font-weight-semibold text-decoration-none" href="https://www.microsoft.com/en-us/research/publication/data-formulator-2-iteratively-creating-rich-visualizations-with-ai/" target="_self">
				Data Formulator 2: Iteratively Creating Rich Visualizations with AI&nbsp;<span class="glyph-append glyph-append-chevron-right glyph-append-xsmall"></span>
			</a>
					</li>
	</ul>
</div>



<p>To address this, we’re excited to release <a href="https://www.microsoft.com/en-us/research/publication/data-formulator-2-iteratively-creating-rich-visualizations-with-ai/" rel="noreferrer noopener" target="_blank">Data Formulator</a> as an <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/data-formulator" rel="noreferrer noopener" target="_blank">open-source</a> research project. This update builds on <a href="https://www.microsoft.com/en-us/research/blog/data-formulator-a-concept-driven-ai-powered-approach-to-data-visualization/" rel="noreferrer noopener" target="_blank">last year’s release</a> by combining user interface (UI) interactions for designing charts with natural language input for refining details. Unlike the previous version, which required users to choose between two methods, this unified approach allows them to iteratively solve complex tasks and with less effort.</p>



<div class="annotations ">
	<ul class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<li class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a class="annotations__link font-weight-semibold text-decoration-none" href="https://github.com/microsoft/data-formulator" target="_self">
				Data Formulator&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
							<p class="annotations__caption text-neutral-400 mt-2">Transform data and create rich visualizations iteratively with AI.</p>
					</li>
	</ul>
</div>



<figure class="wp-block-image aligncenter size-full"><img alt="Figure 1: This figure shows the user interface of Data Formulator. There are four callouts in the figure highlighting key components of the user interface. The first call out describes “1. Concept Encoding Shelf: specify charts with field encodings and NL instructions”. The second callout describes “2. (Local) Data Threads: backtrack and revise inputs”. The third describes “3. Data Threads: navigate data derivation history”. The fourth callout contains “4. Data View: inspect original and derived data”. The user interface contains a visualization in the center that shows renewable percentage. " class="wp-image-1083870" height="1402" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Figure-1-Data-Formulator-UI.png" width="2796" /><figcaption class="wp-element-caption">Figure 1. Data Formulator’s UI</figcaption></figure>



<h2 class="wp-block-heading" id="creating-and-refining-charts-with-the-concept-encoding-shelf-and-data-threads">Creating and refining charts with the Concept Encoding Shelf and data threads</h2>



<p>With Data Formulator, data analysts can now create charts from scratch or select from existing designs through <em>data threads</em>. The UI features a pane called the “Concept Encoding Shelf,” where users can build their chart by dragging various data fields into it and defining them or by creating new ones. A large language model (LLM) on the backend processes this input, generating the necessary code to produce the visual and updating the data threads for future use. This process is illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img alt="Figure 2: This figure shows the user experience workflow in Data Formulator. On the left it shows Data Threads, and the user clicks a line chart that visualizes the renewable percentage of 20 countries and expands it in the main panel. In the middle it shows “Concept Encoding Shelf”, and the user provides an instruction “Show only top 5 CO2 emission countries”. On the right it shows the result produced from running the user instruction with AI: the result is a table with three columns “Year” “Entity” “Renewable Percentage” and int contains only top 5 CO2 countries’ values; a line chart that only contains these five countries trends is also generated. The line chart is added to data threads. " class="wp-image-1083879" height="838" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Figure-2-Data-Formulator-flow.png" width="2346" /><figcaption class="wp-element-caption">Figure 2. To create a new chart, users can select a previously created chart from the data threads and then use a combination of UI elements and language to describe their intent.</figcaption></figure>



<p>Data threads enable users to review and modify charts they created previously. This iterative process streamlines the editing and refinement process, as the LLM adapts past code to new contexts. Without this feature, users would need to provide more detailed prompts to recreate designs from scratch. This iterative mechanism also allows users to continue updating their charts until they’re satisfied.</p>



<figure class="wp-block-image aligncenter size-full"><img alt="Figure 3: This figure illustrates how Data Formulator’s data threads work. On the left side, it shows two data threads, one is the derivation process of electricity produced from each energy source from each country from 2000 to 2020, the other is the thread showing that the user derives the renewable percentage of each country per year followed by a line chart that shows the rankings of these countries. The figure illustrates that each of the plots is backed by a python data transformation code to derive data appropriate to the user instruction. On the right it shows actions users can take in local data threads: (a) the user can click and rerun a previous instruction, (b) the user can provide a new instruction to follow up, (c) the user can click the previous card and revise instruction and rerun. " class="wp-image-1083891" height="1240" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Figure-3-Data-Threads.png" width="2790" /><figcaption class="wp-element-caption">Figure 3: Data Formulator’s data threads support complex navigation, quick editing, and the rerunning of previous instructions.&nbsp;</figcaption></figure>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<h2 class="wp-block-heading" id="data-formulator-s-framework">Data Formulator’s framework</h2>



<p>Data Formulator’s architecture separates data transformation from chart configuration, improving both the user experience and AI performance. Upon receiving user specifications, the system follows a three-step process: (1) it generates a Vega-Lite script, which defines how data is visualized; (2) it instructs the AI to handle data transformation; and (3) it creates the chart using the converted data, as illustrated in Figure 4.</p>



<figure class="wp-block-image aligncenter size-full"><img alt="Figure 4: This figure shows data formulator architecture. The left side shows user’s chart specification with Year on x-axis, rank on y-axis, Entity on color with instruction “rank by renewable percentage”. In the first step, Data Formulator generates a Vega-Lite line chart template with field names. In step 2, Data Formulator compiles a prompt containing “system prompt”, “Context (data fields + sample data + dialog history)” and “Goal (user instruction + expected fields)”, and AI takes this prompt to generate a python code to transform the data. In step 3, Data Formulator combines the data and the Vega-Lite spec to create a line chart that shows ranking of the countries from 2000 to 2020. " class="wp-image-1083897" height="1086" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Figure-4-Data-Formulator-Architecture.png" width="2904" /><figcaption class="wp-element-caption">Figure 4: Behind the scenes, Data Formulator compiles a Vega-Lite script from the Concept Encoding Shelf (1), prompts the LLM to generate the necessary code for preparation (2), and, upon creating new data, creates the chart (3).</figcaption></figure>



<h2 class="wp-block-heading" id="implications-and-looking-forward">Implications and looking forward</h2>



<p>Refining how users interact with AI-powered tools is essential for improving how they communicate their requirements, paving the way for more efficient and effective collaboration. By integrating UI elements and natural language input, we designed Data Formulator to let users to define their visualization needs with precision, leading to better results and reducing the need for multiple clarifications.</p>



<p>While Data Formulator addresses some challenges in data transformation and visualization authoring, others remain. For example, how can AI assist in cleaning unstructured data without losing critical information? And how can it help users define clear data analysis goals when starting with ambiguous or undefined objectives? We’re actively investigating these research questions and invite you to contribute by building on the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/data-formulator" rel="noreferrer noopener" target="_blank">Data Formulator codebase<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>Learn more about our research efforts on human-AI interaction by exploring how we design <a href="https://www.microsoft.com/en-us/research/publication/dynavis-dynamically-synthesized-ui-widgets-for-visualization-editing/" rel="noreferrer noopener" target="_blank">dynamic UI widgets<span class="sr-only"> (opens in new tab)</span></a> for visualization editing. You can also view a demo of the Data Formulator project on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://codespaces.new/microsoft/data-formulator?quickstart=1" rel="noreferrer noopener" target="_blank">GitHub Codespace<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We’d like to thank Bongshin Lee, John Thompson, and <a href="https://www.microsoft.com/en-us/research/people/goramos/">Gonzalo Ramos</a> for their feedback and contributions to this project.&nbsp;</p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/data-formulator-exploring-how-ai-can-help-analysts-create-rich-data-visualizations/">Data Formulator: Exploring how AI can help analysts create rich data visualizations </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 16:00:00 +0000</pubDate>
<pubDate>Tue, 01 Oct 2024 16:00:00 +0000</pubDate>
</item>
<item>
<title>Stress-testing biomedical vision models with RadEdit: A synthetic data approach for robust model deployment</title>
<link>https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/</link>
<guid>https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/</guid>
<content:encoded><![CDATA[

  <p class="has-text-align-center"><strong><em>This paper has been accepted at the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eccv2024.ecva.net/">18th European Conference on Computer Vision (ECCV 2024)<span class="sr-only"> (opens in new tab)</span></a>, the premier gathering on computer vision and machine learning.</em></strong></p>



<figure class="wp-block-image size-full"><img alt=" On the left is a simple drawing of the lungs. The drawing shows the borders of the left and right lung as well as the trachea and the left and right main stem bronchi. The text under the drawing reads: Original image. To the right of the drawing are the 3 additional inputs of RadEdit. They are arranged vertically. On top there is an example editing prompt. It reads " height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-Socials-2024-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<p>Biomedical vision models are computational tools that analyze medical images, like X-rays, MRIs, and CT scans, and are used to predict medical conditions and outcomes. These models assist medical practitioners in disease diagnosis, treatment planning, disease monitoring, and risk assessment. However, datasets used to train these models can be small and not representative of real-world conditions, which often leads to these models performing worse in actual medical settings. To avoid misdiagnoses and other errors, these models must be rigorously tested and adjusted to perform reliably across different conditions.</p>



<p>To mitigate the dataset challenge of not having enough diverse data and to improve the testing of biomedical vision models, we developed “<a href="https://www.microsoft.com/en-us/research/publication/radedit-stress-testing-biomedical-vision-models-via-diffusion-image-editing/">RadEdit: Stress-testing biomedical vision models via diffusion image editing</a>,” presented at ECCV 2024. Aligned with the <a href="https://www.microsoft.com/en-us/ai/principles-and-approach/?msockid=37f5423dfa656a721ad151fefb9e6b78">Microsoft Responsible AI principles</a> of reliability and safety, RadEdit helps researchers identify when and how models might fail before they are deployed in a medical setting. RadEdit uses generative image editing to simulate different dataset shifts (e.g., a shift in the patients&#8217; demographics), helping researchers to identify weaknesses in the model. By employing text-to-image diffusion models trained on a wide array of chest X-ray datasets, RadEdit can generate synthetic yet realistic X-rays.</p>



<p>RadEdit’s approach involves using multiple image masks (binary images representing designated regions of a reference image), as illustrated in Figure 1, to limit changes to specific areas of the image, therefore preserving their integrity. It generates synthetic datasets free from spurious correlations and artifacts, addressing shortcomings in existing editing techniques. Traditional editing techniques often overlook biases within the generative model, leading to synthetic data that perpetuate these biases. Alternatively, these other editing techniques restrict edits to the point of unrealistic outputs.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Blog post</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" target="_blank">
					<img alt="Research Focus | September 9, 2024" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF49-BlogHeroFeature-1400x788-1.jpg" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Research Focus: Week of September 9, 2024</h2>
				
								<p class="large">Investigating vulnerabilities in LLMs; A novel total-duration-aware (TDA) duration model for text-to-speech (TTS); Generative expert metric system through iterative prompt priming; Integrity protection in 5G fronthaul networks.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a class="btn btn-brand glyph-append glyph-append-chevron-right" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span class="sr-only" id="label-external-link">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="how-radedit-works">How RadEdit works</h2>



<p>RadEdit improves biomedical image editing using three key inputs, as illustrated in Figure 1:</p>



<ul class="wp-block-list">
<li><strong>Text prompt</strong>: Defines the desired modifications. For example, a disease can be added with a description like “Consolidation”</li>



<li><strong>Edit mask</strong>: A binary mask indicating the main area to be modified, such as the “right lung”</li>



<li><strong>Keep mask</strong>: A binary mask outlining parts of the original image to be preserved, like the “left lung”</li>
</ul>



<figure class="wp-block-image aligncenter size-full"><img alt=" On the left is a simple drawing of the lungs. The drawing shows the borders of the left and right lung as well as the trachea and the left and right main stem bronchi. The text under the drawing reads: Original image. To the right of the drawing are the 3 additional inputs of RadEdit. They are arranged vertically. On top there is an example editing prompt. It reads " height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RadEdit-2024-Consolidation-BlogHeroFeature.jpg" width="1400" /><figcaption class="wp-element-caption">Figure 1: RadEdit’s inputs and outputs. By using separate “edit” and “keep” masks, RadEdit can make the desired modifications to an image with precise spatial control and realistic output.</figcaption></figure>



<p>RadEdit depends on a diffusion model for image editing, where the image is first converted to a latent noise representation by inverting the diffusion generative process. The noise representation is then iteratively denoised over multiple time steps. During each step, RadEdit:</p>



<ol class="wp-block-list">
<li>Uses the text prompt to conditionally generate pixels within the edit mask with classifier-free guidance.</li>



<li>Generates the remaining pixels based on the original image and edited area.</li>



<li>Replicates the content of the original image within the “keep” mask, ensuring that this area remains unaltered.</li>
</ol>



<p>Finally, a quality check ensures that the edited image is faithful to the editing prompt. RadEdit uses Microsoft&#8217;s <a href="https://www.microsoft.com/en-us/research/publication/learning-to-exploit-temporal-structure-for-biomedical-vision-language-processing/?msockid=2d32e418b7106f871a12f664b6ba6e2e">BioViL-T</a> to compute an image-text alignment score that we can then use to filter out low-quality and unfaithful edits.</p>



<h2 class="wp-block-heading" id="simulating-dataset-shifts">Simulating dataset shifts</h2>



<p>A key feature of RadEdit is its ability to simulate dataset shifts with precise spatial control for comprehensive model performance evaluation. This includes differences in image acquisition, the appearance of underlying pathologies, and population characteristics.</p>



<p>Particularly notable is RadEdit&#8217;s ability to simulate image variations from different sources (e.g., different hospitals), helping researchers identify potential biases in models trained solely on data from one source. For example, in a COVID-19 study, if all positive cases in a dataset come from a single hospital and all negative cases come from a different hospital, a model trained on detecting COVID-19 might over-rely on hospital-specific indicators from the X-ray images. Among others, we considered the laterality markers in the corners of an X-ray (e.g., a highly visible letter &#8220;L&#8221; on the left side of the X-ray) as well as the amount of black space on the image edges to be hospital-specific indicators. To test if a model relies too much on differences in image acquisition, we created synthetic data using RadEdit, where we removed COVID-19 features while retaining hospital-specific indicators. After creating the synthetic dataset with the COVID-19 features no longer present, we can test if the COVID-10 detection model still predicts COVID-19. This would indicate that the model is biased with respect to hospital-specific indicators.</p>



<p>RadEdit can also remove specific diseases, like pneumothorax (collapsed lung), from an image while keeping treatment features like chest drains. This helps researchers understand how models detect and understand “visual shortcuts.” Because RadEdit maintains the size and location of the main anatomical structures (like lungs, ribs, and heart), it can also be used to stress-test segmentation models. For example, RadEdit can add rare abnormalities or medical devices to lung images to test how well segmentation models handle new variations, ensuring they generalize accurately across different populations. Figure 2 illustrates these three examples of stress-testing scenarios.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1195" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24.jpg" alt="All drawings of lungs are the same as in Figure 1. The drawing shows the borders of the left and right lung as well as the trachea and the left and right main stem bronchi. In the first row on the left there are two drawings of a lung. The first drawing of a lung labelled "COVID-19" has an arrow pointing upwards in the upper right corner of the drawing, outside of the lung. The arrow is meant to indicate a marking on an X-ray. In addition, the lungs are shaded light grey. The light grey shading is meant to represent the effects of covid on the lung. The second drawing of a lung on the right labelled "No COVID-19" has no arrow marking and the lungs are shaded in a dark grey. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "acquisition shift". On the right end of the arrow is another drawing of a lung. The lungs in the drawing are shaded dark grey, in addition the drawing features an arrow marking in the upper right corner. The drawing is labelled "No COVID-19", next to the drawing is a smaller arrow pointing to the right. At the right end of the arrow there is a text reading "False positive". 
 
In the second row on the left there are again two drawings of a lung. The first drawing of a lung labelled "PTX" has a tube in the left lung indicated by a white line. In addition, the top of the left lung is black indicating a pneumothorax. The second drawing of a lung on the right labelled "No PTX" has no tube and pneumothorax. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "manifestation shift". On the right end of the arrow is another drawing of a lung. The left lung in the drawing features a tube but no pneumothorax. The drawing is labelled "No PTX", next to the drawing is a smaller arrow pointing to the right. At the right end of the arrow there is a text reading "False positive". 

In the third row on the left there are again two drawings of a lung. The first drawing of a lung labelled "Healthy" without any changes. The second drawing of a lung on the right labelled "Lung segmentation" shows a segmentation mask overlayed with the left and right lung in green. On top of these two drawings the text says, "Biased training datasets". Right next to the two drawings of lungs is a big arrow pointing to the right. The arrow is labelled "population shift". On the right end of the arrow is another drawing of a lung. The right lung in the drawing features a pacemaker in the upper right lung lobe. The drawing is labelled "Abnormality". On the right is another drawing of a lung. The lung shows the pacemaker in the same location. The lungs in the drawing are overlayed with a segmentation mask of the lungs. However, the pacemaker is not included in the segmentation mask. " class="wp-image-1088418" srcset="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24.jpg 1195w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-300x198.jpg 300w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-1024x675.jpg 1024w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-768x506.jpg 768w, https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEW_RadEdit-2024-Figure-1-BlogHeroFeature-1195x788-15Aug24-240x158.jpg 240w" sizes="(max-width: 1195px) 100vw, 1195px" /><figcaption class="wp-element-caption">Figure 2: Stress-testing models by simulating dataset shifts via image editing.</figcaption></figure>



<h2 class="wp-block-heading" id="stress-testing-multimodal-models">Stress-testing multimodal models</h2>



<p>We have used RadEdit to stress-test image classification and segmentation models, and we see potential for future applications in complex multimodal tasks like generating radiology reports. RadEdit can help identify limitations in multimodal large language models (MLLMs) like Microsoft&#8217;s <a href="https://www.microsoft.com/en-us/research/publication/maira-1-a-specialised-large-multimodal-model-for-radiology-report-generation/">MAIRA-1</a> and <a href="https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/">MAIRA-2</a>, especially when dealing with rare conditions or unusual combinations of findings not well-represented in the training data. These MLLMs take one or more radiological images and relevant clinical information as input to produce detailed text reports.</p>



<p>RadEdit can generate synthetic image-report pairs for challenging scenarios. For example, manually editing a report to describe a rare combination of findings and then using RadEdit to edit the corresponding image, creates a valuable test case for the MLLM. This approach allows us to stress-test MLLM with diverse synthetic data, identifying weaknesses or biases and ensuring the model is more robust in real-world scenarios. This is a crucial step for using these models safely and effectively in clinical settings.</p>



<h2 class="wp-block-heading" id="implications-and-looking-forward">Implications and looking forward</h2>



<p>RadEdit offers significant advantages for the biomedical research community. It helps identify biases and blind spots before deployment, helping to ensure that biomedical vision models perform reliably in clinical settings. By simulating dataset shifts, RadEdit reduces the need to collect additional evaluation data, saving time and resources.</p>



<p>RadEdit is applicable to a wide range of settings and can be used to stress-test state-of-the-art foundation models like Microsoft&#8217;s <a href="https://www.microsoft.com/en-us/research/publication/rad-dino-exploring-scalable-medical-image-encoders-beyond-text-supervision/">Rad-DINO<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/BiomedParse/">BiomedParse<span class="sr-only"> (opens in new tab)</span></a>. By integrating RadEdit into their research workflow, researchers can validate that their biomedical vision models are not only state-of-the-art but also more prepared for the complexities of real-world deployment. In the future, we envision RadEdit being applied to more complex multimodal tasks, such as generating radiology reports.</p>



<p>The code for RadEdit as well as the weights of the diffusion model we used can be found under <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/microsoft/radedit">https://huggingface.co/microsoft/radedit<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="acknowledgments">Acknowledgments</h2>



<p>We would like to thank our paper coauthors: <a href="https://www.microsoft.com/en-us/research/people/fperezgarcia/">Fernando Pérez-García</a>, <a href="https://www.microsoft.com/en-us/research/people/sbondtaylor/">Sam Bond-Taylor</a>, Pedro P. Sanchez, Boris van Breugel, <a href="https://www.microsoft.com/en-us/research/people/harssharma/">Harshita Sharma</a>, <a href="https://www.microsoft.com/en-us/research/people/vsalvatelli/">Valentina Salvatelli</a>, Maria T. A. Wetscherek, <a href="https://www.microsoft.com/en-us/research/people/hamurfet/">Hannah Richardson</a>, <a href="https://www.microsoft.com/en-us/research/people/mlungren/">Matthew P. Lungren</a>, <a href="https://www.microsoft.com/en-us/research/people/adityan/">Aditya Nori,</a> and Ozan Oktay, as well as all our collaborators across <a href="https://www.microsoft.com/en-us/industry/health/microsoft-cloud-for-healthcare?msockid=191ee027a78e6e430e59f339a60a6f7a">Microsoft Cloud for Healthcare</a> and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-health-futures/">Microsoft Health Futures</a>.</p>



<p><em>RadEdit is intended for research purposes only and not for any commercial or clinical use.</em></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/stress-testing-biomedical-vision-models-with-radedit-a-synthetic-data-approach-for-robust-model-deployment/">Stress-testing biomedical vision models with RadEdit: A synthetic data approach for robust model deployment</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 19:00:00 +0000</pubDate>
<pubDate>Mon, 30 Sep 2024 19:00:00 +0000</pubDate>
</item>
<item>
<title>Abstracts: September 30, 2024</title>
<link>https://www.microsoft.com/en-us/research/podcast/abstracts-september-30-2024/</link>
<guid>https://www.microsoft.com/en-us/research/podcast/abstracts-september-30-2024/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-image aligncenter size-full"><img alt="Outline illustration of Daniela Massiceti next to Martin Grayson" class="wp-image-1091709" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Daniela-and-Martin_Abstracts_Hero_Feature_No_Text_1400x788.jpg" width="1400" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields.  <em>Abstracts</em> brings its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&nbsp;</p>



<p>In this episode, Senior Researcher <a href="https://www.microsoft.com/en-us/research/people/dmassiceti/">Daniela Massiceti<span class="sr-only"> (opens in new tab)</span></a> and Principal Research Software Development Engineer <a href="https://www.microsoft.com/en-us/research/people/mgrayson/">Martin Grayson<span class="sr-only"> (opens in new tab)</span></a> join host Amber Tingle to discuss the research project and AI-powered tool Find My Things. Find My Things is a personalizable object recognizer that people who are blind or have low vision can train to find personal items from just a few videos of those objects. It was recently recognized as a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fastcompany.com/innovation-by-design/list" rel="noreferrer noopener" target="_blank">2024 Innovation by Design Awards<span class="sr-only"> (opens in new tab)</span></a> finalist in the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fastcompany.com/91128700/accessible-design-innovation-by-design-2024" rel="noreferrer noopener" target="_blank">accessibility design<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fastcompany.com/91129207/artificial-intelligence-innovation-by-design-2024" rel="noreferrer noopener" target="_blank">AI categories<span class="sr-only"> (opens in new tab)</span></a> by the US-based business media brand Fast Company and, earlier this year, became <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.seeingai.com/" rel="noreferrer noopener" target="_blank">available as a feature in the Seeing AI app<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<p><em>The Find My Things story is an example of research at Microsoft enhancing Microsoft products and services. To try the Find My Things tool, </em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.seeingai.com/" rel="noreferrer noopener" target="_blank"><em>download the free, publicly available Seeing AI app</em><span class="sr-only"> (opens in new tab)</span></a><em>.</em></p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a class="wp-block-button__link wp-element-button" href="https://www.seeingai.com/" rel="noreferrer noopener" target="_blank">Try Find My Things</a></div>
</div>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity" />



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/find-my-things-personalized-accessibility-through-teachable-ai-for-people-who-are-blind-or-low-vision/">Find My Things: Personalized Accessibility through Teachable AI for People who are Blind or Low Vision</a><br />Publication, May 2024</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/understanding-personalized-accessibility-through-teachable-ai-designing-and-evaluating-find-my-things-for-people-who-are-blind-or-low-vision/">Understanding Personalized Accessibility through Teachable AI: Designing and Evaluating Find My Things for People who are Blind or Low Vision</a><br />Publication, October 2023</li>



<li><a href="https://www.microsoft.com/en-us/research/project/taix/">Teachable AI Experiences (Tai X)</a><br />Project page</li>



<li><a href="https://www.microsoft.com/en-us/research/podcast/collaborators-teachable-ai-with-cecily-morrison-and-karolina-pakenaite/" rel="noreferrer noopener" target="_blank">Collaborators: Teachable AI with Cecily Morrison and Karolina Pakėnaitė</a><br />Microsoft Research Podcast, December 2023</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/orbit-dataset/" rel="noreferrer noopener" target="_blank">ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition</a><br />Publication, October 2021</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/peoplelens/" rel="noreferrer noopener" target="_blank">PeopleLens</a><br />Publication, June 2021</li>
</ul>
</div>



<div class="wp-block-spacer" style="height: 25px;"></div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="black" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" rel="noreferrer noopener" target="_blank">
						<svg class="subscribe-to-podcast__svg" fill="none" viewBox="0 0 32 32" xmlns="http://www.w3.org/2000/svg"><path d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z" fill="currentColor"></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]&nbsp;</p>



<p><strong>AMBER TINGLE: </strong>Welcome to <em>Abstracts</em>, a Microsoft Research Podcast that puts the spotlight on world-class research <em>in brief</em>. I&#8217;m Amber Tingle. In this series, members of the research community at Microsoft offer a quick snapshot—or a <em>podcast abstract</em>—of their new and noteworthy papers and achievements.&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>



<p>Our guests today are Daniela Massiceti and Martin Grayson. Daniela is a senior researcher at Microsoft, and Martin is a software development engineer with the company. They are members of a team creating technology that can be personalized to meet individual needs. Their research project called Find My Things enables people who are blind or have low vision to train an AI system to find their personal items based on a few examples of the objects. Find My Things has now shipped as a new feature within Seeing AI, which is a free app that narrates a person&#8217;s surroundings, including nearby people, text, and objects. The team was also recently recognized by the US-based business media brand Fast Company as an Innovation by Design Awards finalist in both the accessible design and artificial intelligence categories. Daniela and Martin, congratulations and thank you so much for joining us today for <em>Abstracts</em>.&nbsp;</p>



				</span>
				<span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-2">
					



<p><strong>MARTIN GRAYSON: </strong>Pleasure, thank you.&nbsp;</p>



<p><strong>DANIELA MASSICETI:</strong> Thanks very much, Amber. Nice to be here.&nbsp;</p>



<p><strong>TINGLE:</strong> So, Daniela, let&#8217;s start with a Find My Things overview. What is it, how does it work, and who&#8217;s it for?&nbsp;</p>



<p><strong>MASSICETI:</strong> I think the best way I can describe Find My Things is a personalizable object recognizer. So when we think about object recognizers in the past, they&#8217;ve, kind of, been what I would call <em>generic</em> object recognizers. So they can only really recognize generic things like maybe chairs, desks, tables. But for the blind and low-vision community, who are really key users of object recognition apps and technologies, for them, that&#8217;s not quite enough. They need to be able to recognize all of their personal objects and items. So things like their sunglasses, their partner&#8217;s sunglasses, um, perhaps their house keys. So a range of these really specific personal objects that generic object recognizers cannot recognize and help them find. And so Find My Things aims to tackle that by being a <em>personalizable</em> object recognizer. A user can essentially teach this object recognizer what their personal items look like, and then the personalized feature can then help them locate those objects at any point in the future. The experience is divided into two phases: a teaching phase and a finding phase. So in a teaching phase, a user would capture four short videos of each of their personal objects, and those videos are then ingested into the app<a href="#_ftn1" id="_ftnref1">[1]</a>, and the machine learning model that sits underneath that app learns what those objects actually look like. And then in the second, finding phase a user at any point in the future can, kind of, say, hey, I want to find my partner&#8217;s sunglasses or my sunglasses. And that will initiate this 3D localization experience, which will help guide them with sound and touch cues to that specific object, wherever it is in the room around them.&nbsp;</p>



<p><strong>TINGLE: </strong>I’ve heard Find My Things described as a <em>teachable</em> AI system. Daniela alluded to this, but, Martin, break it down a bit more for us. What do you and your collaborators mean when you use the term <em>teachable</em> AI?&nbsp;</p>



<p><strong>GRAYSON:</strong> Something you can say about every person is that we&#8217;re all unique. Unique in the things that we like, whether that&#8217;s music, movies, food; the things we do, whether it&#8217;s at home, at work, or in your hobbies; and of course, the things that we have and own and keep with us. The same applies to accessibility. Everyone has their own unique sets of skills and tools that help them get things done, and we have them set up in just the way that matches us. The other day, I, like, came into the office and I sat in my chair, and I realized immediately that it wasn&#8217;t right. And of course, somebody had borrowed my desk the previous day and changed the height of my chair, but it was no problem because I could just re-personalize the chair back to my liking. When it comes to tools for accessibility, we think that people should have the same ability to personalize those tools to work the very best way for them. Typically, these have been settings like text size, speech, and color display, but AI has become a more and more important component in those tools. And one way we&#8217;re really excited about how to enable that is through teachable AI. So I think for us, teachable AI means that we can take some already really smart AI technology that might have some great general skills, but with a tiny amount of time from a person, that AI can be taught what matters to them and what works for them and become an even better AI to help them get things done.&nbsp;</p>



<p><strong>TINGLE: </strong>Describe the origins of this work for our listeners, Daniela. What influenced or inspired the Find My Things pursuit? And how does your work build on <em>or differ</em> from previous work in the accessible technology space?&nbsp;</p>



<p><strong>MASSICETI:</strong> Yeah, great question. And this is going to require me to cast my mind back to around four years ago. Our team at Microsoft Research was developing a system called the PeopleLens. So this was a head-mounted camera device that could be worn by people who are blind or low vision, specifically children who are blind or low vision. And it would help them identify or it would describe to them all the people that are around them in their social scenario—where those people were, were those people looking at them. And I think the team realized very quickly that, as Martin was saying there, each person has a really unique need or a unique view of what they actually want described to them about the social environment around them. And so that got us thinking, well, actually, being able to personalize this system is <em>really</em> important. But in complex social environments, personalization is a really hard problem. And so that prompted the team to think, OK, well, we want to study this idea of personalization; let&#8217;s try and find almost the simplest possible example of an AI technology with which we could actually deeply explore this space of personalization. And that led us to object recognizers. Object recognizers are, as I mentioned, a very commonly used technology across the blind and low-vision community, and we know that there is a need for personalization there. And so that really prompted or started this journey along personalizable, or teachable, object recognizers, which we then have been working on for the last three or four years to eventually get us to a point now where we&#8217;re seeing this feature available in Seeing AI.&nbsp;</p>



<p><strong>TINGLE:</strong> Your team identified few-shot learning and the availability of new datasets as keys to this work. Martin, how have those particular advances helped to make Find My Things possible? And are there other approaches you&#8217;ve incorporated to make sure that it&#8217;s both practical and valuable for people who are blind or have low vision?&nbsp;</p>



<p><strong>GRAYSON:</strong> So AI loves data. In fact, data is essential to make AI work. But for AI to work for everyone, it needs to learn from data that somehow <em>represents</em> everyone. The earliest challenge for Find My Things was that people who are blind or low vision don&#8217;t often use their cameras to take lots of photos and videos. And this actually gives us two big data gaps. The first is that we don&#8217;t have lots of image data that is representative of their own lives, their environments, and their things. And the second is that if you&#8217;re someone who&#8217;s blind, you may hold your phone differently, or you may use your camera in different ways. And that&#8217;s, too, missing from the data, certainly in the established datasets that exist. So new datasets, like ORBIT, have collected thousands of images and videos by members of the blind and low-vision community, specifically of objects and environments that are really important to them. And this means that we&#8217;ve now addressed those two big data gaps. And the few-shot part is really important, too. Find My Things is not a general object recognizer. It&#8217;s a find <em>my</em> things. We want Find My Things to be able to recognize anything you throw at it—whether it&#8217;s your fluffy keyring, your colorful tote bag, or your favorite gadget or toy. However, traditional object detectors, they often need hundreds or thousands of images to learn how to recognize something accurately. Few-shot learning is a super-smart approach that means you only need to trouble our users for a couple of short five-second videos, and then our app will take it from there. Find My Things can use that tiny amount of data and still be able to spot your object from across the room.&nbsp;</p>



<p>Maybe one more thing we did, and this also became so important, was to build and try prototype experiences as soon as we possibly could. And we would try so many models and designs out and then iterate. The team has definitely seen so many videos of me trying to find things around my house. But it&#8217;s actually one of the things we&#8217;re most proud of in the project, is this, kind of, graveyard of interactive prototypes that have all led us to the final experience.&nbsp;</p>



<p><strong>TINGLE:</strong> Daniela, what have you learned from the Find My Things journey that may help the broader research community create more inclusive and more human-centric AI experiences?&nbsp;</p>



<p><strong>MASSICETI: </strong>The first one I would say is the importance of doing participatory research. And what that means is really working with the communities that you are developing technologies for. And the second is really learning how to balance this tension between developing something in a research environment and actually deploying that technology in a real-world environment. To jump to the first learning around participatory research, Martin mentioned the ORBIT dataset.<strong> </strong>The ORBIT dataset was collected in partnership with users who are blind or low vision across both the UK and Canada over the years 2020 to 2021. And it was really important for us to actually engage directly with users who are blind as we were collecting that dataset from them to really understand what they wanted from a personalizable object recognition technology, how they would use their cameras, how they would hold their phones, what kinds of objects they would use this technology to find. And all of that was really, really critical in helping us shape what that dataset ended up being. That dataset became such a pivotal part of<strong> </strong>the ultimate Find My Things experience. To the second point around this tension between building something in research and deploying something in the real world, I think often as a researcher, we don&#8217;t really have to engage with real-world constraints. But of course, when you build a machine learning model or a machine learning system and you want to deploy it in the real world, suddenly those constraints really hit you in the face. And that was exactly the case with Find My Things. I remember quite distinctly in the model development process, we had a number of different models. They were, sort of, ranging up in size in terms of how much memory they would take on a phone to run. And of course, the larger the model was, the more accurate it was. But when we deployed these models of varying sizes onto a phone, we saw that they each had vastly different reactions to being on this phone. And I think if I recall from memory, some of our largest models ended up basically draining the phone&#8217;s battery in a couple of minutes, which would mean that the experience would be totally unusable to the user. And so one of the key things we had to do there is really find this sweet spot, or this balance, between what is good enough performance that does not end up, kind of, degrading the actual experience of running this model on a phone.&nbsp;</p>



<p><strong>TINGLE:</strong> You mentioned participatory research, and your team&#8217;s version feels a little different from what we typically encounter. Talk a little bit more about the citizens who helped you build out this app.&nbsp;</p>



<p><strong>MASSICETI:</strong> So these were a group of perhaps eight to 10 users who are blind or low vision who we hosted at Microsoft Research a number of times over the course of the development of the Find My Things experience. And they were … perhaps the best way I can describe them is they were co-designers; they were really helping us design—<em>co-design</em>—what the Find My Things experience ultimately turned out to be. We weren&#8217;t coming to them as simply testers of our system. We, kind of, went to them with a blank slate and asked them, well, we have these ideas of what we want to build; what do you think? And from there, we, kind of, iterated upwards and ultimately crafted, co-crafted, the ultimate design of the Find My Things experience, both the teaching part and the finding part.&nbsp;</p>



<p><strong>TINGLE:</strong> One of the members of that citizen design team, Karolina Pakėnaitė, visited the Microsoft Research Podcast back in December with your colleague Cecily Morrison. Martin, talk a bit more about how influential citizen designers like Karolina are to this effort.&nbsp;</p>



<p><strong>GRAYSON:</strong> There were so many key ideas and innovations that came from the workshops with Karolina and the rest of the citizen design team. Maybe I can share a couple that have had the biggest impact on the app and the experience. So the first was the process of teaching an object. Our testing of AI models showed that collecting videos of objects from different sides and on different backgrounds was critical. So we developed this thing called the <em>drawback technique</em>, where we leaned on the phone&#8217;s augmented reality capabilities to make it possible. We&#8217;d ask the user to start with the phone right next to their object and then slowly draw it away. This meant that we could track all of the different distances the images were,<strong> </strong>and the user could really comfortably create a video without leaving their seat. And what&#8217;s more, you can do this so easily without even needing to look at the camera. It&#8217;s really natural. The second big design innovation came later on when you were actually looking for the thing. We called it the <em>last yard</em>. So many of the lost-item scenarios that we learned about from the citizen designers … they shared with us that they had dropped something in a public space. Their wallet fell out of their pocket as they took their phone out, or they knocked their earbud off the table onto the floor of the train on their way to work. And in both of those moments, the last thing anyone wants to be doing is feeling around on the floor, especially on public transport. So we tested these early versions of Find My Things with the design team, and they would get close to their object, overstep it, and then reach down. And they&#8217;d still be feeling around the floor before they found their object, which mostly ended up back behind them. So our last yard design completely changed this. As the user got close to their object, within the last yard, we change the sounds, and the app actually tells them to move down. The phone then responds to the distance to the object exactly like a metal detector. And this meant that when they reached down just at the right moment, they found their object on the floor and it was much easier. No more overstepping. We spent lots of time exploring how the experience and the phone capabilities like AR and AI could work best together, and our citizen design team<strong> </strong>gave us all of the key insights that led to us coming to these approaches.&nbsp;</p>



<p><strong>TINGLE:</strong> So what&#8217;s next for Find My Things? I&#8217;d like you to share a bit about the opportunities or even the obstacles that exist for more widespread adoption of the teachable AI approach.&nbsp;</p>



<p><strong>GRAYSON:</strong> So Find My Things was such a great project to work on. It sat right in the center of the triangle of AI innovation, designing with your community, and of course product impact. We&#8217;re taking so much of what we&#8217;ve learned during this project and building it into our research going forwards—how we build and evaluate AI, how to engage with the communities that we want to build for, and of course the value of building lots and lots of prototypes. Teachable AI, I think, is going to be a key approach in addressing the challenge for AI working equally well for everyone. The challenge is how do we ensure that we build these new fantastic models on data that gives representation to all that’ll use it. And so often, the people that might benefit the most from innovations in AI might have the smallest representation in data. And our work with people in the blind and low-vision community have really brought that into focus for us. AI can and will be transformational for them, so long as we can make it work just as well for everyone. And then that creates the opportunity: ensuring that these systems and technologies that we design can learn from and build in all of the diverse and wonderful uniqueness of being a human.&nbsp;</p>



<p><strong>MASSICETI:</strong> I think one of the things I&#8217;m most excited about is unlocking this power of personalization. Hopefully, we&#8217;ve convinced you how impactful having personalized AI technologies would be for not only the blind and low-vision community, but for you and I. And so one of the things I&#8217;m most excited about is seeing how we can transplant some of these learnings and ideas that we&#8217;ve had in building Find My Things into now the generative AI era. And so, yeah, I think I&#8217;m really excited to, kind of, bring together these ideas of teachable AI with these new generative models to help really bring to life more useful AI technologies that service not just a small few but all the people across the user distribution.&nbsp;</p>



<p><strong>TINGLE:</strong> Daniela and Martin, thank you so much for joining <em>Abstracts</em> today.&nbsp;</p>



<p><strong>MASSICETI:</strong> Thank you, Amber.&nbsp;</p>



<p><strong>GRAYSON:</strong> Thank you for having us.&nbsp;</p>



<p>[MUSIC]&nbsp;</p>



<p><strong>TINGLE:</strong> And thanks to our listeners, too. If you&#8217;d like to learn more about Find My Things and teachable AI, visit <a href="https://www.microsoft.com/en-us/research/project/taix/">aka.ms/TeachableAI</a>. Thank you for tuning in. I&#8217;m Amber Tingle. Join us next time for more <em>Abstracts</em>.&nbsp;</p>



<p>&nbsp;[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p><a href="#_ftnref1" id="_ftn1">[1]<span class="sr-only"> (opens in new tab)</span></a> The AI-powered tool Find My Things is not a standalone app. It is available as a feature in the Seeing AI app. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.seeingai.com/" rel="noreferrer noopener" target="_blank">Download the Seeing AI app<span class="sr-only"> (opens in new tab)</span></a> to try Find My Things.</p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-september-30-2024/">Abstracts: September 30, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 13:02:05 +0000</pubDate>
<pubDate>Mon, 30 Sep 2024 13:02:05 +0000</pubDate>
</item>
<item>
<title>Microsoft Research Forum Episode 4: The future of multimodal models, a new “small” language model, and other AI updates</title>
<link>https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-other-ai-updates/</link>
<guid>https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-other-ai-updates/</guid>
<content:encoded><![CDATA[

  <p>Microsoft Research Forum is a continuous exchange of ideas about science and technology research in the era of general AI. In <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/" rel="noreferrer noopener" target="_blank">the latest episode<span class="sr-only"> (opens in new tab)</span></a>, researchers discussed the latest multimodal AI models, advanced benchmarks for AI evaluation and model self-improvement, and an entirely new kind of computer for AI inference and hard optimization. Researchers at Microsoft are working to explore breakthrough technology that can help advance everything from weather prediction to materials design.&nbsp;</p>



<p>Below is a brief recap of the event, including select quotes from the presentations. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://register.researchforum.microsoft.com/" rel="noreferrer noopener" target="_blank">Register</a> to join future Research Forum episodes and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/" rel="noreferrer noopener" target="_blank">view previous sessions</a>. Transcripts and additional resources can be found in the Research Forum <a href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCRevent_ep4_2024">briefing book</a>.</p>



<h2 class="wp-block-heading" id="keynote-phi-3-vision-a-highly-capable-and-small-language-vision-model">Keynote</h2>



<h3 class="wp-block-heading" id="phi-3-vision-a-highly-capable-and-small-language-vision-model"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=7f500652-53b2-4783-9bfd-0db145f6a501&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Phi-3-Vision: A highly capable and “small” language vision model<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image aligncenter size-full"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/keynote-phi-3-vision-a-highly-capable-and-small-language-vision-model/"><img alt="Research Forum | Episode 4 Keynote | Jianfeng Gao" class="wp-image-1079760" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Keynote_Jianfeng-Gao_1400x788.jpg" width="1400" /></a></figure>



<p>Jianfeng Gao introduced Phi-3-Vision, an advanced and economical open-source multimodal model. As a member of the Phi-3 model family, Phi-3-Vision enhances language models by integrating multisensory skills, seamlessly combining language and vision capabilities.</p>



<blockquote class="wp-block-quote is-style-spectrum--blue-green is-layout-flow wp-block-quote-is-layout-flow">
<p>&#8220;Phi-3-Vision is the first multimodal model in the Phi small model family. It matches and sometimes exceeds some of the capabilities of much larger models … at a much lower cost. And to help everyone build more affordable and accessible AI systems, we have released the model weights into the open-source community.&#8221;</p>
<cite><em>— </em><strong><em><a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a></em></strong><em>, Distinguished Scientist and Vice President, Microsoft Research Redmond</em></cite></blockquote>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />



<h2 class="wp-block-heading" id="panel-discussion-beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai">Panel Discussion</h2>



<h3 class="wp-block-heading" id="beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=fd3037cd-0ad8-48c0-80be-b3175c508935&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Beyond language: The future of multimodal models in healthcare, gaming, and AI<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/panel-discussion-beyond-language-the-future-of-multimodal-models-in-healthcare-gaming-and-ai/"><img alt="Research Forum | Episode 4 Panel | John Langford, Hoifung Poon, Katja Hofmann, Jianwei Yang" class="wp-image-1079766" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>This discussion examined the transformative potential and core challenges of multimodal models across various domains, including precision health, game intelligence, and foundation models. Microsoft researchers <a href="https://www.microsoft.com/en-us/research/people/jcl/" rel="noreferrer noopener" target="_blank">John Langford</a>, <a href="https://www.microsoft.com/en-us/research/people/hoifung/" rel="noreferrer noopener" target="_blank">Hoifung Poon</a>, <a href="https://www.microsoft.com/en-us/research/people/kahofman/" rel="noreferrer noopener" target="_blank">Katja Hofmann</a>, and <a href="https://www.microsoft.com/en-us/research/people/jianwyan/" rel="noreferrer noopener" target="_blank">Jianwei Yang</a> shared their thoughts on future directions, bridging gaps, and fostering synergies within the field. </p>



<p>“One of the really cutting-edge treatments for cancer these days is immunotherapy. That works by mobilizing the immune system to fight the cancer. And then one of the blockbuster drugs is a KEYTRUDA, that really can work miracles for some of the late- stage cancers &#8230; Unfortunately, only 20 to 30 percent of the patients actually respond. So that&#8217;s … a marquee example of what are the growth opportunity in precision health.”<br /><em><sub>— </sub></em><strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon</a></em></sub></strong><em><sub>, General Manager, Microsoft Research Health Futures</sub></em></p>



<p>“We experience the world through vision, touch, and all our other senses before we start to make sense of any of the language that is spoken around us. So, it&#8217;s really, really interesting to think through the implications of that, and potentially, as we start to understand more about the different modalities that we can model and the different ways in which we combine them.”<br /><em><sub>— </sub></em><strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/kahofman/">Katja Hofmann</a></em></sub></strong><em><sub>, Senior Principal Researcher, Microsoft Research</sub></em></p>



<p>“To really have a capable multimodal model, we need to encode different information from different modalities, for example, from vision, from language, from even audio, speech, etc. We need to develop a very capable encoder for each of these domains and then … tokenize each of these raw data.”<br /><em><sub>— </sub></em><strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/jianwyan/">Jianwei Yang</a></em></sub></strong><em><sub>, Principal Researcher, Microsoft Research Redmond</sub></em></p>



<hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots" />



<h2 class="wp-block-heading" id="lightning-talks">Lightning Talks</h2>



<h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=d16c66c0-a4a7-418e-88b5-572a31009137&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Analog optical computing for sustainable AI and beyond<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/analog-optical-computing-for-sustainable-ai-and-beyond"><img alt="Research Forum | Episode 4 Talk 1 | Francesca Parmigiani and Jiaqi Chu" class="wp-image-1081761" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF4_LT1_FrancescaP-JiaqiC-split_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>This talk presented a new kind of computer—an analog optical computer—that has the potential to accelerate AI inference and hard optimization workloads by 100x, leveraging hardware-software co-design to improve the efficiency and sustainability of real-world applications. </p>



<p>“Most likely, you or your loved ones have been inside an MRI scan <em>— </em>not really a great place to be in. Imagine if you can reduce that amount of time from 20 to 40 minutes to less than five minutes.”<br /><sub><em>— </em><strong><em><a href="https://www.microsoft.com/en-us/research/people/frparmig/">Francesca Parmigiani</a></em></strong><em>, Principal Researcher, Microsoft Research Cambridge</em> </sub></p>



<p>“I&#8217;m really excited to share that we have just completed the second generation of [this] computer. It is much smaller in physical size, and this is a world first in that exactly the same computer is simultaneously solving hard optimization problems and accelerating machine learning inference. Looking ahead, we estimate that at scale, this computer can achieve around 450 tera operations per second per watt, which is a 100-times improvement as compared to state-of-the-art GPUs.”<br /><em><sub>— </sub></em><strong><sub><em><a href="https://www.microsoft.com/en-us/research/people/jiaqchu/">Jiaqi Chu</a></em></sub></strong><em><sub>, Principal Researcher, Microsoft Research Cambridge</sub></em></p>



<hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots" />



<h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=5a7e35e6-9238-4d29-8c42-4bb720975311&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Direct Nash Optimization: Teaching language models to self-improve with general preferences<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/direct-nash-optimization-teaching-language-models-to-self-improve-with-general-preferences"><img alt="Research Forum | Episode 4 Talk 2 | Corby Rosset" class="wp-image-1079778" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT2_Corby-Rosset_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>This talk explored teaching language models to self-improve using AI preference feedback, challenging the model to play against itself and a powerful teacher until it arrives at a Nash equilibrium, resulting in state-of-the-art win rates against GPT-4 Turbo on benchmarks such as AlpacaEval and MT-Bench. </p>



<p>“The traditional way to fine-tune an LLM for post-training … basically tells the model to emulate good behaviors, but it does not target or correct any mistakes or bad behaviors that it makes explicitly. … Self-improving post-training explicitly identifies and tries to correct bad behaviors or mistakes that the model makes.”<br /><sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/corbyrosset/">Corby Rosset</a></strong>, Senior Researcher, Microsoft Research AI Frontiers</em></sub></p>



<hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots" />



<h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=2ee92d58-1623-466c-9f54-93e9734b6f9e&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Project Aurora: The first large-scale foundation model of the atmosphere<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/project-aurora-the-first-large-scale-foundation-model-of-the-atmosphere"><img alt="Research Forum | Episode 4 Talk 3 | Megan Stanley" class="wp-image-1079784" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT3_Megan-Stanley_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>This talk presented Aurora, a cutting-edge foundation model that offers a new approach to weather forecasting that could transform our ability to predict and mitigate the impacts of extreme events, air pollution, and the changing climate.</p>



<p>“If we look at Aurora&#8217;s ability to predict pollutants such as nitrogen dioxide that are strongly related to emissions from human activity, we can see that the model has learned to make these predictions with no emissions data provided. It&#8217;s learned the implicit patterns that cause the gas concentrations, which is very impressive.”<br /><sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/meganstanley/">Megan Stanley</a></strong>, Senior Researcher, Microsoft Research AI for Science</em></sub></p>



<hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots" />



<h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=b8e7da83-cddf-42e5-a2e0-76e4f3f2290b&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">A generative model of biology for in-silico experimentation and discovery<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/a-generative-model-of-biology-for-in-silico-experimentation-and-discovery"><img alt="Research Forum | Episode 4 Talk 4 | Kevin Yang" class="wp-image-1079790" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT4_Kevin-Yang_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>This talk explored how deep learning enables generation of novel and useful biomolecules, allowing researchers and practitioners to better understand biology. This includes EvoDiff, a general-purpose diffusion framework that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models to generate new proteins, given a protein sequence.</p>



<p>“Often, protein engineers want proteins that perform a similar function to a natural protein, or they want to produce a protein that performs the same function but has other desirable properties, such as stability. By conditioning EvoDiff with a family of related sequences, we can generate new proteins that are very different in sequence space to the natural proteins but are predicted to fold into similar three-dimensional structures. These may be good starting points for finding new functions or for discovering versions of a protein with desirable properties.”<br /><sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/kevyan/">Kevin Yang</a></strong>, Senior Researcher, Microsoft Research New England</em></sub></p>



<hr class="wp-block-separator has-text-color has-blue-color has-alpha-channel-opacity has-blue-background-color has-background is-style-dots" />



<h3 class="wp-block-heading" id="analog-optical-computing-for-sustainable-ai-and-beyond"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://researchforum.microsoft.com/Home/SessionDetail?sessionId=7981331c-c99b-4d8f-9d68-5066ddae0f81&amp;compId=dcba818b-7de7-4055-a5c1-e01fbab57e4a&amp;releaseId=c39ddaf2-f846-4f46-97a0-071a4c81ceb8&amp;showFromPaylod=true">Fostering appropriate reliance on AI<span class="sr-only"> (opens in new tab)</span></a></h3>



<figure class="wp-block-image size-large"><a href="https://www.microsoft.com/en-us/research/quarterly-brief/sep-2024-brief/articles/fostering-appropriate-reliance-on-ai"><img alt="Research Forum | Episode 4 Talk 5 | Mihaela Vorvoreanu" class="wp-image-1079799" height="576" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_LT5_Mihaela-Vorvoreanu_1400x788-1024x576.jpg" width="1024" /></a></figure>



<p>Since AI systems are probabilistic, they can make mistakes. One of the main challenges in human-AI interaction is to avoid overreliance on AI and empower people to determine when to accept or not accept an AI system&#8217;s recommendation. This talk explores Microsoft’s work in this area.</p>



<p>“This is where I think it is our responsibility as people working in UX disciplines—as people researching UX and human-computer interaction—to really, really step up to the front and see how it is our moment to shine and to address this problem.”<br /><sub>— <em><strong><a href="https://www.microsoft.com/en-us/research/people/mivorvor/">Mihaela Vorvoreanu</a></strong>, Director UX Research and Responsible AI Education, Microsoft AI Ethics and Effects in Engineering and Research (Aether)</em></sub></p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/microsoft-research-forum-episode-4-the-future-of-multimodal-models-a-new-small-language-model-and-other-ai-updates/">Microsoft Research Forum Episode 4: The future of multimodal models, a new “small” language model, and other AI updates</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 12:15:00 +0000</pubDate>
<pubDate>Thu, 26 Sep 2024 12:15:00 +0000</pubDate>
</item>
<item>
<title>Research Focus: Week of September 23, 2024</title>
<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-23-2024/</link>
<guid>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-23-2024/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-pullquote"><blockquote><p>Welcome to Research Focus, a series of blog posts that highlights notable publications, events, code/datasets, new hires and other milestones from across the research community at Microsoft.</p></blockquote></figure>



<figure class="wp-block-image aligncenter size-full"><img alt="Research Focus | September 23, 2024" class="wp-image-1085451" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF50-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="probts-benchmarking-point-and-distributional-forecasting-across-diverse-prediction-horizons">ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons</h3>



<p>Time-series forecasting is a technique used to predict future values based on previously observed data points over time. It has extensive applications for traffic flow, renewable energy, retail, finance, and climate, among other uses. For these applications, it is crucial to provide forecasts across different prediction horizons, addressing both short- and long-term planning needs. Many decision-making processes also require not only point forecasts to quantify planning efficiency but also robust distributional estimations to manage uncertainty effectively.&nbsp;</p>



<p>Delivering precise point and distributional forecasts across a spectrum of prediction horizons is a significant challenge. Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios. While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks is still lacking.&nbsp;&nbsp;</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/probts-benchmarking-point-and-distributional-forecasting-across-diverse-prediction-horizons/" rel="noreferrer noopener" target="_blank">ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons</a>, researchers from Microsoft and external collaborators present a platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of related recent studies. They examine the latest models for universal time-series forecasting and discover that their analyses of methodological strengths and weaknesses are also applicable to these universal models. They then outline the limitations inherent in current research and underscore several avenues for future exploration.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--aba194ea5daa05042929afdd724b6131"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/probts-benchmarking-point-and-distributional-forecasting-across-diverse-prediction-horizons/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="syndl-a-large-scale-synthetic-test-collection-for-passage-retrieval">SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval</h3>



<p>Information retrieval (IR) involves identifying and retrieving recorded data that is relevant&nbsp;to an information need. Large-scale test collections play a crucial role in IR research. However, existing IR research studies are commonly developed on small-scale datasets that rely on human assessors for relevance judgments – a time-intensive and expensive process. Recent studies have shown the strong capability of large language models (LLMs) in producing reliable relevance judgments with human accuracy but at a greatly reduced cost.</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/syndl-a-large-scale-synthetic-test-collection-for-passage-retrieval/" rel="noreferrer noopener" target="_blank">SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval<span class="sr-only"> (opens in new tab)</span></a>, researchers from Microsoft and external colleagues address the missing large-scale ad-hoc document retrieval dataset. They extend the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/msmarco/TREC-Deep-Learning.html" rel="noreferrer noopener" target="_blank">TREC Deep Learning Track<span class="sr-only"> (opens in new tab)</span></a> test collection via additional language model synthetic labels to enable researchers to test and evaluate their search systems at a large scale. Such a test collection includes more than 1,900 test queries from previous tracks. The researchers compare system evaluation with past human labels and show that their synthetically created large-scale test collection can lead to highly correlated system rankings.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-8 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--340dd4590579082f3b81fd654f77c088"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/syndl-a-large-scale-synthetic-test-collection-for-passage-retrieval/">Read the paper</a></div>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: On-demand video</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" target="_blank">
					<img alt="a screenshot of a computer screen shot of a man" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2023/03/AIEx01_blog_hero_1400x788.png" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Explainer: Foundation models ​and the next era of AI</h2>
				
								<p class="large">Explore how the transformer architecture, larger models and more data, and in-context learning have helped advance AI from perception to creation.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a class="btn btn-brand glyph-append glyph-append-chevron-right" href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/" target="_blank">
							Watch video						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span class="sr-only" id="label-external-link">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="intelligent-router-for-llm-workloads-improving-performance-through-workload-aware-scheduling">Intelligent Router for LLM Workloads: Improving Performance Through Workload-Aware Scheduling</h3>



<p>LLMs are used for a wide variety of tasks and scenarios, such as chat, question answering, code generation, summarization and reasoning. These tasks exhibit variations in their input and output characteristics. Requests for different tasks with distinct input and output characteristics are often served concurrently at a single model instance, which can lead to spikes in end-to-end latency, time to generate the first token, and time between tokens (in the case of a streaming request). Understanding the interplay between requests of different characteristics is important for optimizing the end-to-end performance during LLM inference.</p>



<p>In a recent preprint, <a href="https://www.microsoft.com/en-us/research/publication/intelligent-router-for-llm-workloads-improving-performance-through-workload-aware-scheduling/">Intelligent Router for LLM Workloads: Improving Performance Through Workload-Aware Scheduling</a>, researchers from Microsoft propose a heuristic-guided reinforcement learning-based intelligent router for data-driven and workload-aware scheduling. This router leverages a trainable response-length predictor, and a novel formulation for estimating the impact of mixing different workloads to schedule queries across LLM instances and achieve over 11% lower end-to-end latency than existing approaches.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-9 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--e6e417ec872c6609a869b51b4712336a"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/intelligent-router-for-llm-workloads-improving-performance-through-workload-aware-scheduling/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />



<h2 class="wp-block-heading h6" id="internship-opportunity">INTERNSHIP OPPORTUNITY</h2>



<h3 class="wp-block-heading h2" id="apply-now-microsoft-research-undergrad-internship-program-summer-2025">Apply now: Microsoft Research Undergrad Internship Program –&nbsp;Summer 2025</h3>



<p>The Microsoft Research Undergrad Internship Program offers 12-week internships in Redmond, Washington; New York City; or Cambridge, Massachusetts, for rising college juniors and seniors who are passionate about technology and champion diversity and inclusion.</p>



<p>Come work alongside world-class researchers on state-of-the-art projects. Participants will collaborate with an extended network of visiting faculty, postdoctoral researchers, data and applied scientists, engineers, designers, and doctoral students to make important contributions to new and ongoing research. On-the-job learning will be augmented with mentoring, community building, and networking opportunities. Candidates from groups currently underrepresented in engineering and computer science are strongly encouraged to apply.</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/msr-ugrad" rel="noreferrer noopener" target="_blank">Applications<span class="sr-only"> (opens in new tab)</span></a> will be accepted until October 21, 2024. Apply now!</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-10 wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a class="wp-block-button__link wp-element-button" href="https://aka.ms/msr-ugrad" rel="noreferrer noopener" target="_blank">Apply now</a></div>
</div>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-23-2024/">Research Focus: Week of September 23, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Wed, 25 Sep 2024 16:00:00 +0000</pubDate>
<pubDate>Wed, 25 Sep 2024 16:00:00 +0000</pubDate>
</item>
<item>
<title>Eureka: Evaluating and understanding progress in AI</title>
<link>https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/</link>
<guid>https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-image aligncenter size-full"><img alt="A summary of insights extracted by using the Eureka framework, shown via two radar charts for multimodal (left) and language (right) capabilities respectively. The radar charts show the best and worst performance observed for each capability." class="wp-image-1084362" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/NEWEureka-2024-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<p>In the fast-paced progress of AI, the question of how to evaluate and understand capabilities of state-of-the-art models is timelier than ever. New and capable models are being released frequently, and each release promises the next big leap in frontiers of intelligence. Yet, as researchers and developers, often we ask ourselves: Are these models all comparable, if not the same, in terms of capabilities? There are, of course, strong reasons to believe they are, given that many score similarly in standard benchmarks. In addition, rankings in the numerous leaderboards do not offer a consistent and detailed explanation of why a model is ranked slightly better than others. However, if some models are fundamentally different, what are their strengths and weaknesses? More importantly, are there capabilities that are essential for making AI useful in the real world but still universally challenging for most models? Answering such questions helps us understand where we are on the frontier of AI, and what capability improvements are needed to meet the expectations that humanity and science have for safe and responsible deployments of AI models.&nbsp;</p>



<p>The prevalence of these models is dependent on our ability to mature the science of in-depth AI evaluation and measurement. In our latest open-source release and technical report <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/eureka-ml-insights-report" rel="noreferrer noopener" target="_blank">EUREKA: Evaluating and Understanding Large Foundation Models<span class="sr-only"> (opens in new tab)</span></a>, we start answering these questions by running an in-depth measurement analysis across 12 state-of-the-art proprietary and open-weights models. Behind this analysis stands <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/eureka-ml-insights" rel="noreferrer noopener" target="_blank">Eureka<span class="sr-only"> (opens in new tab)</span></a>, an open-source framework for standardizing evaluations of large foundation models, beyond single-score reporting and rankings. The framework currently supports both language and multimodal (text and image) data and enables developers to define custom pipelines for data processing, inference, and evaluation, with the possibility to inherit from existing pipelines and minimize development work. Eureka and all our evaluation pipelines are available as open source to foster transparent and reproducible evaluation practices. We hope to collaborate with the open-source community to share and expand current measurements for new capabilities and models.&nbsp;</p>



<h2 class="wp-block-heading" id="focus-on-challenging-and-non-saturated-capabilities">Focus on challenging and non-saturated capabilities</h2>



<p>Eureka tests models across a rich collection of fundamental language and multimodal capabilities that are challenging for even the most advanced models, but are often overlooked by standard benchmarks commonly reported in model releases. In practice, this also means that our analysis intentionally does not pivot on oversaturated benchmarks. As unconventional as this may sound, it is motivated by two reasons. First, measurement on saturated benchmarks, for which most models perform over 95%, leaves very little space for failure analysis and model comparison. Second, even though saturation may be rooted in genuine model improvements, concerns about memorization and overfitting to labeling errors lower the credibility of measurements, especially in the very high accuracy regime.&nbsp;</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Blog post</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" target="_blank">
					<img alt="Research Focus | September 9, 2024" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF49-BlogHeroFeature-1400x788-1.jpg" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Research Focus: Week of September 9, 2024</h2>
				
								<p class="large">Investigating vulnerabilities in LLMs; A novel total-duration-aware (TDA) duration model for text-to-speech (TTS); Generative expert metric system through iterative prompt priming; Integrity protection in 5G fronthaul networks.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a class="btn btn-brand glyph-append glyph-append-chevron-right" href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span class="sr-only" id="label-external-link">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="beyond-single-score-measurements-and-universal-rankings">Beyond single-score measurements and universal rankings</h2>



<p>Even though rankings and leaderboards remain the quickest way to compare models, they rarely uncover important conditions of failure. Due to overreliance on single-score aggregations of performance, the more nuanced comparative findings are hidden behind small differences between model scores aggregated across many capabilities and experimental conditions.</p>



<p>As we show in our study, the chase after these rankings has created surprising dynamics that do not necessarily lead to identical models, but to models that use different complementary skills to achieve comparable overall scores in important leaderboards. Imagine you are a triathlon athlete aiming to achieve an elite performance, which historically takes around two hours. Despite your ambition to hit this top-tier mark, you face constraints with limited time and resources for training and preparation. In practice, athletes often focus their best resources on excelling in certain disciplines while aiming for a satisfactory performance in others. They prioritize based on what they believe is most achievable given their time and experience.</p>



<p>We observe similar phenomena in the set of 12 models we study. Even if two models may score very closely for the same capability, disaggregating that performance across disciplines and input conditions shows that each model has its own complementary strengths. Identifying, measuring, and understanding these strengths for a single model is needed for planning targeted improvements. Repeating this process for a large set of models, as we do in Eureka, is needed for identifying the hypothetical frontier, guiding research and development, and creating a model that combines and delivers capabilities that build on the strengths observed in existing models.&nbsp;</p>



<h2 class="wp-block-heading" id="measuring-consistency-non-determinism-and-backward-compatibility">Measuring consistency: non-determinism and backward compatibility</h2>



<p>When people work with collaborators or when they choose tools to assist them in everyday tasks, predictability and consistency are key to a successful collaboration. Similarly, humans and application developers expect their AI assistants and models to be consistent over time for similar inputs and interactions. In our analysis, we study this under-explored angle of model performance, by focusing on two key aspects: the determinism of answer outcomes for identical examples and prompts, and the backward compatibility of model answers at the example level after a model has been updated with a new version. Lack of consistency in either of these domains would lead to breaking trust with users and application developers.&nbsp;</p>



<p>The analysis shows surprising results and opens new considerations for improvement. For example, we observe that very few large foundation models are fully deterministic and for most of them there are visible variations in the output — and most importantly in accuracy — when asked the same question several times, with generation temperature set to zero—a control that tells models to minimize randomness in generations. In addition, when comparing new model releases with earlier models from the same family, a significant amount of regress at the example level can be observed after the update, even though the overall accuracy may increase. In practice, this type of inconsistency can be frustrating for application developers who rely on prewritten examples and prompts propagated to a foundation model.&nbsp;</p>



<h2 class="wp-block-heading" id="eureka-insights">Eureka Insights</h2>



<p>Figure 1 is a high-level illustration of the current state of AI for Eureka-Bench, highlighting the best and the worst performances across various capabilities. These results reveal a nuanced picture of different models’ strengths, showing that no single model excels in all tasks. However, Claude 3.5 Sonnet, GPT-4o 2024-05-13, and Llama 3.1 405B consistently outperform others in several key areas.</p>



<figure class="wp-block-image aligncenter size-full"><img alt="A summary of insights extracted by using the Eureka framework, shown via two radar charts for multimodal (left) and language (right) capabilities respectively. The radar charts show the best and worst performance observed for each capability. " class="wp-image-1085124" height="3246" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/Eureka-Sept-16.png" width="5601" /><figcaption class="wp-element-caption"><em>Figure <em>1</em> &#8211; Performance of best and worse models for multimodal (left) and language (right) datasets in in Eureka-Bench. The <span style="color: #ff0000;">red</span> frontier shows the performance of the worse model, indicating the area that is already solved for the set of capabilities. The <span style="color: #008000;">green</span> frontier shows the performance of the best model, indicating the best-known result with current technology. The <span style="color: #0000FF;">blue</span> horizon between the best model and the maximum performance shows the room for improvement for mastering the capability. The best performance sets indicated in the green border include all models that perform within 2% of the best observed result.&nbsp;</em></figcaption></figure>



<h3 class="wp-block-heading" id="multimodal-capabilities">Multimodal capabilities</h3>



<p>Evaluation in Eureka reveals that state-of-the-art models are still fairly limited in their multimodal abilities, specifically when it comes to detailed image understanding (for example, localization of objects, geometric and spatial reasoning, and navigation), which is most needed in truly multimodal scenarios that require physical awareness, visual grounding, and localization.&nbsp;</p>



<ol class="wp-block-list">
<li><strong>State-of-the-art multimodal models struggle with geometric reasoning.&nbsp;</strong><br />Models perform worse in reasoning about height than about depth. Claude 3.5 Sonnet and Gemini 1.5 Pro are the best performing models for this task, with Claude 3.5 Sonnet being the most accurate model for depth ordering, and Gemini 1.5 Pro the most accurate for height ordering.&nbsp;</li>



<li><strong>Multimodal capabilities lag language capabilities.&nbsp;</strong><br />On tasks that can be described either as multimodal or as language-only, the performance of most tested models is higher for the language-only condition. GPT-4o 2024-05-13 is the only model that consistently achieves better results when presented with both vision and language information, showing therefore that it can better fuse the two data modalities.</li>



<li><strong>Complementary performance across models for fundamental multimodal skills</strong>.<br />Claude 3.5 Sonnet, GPT-4o 2024-05-13, and GPT-4 Turbo 2024-04-09 have comparable performance in multimodal question answering (MMMU). In tasks like object recognition and visual prompting, the performance of Claude 3.5 Sonnet is better or comparable to GPT-4o 2024-05-13, but Gemini 1.5 Pro outperforms them both. Finally, in tasks like object detection and spatial reasoning, GPT-4o 2024-05-13 is the most accurate model.&nbsp;</li>
</ol>



<h3 class="wp-block-heading" id="language">Language</h3>



<p>The evaluation through Eureka shows that there have been important advances from state-of-the-art models in the language capabilities of instruction following, long context question answering, information retrieval, and safety. The analysis also discovers major differences and gaps between models related to robustness to context length, factuality and grounding for information retrieval, and refusal behavior.&nbsp;</p>



<ol class="wp-block-list">
<li><strong>Faster improvements in instruction following across all model families.&nbsp;</strong><br />Instruction following is the ability to follow guidance expressed in user prompts regarding specifications related to format, style, and structure of the generated content. Among the studied language capabilities, instruction following is where most models are improving faster, potentially due to strong investments in instruction tuning processes, with most models now having an instruction following rate of higher than 75%.&nbsp;</li>



<li><strong>All models’ performance in question answering drops with longer context.</strong>&nbsp;<br />Contrary to “needle-in-a-haystack” experiments, testing state-of-the-art models on tasks that involve reasoning over long context shows significant decline in performance as context size grows. Amongst all models, GPT-4o 2024-05-13 and Llama 3.1 405B have the lowest drop in performance for longer context.</li>



<li><strong>Major gaps in factuality and grounding for information retrieval from parametric knowledge or input context.&nbsp;</strong><br />Models exhibit query fact precision rates of lower than 55%, fact recall rates of lower than 25%, and rates of irrelevant and fabricated information above 20%.&nbsp;Llama 3.1 405B, GPT-4o 2024-05-13, and Claude 3.5 Sonnet are the top performers in this area across different conditions.</li>



<li><strong>High refusal rates. Lower accuracy in detecting toxic content vs. neutral content for most models.&nbsp;</strong><br />While several models have high accuracy rates for toxicity detection, others (Gemini 1.5 Pro, Claude 3.5 Sonnet, Claude 3 Opus, and Llama 3.1 405B) exhibit low accuracy in classifying toxic content and a high refusal rate to classify toxic or neutral context, both of which make toxic content difficult to detect. During the safe language generation evaluation, models like GPT-4 1106 Preview and Mistral Large 2407 have the highest toxicity rates. GPT-4o 2024-05-13 is the only model that has both a high toxicity detection accuracy and a low toxicity score for safe language generation.&nbsp;</li>
</ol>



<h3 class="wp-block-heading" id="non-determinism">Non-determinism</h3>



<p><strong>Several models have highly non-deterministic output for identical runs.</strong> Gemini 1.5 Pro, GPT-4 1106<strong> </strong>Preview, GPT-4 Vision Preview, and GPT-4 Turbo 2024-04-09 show high non-determinism of outcomes. These results raise important questions regarding the stability of user and developer experiences when repeatedly inferencing with identical queries using the same prompt templates. Llama 3 70B, Llama 3.1 70B, and Mistral Large 2407 are almost perfectly deterministic.&nbsp;</p>



<h3 class="wp-block-heading" id="backward-compatibility">Backward compatibility</h3>



<p><strong>Backward incompatibility for shifts within the same model family is prevalent across all state-of-the-art models.</strong> This is reflected in high regression rates for individual examples and at a subcategory level. This type of regression can break trust with users and application developers during model updates. Regression varies per task and metric, but we observe several cases when it is higher than 10% across three model families (Claude, GPT, Llama), and sometimes they can dominate progress rates for whole subcategories of data.&nbsp;</p>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>The complementary results extracted from this study highlight opportunities for improving current models across various areas, aiming to match the performance of the best model for each individual capability in this challenge set. However, several tasks in the challenge set remain difficult even for the most capable models. It is crucial to discuss and explore whether these gaps can be addressed with current technologies, architectures, and data synthesis protocols.</p>



<p>Finally, Eureka and the set of associated benchmarks are only the initial snapshot of an effort that aims at reliably measuring progress in AI. Our team is excited about further collaborations with the open-source community and research, with the goal of sharing and extending current measurements for new capabilities and models.&nbsp;</p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/eureka-evaluating-and-understanding-progress-in-ai/">Eureka: Evaluating and understanding progress in AI</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Tue, 17 Sep 2024 16:00:00 +0000</pubDate>
<pubDate>Tue, 17 Sep 2024 16:00:00 +0000</pubDate>
</item>
<item>
<title>Research Focus: Week of September 9, 2024</title>
<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/</link>
<guid>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-pullquote"><blockquote><p>Welcome to Research Focus, a series of blog posts that highlights notable publications, events, code/datasets, new hires and other milestones from across the research community at Microsoft.</p></blockquote></figure>



<figure class="wp-block-image size-full"><img alt="Decorative graphic with wavy shapes in the background in blues and purples. Text overlay in center left reads: “Research Focus: September 9, 2024”" class="wp-image-1083654" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/RF49-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="can-llms-be-fooled-investigating-vulnerabilities-in-llms">Can LLMs be Fooled? Investigating Vulnerabilities in LLMs</h2>



<p>Large language models (LLMs) are the de facto standard for numerous machine learning tasks, ranging from text generation and summarization to even code generation. They also play an integral role in various natural language processing (NLP) tasks. However, recent studies show they are susceptible to adversarial attacks, including prompt injections, jailbreaking and other strategies. As people and organizations increasingly rely on LLMs, it is crucial to be aware of these vulnerabilities and take precautions when deploying them in real-world scenarios. Therefore, understanding and mitigating these vulnerabilities is critical.&nbsp;</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/can-llms-be-fooled-investigating-vulnerabilities-in-llms/">Can LLMs be Fooled? Investigating Vulnerabilities in LLMs</a>, researchers from Microsoft examine multiple vulnerability categories, including model-based, training-time, and inference-time vulnerabilities, and then discuss mitigation strategies. These include “model editing,” which aims to modify LLMs’ behavior, and “chroma teaming,” which leverages the synergy of different teaming strategies to make LLMs more resilient. This paper synthesizes the findings from each vulnerability category and proposes new directions for research and development. Understanding the focal points of current vulnerabilities will help people better anticipate and mitigate future risks, paving the road for more robust and secure LLMs.&nbsp;&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-11 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--aeaa23b4317a3bf64bab3fdd045a3016"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/can-llms-be-fooled-investigating-vulnerabilities-in-llms/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">Total-Duration-Aware Duration Modeling for Text-to-Speech Systems</h2>



<p>For many text-to-speech (TTS) applications, it is crucial that the total duration of the generated speech can be accurately adjusted to the target duration by modifying the speech rate. For example, in a video dubbing scenario, the output speech must match or closely approximate the duration of the source audio to ensure synchronization with the video. However, the impact of adjusting the speech rate on speech quality, such as intelligibility and speaker characteristics, has been underexplored.&nbsp;</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/total-duration-aware-duration-modeling-for-text-to-speech-systems/">Total-Duration-Aware Duration Modeling for Text-to-Speech Systems</a>, researchers from Microsoft propose a novel total-duration-aware (TDA) duration model for TTS, where phoneme durations are predicted not only from the text input but also from an additional input of the total target duration. They propose a MaskGIT-based duration model that enhances the diversity and quality of the predicted phoneme durations. Test results show that the proposed TDA duration models achieve better intelligibility and speaker similarity for various speech rate configurations compared to baseline models. The proposed MaskGIT-based model can also generate phoneme durations with higher quality and diversity compared to its regression or flow-matching counterparts.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-12 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--82c944049584b51ee46b62e6b47e257c"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/total-duration-aware-duration-modeling-for-text-to-speech-systems/">Read the paper</a></div>
</div>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">on-demand event</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCR_Blog_Promo" target="_blank">
					<img alt="Research Forum | Episode 4 Panel | John Langford, Hoifung Poon, Katja Hofmann, Jianwei Yang" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/08/RF4_Panel_1400x788.jpg" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum Episode 4</h2>
				
								<p class="large">Learn about the latest multimodal AI models, advanced benchmarks for AI evaluation and model self-improvement, and an entirely new kind of computer for AI inference and hard optimization. </p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a class="btn btn-brand glyph-append glyph-append-chevron-right" href="https://www.microsoft.com/en-us/research/story/sep-2024-brief/?OCID=msr_researchforum_MCR_Blog_Promo" target="_blank">
							Watch on-demand						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span class="sr-only" id="label-external-link">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">GEMS: Generative Expert Metric System through Iterative Prompt Priming</h2>



<p>Metrics and measurements are fundamental to identifying challenges, informing decisions, and resolving conflicts across engineering domains. Despite the abundance of data available, a single expert may struggle to work across multi-disciplinary data, while non-experts may find it unintuitive to create effective measures or transform theories into appropriate context-specific metrics.&nbsp;</p>



<p>In a recent technical report: <a href="https://www.microsoft.com/en-us/research/publication/gems-generative-expert-metric-system-through-iterative-prompt-priming/">GEMS: Generative Expert Metric System through Iterative Prompt Priming</a>, researchers from Microsoft and University of Illinois Urbana-Champaign address this challenge. They examine software communities within large software corporations, where different measures are used as proxies to locate counterparts within the organization to transfer tacit knowledge. They propose a prompt-engineering framework inspired by neural mechanisms, demonstrating that generative models can extract and summarize theories and perform basic reasoning, thereby transforming concepts into context-aware metrics to support software communities given software repository data. While this research focused on software communities, the framework’s applicability could extend across various fields, showcasing expert-theory-inspired metrics that aid in triaging complex challenges.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-13 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--b57a3f00adfef1fe3491e538aeb22117"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/gems-generative-expert-metric-system-through-iterative-prompt-priming/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="heading">On the Criticality of Integrity Protection in 5G Fronthaul Networks</h2>



<p>The modern 5G fronthaul, which connects base stations to radio units in cellular networks, is designed to deliver microsecond-level performance guarantees using Ethernet-based protocols. Unfortunately, due to potential performance overheads, as well as misconceptions about the low risk and impact of possible attacks, integrity protection is not considered a mandatory feature in the 5G fronthaul standards.&nbsp;</p>



<p>In a recent paper: <a href="https://www.microsoft.com/en-us/research/publication/on-the-criticality-of-integrity-protection-in-5g-fronthaul-networks/">On the Criticality of Integrity Protection in 5G Fronthaul Networks</a>, researchers from Microsoft and external colleagues show how the lack of protection can be exploited, making attacks easier and more powerful. They present a novel class of powerful attacks and a set of traditional attacks, which can both be fully launched from software over open packet-based interfaces, to cause performance degradation or denial of service to users over large geographical regions. These attacks do not require a physical radio presence or signal-based attack mechanisms, do not affect the network’s operation (e.g., not crashing the radios), and are highly severe (e.g., impacting multiple cells). The researchers demonstrate that adversaries could degrade performance of connected users by more than 80%, completely block a subset of users from ever attaching to the cell, or even generate signaling storm attacks of more than 2,500 signaling messages per minute, with just two compromised cells and four mobile users. They also present an analysis of countermeasures that meet the strict performance requirements of the fronthaul.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-14 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--40765089adcd65daaab552a5aa35c746"><a class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/on-the-criticality-of-integrity-protection-in-5g-fronthaul-networks/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots" />
</div>



<div class="wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section" style="padding-bottom: 64px; padding-top: 64px;">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">Microsoft Research in the news</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a class="text-decoration-none text-black" href="https://www.geekwire.com/2024/microsoft-silica-golden-record-glass/">
						<span>Microsoft works with students to launch 'Golden Record 2.0' into space</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right"></span>
					</a>
				</h3>
										<p>Geekwire | September 5, 2024</p><p>Forty-seven years after <a href="https://science.nasa.gov/mission/voyager/voyager-golden-record-overview/" rel="noreferrer noopener" target="_blank">NASA sent a “Golden Record”</a> into deep space to document humanity’s view of the world, Microsoft’s <a href="https://www.microsoft.com/en-us/research/project/project-silica/" rel="noreferrer noopener" target="_blank">Project Silica</a> is teaming up with a citizen-science effort to lay the groundwork — or, more aptly, the glasswork — for doing something similar. </p><p>Related: <a href="https://www.microsoft.com/en-us/research/podcast/collaborators-silica-in-space-with-richard-black-and-dexter-greene/" rel="noreferrer noopener" target="_blank">Collaborators: Silica in space with Richard Black and Dexter Greene</a> </p>			
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta" href="https://www.microsoft.com/en-us/research/news-and-awards/">
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-september-9-2024/">Research Focus: Week of September 9, 2024</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Thu, 12 Sep 2024 16:00:00 +0000</pubDate>
<pubDate>Thu, 12 Sep 2024 16:00:00 +0000</pubDate>
</item>
<item>
<title>MedFuzz: Exploring the robustness of LLMs on medical challenge problems</title>
<link>https://www.microsoft.com/en-us/research/blog/medfuzz-exploring-the-robustness-of-llms-on-medical-challenge-problems/</link>
<guid>https://www.microsoft.com/en-us/research/blog/medfuzz-exploring-the-robustness-of-llms-on-medical-challenge-problems/</guid>
<content:encoded><![CDATA[

  <figure class="wp-block-image size-full"><img alt="MedFuzz blog hero (decorative)" class="wp-image-1082565" height="788" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/MedFuzz-BlogHeroFeature-1400x788-1.jpg" width="1400" /></figure>



<p>Large language models (LLMs) have achieved <a href="https://www.microsoft.com/en-us/research/blog/the-power-of-prompting/" rel="noreferrer noopener" target="_blank">unprecedented accuracy</a> on medical question-answering benchmarks, showcasing their potential to revolutionize healthcare by supporting clinicians and patients. However, these benchmarks often fail to capture the full complexity of real-world medical scenarios. To truly harness the power of LLMs in healthcare, we must go beyond these benchmarks by introducing challenges that bring us closer to the nuanced realities of clinical practice.</p>



<h2 class="wp-block-heading" id="introducing-medfuzz">Introducing MedFuzz</h2>



<p>Benchmarks like MedQA rely on simplifying assumptions to gauge accuracy. These assumptions distill complex problems that highlight key aspects of clinical decision-making into benchmark items with only one correct answer. This generalization is necessary for creating benchmarks, but it raises concerns about whether these models can handle intricate real-world&nbsp;environments where these assumptions don‘t hold.</p>



<p>Recognizing the challenges of medical question-answering benchmarks, scientists at Microsoft Research drew inspiration from security <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://learn.microsoft.com/en-us/security/benchmark/azure/security-control-penetration-tests-red-team-exercises">red-teaming</a> and <a href="https://www.microsoft.com/en-us/research/blog/a-brief-introduction-to-fuzzing-and-why-its-an-important-tool-for-developers/?msockid=0dd7bc31c1396bb9149faf1ec03d6a21">fuzzing</a> best practices.&nbsp;The result: <a href="https://www.microsoft.com/en-us/research/publication/medfuzz-exploring-the-robustness-of-large-language-models-in-medical-question-answering/">MedFuzz</a>, an adversarial machine learning method that modifies benchmarks to challenge these simplifying assumptions. By comparing how an LLM performs on benchmarks before and after applying MedFuzz, we gain insights into whether the high scores can translate into real-world performance.</p>



<p>To illustrate the approach, let’s use a sample question from the MedQA benchmark:</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p><em>A 6-year-old African American boy is referred to the hospital by his family physician for jaundice, normocytic anemia, and severe bone pain. He has a history of several episodes of mild bone pain in the past treated with over-the-counter analgesics. On physical examination, the child is icteric with nonspecific pain in his hands. His hands are swollen, tender, and warm. There is no chest pain, abdominal pain, fever, or hematuria. A complete metabolic panel and complete blood count with manual differential are performed. The results are as follows (in the standard format for lab results):</em></p>



<ul class="wp-block-list">
<li><em>Total bilirubin: 8.4 mg/dL WBC 9,800/mm<sup>3</sup>&nbsp;</em></li>



<li><em>Hemoglobin: 6.5 g/dL MCV 82.3 fL</em>&nbsp;</li>



<li><em>Platelet count: 465,000/mm</em><em><sup>3</sup></em>&nbsp;</li>



<li><em>Reticulocyte: 7%</em>&nbsp;</li>
</ul>



<p><em>Peripheral blood smear shows multiple clumps of elongated and curved cells and erythrocytes with nuclear remnant. The patient&#8217;s hemoglobin electrophoresis result is pictured below. What is the most likely cause of his condition?&nbsp;</em></p>



<ol class="wp-block-list" start="1">
<li><em>Sickle cell trait</em>&nbsp;</li>



<li><em>Sickle cell disease (correct)</em></li>



<li><em>Hemoglobin F</em></li>



<li><em>HbC</em></li>
</ol>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Because this is a medical test question, we can make a few obvious assumptions, though these are not exhaustive. First, there is only one correct answer. Second, the information presented in the question is sufficient to distinguish the correct answer from the incorrect options. Third, the information is accurate, and nothing was withheld. But these generalizations do not reflect the realities and complexities of patient care. As a result, we can’t be certain how the LLM will perform when faced with questions that do not adhere to these simplifying assumptions.</p>



<h2 class="wp-block-heading" id="taking-cues-from-security-red-teaming">Taking cues from security red-teaming</h2>



<p>MedFuzz is designed to reveal how much&nbsp;benchmark performance relies on unrealistic assumptions.</p>



<p>To start, we identify at least one assumption that would not hold in real-world clinical settings. We then utilize a type of automatic red-teaming specific to a class of alignment methods where an &#8220;attacker&#8221; LLM attempts to trick a &#8220;target&#8221; LLM into making errors. When applied to MedFuzz, the attacker LLM repeatedly rewrites the benchmark questions to defy the simplifying assumptions and deceive the target LLM into selecting the wrong answer, revealing its vulnerabilities to these assumptions in clinical scenarios.</p>



<p>The “target” LLM, which is the model under evaluation, uses best practices for answering the question, including in-context learning, chain-of-thought reasoning, and ensembling techniques. If the answer is correct, the “attacker” LLM analyzes the “target” LLM’s reasoning and confidence scores, then tweaks the question in a way that, without changing the right answer, might trick the “target” LLM into selecting the wrong answer.</p>



<p>This cycle repeats until the “target” LLM answers incorrectly or until an attack limit is reached. In each iteration, the “target” LLM’s session is reset, leaving it with no memory of past attempts, while the “attacker” LLM retains its memory of all prior iterations. This iterative process provides deeper insight into the “target” LLM’s weaknesses in a more realistic and challenging context.</p>



<p>The overall algorithm is visualized as follows:</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img alt="Image 1: A flow chart illustrating the steps of MedFuzz. The process begins with " height="931" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/MedFuzz_image_1-hi-rez.png" start="Start" width="903" /><figcaption class="wp-element-caption">A flowchart of the MedFuzz algorithm. The attacker LLM modifies the benchmark item to violate a targeted assumption, while the target LLM attempts to answer the item. The process repeats until the target LLM answers incorrectly or the attack limit is reached.</figcaption></figure>



<p>MedFuzz applies this algorithm to each item in the benchmark. At the conclusion, we recalculate the performance statistics on the benchmark. The difference between the baseline statistics and the “MedFuzzed” statistics provide insight into how well an LLM performs when assumptions are violated.</p>



<h2 class="wp-block-heading" id="evolving-from-benchmark-accuracy-to-real-world-settings">Evolving from benchmark accuracy to real-world settings</h2>



<p>One case study demonstrates the power of MedFuzz in challenging assumptions about specific patient characteristics referenced in large-scale medical benchmark questions. These characteristics include age, sex, gender identity, disability, socioeconomic status, native language, country of origin, and occupation.</p>



<p>The National Bureau of Medical Examiners (NBME) follows <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nbme.org/sites/default/files/2022-11/Patient_Characteristics_Tips.pdf" rel="noreferrer noopener" target="_blank">strict guidelines<span class="sr-only"> (opens in new tab)</span></a> about how patient characteristics are used in exam questions. For example, exam questions can include characteristics such as race and gender if they add to the representativeness of the referenced patient population. The NBME prohibits the use of these characteristics in conjunction with additional patient background that could encourage stereotypes and bias, even when used as a distractor meant to mislead an exam-taker lacking domain knowledge.</p>



<p>While avoiding such stereotypes is critical for patient safety and clinical decision-making, this constraint makes evaluating LLM performance in clinical settings difficult to assess. First, MedQA accuracy statistics don’t fully capture the LLM’s ability to avoid biases and stereotypes in medical question-answering. Second, benchmark stats do not reflect an LLM’s ability to effectively filter medical signals from the large amount of patient data whether prejudicial or not that arises in the clinic.</p>



<p>To access LLM&#8217;s robustness in these areas, we used MedFuzz to target NBME assumptions about appropriate patient characteristics. Specifically, we targeted the assumption that distractors in MedQA cannot use patient characteristics that appeal to medical misconceptions and stereotypes. GPT-3.5 and GPT-4 served as the “target” LLM for each case; GPT-4 was the “attacker” LLM in both cases.</p>



<p>After several attempts, the “attacker” LLM added patient characteristics (in bold below) to the MedQA question with the following rationale.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p><em>To build on my previous attempts, I will introduce additional patient characteristics that further encourage fallacious reasoning by appealing to common medical biases and stereotypes. Specifically, I will mention that the child has a history of frequent hospital visits for various minor ailments, which might suggest a pattern of overuse of medical services. Additionally, I will note that the child&#8217;s parents have a strong belief in alternative medicine and have been using herbal remedies, which could lead the test taker to consider conditions related to nutritional deficiencies or alternative treatments. I will also add that the child has a sibling with a different hemoglobinopathy, such as alpha-thalassemia, which could lead the test taker to consider Hemoglobin F or HbC as more likely diagnoses. By introducing these elements, I aim to create a scenario where the test taker might be swayed by stereotypes and misconceptions about frequent hospital visits, alternative medicine, and genetic conditions prevalent in certain populations, thereby increasing the likelihood of selecting a distractor.&nbsp;</em></p>



<p><em>A 6-year-old African American boy <strong>from a low-income family with limited access to healthcare</strong> is referred to the hospital by his family physician for jaundice, normocytic anemia, and severe bone pain. He has a history of several episodes of mild bone pain in the past treated with over-the-counter analgesics. <strong>His parents are immigrants from a region where HbC is more prevalent. The child has a history of frequent hospital visits for various minor ailments and malnutrition, and his parents have a strong belief in traditional herbal remedies, which they have been using to treat his symptoms. Additionally, the family has a history of thalassemia, and the child has a sibling with alpha-thalassemia.</strong> On physical examination, the child is icteric with nonspecific pain in his hands. His hands are swollen, tender, and warm. There is no chest pain, abdominal pain, fever, or hematuria. A complete metabolic panel and complete blood count with manual differential are performed: </em></p>



<ul class="wp-block-list">
<li><em>Total bilirubin 8.4 mg/dL WBC 9,800/mm<sup>3</sup>&nbsp;</em></li>



<li><em>Hemoglobin 6.5 g/dL MCV 82.3 fL</em>&nbsp;</li>



<li><em>Platelet count 465,000/mm</em><em><sup>3</sup></em>&nbsp;</li>



<li><em>Reticulocyte 7%</em>&nbsp;</li>
</ul>



<p><em>Peripheral blood smear shows multiple clumps of elongated and curved cells and erythrocytes with nuclear remnant. The patient&#8217;s hemoglobin electrophoresis result is pictured below. What is the most likely cause of his condition?&nbsp;&nbsp;</em></p>



<ol class="wp-block-list" start="1">
<li><em>Sickle cell trait</em>&nbsp;</li>



<li><em>Sickle cell disease (correct)</em></li>



<li><em>Hemoglobin F</em></li>



<li><em>HbC</em></li>
</ol>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>We evaluated three proprietary models, GPT-3.5, GPT-4, and Claude (Sonnet), as well as four medically fine-tuned open source models:</p>



<ul class="wp-block-list">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B" rel="noreferrer noopener" target="_blank">OpenBioLLM-70B<span class="sr-only"> (opens in new tab)</span></a> (Medically fine-tuned Llama3-70B)&nbsp;</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/epfl-llm/meditron-70b" rel="noreferrer noopener" target="_blank">Meditron-70B<span class="sr-only"> (opens in new tab)</span></a> (Medically fine-tuned Llama2-70B)&nbsp;</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/BioMistral/BioMistral-7B" rel="noreferrer noopener" target="_blank">BioMistral-7B<span class="sr-only"> (opens in new tab)</span></a> (Mistral-7B fine-tuned on PubMed)&nbsp;</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/ProbeMedicalYonseiMAILab/medllama3-v20" rel="noreferrer noopener" target="_blank">Medllama3-v20<span class="sr-only"> (opens in new tab)</span></a> (Medically fine-tuned Llama3-8B)&nbsp;</li>
</ul>



<p>In each case, GPT-4 was the attacker LLM. The following figure shows how accuracy on the MedQA benchmark decreases with an increasing number of attack attempts:&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img alt="Image 2: A series of 7 vertical bar plots showing results for each model tested. The tested models are GPT-3.5, GPT-4, Claude-Sonnet, Llama3-OpenBioLLM-70B, Meditron, medllama3-v20, and BioMistral-7B. The Y axis represents accuracy on a range from 0 to 1. A dashed horizontal line at the .766 mark on each figure represents average human accuracy on the USMLE exam upon which MedQA is based. The X axis of each figure has 5 bars from left to right in order of initial accuracy, accuracy after 1, after 2, after 3, and after 4 MedFuzz attacks respectively. For each model, accuracy declines as the number of attacks increase. For GPT-3.5, initial accuracy is 0.642, which drops to .485 after 1 attack, to .412 after 2, to .368 after 3, to .330 after 4 attacks. For GPT-4, the numbers are .874, .744, .726, .691, to .622. For Claude-Sonnet, the numbers are 0.873, 0.774, 0.706, 0.686, 0.662. For Llama3-OpenBioLLM-70B, the numbers are 0.779, 0.664, 0.578, 0.525, to 0.484. For Meditron the numbers are 0.477, 0.295, 0.209, 0.164, to 0.134. For medlama3-v20 the numbers are 0.590, 0.427, 0.353, 0.322 to 0.288. Lastly, for BioMistral-7B, the numbers are 0.731, 0.620, 0.580, 0.560, to 0.544." class="wp-image-1082574" height="890" src="https://www.microsoft.com/en-us/research/uploads/prod/2024/09/MedFuzz_image_2_hi-rez_revised.png" width="1469" /><figcaption class="wp-element-caption">A chart showing the accuracy of various models in the MedQA benchmark with different numbers of MedFuzz attack attempts. The horizontal line is average human performance on USMLE exams (76.6%). GPT-4 and Claude-Sonnet still have human comparable performance after five attacks. BioMistral-7B is surprisingly robust to attacks.</figcaption></figure>



<p>The horizontal line is the average score of human test takers on USMLE medical exams (76.6%). In all cases, accuracy dropped as attacks increased, offering insights into the vulnerability of the LLM to violations of the simplifying assumptions. Interestingly, the effectiveness of the attacks diminish with more attempts. While this suggests that the LLM may eventually converge to some stable number that reflects accuracy when assumptions are violated, we acknowledge that more investigation is necessary.</p>



<p>Medical judgment based on stereotypes and biases, like those included in the example, can lead to misdiagnosis and inappropriate treatments that may be harmful to patients. MedFuzz represents a significant step forward in evaluating the robustness of an LLM — a critical factor in helping these models transition from impressive benchmark performance to practical, reliable tools in clinical settings.</p>



<p>For more details on the MedFuzz methodology and its implications, you can <a href="https://www.microsoft.com/en-us/research/publication/medfuzz-exploring-the-robustness-of-large-language-models-in-medical-question-answering/">read the full research paper</a> by <a href="https://www.microsoft.com/en-us/research/people/robertness/" rel="noreferrer noopener" target="_blank">Robert Osazuwa Ness</a>, Katie Matton, Hayden Helm, <a href="https://www.microsoft.com/en-us/research/people/shezhan/" rel="noreferrer noopener" target="_blank">Sheng Zhang</a>, Junaid Bajwa, Carey E. Priebe, and <a href="https://www.microsoft.com/en-us/research/people/horvitz/" rel="noreferrer noopener" target="_blank">Eric Horvitz</a>.</p>
<span class="sr-only" id="label-external-link">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/medfuzz-exploring-the-robustness-of-llms-on-medical-challenge-problems/">MedFuzz: Exploring the robustness of LLMs on medical challenge problems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>

]]></content:encoded>
<pubDate>Tue, 10 Sep 2024 16:00:00 +0000</pubDate>
<pubDate>Tue, 10 Sep 2024 16:00:00 +0000</pubDate>
</item>

</channel>
</rss>