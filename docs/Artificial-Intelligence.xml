<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>AutoDAN-Turbo：一种自动化黑盒越狱方法的研究</title>
<link>https://arxiv.org/abs/2410.05295</link>
<guid>https://arxiv.org/abs/2410.05295</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出AutoDAN-Turbo，一种高效的黑盒越狱方法，能够自动生成多种越狱策略，并显著提升攻击成功率。</p><br><br><p><strong>摘要：</strong> 本文提出了一种新的黑盒越狱方法——AutoDAN-Turbo，该方法能够从头开始自动发现尽可能多的越狱策略，而无需人工干预或预定义范围。与基线方法相比，AutoDAN-Turbo在公共基准测试中显著提高了74.3%的平均攻击成功率。值得注意的是，AutoDAN-Turbo在GPT-4-1106-turbo上的攻击成功率达到了88.5%。此外，AutoDAN-Turbo还具有统一框架，可以以即插即用的方式集成现有人类设计的越狱策略。通过整合这些策略，AutoDAN-Turbo在GPT-4-1106-turbo上实现了更高的攻击成功率，达到了93.4%。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05295 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 04:35:20 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 04:35:20 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的情境安全评估研究</title>
<link>https://arxiv.org/abs/2410.06172</link>
<guid>https://arxiv.org/abs/2410.06172</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出多模态情境安全的评估和分析，探讨当前大语言模型在安全性上的挑战。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种新的安全挑战，称为多模态情境安全，探讨了用户或代理所处的具体情境如何影响安全考虑。为了安全地响应语言查询，MLLM（多模态大语言模型）需要评估语言查询在相应视觉背景下的安全影响。为此，我们开发了多模态情境安全基准（MSSBench），评估当前MLLM在安全性方面的表现。数据集包含1820对语言查询-图像对，其中一半为安全图像背景，另一半为不安全图像背景。我们还构建了一个评估框架，分析关键安全方面，包括明确的安全推理、视觉理解及情境安全推理。研究结果表明，当前MLLM在遵循指令的设置下，面临这一细致安全问题的挑战，未能同时应对多个情境安全挑战，突显了未来研究的关键领域。此外，我们开发了多代理管道以协调解决安全挑战，显示出在安全性提升方面的持续进展。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.06172 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 03:52:32 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 03:52:32 GMT</pubDate>
</item>
<item>
<title>通过解释方差适应优化低秩适配方法</title>
<link>https://arxiv.org/abs/2410.07170</link>
<guid>https://arxiv.org/abs/2410.07170</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出新方法EVA，通过对激活向量进行奇异值分解初始化LoRA矩阵，实现快速收敛和优异性能。</p><br><br><p><strong>摘要：</strong> 本研究提出了一种名为解释方差适应（EVA）的方法，用于改进低秩适配（LoRA）在小规模数据集上的微调效率。LoRA通常通过在随机初始化的低秩权重矩阵上进行微调，但仅依赖于这种随机初始化或在训练期间学习自适应秩的研究，结果通常是收敛速度缓慢或权重均匀分布，导致性能不佳。为此，EVA方法基于对激活向量的最小批量计算的奇异值分解（SVD）进行数据驱动的权重初始化，利用右奇异向量来初始化LoRA矩阵，并在所有权重矩阵间重新分配秩，以解释尽可能多的方差。最终，EVA不仅在语言生成、理解、图像分类及强化学习等多种细化任务中表现出比竞争对手更快的收敛速度，还在各个领域的任务上达到了最高的平均分数。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07170 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 03:49:39 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 03:49:39 GMT</pubDate>
</item>
<item>
<title>个性化视觉指令调优：解决多模态大语言模型的面孔盲目信息</title>
<link>https://arxiv.org/abs/2410.07113</link>
<guid>https://arxiv.org/abs/2410.07113</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文介绍了个性化视觉指令调优（PVIT）框架，旨在提升多模态大语言模型在个性化对话中的表现。</p><br><br><p><strong>摘要：</strong> 随着多模态大语言模型（MLLM）在各个领域的快速发展，它们在进行通用对话时能够展现出色的能力，但在针对具体个体进行个性化对话时存在明显的不足，称之为“面孔盲症”。这种局限性阻碍了MLLM在个性化场景中的应用，如移动设备上的定制视觉助手或需识别人类家庭成员的家用机器人。本文提出了个性化视觉指令调优（PVIT），该框架是一种新型数据策划和训练方法，旨在使MLLM能够识别图像中的目标个体，并进行个性化且连贯的对话。我们开发了一条复杂的管道，能够自动生成包含个性化对话的训练数据，该管道依赖于各类视觉专家、图像生成模型及多模态大语言模型的能力。为评估MLLM的个性化潜能，我们还提出了一个名为P-Bench的基准测试，涵盖了不同难度等级的各类问题。实验结果表明，通过我们的精心策划的数据集进行微调后，个性化性能得到了显著提升。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07113 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 03:18:35 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 03:18:35 GMT</pubDate>
</item>
<item>
<title>TweedieMix：融合个性化概念的扩散模型生成方法</title>
<link>https://arxiv.org/abs/2410.05591</link>
<guid>https://arxiv.org/abs/2410.05591</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>TweedieMix是一种新颖的方法，能在推断阶段生成高保真度的个性化图像和视频。</p><br><br><p><strong>摘要：</strong> TweedieMix是一种在推断阶段组装多样化扩散模型的新方法，旨在解决将多个个性化概念有效整合到图像和视频生成中的挑战。该方法通过分析逆扩散采样的特性，将采样过程分为两个阶段。在初始步骤中，采用多目标感知采样技术，以确保所需目标对象的包含。在后续步骤中，使用Tweedie公式在去噪图像空间中融合定制概念的外观。实验结果表明，TweedieMix能够以更高的保真度生成多个个性化概念，且该框架可以轻松扩展到图像到视频的扩散模型，能够生成包含多种个性化概念的视频。相关结果和源代码信息可在我们匿名项目页面中找到。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05591 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 03:03:18 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 03:03:18 GMT</pubDate>
</item>
<item>
<title>基于一致性模型的文本到视频生成提升方法</title>
<link>https://arxiv.org/abs/2410.05677</link>
<guid>https://arxiv.org/abs/2410.05677</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出了T2V-Turbo-v2，通过一致性模型蒸馏和多种监督信号增强文本到视频生成的质量。</p><br><br><p><strong>摘要：</strong> 本文聚焦于在后期训练阶段，通过从预训练的文本到视频(T2V)模型中蒸馏高效的一致性模型，来增强扩散基础的T2V模型。我们提出的方法T2V-Turbo-v2通过在一致性蒸馏过程中整合多种监督信号，包括高质量训练数据、奖励模型反馈和条件引导，带来了显著的进展。通过全面的消融研究，我们突出了为特定学习目标定制数据集的重要性，以及从多样化奖励模型学习的有效性，这对提高视觉质量和文本视频对齐效果具有积极影响。此外，我们还强调了条件引导策略的广阔设计空间，特别是设计有效的能量函数以增强教师常微分方程(ODE)求解器的能力。我们展示了通过提取来自训练数据集的运动引导并将其整合到ODE求解器中，显著提升生成视频的运动质量，并在VBench和T2V-CompBench上取得更佳的运动相关指标。实证研究表明，T2V-Turbo-v2在VBench上建立了新的状态-of-the-art结果，总分达到85.13，超越了如Gen-3和Kling等专有系统。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05677 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:35:24 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:35:24 GMT</pubDate>
</item>
<item>
<title>Do great minds think alike? Investigating Human-AI Complementarity in Question Answering with CAIMIRA</title>
<link>https://arxiv.org/abs/2410.06524</link>
<guid>https://arxiv.org/abs/2410.06524</guid>
<content:encoded><![CDATA[

  Recent advancements of large language models (LLMs) have led to claims of AI surpassing humans in natural language processing (NLP) tasks such as textual understanding and reasoning. This work investigates these assertions by introducing CAIMIRA, a novel framework rooted in item response theory (IRT) that enables quantitative assessment and comparison of problem-solving abilities of question-answering (QA) agents: humans and AI systems. Through analysis of over 300,000 responses from ~70 AI systems and 155 humans across thousands of quiz questions, CAIMIRA uncovers distinct proficiency patterns in knowledge domains and reasoning skills. Humans outperform AI systems in knowledge-grounded abductive and conceptual reasoning, while state-of-the-art LLMs like GPT-4 and LLaMA show superior performance on targeted information retrieval and fact-based reasoning, particularly when information gaps are well-defined and addressable through pattern matching or data retrieval. These findings highlight the need for future QA tasks to focus on questions that challenge not only higher-order reasoning and scientific thinking, but also demand nuanced linguistic interpretation and cross-contextual knowledge application, helping advance AI developments that better emulate or complement human cognitive abilities in real-world problem-solving.

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:35:24 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:35:24 GMT</pubDate>
</item>
<item>
<title>高效的双向采样策略用于关键帧插值的视频生成</title>
<link>https://arxiv.org/abs/2410.05651</link>
<guid>https://arxiv.org/abs/2410.05651</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究提出了一种双向采样策略，能高效生成高质量的关键帧插值视频。</p><br><br><p><strong>摘要：</strong> 随着大规模文本到视频（T2V）和图像到视频（I2V）扩散模型的进步，视频生成技术得到了显著提升，尤其是在关键帧插值方面。然而，当前的图像到视频扩散模型在从单一条件帧生成视频时表现出色，但在进行双帧（起始帧和结束帧）条件生成时则需改进以实现有效插值。本研究提出了一种新颖的双向采样策略，旨在解决现有方法在并行融合时间前向和后向路径时所产生的离散性问题，避免了伪影的出现，并减少了多次迭代去噪的需求。该方法沿着前向和后向路径进行顺序采样，并分别以起始帧和结束帧作为条件，从而确保生成中间帧时保持更为一致和在流形上的表现。此外，我们还结合了先进的引导技术CFG++和DDS，进一步提升了插值过程的效果。实验表明，该方法在生成平滑高质量视频方面达到了最先进的性能，能够在单张3090 GPU上以195秒的速度处理1024 x 576分辨率的插值25帧，确立了其在关键帧插值中的领先地位。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05651 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:30:29 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:30:29 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在第一人称视频理解中的应用研究</title>
<link>https://arxiv.org/abs/2410.07177</link>
<guid>https://arxiv.org/abs/2410.07177</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究开发了MM-Ego模型，旨在提高第一人称视频理解的能力，构建了数据引擎、基准和新架构。</p><br><br><p><strong>摘要：</strong> 本研究旨在全面探索多模态基础模型在第一人称视频理解中的应用。研究从三个方面入手：第一，针对第一人称视频理解中QA数据的缺乏，我们开发了一种数据引擎，基于人工标注数据高效生成700万条高质量QA样本，形成当前最大的第一人称QA数据集。第二，我们建立了一个包含629个视频和7026个问题的具有挑战性的第一人称QA基准，以评估模型在不同长度视频中识别和记忆视觉细节的能力，并引入新的去偏评估方法，减轻模型评估中不可避免的语言偏见。第三，我们提出了一种专门的多模态架构，采用一种新颖的“记忆指针提示”机制。该设计包括一个全球概览步骤，以获得对整个视频的整体理解和识别关键视觉信息，之后通过一个回退步骤利用这些关键视觉信息生成答复。这种设计使模型能够更有效地理解扩展的视频内容。通过数据、基准和模型，我们成功构建了MM-Ego，一个在第一人称视频理解上表现强大的多模态LLM。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07177 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:28:04 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:28:04 GMT</pubDate>
</item>
<item>
<title>统一金字塔流匹配算法在视频生成中的应用</title>
<link>https://arxiv.org/abs/2410.05954</link>
<guid>https://arxiv.org/abs/2410.05954</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出通过统一金字塔流匹配算法改善视频生成效率，确保多个阶段之间的知识共享与连贯性。</p><br><br><p><strong>摘要：</strong> 视频生成需要建模庞大的时空空间，传统方法使用级联架构降低计算复杂性，但各子阶段的独立优化影响了知识共享和灵活性。本文提出了一种统一的金字塔流匹配算法，将原始去噪轨迹重解读为一系列金字塔阶段，最终阶段在全分辨率下工作，从而提高视频生成建模的效率。不同金字塔阶段的流可以相互链接，以保持连续性。同时，我们设计了自回归视频生成的时间金字塔，以压缩全分辨率历史。整个框架可以通过单个统一的扩散变换器（DiT）进行端到端优化。大量实验表明，我们的方法能够在20.7k A100 GPU训练小时内生成高质量的5秒（最长可达10秒）视频，分辨率为768p，帧率为24 FPS。所有代码和模型将以开源形式发布。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05954 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:22:19 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:22:19 GMT</pubDate>
</item>
<item>
<title>深入分析文本到图像扩散模型的去学习效应</title>
<link>https://arxiv.org/abs/2410.05664</link>
<guid>https://arxiv.org/abs/2410.05664</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文探讨模型去学习在文本到图像扩散模型中的应用及其副作用。</p><br><br><p><strong>摘要：</strong> 随着文本到图像扩散模型商业应用的不断发展，关于其恶意和有害用途的担忧也日益增加。为了缓解这些顾虑，模型去学习的概念被提出，以移除预训练模型中不希望出现的有害信息。目前，去学习的效果主要通过诞生目标概念的能力和图像质量的保持来衡量，但现有研究往往在有限的场景下进行，且去学习的副作用鲜有探讨。本文对去学习进行了全面分析，涵盖了五个关键方面。研究表明，各种方法在更复杂和更现实的场景中都存在副作用或局限性。为此，作者发布了综合评估框架及其源代码和相关工具，希望能够激励该领域的进一步研究，从而推动开发出更可靠和有效的去学习方法。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.05664 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:19:38 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:19:38 GMT</pubDate>
</item>
<item>
<title>IterComp：基于反馈学习的组合生成框架</title>
<link>https://arxiv.org/abs/2410.07171</link>
<guid>https://arxiv.org/abs/2410.07171</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出了IterComp框架，通过多模型组合和迭代反馈学习提升文本到图像生成的组合能力。</p><br><br><p><strong>摘要：</strong> 本文介绍了IterComp，一个新颖的框架，通过聚合多个模型的组合偏好并采用迭代反馈学习方法，提升文本到图像生成的组合能力。鉴于现有的先进扩散模型如RPG、Stable Diffusion 3和FLUX在组合生成方面各有所长，IterComp旨在利用这些模型的互补优势。我们精心策划了六个强大的开源扩散模型，并评估其在属性绑定、空间关系和非空间关系等三项关键组合指标上的表现。基于评估结果，我们构建了一个组合偏好数据集，以便训练组合奖励模型。此外，我们提出了一种迭代反馈学习方法，能够在闭环中逐步自我提升基础扩散模型和奖励模型。理论证明了我们方法的有效性，广泛实验表明IterComp在多类别物体组合和复杂语义对齐方面优于以前的最新技术（如Omost和FLUX）。这项工作为扩散模型的奖励反馈学习和组合生成开辟了新的研究方向。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07171 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:10:13 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:10:13 GMT</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning</title>
<link>https://arxiv.org/abs/2410.04223</link>
<guid>https://arxiv.org/abs/2410.04223</guid>
<content:encoded><![CDATA[

  While large language models (LLMs) have integrated images, adapting them to graphs remains challenging, limiting their applications in materials and drug design. This difficulty stems from the need for coherent autoregressive generation across texts and graphs. To address this, we introduce Llamole, the first multimodal LLM capable of interleaved text and graph generation, enabling molecular inverse design with retrosynthetic planning. Llamole integrates a base LLM with the Graph Diffusion Transformer and Graph Neural Networks for multi-conditional molecular generation and reaction inference within texts, while the LLM, with enhanced molecular understanding, flexibly controls activation among the different graph modules. Additionally, Llamole integrates A* search with LLM-based cost functions for efficient retrosynthetic planning. We create benchmarking datasets and conduct extensive experiments to evaluate Llamole against in-context learning and supervised fine-tuning. Llamole significantly outperforms 14 adapted LLMs across 12 metrics for controllable molecular design and retrosynthetic planning.

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 02:02:23 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 02:02:23 GMT</pubDate>
</item>
<item>
<title>BroadWay：提升文本到视频生成质量的新方法</title>
<link>https://arxiv.org/abs/2410.06241</link>
<guid>https://arxiv.org/abs/2410.06241</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>BroadWay通过自我引导和傅里叶运动增强技术，显著提升文本生成视频的质量。</p><br><br><p><strong>摘要：</strong> 文本到视频生成模型（T2V）近年来备受关注，但生成的视频常出现结构不合理、时间不一致和缺乏运动等问题，导致视频近乎静止。我们发现，不同解码器块之间的时间注意力图的差异与时间不一致性存在相关性。此外，时间注意力图所包含的能量与生成视频中的运动幅度直接相关。基于这些观察，我们提出了一种名为BroadWay的方法，旨在提高文本到视频生成的质量，该方法不需额外参数，且不增加内存或采样时间。BroadWay包括两个主要组成部分：1) 时间自我引导通过减少解码器块之间的时间注意力图的差异，提高生成视频的结构合理性和时间一致性；2) 基于傅里叶的运动增强则通过放大时间注意力图的能量来提升运动幅度和丰富性。大量实验表明，BroadWay以极小的额外成本显著提升了文本到视频生成的质量。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.06241 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:56:59 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:56:59 GMT</pubDate>
</item>
<item>
<title>模态集成率：评估大型视觉语言模型预训练质量的新指标</title>
<link>https://arxiv.org/abs/2410.07167</link>
<guid>https://arxiv.org/abs/2410.07167</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出模态集成率（MIR）作为评估大型视觉语言模型预训练质量的新指标，能有效指示模型的多模态预训练效果。</p><br><br><p><strong>摘要：</strong> 本文提出了模态集成率（MIR），作为评估大型视觉语言模型（LVLMs）多模态预训练质量的有效、稳健和通用的指标。随着大规模预训练的关键性日益突显，评估训练质量却隔绝了代价昂贵的监督微调阶段，因而成为研究空白。现有的损失、困惑度及上下文评估结果在将训练良好的语言模型与新模态对齐时不够可靠。针对缺乏合适指标的问题，本文从跨模态分布距离的角度提出MIR，以评估预训练质量。MIR不仅能有效反映预训练质量，并且与经过监督微调后的基准性能呈正相关；其对不同的训练/评估数据表现出良好的鲁棒性；此外，MIR能广泛适应不同训练配置和体系结构选择。通过一系列的预训练实验，观察MIR在训练数据选择、训练策略安排及模型架构设计等方面的有效性，结果令人满意。希望MIR能为构建高效LVLMs提供有用的指标，并激发在不同领域关于模态对齐的后续研究。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07167 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:55:20 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:55:20 GMT</pubDate>
</item>
<item>
<title>Hallucinating AI Hijacking Attack: Large Language Models and Malicious Code Recommenders</title>
<link>https://arxiv.org/abs/2410.06462</link>
<guid>https://arxiv.org/abs/2410.06462</guid>
<content:encoded><![CDATA[

  The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI recommendations for malicious code in popular code repositories. While foundational large language models (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic strings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may differ between expert contexts. These loopholes would appear in mixture of expert's models when the context of the question changes and may offer fewer malicious training examples to filter toxic comments or recommended offensive actions. The present work demonstrates that foundational models may refuse to propose destructive actions correctly when prompted overtly but may unfortunately drop their guard when presented with a sudden change of context, like solving a computer programming challenge. We show empirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content delivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM's directives to be helpful, example recommendations propose application programming interface (API) endpoints which a determined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the naively copied code. We compare this attack to previous work on context-shifting and contrast the attack surface as a novel version of "living off the land" attacks in the malware literature. In the latter case, foundational language models can hijack otherwise innocent user prompts to recommend actions that violate their owners' safety policies when posed directly without the accompanying coding support request.

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:48:47 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:48:47 GMT</pubDate>
</item>
<item>
<title>Pixtral-12B：领先的多模态语言模型</title>
<link>https://arxiv.org/abs/2410.07073</link>
<guid>https://arxiv.org/abs/2410.07073</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Pixtral-12B是一款拥有120亿参数的多模态语言模型，兼具图像和文本理解能力，表现优越。</p><br><br><p><strong>摘要：</strong> Pixtral-12B是一款拥有120亿参数的多模态语言模型，旨在理解自然图像和文档。它在多个多模态基准测试中表现出色，超越了一些更大规模的模型。与许多开源模型不同，Pixtral在不削弱自然语言处理性能的情况下，依然在多模态任务中表现突出。该模型采用从头开始训练的新视觉编码器，能够原生处理图像，提供灵活的图像处理选项。Pixtral的长上下文窗口限制为128K个标记，支持同时处理任意数量的图像。与同类开源模型（如Llama-3.2 11B和Qwen-2-VL 7B）相比，Pixtral-12B的表现大幅领先，且在与更大模型（如Llama-3.2 90B）相比时，体积仅为其七分之一。此外，Pixtral-12B还推出了一个开源基准MM-MT-Bench，用于在实际场景中评估视觉语言模型，并提供了标准化评估协议的详细分析和代码。Pixtral-12B在Apache 2.0许可下发布。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07073 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:46:46 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:46:46 GMT</pubDate>
</item>
<item>
<title>Story-Adapter：一种高效的长故事视觉生成框架</title>
<link>https://arxiv.org/abs/2410.06244</link>
<guid>https://arxiv.org/abs/2410.06244</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Story-Adapter是一个无训练、高效的框架，提升长故事的图像生成质量与一致性。</p><br><br><p><strong>摘要：</strong> 本研究提出了一个名为Story-Adapter的训练-free且计算高效的框架，用于增强长故事的图像生成能力。文章应对了在长达100帧的故事视觉化中面临的挑战，包括保持语义一致性、生成高质量的细节互动以及确保计算的可行性。我们提出了一个迭代的框架，通过利用文本提示和之前迭代生成的所有图像来细化每个生成的图像。其中，核心模块是一个无训练的全局参考交叉注意力模块，汇聚了先前迭代中生成的所有图像，以保持整个故事的语义一致性，同时通过全局嵌入来减少计算成本。这一迭代过程逐步优化图像生成，反复结合文本约束，从而实现更精准且细腻的互动。通过大量实验，验证了Story-Adapter在提高语义一致性和生成细腻互动上的优越性，特别是在长故事场景中。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.06244 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:42:13 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:42:13 GMT</pubDate>
</item>
<item>
<title>Aria: An Open Multimodal Native Mixture-of-Experts Model</title>
<link>https://arxiv.org/abs/2410.05993</link>
<guid>https://arxiv.org/abs/2410.05993</guid>
<content:encoded><![CDATA[

  Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:41:49 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:41:49 GMT</pubDate>
</item>
<item>
<title>SynPO：通过合成偏好数据提升大语言模型对人类偏好的对齐</title>
<link>https://arxiv.org/abs/2410.06961</link>
<guid>https://arxiv.org/abs/2410.06961</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>SynPO是一种利用合成偏好数据提升大语言模型对齐能力的自增强方法，经过多次迭代取得显著成效。</p><br><br><p><strong>摘要：</strong> 本文介绍了SynPO，一种新颖的自增强范式，通过合成偏好数据来提升大型语言模型的对齐能力。SynPO采用迭代机制，其中自提示生成器产生多样化的提示，响应改进器逐步改善模型的输出。该方法使LLM能够自主学习其输出的生成奖励，从而不再需要大规模的提示和人类偏好的注解。在经过四次SynPO迭代后，Llama3-8B和Mistral-7B在遵循指令的能力上显著提高，在AlpacaEval 2.0和ArenaHard上赢得率提高超过22.1%。同时，SynPO还提升了LLM在多种任务上的整体表现，Open LLM排行榜上平均得分提高了3.2到5.0。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.06961 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:39:09 GMT</pubDate>
<pubDate>Thu, 10 Oct 2024 01:39:09 GMT</pubDate>
</item>

<item>
<title>基于因果事件建模的时序视频定位框架TRACE</title>
<link>https://arxiv.org/abs/2410.05643</link>
<guid>https://arxiv.org/abs/2410.05643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了TRACE，一个基于因果事件建模的任务交错视频大语言模型，提升视频时序定位性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的因果事件建模框架，将视频表示为事件序列，并利用前面的事件、视频输入和文本指令来预测当前事件。每个事件由时间戳、显著性评分和文本标题三部分组成。为有效实施该框架，提出了一种新的视频大语言模型TRACE，能够分别处理视觉帧、时间戳、显著性评分和文本。TRACE通过不同的编码器和解码头来处理这些任务，并按照因果事件建模框架的公式将任务标记排列成交错序列。在各种视频时序定位任务和数据集上的大量实验表明，TRACE在性能上优于现有的最先进视频大语言模型。模型和代码已在GitHub上公开。 </p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 01:28:18 GMT</pubDate>
</item>
<item>
<title>视觉模型中的骨干网与优化器的耦合偏差研究</title>
<link>https://arxiv.org/abs/2410.06373</link>
<guid>https://arxiv.org/abs/2410.06373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了骨干网与优化器之间的耦合偏差现象，并提出相关建议。</p><br /><br /><p><strong>摘要：</strong> 本文深入探讨了视觉骨干网络与优化器之间的相互作用，揭示了一种称为骨干优化器耦合偏差（BOCB）的现象。研究发现，传统的卷积神经网络（CNN），如VGG和ResNet，与随机梯度下降（SGD）优化器展现出显著的共依赖关系，而近年来的架构，如视觉 Transformer（ViT）和ConvNeXt，与自适应学习率的优化器紧密相连。本文进一步表明，BOCB可以通过优化器及某些骨干网络设计引入，并可能显著影响视觉模型的预训练和后续微调。通过深入的实证分析，本文总结了关于推荐优化器的建议，以及对稳健视觉骨干架构的见解。希望本研究能激发学术界对骨干网和优化器的长期假设进行质疑，并激励进一步研究，从而促进更稳健的视觉系统发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 00:56:29 GMT</pubDate>
</item>
<item>
<title>基于Pontryagin最大原理的数据选择框架提升语言模型性能</title>
<link>https://arxiv.org/abs/2410.07064</link>
<guid>https://arxiv.org/abs/2410.07064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种基于PMP的数据选择框架，有效提升大型语言模型在下游任务中的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了从大规模语料中选择高质量预训练数据，旨在提升语言模型的下游应用能力。我们将数据选择形式化为一种广义最优控制问题，理论上可以通过Pontryagin最大原理（PMP）解决，得出了表征最优数据选择与语言模型训练动态关系的必要条件。基于这些理论结果，我们提出了PMP基础的数据选择（PDS）框架，通过解决PMP条件来近似优化数据选择。在实验中，我们应用PDS从CommmonCrawl选择数据，结果表明，PDS选择的语料加速了语言模型的学习，并在多种下游任务中不断提升了不同模型规模的性能。此外，PDS的收益对于训练于约10万亿 token的模型（如400B模型）也有所体现，且通过对测试损失曲线的外推，验证了该框架的有效性。PDS还在预训练数据有限的情况下提升了数据的利用率，减少了1.8倍的数据需求，从而缓解了可用网页爬取语料的快速耗尽。我们的代码、数据和模型检查点可在https://github.com/microsoft/LMOps/tree/main/data_selection找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 00:51:34 GMT</pubDate>
</item>
<item>
<title>利用大语言模型提升代码异常处理能力的框架研究</title>
<link>https://arxiv.org/abs/2410.06949</link>
<guid>https://arxiv.org/abs/2410.06949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了如何通过大语言模型提高软件开发中的异常处理，提出了Seeker多代理框架。</p><br /><br /><p><strong>摘要：</strong> 在实际软件开发中，不当或缺失的异常处理严重影响代码的健壮性和可靠性。开发者在检测、捕获和管理异常时常面临挑战，导致代码脆弱，尤其在开源项目中更为明显。因此，我们探讨利用大型语言模型（LLMs）来改进异常处理。通过深入分析，我们识别了三个主要问题：脆弱代码的敏感性检测不足、异常类型捕获不准确、处理方案扭曲。这些问题在现实代码库中广泛存在，表明健壮的异常处理实践往往被忽视或处理不当。为了解决这一挑战，我们提出了Seeker，一个受专家开发者策略启发的多代理框架，以帮助LLMs更有效地检测、捕获和解决异常。Seeker使用多个代理，包括扫描器、检测器、捕食者、排序器和处理器，提升异常处理的有效性。这项工作是首次系统性研究利用LLMs增强异常处理实践，为未来提升代码可靠性提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 00:45:18 GMT</pubDate>
</item>
<item>
<title>文本时序推理迁移方法提升视频LLM的时序理解能力</title>
<link>https://arxiv.org/abs/2410.06166</link>
<guid>https://arxiv.org/abs/2410.06166</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入文本时序推理迁移方法，视频LLM在时序理解上取得显著提升。</p><br /><br /><p><strong>摘要：</strong> 视频大语言模型（Video LLMs）在视频理解上表现出色，但在时序推理方面仍面临挑战。尽管研究指出视觉输入的时序编码有限制，诊断研究表明视频表示具备足够信息以支持小型分类器实现完美准确率。进一步分析发现，Video LLMs在处理时序概念上存在自身局限，导致在文本时序问答任务上表现不佳。为解决这一问题，本文提出了文本时序推理迁移（T3）方法，从现有的图像-文本数据集中合成多样的文本时序推理任务，以弥补视频样本稀缺。值得注意的是，T3不依赖视频数据，显著提升了LongVA-7B模型的时序理解能力，在TempCompass基准测试中实现5.3的绝对准确率提升，超越了在28,000个视频样本上训练的ShareGPT4Video-8B。此外，T3增强的LongVA-7B模型在多个视频基准测试中表现出色，如在Video-MME的时序推理任务中获得49.7的准确率，超越了InternVL-Chat-V1.5-20B和VILA1.5-40B等强大模型。最后，分析表明文本与视频时序任务性能之间存在强相关性，验证了从文本到视频领域迁移时序推理能力的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06166" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 00:45:02 GMT</pubDate>
</item>
<item>
<title>评估生成视频模型物理常识理解的PhyGenBench基准</title>
<link>https://arxiv.org/abs/2410.05363</link>
<guid>https://arxiv.org/abs/2410.05363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍PhyGenBench基准，评估文本到视频模型的物理常识理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PhyGenBench，这是一个综合的物理生成基准，旨在评估文本到视频（T2V）模型在生成视频时对物理常识的理解能力。基准包含160个精心设计的提示，涵盖27条不同的物理定律，跨越四个基本领域，以全面评估模型对物理常识的掌握。同时，论文还建议了一个新的评估框架PhyGenEval，采用分层评估结构，结合先进的视觉语言模型和大型语言模型来评估物理常识。通过PhyGenBench和PhyGenEval，研究者能够进行大规模的自动化评估，结果表明当前模型在产生符合物理常识的视频方面存在困难，仅靠模型规模扩大或提示工程并不足以解决PhyGenBench所提出的挑战。希望这项研究能激励相关领域的研究者重视模型对物理常识的学习，而不仅仅局限于娱乐应用。数据和代码将发布在https://github.com/OpenGVLab/PhyGenBench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Oct 2024 00:08:47 GMT</pubDate>
</item>
<item>
<title>动态推理轨迹搜索方法DOTS提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2410.03864</link>
<guid>https://arxiv.org/abs/2410.03864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出DOTS方法，通过动态推理轨迹搜索提升大语言模型的推理能力，对问题特征和模型能力进行定制化。</p><br /><br /><p><strong>摘要：</strong> 随着推理能力提升的重要性日益受到关注，本文提出了一种名为DOTS的动态推理轨迹搜索方法，以增强大语言模型的推理能力。传统研究往往使用静态、预定义的推理策略，未考虑问题的具体特征和模型的任务解决能力。DOTS通过三个步骤实现了动态推理：首先，定义多种可组合的原子推理模块；其次，通过迭代探索和评估为每个训练问题寻找最优推理轨迹；最后，利用收集到的最优轨迹训练语言模型，以规划未知问题的推理轨迹。本文还提出了两种学习范式：一是对外部语言模型进行微调，使其成为规划者来指导任务解决模型，二是直接对任务解决模型进行微调，使其具备内部推理规划能力。实验结果显示，DOTS方法在八个推理任务上表现优于静态推理技术和传统指令调优方法，进一步分析表明，该方法使模型能够根据问题复杂度调整推理深度，对难度较大的问题进行更深入的思考和推理。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 16:51:18 GMT</pubDate>
</item>
<item>
<title>TidalDecode: 一种基于位置持久稀疏注意力的高效解码算法</title>
<link>https://arxiv.org/abs/2410.05076</link>
<guid>https://arxiv.org/abs/2410.05076</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TidalDecode通过位置持久的稀疏注意力显著提高了LLM解码速度，保持生成质量。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在自然语言处理任务中的广泛应用，长上下文模型因其处理扩展输入的能力而受到关注。然而，变压器架构中的扩展键值（KV）缓存尺寸在解码阶段带来了显著的内存约束，成为瓶颈。现有的稀疏注意力机制虽然旨在解决该瓶颈，但往往无法可靠识别最相关的注意令牌，并且忽略连续变压器层间的令牌选择空间一致性，这可能导致性能下降并增加令牌选择的开销。为此，本文介绍了TidalDecode，一种通过位置持久稀疏注意力实现快速准确LLM解码的简单有效算法和系统。TidalDecode利用现有稀疏注意力方法中令牌选择的空间一致性，引入少量令牌选择层进行全注意力计算，以识别注意力分数最高的令牌，而其他层则使用预选令牌执行稀疏注意力。该设计显著减少了稀疏注意力的令牌选择开销，同时不影响生成结果的质量。在多种LLM和任务的评估中，TidalDecode的生成性能紧密匹配全注意力方法，同时将LLM解码延迟降低了高达2.1倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05076" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 14:56:52 GMT</pubDate>
</item>
<item>
<title>EBES：针对事件序列的新型基准工具</title>
<link>https://arxiv.org/abs/2410.03399</link>
<guid>https://arxiv.org/abs/2410.03399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EBES是一个全面的基准工具，旨在标准化事件序列的评估，促进可重复研究并加速进展。</p><br /><br /><p><strong>摘要：</strong> 事件序列在多个现实世界领域中广泛存在，例如医疗、金融和用户交互日志，其特点是采样间隔不规则，并且包含分类和数值特征。尽管在时间数据建模技术上已有所进展，但目前缺乏标准化的基准测试，这使得不同论文之间的结果比较变得复杂，潜在地误导了该领域的进展。为此，我们推出了EBES，一个综合性的基准工具，提供标准化的评估场景和协议，专注于带有序列级目标的回归和分类问题。我们的库通过统一接口简化了基准测试、数据集添加和方法集成，包含了一种新颖的合成数据集，并提供预处理的真实世界数据集，包括最大公开银行数据集。我们的结果深入分析了数据集，识别出一些数据集不适合模型比较。同时，我们探讨了建模时间和序列组件的重要性，以及模型的鲁棒性和缩放特性。这些发现为未来研究指明了潜在方向。我们的基准旨在促进可重复的研究，加速进展并提高现实世界的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 10:45:38 GMT</pubDate>
</item>
<item>
<title>Grounded-VideoLLM：提升细粒度视频理解的新模型</title>
<link>https://arxiv.org/abs/2410.03290</link>
<guid>https://arxiv.org/abs/2410.03290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Grounded-VideoLLM，专注于细粒度视频时序定位，克服了现有Video-LLM在时间建模方面的不足。</p><br /><br /><p><strong>摘要：</strong> 当前的视频大语言模型（Video-LLMs）在粗粒度视频理解方面表现出色，但在细粒度时间定位上存在困难。为此，本文提出了一种新颖的模型——Grounded-VideoLLM，该模型能够在细粒度层面上感知和推理视频中的特定时刻。我们发现，现有的Video-LLMs在细粒度视频理解上表现不佳，主要原因在于其缺乏有效的时间建模和时间戳表示。为了解决这一问题，我们对模型进行了优化，具体方法包括：1）增加一个额外的时间流，以编码帧与帧之间的关系；2）用特定时间知识丰富的离散时间标记来表示时间戳。为优化Grounded-VideoLLM的训练，我们采用了多阶段训练方案，先进行简单的视频-captioning任务，然后逐步引入复杂度递增的视频时间定位任务。此外，为了进一步提升模型的时间推理能力，我们还通过自动标注流程整理了一个针对视频问答的基础数据集。实验结果表明，Grounded-VideoLLM在时序句子定位、稠密视频标注和基础视频问答等细粒度定位任务中表现优异，展示了作为通用视频理解助手的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 07:40:30 GMT</pubDate>
</item>
<item>
<title>二维自回归变换器：突破向量量化图像生成的信息损失瓶颈</title>
<link>https://arxiv.org/abs/2410.01912</link>
<guid>https://arxiv.org/abs/2410.01912</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新型模型DnD-Transformer，提升了向量量化自回归图像生成的质量与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型架构——二维自回归变换器（DnD-Transformer），旨在解决向量量化（VQ）自回归图像生成中的信息损失瓶颈。DnD-Transformer通过引入新的自回归方向、模型深度以及序列长度方向，能够为图像预测更多的代码。与传统的1D自回归模型和类似的2D图像分解模型（如RQ-Transformer）相比，DnD-Transformer是一种端到端的模型，其在保持相同主干模型大小和序列长度的情况下，能够生成更高质量的图像。这为自回归图像生成提供了新的优化视角。此外，实验结果表明，DnD-Transformer的潜力不仅限于生成自然图像，还能以自监督的方式生成富有文本和图形元素的图像，展现出对这些组合模态的理解。这一能力在流行的视觉生成模型（如扩散模型）中尚未得到验证，显示出其在仅使用图像训练时所具有的视觉语言智能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01912" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 06:40:12 GMT</pubDate>
</item>
<item>
<title>ControlAR：自回归图像生成中的空间控制框架</title>
<link>https://arxiv.org/abs/2410.02705</link>
<guid>https://arxiv.org/abs/2410.02705</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ControlAR提出了一种在自回归图像生成中高效整合空间控制的新框架。</p><br /><br /><p><strong>摘要：</strong> 自回归（AR）模型在图像生成中通过下一个标记预测重新定义了过程，并显现出强大的潜力。然而，控制图像生成，类似于ControlNet，在AR模型中仍然鲜有探索。尽管受到大型语言模型的启发，可以将控制图像标记化并预填充到自回归模型中，但生成质量仍未达到ControlNet的水平，且效率较低。本研究提出了一个高效且有效的框架ControlAR，用于将空间控制集成到自回归图像生成模型中。首先，我们探讨了AR模型中的控制编码，提出了一种轻量级控制编码器，将空间输入（如边缘图或深度图）转化为控制标记。然后，ControlAR利用条件解码方法，通过对控制和图像标记的逐标记融合，生成下一个图像标记。该方法显著增强了AR模型的控制能力，同时保持了模型的效率。ControlAR还通过条件解码和特定控制能力，赋予AR模型任意分辨率的图像生成能力。实验结果显示，ControlAR在边缘、深度和分割掩码等多种输入条件下，展现了良好的控制能力，且在定量和定性结果上，都超过了以前的可控扩散模型，如ControlNet++。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02705" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 06:24:02 GMT</pubDate>
</item>
<item>
<title>长上下文语言模型在复杂任务中的挑战与研究</title>
<link>https://arxiv.org/abs/2410.04422</link>
<guid>https://arxiv.org/abs/2410.04422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究长上下文任务难度的根本原因，揭示多匹配检索与逻辑检索问题对LCLM的挑战。</p><br /><br /><p><strong>摘要：</strong> 长上下文语言模型（LCLM）因其广泛的上下文窗口而受到越来越多的关注。然而，许多长上下文基准测试中存在的挑战性任务，即便是最先进的LCLM也难以完成。针对这些问题，本文进行实验研究，表明长上下文任务的难度主要源自两个基本问题：'多匹配检索'和'逻辑检索'。前者要求同时检索多个条目，后者则需要在检索标准中进行逻辑判断。虽然这两个问题看似简单，但它们实际上超出了LCLM的能力，因其被证明是超多步骤的（需要大量步骤才能解决）。这一发现可能解释了为何大语言模型在更复杂的长上下文任务中表现不佳，从而为重新思考这些任务的解决方案提供了更准确的视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 06:14:32 GMT</pubDate>
</item>
<item>
<title>基于宏观动作的强化学习人类反馈框架研究</title>
<link>https://arxiv.org/abs/2410.02743</link>
<guid>https://arxiv.org/abs/2410.02743</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MA-RLHF框架，通过宏观动作优化RLHF，提高学习效率和性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的强化学习框架，名为MA-RLHF，旨在改善基于人类反馈的强化学习（RLHF）在大语言模型（LLM）中的应用。现有的token级RLHF面临长序列中的信用分配问题，延迟奖励使得模型难以辨识哪些动作贡献了成功的结果。MA-RLHF框架通过引入宏观动作——即一系列tokens或更高层次的语言构造，降低了动作与奖励之间的时间距离，从而促进更快、更准确的信用分配。这种方法在每个训练过程中提高了策略梯度估计的稳定性与学习效率，而未增加训练或推理的计算复杂性。通过在多个模型规模和任务上进行广泛实验，包括文本摘要、对话生成、问答及程序合成，验证了该方法的有效性。实验结果显示，MA-RLHF在文本摘要和代码生成上实现了高达30%的性能提升，在对话任务中提高了18%，在问答任务中提高了8%。值得注意的是，该方法在训练时间方面以1.7到2倍的速度超过了常规RLHF，并在进一步训练中持续表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02743" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 02:29:33 GMT</pubDate>
</item>
<item>
<title>LongGenBench: Long-context Generation Benchmark</title>
<link>https://arxiv.org/abs/2410.04199</link>
<guid>https://arxiv.org/abs/2410.04199</guid>
<content:encoded><![CDATA[
Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which allows for flexible configurations of customized generation context lengths. LongGenBench advances beyond traditional benchmarks by redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the Gemini-1.5-Flash model showing the least degradation among API accessed models, and the Qwen2 series exhibiting the least degradation in LongGenBench among open source models.
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 01:11:06 GMT</pubDate>
</item>
<item>
<title>提升模型泛化能力的关键因素研究</title>
<link>https://arxiv.org/abs/2410.04717</link>
<guid>https://arxiv.org/abs/2410.04717</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨数据多样性对指令调优模型泛化能力的影响，分析跨领域数据的优势。</p><br /><br /><p><strong>摘要：</strong> 本文深入研究了使模型在面对未见指令时能够有效泛化的关键因素，为指令调优的数据收集提供了指导。通过受图灵完备马尔可夫算法启发的控制实验，我们证明了只有当训练数据在语义领域上足够多样化时，模型才可能实现良好的泛化。研究显示，仅在有限领域内进行多样化并不足以确保稳健的泛化能力，而跨领域的数据多样化，即便在数据预算有限的情况下，显著提高了模型的适应性。此外，我们还将分析扩展到实际场景，包括对专业模型和通用模型的微调。在这两种情况下，我们的结果表明：1）在保持数据量不变的情况下降低数据集多样性可以提升性能；2）在扩大数据集规模时，丰富指令的语义内容比仅仅增加相似数据的数量更有效。我们的研究为数据集的整理提供重要见解，强调了战略性多样化在提升数据质量和模型性能方面的关键作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04717" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 00:56:19 GMT</pubDate>
</item>
<item>
<title>基于响应自适应参考的文本生成评估新范式RevisEval</title>
<link>https://arxiv.org/abs/2410.05193</link>
<guid>https://arxiv.org/abs/2410.05193</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RevisEval通过响应自适应参考提升了文本生成评估的可靠性与效果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLM）在文本生成质量评估中的广泛应用，LLM-as-a-Judge逐渐成为一种高效且经济的替代选择，然而其与人类评估之间仍存在可靠性差距。本文提出了一种新颖的文本生成评估范式RevisEval，旨在通过响应自适应参考提升评估效果。RevisEval的核心思想是，理想的参考需要与待评估的响应保持必要的相关性。具体而言，该方法利用LLM的文本修订能力，动态修订待评估的响应，并将修订后的文本作为后续评估的参考。通过广泛的实验，RevisEval在自然语言生成（NLG）任务及开放式指令跟随任务中，均表现出优于传统的无参考和有参考的评估范式。此外，与传统参考相比，RevisEval的响应自适应参考甚至能够提升经典文本评估指标（如BLEU和BERTScore）的效果，并与LLM-as-a-Judge的表现相媲美。最后，对RevisEval在偏差减少、推理成本影响和参考相关性方面的有效性进行了详细分析。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05193" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Oct 2024 00:03:21 GMT</pubDate>
</item>
<item>
<title>框架感知视频扩散模型：一种新型时序变量应用于视频生成</title>
<link>https://arxiv.org/abs/2410.03160</link>
<guid>https://arxiv.org/abs/2410.03160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种框架感知视频扩散模型，利用向量时间变量提升视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 视频扩散模型（VDMs）在图像生成中取得了重大突破，但其在视频生成中的应用受到时序变量的一些限制。为了解决这些限制，我们提出了一种框架感知视频扩散模型（FVDM）。该模型引入了一种新型向量化时间变量（VTV），使得每一帧可以遵循独立的噪声时间表，从而增强了模型捕捉细粒度时序依赖的能力。FVDM在多个任务中展示了其灵活性，包括标准视频生成、图像到视频生成、视频插值和长视频合成。通过多种VTV配置，我们在生成视频的质量上取得了优异成绩，克服了微调过程中的灾难性遗忘以及零-shot 方法中的局限性。我们的实证评估表明，FVDM在视频生成质量上超过了现有最先进的方法，同时在扩展任务上也表现出色。FVDM通过解决现有VDMs的基本缺陷，为视频合成设立了新的范式，提供了一个具有显著影响力的生成建模和多媒体应用的框架。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 17:48:41 GMT</pubDate>
</item>
<item>
<title>SwiftKV：提升大模型推理效率的新方法</title>
<link>https://arxiv.org/abs/2410.03960</link>
<guid>https://arxiv.org/abs/2410.03960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SwiftKV通过优化输入和缓存机制，显著提升大模型推理效率，降低资源需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了SwiftKV，一种新颖的模型变换与蒸馏方法，旨在降低企业应用中大规模语言模型（LLM）推理的时间和成本，尤其是在处理提示令牌时。同时保持生成令牌的高质量。SwiftKV结合了三个关键机制：首先，SingleInputKV通过使用早期层的输出来预填充后续层的KV缓存，从而使提示令牌跳过大部分模型计算；其次，AcrossKV通过合并相邻层的KV缓存，降低内存占用，并支持更大的批处理大小以提升吞吐量；最后，通过知识保留蒸馏程序，针对现有的LLM进行适配，确保最小的精度损失和低计算、数据需求。对Llama-3.1-8B和70B模型的实验表明，SwiftKV在预填充计算需求上减少了50%，KV缓存的内存需求减少了62.5%，并在多项任务上保持了最小的质量下降。在使用优化的vLLM实现的端到端推理服务中，SwiftKV实现了多达2倍的聚合吞吐量和60%的输出令牌时间降低。其在4个H100 GPU上实现了560 TFlops/GPU的标准化推理吞吐量，达到每秒16K个令牌的处理速度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 15:40:09 GMT</pubDate>
</item>
<item>
<title>评估语言模型在科学发现中的自动化潜力：ScienceAgentBench基准测试</title>
<link>https://arxiv.org/abs/2410.05080</link>
<guid>https://arxiv.org/abs/2410.05080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了ScienceAgentBench基准，评估语言代理在科学发现中的能力，揭示现有模型在任务执行上的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ScienceAgentBench，这是一项用于评估基于语言模型的代理在数据驱动科学发现中的能力的基准。为了确保基准的科学真实性与现实相关性，研究团队从44篇同行评审的文献中提取了102个任务，并邀请九位领域专家进行验证。每个任务的目标输出统一为自包含的Python程序文件，并使用多种评估指标来检查生成的程序、执行结果以及成本。所有任务经过多轮人工验证，以确保注释质量和科学合理性。同时，提出了两种有效策略以减轻数据污染问题。研究还评估了五种开源和专有语言模型，采用三种框架（直接提示、OpenHands和自我调试），结果显示最佳代理在三次尝试中独立完成任务的比例仅为32.4%，在专家提供知识的情况下为34.3%。这些结果凸显了当前语言代理在生成数据驱动发现代码方面的能力有限，更不用说实现科学研究的端到端自动化。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:58:13 GMT</pubDate>
</item>
<item>
<title>基于偏好的半政策优化方法及其在视觉生成中的应用</title>
<link>https://arxiv.org/abs/2410.05255</link>
<guid>https://arxiv.org/abs/2410.05255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的方法SePPO，用于优化扩散模型的生成性能，克服了传统RLHF的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为半政策偏好优化方法（SePPO），旨在克服传统强化学习方法在视觉生成任务中的局限性，如对奖励模型的普遍化能力的限制和需要大量配对人工标注数据的问题。SePPO方法利用历史检查点作为参考模型，生成在线偏好样本，替代“失败图像”中的图像，从而仅优化“胜利图像”。该方法还设计了一种参考模型选择策略，以扩展策略空间中的探索。此外，SePPO并不是简单地将参考样本视为负例，而是设计了一种基于锚点的准则，评估参考样本更可能是胜利或失败图像，从而使模型选择性学习生成的参考样本。这种方法能够减轻因参考样本质量不确定性而导致的性能下降。SePPO在文本到图像和文本到视频的基准测试中得到了验证，超越了所有先前的方法，并在文本到视频的基准测试中也表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:57:46 GMT</pubDate>
</item>
<item>
<title>基于视觉感知的GUI代理的通用视觉定位模型UGround</title>
<link>https://arxiv.org/abs/2410.05243</link>
<guid>https://arxiv.org/abs/2410.05243</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出UGround，一个基于视觉感知的通用GUI定位模型，显著提高了代理在复杂环境中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多模态大型语言模型（MLLMs）在图形用户界面（GUI）代理中的应用，强调了其依赖于强大的定位能力。现有的GUI代理主要依赖文本表示，存在噪声、信息不完整和计算开销大的问题。我们倡导一种人类般的存在形式，设计视觉感知的GUI代理，直接对GUI进行像素级操作。关键在于视觉定位模型，能够准确将GUI元素的多样指代表达映射到不同平台的坐标。本文提供了一种简单有效的训练方法，使用基于网页的合成数据和微调LLaVA架构，构建了目前最大的GUI视觉定位数据集，包含1000万个GUI元素与其指代表达，跨越130万张截图。我们训练了UGround，一个强大的通用视觉定位模型，并在六个基准测试中进行了实证验证。结果表明，UGround在GUI代理的视觉定位性能上显著优于现有模型，提升幅度达到20%。使用UGround的代理在多个在线和离线任务中表现超越现有的最先进代理，尽管后者使用了额外的文本输入。这些成果表明GUI代理以人类方式导航数字世界的可行性和潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05243" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:56:24 GMT</pubDate>
</item>
<item>
<title>大规模模型合并的实证研究：影响因素与效果</title>
<link>https://arxiv.org/abs/2410.03617</link>
<guid>https://arxiv.org/abs/2410.03617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究系统评估不同因素对大规模模型合并效果的影响，揭示了合并在提高模型泛化能力等方面的优势。</p><br /><br /><p><strong>摘要：</strong> 本研究旨在系统评估大规模模型合并的有效性，探讨模型规模、基模型质量和专家模型数量等关键因素对合并模型性能的影响。我们使用四种流行的合并方法（均值、任务算术、Dare和TIES）对1B至64B参数范围的多个完全微调模型进行合并实验，并评估合并模型在已知任务和零-shot 泛化任务上的表现。实验结果显示，当专家来源于强基模型时，合并效果更显著；同时，更大的模型能够更容易进行合并，并且合并后模型的泛化能力持续提升。尤其是在合并8个大型专家模型时，合并模型在某些情况下的泛化性能优于多任务训练模型。此外，较大的基模型能够促进更多专家模型的有效合并，而不同合并方法在大规模情况下表现相似。这些发现为未来的研究提供了重要参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:31:14 GMT</pubDate>
</item>
<item>
<title>SELECT：图像分类数据策展策略的大规模基准</title>
<link>https://arxiv.org/abs/2410.05057</link>
<guid>https://arxiv.org/abs/2410.05057</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了SELECT基准，评估图像分类数据策展策略，并创建了包含多种数据转变的ImageNet++数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SELECT基准，这是首个针对图像分类数据策展策略的大规模评估。为生成基准方法，我们创建了一个新的数据集ImageNet++，它是迄今为止最大的一组ImageNet-1K的扩展数据集。ImageNet++通过五种新的训练数据转变，每种转变的规模大约与ImageNet-1K相同，且采用了不同的策展策略。我们通过两种方式评估了策展基线：第一，利用每个训练数据转变从头开始训练相同的图像分类模型；第二，使用数据本身去拟合预训练的自监督表示。我们的研究发现了一些有趣的趋势，尤其是在数据生成和基于CLIP嵌入的查找等近期策展方法方面。尽管这些策略在某些任务中具有较强的竞争力，但组建原始ImageNet-1K数据集的策展策略仍然是金标准。我们期望我们的基准可以为新方法的探索铺平道路，以进一步弥补这一差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05057" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 12:07:26 GMT</pubDate>
</item>
<item>
<title>GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2410.05229</link>
<guid>https://arxiv.org/abs/2410.05229</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 11:36:59 GMT</pubDate>
</item>
<item>
<title>医疗领域命名实体识别基准报告</title>
<link>https://arxiv.org/abs/2410.05046</link>
<guid>https://arxiv.org/abs/2410.05046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">报告介绍了医疗领域的命名实体识别基准，评估语言模型在临床叙述中提取结构化信息的能力。</p><br /><br /><p><strong>摘要：</strong> 本技术报告提出了一个命名临床实体识别基准，用于评估语言模型在医疗领域的能力，关键在于从临床叙述中提取结构化信息，以支持自动编码、临床试验人群识别和临床决策支持等应用。该评分榜为不同语言模型，包括编码器和解码器架构，提供了一个标准化的平台，以评估其在多个医疗领域内识别和分类临床实体的能力。报告采用了一系列公开可用的临床数据集，涵盖疾病、症状、药物、程序和实验室测量等实体。这些实体均按照观察医学结果伙伴关系（OMOP）通用数据模型进行标准化，确保不同医疗系统和数据集之间的一致性和互操作性，从而对模型性能进行全面评估。模型性能的主要评估指标为F1分数，并辅以多种评估模式，以提供对模型性能的全面洞察。报告还对迄今为止评估的模型进行了简要分析，突出了观察到的趋势和局限性。通过建立这一基准框架，评分榜旨在促进透明度，促进比较分析，并推动临床实体识别任务的创新，满足医疗NLP领域对稳健评估方法的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 09:20:30 GMT</pubDate>
</item>
<item>
<title>LLaMA-Berry: 一种增强大型语言模型数学推理能力的框架</title>
<link>https://arxiv.org/abs/2410.02884</link>
<guid>https://arxiv.org/abs/2410.02884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaMA-Berry框架结合MCTS和自我精炼，优化语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的数学问题求解框架LLaMA-Berry，旨在提升大型语言模型（LLMs）的数学推理能力。该框架将蒙特卡洛树搜索（MCTS）与迭代自我精炼（Self-Refine）相结合，以优化推理路径，并使用成对奖励模型（PPRM）对不同路径进行全局评估。通过利用LLM的自我评价和重写能力，SR-MCTS克服了传统逐步和贪婪搜索算法的低效性和局限性，从而更有效地探索解空间。此外，受人类反馈的强化学习（RLHF）启发，PPRM模型根据解决方案之间的成对偏好，并利用增强博尔达计数（EBC）方法将这些偏好合成一个全球排名分数，以寻找更优答案。该方法有效地解决了数学推理任务中的评分变异性和非独立分布挑战。LLaMA-Berry框架在一般与高级基准测试上进行了测试，显示出相较于现有方法（如ToT和rStar）在搜索效率和问题解决能力方面的优越性能，尤其是在复杂的奥林匹克级基准测试中，如GPQA、AIME24和AMC23。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 06:56:51 GMT</pubDate>
</item>
<item>
<title>基于文本指令的3D环境中多阶段人类动作合成框架</title>
<link>https://arxiv.org/abs/2410.03187</link>
<guid>https://arxiv.org/abs/2410.03187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新框架，能从单一文本指令合成场景感知的人类动作。</p><br /><br /><p><strong>摘要：</strong> 本文旨在解决在复杂3D环境中合成多阶段人类动作的挑战，如行走、手部伸展和人机交互。为此，提出了一种全面框架，能够直接从单一文本指令和目标位置合成场景感知的互动动作。该方法采用自回归扩散模型合成下一个动作片段，并利用自主调度器预测每个动作阶段的过渡。为了确保合成的动作能够与环境无缝结合，提出了一种场景表示方法，考虑起始点和目标位置的局部感知。此外，通过将帧嵌入与语言输入结合，提升生成动作的连贯性。为了支持模型训练，本文提供了一个包含16小时动作序列的综合动作捕捉数据集，该数据集覆盖120个室内场景和40种不同动作类型，并附有精确的语言描述。实验结果表明，该方法在生成符合环境和文本条件的高质量多阶段动作方面具有显著效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 06:32:51 GMT</pubDate>
</item>
<item>
<title>揭秘大型语言模型内部的真相编码与错误检测</title>
<link>https://arxiv.org/abs/2410.02707</link>
<guid>https://arxiv.org/abs/2410.02707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型的内部状态能显著增强错误检测，但其通用性有限，并能预测错误类型。</p><br /><br /><p><strong>摘要：</strong> 本研究揭示大型语言模型（LLMs）内部状态在真相编码方面的丰富信息，并评估其在错误检测与预测中的应用。首先，我们发现真相信息主要集中在特定的标记中，利用这一特性能够显著提升错误检测的性能。然而，我们的研究还表明，这些错误检测器在不同数据集间的泛化能力差，表明真相编码并非普遍适用，而是具有多样性。接着，我们展示了内部表示可以用于预测模型可能产生的错误类型，从而便于制定针对性的减缓策略。最后，我们发现LLMs的内部编码与外部行为之间存在差异：尽管模型可能编码了正确答案，但却可能不断生成错误的答案。这些发现加深了我们对LLM错误的理解，能够为未来的错误分析与缓解研究提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 05:52:35 GMT</pubDate>
</item>
<item>
<title>MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2410.04698</link>
<guid>https://arxiv.org/abs/2410.04698</guid>
<content:encoded><![CDATA[
Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 05:33:52 GMT</pubDate>
</item>
<item>
<title>OmniBooth：实现实例级多模态定制的图像生成框架</title>
<link>https://arxiv.org/abs/2410.04932</link>
<guid>https://arxiv.org/abs/2410.04932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniBooth框架实现了空间控制与实例级多模态定制，提升图像生成的灵活性与可控性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniBooth，一种图像生成框架，实现了空间控制与实例级多模态定制。用户可以通过文本提示或图像参考来描述多种实例的指令。该框架允许根据用户定义的掩码及相关文本或图像指导生成图像，实现多个物体在指定坐标上定位，其属性与指导信息精确对齐。这种方法扩展了文本到图像生成的范围，并提升了其可控性。我们提出的潜在控制信号提供了一个高维空间特征，能够无缝整合空间、文本和图像条件。文本条件延伸了ControlNet，实现了实例级开放词汇生成。图像条件进一步支持精准控制和个性化身份。实际应用中，用户可以根据需要选择多模态条件，提高了生成的灵活性。此外，实验结果表明我们的方法在图像合成的保真度和对齐上表现优越，适用于不同任务和数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 04:05:24 GMT</pubDate>
</item>
<item>
<title>动态场景下的几何估计方法：Motion DUSt3R (MonST3R)</title>
<link>https://arxiv.org/abs/2410.03825</link>
<guid>https://arxiv.org/abs/2410.03825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新方法MonST3R，通过直接从动态场景中估计几何，克服了训练数据稀缺的问题。</p><br /><br /><p><strong>摘要：</strong> 在动态场景中进行几何估计仍是计算机视觉的核心挑战。传统方法常依赖于多阶段管道或全局优化，导致系统复杂并易出错。本研究提出了Motion DUSt3R (MonST3R)，一种几何优先的方法，直接对动态场景进行逐时刻的几何估计。我们通过估计每个时刻的点图，成功地将以往仅用于静态场景的DUST3R表示方法适应于动态场景。然而，动态场景的标注数据稀缺是一个显著挑战。我们通过将问题视为微调任务，识别多种合适数据集，并在有限的数据上进行战略性训练，使得该模型能够处理动态场景，即使没有显式的运动表示。基于此，我们为多个视频特定任务引入了新的优化，实现在视频深度和相机姿态估计上的强大性能，超越了以往工作的鲁棒性和效率。此外，MonST3R在前馈四维重建方面也显示了良好的成果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 03:56:16 GMT</pubDate>
</item>
<item>
<title>UniMuMo：统一多模态生成模型</title>
<link>https://arxiv.org/abs/2410.04534</link>
<guid>https://arxiv.org/abs/2410.04534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniMuMo模型通过对齐音乐和动作数据，支持多模态生成任务，实现文本、音乐与动作的统一生成。</p><br /><br /><p><strong>摘要：</strong> UniMuMo是一种统一多模态模型，能够将文本、音乐和动作数据作为输入条件，生成三种模态的输出。为了解决时间同步数据的不足， UniMuMo通过基于节奏模式对无配对的音乐和动作数据进行对齐，以利用现有的大规模音乐和动作数据集。模型将音乐、动作和文本转换为基于标记的表示，通过一个统一的编码-解码器变换器架构连接这些模态。为了支持在单一框架内的多种生成任务，UniMuMo引入了若干架构改进，包括使用音乐代码本对动作进行编码，将动作映射到与音乐相同的特征空间。此外，还提出了音乐与动作并行生成方案，将所有音乐和动作生成任务统一到一个变换器解码器架构中，仅通过音乐-动作联合生成的单一训练任务来实现。该模型通过微调现有的预训练单模态模型，显著降低了计算需求。通过广泛的实验，UniMuMo在音乐、动作和文本模态的所有单向生成基准上实现了具有竞争力的结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 03:00:38 GMT</pubDate>
</item>
<item>
<title>Diff Transformer：聚焦相关上下文的新型注意力机制</title>
<link>https://arxiv.org/abs/2410.05258</link>
<guid>https://arxiv.org/abs/2410.05258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diff Transformer通过差异注意力机制，增强相关上下文的关注，减轻无关信息的干扰，提升了语言建模的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新型的Diff Transformer架构，旨在解决传统Transformer在处理上下文时过于关注无关信息的问题。Diff Transformer通过差异注意力机制，通过计算两个单独的softmax注意力图之间的差异来生成注意力分数，从而抵消噪声，促进稀疏注意力模式的出现。在语言建模方面的实验结果表明，在不同的模型规模和训练数据量下，Diff Transformer的表现优于传统Transformer。更加重要的是，Diff Transformer在实践应用中表现出显著的优势，包括长上下文建模、关键信息检索、减轻幻觉现象、提升上下文学习能力和降低激活异常。通过减少对无关上下文的干扰，Diff Transformer能够有效减轻问答和文本摘要中的幻觉现象。在上下文学习方面，Diff Transformer不仅提高了准确性，还对顺序排列的变化表现出更强的鲁棒性，解决了长期以来存在的鲁棒性问题。研究结果显示，Diff Transformer作为一种具有高度有效性的架构，具有推动大型语言模型发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 02:49:40 GMT</pubDate>
</item>
<item>
<title>基于令牌级别的侦测奖励模型：提升多模态语言模型的细粒度反馈</title>
<link>https://arxiv.org/abs/2410.04734</link>
<guid>https://arxiv.org/abs/2410.04734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Token-Level Detective Reward Model（TLDR），用于为多模态语言模型提供细粒度的文本标注。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的Token-Level Detective Reward Model（TLDR），旨在为多模态语言模型提供细粒度的文本标注。现有的奖励模型仅提供二元反馈，无法满足长文本的多样性需求。在多模态环境中，简单的奖励模型可能导致对文本的偏见，影响对图像的理解。为了解决这一问题，本文采用了一种基于扰动的方法生成合成的困难负样本，并为这些样本生成令牌级的标签，从而训练TLDR模型。研究表明，TLDR模型在帮助现有模型自我修正生成结果和作为幻觉评估工具方面具有丰富的应用潜力。此外，TLDR模型还能够显著提高人类注释的效率，达到提高三倍的效果，从而获得更广泛的高质量视觉语言数据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 02:38:49 GMT</pubDate>
</item>
<item>
<title>VideoGuide: 提升文本到视频生成的时间一致性</title>
<link>https://arxiv.org/abs/2410.04364</link>
<guid>https://arxiv.org/abs/2410.04364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoGuide框架通过利用已有的视频扩散模型，提升文本生成视频的时间一致性和图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了VideoGuide，一个新颖的框架，旨在在不额外训练或微调的情况下，提升预训练文本到视频(T2V)模型的时间一致性。现有方法在提高一致性的同时，往往会导致图像质量下降和计算时间不切实际等问题。VideoGuide通过在推理早期阶段利用任何预训练的视频扩散模型(VDM)作为指导，改善时间质量。该方法通过在采样模型的去噪过程中的插值，增强了指导模型的去噪样本，从而实现显著的时间一致性和图像清晰度改进。VideoGuide提供了一种经济实用的解决方案，能够有效整合不同视频扩散模型的优势。此外，我们还展示了先前蒸馏的效果，表明基本模型在通过该方法利用指导模型的优越数据先验后，可以获得更好的文本一致性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 02:38:43 GMT</pubDate>
</item>
<item>
<title>Presto!: 加速文本生成音乐的新方法</title>
<link>https://arxiv.org/abs/2410.05167</link>
<guid>https://arxiv.org/abs/2410.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Presto!通过降采样步骤和降低每步成本，显著加快文本到音乐生成的速度和质量。</p><br /><br /><p><strong>摘要：</strong> 随着扩散基础文本到音乐（TTM）方法的进步，高效优质生成仍然是一个挑战。我们介绍了Presto!，这是一种通过减少采样步骤和每步成本来加速基于分数的扩散变换器推理的方法。为了减少步骤，我们开发了一种新的基于分数的分布匹配蒸馏（DMD）方法，适用于EDM系列的扩散模型，这是首个用于TTM的基于GAN的蒸馏方法。为降低每步成本，我们对近期的层蒸馏方法进行了简单而强大的改进，改善了隐藏状态方差的保持。最后，我们将步骤和层蒸馏方法结合起来，形成双重方法。我们分别评估了我们的步骤和层蒸馏方法，结果显示每种方法均可实现最佳性能。我们的联合蒸馏方法能够生成高质量的输出，改善多样性，加速基础模型10-18倍（32秒单声道/立体声44.1kHz的延迟为230/435毫秒，比可比的SOTA快15倍），这是我们所知的最快高质量TTM。声音示例可以在https://presto-music.github.io/web/找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 02:16:49 GMT</pubDate>
</item>
<item>
<title>TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles</title>
<link>https://arxiv.org/abs/2410.05262</link>
<guid>https://arxiv.org/abs/2410.05262</guid>
<content:encoded><![CDATA[
As the application of Large Language Models (LLMs) expands, the demand for reliable evaluations increases. Existing LLM evaluation benchmarks primarily rely on static datasets, making it challenging to assess model performance in dynamic interactions with users. Moreover, these benchmarks often depend on specific background knowledge, complicating the measurement of a model's logical reasoning capabilities. Other dynamic evaluation methods based on strong models or manual efforts may introduce biases and incur high costs and time demands, hindering large-scale application. To address these issues, we propose TurtleBench. TurtleBench collects real user guesses from our online Turtle Soup Puzzle platform that we developed. This approach allows for the relatively dynamic generation of evaluation datasets, mitigating the risk of model cheating while aligning assessments more closely with genuine user needs for reasoning capabilities, thus enhancing the reliability of evaluations. TurtleBench includes 1,532 user guesses along with the correctness of guesses after annotation. Using this dataset, we thoroughly evaluated nine of the most advanced LLMs available today. Notably, the OpenAI o1 series models did not achieve leading results in these evaluations. We propose several hypotheses for further research, such as "the latent reasoning of o1 utilizes trivial Chain-of-Thought (CoT) techniques" and "increasing CoT length not only provides reasoning benefits but also incurs noise costs."
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 01:19:38 GMT</pubDate>
</item>
<item>
<title>多智能体环境中的指代表达生成与理解任务研究</title>
<link>https://arxiv.org/abs/2410.03959</link>
<guid>https://arxiv.org/abs/2410.03959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究探讨多智能体环境下的指代表达生成与理解，评估自动化模型的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项多智能体环境中的指代表达生成与理解任务，旨在研究两个代理在共享场景中如何理解彼此的视觉角度，以产生和理解对场景中物体及其空间关系的指代。我们收集了2970个由人类撰写的指代表达，并对其进行与人类理解判断的配对。通过对自动化模型在与人类伙伴配对时的表现进行评估发现，在指代生成和理解方面，模型的表现均低于人类代理配对的表现。最后，我们还实验训练了一种开放权重的发言者模型，该模型在与听者配对时表现出有效的沟通成功率。经过训练，模型的沟通成功率从58.9%提升至69.3%，甚至超过了最强的专有模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 01:07:27 GMT</pubDate>
</item>
<item>
<title>基于傅里叶分析的网络架构FAN：增强周期性建模与推理能力</title>
<link>https://arxiv.org/abs/2410.02675</link>
<guid>https://arxiv.org/abs/2410.02675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出FAN网络，利用傅里叶分析增强对周期性现象的建模与推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管神经网络，尤其是多层感知机（MLP）和Transformer，取得了显著成功，但它们在建模和推理周期性方面存在潜在缺陷，往往侧重于记忆周期性数据而非理解其内在规律。本研究提出了一种新颖的网络架构FAN，基于傅里叶分析，增强了对周期现象的高效建模与推理能力。通过引入傅里叶级数，周期性自然地融入了神经网络的结构和计算过程中，从而实现了对周期模式的更准确表达与预测。作为多层感知机（MLP）的有力替代，FAN在多个模型中能够以更少的参数和计算量顺利替代MLP。通过大量实验，验证了FAN在周期函数建模与推理方面的有效性，以及其在符号公式表示、时间序列预测和语言建模等多种现实任务中的优越性和普适性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Oct 2024 00:09:36 GMT</pubDate>
</item>
<item>
<title>MLP-KAN: 一种适应性强的混合模型用于表示学习与函数学习的集成</title>
<link>https://arxiv.org/abs/2410.03027</link>
<guid>https://arxiv.org/abs/2410.03027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLP-KAN是一种新的方法，通过混合专家架构实现表示学习与函数学习的自动选择，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的方法MLP-KAN，旨在消除用户手动选择表示学习或函数学习模型的需求。MLP-KAN将多层感知机（MLP）用于表示学习，Kolmogorov-Arnold网络（KAN）用于函数学习，并嵌入在混合专家（MoE）架构中，从而能够根据任务特性动态适应，实现最佳性能。这一方法被集成在基于Transformer的框架中，在四个广泛使用的数据集上表现出色，涵盖不同领域的任务。通过广泛的实验评估，结果表明MLP-KAN在深度表示学习和函数学习任务中具有优异的适应性和竞争力表现。这些发现突显了MLP-KAN在简化模型选择过程方面的潜力，提供了一种在各个领域实现综合适应性解决方案的途径。代码与权重可在https://github.com/DLYuanGod/MLP-KAN上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 21:06:53 GMT</pubDate>
</item>
<item>
<title>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</title>
<link>https://arxiv.org/abs/2410.03051</link>
<guid>https://arxiv.org/abs/2410.03051</guid>
<content:encoded><![CDATA[
Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 20:34:15 GMT</pubDate>
</item>
<item>
<title>CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs</title>
<link>https://arxiv.org/abs/2410.01999</link>
<guid>https://arxiv.org/abs/2410.01999</guid>
<content:encoded><![CDATA[
Recent advancements in Code Large Language Models (CodeLLMs) have predominantly focused on open-ended code generation tasks, often neglecting the critical aspect of code understanding and comprehension. To bridge this gap, we present CodeMMLU, a comprehensive multiple-choice question-answer benchmark designed to evaluate the depth of software and code understanding in LLMs. CodeMMLU includes over 10,000 questions sourced from diverse domains, encompassing tasks such as code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks, CodeMMLU assesses models's ability to reason about code rather than merely generate it, providing deeper insights into their grasp of complex software concepts and systems. Our extensive evaluation reveals that even state-of-the-art models face significant challenges with CodeMMLU, highlighting deficiencies in comprehension beyond code generation. By underscoring the crucial relationship between code understanding and effective generation, CodeMMLU serves as a vital resource for advancing AI-assisted software development, ultimately aiming to create more reliable and capable coding assistants.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 17:35:59 GMT</pubDate>
</item>
<item>
<title>改进代码填充模型的水平长度预测方法</title>
<link>https://arxiv.org/abs/2410.03103</link>
<guid>https://arxiv.org/abs/2410.03103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的训练目标HLP，显著提高了代码填充任务中的模型性能。</p><br /><br /><p><strong>摘要：</strong> 填充中间(FIM)在代码语言模型中变得至关重要，但当前的方法在生成与上下文一致的内容时存在困难。现有的基于规则的后处理方法不适用于开放域代码补全任务，因此，我们提出了水平长度预测(HLP)，作为新颖的训练目标，旨在让模型在每一步预测剩余中间令牌的数量。HLP通过前瞻性规划，推动FIM的进展，使模型能够独立于数据集特定后处理，自然学习填充边界。我们的评估显示，HLP在不同模型和规模上显著提升了FIM性能，改善幅度最高可达24%，并且在文件级和仓库级任务中均有提升。此外，HLP增强的规划能力也提升了模型在代码推理方面的表现，且HLP仅带来少量训练开销，不增加推理成本，确保其实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 14:11:08 GMT</pubDate>
</item>
<item>
<title>加速自回归文本到图像生成的概率并行解码算法</title>
<link>https://arxiv.org/abs/2410.01699</link>
<guid>https://arxiv.org/abs/2410.01699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种无训练的概率并行解码算法，Speculative Jacobi Decoding，加速自回归文本到图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Speculative Jacobi Decoding（SJD）的无训练概率并行解码算法，旨在加速自回归文本到图像生成过程。与传统的自回归生成模型需要数百或数千步的下一个token预测不同，SJD通过引入概率收敛标准，允许模型在每一步预测多个token，从而在保证采样质量和多样性的前提下，显著减少生成所需的步骤。该方法克服了现有Jacobi解码方法对确定性收敛标准的依赖，使其不再局限于贪心解码。SJD的设计注重于采样过程中保留随机性，使得生成的图像更具多样性。此外，研究还探讨了利用视觉数据的空间局部性进行token初始化的策略，在特定场景下进一步提高加速比。通过在多个自回归文本到图像生成模型上的实验，我们验证了SJD的有效性，证明其在加速推理的同时不牺牲视觉质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01699" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 13:20:54 GMT</pubDate>
</item>
<item>
<title>GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs</title>
<link>https://arxiv.org/abs/2410.03645</link>
<guid>https://arxiv.org/abs/2410.03645</guid>
<content:encoded><![CDATA[
Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated objects. To automatically generate demonstration data for these tasks at scale, we propose planning and RL solvers that generalize within object categories. The pipeline can generate data for up to 100 articulated tasks with 200 objects and reduce the required human efforts. To utilize such data, we propose an effective multi-task language-conditioned policy architecture, dubbed proprioceptive point-cloud transformer (PPT), that learns from the generated demonstrations and exhibits strong sim-to-real zero-shot transfer. Combining the proposed pipeline and the policy architecture, we show a promising usage of GenSim2 that the generated data can be used for zero-shot transfer or co-train with real-world collected data, which enhances the policy performance by 20% compared with training exclusively on limited real data.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 12:57:34 GMT</pubDate>
</item>
<item>
<title>生成性扩展的树模型：一种基于能量的增强算法</title>
<link>https://arxiv.org/abs/2410.03535</link>
<guid>https://arxiv.org/abs/2410.03535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种基于能量的生成性增强算法，能够在处理表格数据时表现出与GBDT相似的性能。</p><br /><br /><p><strong>摘要：</strong> 尽管深度学习在非结构化数据领域占据主导地位，但树模型如随机森林（RF）和梯度提升决策树（GBDT）仍然是处理表格数据任务的主要工具。本文探讨了这些流行算法的生成性扩展，重点在于显式建模数据密度（至归一化常数），从而实现除采样外的其他应用。我们的主要贡献是提出一种能量基础的生成性增强算法，类似于XGBoost中实现的二阶增强。尽管生成模型能够处理任何输入变量的推断任务，我们的算法在多个真实世界的表格数据集上实现了与GBDT相似的判别性能，且优于其他生成性方法。同时，我们还表明该模型在采样方面与神经网络模型具有竞争力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 07:17:36 GMT</pubDate>
</item>
<item>
<title>NL-Eye: 评估视觉语言模型的视觉推理能力</title>
<link>https://arxiv.org/abs/2410.02613</link>
<guid>https://arxiv.org/abs/2410.02613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NL-Eye基准评估VLM在视觉推理方面的能力，展示了其在推理任务上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NL-Eye基准，该基准旨在评估视觉语言模型（VLM）在视觉推理中的表现。尽管现代VLM展现了显著能力，但它们在推理因果关系及结果的能力上仍未得到充分探索。NL-Eye通过将自然语言推理(NLI)任务的推理方式适应于视觉领域，要求模型根据前提图像评估假设图像的合理性，并解释其决策。NL-Eye包含了350个精心策划的三重示例，共计1050幅图像，涵盖了物理、功能、逻辑、情感、文化和社会等多种推理类别。数据策划过程包括两个步骤：撰写文字描述和使用文本到图像模型生成图像，均需大量人力参与以确保场景的高质量和挑战性。实验结果显示，VLM在NL-Eye上表现不佳，表现水平接近随机基线，而人类在合理性预测和解释质量上表现优异。这表明现代VLM在推理能力方面存在缺陷。NL-Eye是向开发具有强大多模态推理能力的VLM迈出的重要一步，适用于现实世界应用，包括事故预防机器人和生成视频验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:59:53 GMT</pubDate>
</item>
<item>
<title>结合视觉与语言指导的评论意识导航框架CANVAS</title>
<link>https://arxiv.org/abs/2410.01273</link>
<guid>https://arxiv.org/abs/2410.01273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CANVAS框架通过人类行为模仿学习，实现了更好的机器人导航，适应噪声指令，显示出人类指导的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CANVAS框架，该框架结合了视觉和语言指令，以实现评论意识导航。为了使机器人有效地理解和执行人类的抽象导航指令，我们构建了COMMAND数据集，包含超过48小时和219公里的人工标注导航结果。实验结果表明，CANVAS在所有测试环境中均优于强大的基于规则的系统ROS NavStack，尤其在面对噪声指令时表现出色。在果园环境中，ROS NavStack的总成功率为0%，而CANVAS则达到了67%的总成功率。此外，CANVAS能够在未见环境中与人类示范和常识约束保持紧密一致。实际上，当将CANVAS部署到现实世界中时，显示出69%的总成功率，突显了从模拟环境中的人类示范学习在实际应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01273" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:56:55 GMT</pubDate>
</item>
<item>
<title>Mamba模型在医学图像分析中的应用潜力</title>
<link>https://arxiv.org/abs/2410.02362</link>
<guid>https://arxiv.org/abs/2410.02362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mamba模型以线性复杂度优势，适合医学成像数据分析，提升诊断精度并改善患者结果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mamba作为状态空间模型的特殊案例，在医学图像分析中日益受到关注。与强大的变换器架构相比，Mamba具有线性时间复杂性，并能够高效处理长序列数据，解决了处理大型复杂数据集中的长程依赖问题。Mamba在融合多模态数据方面表现出色，能够提高诊断准确性和改善患者结果。文章分步骤阐述Mamba的能力，首先定义状态空间模型及其核心概念，然后探讨Mamba架构，如纯Mamba、U-Net变体，以及与卷积神经网络、变换器和图神经网络的混合模型。此外，文章还涵盖了Mamba的优化技术、扫描方法、数据集与应用、实验结果，最后讨论其在医学成像领域面临的挑战和未来发展方向。本综述旨在展示Mamba在克服医学图像分析中的现有障碍方面的变革潜力，并为该领域的创新进展铺平道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:23:18 GMT</pubDate>
</item>
<item>
<title>RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2409.19989</link>
<guid>https://arxiv.org/abs/2409.19989</guid>
<content:encoded><![CDATA[
Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:12:06 GMT</pubDate>
</item>
<item>
<title>基于混合专家与组聚合的股票市场预测方法MIGA</title>
<link>https://arxiv.org/abs/2410.02241</link>
<guid>https://arxiv.org/abs/2410.02241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIGA框架通过动态切换专家提升股票风格预测，显著提高预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MIGA，一种新颖的混合专家与组聚合框架，旨在针对不同风格的股票生成专业化预测。由于股票风格和市场趋势的显著差异，传统的单一模型很难充分捕捉各类股票的特性，从而导致预测准确性下降。MIGA通过动态切换不同的风格专家来克服这一问题，并引入了一种新型的内部组注意力架构，使得同组专家之间能够有效共享信息，从而提升整体性能。实验证明，MIGA在中国三大股票指数基准（CSI300、CSI500和CSI1000）上显著优于其他端到端模型。其中，MIGA-Conv在CSI300基准上的年超额收益达到24%，领先于之前的最优模型8%。此外，本文还对混合专家在股票市场预测中的应用进行了全面分析，提供了未来研究的有价值见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 04:50:12 GMT</pubDate>
</item>
<item>
<title>基于整数加法的高精度浮点数乘法近似算法 L-Mul</title>
<link>https://arxiv.org/abs/2410.00907</link>
<guid>https://arxiv.org/abs/2410.00907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">L-Mul算法通过整数加法近似浮点数乘法，显著降低能耗且保持高精度。</p><br /><br /><p><strong>摘要：</strong> 本研究发现，浮点乘法可以通过一个整数加法器以高精度进行近似，从而提出了线性复杂度乘法算法L-Mul。L-Mul在计算资源上显著低于8位浮点乘法，且实现了更高的精度。与8位浮点乘法相比，L-Mul在位级计算上消耗显著更少的资源，同时，由于整数加法所需能量远低于浮点乘法，使用L-Mul操作可以显著降低能耗，估计在逐元素浮点张量乘法中可减少95%的能量消耗，在点积中可减少80%。理论误差期望的计算表明，L-Mul具有良好的精度，实验验证显示4位尾数的L-Mul表现与float8_e4m3乘法相当，而3位尾数的L-Mul则优于float8_e5m2。在多个任务，包括自然语言理解、结构推理、数学及常识问答的基准测试中，直接将L-Mul应用于注意力机制几乎无损。此外，在变换器模型中用3位尾数的L-Mul替代所有浮点乘法，在微调及推理中与使用float8_e4m3作为累积精度达到相同的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 03:23:44 GMT</pubDate>
</item>
<item>
<title>Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise</title>
<link>https://arxiv.org/abs/2410.03017</link>
<guid>https://arxiv.org/abs/2410.03017</guid>
<content:encoded><![CDATA[
Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p&lt;0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 03:03:15 GMT</pubDate>
</item>
<item>
<title>语言模型中的概念抹除评估框架与新方法ELM</title>
<link>https://arxiv.org/abs/2410.02760</link>
<guid>https://arxiv.org/abs/2410.02760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了针对语言模型的概念抹除评估框架，并介绍了新方法ELM。</p><br /><br /><p><strong>摘要：</strong> 概念抹除在语言模型中传统上缺乏全面评估框架，导致对抹除方法的效果评估不全面。我们提出了一个评估范式，围绕三个关键标准进行评估：无辜性（完全知识移除）、无缝性（保持条件流畅生成）和特异性（保留无关任务表现）。我们的评估指标自然促使了语言记忆抹除（ELM）新方法的发展，旨在解决这三个维度的问题。ELM通过有针对性的低秩更新改变被抹除概念的输出分布，同时保持模型在提示被抹除概念时的流畅性。我们在生物安全、网络安全和文学领域的抹除任务上展示了ELM的有效性。比较分析显示，ELM在我们提出的指标上表现优越，包括在被抹除主题评估中的接近随机得分、生成的流畅性、在无关基准上保持的准确性以及在对抗攻击下的鲁棒性。我们的代码、数据和训练模型可在https://elm.baulab.info获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 02:44:40 GMT</pubDate>
</item>
<item>
<title>选择性注意机制提升语言模型性能</title>
<link>https://arxiv.org/abs/2410.02703</link>
<guid>https://arxiv.org/abs/2410.02703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">选择性注意通过减少对无关元素的关注，显著提高了语言模型的性能，并降低了计算和内存需求。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种选择性注意机制，它是对标准注意力机制的简单无参数改进，能够有效减少对不必要元素的关注。选择性注意在多种模型规模和上下文长度的语言模型中提升性能。例如，在C4数据集上训练的一系列变压器模型，采用选择性注意机制后，其性能与标准变压器模型相当，但后者拥有约两倍的注意头和参数。此外，选择性注意还允许减小注意力上下文缓冲区的大小，有效降低推理过程中的内存和计算需求。研究表明，拥有选择性注意的100M参数的变压器在C4上训练，使用512、1024和2048的上下文大小时，分别需要比不使用选择性注意的模型少16倍、25倍和47倍的注意力模块内存，同时保持相同的验证困惑度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 02:26:31 GMT</pubDate>
</item>
<item>
<title>上下文感知的文档嵌入方法研究</title>
<link>https://arxiv.org/abs/2410.02525</link>
<guid>https://arxiv.org/abs/2410.02525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种考虑文档邻居的上下文感知文档嵌入方法，显著提升了信息检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了密集文档嵌入在神经检索中的应用，指出传统的文档嵌入在某些特定检索场景下显得缺乏上下文。我们提出两种方法以实现上下文感知的文档嵌入：第一，采用替代对比学习目标，显式将邻近文档纳入批内上下文损失；第二，设计一种新的上下文架构，直接编码邻近文档的信息。实验结果表明，这两种方法均在多个场景中显著优于双编码器，尤其在域外任务中表现尤为突出。在MTEB基准上，我们的方法在没有困难负样本挖掘、分数蒸馏、特定数据集指令、跨GPU例子共享或极大批量大小的情况下，达到了最新的研究成果。该方法适用于提高任何对比学习数据集和双编码器的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02525" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:51:54 GMT</pubDate>
</item>
<item>
<title>通过模型合并提升非英语数学推理能力的跨语言迁移</title>
<link>https://arxiv.org/abs/2410.01335</link>
<guid>https://arxiv.org/abs/2410.01335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种模型融合方法，以提升非英语语言中的数学推理能力，通过层交换实现跨语言传递。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种模型合并 methodology，旨在解决在非英语语言中对大型语言模型（LLMs）进行微调的困难，尤其是在缺少任务特定数据的情况下。我们专注于数学推理，通过从相同的预训练模型出发，分别对英语的数学教学数据和目标语言的通用教学数据进行微调，培养出不同的“专家”。然后，我们直接替换数学专家的顶部和底部变压器层，采用语言专家的层，从而增强目标语言中的数学性能。经过这一过程，合并后的模型在数学基准测试MGSM上表现超越个别专家及其他合并方法，提升幅度达10%横跨四种在数学教学数据稀缺的主要语言。此外，这种层交换方法简单、便宜且直观，基于对微调过程中最重要的参数变化的解释分析。成功地以这种方式重新组合LLMs以实现跨语言迁移，开启了未来结合模型专业知识、创造模块化解决方案和跨语言转移推理能力的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:44:51 GMT</pubDate>
</item>
<item>
<title>减少视觉语言模型幻觉的知识消除算法研究</title>
<link>https://arxiv.org/abs/2410.02762</link>
<guid>https://arxiv.org/abs/2410.02762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过消除视觉语言模型幻觉的算法，优化模型内在表示，提升可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）内部表示以应对幻觉现象，尽管模型规模和训练已有进展。我们将VLMs的内部图像表示投影到语言词汇，观察到对真实物体的输出概率高于幻觉物体。此外，我们利用这些输出概率进行真实物体的空间定位。基于此方法，我们提出了一种知识消除算法，通过对幻觉物体特征进行线性正交化，来去除幻觉。我们展示了有针对性地编辑模型的潜在表示可以在保留性能的同时，减少多达25.7%的幻觉发生率，具体是在COCO2014数据集上进行测试。我们的研究表明，深入理解VLMs的潜在表示能够提升其可靠性，并实现新的能力，例如零样本分割。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:31:50 GMT</pubDate>
</item>
<item>
<title>自反蒙特卡罗树搜索算法的应用于多步骤决策任务</title>
<link>https://arxiv.org/abs/2410.02052</link>
<guid>https://arxiv.org/abs/2410.02052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出自反蒙特卡罗树搜索（R-MCTS）算法，显著提升了VLM在复杂决策任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为自反蒙特卡罗树搜索（R-MCTS）的新测试时算法，它旨在增强GPT-4o等AI代理在复杂多步骤决策任务中的能力。R-MCTS通过结合对比反思以及多智能体辩论来扩展传统的蒙特卡罗树搜索（MCTS），使得代理能够在测试期间实时探索决策空间并学习过去的交互以提高搜索效率。在VisualWebArena基准测试中，基于GPT-4o的R-MCTS代理在多个任务中相较于之前的最佳水平实现了6%到30%的相对提升。此外，测试时的搜索得到的知识能够有效地通过自学习方法进行传回，精调后的GPT-4o能够达到97%的R-MCTS性能，同时在测试时将计算资源的使用降低了四倍。研究结果还表明，精调后的GPT-4o具有良好的环境探索能力，能够评估状态并在检测到当前状态无法带来成功时进行回溯。这些发现表明，运用测试时搜索和自学习提升视觉语言模型的推理与规划能力具有良好的研究前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:15:21 GMT</pubDate>
</item>
<item>
<title>Open-RAG：增强推理能力的检索增强生成框架</title>
<link>https://arxiv.org/abs/2410.01782</link>
<guid>https://arxiv.org/abs/2410.01782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Open-RAG框架提升开源LLM的推理能力，通过动态专家选择和混合检索提高准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的框架Open-RAG，旨在提高开源大语言模型（LLM）在检索增强生成（RAG）中的推理能力。不同于现有方法，Open-RAG将任意稠密LLM转化为一种参数高效的稀疏专家混合（MoE）模型，专门处理复杂推理任务，包括单步和多步查询。该框架训练模型有效应对那些客观上相关但具有误导性的困难干扰项，从而利用潜在学习机制，动态选择相关专家并有效整合外部知识，提供更准确且上下文相关的回答。此外，研究还提出了一种混合自适应检索方法，以确定检索的必要性，并在性能提升与推理速度之间取得平衡。实验结果表明，基于Llama2-7B的Open-RAG在各种知识密集型任务中超越了现有的LLM和RAG模型，比如ChatGPT、自我检索（Self-RAG）和Command R+。研究团队已将代码和模型开源，地址为https://openragmoe.github.io/</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:40:20 GMT</pubDate>
</item>
<item>
<title>SciPrompt：低资源条件下科学文本分类的自动语境增强框架</title>
<link>https://arxiv.org/abs/2410.01946</link>
<guid>https://arxiv.org/abs/2410.01946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciPrompt框架通过自动获取科学领域相关术语，提升低资源文本分类任务中的模型性能。</p><br /><br /><p><strong>摘要：</strong> 在预训练语言模型的基础上，提示式微调已成为多类别分类任务的重要方法，尤其在低资源场景中表现出与完全微调方法相当的性能。以往的研究集中在手工设计提示模板和通过标签词映射解决分类问题，而跨领域和细粒度的提示式微调及其自动增强的语言标示器尚未得到探讨。针对这一挑战，本文提出了SciPrompt框架，旨在自动检索科学主题相关术语，以支持低资源文本分类任务。该框架在科学文献中选择语义相关的领域特定标签术语进行标示器增强。此外，本文还提出了一种新的标示策略，通过关联分数作为额外权重来提升语言模型的预测性能。实验证明，我们的方法在少样本和零样本设置下的科学文本分类任务中优于现有的最先进提示式微调方法，尤其在细粒度和新兴科学主题的分类中取得了显著效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:38:15 GMT</pubDate>
</item>
<item>
<title>基于复杂性对智能行为的探讨：元胞自动机与大型语言模型的关系</title>
<link>https://arxiv.org/abs/2410.02536</link>
<guid>https://arxiv.org/abs/2410.02536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究复杂规则对人工智能系统行为的影响，发现适度复杂性与智能表现提升之间的关系。</p><br /><br /><p><strong>摘要：</strong> 本文研究了复杂规则如何影响人工系统中智能行为的出现，重点探讨了一维的元胞自动机（ECA）。通过将不同的语言模型（LLM）训练于不同的元胞自动机，分析了规则行为的复杂性与LLM在下游任务中的表现之间的关系。研究发现，较高复杂性的规则生成的模型在推理和国际象棋走法预测任务上表现更好，而均匀系统、周期系统及高度混沌系统却表现较差。研究指出，存在一个复杂性适中的“甜点”，这有助于智能的表现。我们推测，智能的产生与预测复杂性的能力有关，并认为创造智能可能仅需接触到复杂性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:23:17 GMT</pubDate>
</item>
<item>
<title>Robin3D：基于新型数据引擎的高效3D大型语言模型</title>
<link>https://arxiv.org/abs/2410.00255</link>
<guid>https://arxiv.org/abs/2410.00255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Robin3D是一个强大的3DLLM，通过丰富的指令生成数据提升了模型的区分能力与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Robin3D，一个强大的3D大型语言模型(3DLLM)，它通过我们新型的数据引擎——鲁棒指令生成(RIG)引擎，训练于大规模的指令跟随数据。RIG生成了两类关键指令数据：一是对抗性指令跟随数据，包含混合的负样本和正样本，以增强模型的区分理解能力；二是多样性指令跟随数据，包含各种指令风格，以提高模型的泛化能力。最终，我们构建了100万个指令跟随数据，包括344,000个对抗样本、508,000个多样样本和165,000个基准训练集样本。为了更好地处理这些复杂的指令，Robin3D首先通过关系增强投影器提升空间理解能力，然后通过ID特征绑定增强对象引用和定位能力。Robin3D在五个广泛使用的3D多模态学习基准上，始终优于以往的方法，且无需任务特定的微调。在定位任务(Multi3DRefer)中取得了7.8%的提升，在描述任务(Scan2Cap)中提升了6.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:08:16 GMT</pubDate>
</item>
<item>
<title>VinePPO：优化大型语言模型推理任务的信用分配机制</title>
<link>https://arxiv.org/abs/2410.01679</link>
<guid>https://arxiv.org/abs/2410.01679</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VinePPO通过蒙特卡洛基于估计改进信用分配，超越PPO在推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究系统评估了价值网络在复杂推理任务中应用的有效性，发现其在任务中的预测表现不佳，导致高方差更新和次优性能。通过与随机基线比较，结果表明价值网络在推理任务中的表现微弱优于随机策略。为了解决这一问题，提出了VinePPO，利用语言环境的灵活性来计算无偏的蒙特卡洛估计，避免了大型价值网络的需求。实验结果显示，VinePPO在MATH和GSM8K数据集上的表现持续优于PPO和其他无RL基线，且所需的梯度更新次数减少了最多9倍，墙钟时间缩短最多3倍。这些结果强调了准确信用分配在大型语言模型强化学习微调中的重要性，展示了VinePPO作为更优替代方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01679" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 12:05:54 GMT</pubDate>
</item>
<item>
<title>小型预训练生成语言模型在国际象棋规则学习中的应用</title>
<link>https://arxiv.org/abs/2410.02426</link>
<guid>https://arxiv.org/abs/2410.02426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，小型语言模型可以通过微调学习国际象棋规则并提出合法走棋指令。</p><br /><br /><p><strong>摘要：</strong> 本文展示了小型预训练基础生成语言模型（SLMs）在学习复杂过程潜在规则方面的能力。通过受到斯特凡·茨威格的小说《棋王》的启发，研究者们使用28M和125M参数的小型语言模型，进行指令微调，样本量从1,000到1,000,000个不等，成功学习了国际象棋的规则，提出合法走棋，并准确解决棋局问题。本文还探讨了模型微调历程中各个时代对结果的影响，并通过增加指令微调样本的数量，展示了在降低模型幻觉现象方面的成效。该研究表明，即使是小型模型，也能在特定领域达到良好的学习效果，为日后在其他领域的应用提供了新的思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 11:11:31 GMT</pubDate>
</item>
<item>
<title>大语言模型性能的理论分析: 马尔可夫链的视角</title>
<link>https://arxiv.org/abs/2410.02724</link>
<guid>https://arxiv.org/abs/2410.02724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文通过将自回归语言模型与有限状态空间的马尔可夫链等价，分析了其性能的理论基础。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）出色性能的理论基础，建立了自回归语言模型与一维大小为O(T^K)的马尔可夫链之间的等价关系。通过此等价关系，我们得出了一些关于马尔可夫链的平稳分布存在性、收敛速度以及温度影响的意外发现。同时，论文还证明了预训练和上下文泛化的界限，揭示了这些结果的丰富解读。最后，通过对多种最新大语言模型的实验展示，我们验证了理论结果与实际观察之间的对应关系，突出其在自然语言处理任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 07:26:26 GMT</pubDate>
</item>
<item>
<title>多样化多重提升：提升CLIP模型的性能</title>
<link>https://arxiv.org/abs/2409.19291</link>
<guid>https://arxiv.org/abs/2409.19291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新方法，优化CLIP模型，通过稀疏激活的MoE结构显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，Contrastive Language-Image Pre-training (CLIP) 已成为多模态智能的重要基础。然而，研究发现，CLIP编码过程中的信息损失显著，仅能捕捉输入的粗粒度特征，限制了单一CLIP模型在视觉细节丰富的图像处理能力。为解决这一问题，本文提出了一种简单而有效的模型无关策略——多样化多重提升（DMU）。DMU高效地微调了一系列从稠密预训练CLIP检查点中提取的模型，这些模型捕获了不同的特征空间，仅在前馈网络（FFN）上共享参数。随后，这些模型可以转变为具有更大模型容量的CLIP-MoE，在保持计算开销最小的情况下显著提高性能。通过 extensive 实验表明，CLIP-MoE在多种零-shot 检索、零-shot 图像分类任务及下游多模态大语言模型基准测试中表现优异。此外，DMU使得将任何稠密CLIP模型转换为CLIP-MoE成为可能，可以在下游框架中无缝替代CLIP，且无需进一步调整。通过DMU，我们期望为未来多模态学习系统的高效和有效发展提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 07:19:06 GMT</pubDate>
</item>
<item>
<title>Vinoground：评估长视频理解中的时序推理能力</title>
<link>https://arxiv.org/abs/2410.02763</link>
<guid>https://arxiv.org/abs/2410.02763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明现有的多模态模型在短视频的时序推理上仍存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 如今，现代大型多模态模型（LMMs）在短视频理解上取得了一定进展，但研究表明这些模型在关键推理能力上仍存在缺陷。为此，我们引入了Vinoground，这是一项包含1000对短视频和文本描述的时序反事实LMM评估基准。我们的实验显示，现有的LMMs在判断不同动作和物体转变的时间差异时表现不佳。例如，性能最优的模型GPT-4o在我们的文本和视频评分中仅获得约50%的分数，远低于人类基准的90%。所有开源的多模态模型和基于CLIP的模型表现更差，几乎仅能达到随机猜测的水平。这项研究突显了在短视频时序推理上的问题尚未得到解决，促进了对该领域进一步研究的关注。数据集和评估代码可在https://vinoground.github.io获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 06:25:59 GMT</pubDate>
</item>
<item>
<title>自适应投影引导：提高扩散模型生成质量的新方法</title>
<link>https://arxiv.org/abs/2410.02416</link>
<guid>https://arxiv.org/abs/2410.02416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了自适应投影引导（APG）方法，以改进扩散模型中的无分类器引导，解决过饱和问题。</p><br /><br /><p><strong>摘要：</strong> 本文回顾了分类器无关引导（CFG）的更新规则，并提出了改进方案以解决生成图像的过饱和和不真实伪影问题。通过将CFG的更新项分解为与条件模型预测的平行和正交分量，发现平行分量导致过饱和，而正交分量增强了图像质量。基于此，本文建议对平行分量进行下调，以便在不出现过饱和的情况下实现高质量生成。此外，本文还将CFG与梯度上升联系起来，提出了一种新的重标定和动量方法，形成自适应投影引导（APG）。APG保持了CFG在提高质量方面的优势，同时能够在无过饱和的情况下使用更高的引导尺度。APG易于实现，并且对采样过程几乎不增加计算开销。通过广泛的实验，验证了APG与各种条件扩散模型和采样器兼容，改进了FID、召回率和饱和度评分，同时保持了与CFG相当的精度，使其成为一种优于传统分类器无关引导的即插即用替代方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 04:16:57 GMT</pubDate>
</item>
<item>
<title>基于LintSeq生成编辑数据提升代码合成性能</title>
<link>https://arxiv.org/abs/2410.02749</link>
<guid>https://arxiv.org/abs/2410.02749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过LintSeq算法生成编辑序列，提升小型LLM在代码合成任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种合成数据生成算法LintSeq，旨在解决现有开源代码编辑数据匮乏的问题。该算法利用代码检查工具将现有代码重构为一系列代码编辑，通过程序增量的方式生成错误-free的插入操作，输出程序差异的文本序列。为验证LintSeq的有效性，研究者将其用于将指令和程序对的数据集重构为指令和程序差异序列元组，并对一系列从2.6B到14B参数的小型LLM进行指令微调，比较其在无零-shot情况下的代码合成基准表现。结果表明，通过多次采样，基于编辑序列微调的模型在输出多样性和基准覆盖的推理时刻表现优异。例如，在HumanEval的pass@50评估中，经过合成编辑序列微调的小型LLM的表现与GPT-4相当，且较基线模型提高了20%（+/-3%）。此外，本研究还展示了预训练的轻量级模型在代码理解上的潜力，微调150M参数的编辑序列模型时，其性能可与代码模型对比，甚至在一些情况下超越Codex和AlphaCode。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 04:10:28 GMT</pubDate>
</item>
<item>
<title>Distilling an End-to-End Voice Assistant Without Instruction Training Data</title>
<link>https://arxiv.org/abs/2410.02678</link>
<guid>https://arxiv.org/abs/2410.02678</guid>
<content:encoded><![CDATA[
Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using &gt;100x less training compute.
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:45:46 GMT</pubDate>
</item>
<item>
<title>L-CiteEval: 评估长上下文模型理解能力与可信度的全新基准</title>
<link>https://arxiv.org/abs/2410.02115</link>
<guid>https://arxiv.org/abs/2410.02115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究推出L-CiteEval基准，评估长上下文模型的理解能力与引用准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出L-CiteEval，一个全面的多任务基准，旨在评估长上下文模型（LCMs）在理解能力和可信度方面的表现。L-CiteEval涵盖了来自不同领域的11个任务，支持的上下文长度从8K到48K，并提供一个完全自动化的评估工具。通过对11个顶尖封闭源和开源LCMs的测试，发现这些模型在生成结果上的差异较小，但开源模型在引用准确性和召回率方面显著落后于封闭源模型，表明当前的开源LCMs倾向于依赖其自身知识而非给定上下文，从而在实际应用中可能影响用户体验。此外，研究还评估了RAG方法，发现RAG可以显著改善LCMs的可信度，尽管生成质量略有下降。最后，研究揭示LCMs的注意力机制与引用生成过程之间存在相关性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:35:24 GMT</pubDate>
</item>
<item>
<title>将预训练的大型语言模型应用于医学图像分割的研究</title>
<link>https://arxiv.org/abs/2410.02458</link>
<guid>https://arxiv.org/abs/2410.02458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探索了通过集成预训练LLM来增强医学图像分割的ViT模型，取得了显著性能提升。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了将预训练的大型语言模型（LLM）应用于医学图像分割的方法，聚焦于增强视觉变换器（ViT）模型的性能。研究中，我们将一个冻结的LLM变换器模块集成到ViT模型的编码器中，从而在多个医学影像模态上显著提高了分割效果。我们提出了一种混合注意力机制，结合了全局和局部特征学习，并使用多尺度融合模块在不同尺度上聚合特征。经过实验验证，增强模型的平均Dice系数从0.74提升至0.79，同时在准确率、精确度和Jaccard指数等指标上也表现出显著改善。这些结果表明，基于LLM的变换器在提升医学图像分割性能方面的有效性，显示出其在提高模型准确性和鲁棒性方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:07:56 GMT</pubDate>
</item>
<item>
<title>Depth Pro：无须元数据的零样本测量单目深度估计模型</title>
<link>https://arxiv.org/abs/2410.02073</link>
<guid>https://arxiv.org/abs/2410.02073</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Depth Pro是一种快速生成高分辨率深度图的零样本模型，具有绝对尺度和边界精度。</p><br /><br /><p><strong>摘要：</strong> Depth Pro是一种新型的基础模型，专注于零样本测量单目深度估计。该模型能够合成高分辨率的深度图，具备无与伦比的清晰度和高频细节，且输出的深度预测具有绝对尺度，完全不依赖于摄像头内参等元数据。此外，模型具备高处理速度，能够在标准GPU上以0.3秒生成2.25兆像素深度图。Depth Pro的优越性能源于多项技术创新，包括高效多尺度视觉变换器用于密集预测、结合真实与合成数据集的训练协议以实现高度的度量精度和细致边界跟踪、专门用于评估深度图边界精度的评价指标，以及从单幅图像中实现的最先进的焦距估计。通过大量实验，本文分析了具体的设计选择，并展示了Depth Pro在多个维度上超越以往工作的表现。相关代码和权重已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02073" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 02:00:44 GMT</pubDate>
</item>
<item>
<title>增强CLIP的区域表征能力：对比定位语言图像预训练的进展</title>
<link>https://arxiv.org/abs/2410.02746</link>
<guid>https://arxiv.org/abs/2410.02746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种改进CLIP模型的方法，增强其区域理解能力，以支持多模态大语言模型的下游任务。</p><br /><br /><p><strong>摘要：</strong> 近年来，对比语言图像预训练（CLIP）方法因其在图像和文本表示生成方面的有效性而备受关注。CLIP的成功使其成为多模态大语言模型（MLLM）的视觉骨干，连接图像输入和语言交互。然而，CLIP的基于网络爬虫的文本注释对图像的对齐方式在需要细粒度视觉表示的下游任务中可能不足，尤其是区域级理解任务。为了解决这一问题，作者提出了对比定位语言图像预训练（CLOC）的方法，通过结合区域-文本对比损失和模块来提高CLIP的定位能力。此外，作者还提出了“可提示嵌入”这一新概念，使编码器可以根据空间提示将图像嵌入轻松转换为区域表示。为支持大规模预训练，设计了一种视觉丰富和空间定位的标注框架，以有效生成区域-文本伪标签。通过扩展到数十亿张带注释的图像，CLOC为图像区域识别和检索任务提供了高质量的区域嵌入，并可以作为CLIP的替代方案，提升MLLM在指代和定位任务上的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:25:38 GMT</pubDate>
</item>
<item>
<title>Loong：一种新型自回归大语言模型视频生成器</title>
<link>https://arxiv.org/abs/2410.02757</link>
<guid>https://arxiv.org/abs/2410.02757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Loong模型能生成分钟级长视频，采用短至长渐进训练和特殊推断策略，提升视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 本研究深入分析了自回归大语言模型（LLM）在视频生成领域面临的挑战，尤其是生成长视频的困难。尽管LLM在自然语言处理方面取得了显著成功，但现有研究主要集中在短视频（几秒钟）的生成上。为了解决这一问题，本文提出了一种新的自回归LLM视频生成器Loong，能够生成分钟长的视频。我们将文本和视频标记建模为一个统一的序列进行训练，并提出了一种短至长的渐进训练方式，结合损失重加权策略，缓解长视频训练中的损失不平衡问题。此外，本文还探讨了推断策略，包括视频标记重新编码和采样策略，以减少推断过程中的误差累积。Loong在10秒视频上进行训练，并能够基于文本提示扩展生成分钟级长视频，展现了良好的效果。更多示例可参考： https://epiphqny.github.io/Loong-video</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:25:31 GMT</pubDate>
</item>
<item>
<title>改进多模态模型的标题生成与原始描述的互动研究</title>
<link>https://arxiv.org/abs/2410.02740</link>
<guid>https://arxiv.org/abs/2410.02740</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的标题生成流程，以优化多模态模型的预训练效果，探讨合成标题与AltText的关系。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖、可控且可扩展的标题生成流程，旨在生成多种不同格式的标题，以满足各类多模态模型的需求。通过将短合成标题（Short Synthetic Captions，SSC）和密集合成标题（Dense Synthetic Captions，DSC+）作为案例研究，系统性地探讨了这些标题在多模态模型（如CLIP、多模态大语言模型及扩散模型）中的效果及其与原始网页抓取的替代文本（AltTexts）的相互作用。研究结果表明，采用混合方法同时保留合成标题和AltTexts的策略，能够在模型对齐和性能方面超越单独使用合成标题的效果，各个模型对特定标题格式存在独特偏好。这一综合分析为优化标题生成策略提供了宝贵的见解，从而推动多模态基础模型的预训练进程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02740" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:11:19 GMT</pubDate>
</item>
<item>
<title>SageAttention：一种高效的注意力量化方法</title>
<link>https://arxiv.org/abs/2410.02367</link>
<guid>https://arxiv.org/abs/2410.02367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SageAttention是一种新型的注意力量化方法，相比于传统方法在性能和准确性上显著提升。</p><br /><br /><p><strong>摘要：</strong> 本文分析了注意力机制的量化可行性，并提出一种名为SageAttention的高效量化方法。该方法在处理长序列时，显著降低了计算复杂度。实验表明，SageAttention在每秒操作次数（OPS）上比FlashAttention2和xformers分别快约2.1倍和2.7倍。此外，SageAttention在准确性方面也优于FlashAttention3。综合实验结果表明，该方法在大型语言处理、图像生成和视频生成等多种模型中，几乎没有造成端到端指标的损失，为加速模型推理提供了有效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:08:56 GMT</pubDate>
</item>
<item>
<title>Synthio：基于文本到音频扩散模型的小规模音频分类数据集增强方法</title>
<link>https://arxiv.org/abs/2410.02056</link>
<guid>https://arxiv.org/abs/2410.02056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Synthio通过合成音频数据提升小规模音频分类的准确性，克服了传统增强技术的局限。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Synthio的方法，通过合成音频数据增强小规模音频分类数据集，以提高分类准确性。传统的数据增强技术（如添加随机噪声或遮罩片段）难以捕捉真实音频的多样性。为了解决这一问题，Synthio利用文本到音频（T2A）扩散模型生成合成音频。然而，合成有效的增强数据面临挑战：生成的数据不仅需要与小规模数据集的声学特性保持一致，还需具备足够的组成多样性。为克服第一个挑战，Synthio通过偏好优化对T2A模型生成的数据与小规模数据集进行对齐，从而确保生成的音频在声学特征上与原数据集一致。而为了解决第二个挑战，文章提出了一种新颖的字幕生成技术，利用大型语言模型的推理能力生成多样化且富有意义的音频字幕，并迭代优化其质量。生成的字幕用于提示对齐的T2A模型。通过在十个数据集和四种模拟的有限数据场景中进行广泛评估，结果显示，Synthio在所有基线方法上均提高了0.1%-39%的性能，尽管该T2A模型仅在弱标注的AudioSet上进行训练。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:08:20 GMT</pubDate>
</item>
<item>
<title>基于多视角优化的3D高斯注入生成方法研究</title>
<link>https://arxiv.org/abs/2410.02103</link>
<guid>https://arxiv.org/abs/2410.02103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的3D高斯优化方法，改善了体积渲染的质量和效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的3D高斯优化方法，旨在解决传统3D高斯渲染在新视图合成中出现的不满意外观和不精确3D几何体等问题。该方法有四个关键创新：第一，转变为多视角训练策略，通过多视角调节优化3D高斯属性，以避免对某些训练视图的过拟合，从而提升多种场景中的整体准确性。第二，借鉴额外视图的益处， propone了一种交叉内在引导方案，推动不同分辨率的粗到细训练过程。第三，基于多视角调节训练，提出了一种交叉射线稠密化策略，在多视角选择中稠密化射线交点区域的高斯内核。第四，通过研究稠密化策略发现，当某些视图存在显著差异时，需增强稠密化效果。为解决此问题，提出了一种新异的多视角增强稠密化策略，鼓励3D高斯根据需要稠密化，从而改善重建准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:02:07 GMT</pubDate>
</item>
<item>
<title>LLaVA-Critic：首个开源多模态评估模型</title>
<link>https://arxiv.org/abs/2410.02712</link>
<guid>https://arxiv.org/abs/2410.02712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-Critic是一种开源多模态模型，能有效进行任务评估和偏好学习，推动未来的模型对齐研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LLaVA-Critic，这是第一个开源的大型多模态模型，旨在作为通用性评估者来评估多种多模态任务的性能。LLaVA-Critic通过一个高质量的评论指令集进行了训练，该数据集涵盖了多种评价标准和场景。实验结果表明，该模型在两个主要领域的有效性：（1）LMM作为评委，LLaVA-Critic提供可靠的评估分数，其表现与多个评估基准上的GPT模型相当或更佳；（2）偏好学习方面，该模型为偏好学习生成奖励信号，增强了模型的对齐能力。这项工作强调了开源多模态模型在自我评价和评估中的潜力，为未来关于可扩展的超人类模型对齐反馈机制的研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 00:55:11 GMT</pubDate>
</item>
<item>
<title>LLaVA-Video-178K：高质量合成视频指令遵循数据集</title>
<link>https://arxiv.org/abs/2410.02713</link>
<guid>https://arxiv.org/abs/2410.02713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出LLaVA-Video-178K数据集，助力视频指令遵循模型LLaVA-Video的训练与性能提升。</p><br /><br /><p><strong>摘要：</strong> 视频大规模多模态模型（LMMs）在发展中面临从网络获取高质量原始数据的挑战。为了解决这一问题，我们提出了一种替代方法，创建了一个专门用于视频指令遵循的高质量合成数据集——LLaVA-Video-178K。该数据集涵盖了详细的字幕生成、开放式问题回答以及多选题回答等关键任务。通过在该数据集上进行训练，并结合已有的视觉指令调优数据，我们推出了新的视频LMM——LLaVA-Video。我们的实验证明，LLaVA-Video在多个视频基准测试中表现优异，显示出该数据集的有效性。我们计划发布该数据集、其生成管道和模型检查点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 00:45:02 GMT</pubDate>
</item>
<item>
<title>基于多教师知识蒸馏的视觉模型优化研究</title>
<link>https://arxiv.org/abs/2410.01680</link>
<guid>https://arxiv.org/abs/2410.01680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了教师激活统计对学生模型质量的影响并提出了PHI标准化技术。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多教师知识蒸馏在视觉基础模型中的应用，重点分析了教师的激活统计特性对学生模型质量的影响。通过研究损失函数和统计归一化技术，旨在更好地对齐不同的分布并评估其效果。此外，还考察了在下游教师匹配指标上的影响，强调了Hadamard矩阵的使用。通过使用Hadamard矩阵，本文展示了其在各维度标准化中的有用属性，提出了一种新技术“PHI标准化”(PHI-S)，其能够实现各维度的等距标准化。实验证明，PHI-S方法在所研究的方法中产生了最佳的学生模型效果，展示了其在非标签的异质多教师知识蒸馏中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 16:44:28 GMT</pubDate>
</item>
<item>
<title>EmoKnob: 细粒度情感控制的语音合成框架</title>
<link>https://arxiv.org/abs/2410.00316</link>
<guid>https://arxiv.org/abs/2410.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmoKnob框架实现了情感语音的细粒度控制，超越了商业TTS服务的表现。</p><br /><br /><p><strong>摘要：</strong> EmoKnob是一个新提出的框架，旨在实现语音合成中的细粒度情感控制。尽管现有的文本到语音技术已经能够生成自然和富有表现力的语音，但用户无法选择情感和控制情感强度。该框架利用最新的基础语音克隆模型所带来的表现力发言者表征空间，并通过少量示范样本来实现任意情感的控制。此外，EmoKnob还提出了两种方法，使用户能够根据开放式文本描述的情感进行控制，提供直观的界面以应对多样化的情感需求。为了推动情感语音合成领域的系统化发展，EmoKnob引入了一套评估指标，旨在严格评估情感控制框架的真实度和可识别性。通过客观和主观评估，研究表明，该情感控制框架能够有效地在语音中嵌入情感，并超越了商业TTS服务的情感表现力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 15:52:40 GMT</pubDate>
</item>
<item>
<title>跨语言检索增强生成模型在地缘政治争议中的表现研究</title>
<link>https://arxiv.org/abs/2410.01171</link>
<guid>https://arxiv.org/abs/2410.01171</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了跨语言检索增强生成模型在地缘政治争议中的局限性及其数据集构建。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了跨语言检索增强生成（RAG）模型在处理地缘政治争议时所面临的挑战。尽管大型语言模型在创意生成方面表现出色，但在幻觉和偏见问题上依然存在不足。RAG模型旨在通过依赖准确和最新的信息来源来增强响应的真实性，但如何选择和加权这些信息来源仍是一个重要问题。我们创建了一个数据集，以研究现有系统在回答有关跨语言地缘政治争议的查询时的鲁棒性。该数据集来自于维基百科相关页面，研究了额外上下文的包含及其语言和来源的组成对模型响应的影响。结果表明，现有的RAG系统在跨语言使用案例中依然面临挑战，当提供多种语言的竞争性信息时，响应的连贯性和准确性往往受到影响。文中还展示了一些案例研究，并针对未来研究提出了改进建议。我们将数据集和代码公开提供，网址为https://github.com/manestay/bordIRlines。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01171" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 14:07:00 GMT</pubDate>
</item>
<item>
<title>SonicSim：针对移动声源的合成数据生成工具及其应用</title>
<link>https://arxiv.org/abs/2410.01481</link>
<guid>https://arxiv.org/abs/2410.01481</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SonicSim是一种生成移动声源合成数据的工具，旨在提升语音分离与增强模型的效果。</p><br /><br /><p><strong>摘要：</strong> 在移动声源条件下，语音分离与增强模型的系统评估通常需要丰富多样的数据。然而，真实世界的数据集往往无法满足模型的训练与评估需求，合成数据集虽然数量庞大，但其声学模拟却缺乏真实感。为了解决这些问题，我们提出了SonicSim，一个旨在生成高度可定制化的移动声源合成数据的工具。SonicSim建立在Habitat-sim的基础上，支持场景级、麦克风级和源级的多层次调整，从而生成更为多样的合成数据。基于SonicSim，我们构建了一个移动声源基准数据集SonicSet，结合Librispeech、FSD50K和FMA等数据集，以及来自Matterport3D的90个场景，用于评估语音分离和增强模型。此外，我们随机选取了SonicSet验证集中的5小时无混响原始数据，录制了一个真实世界的语音分离数据集，并与对应的合成数据集进行了比较。同时，我们还利用真实世界的语音增强数据集RealMAN来验证SonicSet与其他合成数据集之间的声学差距。结果表明，SonicSim生成的合成数据能够有效地推广到真实世界场景中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01481" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 13:54:50 GMT</pubDate>
</item>
<item>
<title>Old Optimizer, New Norm: An Anthology</title>
<link>https://arxiv.org/abs/2409.20325</link>
<guid>https://arxiv.org/abs/2409.20325</guid>
<content:encoded><![CDATA[
Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of R^{mtimes n}, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 13:04:12 GMT</pubDate>
</item>
<item>
<title>FactAlign：提升大型语言模型长篇响应的事实准确性</title>
<link>https://arxiv.org/abs/2410.01691</link>
<guid>https://arxiv.org/abs/2410.01691</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FactAlign是一个新框架，用于增强大型语言模型长篇回答的事实准确性，融合细粒度评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FactAlign，一个旨在提升大型语言模型（LLMs）长篇响应事实准确性的对齐框架，同时保持其有用性。我们引入了fKTO，一种细粒度句子级对齐算法，扩展了Kahneman-Tversky优化（KTO）对齐方法。FactAlign利用近期在自动事实性评估方面的进展，通过细粒度事实性评估指导对齐过程。实验结果表明，在开放领域提示和信息寻求问题上，FactAlign显著提高了LLM响应的事实准确性和有用性。进一步分析表明，FactAlign能够训练LLM提供更多信息而不损失事实精确性，从而提高事实F1得分。我们的源代码、数据集和训练模型已公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01691" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 12:50:05 GMT</pubDate>
</item>
<item>
<title>InfiniPot：高效管理长输入上下文的框架</title>
<link>https://arxiv.org/abs/2410.01518</link>
<guid>https://arxiv.org/abs/2410.01518</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniPot是一个新颖的KV缓存控制框架，旨在解决LLM在内存受限环境中的长输入上下文处理问题。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了InfiniPot，一个新颖的KV缓存控制框架，旨在使预训练的大型语言模型（LLMs）在固定内存限制下有效管理长序列。InfiniPot利用持续上下文蒸馏（CCD）技术，这是一种通过新颖的重要性指标压缩和保留关键信息的迭代过程，即使在没有未来上下文的情况下，依然能够保持关键信息。综合评估结果表明，InfiniPot在多种自然语言处理（NLP）任务中明显优于为长上下文训练的模型，证明了其有效性和多样性。这项工作标志着向开发适用于更广泛现实场景的大型语言模型迈出了重要一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01518" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 12:25:16 GMT</pubDate>
</item>
<item>
<title>构建开放源代码的欧盟语言语音基础模型</title>
<link>https://arxiv.org/abs/2410.01036</link>
<guid>https://arxiv.org/abs/2410.01036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们收集了950k小时的语音数据，并发布了441k小时的自动转录，为欧盟语言的开放源语音模型奠定基础。</p><br /><br /><p><strong>摘要：</strong> 随着基础模型的兴起以及针对其风险与影响的监管努力，开源模型引起了广泛关注。然而，目前的语音基础模型并未完全符合开源原则，尽管有相关声明，因为现有模型并不公开可用的权重、代码和训练数据。本文的工作旨在填补这一空白，专注于欧盟的24种官方语言。我们通过调查自动语音识别数据集和符合开源许可的未标记语音语料，收集了950k小时的训练数据。此外，我们还在宽松的CC-BY许可下发布了441k小时未标记数据的自动转录，从而为欧盟语言的开源语音基础模型的创建提供了支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 11:00:01 GMT</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 10:14:25 GMT</pubDate>
</item>
<item>
<title>E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</title>
<link>https://arxiv.org/abs/2409.18111</link>
<guid>https://arxiv.org/abs/2409.18111</guid>
<content:encoded><![CDATA[
Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level &amp; Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 08:16:21 GMT</pubDate>
</item>
<item>
<title>在联邦学习中结合LoRA的联邦共享低秩适应方法</title>
<link>https://arxiv.org/abs/2410.01463</link>
<guid>https://arxiv.org/abs/2410.01463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FedSA-LoRA，结合LoRA的低秩矩阵在联邦学习中实现知识共享与客户端专用知识的有效整合。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在联邦学习中应用低秩适应（LoRA）的方式，分析了学习到的A和B矩阵的非对称性，发现A矩阵负责学习一般知识，而B矩阵则专注于捕捉客户端的特定知识。基于此发现，本文引入了联邦共享低秩适应（FedSA-LoRA）方法，该方法使用两个低秩可训练矩阵A和B对权重更新进行建模，但仅将A矩阵与服务器共享以进行聚合。同时，本文还探讨了其他LoRA变体（如rsLoRA和VeRA）中学习到的A和B矩阵之间的关系，发现这一模式是一致的。因此，我们将FedSA-LoRA方法扩展到这些LoRA变体中，形成FedSA-rsLoRA和FedSA-VeRA。通过这种方式，我们建立了一种将LoRA与联邦学习相结合的通用范式，为未来的LoRA变体与FL的结合研究提供了指导。实验证明了该方法在自然语言理解和生成任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 07:20:19 GMT</pubDate>
</item>
<item>
<title>大型语言模型在小学数学问题求解中的推理能力研究</title>
<link>https://arxiv.org/abs/2410.01748</link>
<guid>https://arxiv.org/abs/2410.01748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大型语言模型在解答组合数学问题时存在显著推理差距，尤其是较小和专门化的模型表现更为明显。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在解决小学数学问题（GSM）中的推理能力。我们通过评估多对现有数学文字问题，让第二个问题的答案依赖于第一个问题的解答。研究发现，大多数LLMs在解决组合问题和独立问题时存在显著的推理差距。这种差距在较小、成本更低且专注于数学的模型中更加明显。此外，指令调优和代码生成对不同规模的LLMs影响各异，而在GSM上的微调会导致任务过拟合。我们的分析表明，较大的推理差距并非由于测试集泄漏，而是由于额外上下文的干扰和第二步推理的不足。总体而言，尽管LLMs在标准基准测试中的表现看似良好，它们在推理能力上存在系统性的差异。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 07:04:21 GMT</pubDate>
</item>
<item>
<title>基于对比偏好的翻译质量优化研究</title>
<link>https://arxiv.org/abs/2409.20059</link>
<guid>https://arxiv.org/abs/2409.20059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了对比偏好优化对翻译质量的影响，发现其在高质量数据上优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于利用对比偏好优化（CPO）技术提高机器翻译（MT）质量。CPO与监督微调（SFT）进行了广泛实验比较，结果表明CPO在高质量数据上的表现优于SFT，并在对齐度量方面持续取得良好效果。然而，CPO在下游评价指标中的稳定性存在问题，尤其是在神经指标和词汇指标间。研究还表明，依赖基础模型生成候选翻译的效果可与多种外部系统相媲美，同时在下游指标上保持更好的稳定性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 06:30:16 GMT</pubDate>
</item>
<item>
<title>3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection</title>
<link>https://arxiv.org/abs/2410.01647</link>
<guid>https://arxiv.org/abs/2410.01647</guid>
<content:encoded><![CDATA[
Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 05:50:23 GMT</pubDate>
</item>
<item>
<title>Scylla：量化评估大型语言模型的泛化能力框架</title>
<link>https://arxiv.org/abs/2410.01769</link>
<guid>https://arxiv.org/abs/2410.01769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Scylla框架，量化评估LLMs的泛化与记忆能力，研究任务复杂性对模型表现的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Scylla，一个动态评估框架，用于量化大型语言模型（LLMs）的泛化能力。Scylla通过评估模型在同分布（ID）和异分布（OOD）数据上的表现，分析了20个任务在5个复杂度水平下的表现，从而将泛化与记忆分离。研究发现任务复杂性与ID、OOD数据表现之间存在非单调关系，形成了所谓的“泛化谷”。具体而言，揭示了一个关键阈值——被称为关键复杂性——在该点上，模型对不可泛化行为的依赖达到峰值，标志着LLMs泛化能力的上限。随着模型规模的增加，关键复杂性向更高的任务复杂度移动，表明更大规模的模型在复杂推理任务中能够处理更多复杂性，而不依赖于记忆。利用Scylla和关键复杂性概念，本文对28个LLMs进行了基准测试，包括LLaMA和Qwen等开源模型，以及Claude和GPT等闭源模型，从而提供了更为稳健的评估，帮助更清晰地理解LLMs的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 03:41:52 GMT</pubDate>
</item>
<item>
<title>自我精炼规划方法：一种高效的长远行动序列生成方案</title>
<link>https://arxiv.org/abs/2410.01440</link>
<guid>https://arxiv.org/abs/2410.01440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种自我精炼的规划策略，通过循环优化生成行动序列，提升机器人任务规划能力。</p><br /><br /><p><strong>摘要：</strong> 在自主机器人行动实施过程中，任务规划是一个主要挑战，需要将高级任务描述转化为长远的行动序列。尽管最近的语言模型代理在此领域取得了一些进展，但仍然存在规划错误和前瞻性有限的问题。为了解决这些局限性，本文提出了一种自我精炼的方案，通过迭代完善草案计划直到达到平衡。这一过程从分析的角度进行端到端优化，无需额外的验证器或奖励模型，简化为监督学习方式训练自我精炼规划器。此外，设计了一种嵌套平衡序列建模程序，实现高效的闭环规划，并从环境（或内部世界模型）中获取有用反馈。我们在VirtualHome-Env基准上评估了该方法，结果显示出先进的表现和更好的推理计算扩展能力。代码已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 03:05:45 GMT</pubDate>
</item>
<item>
<title>LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks</title>
<link>https://arxiv.org/abs/2410.01744</link>
<guid>https://arxiv.org/abs/2410.01744</guid>
<content:encoded><![CDATA[
Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose \OurMethod, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across a wide range of benchmarks demonstrate our model's superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 02:55:21 GMT</pubDate>
</item>
<item>
<title>多粒度调试器（MGDebugger）：一种层次化代码调试新方法</title>
<link>https://arxiv.org/abs/2410.01215</link>
<guid>https://arxiv.org/abs/2410.01215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MGDebugger通过多级别分析及修复代码错误，显著提升生成代码的调试准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为多粒度调试器（MGDebugger）的新型层次化代码调试系统，旨在解决现有LLM（大型语言模型）生成代码中存在的细微错误问题。MGDebugger通过将有问题的代码分解为层次化的子函数树结构，针对不同粒度的错误进行隔离、识别和修复。该系统采用自下而上的方式逐级分析和调试每个子函数。此外，MGDebugger提出了一种模拟Python执行器以跟踪代码执行和重要变量状态，从而准确定位错误。实验结果显示，MGDebugger在HumanEval测试中，比现有调试系统的准确率提高了18.9%，在HumanEvalFix中的修复成功率高达97.6%。MGDebugger能够有效修复不同类别和难度级别的错误，展现出其强大的鲁棒性和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 02:20:13 GMT</pubDate>
</item>
<item>
<title>精确体积椭球渲染方法的研究</title>
<link>https://arxiv.org/abs/2410.01804</link>
<guid>https://arxiv.org/abs/2410.01804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出了一种实时可微分的精确体积椭球渲染方法，具有更高的渲染准确性和更低的混合问题。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为Exact Volumetric Ellipsoid Rendering (EVER)的实时可微分发射仅体积渲染方法。与最近的3D高斯点云化（3DGS）基于光栅化的方法不同，我们的基于基元的表示允许进行精确的体积渲染，而不是3D高斯广告牌的阿尔法合成。因此，Our方法不受跳跃伪影和视依赖密度的影响，同时在NVIDIA RTX4090上以720p实现了30 FPS的帧率。由于我们的方案建立在光线追踪的基础上，因此能够实现景深模糊和相机畸变（例如鱼眼相机）的效果，这些效果在光栅化中难以实现。我们的实验表明，与3DGS及其后续的视一致性渲染工作相比，EVER在大规模Zip-NeRF数据集中的表现更为准确，混合问题更少，在实时技术中Achieves最佳的清晰结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:51:45 GMT</pubDate>
</item>
<item>
<title>基于提示自适应工作流生成的文本到图像生成方法</title>
<link>https://arxiv.org/abs/2410.01731</link>
<guid>https://arxiv.org/abs/2410.01731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种自动调整工作流的方法，以提高文本到图像生成的质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的任务——提示自适应工作流生成，旨在根据用户提示自动调整生成流程。随着文本到图像生成技术的发展，工作流的使用从简单模型转向结合多个专用组件的复杂流程。尽管这种工作流方法可提升图像质量，但有效构建工作流需要专业知识，因为涉及组件众多且相互依赖性复杂，并且需依赖生成的提示。为解决这一问题，本文提出两种基于大语言模型（LLM）的方法：一种是基于调优的方法，利用用户偏好数据进行学习；另一种是无训练方法，借助LLM选择现有的工作流。与单一模型或通用、与提示无关的工作流相比，两种方法均能显著提升生成图像的质量。研究表明，基于提示的工作流预测为改善文本到图像生成质量提供了新途径，补充了该领域现有的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:50:07 GMT</pubDate>
</item>
<item>
<title>奖励模型的比较与新方法的提出</title>
<link>https://arxiv.org/abs/2410.01257</link>
<guid>https://arxiv.org/abs/2410.01257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文发布新数据集，比较Bradley-Terry与回归模型的效果，并提出结合方法，训练出高性能奖励模型。</p><br /><br /><p><strong>摘要：</strong> 奖励模型在对齐模型跟随指令时至关重要，通常采用两种流行的训练范式：Bradley-Terry风格或回归风格。然而，由于这两种方法所需的数据格式不兼容，缺乏足够对比的证据。为解决此问题，本文发布了为HelpSteer2数据集设计的偏好注释，以补充现有的评分数据，并提供人类撰写的理由以提高数据可解释性。基于这些数据，进行了一次首次Bradley-Terry与回归模型的公平比较，并在此基础上提出了一种结合这两种模型的新方法。经过调优的Llama-3.1-70B-Instruct模型在RewardBench上得分达到94.1，成为截至2024年10月1日140多个奖励模型中的顶尖模型。同时，本研究展示了该奖励模型在强化学习中有效对齐模型执行指令的能力。研究团队以CC-BY-4.0许可开源了此数据集，地址为https://huggingface.co/datasets/nvidia/HelpSteer2，并开放发布了训练好的奖励模型，地址为https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:39:11 GMT</pubDate>
</item>
<item>
<title>RATIONALYST：基于理性注解的推理过程监督模型</title>
<link>https://arxiv.org/abs/2410.01044</link>
<guid>https://arxiv.org/abs/2410.01044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RATIONALYST通过海量理性注解进行推理模型的预训练，显著提升多种推理任务的准确性。</p><br /><br /><p><strong>摘要：</strong> 为了解决大型语言模型（LLMs）推理步骤不完整的问题，提出了RATIONALYST模型，通过对大量无标签数据提取的79k理性注解进行预训练，以实现推理过程的监督。该模型利用来自Web规模未标记数据集（如Pile）和多种推理数据集的理性注解，最小化了人工干预。RATIONALYST经过LLaMa-3-8B的微调，在7个代表性推理基准测试中平均提高了推理准确性3.9%。此外，RATIONALYST在推理性能上优于更大型的验证模型（如GPT-4）和相似规模的模型，证明了其在数学、常识、科学和逻辑推理等多样化推理任务中的优秀泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:26:20 GMT</pubDate>
</item>
<item>
<title>Embodied-RAG：一种提升机器人导航与语言生成能力的非参数记忆框架</title>
<link>https://arxiv.org/abs/2409.18313</link>
<guid>https://arxiv.org/abs/2409.18313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Embodied-RAG框架，提升机器人在多模态环境中的导航和语言生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Embodied-RAG框架，它旨在通过非参数记忆系统增强具身代理的基础模型。Embodied-RAG能够自主构建分层知识，以支持导航和语言生成任务。该框架处理多样化环境中的空间和语义解析需求，支持特定物体的查询及整体氛围的描述。其核心记忆被组织为语义森林，保存不同细节层次的语言描述。这种分层结构使系统有效地生成上下文敏感的输出，并适应不同的机器人平台。实验结果表明，Embodied-RAG能够成功应对19个环境中的200多个解释和导航查询，展示了其在具身代理领域的一般性非参数系统的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 15:05:01 GMT</pubDate>
</item>
<item>
<title>机器翻译中的性别偏见对服务质量的影响研究</title>
<link>https://arxiv.org/abs/2410.00545</link>
<guid>https://arxiv.org/abs/2410.00545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究揭示了机器翻译中的性别偏见如何导致翻译服务质量差异，特别是女性用户的额外努力和成本。</p><br /><br /><p><strong>摘要：</strong> 性别偏见在机器翻译（MT）中被广泛认可，并可能对社会和个人产生负面影响。然而，当前的技术进步往往没有考虑最终用户的意见或其可能受到的影响。现有评估主要依赖自动化方法，无法有效评估性别差异对使用者的实际后果。本研究通过一项大规模以人为中心的研究，探讨了机器翻译中的性别偏见对服务质量的影响，尤其是在女性和男性之间的服务差距。研究招募了90名参与者对机器翻译输出进行后编辑，以确保性别翻译的准确性。结果表明，女性的后编辑工作相较于男性需要更多的技术和时间投入，且相应地产生更高的财务成本。然而，现有的偏见测量方法未能反映出这种差异。因此，该研究结果呼吁采取以人为中心的方法，以便更好地理解偏见对社会的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 12:00:24 GMT</pubDate>
</item>
<item>
<title>针对长视频理解的LMM适应策略</title>
<link>https://arxiv.org/abs/2409.20018</link>
<guid>https://arxiv.org/abs/2409.20018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对长视频理解，提出了一种通过扩展视觉上下文窗口的方法，显著提高了长视频处理性能。</p><br /><br /><p><strong>摘要：</strong> 本论文针对大型多模态模型在长视频理解中的挑战，提出了一种通过扩展视觉上下文窗口来适应长视频任务的方法。研究发现，预训练的LMMs在处理长视频内容时，由于视觉和语言模态之间的差异，导致视觉和语言token的上下文窗口不一致，造成理解困难。因此，论文提出通过扩展视觉上下文窗口的方式来解决这一问题，避免在大规模的长视频数据集上重新训练。此外，论文还引入了渐进池化推理策略，通过有选择性地调整帧嵌入的空间分辨率，减少视觉token的数量，同时保持重要的空间信息。这种方法在多个长视频理解基准测试中表现出色，在MLVU基准中，即使模型大小只有7B，也超越了GPT-4o。在256帧设置下，所提方法将内存使用率降低约45%，且无性能损失。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 07:54:24 GMT</pubDate>
</item>
<item>
<title>SyntheOcc：一种用于生成可控3D占用数据集的扩散模型</title>
<link>https://arxiv.org/abs/2410.00337</link>
<guid>https://arxiv.org/abs/2410.00337</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SyntheOcc 提出了一种生成多样化控制的3D占用数据集的扩散模型，以支持自动驾驶领域的感知模型训练。</p><br /><br /><p><strong>摘要：</strong> SyntheOcc是一种扩散模型，旨在通过条件占用标签生成高度真实感和几何控制的图像，特别关注3D占用预测任务。该模型在自动驾驶场景中能够生成丰富多样且可控的数据集，从而解决了3D几何信息在2D扩散模型中有效编码的难题。SyntheOcc创新地结合了3D语义多平面图像（MPI）来提供全面且空间对齐的3D场景描述，从而用于条件生成。通过对nuScenes数据集的详细定性和定量评估，SyntheOcc证明了其在生成可控占用数据集方面的有效性，为感知模型的训练提供了有效的数据增强手段。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00337" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 07:41:04 GMT</pubDate>
</item>
<item>
<title>一种基于后验均值的图像恢复算法</title>
<link>https://arxiv.org/abs/2410.00418</link>
<guid>https://arxiv.org/abs/2410.00418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PMRF算法通过最优运输后验均值实现高质量图像恢复。它在多种任务中超越了前沿技术。</p><br /><br /><p><strong>摘要：</strong> 本文关注在完美感知指标约束下，最小化均方误差（MSE）的最优估计器。提出的Posterior-Mean Rectified Flow (PMRF)算法通过预先预测后验均值，然后利用直流模型将该结果转移到高质量图像上，近似于理想的最优运输映射。PMRF基于最近的理论结果，能够有效地将后验均值预测搬运至与真实图像相同的分布。通过理论分析和实证研究，PMRF在多种图像恢复任务中表现优于现有方法。该算法的引入为图像恢复领域带来了新的视角，旨在在保持感知质量的同时最小化重建失真。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 06:14:13 GMT</pubDate>
</item>
<item>
<title>VideoLISA：语言指导下的视频多模态大语言模型</title>
<link>https://arxiv.org/abs/2409.19603</link>
<guid>https://arxiv.org/abs/2409.19603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoLISA是一种基于视频的多模态大语言模型，通过语言指令实现视频中对象的时间一致分割。</p><br /><br /><p><strong>摘要：</strong> VideoLISA是一种新颖的视频多模态大语言模型，旨在解决视频中语言指导下的推理分割问题。该模型结合了大型语言模型的推理能力和世界知识，以及Segment Anything Model的增强，能够根据语言指令生成时间一致的视频分割掩码。现有的基于图像的方法（如LISA）在处理视频任务时遇到困难，因为视频增加了时间维度，要求更强的时间动态理解能力和帧间一致性分割。为解决这些挑战，VideoLISA引入了一种稀疏密集采样策略，以平衡计算约束下的时间上下文和空间细节。此外，模型还提出了一种One-Token-Seg-All方法，利用特殊设计的标记在多个帧中实现对象的分割和跟踪。在我们新推出的ReasonVOS基准及其他多样化基准上的广泛评估显示，VideoLISA在处理复杂推理、时间理解和对象跟踪的视频对象分割任务中表现优越。虽然重点在视频分割，VideoLISA在图像分割上也显示出良好的泛化能力，揭示了其作为语言指导对象分割的统一基础模型的潜力。代码和模型将在https://github.com/showlab/VideoLISA上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 06:12:40 GMT</pubDate>
</item>
<item>
<title>Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect</title>
<link>https://arxiv.org/abs/2409.17912</link>
<guid>https://arxiv.org/abs/2409.17912</guid>
<content:encoded><![CDATA[
We introduce Atlas-Chat, the first-ever collection of large language models specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource language variants, which are often neglected in favor of data-rich languages by contemporary LLMs.
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 05:02:45 GMT</pubDate>
</item>
<item>
<title>Illustrious：一款先进的动漫图像生成模型</title>
<link>https://arxiv.org/abs/2409.19946</link>
<guid>https://arxiv.org/abs/2409.19946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Illustrious模型在生成高质量动漫图像方面的三大关键改进策略。</p><br /><br /><p><strong>摘要：</strong> 本文分享了在我们文本到图像动漫图像生成模型Illustrious中实现尖端质量的见解。为了实现高分辨率、动态色彩范围图像和强大的恢复能力，我们专注于三种关键改进方法。首先，我们深入探讨了批量大小和 dropout 控制的重要性，这能够加速可控标记概念激活的学习。其次，我们提高了图像的训练分辨率，从而在更高的分辨率下准确描绘角色解剖结构，实现超过20MP的生成能力。最后，我们提出了经过优化的多层次标签覆盖方案，结合各种自然语言标题，作为模型发展的重要因素。通过广泛的分析和实验，Illustrious在动画风格方面展现了尖端表现，在插画领域超越了广泛使用的模型，推动了更容易的定制和个性化，并具备开源特性。我们计划公开发布更新的Illustrious模型系列，并制定持续改进的方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 04:30:24 GMT</pubDate>
</item>
<item>
<title>Law of the Weakest Link: Cross Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2409.19951</link>
<guid>https://arxiv.org/abs/2409.19951</guid>
<content:encoded><![CDATA[
The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios.
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 02:10:28 GMT</pubDate>
</item>
<item>
<title>Flex3D：一种灵活的3D内容生成框架</title>
<link>https://arxiv.org/abs/2410.00890</link>
<guid>https://arxiv.org/abs/2410.00890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flex3D提出了一种新颖的两阶段框架，有效利用任意数量的高质量输入视图生成3D内容。</p><br /><br /><p><strong>摘要：</strong> Flex3D是一种新型的两阶段框架，旨在克服现有3D生成方法中固定视图数量的局限性。第一阶段采用候选视图生成和选择流程，通过微调的多视图图像扩散模型和视频扩散模型，生成丰富的目标3D对象表示。随后，视图选择流程对这些视图进行质量和一致性筛选，确保只使用高质量、可靠的视图进行重建。第二阶段将经过筛选的视图输入到灵活的重建模型（FlexRM），该模型基于变换器架构，能够有效处理任意数量的输入，并利用三平面表示直接输出3D高斯点，实现高效且详细的3D生成。通过对设计和训练策略的深入探索，我们优化了FlexRM，以在重建和生成任务中获得卓越性能。结果表明，Flex3D在3D生成任务中表现出色，与多种最新的前馈3D生成模型相比，其用户研究胜率超过92%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:32:05 GMT</pubDate>
</item>
<item>
<title>面向边缘设备的TPI-LLM：高效的张量并行推理系统</title>
<link>https://arxiv.org/abs/2410.00531</link>
<guid>https://arxiv.org/abs/2410.00531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPI-LLM是一种高效的张量并行推理系统，解决了边缘设备上的大模型推理问题。</p><br /><br /><p><strong>摘要：</strong> 随着用户隐私数据保护的关注，大型模型推理正逐渐从云端转向边缘设备。然而，边缘设备经常面临计算能力、内存和带宽的限制，需要多设备协作以加快大规模语言模型的推理。本文指出，张量并行在资源有限的设备上相较于流水线并行更具有效性，提出了一种名为TPI-LLM的计算和内存高效的张量并行推理系统，支持70B规模的模型。TPI-LLM允许用户的敏感数据留在本地，并通过滑动窗口内存调度器动态管理推理过程中的层权重，重叠处理磁盘I/O延迟与计算和通信，这样可以使内存受限的设备顺利运行更大的模型。本研究分析了通信瓶颈，发现链接延迟而非带宽是主要问题，因此实施了基于星型结构的全归约算法。通过在仿真和真实测试平台上的大量实验，TPI-LLM显示出比Accelerate缩短80%以上的首个标记生成时间和标记延迟，而与Transformers和Galaxy相比缩短了90%以上，同时将Llama 2-70B的峰值内存占用减少了90%，使其仅需3.1 GB的内存即可处理70B规模模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:25:30 GMT</pubDate>
</item>
<item>
<title>室内环境中四足机器人移动操控系统的研究</title>
<link>https://arxiv.org/abs/2410.00231</link>
<guid>https://arxiv.org/abs/2410.00231</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种四足移动操控系统，能在未见环境中执行任务并进行对象操控。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的四足移动操控系统，旨在解决四足机器人在室内环境中学习和执行任务时遇到的挑战。该系统配备了前置抓手用于物体操控，采用在模拟环境中训练的低级控制器，利用自我中心的深度信息完成攀爬和全身倾斜等灵活技能。同时，系统结合了预训练的视觉语言模型（VLM），使用第三人称鱼眼和自我中心的RGB相机进行语义理解和指令生成。我们在两个未见环境中评估了该系统，无需任何真实世界数据收集或训练，系统能以零-shot方式对这些环境进行泛化，并成功完成任务，如根据用户的指令抓取随机放置的玩具，成功率达60%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00231" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:20:04 GMT</pubDate>
</item>
<item>
<title>基于单目视频的人体模型重建方法</title>
<link>https://arxiv.org/abs/2409.20563</link>
<guid>https://arxiv.org/abs/2409.20563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种从单目视频重建时间一致的人体模型的新方法，适用于松散衣物和物体交互。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，通过单目视频重建时间一致的人体模型，尤其关注极其松散的衣物和手持物体的交互。以往的人体重建研究主要集中在紧身衣物，没有考虑物体交互，或者需要标定的多视角捕捉和个性化模板扫描，这些方法在规模上难以推广。我们的关键创新在于将大规模训练数据学习的通用人体形状先验与特定于视频的“袋装骨架”变形相结合，利用测试时优化方法在单个视频中实现。我们通过学习一个神经隐式模型，将人体和衣物的变形分开处理。为捕捉衣物的细微几何变化，在优化过程中利用了姿势、表面法线和光流等基于图像的先验信息。最终，生成的神经场可提取为时间一致的网格，或进一步优化为高保真的三维高斯体，便于交互式渲染。在面对具有挑战性的衣物变形和物体交互的数据集时，DressRecon方法提供了比现有技术更高保真的三维重建效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:13:51 GMT</pubDate>
</item>
<item>
<title>ACE：全能创作者与编辑的统一模型</title>
<link>https://arxiv.org/abs/2410.00086</link>
<guid>https://arxiv.org/abs/2410.00086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ACE模型通过统一的条件格式和多任务联合训练，显著提升了视觉生成能力。</p><br /><br /><p><strong>摘要：</strong> 传统的扩散模型多用于文本引导的视觉生成，难以支持多模态条件，限制了其在视觉编辑中的应用。为了解决这一问题，本文提出了ACE（All-round Creator and Editor）模型，旨在在多种视觉生成任务中实现与专家模型相媲美的性能。通过引入统一的条件格式——长上下文条件单元（LCU），并设计了一种新的基于Transformer的扩散模型，ACE实现了多个生成和编辑任务的联合训练。此外，本文还提出了一种高效的数据收集方法，以弥补缺乏可用训练数据的问题，通过合成或聚类管道获得成对图像，并利用经过微调的多模态大语言模型提供准确的文本指令。为了全面评估模型性能，构建了一个手动注释的视觉生成任务基准数据集。实验结果表明，ACE在视觉生成领域的优越性，使其能够通过单一模型轻松构建多模态聊天系统，以应对各种图像创作的互动请求，从而避免复杂的视觉代理流水线。代码和模型将会在项目页面公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:12:17 GMT</pubDate>
</item>
<item>
<title>视觉问答中的多模态问题分解研究</title>
<link>https://arxiv.org/abs/2409.19339</link>
<guid>https://arxiv.org/abs/2409.19339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了多模态大语言模型在视觉问题分解中的表现，提出DecoVQA+数据集及优化训练流程。</p><br /><br /><p><strong>摘要：</strong> 本论文探讨了多模态大语言模型(MLLMs)在视觉问题分解方面的能力，尤其是针对复杂问题的分解能力。为此，研究团队构建了一个系统的评估框架，包括数据集和多个评估标准，以评估分解后子问题的质量。研究发现，现有的MLLMs在产生高质量子问题上存在困难。为了解决这一问题，作者提出了一个特定的微调数据集DecoVQA+，旨在提升模型在问题分解任务上的能力。论文还提出了一个高效的微调流程，该流程结合了所提数据集和特定的选择性分解训练目标。经过微调的MLLMs在子问题质量和选择性提问分解策略上均显著提升，并在视觉问答基准数据集上实现了更高的准确率。通过这项研究，可以看出多模态模型在处理视觉问题时需要改进其分解策略，以提高综合性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 16:44:57 GMT</pubDate>
</item>
<item>
<title>异构预训练变压器在机器人学习中的应用</title>
<link>https://arxiv.org/abs/2409.20537</link>
<guid>https://arxiv.org/abs/2409.20537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了异构预训练变压器HPT，旨在跨不同机器人构型和任务学习通用策略。</p><br /><br /><p><strong>摘要：</strong> 随着机器人学习方法的推进，异构性成为影响训练的关键阻碍。针对以往仅依赖单一机器人构型和特定任务收集数据的问题，本文研究了如何通过大规模异构数据预训练策略表示。我们提出了异构预训练变压器（HPT），该方法有效学习到了与任务和构型无关的共享表示。HPT的架构将不同机器人构型的本体感知和视觉输入对齐为短序列的标记，随后将这些标记处理为控制多种任务的机器人指令。借助于大规模的多构型真实世界机器人数据集，以及仿真环境、部署机器人和人类视频数据集，本文探索了跨异构性预训练策略的可行性。实验表明，HPT优于多项基准模型，并在多种未知任务的仿真基准和实际应用中提升了精细调整策略的表现超过20%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 11:01:26 GMT</pubDate>
</item>
<item>
<title>UniAff：统一的3D物体中心操控与任务理解范式</title>
<link>https://arxiv.org/abs/2409.20551</link>
<guid>https://arxiv.org/abs/2409.20551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniAff提出了一种整合3D物体操作与任务理解的新范式，显著提高了机器人操控的通用性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的综合范式UniAff，旨在解决机器人操控中对3D运动约束和可使用性的不足理解。我们创建了一个包含900个来自19个类别的关节对象和600个来自12个类别的工具的标注数据集，涵盖了与操控相关的关键属性。此外，我们利用多层次大语言模型（MLLMs）来推断用于操控任务的物体中心表示，包括可使用性识别和3D运动约束推理。通过在仿真和真实世界环境下进行的全面实验，结果表明UniAff显著提升了工具和关节物体的机器人操控通用性。我们希望UniAff能够成为未来统一机器人操控任务的通用基准。项目网站上已经发布了图片、视频、数据集和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 10:43:32 GMT</pubDate>
</item>
<item>
<title>基于元长度标记的长度生成任务提升方法研究</title>
<link>https://arxiv.org/abs/2409.18943</link>
<guid>https://arxiv.org/abs/2409.18943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Ruler方法，通过元长度标记提升大语言模型的长度控制能力，表现显著增强。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为Ruler的新方法，旨在提升大语言模型在生成特定长度响应时的能力。为此，我们设计了目标长度生成任务（TLG）和两个评估指标，精确匹配（PM）与灵活匹配（FM），以评估模型在长度约束下的表现。Ruler通过引入元长度标记（MLTs），增强了模型在长度限制指令下的响应生成能力，同时在缺乏长度约束的情况下，也能自动生成适当的MLT，展现了极佳的通用性和适应性。综合实验结果显示，Ruler在不同的大语言模型中均能有效提升其在目标长度生成任务上的表现，例如在精确匹配和灵活匹配方面分别平均提升27.97和29.57。此外，我们还进行了广泛的消融实验，以进一步验证Ruler的有效性和广泛适应能力。相关代码和数据可在https://github.com/Geaming2002/Ruler获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 10:13:22 GMT</pubDate>
</item>
<item>
<title>Coffee-Gym：用于代码编辑反馈的强化学习环境</title>
<link>https://arxiv.org/abs/2409.19715</link>
<guid>https://arxiv.org/abs/2409.19715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Coffee-Gym是一个RL环境，通过数据集和奖励函数，训练代码编辑反馈模型，提高开源代码LLM的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Coffee-Gym，一个针对代码编辑反馈模型培训的综合强化学习(RL)环境。Coffee-Gym主要由两个组成部分构成：首先是Coffee，一个包含人类代码编辑轨迹的数据集，涵盖了编程问题与机器编写的反馈；其次是CoffeeEval，一个奖励函数，能够通过评估修订后代码在单元测试中的表现，真实反映反馈的 helpfulness。Coffee-Gym旨在解决用于RL反馈模型培训的高质量数据集匮乏问题，并且提供比现有最先进的奖励模型（如GPT-4）更准确的奖励。通过应用Coffee-Gym，开发出的反馈模型在提升开源代码LLM的代码编辑能力方面超越了基准，与封闭源LLM的效果相当。我们将数据集和模型检查点公开分享。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 07:43:58 GMT</pubDate>
</item>
<item>
<title>Can Models Learn Skill Composition from Examples?</title>
<link>https://arxiv.org/abs/2409.19808</link>
<guid>https://arxiv.org/abs/2409.19808</guid>
<content:encoded><![CDATA[
As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6.   In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 06:35:36 GMT</pubDate>
</item>
<item>
<title>低秩近似之路（二）：SVD</title>
<link>https://spaces.ac.cn/archives/10407</link>
<guid>https://spaces.ac.cn/archives/10407</guid>
<content:encoded><![CDATA[
<p>上一篇文章中我们介绍了“<a href="https://kexue.fm/archives/10366" target="_blank">伪逆</a>”，它关系到给定矩阵$M$和$A$（或$B$）时优化目标$\Vert AB - M\Vert_F^2$的最优解。这篇文章我们来关注$A,B$都不给出时的最优解，即<br />
\begin{equation}\mathop{\text{argmin}}_{A,B}\Vert AB - M\Vert_F^2\label{eq:loss-ab}\end{equation}<br />
其中$A\in\mathbb{R}^{n\times r}, B\in\mathbb{R}^{r\times m}, M\in\mathbb{R}^{n\times m},r < \min(n,m)$。说白了，这就是要寻找矩阵$M$的“最优$r$秩近似（秩不超过$r$的最优近似）”。而要解决这个问题，就需要请出大名鼎鼎的“SVD（奇异值分解）”了。虽然本系列把伪逆作为开篇，但它的“名声”远不如SVD，听过甚至用过SVD但没听说过伪逆的应该大有人在，包括笔者也是先了解SVD后才看到伪逆。</p><p>接下来，我们将围绕着矩阵的最优低秩近似来展开介绍SVD。</p><h2>基本形式</h2><p>对于任意矩阵$M\in\mathbb{R}^{n\times m}$，都可以找到如下形式的奇异值分解（SVD，Singular Value Decomposition）：<br />
\begin{equation}M = U\Sigma V^{\top}\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10407" title="低秩近似之路（二）：SVD">[...]</a></p>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 17:45:00 +0800</pubDate>
</item>
<item>
<title>双重嵌入模型的神经音频水印技术研究</title>
<link>https://arxiv.org/abs/2409.19627</link>
<guid>https://arxiv.org/abs/2409.19627</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种双重嵌入的神经音频水印模型，提升了水印的鲁棒性和定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文设计了一种双重嵌入的神经音频水印模型，旨在提高水印的定位能力和鲁棒性。随着深度学习的发展，神经音频水印相较传统方法，在水印的嵌入和提取效果上更具优势，但仍存在低容量和不可感知性差的问题。此外，水印定位问题在神经水印中尤为重要，尚未得到充分研究。为此，本文提出的IDEAW模型针对攻击层对可逆神经网络的影响进行优化，加强了模型的合理性和稳定性。实验结果表明，IDEAW模型在抵御各种攻击的能力、容量以及高效的定位功能上均优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19627" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:52:47 GMT</pubDate>
</item>
<item>
<title>Cottention：一种基于余弦相似度的新型注意力机制</title>
<link>https://arxiv.org/abs/2409.18747</link>
<guid>https://arxiv.org/abs/2409.18747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cottention通过余弦相似度替代softmax，具备线性内存复杂度，适合处理长序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的注意力机制Cottention，它用余弦相似度替代传统的softmax操作，从而实现了序列长度上原生的线性内存复杂度。这一特性使得Cottention在处理长序列时比softmax注意力显著更为高效。Cottention可以重构为具有有限隐藏状态的递归神经网络（RNN），在推理过程中能够保证常量内存使用。通过在双向BERT和因果GPT任务上的评估，Cottention展现出了与softmax注意力相当的性能，同时显著降低了内存需求。为了确保有效的计算，我们为Cottention开发了一个自定义的CUDA内核。实验结果表明，Cottention是softmax注意力的有希望的替代方案，能在不牺牲性能的情况下处理更长的序列，因为它具备原生线性内存复杂度和在推理时维持常量内存占用的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:33:45 GMT</pubDate>
</item>
<item>
<title>MM1.5：提升多模态大语言模型的文本与图像理解</title>
<link>https://arxiv.org/abs/2409.20566</link>
<guid>https://arxiv.org/abs/2409.20566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MM1.5是一种新型多模态大语言模型，专注于文本丰富的图像理解和多图像推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MM1.5，这是一个新兴的多模态大语言模型（MLLM）系列，旨在提升文本丰富图像理解、视觉参考与定位以及多图像推理的能力。MM1.5在MM1架构的基础上，采用数据中心化的训练方式，系统性地探索了整个模型训练生命周期中不同数据混合对模型性能的影响。这包括高质量的OCR数据和合成描述用于持续预训练，以及针对监督微调的优化视觉指令调优数据混合。我们的模型参数范围从1B到30B，涵盖了密集型和专家混合（MoE）变体，证明了精心的数据策划和训练策略可以在小规模模型（1B和3B）上实现强大的性能。此外，我们还介绍了两个专业变体：MM1.5-Video，专门用于视频理解，以及MM1.5-UI，专为移动用户界面理解而设计。通过广泛的经验研究和消融实验，我们提供了关于训练过程和设计决策的详细洞见，为未来的MLLM开发研究提供了宝贵的指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:30:20 GMT</pubDate>
</item>
<item>
<title>超连接：替代残差连接的有效方法</title>
<link>https://arxiv.org/abs/2409.19606</link>
<guid>https://arxiv.org/abs/2409.19606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了超连接方法，旨在改善残差连接的不足，并在大语言模型及视觉任务中取得显著性能提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种简单有效的超连接方法，作为残差连接的替代方案，主要解决了残差连接变体中常见的几个缺点，如梯度消失与表征崩溃之间的秋千效应。超连接理论上允许网络调整不同深度特征之间的连接强度，并动态重排列。通过对大语言模型的预训练实验，包括稠密模型与稀疏模型，超连接显示出显著的性能提升。此外，针对视觉任务的附加实验也显示出了类似的改进。我们预期这一方法将在广泛的人工智能问题中得到广泛应用与好处。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:27:44 GMT</pubDate>
</item>
<item>
<title>DiaSynth：一种高质量领域特定对话生成框架</title>
<link>https://arxiv.org/abs/2409.19020</link>
<guid>https://arxiv.org/abs/2409.19020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiaSynth框架通过大规模语言模型生成丰富的领域特定对话，提升对话系统的训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DiaSynth的合成对话生成框架，旨在解决领域特定对话数据集稀缺的问题。当前对话系统的研发受到通用对话数据集和规模小的特定领域数据集的限制。DiaSynth通过动态生成对话，模拟不同的人物角色、子主题和多样的对话特征，利用大规模语言模型及其链式推理能力，生产具有上下文丰富性和真实性的领域特定对话。通过对不同大规模语言模型生成的合成数据进行实验，发现经过合成数据微调的预训练语言模型比基础模型提升了16.47%的表现。此外，合成数据能够捕捉到领域特定数据的90.48%的分布特征，且生成数据的质量会随着大规模语言模型的规模增加而提高。这些结果证实了DiaSynth作为传统数据收集方法的有力替代方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:22:04 GMT</pubDate>
</item>
<item>
<title>ICDiff：专为扩散模型设计的图像复制检测方法</title>
<link>https://arxiv.org/abs/2409.19952</link>
<guid>https://arxiv.org/abs/2409.19952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了ICDiff以及D-Rep数据集，用于检测扩散模型生成图像的复制情况。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型产生的图像在数字艺术和视觉营销中的受到越来越多的关注，内容原创性的问题也随之而来。现有的图像复制检测（ICD）模型在检测手工制作的复制品方面虽然准确，但忽略了来自扩散模型所产生图像的挑战。为此，本文介绍了ICDiff，这是首个专为扩散模型开发的ICD方法。我们构建了Diffusion-Replication（D-Rep）数据集，并提出了一种新颖的深度嵌入方法。D-Rep基于最先进的扩散模型Stable Diffusion V1.5生成4万对图像复制品，并将这些复制品手动标注为6个复制级别，范围从0（无复制）到5（完全复制）。我们的PDF-Embedding方法将每对图像复制品的复制级别转化为概率密度函数（PDF），作为监督信号。实验结果表明，PDF-Embedding在D-Rep测试集上表现优于基于协议的方法和非PDF选择。此外，通过利用PDF-Embedding，我们发现著名扩散模型与开放源代码图库之间的复制比例介于10%到20%之间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 03:46:15 GMT</pubDate>
</item>
<item>
<title>HDFlow：一种结合快速与慢速思维的大语言模型复杂推理框架</title>
<link>https://arxiv.org/abs/2409.17433</link>
<guid>https://arxiv.org/abs/2409.17433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HDFlow框架，结合快速与慢速思维，显著提升大语言模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为HDFlow的新框架，旨在通过结合快速与慢速思维的适应性方式，提升大语言模型在复杂推理问题上的能力。该框架包含两个关键组成部分：1) 动态工作流（Dynamic Workflow），自动将复杂问题拆解为更易管理的小任务，并动态设计工作流程，组合专门的语言模型或符号推理工具来解决子任务；2) 混合思维（Hybrid Thinking），根据问题的复杂性，动态结合快速与慢速思维。我们还提出了一种易于扩展的方法，自动合成包含27K个挑战性推理问题的大规模数据集，并开发了一种混合思维微调方法，以训练较小的语言模型，使其内化快速/慢速混合推理策略。实验结果显示，在四个推理基准数据集上，使用动态工作流的慢速推理显著优于链式推理，而混合思维在计算效率和性能之间达到了最佳平衡。通过我们的混合思维方法进行微调，显著增强了开放源码语言模型的复杂推理能力。实验结果展示了慢速思维、动态工作流和混合思维在拓展大语言模型复杂问题解决能力方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 15:42:07 GMT</pubDate>
</item>
<item>
<title>大语言模型诚实性研究综述</title>
<link>https://arxiv.org/abs/2409.18786</link>
<guid>https://arxiv.org/abs/2409.18786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究综述了大语言模型的诚实性问题及改进策略，旨在促进相关领域的进一步探索。</p><br /><br /><p><strong>摘要：</strong> 诚实是将大语言模型（LLMs）与人类价值观对齐的基本原则，这要求模型能够认识到自己所知与未知的知识，并真实地表达这些知识。然而，尽管后续发展有希望，目前的LLMs仍然表现出显著的不诚实行为，例如自信地给出错误答案，或未能明确表达自己所知的知识。此外，关于LLMs诚实性的研究面临多重挑战，包括对诚实性的定义不一、确切区分已知和未知知识的困难，以及缺乏对相关研究的全面理解。为解决这些问题，我们提供了LLMs诚实性问题的综述，涵盖了这一主题的澄清、评估方法及改进策略。此外，我们还提供了未来研究的见解，旨在激励这一重要领域的进一步探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 14:41:55 GMT</pubDate>
</item>
<item>
<title>基于语言模型学习的新型分类方法</title>
<link>https://arxiv.org/abs/2409.18957</link>
<guid>https://arxiv.org/abs/2409.18957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法，利用大型语言模型进行分类，验证了其优越性与解释性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的利用大型语言模型（LLMs）进行分类任务的方法，称为“语言模型学习”（LML），并提出了一种名为“数据增强预测”（DAP）的新方法。与传统机器学习模型依赖数据清洗和特征工程不同，该方法通过使用LLMs简化了分类过程。分类过程模拟人类对数据的探索与理解，通过对训练数据的总结和评估，确定对每个标签分类最有帮助的特征。在DAP过程中，系统利用数据摘要自动生成查询，从数据集中检索相关行，以确保在复杂数据条件下生成准确的分类结果。通过使用数据摘要和类似数据，DAP确保了上下文感知的决策。此外，系统在提示中使用“充当可解释的机器学习模型”来增强预测的可解释性，让用户能够审核每个预测背后的逻辑。测试中，该方法在多个场景中取得了超过90%的准确率，表明其有效性，并可能优于传统的机器学习模型。相关代码已在GitHub上提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 13:30:27 GMT</pubDate>
</item>
<item>
<title>调节干预偏好优化方法的研究</title>
<link>https://arxiv.org/abs/2409.17545</link>
<guid>https://arxiv.org/abs/2409.17545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了调节干预偏好优化（MIPO）方法，以改善模型对参考模型的干预程度，进而提升模型的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 在偏好优化方法中，常以训练良好的SFT模型作为参考模型进行训练。强化学习与人类反馈（RLHF）和差异化偏好优化（DPO）在优化过程中使用正则化项，以防止模型过度偏离参考模型，从而避免生成异常响应。然而，当参考模型与给定数据对齐不佳且需要显著调整时，正则化项可能会妨碍模型的对齐。因此，我们提出了一种新的方法——调节干预偏好优化（MIPO）。MIPO根据数据与参考模型的对齐程度，调节对参考模型的干预程度：当数据和参考模型良好对齐时，增加干预；若对齐较差则减小干预，以促进更广泛的训练。通过在Alpaca Eval 2.0和MT-Bench上比较MIPO和DPO的性能，采用Mistral-7B和Llama3-8B的实验结果表明，MIPO在多种评估场景中均优于DPO。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 05:36:26 GMT</pubDate>
</item>
<item>
<title>MinerU：高精度文档内容提取开源解决方案</title>
<link>https://arxiv.org/abs/2409.18839</link>
<guid>https://arxiv.org/abs/2409.18839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MinerU提供了一种高精度的文档内容提取方法，有效应对多样化文档类型。</p><br /><br /><p><strong>摘要：</strong> 文档内容分析在计算机视觉领域一直是一个重要的研究方向。尽管在OCR、布局检测和公式识别等方法上取得了显著进展，现有开源解决方案在高质量内容提取方面仍面临挑战，特别是面对多样化的文档类型和内容。为了解决这些问题，我们提出了MinerU，一个开源的高精度文档内容提取解决方案。MinerU利用先进的PDF-Extract-Kit模型，能够有效地从各种文档中提取内容。同时，该系统还采用精细调整的预处理和后处理规则，以确保最终结果的准确性。实验结果表明，MinerU在多种文档类型中均表现出色，显著提高了内容提取的质量和一致性。MinerU开源项目已在https://github.com/opendatalab/MinerU上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 05:17:52 GMT</pubDate>
</item>
<item>
<title>Emu3：一种基于下一个令牌预测的多模态模型</title>
<link>https://arxiv.org/abs/2409.18869</link>
<guid>https://arxiv.org/abs/2409.18869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Emu3通过下一个令牌预测在多模态任务中超越了多个主流模型，展示了其在生成和感知能力上的优越性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Emu3，一套新的多模态模型，完全基于下一个令牌预测进行训练。通过将图像、文本和视频转化为离散的令牌，Emu3从零开始训练一个单一的变换器，利用多种多模态序列的混合进行学习。研究表明，Emu3在生成和感知任务中优于多种任务专用的模型，明显超越了SDXL和LLaVA-1.6等知名模型，同时淘汰了扩散或组合架构。Emu3还能通过预测视频序列中的下一个令牌生成高保真视频。我们简化了复杂的多模态模型设计，专注于令牌这一单一方面，从而在训练和推理过程中释放出巨大的潜力。研究结果表明，下一个令牌预测是构建超越语言的通用多模态智能的有前景的方向。我们开源了关键技术和模型，以支持该领域的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 03:23:21 GMT</pubDate>
</item>
<item>
<title>PhysGen：一种基于单幅图像生成视频的新方法</title>
<link>https://arxiv.org/abs/2409.18964</link>
<guid>https://arxiv.org/abs/2409.18964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysGen通过物理模拟与数据驱动的方法，为单幅图像生成现实与物理一致性的视频。</p><br /><br /><p><strong>摘要：</strong> PhysGen是一种新颖的图像到视频生成方法，能够将单幅图像与输入条件（如施加于图像中对象的力和扭矩）转换为现实、物理上合理且时间上连贯的视频。该方法的关键在于结合了基于模型的物理模拟和数据驱动的视频生成过程，从而在图像空间内实现合理的动态。系统核心包括三大组件：一是图像理解模块，有效捕捉图像的几何形状、材料和物理参数；二是图像空间动态模拟模型，利用刚体物理和推断的参数进行真实行为的模拟；三是基于图像的渲染和精炼模块，利用生成的视频扩散技术生产具有模拟运动的真实视频。生成的视频在物理和外观上都高度真实，且可精确控制，经过量化比较和全面的用户研究显示出优于现有的数据驱动图像到视频生成工作的效果。PhysGen生成的视频可应用于多种下游任务，例如将图像转化为真实动画，或允许用户与图像互动并创建各种动态。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 03:17:46 GMT</pubDate>
</item>
<item>
<title>MIO：一种新型的多模态基础模型</title>
<link>https://arxiv.org/abs/2409.17692</link>
<guid>https://arxiv.org/abs/2409.17692</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIO是一种采用多模态令牌构建的新模型，能实现多种形式的智能生成与理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MIO的新型基础模型，具有处理多模态令牌的能力，能够理解和生成语音、文本、图像和视频。尽管近期大型语言模型（LLMs）和多模态大型语言模型（MM-LLMs）的出现推动了人工通用智能的发展，但它们仍然缺乏真正的任意到任意的理解和生成能力。最近发布的GPT-4o展示了这一领域的潜力，但因闭源且不支持生成多模态交错序列而存在局限。为了填补这一空白，MIO采用因果多模态建模，通过混合离散令牌的方式进行训练。MIO经历了四个阶段的训练过程：对齐预训练、交错预训练、语音增强预训练和多样化任务的综合监督微调。实验结果表明，MIO在多个方面表现出竞争力，甚至比之前的双模态基线和任意对任意模型基线更为优越。此外，MIO还展现了其任意对任意特性带来的高级能力，如交错视频文本生成、视觉链推理、视觉指导生成和图像编辑等。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17692" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 02:16:43 GMT</pubDate>
</item>
<item>
<title>基于向量的后训练量化方法用于极低位宽的大语言模型</title>
<link>https://arxiv.org/abs/2409.17066</link>
<guid>https://arxiv.org/abs/2409.17066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种向量后训练量化方法，显著提高大语言模型的低位量化性能与推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的向量后训练量化（VPTQ）方法，旨在对大型语言模型（LLMs）进行极低位宽量化。为克服传统标量量化在极低位宽时的数值表示局限性，我们引入了第二阶优化技术来构建LLM的向量量化问题，并通过求解该优化问题指导量化算法设计。此外，我们采用与通道无关的第二阶优化来精细调整权重，以实现更精细的向量量化。我们还提出了一种简洁有效的码本初始化算法，以简化优化问题的分解。同时，VPTQ方法支持残差和异常值量化，进而提高模型准确性并进一步压缩模型。实验结果表明，VPTQ在2位宽下，相较于当前最先进技术（SOTA），在LLaMA-2上量化困惑度下降0.01-0.34，Mistral-7B下降0.38-0.68，LLaMA-3下降4.41-7.34，并且在LLaMA-2、Mistral-7B和LLaMA-3的问答任务中平均精度提高了0.79-1.5%、1%和11-22%。我们仅利用10.4-18.6%的量化算法执行时间，使推理吞吐量提升了1.6-1.8倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 01:55:09 GMT</pubDate>
</item>
<item>
<title>多尺度洞察智能体（MSI-Agent）在决策优化中的应用</title>
<link>https://arxiv.org/abs/2409.16686</link>
<guid>https://arxiv.org/abs/2409.16686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍多尺度洞察智能体（MSI-Agent），通过优化洞察生成和选择提高决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出多尺度洞察智能体（MSI-Agent），旨在提升大语言模型（LLM）在计划和决策中的效率。MSI-Agent通过经验选择器、洞察生成器和洞察选择器的三部分管道，能够有效生成任务特定和高层洞察，并将其存储在数据库中，以便于在决策过程中调用相关洞察。实验结果表明，MSI-Agent在规划任务上优于其他洞察策略，并展示出在面对领域转移场景时的更强鲁棒性。本文还探讨了选择种子经验和洞察的策略，以便为大语言模型提供更有用和相关的洞察，从而改善其决策能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.16686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 01:22:44 GMT</pubDate>
</item>
<item>
<title>利用“熄火保护 + 通断器”实现燃气灶智能关火</title>
<link>https://spaces.ac.cn/archives/10394</link>
<guid>https://spaces.ac.cn/archives/10394</guid>
<content:encoded><![CDATA[
<div>燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气...</div>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
</item>
<item>
<title>Softmax后传：寻找Top-K的光滑近似</title>
<link>https://spaces.ac.cn/archives/10373</link>
<guid>https://spaces.ac.cn/archives/10373</guid>
<content:encoded><![CDATA[
<p>Softmax，顾名思义是“soft的max”，是$\max$算子（准确来说是$\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\boldsymbol{x}\in\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在<a href="https://kexue.fm/archives/10145" target="_blank">《通向概率分布之路：盘点Softmax及其替代品》</a>也介绍过其他一些能实现相同效果的方案。</p><p>我们知道，最大值通常又称Top-1，它的光滑近似方案看起来已经相当成熟，那读者有没有思考过，一般的Top-$k$的光滑近似又是怎么样的呢？下面让我们一起来探讨一下这个问题。</p><h2>问题描述</h2><p>设向量$\boldsymbol{x}=(x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$，简单起见我们假设它们两两不相等，即$i\neq j \Leftrightarrow x_i\neq x_j$。记$\Omega_k(\boldsymbol{x})$为$\boldsymbol{x}$最大的$k$个分量的下标集合，即$|\Omega_k(\boldsymbol{x})|=k$以及$\forall i\in \Omega_k(\boldsymbol{x}), j \not\in \Omega_k(\boldsymbol{x})\Rightarrow x_i > x_j$。我们定义Top-$k$算子$\mathcal{T}_k$为$\mathbb{R}^n\mapsto\{0,1\}^n$的映射：<br />
\begin{equation}<br />
[\mathcal{T}_k(\boldsymbol{x})]_i = \left\{\begin{aligned}1,\,\, i\in \Omega_k(\boldsymbol{x}) \\ 0,\,\, i \not\in \Omega_k(\boldsymbol{x})\end{aligned}\right.<br />
\end{equation}<br />
说白了，如果$x_i$属于最大的$k$个元素之一，那么对应的位置变成1，否则变成0，最终结果是一个Multi-Hot向量，比如$\mathcal{T}_2([3,2,1,4]) = [1,0,0,1]$。</p><p class="more"><a href="https://spaces.ac.cn/archives/10373" title="Softmax后传：寻找Top-K的光滑近似">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>深入理解矩阵低秩近似：伪逆的理论与应用</title>
<link>https://spaces.ac.cn/archives/10366</link>
<guid>https://spaces.ac.cn/archives/10366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了矩阵低秩近似中的伪逆概念，探讨其广义逆矩阵的定义及应用。</p><br /><br /><p><strong>摘要：</strong> 矩阵的低秩近似虽然概念易懂，但内容广泛，相关研究中常出现新技巧，令人感到陌生。本文首篇关注伪逆，即广义逆矩阵，是逆矩阵对不可逆矩阵的推广。在低秩近似中，伪逆为优化问题提供重要工具，尤其在数据降维和特征提取中具有广泛应用。伪逆不仅可用于求解线性方程，还能帮助我们理解和构建更高效的模型。如图所示，这一概念与实际应用密切相关，深化了对矩阵处理技术的理解。<img alt="伪逆示意图" src="图片链接1" />随着 LoRA 等技术的兴起，低秩近似的应用正逐渐增多，使得这一数学工具变得愈加重要。<img alt="低秩近似应用示意图" src="图片链接2" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
</item>
<item>
<title>多模态位置编码研究：从RoPE到RoPE-3D的演变</title>
<link>https://spaces.ac.cn/archives/10352</link>
<guid>https://spaces.ac.cn/archives/10352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨多模态LLM中的位置编码，分析传统1D RoPE及其在2D和3D序列中的推广，从而提出更理想的方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态语言模型（LLM）中位置编码的重要性，指出目前尚未达成共识，尤其在多模态模型中。已有的主流位置编码方法是RoPE（相对位置编码），其原设计仅适用于一维序列（RoPE-1D）。为了拓展至其他维度，本文讨论了如何将RoPE推广至二维（RoPE-2D），以适应图像等数据形式，并进一步推导出RoPE-3D用于视频等三维序列。通过对这个问题的深度梳理，作者希望提出更为理想的解决方案。更多内容请参考[原文图示](http://example.com/image1)。此外，文章中首次提到的RoPE-Tie方案虽有其价值，但仍需细化与发展，以期为多模态LLM领域提供贡献。关于RoPE的详细介绍，读者可参考相关文献。当前多模态模型的发展亟待建立统一的方法论，以推动整个领域的进步和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
</item>
<item>
<title>探讨Causal Attention中的位置编码问题</title>
<link>https://spaces.ac.cn/archives/10347</link>
<guid>https://spaces.ac.cn/archives/10347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨Causal Attention中位置编码的作用及NoPE模型的优缺点。</p><br /><br /><p><strong>摘要：</strong> 众所周知，目前主流的LLM都是基于Causal Attention的Decoder-only模型。虽然已经有不少研究表明，Causal Attention可以在不使用额外的位置编码的情况下取得良好效果，但仍然有许多主流模型（如RoPE、ALIBI等）依然加入了位置编码。本文从三个角度分析这一现象：首先，位置编码在Attention中的作用是为了解决序列中单词的顺序问题；其次，NoPE的Causal Attention通过自适应的方式在计算过程中实现了位置编码；最后，NoPE所实现的位置编码在信息传递的准确性和效率上存在一定不足。整体来看，尽管NoPE在某些情境下能够工作，但主流模型仍选择使用额外的位置编码，可能是出于对模型性能的进一步优化考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>解决MathJax与Marked的兼容性问题</title>
<link>https://spaces.ac.cn/archives/10332</link>
<guid>https://spaces.ac.cn/archives/10332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了MathJax与Markdown解析器Marked之间的兼容性问题，并给出了解决方案。</p><br /><br /><p><strong>摘要：</strong> 在我们引入MathJax解析LaTeX公式后，遇到了与Markdown解析器Marked之间的兼容性问题。Markdown是一种轻量级的标记语言，通常在展示前需要转为HTML格式。尽管部分问题源于笔者的严格要求，但我们追求完美的解决方案是值得努力的。通过合理的配置和调整，我们可以实现MathJax与Marked之间的无缝协作，确保渲染后的文档保持美观和准确。最终，这将提升用户体验，让文档在科学和学术写作中更具吸引力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
</item>
<item>
<title>让MathJax更好地兼容谷歌翻译和延时加载</title>
<link>https://spaces.ac.cn/archives/10320</link>
<guid>https://spaces.ac.cn/archives/10320</guid>
<content:encoded><![CDATA[
<div>很早之前，就有读者提出希望把Cool Papers上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一...</div>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
</item>
<item>
<title>“Cool Papers + 站内搜索”的一些新尝试</title>
<link>https://spaces.ac.cn/archives/10311</link>
<guid>https://spaces.ac.cn/archives/10311</guid>
<content:encoded><![CDATA[
<div>在《Cool Papers更新：简单搭建了一个站内检索系统》这篇文章中，我们介绍了Cool Papers新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何...</div>
]]></content:encoded>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
</item>
<item>
<title>概率分布优化问题的探索与分析</title>
<link>https://spaces.ac.cn/archives/10289</link>
<guid>https://spaces.ac.cn/archives/10289</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文章探讨了如何在概率空间中优化目标函数，并提出新的分析和计算方法。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了在概率空间中进行优化的复杂性，以及传统的无约束优化方法如求导与梯度下降在处理概率分布时的局限。由于概率分布的输入特性，直接求解梯度零点可能导致不再是有效的概率分布，因此需要探索新的优化方法来确保结果的合理性。作者分享了自己在学习概率分布优化过程中的所思所感，并希望通过整理总结为读者提供参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10289" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
</item>
<item>
<title>对齐全量微调！这是我看过最精彩的LoRA改进（二）</title>
<link>https://spaces.ac.cn/archives/10266</link>
<guid>https://spaces.ac.cn/archives/10266</guid>
<content:encoded><![CDATA[
<p>前两周笔者写了<a href="https://kexue.fm/archives/10226" target="_blank">《对齐全量微调！这是我看过最精彩的LoRA（一）》</a>（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，这样做也只能尽量对齐第一步更新后的$W_1$，所以当时就有读者提出了“后面的$W_2,W_3,\cdots$不管了吗？”的疑问，当时笔者也没想太深入，就单纯觉得对齐了第一步后，后面的优化也会严格一条较优的轨迹走。</p><p>有趣的是，LoRA-GA才出来没多久，arXiv上就新出了<a href="https://arxiv.org/abs/2407.18242" target="_blank">《LoRA-Pro: Are Low-Rank Adapters Properly Optimized?》</a>，其所提的LoRA-Pro正好能回答这个问题！LoRA-Pro同样是想着对齐全量微调，但它对齐的是每一步梯度，从而对齐整条优化轨迹，这正好是跟LoRA-GA互补的改进点。</p><h2>对齐全量</h2><p>本文接着上一篇文章的记号和内容进行讲述，所以这里仅对上一节的内容做一个简单回顾，不再详细重复介绍。LoRA的参数化方式是<br />
\begin{equation}W = (W_0 - A_0 B_0) + AB\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10266" title="对齐全量微调！这是我看过最精彩的LoRA改进（二）">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
</item>
<item>
<title>会话分析任务的系统化研究与应用前景</title>
<link>https://arxiv.org/abs/2409.14195</link>
<guid>https://arxiv.org/abs/2409.14195</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统化了会话分析任务，明确其四个关键步骤，探讨行业应用与未来研究方向。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的发展，会话日志的积累日益增多，会话分析（CA）成为获取关键商业洞察的手段。本文定义了CA任务，并系统梳理相关研究，指出CA的四个关键步骤：会话场景重建、深入归因分析、针对性训练以及基于训练生成对话。我们还展示了相关基准，讨论了行业与学术面临的挑战，以及未来研究方向。目前，大部分研究集中在浅层对话元素的分析上，亟需提高分析深度。借助大型语言模型，最新的研究逐渐向因果关系与战略任务发展，呈现出复杂和高级的研究趋势。这些经验和洞察将在商业运营中，尤其是针对会话日志的分析上，有广泛的应用价值。<img alt="会话分析示意图" src="your_image_link_here" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14195" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 11:20:10 GMT</pubDate>
</item>
<item>
<title>隐性指令调优：语言模型的指令跟随能力探析</title>
<link>https://arxiv.org/abs/2409.14254</link>
<guid>https://arxiv.org/abs/2409.14254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型可通过隐性调优实现指令跟随，且不依赖于指令-响应对训练。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了隐性指令调优在语言模型中的应用。首先发现，训练仅基于响应，而无需对应的指令，仍能实现指令跟随，说明预训练模型已具备潜在的指令-响应映射。其次，即使是在狭域数据上进行指令-响应训练，仍能产生广泛的指令跟随行为，如从诗歌生成食谱。当指令与狭域微调内容相去甚远时，模型的响应往往不再遵循微调领域的风格。为解释隐性指令调优，研究者假设简单的分布变化可促进指令跟随。通过手动编写一种规则驱动的语言模型，并与预训练模型结合，研究者发现只需缓慢增加序列结束概率、惩罚重复、均匀调整15个单词的概率即可实现指令跟随。综上所述，那些未专门设计为实现指令跟随的适应性调优也可以隐性地发挥这一作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 10:24:59 GMT</pubDate>
</item>
<item>
<title>Structured-GraphRAG：提升自然语言查询中的信息检索效率</title>
<link>https://arxiv.org/abs/2409.17580</link>
<guid>https://arxiv.org/abs/2409.17580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Structured-GraphRAG通过知识图谱提升信息检索有效性，适用于复杂数据集和自然语言查询。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Structured-GraphRAG框架，旨在提升自然语言查询中对复杂结构数据集的信息检索能力。传统的数据检索方法，如顺序搜索和基于索引的检索，往往在处理复杂且相互连接的数据结构时表现不佳，导致结果不完整或误导。Structured-GraphRAG利用多个知识图谱，以结构化的形式展示数据，捕捉实体之间的复杂关系，从而实现对信息的更细致和全面的检索。通过将响应置于结构化格式中，该图形化方法减少了语言模型输出中的错误风险，提高了结果的可靠性。本文通过与最新发布的方法进行比较，展示了Structured-GraphRAG的有效性，研究发现，该框架显著提升了查询处理效率并缩短了响应时间。尽管案例研究集中于足球数据，但该框架的设计广泛适用，可为数据分析提供强大工具，并提升各类结构化领域的语言模型应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 07:22:46 GMT</pubDate>
</item>
<item>
<title>基于聚类的令牌池化方法以降低ColBERT向量存储需求</title>
<link>https://arxiv.org/abs/2409.14683</link>
<guid>https://arxiv.org/abs/2409.14683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种聚类池化方法，显著降低ColBERT索引的存储需求，性能几乎不受影响。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们介绍了一种简单的基于聚类的令牌池化方法，以大幅减少ColBERT方法中的向量存储数量。通过这一方法，可将ColBERT索引的空间和内存占用减少50%，且几乎不影响检索性能。此外，该方法支持进一步的向量数量削减，达到66%-75%之间，而在大多数数据集上性能降级保持在5%以下。重要的是，该方法无需进行架构改变或查询时处理，可以作为简单的替代方法在任何类似ColBERT模型的索引阶段使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 04:54:39 GMT</pubDate>
</item>
<item>
<title>提升潜在扩散模型生成高频细节的像素空间监督方法</title>
<link>https://arxiv.org/abs/2409.17565</link>
<guid>https://arxiv.org/abs/2409.17565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在后训练过程中增加像素空间监督，显著提升潜在扩散模型的生成质量。</p><br /><br /><p><strong>摘要：</strong> 潜在扩散模型（LDMs）在图像生成领域取得了显著进展。虽然LDM在压缩的潜在空间中进行训练与部署更为高效，但仍存在高频细节与复杂构图生成不佳的问题。我们认为，这部分由于训练过程中的潜在空间分辨率较低。为了解决这一问题，我们提出在后训练过程中增加像素空间监督，以更好地保留高频细节。实验结果显示，在最先进的DiT transformer和U-Net扩散模型下，增加像素空间目标能够显著提升监督质量微调和偏好基础后训练的效果，尤其在视觉质量和视觉缺陷指标上有大幅改善。同时，文本对齐质量保持不变。<img alt="Sample Image 1" src="image_link_1" /><img alt="Sample Image 2" src="image_link_2" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:58:25 GMT</pubDate>
</item>
<item>
<title>LLaVA-3D：一种高效的三维场景理解框架</title>
<link>https://arxiv.org/abs/2409.18125</link>
<guid>https://arxiv.org/abs/2409.18125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-3D结合2D理解能力与3D场景理解，采用3D Patch表示，提高训练速度及任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LLaVA-3D的框架，旨在提升三维场景理解能力，同时不损害二维理解性能。LLaVA-3D利用了LLaVA的强大二维理解先验，通过引入一种简单有效的3D Patch表示，将2D CLIP特征与3D空间位置相连接。通过将3D Patch集成到2D大规模多模态模型中，并进行二维和三维视觉-语言指令的联合调优，LLaVA-3D实现了统一的架构。实验结果表明，LLaVA-3D在训练3D视觉-语言数据集时收敛速度比现有的3D大规模多模态模型快3.5倍。此外，LLaVA-3D在多个3D任务上表现出色，并且在二维图像理解和视觉-语言对话能力方面，性能与LLaVA相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:29:44 GMT</pubDate>
</item>
<item>
<title>Lotus：基于扩散模型的视觉基础模型在密集预测任务中的应用</title>
<link>https://arxiv.org/abs/2409.18124</link>
<guid>https://arxiv.org/abs/2409.18124</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lotus模型通过直接预测标注和简化扩散过程，提升了在零-shot情境下的深度和法线估计性能。</p><br /><br /><p><strong>摘要：</strong> Lotus是一种新型的扩散基础视觉模型，其通过直接预测标注而非噪声来提升密集预测任务的性能。针对现有扩散模型在图像生成和密集预测之间的差异，Lotus重构了扩散过程为单步过程，简化了优化流程并显著提升了推理速度。同时，Lotus引入了一种新的调优策略——细节保持器，使得预测结果更加准确和细致。经过验证，Lotus在多个数据集上实现了深度和法线估计的最先进表现，而且在不增加训练数据或模型容量的情况下，效率提升达到数百倍。其研究为提高密集预测任务中的零-shot泛化提供了新的思路和解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18124" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:19:47 GMT</pubDate>
</item>
<item>
<title>机器人模仿人类操作新物体的方法</title>
<link>https://arxiv.org/abs/2409.18121</link>
<guid>https://arxiv.org/abs/2409.18121</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了机器人观察人类演示操作的方法，使用4D可微分部件模型来实现对象的三维运动恢复与模仿。</p><br /><br /><p><strong>摘要：</strong> 本研究开发了机器人观看和模仿（RSRD）的方法，使机器人能够通过观看人类的单视角RGB演示，学习操控新物体。首先提出了4D可微分部件模型（4D-DPM），利用可微分渲染从单一视频中恢复三维部分运动。该分析合成方法采用部分中心特征场进行迭代优化，应用几何正则化器，仅使用单一视频恢复三维运动。在获得4D重建后，机器人规划双手臂动作，复制演示的物体轨迹。通过将演示表示为部分中心轨迹，RSRD侧重于复制演示的预期行为，同时考虑到机器人的形态限制，而非单纯重现手部运动。研究评估了4D-DPM在真实标注的3D部分轨迹上的跟踪准确性，以及RSRD在9个物体上各进行10次试验的物理执行表现。其中每个阶段的平均成功率为87%，在90次试验中的总端到端成功率为60%。值得注意的是，这一切只使用了从大型预训练视觉模型中提取的特征场，而无需任何任务特定的训练、微调、数据集收集或标注。项目页面：<a href="https://robot-see-robot-do.github.io">https://robot-see-robot-do.github.io</a></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18121" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:03:23 GMT</pubDate>
</item>
<item>
<title>EMOVA：情感全方位的语音助手</title>
<link>https://arxiv.org/abs/2409.18042</link>
<guid>https://arxiv.org/abs/2409.18042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EMOVA通过分离语音语义和声学特征，实现语音生成与视觉理解的优越表现。</p><br /><br /><p><strong>摘要：</strong> EMOVA（EMotionally Omni-present Voice Assistant）是一个新型的情感语音助手，旨在弥补现有大型语言模型在语音与视觉理解上的不足。通过语义-声学分离的语音分词器，EMOVA显著提升了视觉-语言和语音能力，且超越了传统的双模态对齐模型。此外，EMOVA还引入了轻量级风格模块，实现灵活的语音风格控制，例如情感和音调调节。这一成果实现了在视觉-语言及语音基准测试上达到最先进的性能，并首次支持具有生动情感的全模态对话能力，标志着在开放源代码社区中对多模态模型的进一步发展。<img alt="EMOVA示意图" src="原文中的图片链接" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18042" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:28:22 GMT</pubDate>
</item>
<item>
<title>MaskLLM：一种用于大语言模型的可学习剪枝方法</title>
<link>https://arxiv.org/abs/2409.17481</link>
<guid>https://arxiv.org/abs/2409.17481</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaskLLM通过学习掩码分布，在LLMs中实现了半结构化稀疏性，显著降低了推理时的计算开销。</p><br /><br /><p><strong>摘要：</strong> 该研究提出了MaskLLM，一种可学习剪枝方法，实现在大型语言模型（LLMs）中引入半结构化稀疏性（2:4稀疏性），以减少推理的计算负担。MaskLLM通过Gumbel Softmax采样，将N:M模式显式建模为可学习的分布，避免了新重要性标准的开发。该方法在大规模数据集上进行端到端训练，展现出两个显著优势：一是高质量的掩码，能有效扩展至大数据集并学习准确掩码；二是可转移性，通过概率建模掩码分布，可以实现跨领域或任务的稀疏性迁移。研究使用2:4稀疏性评估了MaskLLM在多种LLM（包括LLaMA-2、Nemotron-4和GPT-3，参数量从843M到15B不等）上的表现，结果显示其显著优于现有方法。具体而言，虽然领先的方法在Wikitext上达到的困惑度（PPL）大于10，而Dense模型的PPL为5.12，但MaskLLM通过学习掩码，并保持权重不变，取得了显著更低的6.72 PPL。此外，MaskLLM的可学习特性允许为下游任务或领域定制无损应用2:4稀疏性。代码可在 <a href="https://github.com/NVlabs/MaskLLM">这里</a>获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17481" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:18:19 GMT</pubDate>
</item>
<item>
<title>GemFilter：一种加速长文本输入处理的新方法</title>
<link>https://arxiv.org/abs/2409.17422</link>
<guid>https://arxiv.org/abs/2409.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GemFilter通过初级层筛选和压缩输入令牌，提高LLM推理速度与内存效率。</p><br /><br /><p><strong>摘要：</strong> 本研究提出GemFilter，一种新的算法，通过使用LLM的早期层作为过滤器，选择和压缩输入令牌，从而显著减少后续处理的上下文长度。GemFilter在速度和内存效率方面相较于现有技术（如标准注意力和SnapKV/H2O）显示出显著改善，尤其是在Needle in a Haystack任务中表现优异，达到了2.4倍的速度提升以及30%的GPU内存使用减少。此外，GemFilter在LongBench挑战中与其他方法表现相当，且无需训练，简单可广泛适用于不同的LLM。更重要的是，GemFilter提供的可解释性使人们可以检查所选输入序列，从而增强了对LLM内部机制的理解，推动了对LLM设计和推理的进一步优化。更多信息可访问 [GemFilter项目页面](https://github.com/SalesforceAIResearch/GemFilter)。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:15:22 GMT</pubDate>
</item>
<item>
<title>Disco4D：基于单幅图像的4D人体生成与动画新框架</title>
<link>https://arxiv.org/abs/2409.17280</link>
<guid>https://arxiv.org/abs/2409.17280</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Disco4D通过高斯建模，实现单幅图像下的4D人体生成与动画，增强了细节与灵活性。</p><br /><br /><p><strong>摘要：</strong> 我们提出了Disco4D，这是一个创新的高斯点云框架，用于从单幅图像生成和动画化4D人类形象。与现有方法不同，Disco4D独特地将衣物（通过高斯模型表示）与人体（采用SMPL-X模型）解耦，从而显著提高了生成细节和灵活性。其技术创新包括：1）Disco4D能够高效地将衣物高斯模型与SMPL-X高斯模型进行拟合；2）它采用扩散模型增强3D生成过程，如对输入图像中不可见的遮挡部分进行建模；3）它为每个衣物高斯学习了身份编码，以便于衣物资产的分离与提取。此外，Disco4D自然支持具有生动动态的4D人类动画。大量实验表明，Disco4D在4D人类生成与动画任务中优于其他方法。我们的可视化结果可以在 [Disco4D官网](https://disco-4d.github.io/) 中查看。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17280" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:10:00 GMT</pubDate>
</item>
</channel>
</rss>