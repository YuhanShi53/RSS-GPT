<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>生成性扩展的树模型：一种基于能量的增强算法</title>
<link>https://arxiv.org/abs/2410.03535</link>
<guid>https://arxiv.org/abs/2410.03535</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究提出了一种基于能量的生成性增强算法，能够在处理表格数据时表现出与GBDT相似的性能。</p><br><br><p><strong>摘要：</strong> 尽管深度学习在非结构化数据领域占据主导地位，但树模型如随机森林（RF）和梯度提升决策树（GBDT）仍然是处理表格数据任务的主要工具。本文探讨了这些流行算法的生成性扩展，重点在于显式建模数据密度（至归一化常数），从而实现除采样外的其他应用。我们的主要贡献是提出一种能量基础的生成性增强算法，类似于XGBoost中实现的二阶增强。尽管生成模型能够处理任何输入变量的推断任务，我们的算法在多个真实世界的表格数据集上实现了与GBDT相似的判别性能，且优于其他生成性方法。同时，我们还表明该模型在采样方面与神经网络模型具有竞争力。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.03535 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 07:17:36 GMT</pubDate>
<pubDate>Mon, 07 Oct 2024 07:17:36 GMT</pubDate>
</item>
<item>
<title>NL-Eye: 评估视觉语言模型的视觉推理能力</title>
<link>https://arxiv.org/abs/2410.02613</link>
<guid>https://arxiv.org/abs/2410.02613</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>NL-Eye基准评估VLM在视觉推理方面的能力，展示了其在推理任务上的不足。</p><br><br><p><strong>摘要：</strong> 本文介绍了NL-Eye基准，该基准旨在评估视觉语言模型（VLM）在视觉推理中的表现。尽管现代VLM展现了显著能力，但它们在推理因果关系及结果的能力上仍未得到充分探索。NL-Eye通过将自然语言推理(NLI)任务的推理方式适应于视觉领域，要求模型根据前提图像评估假设图像的合理性，并解释其决策。NL-Eye包含了350个精心策划的三重示例，共计1050幅图像，涵盖了物理、功能、逻辑、情感、文化和社会等多种推理类别。数据策划过程包括两个步骤：撰写文字描述和使用文本到图像模型生成图像，均需大量人力参与以确保场景的高质量和挑战性。实验结果显示，VLM在NL-Eye上表现不佳，表现水平接近随机基线，而人类在合理性预测和解释质量上表现优异。这表明现代VLM在推理能力方面存在缺陷。NL-Eye是向开发具有强大多模态推理能力的VLM迈出的重要一步，适用于现实世界应用，包括事故预防机器人和生成视频验证。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.02613 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:59:53 GMT</pubDate>
<pubDate>Mon, 07 Oct 2024 06:59:53 GMT</pubDate>
</item>
<item>
<title>结合视觉与语言指导的评论意识导航框架CANVAS</title>
<link>https://arxiv.org/abs/2410.01273</link>
<guid>https://arxiv.org/abs/2410.01273</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>CANVAS框架通过人类行为模仿学习，实现了更好的机器人导航，适应噪声指令，显示出人类指导的应用潜力。</p><br><br><p><strong>摘要：</strong> 本文介绍了CANVAS框架，该框架结合了视觉和语言指令，以实现评论意识导航。为了使机器人有效地理解和执行人类的抽象导航指令，我们构建了COMMAND数据集，包含超过48小时和219公里的人工标注导航结果。实验结果表明，CANVAS在所有测试环境中均优于强大的基于规则的系统ROS NavStack，尤其在面对噪声指令时表现出色。在果园环境中，ROS NavStack的总成功率为0%，而CANVAS则达到了67%的总成功率。此外，CANVAS能够在未见环境中与人类示范和常识约束保持紧密一致。实际上，当将CANVAS部署到现实世界中时，显示出69%的总成功率，突显了从模拟环境中的人类示范学习在实际应用中的潜力。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.01273 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:56:55 GMT</pubDate>
<pubDate>Mon, 07 Oct 2024 06:56:55 GMT</pubDate>
</item>

<item>
<title>Mamba模型在医学图像分析中的应用潜力</title>
<link>https://arxiv.org/abs/2410.02362</link>
<guid>https://arxiv.org/abs/2410.02362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mamba模型以线性复杂度优势，适合医学成像数据分析，提升诊断精度并改善患者结果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mamba作为状态空间模型的特殊案例，在医学图像分析中日益受到关注。与强大的变换器架构相比，Mamba具有线性时间复杂性，并能够高效处理长序列数据，解决了处理大型复杂数据集中的长程依赖问题。Mamba在融合多模态数据方面表现出色，能够提高诊断准确性和改善患者结果。文章分步骤阐述Mamba的能力，首先定义状态空间模型及其核心概念，然后探讨Mamba架构，如纯Mamba、U-Net变体，以及与卷积神经网络、变换器和图神经网络的混合模型。此外，文章还涵盖了Mamba的优化技术、扫描方法、数据集与应用、实验结果，最后讨论其在医学成像领域面临的挑战和未来发展方向。本综述旨在展示Mamba在克服医学图像分析中的现有障碍方面的变革潜力，并为该领域的创新进展铺平道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:23:18 GMT</pubDate>
</item>
<item>
<title>RoCoTex: A Robust Method for Consistent Texture Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2409.19989</link>
<guid>https://arxiv.org/abs/2409.19989</guid>
<content:encoded><![CDATA[
Text-to-texture generation has recently attracted increasing attention, but existing methods often suffer from the problems of view inconsistencies, apparent seams, and misalignment between textures and the underlying mesh. In this paper, we propose a robust text-to-texture method for generating consistent and seamless textures that are well aligned with the mesh. Our method leverages state-of-the-art 2D diffusion models, including SDXL and multiple ControlNets, to capture structural features and intricate details in the generated textures. The method also employs a symmetrical view synthesis strategy combined with regional prompts for enhancing view consistency. Additionally, it introduces novel texture blending and soft-inpainting techniques, which significantly reduce the seam regions. Extensive experiments demonstrate that our method outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 06:12:06 GMT</pubDate>
</item>
<item>
<title>基于混合专家与组聚合的股票市场预测方法MIGA</title>
<link>https://arxiv.org/abs/2410.02241</link>
<guid>https://arxiv.org/abs/2410.02241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIGA框架通过动态切换专家提升股票风格预测，显著提高预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MIGA，一种新颖的混合专家与组聚合框架，旨在针对不同风格的股票生成专业化预测。由于股票风格和市场趋势的显著差异，传统的单一模型很难充分捕捉各类股票的特性，从而导致预测准确性下降。MIGA通过动态切换不同的风格专家来克服这一问题，并引入了一种新型的内部组注意力架构，使得同组专家之间能够有效共享信息，从而提升整体性能。实验证明，MIGA在中国三大股票指数基准（CSI300、CSI500和CSI1000）上显著优于其他端到端模型。其中，MIGA-Conv在CSI300基准上的年超额收益达到24%，领先于之前的最优模型8%。此外，本文还对混合专家在股票市场预测中的应用进行了全面分析，提供了未来研究的有价值见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 04:50:12 GMT</pubDate>
</item>
<item>
<title>基于整数加法的高精度浮点数乘法近似算法 L-Mul</title>
<link>https://arxiv.org/abs/2410.00907</link>
<guid>https://arxiv.org/abs/2410.00907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">L-Mul算法通过整数加法近似浮点数乘法，显著降低能耗且保持高精度。</p><br /><br /><p><strong>摘要：</strong> 本研究发现，浮点乘法可以通过一个整数加法器以高精度进行近似，从而提出了线性复杂度乘法算法L-Mul。L-Mul在计算资源上显著低于8位浮点乘法，且实现了更高的精度。与8位浮点乘法相比，L-Mul在位级计算上消耗显著更少的资源，同时，由于整数加法所需能量远低于浮点乘法，使用L-Mul操作可以显著降低能耗，估计在逐元素浮点张量乘法中可减少95%的能量消耗，在点积中可减少80%。理论误差期望的计算表明，L-Mul具有良好的精度，实验验证显示4位尾数的L-Mul表现与float8_e4m3乘法相当，而3位尾数的L-Mul则优于float8_e5m2。在多个任务，包括自然语言理解、结构推理、数学及常识问答的基准测试中，直接将L-Mul应用于注意力机制几乎无损。此外，在变换器模型中用3位尾数的L-Mul替代所有浮点乘法，在微调及推理中与使用float8_e4m3作为累积精度达到相同的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 03:23:44 GMT</pubDate>
</item>
<item>
<title>Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise</title>
<link>https://arxiv.org/abs/2410.03017</link>
<guid>https://arxiv.org/abs/2410.03017</guid>
<content:encoded><![CDATA[
Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p&lt;0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 03:03:15 GMT</pubDate>
</item>
<item>
<title>语言模型中的概念抹除评估框架与新方法ELM</title>
<link>https://arxiv.org/abs/2410.02760</link>
<guid>https://arxiv.org/abs/2410.02760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了针对语言模型的概念抹除评估框架，并介绍了新方法ELM。</p><br /><br /><p><strong>摘要：</strong> 概念抹除在语言模型中传统上缺乏全面评估框架，导致对抹除方法的效果评估不全面。我们提出了一个评估范式，围绕三个关键标准进行评估：无辜性（完全知识移除）、无缝性（保持条件流畅生成）和特异性（保留无关任务表现）。我们的评估指标自然促使了语言记忆抹除（ELM）新方法的发展，旨在解决这三个维度的问题。ELM通过有针对性的低秩更新改变被抹除概念的输出分布，同时保持模型在提示被抹除概念时的流畅性。我们在生物安全、网络安全和文学领域的抹除任务上展示了ELM的有效性。比较分析显示，ELM在我们提出的指标上表现优越，包括在被抹除主题评估中的接近随机得分、生成的流畅性、在无关基准上保持的准确性以及在对抗攻击下的鲁棒性。我们的代码、数据和训练模型可在https://elm.baulab.info获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 02:44:40 GMT</pubDate>
</item>
<item>
<title>选择性注意机制提升语言模型性能</title>
<link>https://arxiv.org/abs/2410.02703</link>
<guid>https://arxiv.org/abs/2410.02703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">选择性注意通过减少对无关元素的关注，显著提高了语言模型的性能，并降低了计算和内存需求。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种选择性注意机制，它是对标准注意力机制的简单无参数改进，能够有效减少对不必要元素的关注。选择性注意在多种模型规模和上下文长度的语言模型中提升性能。例如，在C4数据集上训练的一系列变压器模型，采用选择性注意机制后，其性能与标准变压器模型相当，但后者拥有约两倍的注意头和参数。此外，选择性注意还允许减小注意力上下文缓冲区的大小，有效降低推理过程中的内存和计算需求。研究表明，拥有选择性注意的100M参数的变压器在C4上训练，使用512、1024和2048的上下文大小时，分别需要比不使用选择性注意的模型少16倍、25倍和47倍的注意力模块内存，同时保持相同的验证困惑度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Oct 2024 02:26:31 GMT</pubDate>
</item>
<item>
<title>上下文感知的文档嵌入方法研究</title>
<link>https://arxiv.org/abs/2410.02525</link>
<guid>https://arxiv.org/abs/2410.02525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种考虑文档邻居的上下文感知文档嵌入方法，显著提升了信息检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了密集文档嵌入在神经检索中的应用，指出传统的文档嵌入在某些特定检索场景下显得缺乏上下文。我们提出两种方法以实现上下文感知的文档嵌入：第一，采用替代对比学习目标，显式将邻近文档纳入批内上下文损失；第二，设计一种新的上下文架构，直接编码邻近文档的信息。实验结果表明，这两种方法均在多个场景中显著优于双编码器，尤其在域外任务中表现尤为突出。在MTEB基准上，我们的方法在没有困难负样本挖掘、分数蒸馏、特定数据集指令、跨GPU例子共享或极大批量大小的情况下，达到了最新的研究成果。该方法适用于提高任何对比学习数据集和双编码器的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02525" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:51:54 GMT</pubDate>
</item>
<item>
<title>通过模型合并提升非英语数学推理能力的跨语言迁移</title>
<link>https://arxiv.org/abs/2410.01335</link>
<guid>https://arxiv.org/abs/2410.01335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种模型融合方法，以提升非英语语言中的数学推理能力，通过层交换实现跨语言传递。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种模型合并 methodology，旨在解决在非英语语言中对大型语言模型（LLMs）进行微调的困难，尤其是在缺少任务特定数据的情况下。我们专注于数学推理，通过从相同的预训练模型出发，分别对英语的数学教学数据和目标语言的通用教学数据进行微调，培养出不同的“专家”。然后，我们直接替换数学专家的顶部和底部变压器层，采用语言专家的层，从而增强目标语言中的数学性能。经过这一过程，合并后的模型在数学基准测试MGSM上表现超越个别专家及其他合并方法，提升幅度达10%横跨四种在数学教学数据稀缺的主要语言。此外，这种层交换方法简单、便宜且直观，基于对微调过程中最重要的参数变化的解释分析。成功地以这种方式重新组合LLMs以实现跨语言迁移，开启了未来结合模型专业知识、创造模块化解决方案和跨语言转移推理能力的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:44:51 GMT</pubDate>
</item>
<item>
<title>减少视觉语言模型幻觉的知识消除算法研究</title>
<link>https://arxiv.org/abs/2410.02762</link>
<guid>https://arxiv.org/abs/2410.02762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过消除视觉语言模型幻觉的算法，优化模型内在表示，提升可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）内部表示以应对幻觉现象，尽管模型规模和训练已有进展。我们将VLMs的内部图像表示投影到语言词汇，观察到对真实物体的输出概率高于幻觉物体。此外，我们利用这些输出概率进行真实物体的空间定位。基于此方法，我们提出了一种知识消除算法，通过对幻觉物体特征进行线性正交化，来去除幻觉。我们展示了有针对性地编辑模型的潜在表示可以在保留性能的同时，减少多达25.7%的幻觉发生率，具体是在COCO2014数据集上进行测试。我们的研究表明，深入理解VLMs的潜在表示能够提升其可靠性，并实现新的能力，例如零样本分割。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:31:50 GMT</pubDate>
</item>
<item>
<title>自反蒙特卡罗树搜索算法的应用于多步骤决策任务</title>
<link>https://arxiv.org/abs/2410.02052</link>
<guid>https://arxiv.org/abs/2410.02052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出自反蒙特卡罗树搜索（R-MCTS）算法，显著提升了VLM在复杂决策任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为自反蒙特卡罗树搜索（R-MCTS）的新测试时算法，它旨在增强GPT-4o等AI代理在复杂多步骤决策任务中的能力。R-MCTS通过结合对比反思以及多智能体辩论来扩展传统的蒙特卡罗树搜索（MCTS），使得代理能够在测试期间实时探索决策空间并学习过去的交互以提高搜索效率。在VisualWebArena基准测试中，基于GPT-4o的R-MCTS代理在多个任务中相较于之前的最佳水平实现了6%到30%的相对提升。此外，测试时的搜索得到的知识能够有效地通过自学习方法进行传回，精调后的GPT-4o能够达到97%的R-MCTS性能，同时在测试时将计算资源的使用降低了四倍。研究结果还表明，精调后的GPT-4o具有良好的环境探索能力，能够评估状态并在检测到当前状态无法带来成功时进行回溯。这些发现表明，运用测试时搜索和自学习提升视觉语言模型的推理与规划能力具有良好的研究前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 16:15:21 GMT</pubDate>
</item>
<item>
<title>Open-RAG：增强推理能力的检索增强生成框架</title>
<link>https://arxiv.org/abs/2410.01782</link>
<guid>https://arxiv.org/abs/2410.01782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Open-RAG框架提升开源LLM的推理能力，通过动态专家选择和混合检索提高准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的框架Open-RAG，旨在提高开源大语言模型（LLM）在检索增强生成（RAG）中的推理能力。不同于现有方法，Open-RAG将任意稠密LLM转化为一种参数高效的稀疏专家混合（MoE）模型，专门处理复杂推理任务，包括单步和多步查询。该框架训练模型有效应对那些客观上相关但具有误导性的困难干扰项，从而利用潜在学习机制，动态选择相关专家并有效整合外部知识，提供更准确且上下文相关的回答。此外，研究还提出了一种混合自适应检索方法，以确定检索的必要性，并在性能提升与推理速度之间取得平衡。实验结果表明，基于Llama2-7B的Open-RAG在各种知识密集型任务中超越了现有的LLM和RAG模型，比如ChatGPT、自我检索（Self-RAG）和Command R+。研究团队已将代码和模型开源，地址为https://openragmoe.github.io/</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:40:20 GMT</pubDate>
</item>
<item>
<title>SciPrompt：低资源条件下科学文本分类的自动语境增强框架</title>
<link>https://arxiv.org/abs/2410.01946</link>
<guid>https://arxiv.org/abs/2410.01946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciPrompt框架通过自动获取科学领域相关术语，提升低资源文本分类任务中的模型性能。</p><br /><br /><p><strong>摘要：</strong> 在预训练语言模型的基础上，提示式微调已成为多类别分类任务的重要方法，尤其在低资源场景中表现出与完全微调方法相当的性能。以往的研究集中在手工设计提示模板和通过标签词映射解决分类问题，而跨领域和细粒度的提示式微调及其自动增强的语言标示器尚未得到探讨。针对这一挑战，本文提出了SciPrompt框架，旨在自动检索科学主题相关术语，以支持低资源文本分类任务。该框架在科学文献中选择语义相关的领域特定标签术语进行标示器增强。此外，本文还提出了一种新的标示策略，通过关联分数作为额外权重来提升语言模型的预测性能。实验证明，我们的方法在少样本和零样本设置下的科学文本分类任务中优于现有的最先进提示式微调方法，尤其在细粒度和新兴科学主题的分类中取得了显著效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:38:15 GMT</pubDate>
</item>
<item>
<title>基于复杂性对智能行为的探讨：元胞自动机与大型语言模型的关系</title>
<link>https://arxiv.org/abs/2410.02536</link>
<guid>https://arxiv.org/abs/2410.02536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究复杂规则对人工智能系统行为的影响，发现适度复杂性与智能表现提升之间的关系。</p><br /><br /><p><strong>摘要：</strong> 本文研究了复杂规则如何影响人工系统中智能行为的出现，重点探讨了一维的元胞自动机（ECA）。通过将不同的语言模型（LLM）训练于不同的元胞自动机，分析了规则行为的复杂性与LLM在下游任务中的表现之间的关系。研究发现，较高复杂性的规则生成的模型在推理和国际象棋走法预测任务上表现更好，而均匀系统、周期系统及高度混沌系统却表现较差。研究指出，存在一个复杂性适中的“甜点”，这有助于智能的表现。我们推测，智能的产生与预测复杂性的能力有关，并认为创造智能可能仅需接触到复杂性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:23:17 GMT</pubDate>
</item>
<item>
<title>Robin3D：基于新型数据引擎的高效3D大型语言模型</title>
<link>https://arxiv.org/abs/2410.00255</link>
<guid>https://arxiv.org/abs/2410.00255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Robin3D是一个强大的3DLLM，通过丰富的指令生成数据提升了模型的区分能力与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Robin3D，一个强大的3D大型语言模型(3DLLM)，它通过我们新型的数据引擎——鲁棒指令生成(RIG)引擎，训练于大规模的指令跟随数据。RIG生成了两类关键指令数据：一是对抗性指令跟随数据，包含混合的负样本和正样本，以增强模型的区分理解能力；二是多样性指令跟随数据，包含各种指令风格，以提高模型的泛化能力。最终，我们构建了100万个指令跟随数据，包括344,000个对抗样本、508,000个多样样本和165,000个基准训练集样本。为了更好地处理这些复杂的指令，Robin3D首先通过关系增强投影器提升空间理解能力，然后通过ID特征绑定增强对象引用和定位能力。Robin3D在五个广泛使用的3D多模态学习基准上，始终优于以往的方法，且无需任务特定的微调。在定位任务(Multi3DRefer)中取得了7.8%的提升，在描述任务(Scan2Cap)中提升了6.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 15:08:16 GMT</pubDate>
</item>
<item>
<title>VinePPO：优化大型语言模型推理任务的信用分配机制</title>
<link>https://arxiv.org/abs/2410.01679</link>
<guid>https://arxiv.org/abs/2410.01679</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VinePPO通过蒙特卡洛基于估计改进信用分配，超越PPO在推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究系统评估了价值网络在复杂推理任务中应用的有效性，发现其在任务中的预测表现不佳，导致高方差更新和次优性能。通过与随机基线比较，结果表明价值网络在推理任务中的表现微弱优于随机策略。为了解决这一问题，提出了VinePPO，利用语言环境的灵活性来计算无偏的蒙特卡洛估计，避免了大型价值网络的需求。实验结果显示，VinePPO在MATH和GSM8K数据集上的表现持续优于PPO和其他无RL基线，且所需的梯度更新次数减少了最多9倍，墙钟时间缩短最多3倍。这些结果强调了准确信用分配在大型语言模型强化学习微调中的重要性，展示了VinePPO作为更优替代方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01679" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 12:05:54 GMT</pubDate>
</item>
<item>
<title>小型预训练生成语言模型在国际象棋规则学习中的应用</title>
<link>https://arxiv.org/abs/2410.02426</link>
<guid>https://arxiv.org/abs/2410.02426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，小型语言模型可以通过微调学习国际象棋规则并提出合法走棋指令。</p><br /><br /><p><strong>摘要：</strong> 本文展示了小型预训练基础生成语言模型（SLMs）在学习复杂过程潜在规则方面的能力。通过受到斯特凡·茨威格的小说《棋王》的启发，研究者们使用28M和125M参数的小型语言模型，进行指令微调，样本量从1,000到1,000,000个不等，成功学习了国际象棋的规则，提出合法走棋，并准确解决棋局问题。本文还探讨了模型微调历程中各个时代对结果的影响，并通过增加指令微调样本的数量，展示了在降低模型幻觉现象方面的成效。该研究表明，即使是小型模型，也能在特定领域达到良好的学习效果，为日后在其他领域的应用提供了新的思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 11:11:31 GMT</pubDate>
</item>
<item>
<title>大语言模型性能的理论分析: 马尔可夫链的视角</title>
<link>https://arxiv.org/abs/2410.02724</link>
<guid>https://arxiv.org/abs/2410.02724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文通过将自回归语言模型与有限状态空间的马尔可夫链等价，分析了其性能的理论基础。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）出色性能的理论基础，建立了自回归语言模型与一维大小为O(T^K)的马尔可夫链之间的等价关系。通过此等价关系，我们得出了一些关于马尔可夫链的平稳分布存在性、收敛速度以及温度影响的意外发现。同时，论文还证明了预训练和上下文泛化的界限，揭示了这些结果的丰富解读。最后，通过对多种最新大语言模型的实验展示，我们验证了理论结果与实际观察之间的对应关系，突出其在自然语言处理任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 07:26:26 GMT</pubDate>
</item>
<item>
<title>多样化多重提升：提升CLIP模型的性能</title>
<link>https://arxiv.org/abs/2409.19291</link>
<guid>https://arxiv.org/abs/2409.19291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新方法，优化CLIP模型，通过稀疏激活的MoE结构显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，Contrastive Language-Image Pre-training (CLIP) 已成为多模态智能的重要基础。然而，研究发现，CLIP编码过程中的信息损失显著，仅能捕捉输入的粗粒度特征，限制了单一CLIP模型在视觉细节丰富的图像处理能力。为解决这一问题，本文提出了一种简单而有效的模型无关策略——多样化多重提升（DMU）。DMU高效地微调了一系列从稠密预训练CLIP检查点中提取的模型，这些模型捕获了不同的特征空间，仅在前馈网络（FFN）上共享参数。随后，这些模型可以转变为具有更大模型容量的CLIP-MoE，在保持计算开销最小的情况下显著提高性能。通过 extensive 实验表明，CLIP-MoE在多种零-shot 检索、零-shot 图像分类任务及下游多模态大语言模型基准测试中表现优异。此外，DMU使得将任何稠密CLIP模型转换为CLIP-MoE成为可能，可以在下游框架中无缝替代CLIP，且无需进一步调整。通过DMU，我们期望为未来多模态学习系统的高效和有效发展提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 07:19:06 GMT</pubDate>
</item>
<item>
<title>Vinoground：评估长视频理解中的时序推理能力</title>
<link>https://arxiv.org/abs/2410.02763</link>
<guid>https://arxiv.org/abs/2410.02763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明现有的多模态模型在短视频的时序推理上仍存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 如今，现代大型多模态模型（LMMs）在短视频理解上取得了一定进展，但研究表明这些模型在关键推理能力上仍存在缺陷。为此，我们引入了Vinoground，这是一项包含1000对短视频和文本描述的时序反事实LMM评估基准。我们的实验显示，现有的LMMs在判断不同动作和物体转变的时间差异时表现不佳。例如，性能最优的模型GPT-4o在我们的文本和视频评分中仅获得约50%的分数，远低于人类基准的90%。所有开源的多模态模型和基于CLIP的模型表现更差，几乎仅能达到随机猜测的水平。这项研究突显了在短视频时序推理上的问题尚未得到解决，促进了对该领域进一步研究的关注。数据集和评估代码可在https://vinoground.github.io获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 06:25:59 GMT</pubDate>
</item>
<item>
<title>自适应投影引导：提高扩散模型生成质量的新方法</title>
<link>https://arxiv.org/abs/2410.02416</link>
<guid>https://arxiv.org/abs/2410.02416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了自适应投影引导（APG）方法，以改进扩散模型中的无分类器引导，解决过饱和问题。</p><br /><br /><p><strong>摘要：</strong> 本文回顾了分类器无关引导（CFG）的更新规则，并提出了改进方案以解决生成图像的过饱和和不真实伪影问题。通过将CFG的更新项分解为与条件模型预测的平行和正交分量，发现平行分量导致过饱和，而正交分量增强了图像质量。基于此，本文建议对平行分量进行下调，以便在不出现过饱和的情况下实现高质量生成。此外，本文还将CFG与梯度上升联系起来，提出了一种新的重标定和动量方法，形成自适应投影引导（APG）。APG保持了CFG在提高质量方面的优势，同时能够在无过饱和的情况下使用更高的引导尺度。APG易于实现，并且对采样过程几乎不增加计算开销。通过广泛的实验，验证了APG与各种条件扩散模型和采样器兼容，改进了FID、召回率和饱和度评分，同时保持了与CFG相当的精度，使其成为一种优于传统分类器无关引导的即插即用替代方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 04:16:57 GMT</pubDate>
</item>
<item>
<title>基于LintSeq生成编辑数据提升代码合成性能</title>
<link>https://arxiv.org/abs/2410.02749</link>
<guid>https://arxiv.org/abs/2410.02749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过LintSeq算法生成编辑序列，提升小型LLM在代码合成任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种合成数据生成算法LintSeq，旨在解决现有开源代码编辑数据匮乏的问题。该算法利用代码检查工具将现有代码重构为一系列代码编辑，通过程序增量的方式生成错误-free的插入操作，输出程序差异的文本序列。为验证LintSeq的有效性，研究者将其用于将指令和程序对的数据集重构为指令和程序差异序列元组，并对一系列从2.6B到14B参数的小型LLM进行指令微调，比较其在无零-shot情况下的代码合成基准表现。结果表明，通过多次采样，基于编辑序列微调的模型在输出多样性和基准覆盖的推理时刻表现优异。例如，在HumanEval的pass@50评估中，经过合成编辑序列微调的小型LLM的表现与GPT-4相当，且较基线模型提高了20%（+/-3%）。此外，本研究还展示了预训练的轻量级模型在代码理解上的潜力，微调150M参数的编辑序列模型时，其性能可与代码模型对比，甚至在一些情况下超越Codex和AlphaCode。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 04:10:28 GMT</pubDate>
</item>
<item>
<title>Distilling an End-to-End Voice Assistant Without Instruction Training Data</title>
<link>https://arxiv.org/abs/2410.02678</link>
<guid>https://arxiv.org/abs/2410.02678</guid>
<content:encoded><![CDATA[
Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT)   have led to models ``forgetting" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using &gt;100x less training compute.
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:45:46 GMT</pubDate>
</item>
<item>
<title>L-CiteEval: 评估长上下文模型理解能力与可信度的全新基准</title>
<link>https://arxiv.org/abs/2410.02115</link>
<guid>https://arxiv.org/abs/2410.02115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究推出L-CiteEval基准，评估长上下文模型的理解能力与引用准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出L-CiteEval，一个全面的多任务基准，旨在评估长上下文模型（LCMs）在理解能力和可信度方面的表现。L-CiteEval涵盖了来自不同领域的11个任务，支持的上下文长度从8K到48K，并提供一个完全自动化的评估工具。通过对11个顶尖封闭源和开源LCMs的测试，发现这些模型在生成结果上的差异较小，但开源模型在引用准确性和召回率方面显著落后于封闭源模型，表明当前的开源LCMs倾向于依赖其自身知识而非给定上下文，从而在实际应用中可能影响用户体验。此外，研究还评估了RAG方法，发现RAG可以显著改善LCMs的可信度，尽管生成质量略有下降。最后，研究揭示LCMs的注意力机制与引用生成过程之间存在相关性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:35:24 GMT</pubDate>
</item>
<item>
<title>将预训练的大型语言模型应用于医学图像分割的研究</title>
<link>https://arxiv.org/abs/2410.02458</link>
<guid>https://arxiv.org/abs/2410.02458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探索了通过集成预训练LLM来增强医学图像分割的ViT模型，取得了显著性能提升。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了将预训练的大型语言模型（LLM）应用于医学图像分割的方法，聚焦于增强视觉变换器（ViT）模型的性能。研究中，我们将一个冻结的LLM变换器模块集成到ViT模型的编码器中，从而在多个医学影像模态上显著提高了分割效果。我们提出了一种混合注意力机制，结合了全局和局部特征学习，并使用多尺度融合模块在不同尺度上聚合特征。经过实验验证，增强模型的平均Dice系数从0.74提升至0.79，同时在准确率、精确度和Jaccard指数等指标上也表现出显著改善。这些结果表明，基于LLM的变换器在提升医学图像分割性能方面的有效性，显示出其在提高模型准确性和鲁棒性方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 03:07:56 GMT</pubDate>
</item>
<item>
<title>Depth Pro：无须元数据的零样本测量单目深度估计模型</title>
<link>https://arxiv.org/abs/2410.02073</link>
<guid>https://arxiv.org/abs/2410.02073</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Depth Pro是一种快速生成高分辨率深度图的零样本模型，具有绝对尺度和边界精度。</p><br /><br /><p><strong>摘要：</strong> Depth Pro是一种新型的基础模型，专注于零样本测量单目深度估计。该模型能够合成高分辨率的深度图，具备无与伦比的清晰度和高频细节，且输出的深度预测具有绝对尺度，完全不依赖于摄像头内参等元数据。此外，模型具备高处理速度，能够在标准GPU上以0.3秒生成2.25兆像素深度图。Depth Pro的优越性能源于多项技术创新，包括高效多尺度视觉变换器用于密集预测、结合真实与合成数据集的训练协议以实现高度的度量精度和细致边界跟踪、专门用于评估深度图边界精度的评价指标，以及从单幅图像中实现的最先进的焦距估计。通过大量实验，本文分析了具体的设计选择，并展示了Depth Pro在多个维度上超越以往工作的表现。相关代码和权重已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02073" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 02:00:44 GMT</pubDate>
</item>
<item>
<title>增强CLIP的区域表征能力：对比定位语言图像预训练的进展</title>
<link>https://arxiv.org/abs/2410.02746</link>
<guid>https://arxiv.org/abs/2410.02746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种改进CLIP模型的方法，增强其区域理解能力，以支持多模态大语言模型的下游任务。</p><br /><br /><p><strong>摘要：</strong> 近年来，对比语言图像预训练（CLIP）方法因其在图像和文本表示生成方面的有效性而备受关注。CLIP的成功使其成为多模态大语言模型（MLLM）的视觉骨干，连接图像输入和语言交互。然而，CLIP的基于网络爬虫的文本注释对图像的对齐方式在需要细粒度视觉表示的下游任务中可能不足，尤其是区域级理解任务。为了解决这一问题，作者提出了对比定位语言图像预训练（CLOC）的方法，通过结合区域-文本对比损失和模块来提高CLIP的定位能力。此外，作者还提出了“可提示嵌入”这一新概念，使编码器可以根据空间提示将图像嵌入轻松转换为区域表示。为支持大规模预训练，设计了一种视觉丰富和空间定位的标注框架，以有效生成区域-文本伪标签。通过扩展到数十亿张带注释的图像，CLOC为图像区域识别和检索任务提供了高质量的区域嵌入，并可以作为CLIP的替代方案，提升MLLM在指代和定位任务上的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:25:38 GMT</pubDate>
</item>
<item>
<title>Loong：一种新型自回归大语言模型视频生成器</title>
<link>https://arxiv.org/abs/2410.02757</link>
<guid>https://arxiv.org/abs/2410.02757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Loong模型能生成分钟级长视频，采用短至长渐进训练和特殊推断策略，提升视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 本研究深入分析了自回归大语言模型（LLM）在视频生成领域面临的挑战，尤其是生成长视频的困难。尽管LLM在自然语言处理方面取得了显著成功，但现有研究主要集中在短视频（几秒钟）的生成上。为了解决这一问题，本文提出了一种新的自回归LLM视频生成器Loong，能够生成分钟长的视频。我们将文本和视频标记建模为一个统一的序列进行训练，并提出了一种短至长的渐进训练方式，结合损失重加权策略，缓解长视频训练中的损失不平衡问题。此外，本文还探讨了推断策略，包括视频标记重新编码和采样策略，以减少推断过程中的误差累积。Loong在10秒视频上进行训练，并能够基于文本提示扩展生成分钟级长视频，展现了良好的效果。更多示例可参考： https://epiphqny.github.io/Loong-video</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:25:31 GMT</pubDate>
</item>
<item>
<title>改进多模态模型的标题生成与原始描述的互动研究</title>
<link>https://arxiv.org/abs/2410.02740</link>
<guid>https://arxiv.org/abs/2410.02740</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的标题生成流程，以优化多模态模型的预训练效果，探讨合成标题与AltText的关系。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖、可控且可扩展的标题生成流程，旨在生成多种不同格式的标题，以满足各类多模态模型的需求。通过将短合成标题（Short Synthetic Captions，SSC）和密集合成标题（Dense Synthetic Captions，DSC+）作为案例研究，系统性地探讨了这些标题在多模态模型（如CLIP、多模态大语言模型及扩散模型）中的效果及其与原始网页抓取的替代文本（AltTexts）的相互作用。研究结果表明，采用混合方法同时保留合成标题和AltTexts的策略，能够在模型对齐和性能方面超越单独使用合成标题的效果，各个模型对特定标题格式存在独特偏好。这一综合分析为优化标题生成策略提供了宝贵的见解，从而推动多模态基础模型的预训练进程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02740" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:11:19 GMT</pubDate>
</item>
<item>
<title>SageAttention：一种高效的注意力量化方法</title>
<link>https://arxiv.org/abs/2410.02367</link>
<guid>https://arxiv.org/abs/2410.02367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SageAttention是一种新型的注意力量化方法，相比于传统方法在性能和准确性上显著提升。</p><br /><br /><p><strong>摘要：</strong> 本文分析了注意力机制的量化可行性，并提出一种名为SageAttention的高效量化方法。该方法在处理长序列时，显著降低了计算复杂度。实验表明，SageAttention在每秒操作次数（OPS）上比FlashAttention2和xformers分别快约2.1倍和2.7倍。此外，SageAttention在准确性方面也优于FlashAttention3。综合实验结果表明，该方法在大型语言处理、图像生成和视频生成等多种模型中，几乎没有造成端到端指标的损失，为加速模型推理提供了有效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:08:56 GMT</pubDate>
</item>
<item>
<title>Synthio：基于文本到音频扩散模型的小规模音频分类数据集增强方法</title>
<link>https://arxiv.org/abs/2410.02056</link>
<guid>https://arxiv.org/abs/2410.02056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Synthio通过合成音频数据提升小规模音频分类的准确性，克服了传统增强技术的局限。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Synthio的方法，通过合成音频数据增强小规模音频分类数据集，以提高分类准确性。传统的数据增强技术（如添加随机噪声或遮罩片段）难以捕捉真实音频的多样性。为了解决这一问题，Synthio利用文本到音频（T2A）扩散模型生成合成音频。然而，合成有效的增强数据面临挑战：生成的数据不仅需要与小规模数据集的声学特性保持一致，还需具备足够的组成多样性。为克服第一个挑战，Synthio通过偏好优化对T2A模型生成的数据与小规模数据集进行对齐，从而确保生成的音频在声学特征上与原数据集一致。而为了解决第二个挑战，文章提出了一种新颖的字幕生成技术，利用大型语言模型的推理能力生成多样化且富有意义的音频字幕，并迭代优化其质量。生成的字幕用于提示对齐的T2A模型。通过在十个数据集和四种模拟的有限数据场景中进行广泛评估，结果显示，Synthio在所有基线方法上均提高了0.1%-39%的性能，尽管该T2A模型仅在弱标注的AudioSet上进行训练。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:08:20 GMT</pubDate>
</item>
<item>
<title>基于多视角优化的3D高斯注入生成方法研究</title>
<link>https://arxiv.org/abs/2410.02103</link>
<guid>https://arxiv.org/abs/2410.02103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的3D高斯优化方法，改善了体积渲染的质量和效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的3D高斯优化方法，旨在解决传统3D高斯渲染在新视图合成中出现的不满意外观和不精确3D几何体等问题。该方法有四个关键创新：第一，转变为多视角训练策略，通过多视角调节优化3D高斯属性，以避免对某些训练视图的过拟合，从而提升多种场景中的整体准确性。第二，借鉴额外视图的益处， propone了一种交叉内在引导方案，推动不同分辨率的粗到细训练过程。第三，基于多视角调节训练，提出了一种交叉射线稠密化策略，在多视角选择中稠密化射线交点区域的高斯内核。第四，通过研究稠密化策略发现，当某些视图存在显著差异时，需增强稠密化效果。为解决此问题，提出了一种新异的多视角增强稠密化策略，鼓励3D高斯根据需要稠密化，从而改善重建准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 01:02:07 GMT</pubDate>
</item>
<item>
<title>LLaVA-Critic：首个开源多模态评估模型</title>
<link>https://arxiv.org/abs/2410.02712</link>
<guid>https://arxiv.org/abs/2410.02712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-Critic是一种开源多模态模型，能有效进行任务评估和偏好学习，推动未来的模型对齐研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LLaVA-Critic，这是第一个开源的大型多模态模型，旨在作为通用性评估者来评估多种多模态任务的性能。LLaVA-Critic通过一个高质量的评论指令集进行了训练，该数据集涵盖了多种评价标准和场景。实验结果表明，该模型在两个主要领域的有效性：（1）LMM作为评委，LLaVA-Critic提供可靠的评估分数，其表现与多个评估基准上的GPT模型相当或更佳；（2）偏好学习方面，该模型为偏好学习生成奖励信号，增强了模型的对齐能力。这项工作强调了开源多模态模型在自我评价和评估中的潜力，为未来关于可扩展的超人类模型对齐反馈机制的研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 00:55:11 GMT</pubDate>
</item>
<item>
<title>LLaVA-Video-178K：高质量合成视频指令遵循数据集</title>
<link>https://arxiv.org/abs/2410.02713</link>
<guid>https://arxiv.org/abs/2410.02713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出LLaVA-Video-178K数据集，助力视频指令遵循模型LLaVA-Video的训练与性能提升。</p><br /><br /><p><strong>摘要：</strong> 视频大规模多模态模型（LMMs）在发展中面临从网络获取高质量原始数据的挑战。为了解决这一问题，我们提出了一种替代方法，创建了一个专门用于视频指令遵循的高质量合成数据集——LLaVA-Video-178K。该数据集涵盖了详细的字幕生成、开放式问题回答以及多选题回答等关键任务。通过在该数据集上进行训练，并结合已有的视觉指令调优数据，我们推出了新的视频LMM——LLaVA-Video。我们的实验证明，LLaVA-Video在多个视频基准测试中表现优异，显示出该数据集的有效性。我们计划发布该数据集、其生成管道和模型检查点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.02713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Oct 2024 00:45:02 GMT</pubDate>
</item>
<item>
<title>基于多教师知识蒸馏的视觉模型优化研究</title>
<link>https://arxiv.org/abs/2410.01680</link>
<guid>https://arxiv.org/abs/2410.01680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了教师激活统计对学生模型质量的影响并提出了PHI标准化技术。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多教师知识蒸馏在视觉基础模型中的应用，重点分析了教师的激活统计特性对学生模型质量的影响。通过研究损失函数和统计归一化技术，旨在更好地对齐不同的分布并评估其效果。此外，还考察了在下游教师匹配指标上的影响，强调了Hadamard矩阵的使用。通过使用Hadamard矩阵，本文展示了其在各维度标准化中的有用属性，提出了一种新技术“PHI标准化”(PHI-S)，其能够实现各维度的等距标准化。实验证明，PHI-S方法在所研究的方法中产生了最佳的学生模型效果，展示了其在非标签的异质多教师知识蒸馏中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 16:44:28 GMT</pubDate>
</item>
<item>
<title>EmoKnob: 细粒度情感控制的语音合成框架</title>
<link>https://arxiv.org/abs/2410.00316</link>
<guid>https://arxiv.org/abs/2410.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmoKnob框架实现了情感语音的细粒度控制，超越了商业TTS服务的表现。</p><br /><br /><p><strong>摘要：</strong> EmoKnob是一个新提出的框架，旨在实现语音合成中的细粒度情感控制。尽管现有的文本到语音技术已经能够生成自然和富有表现力的语音，但用户无法选择情感和控制情感强度。该框架利用最新的基础语音克隆模型所带来的表现力发言者表征空间，并通过少量示范样本来实现任意情感的控制。此外，EmoKnob还提出了两种方法，使用户能够根据开放式文本描述的情感进行控制，提供直观的界面以应对多样化的情感需求。为了推动情感语音合成领域的系统化发展，EmoKnob引入了一套评估指标，旨在严格评估情感控制框架的真实度和可识别性。通过客观和主观评估，研究表明，该情感控制框架能够有效地在语音中嵌入情感，并超越了商业TTS服务的情感表现力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 15:52:40 GMT</pubDate>
</item>
<item>
<title>跨语言检索增强生成模型在地缘政治争议中的表现研究</title>
<link>https://arxiv.org/abs/2410.01171</link>
<guid>https://arxiv.org/abs/2410.01171</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了跨语言检索增强生成模型在地缘政治争议中的局限性及其数据集构建。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了跨语言检索增强生成（RAG）模型在处理地缘政治争议时所面临的挑战。尽管大型语言模型在创意生成方面表现出色，但在幻觉和偏见问题上依然存在不足。RAG模型旨在通过依赖准确和最新的信息来源来增强响应的真实性，但如何选择和加权这些信息来源仍是一个重要问题。我们创建了一个数据集，以研究现有系统在回答有关跨语言地缘政治争议的查询时的鲁棒性。该数据集来自于维基百科相关页面，研究了额外上下文的包含及其语言和来源的组成对模型响应的影响。结果表明，现有的RAG系统在跨语言使用案例中依然面临挑战，当提供多种语言的竞争性信息时，响应的连贯性和准确性往往受到影响。文中还展示了一些案例研究，并针对未来研究提出了改进建议。我们将数据集和代码公开提供，网址为https://github.com/manestay/bordIRlines。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01171" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 14:07:00 GMT</pubDate>
</item>
<item>
<title>SonicSim：针对移动声源的合成数据生成工具及其应用</title>
<link>https://arxiv.org/abs/2410.01481</link>
<guid>https://arxiv.org/abs/2410.01481</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SonicSim是一种生成移动声源合成数据的工具，旨在提升语音分离与增强模型的效果。</p><br /><br /><p><strong>摘要：</strong> 在移动声源条件下，语音分离与增强模型的系统评估通常需要丰富多样的数据。然而，真实世界的数据集往往无法满足模型的训练与评估需求，合成数据集虽然数量庞大，但其声学模拟却缺乏真实感。为了解决这些问题，我们提出了SonicSim，一个旨在生成高度可定制化的移动声源合成数据的工具。SonicSim建立在Habitat-sim的基础上，支持场景级、麦克风级和源级的多层次调整，从而生成更为多样的合成数据。基于SonicSim，我们构建了一个移动声源基准数据集SonicSet，结合Librispeech、FSD50K和FMA等数据集，以及来自Matterport3D的90个场景，用于评估语音分离和增强模型。此外，我们随机选取了SonicSet验证集中的5小时无混响原始数据，录制了一个真实世界的语音分离数据集，并与对应的合成数据集进行了比较。同时，我们还利用真实世界的语音增强数据集RealMAN来验证SonicSet与其他合成数据集之间的声学差距。结果表明，SonicSim生成的合成数据能够有效地推广到真实世界场景中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01481" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 13:54:50 GMT</pubDate>
</item>
<item>
<title>Old Optimizer, New Norm: An Anthology</title>
<link>https://arxiv.org/abs/2409.20325</link>
<guid>https://arxiv.org/abs/2409.20325</guid>
<content:encoded><![CDATA[
Deep learning optimizers are often motivated through a mix of convex and approximate second-order theory. We select three such methods -- Adam, Shampoo and Prodigy -- and argue that each method can instead be understood as a squarely first-order method without convexity assumptions. In fact, after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm. By generalizing this observation, we chart a new design space for training algorithms. Different operator norms should be assigned to different tensors based on the role that the tensor plays within the network. For example, while linear and embedding layers may have the same weight space of R^{mtimes n}, these layers play different roles and should be assigned different norms. We hope that this idea of carefully metrizing the neural architecture might lead to more stable, scalable and indeed faster training.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 13:04:12 GMT</pubDate>
</item>
<item>
<title>FactAlign：提升大型语言模型长篇响应的事实准确性</title>
<link>https://arxiv.org/abs/2410.01691</link>
<guid>https://arxiv.org/abs/2410.01691</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FactAlign是一个新框架，用于增强大型语言模型长篇回答的事实准确性，融合细粒度评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FactAlign，一个旨在提升大型语言模型（LLMs）长篇响应事实准确性的对齐框架，同时保持其有用性。我们引入了fKTO，一种细粒度句子级对齐算法，扩展了Kahneman-Tversky优化（KTO）对齐方法。FactAlign利用近期在自动事实性评估方面的进展，通过细粒度事实性评估指导对齐过程。实验结果表明，在开放领域提示和信息寻求问题上，FactAlign显著提高了LLM响应的事实准确性和有用性。进一步分析表明，FactAlign能够训练LLM提供更多信息而不损失事实精确性，从而提高事实F1得分。我们的源代码、数据集和训练模型已公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01691" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 12:50:05 GMT</pubDate>
</item>
<item>
<title>InfiniPot：高效管理长输入上下文的框架</title>
<link>https://arxiv.org/abs/2410.01518</link>
<guid>https://arxiv.org/abs/2410.01518</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniPot是一个新颖的KV缓存控制框架，旨在解决LLM在内存受限环境中的长输入上下文处理问题。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了InfiniPot，一个新颖的KV缓存控制框架，旨在使预训练的大型语言模型（LLMs）在固定内存限制下有效管理长序列。InfiniPot利用持续上下文蒸馏（CCD）技术，这是一种通过新颖的重要性指标压缩和保留关键信息的迭代过程，即使在没有未来上下文的情况下，依然能够保持关键信息。综合评估结果表明，InfiniPot在多种自然语言处理（NLP）任务中明显优于为长上下文训练的模型，证明了其有效性和多样性。这项工作标志着向开发适用于更广泛现实场景的大型语言模型迈出了重要一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01518" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 12:25:16 GMT</pubDate>
</item>
<item>
<title>构建开放源代码的欧盟语言语音基础模型</title>
<link>https://arxiv.org/abs/2410.01036</link>
<guid>https://arxiv.org/abs/2410.01036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们收集了950k小时的语音数据，并发布了441k小时的自动转录，为欧盟语言的开放源语音模型奠定基础。</p><br /><br /><p><strong>摘要：</strong> 随着基础模型的兴起以及针对其风险与影响的监管努力，开源模型引起了广泛关注。然而，目前的语音基础模型并未完全符合开源原则，尽管有相关声明，因为现有模型并不公开可用的权重、代码和训练数据。本文的工作旨在填补这一空白，专注于欧盟的24种官方语言。我们通过调查自动语音识别数据集和符合开源许可的未标记语音语料，收集了950k小时的训练数据。此外，我们还在宽松的CC-BY许可下发布了441k小时未标记数据的自动转录，从而为欧盟语言的开源语音基础模型的创建提供了支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 11:00:01 GMT</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 10:14:25 GMT</pubDate>
</item>
<item>
<title>E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding</title>
<link>https://arxiv.org/abs/2409.18111</link>
<guid>https://arxiv.org/abs/2409.18111</guid>
<content:encoded><![CDATA[
Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level &amp; Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 08:16:21 GMT</pubDate>
</item>
<item>
<title>在联邦学习中结合LoRA的联邦共享低秩适应方法</title>
<link>https://arxiv.org/abs/2410.01463</link>
<guid>https://arxiv.org/abs/2410.01463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FedSA-LoRA，结合LoRA的低秩矩阵在联邦学习中实现知识共享与客户端专用知识的有效整合。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在联邦学习中应用低秩适应（LoRA）的方式，分析了学习到的A和B矩阵的非对称性，发现A矩阵负责学习一般知识，而B矩阵则专注于捕捉客户端的特定知识。基于此发现，本文引入了联邦共享低秩适应（FedSA-LoRA）方法，该方法使用两个低秩可训练矩阵A和B对权重更新进行建模，但仅将A矩阵与服务器共享以进行聚合。同时，本文还探讨了其他LoRA变体（如rsLoRA和VeRA）中学习到的A和B矩阵之间的关系，发现这一模式是一致的。因此，我们将FedSA-LoRA方法扩展到这些LoRA变体中，形成FedSA-rsLoRA和FedSA-VeRA。通过这种方式，我们建立了一种将LoRA与联邦学习相结合的通用范式，为未来的LoRA变体与FL的结合研究提供了指导。实验证明了该方法在自然语言理解和生成任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 07:20:19 GMT</pubDate>
</item>
<item>
<title>大型语言模型在小学数学问题求解中的推理能力研究</title>
<link>https://arxiv.org/abs/2410.01748</link>
<guid>https://arxiv.org/abs/2410.01748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大型语言模型在解答组合数学问题时存在显著推理差距，尤其是较小和专门化的模型表现更为明显。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在解决小学数学问题（GSM）中的推理能力。我们通过评估多对现有数学文字问题，让第二个问题的答案依赖于第一个问题的解答。研究发现，大多数LLMs在解决组合问题和独立问题时存在显著的推理差距。这种差距在较小、成本更低且专注于数学的模型中更加明显。此外，指令调优和代码生成对不同规模的LLMs影响各异，而在GSM上的微调会导致任务过拟合。我们的分析表明，较大的推理差距并非由于测试集泄漏，而是由于额外上下文的干扰和第二步推理的不足。总体而言，尽管LLMs在标准基准测试中的表现看似良好，它们在推理能力上存在系统性的差异。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 07:04:21 GMT</pubDate>
</item>
<item>
<title>基于对比偏好的翻译质量优化研究</title>
<link>https://arxiv.org/abs/2409.20059</link>
<guid>https://arxiv.org/abs/2409.20059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了对比偏好优化对翻译质量的影响，发现其在高质量数据上优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于利用对比偏好优化（CPO）技术提高机器翻译（MT）质量。CPO与监督微调（SFT）进行了广泛实验比较，结果表明CPO在高质量数据上的表现优于SFT，并在对齐度量方面持续取得良好效果。然而，CPO在下游评价指标中的稳定性存在问题，尤其是在神经指标和词汇指标间。研究还表明，依赖基础模型生成候选翻译的效果可与多种外部系统相媲美，同时在下游指标上保持更好的稳定性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 06:30:16 GMT</pubDate>
</item>
<item>
<title>3DGS-DET: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection</title>
<link>https://arxiv.org/abs/2410.01647</link>
<guid>https://arxiv.org/abs/2410.01647</guid>
<content:encoded><![CDATA[
Neural Radiance Fields (NeRF) are widely used for novel-view synthesis and have been adapted for 3D Object Detection (3DOD), offering a promising approach to 3DOD through view-synthesis representation. However, NeRF faces inherent limitations: (i) limited representational capacity for 3DOD due to its implicit nature, and (ii) slow rendering speeds. Recently, 3D Gaussian Splatting (3DGS) has emerged as an explicit 3D representation that addresses these limitations. Inspired by these advantages, this paper introduces 3DGS into 3DOD for the first time, identifying two main challenges: (i) Ambiguous spatial distribution of Gaussian blobs: 3DGS primarily relies on 2D pixel-level supervision, resulting in unclear 3D spatial distribution of Gaussian blobs and poor differentiation between objects and background, which hinders 3DOD; (ii) Excessive background blobs: 2D images often include numerous background pixels, leading to densely reconstructed 3DGS with many noisy Gaussian blobs representing the background, negatively affecting detection. To tackle the challenge (i), we leverage the fact that 3DGS reconstruction is derived from 2D images, and propose an elegant and efficient solution by incorporating 2D Boundary Guidance to significantly enhance the spatial distribution of Gaussian blobs, resulting in clearer differentiation between objects and their background. To address the challenge (ii), we propose a Box-Focused Sampling strategy using 2D boxes to generate object probability distribution in 3D spaces, allowing effective probabilistic sampling in 3D to retain more object blobs and reduce noisy background blobs. Benefiting from our designs, our 3DGS-DET significantly outperforms the SOTA NeRF-based method, NeRF-Det, achieving improvements of +6.6 on mAP@0.25 and +8.1 on mAP@0.5 for the ScanNet dataset, and impressive +31.5 on mAP@0.25 for the ARKITScenes dataset.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 05:50:23 GMT</pubDate>
</item>
<item>
<title>Scylla：量化评估大型语言模型的泛化能力框架</title>
<link>https://arxiv.org/abs/2410.01769</link>
<guid>https://arxiv.org/abs/2410.01769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Scylla框架，量化评估LLMs的泛化与记忆能力，研究任务复杂性对模型表现的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Scylla，一个动态评估框架，用于量化大型语言模型（LLMs）的泛化能力。Scylla通过评估模型在同分布（ID）和异分布（OOD）数据上的表现，分析了20个任务在5个复杂度水平下的表现，从而将泛化与记忆分离。研究发现任务复杂性与ID、OOD数据表现之间存在非单调关系，形成了所谓的“泛化谷”。具体而言，揭示了一个关键阈值——被称为关键复杂性——在该点上，模型对不可泛化行为的依赖达到峰值，标志着LLMs泛化能力的上限。随着模型规模的增加，关键复杂性向更高的任务复杂度移动，表明更大规模的模型在复杂推理任务中能够处理更多复杂性，而不依赖于记忆。利用Scylla和关键复杂性概念，本文对28个LLMs进行了基准测试，包括LLaMA和Qwen等开源模型，以及Claude和GPT等闭源模型，从而提供了更为稳健的评估，帮助更清晰地理解LLMs的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 03:41:52 GMT</pubDate>
</item>
<item>
<title>自我精炼规划方法：一种高效的长远行动序列生成方案</title>
<link>https://arxiv.org/abs/2410.01440</link>
<guid>https://arxiv.org/abs/2410.01440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种自我精炼的规划策略，通过循环优化生成行动序列，提升机器人任务规划能力。</p><br /><br /><p><strong>摘要：</strong> 在自主机器人行动实施过程中，任务规划是一个主要挑战，需要将高级任务描述转化为长远的行动序列。尽管最近的语言模型代理在此领域取得了一些进展，但仍然存在规划错误和前瞻性有限的问题。为了解决这些局限性，本文提出了一种自我精炼的方案，通过迭代完善草案计划直到达到平衡。这一过程从分析的角度进行端到端优化，无需额外的验证器或奖励模型，简化为监督学习方式训练自我精炼规划器。此外，设计了一种嵌套平衡序列建模程序，实现高效的闭环规划，并从环境（或内部世界模型）中获取有用反馈。我们在VirtualHome-Env基准上评估了该方法，结果显示出先进的表现和更好的推理计算扩展能力。代码已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 03:05:45 GMT</pubDate>
</item>
<item>
<title>LEOPARD : A Vision Language Model For Text-Rich Multi-Image Tasks</title>
<link>https://arxiv.org/abs/2410.01744</link>
<guid>https://arxiv.org/abs/2410.01744</guid>
<content:encoded><![CDATA[
Text-rich images, where text serves as the central visual element guiding the overall understanding, are prevalent in real-world applications, such as presentation slides, scanned documents, and webpage snapshots. Tasks involving multiple text-rich images are especially challenging, as they require not only understanding the content of individual images but reasoning about inter-relationships and logical flows across multiple visual inputs. Despite the importance of these scenarios, current multimodal large language models (MLLMs) struggle to handle such tasks due to two key challenges: (1) the scarcity of high-quality instruction tuning datasets for text-rich multi-image scenarios, and (2) the difficulty in balancing image resolution with visual feature sequence length. To address these challenges, we propose \OurMethod, a MLLM designed specifically for handling vision-language tasks involving multiple text-rich images. First, we curated about one million high-quality multimodal instruction-tuning data, tailored to text-rich, multi-image scenarios. Second, we developed an adaptive high-resolution multi-image encoding module to dynamically optimize the allocation of visual sequence length based on the original aspect ratios and resolutions of the input images. Experiments across a wide range of benchmarks demonstrate our model's superior capabilities in text-rich, multi-image evaluations and competitive performance in general domain evaluations.
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 02:55:21 GMT</pubDate>
</item>
<item>
<title>多粒度调试器（MGDebugger）：一种层次化代码调试新方法</title>
<link>https://arxiv.org/abs/2410.01215</link>
<guid>https://arxiv.org/abs/2410.01215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MGDebugger通过多级别分析及修复代码错误，显著提升生成代码的调试准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为多粒度调试器（MGDebugger）的新型层次化代码调试系统，旨在解决现有LLM（大型语言模型）生成代码中存在的细微错误问题。MGDebugger通过将有问题的代码分解为层次化的子函数树结构，针对不同粒度的错误进行隔离、识别和修复。该系统采用自下而上的方式逐级分析和调试每个子函数。此外，MGDebugger提出了一种模拟Python执行器以跟踪代码执行和重要变量状态，从而准确定位错误。实验结果显示，MGDebugger在HumanEval测试中，比现有调试系统的准确率提高了18.9%，在HumanEvalFix中的修复成功率高达97.6%。MGDebugger能够有效修复不同类别和难度级别的错误，展现出其强大的鲁棒性和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 02:20:13 GMT</pubDate>
</item>
<item>
<title>精确体积椭球渲染方法的研究</title>
<link>https://arxiv.org/abs/2410.01804</link>
<guid>https://arxiv.org/abs/2410.01804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出了一种实时可微分的精确体积椭球渲染方法，具有更高的渲染准确性和更低的混合问题。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为Exact Volumetric Ellipsoid Rendering (EVER)的实时可微分发射仅体积渲染方法。与最近的3D高斯点云化（3DGS）基于光栅化的方法不同，我们的基于基元的表示允许进行精确的体积渲染，而不是3D高斯广告牌的阿尔法合成。因此，Our方法不受跳跃伪影和视依赖密度的影响，同时在NVIDIA RTX4090上以720p实现了30 FPS的帧率。由于我们的方案建立在光线追踪的基础上，因此能够实现景深模糊和相机畸变（例如鱼眼相机）的效果，这些效果在光栅化中难以实现。我们的实验表明，与3DGS及其后续的视一致性渲染工作相比，EVER在大规模Zip-NeRF数据集中的表现更为准确，混合问题更少，在实时技术中Achieves最佳的清晰结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:51:45 GMT</pubDate>
</item>
<item>
<title>基于提示自适应工作流生成的文本到图像生成方法</title>
<link>https://arxiv.org/abs/2410.01731</link>
<guid>https://arxiv.org/abs/2410.01731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种自动调整工作流的方法，以提高文本到图像生成的质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的任务——提示自适应工作流生成，旨在根据用户提示自动调整生成流程。随着文本到图像生成技术的发展，工作流的使用从简单模型转向结合多个专用组件的复杂流程。尽管这种工作流方法可提升图像质量，但有效构建工作流需要专业知识，因为涉及组件众多且相互依赖性复杂，并且需依赖生成的提示。为解决这一问题，本文提出两种基于大语言模型（LLM）的方法：一种是基于调优的方法，利用用户偏好数据进行学习；另一种是无训练方法，借助LLM选择现有的工作流。与单一模型或通用、与提示无关的工作流相比，两种方法均能显著提升生成图像的质量。研究表明，基于提示的工作流预测为改善文本到图像生成质量提供了新途径，补充了该领域现有的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:50:07 GMT</pubDate>
</item>
<item>
<title>奖励模型的比较与新方法的提出</title>
<link>https://arxiv.org/abs/2410.01257</link>
<guid>https://arxiv.org/abs/2410.01257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文发布新数据集，比较Bradley-Terry与回归模型的效果，并提出结合方法，训练出高性能奖励模型。</p><br /><br /><p><strong>摘要：</strong> 奖励模型在对齐模型跟随指令时至关重要，通常采用两种流行的训练范式：Bradley-Terry风格或回归风格。然而，由于这两种方法所需的数据格式不兼容，缺乏足够对比的证据。为解决此问题，本文发布了为HelpSteer2数据集设计的偏好注释，以补充现有的评分数据，并提供人类撰写的理由以提高数据可解释性。基于这些数据，进行了一次首次Bradley-Terry与回归模型的公平比较，并在此基础上提出了一种结合这两种模型的新方法。经过调优的Llama-3.1-70B-Instruct模型在RewardBench上得分达到94.1，成为截至2024年10月1日140多个奖励模型中的顶尖模型。同时，本研究展示了该奖励模型在强化学习中有效对齐模型执行指令的能力。研究团队以CC-BY-4.0许可开源了此数据集，地址为https://huggingface.co/datasets/nvidia/HelpSteer2，并开放发布了训练好的奖励模型，地址为https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:39:11 GMT</pubDate>
</item>
<item>
<title>RATIONALYST：基于理性注解的推理过程监督模型</title>
<link>https://arxiv.org/abs/2410.01044</link>
<guid>https://arxiv.org/abs/2410.01044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RATIONALYST通过海量理性注解进行推理模型的预训练，显著提升多种推理任务的准确性。</p><br /><br /><p><strong>摘要：</strong> 为了解决大型语言模型（LLMs）推理步骤不完整的问题，提出了RATIONALYST模型，通过对大量无标签数据提取的79k理性注解进行预训练，以实现推理过程的监督。该模型利用来自Web规模未标记数据集（如Pile）和多种推理数据集的理性注解，最小化了人工干预。RATIONALYST经过LLaMa-3-8B的微调，在7个代表性推理基准测试中平均提高了推理准确性3.9%。此外，RATIONALYST在推理性能上优于更大型的验证模型（如GPT-4）和相似规模的模型，证明了其在数学、常识、科学和逻辑推理等多样化推理任务中的优秀泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Oct 2024 01:26:20 GMT</pubDate>
</item>
<item>
<title>Embodied-RAG：一种提升机器人导航与语言生成能力的非参数记忆框架</title>
<link>https://arxiv.org/abs/2409.18313</link>
<guid>https://arxiv.org/abs/2409.18313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Embodied-RAG框架，提升机器人在多模态环境中的导航和语言生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Embodied-RAG框架，它旨在通过非参数记忆系统增强具身代理的基础模型。Embodied-RAG能够自主构建分层知识，以支持导航和语言生成任务。该框架处理多样化环境中的空间和语义解析需求，支持特定物体的查询及整体氛围的描述。其核心记忆被组织为语义森林，保存不同细节层次的语言描述。这种分层结构使系统有效地生成上下文敏感的输出，并适应不同的机器人平台。实验结果表明，Embodied-RAG能够成功应对19个环境中的200多个解释和导航查询，展示了其在具身代理领域的一般性非参数系统的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 15:05:01 GMT</pubDate>
</item>
<item>
<title>机器翻译中的性别偏见对服务质量的影响研究</title>
<link>https://arxiv.org/abs/2410.00545</link>
<guid>https://arxiv.org/abs/2410.00545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究揭示了机器翻译中的性别偏见如何导致翻译服务质量差异，特别是女性用户的额外努力和成本。</p><br /><br /><p><strong>摘要：</strong> 性别偏见在机器翻译（MT）中被广泛认可，并可能对社会和个人产生负面影响。然而，当前的技术进步往往没有考虑最终用户的意见或其可能受到的影响。现有评估主要依赖自动化方法，无法有效评估性别差异对使用者的实际后果。本研究通过一项大规模以人为中心的研究，探讨了机器翻译中的性别偏见对服务质量的影响，尤其是在女性和男性之间的服务差距。研究招募了90名参与者对机器翻译输出进行后编辑，以确保性别翻译的准确性。结果表明，女性的后编辑工作相较于男性需要更多的技术和时间投入，且相应地产生更高的财务成本。然而，现有的偏见测量方法未能反映出这种差异。因此，该研究结果呼吁采取以人为中心的方法，以便更好地理解偏见对社会的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 12:00:24 GMT</pubDate>
</item>
<item>
<title>针对长视频理解的LMM适应策略</title>
<link>https://arxiv.org/abs/2409.20018</link>
<guid>https://arxiv.org/abs/2409.20018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对长视频理解，提出了一种通过扩展视觉上下文窗口的方法，显著提高了长视频处理性能。</p><br /><br /><p><strong>摘要：</strong> 本论文针对大型多模态模型在长视频理解中的挑战，提出了一种通过扩展视觉上下文窗口来适应长视频任务的方法。研究发现，预训练的LMMs在处理长视频内容时，由于视觉和语言模态之间的差异，导致视觉和语言token的上下文窗口不一致，造成理解困难。因此，论文提出通过扩展视觉上下文窗口的方式来解决这一问题，避免在大规模的长视频数据集上重新训练。此外，论文还引入了渐进池化推理策略，通过有选择性地调整帧嵌入的空间分辨率，减少视觉token的数量，同时保持重要的空间信息。这种方法在多个长视频理解基准测试中表现出色，在MLVU基准中，即使模型大小只有7B，也超越了GPT-4o。在256帧设置下，所提方法将内存使用率降低约45%，且无性能损失。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 07:54:24 GMT</pubDate>
</item>
<item>
<title>SyntheOcc：一种用于生成可控3D占用数据集的扩散模型</title>
<link>https://arxiv.org/abs/2410.00337</link>
<guid>https://arxiv.org/abs/2410.00337</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SyntheOcc 提出了一种生成多样化控制的3D占用数据集的扩散模型，以支持自动驾驶领域的感知模型训练。</p><br /><br /><p><strong>摘要：</strong> SyntheOcc是一种扩散模型，旨在通过条件占用标签生成高度真实感和几何控制的图像，特别关注3D占用预测任务。该模型在自动驾驶场景中能够生成丰富多样且可控的数据集，从而解决了3D几何信息在2D扩散模型中有效编码的难题。SyntheOcc创新地结合了3D语义多平面图像（MPI）来提供全面且空间对齐的3D场景描述，从而用于条件生成。通过对nuScenes数据集的详细定性和定量评估，SyntheOcc证明了其在生成可控占用数据集方面的有效性，为感知模型的训练提供了有效的数据增强手段。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00337" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 07:41:04 GMT</pubDate>
</item>
<item>
<title>一种基于后验均值的图像恢复算法</title>
<link>https://arxiv.org/abs/2410.00418</link>
<guid>https://arxiv.org/abs/2410.00418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PMRF算法通过最优运输后验均值实现高质量图像恢复。它在多种任务中超越了前沿技术。</p><br /><br /><p><strong>摘要：</strong> 本文关注在完美感知指标约束下，最小化均方误差（MSE）的最优估计器。提出的Posterior-Mean Rectified Flow (PMRF)算法通过预先预测后验均值，然后利用直流模型将该结果转移到高质量图像上，近似于理想的最优运输映射。PMRF基于最近的理论结果，能够有效地将后验均值预测搬运至与真实图像相同的分布。通过理论分析和实证研究，PMRF在多种图像恢复任务中表现优于现有方法。该算法的引入为图像恢复领域带来了新的视角，旨在在保持感知质量的同时最小化重建失真。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 06:14:13 GMT</pubDate>
</item>
<item>
<title>VideoLISA：语言指导下的视频多模态大语言模型</title>
<link>https://arxiv.org/abs/2409.19603</link>
<guid>https://arxiv.org/abs/2409.19603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoLISA是一种基于视频的多模态大语言模型，通过语言指令实现视频中对象的时间一致分割。</p><br /><br /><p><strong>摘要：</strong> VideoLISA是一种新颖的视频多模态大语言模型，旨在解决视频中语言指导下的推理分割问题。该模型结合了大型语言模型的推理能力和世界知识，以及Segment Anything Model的增强，能够根据语言指令生成时间一致的视频分割掩码。现有的基于图像的方法（如LISA）在处理视频任务时遇到困难，因为视频增加了时间维度，要求更强的时间动态理解能力和帧间一致性分割。为解决这些挑战，VideoLISA引入了一种稀疏密集采样策略，以平衡计算约束下的时间上下文和空间细节。此外，模型还提出了一种One-Token-Seg-All方法，利用特殊设计的标记在多个帧中实现对象的分割和跟踪。在我们新推出的ReasonVOS基准及其他多样化基准上的广泛评估显示，VideoLISA在处理复杂推理、时间理解和对象跟踪的视频对象分割任务中表现优越。虽然重点在视频分割，VideoLISA在图像分割上也显示出良好的泛化能力，揭示了其作为语言指导对象分割的统一基础模型的潜力。代码和模型将在https://github.com/showlab/VideoLISA上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 06:12:40 GMT</pubDate>
</item>
<item>
<title>Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect</title>
<link>https://arxiv.org/abs/2409.17912</link>
<guid>https://arxiv.org/abs/2409.17912</guid>
<content:encoded><![CDATA[
We introduce Atlas-Chat, the first-ever collection of large language models specifically developed for dialectal Arabic. Focusing on Moroccan Arabic, also known as Darija, we construct our instruction dataset by consolidating existing Darija language resources, creating novel datasets both manually and synthetically, and translating English instructions with stringent quality control. Atlas-Chat-9B and 2B models, fine-tuned on the dataset, exhibit superior ability in following Darija instructions and performing standard NLP tasks. Notably, our models outperform both state-of-the-art and Arabic-specialized LLMs like LLaMa, Jais, and AceGPT, e.g., achieving a 13% performance boost over a larger 13B model on DarijaMMLU, in our newly introduced evaluation suite for Darija covering both discriminative and generative tasks. Furthermore, we perform an experimental analysis of various fine-tuning strategies and base model choices to determine optimal configurations. All our resources are publicly accessible, and we believe our work offers comprehensive design methodologies of instruction-tuning for low-resource language variants, which are often neglected in favor of data-rich languages by contemporary LLMs.
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 05:02:45 GMT</pubDate>
</item>
<item>
<title>Illustrious：一款先进的动漫图像生成模型</title>
<link>https://arxiv.org/abs/2409.19946</link>
<guid>https://arxiv.org/abs/2409.19946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Illustrious模型在生成高质量动漫图像方面的三大关键改进策略。</p><br /><br /><p><strong>摘要：</strong> 本文分享了在我们文本到图像动漫图像生成模型Illustrious中实现尖端质量的见解。为了实现高分辨率、动态色彩范围图像和强大的恢复能力，我们专注于三种关键改进方法。首先，我们深入探讨了批量大小和 dropout 控制的重要性，这能够加速可控标记概念激活的学习。其次，我们提高了图像的训练分辨率，从而在更高的分辨率下准确描绘角色解剖结构，实现超过20MP的生成能力。最后，我们提出了经过优化的多层次标签覆盖方案，结合各种自然语言标题，作为模型发展的重要因素。通过广泛的分析和实验，Illustrious在动画风格方面展现了尖端表现，在插画领域超越了广泛使用的模型，推动了更容易的定制和个性化，并具备开源特性。我们计划公开发布更新的Illustrious模型系列，并制定持续改进的方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 04:30:24 GMT</pubDate>
</item>
<item>
<title>Law of the Weakest Link: Cross Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2409.19951</link>
<guid>https://arxiv.org/abs/2409.19951</guid>
<content:encoded><![CDATA[
The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the "Law of the Weakest Link," where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios.
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 02:10:28 GMT</pubDate>
</item>
<item>
<title>Flex3D：一种灵活的3D内容生成框架</title>
<link>https://arxiv.org/abs/2410.00890</link>
<guid>https://arxiv.org/abs/2410.00890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flex3D提出了一种新颖的两阶段框架，有效利用任意数量的高质量输入视图生成3D内容。</p><br /><br /><p><strong>摘要：</strong> Flex3D是一种新型的两阶段框架，旨在克服现有3D生成方法中固定视图数量的局限性。第一阶段采用候选视图生成和选择流程，通过微调的多视图图像扩散模型和视频扩散模型，生成丰富的目标3D对象表示。随后，视图选择流程对这些视图进行质量和一致性筛选，确保只使用高质量、可靠的视图进行重建。第二阶段将经过筛选的视图输入到灵活的重建模型（FlexRM），该模型基于变换器架构，能够有效处理任意数量的输入，并利用三平面表示直接输出3D高斯点，实现高效且详细的3D生成。通过对设计和训练策略的深入探索，我们优化了FlexRM，以在重建和生成任务中获得卓越性能。结果表明，Flex3D在3D生成任务中表现出色，与多种最新的前馈3D生成模型相比，其用户研究胜率超过92%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:32:05 GMT</pubDate>
</item>
<item>
<title>面向边缘设备的TPI-LLM：高效的张量并行推理系统</title>
<link>https://arxiv.org/abs/2410.00531</link>
<guid>https://arxiv.org/abs/2410.00531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPI-LLM是一种高效的张量并行推理系统，解决了边缘设备上的大模型推理问题。</p><br /><br /><p><strong>摘要：</strong> 随着用户隐私数据保护的关注，大型模型推理正逐渐从云端转向边缘设备。然而，边缘设备经常面临计算能力、内存和带宽的限制，需要多设备协作以加快大规模语言模型的推理。本文指出，张量并行在资源有限的设备上相较于流水线并行更具有效性，提出了一种名为TPI-LLM的计算和内存高效的张量并行推理系统，支持70B规模的模型。TPI-LLM允许用户的敏感数据留在本地，并通过滑动窗口内存调度器动态管理推理过程中的层权重，重叠处理磁盘I/O延迟与计算和通信，这样可以使内存受限的设备顺利运行更大的模型。本研究分析了通信瓶颈，发现链接延迟而非带宽是主要问题，因此实施了基于星型结构的全归约算法。通过在仿真和真实测试平台上的大量实验，TPI-LLM显示出比Accelerate缩短80%以上的首个标记生成时间和标记延迟，而与Transformers和Galaxy相比缩短了90%以上，同时将Llama 2-70B的峰值内存占用减少了90%，使其仅需3.1 GB的内存即可处理70B规模模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:25:30 GMT</pubDate>
</item>
<item>
<title>室内环境中四足机器人移动操控系统的研究</title>
<link>https://arxiv.org/abs/2410.00231</link>
<guid>https://arxiv.org/abs/2410.00231</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种四足移动操控系统，能在未见环境中执行任务并进行对象操控。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的四足移动操控系统，旨在解决四足机器人在室内环境中学习和执行任务时遇到的挑战。该系统配备了前置抓手用于物体操控，采用在模拟环境中训练的低级控制器，利用自我中心的深度信息完成攀爬和全身倾斜等灵活技能。同时，系统结合了预训练的视觉语言模型（VLM），使用第三人称鱼眼和自我中心的RGB相机进行语义理解和指令生成。我们在两个未见环境中评估了该系统，无需任何真实世界数据收集或训练，系统能以零-shot方式对这些环境进行泛化，并成功完成任务，如根据用户的指令抓取随机放置的玩具，成功率达60%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00231" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:20:04 GMT</pubDate>
</item>
<item>
<title>基于单目视频的人体模型重建方法</title>
<link>https://arxiv.org/abs/2409.20563</link>
<guid>https://arxiv.org/abs/2409.20563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种从单目视频重建时间一致的人体模型的新方法，适用于松散衣物和物体交互。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，通过单目视频重建时间一致的人体模型，尤其关注极其松散的衣物和手持物体的交互。以往的人体重建研究主要集中在紧身衣物，没有考虑物体交互，或者需要标定的多视角捕捉和个性化模板扫描，这些方法在规模上难以推广。我们的关键创新在于将大规模训练数据学习的通用人体形状先验与特定于视频的“袋装骨架”变形相结合，利用测试时优化方法在单个视频中实现。我们通过学习一个神经隐式模型，将人体和衣物的变形分开处理。为捕捉衣物的细微几何变化，在优化过程中利用了姿势、表面法线和光流等基于图像的先验信息。最终，生成的神经场可提取为时间一致的网格，或进一步优化为高保真的三维高斯体，便于交互式渲染。在面对具有挑战性的衣物变形和物体交互的数据集时，DressRecon方法提供了比现有技术更高保真的三维重建效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:13:51 GMT</pubDate>
</item>
<item>
<title>ACE：全能创作者与编辑的统一模型</title>
<link>https://arxiv.org/abs/2410.00086</link>
<guid>https://arxiv.org/abs/2410.00086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ACE模型通过统一的条件格式和多任务联合训练，显著提升了视觉生成能力。</p><br /><br /><p><strong>摘要：</strong> 传统的扩散模型多用于文本引导的视觉生成，难以支持多模态条件，限制了其在视觉编辑中的应用。为了解决这一问题，本文提出了ACE（All-round Creator and Editor）模型，旨在在多种视觉生成任务中实现与专家模型相媲美的性能。通过引入统一的条件格式——长上下文条件单元（LCU），并设计了一种新的基于Transformer的扩散模型，ACE实现了多个生成和编辑任务的联合训练。此外，本文还提出了一种高效的数据收集方法，以弥补缺乏可用训练数据的问题，通过合成或聚类管道获得成对图像，并利用经过微调的多模态大语言模型提供准确的文本指令。为了全面评估模型性能，构建了一个手动注释的视觉生成任务基准数据集。实验结果表明，ACE在视觉生成领域的优越性，使其能够通过单一模型轻松构建多模态聊天系统，以应对各种图像创作的互动请求，从而避免复杂的视觉代理流水线。代码和模型将会在项目页面公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.00086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Oct 2024 01:12:17 GMT</pubDate>
</item>
<item>
<title>视觉问答中的多模态问题分解研究</title>
<link>https://arxiv.org/abs/2409.19339</link>
<guid>https://arxiv.org/abs/2409.19339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了多模态大语言模型在视觉问题分解中的表现，提出DecoVQA+数据集及优化训练流程。</p><br /><br /><p><strong>摘要：</strong> 本论文探讨了多模态大语言模型(MLLMs)在视觉问题分解方面的能力，尤其是针对复杂问题的分解能力。为此，研究团队构建了一个系统的评估框架，包括数据集和多个评估标准，以评估分解后子问题的质量。研究发现，现有的MLLMs在产生高质量子问题上存在困难。为了解决这一问题，作者提出了一个特定的微调数据集DecoVQA+，旨在提升模型在问题分解任务上的能力。论文还提出了一个高效的微调流程，该流程结合了所提数据集和特定的选择性分解训练目标。经过微调的MLLMs在子问题质量和选择性提问分解策略上均显著提升，并在视觉问答基准数据集上实现了更高的准确率。通过这项研究，可以看出多模态模型在处理视觉问题时需要改进其分解策略，以提高综合性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 16:44:57 GMT</pubDate>
</item>
<item>
<title>异构预训练变压器在机器人学习中的应用</title>
<link>https://arxiv.org/abs/2409.20537</link>
<guid>https://arxiv.org/abs/2409.20537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了异构预训练变压器HPT，旨在跨不同机器人构型和任务学习通用策略。</p><br /><br /><p><strong>摘要：</strong> 随着机器人学习方法的推进，异构性成为影响训练的关键阻碍。针对以往仅依赖单一机器人构型和特定任务收集数据的问题，本文研究了如何通过大规模异构数据预训练策略表示。我们提出了异构预训练变压器（HPT），该方法有效学习到了与任务和构型无关的共享表示。HPT的架构将不同机器人构型的本体感知和视觉输入对齐为短序列的标记，随后将这些标记处理为控制多种任务的机器人指令。借助于大规模的多构型真实世界机器人数据集，以及仿真环境、部署机器人和人类视频数据集，本文探索了跨异构性预训练策略的可行性。实验表明，HPT优于多项基准模型，并在多种未知任务的仿真基准和实际应用中提升了精细调整策略的表现超过20%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 11:01:26 GMT</pubDate>
</item>
<item>
<title>UniAff：统一的3D物体中心操控与任务理解范式</title>
<link>https://arxiv.org/abs/2409.20551</link>
<guid>https://arxiv.org/abs/2409.20551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniAff提出了一种整合3D物体操作与任务理解的新范式，显著提高了机器人操控的通用性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的综合范式UniAff，旨在解决机器人操控中对3D运动约束和可使用性的不足理解。我们创建了一个包含900个来自19个类别的关节对象和600个来自12个类别的工具的标注数据集，涵盖了与操控相关的关键属性。此外，我们利用多层次大语言模型（MLLMs）来推断用于操控任务的物体中心表示，包括可使用性识别和3D运动约束推理。通过在仿真和真实世界环境下进行的全面实验，结果表明UniAff显著提升了工具和关节物体的机器人操控通用性。我们希望UniAff能够成为未来统一机器人操控任务的通用基准。项目网站上已经发布了图片、视频、数据集和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 10:43:32 GMT</pubDate>
</item>
<item>
<title>基于元长度标记的长度生成任务提升方法研究</title>
<link>https://arxiv.org/abs/2409.18943</link>
<guid>https://arxiv.org/abs/2409.18943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Ruler方法，通过元长度标记提升大语言模型的长度控制能力，表现显著增强。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为Ruler的新方法，旨在提升大语言模型在生成特定长度响应时的能力。为此，我们设计了目标长度生成任务（TLG）和两个评估指标，精确匹配（PM）与灵活匹配（FM），以评估模型在长度约束下的表现。Ruler通过引入元长度标记（MLTs），增强了模型在长度限制指令下的响应生成能力，同时在缺乏长度约束的情况下，也能自动生成适当的MLT，展现了极佳的通用性和适应性。综合实验结果显示，Ruler在不同的大语言模型中均能有效提升其在目标长度生成任务上的表现，例如在精确匹配和灵活匹配方面分别平均提升27.97和29.57。此外，我们还进行了广泛的消融实验，以进一步验证Ruler的有效性和广泛适应能力。相关代码和数据可在https://github.com/Geaming2002/Ruler获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 10:13:22 GMT</pubDate>
</item>
<item>
<title>Coffee-Gym：用于代码编辑反馈的强化学习环境</title>
<link>https://arxiv.org/abs/2409.19715</link>
<guid>https://arxiv.org/abs/2409.19715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Coffee-Gym是一个RL环境，通过数据集和奖励函数，训练代码编辑反馈模型，提高开源代码LLM的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Coffee-Gym，一个针对代码编辑反馈模型培训的综合强化学习(RL)环境。Coffee-Gym主要由两个组成部分构成：首先是Coffee，一个包含人类代码编辑轨迹的数据集，涵盖了编程问题与机器编写的反馈；其次是CoffeeEval，一个奖励函数，能够通过评估修订后代码在单元测试中的表现，真实反映反馈的 helpfulness。Coffee-Gym旨在解决用于RL反馈模型培训的高质量数据集匮乏问题，并且提供比现有最先进的奖励模型（如GPT-4）更准确的奖励。通过应用Coffee-Gym，开发出的反馈模型在提升开源代码LLM的代码编辑能力方面超越了基准，与封闭源LLM的效果相当。我们将数据集和模型检查点公开分享。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 07:43:58 GMT</pubDate>
</item>
<item>
<title>Can Models Learn Skill Composition from Examples?</title>
<link>https://arxiv.org/abs/2409.19808</link>
<guid>https://arxiv.org/abs/2409.19808</guid>
<content:encoded><![CDATA[
As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified k-tuple of language skills. While small models struggled with composing even with k=3, larger models like GPT-4 performed reasonably well with k=5 and 6.   In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of k skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: (1) Training on combinations of k=2 and 3 skills results in noticeable improvements in the ability to compose texts with k=4 and 5 skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 06:35:36 GMT</pubDate>
</item>
<item>
<title>低秩近似之路（二）：SVD</title>
<link>https://spaces.ac.cn/archives/10407</link>
<guid>https://spaces.ac.cn/archives/10407</guid>
<content:encoded><![CDATA[
<p>上一篇文章中我们介绍了“<a href="https://kexue.fm/archives/10366" target="_blank">伪逆</a>”，它关系到给定矩阵$M$和$A$（或$B$）时优化目标$\Vert AB - M\Vert_F^2$的最优解。这篇文章我们来关注$A,B$都不给出时的最优解，即<br />
\begin{equation}\mathop{\text{argmin}}_{A,B}\Vert AB - M\Vert_F^2\label{eq:loss-ab}\end{equation}<br />
其中$A\in\mathbb{R}^{n\times r}, B\in\mathbb{R}^{r\times m}, M\in\mathbb{R}^{n\times m},r < \min(n,m)$。说白了，这就是要寻找矩阵$M$的“最优$r$秩近似（秩不超过$r$的最优近似）”。而要解决这个问题，就需要请出大名鼎鼎的“SVD（奇异值分解）”了。虽然本系列把伪逆作为开篇，但它的“名声”远不如SVD，听过甚至用过SVD但没听说过伪逆的应该大有人在，包括笔者也是先了解SVD后才看到伪逆。</p><p>接下来，我们将围绕着矩阵的最优低秩近似来展开介绍SVD。</p><h2>基本形式</h2><p>对于任意矩阵$M\in\mathbb{R}^{n\times m}$，都可以找到如下形式的奇异值分解（SVD，Singular Value Decomposition）：<br />
\begin{equation}M = U\Sigma V^{\top}\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10407" title="低秩近似之路（二）：SVD">[...]</a></p>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 17:45:00 +0800</pubDate>
</item>
<item>
<title>双重嵌入模型的神经音频水印技术研究</title>
<link>https://arxiv.org/abs/2409.19627</link>
<guid>https://arxiv.org/abs/2409.19627</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种双重嵌入的神经音频水印模型，提升了水印的鲁棒性和定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文设计了一种双重嵌入的神经音频水印模型，旨在提高水印的定位能力和鲁棒性。随着深度学习的发展，神经音频水印相较传统方法，在水印的嵌入和提取效果上更具优势，但仍存在低容量和不可感知性差的问题。此外，水印定位问题在神经水印中尤为重要，尚未得到充分研究。为此，本文提出的IDEAW模型针对攻击层对可逆神经网络的影响进行优化，加强了模型的合理性和稳定性。实验结果表明，IDEAW模型在抵御各种攻击的能力、容量以及高效的定位功能上均优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19627" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:52:47 GMT</pubDate>
</item>
<item>
<title>Cottention：一种基于余弦相似度的新型注意力机制</title>
<link>https://arxiv.org/abs/2409.18747</link>
<guid>https://arxiv.org/abs/2409.18747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cottention通过余弦相似度替代softmax，具备线性内存复杂度，适合处理长序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的注意力机制Cottention，它用余弦相似度替代传统的softmax操作，从而实现了序列长度上原生的线性内存复杂度。这一特性使得Cottention在处理长序列时比softmax注意力显著更为高效。Cottention可以重构为具有有限隐藏状态的递归神经网络（RNN），在推理过程中能够保证常量内存使用。通过在双向BERT和因果GPT任务上的评估，Cottention展现出了与softmax注意力相当的性能，同时显著降低了内存需求。为了确保有效的计算，我们为Cottention开发了一个自定义的CUDA内核。实验结果表明，Cottention是softmax注意力的有希望的替代方案，能在不牺牲性能的情况下处理更长的序列，因为它具备原生线性内存复杂度和在推理时维持常量内存占用的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:33:45 GMT</pubDate>
</item>
<item>
<title>MM1.5：提升多模态大语言模型的文本与图像理解</title>
<link>https://arxiv.org/abs/2409.20566</link>
<guid>https://arxiv.org/abs/2409.20566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MM1.5是一种新型多模态大语言模型，专注于文本丰富的图像理解和多图像推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MM1.5，这是一个新兴的多模态大语言模型（MLLM）系列，旨在提升文本丰富图像理解、视觉参考与定位以及多图像推理的能力。MM1.5在MM1架构的基础上，采用数据中心化的训练方式，系统性地探索了整个模型训练生命周期中不同数据混合对模型性能的影响。这包括高质量的OCR数据和合成描述用于持续预训练，以及针对监督微调的优化视觉指令调优数据混合。我们的模型参数范围从1B到30B，涵盖了密集型和专家混合（MoE）变体，证明了精心的数据策划和训练策略可以在小规模模型（1B和3B）上实现强大的性能。此外，我们还介绍了两个专业变体：MM1.5-Video，专门用于视频理解，以及MM1.5-UI，专为移动用户界面理解而设计。通过广泛的经验研究和消融实验，我们提供了关于训练过程和设计决策的详细洞见，为未来的MLLM开发研究提供了宝贵的指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.20566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:30:20 GMT</pubDate>
</item>
<item>
<title>超连接：替代残差连接的有效方法</title>
<link>https://arxiv.org/abs/2409.19606</link>
<guid>https://arxiv.org/abs/2409.19606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了超连接方法，旨在改善残差连接的不足，并在大语言模型及视觉任务中取得显著性能提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种简单有效的超连接方法，作为残差连接的替代方案，主要解决了残差连接变体中常见的几个缺点，如梯度消失与表征崩溃之间的秋千效应。超连接理论上允许网络调整不同深度特征之间的连接强度，并动态重排列。通过对大语言模型的预训练实验，包括稠密模型与稀疏模型，超连接显示出显著的性能提升。此外，针对视觉任务的附加实验也显示出了类似的改进。我们预期这一方法将在广泛的人工智能问题中得到广泛应用与好处。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:27:44 GMT</pubDate>
</item>
<item>
<title>DiaSynth：一种高质量领域特定对话生成框架</title>
<link>https://arxiv.org/abs/2409.19020</link>
<guid>https://arxiv.org/abs/2409.19020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiaSynth框架通过大规模语言模型生成丰富的领域特定对话，提升对话系统的训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DiaSynth的合成对话生成框架，旨在解决领域特定对话数据集稀缺的问题。当前对话系统的研发受到通用对话数据集和规模小的特定领域数据集的限制。DiaSynth通过动态生成对话，模拟不同的人物角色、子主题和多样的对话特征，利用大规模语言模型及其链式推理能力，生产具有上下文丰富性和真实性的领域特定对话。通过对不同大规模语言模型生成的合成数据进行实验，发现经过合成数据微调的预训练语言模型比基础模型提升了16.47%的表现。此外，合成数据能够捕捉到领域特定数据的90.48%的分布特征，且生成数据的质量会随着大规模语言模型的规模增加而提高。这些结果证实了DiaSynth作为传统数据收集方法的有力替代方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 04:22:04 GMT</pubDate>
</item>
<item>
<title>ICDiff：专为扩散模型设计的图像复制检测方法</title>
<link>https://arxiv.org/abs/2409.19952</link>
<guid>https://arxiv.org/abs/2409.19952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了ICDiff以及D-Rep数据集，用于检测扩散模型生成图像的复制情况。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型产生的图像在数字艺术和视觉营销中的受到越来越多的关注，内容原创性的问题也随之而来。现有的图像复制检测（ICD）模型在检测手工制作的复制品方面虽然准确，但忽略了来自扩散模型所产生图像的挑战。为此，本文介绍了ICDiff，这是首个专为扩散模型开发的ICD方法。我们构建了Diffusion-Replication（D-Rep）数据集，并提出了一种新颖的深度嵌入方法。D-Rep基于最先进的扩散模型Stable Diffusion V1.5生成4万对图像复制品，并将这些复制品手动标注为6个复制级别，范围从0（无复制）到5（完全复制）。我们的PDF-Embedding方法将每对图像复制品的复制级别转化为概率密度函数（PDF），作为监督信号。实验结果表明，PDF-Embedding在D-Rep测试集上表现优于基于协议的方法和非PDF选择。此外，通过利用PDF-Embedding，我们发现著名扩散模型与开放源代码图库之间的复制比例介于10%到20%之间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.19952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Oct 2024 03:46:15 GMT</pubDate>
</item>
<item>
<title>HDFlow：一种结合快速与慢速思维的大语言模型复杂推理框架</title>
<link>https://arxiv.org/abs/2409.17433</link>
<guid>https://arxiv.org/abs/2409.17433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HDFlow框架，结合快速与慢速思维，显著提升大语言模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为HDFlow的新框架，旨在通过结合快速与慢速思维的适应性方式，提升大语言模型在复杂推理问题上的能力。该框架包含两个关键组成部分：1) 动态工作流（Dynamic Workflow），自动将复杂问题拆解为更易管理的小任务，并动态设计工作流程，组合专门的语言模型或符号推理工具来解决子任务；2) 混合思维（Hybrid Thinking），根据问题的复杂性，动态结合快速与慢速思维。我们还提出了一种易于扩展的方法，自动合成包含27K个挑战性推理问题的大规模数据集，并开发了一种混合思维微调方法，以训练较小的语言模型，使其内化快速/慢速混合推理策略。实验结果显示，在四个推理基准数据集上，使用动态工作流的慢速推理显著优于链式推理，而混合思维在计算效率和性能之间达到了最佳平衡。通过我们的混合思维方法进行微调，显著增强了开放源码语言模型的复杂推理能力。实验结果展示了慢速思维、动态工作流和混合思维在拓展大语言模型复杂问题解决能力方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 15:42:07 GMT</pubDate>
</item>
<item>
<title>大语言模型诚实性研究综述</title>
<link>https://arxiv.org/abs/2409.18786</link>
<guid>https://arxiv.org/abs/2409.18786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究综述了大语言模型的诚实性问题及改进策略，旨在促进相关领域的进一步探索。</p><br /><br /><p><strong>摘要：</strong> 诚实是将大语言模型（LLMs）与人类价值观对齐的基本原则，这要求模型能够认识到自己所知与未知的知识，并真实地表达这些知识。然而，尽管后续发展有希望，目前的LLMs仍然表现出显著的不诚实行为，例如自信地给出错误答案，或未能明确表达自己所知的知识。此外，关于LLMs诚实性的研究面临多重挑战，包括对诚实性的定义不一、确切区分已知和未知知识的困难，以及缺乏对相关研究的全面理解。为解决这些问题，我们提供了LLMs诚实性问题的综述，涵盖了这一主题的澄清、评估方法及改进策略。此外，我们还提供了未来研究的见解，旨在激励这一重要领域的进一步探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 14:41:55 GMT</pubDate>
</item>
<item>
<title>基于语言模型学习的新型分类方法</title>
<link>https://arxiv.org/abs/2409.18957</link>
<guid>https://arxiv.org/abs/2409.18957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法，利用大型语言模型进行分类，验证了其优越性与解释性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的利用大型语言模型（LLMs）进行分类任务的方法，称为“语言模型学习”（LML），并提出了一种名为“数据增强预测”（DAP）的新方法。与传统机器学习模型依赖数据清洗和特征工程不同，该方法通过使用LLMs简化了分类过程。分类过程模拟人类对数据的探索与理解，通过对训练数据的总结和评估，确定对每个标签分类最有帮助的特征。在DAP过程中，系统利用数据摘要自动生成查询，从数据集中检索相关行，以确保在复杂数据条件下生成准确的分类结果。通过使用数据摘要和类似数据，DAP确保了上下文感知的决策。此外，系统在提示中使用“充当可解释的机器学习模型”来增强预测的可解释性，让用户能够审核每个预测背后的逻辑。测试中，该方法在多个场景中取得了超过90%的准确率，表明其有效性，并可能优于传统的机器学习模型。相关代码已在GitHub上提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 13:30:27 GMT</pubDate>
</item>
<item>
<title>调节干预偏好优化方法的研究</title>
<link>https://arxiv.org/abs/2409.17545</link>
<guid>https://arxiv.org/abs/2409.17545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了调节干预偏好优化（MIPO）方法，以改善模型对参考模型的干预程度，进而提升模型的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 在偏好优化方法中，常以训练良好的SFT模型作为参考模型进行训练。强化学习与人类反馈（RLHF）和差异化偏好优化（DPO）在优化过程中使用正则化项，以防止模型过度偏离参考模型，从而避免生成异常响应。然而，当参考模型与给定数据对齐不佳且需要显著调整时，正则化项可能会妨碍模型的对齐。因此，我们提出了一种新的方法——调节干预偏好优化（MIPO）。MIPO根据数据与参考模型的对齐程度，调节对参考模型的干预程度：当数据和参考模型良好对齐时，增加干预；若对齐较差则减小干预，以促进更广泛的训练。通过在Alpaca Eval 2.0和MT-Bench上比较MIPO和DPO的性能，采用Mistral-7B和Llama3-8B的实验结果表明，MIPO在多种评估场景中均优于DPO。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 05:36:26 GMT</pubDate>
</item>
<item>
<title>MinerU：高精度文档内容提取开源解决方案</title>
<link>https://arxiv.org/abs/2409.18839</link>
<guid>https://arxiv.org/abs/2409.18839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MinerU提供了一种高精度的文档内容提取方法，有效应对多样化文档类型。</p><br /><br /><p><strong>摘要：</strong> 文档内容分析在计算机视觉领域一直是一个重要的研究方向。尽管在OCR、布局检测和公式识别等方法上取得了显著进展，现有开源解决方案在高质量内容提取方面仍面临挑战，特别是面对多样化的文档类型和内容。为了解决这些问题，我们提出了MinerU，一个开源的高精度文档内容提取解决方案。MinerU利用先进的PDF-Extract-Kit模型，能够有效地从各种文档中提取内容。同时，该系统还采用精细调整的预处理和后处理规则，以确保最终结果的准确性。实验结果表明，MinerU在多种文档类型中均表现出色，显著提高了内容提取的质量和一致性。MinerU开源项目已在https://github.com/opendatalab/MinerU上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 05:17:52 GMT</pubDate>
</item>
<item>
<title>Emu3：一种基于下一个令牌预测的多模态模型</title>
<link>https://arxiv.org/abs/2409.18869</link>
<guid>https://arxiv.org/abs/2409.18869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Emu3通过下一个令牌预测在多模态任务中超越了多个主流模型，展示了其在生成和感知能力上的优越性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Emu3，一套新的多模态模型，完全基于下一个令牌预测进行训练。通过将图像、文本和视频转化为离散的令牌，Emu3从零开始训练一个单一的变换器，利用多种多模态序列的混合进行学习。研究表明，Emu3在生成和感知任务中优于多种任务专用的模型，明显超越了SDXL和LLaVA-1.6等知名模型，同时淘汰了扩散或组合架构。Emu3还能通过预测视频序列中的下一个令牌生成高保真视频。我们简化了复杂的多模态模型设计，专注于令牌这一单一方面，从而在训练和推理过程中释放出巨大的潜力。研究结果表明，下一个令牌预测是构建超越语言的通用多模态智能的有前景的方向。我们开源了关键技术和模型，以支持该领域的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 03:23:21 GMT</pubDate>
</item>
<item>
<title>PhysGen：一种基于单幅图像生成视频的新方法</title>
<link>https://arxiv.org/abs/2409.18964</link>
<guid>https://arxiv.org/abs/2409.18964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysGen通过物理模拟与数据驱动的方法，为单幅图像生成现实与物理一致性的视频。</p><br /><br /><p><strong>摘要：</strong> PhysGen是一种新颖的图像到视频生成方法，能够将单幅图像与输入条件（如施加于图像中对象的力和扭矩）转换为现实、物理上合理且时间上连贯的视频。该方法的关键在于结合了基于模型的物理模拟和数据驱动的视频生成过程，从而在图像空间内实现合理的动态。系统核心包括三大组件：一是图像理解模块，有效捕捉图像的几何形状、材料和物理参数；二是图像空间动态模拟模型，利用刚体物理和推断的参数进行真实行为的模拟；三是基于图像的渲染和精炼模块，利用生成的视频扩散技术生产具有模拟运动的真实视频。生成的视频在物理和外观上都高度真实，且可精确控制，经过量化比较和全面的用户研究显示出优于现有的数据驱动图像到视频生成工作的效果。PhysGen生成的视频可应用于多种下游任务，例如将图像转化为真实动画，或允许用户与图像互动并创建各种动态。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 03:17:46 GMT</pubDate>
</item>
<item>
<title>MIO：一种新型的多模态基础模型</title>
<link>https://arxiv.org/abs/2409.17692</link>
<guid>https://arxiv.org/abs/2409.17692</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIO是一种采用多模态令牌构建的新模型，能实现多种形式的智能生成与理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MIO的新型基础模型，具有处理多模态令牌的能力，能够理解和生成语音、文本、图像和视频。尽管近期大型语言模型（LLMs）和多模态大型语言模型（MM-LLMs）的出现推动了人工通用智能的发展，但它们仍然缺乏真正的任意到任意的理解和生成能力。最近发布的GPT-4o展示了这一领域的潜力，但因闭源且不支持生成多模态交错序列而存在局限。为了填补这一空白，MIO采用因果多模态建模，通过混合离散令牌的方式进行训练。MIO经历了四个阶段的训练过程：对齐预训练、交错预训练、语音增强预训练和多样化任务的综合监督微调。实验结果表明，MIO在多个方面表现出竞争力，甚至比之前的双模态基线和任意对任意模型基线更为优越。此外，MIO还展现了其任意对任意特性带来的高级能力，如交错视频文本生成、视觉链推理、视觉指导生成和图像编辑等。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17692" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 02:16:43 GMT</pubDate>
</item>
<item>
<title>基于向量的后训练量化方法用于极低位宽的大语言模型</title>
<link>https://arxiv.org/abs/2409.17066</link>
<guid>https://arxiv.org/abs/2409.17066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种向量后训练量化方法，显著提高大语言模型的低位量化性能与推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的向量后训练量化（VPTQ）方法，旨在对大型语言模型（LLMs）进行极低位宽量化。为克服传统标量量化在极低位宽时的数值表示局限性，我们引入了第二阶优化技术来构建LLM的向量量化问题，并通过求解该优化问题指导量化算法设计。此外，我们采用与通道无关的第二阶优化来精细调整权重，以实现更精细的向量量化。我们还提出了一种简洁有效的码本初始化算法，以简化优化问题的分解。同时，VPTQ方法支持残差和异常值量化，进而提高模型准确性并进一步压缩模型。实验结果表明，VPTQ在2位宽下，相较于当前最先进技术（SOTA），在LLaMA-2上量化困惑度下降0.01-0.34，Mistral-7B下降0.38-0.68，LLaMA-3下降4.41-7.34，并且在LLaMA-2、Mistral-7B和LLaMA-3的问答任务中平均精度提高了0.79-1.5%、1%和11-22%。我们仅利用10.4-18.6%的量化算法执行时间，使推理吞吐量提升了1.6-1.8倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 01:55:09 GMT</pubDate>
</item>
<item>
<title>多尺度洞察智能体（MSI-Agent）在决策优化中的应用</title>
<link>https://arxiv.org/abs/2409.16686</link>
<guid>https://arxiv.org/abs/2409.16686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍多尺度洞察智能体（MSI-Agent），通过优化洞察生成和选择提高决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出多尺度洞察智能体（MSI-Agent），旨在提升大语言模型（LLM）在计划和决策中的效率。MSI-Agent通过经验选择器、洞察生成器和洞察选择器的三部分管道，能够有效生成任务特定和高层洞察，并将其存储在数据库中，以便于在决策过程中调用相关洞察。实验结果表明，MSI-Agent在规划任务上优于其他洞察策略，并展示出在面对领域转移场景时的更强鲁棒性。本文还探讨了选择种子经验和洞察的策略，以便为大语言模型提供更有用和相关的洞察，从而改善其决策能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.16686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Sep 2024 01:22:44 GMT</pubDate>
</item>
<item>
<title>利用“熄火保护 + 通断器”实现燃气灶智能关火</title>
<link>https://spaces.ac.cn/archives/10394</link>
<guid>https://spaces.ac.cn/archives/10394</guid>
<content:encoded><![CDATA[
<div>燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气...</div>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
</item>
<item>
<title>Softmax后传：寻找Top-K的光滑近似</title>
<link>https://spaces.ac.cn/archives/10373</link>
<guid>https://spaces.ac.cn/archives/10373</guid>
<content:encoded><![CDATA[
<p>Softmax，顾名思义是“soft的max”，是$\max$算子（准确来说是$\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\boldsymbol{x}\in\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在<a href="https://kexue.fm/archives/10145" target="_blank">《通向概率分布之路：盘点Softmax及其替代品》</a>也介绍过其他一些能实现相同效果的方案。</p><p>我们知道，最大值通常又称Top-1，它的光滑近似方案看起来已经相当成熟，那读者有没有思考过，一般的Top-$k$的光滑近似又是怎么样的呢？下面让我们一起来探讨一下这个问题。</p><h2>问题描述</h2><p>设向量$\boldsymbol{x}=(x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$，简单起见我们假设它们两两不相等，即$i\neq j \Leftrightarrow x_i\neq x_j$。记$\Omega_k(\boldsymbol{x})$为$\boldsymbol{x}$最大的$k$个分量的下标集合，即$|\Omega_k(\boldsymbol{x})|=k$以及$\forall i\in \Omega_k(\boldsymbol{x}), j \not\in \Omega_k(\boldsymbol{x})\Rightarrow x_i > x_j$。我们定义Top-$k$算子$\mathcal{T}_k$为$\mathbb{R}^n\mapsto\{0,1\}^n$的映射：<br />
\begin{equation}<br />
[\mathcal{T}_k(\boldsymbol{x})]_i = \left\{\begin{aligned}1,\,\, i\in \Omega_k(\boldsymbol{x}) \\ 0,\,\, i \not\in \Omega_k(\boldsymbol{x})\end{aligned}\right.<br />
\end{equation}<br />
说白了，如果$x_i$属于最大的$k$个元素之一，那么对应的位置变成1，否则变成0，最终结果是一个Multi-Hot向量，比如$\mathcal{T}_2([3,2,1,4]) = [1,0,0,1]$。</p><p class="more"><a href="https://spaces.ac.cn/archives/10373" title="Softmax后传：寻找Top-K的光滑近似">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>深入理解矩阵低秩近似：伪逆的理论与应用</title>
<link>https://spaces.ac.cn/archives/10366</link>
<guid>https://spaces.ac.cn/archives/10366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了矩阵低秩近似中的伪逆概念，探讨其广义逆矩阵的定义及应用。</p><br /><br /><p><strong>摘要：</strong> 矩阵的低秩近似虽然概念易懂，但内容广泛，相关研究中常出现新技巧，令人感到陌生。本文首篇关注伪逆，即广义逆矩阵，是逆矩阵对不可逆矩阵的推广。在低秩近似中，伪逆为优化问题提供重要工具，尤其在数据降维和特征提取中具有广泛应用。伪逆不仅可用于求解线性方程，还能帮助我们理解和构建更高效的模型。如图所示，这一概念与实际应用密切相关，深化了对矩阵处理技术的理解。<img alt="伪逆示意图" src="图片链接1" />随着 LoRA 等技术的兴起，低秩近似的应用正逐渐增多，使得这一数学工具变得愈加重要。<img alt="低秩近似应用示意图" src="图片链接2" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
</item>
<item>
<title>多模态位置编码研究：从RoPE到RoPE-3D的演变</title>
<link>https://spaces.ac.cn/archives/10352</link>
<guid>https://spaces.ac.cn/archives/10352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨多模态LLM中的位置编码，分析传统1D RoPE及其在2D和3D序列中的推广，从而提出更理想的方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态语言模型（LLM）中位置编码的重要性，指出目前尚未达成共识，尤其在多模态模型中。已有的主流位置编码方法是RoPE（相对位置编码），其原设计仅适用于一维序列（RoPE-1D）。为了拓展至其他维度，本文讨论了如何将RoPE推广至二维（RoPE-2D），以适应图像等数据形式，并进一步推导出RoPE-3D用于视频等三维序列。通过对这个问题的深度梳理，作者希望提出更为理想的解决方案。更多内容请参考[原文图示](http://example.com/image1)。此外，文章中首次提到的RoPE-Tie方案虽有其价值，但仍需细化与发展，以期为多模态LLM领域提供贡献。关于RoPE的详细介绍，读者可参考相关文献。当前多模态模型的发展亟待建立统一的方法论，以推动整个领域的进步和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
</item>
<item>
<title>探讨Causal Attention中的位置编码问题</title>
<link>https://spaces.ac.cn/archives/10347</link>
<guid>https://spaces.ac.cn/archives/10347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨Causal Attention中位置编码的作用及NoPE模型的优缺点。</p><br /><br /><p><strong>摘要：</strong> 众所周知，目前主流的LLM都是基于Causal Attention的Decoder-only模型。虽然已经有不少研究表明，Causal Attention可以在不使用额外的位置编码的情况下取得良好效果，但仍然有许多主流模型（如RoPE、ALIBI等）依然加入了位置编码。本文从三个角度分析这一现象：首先，位置编码在Attention中的作用是为了解决序列中单词的顺序问题；其次，NoPE的Causal Attention通过自适应的方式在计算过程中实现了位置编码；最后，NoPE所实现的位置编码在信息传递的准确性和效率上存在一定不足。整体来看，尽管NoPE在某些情境下能够工作，但主流模型仍选择使用额外的位置编码，可能是出于对模型性能的进一步优化考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>解决MathJax与Marked的兼容性问题</title>
<link>https://spaces.ac.cn/archives/10332</link>
<guid>https://spaces.ac.cn/archives/10332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了MathJax与Markdown解析器Marked之间的兼容性问题，并给出了解决方案。</p><br /><br /><p><strong>摘要：</strong> 在我们引入MathJax解析LaTeX公式后，遇到了与Markdown解析器Marked之间的兼容性问题。Markdown是一种轻量级的标记语言，通常在展示前需要转为HTML格式。尽管部分问题源于笔者的严格要求，但我们追求完美的解决方案是值得努力的。通过合理的配置和调整，我们可以实现MathJax与Marked之间的无缝协作，确保渲染后的文档保持美观和准确。最终，这将提升用户体验，让文档在科学和学术写作中更具吸引力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
</item>
<item>
<title>让MathJax更好地兼容谷歌翻译和延时加载</title>
<link>https://spaces.ac.cn/archives/10320</link>
<guid>https://spaces.ac.cn/archives/10320</guid>
<content:encoded><![CDATA[
<div>很早之前，就有读者提出希望把Cool Papers上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一...</div>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
</item>
<item>
<title>“Cool Papers + 站内搜索”的一些新尝试</title>
<link>https://spaces.ac.cn/archives/10311</link>
<guid>https://spaces.ac.cn/archives/10311</guid>
<content:encoded><![CDATA[
<div>在《Cool Papers更新：简单搭建了一个站内检索系统》这篇文章中，我们介绍了Cool Papers新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何...</div>
]]></content:encoded>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
</item>
<item>
<title>概率分布优化问题的探索与分析</title>
<link>https://spaces.ac.cn/archives/10289</link>
<guid>https://spaces.ac.cn/archives/10289</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文章探讨了如何在概率空间中优化目标函数，并提出新的分析和计算方法。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了在概率空间中进行优化的复杂性，以及传统的无约束优化方法如求导与梯度下降在处理概率分布时的局限。由于概率分布的输入特性，直接求解梯度零点可能导致不再是有效的概率分布，因此需要探索新的优化方法来确保结果的合理性。作者分享了自己在学习概率分布优化过程中的所思所感，并希望通过整理总结为读者提供参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://spaces.ac.cn/archives/10289" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
</item>
<item>
<title>对齐全量微调！这是我看过最精彩的LoRA改进（二）</title>
<link>https://spaces.ac.cn/archives/10266</link>
<guid>https://spaces.ac.cn/archives/10266</guid>
<content:encoded><![CDATA[
<p>前两周笔者写了<a href="https://kexue.fm/archives/10226" target="_blank">《对齐全量微调！这是我看过最精彩的LoRA（一）》</a>（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，这样做也只能尽量对齐第一步更新后的$W_1$，所以当时就有读者提出了“后面的$W_2,W_3,\cdots$不管了吗？”的疑问，当时笔者也没想太深入，就单纯觉得对齐了第一步后，后面的优化也会严格一条较优的轨迹走。</p><p>有趣的是，LoRA-GA才出来没多久，arXiv上就新出了<a href="https://arxiv.org/abs/2407.18242" target="_blank">《LoRA-Pro: Are Low-Rank Adapters Properly Optimized?》</a>，其所提的LoRA-Pro正好能回答这个问题！LoRA-Pro同样是想着对齐全量微调，但它对齐的是每一步梯度，从而对齐整条优化轨迹，这正好是跟LoRA-GA互补的改进点。</p><h2>对齐全量</h2><p>本文接着上一篇文章的记号和内容进行讲述，所以这里仅对上一节的内容做一个简单回顾，不再详细重复介绍。LoRA的参数化方式是<br />
\begin{equation}W = (W_0 - A_0 B_0) + AB\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10266" title="对齐全量微调！这是我看过最精彩的LoRA改进（二）">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
</item>
<item>
<title>会话分析任务的系统化研究与应用前景</title>
<link>https://arxiv.org/abs/2409.14195</link>
<guid>https://arxiv.org/abs/2409.14195</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统化了会话分析任务，明确其四个关键步骤，探讨行业应用与未来研究方向。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的发展，会话日志的积累日益增多，会话分析（CA）成为获取关键商业洞察的手段。本文定义了CA任务，并系统梳理相关研究，指出CA的四个关键步骤：会话场景重建、深入归因分析、针对性训练以及基于训练生成对话。我们还展示了相关基准，讨论了行业与学术面临的挑战，以及未来研究方向。目前，大部分研究集中在浅层对话元素的分析上，亟需提高分析深度。借助大型语言模型，最新的研究逐渐向因果关系与战略任务发展，呈现出复杂和高级的研究趋势。这些经验和洞察将在商业运营中，尤其是针对会话日志的分析上，有广泛的应用价值。<img alt="会话分析示意图" src="your_image_link_here" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14195" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 11:20:10 GMT</pubDate>
</item>
<item>
<title>隐性指令调优：语言模型的指令跟随能力探析</title>
<link>https://arxiv.org/abs/2409.14254</link>
<guid>https://arxiv.org/abs/2409.14254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型可通过隐性调优实现指令跟随，且不依赖于指令-响应对训练。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了隐性指令调优在语言模型中的应用。首先发现，训练仅基于响应，而无需对应的指令，仍能实现指令跟随，说明预训练模型已具备潜在的指令-响应映射。其次，即使是在狭域数据上进行指令-响应训练，仍能产生广泛的指令跟随行为，如从诗歌生成食谱。当指令与狭域微调内容相去甚远时，模型的响应往往不再遵循微调领域的风格。为解释隐性指令调优，研究者假设简单的分布变化可促进指令跟随。通过手动编写一种规则驱动的语言模型，并与预训练模型结合，研究者发现只需缓慢增加序列结束概率、惩罚重复、均匀调整15个单词的概率即可实现指令跟随。综上所述，那些未专门设计为实现指令跟随的适应性调优也可以隐性地发挥这一作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 10:24:59 GMT</pubDate>
</item>
<item>
<title>Structured-GraphRAG：提升自然语言查询中的信息检索效率</title>
<link>https://arxiv.org/abs/2409.17580</link>
<guid>https://arxiv.org/abs/2409.17580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Structured-GraphRAG通过知识图谱提升信息检索有效性，适用于复杂数据集和自然语言查询。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Structured-GraphRAG框架，旨在提升自然语言查询中对复杂结构数据集的信息检索能力。传统的数据检索方法，如顺序搜索和基于索引的检索，往往在处理复杂且相互连接的数据结构时表现不佳，导致结果不完整或误导。Structured-GraphRAG利用多个知识图谱，以结构化的形式展示数据，捕捉实体之间的复杂关系，从而实现对信息的更细致和全面的检索。通过将响应置于结构化格式中，该图形化方法减少了语言模型输出中的错误风险，提高了结果的可靠性。本文通过与最新发布的方法进行比较，展示了Structured-GraphRAG的有效性，研究发现，该框架显著提升了查询处理效率并缩短了响应时间。尽管案例研究集中于足球数据，但该框架的设计广泛适用，可为数据分析提供强大工具，并提升各类结构化领域的语言模型应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 07:22:46 GMT</pubDate>
</item>
<item>
<title>基于聚类的令牌池化方法以降低ColBERT向量存储需求</title>
<link>https://arxiv.org/abs/2409.14683</link>
<guid>https://arxiv.org/abs/2409.14683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种聚类池化方法，显著降低ColBERT索引的存储需求，性能几乎不受影响。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们介绍了一种简单的基于聚类的令牌池化方法，以大幅减少ColBERT方法中的向量存储数量。通过这一方法，可将ColBERT索引的空间和内存占用减少50%，且几乎不影响检索性能。此外，该方法支持进一步的向量数量削减，达到66%-75%之间，而在大多数数据集上性能降级保持在5%以下。重要的是，该方法无需进行架构改变或查询时处理，可以作为简单的替代方法在任何类似ColBERT模型的索引阶段使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.14683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 04:54:39 GMT</pubDate>
</item>
<item>
<title>提升潜在扩散模型生成高频细节的像素空间监督方法</title>
<link>https://arxiv.org/abs/2409.17565</link>
<guid>https://arxiv.org/abs/2409.17565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在后训练过程中增加像素空间监督，显著提升潜在扩散模型的生成质量。</p><br /><br /><p><strong>摘要：</strong> 潜在扩散模型（LDMs）在图像生成领域取得了显著进展。虽然LDM在压缩的潜在空间中进行训练与部署更为高效，但仍存在高频细节与复杂构图生成不佳的问题。我们认为，这部分由于训练过程中的潜在空间分辨率较低。为了解决这一问题，我们提出在后训练过程中增加像素空间监督，以更好地保留高频细节。实验结果显示，在最先进的DiT transformer和U-Net扩散模型下，增加像素空间目标能够显著提升监督质量微调和偏好基础后训练的效果，尤其在视觉质量和视觉缺陷指标上有大幅改善。同时，文本对齐质量保持不变。<img alt="Sample Image 1" src="image_link_1" /><img alt="Sample Image 2" src="image_link_2" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:58:25 GMT</pubDate>
</item>
<item>
<title>LLaVA-3D：一种高效的三维场景理解框架</title>
<link>https://arxiv.org/abs/2409.18125</link>
<guid>https://arxiv.org/abs/2409.18125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-3D结合2D理解能力与3D场景理解，采用3D Patch表示，提高训练速度及任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LLaVA-3D的框架，旨在提升三维场景理解能力，同时不损害二维理解性能。LLaVA-3D利用了LLaVA的强大二维理解先验，通过引入一种简单有效的3D Patch表示，将2D CLIP特征与3D空间位置相连接。通过将3D Patch集成到2D大规模多模态模型中，并进行二维和三维视觉-语言指令的联合调优，LLaVA-3D实现了统一的架构。实验结果表明，LLaVA-3D在训练3D视觉-语言数据集时收敛速度比现有的3D大规模多模态模型快3.5倍。此外，LLaVA-3D在多个3D任务上表现出色，并且在二维图像理解和视觉-语言对话能力方面，性能与LLaVA相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:29:44 GMT</pubDate>
</item>
<item>
<title>Lotus：基于扩散模型的视觉基础模型在密集预测任务中的应用</title>
<link>https://arxiv.org/abs/2409.18124</link>
<guid>https://arxiv.org/abs/2409.18124</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lotus模型通过直接预测标注和简化扩散过程，提升了在零-shot情境下的深度和法线估计性能。</p><br /><br /><p><strong>摘要：</strong> Lotus是一种新型的扩散基础视觉模型，其通过直接预测标注而非噪声来提升密集预测任务的性能。针对现有扩散模型在图像生成和密集预测之间的差异，Lotus重构了扩散过程为单步过程，简化了优化流程并显著提升了推理速度。同时，Lotus引入了一种新的调优策略——细节保持器，使得预测结果更加准确和细致。经过验证，Lotus在多个数据集上实现了深度和法线估计的最先进表现，而且在不增加训练数据或模型容量的情况下，效率提升达到数百倍。其研究为提高密集预测任务中的零-shot泛化提供了新的思路和解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18124" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:19:47 GMT</pubDate>
</item>
<item>
<title>机器人模仿人类操作新物体的方法</title>
<link>https://arxiv.org/abs/2409.18121</link>
<guid>https://arxiv.org/abs/2409.18121</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了机器人观察人类演示操作的方法，使用4D可微分部件模型来实现对象的三维运动恢复与模仿。</p><br /><br /><p><strong>摘要：</strong> 本研究开发了机器人观看和模仿（RSRD）的方法，使机器人能够通过观看人类的单视角RGB演示，学习操控新物体。首先提出了4D可微分部件模型（4D-DPM），利用可微分渲染从单一视频中恢复三维部分运动。该分析合成方法采用部分中心特征场进行迭代优化，应用几何正则化器，仅使用单一视频恢复三维运动。在获得4D重建后，机器人规划双手臂动作，复制演示的物体轨迹。通过将演示表示为部分中心轨迹，RSRD侧重于复制演示的预期行为，同时考虑到机器人的形态限制，而非单纯重现手部运动。研究评估了4D-DPM在真实标注的3D部分轨迹上的跟踪准确性，以及RSRD在9个物体上各进行10次试验的物理执行表现。其中每个阶段的平均成功率为87%，在90次试验中的总端到端成功率为60%。值得注意的是，这一切只使用了从大型预训练视觉模型中提取的特征场，而无需任何任务特定的训练、微调、数据集收集或标注。项目页面：<a href="https://robot-see-robot-do.github.io">https://robot-see-robot-do.github.io</a></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18121" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:03:23 GMT</pubDate>
</item>
<item>
<title>EMOVA：情感全方位的语音助手</title>
<link>https://arxiv.org/abs/2409.18042</link>
<guid>https://arxiv.org/abs/2409.18042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EMOVA通过分离语音语义和声学特征，实现语音生成与视觉理解的优越表现。</p><br /><br /><p><strong>摘要：</strong> EMOVA（EMotionally Omni-present Voice Assistant）是一个新型的情感语音助手，旨在弥补现有大型语言模型在语音与视觉理解上的不足。通过语义-声学分离的语音分词器，EMOVA显著提升了视觉-语言和语音能力，且超越了传统的双模态对齐模型。此外，EMOVA还引入了轻量级风格模块，实现灵活的语音风格控制，例如情感和音调调节。这一成果实现了在视觉-语言及语音基准测试上达到最先进的性能，并首次支持具有生动情感的全模态对话能力，标志着在开放源代码社区中对多模态模型的进一步发展。<img alt="EMOVA示意图" src="原文中的图片链接" /></p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.18042" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:28:22 GMT</pubDate>
</item>
<item>
<title>MaskLLM：一种用于大语言模型的可学习剪枝方法</title>
<link>https://arxiv.org/abs/2409.17481</link>
<guid>https://arxiv.org/abs/2409.17481</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaskLLM通过学习掩码分布，在LLMs中实现了半结构化稀疏性，显著降低了推理时的计算开销。</p><br /><br /><p><strong>摘要：</strong> 该研究提出了MaskLLM，一种可学习剪枝方法，实现在大型语言模型（LLMs）中引入半结构化稀疏性（2:4稀疏性），以减少推理的计算负担。MaskLLM通过Gumbel Softmax采样，将N:M模式显式建模为可学习的分布，避免了新重要性标准的开发。该方法在大规模数据集上进行端到端训练，展现出两个显著优势：一是高质量的掩码，能有效扩展至大数据集并学习准确掩码；二是可转移性，通过概率建模掩码分布，可以实现跨领域或任务的稀疏性迁移。研究使用2:4稀疏性评估了MaskLLM在多种LLM（包括LLaMA-2、Nemotron-4和GPT-3，参数量从843M到15B不等）上的表现，结果显示其显著优于现有方法。具体而言，虽然领先的方法在Wikitext上达到的困惑度（PPL）大于10，而Dense模型的PPL为5.12，但MaskLLM通过学习掩码，并保持权重不变，取得了显著更低的6.72 PPL。此外，MaskLLM的可学习特性允许为下游任务或领域定制无损应用2:4稀疏性。代码可在 <a href="https://github.com/NVlabs/MaskLLM">这里</a>获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17481" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:18:19 GMT</pubDate>
</item>
<item>
<title>GemFilter：一种加速长文本输入处理的新方法</title>
<link>https://arxiv.org/abs/2409.17422</link>
<guid>https://arxiv.org/abs/2409.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GemFilter通过初级层筛选和压缩输入令牌，提高LLM推理速度与内存效率。</p><br /><br /><p><strong>摘要：</strong> 本研究提出GemFilter，一种新的算法，通过使用LLM的早期层作为过滤器，选择和压缩输入令牌，从而显著减少后续处理的上下文长度。GemFilter在速度和内存效率方面相较于现有技术（如标准注意力和SnapKV/H2O）显示出显著改善，尤其是在Needle in a Haystack任务中表现优异，达到了2.4倍的速度提升以及30%的GPU内存使用减少。此外，GemFilter在LongBench挑战中与其他方法表现相当，且无需训练，简单可广泛适用于不同的LLM。更重要的是，GemFilter提供的可解释性使人们可以检查所选输入序列，从而增强了对LLM内部机制的理解，推动了对LLM设计和推理的进一步优化。更多信息可访问 [GemFilter项目页面](https://github.com/SalesforceAIResearch/GemFilter)。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:15:22 GMT</pubDate>
</item>
<item>
<title>Disco4D：基于单幅图像的4D人体生成与动画新框架</title>
<link>https://arxiv.org/abs/2409.17280</link>
<guid>https://arxiv.org/abs/2409.17280</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Disco4D通过高斯建模，实现单幅图像下的4D人体生成与动画，增强了细节与灵活性。</p><br /><br /><p><strong>摘要：</strong> 我们提出了Disco4D，这是一个创新的高斯点云框架，用于从单幅图像生成和动画化4D人类形象。与现有方法不同，Disco4D独特地将衣物（通过高斯模型表示）与人体（采用SMPL-X模型）解耦，从而显著提高了生成细节和灵活性。其技术创新包括：1）Disco4D能够高效地将衣物高斯模型与SMPL-X高斯模型进行拟合；2）它采用扩散模型增强3D生成过程，如对输入图像中不可见的遮挡部分进行建模；3）它为每个衣物高斯学习了身份编码，以便于衣物资产的分离与提取。此外，Disco4D自然支持具有生动动态的4D人类动画。大量实验表明，Disco4D在4D人类生成与动画任务中优于其他方法。我们的可视化结果可以在 [Disco4D官网](https://disco-4d.github.io/) 中查看。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2409.17280" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:10:00 GMT</pubDate>
</item>
</channel>
</rss>