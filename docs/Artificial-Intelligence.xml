<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>利用“熄火保护 + 通断器”实现燃气灶智能关火</title>
<link>https://spaces.ac.cn/archives/10394</link>
<guid>https://spaces.ac.cn/archives/10394</guid>
<content:encoded><![CDATA[

<p>燃气灶智能化主要有两个方向：一是检测开关火状态，实现跟抽油烟机等其他设备的联动；二是实现智能关火，这包括定时关火以及接入米家（或者其他智能家居）实现语音关火、远程关火等。目前带有这两点功能的燃气灶选择并不多，并且相比普通燃气灶贵不少，单纯为了这两点功能而换一个新燃气灶并不划算，所以就出现了一些将普通燃气灶智能化的的魔改方案。</p><p><a href="https://kexue.fm/usr/uploads/2024/09/2699197935.png" target="_blank" title="点击查看原图"><img alt="接入方案示意图.png" src="https://kexue.fm/usr/uploads/2024/09/2699197935.png" width="300" /></a></p><p>本文主要分享基于燃气灶自带的熄火保护装置，利用通断器将燃气灶接入米家，实现智能关火功能。</p><p class="more"><a href="https://spaces.ac.cn/archives/10394" title="利用“熄火保护 + 通断器”实现燃气灶智能关火">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
<pubDate>Thu, 26 Sep 2024 10:39:00 +0800</pubDate>
</item>
<item>
<title>Softmax后传：寻找Top-K的光滑近似</title>
<link>https://spaces.ac.cn/archives/10373</link>
<guid>https://spaces.ac.cn/archives/10373</guid>
<content:encoded><![CDATA[

<p>Softmax，顾名思义是“soft的max”，是$\max$算子（准确来说是$\text{argmax}$）的光滑近似，它通过指数归一化将任意向量$\boldsymbol{x}\in\mathbb{R}^n$转化为分量非负且和为1的新向量，并允许我们通过温度参数来调节它与$\text{argmax}$（的one hot形式）的近似程度。除了指数归一化外，我们此前在<a href="https://kexue.fm/archives/10145" target="_blank">《通向概率分布之路：盘点Softmax及其替代品》</a>也介绍过其他一些能实现相同效果的方案。</p><p>我们知道，最大值通常又称Top-1，它的光滑近似方案看起来已经相当成熟，那读者有没有思考过，一般的Top-$k$的光滑近似又是怎么样的呢？下面让我们一起来探讨一下这个问题。</p><h2>问题描述</h2><p>设向量$\boldsymbol{x}=(x_1,x_2,\cdots,x_n)\in\mathbb{R}^n$，简单起见我们假设它们两两不相等，即$i\neq j \Leftrightarrow x_i\neq x_j$。记$\Omega_k(\boldsymbol{x})$为$\boldsymbol{x}$最大的$k$个分量的下标集合，即$|\Omega_k(\boldsymbol{x})|=k$以及$\forall i\in \Omega_k(\boldsymbol{x}), j \not\in \Omega_k(\boldsymbol{x})\Rightarrow x_i > x_j$。我们定义Top-$k$算子$\mathcal{T}_k$为$\mathbb{R}^n\mapsto\{0,1\}^n$的映射：<br />
\begin{equation}<br />
[\mathcal{T}_k(\boldsymbol{x})]_i = \left\{\begin{aligned}1,\,\, i\in \Omega_k(\boldsymbol{x}) \\ 0,\,\, i \not\in \Omega_k(\boldsymbol{x})\end{aligned}\right.<br />
\end{equation}<br />
说白了，如果$x_i$属于最大的$k$个元素之一，那么对应的位置变成1，否则变成0，最终结果是一个Multi-Hot向量，比如$\mathcal{T}_2([3,2,1,4]) = [1,0,0,1]$。</p><p class="more"><a href="https://spaces.ac.cn/archives/10373" title="Softmax后传：寻找Top-K的光滑近似">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
<pubDate>Thu, 19 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>低秩近似之路（一）：伪逆</title>
<link>https://spaces.ac.cn/archives/10366</link>
<guid>https://spaces.ac.cn/archives/10366</guid>
<content:encoded><![CDATA[

<p>可能很多读者跟笔者一样，对矩阵的低秩近似有种熟悉而又陌生的感觉。熟悉是因为，低秩近似的概念和意义都不难理解，加之目前诸如LoRA等基于低秩近似的微调技术遍地开花，让低秩近似的概念在耳濡目染间就已经深入人心；然而，低秩近似所覆盖的内容非常广，在低秩近似相关的论文中时常能看到一些不熟悉但又让我们叹为观止的新技巧，这就导致了一种似懂非懂的陌生感。</p><p>因此，在这个系列文章中，笔者将试图系统梳理一下矩阵低秩近似相关的理论内容，以补全对低秩近似的了解。而在第一篇文章中，我们主要介绍低秩近似系列中相对简单的一个概念——伪逆。</p><h2>优化视角</h2><p>伪逆（Pseudo Inverse），也称“广义逆（Generalized Inverse）”，顾名思义就是“广义的逆矩阵”，它实际上是“逆矩阵”的概念对于不可逆矩阵的推广。</p><p class="more"><a href="https://spaces.ac.cn/archives/10366" title="低秩近似之路（一）：伪逆">[...]</a></p>
]]></content:encoded>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
<pubDate>Sun, 15 Sep 2024 16:53:00 +0800</pubDate>
</item>
<item>
<title>“闭门造车”之多模态思路浅谈（三）：位置编码</title>
<link>https://spaces.ac.cn/archives/10352</link>
<guid>https://spaces.ac.cn/archives/10352</guid>
<content:encoded><![CDATA[

<p>在前面的文章中，我们曾表达过这样的观点：多模态LLM相比纯文本LLM的主要差异在于，前者甚至还没有形成一个公认为标准的方法论。这里的方法论，不仅包括之前讨论的生成和训练策略，还包括一些基础架构的设计，比如本文要谈的“多模态位置编码”。</p><p>对于这个主题，我们之前在<a href="https://kexue.fm/archives/10040" target="_blank">《Transformer升级之路：17、多模态位置编码的简单思考》</a>就已经讨论过一遍，并且提出了一个方案（RoPE-Tie）。然而，当时笔者对这个问题的思考仅处于起步阶段，存在细节考虑不周全、认识不够到位等问题，所以站在现在的角度回看，当时所提的方案与完美答案还有明显的距离。</p><p>因此，本文我们将自上而下地再次梳理这个问题，并且给出一个自认为更加理想的结果。</p><h2>多模位置</h2><p>多模态模型居然连位置编码都没有形成共识，这一点可能会让很多读者意外，但事实上确实如此。对于文本LLM，目前主流的位置编码是<a href="https://kexue.fm/archives/8265" target="_blank">RoPE</a>（RoPE就不展开介绍了，假设读者已经熟知），更准确来说是RoPE-1D，因为原始设计只适用于1D序列。后来我们推导了<a href="https://kexue.fm/archives/8397" target="_blank">RoPE-2D</a>，这可以用于图像等2D序列，按照RoPE-2D的思路我们可以平行地推广到RoPE-3D，用于视频等3D序列。</p><p class="more"><a href="https://spaces.ac.cn/archives/10352" title="“闭门造车”之多模态思路浅谈（三）：位置编码">[...]</a></p>
]]></content:encoded>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
<pubDate>Fri, 06 Sep 2024 17:57:00 +0800</pubDate>
</item>
<item>
<title>Decoder-only的LLM为什么需要位置编码？</title>
<link>https://spaces.ac.cn/archives/10347</link>
<guid>https://spaces.ac.cn/archives/10347</guid>
<content:encoded><![CDATA[

<p>众所周知，目前主流的LLM，都是基于Causal Attention的Decoder-only模型（对此我们在<a href="https://kexue.fm/archives/9529" target="_blank">《为什么现在的LLM都是Decoder-only的架构？》</a>也有过相关讨论），而对于Causal Attention，已经有不少工作表明它不需要额外的位置编码（简称NoPE）就可以取得非平凡的结果。然而，事实是主流的Decoder-only LLM都还是加上了额外的位置编码，比如RoPE、ALIBI等。</p><p>那么问题就来了：明明说了不加位置编码也可以，为什么主流的LLM反而都加上了呢？不是说“多一事不如少一事”吗？这篇文章我们从三个角度给出笔者的看法：</p><blockquote><p>1、位置编码对于Attention的作用是什么？</p><p>2、NoPE的Causal Attention是怎么实现位置编码的？</p><p>3、NoPE实现的位置编码有什么不足？</p></blockquote><p></p><p class="more"><a href="https://spaces.ac.cn/archives/10347" title="Decoder-only的LLM为什么需要位置编码？">[...]</a></p>
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
<pubDate>Sun, 01 Sep 2024 15:09:00 +0800</pubDate>
</item>
<item>
<title>近乎完美地解决MathJax与Marked的冲突</title>
<link>https://spaces.ac.cn/archives/10332</link>
<guid>https://spaces.ac.cn/archives/10332</guid>
<content:encoded><![CDATA[

<p>在<a href="https://kexue.fm/archives/10320" target="_blank">《让MathJax更好地兼容谷歌翻译和延时加载》</a>我们提到<a href="https://papers.cool/" target="_blank">Cool Papers</a>加入了MathJax来解析LaTeX公式，不过万万没想到引发了诸多兼容性问题，虽然部分问题纯粹是笔者的强迫症作祟，但一个尽可能完美的解决方案终究是让人赏心悦目的，所以还是愿意在上面花一点心思。</p><p>上一篇文章我们已经解决了MathJax与谷歌翻译、延时加载的兼容性，这篇文章我们则来解决MathJax与Marked的冲突。</p><h2>问题简述</h2><p>Markdown是一种轻量级标记语言，允许人们使用易读易写的纯文本格式编写文档，可谓是目前最流行的写作语法之一，Cool Papers中的[Kimi]功能，基本上也是按照Markdown语法输出。然而。Markdown并不是直接面向浏览器的语言，面向浏览器的语言叫做HTML，所以在展示给用户之前，有一个Markdown转HTML的过程（渲染）。</p><p class="more"><a href="https://spaces.ac.cn/archives/10332" title="近乎完美地解决MathJax与Marked的冲突">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
<pubDate>Mon, 26 Aug 2024 11:03:00 +0800</pubDate>
</item>
<item>
<title>让MathJax更好地兼容谷歌翻译和延时加载</title>
<link>https://spaces.ac.cn/archives/10320</link>
<guid>https://spaces.ac.cn/archives/10320</guid>
<content:encoded><![CDATA[

<p>很早之前，就有读者提出希望把<a href="https://papers.cool/" target="_blank">Cool Papers</a>上面的数学公式渲染一下，因为很多偏数学的论文，它们的摘要甚至标题上都带有LaTeX代码写的数学公式，如果不把这些公式渲染出来，那么看上去就像是一堆乱码，确实会比较影响阅读体验。然而，之前的测试显示，负责渲染公式的MathJax跟谷歌翻译和延时加载都不大兼容，所以尽管需求存在已久，但笔者一直没有把它加上去。</p><p>不过好消息是，经过反复查阅和调试，这两天笔者总算把兼容性问题解决了，所以现在大家看到的Cool Papers已经能够渲染数学公式了。这篇文章总结一下解决方案，供大家参考。</p><p><a href="https://kexue.fm/usr/uploads/2024/08/3020393852.png" target="_blank" title="点击查看原图"><img alt="摘要带有公式的论文.png" src="https://kexue.fm/usr/uploads/2024/08/3020393852.png" width="600" /></a></p><p class="more"><a href="https://spaces.ac.cn/archives/10320" title="让MathJax更好地兼容谷歌翻译和延时加载">[...]</a></p>
]]></content:encoded>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
<pubDate>Thu, 15 Aug 2024 20:24:00 +0800</pubDate>
</item>
<item>
<title>“Cool Papers + 站内搜索”的一些新尝试</title>
<link>https://spaces.ac.cn/archives/10311</link>
<guid>https://spaces.ac.cn/archives/10311</guid>
<content:encoded><![CDATA[

<p>在<a href="https://kexue.fm/archives/10088" target="_blank">《Cool Papers更新：简单搭建了一个站内检索系统》</a>这篇文章中，我们介绍了<a href="https://papers.cool/" target="_blank">Cool Papers</a>新增的站内搜索系统。搜索系统的目的，自然希望能够帮助用户快速找到他们需要的论文。然而，如何高效地检索到对自己有价值的结果，并不是一件简单的事情，这里边往往需要一些技巧，比如精准提炼关键词。</p><p>这时候算法的价值就体现出来了，有些步骤人工来做会比较繁琐，但用算法来却很简单。所以接下来，我们将介绍几点通过算法来提高Cool Papers的搜索和筛选论文效率的新尝试。</p><h2>相关论文</h2><p>站内搜索背后的技术是全文检索引擎（Full-text Search Engine），简单来说，这就是一个基于关键词匹配的搜索算法，其相似度指标是<a href="https://en.wikipedia.org/wiki/Okapi_BM25" target="_blank">BM25</a>。</p><p class="more"><a href="https://spaces.ac.cn/archives/10311" title="“Cool Papers + 站内搜索”的一些新尝试">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
<pubDate>Mon, 12 Aug 2024 16:51:00 +0800</pubDate>
</item>
<item>
<title>通向最优分布之路：概率空间的最小化</title>
<link>https://spaces.ac.cn/archives/10289</link>
<guid>https://spaces.ac.cn/archives/10289</guid>
<content:encoded><![CDATA[

<p>当要求函数的最小值时，我们通常会先求导函数然后寻找其零点，比较幸运的情况下，这些零点之一正好是原函数的最小值点。如果是向量函数，则将导数改为梯度并求其零点。当梯度零点不易求得时，我们可以使用梯度下降来逐渐逼近最小值点。</p><p>以上这些都是无约束优化的基础结果，相信不少读者都有所了解。然而，本文的主题是概率空间中的优化，即目标函数的输入是一个概率分布，这类目标的优化更为复杂，因为它的搜索空间不再是无约束的，如果我们依旧去求解梯度零点或者执行梯度下降，所得结果未必能保证是一个概率分布。因此，我们需要寻找一种新的分析和计算方法，以确保优化结果能够符合概率分布的特性。</p><p>对此，笔者一直以来也感到颇为头疼，所以近来决定”痛定思痛“，针对概率分布的优化问题系统学习了一番，最后将学习所得整理在此，供大家参考。</p><p class="more"><a href="https://spaces.ac.cn/archives/10289" title="通向最优分布之路：概率空间的最小化">[...]</a></p>
]]></content:encoded>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
<pubDate>Tue, 06 Aug 2024 14:52:00 +0800</pubDate>
</item>
<item>
<title>对齐全量微调！这是我看过最精彩的LoRA改进（二）</title>
<link>https://spaces.ac.cn/archives/10266</link>
<guid>https://spaces.ac.cn/archives/10266</guid>
<content:encoded><![CDATA[

<p>前两周笔者写了<a href="https://kexue.fm/archives/10226" target="_blank">《对齐全量微调！这是我看过最精彩的LoRA（一）》</a>（当时还没有编号“一”），里边介绍了一个名为“LoRA-GA”的LoRA变体，它通过梯度SVD来改进LoRA的初始化，从而实现LoRA与全量微调的对齐。当然，从理论上来讲，这样做也只能尽量对齐第一步更新后的$W_1$，所以当时就有读者提出了“后面的$W_2,W_3,\cdots$不管了吗？”的疑问，当时笔者也没想太深入，就单纯觉得对齐了第一步后，后面的优化也会严格一条较优的轨迹走。</p><p>有趣的是，LoRA-GA才出来没多久，arXiv上就新出了<a href="https://arxiv.org/abs/2407.18242" target="_blank">《LoRA-Pro: Are Low-Rank Adapters Properly Optimized?》</a>，其所提的LoRA-Pro正好能回答这个问题！LoRA-Pro同样是想着对齐全量微调，但它对齐的是每一步梯度，从而对齐整条优化轨迹，这正好是跟LoRA-GA互补的改进点。</p><h2>对齐全量</h2><p>本文接着上一篇文章的记号和内容进行讲述，所以这里仅对上一节的内容做一个简单回顾，不再详细重复介绍。LoRA的参数化方式是<br />
\begin{equation}W = (W_0 - A_0 B_0) + AB\end{equation}</p><p class="more"><a href="https://spaces.ac.cn/archives/10266" title="对齐全量微调！这是我看过最精彩的LoRA改进（二）">[...]</a></p>
]]></content:encoded>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
<pubDate>Mon, 29 Jul 2024 16:31:00 +0800</pubDate>
</item>
<item>
<title>The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends</title>
<link>https://arxiv.org/abs/2409.14195</link>
<guid>https://arxiv.org/abs/2409.14195</guid>
<content:encoded><![CDATA[

In the era of large language models (LLMs), a vast amount of conversation logs will be accumulated thanks to the rapid development trend of language UI. Conversation Analysis (CA) strives to uncover and analyze critical information from conversation data, streamlining manual processes and supporting business insights and decision-making. The need for CA to extract actionable insights and drive empowerment is becoming increasingly prominent and attracting widespread attention. However, the lack of a clear scope for CA leads to a dispersion of various techniques, making it difficult to form a systematic technical synergy to empower business applications. In this paper, we perform a thorough review and systematize CA task to summarize the existing related work. Specifically, we formally define CA task to confront the fragmented and chaotic landscape in this field, and derive four key steps of CA from conversation scene reconstruction, to in-depth attribution analysis, and then to performing targeted training, finally generating conversations based on the targeted training for achieving the specific goals. In addition, we showcase the relevant benchmarks, discuss potential challenges and point out future directions in both industry and academia. In view of current advancements, it is evident that the majority of efforts are still concentrated on the analysis of shallow conversation elements, which presents a considerable gap between the research and business, and with the assist of LLMs, recent work has shown a trend towards research on causality and strategic tasks which are sophisticated and high-level. The analyzed experiences and insights will inevitably have broader application value in business operations that target conversation logs.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 11:20:10 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 11:20:10 GMT</pubDate>
</item>
<item>
<title>Instruction Following without Instruction Tuning</title>
<link>https://arxiv.org/abs/2409.14254</link>
<guid>https://arxiv.org/abs/2409.14254</guid>
<content:encoded><![CDATA[

Instruction tuning commonly means finetuning a language model on instruction-response pairs. We discover two forms of adaptation (tuning) that are deficient compared to instruction tuning, yet still yield instruction following; we call this implicit instruction tuning. We first find that instruction-response pairs are not necessary: training solely on responses, without any corresponding instructions, yields instruction following. This suggests pretrained models have an instruction-response mapping which is revealed by teaching the model the desired distribution of responses. However, we then find it's not necessary to teach the desired distribution of responses: instruction-response training on narrow-domain data like poetry still leads to broad instruction-following behavior like recipe generation. In particular, when instructions are very different from those in the narrow finetuning domain, models' responses do not adhere to the style of the finetuning domain. To begin to explain implicit instruction tuning, we hypothesize that very simple changes to a language model's distribution yield instruction following. We support this by hand-writing a rule-based language model which yields instruction following in a product-of-experts with a pretrained model. The rules are to slowly increase the probability of ending the sequence, penalize repetition, and uniformly change 15 words' probabilities. In summary, adaptations made without being designed to yield instruction following can do so implicitly.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 10:24:59 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 10:24:59 GMT</pubDate>
</item>
<item>
<title>Enhancing Structured-Data Retrieval with GraphRAG: Soccer Data Case Study</title>
<link>https://arxiv.org/abs/2409.17580</link>
<guid>https://arxiv.org/abs/2409.17580</guid>
<content:encoded><![CDATA[

Extracting meaningful insights from large and complex datasets poses significant challenges, particularly in ensuring the accuracy and relevance of retrieved information. Traditional data retrieval methods such as sequential search and index-based retrieval often fail when handling intricate and interconnected data structures, resulting in incomplete or misleading outputs. To overcome these limitations, we introduce Structured-GraphRAG, a versatile framework designed to enhance information retrieval across structured datasets in natural language queries. Structured-GraphRAG utilizes multiple knowledge graphs, which represent data in a structured format and capture complex relationships between entities, enabling a more nuanced and comprehensive retrieval of information. This graph-based approach reduces the risk of errors in language model outputs by grounding responses in a structured format, thereby enhancing the reliability of results. We demonstrate the effectiveness of Structured-GraphRAG by comparing its performance with that of a recently published method using traditional retrieval-augmented generation. Our findings show that Structured-GraphRAG significantly improves query processing efficiency and reduces response times. While our case study focuses on soccer data, the framework's design is broadly applicable, offering a powerful tool for data analysis and enhancing language model applications across various structured domains.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 07:22:46 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 07:22:46 GMT</pubDate>
</item>
<item>
<title>Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling</title>
<link>https://arxiv.org/abs/2409.14683</link>
<guid>https://arxiv.org/abs/2409.14683</guid>
<content:encoded><![CDATA[

Over the last few years, multi-vector retrieval methods, spearheaded by ColBERT, have become an increasingly popular approach to Neural IR. By storing representations at the token level rather than at the document level, these methods have demonstrated very strong retrieval performance, especially in out-of-domain settings. However, the storage and memory requirements necessary to store the large number of associated vectors remain an important drawback, hindering practical adoption. In this paper, we introduce a simple clustering-based token pooling approach to aggressively reduce the number of vectors that need to be stored. This method can reduce the space &amp; memory footprint of ColBERT indexes by 50% with virtually no retrieval performance degradation. This method also allows for further reductions, reducing the vector count by 66%-to-75% , with degradation remaining below 5% on a vast majority of datasets. Importantly, this approach requires no architectural change nor query-time processing, and can be used as a simple drop-in during indexation with any ColBERT-like model.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 04:54:39 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 04:54:39 GMT</pubDate>
</item>
<item>
<title>Pixel-Space Post-Training of Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2409.17565</link>
<guid>https://arxiv.org/abs/2409.17565</guid>
<content:encoded><![CDATA[

Latent diffusion models (LDMs) have made significant advancements in the field of image generation in recent years. One major advantage of LDMs is their ability to operate in a compressed latent space, allowing for more efficient training and deployment. However, despite these advantages, challenges with LDMs still remain. For example, it has been observed that LDMs often generate high-frequency details and complex compositions imperfectly. We hypothesize that one reason for these flaws is due to the fact that all pre- and post-training of LDMs are done in latent space, which is typically 8 times 8 lower spatial-resolution than the output images. To address this issue, we propose adding pixel-space supervision in the post-training process to better preserve high-frequency details. Experimentally, we show that adding a pixel-space objective significantly improves both supervised quality fine-tuning and preference-based post-training by a large margin on a state-of-the-art DiT transformer and U-Net diffusion models in both visual quality and visual flaw metrics, while maintaining the same text alignment quality.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:58:25 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 02:58:25 GMT</pubDate>
</item>
<item>
<title>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness</title>
<link>https://arxiv.org/abs/2409.18125</link>
<guid>https://arxiv.org/abs/2409.18125</guid>
<content:encoded><![CDATA[

Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we employ a simple yet effective representation, 3D Patch, which connects 2D CLIP patch features with their corresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:29:44 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 02:29:44 GMT</pubDate>
</item>
<item>
<title>Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction</title>
<link>https://arxiv.org/abs/2409.18124</link>
<guid>https://arxiv.org/abs/2409.18124</guid>
<content:encoded><![CDATA[

Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce Lotus, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also significantly enhances efficiency, being hundreds of times faster than most existing diffusion-based methods.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:19:47 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 02:19:47 GMT</pubDate>
</item>
<item>
<title>Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction</title>
<link>https://arxiv.org/abs/2409.18121</link>
<guid>https://arxiv.org/abs/2409.18121</guid>
<content:encoded><![CDATA[

Humans can learn to manipulate new objects by simply watching others; providing robots with the ability to learn from such demonstrations would enable a natural interface specifying new behaviors. This work develops Robot See Robot Do (RSRD), a method for imitating articulated object manipulation from a single monocular RGB human demonstration given a single static multi-view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a method for recovering 3D part motion from a monocular video with differentiable rendering. This analysis-by-synthesis approach uses part-centric feature fields in an iterative optimization which enables the use of geometric regularizers to recover 3D motions from only a single video. Given this 4D reconstruction, the robot replicates object trajectories by planning bimanual arm motions that induce the demonstrated object part motion. By representing demonstrations as part-centric trajectories, RSRD focuses on replicating the demonstration's intended behavior while considering the robot's own morphological limits, rather than attempting to reproduce the hand's motion. We evaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part trajectories and RSRD's physical execution performance on 9 objects across 10 trials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of 87% success rate, for a total end-to-end success rate of 60% across 90 trials. Notably, this is accomplished using only feature fields distilled from large pretrained vision models -- without any task-specific training, fine-tuning, dataset collection, or annotation. Project page: https://robot-see-robot-do.github.io
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 02:03:23 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 02:03:23 GMT</pubDate>
</item>
<item>
<title>EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions</title>
<link>https://arxiv.org/abs/2409.18042</link>
<guid>https://arxiv.org/abs/2409.18042</guid>
<content:encoded><![CDATA[

GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging in the open-source community. Existing vision-language models rely on external tools for the speech processing, while speech-language models still suffer from limited or even without vision-understanding abilities. To address this gap, we propose EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech capabilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we notice surprisingly that omni-modal alignment can further enhance vision-language and speech abilities compared with the corresponding bi-modal aligned counterparts. Moreover, a lightweight style module is proposed for flexible speech style controls (e.g., emotions and pitches). For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:28:22 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 01:28:22 GMT</pubDate>
</item>
<item>
<title>MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models</title>
<link>https://arxiv.org/abs/2409.17481</link>
<guid>https://arxiv.org/abs/2409.17481</guid>
<content:encoded><![CDATA[

Large Language Models (LLMs) are distinguished by their massive parameter counts, which typically result in significant redundancy. This work introduces MaskLLM, a learnable pruning method that establishes Semi-structured (or ``N:M'') Sparsity in LLMs, aimed at reducing computational overhead during inference. Instead of developing a new importance criterion, MaskLLM explicitly models N:M patterns as a learnable distribution through Gumbel Softmax sampling. This approach facilitates end-to-end training on large-scale datasets and offers two notable advantages: 1) High-quality Masks - our method effectively scales to large datasets and learns accurate masks; 2) Transferability - the probabilistic modeling of mask distribution enables the transfer learning of sparsity across domains or tasks. We assessed MaskLLM using 2:4 sparsity on various LLMs, including LLaMA-2, Nemotron-4, and GPT-3, with sizes ranging from 843M to 15B parameters, and our empirical results show substantial improvements over state-of-the-art methods. For instance, leading approaches achieve a perplexity (PPL) of 10 or greater on Wikitext compared to the dense model's 5.12 PPL, but MaskLLM achieves a significantly lower 6.72 PPL solely by learning the masks with frozen weights. Furthermore, MaskLLM's learnable nature allows customized masks for lossless application of 2:4 sparsity to downstream tasks or domains. Code is available at https://github.com/NVlabs/MaskLLM.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:18:19 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 01:18:19 GMT</pubDate>
</item>
<item>
<title>Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction</title>
<link>https://arxiv.org/abs/2409.17422</link>
<guid>https://arxiv.org/abs/2409.17422</guid>
<content:encoded><![CDATA[

Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing. Our method, GemFilter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4times speedup and 30\% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge. GemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference. Our code is available at https://github.com/SalesforceAIResearch/GemFilter.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:15:22 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 01:15:22 GMT</pubDate>
</item>
<item>
<title>Disco4D: Disentangled 4D Human Generation and Animation from a Single Image</title>
<link>https://arxiv.org/abs/2409.17280</link>
<guid>https://arxiv.org/abs/2409.17280</guid>
<content:encoded><![CDATA[

We present Disco4D, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. 1) Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. 2) It adopts diffusion models to enhance the 3D generation process, e.g., modeling occluded parts not visible in the input image. 3) It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks. Our visualizations can be found in https://disco-4d.github.io/.
]]></content:encoded>
<pubDate>Fri, 27 Sep 2024 01:10:00 GMT</pubDate>
<pubDate>Fri, 27 Sep 2024 01:10:00 GMT</pubDate>
</item>

</channel>
</rss>