<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>xAR框架：一种扩展自回归模型的生成方法</title>
<link>https://arxiv.org/abs/2502.20388</link>
<guid>https://arxiv.org/abs/2502.20388</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究提出xAR，一个新型自回归模型，缓解了曝光偏差并优化了生成效率。</p><br><br><p><strong>摘要：</strong> 自回归建模（AR）在语言和视觉生成模型中扮演着重要角色，但在2D图像结构中最优的token定义仍待探讨。本文提出xAR，一种将token概念扩展至实体X的通用自回归框架，实体X可以代表单个补丁、邻近补丁的聚合、非局部补丁分组或整个图像。同时，我们将离散token分类重构为连续实体回归，采用流匹配方法在每个AR步骤中训练，从而引入了噪声上下文学习，有效缓解了曝光偏差。xAR提供了两个主要优势：灵活的预测单元和避免依赖教师强迫的方法。在ImageNet-256生成基准测试中，xAR-B模型以172M参数超越675M参数的DiT-XL和SiT-XL，并实现了20倍的推理速度提高。xAR-H则以1.24的FID分数设定新基准，速度比前一最佳模型快2.2倍。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.20388 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 13:21:13 GMT</pubDate>
<pubDate>Fri, 28 Feb 2025 13:21:13 GMT</pubDate>
</item>

<item>
<title>探讨大语言模型中的关系特定神经元</title>
<link>https://arxiv.org/abs/2502.17355</link>
<guid>https://arxiv.org/abs/2502.17355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大语言模型中存在关系特定的神经元，影响知识生成。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大语言模型（LLMs）中，某些神经元是否专注于关系而非具体实体。研究假设这些神经元能够识别输入文本中的关系，并指导相关生成。通过对Llama-2系列进行实验，结果表明存在关系特定的神经元，并通过选择性去激活这些神经元，评估了其对处理特定关系事实的影响。研究显示，关系特定神经元具备三个显著特性：（i）神经元累积效应，即去激活更多相关神经元会导致相关事实的更大降解；（ii）神经元多样性，不同关系的神经元可以共享，且部分神经元可跨语言传递；（iii）神经元干扰，去激活特定关系的神经元能够提高对其他关系事实的生成表现。这些发现为理解大语言模型中知识存储的机制提供了新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:54:03 GMT</pubDate>
</item>
<item>
<title>提升自主AI代理的安全性：应对攻击与脆弱性</title>
<link>https://arxiv.org/abs/2502.16750</link>
<guid>https://arxiv.org/abs/2502.16750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了自主AI代理的安全威胁及其防护框架。</p><br /><br /><p><strong>摘要：</strong> 自主AI代理利用大语言模型为社会各个领域创造了重要价值，但面临着来自对手的安全威胁，亟需采取保护措施以保障信任与安全。文章指出，静态守卫措施无法有效应对许多种越狱与欺骗性对齐等高级攻击，并强调在真实环境中增强鲁棒性的重要性。通过开发新的评估框架，本文旨在提升基于LLM的代理安全性，采用反图灵测试和多代理模拟进行攻击检测，并用GEMINI 1.5 pro与lama-3.3-70B等模型进行反越狱系统的测试。尽管检测能力达到了94%的准确率，但在长时间攻击下系统仍表现出持续的脆弱性，攻击成功率随着提示长度增加而上升，揭示了复杂系统的多个故障。因此，提出了基于主动监控的灵活安全体系，以应对现有模型的脆弱性，是文章的核心贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:46:19 GMT</pubDate>
</item>
<item>
<title>Training Consistency Models with Variational Noise Coupling</title>
<link>https://arxiv.org/abs/2502.18197</link>
<guid>https://arxiv.org/abs/2502.18197</guid>
<content:encoded><![CDATA[
Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at 64 times 64 resolution in 2-step generation. Our code is available at https://github.com/sony/vct .
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:55:48 GMT</pubDate>
</item>
<item>
<title>高效动态高斯点阵的渲染技术研究</title>
<link>https://arxiv.org/abs/2502.20378</link>
<guid>https://arxiv.org/abs/2502.20378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出高效动态高斯点阵方法显著提升动态场景渲染速度与质量。</p><br /><br /><p><strong>摘要：</strong> 动态场景的单目视频渲染是一项重要且具有挑战性的任务。尽管近期的可变形高斯点阵技术为动态场景的表现提供了有效解决方案，但其在训练视角下生成过多冗余高斯，导致渲染速度变慢。此外，静态区域的高斯属性是时间不变的，这使得不必要的高斯建模会引起静态区域的抖动。本文提出了一种高效动态高斯点阵（EDGS）方法，通过稀疏时间变属性建模表示动态场景，利用稀疏锚点网格表示，结合经典内核表示计算密集高斯的运动流，并引入无监督策略以高效筛选静态区域锚点。实验结果表明，EDGS在两个真实数据集上的渲染速度显著提高，同时相比于先前的最先进方法，渲染质量也得到了提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:25:35 GMT</pubDate>
</item>
<item>
<title>ArtGS：一种用于多部件关节物体建模的新方法</title>
<link>https://arxiv.org/abs/2502.19459</link>
<guid>https://arxiv.org/abs/2502.19459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtGS是一种利用3D高斯表示的新方法，提升多部件关节物体的重建和动态建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ArtGS，一种新颖的方法，利用3D高斯作为灵活高效的表示，解决计算机视觉中多部件关节物体的建模挑战。ArtGS通过结合规范高斯的粗到细初始化和更新，旨在对齐不同物体状态下的关节部件信息，并采用启发自皮肤绑定的部件动态建模模块，以提升部件网格重建和关节学习。通过在合成和真实世界数据集上的广泛实验，ArtGS展示了在联合参数估计和部件网格重建方面的最高性能，尤其是在处理复杂的多部件关节物体时显著提高了重建质量与效率。此外，本文还对设计选择进行了详细分析，以验证每个组件的有效性，并指明未来的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:47:08 GMT</pubDate>
</item>
<item>
<title>MedVLM-R1：提升医疗图像分析的透明度与可信度</title>
<link>https://arxiv.org/abs/2502.19634</link>
<guid>https://arxiv.org/abs/2502.19634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedVLM-R1通过生成自然语言推理提升医疗图像分析的透明度与准确性。</p><br /><br /><p><strong>摘要：</strong> MedVLM-R1是一种新的医疗视觉语言模型，旨在增强医疗图像分析中的透明度和可信度。现有的医疗视觉语言模型多只提供最终答案，缺乏有效的推理过程。MedVLM-R1通过强化学习框架，鼓励模型发现可人类解释的推理路径，避免了监督fine-tuning所带来的过拟合问题。在限量训练数据（600个视觉问题回答样本）和2B参数的情况下，MedVLM-R1在MRI、CT和X光基准测试中的准确率从55.11%提升至78.22%，超越了在超过100万样本上训练的更大模型，并展示了在分布外任务中的良好领域泛化能力。此模型的推出标志着医疗图像分析与明确推理结合的重要进展，为临床实践中的可信和可解释AI奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:36:05 GMT</pubDate>
</item>
<item>
<title>Dream Engine：一种高效的文本-图像交错控制生成框架</title>
<link>https://arxiv.org/abs/2502.20172</link>
<guid>https://arxiv.org/abs/2502.20172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出Dream Engine框架，实现高效的文本-图像交错控制生成。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像生成领域的进步，出现了将强大的文本编码器与扩散变换器骨架相结合的统一框架。现有方法在控制输出图像方面有所探索，但对于任意文本-图像交错控制的全面框架仍然缺乏。为此，本文提出了Dream Engine，一个高效的生成框架，旨在实现任意文本-图像交错控制。通过融合多模态信息编码器，该框架提高了文本与图像之间的对齐性能，利用两阶段训练策略，实现了文本与图像的联合对齐和多模态指令调优。实验结果表明，本方法在GenEval基准测试中取得了0.69的整体得分，表现出色，接近当前最先进的文本到图像生成模型SD3.5和FLUX的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:02:19 GMT</pubDate>
</item>
<item>
<title>NeoBERT：下一代双向编码器的创新与突破</title>
<link>https://arxiv.org/abs/2502.19587</link>
<guid>https://arxiv.org/abs/2502.19587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBERT通过融合先进架构和数据，重定义了双向模型的能力。</p><br /><br /><p><strong>摘要：</strong> 随着架构、预训练和微调的创新，NeoBERT作为下一代双向编码器，极大提升了大规模自回归语言模型的学习和推理能力。NeoBERT采用了最优的深度宽度比例和4096个令牌的扩展上下文长度，成为现有基础模型的即插即用替代品。尽管参数仅为250M，NeoBERT在大规模MTEB基准上取得了超越BERT large、RoBERTa large以及其他现代编码器的卓越成绩。本文还评估了各项修改对GLUE的影响，并设计了统一的微调和评估框架以适应MTEB。为推动研究和实际应用，所有代码、数据、检查点及训练脚本均已公布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 03:27:32 GMT</pubDate>
</item>
<item>
<title>基于去耦价值策略优化的强化学习框架</title>
<link>https://arxiv.org/abs/2502.16944</link>
<guid>https://arxiv.org/abs/2502.16944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DVPO框架通过去耦价值模型与策略训练，提升了训练效率和效果。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新颖的去耦价值策略优化（DVPO）框架，旨在克服PPO基础的强化学习从人类反馈中的计算复杂性和不稳定性。DVPO使用预训练的全球价值模型（GVM）替代传统的奖励建模，该模型基于策略轨迹预测令牌级的回报估计。通过去耦价值模型与政策训练，DVPO在不依赖于手动调整奖励的情况下，显著降低了GPU内存使用量达40%和训练时间达35%。实验结果表明，DVPO在多个基准测试中表现优越，超越了现有的高效RLHF方法（如DPO），同时在性能上与最先进的PPO相匹配。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:55:41 GMT</pubDate>
</item>
<item>
<title>FINEREASON: 细粒度评估大语言模型推理能力的逻辑难题基准</title>
<link>https://arxiv.org/abs/2502.20238</link>
<guid>https://arxiv.org/abs/2502.20238</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍FINEREASON，一种用于评估大语言模型推理过程的新的基准。</p><br /><br /><p><strong>摘要：</strong> 近年来，大语言模型（LLMs）取得了显著进展，展现了从快速反应的“系统1”思维转向反思和纠错的“系统2”思维的重要转变。然而，现有的基准测试主要依赖最终答案的准确性，未能充分评估模型在推理过程中的中间步骤与反思能力。为了解决这一问题，本文引入了FINEREASON，一个逻辑难题基准，旨在对LLMs的推理能力进行细粒度评估。每个难题都可以分解为原子步骤，非常适合验证中间结果的正确性。同时，我们介绍了两个任务：状态检查和状态转移，以全面评估模型如何评估当前情况及计划下一步。此外，我们还提供了旨在增强一般数学任务上表现的难题训练集。结果表明，在我们的状态检查和转移数据上训练的模型在GSM8K上数学推理能力提升了最高5.1%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20238" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:14:11 GMT</pubDate>
</item>
<item>
<title>Mobius：无注释文本生成无缝循环视频的新方法</title>
<link>https://arxiv.org/abs/2502.20307</link>
<guid>https://arxiv.org/abs/2502.20307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobius方法实现了无注释文本直接生成无缝循环视频。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法Mobius，能够根据文本描述直接生成无缝循环视频，无需用户注释。该方法利用预训练的视频潜在扩散模型，在推断过程中通过构建潜在循环，将视频的起始和结束噪声连接。通过逐步在每一步中将第一帧潜在图像平移到最后，保持时间一致性，同时逐步提取多帧潜在图像的去噪信息。与传统的动图不同，Mobius方法不需要图像作为外观，从而避免了生成结果运动的限制，能够产生更动态的运动和更好的视觉质量。我们进行了多次实验和比较，以验证该方法在不同场景下的有效性，所有代码将公开提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:14:01 GMT</pubDate>
</item>
<item>
<title>FlexiDiT：一种动态计算预算的生成变换器</title>
<link>https://arxiv.org/abs/2502.20126</link>
<guid>https://arxiv.org/abs/2502.20126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiDiT通过动态计算预算提高生成效率，降低资源需求。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的生成模型FlexiDiT，它通过动态计算预算改善了现代扩散变换器在推理过程中对资源的需求。传统的静态计算预算方式在每个去噪步骤中分配固定的计算资源，这限制了其灵活性。我们提出的框架允许预训练的扩散变换器（DiT）模型转变为灵活的模型，能够在不降低图像生成质量的情况下，灵活处理不同的计算预算。在实验中，我们证明了相较于静态模型，FlexiDiT在类别条件和文本条件图像生成中，计算需求可降低超过40%。此外，我们的方法也适用于视频生成，FlexiDiT模型在生成样本时计算需求最高可减少75%，仍能保持优异的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:10:30 GMT</pubDate>
</item>
<item>
<title>R1-Translator: 增强推理能力的通用机器翻译框架</title>
<link>https://arxiv.org/abs/2502.19735</link>
<guid>https://arxiv.org/abs/2502.19735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出R1-Translator框架，通过推理增强实现通用机器翻译。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新框架R1-Translator (R1-T1)，旨在通过强化学习实现通用机器翻译中的推理能力，提升翻译质量。当前的方法多局限于特定翻译子任务或依赖于与人类不相符的推理链。这项研究的创新在于扩展推理翻译的范围到六种语言以及法律、医疗等多种领域，并制定了六种专家策划的推理模板，以反映人类的多层次推理策略。此外，R1-Translator还通过强化学习实现推理链的自我发现与避免遗忘，从而提升了翻译的灵活性。实验结果表明，在Flores-101测试集上，R1-Translator在21种语言和80个翻译方向上的表现稳步提升，尤其在15种训练中未见的语言上表现突出，展示出其对多语言翻译的良好适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:03:34 GMT</pubDate>
</item>
<item>
<title>UniTok：统一视觉生成与理解的新型离散标记器</title>
<link>https://arxiv.org/abs/2502.20321</link>
<guid>https://arxiv.org/abs/2502.20321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniTok通过多代码本量化缩小视觉生成与理解之间的差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UniTok，一种新的离散视觉标记器，旨在缩小视觉生成与理解之间的表现差异。该模型编码了细粒度细节以支持生成，同时捕获高层语义以增强理解。研究表明，这两种目标在训练过程中可能导致损失冲突，然而，我们发现这一瓶颈源于离散标记的表示能力有限。为了解决这一问题，我们提出了多代码本量化，通过将向量量化分为多个独立的子代码本来扩展潜在特征空间，并避免因过大代码本导致的训练不稳定。实验结果显示，UniTok在提升统一离散标记器的性能上显著超越了领域特定的连续标记器，如在ImageNet上，UniTok取得了0.38的rFID（相比SD-VAE的0.87）和78.6%的零样本准确率（相比CLIP的76.2%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:34:45 GMT</pubDate>
</item>
<item>
<title>CODESYNC：适应动态代码演变的语言模型评估基准</title>
<link>https://arxiv.org/abs/2502.16645</link>
<guid>https://arxiv.org/abs/2502.16645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍CODESYNC，一个应对动态代码演变的实时更新数据引擎。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在软件工程中的表现出色，但在适应频繁更新的第三方库API方面面临挑战。本文提出CODESYNC，一个能够识别过时代码模式并收集Python第三方库实时代码知识更新的数据引擎。同时开发了CODESYNCBENCH，涵盖220个API的基准测试，提供3300个测试案例，评估LLMs与代码演变同步的能力。实验结果显示，不论是使用何种知识更新方法，这些语言模型在动态代码演变中仍然表现不佳。我们相信，该基准可以为未来实时代码知识更新的有效方法发展奠定坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:04:14 GMT</pubDate>
</item>
<item>
<title>Subtask导向的强化微调（SoRFT）：提升大语言模型的issue解决能力</title>
<link>https://arxiv.org/abs/2502.20127</link>
<guid>https://arxiv.org/abs/2502.20127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的SoRFT方法，通过分解任务提升问题解决效果。</p><br /><br /><p><strong>摘要：</strong> 当前主流问题解决框架大多依赖商业模型，导致高成本和隐私问题。现有的训练方法普遍存在泛化能力差、未能充分利用开源资源等缺陷。我们提出了子任务导向的强化微调（SoRFT），这种新方法将问题解决分解为文件定位、函数定位、行定位和代码编辑生成等结构化子任务。SoRFT分为两个训练阶段：首先，通过拒绝采样进行监督微调，使用真实数据过滤链式思维（CoT）数据；随后，利用基于真实数据奖励的PPO进行规则基础的强化学习。我们在SWE-Bench Verified和SWE-Bench Lite上评估了SoRFT训练的模型，在开源模型中取得了最先进的性能（例如，SoRFT-Qwen-7B在SWE-Bench Verified上解决了21.4%的问题）。实验结果表明，SoRFT显著提升了问题解决能力，改善了模型的泛化性，为商业模型提供了一种成本效益高的替代方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:38:04 GMT</pubDate>
</item>
<item>
<title>提升大型多模态模型性能的新策略：测试时重路由</title>
<link>https://arxiv.org/abs/2502.20395</link>
<guid>https://arxiv.org/abs/2502.20395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出的R2-T2方法提高了多模态模型在挑战性任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 在大型多模态模型（LMMs）中，视觉表示的感知能力通常不如大型语言模型（LLMs），这影响了模型在复杂下游任务中的表现。为了解决这一问题，研究者们引入了专家混合（MoE）机制来提供丰富的多粒度表示，然而，基于端到端训练的路由器并不总能为每个测试样本生成最优的路由权重。为弥补这一不足，本文提出了一种新颖且高效的方法——测试时重路由（R2-T2），该方法通过将路由权重向邻近正确预测样本的权重向量移动，来局部优化测试时的路由权重。本文还提出了三种不同优化目标和邻域搜索空间的R2-T2策略。实验结果表明，R2-T2在多项挑战基准测试中显著提升了LMM的性能，且不需再训练任何基本模型参数。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:27:24 GMT</pubDate>
</item>
<item>
<title>LongRoPE2：扩展大语言模型的上下文窗口</title>
<link>https://arxiv.org/abs/2502.20082</link>
<guid>https://arxiv.org/abs/2502.20082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongRoPE2提升了预训练大语言模型的上下文处理能力。</p><br /><br /><p><strong>摘要：</strong> LongRoPE2是一种创新方法，旨在扩展预训练大语言模型的有效上下文窗口，同时保持短上下文的性能。该方法的三大贡献包括：一是提出假设，现有方法在更高RoPE维度上的训练不足导致了持续的分布外（OOD）问题；二是开发了一种有效的RoPE重缩放算法，通过“针导向”的困惑度演化搜索来解决训练不足问题；三是采用混合上下文窗口训练方法，对模型权重进行微调，以适应长上下文序列的重缩放RoPE，同时保留使用原始RoPE的短上下文的性能。基于LLaMA3-8B和Phi3-mini-3.8B的广泛实验结果验证了该假设，并证明了LongRoPE2的有效性。LongRoPE2使得LLaMA3-8B达到128K的有效上下文长度，同时保持短上下文性能超过98.5%，仅需10B标记数据，比Meta的方法减少了80倍，却未能达成目标有效上下文长度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:22:53 GMT</pubDate>
</item>
<item>
<title>自我奖励推理大型语言模型的研究</title>
<link>https://arxiv.org/abs/2502.19613</link>
<guid>https://arxiv.org/abs/2502.19613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨自我奖励推理的语言模型及其自我校正能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了一种自我奖励推理的大型语言模型（LLMs），能够在推理时同时生成逐步推理过程并评估输出的正确性，而无需外部反馈。重点关注自我校正任务，该模型可以自主检测响应中的错误、修改输出，并决定何时终止迭代修正循环。为此，提出了一种两阶段的算法框架，首阶段通过序列拒绝采样合成包含自我奖励和自我校正机制的长链推理轨迹，随后对模型进行微调，以学习相关模式。第二阶段通过使用基于规则的信号的强化学习进一步提升模型评估响应准确性和修正输出的能力。在 Llama-3 和 Qwen-2.5 的实验中，结果表明该方法超越了内在的自我校正能力，性能与依赖外部奖励模型的系统相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:15:54 GMT</pubDate>
</item>
<item>
<title>多草稿推测解码的效率优化研究</title>
<link>https://arxiv.org/abs/2502.18779</link>
<guid>https://arxiv.org/abs/2502.18779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了多草稿推测解码在效率优化中的关键设计选择与理论上限。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了多草稿推测解码（MDSD）在自然语言处理中的效率瓶颈，重点分析了草稿采样方法和验证算法。通过研究最优传输问题的对偶，本文首次高效计算了MDSD的最优接受率，并评估了现有验证算法与理论上限之间的差距。研究表明，不同的草稿采样方法显著影响最优接受率，其中不重复采样优于重复采样。此外，目前的验证算法在两种采样方式下均未能达到理论上限。 findings建议，精心设计的草稿采样方法有可能提高最优接受率，并促进验证算法的开发，使其更接近理论最优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:03:36 GMT</pubDate>
</item>
<item>
<title>基于少量偏好的个性化优化框架FSPO研究</title>
<link>https://arxiv.org/abs/2502.19312</link>
<guid>https://arxiv.org/abs/2502.19312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FSPO框架，通过用户偏好实现语言模型的个性化快速适应。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为少量偏好优化（FSPO）的新框架，旨在提升大型语言模型（LLM）在用户交互应用中的个性化能力。FSPO将奖励建模重新定义为一种元学习问题，使得LLM可以通过很少的用户标记偏好快速适应，并构建专属的奖励函数。同时，为了解决实际偏好数据匮乏的问题，研究团队设计了合成偏好数据集，并成功生成超过100万个合成个性化偏好数据。研究表明，合成数据必须具备高多样性和一致性，以确保成功转移到真实用户。在对1500个合成用户进行电影评论、教育背景适应和一般问答等三种领域的个性化生成评估中，FSPO在合成用户的响应生成中实现了87%的胜率，而在与真实用户的开放性问答中，也有72%的胜率。这些结果表明FSPO在个性化生成任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19312" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 11:09:15 GMT</pubDate>
</item>
<item>
<title>Drop-Upcycling：提升混合专家模型训练效率的新方法</title>
<link>https://arxiv.org/abs/2502.19261</link>
<guid>https://arxiv.org/abs/2502.19261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Drop-Upcycling方法优化混合专家模型的训练效率，解决了上游回收的性能瓶颈问题。</p><br /><br /><p><strong>摘要：</strong> Mixture of Experts (MoE) 架构在训练和推理成本上远低于同等容量的密集模型。尽管上游回收（upcycling）能为基于预训练密集模型初始化的MoE模型带来初期性能提升，但其训练效率相比从零开始训练则会显著降低，导致长期效果不佳。为了解决这一问题，我们提出了Drop-Upcycling方法，该方法融合了利用预训练密集模型知识与对部分权重进行统计重初始化的两种看似矛盾的策略。在专家专精培养方面，该方法显著提升了MoE模型的知识获取效率。我们的实验结果展示，Drop-Upcycling在长期训练中显著超越了以往的MoE构建方法，尤其是在处理数千亿令牌的数据时表现出色。经过验证，我们的MoE模型包含5.9B活跃参数，达到了与同家族13B密集模型相媲美的性能，同时所需训练FLOPs约为其1/4。所有实验资源，包括源代码、训练数据、模型检查点及日志都已公开，以促进可再现性及未来MoE研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 10:12:03 GMT</pubDate>
</item>
<item>
<title>Rank1：基于测试时间计算的新型重排序模型</title>
<link>https://arxiv.org/abs/2502.18418</link>
<guid>https://arxiv.org/abs/2502.18418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rank1 是首个利用测试时间计算的重排序模型，提升检索性能。</p><br /><br /><p><strong>摘要：</strong> Rank1 是首个经过训练，能够利用测试时间计算的重排序模型。该模型展示了在信息检索中使用推理语言模型（如 OpenAI 的 o1 和 Deepseek 的 R1）进行蒸馏的有效性，从而快速提升小型模型的性能。我们收集并开源了超过60万个 MS MARCO 查询和段落的 R1 推理轨迹数据集。基于该数据集训练的模型显示出：在高级推理和指令遵循数据集上具有最先进的性能；在出分布时表现出色，能够响应用户输入的提示；具有可解释的推理链，可以提供给用户或基于 RAG 的系统。此外，我们还展示了这些模型的量化版本在减少计算和内存使用的同时，依然保留了强大的性能。总体而言，Rank1 展示了测试时间计算能够实现一种全新的可解释且高效的重排序检索模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 09:41:49 GMT</pubDate>
</item>
<item>
<title>双重优化嵌入信息的方法提升弱监督语义分割性能</title>
<link>https://arxiv.org/abs/2502.15885</link>
<guid>https://arxiv.org/abs/2502.15885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DOEI方法，通过双重优化嵌入信息提升弱监督语义分割效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法DOEI（Dual Optimization of Embedding Information），旨在提升弱监督语义分割（WSSS）的性能。传统的类激活图（CAM）在高维空间中由于类激活响应与语义信息的耦合不足，常导致目标共现或低激活，从而影响识别准确度。DOEI通过语义感知的注意力权重矩阵重建嵌入表示，优化嵌入信息的表达能力。具体而言，该方法在类到补丁的交互中放大高置信度的标记，抑制低置信度的标记。此外，DOEI还引入了混合特征对齐模块，结合RGB值、嵌入引导特征和自注意力权重，以增强候选标记的可靠性。全面实验表明，DOEI是一个有效的可插拔模块，能够显著提高先进的视觉变换器基础WSSS模型在PASCAL VOC和MS COCO等流行基准上的类激活图质量和分割性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 07:31:45 GMT</pubDate>
</item>
<item>
<title>知识单位：破解科学知识传播的版权壁垒</title>
<link>https://arxiv.org/abs/2502.19413</link>
<guid>https://arxiv.org/abs/2502.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出将学术文本转化为知识单位，以突破版权限制，促进科学知识传播。</p><br /><br /><p><strong>摘要：</strong> 随着版权法规限制了科学知识的传播，本文提出了一种新方法，即将学术文献转换为知识单位。这些知识单位利用大语言模型（LLM）提取文本的结构化数据，捕捉实体、属性和关系，而非风格内容，从而在法律和技术上都能有效传播科学知识。通过对德国和美国的法律分析，我们证明了知识单位提供了一种合法的知识分享框架，并且在保护版权的前提下能够保留约95%的原始文本事实内容。我们的研究表明，免于版权限制的科学知识可以为研究与教育带来变革性的好处。此外，我们还分享了一些开源工具，帮助将研究文档转换为知识单位，以促进科学知识的开放获取，同时尊重版权。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:18:26 GMT</pubDate>
</item>
<item>
<title>GHOST 2.0: generative high-fidelity one shot transfer of heads</title>
<link>https://arxiv.org/abs/2502.18417</link>
<guid>https://arxiv.org/abs/2502.18417</guid>
<content:encoded><![CDATA[
While the task of face swapping has recently gained attention in the research community, a related problem of head swapping remains largely unexplored. In addition to skin color transfer, head swap poses extra challenges, such as the need to preserve structural information of the whole head during synthesis and inpaint gaps between swapped head and background. In this paper, we address these concerns with GHOST 2.0, which consists of two problem-specific modules. First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations. Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions. Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping. We also tackle complex cases, such as large difference in hair styles of source and target. Code is available at https://github.com/ai-forever/ghost-2.0
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:15:43 GMT</pubDate>
</item>
<item>
<title>推出BIG-Bench Extra Hard：评估大型语言模型推理能力的新基准</title>
<link>https://arxiv.org/abs/2502.19187</link>
<guid>https://arxiv.org/abs/2502.19187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了BIG-Bench Extra Hard基准，旨在提升大型语言模型的推理能力评估。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在日常应用中的日益普及，要求其具备强大的推理能力。然而，目前的大多数推理基准主要集中在数学和编码能力上，未能全面评估更广泛的推理技能。为此，本文推出了BIG-Bench Extra Hard（BBEH）基准，通过引入更具挑战性的任务来测试LLMs的推理能力，旨在填补这一空白。不同于之前的BIG-Bench Hard（BBH），BBEH对每个任务进行替换，显著提高了难度。我们的评估结果显示，最佳通用模型在BBEH上仅达到9.8%的平均准确率，而最佳推理专用模型的准确率为44.8%，表明LLMs在推理能力上仍有显著的提升空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:43:05 GMT</pubDate>
</item>
<item>
<title>提升语言模型反驳能力以加速科学发现</title>
<link>https://arxiv.org/abs/2502.19414</link>
<guid>https://arxiv.org/abs/2502.19414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出评估语言模型反驳能力的新基准，以加速科学研究。</p><br /><br /><p><strong>摘要：</strong> 随着对语言模型（LMs）在加速科学发现潜力的兴奋不断增加，反驳假设成为科学进步的关键。然而，目前的基准主要评估模型生成解决方案的能力，而缺乏对其反驳能力的评估。我们建议开发能够评估模型生成反例的基准。为此，我们引入REFUTE，一个动态更新的基准，包括近期问题和编程竞赛中的错误提交，通过代码执行自动评估反例。我们的分析显示即使是最佳的推理代理（如OpenAI o3-mini）在REFUTE上仅能为不足9%的错误解决方案生成反例，尽管其评分表明其能从零解决48%的问题。希望我们的研究能够推动对语言模型反驳能力的评估与提升，这是加速研究和模型自我改进的关键能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:36:29 GMT</pubDate>
</item>
<item>
<title>CritiQ：基于人类偏好的自动数据选择方法</title>
<link>https://arxiv.org/abs/2502.19279</link>
<guid>https://arxiv.org/abs/2502.19279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CritiQ是一种新型的数据选择方法，通过人类偏好自动挖掘数据质量标准。</p><br /><br /><p><strong>摘要：</strong> CritiQ是一种创新的数据选择方法，旨在提高语言模型的性能，依赖于人类偏好的高质量数据。与传统的依赖手动启发式、困惑度指标和分类器的方法相对，CritiQ通过仅使用30对人工标注的样本，自动挖掘数据质量标准。其核心组件CritiQ Flow通过经理代理演变质量标准，工作代理进行成对判断，并建立知识库以提取前期工作的质量标准，从而增强CritiQ Flow。与基于困惑度和分类器的方法相比，基于语言的标准更具可解释性且具有可重用性。经过标准的推导后，CritiQ Scorer被训练用于赋予数据质量分数并执行高效的数据选择。我们在代码、数学和逻辑领域展示了该方法的有效性，在人类标注的测试集上实现了高准确率，并在持续训练Llama 3.1模型后，观察到相比均匀采样在下游任务上的表现提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:47:02 GMT</pubDate>
</item>
<item>
<title>PosterSum：科学海报总结的前沿基准</title>
<link>https://arxiv.org/abs/2502.17540</link>
<guid>https://arxiv.org/abs/2502.17540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PosterSum为科学海报与其摘要的总结提供新基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PosterSum，这是一个新颖的基准，旨在推动视觉语言模型的发展，以便理解并总结科学海报为研究论文摘要。数据集中包含16,305个会议海报及其对应摘要，这些海报在图像格式中呈现，并面临复杂布局、密集文本区域、表格和图形等多种视觉理解挑战。我们对当前最先进的多模态大型语言模型（MLLMs）进行了基准测试，结果显示它们在准确理解和总结科学海报方面表现不佳。为此，我们提出了Segment & Summarize，一种分层方法，其在自动化指标上表现超过现有MLLMs，ROUGE-L指标提升了3.14%。该研究为未来海报总结的研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:37:24 GMT</pubDate>
</item>
<item>
<title>多语言模型的事实知识回忆与跨语言转移研究</title>
<link>https://arxiv.org/abs/2502.17955</link>
<guid>https://arxiv.org/abs/2502.17955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多语言模型在跨语言知识转移中的局限性与改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多语言模型在跨语言知识转移中的不足，尽管模型在某一语言中能够正确回忆事实，但在其他语言中却经常出现知识传递失败的现象。我们提出了一个包含10,000个国家相关事实的基准测试，涵盖了13种语言，并引入了三种新的评估指标：事实回忆分数、知识转移性分数和跨语言事实知识转移性分数，以量化多语言模型的事实回忆与知识转移能力。研究结果显示当前最先进的多语言模型在跨语言泛化方面存在根本性缺陷，表现出对使用语言的敏感性，知识转移不够有效。这一发现强调了模型需识别语言特有的事实可靠性，并在不同语言间有效利用最可信的信息。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:17:58 GMT</pubDate>
</item>
<item>
<title>首个希腊金融评估基准与语言模型的推出</title>
<link>https://arxiv.org/abs/2502.18772</link>
<guid>https://arxiv.org/abs/2502.18772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推动希腊金融NLP发展的Plutus-ben和Plutus-8B基准与模型发布。</p><br /><br /><p><strong>摘要：</strong> 尽管希腊在全球经济中扮演重要角色，但因希腊语的复杂性及缺乏特定领域数据，希腊金融语境下的大型语言模型（LLMs）仍未得到充分探索。为弥补这一空白，本文推出了Plutus-ben，这是第一个希腊金融评估基准，以及Plutus-8B，首个经过希腊领域特定数据微调的希腊金融LLM。Plutus-ben涵盖了五个核心金融NLP任务，包括命名实体识别、问答、摘要归纳及主题分类，旨在推动系统化和可重复的LLM评估。同时，我们还呈现了三种高质量的希腊金融数据集，均由专业母语者仔细注释，并与两个现有资源相结合。对22个LLMs在Plutus-ben上的全面评估显示，希腊金融NLP面临语言复杂性、特定领域术语及金融推理能力不足的挑战。这些结果强调了跨语言转移的局限性、希腊训练模型对金融专业知识的需求，以及将金融LLMs适配于希腊文本的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:08:09 GMT</pubDate>
</item>
<item>
<title>Kanana系列双语语言模型的高效预训练与适应性方法</title>
<link>https://arxiv.org/abs/2502.18934</link>
<guid>https://arxiv.org/abs/2502.18934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kanana模型在韩语表现卓越，英语竞争力强，且计算成本显著低。</p><br /><br /><p><strong>摘要：</strong> Kanana是一系列双语语言模型，在韩语方面表现卓越，而在英语方面具有竞争力。该系列模型的计算成本显著低于同类的最先进模型。报告详细介绍了在预训练过程中采用的技术，包括高质量数据筛选、分阶段预训练、深度缩放及剪枝和蒸馏等，以实现计算高效且表现竞争力的模型。此外，报告还概述了Kanana模型后训练过程中的方法论，包括监督微调和偏好优化，旨在增强与用户的无缝交互能力。同时，报告详细说明了适应特定场景的模型调整方法，例如嵌入、检索增强生成和功能调用。Kanana模型系列的参数范围从21亿到325亿不等，其中21亿模型（基础、指令、嵌入）已公开发布，以促进对韩语语言模型的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18934" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:05:13 GMT</pubDate>
</item>
<item>
<title>DeltaBench：评估o1-like模型在长推理链上的表现</title>
<link>https://arxiv.org/abs/2502.19361</link>
<guid>https://arxiv.org/abs/2502.19361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍DeltaBench，用于评估o1-like模型在长推理链上的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DeltaBench，旨在评估不同o1-like模型（如QwQ和DeepSeek-R1）在长Chain-of-Thought（CoT）推理步骤方面的表现及现有大型语言模型（LLMs）对这些长CoT的批判能力。DeltaBench包含来自不同o1-like模型生成的长CoT，用于多种推理任务（如数学、代码和一般推理），并测量检测长CoT推理中错误的能力。通过对生成的长CoT进行细致分析，我们发现了不同o1-like模型的有效性和效率。此外，研究评估了现有的过程奖励模型（PRMs）和批评模型在检测每个标注过程错误方面的表现，旨在探讨现有模型的边界和限制。最终，本文希望DeltaBench能够指导开发者更好地理解模型在长CoT推理能力上的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:04:47 GMT</pubDate>
</item>
<item>
<title>利用量子力学知识提升3D分子表示的能谱预训练</title>
<link>https://arxiv.org/abs/2502.16284</link>
<guid>https://arxiv.org/abs/2502.16284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过能谱增强3D分子表示的预训练以融入量子力学知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了3D结构与分子能态之间的关系，并提出运用量子力学的能谱增强分子表示的预训练方法。现有方法多依赖经典力学建模，忽视了量子力学的效应。在此框架下，我们提出了SpecFormer，一种用于通过掩蔽补丁重建编码分子能谱的多谱编码器。通过对3D编码器与能谱编码器输出的对比目标进行对齐，我们提升了3D编码器对分子的理解。评估结果显示，基于我们预训练的表示在分子性质预测和动力学建模方面超越了现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:29:40 GMT</pubDate>
</item>
<item>
<title>AI助力科学发现：多智能体系统在生物医学领域的应用</title>
<link>https://arxiv.org/abs/2502.18864</link>
<guid>https://arxiv.org/abs/2502.18864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了一种AI共同科学家系统，旨在增强科学假设生成与实验验证。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于Gemini 2.0的AI共同科学家系统，旨在通过生成、辩论和进化的方法，辅助科学家发现新的研究假设。此系统具备多智能体架构和异步任务执行框架，支持灵活的计算扩展，特别关注生物医学领域的应用，如药物再利用、新靶点发现和细菌进化机制解析。具体而言，系统为急性髓性白血病提供了具有临床应用潜力的药物候选，而在新靶点发现中，提出了在肝纤维化研究中可验证的表观遗传靶点。此外，AI共同科学家还总结了细菌进化中的新基因转移机制，展示了AI在增强生物医学发现方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:18:06 GMT</pubDate>
</item>
<item>
<title>AISafetyLab: 统一的AI安全框架与工具包</title>
<link>https://arxiv.org/abs/2502.16776</link>
<guid>https://arxiv.org/abs/2502.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍AISafetyLab，一个集成AI安全方法的工具包。</p><br /><br /><p><strong>摘要：</strong> 随着AI模型在实际场景中的广泛应用，确保其安全性变得愈发重要。尽管在AI安全的评估与提升方面进行了大量努力，但缺乏标准化框架和全面工具包仍然是系统研究和实际应用的重大障碍。为此，文章提出了AISafetyLab，一个统一的框架和工具包，整合了代表性的攻击、防御及评估方法。AISafetyLab具有直观的界面，使开发者能够无缝应用各种技术，并保持结构良好、可扩展的代码库以支持未来的技术进步。此外，本文对Vicuna进行了实证研究，分析了不同攻击和防御策略的有效性，为未来研究提供了重要见解。AISafetyLab已在GitHub公开发布，致力于持续维护和改进。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:16:03 GMT</pubDate>
</item>
<item>
<title>跨上下文蒸馏方法提升单目深度估计精度</title>
<link>https://arxiv.org/abs/2502.19204</link>
<guid>https://arxiv.org/abs/2502.19204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出跨上下文蒸馏方法，显著提升单目深度估计的准确性。</p><br /><br /><p><strong>摘要：</strong> 单目深度估计(MDE)旨在从单张RGB图像中预测场景深度，对3D场景理解至关重要。近期零-shot MDE的进展利用了标准化深度表示和基于蒸馏的学习，以提高在多样场景中的泛化能力。然而，目前的蒸馏深度标准化方法依赖于全局标准化，可能会放大噪声伪标签，从而降低蒸馏效果。本文系统分析了不同深度标准化策略对伪标签蒸馏的影响，并提出了跨上下文蒸馏方法，结合全局和局部深度线索以增强伪标签质量。此外，我们还引入了一种多教师蒸馏框架，充分利用不同深度估计模型的互补优势，旨在提供更稳健和准确的深度预测。大量在基准数据集上的实验证明，我们的方法在定量和定性方面均显著优于当前最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19204" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:10:20 GMT</pubDate>
</item>
<item>
<title>基于Manim动画的定理解释视频生成与评估</title>
<link>https://arxiv.org/abs/2502.19400</link>
<guid>https://arxiv.org/abs/2502.19400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨使用TheoremExplainAgent生成定理解释视频的有效性及评估基准。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了TheoremExplainAgent，这是一种利用Manim动画生成长达5分钟的定理解释视频的代理方法。我们提出了TheoremExplainBench，一个涵盖240个定理的多学科基准测试，配备5种自动评估指标，以系统性地评估多模态定理解释。研究结果表明，代理计划对于生成详细的长格式视频至关重要，其中o3-mini代理的成功率为93.8%，整体得分为0.77。然而，定量和定性研究也揭示出大多数视频存在视觉元素布局的小问题。此外，多模态解释揭示了文本解释未能暴露的更深层次的推理缺陷，强调了多模态解释的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:07:49 GMT</pubDate>
</item>
<item>
<title>一种结合可验证正确性信号的代理奖励建模方法</title>
<link>https://arxiv.org/abs/2502.19328</link>
<guid>https://arxiv.org/abs/2502.19328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了代理奖励建模方法，提高大语言模型的训练和推理效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了代理奖励建模（Agentic Reward Modeling）方法，强调奖励模型在训练和推理中对于大语言模型的重要性。现有的奖励模型大多集中于人类偏好，忽视了可验证的正确性信号。我们实现了一个名为RewardAgent的奖励代理，将人类偏好奖励与事实性和指令遵循等两个可验证信号结合，以提供更可靠的奖励。经过对现有奖励模型基准的全面实验，RewardAgent在真实世界下游任务的推理时间最优搜索中显著优于传统奖励模型。此外，我们使用RewardAgent构建了训练偏好对，并以DPO目标训练大语言模型，在各种自然语言处理基准测试中取得了优秀表现。代码已公开发布，以便进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:05:16 GMT</pubDate>
</item>
<item>
<title>基于预训练值环境模型的无环境强化学习框架</title>
<link>https://arxiv.org/abs/2502.18906</link>
<guid>https://arxiv.org/abs/2502.18906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于预训练模型的无环境强化学习框架，提高GUI代理的性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对图形用户界面（GUI）代理的无环境强化学习框架，旨在解决传统环境基于RL方法在交互成本和奖励泛化上的挑战。该框架利用预训练的值环境模型（VEM），有效地将价值估计与策略优化解耦。VEM通过离线数据直接预测状态-动作值，从而提取人类交互结果的先验知识，而无需环境反馈或下一个状态预测。这种方法能够避免错误累积，提高对用户界面变化的韧性，强调语义推理在决策中的重要性。该框架分为两个阶段：首先预训练VEM以估计长期行动效用，然后利用固定的VEM信号引导策略探索，支持无布局依赖的GUI自动化。在Android-in-the-Wild基准测试中，VEM在离线和在线设置中均取得了领先的表现，显著优于传统的无环境基线，与无需交互成本的环境基方法相匹配，展示了语义感知价值估计可以达到与在线训练方法相当的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18906" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:02:50 GMT</pubDate>
</item>
<item>
<title>通过词汇课程学习提升语言模型的预训练效率</title>
<link>https://arxiv.org/abs/2502.17910</link>
<guid>https://arxiv.org/abs/2502.17910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种词汇课程学习方法，提高语言模型的预训练效率。</p><br /><br /><p><strong>摘要：</strong> 现代语言模型依赖于预训练前确定的静态词汇，而人类语言学习则表现出适应性词汇获取。为弥补这一差距，本文提出了词汇课程学习的方法，该方法通过词汇规模的对数线性缩放增益来提高预训练效率。该方法在熵引导的词汇扩展与模型优化之间交替进行，使模型能够在不同的标记粒度间学习可迁移的表示。研究表明，较长的标记捕捉可预测的内容，而较短的标记则聚焦于更复杂难测的上下文。我们在小规模GPT模型上的实验显示了这种动态标记化的有效性，提升了扩展效率。我们还发布了代码以支持进一步研究，并计划将实验扩展到更大模型和不同领域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 18:40:15 GMT</pubDate>
</item>
<item>
<title>LDGen：高效的多语言文本到图像生成方法</title>
<link>https://arxiv.org/abs/2502.18302</link>
<guid>https://arxiv.org/abs/2502.18302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LDGen是一种将大语言模型高效整合入文本到图像生成的创新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LDGen，一种将大语言模型（LLMs）整合到现有文本到图像扩散模型中的新方法，旨在降低计算需求。传统的文本编码器（如CLIP和T5）在多语言处理方面存在局限，影响了跨语言的图像生成。为了解决这些问题，我们利用LLMs的高级能力，采用分层字幕优化和人类指令技术来提取精确的语义信息。同时，我们融入了一个轻量级适配器和跨模态精炼器，以促进LLMs和图像特征之间的高效特征对齐和交互。实验结果表明，LDGen在提示遵循性和图像美学质量上均超越了基线模型，同时无缝支持多个语言的零-shot图像生成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 16:56:34 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在视觉识别中的效果与干预研究</title>
<link>https://arxiv.org/abs/2502.17422</link>
<guid>https://arxiv.org/abs/2502.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在视觉识别中的局限性及提高方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了多模态大语言模型（MLLMs）在处理图像问答时，对于小视觉细节的感知能力。实验结果表明，MLLMs的表现对视觉主题的大小非常敏感，并通过干预研究证明此效果是因果关系。文章还分析了MLLMs在回答视觉问题时的注意力模式，发现即使在错误回答时，它们也能准确定位注意力。基于这些发现，提出了一种无需训练的视觉干预方法，利用MLLM内在的注意力和梯度图来增强其对小视觉细节的感知。通过在两个广泛使用的MLLM和七个视觉问答基准上的评估，证明该方法能够显著提高模型的准确性，而无需额外训练。研究结果揭示了在小细节视觉识别任务中应用MLLMs的风险，同时提供了利用模型内部状态进行视觉干预的有效解决方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 14:46:51 GMT</pubDate>
</item>
<item>
<title>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</title>
<link>https://arxiv.org/abs/2502.16069</link>
<guid>https://arxiv.org/abs/2502.16069</guid>
<content:encoded><![CDATA[
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4times improvement in correctly answering experimental questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:51:05 GMT</pubDate>
</item>
<item>
<title>统计学在大型语言模型中的作用与挑战</title>
<link>https://arxiv.org/abs/2502.17814</link>
<guid>https://arxiv.org/abs/2502.17814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨统计学如何提升大型语言模型的可信性与透明性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在人工智能领域的迅速发展，其在文本生成、推理和决策等多项任务中的卓越能力得到认可。然而，当前的成功主要依赖于计算能力和深度学习架构的进步，但在不确定性量化、决策制定、因果推断以及分布转移等方面却面临着挑战。本文探讨了统计学在这些领域中的重要作用，特别是在提高模型的可信性和透明性方面，包括不确定性量化、可解释性、公平性、隐私、掩码和模型适应性等问题。同时，本文还讨论了大型语言模型在统计分析中的潜在作用。通过促进人工智能与统计学之间的深度合作，我们希望推动LLMs理论基础和实践应用的进步，从而更好地应对复杂的社会挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:34:59 GMT</pubDate>
</item>
<item>
<title>WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging</title>
<link>https://arxiv.org/abs/2502.18316</link>
<guid>https://arxiv.org/abs/2502.18316</guid>
<content:encoded><![CDATA[
We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:53:44 GMT</pubDate>
</item>
<item>
<title>基于提示的语言模型评估方法P2L的提案</title>
<link>https://arxiv.org/abs/2502.14855</link>
<guid>https://arxiv.org/abs/2502.14855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法P2L，用于更加精准的语言模型评估。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Prompt-to-Leaderboard（P2L）的方法，用于解决现有语言模型（LLM）评估中的问题，特别是平均指标无法展示用户与提示特定的性能差异。P2L通过将自然语言提示输入到LLM中，生成Bradley-Terry系数的向量，从而预测人类偏好的投票。此方法允许实现针对特定提示的无监督任务评估、最优的查询路由、个性化和模型优缺点的自动评估。根据Chatbot Arena的数据，P2L能更好地反映语言模型性能的细微差异。研究还发现，P2L的提示特定评估能力遵循与LLM相似的幂律缩放。在2025年1月，基于该方法训练的路由器在Chatbot Arena排行榜上获得第一名。相关代码可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:43:07 GMT</pubDate>
</item>
<item>
<title>提升大语言模型调优性能的可扩展偏好数据构建策略</title>
<link>https://arxiv.org/abs/2502.16825</link>
<guid>https://arxiv.org/abs/2502.16825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个可扩展的偏好数据构建策略以提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了通过重复随机抽样来扩大大语言模型（LLMs）对齐过程中的采样规模，以提高性能。传统方法选择最高奖励的样本作为已选择回应，最低奖励的样本作为拒绝回应进行直接偏好优化（DPO），然而实验证实这一策略在样本量增加时表现不佳。为解决此问题，研究者基于样本奖励的正态分布特征构建偏好数据，定义七个代表性奖励点，并系统探讨其21种成对组合。通过对四个模型使用AlpacaEval 2进行评估，发现选择奖励位置为μ - 2σ的拒绝回应而不是最低奖励，对于优化性能至关重要。最终，文章提出了一种可扩展的偏好数据构建策略，能够随着样本规模的增加持续提升模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 09:40:23 GMT</pubDate>
</item>
<item>
<title>LaTIM: 基于Mamba模型的细粒度令牌级可解释性方法</title>
<link>https://arxiv.org/abs/2502.15612</link>
<guid>https://arxiv.org/abs/2502.15612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LaTIM方法，以提升Mamba模型的可解释性。</p><br /><br /><p><strong>摘要：</strong> 本研究引入LaTIM，一种新颖的令牌级分解方法，旨在提高Mamba-1和Mamba-2模型的可解释性。尽管状态空间模型（SSMs）如Mamba在长上下文序列建模方面展示出优越性能，但其在可解释性工具方面的缺乏限制了对其内部机制的理解。我们的方法通过细粒度地分解令牌的贡献，使得用户能够清晰地了解不同层次中Mamba的选择性序列处理方式。我们在机器翻译、复制以及基于检索的生成等多个任务上对LaTIM进行了广泛评估，结果显示其有效揭示了Mamba模型的令牌间交互模式，增强了模型的可解释性和透明度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 07:28:05 GMT</pubDate>
</item>
<item>
<title>引入视觉感知标记提升多模态大语言模型性能</title>
<link>https://arxiv.org/abs/2502.17425</link>
<guid>https://arxiv.org/abs/2502.17425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过视觉感知标记提升多模态大语言模型的视觉感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLM）在视觉信息利用中的感知过程的不足之处，尤其是在视觉感知的自主控制方面。为此，提出了视觉感知标记的概念，旨在赋予MLLM控制其视觉感知过程的机制。设计了两种类型的视觉感知标记：区域选择标记和视觉再编码标记。MLLM以此生成的标记来触发额外的视觉感知操作，从而显著改善空间推理和细致理解等任务的表现。实验结果表明，引入视觉感知标记后，2B模型的平均性能提高了23.6%，得分达到0.708，且比7B参数模型出色13.4%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 02:37:36 GMT</pubDate>
</item>
<item>
<title>压缩LLM的最新进展与彩票模型假设</title>
<link>https://arxiv.org/abs/2502.17535</link>
<guid>https://arxiv.org/abs/2502.17535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨LLM的压缩技术及彩票模型假设对性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文旨在减少大型语言模型（LLMs）的计算和存储成本，探讨模型压缩和KV缓存压缩的研究进展。现有方法主要关注压缩后LLMs的性能保持，通常通过困惑度或准确性来评估在常识知识问答和基本算术推理任务上的表现。文中回顾了检索增强生成、多步骤推理、外部工具和计算表达能力对LLM性能的显著影响，提出了彩票LLM假设，认为对于特定的LLM和任务，存在一个较小的彩票LLM，在多步骤推理和外部工具的辅助下，能够达到与原始LLM相同的性能。同时，讨论了彩票LLM和KV缓存压缩在当前方法中被忽略的关键能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 01:04:23 GMT</pubDate>
</item>
<item>
<title>K-LoRA：一种无训练的内容与风格融合方法</title>
<link>https://arxiv.org/abs/2502.18461</link>
<guid>https://arxiv.org/abs/2502.18461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出K-LoRA，通过有效融合内容与风格，优化LoRA的应用。</p><br /><br /><p><strong>摘要：</strong> 近年来的研究探讨了不同LoRA的结合，以共同生成学习的风格和内容。然而，现有方法要么无法有效同时保留原始主题和风格，要么需要额外的训练。本文认为LoRA的内在性质可以有效指导扩散模型合并学习到的主题和风格。在此基础上，提出了一种简单但有效的无训练的LoRA融合方法K-LoRA。K-LoRA在每个注意力层中比较要融合的每个LoRA中的前K个元素，确定最优融合的LoRA选择。这一选择机制确保在融合过程中保留主题和风格的最具代表性的特征，有效平衡其贡献。实验结果表明，所提方法能有效集成原始LoRA学习到的主题和风格信息，在定性和定量结果上均优于现有的训练基础方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18461" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:56:27 GMT</pubDate>
</item>
<item>
<title>Shakti VLM：高效的视觉语言模型家族</title>
<link>https://arxiv.org/abs/2502.17092</link>
<guid>https://arxiv.org/abs/2502.17092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Shakti VLM通过模型设计提升多模态学习的数据效率。</p><br /><br /><p><strong>摘要：</strong> Shakti VLM是一组具有10亿和40亿参数的视觉语言模型，旨在解决多模态学习中的数据效率挑战。尽管近期的视觉语言模型依赖于大量训练数据以实现强大性能，Shakti模型通过架构创新在较少的标记下获得了竞争力的成果。关键进展包括QK-Normalization以增强注意力稳定性、混合归一化技术及改进的位置信息编码。此外，三阶段训练策略进一步优化了学习效率。评估结果显示，Shakti-VLM-1B和Shakti-VLM-4B在文档理解、视觉推理、OCR提取及一般多模态推理中表现出色。这些结果强调，通过模型设计和训练策略而非单纯依赖大量数据，Shakti能够成为企业级多模态任务的高效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:38:42 GMT</pubDate>
</item>
<item>
<title>Scale-Distribution Decoupling: 稳定大规模语言模型训练的新方法</title>
<link>https://arxiv.org/abs/2502.15499</link>
<guid>https://arxiv.org/abs/2502.15499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法SDD，以稳定大规模语言模型的训练过程。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模语言模型（LLMs）预训练中的训练稳定性问题，提出了一种名为规模-分布解耦（SDD）的新方法。SDD通过明确地解耦全连接层中权重矩阵的规模和分布，利用归一化机制调节激活和可学习的缩放向量，保持良好的梯度条件，从而有效防止梯度爆炸和消散。实验结果表明，该方法在多个LLM架构中稳定训练效果，并在不同归一化配置下优于现有技术。SDD还轻量化且与现有框架兼容，为大规模语言模型训练的稳定提供了一种实用解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:26:11 GMT</pubDate>
</item>
<item>
<title>WebGames：评估通用网页浏览AI代理的新基准套件</title>
<link>https://arxiv.org/abs/2502.18356</link>
<guid>https://arxiv.org/abs/2502.18356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebGames通过50多个挑战评估网页浏览AI的性能和限度。</p><br /><br /><p><strong>摘要：</strong> WebGames是一个综合基准套件，旨在通过50多个互动挑战评估通用网页浏览AI代理的能力。这些挑战针对人类易于理解的操作，同时系统性地测试当前AI系统在基本浏览器交互、高级输入处理、认知任务、工作流自动化和互动娱乐等方面的局限性。该框架通过密闭测试环境消除外部依赖，确保可再现的评估与可验证的真实解决方案。我们对领先的视觉语言模型进行了评估，包括GPT-4o、Claude Computer-Use、Gemini-1.5-Pro和Qwen2-VL，结果显示这些AI系统与人类表现之间存在显著能力差距，最佳AI系统的成功率仅为43.1%，而人类的表现为95.7%，突显了当前AI系统在处理人类认为直观的常见网页交互模式方面的基本限制。该基准在webgames.convergence.ai上公开可用，提供轻量级的客户端实现以促进快速评估周期。通过其模块化架构和标准化挑战规范，WebGames为测量更强大网页浏览代理的发展进展提供了坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:16 GMT</pubDate>
</item>
<item>
<title>兼顾人类听觉选择性的听觉注意驱动大型语言模型研究</title>
<link>https://arxiv.org/abs/2502.16794</link>
<guid>https://arxiv.org/abs/2502.16794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型，结合脑信号以改进听觉处理中的选择性注意力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Intention-Informed Auditory Scene Understanding (II-ASU)与Auditory Attention-Driven LLM (AAD-LLM)，一种原型系统，通过集成脑信号来推断听众注意力。研究指出，现有的听觉基础模型未能捕捉人类选择性听觉这一特性，限制了其产生与听者意图相符的回应能力。AAD-LLM通过采用颅内脑电图(iEEG)记录，首先预测听众关注的发言者，然后基于此推测的注意状态来生成回应。在多发言者场景中评估AAD-LLM，包括发言者描述、语音转录和提问回答，结果显示该模型在客观和主观评分上均表现出更好的意图一致性。这项研究为迈向意图意识的听觉人工智能开辟了新途径，探索了让机器听觉受到听众感知影响的新范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:08 GMT</pubDate>
</item>
<item>
<title>基于难度聚类的下游性能预测框架在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2502.17262</link>
<guid>https://arxiv.org/abs/2502.17262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于难度聚类的框架，以提高大语言模型的性能预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的下游性能预测框架——基于难度聚类（COD），旨在解决大型语言模型（LLMs）训练中资源配置和性能预测的挑战。由于存在“出现现象”和任务难度分布不均等问题，现有性能预测方法面临准确性和可靠性不足的困境。COD通过根据任务难度特征聚类，构建可预测的支持子集，排除非出现和不可扩展的聚类，从而确保所选子集的分数能有效预测整体评估集的下游表现。同时，本文推导了性能指标从支持子集到全评估集的映射函数，确保了LLM下游性能的准确外推。经过应用于70B LLM的性能预测后，研究结果表明COD显著提高了预测准确性，平均绝对偏差仅为1.36%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17262" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:18:24 GMT</pubDate>
</item>
<item>
<title>SpargeAttn：通用稀疏和量化注意力机制的实现</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SpargeAttn，以加速多种模型的稀疏注意力机制。</p><br /><br /><p><strong>摘要：</strong> 本文提出了SpargeAttn，一种通用的稀疏和量化注意力机制，旨在提高大型模型的效率。由于传统注意力机制的时间复杂度为平方级别，SpargeAttn利用注意力图中的稀疏性预测注意力图，并优化矩阵乘法的计算。我们的方法包括两个阶段的在线过滤器：第一阶段快速准确地预测注意力图，跳过部分矩阵乘法；第二阶段设计了一个在线softmax感知过滤器，进一步减少计算开销。实验表明，SpargeAttn在语言、图像和视频生成等多种模型上显著加速计算，同时保留了端到端的性能指标。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:04:57 GMT</pubDate>
</item>
<item>
<title>SWE-RL：通过强化学习提升软件工程领域的大语言模型推理能力</title>
<link>https://arxiv.org/abs/2502.18449</link>
<guid>https://arxiv.org/abs/2502.18449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-RL方法提升了大型语言模型在软件工程上的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SWE-RL，这是一种首个将强化学习应用于现实世界软件工程推理的方案。尽管DeepSeek-R1重点关注编码和数学问题，SWE-RL通过使用轻量级规则奖励机制，如生成解决方案与真实解决方案间的相似度分数，使得大型语言模型（LLMs）能够自动恢复开发者的推理过程。该模型通过学习广泛的开源软件演化数据进行训练，取得了41.0%的解决率，成为中型LLM中的最佳表现，并在五个超领域任务上展现了普适的推理能力，展示了其在软件工程领域的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:03:08 GMT</pubDate>
</item>
<item>
<title>OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference</title>
<link>https://arxiv.org/abs/2502.18411</link>
<guid>https://arxiv.org/abs/2502.18411</guid>
<content:encoded><![CDATA[
Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:01:56 GMT</pubDate>
</item>
<item>
<title>匿名区域变换器（ART）：革命性的多层透明图像生成技术</title>
<link>https://arxiv.org/abs/2502.18364</link>
<guid>https://arxiv.org/abs/2502.18364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新方法ART，能高效生成多层透明图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为匿名区域变换器（ART）的新方法，能够根据全球文本提示和匿名区域布局直接生成可变多层透明图像。该方法的核心在于匿名区域布局，使生成模型能够自主决定视觉标记与文本标记的对应关系，从而与以往主导的语义布局形成鲜明对比。此外，层级区域裁剪机制能高效选择每个匿名区域的视觉标记，大幅减少注意力计算成本，且与全注意力方法相比，生成速度快12倍以上，层冲突也显著减少。值得一提的是，ART还提出了一种高质量多层透明图像自动编码器，支持透明度的直接编码和解码。这一技术为互动内容创作建立了新的范式，推动了多层图像生成的精确控制与可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:50:19 GMT</pubDate>
</item>
<item>
<title>KV-Edit：一种无训练的图像编辑背景一致性方法</title>
<link>https://arxiv.org/abs/2502.17363</link>
<guid>https://arxiv.org/abs/2502.17363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KV-Edit 提供了一种无训练的方法以提升图像编辑中的背景一致性。</p><br /><br /><p><strong>摘要：</strong> 背景一致性在图像编辑任务中仍然是一个重大挑战。尽管已有多项研究，现有方法在保持与原始图像相似性和生成符合目标内容之间仍面临权衡。本文提出了KV-Edit，这是一种利用KV缓存的无训练方法，通过保留背景标记而非重新生成背景，简化了复杂机制和训练需求，能够在用户提供的区域内生成与背景无缝结合的新内容。此外，我们还探讨了KV缓存编辑过程中的内存消耗，并通过无反转方法将空间复杂度优化至O(1)。该方法与任何基于DiT的生成模型兼容，无需额外训练。实验结果表明，KV-Edit在背景和图像质量上显著优于现有方法，甚至超越了基于训练的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:36:19 GMT</pubDate>
</item>
<item>
<title>MutaGReP: 基于变异的代码库计划搜索方法</title>
<link>https://arxiv.org/abs/2502.15872</link>
<guid>https://arxiv.org/abs/2502.15872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MutaGReP通过变异引导的方式优化代码库搜索，提升编码任务性能。</p><br /><br /><p><strong>摘要：</strong> MutaGReP（变异引导的代码库计划搜索）是一个针对如何向大型代码库中的LLM提供上下文的创新方法。与直接将整个代码库置入LLM的上下文窗口不同，MutaGReP通过执行神经树搜索，探索通过变异生成的计划，并结合符号检索器实现对代码库的自然语言化分析。这种方法在LongCodeArena基准测试中表现出色，使用不到5%的128K上下文窗口来生成计划，仍能够与填充整个代码库的GPT-4o的编码性能相媲美。同时，MutaGReP生成的计划使Qwen 2.5 Coder的32B和72B模型在面对最困难的LongCodeArena任务时，能够匹敌GPT-4o。这一研究为大规模代码库的有效利用和任务解决提供了新思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 19:35:42 GMT</pubDate>
</item>
<item>
<title>多样化输入提示下的高质量3D形状和纹理生成框架</title>
<link>https://arxiv.org/abs/2502.14247</link>
<guid>https://arxiv.org/abs/2502.14247</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于多种输入的高质量3D形状和纹理生成框架。</p><br /><br /><p><strong>摘要：</strong> 本报告提出了一种全面的框架，用于从多样化输入提示（包括单幅图像、多视角图像和文本描述）生成高质量的3D形状和纹理。框架分为两个主要部分：3D形状生成和纹理生成。3D形状生成采用变分自编码器（VAE）将隐式3D几何形状编码到潜在空间，并使用扩散网络生成受输入提示条件的潜在变量，还探索了替代的艺术创作网格生成方法，展现了对简单几何形状的良好效果。纹理生成则采用多阶段流程，从生成正面图像开始，再生成多视角图像、RGB转PBR纹理转换以及高分辨率的多视角纹理细化。在每个阶段中，嵌入一致性调度器以保证多视角纹理的像素级一致性，确保无缝集成。该管道有效处理多种输入格式，利用先进的神经网络架构和新颖的方法生成高质量的3D内容。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14247" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 17:06:48 GMT</pubDate>
</item>
<item>
<title>语音交互中的大音频模型(LAM)评估研究</title>
<link>https://arxiv.org/abs/2502.15919</link>
<guid>https://arxiv.org/abs/2502.15919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过互动评估7,500个大音频模型的用户交互，探讨用户偏好与模型性能的关联。</p><br /><br /><p><strong>摘要：</strong> 随着AI聊天机器人普及，语音交互成为高效传达语义和社交信号的重要方式。本文研究通过互动方式评估大音频模型(LAM)，收集了484名参与者的7,500次交互。利用主题建模，我们识别了音频接口的主要使用案例，并分析了用户偏好排名和定性反馈，以确定最符合用户需求的模型。研究发现，静态基准与互动性能的相关性不强，单一基准的相关性不超过0.33。尽管结合多个粗粒度特征可产生适度的预测能力（R^2=0.30），但只有两个与口语问答和年龄预测相关的数据集显示出显著的正相关性。这表明，迫切需要开发与用户偏好更好关联的大音频模型评估方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 16:46:31 GMT</pubDate>
</item>
<item>
<title>Agentic Long-Context Understanding: 提升LLMs的复杂问题回答能力</title>
<link>https://arxiv.org/abs/2502.15920</link>
<guid>https://arxiv.org/abs/2502.15920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgenticLU框架通过自我澄清和上下文获取提升了LLMs的复杂问题处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Agentic Long-Context Understanding（AgenticLU）的框架，旨在通过自我澄清和上下文获取来增强大型语言模型（LLMs）对复杂问题的理解。AgenticLU的核心是Chain-of-Clarifications（CoC），通过模型自生成澄清问题和相应的上下文基础来完善其理解。通过将推理扩展为树搜索，我们在NarrativeQA上实现了97.8%的答案召回率，搜索深度达到三，分支因子为八。为了解决高成本搜索过程的训练问题，我们利用CoC工作流获取的偏好对进行两阶段模型微调：首先，进行监督微调以学习有效分解策略；其次，进行直接偏好优化以提升推理质量。实验结果显示，AgenticLU在七个长上下文任务中显著超越了最先进的提示方法和专门的长上下文LLMs，展现出稳健的多步推理能力，并在上下文长度增加时维持一致表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:50:27 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的房地产市场营销内容自动生成框架</title>
<link>https://arxiv.org/abs/2502.16810</link>
<guid>https://arxiv.org/abs/2502.16810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于大语言模型的框架，用于自动生成房地产营销内容，符合用户偏好。</p><br /><br /><p><strong>摘要：</strong> 本文开发了一种基于大语言模型（LLMs）的自主框架，旨在自动生成具有说服力和依据的房地产营销内容，聚焦于房地产 listings 的描述。该方法旨在使生成的内容与用户偏好相一致，同时突出有用的事实属性。框架包括三个关键模块：1）模拟专家行为以预测可销售特征的基础模块；2）将内容与用户偏好对齐的个性化模块；3）确保事实准确性和包含当地特色的营销模块。通过在房地产营销领域进行系统的人体实验，结果表明，我们的方法生成的营销描述明显优于人类专家的写作。研究结果表明，该基于 LLM 的自主框架在确保使用事实进行负责任生成的同时，有望实现大规模的目标营销自动化。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:26:42 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型的归纳推理能力：InductionBench基准介绍</title>
<link>https://arxiv.org/abs/2502.15823</link>
<guid>https://arxiv.org/abs/2502.15823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在归纳推理方面的能力，提出InductionBench基准。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理方面取得了显著进展，但现有基准大多侧重于演绎推理，而归纳推理的研究较少。归纳推理是科学发现的核心，能够从观察的数据中推导出一般原则。为评估LLMs的归纳推理能力，本文介绍了InductionBench这一新基准。实验结果表明，即便是最先进的模型，在处理简单的复杂性类别时仍然难以有效掌握，揭示了当前LLMs在归纳推理能力方面的显著不足。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:58:10 GMT</pubDate>
</item>
<item>
<title>量化技术在大语言模型安全性评估中的应用</title>
<link>https://arxiv.org/abs/2502.15799</link>
<guid>https://arxiv.org/abs/2502.15799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨量化技术与大语言模型的安全性评估。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）的发展，它们在解决现代挑战和实际应用中发挥了重要作用。然而，其计算成本仍是广泛采用的一大障碍。量化技术被视为降低资源需求的有前景的解决方案，但量化模型的安全性和可信度尚未得到充分研究。为此，本文引入了OpenSafetyMini，一个新颖的开放式安全数据集，以更好地区分不同模型。此外，我们对4种先进的量化技术在LLaMA和Mistral模型上的表现进行了评估，使用4项基准测试，包括人类评估。结果显示，在4位精度下的最佳量化方法各异，而在2位精度下，向量量化技术展现了最佳的安全性和可信性表现，为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>利用机器学习分析胸部X光片预测COVID-19病程严重性</title>
<link>https://arxiv.org/abs/2502.16622</link>
<guid>https://arxiv.org/abs/2502.16622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用机器学习预测COVID-19患者病程严重性，取得显著效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了如何通过机器学习技术，特别是使用胸部X光片（CXR），来缓解COVID-19大流行中医护工作者的压力。我们整合了三种来源的数据，构建了一个大规模的COVID严重性数据集，并评估了转移学习在病情严重性回归和分类任务中的有效性。结果显示，预训练的DenseNet161模型在三类严重性预测中表现最佳，总体准确率达到80%；在轻度、中度和重度病例中的准确率分别为77.3%、83.9%和70%。同时，使用视图变换器（ViT）进行回归预测的平均绝对误差为0.5676，优于放射科医生的预测结果。项目源代码已公开，便于进一步研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:02:34 GMT</pubDate>
</item>
<item>
<title>提高机器翻译质量估计效率的模型</title>
<link>https://arxiv.org/abs/2502.14429</link>
<guid>https://arxiv.org/abs/2502.14429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出高效的质量估计模型，降低评估成本并减少性能损失。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器翻译质量估计的两个挑战进行研究：降低大规模质量估计的计算成本，以及开发一种经济的不确定性估计方法。我们提出Instant Confidence COMET模型，该模型在成本大幅降低的同时，性能与之前的高成本方法相当。进一步发展为Early-Exit COMET，这个模型能够在早期层级计算质量分数及其置信度，从而实现早期退出计算，降低评估成本。此外，我们将该模型应用于机器翻译的重排序任务，与上置信界限算法结合，能够在不对所有候选运行完整评估模型的情况下，从大池中找到最佳候选。通过这两种方法（评估和重排序），我们的模型在计算需求上减少了50%，且性能损失极小。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:17:04 GMT</pubDate>
</item>
<item>
<title>MegaLoc：多任务图像检索模型的研究</title>
<link>https://arxiv.org/abs/2502.17237</link>
<guid>https://arxiv.org/abs/2502.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MegaLoc模型在多个计算机视觉任务上展现出优异性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MegaLoc的图像检索模型，旨在解决在视觉场所识别、地标检索、视觉定位、3D重建和SLAM等多种计算机视觉任务中的图像检索问题。之前的解决方案通常针对特定任务，面对稍有变化的需求或外部数据时往往会失效。MegaLoc结合了多种现有方法、训练技术和数据集，取得了显著的效果。研究结果表明，MegaLoc在多个视觉场所识别数据集上达到了最新的最佳表现，且在常见的地标检索数据集上也表现优异。此外，在LaMAR数据集的视觉定位任务中，仅通过修改检索方法，MegaLoc创造了新的最佳结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:00:19 GMT</pubDate>
</item>
<item>
<title>TAME Agent Framework：构建去中心化层次多智能体系统</title>
<link>https://arxiv.org/abs/2502.15425</link>
<guid>https://arxiv.org/abs/2502.15425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAME Agent Framework提升了多智能体系统的可扩展性与适应性。</p><br /><br /><p><strong>摘要：</strong> TAME Agent Framework (TAG) 是一个新颖的去中心化层次多智能体系统构建框架，旨在克服现有层次强化学习方法的局限性，如仅限于双层结构或依赖集中式训练。TAG引入了一种名为LevelEnv的概念，将每一层的环境抽象化，从而支持任意深度的层次结构，标准化不同层之间的信息流，同时保持松耦合。通过实现不同类型的强化学习智能体在多个层级的组合，TAG在标准基准上实现了显著优于传统多智能体强化学习基线的性能，显示出去中心化层次组织在学习速度和最终性能提升方面的有效性，展示了TAG作为可扩展多智能体系统的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:51:02 GMT</pubDate>
</item>
<item>
<title>Stable-SPAM: 提高4位训练稳定性的优化器</title>
<link>https://arxiv.org/abs/2502.17055</link>
<guid>https://arxiv.org/abs/2502.17055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Stable-SPAM优化器，有效提升4位训练的梯度稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文全面评估了几种最近提出的4位训练优化器，指出低位精度加剧了对学习率的敏感性，且常常导致梯度范数不稳定，在较高学习率下会出现发散现象。其中，SPAM优化器实现了较好的性能，但在梯度范数稳定性上仍然存在困难。为解决这些问题，本文提出了Stable-SPAM，其通过增强的梯度归一化和剪切技术来稳定梯度。具体来说，Stable-SPAM 采用历史最大值自适应更新尖峰梯度的剪切阈值，基于历史l_2-范数统计对整个梯度矩阵进行归一化，并继承SPAM的动量重置策略，定期重置Adam的第一和第二动量，从而减少尖峰梯度的累积。大量实验表明，Stable-SPAM在4位大语言模型训练中显著稳定了梯度范数，性能优于Adam和SPAM。尤其是，使用Stable-SPAM训练的4位LLaMA-1B模型，在困惑度上比使用Adam训练的BF16 LLaMA-1B高出2点，同时在4位训练时，Stable-SPAM与Adam模型在损失上达成一致，而训练步骤数量仅为后者的一半。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:40:40 GMT</pubDate>
</item>
<item>
<title>社交媒体上信息获取与社区审核的互动研究</title>
<link>https://arxiv.org/abs/2502.14132</link>
<guid>https://arxiv.org/abs/2502.14132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明社区审核依赖于专业事实核查以对抗虚假信息。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了社交媒体上两种常用的抗击虚假信息策略：专业机构的事实核查与社区用户的内容审核。近期Twitter/X和Meta的政策变化显示，逐渐减少与事实核查组织的合作，转而依赖众包的社区备注。通过使用语言模型对大量Twitter/X社区备注进行标注，分析显示，社区备注引用事实核查来源的频率比以前报告的高出五倍，尤其是与更广泛虚假信息叙事关联的帖子，其备注引用事实核查来源的概率是其他来源的两倍。结果表明，成功的社区审核在很大程度上依赖于专业的事实核查。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14132" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:11:18 GMT</pubDate>
</item>
<item>
<title>布朗球面的连续CVS双射逆过程</title>
<link>https://arxiv.org/abs/2502.13074</link>
<guid>https://arxiv.org/abs/2502.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨布朗球面的CVS双射的逆过程及布朗蛇的构造。</p><br /><br /><p><strong>摘要：</strong> 布朗球面是一种随机度量空间，与二维球面同胚，作为多种随机平面图的普遍尺度极限而出现。其直接构造是通过连续的Cori-Vauquelin-Schaeffer (CVS)双射，该双射将标记树映射到平面图中，连续版本将Aldous的连续随机树（布朗蛇）与布朗球面关联。本文详细描述了连续CVS双射的逆过程，通过构造布朗蛇作为布朗球面的可测函数，强调了在处理布朗球面的方向时所需的特殊注意。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:03:39 GMT</pubDate>
</item>
<item>
<title>M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</title>
<link>https://arxiv.org/abs/2502.15167</link>
<guid>https://arxiv.org/abs/2502.15167</guid>
<content:encoded><![CDATA[
The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 03:36:50 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的通用颜色一致性方法GCC</title>
<link>https://arxiv.org/abs/2502.17435</link>
<guid>https://arxiv.org/abs/2502.17435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GCC方法通过扩散模型提升颜色一致性，适应不同相机传感器。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法GCC，旨在解决颜色一致性方法在不同相机传感器下泛化能力不足的问题。GCC利用扩散模型对图像中的颜色检查器进行重绘，以估算光照，主要创新包括：1) 单步确定性推断方法，能够重绘反映场景光照的颜色检查器；2) 拉普拉斯分解技术，保持检查器结构，同时允许基于光照的颜色适应；3) 基于遮罩的数据增强策略，用于处理不准确的颜色检查器标注。GCC在跨相机场景中表现出卓越的鲁棒性，在双向评估中达到了最优的25%误差率，分别为5.15°和4.32°，体现了该方法在不同相机特性下的稳定性和泛化能力，无需特定传感器的训练，适用于真实世界应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17435" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 02:06:00 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型在复杂机器人操作中的物理推理能力</title>
<link>https://arxiv.org/abs/2502.16707</link>
<guid>https://arxiv.org/abs/2502.16707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种提升视觉语言模型物理推理的框架，以改善多阶段机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的测试时间计算框架，以增强视觉语言模型(VLM)在多阶段机器人操作任务中的物理推理能力。当前的VLM在处理复杂的物理操作和长时间范围的推理时存在不足。我们的方法通过引入“反思”机制，逐步改进预训练的VLM：它利用生成模型想象未来的世界状态，利用这些预测来指导动作选择，并反思潜在的次优决策以优化推理。实验结果表明，该方法显著优于多种先进的商用VLM以及其他后训练方法，例如蒙特卡洛树搜索(MCTS)。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 01:02:05 GMT</pubDate>
</item>
<item>
<title>MONSTER：针对时间序列分类的大型数据集评估库</title>
<link>https://arxiv.org/abs/2502.15122</link>
<guid>https://arxiv.org/abs/2502.15122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MONSTER提供了大型数据集以推动时间序列分类领域的发展。</p><br /><br /><p><strong>摘要：</strong> MONSTER，即MONash可扩展时间序列评估库，是一个专为时间序列分类设计的大型数据集集合。现有的UCR和UEA时间序列分类库的基准测试虽然在该领域已有贡献，但其数据集规模较小，分别只有217和255个示例，限制了模型的多样性。MONSTER旨在通过引入更大规模的数据集来拓宽该领域的研究，使得研究人员在面对更大的数据时能够有效学习，从而在理论和实践上推动时间序列分类的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:37:53 GMT</pubDate>
</item>
<item>
<title>X-Dancer：基于音乐驱动的零样本人类舞蹈视频生成新方法</title>
<link>https://arxiv.org/abs/2502.17414</link>
<guid>https://arxiv.org/abs/2502.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Dancer通过静态图像生成多样化的舞蹈视频，提升真实感和表现力。</p><br /><br /><p><strong>摘要：</strong> X-Dancer是一种创新的零样本音乐驱动图像动画管道，能够从单一静态图像生成多样且真实感强的人类舞蹈视频。其核心是一个统一的变换器-扩散框架，使用自回归变换器模型生成与音乐同步的2D舞蹈姿态序列，指导扩散模型生成连贯且真实的舞蹈视频帧。与传统的3D人类动作生成方法不同，X-Dancer通过建模广泛的2D舞蹈动作，克服了数据限制，增强了可扩展性，能够捕捉细微的动作与音乐节拍的对齐。这一过程涉及构建空间组成的令牌表示，从与关键点置信度相关的2D人类姿态标签中编码大规模的身体运动及细微动作。X-Dancer的实验结果显示，其在多样性、表现力和真实感方面显著超越了最先进的技术，展示了强大的实用性和应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:17:51 GMT</pubDate>
</item>
<item>
<title>VideoGrain：实现细粒度视频编辑的零-shot方法</title>
<link>https://arxiv.org/abs/2502.17258</link>
<guid>https://arxiv.org/abs/2502.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍VideoGrain，一种实现细粒度视频编辑的新方法。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型的进步，视频生成和编辑能力显著提高，但多粒度视频编辑仍面临挑战。主要难点包括文本到区域控制的语义不匹配和扩散模型内部特征耦合。为解决这些问题，我们提出了VideoGrain，一种零-shot方法，通过调节时空注意力机制来实现视频内容的细粒度控制。该方法通过增强每个局部提示对其对应空间解耦区域的关注，同时最小化与无关区域的交互，来改善文本到区域的控制。此外，通过提高区域内部注意力和降低区域间干扰，来增强特征分离。通过大量实验证明，我们的方案在实际场景中实现了行业领先的性能。相关代码、数据及演示可在其项目页面找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:13:12 GMT</pubDate>
</item>
<item>
<title>RIFLEx：高效视频生成的频率成分分析与应用</title>
<link>https://arxiv.org/abs/2502.15894</link>
<guid>https://arxiv.org/abs/2502.15894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RIFLEx通过频率成分分析提升视频生成质量与时长扩展能力。</p><br /><br /><p><strong>摘要：</strong> 近年来的视频生成技术已能合成高质量的一分钟视频，但生成更长时间的视频并保持时间一致性仍面临重大挑战。现有的长度外推方法常导致时间重复或运动减速。本文系统分析位置嵌入中的频率成分，发现主导外推行为的内在频率。基于此洞察，提出RIFLEx，这是一种简约而有效的方法，通过减少内在频率来抑制重复，同时保持运动一致性，无需额外修改。RIFLEx实现了在最先进的视频扩散变换器上以训练无关方式进行高质量的2倍外推，并通过最少的微调提升质量，实现3倍外推而无需长视频。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:09:04 GMT</pubDate>
</item>
<item>
<title>多语种数学基准测试中的测试时间扩展方法研究</title>
<link>https://arxiv.org/abs/2502.17407</link>
<guid>https://arxiv.org/abs/2502.17407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究测试时间扩展如何影响多语种大模型的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了测试时间扩展在多语种大模型中的有效性，推出了多语种数学基准测试（MCLM），涵盖55种语言的竞赛级问题。我们对三种测试时间扩展方法进行了实验：结果奖励建模（ORM）、过程奖励建模（ORM）和预算强制（BF），应用于Qwen2.5-1.5B Math和我们训练的多语种大模型MR1-1.5B。实验结果显示，Qwen2.5-1.5B Math与ORM结合在MCLM上获得了35.8的分数，而MR1-1.5B与BF结合则获得了35.2的分数。尽管“思考型大模型”受到广泛关注，但在与传统扩展方法（如最佳N法）在推理FLOPs水平相近的情况下，它们的表现相当。此外，虽然BF方法在英语AIME上提高了20分，但在其他语言的平均增益仅为1.94分，表明测试时间扩展在多语种任务中的泛化能力有限。为推动进一步的研究，我们发布了MCLM、MR1-1.5B及其评估结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:37:53 GMT</pubDate>
</item>
<item>
<title>开放权重模型影响力演变框架研究</title>
<link>https://arxiv.org/abs/2502.15987</link>
<guid>https://arxiv.org/abs/2502.15987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架量化开放权重模型的影响力演变。</p><br /><br /><p><strong>摘要：</strong> 随着开放权重AI模型的快速发展，对哪些模型将推动创新及塑造AI生态系统的预测变得愈发重要。本文提出了一种基于引文动态的框架，旨在量化开放权重模型影响力的演变。我们借鉴Wang等人提出的科学引文模型，采用了即时性、持久性和相对适应性三个关键参数，以追踪开放权重模型的累积微调模型数量。研究发现，这种引文式的方法能够有效捕捉开放权重模型采用的不同轨迹，大部分模型的适配性良好，而异常值则显示了使用模式的独特性或突增情况。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:30:36 GMT</pubDate>
</item>
<item>
<title>Audio-FLAN: 统一音频理解与生成的指令调优数据集</title>
<link>https://arxiv.org/abs/2502.16584</link>
<guid>https://arxiv.org/abs/2502.16584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Audio-FLAN数据集促进音频理解与生成的统一建模。</p><br /><br /><p><strong>摘要：</strong> Recent advancements in audio tokenization have significantly improved the integration of audio capabilities into large language models (LLMs). 在音频理解和生成任务之间仍存在区分，限制了统一音频语言模型的发展。虽然指令调优在文本和视觉领域展现了卓越的泛化能力与零样本学习，但在音频领域的应用依然缺乏探索。为了解决这一问题，Audio-FLAN作为一个大规模指令调优数据集诞生，涵盖了80种跨越语音、音乐和声音领域的多样任务，包含超过1亿个实例。Audio-FLAN为统一音频语言模型奠定了基础，能够在零样本场景下无缝处理理解（如转录、理解）和生成（如语音、音乐、声音）任务。该数据集已在HuggingFace和GitHub上发布并将不断更新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:20 GMT</pubDate>
</item>
<item>
<title>Slam：24小时内在单一GPU上训练高质量语言模型的Recipe</title>
<link>https://arxiv.org/abs/2502.15814</link>
<guid>https://arxiv.org/abs/2502.15814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Slam是提升单GPU训练高质量语言模型的有效方案。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Slam的有效方案，可在单一学术GPU上于24小时内训练高质量的语言模型（SLM）。通过对模型初始化、架构、合成训练数据的偏好优化及其他组件的调试，本文进行了实证分析。研究表明，该训练方案具备良好的可扩展性，能够以更低的计算成本实现与领先SLM相媲美的结果。此外，在SLM扩展法则的背景下，研究结果远超计算最优性能预测，为SLM训练的可行性带来了乐观的前景。相关代码、数据、模型和样本可在指定网址查阅。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:12 GMT</pubDate>
</item>
<item>
<title>CTM基准：评估语言模型的时间推理能力</title>
<link>https://arxiv.org/abs/2502.16922</link>
<guid>https://arxiv.org/abs/2502.16922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CTM基准通过丰富的历史语境评估语言模型的时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 时间推理是人类认知的重要组成部分，对许多现实世界应用至关重要。尽管大型语言模型在时间推理方面取得了一些进展，但现有基准多依赖规则构建，缺乏上下文深度，且涉及的时间实体范围有限。为解决这些问题，我们推出了中文时间推理(CTM)基准，旨在评估语言模型在中国历史年代学的广泛背景下的时间推理能力。CTM强调跨实体关系、对时间的成对对齐，以及上下文化和文化根植的推理，提供了一个全面的评估。大量实验结果揭示了CTM所带来的挑战，并突出了改进的潜在方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:48:30 GMT</pubDate>
</item>
<item>
<title>DICEPTION：一种高效的通用视觉感知模型</title>
<link>https://arxiv.org/abs/2502.17157</link>
<guid>https://arxiv.org/abs/2502.17157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICEPTION是一种高效的视觉感知模型，能在低资源下完成多任务。</p><br /><br /><p><strong>摘要：</strong> DICEPTION是一个旨在创建高效通用视觉感知模型的研究，其目标是在有限的计算资源和训练数据上，运用经过数十亿图像预训练的文本到图像扩散模型。研究显示，DICEPTION在多个视觉感知任务上达到与最先进模型相当的效果，仅使用了0.06%的数据（600K对比1B像素级标注图像）。该模型采用颜色编码策略，将各类任务的输出进行统一，证明了为不同实例赋予随机颜色的策略在实体和语义分割中极为有效。此外，DICEPTION整合了各种视觉任务为条件图像生成，充分利用预训练的文本到图像模型，从而在训练成本上大幅降低。调整此模型到其他任务时，仅需在少量图像（如50张）和1%的参数上进行微调。DICEPTION为视觉通用模型提供了有效的解决方案和重要的洞察。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:39:29 GMT</pubDate>
</item>
<item>
<title>GOAT：一种提升LoRA性能的新型专家模型框架</title>
<link>https://arxiv.org/abs/2502.16894</link>
<guid>https://arxiv.org/abs/2502.16894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GOAT框架通过SVD结构改善LoRA在大规模语言模型中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管低秩适应(LoRA)为大规模语言模型的高效微调提供了可能，但其性能往往不及全量微调(Full FT)。目前的方法通过使用静态奇异值分解(SVD)子集来优化LOra，但这限制了对预训练知识的有效利用。为此，我们提出了大规模LoRA混合专家(GOAT)框架，该框架通过自适应整合相关先验和采用SVD结构的混合专家模型，解决了权重不匹配和复杂梯度动态的问题。此外，本研究还通过推导理论缩放因子，使优化与全量微调的混合专家模型对齐。我们的实验涵盖了25个数据集，包括自然语言理解、常识推理、图像分类和自然语言生成，结果表明GOAT在性能上已接近全量微调，展示了其卓越的效率与效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:35:41 GMT</pubDate>
</item>
<item>
<title>Mobile-Agent-V：基于视频指导的移动自动化框架</title>
<link>https://arxiv.org/abs/2502.17110</link>
<guid>https://arxiv.org/abs/2502.17110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobile-Agent-V框架通过视频指导提升移动设备任务管理效率。</p><br /><br /><p><strong>摘要：</strong> 随着移动设备使用的快速增长，提升任务管理的自动化水平显得尤为重要。然而，许多基于AI的框架由于缺乏充分的操作知识而面临挑战。虽然手动编写的知识可以帮助解决这一问题，但过程既繁琐又低效。为了解决这些问题，我们提出了Mobile-Agent-V框架，该框架利用视频指导提供丰富且具有成本效益的操作知识，从而促进移动自动化的发展。Mobile-Agent-V通过视频输入增强任务执行能力，无需专门的采样或预处理。同时，该框架集成了滑动窗口策略，并引入视频代理和深度反思代理，确保用户指令与执行动作保持一致。实验结果表明，相较于现有框架，Mobile-Agent-V实现了30%的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17110" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:31:17 GMT</pubDate>
</item>
<item>
<title>长上下文对大型语言模型的影响与挑战</title>
<link>https://arxiv.org/abs/2502.17129</link>
<guid>https://arxiv.org/abs/2502.17129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了长上下文对大型语言模型的影响、挑战及研究前景。</p><br /><br /><p><strong>摘要：</strong> 长上下文是自然语言处理领域的重要主题，为大型语言模型（LLMs）的发展提供了巨大机会，同时也伴随着诸多挑战。近年来，LLMs的上下文长度突破扩展至数百万个词元，研究从长度外推扩展至架构、基础设施、训练和评估等多方面。本文通过与人类超越有限性的比喻，探讨了LLMs在追求长上下文与面对其有限性之间的挣扎。我们全面展示了长上下文LLMs的生命周期，包括架构、基础设施、训练和评估等四个方面，并展示与之相关的技术全景。最后，本文提出了目前长上下文LLMs所面临的10个未解问题，希望为相关研究提供系统性的介绍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:27:11 GMT</pubDate>
</item>
<item>
<title>CodeCriticBench：全面评估大型语言模型的代码批判能力</title>
<link>https://arxiv.org/abs/2502.16614</link>
<guid>https://arxiv.org/abs/2502.16614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍CodeCriticBench，一个评估LLMs代码批判能力的全新基准。</p><br /><br /><p><strong>摘要：</strong> 本文引入了CodeCriticBench，一个针对大型语言模型(LLMs)的全面代码批判基准，旨在解决现有基准在代码任务评估中的局限性。现有基准通常仅关注一般领域的多样推理任务，对代码任务的评估则显不足，且缺乏不同维度的全面评价。CodeCriticBench覆盖了两个主要的代码任务：代码生成和代码问答，且根据难度进行了分类。评估协议包括基础的批判评价和针对不同特征的高级批判评价，为后者设计了细致的评估清单。通过对现有LLMs进行广泛实验，结果表明CodeCriticBench的有效性，为代码批判能力的系统评估提供了新的思路和工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:17:28 GMT</pubDate>
</item>
<item>
<title>多模态不一致性推理基准的建立与评估</title>
<link>https://arxiv.org/abs/2502.16033</link>
<guid>https://arxiv.org/abs/2502.16033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出多模态不一致性推理基准评估大型语言模型对现实内容的不一致性处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了多模态不一致性推理（MMIR）基准，旨在评估现有大型多模态语言模型（MLLMs）在处理真实世界布局丰富内容时的能力。基准包含534个具有挑战性的样本，涉及事实矛盾、身份误归、上下文不匹配、数量差异及时间/空间不一致等五个推理类别。经过评估，具备专门多模态推理能力的模型表现优异，而开源模型对不一致性错误尤为敏感。详细的错误分析显示，模型在检测单一模态中的不一致性时表现良好，特别是在文本中，但在跨模态冲突和复杂布局方面存在挑战。探测实验表明，单模态提示（如链式思维和集合标记方法）带来的改进微乎其微，揭示了跨模态推理中的瓶颈问题。这些发现强调了提升多模态推理能力的必要性，并指向未来的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:50 GMT</pubDate>
</item>
<item>
<title>生成性人工智能系统发布及其接入性分析</title>
<link>https://arxiv.org/abs/2502.16701</link>
<guid>https://arxiv.org/abs/2502.16701</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章分析了生成性AI系统的发布及接入性对用户影响的多维度考量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨生成性人工智能系统的发布决策对用户和利益相关者参与系统的影响，指出发布并未涵盖影响接入的诸多因素。作者将接入性分解为资源配置、技术可用性和实际效用三个维度，分析各种系统组件的权衡。通过对四种高性能语言模型的比较，揭示了开放式与闭源模型在接入性方面的共同性。接入变量为用户扩展和增加接入奠定基础，研究还考察了接入规模如何影响风险管理能力。此框架为系统发布决策、研究和政策制定提供了更全面的风险收益权衡视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16701" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:15 GMT</pubDate>
</item>
<item>
<title>Seq2Exp：精准预测基因表达的序列转表达网络</title>
<link>https://arxiv.org/abs/2502.13991</link>
<guid>https://arxiv.org/abs/2502.13991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍Seq2Exp，一个用于预测基因表达的创新网络。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了从DNA序列预测基因表达的问题，尤其是寻找控制基因表达的调控元件。我们提出了Seq2Exp，一种专门设计的序列转表达网络，旨在识别并提取驱动目标基因表达的调控元件，从而提高基因表达预测的准确度。Seq2Exp能够捕捉表观基因组信号、DNA序列及其相关调控元件之间的因果关系。具体而言，我们通过条件化因果活跃的调控元件对表观基因组信号和DNA序列进行分解，结合信息瓶颈和Beta分布来合并它们的效应，同时过滤非因果成分。实验结果表明，Seq2Exp在基因表达预测任务中优于现有基线方法，并能够发现比MACS3等常用统计方法在峰值检测中更具影响力的区域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13991" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 17:06:56 GMT</pubDate>
</item>
<item>
<title>RareScale：结合语言模型与专家系统的罕见疾病诊断方法</title>
<link>https://arxiv.org/abs/2502.15069</link>
<guid>https://arxiv.org/abs/2502.15069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RareScale，提升罕见疾病的诊断准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RareScale，一个结合大语言模型（LLMs）与专家系统的新方法，以提高罕见疾病的识别能力。随着LLMs在医疗领域的应用日益增多，识别罕见疾病的能力显得尤为重要。RareScale通过模拟罕见疾病对话的数据训练罕见疾病候选预测模型，并将这些候选结果作为额外输入，集成到黑箱LLMs中，以实现最终的鉴别诊断。实验结果表明，RareScale在575种罕见疾病的诊断中，较传统黑箱LLMs的表现提升了超过17%的Top-5准确率，且在候选生成性能上也表现出色，达到88.8%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 15:59:07 GMT</pubDate>
</item>
<item>
<title>利用Tree-of-Debate框架提升科学文献评估的创新性辩论</title>
<link>https://arxiv.org/abs/2502.14767</link>
<guid>https://arxiv.org/abs/2502.14767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tree-of-Debate框架提升了科学文献的创新性辩论和评价能力。</p><br /><br /><p><strong>摘要：</strong> 随着现代技术的迅速发展，科学研究的成果日益分散，特别是在不同研究领域之间，评估其重要性和创新性变得愈加困难。为此，本文提出了Tree-of-Debate（ToD）框架，该框架利用大型语言模型（LLMs）将科学论文转化为具有不同观点的辩论角色。这一创新旨在强调结构化和批判性推理，而不仅仅是结果导向。ToD动态构建辩论树，允许对独立的创新论点进行细致分析。通过对多个领域的科学文献进行实验，并由专家学者进行评估，结果表明，ToD能够生成具有信息量的论点，有效对比不同论文，辅助研究人员在文献综述中的工作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:53:11 GMT</pubDate>
</item>
<item>
<title>大语言模型在联合国决策中的应用研究</title>
<link>https://arxiv.org/abs/2502.14122</link>
<guid>https://arxiv.org/abs/2502.14122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨大语言模型在联合国高风险政治决策中的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大语言模型（LLMs）在联合国（UN）决策过程中的应用，填补了该领域的研究空白。文章引入了一个新数据集，涵盖1994至2024年的联合国安全理事会（UNSC）记录，包括草案、投票记录和外交演讲，并提出首个全面评估LLMs的基准——联合国基准（UNBench）。该基准包含四个互相关联的政治科学任务，涉及联合国决策过程的三个阶段：起草、投票和讨论。通过实证分析，本文展示了LLMs在这一领域的潜力与挑战，提供了对其在政治科学中优势与局限性的深入见解。此外，该研究为AI与政治科学的交叉点贡献了新的研究方向和实际应用，UNBench数据集可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:18:28 GMT</pubDate>
</item>
<item>
<title>无调优身份保护文本到视频生成的新框架FantasyID</title>
<link>https://arxiv.org/abs/2502.13995</link>
<guid>https://arxiv.org/abs/2502.13995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种无调优的身份保护文本到视频生成框架FantasyID。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的无调优身份保护文本到视频生成框架FantasyID，旨在提高生成视频的面部动态和身份保持能力。该框架通过增强已经预训练的视频模型的面部知识，引入3D面部几何先验，确保视频合成过程中的面部结构合理性。此外，采用多视角面部增强策略以捕捉多样的2D面部外观特征，从而增强面部表情和头部姿势的动态性。在融合2D和3D特征时，本文采用了一种可学习的层感知自适应机制，选择性地将融合特征注入到每个DiT层，平衡身份保持与运动动态的建模。实验结果表明，FantasyID在当前无调优身份保护文本到视频生成方法中具有显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 11:28:12 GMT</pubDate>
</item>
<item>
<title>MedHallu：检测医疗领域大语言模型幻觉的新基准</title>
<link>https://arxiv.org/abs/2502.14302</link>
<guid>https://arxiv.org/abs/2502.14302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍MedHallu基准，评估医疗领域大语言模型的幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在医学问答中的使用日益增加，对其可靠性的严格评估变得至关重要。幻觉现象，即模型生成看似合理但实际上错误的输出，对患者安全及临床决策构成严重风险。为此，本文提出了MedHallu，这是首个专门设计用于医疗幻觉检测的基准，包含从PubMedQA衍生的10,000个高质量问答对，并通过控制流程系统生成幻觉回答。实验表明，最先进的LLMs，如GPT-4o、Llama-3.1和经过医学微调的UltraMedical，在这一二元幻觉检测任务上表现不佳，最佳模型在检测“难”类别幻觉时F1分数仅为0.625。通过双向蕴含聚类，我们发现更难检测的幻觉在语义上更接近真实答案。进一步的实验还表明，结合领域特定知识并引入“未确定”作为答案类别之一，可以将精度和F1分数相对于基线提高最多38%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:54:53 GMT</pubDate>
</item>
<item>
<title>多语言风格嵌入模型mStyleDistance的介绍与应用</title>
<link>https://arxiv.org/abs/2502.15168</link>
<guid>https://arxiv.org/abs/2502.15168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mStyleDistance是一个多语言风格嵌入模型，超越了现有的单语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多语言风格嵌入模型mStyleDistance，该模型利用合成数据和对比学习进行训练，支持九种语言。通过创建多语言STEL-or-Content基准，评估嵌入的质量，并在涉及不同语言的作者验证任务中应用这些嵌入。结果表明，mStyleDistance的嵌入在多语言风格基准上表现优越，且能够很好地泛化到未见特征和语言。该模型已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:33:37 GMT</pubDate>
</item>
<item>
<title>EgoSpeak：实时语音启动预测的创新框架</title>
<link>https://arxiv.org/abs/2502.14892</link>
<guid>https://arxiv.org/abs/2502.14892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoSpeak框架提升了对话代理在真实环境中的语音启动预测能力。</p><br /><br /><p><strong>摘要：</strong> EgoSpeak是一个新颖的框架，旨在解决对话代理在真实环境中语音启动时机的预测挑战。该框架从说话者的第一人称视角建模对话，能够实现类人交互，允许对话代理在不断观察环境的同时动态决定何时发言。EgoSpeak整合了四个关键功能：第一人称视角、RGB处理、在线处理和未剪辑视频处理，弥合了简化实验设置与复杂自然对话之间的差距。此外，我们推出了YT-Conversation，这是一个来自YouTube的丰富多样的对话视频集合，作为大规模预训练的资源。通过在EasyCom和Ego4D上的实验，EgoSpeak在实时预测方面优于随机及基于静默的基线，结果还强调了多模态输入和上下文长度在决定何时发言中的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 08:37:35 GMT</pubDate>
</item>
<item>
<title>针对用户特定安全标准的LLM安全性评估新基准</title>
<link>https://arxiv.org/abs/2502.15086</link>
<guid>https://arxiv.org/abs/2502.15086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出了U-SAFEBENCH以评估用户特定的LLM安全性。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLM）代理的广泛使用，其安全漏洞日益明显。尽管现有的安全评估基准主要依赖于通用标准，但这些标准未能考虑用户特定的需求，导致LLM在满足个体用户安全标准时表现不佳。为解决这一问题，本文首次提出U-SAFEBENCH基准，用于评估LLM的用户特定安全性。通过对18个广泛使用的LLM进行评估，我们发现当前的LLM在考虑用户特定安全标准时并未能有效确保安全。此外，我们基于链式思维提出了一种简单的改进方法，展示其在提升用户特定安全性方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:41:08 GMT</pubDate>
</item>
<item>
<title>WHAC框架：精确恢复人类模型与相机轨迹</title>
<link>https://arxiv.org/abs/2403.12959</link>
<guid>https://arxiv.org/abs/2403.12959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WHAC框架，实现了准确的人类模型与相机姿态恢复。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的框架WHAC，旨在从单目视频中准确恢复世界坐标系中的人类模型（SMPL-X）及相机姿态。通过结合世界、人体和相机三者的关键作用，我们着重于两个观察：相机帧下的SMPL-X估计方法可以直接恢复人类的绝对深度，而人类运动本身提供了空间线索。我们的框架不依赖传统优化技术，并且我们还创建了新的合成数据集WHAC-A-Mole，包含准确标注的人类和相机，展示了各种互动人类动作及真实的相机轨迹。实验结果显示，在标准和新建立的基准上，我们的方法具有显著的优越性和有效性，代码和数据集将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2403.12959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:37:26 GMT</pubDate>
</item>
<item>
<title>Evaluating Multimodal Generative AI with Korean Educational Standards</title>
<link>https://arxiv.org/abs/2502.15422</link>
<guid>https://arxiv.org/abs/2502.15422</guid>
<content:encoded><![CDATA[
This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:58:46 GMT</pubDate>
</item>
<item>
<title>大型语言模型情感边界处理评估框架的开放源代码基准</title>
<link>https://arxiv.org/abs/2502.14975</link>
<guid>https://arxiv.org/abs/2502.14975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们开发了一个框架以评估LLMs的情感边界处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种开放源代码的基准评估框架，用于评估大型语言模型（LLMs）在情感边界处理方面的能力。通过使用涵盖六种语言的1156条提示数据集，我们对三种主流LLMs（GPT-4o、Claude-3.5 Sonnet和Mistral-large）进行了评估，量化分析了它们在通过七种关键模式进行响应时的表现，包括直接拒绝、道歉、解释、转移、认可、设定边界和情感意识。结果显示，Claude-3.5在总体评分（8.69/10）上表现最佳，且其平均响应长度（86.51字）更长、更细致。分析还发现，英语交互的平均得分（25.62）显著高于非英语交互（< 0.22），英语响应中的拒绝率也显著高于非英语（43.20%对< 1%）。本研究揭示了模型特有的策略和局限性，提出未来可探讨更细致的评分方法、扩展语言覆盖范围及探究文化差异。我们的基准和方法论为LLM情感智能及边界设定能力的系统评估奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14975" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:44:41 GMT</pubDate>
</item>
<item>
<title>KITAB-Bench: 阿拉伯语OCR性能的新基准与挑战</title>
<link>https://arxiv.org/abs/2502.14949</link>
<guid>https://arxiv.org/abs/2502.14949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KITAB-Bench为阿拉伯语OCR提供全面基准，展示了新模型的优势与现有技术的局限。</p><br /><br /><p><strong>摘要：</strong> 随着检索增强生成技术(RAG)在文档处理中的广泛应用，文本识别的鲁棒性对知识提取至关重要。阿拉伯语OCR因其独特的连写脚本和复杂的排版特征面临更多挑战。为此，我们提出了KITAB-Bench，一个涵盖8809个样本的全面阿拉伯语OCR基准，涉及9个主要领域和36个子领域，包括手写文本和结构化表格等多种文档类型。研究表明，现代视觉语言模型如GPT-4和Gemini在字符错误率(CER)上相比传统OCR方法平均提高了60%。尽管如此，我们也指出了当前阿拉伯OCR模型的重大局限性，特别是在PDF到Markdown转换中，表现最好的模型Gemini-2.0-Flash准确率仅为65%。这些发现突显了在识别阿拉伯文本时的诸多挑战，如复杂字体、数字识别错误与表格结构检测等。本研究建立了一个严格的评估框架，可推动阿拉伯文档分析方法的改进，并缩小与英语OCR技术之间的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:43:47 GMT</pubDate>
</item>
<item>
<title>快速高质量蛋白质骨架生成的ReQFlow方法</title>
<link>https://arxiv.org/abs/2502.14637</link>
<guid>https://arxiv.org/abs/2502.14637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReQFlow方法，实现高效的蛋白质骨架生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的矩阵流匹配方法ReQFlow，用于快速且高质量的蛋白质骨架生成。该方法通过从随机噪声中产生局部平移和三维旋转，利用单位四元数表示每个残基的三维旋转，并通过球形线性插值（SLERP）构建其流。模型经过改进的四元数流（QFlow）匹配训练，保证数值稳定性，并加速推理，提升生成蛋白质骨架的设计性。实验结果表明，ReQFlow在蛋白质骨架生成上达到了领先性能，同时采样步骤显著减少，推理时间大幅降低，例如生成长度为300的骨架时，速度比RFDiffusion快37倍，比Genie2快62倍，展示了其有效性与效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:35:44 GMT</pubDate>
</item>
<item>
<title>创新的混合块注意力机制提升长上下文任务效率</title>
<link>https://arxiv.org/abs/2502.13189</link>
<guid>https://arxiv.org/abs/2502.13189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出混合块注意力（MoBA），优化长上下文任务中的计算效率。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能（AGI）领域的发展，有效扩大大型语言模型（LLMs）的上下文长度变得至关重要。然而，传统注意力机制在计算复杂度上的二次增长造成了巨大的负担。现有方法要么采用强偏置结构，如池化或窗口注意力，这些通常是任务特定的；要么对注意力机制进行根本性修改，使其线性近似，但在复杂推理任务中的表现仍然未得到充分探索。本文提出了一种遵循“少结构”原则的解决方案，即混合块注意力（MoBA），允许模型自主决定关注的内容，而不是引入先验偏置。MoBA通过借鉴专家混合（MoE）原则，展示了在长上下文任务中卓越的性能，并具备全注意力与稀疏注意力之间无缝切换的关键优势，提高了计算效率而不妥协表现。MoBA已被应用于支持Kimi的长上下文请求，并在大型语言模型的注意力计算效率上取得了显著进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:52:30 GMT</pubDate>
</item>
<item>
<title>JL1-CD数据集与多教师知识蒸馏框架在遥感影像变化检测中的应用</title>
<link>https://arxiv.org/abs/2502.13407</link>
<guid>https://arxiv.org/abs/2502.13407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出JL1-CD数据集和MTKD框架，提升遥感影像变化检测性能。</p><br /><br /><p><strong>摘要：</strong> 深度学习在遥感影像变化检测(CD)方面取得了显著成果，但仍面临两大挑战：缺乏亚米级的全面开放源CD数据集，以及在人们多变的变化区域中实现一致且满意的检测结果难度大。为此，本文介绍了JL1-CD数据集，包含5000对分辨率为0.5到0.75米的512 x 512像素图像。同时，提出了一种多教师知识蒸馏(MTKD)框架来解决CD问题。在JL1-CD和SYSU-CD数据集上的实验结果表明，MTKD框架显著提升了不同网络架构和参数规模的CD模型性能，达到了新的最先进结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:29:42 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的上下文信息存储与量化</title>
<link>https://arxiv.org/abs/2502.15007</link>
<guid>https://arxiv.org/abs/2502.15007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示小词汇在上下文保持中扮演重要角色，影响模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了量化大型语言模型（LLMs）如何编码和存储上下文信息的方法，发现一些被视为次要的标记（如限定词和标点符号）实际上对上下文具有显著影响。特别地，去除这些标记，特别是停用词、冠词和逗号，都会显著降低在MMLU和BABILong-4k上的性能，即使移除的是无关的标记。此外，分析显示上下文化与线性之间存在强关联，其中线性度衡量从一层嵌入到下一层的变换如何可以用单一线性映射近似。这些发现突显了填充词在维持上下文方面的潜在重要性。为进一步探索，我们提出了LLM-Microscope，这是一个开源工具包，用于评估标记级的非线性、评估上下文记忆、可视化中间层贡献（通过调整的Logit Lens），并测量表示的内在维度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 02:07:41 GMT</pubDate>
</item>
<item>
<title>基于视频掩码重建的可泛化驾驶世界模型</title>
<link>https://arxiv.org/abs/2502.11663</link>
<guid>https://arxiv.org/abs/2502.11663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个增强泛化能力的驾驶世界模型，结合视频掩码重建技术。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过结合生成损失与MAE风格特征级上下文学习，来提升驾驶世界模型的泛化能力。我们提出了MaskGWM（驾驶世界模型）的新架构，包括两个变体：MaskGWM-long和MaskGWM-mview，分别专注于长时间预测和多视角生成。关键设计包括可扩展的Diffusion Transformer结构、处理模糊关系的扩散相关掩码令牌，以及通过行级掩码实现时空域扩展的掩码构建任务。我们在多个标准基准上进行了全面实验，包含Nuscene数据集的常规验证、OpenDV-2K数据集的长时间展望卷出和Waymo数据集的零-shot验证，结果显示我们的方法显著提升了现有驾驶世界模型的效能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:16:03 GMT</pubDate>
</item>
<item>
<title>CrossOver：灵活的跨模态三维场景理解框架</title>
<link>https://arxiv.org/abs/2502.15011</link>
<guid>https://arxiv.org/abs/2502.15011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CrossOver框架通过灵活的模态对齐实现三维场景理解，有效处理缺失模态问题。</p><br /><br /><p><strong>摘要：</strong> 当前的多模态三维物体理解面临的数据缺失和模态对齐限制促使了CrossOver框架的提出。该框架通过灵活的场景层级模态对齐，创建一个统一的模态无关嵌入空间，支持RGB图像、点云、CAD模型、平面图和文本描述的整合。CrossOver利用特定维度的编码器和多阶段训练管道，有效应对缺失模态问题，展示了其在场景检索和物体定位中的强大能力。通过在ScanNet和3RScan数据集上的评估，CrossOver在多项指标上表现优异，展现了在实际三维场景理解应用中的广泛适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:13:24 GMT</pubDate>
</item>
<item>
<title>VLM^2-Bench：评估视觉语言模型的匹配线索能力</title>
<link>https://arxiv.org/abs/2502.12084</link>
<guid>https://arxiv.org/abs/2502.12084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视语言模型在视觉匹配线索能力方面的表现与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLM^2-Bench，一个针对视觉语言模型（VLMs）在视觉匹配线索能力方面的评测基准，包含9个子任务和超过3000个测试案例。通过对八种开源VLM和GPT-4o的综合评估，并分析多种语言和视觉提示方法，结果显示模型在链接视觉线索方面存在重大性能差距，具体表现为GPT-4o的表现比人类低34.80%。研究强调了若干核心挑战，并提出改进建议，包括增强模型的核心视觉能力，明确语言推理与视觉任务的整合原则，以及转变视觉文本训练方式，以增强模型独立构建和推断视觉线索关系的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12084" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:36:34 GMT</pubDate>
</item>
<item>
<title>LightThinker：动态压缩增强大型语言模型推理效率</title>
<link>https://arxiv.org/abs/2502.15589</link>
<guid>https://arxiv.org/abs/2502.15589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LightThinker通过动态压缩推理过程的思维步骤，提高了大型语言模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了一种名为LightThinker的新方法，旨在提高大型语言模型（LLMs）在复杂推理任务中的效率。LightThinker通过动态压缩推理过程中产生的冗长思维步骤，将其转化为紧凑的表示，从而显著减少存储在上下文窗口中的标记数量。借鉴人类认知过程，该方法训练模型在何时以及如何执行压缩，通过构建数据、将隐藏状态映射到压缩概括标记以及创建专门的注意力掩码来实现。此外，本文引入了依赖性（Dep）指标，用以量化压缩程度，测量生成过程中对历史标记的依赖性。在四个数据集和两种模型上的广泛实验表明，LightThinker有效减少了峰值内存使用量和推理时间，同时保持了竞争力的准确性。本研究为改善大型语言模型在复杂推理任务中的效率提供了一种新的方向，且不会牺牲性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:07:05 GMT</pubDate>
</item>
<item>
<title>推动非代理性AI的发展以确保安全与创新</title>
<link>https://arxiv.org/abs/2502.15657</link>
<guid>https://arxiv.org/abs/2502.15657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出开发非代理性AI系统以降低AI风险，促进科学进步。</p><br /><br /><p><strong>摘要：</strong> 随着领先的人工智能公司越来越多地关注构建通用人工智能代理系统，虽其潜在价值显著，但失控的AI代理可能会给公共安全和安全性带来严重风险，如恶意行为的利用及人类控制权的永久丧失。本文讨论了这些风险如何源于当前的AI训练方法，并提出了研发一种称为科学家AI的非代理性AI系统，以遵循预防原则，确保其设计的可信性和安全性。科学家AI可以通过观察解释世界，而非执行/actions，以满足人类需求，其内核组成包括一个生成理论的世界模型与具备不确定性概念的问答推理机器。通过这些方法，科学家AI能够作为对抗潜在危险AI代理的保护措施，助力人类研究人员加速科学进步。文章希望引导研究人员和政策制定者选择更安全之路，推动人工智能创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:02:52 GMT</pubDate>
</item>
<item>
<title>StructFlowBench：一项多轮指令跟随评估基准</title>
<link>https://arxiv.org/abs/2502.14494</link>
<guid>https://arxiv.org/abs/2502.14494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StructFlowBench 提出了多轮指令跟随的结构流评估基准。</p><br /><br /><p><strong>摘要：</strong> 多轮指令跟随能力是大型语言模型（LLMs）在现实应用中的核心能力，现有评估基准主要集中在细粒度约束满足和领域特定能力评估，但忽视了多轮对话中的结构依赖性。本文提出了StructFlowBench，一个基于结构流建模的多轮指令跟随基准，定义了六种基本的轮间关系，以引入新的结构约束用于模型评估，同时作为生成自定义对话流的参数。通过对13个优秀开源及闭源LLMs的系统评估，实验结果显示当前模型在理解多轮对话结构方面存在显著不足。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:43:43 GMT</pubDate>
</item>
<item>
<title>UPCORE：平衡信息删除与模型保持的高效方法</title>
<link>https://arxiv.org/abs/2502.15082</link>
<guid>https://arxiv.org/abs/2502.15082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UPCORE是一种用于优化模型卸载过程的方法，兼顾删除效率与模型保持。</p><br /><br /><p><strong>摘要：</strong> 本文提出了UPCORE（Utility-Preserving Coreset Selection），一种机制无关的数据选择框架，旨在平衡从预训练模型中删除特定信息与保持其他性能之间的矛盾。在模型卸载过程中，数据点的删除往往会导致模型在其他数据上的性能下降。研究表明，模型损害与忘记集表示的方差相关，因此UPCORE通过选择性修剪忘记集，去除异常值，从而最小化模型的退化。经过对三种标准卸载方法的评估，UPCORE在删除效率与模型保持之间实现了优越的平衡。为更好地评估这一权衡，本文引入了一种新指标，用于测量标准指标的曲线下面积（AUC），发现UPCORE在提升标准指标和AUC方面具有积极效果，能够有效减少忘记集对外部点的负面影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:17:33 GMT</pubDate>
</item>
<item>
<title>PhotoDoodle：新型图像编辑框架促进艺术涂鸦</title>
<link>https://arxiv.org/abs/2502.14397</link>
<guid>https://arxiv.org/abs/2502.14397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhotoDoodle是一种方便艺术家在照片上进行涂鸦的图像编辑框架。</p><br /><br /><p><strong>摘要：</strong> PhotoDoodle是一种新颖的图像编辑框架，旨在帮助艺术家在照片上叠加装饰元素，实现场景与新元素的无缝融合。该方法克服了传统技术在合理的透视对齐、上下文一致性及背景保持方面的不足，采用两阶段训练策略：首先用大规模数据训练通用模型OmniEditor，随后利用小型艺术家策划的数据集通过EditLoRA进行细化训练，以捕捉特定的编辑风格。为增强生成结果的一致性，PhotoDoodle引入了位置编码重用机制。此外，我们发布了六种高质量风格的PhotoDoodle数据集，广泛实验表明该方法在定制图像编辑方面展现了先进的性能和可靠性，开辟了艺术创作的新可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:55:04 GMT</pubDate>
</item>
<item>
<title>一种基于f散度最小化的扩散模型快速生成方法</title>
<link>https://arxiv.org/abs/2502.15681</link>
<guid>https://arxiv.org/abs/2502.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的f散度最小化框架以加速扩散模型生成过程。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在生成样本时通常需要缓慢的迭代过程，这限制了其在实际应用中的部署。为了解决这一问题，本文提出了一种新的分布匹配方法，称为f-distill，利用f散度最小化框架，解决了当前变分分数蒸馏方法使用反向Kullback-Leibler散度时的模式追求问题。通过对教师和学生分布之间的f散度梯度进行推导，我们显示该梯度可表达为它们的分数差异与由密度比确定的加权函数的乘积。在不同的f散度选择下，f-distill能够更好地覆盖模式且降低训练方差。实验表明，使用例如前向KL散度和Jensen-Shannon散度等其他选择时，f-distill在图像生成任务上超越了现有最佳变分分数蒸馏方法。此外，特别是在Jensen-Shannon散度下，f-distill在ImageNet64上实现了最先进的一步生成性能，并在MS-COCO上实现了零-shot文本到图像生成的最佳效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:24:55 GMT</pubDate>
</item>
<item>
<title>使用SIFT技术提升大语言模型推理的准确性</title>
<link>https://arxiv.org/abs/2502.14922</link>
<guid>https://arxiv.org/abs/2502.14922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SIFT技术，提升大语言模型的推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型在推理过程中因语境误读可能导致的问题，并提出了一种名为**Stick to the Facts (SIFT)**的新后训练方法。该方法借助增强推理时计算量，旨在将模型推理与上下文紧密结合。SIFT的核心在于生成的*Sticker*，强调上下文中的关键信息。通过对比原始查询和增强查询生成的两个预测，SIFT可有效优化Sticker。研究显示，SIFT在多个模型（从3B到100B+）和基准测试（如GSM8K、MATH-500）上均展现出显著的性能提升，特别是在AIME2024上，使DeepSeek-R1的pass@1准确率从78.33%提升至85.67%，在开源社区创造了新的最高纪录。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:17:18 GMT</pubDate>
</item>
<item>
<title>利用深度强化学习提升大语言模型的模式遵循能力</title>
<link>https://arxiv.org/abs/2502.14905</link>
<guid>https://arxiv.org/abs/2502.14905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过深度强化学习方法，本研究提高了大语言模型的模式遵循能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对在大语言模型生成过程中强化严格模式遵循的挑战，利用LLM的推理能力提出了一种新方法。基于DeepSeek R1强化学习框架，采用合成推理数据集构建与定制奖励函数相结合的管道，训练一款1.5亿参数的模型的结构化推理能力。研究首先在一份2万样本的非结构化到结构化数据集上进行R1强化学习，其后在一份独立的1万样本推理数据集上进行监督微调，重点提升下游任务的模式遵循。尽管训练范围较小，我们的模型在约20小时的GRPO训练与3小时的SFT训练下，依然展示了在强化模式一致性方面的出色性能。通过与原DeepSeek R1、其蒸馏版本及Gemini 2.0 Flash的比较，结果显示该资源高效框架在模式约束文本生成中的实际应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:11:17 GMT</pubDate>
</item>
<item>
<title>Mol-LLaMA：跨学科的通用分子语言模型</title>
<link>https://arxiv.org/abs/2502.13449</link>
<guid>https://arxiv.org/abs/2502.13449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mol-LLaMA通过多模态调优提升了分子知识的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mol-LLaMA，一个旨在提升分子知识理解的通用分子语言模型。尽管大规模的分子语言模型在解释分子结构方面取得了一定的成功，但其训练数据集局限于特定任务，未能全面涵盖分子的基本特征，限制了其作为通用分子助手的能力。为了解决这一问题，我们采用多模态调优的方法，设计了包含分子基本特征的关键数据类型，以融合分子结构的必要知识。此外，我们还引入了一个模块，通过整合来自不同分子编码器的互补信息，进一步提升分子特征的理解能力。实验结果显示，Mol-LLaMA能够理解分子的通用特征，并准确生成相关的用户查询响应，具备成为分子分析通用助手的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:52:51 GMT</pubDate>
</item>
<item>
<title>评估大型多模态模型的交互智能新工具与方法</title>
<link>https://arxiv.org/abs/2502.15027</link>
<guid>https://arxiv.org/abs/2502.15027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InterFeedback框架评估大型多模态模型的交互智能。</p><br /><br /><p><strong>摘要：</strong> 现有基准未能测试大型多模态模型（LMM）与用户的交互智能，这对开发通用人工智能助手至关重要。为此，本文设计了InterFeedback，一个可应用于任何LMM和数据集的互动框架，以自主评估这一能力。我们还介绍了InterFeedback-Bench，利用MMM-Pro和MathVerse两个代表性数据集评估10种不同开源LMM的交互智能。此外，我们发布了InterFeedback-Human，收集了120个案例，以手动测试OpenAI-o1和Claude-3.5-Sonnet等领先模型的交互性能。评估结果表明，即便是最先进的LMM（如OpenAI-o1），通过人类反馈纠正其结果的比例不足50%。研究发现，急需改进方法，以增强LMM解读和利用反馈的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:44:33 GMT</pubDate>
</item>
<item>
<title>大型语言模型在数学推理中的效率与准确性分析</title>
<link>https://arxiv.org/abs/2502.15631</link>
<guid>https://arxiv.org/abs/2502.15631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究不同模型在数学推理中的链式思维与准确性的关系。</p><br /><br /><p><strong>摘要：</strong> 本文系统分析了o1-mini和o3-mini两种模型在Omni-MATH基准测试中的推理链长度与准确性之间的关系。结果表明，o3-mini(m)在不需要更长推理链的情况下实现了更高的准确性。此外，研究发现无论是在模型还是计算设置中，推理链的长度增加通常会导致准确性的下降，尤其是在控制问题难度后。然而，在更高效的模型中，这种准确性下降明显较小，暗示新一代模型在计算时的推理效率更高。同时，尽管o3-mini(h)在准确性上比o3-mini(m)有所提升，但其在所有问题上分配的推理令牌大幅增加，甚至是o3-mini(m)已能解决的问题。这些发现为模型能力与推理长度之间的关系提供了新见解，并对效率、扩展与评估方法学有重要启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:40:17 GMT</pubDate>
</item>
<item>
<title>SurveyX：高效自动化问卷生成系统</title>
<link>https://arxiv.org/abs/2502.14776</link>
<guid>https://arxiv.org/abs/2502.14776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyX系统通过创新技术提升自动化问卷生成的效果。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在理解能力和知识储备方面表现出色，成为自动化问卷生成的有效工具。但目前的研究存在一些局限性，如有限的上下文窗口、缺乏深入讨论和系统评估框架。为此，我们提出了SurveyX，一个高效有序的自动化问卷生成系统。该系统将问卷编写过程分为准备和生成两个阶段，创新性地引入在线参考检索、名为AttributeTree的预处理方法以及重新润色流程，从而显著提升问卷编写的效率。实验结果表明，SurveyX在内容质量和引用质量上均优于现有的自动化问卷生成系统，接近人类专家的表现，展现出其在多个评估维度上的显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:39:54 GMT</pubDate>
</item>
<item>
<title>MODis框架：基于多目标优化的数据集发现方法</title>
<link>https://arxiv.org/abs/2502.11262</link>
<guid>https://arxiv.org/abs/2502.11262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MODis框架，以优化多用户定义的模型性能指标发现数据集。</p><br /><br /><p><strong>摘要：</strong> 随着数据驱动分析的兴起，高质量数据集的准备已成为AI和机器学习模型的核心任务。传统的数据发现方法通常使用单一预定义的质量衡量标准集成数据集，可能导致下游任务的偏见。本文介绍了MODis框架，通过优化多个用户定义的模型性能衡量标准来发现数据集。MODis从一组数据源和模型出发，将数据源选择和集成到一个天际线数据集中，以确保模型在所有性能测量中达到期望表现。我们将MODis形式化为一个多目标有限状态转导器，并提出了三种可行的天际线数据集生成算法。第一种算法采用“从通用减少”策略，基于通用模式并逐步剪除不乐观的数据。第二种算法通过相互交织的数据增强和减少进一步降低成本。同时，我们引入了一种多样化算法以减轻天际线数据集中的偏见。实验验证了我们的天际线数据发现算法的效率和有效性，并展示了其在优化数据科学流程中的应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11262" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 21:19:35 GMT</pubDate>
</item>
<item>
<title>S-VCO：提升大规模视觉语言模型对细粒度图像细节的敏感性</title>
<link>https://arxiv.org/abs/2502.13928</link>
<guid>https://arxiv.org/abs/2502.13928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S-VCO通过细粒度图像细节训练，显著提升视觉语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 最新研究表明，大规模视觉语言模型（VLMs）在视觉基础任务中常常忽略图像内容，过度依赖语言模型先验，导致错误和幻觉。为了解决这一问题，本文提出了一种新的微调目标S-VCO（对称视觉对比优化），旨在增强VLM训练中的视觉反馈，帮助模型捕捉重要的视觉细节并与相应的文本标记对齐。我们还介绍了MVC，一个通过自动过滤和增强视觉反事实数据建立的配对图像-文本数据集，以挑战模型并实现更为精准的对比训练。实验结果显示，我们的方法在多个基准测试中一致提升VLM性能，尤其在视觉依赖性较高的测试中，幻觉减少了最高达22%，并且在视觉中心和一般任务中也取得显著进展。总之，S-VCO不仅显著提升了VLM在视觉任务中的表现，还保留或改善了模型的通用能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:42:50 GMT</pubDate>
</item>
<item>
<title>Generating $π$-Functional Molecules Using STGG+ with Active Learning</title>
<link>https://arxiv.org/abs/2502.14842</link>
<guid>https://arxiv.org/abs/2502.14842</guid>
<content:encoded><![CDATA[
Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explore new chemical spaces but often conducts 'reward-hacking' and generates non-synthesizable molecules. In this work, we address this problem by integrating a state-of-the-art supervised learning method, STGG+, in an active learning loop. Our approach iteratively generates, evaluates, and fine-tunes STGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We apply STGG+AL to the design of organic pi-functional materials, specifically two challenging tasks: 1) generating highly absorptive molecules characterized by high oscillator strength and 2) designing absorptive molecules with reasonable oscillator strength in the near-infrared (NIR) range. The generated molecules are validated and rationalized in-silico with time-dependent density functional theory. Our results demonstrate that our method is highly effective in generating novel molecules with high oscillator strength, contrary to existing methods such as reinforcement learning (RL) methods. We open-source our active-learning code along with our Conjugated-xTB dataset containing 2.9 million pi-conjugated molecules and the function for approximating the oscillator strength and absorption wavelength (based on sTDA-xTB).
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:05:36 GMT</pubDate>
</item>
<item>
<title>CHASE框架：无人工参与的挑战性问题生成</title>
<link>https://arxiv.org/abs/2502.14678</link>
<guid>https://arxiv.org/abs/2502.14678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了CHASE框架，以无人工参与的方式合成具有挑战性的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CHASE，一个新的框架，用于合成生成具有挑战性的问题，以应对大型语言模型（LLMs）评估的不断变化的需求。传统的人力标注方法在面对复杂、高质量问题时变得难以实施，因此我们开发了一个无人工干预的自下而上的生成方法，通过更简单的组件构建复杂问题。同时，CHASE框架将生成过程拆分为可独立验证的子任务，以确保高质量和正确性。我们在三个不同领域（文档问答、代码补全和数学推理）验证了CHASE的有效性，结果显示最先进的LLMs在这些合成基准上的准确率在40-60%之间，突显了该框架生成挑战性问题的能力。我们会公开发布这些基准和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models</title>
<link>https://arxiv.org/abs/2502.14191</link>
<guid>https://arxiv.org/abs/2502.14191</guid>
<content:encoded><![CDATA[
Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:34:53 GMT</pubDate>
</item>
<item>
<title>LServe：基于混合稀疏注意力的长序列大语言模型高效服务</title>
<link>https://arxiv.org/abs/2502.14866</link>
<guid>https://arxiv.org/abs/2502.14866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LServe通过混合稀疏注意力加速长序列大语言模型的服务效率。</p><br /><br /><p><strong>摘要：</strong> 大语言模型在处理长序列时表现出色，但由于前填充阶段的注意力计算复杂度及解码阶段的KV缓存内存开销，服务这些模型仍面临挑战。为了解决这些问题，我们提出了LServe，一个高效系统，通过混合稀疏注意力加速长序列大语言模型的服务。该方法将不同的硬件友好结构稀疏模式统一到一个框架内，省略对不重要标记的计算，显示出静态和动态稀疏在长上下文模型中兼容性。通过将一半的注意力头转换为几乎免费的流式头，LServe在前填充和解码阶段实现乘法加速。此外，我们发现保持长上下文能力只需固定数量的KV页面，并设计了一个层级KV页面选择策略，根据查询的相似性动态修剪KV页面。总体而言，LServe在保持长上下文准确度的同时，使LLM的前填充加速达到2.9倍，解码加速达到1.3-2.1倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 09:39:36 GMT</pubDate>
</item>
<item>
<title>提升大型多模态模型的视觉推理与可解释性的框架</title>
<link>https://arxiv.org/abs/2502.14044</link>
<guid>https://arxiv.org/abs/2502.14044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，增强大型多模态模型的视觉推理与解释能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视觉拒绝采样框架，旨在提高大型多模态模型（LMMs）在视觉任务中的认知能力和可解释性。当前LMMs在细粒度视觉推理中表现不佳，常常无法识别特定领域的目标，也无法对其预测提供合理的解释。我们的框架通过自合成数据来解决这些问题，具体方法包括合成可解释的答案，答案中包含可供人验证的视觉特征，这些特征基于专家定义的概念，经过精心选择以与图像内容对齐。在每次微调后，我们应用无奖励模型的过滤机制，筛选出最高质量的可解释答案用于下一轮的调优。实验结果显示，该方法在提高专业视觉分类任务的准确性和可解释性方面显著有效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:26:31 GMT</pubDate>
</item>
<item>
<title>NaviClues数据集与Navig框架推动影像地理定位进步</title>
<link>https://arxiv.org/abs/2502.14638</link>
<guid>https://arxiv.org/abs/2502.14638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出NaviClues数据集和Navig框架，提升影像地理定位精度。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦影像地理定位任务，提出了新的高质量数据集NaviClues，来源于流行的地理游戏GeoGuessr，旨在提供语言方面专家推理的示例。我们还提出了Navig，一个综合的影像地理定位框架，该框架整合了全球和细粒度的影像信息。通过语言推理，Navig相较于以往的最先进模型减少了14%的平均距离误差，同时训练样本不足1000。本文贡献不仅提升了影像地理定位的精确性，也为相关研究提供了数据集和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:18:34 GMT</pubDate>
</item>
<item>
<title>HippoRAG 2: 近似人类长期记忆的高效检索增强生成框架</title>
<link>https://arxiv.org/abs/2502.14802</link>
<guid>https://arxiv.org/abs/2502.14802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HippoRAG 2 提出了一种新框架，提升了长短期记忆任务的表现。</p><br /><br /><p><strong>摘要：</strong> HippoRAG 2 是针对持续学习中的大语言模型（LLMs）优化的一种新框架，旨在增强其知识获取与组织能力。传统的检索增强生成（RAG）方法由于依赖于向量检索，未能有效模拟人类动态的长期记忆。为解决这一问题，HippoRAG 2 引入了个性化的 PageRank 算法并改进了段落集成和 LLM 的在线使用。实验结果显示，HippoRAG 2 在事实、认知和关联记忆任务上的表现均优于传统的 RAG 方法，尤其在关联记忆任务上实现了 7% 的提升。此外，该框架拓展了非参数持续学习的可能性，推动了大语言模型的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:00:41 GMT</pubDate>
</item>
<item>
<title>CLIPPER：用于叙述主张验证的合成数据生成方法</title>
<link>https://arxiv.org/abs/2502.14854</link>
<guid>https://arxiv.org/abs/2502.14854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLIPPER通过压缩方法生成更高质量的叙述主张验证合成数据。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型开发者越来越依赖合成数据，生成高质量的数据以应对复杂的长上下文推理任务仍然具有挑战性。本文介绍了CLIPPER，这是一种基于压缩的方法，专门用于生成适合叙述主张验证的合成数据。CLIPPER首先将书籍压缩为章节大纲和书籍摘要，然后利用这些中间表示生成复杂的主张及其推理链。与直接从原文生成主张相比，CLIPPER能够生成更有效、扎实和复杂的主张。我们利用CLIPPER构建了19K条合成书籍主张的数据集，并将其与源文本和推理链配对，进一步调整了三个开放权重模型，使得最佳模型在叙述主张验证任务上取得了突破性的结果，将准确率从28%提升至76%。此外，我们的分析表明，模型生成的推理链更加详尽和扎实，同时在其他叙述理解任务上（如NarrativeQA）性能也有所提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:52:55 GMT</pubDate>
</item>
<item>
<title>LLM-based User Profile Management for Recommender System</title>
<link>https://arxiv.org/abs/2502.14541</link>
<guid>https://arxiv.org/abs/2502.14541</guid>
<content:encoded><![CDATA[
The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title>
<link>https://arxiv.org/abs/2502.14502</link>
<guid>https://arxiv.org/abs/2502.14502</guid>
<content:encoded><![CDATA[
The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:29:18 GMT</pubDate>
</item>
<item>
<title>How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild</title>
<link>https://arxiv.org/abs/2502.12769</link>
<guid>https://arxiv.org/abs/2502.12769</guid>
<content:encoded><![CDATA[
In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:28:42 GMT</pubDate>
</item>
<item>
<title>S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12853</link>
<guid>https://arxiv.org/abs/2502.12853</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S^2R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\% to 81.6\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S^2R. Our code and data are available at https://github.com/NineAbyss/S2R.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:00:18 GMT</pubDate>
</item>
<item>
<title>改进长文本摘要的无结构证据引用方法</title>
<link>https://arxiv.org/abs/2502.14409</link>
<guid>https://arxiv.org/abs/2502.14409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出改进长文本摘要的方法，通过无结构证据引用提升透明度与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前大语言模型（LLMs）在长文本上下文中生成摘要时的局限性，尤其是在证据引用方面。现有研究主要集中在使用预定义的粒度（如句子、段落等）进行证据引用，而我们提出了一种新的任务——侧重于用户查询的长文本摘要，结合无结构证据引用。通过创建一个名为SUnsET的合成数据集，我们展示了不同规模的LLMs在经过该数据适应后，能更有效地引用长文本中的证据，提取更广泛的信息位置，从而生成更具相关性和事实一致性的摘要。实验结果表明，适应后模型在多个测试集上表现优于基础模型，显示出显著的提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:40 GMT</pubDate>
</item>
<item>
<title>提升图像地理定位的综合框架与新方法</title>
<link>https://arxiv.org/abs/2502.13759</link>
<guid>https://arxiv.org/abs/2502.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架，通过大规模数据集和推理方法提升图像地理定位精度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种综合的地理定位框架，旨在解决当前地理定位方法中存在的精度低、解释性差的问题。框架由三个核心组件组成：GeoComp（大规模数据集）、GeoCoT（新型推理方法）和GeoEval（评估指标）。GeoComp是一个由740K用户在两年内通过地理定位游戏平台收集的大规模数据集，包含2500万条元数据和300万个地理标签位置，提供各种难度的样本以供详细分析。基于此数据集，GeoCoT是一个多步骤推理框架，旨在提升大规模视觉模型在地理定位任务中的推理能力。通过整合上下文和空间线索，GeoCoT的推理方式更接近人类思维，最终表明GeoEval指标显示其精度提高了多达25%，并提升了模型的可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:28 GMT</pubDate>
</item>
<item>
<title>基于强化学习的量子码权重优化方法研究</title>
<link>https://arxiv.org/abs/2502.14372</link>
<guid>https://arxiv.org/abs/2502.14372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了基于强化学习的量子码权重优化方法，显著提高了编码效率。</p><br /><br /><p><strong>摘要：</strong> 随着可扩展容错量子计算的实现日益依赖量子错误纠正码，测量权重的优化变得至关重要。本文介绍了一种基于强化学习的有效方法，旨在降低稳定器码的测量权重，结果显示这些低权重代码在实际相关参数领域的表现显著优于当前技术的最优解，尤其是在小距离编码中实现了更大幅度的提升。例如，对于权重为6的码，我们的方法比现有结果节省了1到2个数量级的物理量子位开销，使得开销进入未来实验可行范围。此外，我们还利用强化学习框架研究了码参数之间的相互作用，为实践中可行的编码策略提供了新见解。本研究结果展示了强化学习在解决量子码发现中的潜力，对于推进容错量子技术的实际应用具有重要意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14372" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 01:11:34 GMT</pubDate>
</item>
<item>
<title>语言模型的时间知识处理及其局部特征分析</title>
<link>https://arxiv.org/abs/2502.14258</link>
<guid>https://arxiv.org/abs/2502.14258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型中的特定注意力头负责处理时间相关知识。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了语言模型如何处理时变事实，特别是识别出特定的注意力头（Temporal Heads），它们在处理时间知识方面起重要作用。通过电路分析，我们确认这些头在多个模型中均存在，其具体位置可能不同，且对不同类型知识和年份的响应有所差异。禁用这些头会降低模型对时间特定知识的回忆能力，但不会影响模型在时间不变和问答任务上的表现。此外，这些头不仅响应数字条件（如“在2004年”），还对文本别名（如“在那一年...”）做出反应，表明它们编码了超越简单数字表示的时间维度。我们还展示了如何通过调整这些头的值来编辑时间知识，拓展了研究的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 23:02:42 GMT</pubDate>
</item>
<item>
<title>Set-and-Sequence：动态概念个性化的视频生成框架</title>
<link>https://arxiv.org/abs/2502.14844</link>
<guid>https://arxiv.org/abs/2502.14844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Set-and-Sequence框架，旨在个性化生成视频模型中的动态概念。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的框架Set-and-Sequence，旨在个性化基于Diffusion Transformers的生成视频模型，以捕捉动态概念。与静态概念不同，动态概念不仅由外观定义，还包含运动信息。该框架通过在一个不明确分离空间和时间特征的架构内施加时空权重空间，实现个性化生成。具体而言，框架分为两个关键阶段：首先，通过无序帧集微调低秩适应（LoRA）层，以学习表示外观的身份LoRA基础；其次，冻结身份LoRA后，对其系数进行运动残差增强，进一步微调完整视频序列以捕捉运动动态。最终，Set-and-Sequence框架有效嵌入动态概念，实现前所未有的可编辑性和组合性，成为个性化动态概念的新基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:41:47 GMT</pubDate>
</item>
<item>
<title>PC-Agent: 复杂交互环境下的分层代理框架</title>
<link>https://arxiv.org/abs/2502.14282</link>
<guid>https://arxiv.org/abs/2502.14282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PC-Agent框架旨在改善PC场景下的复杂用户指令处理能力。</p><br /><br /><p><strong>摘要：</strong> PC-Agent是一个专为PC场景设计的分层代理框架，旨在克服当前大规模语言模型(MLLM)在复杂交互环境中的局限性。文章首先介绍了主动感知模块（APM），以提升对截图内容的感知能力。其次，提出了一种层次化多代理协作架构，将决策过程细分为指令、子任务和行动三个层面，设立了管理、进度和决策三个代理以优化指令分解、进度跟踪和逐步决策。此外，反思代理的引入能够实现及时的自下而上的错误反馈和调整。通过新基准PC-Eval进行实证测试，PC-Agent在任务成功率上较之前的最先进方法提高了32%。代码将会公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:48 GMT</pubDate>
</item>
<item>
<title>LongWriter-V-22k: 提升长文本生成能力的视觉语言模型</title>
<link>https://arxiv.org/abs/2502.14834</link>
<guid>https://arxiv.org/abs/2502.14834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongWriter-V-22k解决了LVLM生成超千字文本的能力不足问题。</p><br /><br /><p><strong>摘要：</strong> 现有的大型视觉语言模型（LVLM）能处理最长达128k的文本和视觉令牌输入，但在生成超千字的连贯输出时却存在困难。研究发现，主要限制因素是缺乏长输出的监督微调（SFT）示例。为此，我们引入了LongWriter-V-22k数据集，包含22,158个示例，每个示例都有多张输入图像、指令和对应的输出（范围从0到10,000字）。此外，为确保生成的长输出高保真于输入图像，我们对SFT模型采用了直接偏好优化（DPO）。由于收集人类反馈成本高，我们提出了IterDPO方法，通过将长输出进行分段处理并进行迭代修正生成偏好对。我们还开发了MMLongBench-Write基准，评估VLM的长生成能力。在使用LongWriter-V-22k和IterDPO训练的7B参数模型上，我们在该基准上表现出色，优于更大的专有模型如GPT-4o。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:21 GMT</pubDate>
</item>
<item>
<title>CoSyn框架：自动生成文本丰富的多模态数据</title>
<link>https://arxiv.org/abs/2502.14846</link>
<guid>https://arxiv.org/abs/2502.14846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSyn框架利用LLM自动生成文本丰富的图像数据，提升VLM性能。</p><br /><br /><p><strong>摘要：</strong> CoSyn是一个框架，旨在解决视觉语言模型(VLMs)在处理文本丰富图像（如图表和文档）时面临的数据稀缺问题。通过输入描述目标领域的文本，例如“营养成分标签”，CoSyn可以促使大语言模型(LLM)生成用于渲染合成图像的代码（如Python、HTML和LaTeX）。这些代码作为合成图像的文本表示，进而产生高质量的指令调优数据。基于CoSyn构建的数据集包含40万张图像和270万行的视觉语言指令调优数据。经过在七个基准上的全面实验，使用我们合成数据训练的模型在性能上超过了包括Llama 3.2在内的竞争性开源模型，并且优于GPT-4V和Gemini 1.5 Flash等专有模型。此外，CoSyn还能生成合成指向数据，使VLM能够在输入图像中定位信息，展示其在开发能够在真实环境中行动的多模态代理的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:38:36 GMT</pubDate>
</item>
<item>
<title>SigLIP 2：新一代多语言视觉-语言编码器</title>
<link>https://arxiv.org/abs/2502.14786</link>
<guid>https://arxiv.org/abs/2502.14786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SigLIP 2在多语言视觉-语言任务中的性能显著提升。</p><br /><br /><p><strong>摘要：</strong> SigLIP 2是基于原始SigLIP成功的一系列新多语言视觉-语言编码器。通过引入不同的训练目标和技术，包括基于标注的预训练、自监督损失和在线数据管理，SigLIP 2在零样本分类、图像-文本检索及视觉表示提取能力上超越了前作。此外，新模型在定位和密集预测任务上也有显著改进，且支持多种分辨率与输入原始长宽比的保留。通过使用更加多样化的数据混合和去偏见技术，SigLIP 2在多语言理解和公平性方面表现出色。最终，我们还发布了四个不同大小的模型检查点，用户可以根据推理成本与性能需求进行选择。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:33:22 GMT</pubDate>
</item>
<item>
<title>RelaCtrl：基于相关性指导的可控生成框架</title>
<link>https://arxiv.org/abs/2502.14377</link>
<guid>https://arxiv.org/abs/2502.14377</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RelaCtrl框架通过优化控制信号集成提升了Diffusion Transformer的效率。</p><br /><br /><p><strong>摘要：</strong> RelaCtrl是一个旨在提高Diffusion Transformer在文本到图像和文本到视频生成中的效率和资源利用率的可控生成框架。我们通过评估每个Transformer层与控制信息的相关性，提出了“ControlNet相关性评分”来量化控制层的重要性，从而优化控制层的布局、参数规模和建模能力，减少不必要的参数和计算量。此外，RelaCtrl还通过引入双维混合器（TDSM）替代传统的自注意力和前馈神经网络模块，实现了令牌混合器和通道混合器的高效实现。实验结果显示，与PixArt-delta相比，RelaCtrl在参数和计算复杂度方面仅占15%，且性能显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14377" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:30:51 GMT</pubDate>
</item>
<item>
<title>基于规则的强化学习在大型推理模型中的应用研究</title>
<link>https://arxiv.org/abs/2502.14768</link>
<guid>https://arxiv.org/abs/2502.14768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究基于规则的强化学习在推理模型中的应用，取得显著成效。</p><br /><br /><p><strong>摘要：</strong> 本研究受DeepSeek-R1的启发，探讨规则基础的强化学习在大型推理模型中的潜力。我们使用合成逻辑难题作为训练数据，以便分析推理动态，其复杂性可控且答案验证简单。我们提出了关键的技术贡献，以实现有效且稳定的强化学习训练，包括强调思考与回答过程的系统提示、对走捷径的输出进行惩罚的严格格式奖励函数，以及实现稳定收敛的简单训练食谱。经过训练，我们的7B模型发展出反思、验证和总结等高级推理技能，虽然这些能力在逻辑语料库中并不存在。令人瞩目的是，在仅训练5K个逻辑问题后，该模型在挑战性数学基准AIME和AMC上展示出了良好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14768" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:19:05 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在多学科领域的知识和推理能力</title>
<link>https://arxiv.org/abs/2502.14739</link>
<guid>https://arxiv.org/abs/2502.14739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SuperGPQA基准以评估大语言模型在285个学科的表现。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）在数学、物理和计算机科学等学术领域表现出色，但在其他200多个专业学科上的能力评估尚显不足。为填补这一空白，本文提出SuperGPQA，一个全面的评估基准，旨在测试研究生水平的知识和推理能力，涵盖285个学科。该基准采用新的人类-LLM协作过滤机制，通过基于LLM响应和专家反馈的迭代优化，消除琐碎或模糊的问题。实验结果显示，当前最先进的LLMs在各知识领域的表现仍有显著提升空间，最高准确率仅为61.82%。此外，本文还分享了在大型注释过程中管理80多位专家评审者的经验，为未来类似研究提供了方法论指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:15:33 GMT</pubDate>
</item>
<item>
<title>增强语言模型的视觉空间推理能力的两阶段训练框架</title>
<link>https://arxiv.org/abs/2502.14669</link>
<guid>https://arxiv.org/abs/2502.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升语言模型的视觉空间推理能力，成功用于迷宫导航。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的两阶段训练框架，旨在增强标准大型语言模型（LLMs）在迷宫导航中的视觉推理能力。第一阶段采用监督微调（SFT），利用经过处理的迷宫表示数据集教会模型逐步预测移动命令。第二阶段利用深度强化学习中的群体相对策略优化（GRPO）技术，通过精心设计的奖励函数改善模型的决策制定能力。实验结果显示，基线模型无法完成迷宫导航，而经过SFT训练的模型准确率达到86%，进一步的GRPO微调后，准确率提升至93%。定性分析表明，GRPO促进了模型更强的自我修正推理能力，突显了我们的方案在将语言模型与视觉空间任务相结合方面的潜力，这些发现对机器人、自动导航等领域具有重要的应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:11:45 GMT</pubDate>
</item>
<item>
<title>Meta MLGym：评估与开发 LLM 代理的新框架与基准</title>
<link>https://arxiv.org/abs/2502.14499</link>
<guid>https://arxiv.org/abs/2502.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了 Meta MLGym 和 MLGym-Bench，旨在评估 LLM 代理在 AI 研究任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Meta MLGym 和 MLGym-Bench，这是一个用于评估和开发大语言模型（LLM）代理在 AI 研究任务上的新框架和基准。MLGym-Bench 包含来自计算机视觉、自然语言处理、强化学习和博弈论等多个领域的13个多样化且开放式的 AI 研究任务。解决这些任务需要真实的 AI 研究技能，包括生成新想法、数据处理、实现机器学习方法、训练模型及实验分析等。通过对多个前沿的 LLM 进行评估，尽管它们通常能通过找到更好的超参数来改善已有基线，但并未能生成新的假设或显著改进。为促进未来 LLM 代理 AI 研究能力的进步，本文将框架和基准开源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:08:38 GMT</pubDate>
</item>
<item>
<title>S*: Test Time Scaling for Code Generation</title>
<link>https://arxiv.org/abs/2502.14382</link>
<guid>https://arxiv.org/abs/2502.14382</guid>
<content:encoded><![CDATA[
Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:04:42 GMT</pubDate>
</item>
<item>
<title>基于真实对话的长效聊天机器人情感智能研究</title>
<link>https://arxiv.org/abs/2502.13270</link>
<guid>https://arxiv.org/abs/2502.13270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究真实对话数据以增强聊天机器人情感智能和长效记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了REALTALK，一个为期21天的真实聊天记录数据集，旨在填补现有研究对人机对话理解的空白。通过分析数据集的情感智能属性和个性一致性，探讨真实对话所面临的挑战。与生成的对话数据相比，真实对话展示了更丰富的情感表达和个性稳定性。基于这些研究成果，提出了两个基准任务：个性模拟和记忆探测。研究发现，目前的模型在仅依赖对话历史进行用户模拟时表现不佳，而对特定用户聊天进行微调可以有效提升个性仿真能力。此外，现有模型在真实对话中回忆和利用长期上下文方面依然面临重大挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 16:00:25 GMT</pubDate>
</item>
<item>
<title>记忆代码：探索大型语言模型在长期互动中的局限性</title>
<link>https://arxiv.org/abs/2502.13791</link>
<guid>https://arxiv.org/abs/2502.13791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型在长期交互中面临信息整合的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MemoryCode，一个合成的多会话数据集，旨在评估大型语言模型（LLMs）在面对长时间交互时，跟踪和执行简单编码指令的能力。尽管所有测试的模型在处理孤立指令时表现良好，甚至包括最先进的模型如GPT-4o，但在指令分散于多个会话时其性能显著下降。我们的分析表明，这主要是由于当前模型无法有效检索和整合长指令链中的信息。这一发现揭示了目前LLMs在长期互动中合作的根本性限制，影响了其在实际工作环境中的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 14:34:52 GMT</pubDate>
</item>
<item>
<title>大语言模型在相关性评估中的应用与挑战</title>
<link>https://arxiv.org/abs/2502.13908</link>
<guid>https://arxiv.org/abs/2502.13908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大语言模型在信息检索相关性评估中的应用与研究进展。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了大语言模型（LLMs）在相关性评估中的潜力，特别是在信息检索（IR）和自然语言处理（NLP）领域的应用。研究表明，LLMs能够显著减少人工评估所需的时间和精力，尤其是在面对新主题和低资源情况时。作者回顾了在SIGIR 2024上进行的大规模自动相关性评估基准测试——LLMJudge挑战，介绍了42个由国际团队生成的相关性标签。通过对这些自动生成标签的分析，可以研究LLMs引发的系统性偏见、集成模型的有效性、以及不同模型与人工评估者之间的权衡。这项工作为改进自动化评估技术的方法提供了重要的资源和基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 13:47:47 GMT</pubDate>
</item>
<item>
<title>推出大规模多语言文本嵌入基准（MMTEB）</title>
<link>https://arxiv.org/abs/2502.13595</link>
<guid>https://arxiv.org/abs/2502.13595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMTEB基准，以丰富文本嵌入模型的多语言评估任务。</p><br /><br /><p><strong>摘要：</strong> 本文引入了大规模多语言文本嵌入基准（MMTEB），旨在克服现有文本嵌入评估在语言、领域和任务多样性方面的局限性。该基准涵盖了500多项经过质量控制的评估任务，分布在250多种语言上，包含了丰富而新颖的任务，如指令执行、长文档检索和代码检索。我们发现，尽管具有数十亿参数的大型语言模型在特定语言和任务上表现优异，但最佳公开模型multilingual-e5-large-instruct的参数仅为5.6亿。为降低计算成本，我们采用了一种基于任务间相关性的下采样方法，确保选取的任务多样且保持模型排名。此外，我们通过采样困难负例优化检索任务，创建了较小但有效的分割，从而使基准显著降低计算需求。这些改进使得我们的零-shot英语基准在计算成本较小的情况下，仍能维持与完整版本相似的排名顺序。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:26:53 GMT</pubDate>
</item>
<item>
<title>AI驱动探索：优化机器学习工程的创新方法</title>
<link>https://arxiv.org/abs/2502.13138</link>
<guid>https://arxiv.org/abs/2502.13138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIDE利用大型语言模型优化机器学习工程，提升研发效率。</p><br /><br /><p><strong>摘要：</strong> 机器学习是现代人工智能的基础，驱动着世界的创新，但其背后却是一个复杂且耗时的过程，工程师与科学家们在试错任务上耗费大量时间。为了解决这一挑战，本文介绍了AI驱动探索（AIDE），这是一个由大型语言模型支持的机器学习工程代理。AIDE将机器学习工程视为代码优化问题，利用树搜索方法框架进行试错，战略性地重用和完善有前景的解决方案，从而有效地用计算资源换取更优的性能。AIDE在多个机器学习工程基准测试中取得了领先的结果，包括Kaggle评估、OpenAI的MLE-Bench以及METRs的RE-Bench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:23:27 GMT</pubDate>
</item>
<item>
<title>MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching</title>
<link>https://arxiv.org/abs/2502.12852</link>
<guid>https://arxiv.org/abs/2502.12852</guid>
<content:encoded><![CDATA[
Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:09:53 GMT</pubDate>
</item>
<item>
<title>PGMR框架：提升自然语言生成SPARQL查询的准确性</title>
<link>https://arxiv.org/abs/2502.13369</link>
<guid>https://arxiv.org/abs/2502.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PGMR框架以提高基于LLM的SPARQL查询生成准确性，减少URI幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PGMR（后生成记忆检索）框架，该框架通过引入非参数化记忆模块，提升了大型语言模型（LLMs）生成SPARQL查询的准确性及效率。尽管LLMs在知识图谱中的SPARQL查询生成应用广泛，但通常会出现URI幻觉和超出分布的错误，导致生成的内容虽看似合理但却事实错误，从而影响在实际信息检索中的应用。PGMR框架通过有效检索知识图谱元素，显著降低了URI幻觉现象。在不同数据集和LLMs的实验中，PGMR展现出一致的强劲性能，几乎在多个场景下消除了URI幻觉问题，证明其在知识图谱信息检索应用中的潜力和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:07:02 GMT</pubDate>
</item>
<item>
<title>SplatDiff：基于像素喷溅指导的视频扩散模型</title>
<link>https://arxiv.org/abs/2502.12752</link>
<guid>https://arxiv.org/abs/2502.12752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SplatDiff通过像素喷溅引导来合成高保真新视图，有效解决纹理幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出的SplatDiff是一种基于像素喷溅的引导视频扩散模型，旨在从单张图像生成高保真的新视图。该模型使用对齐合成策略精确控制目标视角和几何一致的视图合成。为解决纹理幻觉问题，设计了一种纹理桥接模块，能够通过自适应特征融合实现高保真纹理生成。实验结果表明，SplatDiff在单视图新视图合成任务中表现出色，超越了现有方法。此外，在无需额外训练的情况下，SplatDiff在稀疏视图新视图合成和立体视频转换等多样任务中表现出显著的零-shot性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:53:49 GMT</pubDate>
</item>
<item>
<title>TESS 2：提升指令跟随能力的扩散语言模型</title>
<link>https://arxiv.org/abs/2502.13917</link>
<guid>https://arxiv.org/abs/2502.13917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TESS 2 是一种优于现有模型的指令跟随扩散语言模型。</p><br /><br /><p><strong>摘要：</strong> TESS 2 是一款通用的指令跟随扩散语言模型，它在性能上超越了当前的指令调优扩散模型，并且在某些情况下与强大的自回归模型相抗衡甚至超过它们。TESS 2 的训练首先是对一个强大的自回归模型进行继续预训练，使用交叉熵作为扩散损失，随后进行进一步的指令调优。研究表明，适应训练和基础模型的选择对于训练优秀的指令跟随扩散模型至关重要。此外，我们提出了一种名为奖励引导的新颖推理时间指导程序，以便在不需要训练基础模型的情况下对模型输出进行对齐。最后，结果显示，TESS 2 在增加推理时间计算资源后能进一步改善，这突显了扩散语言模型在推理阶段对计算资源使用的精细控制能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:46:55 GMT</pubDate>
</item>
<item>
<title>REFIND框架：改进大语言模型输出中的幻觉检测</title>
<link>https://arxiv.org/abs/2502.13622</link>
<guid>https://arxiv.org/abs/2502.13622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REFIND框架通过提取文档检测大语言模型的幻觉现象，提高可靠性。</p><br /><br /><p><strong>摘要：</strong> 针对大语言模型（LLM）输出中的幻觉现象，REFIND（检索增强事实性幻觉检测）框架应运而生。该框架通过直接利用检索到的文档，检测LLM输出中的幻觉段落。REFIND引入了上下文敏感度比（CSR）这一新指标，量化LLM输出对检索证据的敏感度，使得幻觉检测更为高效准确。在评估过程中，REFIND在九种语言中展现出良好的鲁棒性，尤其是在低资源环境下，相比于基线模型，其在识别幻觉段落时显著提高了IoU分数。本研究强调了量化上下文敏感度在幻觉检测中的有效性，为不同语言环境下更可靠的LLM应用铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 07:25:12 GMT</pubDate>
</item>
<item>
<title>LoRAM：高效的低秩适应训练方案</title>
<link>https://arxiv.org/abs/2502.13533</link>
<guid>https://arxiv.org/abs/2502.13533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAM通过小模型训练提高LLM的内存效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的内存高效的低秩适应训练方案LoRAM，旨在解决传统Low-Rank Adaption (LoRA)在内存占用方面的限制。LoRAM的核心思想是针对过度参数化的大型语言模型，许多神经元的训练效用较低但在推理中必不可少。该方案通过训练一个经过剪枝的小模型，以获得剪枝的低秩矩阵，然后将其恢复并与原始大型模型结合用于推理。此外，为了调和剪枝模型和原模型之间的知识差异，模型发布者在训练前进行最小成本的持续预训练。通过大量实验，LoRAM在多种剪枝策略和下游任务中表现出色，特别适用于具有700亿参数的模型，使得在只有20G HBM的GPU上进行训练成为可能，替代了用于LoRA训练的A100-80G GPU以及全微调所需的15个GPU。实现的QLoRAM通过结构化剪枝结合4位量化，显著减少了低秩矩阵训练中占主导地位的参数存储成本，且实现了优于原始和LoRA训练模型的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 06:45:40 GMT</pubDate>
</item>
<item>
<title>半监督异构领域适应中的可转移知识探讨</title>
<link>https://arxiv.org/abs/2502.13573</link>
<guid>https://arxiv.org/abs/2502.13573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明在半监督异构领域适应中，源样本的类别和特征信息对目标领域性能影响不大。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了半监督异构领域适应（SHDA）中可转移知识的性质，通过对约330个SHDA任务的广泛实验，使用了两种监督学习方法和七种代表性的SHDA方法。结果表明，源样本的类别和特征信息对目标领域的性能影响不显著。此外，简单分布的噪声作为源样本时，也可能蕴含可转移知识。基于这一发现，我们设计了统一的知识转移框架（KTF）用于SHDA，并发现可转移知识的主要来源是源域的可转移性和可区分性。因此，确保源样本具备这些属性，无论其来源（如图像、文本或噪声），都能提高SHDA任务中的知识转移效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:38:39 GMT</pubDate>
</item>
<item>
<title>全球文化知识评估：GIMMICK多模态基准研究</title>
<link>https://arxiv.org/abs/2502.13766</link>
<guid>https://arxiv.org/abs/2502.13766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GIMMICK是一个评估全球文化知识的多模态基准，涵盖144个国家的文化特色。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GIMMICK，一个旨在评估大型视觉语言模型（LVLM）在不同文化知识方面表现的多模态基准。GIMMICK包含六项任务及三个新数据集，覆盖144个国家的728个独特的文化事件。我们对20个LVLM和11个大型语言模型进行了系统评估。分析发现，模型表现出强烈的西方文化偏见，同时模型大小与性能之间存在显著关联。研究还显示，多模态输入和外部地理线索显著提升了模型的表现。此外，模型在识别具体文化特征（如食物）方面优于理解抽象概念（如仪式），反映出它们更擅长识别广泛文化起源而在细微理解上存在挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:19:11 GMT</pubDate>
</item>
<item>
<title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title>
<link>https://arxiv.org/abs/2502.11573</link>
<guid>https://arxiv.org/abs/2502.11573</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 04:32:22 GMT</pubDate>
</item>
<item>
<title>ActionPiece: 通过上下文增强的动作序列标记方法</title>
<link>https://arxiv.org/abs/2502.13581</link>
<guid>https://arxiv.org/abs/2502.13581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ActionPiece，通过上下文增强提高动作序列的推荐性能。</p><br /><br /><p><strong>摘要：</strong> 生成推荐（GR）是一种新兴的推荐范式，旨在将用户动作标记为离散的模式并进行自回归预测。现有的GR模型独立标记每个动作，未考虑上下文关系，导致相同动作用于不同上下文时表现不佳。为了解决这一问题，本文提出了ActionPiece，将动作与特征集相结合进行标记，构建上下文感知的动作序列。ActionPiece通过合并特征模式生成新的标记，并引入集合排列正则化，以处理特征集的无序性。实验结果表明，ActionPiece在多个公共数据集上显著优于现有的方法，使NDCG@10指标提高了6.00%至12.82%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 03:56:54 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Memories: 提升线性序列建模的新架构</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mixture-of-Memories架构，提升线性序列建模的记忆能力。</p><br /><br /><p><strong>摘要：</strong> 线性序列建模方法如线性注意力、状态空间建模和线性RNN在训练和推理中显著提升了效率，但通常将整个输入序列压缩为单一固定大小的记忆状态，导致在需回忆能力强的下游任务上的性能不佳。本文提出了一种新架构Mixture-of-Memories（MoM），灵感源自神经科学，采用多个独立记忆状态，由路由网络将输入令牌分配给特定记忆状态。这一方案极大增强了记忆容量，减少了记忆干扰，从而使MoM在记忆需求高的任务上表现优异，超越了现有的线性序列建模技术。尽管集成了多个记忆状态，MoM在训练中保持线性复杂度，在推理中保持恒定复杂度，使其保持了计算优势。实验结果表明，MoM在下游语言任务中显著优于当前线性序列模型，在需回忆的任务中甚至表现出与Transformer模型相当的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13685" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 02:40:09 GMT</pubDate>
</item>
<item>
<title>名字与身份：大型语言模型的偏见研究</title>
<link>https://arxiv.org/abs/2502.11995</link>
<guid>https://arxiv.org/abs/2502.11995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨名字对人类身份的影响及其在大型语言模型中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨名字与人类身份之间的深刻联系，分析名字在文化遗产、个体历史和个人化中的作用。尽管名字可以作为身份的标记，但简单地依赖于名字可能会导致对复杂身份的过于简化。在与大型语言模型（LLMs）交互时，用户名字是个人化的重要信息，可能通过直接输入、任务上下文以及存储的用户信息呈现。我们的研究通过测量文化假设，观察在常见建议查询中LLMs生成的反应，发现LLMs对名字 предполагают 强烈的文化身份假设。该研究为设计更加细致的个人化系统提供了重要启示，以避免增强刻板印象，同时保持有意义的定制。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:20:46 GMT</pubDate>
</item>
<item>
<title>SongGen：基于文本的可控歌曲生成模型</title>
<link>https://arxiv.org/abs/2502.13128</link>
<guid>https://arxiv.org/abs/2502.13128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SongGen是一种新型的文本到歌曲生成模型，支持细粒度控制音乐属性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SongGen的开放源代码单阶段自回归变换器，用于可控歌曲生成。该模型实现了对歌词、乐器、流派、情绪和音色等各种音乐属性的细粒度控制，并提供可选的三秒参考音频片段以进行声纹克隆。SongGen在统一的自回归框架中支持混合模式和双轨模式两种输出方式，使得生成的声音合唱和伴奏能够直接混合或分别合成，提供了更大的下游应用灵活性。作者还探索了多种令牌模式策略，显著提高了生成效果并提供了有价值的见解。此外，设计了一套自动数据预处理管道，实现了有效的质量控制。该项目的模型权重、训练代码、注释数据和预处理管道将向社区发布，以促进未来的研究和互动。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:07:44 GMT</pubDate>
</item>
<item>
<title>大型语言模型的安全对齐漏洞分析</title>
<link>https://arxiv.org/abs/2502.13946</link>
<guid>https://arxiv.org/abs/2502.13946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨模板锚定安全对齐对大型语言模型的影响及其脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文分析了大型语言模型（LLMs）在安全对齐方面的脆弱性，认为其初始行为易受到简单攻击的影响。我们提出了“模板锚定安全对齐”的概念，认为模板区域的信息聚合过度影响了模型的安全决策，导致其在面临推理时的越狱攻击时易受影响。经过广泛实验，我们验证了这一现象在多种对齐LLMs中普遍存在。机械分析表明，这种安全对齐方式使模型对攻击高度敏感。此外，本文提出将安全机制与模板区域分离的方案，有望减轻本质上的安全弱点。我们呼吁后续研究以减少对模板区域的依赖，开发更稳健的安全对齐技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:54:57 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-VL：视觉语言系列最新旗舰模型</title>
<link>https://arxiv.org/abs/2502.13923</link>
<guid>https://arxiv.org/abs/2502.13923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-VL在视觉理解与交互方面取得重大进展。</p><br /><br /><p><strong>摘要：</strong> Qwen2.5-VL是Qwen视觉语言系列的最新旗舰模型，在基础能力和创新功能上均取得了显著进展。该模型通过增强的视觉识别、精准的物体定位、强大的文档解析及长视频理解，实现了对世界的深刻理解与互动。其独特的物体定位功能能够通过边界框或点实现精确定位，并提供来自发票、表单和表格的结构化数据提取，此外还支持图表、图示和布局的详细分析。为处理复杂输入，Qwen2.5-VL引入动态分辨率处理和绝对时间编码，使得其能够处理不同尺寸的图像和最长可达数小时的视频，具备秒级事件定位能力。通过从零开始训练本地动态分辨率的视觉变换器（ViT），并结合窗口注意机制，Qwen2.5-VL在节省计算资源的同时，保持原始分辨率，展现出在静态图像和文档理解方面的优异表现，可以在实际场景中作为互动视觉代理进行推理、工具使用及任务执行。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:35:06 GMT</pubDate>
</item>
<item>
<title>提高大语言模型推理时效的信心评估</title>
<link>https://arxiv.org/abs/2502.13962</link>
<guid>https://arxiv.org/abs/2502.13962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨模型推理时的置信度问题，并提出评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在推理过程中大语言模型的计算扩展能力和置信度评估。现有测试时间扩展评估通常假设推理系统应对每个问题提供答案，这忽视了模型对答案信心及提供答案的适宜性。在此基础上，我们提取了推理过程中的置信度评分，以便在模型回答时进行阈值判断。研究发现，增加推理预算不仅能提升正确回答的数量，还能增强正确回答的置信度。此外，本文还扩展了现有的零风险响应评估范式，考虑了非零响应风险的设定，并建议在这些情况下报告评估结果的方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:34:43 GMT</pubDate>
</item>
<item>
<title>Thinking Preference Optimization：提升长链推理能力的有效方法</title>
<link>https://arxiv.org/abs/2502.13173</link>
<guid>https://arxiv.org/abs/2502.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ThinkPO方法以提升模型的长链推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Thinking Preference Optimization（ThinkPO），一种用于提高小型语言模型（LLMs）长链推理能力的后期优化方法。Supervised Fine-Tuning（SFT）虽有效提升推理能力，但获取新数据成本高且重复训练常导致性能停滞。ThinkPO通过利用易得的短链推理回答作为拒绝答案，以及长链推理回答作为选择答案，实施直接偏好优化，鼓励模型倾向于产生更长的推理输出。实验结果表明，ThinkPO显著提升了SFT模型的推理性能，其中数学推理准确率提高了8.6%，输出长度增加了25.9%。此外，ThinkPO还能够持续提升公开精炼的SFT模型性能，如DeepSeek-R1-Distill-Qwen-7B在MATH500数据集上的性能从87.4%提升至91.2%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:31:36 GMT</pubDate>
</item>
<item>
<title>NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2502.12638</link>
<guid>https://arxiv.org/abs/2502.12638</guid>
<content:encoded><![CDATA[
3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:18:32 GMT</pubDate>
</item>
<item>
<title>AdaptiveStep：基于自适应步长的过程奖励模型训练方法</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaptiveStep提供了一种新方法，提高过程奖励模型的训练效率。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的过程奖励模型（PRMs）训练方法，称为AdaptiveStep。现有方法往往依赖于固定的推理步骤和基于规则的技术，但未能有效捕捉文本中的决策点。AdaptiveStep通过模型在预测下一个单词时的置信度来划分推理步骤，从而在每个步骤中提供更多决策信息，这有助于提升下游任务的性能，例如奖励模型学习。实验结果表明，使用AdaptiveStep训练的PRMs在数学推理和代码生成任务中表现出色，超越了贪婪搜索策略和现有开源PRMs，在Best-of-N性能上达到了最新水平，同时减少了超过30%的构建成本。此外，文章还对PRMs的性能、迁移能力和泛化能力进行了深入分析和案例研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:07:01 GMT</pubDate>
</item>
<item>
<title>Crawl4LLM：基于优先评分的高效网页爬取方法</title>
<link>https://arxiv.org/abs/2502.13347</link>
<guid>https://arxiv.org/abs/2502.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crawl4LLM有效提高大型语言模型的预训练数据质量，减少爬取浪费。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了Crawl4LLM，一种高效的网页爬取方法，旨在改善大型语言模型（LLMs）预训练的数据质量。传统的网页抓取方法因其低质量数据而抛弃了大部分爬取的网页，而Crawl4LLM利用网页在LLMs预训练中的影响力，将其作为网页爬虫调度器的优先评分，取代了基于标准图连接的优先级。在对来自商业搜索引擎索引的9亿个网页的实验中，Crawl4LLM以仅爬取21%的URL达到了与之前更大规模爬虫相同的下游性能，显著减少了爬取的浪费，并减轻了对网站的负担。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:57:23 GMT</pubDate>
</item>
<item>
<title>Autellix：优化LLM服务系统的高效调度方法</title>
<link>https://arxiv.org/abs/2502.13965</link>
<guid>https://arxiv.org/abs/2502.13965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Autellix通过优化调度算法显著提升LLM程序的执行效率。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型(LLM)应用的发展，其功能已超越简单的聊天机器人，转向动态的通用代理程序。这一转变要求更高效的LLM服务系统，然而现有的系统忽视了程序与调用之间的依赖关系，导致显著的优化机会缺失。我们的分析指出，LLM请求和程序在处理时会出现长时间的累计等待，主要由于头排阻塞现象。为了解决这一问题，我们提出了Autellix，通过将程序作为一等公民来最小化端到端的延迟。Autellix拦截提交的LLM调用，并为调度器提供程序级上下文，提出了两种调度算法。这些算法能够基于程序之前的调用结果，对LLM请求进行优先级处理。实验表明，与现有最先进的系统，如vLLM相比，Autellix在多种LLM和代理工作负载下，能够以相同延迟提高程序的吞吐量4到15倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:42:06 GMT</pubDate>
</item>
<item>
<title>SearchRAG：提升医疗问答准确性的实时检索框架</title>
<link>https://arxiv.org/abs/2502.13233</link>
<guid>https://arxiv.org/abs/2502.13233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SearchRAG，通过实时检索提升医疗问答准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的SearchRAG框架，旨在提高大型语言模型（LLMs）在医疗问答中的准确性。传统的检索增强生成（RAG）技术通常依赖静态知识库，难以提供最新或详尽的医学信息。SearchRAG通过利用实时搜索引擎，生成合成查询，将复杂的医疗问题转化为更适合搜索引擎处理的查询。同时，该方法还应用不确定性知识选择，筛选和整合最相关的医学知识，确保LLM输入的信息质量。实验结果表明，SearchRAG显著提升了医疗问答任务的响应准确率，尤其在涉及复杂和详细知识的问题上表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:27:22 GMT</pubDate>
</item>
<item>
<title>基于3DGS的闭环强化学习在自动驾驶中的应用</title>
<link>https://arxiv.org/abs/2502.13144</link>
<guid>https://arxiv.org/abs/2502.13144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了一个基于3DGS的闭环强化学习方法提升自动驾驶性能。</p><br /><br /><p><strong>摘要：</strong> 现有的端到端自动驾驶算法通常采用模仿学习（IL）范式，但面临因果混淆和开放环路差距等挑战。本文建立了基于3DGS的闭环强化学习（RL）训练范式，通过利用3DGS技术，我们构建了现实物理世界的光学真实数字复制品，使自动驾驶策略能够广泛探索状态空间，并通过大规模试错来应对分布外场景。为了提高安全性，我们设计了专门的奖励，以指导策略有效应对安全关键事件并理解现实世界中的因果关系。此外，我们在RL训练中将IL纳入作为正则化项，以更好地与人类驾驶行为对齐。最后，我们引入了一个闭环评估基准，由多样化且之前未见过的3DGS环境组成。与基于IL的方法相比，RAD在大多数闭环指标上表现更强，尤其是碰撞率降低了3倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:13:49 GMT</pubDate>
</item>
<item>
<title>克服小模型学习差距的混合蒸馏策略</title>
<link>https://arxiv.org/abs/2502.12143</link>
<guid>https://arxiv.org/abs/2502.12143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出混合蒸馏策略以提高小模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究揭示了小模型学习能力的特殊现象，即小模型在长链推理或来自大模型的蒸馏中并未稳定受益，反而在较短、简单的推理链上表现更佳。为应对这一问题，本文提出了混合蒸馏（Mix Distillation）策略，该策略通过结合长短推理示例，或从大模型与小模型的推理中进行平衡，取得了显著的效果。实验表明，混合蒸馏能有效提升小模型的推理表现，远超单独训练长或短数据的表现。这些发现强调了直接强模型蒸馏的局限性，并突出了在有效转移推理能力时，适应推理复杂性的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:38:13 GMT</pubDate>
</item>
<item>
<title>通过LongPO提升短文档LLM在长文档任务中的表现</title>
<link>https://arxiv.org/abs/2502.13922</link>
<guid>https://arxiv.org/abs/2502.13922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongPO方法使短文档LLM在长文档任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> Large Language Models (LLMs)在短文档任务中表现出色，但在长文档场景中常常由于长文档对齐不足而导致性能下降。本文提出的LongPO方法，通过自我演化，使短文档LLM能够在长文档任务中表现出色。该方法通过生成的短到长偏好数据，包含对于相同指令的长文档输入和其压缩短文档对应的成对响应，帮助LLM在长文档任务中学习和适应。同时，LongPO采用短到长的KL约束，旨在减轻长文档对齐过程中短文档性能的下降。经过应用于Mistral-7B-Instruct-v0.2模型，LongPO在128K到512K上下文长度的实验中，充分保留了短文档性能，并在长短文档任务中表现显著优于传统策略，取得的长文档基准结果可与需大量长文档标注的高级LLM（如GPT-4-128K）相媲美，甚至在某些情况下超越其表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:35:20 GMT</pubDate>
</item>
<item>
<title>提升大型语言模型决策能力的自动化奖励模型框架</title>
<link>https://arxiv.org/abs/2502.12130</link>
<guid>https://arxiv.org/abs/2502.12130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无人工标注学习奖励模型框架，以改善LLM代理的决策能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在文本生成任务上表现优异，但在需要多步骤决策和环境反馈的任务上仍显不足。为了解决LLM代理的局限性，本文提出了一种自动学习奖励模型的框架，该模型无需人工标注数据。通过让一个LLM代理随机导航环境，生成多种动作轨迹，随后利用另一个LLM为每个轨迹分配任务意图，并生成正向和负向响应，构建任务意图-正向响应-负向响应的三元组，作为训练数据来优化奖励模型。此模型能够评分动作轨迹，进而为任务规划提供启发式指导。研究表明该框架在不同代理基准测试中的有效性和普遍性，标志着在复杂互动环境中提升LLM代理决策能力的重要进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 18:20:05 GMT</pubDate>
</item>
<item>
<title>基于注意力机制的YOLOv12框架提升目标检测性能</title>
<link>https://arxiv.org/abs/2502.12524</link>
<guid>https://arxiv.org/abs/2502.12524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOv12实现了速度与注意力机制性能的最佳结合，超越传统CNN模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的YOLO框架YOLOv12，该框架借助注意力机制，实现了与传统CNN模型相同的推理速度，同时提升了目标检测的准确性。与以往的YOLOv10-N和YOLOv11-N相比，YOLOv12-N在T4 GPU上实现了40.6% mAP，仅需1.64毫秒的推理延迟，分别超越了2.1%和1.2%的mAP。此外，YOLOv12还在其他模型规模中展现出显著优势，比如在与RT-DETR系列模型的比较中，YOLOv12-S在运行速度上快42%，计算量及参数量均仅为后者的36%和45%。这些结果表明，YOLOv12在准确性和速度之间达成了优良的平衡，推动了实时目标检测技术的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 13:39:32 GMT</pubDate>
</item>
<item>
<title>视觉模型在时间序列分析中的优势与未来研究方向</title>
<link>https://arxiv.org/abs/2502.08869</link>
<guid>https://arxiv.org/abs/2502.08869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了视觉模型在时间序列分析中的应用及其优势。</p><br /><br /><p><strong>摘要：</strong> 时间序列分析经历了从传统自回归模型、深度学习模型，到最近的变换器和大型语言模型（LLMs）的发展。然而，尽管已有一些利用视觉模型进行时间序列分析的努力，这一领域仍较少被关注。本文讨论了视觉模型在时间序列分析中相较于LLMs的优势，提供了现有方法的全面概述，并回答了如何将时间序列编码为图像以及如何为各种任务建模图像时间序列的关键研究问题。此外，文章还探讨了该框架中预处理和后处理步骤所面临的挑战，并概述了未来研究的方向，以进一步推动视觉模型在时间序列分析中的应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 10:33:08 GMT</pubDate>
</item>
<item>
<title>创新推理方法Flow-of-Options在AutoML中的应用</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flow-of-Options提高了大型语言模型在自动机器学习任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的推理方法Flow-of-Options（FoO），旨在解决大型语言模型（LLMs）中的内在偏见。FoO能够系统地探索各种推理的可能性，本文展示了基于FoO的自主系统在处理机器学习任务（AutoML）方面的应用。该框架在标准数据科学任务上相比于最先进的基线提高了38.2%至69.2%，在治疗化学任务上提高了37.4%至47.9%。整体操作成本每个任务不超过1美元，适合成本敏感的应用场景。除了分类和回归，本文还展示了FoO系统在强化学习和图像生成等任务中的广泛适用性。通过增强LLM解决方案的多样性，FoO提供了显著的进步，并且在结合案例推理时支持长期记忆，带来了更好的可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 08:03:59 GMT</pubDate>
</item>
<item>
<title>Text2World: 基于语言模型的符号世界模型生成新基准</title>
<link>https://arxiv.org/abs/2502.13092</link>
<guid>https://arxiv.org/abs/2502.13092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Text2World基准，通过新的评估方式提升语言模型的世界建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了近年来利用大型语言模型（LLMs）从文本描述生成符号世界模型的研究。针对以往研究中遇到的评估随机性、间接指标依赖和领域范围有限等问题，作者提出了一个新基准Text2World，该基准基于规划领域定义语言（PDDL），涵盖数百个多样化的领域，并采用多标准、执行基础的评估指标，以提供更可靠的评估。通过Text2World基准测试当前的语言模型，发现经过大规模强化学习训练的推理模型表现优异，但即便是表现最佳的模型在世界建模能力上仍显局限。基于这些发现，作者审视了几种有前景的策略，以提升LLMs的世界建模能力，包括测试时扩展、代理训练等。希望Text2World能够为未来的相关研究提供重要资源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 07:53:04 GMT</pubDate>
</item>
<item>
<title>Atom of Thoughts: 通过原子问题提升推理能力的框架</title>
<link>https://arxiv.org/abs/2502.12018</link>
<guid>https://arxiv.org/abs/2502.12018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Atom of Thoughts通过分解问题提升大型语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架Atom of Thoughts (AoT)，旨在解决大型语言模型在推理过程中因历史信息积累而造成的资源浪费和推理干扰问题。AoT通过将当前问题分解为依赖性的无向图，从而形成新的原子问题状态，每个原子问题都是自包含且可验证的。这一迭代的分解-收缩过程直至达到可直接解决的原子问题，从而实现类似于马尔可夫过程的状态转移。实验表明，AoT能够有效提升推理能力，无论作为独立框架还作为现有测试时间扩展方法的插件，均表现优异。尤其在HotpotQA基准测试中，当应用于gpt-4o-mini时，AoT达到了80.6%的F1分数，比o3-mini高出3.4%，比DeepSeek-R1高出10.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:51:04 GMT</pubDate>
</item>
<item>
<title>优化分布式训练的通信与计算重叠技术研究</title>
<link>https://arxiv.org/abs/2502.12996</link>
<guid>https://arxiv.org/abs/2502.12996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了通过重叠通信与计算来优化分布式训练速度的方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了使用分布式优化方法（如DiLoCo）训练大型模型时的通信效率问题。在分布式工作环境下，传统的数据并行训练需要大量通信，尽管DiLoCo的更新拆分为内部优化和外部优化两个阶段减少了通信需求，但在数据中心场景下，外部优化步骤的阻塞仍会导致显著的延迟。为了解决这一问题，本文提出了一种重叠通信与计算的技术，使外部优化步骤与内部优化阶段能够完全重叠。研究表明，名为“急切更新”的特定变体在低带宽的工作环境中，其性能与标准DiLoCo相当，展现出有效的优化潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:13:51 GMT</pubDate>
</item>
<item>
<title>金融领域文本嵌入基准测试与模型评估</title>
<link>https://arxiv.org/abs/2502.10990</link>
<guid>https://arxiv.org/abs/2502.10990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出金融领域的大规模文本嵌入基准FinMTEB及其评估结果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的进展，嵌入模型在自然语言处理（NLP）应用中发挥着至关重要的作用。尽管这些模型通常在通用数据集上进行基准测试，但现实应用迫切需要领域特定的评估。本文介绍了金融大规模文本嵌入基准（FinMTEB），这是一个针对金融领域的专用基准，涵盖64个金融领域特定的嵌入数据集，涉及7个任务，包括金融新闻、公司年报、环境、社会和治理（ESG）报告、监管文件及财报电话会议记录。我们还基于角色数据合成方法开发了一个金融适应模型FinPersona-E5。通过对15个嵌入模型的广泛评估，包括FinPersona-E5，我们的研究发现：通用基准测试的性能与金融领域任务的相关性有限，领域适应模型始终优于通用模型，此外，简单的词袋模型在金融语义文本相似性任务中超越了复杂的密集嵌入技术，突显出密集嵌入方法的当前局限性。我们的工作为金融NLP应用建立了一个稳健的评估框架，并为开发领域特定的嵌入模型提供了关键见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:54:27 GMT</pubDate>
</item>
<item>
<title>序列令牌压缩的极限研究及优化潜力</title>
<link>https://arxiv.org/abs/2502.13063</link>
<guid>https://arxiv.org/abs/2502.13063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示序列令牌的压缩比可达1500，揭示优化空间巨大。</p><br /><br /><p><strong>摘要：</strong> 近年来的研究关注于将令牌序列压缩成较短的实值向量序列，以替代令牌嵌入或键值缓存，这些方法能够减少现有语言模型的计算量。尽管基于强大的编码模型，现有方案的最大无损压缩比通常仅为10。这一现象引人深思，因为理论上即使是16位精度和适中向量大小，大型实值向量的最大信息容量远超此速率。本研究通过用逐样本优化程序替换编码器，探究压缩的极限，结果显示压缩比高达1500，这凸显出现有方法与实用解决方案之间的两个数量级的差距。此外，我们还实证表明，压缩的极限并非由输入长度决定，而是由需减少的不确定性，即该序列的交叉熵损失，这一发现强调了输入嵌入的理论能力与实际利用之间的显著差距，表明在模型设计中存在显著优化空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:43:42 GMT</pubDate>
</item>
<item>
<title>提升变换器性能的层集成记忆方法研究</title>
<link>https://arxiv.org/abs/2502.09245</link>
<guid>https://arxiv.org/abs/2502.09245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出层集成记忆方法，以提升变换器的表示能力和性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对标准变换器在处理历史信息时只使用相邻层的表示，导致表示崩溃和性能欠佳的问题，提出了一种新的解决方案——层集成记忆（LIMe）。该方法允许模型访问早期层的隐藏状态，从而扩展表示能力，而仍然保持整体内存占用。通过在多种架构和查找机制下进行广泛实验，我们展示了在多项任务上表现出一致的性能提升。此外，我们对学习到的表示动态进行分析，并探索了深度电路的应用，揭示了LIMe在层间集成信息的机制，为未来研究指明了有前景的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 03:03:51 GMT</pubDate>
</item>
<item>
<title>增强大型语言模型的领域知识方法综述</title>
<link>https://arxiv.org/abs/2502.10708</link>
<guid>https://arxiv.org/abs/2502.10708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨如何通过知识集成提升大型语言模型的领域适应性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在自然语言理解、文本摘要和机器翻译等任务中取得了显著成功，但其通用特性限制了在特定领域应用中的有效性。为此，研究人员探索了多种方法来增强大型语言模型的领域知识。本综述将这些方法归纳为四种关键途径：动态知识注入、静态知识嵌入、模块化适配器和提示优化。这些方法为大型语言模型提供了领域专业知识的独特机制，平衡了灵活性、可扩展性和效率之间的权衡。文中还讨论了这些方法在特化任务中的应用，比较了领域特定的大型语言模型与通用模型的优缺点，同时指出了这一新兴领域的挑战与机遇。此外，还总结了常用的数据集和基准，以帮助研究者更深入了解这一领域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10708" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:56:09 GMT</pubDate>
</item>
<item>
<title>增强的钙钛矿太阳能电池知识管理系统</title>
<link>https://arxiv.org/abs/2502.12669</link>
<guid>https://arxiv.org/abs/2502.12669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种集成钙钛矿太阳能电池的知识管理系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种集成化的钙钛矿太阳能电池(PSC)知识增强系统，旨在提高该领域研究的知识管理与推理效率。系统包括三个主要组成部分：首先，构建了Perovskite-KG知识图谱，从1517篇研究论文中提取了23789个实体和22272个关系；其次，创建了两个互补数据集：Perovskite-Chat，涵盖了55101个高质量问答对，以及Perovskite-Reasoning，包含2217个精心策划的材料科学问题；最后，引入了两个专门的大型语言模型：用于领域特定知识辅助的Perovskite-Chat-LLM和用于科学推理任务的Perovskite-Reasoning-LLM。实验结果表明，该系统在领域特定知识检索和科学推理任务上显著优于现有模型，为PSCs领域的研究人员提供了有效的文献回顾、实验设计及复杂问题解决工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:47:33 GMT</pubDate>
</item>
<item>
<title>OctoTools：一个面向多领域复杂推理任务的开放源框架</title>
<link>https://arxiv.org/abs/2502.11271</link>
<guid>https://arxiv.org/abs/2502.11271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OctoTools提供了一个高效的框架，用于解决多领域的复杂推理任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OctoTools，这是一个无需训练、用户友好且易于扩展的开源代理框架，旨在解决多领域的复杂推理任务。OctoTools通过标准化工具卡来封装工具功能，设有高层次和低层次的规划器，以及执行器来实现工具的使用。我们在16个不同任务（包括MathVista、MMLU-Pro、MedQA和GAIA-Text）上验证了OctoTools的广泛适用性，平均准确率提高了9.3%，超越了GPT-4o。此外，OctoTools在相同工具集合下的表现，比AutoGen、GPT-Functions和LangChain高出10.6%。通过全面分析与消融实验，OctoTools在任务规划、工具有效使用和多步骤问题解决上展现出明显的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:27:36 GMT</pubDate>
</item>
<item>
<title>ARM4R：基于人类视频数据的自动回归机器人模型</title>
<link>https://arxiv.org/abs/2502.13142</link>
<guid>https://arxiv.org/abs/2502.13142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ARM4R模型，利用4D表示提升机器人控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ARM4R，一个基于人类视频数据的自动回归机器人模型，通过利用低级4D表示，旨在提升机器人的预训练效果。具体而言，ARM4R从视频中提取的3D点追踪表示，通过单目深度估计将2D表示提升为3D，使得这些4D表示在点与机器人状态表示之间保持共享几何结构，仅需线性变换即可实现高效的迁移学习。实验结果表明，ARM4R能够有效地将人类视频数据转移至机器人控制任务中，并在多种机器人环境和配置下显著改进任务表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:24:26 GMT</pubDate>
</item>
<item>
<title>基于动态提示的无提示微调方法研究</title>
<link>https://arxiv.org/abs/2502.12859</link>
<guid>https://arxiv.org/abs/2502.12859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的无提示微调方法，以增强大型语言模型的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种称为无提示微调（PAFT）的方法，旨在提高大型语言模型（LLMs）在微调后的提示鲁棒性。研究表明，微调后，LLMs能够适应下游任务，但这种适应性常常导致提示鲁棒性降低，细微的提示变化会显著影响模型性能。PAFT 通过在微调过程中动态调整提示，激励模型学习任务的基本原则，而不是过于依赖特定的提示表达。该方法分为两个阶段：首先构建多样化的合成候选提示集；其次在微调过程中从该集合中随机采样提示，生成动态训练输入。通过对多个数据集和 LLMs 的广泛实验，证明使用 PAFT 训练的模型在各种提示（包括未见过的提示）下展现出强大的鲁棒性和泛化能力。此外，这一增强的鲁棒性还提高了模型的性能和推理速度，同时保持了训练效率。消融研究进一步确认了 PAFT 的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:21:54 GMT</pubDate>
</item>
<item>
<title>Soundwave: 一种高效的语音到文本大语言模型训练方法</title>
<link>https://arxiv.org/abs/2502.12900</link>
<guid>https://arxiv.org/abs/2502.12900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Soundwave是一种提高语音到文本模型训练效率的新方法，表现优于前作。</p><br /><br /><p><strong>摘要：</strong> 现有的端到端语音大语言模型通常依赖于大规模的标注数据进行训练，但对于数据高效训练的讨论较少。本文聚焦于语音与文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，结合高效的训练策略和新颖的架构，成功解决了这些问题。实验结果表明，Soundwave在语音翻译和AIR-Bench语音任务中，使用仅为训练数据的五十分之一的情况下，超越了先进的Qwen2-Audio模型。此外，进一步分析显示，Soundwave在对话中仍保持其智能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 00:22:36 GMT</pubDate>
</item>
<item>
<title>Magma：多模态AI代理任务的基础模型</title>
<link>https://arxiv.org/abs/2502.13130</link>
<guid>https://arxiv.org/abs/2502.13130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magma是一个新型多模态基础模型，具备数字与物理世界的智能代理能力。</p><br /><br /><p><strong>摘要：</strong> Magma是一个前沿的基础模型，旨在执行多模态AI代理任务，涵盖数字和物理世界。相较于传统的视觉-语言(VL)模型，Magma不仅具备VL理解能力（语言智能），同时具备在视觉-空间世界中进行规划和行动的能力（时空智能）。为了实现智能代理功能，Magma在大量异构数据集上进行了预训练，这些数据集包括图像、视频和机器人数据。具体而言，图像中的可操作视觉对象通过Set-of-Mark (SoM)进行标记，而视频中的对象运动通过Trace-of-Mark (ToM)进行标记，从而为行动提供基础。实验结果显示，SoM和ToM的结合显著提升了Magma的时空智能，使其在UI导航和机器人操作等任务上打破了以往的记录，优于专门为这些任务设计的模型。同时，在图像和视频相关的多模态任务中，Magma也与其他训练在更大数据集上的大型多模态模型相比较，表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:51:36 GMT</pubDate>
</item>
<item>
<title>测试时间缩放在大型语言模型中的应用与效果研究</title>
<link>https://arxiv.org/abs/2502.12215</link>
<guid>https://arxiv.org/abs/2502.12215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了大型语言模型的测试时间缩放及其对推理能力的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在推理过程中测试时间缩放的能力，尤其是OpenAI的o1系列模型。尽管后续模型如QwQ、Deepseek-R1（R1）和LIMO也声称具备类似能力，但其实际效果仍需进一步探讨。研究发现，更长的chain of thought（CoT）并不总能提高准确性，反而对于同一问题，正确答案的长度往往短于错误答案。深入分析后发现，这一现象与模型的自我修正能力密切相关，长CoT中包含较多的自我修正，常导致性能的降低。本文进一步比较了QwQ、R1和LIMO的顺序与并行缩放策略，结果显示并行缩放在覆盖性和可扩展性上更优。基于此发现，提出了一种Shortest Majority Vote的方法，将并行缩放策略与CoT长度特征相结合，显著提高了模型的测试时间可扩展性，较传统多数投票方法表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:37:46 GMT</pubDate>
</item>
<item>
<title>SafeRoute：高效的安全守卫模型自适应路由方案</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeRoute通过自适应路由提升了安全守卫模型的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 部署大型语言模型需要高效的安全守卫模型来检测和阻止有害用户提示。虽然大型安全守卫模型性能强大，但计算成本高昂。为此，研究提出了SafeRoute，一种双重路由器，旨在区分困难示例和简单示例。该方法通过仅在路由器认定为困难的输入上应用大型安全守卫模型，提高了模型选择的效率，同时保持了较高的准确性。实验表明，相比单独使用大型模型，自适应选择显著提升了计算成本与安全性能之间的平衡，并在多个基准数据集上超越了相关基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:23:34 GMT</pubDate>
</item>
<item>
<title>MUDD连接：提升Transformer跨层信息流动的有效方法</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUDD连接改善了Transformer的残差连接，显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种简单有效的MUDD连接方法，以解决残差连接的局限性，并增强Transformer中的跨层信息流动。与现有的静态共享连接权重方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接能够无缝地集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformers，表现出相当于经过1.8X-2.4X计算训练的Transformers的性能。值得注意的是，MUDDPythia-2.8B在预训练的每个词困惑度和下游任务中与Pythia-6.9B相匹配，并在五次少样本设置中甚至与Pythia-12B相媲美，同时仅增加了0.23%的参数和0.4%的计算量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12170" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:59:16 GMT</pubDate>
</item>
<item>
<title>XLM-SWCM：低资源语言文本生成的新框架</title>
<link>https://arxiv.org/abs/2502.10852</link>
<guid>https://arxiv.org/abs/2502.10852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出XLM-SWCM框架以提升低资源语言的文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的框架XLM-SWCM，用于在极低资源语言中适应多语言编码器以进行文本生成。尽管现有的多语言模型如XLM-R在自然语言处理上获得了进展，但在极低资源语言的表现仍然较差。此外，现代大型语言模型支持的语言种类远少于XLM-R，导致许多语言缺乏文本生成模型。通过重用编码器和解码器之间的权重，XLM-SWCM框架充分利用了编码器学习到的语义空间，从而实现了在低资源语言中的高效学习与有效泛化。我们将此框架应用于四种中国少数民族语言，并在多项下游任务中展示了其优越的性能，甚至超越了更大模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:46:16 GMT</pubDate>
</item>
<item>
<title>基于几何性质的连续扩散模型用于语言建模</title>
<link>https://arxiv.org/abs/2502.11564</link>
<guid>https://arxiv.org/abs/2502.11564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的连续扩散模型，针对语言建模中的离散数据。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的连续扩散模型，旨在解决传统离散扩散模型在语言建模中的局限性。现有的离散扩散模型在信号转换过程中易丢失信息，而现有的连续模型在离散数据上表现不佳，限制了扩散模型的发展。我们通过建立离散扩散与连续流动之间的联系，引入一种简化设计的扩散过程，能够更好地利用底层类别分布的几何结构。此外，基于辐射对称性，我们提出了一种无仿真训练框架，以应对流形的高维性。通过对语言建模基准和其他领域的全面实验，显示我们的方法在性能上优于现有的离散扩散模型，并接近自回归模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:43:02 GMT</pubDate>
</item>
<item>
<title>HealthGPT：融合医疗视觉的强大语言模型</title>
<link>https://arxiv.org/abs/2502.09838</link>
<guid>https://arxiv.org/abs/2502.09838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HealthGPT 是一款强大的医疗视觉语言模型，具有优异的性能与可扩展性。</p><br /><br /><p><strong>摘要：</strong> HealthGPT 是一款强大的医疗大型视觉语言模型（Med-LVLM），能够在统一的自回归范式内整合医疗视觉理解与生成能力。其核心是逐步适应异构理解与生成知识到预训练的大语言模型（LLMs），采用新颖的异构低秩适应（H-LoRA）技术，结合量身定制的分层视觉感知方法和三阶段学习策略。为了有效训练 HealthGPT，我们开发了一个名为 VL-Health 的综合医学领域特定理解与生成数据集。实验结果显示，HealthGPT 在医疗视觉统一任务中表现出色且具有良好的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:35:23 GMT</pubDate>
</item>
<item>
<title>mmMamba：一种线性复杂度的多模态状态空间模型框架</title>
<link>https://arxiv.org/abs/2502.13145</link>
<guid>https://arxiv.org/abs/2502.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mmMamba框架通过知识蒸馏实现线性复杂度的多模态模型，提升效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）展现了卓越的性能，但在部署时面临计算复杂度高、缓存需求增长等挑战。为此，我们提出了mmMamba框架，通过对现有MLLM的逐步知识蒸馏，开发线性复杂度的多模态状态空间模型。该方法允许直接将经过训练的解码器-仅模型转化为线性复杂度架构，无需预训练的基于RNN的LLM或视觉编码器。通过提出播种策略和三阶段蒸馏流程，我们有效地将知识从Transformer转移到Mamba，同时保持多模态能力。经过Transformer的解码器-仅模型HoVLE蒸馏的mmMamba-linear在性能方面与现有的线性和二次复杂度视觉语言模型相竞争，而mmMamba-hybrid的性能进一步显著提升，接近HoVLE的能力。在103K tokens时，mmMamba-linear实现了20.6倍的加速和75.8%的GPU内存减少，而mmMamba-hybrid则实现了13.5倍的加速及60.2%的内存节省。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:08:27 GMT</pubDate>
</item>
<item>
<title>FLAG-Trader：一种融合语言处理与强化学习的金融交易模型</title>
<link>https://arxiv.org/abs/2502.11433</link>
<guid>https://arxiv.org/abs/2502.11433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FLAG-Trader，通过强化学习优化金融交易决策，提高多步骤任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FLAG-Trader，一种结合语言处理与强化学习的金融交易模型。大型语言模型（LLMs）在处理多模态金融数据时，展现出卓越的推理能力，但在需复杂决策的多步骤互动金融市场（如交易）中，表现尚不理想。FLAG-Trader通过将部分微调的LLM作为策略网络，利用预训练知识与金融领域的参数高效微调相结合，提升了决策过程的表现。借助政策梯度优化，在交易奖励的驱动下，FLAG-Trader不仅优化了交易性能，还显著改善了其他金融领域任务的表现。我们提供了大量实证数据来验证这些提升效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:06:19 GMT</pubDate>
</item>
<item>
<title>Decomposed Reward Models: 提取人类偏好的新方法</title>
<link>https://arxiv.org/abs/2502.13131</link>
<guid>https://arxiv.org/abs/2502.13131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRMs通过二元比较提取人类偏好，为个性化AI提供了新视角。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的奖赏模型——分解奖赏模型（DRMs），旨在有效提取多样化的人类偏好。传统奖励模型在捕捉偏好的复杂性方面存在局限性。DRMs不需要细粒度的注释，而是通过二元比较来分析人类偏好，并将其表示为向量。采用主成分分析（PCA）对偏好的数据进行分析，构造了偏好与拒绝响应之间的嵌入差异数据集。通过识别正交基向量，DRMs能够捕捉偏好的不同维度，如有帮助性、安全性和幽默感等。这些分解的奖励可以灵活组合，以适应不同用户的需求，从而提供一个可解释且可扩展的替代方案。我们的结果表明，DRMs不仅有效提取偏好维度，还能在无额外训练的情况下适应新用户，展示了其在个性化和可解释性大型语言模型对齐中的强大能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:59:45 GMT</pubDate>
</item>
<item>
<title>HEADINFER: 一种高效的长序列推理策略</title>
<link>https://arxiv.org/abs/2502.12574</link>
<guid>https://arxiv.org/abs/2502.12574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HEADINFER通过头部级别的KV缓存卸载，显著降低推理内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了HEADINFER，一种优化长序列生成模型的推理内存使用的新策略。通过将关键值缓存（KV缓存）卸载到CPU RAM，HEADINFER避免了在GPU上完全存储Transformer层的KV缓存。该方法采用细粒度的头部级别卸载策略，仅在GPU上维护选择性的注意力头的KV缓存，同时动态计算注意力输出。通过Roofline分析，我们展示了HEADINFER在保持计算效率的同时，大幅降低了内存占用。在对Llama-3-8B模型进行评估时，HEADINFER能够将KV缓存的GPU内存占用从128 GB减少到1 GB，总体GPU内存使用从207 GB减少到17 GB，相比BF16基线推理实现了92%的减少。值得一提的是，HEADINFER使得在单个24GB显存的消费级GPU（如NVIDIA RTX 4090）上实现4百万令牌的推理成为可能，而无需采用近似方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:57:00 GMT</pubDate>
</item>
<item>
<title>Phantom: Subject-consistent video generation via cross-modal alignment</title>
<link>https://arxiv.org/abs/2502.11079</link>
<guid>https://arxiv.org/abs/2502.11079</guid>
<content:encoded><![CDATA[
The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:56:39 GMT</pubDate>
</item>
<item>
<title>基于人群比较评估的LLM自动评价方法的改进</title>
<link>https://arxiv.org/abs/2502.12501</link>
<guid>https://arxiv.org/abs/2502.12501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人群比较的评估方法以提升LLM判断的可靠性。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-as-a-Judge逐渐成为自动评估的主流方法，其基于链式推理（CoT）所产生的判断却存在可靠性不足的问题，主要由于CoT推理无法捕捉到深入且全面的细节而导致的输出不完整。现有方法多依赖于多数投票或标准扩展，这未能有效解决CoT的局限性。为此，我们提出了一种人群比较评估方法，增加了额外的人群反馈来与候选回应进行比较，从而揭示候选回应中的更深层次和更全面的细节。这一过程有效地引导LLM-as-a-Judge提供更详尽的CoT判断。通过大量实验，我们的方法在五个基准测试中平均提高了6.7%的评估准确性，并产生了更高质量的CoT，进一步推动了判断蒸馏与在监督微调过程中的更高效表现。我们的分析表明，所生成的CoT在全面性和质量上优于现有方法，并且随着推理规模的扩大，评估准确性不断改善。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:55:26 GMT</pubDate>
</item>
<item>
<title>RealSyn：用于视觉-语言表示学习的真实与合成文本数据集</title>
<link>https://arxiv.org/abs/2502.12513</link>
<guid>https://arxiv.org/abs/2502.12513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealSyn数据集通过真实与合成文本增强视觉-语言表示学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RealSyn数据集的构建，该数据集结合了高质量的真实文本和合成文本，以提升视觉-语言表示学习的性能。通过建立一条真实世界数据提取管道，提取高质量图像和文本，并设计层次检索方法有效关联图像与多个语义相关的文本，本文充分利用了未配对的数据。此外，为了增强细粒度视觉信息，提出了图像语义增强生成模块用于合成文本的生产，同时采用了语义均衡抽样策略以提高数据集多样性，从而更好地学习长尾概念。RealSyn可在15M、30M和100M三个规模上使用，实验表明，基于RealSyn预训练的模型在多个下游任务上达到了最新的性能，极大推动了视觉-语言表示学习的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:52:22 GMT</pubDate>
</item>
<item>
<title>利用自然语言定义物体方向以增强机器人操作能力</title>
<link>https://arxiv.org/abs/2502.13143</link>
<guid>https://arxiv.org/abs/2502.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过自然语言定义物体方向，以提升机器人的操作能力。</p><br /><br /><p><strong>摘要：</strong> 空间智能是具身AI的关键组成部分，使机器人能够理解并与环境互动。尽管现有视觉语言模型在理解物体位置和关系方面已取得进展，但仍缺乏对物体方向的精确理解，尤其是在细致操作任务中的需求。为解决这一限制，本文提出通过自然语言来定义语义方向，形成了一种更灵活的表示方式。我们构建了OrienText300K数据集，包含带有语义方向的3D模型，旨在将几何理解与功能语义连接。通过整合语义方向进VLM系统，我们的研究使机器人能够在操作中同时考虑位置和方向的约束，实验表明，模型在仿真和真实场景中显著提升了操作准确率，如在Open6DOR上达到48.7%的准确率，SIMPLER上达到74.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:51:33 GMT</pubDate>
</item>
<item>
<title>EQ-VAE：提升潜在生成模型的等变性</title>
<link>https://arxiv.org/abs/2502.09509</link>
<guid>https://arxiv.org/abs/2502.09509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出EQ-VAE，提升潜在生成模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 潜在生成模型已成为高质量图像合成的领先方法，然而现有自编码器在应对尺度和旋转等语义保留变换时缺乏等变性，导致潜在空间复杂性增加，从而影响生成性能。为此，本文提出了EQ-VAE，这是一种简单的正则化方法，旨在在潜在空间内强制实现等变性，降低其复杂性而不损害重构质量。通过对预训练自编码器进行EQ-VAE微调，本文显著提升了包括DiT、SiT、REPA和MaskGIT在内的多种最先进生成模型的性能，其中DiT-XL/2在五个SD-VAE微调周期内实现了7倍的加速。EQ-VAE适用于连续和离散自编码器，从而为多种潜在生成模型提供了灵活的增强工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 14:56:45 GMT</pubDate>
</item>
<item>
<title>多模态检索增强生成系统综述</title>
<link>https://arxiv.org/abs/2502.08826</link>
<guid>https://arxiv.org/abs/2502.08826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本综述分析了多模态检索增强生成系统的挑战和进展。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型因依赖静态训练数据而面临幻觉和过时知识的问题。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，进而提高输出的真实性和时效性。近期的多模态学习进展促成了多模态RAG的发展，将文本、图像、音频和视频等多种模态结合以增强生成效果。然而，跨模态对齐和推理为多模态RAG带来了独特挑战。本文综述了多模态RAG系统，涵盖数据集、指标、基准、评估、方法论及创新等方面，详细审视训练策略、鲁棒性增强及损失函数，并探讨多样化的多模态RAG场景及未来研究方向。该综述为构建更强大、可靠的人工智能系统奠定了基础，旨在有效利用多模态动态外部知识库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>IHEval：评估语言模型指令层级遵循能力的新基准</title>
<link>https://arxiv.org/abs/2502.08745</link>
<guid>https://arxiv.org/abs/2502.08745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IHEval基准评估语言模型在指令层级遵循中的表现与挑战。</p><br /><br /><p><strong>摘要：</strong> 指令层级在语言模型（LMs）中至关重要，确保系统消息、用户消息、对话历史和工具输出之间的优先顺序。然而，这一领域的研究相对较少，缺乏全面的评估基准。为此，我们推出了IHEval，一个新基准，包含3,538个示例，涵盖九项任务，特别是对优先级不同的指令的处理。我们的评估显示，流行的语言模型在识别指令优先级方面表现不佳，尤其在面对相互冲突的指令时，性能显著下降。最具竞争力的开源模型在解决此类冲突中仅获得48%的准确率。这些结果强调了未来在语言模型开发中需要针对性优化的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:21:05 GMT</pubDate>
</item>
<item>
<title>高效影响值估计的神经网络方法</title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为NN-CIFT的小型神经网络方法，以降低影响值估计的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用小型神经网络（称为InfluenceNetwork）来估计模型培训中的影响值，显著降低了计算成本，达到了99%的节约。传统的影响函数计算方法由于高昂的计算需求和内存消耗，在处理大型模型和数据集时效果不理想；而我们的方法能够以仅占全语言模型0.0027%的小型模型，进行有效的影响值估计。我们将此算法（NN-CIFT）应用于针对指令细化的子集选择任务中，结果显示在速度显著提升的情况下，无需牺牲性能，与四个先进的影响函数方法对比，均表现出良好的效果。此外，我们还对NN-CIFT进行了深入的超参数分析，证明其有效性和通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:04:04 GMT</pubDate>
</item>
<item>
<title>合成多模态网络任务数据集和探索者代理的研究</title>
<link>https://arxiv.org/abs/2502.11357</link>
<guid>https://arxiv.org/abs/2502.11357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文开发了一个多模态网络任务数据集，提升了代理的性能。</p><br /><br /><p><strong>摘要：</strong> 近期大型多模态模型（LMM）的成功应用已展现出自主完成复杂网络任务的潜力。尽管开源LMM代理在离线评估基准上取得了显著进展，但在更真实的在线环境中仍远未达到人类水平。本文提出了一种可扩展的方法，合成了迄今为止最大的多样化网络任务轨迹数据集，包含超过94K的成功任务轨迹，跨越49K个独特URL、720K张屏幕截图和33M个网页元素。本研究还介绍了“Explorer”多模态网络代理，并在多个基准测试中表现出色，验证了数据扩展对提高网络代理能力的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:57:43 GMT</pubDate>
</item>
<item>
<title>ILIAS：用于实例级图像检索的新测试数据集</title>
<link>https://arxiv.org/abs/2502.11748</link>
<guid>https://arxiv.org/abs/2502.11748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ILIAS数据集旨在评估图像检索模型对特定对象的识别能力。</p><br /><br /><p><strong>摘要：</strong> ILIAS是一个新推出的测试数据集，专为评估当前及未来的基础模型与检索技术在实例级图像检索能力而设计。与现有数据集相比，ILIAS的优势在于其大规模、多样化的领域，以及准确的真实标注，且性能尚未饱和。该数据集包含1,000个对象实例的查询和正面图像，这些图像经过手动收集，旨在捕捉具有挑战性的条件和多样化的领域。检索任务涉及1亿张来自YFCC100M的数据干扰图像。在避免假阴性并减少额外标注工作量的前提下，ILIAS仅包含确认在2014年后出现的查询对象。通过广泛的基准测试，结果显示：针对特定领域的模型表现优异，但在ILIAS上的效果有限；通过多领域类监督训练线性适配层可以提高性能；局部描述符在重排检索中的作用依然重要，特别是在存在严重背景干扰的情况下；此外，视觉-语言基础模型在文本到图像的性能与图像到图像的性能接近。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:42:58 GMT</pubDate>
</item>
<item>
<title>CALM：结合对话与智能能力的统一语言模型</title>
<link>https://arxiv.org/abs/2502.08820</link>
<guid>https://arxiv.org/abs/2502.08820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CALM统一模型提升了对话系统与任务导向的能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过API调用功能，推动了语言智能体（LA）和传统任务导向对话（TOD）范式的变革。然而，现有方法在支持多轮对话时面临重大困境。任务导向系统通常在有限的目标API上训练，需要新数据来维持与新服务的接口质量，而语言智能体在多轮对话中则难以维持用户意图。为了解决这一问题，本文提出CALM（Conversational Agentic Language Model），一种结合对话和智能能力的统一方法。我们创建了CALM-IT，一个精心构建的多任务数据集，以交错多轮ReAct推理与复杂的API使用。使用CALM-IT训练的CALM系列模型（CALM 8B, CALM 70B, CALM 405B），在三个流行基准（MultiWOZ 2.4, BFCL V3, API-Bank）上均超越了包括GPT-4o在内的顶尖领域专用模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 08:59:34 GMT</pubDate>
</item>
<item>
<title>模型编辑在问答系统中的评估与实践研究</title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明当前模型编辑方法在实际应用中的有效性不足。</p><br /><br /><p><strong>摘要：</strong> 尽管现有模型编辑方法在人工评估中表现良好，但其实用性仍待证实。本研究通过建立QAEdit基准和标准化评估框架，探讨模型编辑在问答（QA）中的有效性。实验结果显示，现有编辑方法在真实应用中的表现远低于预期（38.5%对比~96%），分析指出，主要原因在于以往研究中的评估实践不当，特别是教师强迫使用不当，导致错误无法传播。此外，通过模拟真实场景的连续编辑，发现现有方法在仅进行1000次编辑的情况下效果显著下降。我们的分析对现有模型编辑方法的实际应用及其评估实践进行了重新审视，并提出了改进建议，以推动可靠且实用的模型编辑研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:33:17 GMT</pubDate>
</item>
<item>
<title>MIKASA：增强记忆能力的强化学习基准</title>
<link>https://arxiv.org/abs/2502.10550</link>
<guid>https://arxiv.org/abs/2502.10550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIKASA基准为记忆强化学习提供了统一的评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MIKASA（内存密集技能评估套件），旨在为强化学习中的记忆能力提供全面的评估基准。目前，尽管许多强化学习算法采用了记忆机制，但缺乏统一标准来评估其在各种场景中的表现。在台面机器人操控领域，记忆是解决部分可观测任务和确保系统稳定性的关键因素。为此，MIKASA包括三个主要贡献：首先，提出了一种针对记忆密集型强化学习任务的全面分类框架；其次，收集了MIKASA-Base——一个统一的基准，支持对增强记忆智能体进行系统评估；最后，开发了MIKASA-Robo，这是一个包含32个精心设计的内存密集型任务的新基准，评估台面机器人操控中的记忆能力。这些贡献为推动记忆强化学习研究提供了统一框架，助力更可靠的现实应用系统开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:16:07 GMT</pubDate>
</item>
<item>
<title>Dyve：基于动态过程验证的语言模型错误检测增强工具</title>
<link>https://arxiv.org/abs/2502.11157</link>
<guid>https://arxiv.org/abs/2502.11157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dyve通过动态验证提升语言模型的错误检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Dyve，一个动态过程验证工具，通过结合快速与慢速思维（基于Kahneman的系统理论）来增强大型语言模型中的推理错误检测。Dyve根据任务性质，智能地应用即时的token级确认（系统1）处理简单步骤，而对复杂任务则采用全面分析（系统2）。此外，Dyve引入了一种新颖的逐步共识过滤过程监督技术，利用蒙特卡罗估计与基于语言模型的评估相结合，从嘈杂数据中提取高质量的监督信号。在ProcessBench和MATH数据集上的实验结果表明，Dyve在过程验证方面显著优于现有工具，并在最佳选择设置中提升了性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:33:31 GMT</pubDate>
</item>
<item>
<title>NSA：高效的长上下文稀疏注意力机制</title>
<link>https://arxiv.org/abs/2502.11089</link>
<guid>https://arxiv.org/abs/2502.11089</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NSA机制通过高效稀疏注意力实现长上下文建模，提高计算效率。</p><br /><br /><p><strong>摘要：</strong> 长上下文建模对于下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著挑战。我们提出的NSA机制是一种可原生训练的稀疏注意力解决方案，结合算法创新和硬件优化，实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的令牌压缩与细粒度的令牌选择，既保持全局上下文感知又确保局部精度。通过算术强度平衡的算法设计与现代硬件的实现优化，我们显著提升了计算速度，并实现了端到端的训练，减少了预训练计算量而不影响模型性能。实验表明，使用NSA预训练的模型在各项基准测试、长上下文任务和基于指令的推理中表现与全注意力模型持平或更优，同时在64k长度序列上，NSA在解码、前向传播和反向传播中都显著快于全注意力模型，验证了其在模型生命周期中的高效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11089" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:07:36 GMT</pubDate>
</item>
<item>
<title>改进Adam优化器以缓解大语言模型中嵌入的各向异性问题</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Coupled Adam优化器，显著改善大语言模型的嵌入质量。</p><br /><br /><p><strong>摘要：</strong> 尽管大语言模型具有显著的能力，但它们学习的词表示往往表现出各向异性这一不理想且尚不清楚的问题。本文认为，Adam优化器中的二阶矩是导致嵌入各向异性的一个原因，并提出了一种名为Coupled Adam的改进优化器，以减轻这一问题。实验结果表明，Coupled Adam能够显著提高嵌入质量，同时在大规模数据集上也能改善后续和前期性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 05:28:54 GMT</pubDate>
</item>
<item>
<title>提升自动化事实核查工具的有效性</title>
<link>https://arxiv.org/abs/2502.09083</link>
<guid>https://arxiv.org/abs/2502.09083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究自动化事实核查的解释需求，改善信息核查流程。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型和生成性AI在网络媒体中的广泛应用，自动化事实核查的需求日益增强，以帮助事实核查员应对日益增加的信息失真。然而，自动化核查系统的解释如何与事实核查员的决策与推理过程相结合仍然不明确。通过对事实核查专业人士进行半结构化访谈，本研究阐明了事实核查员评估证据、做出决策及解释其过程的方式，探讨了事实核查员在实践中如何使用自动化工具，并识别了他们对这些工具的解释需求。研究结果显示，当前的解释需求未得到满足，并识别了可复制的事实核查解释的重要标准，包括模型的推理路径、具体证据的引用、以及强调不确定性和信息缺口的要求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:37:21 GMT</pubDate>
</item>
<item>
<title>MagicArticulate：自动将静态3D模型转化为可动画资产的有效框架</title>
<link>https://arxiv.org/abs/2502.12135</link>
<guid>https://arxiv.org/abs/2502.12135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicArticulate框架可以自动将静态3D模型转换为支持动画的版本。</p><br /><br /><p><strong>摘要：</strong> 随着3D内容创作的快速发展，对将静态3D模型自动转化为支持真实动画的关节化版本的需求日益增加。传统方法依赖手动注释，效率低下且耗时。为此，我们提出了MagicArticulate框架，能够有效自动转化静态3D模型为可动画资产。我们的主要贡献包括：第一，建立Articulation-XL，这是一项包含超过33,000个高质量关节注释的3D模型的大型基准数据集。第二，提出了一种新颖的骨架生成方法，将任务表述为序列建模问题，利用自回归变换器自然处理骨骼和关节的变化及其依赖关系。第三，使用功能扩散过程预测皮肤加权，结合顶点与关节之间的体积测地距离先验。实验结果表明，MagicArticulate在不同对象类别上显著优于现有方法，能够实现高质量的关节化，支持真实动画效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:34:15 GMT</pubDate>
</item>
<item>
<title>ThinkDiff：增强图文扩散模型的多模态推理能力</title>
<link>https://arxiv.org/abs/2502.10458</link>
<guid>https://arxiv.org/abs/2502.10458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkDiff通过多模态对齐提升图像生成模型的理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的对齐范式ThinkDiff，旨在通过整合视觉语言模型(VLMs)的优势，赋予文本到图像的扩散模型多模态的上下文理解和推理能力。当前的多模态扩散微调方法大多聚焦于像素级重建，而忽略了上下文推理，且受限于推理数据集的复杂性和可用性。ThinkDiff通过将视觉语言训练作为代理任务，简化了与编码-解码大型语言模型(LLM)解码器的对齐过程，有效提升了扩散模型的理解、推理和构成能力。实验证明，ThinkDiff在多模态上下文推理生成的挑战性CoBSAT基准上，准确率从19.2%提升至46.3%，仅需在4个A100 GPU上训练5小时。此外，ThinkDiff在将多张图像和文本组合成逻辑一致的图像方面表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:33:41 GMT</pubDate>
</item>
<item>
<title>深度神经网络模型中的直觉物理理解研究</title>
<link>https://arxiv.org/abs/2502.11831</link>
<guid>https://arxiv.org/abs/2502.11831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明深度神经网络能通过视频预测学习直觉物理知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了普遍深度神经网络模型在自然视频中预测缺失区域的过程中，如何逐步获得直觉物理理解。基于期望违背框架的实验表明，训练在学习表示空间中的视频预测模型能展示对象延续性和形状一致性等直觉物理特性。而在像素空间中的视频预测和通过文本推理的多模态大型语言模型，表现更接近于随机概率。研究比较不同结构显示，联合学习抽象表示空间并预测感官输入的缺失部分，类似于预测编码，足以培养对直觉物理的理解。这一发现挑战了固有知识的概念，即理解世界所需的核心知识并不一定需要内置到模型中，即便是训练一周的独特视频模型也能超越随机表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:20:25 GMT</pubDate>
</item>
<item>
<title>量子属性预测中的预训练质量优于量</title>
<link>https://arxiv.org/abs/2502.11085</link>
<guid>https://arxiv.org/abs/2502.11085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究表明，优质数据集在量子属性预测中优于大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了量子属性预测中近期的范式，认为进展与数据集规模和计算资源的增长相关。研究表明，在精心选择的任务相关数据集上进行预训练，能够匹敌甚至超越大规模预训练，而计算成本仅为1/24。同时，引入了一种新指标——化学相似性指数（CSI），用于量化上游预训练数据集与下游任务的对齐程度。通过选择最相关的数据集，最低化CSI距离，结果表明，基于较小、聚焦数据集的预训练模型在性能上始终优于那些基于大量混合数据集（如JMP）的模型。这一结果表明，数据增加并不总是提升性能，反而可能因低相关性数据的加入而恶化。这一发现突显出在量子属性预测中，预训练的质量常常优于数量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:16:28 GMT</pubDate>
</item>
<item>
<title>PhysReason：评估大语言模型物理推理能力的新基准</title>
<link>https://arxiv.org/abs/2502.12054</link>
<guid>https://arxiv.org/abs/2502.12054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysReason是一个评估大语言模型物理推理能力的新基准，涵盖1200个问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysReason，一个包含1200道问题的新基准，旨在评估大语言模型在物理推理方面的能力。该基准问题分为知识型（25%）和推理型（75%）两类，并依据难度分为简单、中等和困难三级，其中困难问题的平均解题步骤达到15.6步。我们还提出了物理解题自动评分框架，进行高效的答案级和步骤级评估。尽管一些顶尖模型如Deepseek-R1和Gemini-2.0-Flash-Thinking在答案级评估中得分不到60%，为何从知识型问题（75.11%）到困难问题（31.95%）的性能显著下降，借助步骤级评估，我们发现了四个主要瓶颈：物理定理应用、物理过程理解、计算和物理条件分析。这些发现使PhysReason成为评估语言模型物理推理能力的一个新颖且全面的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 03:53:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型的数学推理能力研究</title>
<link>https://arxiv.org/abs/2502.11574</link>
<guid>https://arxiv.org/abs/2502.11574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型在数学推理中存在逻辑缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）的数学推理能力，使用50个新构建的高中水平文字题进行分析。研究不仅关注模型的最终答案正确性，还深入分析了解题步骤，以识别推理失败。评估了八种最新模型，包括Mixtral、Llama、Gemini和GPT系列，发现虽然一些新模型（如o3-mini、deepseek-r1）在准确度上表现较高，但所有模型在空间推理、战略规划和算术方面均存在错误。有些模型通过错误的逻辑得出了正确的答案。常见的失败模式包括不当假设、对数字模式的过度依赖，以及将物理直觉转化为数学步骤的困难。手动分析表明，尽管模型具备广泛的数学知识，它们在处理需多步推理或现实世界知识的问题上表现不佳。研究强调仅仅关注答案而忽视推理过程的评估是有风险的，凸显出LLMs在广泛性和推理能力上的持续差距，需要针对性改进结构化推理和约束处理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:26:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型在语言复杂性测量任务中的表现研究</title>
<link>https://arxiv.org/abs/2502.11578</link>
<guid>https://arxiv.org/abs/2502.11578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示ChatGPT-o1-mini在语言复杂性测量中表现最佳。</p><br /><br /><p><strong>摘要：</strong> 本文研究了当前大型语言模型（LLMs）在语言复杂性测量任务中的表现，重点关注LIX可读性指标和平均依赖距离（ADD）的计算。通过分析瑞典高中的论文和大学级别的论文，我们评估了模型计算LIX分数和进行依赖解析的能力，并将其结果与已建立的基准进行比较。研究发现，所有模型在这些任务上均展现出一定的能力，其中ChatGPT-o1-mini在LIX计算和依赖解析中表现最为稳定，取得了最高的准确率。此外，我们观察到模型在计算LIX时的准确性与其在大规模多任务语言理解基准（MMLU）上的整体表现之间存在显著相关性（-0.875，p = 0.026，N=6）。这些结果表明，语言复杂性测量能力可以作为评估LLMs一般能力的有效零-shot代理，提供了一种无需大量基准数据集的模型评估方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:23:29 GMT</pubDate>
</item>
<item>
<title>SysGen: 提升语言模型响应的系统消息生成管道</title>
<link>https://arxiv.org/abs/2502.11330</link>
<guid>https://arxiv.org/abs/2502.11330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SysGen改善了语言模型响应与系统消息的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SysGen，一个用于生成系统消息的流程，旨在提高大型语言模型（LLMs）响应的对齐度。当前，公开可用的数据常常缺乏系统消息，且在行业中受到严格的许可限制，手动标注需要大量资源。通过对没有系统消息的有监督微调数据集进行训练，SysGen显著提升了模型响应与系统消息及用户指令的一致性，且在多种开源模型的Multifacet基准测试中表现出显著改善，同时对未见的基准测试，如Open LLM Leaderboard 2，的影响极小。定性分析则强调了多样化系统消息在不同环境中的适应性的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:45:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型知识电路演化研究</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型如何内化新知识并优化知识存储过程。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在知识密集型任务中表现卓越，但它们在理解新知识的内化过程方面存在重大缺口。本文通过知识电路演化的视角，识别出促进知识存储和处理的计算子图。我们的系统分析显示，新知识的获取受既有知识相关性的影响，知识电路的演化经历从形成到优化的明显相变，并且遵循从深到浅的演化模式。这些发现不仅深化了我们对LLMs中新知识获取机制的理论理解，也为改善持续预训练策略以提升模型性能提供了潜在的启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:02:25 GMT</pubDate>
</item>
<item>
<title>探索大型语言模型作为代码执行替代者的能力</title>
<link>https://arxiv.org/abs/2502.11167</link>
<guid>https://arxiv.org/abs/2502.11167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究大型语言模型在代码执行预测中的有效性与局限性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）作为通用代码执行替代者的潜力，尤其是预测程序输出和行为而不实际运行代码。我们提出了SURGE基准，涵盖了八个关键方面，包括多语言编程任务、竞争级编程问题和高成本科学计算等。通过对多种开源和专有LLMs的评估及模型规模与训练数据规模对替代执行准确性的影响分析，我们发现LLMs在某些情况下能够预测代码执行结果，但在通用替代执行方面存在显著限制。研究还对模型预测错误进行了分类，并探讨了潜在的改进领域，提供了使用LLMs作为代码执行替代者的可行性实证见解。代码和数据集已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:01:24 GMT</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:58:24 GMT</pubDate>
</item>
<item>
<title>基于学习框架的人形机器人自动起立控制研究</title>
<link>https://arxiv.org/abs/2502.12152</link>
<guid>https://arxiv.org/abs/2502.12152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种学习框架帮助人形机器人从跌倒状态成功起立。</p><br /><br /><p><strong>摘要：</strong> 本文针对人形机器人在跌倒后自动起立的控制问题，提出了一种学习框架。由于人形机器人跌倒后可能处于多种不同配置，并需要在复杂地形上操作，手动设计控制器面临巨大挑战。研究利用两阶段的课程学习方法，首先在松弛约束条件下发现良好的起立轨迹，其后将这些轨迹精炼为适合部署的平滑、缓慢的运动，确保在不同的初始配置和地形下的稳健性。实验表明，该方法使得一款真实的人形机器人能够从仰卧和俯卧两种姿态成功起立，验证了在真实环境中的有效性。这是首个在人形机器人身上成功演示的学习起立策略。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:49:53 GMT</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:28:31 GMT</pubDate>
</item>
<item>
<title>提升视频理解能力的开源多模态LLM video-SALMONN-o1</title>
<link>https://arxiv.org/abs/2502.11775</link>
<guid>https://arxiv.org/abs/2502.11775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出video-SALMONN-o1，提升视频理解与推理能力的开源多模态语言模型。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了video-SALMONN-o1，这是首个专为一般视频理解任务设计的开源推理增强音视频语言模型。为提升推理能力，我们开发了一个包含具有挑战性音视频问题及逐步解决方案的推理密集型数据集，并提出了过程直接偏好优化（pDPO）方法，利用对比步骤选择实现针对多模态输入的高效步级奖励建模。此外，本文还引入了RivaBench，这是首个理由密集型视频理解基准，包含4000多个高质量专家策划的问题-答案对，覆盖单口喜剧、学术演讲和合成视频检测等场景。与LLaVA-OneVision基线相比，video-SALMONN-o1在不同视频推理基准上实现了3-8%的准确性提升，而pDPO在RivaBench上相对于监督微调模型则提高了6-8%的准确率。增强的推理能力使video-SALMONN-o1具备零次合成视频检测能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11775" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:06:55 GMT</pubDate>
</item>
<item>
<title>TalkHier：一种新型LLM-MA系统的结构化沟通框架</title>
<link>https://arxiv.org/abs/2502.11098</link>
<guid>https://arxiv.org/abs/2502.11098</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TalkHier框架，解决LLM-MA系统中的沟通和优化问题。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-MA系统研究的进展，各代理在复杂任务协作中面临沟通管理和输出优化的挑战。本文提出了Talk Structurally, Act Hierarchically (TalkHier)框架，旨在引入结构化沟通协议和层级优化机制，以解决输出错误、虚假信息及偏见等问题。通过在多种任务上的测试，包括开放域问答、领域特定选择性提问和实际广告文本生成，TalkHier在性能上超过了多种前沿技术，如OpenAI的推理缩放模型和AgentVerse等开源多代理模型。这一新框架展示了其成为LLM-MA系统新标准的潜力，推动了更有效、适应性强的多代理协作框架的发展。相关代码已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11098" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:51:50 GMT</pubDate>
</item>
<item>
<title>通过对例增强数学大语言模型的证明能力研究</title>
<link>https://arxiv.org/abs/2502.10454</link>
<guid>https://arxiv.org/abs/2502.10454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明现有数学大语言模型的证明能力受训练数据的影响，并提出通过对例提高其数学推理能力的方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数学大语言模型（LLMs）在证明生成中的能力，认为现有模型的证明效果主要依赖于其训练中是否遇到相关的证明过程。这一依赖限制了模型对数学定理及概念的深入理解。我们受到人类数学教育中常用的“反例证明”方法的启发，致力于通过反例增强LLMs的数学推理和证明能力。为此，手动创建了CounterMATH，一个高质量的大学水平数学基准，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的掌握情况。此外，我们还开发了数据工程框架，以自动获取训练数据以进一步提升模型性能。广泛的实验和详细的分析表明，CounterMATH具有挑战性，显示出LLMs（如OpenAI o1）在反例驱动的证明能力方面不足。我们认为，增强LLMs的反例驱动概念推理能力对提高其整体数学能力至关重要，提出了对数学大语言模型社区的新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:37:16 GMT</pubDate>
</item>
<item>
<title>Diffusion-Sharpening：一种优化采样轨迹的微调方法</title>
<link>https://arxiv.org/abs/2502.12146</link>
<guid>https://arxiv.org/abs/2502.12146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Diffusion-Sharpening方法，通过优化采样轨迹提升微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Diffusion-Sharpening的微调方法，旨在通过优化采样轨迹提升下游任务的对齐效果。现有的基于强化学习的微调方法往往关注单个训练时间步，而忽视了轨迹级别的对齐；而最近的采样轨迹优化方法则带来了显著的推理NFE成本。Diffusion-Sharpening通过路径积分框架在训练过程中选择最优轨迹，利用奖励反馈来克服这些问题，并摊销推理成本。实验结果表明，该方法在训练效率（二次收敛速度）和推理效率（不需要额外的NFE）方面均表现出色，相较于传统的基于强化学习的微调方法和采样轨迹优化方法，Diffusion-Sharpening在文本对齐、组合能力和人类偏好等多项指标上均取得了更好的效果，提供了一种可扩展且高效的扩散模型微调方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:30:53 GMT</pubDate>
</item>
<item>
<title>HermesFlow：弥合多模态大语言模型理解与生成能力的差距</title>
<link>https://arxiv.org/abs/2502.12148</link>
<guid>https://arxiv.org/abs/2502.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HermesFlow有效弥合了多模态大语言模型的理解与生成能力差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HermesFlow，一个旨在优化多模态大语言模型（MLLMs）理解与生成能力的框架。研究表明，MLLMs的理解能力通常强于其生成能力，二者之间存在显著差距。HermesFlow通过输入同源数据，构建理解与生成的同源偏好数据，利用Pair-DPO和自我对抗优化机制，将理解能力与生成能力有效对齐。实验结果显示，HermesFlow在缩小多模态理解与生成的差距方面，显著优于之前的方法。这一发现突显了HermesFlow作为下代多模态基础模型的一般性对齐框架的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:29:29 GMT</pubDate>
</item>
<item>
<title>SAFE-SQL：自增强上下文学习提升Text-to-SQL性能</title>
<link>https://arxiv.org/abs/2502.11438</link>
<guid>https://arxiv.org/abs/2502.11438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAFE-SQL通过自生成示例提升Text-to-SQL的执行准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架SAFE-SQL，旨在将自然语言问题转换为可执行的SQL查询。传统方法如骨架掩码选择在获取相似训练示例以指导大型语言模型时表现出色，但在实际应用中面临示例缺失的挑战。为此，SAFE-SQL采用自增强上下文学习，通过生成并过滤自增强示例，显著提升SQL生成效果。该框架首先引导大型语言模型生成与测试输入相关的多个Text-to-SQL示例，并通过三种相关性评估进行过滤，构建高质量的上下文学习示例。最终，SAFE-SQL在零-shot和少-shot的Text-to-SQL任务中超越了传统框架，尤其在困难和未见场景中表现出额外的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:06:03 GMT</pubDate>
</item>
<item>
<title>CRANE：一种增强推理能力的约束解码算法</title>
<link>https://arxiv.org/abs/2502.09061</link>
<guid>https://arxiv.org/abs/2502.09061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRANE算法在约束生成中平衡了语法和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保证语法和语义正确性的同时，增强大型语言模型（LLMs）的推理能力。我们首先理论上解释了限制LLM输出为严格语法形式为何会减弱其推理能力。接着，我们提出通过增强输出语法、增加设计良好的额外规则，能够在确保输出合规的同时维持推理能力。基于这些理论见解，我们开发了CRANE算法，它在约束生成的正确性与非约束生成的灵活性之间取得有效平衡。通过对多个开源LLM和基准进行实验，结果显示CRANE在严苛的符号推理基准GSM-symbolic和FOLIO上，相较于最先进的约束解码策略和标准的非约束解码，准确性提升达10%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:43:51 GMT</pubDate>
</item>
<item>
<title>利用高质量LLM数据提升信息提取模型性能</title>
<link>https://arxiv.org/abs/2502.11275</link>
<guid>https://arxiv.org/abs/2502.11275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cuckoo模型展示了如何利用LLM数据提升信息提取效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在信息提取（IE）领域中，将大型语言模型（LLM）的数据应用于IE模型的可能性。我们提出了一种新的NTE（下一标记提取）范式，通过将预测下一个标记的过程重新设计为对已存在上下文中标记的提取，从而构建出Cuckoo模型，该模型基于来自LLM的102.6M抽取数据进行训练。在少量样本环境下，Cuckoo能有效适应传统和复杂指令下的信息提取任务，表现优于现有的预训练IE模型。作为一种“搭便车”方案，Cuckoo能随着LLM数据准备的持续进展而自然演化，无需额外的手动干预，即可从LLM训练管道的改进中获益。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11275" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:10:49 GMT</pubDate>
</item>
<item>
<title>合成数据增强在项目级证明导向编程中的应用</title>
<link>https://arxiv.org/abs/2502.11901</link>
<guid>https://arxiv.org/abs/2502.11901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种合成数据增强方法以解决证明导向编程中的数据稀缺问题。</p><br /><br /><p><strong>摘要：</strong> 现有的语言模型在证明导向编程中面临数据稀缺的问题，主要表现为缺乏足够的相关语料库和项目级实现。本文首次提出了一种基于合成数据增强的方法，旨在通过生成和修复项目级的证明导向编程问题来应对这一挑战。该方法通过合成基本的证明导向编程问题提升语言模型在特定编程语言中的熟练程度，同时引入多样化的编码数据，以增强推理能力，并在现有代码库中创建新的证明和修复数据。这一方法使得语言模型能够在函数级和代码库级别生成及修复证明。我们展示了我们微调后的14B参数模型PoPilot，在项目级证明导向编程中超越了GPT-4o模型64%的性能，并能够通过修复其输出，比GPT-4o的自我修复提高54%的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:05:54 GMT</pubDate>
</item>
<item>
<title>深度分析大型推理模型中的过度思考现象</title>
<link>https://arxiv.org/abs/2502.08235</link>
<guid>https://arxiv.org/abs/2502.08235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，过度思考限制了大型推理模型在互动环境中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型推理模型（LRMs）在互动环境中的过度思考现象，这种现象表现为模型倾向于使用冗长的内部推理链而不是进行环境互动。通过在SWE Bench Verified软件工程任务上的实验，我们识别了三种常见行为模式：分析瘫痪、鲁莽行动和过早脱离。分析结果显示，过度思考得分较高与模型表现下降相关，推理模型表现出比非推理模型更强的过度思考倾向。通过采取一些简单措施，如选择过度思考得分较低的解决方案，我们发现可将模型性能提升近30%，同时计算成本降低43%。这些发现表明，缓解过度思考在实际应用中具有重要意义。我们建议可通过利用原生功能调用能力和选择性的强化学习来减轻过度思考倾向，并开源了我们的评估框架和数据集，以推动该研究方向的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08235" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 17:09:38 GMT</pubDate>
</item>
<item>
<title>选择性自我监督微调方法（S3FT）提升大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2502.08130</link>
<guid>https://arxiv.org/abs/2502.08130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S3FT方法在保持性能的同时，改善了大语言模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的微调方法——选择性自我监督微调（S3FT），该方法旨在提高大语言模型（LLMs）在特定任务上的性能，同时改善其泛化能力。S3FT的方法通过利用多个对同一查询的有效响应来减少模型在微调过程中的过拟合，从而避免过度专注于训练数据的特征。具体而言，S3FT首先通过部署适当的评估者，识别训练集中的正确模型响应，然后利用这些正确响应与目标响应（或其同义句）微调模型。实验结果显示，与标准监督微调（SFT）相比，S3FT在数学推理、Python编程和阅读理解任务上显著提升了性能，并将标准SFT导致的平均性能下降（最高达4.4）减少了一半，说明S3FT在提高模型任务性能的同时，具备更好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 12:27:43 GMT</pubDate>
</item>
<item>
<title>CLaMP 3: 一种跨模态与跨语言音乐信息检索统一框架</title>
<link>https://arxiv.org/abs/2502.10362</link>
<guid>https://arxiv.org/abs/2502.10362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaMP 3通过对比学习实现跨模态及语言的音乐信息检索。</p><br /><br /><p><strong>摘要：</strong> CLaMP 3是一个为解决音乐信息检索中跨模态和跨语言泛化挑战而开发的统一框架。它通过对比学习将主要音乐模态（包括乐谱、表演信号和音频记录）与多语言文本对齐至共享表示空间，从而实现以文本为桥梁的各模态检索。该框架具有可适应新语言的多语言文本编码器，显示了强大的跨语言泛化能力。我们利用增强检索生成技术，创建了M4-RAG数据集，包含231万对音乐文本对，并附有丰富的元数据，涵盖广泛的全球音乐传统。为推动未来研究，我们还发布了WikiMT-X基准数据集，包含1000个乐谱、音频以及多样化文本描述的三元组。实验结果显示，CLaMP 3在多个音乐信息检索任务上达到领先性能，显著超越之前的强基线，展现出在多模态和多语言音乐上下文中的优秀泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 10:18:04 GMT</pubDate>
</item>
<item>
<title>高效多级卷积架构在3D视觉定位中的应用</title>
<link>https://arxiv.org/abs/2502.10392</link>
<guid>https://arxiv.org/abs/2502.10392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效的多级卷积架构，优化3D视觉定位性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种高效的多级卷积架构，用于3D视觉定位，旨在克服传统方法因双阶段或点基础架构导致实时推理困难的问题。受3D物体检测中稀疏卷积架构成功的启发，结合文本特征，使3D场景表示与文本特征有效互动，采用文本引导剪枝（TGP）与基于补全的加法（CBA）方法，通过逐渐区域剪枝与目标补全高度融合信息。TGP通过交叉注意力有效地稀疏化3D场景表示，CBA则解决了过度剪枝对几何信息影响的问题，以微小的计算开销修复被过度剪枝区域。实验显示，与以往单阶段方法相比，该方法在推理速度上领先，且在ScanRefer、NR3D和SR3D上存在显著的准确率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 09:25:39 GMT</pubDate>
</item>
<item>
<title>DarwinLM: Evolutionary Structured Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2502.07780</link>
<guid>https://arxiv.org/abs/2502.07780</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>ImageRAG：基于检索增强生成的图像合成方法</title>
<link>https://arxiv.org/abs/2502.09411</link>
<guid>https://arxiv.org/abs/2502.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ImageRAG通过动态检索图像提高了图像生成质量，特别是在稀有概念的合成方面。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ImageRAG的方法，它结合了图像生成模型与检索增强生成技术，以解决现有扩散模型在生成稀有或未见概念时的挑战。ImageRAG会根据给定的文本提示动态检索相关图像，并将这些图像作为上下文来指导图像生成过程。与之前专门针对检索生成训练的模型不同，ImageRAG不需要专门的训练，而是利用现有图像条件模型的能力。这种方法具有高度的适应性，能够在多种基础模型中应用，显著提高了稀有和细粒度概念的生成效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:41:41 GMT</pubDate>
</item>
<item>
<title>小型多语言模型在低资源语言处理中的适应性研究</title>
<link>https://arxiv.org/abs/2502.10140</link>
<guid>https://arxiv.org/abs/2502.10140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究 explores 小型多语言模型在低资源语言下的适应性与性能提升。</p><br /><br /><p><strong>摘要：</strong> 本研究系统探讨了如何有效地使用参数高效的适配器方法，将小型多语言模型（mLMs）适应于低资源语言（LRLs）。研究评估了三种适配器架构：顺序瓶颈、可逆瓶颈和低秩适配方法。结果显示，使用来自GlotCC的非结构化文本和ConceptNet的结构化知识的小型适配数据集（如最多1GB的自由文本或几MB的知识图数据）能显著提升模型在内部（掩码语言建模）和外部任务（主题分类、情感分析和命名实体识别）的表现。研究发现，顺序瓶颈适配器在语言建模方面表现出色，而可逆瓶颈适配器在下游任务上由于更好的嵌入对齐和更多的参数数量略有凌驾于其他方法之上。适配器方法在使用更少参数的情况下，实现了与完整微调相当或更优的性能，而与大型语言模型（如LLaMA-3、GPT-4及DeepSeek-R1基于的蒸馏模型）相比，小型mLMs在低资源语言任务中更为有效。尽管适应性提高了性能，但预训练数据规模仍是决定因素，特别是对于预训练覆盖面广的语言。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10140" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:29:25 GMT</pubDate>
</item>
<item>
<title>CAPI：一种基于聚类的纯MIM框架及其在图像识别中的应用</title>
<link>https://arxiv.org/abs/2502.08769</link>
<guid>https://arxiv.org/abs/2502.08769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAPI框架通过聚类预测提升了自监督学习的表现，达到了高准确率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CAPI，一个新颖的纯蒙面图像建模（MIM）框架，旨在提升自监督表示学习的性能。我们系统分析了目标表示、损失函数和架构，提出了一种基于聚类的损失函数，以提高模型的训练稳定性和扩展性。CAPI使用ViT-L骨干网络，在ImageNet数据集上取得了83.8%的准确率，ADE20K数据集上实现了32.1%的mIoU，相较于现有的MIM方法大幅提升，并接近当前最先进的方法DINOv2。我们将所有代码和模型发布，促进后续研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 07:24:28 GMT</pubDate>
</item>
<item>
<title>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.10235</link>
<guid>https://arxiv.org/abs/2502.10235</guid>
<content:encoded><![CDATA[
Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:36:23 GMT</pubDate>
</item>
<item>
<title>VibeGen：基于生成AI的蛋白质动态设计框架</title>
<link>https://arxiv.org/abs/2502.10173</link>
<guid>https://arxiv.org/abs/2502.10173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VibeGen是一种基于动态特性的蛋白质生成AI设计框架。</p><br /><br /><p><strong>摘要：</strong> VibeGen是一个新颖的蛋白质生成AI框架，旨在通过正常模式振动进行蛋白质的端到端设计。它采用双模型架构，包括一个基于指定振动模式生成序列候选者的蛋白质设计器和一个评估其动态准确性的蛋白质预测器。通过全原子分子模拟确认，设计的蛋白质能够准确再现规定的正常模式幅度，并在此基础上采用多种稳定、功能相关的结构。显著的是，生成的序列为全新设计，与自然蛋白质没有显著相似性，拓展了可及的蛋白质空间，突破了进化限制。本研究将蛋白质动态性引入生成设计过程，建立了序列与振动行为之间的双向联系，为工程化具有定制动态和功能特性的生物分子开辟了新的路径，具有重要的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:09:33 GMT</pubDate>
</item>
<item>
<title>通过新词开发理解人工智能的语言</title>
<link>https://arxiv.org/abs/2502.07586</link>
<guid>https://arxiv.org/abs/2502.07586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">为有效理解AI，需构建新词汇以弥补现有语言的不足。</p><br /><br /><p><strong>摘要：</strong> 本文立论认为，要理解人工智能（AI），我们不能仅依赖现有的人类词汇。相反，我们应努力开发新词汇，以准确表达人类概念或机器概念，从而实现更好的理解。人类与机器的概念不同，因此可将可解释性视为一种沟通问题：人类需要能够参考和控制机器概念，同时将人类概念传达给机器。通过创建共享的人机语言，借助新词汇的发展，可解决这一沟通难题。成功的新词汇应当在抽象程度上适中，既不过于细化，以便于在多个语境中重用，又不太高层，以便于传达精确信息。作为概念验证，文章展示了如何通过“长度新词”控制大型语言模型的回答长度，以及使用“多样性新词”促进更加多变的响应。总体来说，我们认为，无法仅用现有词汇理解AI，而通过新词汇的扩展可为控制和理解机器创造新的机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 04:28:55 GMT</pubDate>
</item>
<item>
<title>通过局部化关注层提升扩散模型文本生成能力</title>
<link>https://arxiv.org/abs/2502.09935</link>
<guid>https://arxiv.org/abs/2502.09935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨如何通过局部化扩散模型的注意力层来优化图像中的文本生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何在扩散模型中通过局部化局部注意力层来提高文本生成的效率和性能。研究表明，扩散模型参数的不到1%影响图像中文本内容的生成，主要集中在注意力层中。通过只对这些局部注意力层进行LoRA微调，可以显著提升大型扩散模型的文本生成能力，并且在保持图像生成质量和多样性的同时，应用于图像中文本编辑和防止有害文本生成。该方法跨多种扩散模型架构（如U-Net和Transformer）均具有广泛适用性，能够兼容多种文本编码器，如CLIP和T5。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 03:06:17 GMT</pubDate>
</item>
<item>
<title>MR采样器：加速可控生成中的扩散模型采样过程</title>
<link>https://arxiv.org/abs/2502.07856</link>
<guid>https://arxiv.org/abs/2502.07856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MR采样器算法，显著加速了MR扩散模型的采样过程。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型应用的不断增长，可控生成的重要性和挑战并存。目前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归(MR)扩散则直接修改随机微分方程(SDE)的结构，使得图像条件的结合更加简便自然。然而，现有的无训练快速采样器并不适用于MR扩散，因此在获得高质量样本时，MR扩散需要数百次函数评估(NFEs)。本文提出了一种新的算法MR采样器(MRS)，旨在减少MR扩散的采样NFEs。该算法解决了与MR扩散相关的逆向时间SDE和概率流普通微分方程(PF-ODE)，并推导出半解析解，包括一个解析函数和一个由神经网络参数化的积分。基于这一解法，我们能够在更少的步骤中生成高质量样本。本方法无需训练，支持所有主流参数化，包括噪声预测、数据预测和速度预测。大量实验表明，MR采样器在十个不同的图像恢复任务中以10到20倍的加速维持高采样质量，极大提高了MR扩散的可控生成实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 02:03:05 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的车辆间合作感知与规划研究</title>
<link>https://arxiv.org/abs/2502.09980</link>
<guid>https://arxiv.org/abs/2502.09980</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出基于大语言模型的车辆间合作感知与规划新方法。</p><br /><br /><p><strong>摘要：</strong> 当前的自动驾驶车辆主要依赖各自的传感器理解周围环境并规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法的可靠性下降。为了解决这一问题，本文提出了一种将大语言模型应用于车辆间合作的创新设定，并构建了车辆间问答数据集（V2V-QA）和基准测试。我们的基线方法，即车辆间大语言模型（V2V-LLM），使用大语言模型融合多个连接自动驾驶车辆的感知信息并回答与驾驶相关的问题，如物体识别和规划。实验结果显示，V2V-LLM在执行不同任务方面表现优越，优于使用其他融合方法的基线。这项研究为未来自动驾驶系统的安全性提供了新的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09980" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 01:33:15 GMT</pubDate>
</item>
<item>
<title>利用LLM进行反监测的创新方法及其潜在风险</title>
<link>https://arxiv.org/abs/2502.09638</link>
<guid>https://arxiv.org/abs/2502.09638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了通过人类干预实现LLM自我越狱的创新方式及其安全隐患。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的LLM作为红队员的方法，通过人类干预使拒绝训练的LLM能够自我越狱或越狱其他LLM。我们将被越狱的LLM称为J_2攻击者，这些模型能够使用各种红队策略系统性地评估目标模型，并通过从以往失败中学习来提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5作为J_2在Harmbench上的攻击成功率分别达到了93.0%和91.0%，显著优于其他LLM。我们的研究不仅展示了一种受人类红队员启发的战略红队方法的可扩展性，还指出了越狱对越狱的被忽视的失败模式，强调了LLM能通过一个越狱版本自身来绕过自身的安全措施。为防止J_2的直接误用并推动AI安全研究，我们分享了我们的研究方法，并对具体提示细节进行了保密。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:04:19 GMT</pubDate>
</item>
<item>
<title>LLaDA：突破自回归模型的扩散模型探索</title>
<link>https://arxiv.org/abs/2502.09992</link>
<guid>https://arxiv.org/abs/2502.09992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA模型挑战自回归模型，展示了扩散模型的潜力。</p><br /><br /><p><strong>摘要：</strong> 本研究推出LLaDA，一个全新训练的扩散模型，旨在挑战自回归模型在大型语言模型中的主导地位。LLaDA通过前向数据掩蔽过程和反向过程来模型化分布，采用简单的Transformer预测被掩蔽的标记，通过优化似然界限提供了一种原则性生成推断方法。在多个基准测试中，LLaDA显示出强大的可扩展性，超越了自构建的自回归模型基线。特别是，LLaDA 8B在上下文学习中与强大的LLM如LLaMA3 8B竞争，并在多轮对话等案例研究中表现出令人印象深刻的指令跟随能力。此外，LLaDA还克服了逆转诅咒，在逆转诗填空任务中超越了GPT-4o。研究结果确定扩散模型作为自回归模型的有效替代方案，挑战了上述关键语言模型能力与自回归模型内在联系的假设。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:03:18 GMT</pubDate>
</item>
<item>
<title>多模型推理方法提升LLM在高级数学和编码任务中的表现</title>
<link>https://arxiv.org/abs/2502.09955</link>
<guid>https://arxiv.org/abs/2502.09955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多模型推理方法，显著提升LLM在高级数学问题上的解题能力。</p><br /><br /><p><strong>摘要：</strong> 尽管推理语言模型如OpenAI的o1、o3和DeepSeek R1在数学和编码方面取得了显著进展，但在国际数学奥林匹克（IMO）组合问题、抽象与推理库（ARC）难题以及人类最后考试（HLE）问题等高级任务中仍面临挑战。我们提出了一种多元推理方法，在测试时结合多种模型和方法，发现自动验证数学和编码问题的正确性，以及其他问题的拒绝采样，简单而有效。具体而言，我们通过Lean自动验证IMO问题的正确性，通过代码验证ARC难题的解，并发现“最佳-N”有效回答HLE问题，使IMO组合问题的解答准确率从33.3%提升至77.8%，HLE问题的准确率从8%提升至37%。我们的实验表明，该方法在解决948名人类无法解出的ARC难题中成功率达80%，并在o3高计算下未解的ARC难题中解决率为26.5%。通过测试模拟、强化学习和带推理反馈的元学习，我们提高了模型的泛化能力，适应图形表示和不同的提示、代码及数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:57:43 GMT</pubDate>
</item>
<item>
<title>傅里叶数字嵌入方法及其在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2502.09741</link>
<guid>https://arxiv.org/abs/2502.09741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出傅里叶数字嵌入法，以提高数字任务的效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为傅里叶数字嵌入（FoNE）的新方法，旨在解决大语言模型在处理数字时的高效性问题。传统上，大语言模型通过多个标记表示数字，这种碎片化的表示方式在训练和推理中降低了效率，影响了解析数字的能力。FoNE通过将每个数字直接映射到傅里叶特征的嵌入空间，允许每个数字作为单个标记进行编码，并仅为每个数字的每个数字提供两个嵌入维度。这种紧凑的表示方式显著加快了训练和推理过程。在数字任务，尤其是加法、减法和乘法中，FoNE的性能远超传统的子词和数字嵌入，使用70万次数据实现99%准确度比子词和数字嵌入少64倍，对每个数字使用的标记分别减少3倍和6倍。此外，FoNE在超过100,000个测试样例上实现了100%的准确率，展示了其优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:07:53 GMT</pubDate>
</item>
<item>
<title>MM-RLHF: 提升多模态大语言模型对人类偏好的对齐研究</title>
<link>https://arxiv.org/abs/2502.10391</link>
<guid>https://arxiv.org/abs/2502.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MM-RLHF数据集，推动多模态大语言模型对人类偏好的对齐。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLM）取得了显著进展，但大多数最先进的模型未能与人类偏好进行彻底对齐。为此，我们引入了MM-RLHF数据集，包含12万对细粒度人类注释的偏好比较，具有更大的规模、更多样性和更高质量。该数据集帮助我们提出多项创新，如基于批评的奖励模型和动态奖励缩放，旨在提升奖励模型的质量和对齐算法的效率。我们的实验显示，通过MM-RLHF和对齐算法微调LLaVA-ov-7B，模型的对话能力提高了19.5%，安全性提高了60%。我们已开源数据集和训练代码，更多细节可查阅我们的项目页面。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:51:55 GMT</pubDate>
</item>
<item>
<title>Step-Video-T2V：先进的文本生成视频预训练模型</title>
<link>https://arxiv.org/abs/2502.10248</link>
<guid>https://arxiv.org/abs/2502.10248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Video-T2V是一款具有30B参数的文本到视频生成模型。</p><br /><br /><p><strong>摘要：</strong> Step-Video-T2V是一款先进的文本生成视频预训练模型，拥有30B个参数，能够生成长达204帧的视频。该模型使用深度压缩变分自编码器(Video-VAE)，实现了16x16空间和8x时间压缩比，同时确保视频重建质量卓越。为了处理英语和汉语的用户提示，采用了两种双语文本编码器。模型结合3D全注意力的DiT，通过Flow Matching去噪输入噪声，转换为潜在帧，利用视频基础的DPO方法减少伪影和提高生成视频的视觉质量。通过新开发的视频生成基准Step-Video-T2V-Eval进行性能评估，Step-Video-T2V在文本到视频生成质量上表现出色。文章还讨论了当前扩散模型的局限性，并指出了未来视频基础模型的发展方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:50:38 GMT</pubDate>
</item>
<item>
<title>RAS：一种高效的动态采样策略以加速扩散模型</title>
<link>https://arxiv.org/abs/2502.10389</link>
<guid>https://arxiv.org/abs/2502.10389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAS通过动态采样区域显著提高扩散模型的实时性能。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多种生成任务中表现出色，但其依赖于多个顺序前向传播的特性限制了实时性能。传统的加速方法主要集中于减少采样步骤或重用中间结果，未能利用图像中空间区域的差异。本文提出的RAS（区域自适应采样）策略，利用扩散变换器在处理变数量标记的灵活性，为图像的不同区域动态分配采样比例。通过观察模型在每个采样步骤专注于语义重要区域的现象，RAS仅更新当前集中区域，而将其他区域使用上一步的缓存噪声进行更新。实验结果显示，RAS在Stable Diffusion 3和Lumina-Next-T2I上分别实现了2.36倍和2.51倍的加速，同时生成质量几乎不受影响。用户研究表明，RAS在人工评估中表现相当，并实现了1.6倍的速度提升，推动了扩散变换器在实时应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:22:08 GMT</pubDate>
</item>
<item>
<title>ZeroBench：一项全新的视觉推理基准挑战大型多模态模型</title>
<link>https://arxiv.org/abs/2502.09696</link>
<guid>https://arxiv.org/abs/2502.09696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroBench旨在挑战大型多模态模型的视觉推理能力，结果显示其表现较差。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型（LMMs）在图像解释上存在显著不足，其空间认知能力甚至不及幼儿或动物。尽管如此，这些模型在许多流行的视觉基准上得分颇高，然而其进展空间正在迅速缩小。因此，急需一些更具挑战性的基准以保持其长期相关性。为此，研究者推出了ZeroBench，这是一项完全不能被现代前沿LMMs解决的轻量级视觉推理基准，由100个精心编制的问题和334个较易于答复的子问题组成。对20个LMMs进行ZeroBench评估，结果显示它们的得分均为0.0%。此基准的推出旨在促进视觉理解的进展，并已公开发布以供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:20:53 GMT</pubDate>
</item>
<item>
<title>基于时空记忆的智能代理框架STMA</title>
<link>https://arxiv.org/abs/2502.10177</link>
<guid>https://arxiv.org/abs/2502.10177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STMA框架通过时空记忆提升智能代理在动态环境中的决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的代理框架——时空记忆代理(STMA)，旨在提高智能代理在动态环境中执行长远任务的能力。STMA集成了三大关键组件：实时捕捉历史和环境变化的时空记忆模块、支持自适应空间推理的动态知识图谱，以及迭代优化任务策略的规划-评估机制。我们在TextWorld环境中评估了STMA在32个任务上的表现，通过多步规划和探索，结果显示STMA相比最先进模型的成功率提高了31.25%，平均得分提高了24.7%。这些结果突显了时空记忆在提升智能代理记忆能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 21:31:11 GMT</pubDate>
</item>
<item>
<title>新框架提升二维潜在空间的三维重建效果</title>
<link>https://arxiv.org/abs/2502.09613</link>
<guid>https://arxiv.org/abs/2502.09613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出新框架，提升二维潜在空间在三维重建中的效果。</p><br /><br /><p><strong>摘要：</strong> 针对现有三维重建方法在二维特征空间与三维表示之间存在的领域差距，本文提出了一种新颖的框架，旨在将三维意识整合到二维潜在空间中。该框架由三个阶段组成，首先通过一种考虑对应关系的自编码方法，增强二维潜在表示的三维一致性；其次，利用潜在辐射场（LRF）将这些具备三维意识的二维表示提升到三维空间；最后，采用VAE-Radiance Field（VAE-RF）对齐策略，提升从渲染的二维表示中解码图像的效果。通过广泛实验，结果表明该方法在合成性能和跨数据集泛化能力方面优于当前最先进的潜在三维重建方法，是首次展示基于二维潜在表示构建的辐射场可以实现逼真的三维重建性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 21:20:14 GMT</pubDate>
</item>
<item>
<title>GSM-Ranges：评估大语言模型数学推理能力的新方法</title>
<link>https://arxiv.org/abs/2502.08680</link>
<guid>https://arxiv.org/abs/2502.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍GSM-Ranges，用于评估大语言模型在不同数值范围下的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大语言模型（LLMs）在数学推理方面的评估局限性，现有基准测试通常使用有限的数值范围，未能真实反映模型在多样化规模下的问题解决能力。为了解决这些问题，我们引入了GSM-Ranges，这是一个基于GSM8K生成的数据集，旨在系统性地扰动数学问题中的数值，以评估模型在不同数值复杂性下的稳健性。此外，我们提出了一种新颖的评分方法，能够区分逻辑错误和非逻辑错误，从而更精准地评估推理过程。实验显示，随着数值复杂性的增加，模型的逻辑错误率显著上升，高达14个百分点，表明模型在处理超出分布的数值时存在普遍弱点。同时，模型在独立的算术任务上表现良好，但在处理嵌入了文字问题的计算时，其性能显著下降。这些发现为进一步研究LMMs的数学推理能力及其数值泛化的改进提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 09:18:18 GMT</pubDate>
</item>
<item>
<title>VFX Creator: 基于AI的可控视觉效果生成新范式</title>
<link>https://arxiv.org/abs/2502.05979</link>
<guid>https://arxiv.org/abs/2502.05979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于文本描述和静态图像的可控视觉效果生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着电影制作中特效技术的发展，视觉效果（VFX）成为实现魔法与幻觉的重要工具。本文提出了一种新的动画视觉效果生成范式，通过用户友好的文本描述和静态参考图像生成动态效果。主要贡献包括：第一，建立了Open-VFX数据集，这是首个高质量的视觉效果视频数据集，涵盖15种多样化的效果类别并附有详细标注；第二，开发了VFX Creator框架，利用视频扩散变换器实现可控VFX生成。该模型具备空间和时间控制的LoRA适配器，支持实例级的空间操控与精准的时间控制。通过在Open-VFX测试集上的广泛实验，证明了该系统在生成真实动态效果上优于现有技术，并引入了一种专门的度量标准以评估时间控制的精确度。VFX Creator通过结合传统特效与生成方法，拓展了高质量视频特效生成的新可能性，使得先进的视觉效果可被更广泛的受众所掌握。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 08:47:33 GMT</pubDate>
</item>
<item>
<title>通用神经追踪控制器的开发与应用</title>
<link>https://arxiv.org/abs/2502.09614</link>
<guid>https://arxiv.org/abs/2502.09614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了从人类参考中开发的通用神经追踪控制器，以实现灵活的操作。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何开发一个通用的神经追踪控制器，以实现灵活的机器人手部操控，能够应对多种物体及其不同的操控需求。我们提出了一种方法，利用大规模成功机器人追踪示例，结合人类参考和机器人动作，来训练一个神经控制器。通过数据飞轮的方式，我们不断提升控制器的性能以及成功追踪示例的数量和质量。同时，采用强化学习与模仿学习相结合的策略，以增强控制器在动态环境中的表现。此外，为了获得高质量的追踪示例，我们还优化了每条轨迹的追踪，通过同伦优化方法，解决复杂的轨迹追踪问题，从而增加示例的多样性。实验表明，我们训练的通用神经控制器在仿真和实际环境中实现了超过10%的成功率提升，优于现有的先进基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:50:27 GMT</pubDate>
</item>
<item>
<title>3CAD：用于工业缺陷检测的新型大规模数据集与检测框架</title>
<link>https://arxiv.org/abs/2502.05761</link>
<guid>https://arxiv.org/abs/2502.05761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出3CAD数据集及其用于工业缺陷检测的CFRG框架，提升检测准确性。</p><br /><br /><p><strong>摘要：</strong> 为提高工业缺陷检测精度，本文提出了一个新型大规模数据集3CAD，源自真实的3C生产线。该数据集包含27,039幅高分辨率图像，涵盖八种不同类型的制造部件，标注了像素级的异常，具备多种异常类型和多异常区域的特征。我们还提出了一种简单有效的无监督异常检测框架——细化检测范式与恢复引导（CFRG），通过粗定位和细定位相结合，以捕捉小缺陷异常。实验结果表明，CFRG框架在3CAD数据集上的表现强劲，为异常检测领域的发展提供了一个极具挑战性的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:00:29 GMT</pubDate>
</item>
<item>
<title>ProbeLog：提高分类模型检索效率的新方法</title>
<link>https://arxiv.org/abs/2502.09619</link>
<guid>https://arxiv.org/abs/2502.09619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ProbeLog，一种高效检索分类模型的方法。</p><br /><br /><p><strong>摘要：</strong> 随着公开可用模型数量的增加，用户对 pretrained 模型的需求不断上升，但现有的模型搜索方法主要依赖于文档中的文本搜索，限制了用户找到相关模型的能力。本文介绍了一种新方法ProbeLog，它能够在没有访问模型元数据或训练数据的情况下，检索识别目标概念（如“狗”）的分类模型。与以往的探测方法不同，ProbeLog通过观测每个模型的输出维度（logit）对固定输入集（探针）的响应，计算出一个描述符。该方法支持基于 logit 的检索和零-shot、基于文本的检索。为降低编码存储库的成本，本文还开发了一种基于协同过滤的方法，使得编码成本降低三倍。实验结果表明，ProbeLog在实际应用和细粒度搜索任务中都实现了高检索准确率，并且能扩展到全尺寸的模型存储库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:58:25 GMT</pubDate>
</item>
<item>
<title>CoSER: 高质量角色扮演语言模型数据集及评估协议</title>
<link>https://arxiv.org/abs/2502.09082</link>
<guid>https://arxiv.org/abs/2502.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSER提供高质量角色扮演语言模型的数据集及评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CoSER，一个为角色扮演语言代理(RPLA)提供的高质量数据集、开放模型及评估协议。CoSER数据集包含来自771部著名书籍的17,966个角色，并提供真实对话及多样化的数据类型，如对话设置、角色体验和内心想法。我们引入了基于表演方法的给定情境表演，来训练和评估角色扮演的语言模型，LLMs能顺序展现书中多个角色。通过该数据集，我们开发了基于LLaMA-3.1的CoSER 8B和CoSER 70B先进的开放角色扮演语言模型。大量实验表明，CoSER数据集在RPLA的训练、评估和检索中具有重要价值，其中CoSER 70B在我们的评估及现有三个基准上表现优异，超越或匹配了GPT-4o，在InCharacter和LifeChoice基准上分别达到了75.80%和93.47%的准确率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:50:35 GMT</pubDate>
</item>
<item>
<title>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09390</link>
<guid>https://arxiv.org/abs/2502.09390</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:35:53 GMT</pubDate>
</item>
<item>
<title>无编码器架构在3D理解中的应用探索</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次探讨无编码器架构在3D理解中的有效性。</p><br /><br /><p><strong>摘要：</strong> 本文对无编码器架构在3D理解的应用潜力进行了全面研究，解决了现有基于编码器的3D大多模态模型所面临的挑战，例如无法适应不同的点云分辨率以及编码器输出的特征无法满足大型语言模型的语义需求。我们提出了两个关键策略：一是在预训练阶段采用LLM嵌入的语义编码策略，并使用混合语义损失提取高级语义；二是在指令调优阶段引入层次化几何聚合策略，帮助LLM关注点云的局部细节。最终，我们提出了首个无编码器的3D大多模态模型ENEL，表现出色，与当前最先进的模型ShapeLLM-13B相竞争，分别在分类、描述和视觉问答任务中取得55.0%、50.92%和42.7%的成绩。这些结果表明，无编码器架构在3D理解领域具有替代基于编码器的架构的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:27:45 GMT</pubDate>
</item>
<item>
<title>MME-CoT：评估大型多模态模型的链式思维推理性能</title>
<link>https://arxiv.org/abs/2502.09621</link>
<guid>https://arxiv.org/abs/2502.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MME-CoT基准，评估多模态模型的链式思维推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MME-CoT，一个专门评估大型多模态模型（LMMs）链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和常规场景等六个领域。作为该领域的首个综合研究，MME-CoT包含三项新颖的评估指标，细致评估推理质量、鲁棒性和效率。通过高质量数据和独特的评估策略，我们深入分析了最先进的LMMs，发现几个关键见解：拥有反思机制的模型在CoT质量上表现优异，其中Kimi k1.5表现优于GPT-4o并取得最高质量结果；CoT提示在感知性任务中通常会降低LMM性能，这表明可能存在有害的过度思考行为；尽管CoT质量较高，但拥有反思机制的LMMs在正常响应和自我修正阶段效率明显不足。我们希望MME-CoT能够为推进LMMs的多模态推理奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:34:58 GMT</pubDate>
</item>
<item>
<title>Typhoon T1：开放的泰语推理模型开发</title>
<link>https://arxiv.org/abs/2502.09042</link>
<guid>https://arxiv.org/abs/2502.09042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Typhoon T1，一个新型泰语推理模型的开放开发项目。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Typhoon T1，这是一个旨在开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）之上，通过生成长链思维来得到最终答案，已被发现能够提高复杂任务的性能。然而，关于如何开发此类模型的细节相对有限，特别是在低资源语言的推理模型方面。Typhoon T1以更具成本效益的方式进行开发，采用监督微调和开放数据集，而非强化学习。文章分享了合成数据生成与训练的细节，以及我们的数据集和模型权重。此外，我们提供了跨领域通用推理模型的开发见解，能够生成低资源语言中的推理痕迹，以泰语为例。我们希望这一开放努力能为该领域的进一步研究奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09042" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:29:44 GMT</pubDate>
</item>
<item>
<title>CoT-Valve: 动态控制推理链长度的方法</title>
<link>https://arxiv.org/abs/2502.09601</link>
<guid>https://arxiv.org/abs/2502.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT-Valve方法动态调节推理链长度，优化推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法CoT-Valve，用于动态控制推理链的长度，以应对不同任务的难度，降低推理模型的推理成本。研究表明，在简单任务中推理路径容易压缩，而在困难任务中则存在挑战。为此，我们引入了一种新的调整和推理策略，使模型能够生成不同长度的推理链。我们在参数空间中识别出一个能够有效控制生成CoT长度的方向，并构建了从长到短的推理链数据集。实验结果表明，CoT-Valve在控制能力和压缩能力上表现优越，相较于传统的提示控制方法，在减少GSM8K和AIME任务中的推理链长短时，仅带来了轻微的性能下降。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 00:16:30 GMT</pubDate>
</item>
<item>
<title>高质量合成多模态数据及其在mmE5模型中的应用</title>
<link>https://arxiv.org/abs/2502.08468</link>
<guid>https://arxiv.org/abs/2502.08468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了高质量合成多模态数据的标准及其在mmE5模型中的应用。</p><br /><br /><p><strong>摘要：</strong> 多模态嵌入模型因能够将文本和图像等不同模态的数据映射到统一的表示空间而备受关注。然而，有限的标注多模态数据常常限制了嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。我们基于这些原则合成了涵盖多任务、多模态和多语言的高质量数据集，并通过多模态大语言模型进行深度思考生成。同时，该数据集结合了真实世界的图像与准确的文本，确保保真度，经过自我评估和精炼。利用这些高质量合成和标注数据，我们训练了多模态多语言E5模型mmE5，并在MMEB基准测试上取得了最先进的表现，在XTD基准测试中展现了卓越的多语言性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
</item>
<item>
<title>构建评估框架以提升多模态大型语言模型在体感代理中的应用</title>
<link>https://arxiv.org/abs/2502.09560</link>
<guid>https://arxiv.org/abs/2502.09560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EmbodiedBench评估框架以提升多模态代理的实际应用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmbodiedBench，一个旨在评估基于多模态大型语言模型（MLLM）的体感代理的全面评估框架。虽然语言为中心的体感代理得到了较多关注，但基于MLLM的代理仍未得到充分探索。EmbodiedBench覆盖了1,128个任务，任务内容多样，从高层的语义任务到低层的原子操作，包括空间意识、视觉感知等六个核心能力的评估。通过对13个领先的MLLM进行测试，我们发现尽管MLLM在高层任务中表现良好，但在低层操作中面临挑战，最佳模型GPT-4o的平均得分仅为28.9%。EmbodiedBench为研究人员提供了一个标准化的评估平台，不仅揭示了当前的挑战，也为提升MLLM-based体感代理的研究提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09560" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
</item>
<item>
<title>Skrr: 提高文本编码器在T2I扩散模型中的内存效率</title>
<link>https://arxiv.org/abs/2502.08690</link>
<guid>https://arxiv.org/abs/2502.08690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skrr通过选择性跳过和重用层来优化文本编码器的内存使用。</p><br /><br /><p><strong>摘要：</strong> 在文本到图像（T2I）扩散模型中，文本编码器在从文本提示生成高质量图像方面表现出色，但其内存消耗却是去噪模块的八倍。本研究提出了一种名为Skip and Re-use layers (Skrr)的修剪策略，旨在针对T2I任务优化文本编码器的内存使用。Skrr通过选择性地跳过或重用变换器块中的某些层，利用其固有冗余，显著减少内存占用而不影响性能。实验结果表明，Skrr在高稀疏级别下仍能维持与原始模型相当的图像质量，且在多个评估指标（包括FID、CLIP、DreamSim和GenEval分数）上实现了最先进的内存效率，超越了现有的逐块修剪方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
</item>
<item>
<title>InfiniteHiP：高效的长序列推理框架</title>
<link>https://arxiv.org/abs/2502.08910</link>
<guid>https://arxiv.org/abs/2502.08910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InfiniteHiP框架，提高长序列处理速度和效率。</p><br /><br /><p><strong>摘要：</strong> 在现代的大型语言模型中，处理超长上下文面临诸多挑战，如推理速度缓慢和内存消耗增加。为此，我们提出了InfiniteHiP，一个新的高效推理框架，采用模块化的层次化token剪枝算法，动态去除无关的上下文token，从而加速处理。此外，该方法允许通过根据内部注意模式选择性地应用不同的RoPE调整方法，来实现更长序列的概括。我们还在推理过程中将键值缓存转移至主内存，显著降低了GPU内存压力，实现了在单个L40s 48GB GPU上处理高达300万token的能力，相比之下是原本的3倍，无任何上下文信息的永久丢失。InfinityHiP在处理100万token上下文时，实现了18.95倍的注意力解码加速，并且不需要额外的训练。通过在SGLang框架中的实现，我们通过广泛评估展示了其有效性和实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
</item>
<item>
<title>TripoSG：高保真3D形状生成的新流行扩散模型</title>
<link>https://arxiv.org/abs/2502.06608</link>
<guid>https://arxiv.org/abs/2502.06608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TripoSG通过扩散技术实现高保真3D形状生成，提升生成质量与通用性。</p><br /><br /><p><strong>摘要：</strong> 随着扩散技术的进步，图像和视频生成质量得以显著提升，但3D形状生成技术仍面临规模和复杂性限制。本文提出TripoSG，一个新型的形状扩散范式，能生成高保真的3D网格，并精准对应输入图像。TripoSG的关键创新包括：1) 一种针对3D形状生成的大规模规范流变换器，基于大量高质量数据进行训练，以达到最佳保真度；2) 结合SDF、法向量和Eikonal损失的混合监督训练策略，显著提升3D重建性能；3) 一条生成200万高质量3D样本的数据处理管道，强调数据质量和数量在训练3D生成模型中的重要性。实验验证了各组成部分的有效性，使TripoSG在3D形状生成方面实现了领先性能，3D形状细节更为丰富，且对输入图像的保真度极高。此外，TripoSG能从多样的图像风格和内容中生成3D模型，展示了强大的通用性。为了推动3D生成领域的发展，我们将公开该模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
</item>
<item>
<title>提升泰语大语言模型推理能力的方法研究</title>
<link>https://arxiv.org/abs/2502.09056</link>
<guid>https://arxiv.org/abs/2502.09056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了增强泰语大语言模型推理能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入语言特定的大语言模型（LLMs），特别关注泰语LLM。我们的目标是在保持语言特性的同时，提升语言特定LLMs的推理能力。DeepSeek R1在推理方面表现出色，但主要集中于英语和中文等高资源语言，导致低资源语言的表现受限。文章展示了仅使用公开数据集和120美元的计算预算，就能提升语言特定LLMs的推理能力，使其水平与DeepSeek R1相当，同时不影响其在目标语言任务上的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
</item>
<item>
<title>对大型语言模型理解能力的系统评估</title>
<link>https://arxiv.org/abs/2502.08946</link>
<guid>https://arxiv.org/abs/2502.08946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，LLMs在理解物理概念任务上落后于人类约40%。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地探讨了大型语言模型（LLMs）是否真正理解其所表达的内容，提出了一项名为PhysiCo的物理概念理解任务。该任务通过网格格式输入减轻了记忆化问题，网格表示不同层次的理解，包括核心现象、应用示例以及与其他抽象模式的类比。研究结果表明，尽管当前最先进的LLMs（如GPT-4o、o1和Gemini 2.0）在自然语言中能够描述和识别相关概念，但在此网格任务中表现显著低于人类，落后约40%。此外，LLMs的表现不佳源于任务的内在难度，而非网格格式的陌生性，因为在相同格式的数据上进行的上下文学习和微调对其表现几乎没有提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的逻辑推理能力研究</title>
<link>https://arxiv.org/abs/2502.09100</link>
<guid>https://arxiv.org/abs/2502.09100</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了大型语言模型在逻辑推理方面的最新进展。</p><br /><br /><p><strong>摘要：</strong> 随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展示了显著的推理能力，但其进行严谨逻辑推理的能力仍然存在疑问。本文综述了LLMs中逻辑推理的最新进展，探讨了其理论基础及评估推理能力的基准。我们分析了在不同推理范式（包括演绎、归纳、溯因和类比）下的现有能力，并评估了提升推理表现的策略，包括数据中心调优、强化学习、解码策略和神经-符号方法。最后，本文提出了未来的研究方向，强调进一步探索以增强人工智能系统中的逻辑推理能力的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09100" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
</item>
<item>
<title>SelfCite：一种自监督方法生成高质量句子级引用</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SelfCite以自监督方式提升LLMs生成高质量引用的能力。</p><br /><br /><p><strong>摘要：</strong> SelfCite是一种新颖的自监督方法，旨在提高大型语言模型(LLMs)生成高质量细粒度句子级引用的能力。该方法依赖于LLM自身提供的奖励信号，通过上下文的消融实验来判断引用的必要性。当引用文本被移除时，若应答变化则说明引用必要；而保留引用文本时，应答不变则说明已提供足够的信息。这一奖励信号可以有效指导推理过程中最佳抽样策略的实施，从而显著改善引用质量。此外，该信号也可以用于偏好优化，直接对模型进行微调，以生成更好的引用。在LongBench-Cite基准测试中，SelfCite在五个长文本问答任务上使引用F1值提升了多达5.3个百分点，展现了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
</item>
<item>
<title>基于GEMINI学习的医疗图像密集对比表示学习</title>
<link>https://arxiv.org/abs/2502.05282</link>
<guid>https://arxiv.org/abs/2502.05282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GEMINI学习以增强医疗图像的密集对比表示效率。</p><br /><br /><p><strong>摘要：</strong> 密集对比表示学习（DCRL）在医疗图像密集预测任务中显著提高了学习效率，但由于医疗图像的特殊性，往往会导致不可靠的对应关系发现，从而产生大量的错误匹配对（假阳性和假阴性）。为了解决这一问题，本文提出了一种名为GEMINI的学习框架，通过将同胚性先验嵌入到DCRL中，实现有效的对应关系发现。我们设计了可形变同胚学习（DHL），该方法通过建模医疗图像的同胚性来学习可变形映射，以在保持拓扑结构的前提下预测像素对应关系，有效减少匹配空间。还提出几何语义相似性（GSS），用于提取特征中的语义信息，从而量化对应学习的对齐度。通过这两种方法，GEMINI不仅提高了学习效率，同时构建可靠的正向匹配对。在多项实验中，我们在七个数据集上实现了比现有方法更优的结果，证明了我们方法的有效性和优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 14:57:40 GMT</pubDate>
</item>
<item>
<title>PDE-Controller: 利用大型语言模型控制偏微分方程系统</title>
<link>https://arxiv.org/abs/2502.00963</link>
<guid>https://arxiv.org/abs/2502.00963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PDE-Controller框架使LLMs能有效控制偏微分方程系统。</p><br /><br /><p><strong>摘要：</strong> PDE-Controller是一个新框架，旨在利用大型语言模型（LLMs）来控制偏微分方程（PDE）系统，充分利用其在应用数学中的潜力。该框架能够将非正式的自然语言指令转化为正式规范，并执行推理和规划步骤，从而提升PDE控制的实用性。为了实现这一目标，我们构建了一个综合解决方案，包含人类撰写的案例及200万条合成样本的数据集、数学推理模型和创新的评估指标，付出了相当大的努力。我们的实验表明，PDE-Controller在推理、自我形式化和程序合成方面，显著优于使用最新开源和GPT模型的提示方法，实现了PDE控制实用性提高62%的显著进展。通过缩小语言生成与PDE系统之间的差距，我们展示了LLMs在解决复杂科学与工程问题方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.00963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 11:41:16 GMT</pubDate>
</item>
<item>
<title>通过增强交叉注意机制实现大型模型知识传输至小型模型</title>
<link>https://arxiv.org/abs/2502.08213</link>
<guid>https://arxiv.org/abs/2502.08213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过增强交叉注意机制实现大模型向小模型的知识传输。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种LLM模块架构，利用增强交叉注意机制将知识从大型预训练模型传递给小型模型。具体而言，通过冻结Qwen2-1.5B模型，并将其表示通过特制注意力层传递给GPT-Neo-125M模型，从而在有限的计算资源下进行训练。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量可与蒸馏方法相媲美。文中讨论了模块化方法的优势，提供了输入查询示例和比较分析，并展望了该方法的进一步扩展前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 05:48:33 GMT</pubDate>
</item>
<item>
<title>改进长效目标优化的语言模型探索方法</title>
<link>https://arxiv.org/abs/2502.06533</link>
<guid>https://arxiv.org/abs/2502.06533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习改进语言模型在长效目标上的探索能力。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）发展过程中，实现长效目标是一项重要挑战。本文研究了如何通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。探索过程中需权衡发现新方案与维持预训练模型基本能力的平衡，通常通过Kullback-Leibler（KL）惩罚来控制。我们通过对小型语言模型在简单算术任务上的探索动态进行研究，发现预训练程度对探索的影响，并强调了“关键标记”的重要性，这些标记对最终结果有显著影响。此外，我们提出了一种对KL惩罚的简单修改，旨在促进关键标记的探索，提升RL微调阶段的效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:47:28 GMT</pubDate>
</item>
<item>
<title>Animate Anyone 2: 结合环境语义的角色动画生成</title>
<link>https://arxiv.org/abs/2502.06145</link>
<guid>https://arxiv.org/abs/2502.06145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍Animate Anyone 2，提升角色与环境的动画一致性与合理性。</p><br /><br /><p><strong>摘要：</strong> 随着基于扩散模型的角色图像动画方法的发展，Animate Anyone 2应运而生，旨在改善角色与其环境之间的关系。与以往仅提取源视频的运动信号不同，Animate Anyone 2还捕捉环境的表现作为条件输入，这样可以在角色与环境之间建立更合理的关联。文章提出了一种形状无关的掩码策略，以更有效地描述角色与环境的关系。同时，引入了对象引导器用于提取交互对象的特征，并通过空间混合来增强特征注入，以提高对象交互的真实感。此外，作者还提出了姿势调节策略，使模型能够处理更多样化的运动模式。实验结果表明，该方法在动画生成方面具有显著的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:45:43 GMT</pubDate>
</item>
<item>
<title>BenchMAX：一种多语言评估基准以测量语言模型的高级能力</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchMAX是一个新兴的多语言评估基准，专注于大型语言模型的高级能力测量。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型(LLMs)的快速发展，传统的多语言基准主要集中于简单理解任务，未能充分评估它们在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。为了解决这一问题，我们引入了BenchMAX，它是一个多方式的多语言评估基准，能公平比较这些关键能力。该基准经过三个不同的母语评审者对所有任务中的样本进行独立标注，并在从英语机器翻译到另外16种语言后进行测试。全面的实验表明，核心能力在不同语言间的有效性存在差异，这些差距不能仅通过扩大模型规模来弥补。BenchMAX为多语言模型的发展提供了一个综合评估平台，数据集和代码均已公开访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:34:47 GMT</pubDate>
</item>
<item>
<title>优化模型合并提升大语言模型性能</title>
<link>https://arxiv.org/abs/2502.04411</link>
<guid>https://arxiv.org/abs/2502.04411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过分层合并与任务级路由技术提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究针对将不同任务微调后的大语言模型（LLMs）合并为更强模型时参数冲突导致性能下降的问题，提出了一种新的优化方法。研究发现，不同层级的模型存在不同程度的参数冲突。因此，本文提出对参数冲突较小的层进行平均，而对参数冲突明显的层采用新颖的任务级专家路由。同时，为了降低存储成本，借鉴任务算术稀疏性，本文将多个微调专家解耦成一个稠密专家和若干个稀疏专家。在处理分布外样本时，依据任务不确定性选择并合并合适的专家。通过在LLaMA和Qwen等多个参数规模的模型上进行大规模实验，结果表明，该方法在真实世界推理任务中始终能实现显著的性能提升，并且所需的系统成本低于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:30:35 GMT</pubDate>
</item>
<item>
<title>动态安全框架优化语言模型推理安全</title>
<link>https://arxiv.org/abs/2502.07985</link>
<guid>https://arxiv.org/abs/2502.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种在推理时优化语言模型安全性的动态安全框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的动态安全框架，用于在推理时优化语言模型的安全性 reasoning，且不需修改模型权重。该方法基于近期自我批评技术的进展，利用一种元批评机制，迭代地更新安全提示（称为规范），以自适应地驱动批评和修正过程。此种测试时优化不仅提高了模型应对对抗性越狱请求的能力，还在避免道德伤害和追求诚实回答等各种安全相关任务中表现出色。通过在多个语言模型上的实证评估，结果显示动态优化的安全提示明显优于固定系统提示和静态自我批评防御策略，显著提高了安全评分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:47:30 GMT</pubDate>
</item>
<item>
<title>WorldGUI：一种新颖的GUI基准用于真实用户交互评估</title>
<link>https://arxiv.org/abs/2502.08047</link>
<guid>https://arxiv.org/abs/2502.08047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WorldGUI基准，评估GUI任务中的初始状态敏感性及其影响。</p><br /><br /><p><strong>摘要：</strong> 当前的GUI代理在元素定位方面表现出色，但规划依然面临巨大挑战，尤其是对于环境初始状态的敏感性。些微的初始状态差异，如目标软件未打开或界面未处于默认状态，往往导致规划错误，这在真实用户场景中普遍存在，但现有基准未能对此进行评估。为此，本文提出WorldGUI，这是一种新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。该基准涵盖了10款流行软件应用的多种任务，包括PowerPoint、VSCode和Adobe Acrobat。此外，为应对动态GUI自动化任务的挑战，我们提出了GUI-Thinker，一个整体框架，利用批判机制，能够有效管理GUI交互的不确定性和复杂性。实验结果显示，GUI-Thinker在WorldGUI任务上相比Claude-3.5（计算机使用）成功率提高了14.9%，突显了基于批判性思维的框架在提升GUI自动化中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:39:08 GMT</pubDate>
</item>
<item>
<title>建立值得信赖的检索增强生成（RAG）系统的综合路线图</title>
<link>https://arxiv.org/abs/2502.06872</link>
<guid>https://arxiv.org/abs/2502.06872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升RAG系统可信度的五大关键视角。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）是一种先进技术，旨在解决人工智能生成内容（AIGC）的挑战，通过将上下文检索集成到内容生成中，RAG提供可靠且最新的外部知识，减少幻觉，并确保跨任务的一致相关性。然而，尽管RAG潜力巨大，最新研究显示其也引入了新的风险，如鲁棒性问题、隐私关注、对抗攻击和责任问题。为应对这些风险，本文提出了一个关于构建值得信赖的RAG系统的综合路线图，围绕可靠性、隐私、安全性、公平性、可解释性和责任感五大关键视角展开讨论，提供一般框架和分类法，以帮助理解当前挑战、评估现有解决方案及确定未来研究方向。同时，强调值得信赖的RAG系统在实际应用中的重要影响，以鼓励更广泛的采用和创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:06:04 GMT</pubDate>
</item>
<item>
<title>NoLiMa基准评估长文本环境下大语言模型的检索能力</title>
<link>https://arxiv.org/abs/2502.05167</link>
<guid>https://arxiv.org/abs/2502.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过NoLiMa基准评估LLMs在长文本中信息检索的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NoLiMa基准，旨在评估当前大型语言模型(LLMs)在长文本环境中的信息检索能力。NIAH测试是一种常用的方法，通过在冗长的上下文中检索相关信息来衡量模型性能。与传统方法不同，NoLiMa设计了一个针集，其中问题与信息的词汇重叠最小，这要求模型推断潜在的关联以定位信息。研究评估了12种声称支持至少128K标记上下文的流行LLMs，结果显示在短文本(<1K)中它们表现良好，但随着上下文长度的增加，表现显著下降。在32K上下文中，10个模型的表现低于50%的短文本基准，并且即便是表现最好的GPT-4o，基准从99.3%降至69.7%。分析表明，长文本中缺乏字面匹配使注意力机制面临更大困难，进而影响信息的检索能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:04:29 GMT</pubDate>
</item>
<item>
<title>TextAtlas5M：评估长文本条件下的图像生成的新数据集</title>
<link>https://arxiv.org/abs/2502.07870</link>
<guid>https://arxiv.org/abs/2502.07870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextAtlas5M是一个用于评估长文本条件下图像生成的新数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本条件下的图像生成受到了广泛关注，尤其是在处理复杂的长文本提示方面。尽管取得了一定进展，现有数据集主要专注于短文本，使得长文本图像生成仍然面临挑战。为了填补这一空白，本文提出了TextAtlas5M，这是一个专门设计用于评估长文本渲染的新数据集，涵盖500万张长文本生成及收集的图像。数据集内容多样，支持对大型生成模型在长文本图像生成上的综合评估。此外，我们精心策划了3000个经过人工改进的测试集TextAtlasEval，建立了长文本条件生成方面的重要基准。评估结果显示，TextAtlasEval基准给当前最先进的专有模型（如GPT4o与DallE-3）带来了显著挑战，而开源模型的性能差距更大。这些证据使TextAtlas5M成为未来文本条件图像生成模型训练和评估的宝贵数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:50:07 GMT</pubDate>
</item>
<item>
<title>Light-A-Video：无训练的视频重光照方法</title>
<link>https://arxiv.org/abs/2502.08590</link>
<guid>https://arxiv.org/abs/2502.08590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-A-Video 提供了一种无训练的视频重光照解决方案，提升了时间一致性。</p><br /><br /><p><strong>摘要：</strong> 近日，图像重光照模型的进步主要得益于大规模数据集和预训练扩散模型，使得一致性照明得以实现。然而，视频重光照仍面临训练成本过高和高质量多样化视频数据集稀缺的挑战。本研究提出了 Light-A-Video，这是一种无训练的方法，旨在实现时间平滑的视频重光照。通过设计一致性光照注意模块（CLA），加强了自注意力层内帧间的交互，从而稳定背景光源的生成。此外，我们利用光传输独立性的物理原则，采用渐进光融合（PLF）策略，在源视频的外观和重光照外观之间进行线性混合，以确保照明的平滑过渡。实验证明，Light-A-Video在保持图像质量的同时，提高了重光照视频的时间一致性，确保了帧间光照的连贯性过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:56 GMT</pubDate>
</item>
<item>
<title>LASP-2: 提升线性注意力变换器模型的序列并行ism方法</title>
<link>https://arxiv.org/abs/2502.07563</link>
<guid>https://arxiv.org/abs/2502.07563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LASP-2 提高了线性注意力模型的训练速度和并行性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LASP-2，这是一种新型序列并行（SP）方法，旨在提升线性注意力变换器模型在处理超长输入序列时的通信和计算并行性。与之前的LASP相比，LASP-2重新审视了线性注意力层对于SP的最小通信需求，并重组了整体的通信-计算工作流。此方法仅需对中间内存状态进行一次集体通信，显著提高了通信与计算的并行性及其重叠。另外，LASP-2延伸至LASP-2H，对标准注意力模块进行类似的通信重设计，为融合线性与标准注意力层的混合模型提供了高效的SP解决方案。在对Linear-Llama3模型的评估中，LASP-2实现了相对LASP快15.2%的训练速度提升，相比Ring Attention则提升了36.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:31 GMT</pubDate>
</item>
<item>
<title>CoCoMix：结合离散的下一个标记预测与连续概念的预训练框架</title>
<link>https://arxiv.org/abs/2502.08524</link>
<guid>https://arxiv.org/abs/2502.08524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoCoMix是一种新颖的预训练框架，通过概念学习提高语言模型性能。</p><br /><br /><p><strong>摘要：</strong> CoCoMix是一种新提出的预训练框架，它结合离散的下一个标记预测与连续概念学习。通过使用预训练的稀疏自编码器，CoCoMix能够预测连续概念，并将其与模型的隐藏状态混合。在多个基准测试中，包括语言建模和下游推理任务，实验结果表明CoCoMix在样本效率上表现更佳，并且在性能上一致优于传统的下一个标记预测、知识蒸馏以及插入暂停标记的方法。研究发现，概念学习和交错处理的结合对于性能提升至关重要。此外，CoCoMix也增强了模型的可解释性和可引导性，可以直接检查和修改预测的概念，从而提供一种透明的方式来指导模型的内部推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:42:44 GMT</pubDate>
</item>
<item>
<title>基于计算预算的模型蒸馏性能估计研究</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一个蒸馏规模法，优化了模型性能与计算预算的分配。</p><br /><br /><p><strong>摘要：</strong> 本研究提供了一种蒸馏规模法，以估算基于计算预算的蒸馏模型性能，并优化了计算在教师和学生模型之间的分配。研究成果降低了大规模使用蒸馏的风险，确保在分配计算时能够最大化学生模型的性能。我们提供了计算最优的蒸馏方案，适用于存在教师模型或需要训练教师模型的情况。如果有多个学生模型进行蒸馏且已有教师模型，则蒸馏的效果优于监督预训练，直到计算水平随着学生规模的增加而可预测性地增长；而如果只有一个学生需要蒸馏且教师也需要训练，则应选择监督学习。此外，基于大规模研究结果，我们提供了关于蒸馏的新见解，增进了对蒸馏的理解并为实验设计提供了指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:41:41 GMT</pubDate>
</item>
<item>
<title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title>
<link>https://arxiv.org/abs/2502.08168</link>
<guid>https://arxiv.org/abs/2502.08168</guid>
<content:encoded><![CDATA[
In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:57:30 GMT</pubDate>
</item>
<item>
<title>CineMaster：3D感知可控文本到视频生成框架</title>
<link>https://arxiv.org/abs/2502.08639</link>
<guid>https://arxiv.org/abs/2502.08639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineMaster框架实现了3D感知与可控的文本到视频生成。</p><br /><br /><p><strong>摘要：</strong> CineMaster是一个创新的3D感知可控文本到视频生成框架，旨在为用户提供与专业导演相似的控制能力，包括场景中的物体精确放置、对象和相机在3D空间中的灵活操控，以及对渲染帧的直观布局控制。该框架分为两个阶段：第一阶段通过交互式工作流程帮助用户在3D空间内构建条件信号；第二阶段利用生成的深度图、相机轨迹和对象类别标签，指导文本到视频扩散模型生成用户意图的视频内容。此外，CineMaster还建立了自动化数据注释管道，以从大规模视频数据中提取3D边界框和相机轨迹，克服野外数据集稀缺的问题。实验结果表明，CineMaster在3D感知文本到视频生成方面明显优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:55:44 GMT</pubDate>
</item>
<item>
<title>基于下一块预测的半自回归视频生成框架</title>
<link>https://arxiv.org/abs/2502.07737</link>
<guid>https://arxiv.org/abs/2502.07737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种半自回归框架，显著提升视频生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为下一块预测（NBP）的半自回归（semi-AR）框架，用于视频生成。通过将视频内容均匀分解成等大小的块（如行或帧），我们将生成单位从单个令牌转变为块，使当前块中的每个令牌可以同时预测下一个块中对应的令牌。这种框架在每个块内应用双向注意力，捕捉到更强的空间依赖性。通过并行预测多个令牌，NBP显著减少了生成步骤，从而提高了推理速度和效率。我们的模型在UCF101和K600数据集上的FVD分数分别达到103.3和25.5，平均超越传统NTP模型4.4。此外，由于推理步骤减少，NBP模型的生成速度达到每秒8.89帧（128x128分辨率），实现了11倍的加速。我们还探索了从700M到3B参数的模型规模，发现生成质量显著提升，UCF101和K600上的FVD分数分别从103.3降至55.3和25.5降至19.5，展示了我们方法的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:48:00 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型在金融推理中的能力与改进</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型在金融推理任务中的表现及改进方法。</p><br /><br /><p><strong>摘要：</strong> 本研究综合评估了16种强大推理和通用大型语言模型(LLMs)在三项复杂金融任务上的表现，包括金融文本、表格数据和方程的处理，重点考察数值推理、表格解读、金融术语理解、长上下文处理与基于方程的问题解决能力。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但如CoT微调等一般性增强并不总是有效。此外，所有推理策略在长上下文和多表格任务中的性能提升面临挑战。为了解决这些限制，研究开发了基于Llama-3.1-8B-Instruct的金融推理增强模型，通过CoT微调和领域特定推理路径的强化学习，简单的单金融数据集微调使模型在任务中平均实现了10%的一致性提升，超越了所有8B模型及Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究强调了金融任务中适应特定领域的必要性，并指明了未来的研究方向，如多表格推理和金融术语理解。所有数据集、模型和代码都已公开，并引入了一个基准排行榜以促进未来的数据集和模型的评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:45:28 GMT</pubDate>
</item>
<item>
<title>DPO-Shift: Shifting the Distribution of Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.07599</link>
<guid>https://arxiv.org/abs/2502.07599</guid>
<content:encoded><![CDATA[
Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:43:42 GMT</pubDate>
</item>
<item>
<title>TransMLA：提升语言模型通信效率的新方法</title>
<link>https://arxiv.org/abs/2502.07864</link>
<guid>https://arxiv.org/abs/2502.07864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransMLA方法有效提升语言模型的通信效率与推理速度。</p><br /><br /><p><strong>摘要：</strong> 现代的大型语言模型在当前硬件上经常面临沟通瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在关键值（KV）层使用低秩矩阵，允许压缩的潜在KV状态被缓存，从而大幅度减少KV缓存大小，提升推理速度。尽管MLA在Deepseek V2/V3/R1中表现出效率和有效性，许多主要模型供应商仍然依赖于组查询注意力（GQA）。本文展示了GQA可以通过MLA进行始终保持相同KV缓存开销的表示，但反之则不成立。为促进MLA的广泛应用，我们引入了**TransMLA**，一种将广泛使用的基于GQA的预训练模型（如LLaMA、Qwen、Mixtral）转换为基于MLA模型的后训练方法。转换后的模型可以在不增加KV缓存大小的情况下经过额外训练来提高表达能力。此外，我们还计划开发MLA特定的推理加速技术，以保持转化模型的低延迟，进一步提升Deepseek R1的提炼效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:41:19 GMT</pubDate>
</item>
<item>
<title>可学习的合规性放弃：提高大规模语言模型的决策可靠性</title>
<link>https://arxiv.org/abs/2502.06884</link>
<guid>https://arxiv.org/abs/2502.06884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的合规性放弃方法，以提升LLM/VLM的可靠决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为可学习合规性放弃的方法，旨在提高安全关键应用中大规模语言模型（LLM）和视觉-语言模型（VLM）的决策可靠性。传统的合规性预测方法在阈值设定上过于静态，难以适应任务复杂性和数据分布的变化。为此，本文将强化学习融入合规性预测，动态优化放弃阈值，从而在最小化预测集大小的同时，确保可靠的覆盖率。经过在多个LLM/VLM基准上的广泛评估，研究表明，该方法在准确性、幻觉检测的AUROC和不确定性引导选择生成方面均优于现有方法，并显著降低了校准误差。这些改进在多种模型和数据集上均表现出色，且始终满足90%的覆盖目标，确立了该方法作为安全关键应用中可靠决策的更有效和灵活的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 18:40:34 GMT</pubDate>
</item>
<item>
<title>Pippo: 从单张照片生成高分辨率密集视频的多视角扩散模型</title>
<link>https://arxiv.org/abs/2502.07785</link>
<guid>https://arxiv.org/abs/2502.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pippo模型可从一张照片生成高分辨率的人物视频，展现出色的多视角表现。</p><br /><br /><p><strong>摘要：</strong> Pippo是一种先进的生成模型，能够仅通过一张随意拍摄的照片生成分辨率达到1000的密集人物视频。该模型采用了多视角扩散变换器，且无需额外输入，如参数模型或图像拍摄的相机参数。欲使模型有效学习，Pippo在与3亿幅无标签人像图像预训练后，进行多视角的中期训练与后期训练，快速吸收工作室数据集。中期训练中，用于去噪的低分辨率视图数量可达48个；后期训练则使用像素对齐控制，提升3D一致性的生成。在推理阶段，Pippo通过注意力偏置技巧，能够生成超过训练时5倍的视图数量。此外，团队还提出了一种改进的3D一致性评估标准，表明Pippo在单图像多视角生成人物方面的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 13:41:46 GMT</pubDate>
</item>
<item>
<title>Hypencoder：一种新型请求编码器提升文档检索性能</title>
<link>https://arxiv.org/abs/2502.05364</link>
<guid>https://arxiv.org/abs/2502.05364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Hypencoder，一个基于神经网络的请求编码器，显著提升文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 传统的检索模型通常依赖向量内积来生成查询与文档之间的相关性评分，限制了评分的表现力。本文提出一种新范式，使用小型神经网络作为学习的相关性函数，而非生成向量来表示查询。该神经网络以文档的表示为输入，输出标量相关性评分。我们应用超网络（hypernetwork）生成该神经网络的权重，称之为Hypencoder。通过在领域内的搜索任务进行实验，Hypencoder的表现显著超越了强大的稠密检索模型，并在重排序模型和规模更大的模型中取得了更高的指标。此外，Hypencoder在领域外检索任务中也展现出良好的泛化能力。为进一步评估其能力，我们对一系列困难检索任务进行了评测，包括“想不起来的检索”和“遵循指令的检索”，结果表明，与标准检索任务相比，性能差距显著扩大。最后，我们实现了一种近似搜索算法，展示该模型能够在60毫秒内搜索880万份文档。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 10:35:08 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover：开源自动化数学证明生成的最优语言模型</title>
<link>https://arxiv.org/abs/2502.07640</link>
<guid>https://arxiv.org/abs/2502.07640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover是一个实现数学证明生成最佳性能的开源语言模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover是一种开源大型语言模型，专门用于自动化数学问题的形式化证明生成，达到了该领域的最优性能。为了解决形式化数学语句和证明稀缺的问题，研究团队训练了语句形式化工具，将自然语言数学问题转化为形式语言（Lean 4），创建了一个包含164万个形式语句的数据集。利用大型语言模型，检查这些形式语句准确保留了原始自然语言问题的内容。然后，团队通过训练一系列证明者，迭代构建了一个大型形式证明数据集。每个新的证明者都成功证明了前一个证明者无法解决的许多语句，并将这些新证明追加到下一轮训练集中。最终的证明者在整体证明生成中超越了现有所有开源模型，在miniF2F基准测试中达到了57.6%的成功率，领先于之前的最佳开源模型7.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:56:56 GMT</pubDate>
</item>
<item>
<title>通过稀疏自编码器理解与控制视觉模型</title>
<link>https://arxiv.org/abs/2502.06755</link>
<guid>https://arxiv.org/abs/2502.06755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，通过稀疏自编码器理解和控制视觉模型。</p><br /><br /><p><strong>摘要：</strong> 为了深入理解视觉模型，我们不仅需要解释其学习的特征，还需通过控制实验验证这些解释。当前的方法要么提供可解释的特征但不能测试其因果影响，要么允许模型编辑但没有可解释的控制。我们提出了一个统一框架，利用稀疏自编码器（SAEs）弥补这一空白，能够发现可人类解释的视觉特征，并精确操纵这些特征以测试模型行为的假设。通过对最先进的视觉模型应用该方法，我们揭示了不同预训练目标的模型在语义抽象学习上的关键差异，并展示了我们框架在多个视觉任务中的实际应用。我们的研究表明，SAEs能够可靠地识别和操纵可解释的视觉特征，而无需对模型进行重新训练，这为理解和控制视觉模型行为提供了强大的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:54:39 GMT</pubDate>
</item>
<item>
<title>基于链式选片的长视频理解优化</title>
<link>https://arxiv.org/abs/2502.06428</link>
<guid>https://arxiv.org/abs/2502.06428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出链式选片方法以优化长视频的任务相关镜头选择。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大语言模型在处理长视频时面临的视觉token过多问题，提出了链式选片（CoS）方法。传统的视频采样方法难以平衡关键细节与冗余内容的选择，导致模型对视频理解的偏差。CoS方法将镜头选择视为测试时视觉提示的优化，通过优化镜头与任务的对齐来选择适合视频理解的镜头。其核心包括：一个二元视频摘要机制，用于发现任务相关镜头，以及一个视频共推理模块，利用二元编码将相关镜头与不相关镜头进行学习对齐。实验结果表明，CoS在多个基线和数据集上的有效性和适应性得到验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:51:18 GMT</pubDate>
</item>
<item>
<title>深入探讨模型架构与超参数对缩放法则的影响</title>
<link>https://arxiv.org/abs/2502.06857</link>
<guid>https://arxiv.org/abs/2502.06857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨模型架构及超参数对缩放法则的影响，发布开放数据集Gemstones。</p><br /><br /><p><strong>摘要：</strong> 本文研究使用广泛的模型架构和超参数选择来探讨缩放法则的适用性，强调这些选择对结果中的建议产生的重要影响。作为研究的主要成果，我们发布了Gemstones，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个检查点，这些变换器模型的参数量高达20亿，采用不同的学习率、冷却周期和架构形状进行训练。我们的数据集支持更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。通过分析模型组合的各个方面，我们发现缩放法则的建议对实验设计过程和使用的具体模型检查点非常敏感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:18:08 GMT</pubDate>
</item>
<item>
<title>基于检索增强生成的金融时间序列预测框架</title>
<link>https://arxiv.org/abs/2502.05878</link>
<guid>https://arxiv.org/abs/2502.05878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的框架，通过有效检索提升金融时间序列预测精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的检索增强生成（RAG）框架，用于金融时间序列预测，解决了现有检索方法在处理复杂金融分析中的不足。该框架的核心创新包括：以一亿参数的大型语言模型（StockLLM）作为基础，采用新颖的候选选择方法并利用LLM反馈，以及通过最大化查询与历史重要序列相似度的训练目标，来提高检索效果。新构建的数据集融合了金融指标和历史股价，确保了FinSeer的有效训练与评估。实验结果表明，该RAG框架在BIGDATA22上实现了比StockLLM和随机检索更高的预测精度，FinSeer在现有检索方法中表现优异，准确率提高了8%。此研究突显了定制化检索模型在金融预测中的重要性，并为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:10:24 GMT</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction：提升大型语言模型信息检索能力</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MEAP方法，显著提升大型语言模型的关键信息检索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Mask-Enhanced Autoregressive Prediction（MEAP）的方法，通过将遮蔽语言建模（MLM）与下一词预测（NTP）结合，显著改善大型语言模型在关键信息检索和长上下文推理任务中的表现。MEAP随机遮蔽输入令牌的一小部分，然后利用解码器Transformer进行自回归的下一词预测，避免了对双向注意力或编码-解码架构的依赖，从而在预训练和推理阶段没有额外的计算开销。实验表明，MEAP在关键检索和长上下文推理任务上优于NTP，并且在常识推理任务上表现相当或更优。在有监督微调中，MEAP在“中间缺失”情境下表现优异，超越NTP达11.77个百分点。分析显示，MEAP能够提升可区分的注意力评分，增强了模型对与任务相关信号的关注，同时减少了外围上下文的影响。这些结果表明，MEAP是大型语言模型训练中的一种有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 06:55:30 GMT</pubDate>
</item>
<item>
<title>参数化技能扩展与组合框架PSEC的研究</title>
<link>https://arxiv.org/abs/2502.05932</link>
<guid>https://arxiv.org/abs/2502.05932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出参数化技能扩展与组合框架PSEC，用于提高自主智能体的技能扩展效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了参数化技能扩展与组合框架PSEC，旨在提升自主智能体在应对新挑战时的技能扩展效率。传统方法在扩展新技能时训练效率低下，未能充分利用已有知识。PSEC通过维护可管理的技能库，以低秩适配（LoRA）模块的形式逐步集成技能原语，实现参数高效的微调，促进灵活的技能扩展。此外，该框架通过合并不同技能的LoRA模块在参数空间中直接进行技能组合，利用技能间的共享信息高效编程新技能。文章还提出了一种上下文感知模块，能够动态激活不同技能，以协同处理新任务。经过在D4RL、DSRL基准和DeepMind控制套件上的实验，结果表明PSEC在有效利用已有知识以应对新挑战以及扩大技能库方面表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:53:50 GMT</pubDate>
</item>
<item>
<title>Eclair: 一种高效的文档级光学字符识别工具</title>
<link>https://arxiv.org/abs/2502.04223</link>
<guid>https://arxiv.org/abs/2502.04223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eclair是一款优化的文本提取工具，旨在提高文档的理解和处理能力。</p><br /><br /><p><strong>摘要：</strong> Eclair是一种新型的光学字符识别（OCR）工具，专门设计用于处理多种文档类型。它不仅能从图像中提取文本，还能识别文档的结构和语义信息，如格式、公式、表格以及多个块的阅读顺序。这些功能对于文档查询、问题回答及训练大语言模型和视觉语言模型至关重要。通过引入人类标注的基准测试，Eclair在文档级OCR及语义分类上达到了最先进的准确率，超越了其他方法。此外，Eclair在多个现有基准上表现出色，展现了其广泛的应用潜力和强大的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:25:54 GMT</pubDate>
</item>
<item>
<title>FailSafeQA：评估金融领域LLM的鲁棒性与上下文意识的新基准</title>
<link>https://arxiv.org/abs/2502.06329</link>
<guid>https://arxiv.org/abs/2502.06329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新的金融基准FailSafeQA，旨在测试LLM的鲁棒性与上下文意识。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个新的长上下文金融基准FailSafeQA，旨在测试大型语言模型（LLM）在面对六种人机交互变化时的鲁棒性和上下文意识。我们聚焦于查询失败和上下文失败这两个案例研究，分别通过改变查询的领域专业性、完整性和语言准确性，及模拟上传降级、无关和空文档来进行测试。使用LLM-as-a-Judge方法及Qwen2.5-72B-Instruct模型，定义并计算24种现成模型的鲁棒性、上下文基础和合规性评分。结果表明，尽管某些模型在应对输入扰动方面表现出色，但在提供鲁棒答案时必须谨慎避免虚构信息的出现。Palmyra-Fin-128k-Instruct在合规性上表现最优，但在17%的测试案例中难以维持鲁棒预测；而OpenAI o3-mini则在41%案例中虚构了信息。这些结果表明，即使是高表现的模型仍有显著改进空间，并强调了FailSafeQA在金融应用中优化LLM可靠性的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 02:51:41 GMT</pubDate>
</item>
<item>
<title>FocalCodec：一种高效的低比特率语音编解码器</title>
<link>https://arxiv.org/abs/2502.04465</link>
<guid>https://arxiv.org/abs/2502.04465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FocalCodec是一种低比特率语音编解码器，解决现有方法的信息损失问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过自监督预训练在海量数据集上推动了自然语言处理的革命。受此启发，研究者开始将这种方法应用于语音，通过使用神经音频编解码器将连续音频离散为令牌。然而，现有方法面临高比特率、语义或声学信息损失等限制，并且为了同时捕获这两者，常常依赖于多代码簿设计，增加了下游任务的复杂性。为此，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，利用单一二进制代码簿在0.16到0.65 kbps之间压缩语音。FocalCodec在语音重合成和声音转换方面提供与当前最先进技术相媲美的性能，同时有效处理多语言语音和噪声环境。在下游任务的评估中，FocalCodec能够保持足够的语义和声学信息，且非常适合生成建模。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 01:31:44 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型的代码生成能力</title>
<link>https://arxiv.org/abs/2502.03492</link>
<guid>https://arxiv.org/abs/2502.03492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出CTRL框架以提升代码生成模型的输出效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在代码生成中的自我批评能力，提出了一种名为CTRL的框架，用于通过强化学习训练批评模型生成反馈，旨在最大化针对固定生成模型的修正效果，而无需人工干预。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并有效减轻了基础和更强生成模型的错误叠加。此外，这些批评模型还作为准确的生成奖励模型，支持测试时通过迭代批评修正进行扩展，在挑战性的代码生成基准上实现了高达106.1%的相对提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:55:37 GMT</pubDate>
</item>
<item>
<title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title>
<link>https://arxiv.org/abs/2502.07701</link>
<guid>https://arxiv.org/abs/2502.07701</guid>
<content:encoded><![CDATA[
In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:27:13 GMT</pubDate>
</item>
<item>
<title>Chameleon Benchmark Overfit Detector：评估大型语言模型的真实理解能力</title>
<link>https://arxiv.org/abs/2502.07445</link>
<guid>https://arxiv.org/abs/2502.07445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">C-BOD框架揭示LLM依赖于表面线索而非真实理解的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Chameleon Benchmark Overfit Detector (C-BOD)，一种通过参数化变换系统性扭曲基准提示的元评估框架，以检测大型语言模型（LLMs）的过拟合现象。通过对输入的重新措辞，同时保持其语义内容和标签，C-BOD可以揭示模型的性能是否源自记忆模式。我们对26个领先的LLM在MMLU基准上的评估显示，经过适度扰动后，平均表现下降2.15%，其中20个模型表现出统计显著差异。研究发现，基线准确率较高的模型在扰动下表现差异较大，而更大的LLM对重新措辞表现出更高的敏感性，这表明它们可能过于依赖固定提示模式。相比之下，Llama系列模型和较低基线准确率的模型显示出微不足道的降幅，暗示其对表面线索的依赖减少。此外，C-BOD具有数据集和模型无关的设计，便于集成到训练流程中，促进更强的语言理解。我们的发现挑战社区超越排行榜分数，重视LLM评估中的抗干扰性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:22:50 GMT</pubDate>
</item>
<item>
<title>VidCRAFT3: 一种精准控制多视觉元素的图像到视频生成框架</title>
<link>https://arxiv.org/abs/2502.07531</link>
<guid>https://arxiv.org/abs/2502.07531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidCRAFT3框架实现了对多个视觉元素的精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VidCRAFT3的创新框架，旨在实现精准的图像到视频生成，同时控制摄像机运动、物体运动和光照方向。通过提出空间三重注意力变换器，VidCRAFT3能够对每个视觉元素进行松耦合控制。由于大多数现实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），其中包含光照方向标注和多样化的物体外观，确保VidCRAFT3有效处理强光传输和反射效应。此外，我们提出了一种三阶段训练策略，消除了对多视觉元素同时注释的训练数据的需求。广泛的实验结果显示，VidCRAFT3在生成高质量视频内容方面的表现优于现有的最先进方法，尤其在控制精度和视觉一致性上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:21:13 GMT</pubDate>
</item>
<item>
<title>CAD-Editor: 基于文本的计算机辅助设计编辑框架</title>
<link>https://arxiv.org/abs/2502.03997</link>
<guid>https://arxiv.org/abs/2502.03997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAD-Editor 是一个创新的基于文本的 CAD 编辑框架，提升了设计模型修改的效率。</p><br /><br /><p><strong>摘要：</strong> CAD-Editor 是一种创新的框架，专注于文本驱动的计算机辅助设计（CAD）模型编辑，旨在克服传统方法在文本控制和现有CAD模型应用中的局限性。该框架通过自动化数据合成管道生成原始与编辑模型的配对，并利用大型视觉语言模型（LVLMs）将它们的差异总结为编辑指令。此外，CAD-Editor 采用了定位-再填充的方法，将任务拆分为两大子任务：定位待修改区域与填充适当编辑。依赖于大型语言模型（LLMs）的强大自然语言理解和CAD知识，实验结果表明，CAD-Editor 在定量和定性性能上均表现优异，证明了其在文本式CAD编辑领域的潜力和应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:16:28 GMT</pubDate>
</item>
<item>
<title>Enhance-A-Video: 一种提升DiT生成视频的一体化方法</title>
<link>https://arxiv.org/abs/2502.07508</link>
<guid>https://arxiv.org/abs/2502.07508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种训练无关的方法，提升DiT视频生成的连贯性与质量。</p><br /><br /><p><strong>摘要：</strong> DiT基础的视频生成已经取得显著成果，但现有模型的增强研究仍较为欠缺。本文介绍了一种名为Enhance-A-Video的无训练方法，旨在提升DiT生成视频的连贯性和质量。其核心思想是增强基于非对角时间注意力分布的跨帧相关性。由于设计简单，该方法可轻松应用于大多数DiT视频生成框架，而无需任何重新训练或微调。在多种DiT视频生成模型上，我们的方法在时间一致性和视觉质量方面均表现出良好的改善。我们希望这项研究能激发未来在视频生成增强方面的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:14:10 GMT</pubDate>
</item>
<item>
<title>大语言模型中的提示缓存引发的隐私泄露风险</title>
<link>https://arxiv.org/abs/2502.07776</link>
<guid>https://arxiv.org/abs/2502.07776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提示缓存导致的时间差异可能引发隐私泄露问题。</p><br /><br /><p><strong>摘要：</strong> 在大语言模型(LLMs)中，提示缓存产生的数据依赖性时延差异可能导致隐私泄露，这意味着缓存的提示处理速度快于非缓存的提示。若缓存在不同用户之间共享，攻击者能够通过快速的API响应时间识别出缓存提示，从而获取其他用户的信息。为此，我们开展了对现实世界LLM API提供者的统计审计，以检测提示缓存。结果发现，七家API提供者（包括OpenAI）之间存在全球性缓存共享，可能导致用户提示的隐私泄露。此外，由于提示缓存导致的时延变化还可能泄露模型架构的信息，我们发现OpenAI的嵌入模型是一个仅解码的Transformer，这一信息之前并未公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:11:49 GMT</pubDate>
</item>
<item>
<title>Nature语言模型：跨领域科学发现的基础模型</title>
<link>https://arxiv.org/abs/2502.07527</link>
<guid>https://arxiv.org/abs/2502.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NatureLM是一个跨科学领域的基础模型，推动科学发现的潜力。</p><br /><br /><p><strong>摘要：</strong> NatureLM是一个新的序列基础科学模型，旨在推动科学发现，通过在多个科学领域的数据上进行预训练，实现了跨领域的生成与设计应用。该模型能够生成和优化小分子、蛋白质、RNA和材料，支持蛋白质到小分子和蛋白质到RNA的跨领域生成，以及在SMILES到IUPAC翻译及USPTO-50k的逆合成任务中实现最先进的性能。NatureLM以不同的模型规模（10亿、80亿和467亿参数）进行开发，且随着模型规模的增大，性能明显改进，展现出在药物发现、材料设计和治疗蛋白或核苷酸开发等领域的广泛应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:10:26 GMT</pubDate>
</item>
<item>
<title>Hephaestus-Forge：提升LLM代理的预训练数据集</title>
<link>https://arxiv.org/abs/2502.06589</link>
<guid>https://arxiv.org/abs/2502.06589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hephaestus-Forge是首个针对LLM代理的预训练数据集，显著提升其基本能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强大型语言模型（LLM）代理在API功能调用、内在推理和规划能力方面的基本能力。Hephaestus-Forge包含1030亿个代理特定数据，涵盖76,537个API，提供工具文档以传授API功能知识及功能调用轨迹以强化内在推理。通过研究规模法则以确定最佳数据混合比率，持续在Hephaestus-Forge上进行预训练，Hephaestus在三个代理基准测试中超越小型到中型开源LLM，并与商业LLM相抗衡，证明了该预训练语料库在提升LLM代理基本能力及其对新任务或环境的泛化能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:04:08 GMT</pubDate>
</item>
<item>
<title>超大规模预训练视觉语言模型的实证研究</title>
<link>https://arxiv.org/abs/2502.07617</link>
<guid>https://arxiv.org/abs/2502.07617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨1000亿实例的预训练对多文化任务的影响。</p><br /><br /><p><strong>摘要：</strong> 本文对在前所未有的1000亿实例规模上预训练视觉语言模型的潜力进行了实证研究。研究发现，尽管传统的西方分类和检索基准（如COCO Captions）在这一规模上模型表现趋于饱和，但在文化多样性相关任务中，利用1000亿规模的网络数据取得了显著提升，特别是对长尾概念的覆盖。此外，研究分析了模型的多语言能力，低资源语言也表现出提升。值得注意的是，通过使用CLIP等质量过滤器减少预训练数据集的规模，虽然通常被认为有助于提升性能，但却可能无意中降低了大规模数据集中所表现的文化多样性。结果表明，尽管传统基准未能显著受益于扩大到1000亿实例的噪声原始网络数据，这一数据规模在构建真正包容的多模态系统中至关重要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:03:08 GMT</pubDate>
</item>
<item>
<title>CodeI/O：提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2502.07316</link>
<guid>https://arxiv.org/abs/2502.07316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeI/O 提出新方法，通过代码输入输出预测提升推理能力。</p><br /><br /><p><strong>摘要：</strong> CodeI/O 是一种新颖的方法，旨在系统性地提炼多样的推理模式，以应对大语言模型在推理任务中的挑战。通过将原始代码转化为代码输入-输出预测格式，并训练模型在自然语言下预测这些输入和输出，CodeI/O 能够帮助模型掌握通用的推理原理，如逻辑流规划、状态空间搜索、决策树遍历和模块化分解。这种方法有效地将结构化推理与特定代码的语法解耦，确保了程序的严格性。实验结果表明，CodeI/O 在多个推理任务上取得了一致的提升，包括符号、科学、逻辑、数学与数字推理以及常识推理。通过匹配现有的真实输出或重新执行代码以验证预测，CodeI/O++ 实现了更高的性能，促进了多轮修订的思考链。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:00:20 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中长链推理的训练与结构探索</title>
<link>https://arxiv.org/abs/2502.07374</link>
<guid>https://arxiv.org/abs/2502.07374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现长链推理在大规模语言模型中依赖于结构而非内容。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大规模语言模型（LLM）如何通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）实现长链推理（Long CoT）。通过仅使用17,000个长链推理培训样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中较大幅度提升了性能，与竞争对手的模型相比表现优异。研究表明，长链推理的结构在学习过程中至关重要，而单个推理步骤的内容对性能影响有限。这意味着纠错样本或去除推理关键词等内容扰动对准确性影响不大，相较之下，破坏逻辑一致性的结构性修改会显著降低准确性。这些发现为观察和提升LLM推理能力提供了新的视角，并为未来推理模型的高效训练提供了关键考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:58:37 GMT</pubDate>
</item>
<item>
<title>大语言模型强化学习在编码与推理任务中的应用</title>
<link>https://arxiv.org/abs/2502.06807</link>
<guid>https://arxiv.org/abs/2502.06807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习提升大语言模型在复杂编码推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习应用于大语言模型（LLMs）对复杂编码和推理任务表现的显著提升。通过比较两种通用推理模型——OpenAI o1和o3的早期检查点，以及一个特定领域系统o1-ioi，后者采用手工设计的推理策略，旨在参加2024国际信息学奥林匹克竞赛（IOI）。o1-ioi在IOI 2024现场比赛中，通过手工测试策略，取得了第49百分位的成绩，而在放宽的比赛约束下，获得了金牌。然而，更新的模型o3在没有手工领域特定策略或放宽约束的情况下，同样获得金牌。这些发现表明，虽然像o1-ioi这样的专用管道显著提高了表现，但扩大规模的通用o3模型在没有依赖手工推理启发式的情况下，能够超越这些成果。整体结果显示，扩大通用强化学习的规模，而不是依赖特定领域的技术，是实现推理领域尖端人工智能的可靠路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:53:19 GMT</pubDate>
</item>
</channel>
</rss>