<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>高质量合成多模态数据及其在mmE5模型中的应用</title>
<link>https://arxiv.org/abs/2502.08468</link>
<guid>https://arxiv.org/abs/2502.08468</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文探讨了高质量合成多模态数据的标准及其在mmE5模型中的应用。</p><br><br><p><strong>摘要：</strong> 多模态嵌入模型因能够将文本和图像等不同模态的数据映射到统一的表示空间而备受关注。然而，有限的标注多模态数据常常限制了嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。我们基于这些原则合成了涵盖多任务、多模态和多语言的高质量数据集，并通过多模态大语言模型进行深度思考生成。同时，该数据集结合了真实世界的图像与准确的文本，确保保真度，经过自我评估和精炼。利用这些高质量合成和标注数据，我们训练了多模态多语言E5模型mmE5，并在MMEB基准测试上取得了最先进的表现，在XTD基准测试中展现了卓越的多语言性能。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.08468 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
</item>
<item>
<title>构建评估框架以提升多模态大型语言模型在体感代理中的应用</title>
<link>https://arxiv.org/abs/2502.09560</link>
<guid>https://arxiv.org/abs/2502.09560</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出EmbodiedBench评估框架以提升多模态代理的实际应用能力。</p><br><br><p><strong>摘要：</strong> 本文介绍了EmbodiedBench，一个旨在评估基于多模态大型语言模型（MLLM）的体感代理的全面评估框架。虽然语言为中心的体感代理得到了较多关注，但基于MLLM的代理仍未得到充分探索。EmbodiedBench覆盖了1,128个任务，任务内容多样，从高层的语义任务到低层的原子操作，包括空间意识、视觉感知等六个核心能力的评估。通过对13个领先的MLLM进行测试，我们发现尽管MLLM在高层任务中表现良好，但在低层操作中面临挑战，最佳模型GPT-4o的平均得分仅为28.9%。EmbodiedBench为研究人员提供了一个标准化的评估平台，不仅揭示了当前的挑战，也为提升MLLM-based体感代理的研究提供了宝贵的见解。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.09560 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
</item>
<item>
<title>Skrr: 提高文本编码器在T2I扩散模型中的内存效率</title>
<link>https://arxiv.org/abs/2502.08690</link>
<guid>https://arxiv.org/abs/2502.08690</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Skrr通过选择性跳过和重用层来优化文本编码器的内存使用。</p><br><br><p><strong>摘要：</strong> 在文本到图像（T2I）扩散模型中，文本编码器在从文本提示生成高质量图像方面表现出色，但其内存消耗却是去噪模块的八倍。本研究提出了一种名为Skip and Re-use layers (Skrr)的修剪策略，旨在针对T2I任务优化文本编码器的内存使用。Skrr通过选择性地跳过或重用变换器块中的某些层，利用其固有冗余，显著减少内存占用而不影响性能。实验结果表明，Skrr在高稀疏级别下仍能维持与原始模型相当的图像质量，且在多个评估指标（包括FID、CLIP、DreamSim和GenEval分数）上实现了最先进的内存效率，超越了现有的逐块修剪方法。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.08690 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
</item>
<item>
<title>InfiniteHiP：高效的长序列推理框架</title>
<link>https://arxiv.org/abs/2502.08910</link>
<guid>https://arxiv.org/abs/2502.08910</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出InfiniteHiP框架，提高长序列处理速度和效率。</p><br><br><p><strong>摘要：</strong> 在现代的大型语言模型中，处理超长上下文面临诸多挑战，如推理速度缓慢和内存消耗增加。为此，我们提出了InfiniteHiP，一个新的高效推理框架，采用模块化的层次化token剪枝算法，动态去除无关的上下文token，从而加速处理。此外，该方法允许通过根据内部注意模式选择性地应用不同的RoPE调整方法，来实现更长序列的概括。我们还在推理过程中将键值缓存转移至主内存，显著降低了GPU内存压力，实现了在单个L40s 48GB GPU上处理高达300万token的能力，相比之下是原本的3倍，无任何上下文信息的永久丢失。InfinityHiP在处理100万token上下文时，实现了18.95倍的注意力解码加速，并且不需要额外的训练。通过在SGLang框架中的实现，我们通过广泛评估展示了其有效性和实用性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.08910 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
</item>
<item>
<title>TripoSG：高保真3D形状生成的新流行扩散模型</title>
<link>https://arxiv.org/abs/2502.06608</link>
<guid>https://arxiv.org/abs/2502.06608</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>TripoSG通过扩散技术实现高保真3D形状生成，提升生成质量与通用性。</p><br><br><p><strong>摘要：</strong> 随着扩散技术的进步，图像和视频生成质量得以显著提升，但3D形状生成技术仍面临规模和复杂性限制。本文提出TripoSG，一个新型的形状扩散范式，能生成高保真的3D网格，并精准对应输入图像。TripoSG的关键创新包括：1) 一种针对3D形状生成的大规模规范流变换器，基于大量高质量数据进行训练，以达到最佳保真度；2) 结合SDF、法向量和Eikonal损失的混合监督训练策略，显著提升3D重建性能；3) 一条生成200万高质量3D样本的数据处理管道，强调数据质量和数量在训练3D生成模型中的重要性。实验验证了各组成部分的有效性，使TripoSG在3D形状生成方面实现了领先性能，3D形状细节更为丰富，且对输入图像的保真度极高。此外，TripoSG能从多样的图像风格和内容中生成3D模型，展示了强大的通用性。为了推动3D生成领域的发展，我们将公开该模型。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.06608 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
</item>
<item>
<title>提升泰语大语言模型推理能力的方法研究</title>
<link>https://arxiv.org/abs/2502.09056</link>
<guid>https://arxiv.org/abs/2502.09056</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究探讨了增强泰语大语言模型推理能力的方法。</p><br><br><p><strong>摘要：</strong> 本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入语言特定的大语言模型（LLMs），特别关注泰语LLM。我们的目标是在保持语言特性的同时，提升语言特定LLMs的推理能力。DeepSeek R1在推理方面表现出色，但主要集中于英语和中文等高资源语言，导致低资源语言的表现受限。文章展示了仅使用公开数据集和120美元的计算预算，就能提升语言特定LLMs的推理能力，使其水平与DeepSeek R1相当，同时不影响其在目标语言任务上的表现。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.09056 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
</item>
<item>
<title>对大型语言模型理解能力的系统评估</title>
<link>https://arxiv.org/abs/2502.08946</link>
<guid>https://arxiv.org/abs/2502.08946</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究表明，LLMs在理解物理概念任务上落后于人类约40%。</p><br><br><p><strong>摘要：</strong> 本文系统性地探讨了大型语言模型（LLMs）是否真正理解其所表达的内容，提出了一项名为PhysiCo的物理概念理解任务。该任务通过网格格式输入减轻了记忆化问题，网格表示不同层次的理解，包括核心现象、应用示例以及与其他抽象模式的类比。研究结果表明，尽管当前最先进的LLMs（如GPT-4o、o1和Gemini 2.0）在自然语言中能够描述和识别相关概念，但在此网格任务中表现显著低于人类，落后约40%。此外，LLMs的表现不佳源于任务的内在难度，而非网格格式的陌生性，因为在相同格式的数据上进行的上下文学习和微调对其表现几乎没有提升。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.08946 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的逻辑推理能力研究</title>
<link>https://arxiv.org/abs/2502.09100</link>
<guid>https://arxiv.org/abs/2502.09100</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文综述了大型语言模型在逻辑推理方面的最新进展。</p><br><br><p><strong>摘要：</strong> 随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展示了显著的推理能力，但其进行严谨逻辑推理的能力仍然存在疑问。本文综述了LLMs中逻辑推理的最新进展，探讨了其理论基础及评估推理能力的基准。我们分析了在不同推理范式（包括演绎、归纳、溯因和类比）下的现有能力，并评估了提升推理表现的策略，包括数据中心调优、强化学习、解码策略和神经-符号方法。最后，本文提出了未来的研究方向，强调进一步探索以增强人工智能系统中的逻辑推理能力的必要性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.09100 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
</item>
<item>
<title>SelfCite：一种自监督方法生成高质量句子级引用</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>SelfCite以自监督方式提升LLMs生成高质量引用的能力。</p><br><br><p><strong>摘要：</strong> SelfCite是一种新颖的自监督方法，旨在提高大型语言模型(LLMs)生成高质量细粒度句子级引用的能力。该方法依赖于LLM自身提供的奖励信号，通过上下文的消融实验来判断引用的必要性。当引用文本被移除时，若应答变化则说明引用必要；而保留引用文本时，应答不变则说明已提供足够的信息。这一奖励信号可以有效指导推理过程中最佳抽样策略的实施，从而显著改善引用质量。此外，该信号也可以用于偏好优化，直接对模型进行微调，以生成更好的引用。在LongBench-Cite基准测试中，SelfCite在五个长文本问答任务上使引用F1值提升了多达5.3个百分点，展现了其有效性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.09604 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
</item>

<item>
<title>基于GEMINI学习的医疗图像密集对比表示学习</title>
<link>https://arxiv.org/abs/2502.05282</link>
<guid>https://arxiv.org/abs/2502.05282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GEMINI学习以增强医疗图像的密集对比表示效率。</p><br /><br /><p><strong>摘要：</strong> 密集对比表示学习（DCRL）在医疗图像密集预测任务中显著提高了学习效率，但由于医疗图像的特殊性，往往会导致不可靠的对应关系发现，从而产生大量的错误匹配对（假阳性和假阴性）。为了解决这一问题，本文提出了一种名为GEMINI的学习框架，通过将同胚性先验嵌入到DCRL中，实现有效的对应关系发现。我们设计了可形变同胚学习（DHL），该方法通过建模医疗图像的同胚性来学习可变形映射，以在保持拓扑结构的前提下预测像素对应关系，有效减少匹配空间。还提出几何语义相似性（GSS），用于提取特征中的语义信息，从而量化对应学习的对齐度。通过这两种方法，GEMINI不仅提高了学习效率，同时构建可靠的正向匹配对。在多项实验中，我们在七个数据集上实现了比现有方法更优的结果，证明了我们方法的有效性和优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 14:57:40 GMT</pubDate>
</item>
<item>
<title>PDE-Controller: 利用大型语言模型控制偏微分方程系统</title>
<link>https://arxiv.org/abs/2502.00963</link>
<guid>https://arxiv.org/abs/2502.00963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PDE-Controller框架使LLMs能有效控制偏微分方程系统。</p><br /><br /><p><strong>摘要：</strong> PDE-Controller是一个新框架，旨在利用大型语言模型（LLMs）来控制偏微分方程（PDE）系统，充分利用其在应用数学中的潜力。该框架能够将非正式的自然语言指令转化为正式规范，并执行推理和规划步骤，从而提升PDE控制的实用性。为了实现这一目标，我们构建了一个综合解决方案，包含人类撰写的案例及200万条合成样本的数据集、数学推理模型和创新的评估指标，付出了相当大的努力。我们的实验表明，PDE-Controller在推理、自我形式化和程序合成方面，显著优于使用最新开源和GPT模型的提示方法，实现了PDE控制实用性提高62%的显著进展。通过缩小语言生成与PDE系统之间的差距，我们展示了LLMs在解决复杂科学与工程问题方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.00963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 11:41:16 GMT</pubDate>
</item>
<item>
<title>通过增强交叉注意机制实现大型模型知识传输至小型模型</title>
<link>https://arxiv.org/abs/2502.08213</link>
<guid>https://arxiv.org/abs/2502.08213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过增强交叉注意机制实现大模型向小模型的知识传输。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种LLM模块架构，利用增强交叉注意机制将知识从大型预训练模型传递给小型模型。具体而言，通过冻结Qwen2-1.5B模型，并将其表示通过特制注意力层传递给GPT-Neo-125M模型，从而在有限的计算资源下进行训练。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量可与蒸馏方法相媲美。文中讨论了模块化方法的优势，提供了输入查询示例和比较分析，并展望了该方法的进一步扩展前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 05:48:33 GMT</pubDate>
</item>
<item>
<title>改进长效目标优化的语言模型探索方法</title>
<link>https://arxiv.org/abs/2502.06533</link>
<guid>https://arxiv.org/abs/2502.06533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习改进语言模型在长效目标上的探索能力。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）发展过程中，实现长效目标是一项重要挑战。本文研究了如何通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。探索过程中需权衡发现新方案与维持预训练模型基本能力的平衡，通常通过Kullback-Leibler（KL）惩罚来控制。我们通过对小型语言模型在简单算术任务上的探索动态进行研究，发现预训练程度对探索的影响，并强调了“关键标记”的重要性，这些标记对最终结果有显著影响。此外，我们提出了一种对KL惩罚的简单修改，旨在促进关键标记的探索，提升RL微调阶段的效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:47:28 GMT</pubDate>
</item>
<item>
<title>Animate Anyone 2: 结合环境语义的角色动画生成</title>
<link>https://arxiv.org/abs/2502.06145</link>
<guid>https://arxiv.org/abs/2502.06145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍Animate Anyone 2，提升角色与环境的动画一致性与合理性。</p><br /><br /><p><strong>摘要：</strong> 随着基于扩散模型的角色图像动画方法的发展，Animate Anyone 2应运而生，旨在改善角色与其环境之间的关系。与以往仅提取源视频的运动信号不同，Animate Anyone 2还捕捉环境的表现作为条件输入，这样可以在角色与环境之间建立更合理的关联。文章提出了一种形状无关的掩码策略，以更有效地描述角色与环境的关系。同时，引入了对象引导器用于提取交互对象的特征，并通过空间混合来增强特征注入，以提高对象交互的真实感。此外，作者还提出了姿势调节策略，使模型能够处理更多样化的运动模式。实验结果表明，该方法在动画生成方面具有显著的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:45:43 GMT</pubDate>
</item>
<item>
<title>BenchMAX：一种多语言评估基准以测量语言模型的高级能力</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchMAX是一个新兴的多语言评估基准，专注于大型语言模型的高级能力测量。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型(LLMs)的快速发展，传统的多语言基准主要集中于简单理解任务，未能充分评估它们在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。为了解决这一问题，我们引入了BenchMAX，它是一个多方式的多语言评估基准，能公平比较这些关键能力。该基准经过三个不同的母语评审者对所有任务中的样本进行独立标注，并在从英语机器翻译到另外16种语言后进行测试。全面的实验表明，核心能力在不同语言间的有效性存在差异，这些差距不能仅通过扩大模型规模来弥补。BenchMAX为多语言模型的发展提供了一个综合评估平台，数据集和代码均已公开访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:34:47 GMT</pubDate>
</item>
<item>
<title>优化模型合并提升大语言模型性能</title>
<link>https://arxiv.org/abs/2502.04411</link>
<guid>https://arxiv.org/abs/2502.04411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过分层合并与任务级路由技术提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究针对将不同任务微调后的大语言模型（LLMs）合并为更强模型时参数冲突导致性能下降的问题，提出了一种新的优化方法。研究发现，不同层级的模型存在不同程度的参数冲突。因此，本文提出对参数冲突较小的层进行平均，而对参数冲突明显的层采用新颖的任务级专家路由。同时，为了降低存储成本，借鉴任务算术稀疏性，本文将多个微调专家解耦成一个稠密专家和若干个稀疏专家。在处理分布外样本时，依据任务不确定性选择并合并合适的专家。通过在LLaMA和Qwen等多个参数规模的模型上进行大规模实验，结果表明，该方法在真实世界推理任务中始终能实现显著的性能提升，并且所需的系统成本低于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:30:35 GMT</pubDate>
</item>
<item>
<title>动态安全框架优化语言模型推理安全</title>
<link>https://arxiv.org/abs/2502.07985</link>
<guid>https://arxiv.org/abs/2502.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种在推理时优化语言模型安全性的动态安全框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的动态安全框架，用于在推理时优化语言模型的安全性 reasoning，且不需修改模型权重。该方法基于近期自我批评技术的进展，利用一种元批评机制，迭代地更新安全提示（称为规范），以自适应地驱动批评和修正过程。此种测试时优化不仅提高了模型应对对抗性越狱请求的能力，还在避免道德伤害和追求诚实回答等各种安全相关任务中表现出色。通过在多个语言模型上的实证评估，结果显示动态优化的安全提示明显优于固定系统提示和静态自我批评防御策略，显著提高了安全评分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:47:30 GMT</pubDate>
</item>
<item>
<title>WorldGUI：一种新颖的GUI基准用于真实用户交互评估</title>
<link>https://arxiv.org/abs/2502.08047</link>
<guid>https://arxiv.org/abs/2502.08047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WorldGUI基准，评估GUI任务中的初始状态敏感性及其影响。</p><br /><br /><p><strong>摘要：</strong> 当前的GUI代理在元素定位方面表现出色，但规划依然面临巨大挑战，尤其是对于环境初始状态的敏感性。些微的初始状态差异，如目标软件未打开或界面未处于默认状态，往往导致规划错误，这在真实用户场景中普遍存在，但现有基准未能对此进行评估。为此，本文提出WorldGUI，这是一种新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。该基准涵盖了10款流行软件应用的多种任务，包括PowerPoint、VSCode和Adobe Acrobat。此外，为应对动态GUI自动化任务的挑战，我们提出了GUI-Thinker，一个整体框架，利用批判机制，能够有效管理GUI交互的不确定性和复杂性。实验结果显示，GUI-Thinker在WorldGUI任务上相比Claude-3.5（计算机使用）成功率提高了14.9%，突显了基于批判性思维的框架在提升GUI自动化中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:39:08 GMT</pubDate>
</item>
<item>
<title>建立值得信赖的检索增强生成（RAG）系统的综合路线图</title>
<link>https://arxiv.org/abs/2502.06872</link>
<guid>https://arxiv.org/abs/2502.06872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升RAG系统可信度的五大关键视角。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）是一种先进技术，旨在解决人工智能生成内容（AIGC）的挑战，通过将上下文检索集成到内容生成中，RAG提供可靠且最新的外部知识，减少幻觉，并确保跨任务的一致相关性。然而，尽管RAG潜力巨大，最新研究显示其也引入了新的风险，如鲁棒性问题、隐私关注、对抗攻击和责任问题。为应对这些风险，本文提出了一个关于构建值得信赖的RAG系统的综合路线图，围绕可靠性、隐私、安全性、公平性、可解释性和责任感五大关键视角展开讨论，提供一般框架和分类法，以帮助理解当前挑战、评估现有解决方案及确定未来研究方向。同时，强调值得信赖的RAG系统在实际应用中的重要影响，以鼓励更广泛的采用和创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:06:04 GMT</pubDate>
</item>
<item>
<title>NoLiMa基准评估长文本环境下大语言模型的检索能力</title>
<link>https://arxiv.org/abs/2502.05167</link>
<guid>https://arxiv.org/abs/2502.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过NoLiMa基准评估LLMs在长文本中信息检索的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NoLiMa基准，旨在评估当前大型语言模型(LLMs)在长文本环境中的信息检索能力。NIAH测试是一种常用的方法，通过在冗长的上下文中检索相关信息来衡量模型性能。与传统方法不同，NoLiMa设计了一个针集，其中问题与信息的词汇重叠最小，这要求模型推断潜在的关联以定位信息。研究评估了12种声称支持至少128K标记上下文的流行LLMs，结果显示在短文本(<1K)中它们表现良好，但随着上下文长度的增加，表现显著下降。在32K上下文中，10个模型的表现低于50%的短文本基准，并且即便是表现最好的GPT-4o，基准从99.3%降至69.7%。分析表明，长文本中缺乏字面匹配使注意力机制面临更大困难，进而影响信息的检索能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:04:29 GMT</pubDate>
</item>
<item>
<title>TextAtlas5M：评估长文本条件下的图像生成的新数据集</title>
<link>https://arxiv.org/abs/2502.07870</link>
<guid>https://arxiv.org/abs/2502.07870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextAtlas5M是一个用于评估长文本条件下图像生成的新数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本条件下的图像生成受到了广泛关注，尤其是在处理复杂的长文本提示方面。尽管取得了一定进展，现有数据集主要专注于短文本，使得长文本图像生成仍然面临挑战。为了填补这一空白，本文提出了TextAtlas5M，这是一个专门设计用于评估长文本渲染的新数据集，涵盖500万张长文本生成及收集的图像。数据集内容多样，支持对大型生成模型在长文本图像生成上的综合评估。此外，我们精心策划了3000个经过人工改进的测试集TextAtlasEval，建立了长文本条件生成方面的重要基准。评估结果显示，TextAtlasEval基准给当前最先进的专有模型（如GPT4o与DallE-3）带来了显著挑战，而开源模型的性能差距更大。这些证据使TextAtlas5M成为未来文本条件图像生成模型训练和评估的宝贵数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:50:07 GMT</pubDate>
</item>
<item>
<title>Light-A-Video：无训练的视频重光照方法</title>
<link>https://arxiv.org/abs/2502.08590</link>
<guid>https://arxiv.org/abs/2502.08590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-A-Video 提供了一种无训练的视频重光照解决方案，提升了时间一致性。</p><br /><br /><p><strong>摘要：</strong> 近日，图像重光照模型的进步主要得益于大规模数据集和预训练扩散模型，使得一致性照明得以实现。然而，视频重光照仍面临训练成本过高和高质量多样化视频数据集稀缺的挑战。本研究提出了 Light-A-Video，这是一种无训练的方法，旨在实现时间平滑的视频重光照。通过设计一致性光照注意模块（CLA），加强了自注意力层内帧间的交互，从而稳定背景光源的生成。此外，我们利用光传输独立性的物理原则，采用渐进光融合（PLF）策略，在源视频的外观和重光照外观之间进行线性混合，以确保照明的平滑过渡。实验证明，Light-A-Video在保持图像质量的同时，提高了重光照视频的时间一致性，确保了帧间光照的连贯性过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:56 GMT</pubDate>
</item>
<item>
<title>LASP-2: 提升线性注意力变换器模型的序列并行ism方法</title>
<link>https://arxiv.org/abs/2502.07563</link>
<guid>https://arxiv.org/abs/2502.07563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LASP-2 提高了线性注意力模型的训练速度和并行性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LASP-2，这是一种新型序列并行（SP）方法，旨在提升线性注意力变换器模型在处理超长输入序列时的通信和计算并行性。与之前的LASP相比，LASP-2重新审视了线性注意力层对于SP的最小通信需求，并重组了整体的通信-计算工作流。此方法仅需对中间内存状态进行一次集体通信，显著提高了通信与计算的并行性及其重叠。另外，LASP-2延伸至LASP-2H，对标准注意力模块进行类似的通信重设计，为融合线性与标准注意力层的混合模型提供了高效的SP解决方案。在对Linear-Llama3模型的评估中，LASP-2实现了相对LASP快15.2%的训练速度提升，相比Ring Attention则提升了36.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:31 GMT</pubDate>
</item>
<item>
<title>CoCoMix：结合离散的下一个标记预测与连续概念的预训练框架</title>
<link>https://arxiv.org/abs/2502.08524</link>
<guid>https://arxiv.org/abs/2502.08524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoCoMix是一种新颖的预训练框架，通过概念学习提高语言模型性能。</p><br /><br /><p><strong>摘要：</strong> CoCoMix是一种新提出的预训练框架，它结合离散的下一个标记预测与连续概念学习。通过使用预训练的稀疏自编码器，CoCoMix能够预测连续概念，并将其与模型的隐藏状态混合。在多个基准测试中，包括语言建模和下游推理任务，实验结果表明CoCoMix在样本效率上表现更佳，并且在性能上一致优于传统的下一个标记预测、知识蒸馏以及插入暂停标记的方法。研究发现，概念学习和交错处理的结合对于性能提升至关重要。此外，CoCoMix也增强了模型的可解释性和可引导性，可以直接检查和修改预测的概念，从而提供一种透明的方式来指导模型的内部推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:42:44 GMT</pubDate>
</item>
<item>
<title>基于计算预算的模型蒸馏性能估计研究</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一个蒸馏规模法，优化了模型性能与计算预算的分配。</p><br /><br /><p><strong>摘要：</strong> 本研究提供了一种蒸馏规模法，以估算基于计算预算的蒸馏模型性能，并优化了计算在教师和学生模型之间的分配。研究成果降低了大规模使用蒸馏的风险，确保在分配计算时能够最大化学生模型的性能。我们提供了计算最优的蒸馏方案，适用于存在教师模型或需要训练教师模型的情况。如果有多个学生模型进行蒸馏且已有教师模型，则蒸馏的效果优于监督预训练，直到计算水平随着学生规模的增加而可预测性地增长；而如果只有一个学生需要蒸馏且教师也需要训练，则应选择监督学习。此外，基于大规模研究结果，我们提供了关于蒸馏的新见解，增进了对蒸馏的理解并为实验设计提供了指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:41:41 GMT</pubDate>
</item>
<item>
<title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title>
<link>https://arxiv.org/abs/2502.08168</link>
<guid>https://arxiv.org/abs/2502.08168</guid>
<content:encoded><![CDATA[
In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:57:30 GMT</pubDate>
</item>
<item>
<title>CineMaster：3D感知可控文本到视频生成框架</title>
<link>https://arxiv.org/abs/2502.08639</link>
<guid>https://arxiv.org/abs/2502.08639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineMaster框架实现了3D感知与可控的文本到视频生成。</p><br /><br /><p><strong>摘要：</strong> CineMaster是一个创新的3D感知可控文本到视频生成框架，旨在为用户提供与专业导演相似的控制能力，包括场景中的物体精确放置、对象和相机在3D空间中的灵活操控，以及对渲染帧的直观布局控制。该框架分为两个阶段：第一阶段通过交互式工作流程帮助用户在3D空间内构建条件信号；第二阶段利用生成的深度图、相机轨迹和对象类别标签，指导文本到视频扩散模型生成用户意图的视频内容。此外，CineMaster还建立了自动化数据注释管道，以从大规模视频数据中提取3D边界框和相机轨迹，克服野外数据集稀缺的问题。实验结果表明，CineMaster在3D感知文本到视频生成方面明显优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:55:44 GMT</pubDate>
</item>
<item>
<title>基于下一块预测的半自回归视频生成框架</title>
<link>https://arxiv.org/abs/2502.07737</link>
<guid>https://arxiv.org/abs/2502.07737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种半自回归框架，显著提升视频生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为下一块预测（NBP）的半自回归（semi-AR）框架，用于视频生成。通过将视频内容均匀分解成等大小的块（如行或帧），我们将生成单位从单个令牌转变为块，使当前块中的每个令牌可以同时预测下一个块中对应的令牌。这种框架在每个块内应用双向注意力，捕捉到更强的空间依赖性。通过并行预测多个令牌，NBP显著减少了生成步骤，从而提高了推理速度和效率。我们的模型在UCF101和K600数据集上的FVD分数分别达到103.3和25.5，平均超越传统NTP模型4.4。此外，由于推理步骤减少，NBP模型的生成速度达到每秒8.89帧（128x128分辨率），实现了11倍的加速。我们还探索了从700M到3B参数的模型规模，发现生成质量显著提升，UCF101和K600上的FVD分数分别从103.3降至55.3和25.5降至19.5，展示了我们方法的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:48:00 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型在金融推理中的能力与改进</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型在金融推理任务中的表现及改进方法。</p><br /><br /><p><strong>摘要：</strong> 本研究综合评估了16种强大推理和通用大型语言模型(LLMs)在三项复杂金融任务上的表现，包括金融文本、表格数据和方程的处理，重点考察数值推理、表格解读、金融术语理解、长上下文处理与基于方程的问题解决能力。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但如CoT微调等一般性增强并不总是有效。此外，所有推理策略在长上下文和多表格任务中的性能提升面临挑战。为了解决这些限制，研究开发了基于Llama-3.1-8B-Instruct的金融推理增强模型，通过CoT微调和领域特定推理路径的强化学习，简单的单金融数据集微调使模型在任务中平均实现了10%的一致性提升，超越了所有8B模型及Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究强调了金融任务中适应特定领域的必要性，并指明了未来的研究方向，如多表格推理和金融术语理解。所有数据集、模型和代码都已公开，并引入了一个基准排行榜以促进未来的数据集和模型的评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:45:28 GMT</pubDate>
</item>
<item>
<title>DPO-Shift: Shifting the Distribution of Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.07599</link>
<guid>https://arxiv.org/abs/2502.07599</guid>
<content:encoded><![CDATA[
Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:43:42 GMT</pubDate>
</item>
<item>
<title>TransMLA：提升语言模型通信效率的新方法</title>
<link>https://arxiv.org/abs/2502.07864</link>
<guid>https://arxiv.org/abs/2502.07864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransMLA方法有效提升语言模型的通信效率与推理速度。</p><br /><br /><p><strong>摘要：</strong> 现代的大型语言模型在当前硬件上经常面临沟通瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在关键值（KV）层使用低秩矩阵，允许压缩的潜在KV状态被缓存，从而大幅度减少KV缓存大小，提升推理速度。尽管MLA在Deepseek V2/V3/R1中表现出效率和有效性，许多主要模型供应商仍然依赖于组查询注意力（GQA）。本文展示了GQA可以通过MLA进行始终保持相同KV缓存开销的表示，但反之则不成立。为促进MLA的广泛应用，我们引入了**TransMLA**，一种将广泛使用的基于GQA的预训练模型（如LLaMA、Qwen、Mixtral）转换为基于MLA模型的后训练方法。转换后的模型可以在不增加KV缓存大小的情况下经过额外训练来提高表达能力。此外，我们还计划开发MLA特定的推理加速技术，以保持转化模型的低延迟，进一步提升Deepseek R1的提炼效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:41:19 GMT</pubDate>
</item>
<item>
<title>可学习的合规性放弃：提高大规模语言模型的决策可靠性</title>
<link>https://arxiv.org/abs/2502.06884</link>
<guid>https://arxiv.org/abs/2502.06884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的合规性放弃方法，以提升LLM/VLM的可靠决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为可学习合规性放弃的方法，旨在提高安全关键应用中大规模语言模型（LLM）和视觉-语言模型（VLM）的决策可靠性。传统的合规性预测方法在阈值设定上过于静态，难以适应任务复杂性和数据分布的变化。为此，本文将强化学习融入合规性预测，动态优化放弃阈值，从而在最小化预测集大小的同时，确保可靠的覆盖率。经过在多个LLM/VLM基准上的广泛评估，研究表明，该方法在准确性、幻觉检测的AUROC和不确定性引导选择生成方面均优于现有方法，并显著降低了校准误差。这些改进在多种模型和数据集上均表现出色，且始终满足90%的覆盖目标，确立了该方法作为安全关键应用中可靠决策的更有效和灵活的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 18:40:34 GMT</pubDate>
</item>
<item>
<title>Pippo: 从单张照片生成高分辨率密集视频的多视角扩散模型</title>
<link>https://arxiv.org/abs/2502.07785</link>
<guid>https://arxiv.org/abs/2502.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pippo模型可从一张照片生成高分辨率的人物视频，展现出色的多视角表现。</p><br /><br /><p><strong>摘要：</strong> Pippo是一种先进的生成模型，能够仅通过一张随意拍摄的照片生成分辨率达到1000的密集人物视频。该模型采用了多视角扩散变换器，且无需额外输入，如参数模型或图像拍摄的相机参数。欲使模型有效学习，Pippo在与3亿幅无标签人像图像预训练后，进行多视角的中期训练与后期训练，快速吸收工作室数据集。中期训练中，用于去噪的低分辨率视图数量可达48个；后期训练则使用像素对齐控制，提升3D一致性的生成。在推理阶段，Pippo通过注意力偏置技巧，能够生成超过训练时5倍的视图数量。此外，团队还提出了一种改进的3D一致性评估标准，表明Pippo在单图像多视角生成人物方面的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 13:41:46 GMT</pubDate>
</item>
<item>
<title>Hypencoder：一种新型请求编码器提升文档检索性能</title>
<link>https://arxiv.org/abs/2502.05364</link>
<guid>https://arxiv.org/abs/2502.05364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Hypencoder，一个基于神经网络的请求编码器，显著提升文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 传统的检索模型通常依赖向量内积来生成查询与文档之间的相关性评分，限制了评分的表现力。本文提出一种新范式，使用小型神经网络作为学习的相关性函数，而非生成向量来表示查询。该神经网络以文档的表示为输入，输出标量相关性评分。我们应用超网络（hypernetwork）生成该神经网络的权重，称之为Hypencoder。通过在领域内的搜索任务进行实验，Hypencoder的表现显著超越了强大的稠密检索模型，并在重排序模型和规模更大的模型中取得了更高的指标。此外，Hypencoder在领域外检索任务中也展现出良好的泛化能力。为进一步评估其能力，我们对一系列困难检索任务进行了评测，包括“想不起来的检索”和“遵循指令的检索”，结果表明，与标准检索任务相比，性能差距显著扩大。最后，我们实现了一种近似搜索算法，展示该模型能够在60毫秒内搜索880万份文档。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 10:35:08 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover：开源自动化数学证明生成的最优语言模型</title>
<link>https://arxiv.org/abs/2502.07640</link>
<guid>https://arxiv.org/abs/2502.07640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover是一个实现数学证明生成最佳性能的开源语言模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover是一种开源大型语言模型，专门用于自动化数学问题的形式化证明生成，达到了该领域的最优性能。为了解决形式化数学语句和证明稀缺的问题，研究团队训练了语句形式化工具，将自然语言数学问题转化为形式语言（Lean 4），创建了一个包含164万个形式语句的数据集。利用大型语言模型，检查这些形式语句准确保留了原始自然语言问题的内容。然后，团队通过训练一系列证明者，迭代构建了一个大型形式证明数据集。每个新的证明者都成功证明了前一个证明者无法解决的许多语句，并将这些新证明追加到下一轮训练集中。最终的证明者在整体证明生成中超越了现有所有开源模型，在miniF2F基准测试中达到了57.6%的成功率，领先于之前的最佳开源模型7.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:56:56 GMT</pubDate>
</item>
<item>
<title>通过稀疏自编码器理解与控制视觉模型</title>
<link>https://arxiv.org/abs/2502.06755</link>
<guid>https://arxiv.org/abs/2502.06755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，通过稀疏自编码器理解和控制视觉模型。</p><br /><br /><p><strong>摘要：</strong> 为了深入理解视觉模型，我们不仅需要解释其学习的特征，还需通过控制实验验证这些解释。当前的方法要么提供可解释的特征但不能测试其因果影响，要么允许模型编辑但没有可解释的控制。我们提出了一个统一框架，利用稀疏自编码器（SAEs）弥补这一空白，能够发现可人类解释的视觉特征，并精确操纵这些特征以测试模型行为的假设。通过对最先进的视觉模型应用该方法，我们揭示了不同预训练目标的模型在语义抽象学习上的关键差异，并展示了我们框架在多个视觉任务中的实际应用。我们的研究表明，SAEs能够可靠地识别和操纵可解释的视觉特征，而无需对模型进行重新训练，这为理解和控制视觉模型行为提供了强大的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:54:39 GMT</pubDate>
</item>
<item>
<title>基于链式选片的长视频理解优化</title>
<link>https://arxiv.org/abs/2502.06428</link>
<guid>https://arxiv.org/abs/2502.06428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出链式选片方法以优化长视频的任务相关镜头选择。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大语言模型在处理长视频时面临的视觉token过多问题，提出了链式选片（CoS）方法。传统的视频采样方法难以平衡关键细节与冗余内容的选择，导致模型对视频理解的偏差。CoS方法将镜头选择视为测试时视觉提示的优化，通过优化镜头与任务的对齐来选择适合视频理解的镜头。其核心包括：一个二元视频摘要机制，用于发现任务相关镜头，以及一个视频共推理模块，利用二元编码将相关镜头与不相关镜头进行学习对齐。实验结果表明，CoS在多个基线和数据集上的有效性和适应性得到验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:51:18 GMT</pubDate>
</item>
<item>
<title>深入探讨模型架构与超参数对缩放法则的影响</title>
<link>https://arxiv.org/abs/2502.06857</link>
<guid>https://arxiv.org/abs/2502.06857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨模型架构及超参数对缩放法则的影响，发布开放数据集Gemstones。</p><br /><br /><p><strong>摘要：</strong> 本文研究使用广泛的模型架构和超参数选择来探讨缩放法则的适用性，强调这些选择对结果中的建议产生的重要影响。作为研究的主要成果，我们发布了Gemstones，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个检查点，这些变换器模型的参数量高达20亿，采用不同的学习率、冷却周期和架构形状进行训练。我们的数据集支持更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。通过分析模型组合的各个方面，我们发现缩放法则的建议对实验设计过程和使用的具体模型检查点非常敏感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:18:08 GMT</pubDate>
</item>
<item>
<title>基于检索增强生成的金融时间序列预测框架</title>
<link>https://arxiv.org/abs/2502.05878</link>
<guid>https://arxiv.org/abs/2502.05878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的框架，通过有效检索提升金融时间序列预测精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的检索增强生成（RAG）框架，用于金融时间序列预测，解决了现有检索方法在处理复杂金融分析中的不足。该框架的核心创新包括：以一亿参数的大型语言模型（StockLLM）作为基础，采用新颖的候选选择方法并利用LLM反馈，以及通过最大化查询与历史重要序列相似度的训练目标，来提高检索效果。新构建的数据集融合了金融指标和历史股价，确保了FinSeer的有效训练与评估。实验结果表明，该RAG框架在BIGDATA22上实现了比StockLLM和随机检索更高的预测精度，FinSeer在现有检索方法中表现优异，准确率提高了8%。此研究突显了定制化检索模型在金融预测中的重要性，并为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:10:24 GMT</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction：提升大型语言模型信息检索能力</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MEAP方法，显著提升大型语言模型的关键信息检索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Mask-Enhanced Autoregressive Prediction（MEAP）的方法，通过将遮蔽语言建模（MLM）与下一词预测（NTP）结合，显著改善大型语言模型在关键信息检索和长上下文推理任务中的表现。MEAP随机遮蔽输入令牌的一小部分，然后利用解码器Transformer进行自回归的下一词预测，避免了对双向注意力或编码-解码架构的依赖，从而在预训练和推理阶段没有额外的计算开销。实验表明，MEAP在关键检索和长上下文推理任务上优于NTP，并且在常识推理任务上表现相当或更优。在有监督微调中，MEAP在“中间缺失”情境下表现优异，超越NTP达11.77个百分点。分析显示，MEAP能够提升可区分的注意力评分，增强了模型对与任务相关信号的关注，同时减少了外围上下文的影响。这些结果表明，MEAP是大型语言模型训练中的一种有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 06:55:30 GMT</pubDate>
</item>
<item>
<title>参数化技能扩展与组合框架PSEC的研究</title>
<link>https://arxiv.org/abs/2502.05932</link>
<guid>https://arxiv.org/abs/2502.05932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出参数化技能扩展与组合框架PSEC，用于提高自主智能体的技能扩展效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了参数化技能扩展与组合框架PSEC，旨在提升自主智能体在应对新挑战时的技能扩展效率。传统方法在扩展新技能时训练效率低下，未能充分利用已有知识。PSEC通过维护可管理的技能库，以低秩适配（LoRA）模块的形式逐步集成技能原语，实现参数高效的微调，促进灵活的技能扩展。此外，该框架通过合并不同技能的LoRA模块在参数空间中直接进行技能组合，利用技能间的共享信息高效编程新技能。文章还提出了一种上下文感知模块，能够动态激活不同技能，以协同处理新任务。经过在D4RL、DSRL基准和DeepMind控制套件上的实验，结果表明PSEC在有效利用已有知识以应对新挑战以及扩大技能库方面表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:53:50 GMT</pubDate>
</item>
<item>
<title>Eclair: 一种高效的文档级光学字符识别工具</title>
<link>https://arxiv.org/abs/2502.04223</link>
<guid>https://arxiv.org/abs/2502.04223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eclair是一款优化的文本提取工具，旨在提高文档的理解和处理能力。</p><br /><br /><p><strong>摘要：</strong> Eclair是一种新型的光学字符识别（OCR）工具，专门设计用于处理多种文档类型。它不仅能从图像中提取文本，还能识别文档的结构和语义信息，如格式、公式、表格以及多个块的阅读顺序。这些功能对于文档查询、问题回答及训练大语言模型和视觉语言模型至关重要。通过引入人类标注的基准测试，Eclair在文档级OCR及语义分类上达到了最先进的准确率，超越了其他方法。此外，Eclair在多个现有基准上表现出色，展现了其广泛的应用潜力和强大的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:25:54 GMT</pubDate>
</item>
<item>
<title>FailSafeQA：评估金融领域LLM的鲁棒性与上下文意识的新基准</title>
<link>https://arxiv.org/abs/2502.06329</link>
<guid>https://arxiv.org/abs/2502.06329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新的金融基准FailSafeQA，旨在测试LLM的鲁棒性与上下文意识。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个新的长上下文金融基准FailSafeQA，旨在测试大型语言模型（LLM）在面对六种人机交互变化时的鲁棒性和上下文意识。我们聚焦于查询失败和上下文失败这两个案例研究，分别通过改变查询的领域专业性、完整性和语言准确性，及模拟上传降级、无关和空文档来进行测试。使用LLM-as-a-Judge方法及Qwen2.5-72B-Instruct模型，定义并计算24种现成模型的鲁棒性、上下文基础和合规性评分。结果表明，尽管某些模型在应对输入扰动方面表现出色，但在提供鲁棒答案时必须谨慎避免虚构信息的出现。Palmyra-Fin-128k-Instruct在合规性上表现最优，但在17%的测试案例中难以维持鲁棒预测；而OpenAI o3-mini则在41%案例中虚构了信息。这些结果表明，即使是高表现的模型仍有显著改进空间，并强调了FailSafeQA在金融应用中优化LLM可靠性的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 02:51:41 GMT</pubDate>
</item>
<item>
<title>FocalCodec：一种高效的低比特率语音编解码器</title>
<link>https://arxiv.org/abs/2502.04465</link>
<guid>https://arxiv.org/abs/2502.04465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FocalCodec是一种低比特率语音编解码器，解决现有方法的信息损失问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过自监督预训练在海量数据集上推动了自然语言处理的革命。受此启发，研究者开始将这种方法应用于语音，通过使用神经音频编解码器将连续音频离散为令牌。然而，现有方法面临高比特率、语义或声学信息损失等限制，并且为了同时捕获这两者，常常依赖于多代码簿设计，增加了下游任务的复杂性。为此，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，利用单一二进制代码簿在0.16到0.65 kbps之间压缩语音。FocalCodec在语音重合成和声音转换方面提供与当前最先进技术相媲美的性能，同时有效处理多语言语音和噪声环境。在下游任务的评估中，FocalCodec能够保持足够的语义和声学信息，且非常适合生成建模。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 01:31:44 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型的代码生成能力</title>
<link>https://arxiv.org/abs/2502.03492</link>
<guid>https://arxiv.org/abs/2502.03492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出CTRL框架以提升代码生成模型的输出效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在代码生成中的自我批评能力，提出了一种名为CTRL的框架，用于通过强化学习训练批评模型生成反馈，旨在最大化针对固定生成模型的修正效果，而无需人工干预。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并有效减轻了基础和更强生成模型的错误叠加。此外，这些批评模型还作为准确的生成奖励模型，支持测试时通过迭代批评修正进行扩展，在挑战性的代码生成基准上实现了高达106.1%的相对提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:55:37 GMT</pubDate>
</item>
<item>
<title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title>
<link>https://arxiv.org/abs/2502.07701</link>
<guid>https://arxiv.org/abs/2502.07701</guid>
<content:encoded><![CDATA[
In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:27:13 GMT</pubDate>
</item>
<item>
<title>Chameleon Benchmark Overfit Detector：评估大型语言模型的真实理解能力</title>
<link>https://arxiv.org/abs/2502.07445</link>
<guid>https://arxiv.org/abs/2502.07445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">C-BOD框架揭示LLM依赖于表面线索而非真实理解的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Chameleon Benchmark Overfit Detector (C-BOD)，一种通过参数化变换系统性扭曲基准提示的元评估框架，以检测大型语言模型（LLMs）的过拟合现象。通过对输入的重新措辞，同时保持其语义内容和标签，C-BOD可以揭示模型的性能是否源自记忆模式。我们对26个领先的LLM在MMLU基准上的评估显示，经过适度扰动后，平均表现下降2.15%，其中20个模型表现出统计显著差异。研究发现，基线准确率较高的模型在扰动下表现差异较大，而更大的LLM对重新措辞表现出更高的敏感性，这表明它们可能过于依赖固定提示模式。相比之下，Llama系列模型和较低基线准确率的模型显示出微不足道的降幅，暗示其对表面线索的依赖减少。此外，C-BOD具有数据集和模型无关的设计，便于集成到训练流程中，促进更强的语言理解。我们的发现挑战社区超越排行榜分数，重视LLM评估中的抗干扰性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:22:50 GMT</pubDate>
</item>
<item>
<title>VidCRAFT3: 一种精准控制多视觉元素的图像到视频生成框架</title>
<link>https://arxiv.org/abs/2502.07531</link>
<guid>https://arxiv.org/abs/2502.07531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidCRAFT3框架实现了对多个视觉元素的精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VidCRAFT3的创新框架，旨在实现精准的图像到视频生成，同时控制摄像机运动、物体运动和光照方向。通过提出空间三重注意力变换器，VidCRAFT3能够对每个视觉元素进行松耦合控制。由于大多数现实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），其中包含光照方向标注和多样化的物体外观，确保VidCRAFT3有效处理强光传输和反射效应。此外，我们提出了一种三阶段训练策略，消除了对多视觉元素同时注释的训练数据的需求。广泛的实验结果显示，VidCRAFT3在生成高质量视频内容方面的表现优于现有的最先进方法，尤其在控制精度和视觉一致性上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:21:13 GMT</pubDate>
</item>
<item>
<title>CAD-Editor: 基于文本的计算机辅助设计编辑框架</title>
<link>https://arxiv.org/abs/2502.03997</link>
<guid>https://arxiv.org/abs/2502.03997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAD-Editor 是一个创新的基于文本的 CAD 编辑框架，提升了设计模型修改的效率。</p><br /><br /><p><strong>摘要：</strong> CAD-Editor 是一种创新的框架，专注于文本驱动的计算机辅助设计（CAD）模型编辑，旨在克服传统方法在文本控制和现有CAD模型应用中的局限性。该框架通过自动化数据合成管道生成原始与编辑模型的配对，并利用大型视觉语言模型（LVLMs）将它们的差异总结为编辑指令。此外，CAD-Editor 采用了定位-再填充的方法，将任务拆分为两大子任务：定位待修改区域与填充适当编辑。依赖于大型语言模型（LLMs）的强大自然语言理解和CAD知识，实验结果表明，CAD-Editor 在定量和定性性能上均表现优异，证明了其在文本式CAD编辑领域的潜力和应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:16:28 GMT</pubDate>
</item>
<item>
<title>Enhance-A-Video: 一种提升DiT生成视频的一体化方法</title>
<link>https://arxiv.org/abs/2502.07508</link>
<guid>https://arxiv.org/abs/2502.07508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种训练无关的方法，提升DiT视频生成的连贯性与质量。</p><br /><br /><p><strong>摘要：</strong> DiT基础的视频生成已经取得显著成果，但现有模型的增强研究仍较为欠缺。本文介绍了一种名为Enhance-A-Video的无训练方法，旨在提升DiT生成视频的连贯性和质量。其核心思想是增强基于非对角时间注意力分布的跨帧相关性。由于设计简单，该方法可轻松应用于大多数DiT视频生成框架，而无需任何重新训练或微调。在多种DiT视频生成模型上，我们的方法在时间一致性和视觉质量方面均表现出良好的改善。我们希望这项研究能激发未来在视频生成增强方面的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:14:10 GMT</pubDate>
</item>
<item>
<title>大语言模型中的提示缓存引发的隐私泄露风险</title>
<link>https://arxiv.org/abs/2502.07776</link>
<guid>https://arxiv.org/abs/2502.07776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提示缓存导致的时间差异可能引发隐私泄露问题。</p><br /><br /><p><strong>摘要：</strong> 在大语言模型(LLMs)中，提示缓存产生的数据依赖性时延差异可能导致隐私泄露，这意味着缓存的提示处理速度快于非缓存的提示。若缓存在不同用户之间共享，攻击者能够通过快速的API响应时间识别出缓存提示，从而获取其他用户的信息。为此，我们开展了对现实世界LLM API提供者的统计审计，以检测提示缓存。结果发现，七家API提供者（包括OpenAI）之间存在全球性缓存共享，可能导致用户提示的隐私泄露。此外，由于提示缓存导致的时延变化还可能泄露模型架构的信息，我们发现OpenAI的嵌入模型是一个仅解码的Transformer，这一信息之前并未公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:11:49 GMT</pubDate>
</item>
<item>
<title>Nature语言模型：跨领域科学发现的基础模型</title>
<link>https://arxiv.org/abs/2502.07527</link>
<guid>https://arxiv.org/abs/2502.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NatureLM是一个跨科学领域的基础模型，推动科学发现的潜力。</p><br /><br /><p><strong>摘要：</strong> NatureLM是一个新的序列基础科学模型，旨在推动科学发现，通过在多个科学领域的数据上进行预训练，实现了跨领域的生成与设计应用。该模型能够生成和优化小分子、蛋白质、RNA和材料，支持蛋白质到小分子和蛋白质到RNA的跨领域生成，以及在SMILES到IUPAC翻译及USPTO-50k的逆合成任务中实现最先进的性能。NatureLM以不同的模型规模（10亿、80亿和467亿参数）进行开发，且随着模型规模的增大，性能明显改进，展现出在药物发现、材料设计和治疗蛋白或核苷酸开发等领域的广泛应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:10:26 GMT</pubDate>
</item>
<item>
<title>Hephaestus-Forge：提升LLM代理的预训练数据集</title>
<link>https://arxiv.org/abs/2502.06589</link>
<guid>https://arxiv.org/abs/2502.06589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hephaestus-Forge是首个针对LLM代理的预训练数据集，显著提升其基本能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强大型语言模型（LLM）代理在API功能调用、内在推理和规划能力方面的基本能力。Hephaestus-Forge包含1030亿个代理特定数据，涵盖76,537个API，提供工具文档以传授API功能知识及功能调用轨迹以强化内在推理。通过研究规模法则以确定最佳数据混合比率，持续在Hephaestus-Forge上进行预训练，Hephaestus在三个代理基准测试中超越小型到中型开源LLM，并与商业LLM相抗衡，证明了该预训练语料库在提升LLM代理基本能力及其对新任务或环境的泛化能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:04:08 GMT</pubDate>
</item>
<item>
<title>超大规模预训练视觉语言模型的实证研究</title>
<link>https://arxiv.org/abs/2502.07617</link>
<guid>https://arxiv.org/abs/2502.07617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨1000亿实例的预训练对多文化任务的影响。</p><br /><br /><p><strong>摘要：</strong> 本文对在前所未有的1000亿实例规模上预训练视觉语言模型的潜力进行了实证研究。研究发现，尽管传统的西方分类和检索基准（如COCO Captions）在这一规模上模型表现趋于饱和，但在文化多样性相关任务中，利用1000亿规模的网络数据取得了显著提升，特别是对长尾概念的覆盖。此外，研究分析了模型的多语言能力，低资源语言也表现出提升。值得注意的是，通过使用CLIP等质量过滤器减少预训练数据集的规模，虽然通常被认为有助于提升性能，但却可能无意中降低了大规模数据集中所表现的文化多样性。结果表明，尽管传统基准未能显著受益于扩大到1000亿实例的噪声原始网络数据，这一数据规模在构建真正包容的多模态系统中至关重要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:03:08 GMT</pubDate>
</item>
<item>
<title>CodeI/O：提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2502.07316</link>
<guid>https://arxiv.org/abs/2502.07316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeI/O 提出新方法，通过代码输入输出预测提升推理能力。</p><br /><br /><p><strong>摘要：</strong> CodeI/O 是一种新颖的方法，旨在系统性地提炼多样的推理模式，以应对大语言模型在推理任务中的挑战。通过将原始代码转化为代码输入-输出预测格式，并训练模型在自然语言下预测这些输入和输出，CodeI/O 能够帮助模型掌握通用的推理原理，如逻辑流规划、状态空间搜索、决策树遍历和模块化分解。这种方法有效地将结构化推理与特定代码的语法解耦，确保了程序的严格性。实验结果表明，CodeI/O 在多个推理任务上取得了一致的提升，包括符号、科学、逻辑、数学与数字推理以及常识推理。通过匹配现有的真实输出或重新执行代码以验证预测，CodeI/O++ 实现了更高的性能，促进了多轮修订的思考链。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:00:20 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中长链推理的训练与结构探索</title>
<link>https://arxiv.org/abs/2502.07374</link>
<guid>https://arxiv.org/abs/2502.07374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现长链推理在大规模语言模型中依赖于结构而非内容。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大规模语言模型（LLM）如何通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）实现长链推理（Long CoT）。通过仅使用17,000个长链推理培训样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中较大幅度提升了性能，与竞争对手的模型相比表现优异。研究表明，长链推理的结构在学习过程中至关重要，而单个推理步骤的内容对性能影响有限。这意味着纠错样本或去除推理关键词等内容扰动对准确性影响不大，相较之下，破坏逻辑一致性的结构性修改会显著降低准确性。这些发现为观察和提升LLM推理能力提供了新的视角，并为未来推理模型的高效训练提供了关键考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:58:37 GMT</pubDate>
</item>
<item>
<title>大语言模型强化学习在编码与推理任务中的应用</title>
<link>https://arxiv.org/abs/2502.06807</link>
<guid>https://arxiv.org/abs/2502.06807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习提升大语言模型在复杂编码推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习应用于大语言模型（LLMs）对复杂编码和推理任务表现的显著提升。通过比较两种通用推理模型——OpenAI o1和o3的早期检查点，以及一个特定领域系统o1-ioi，后者采用手工设计的推理策略，旨在参加2024国际信息学奥林匹克竞赛（IOI）。o1-ioi在IOI 2024现场比赛中，通过手工测试策略，取得了第49百分位的成绩，而在放宽的比赛约束下，获得了金牌。然而，更新的模型o3在没有手工领域特定策略或放宽约束的情况下，同样获得金牌。这些发现表明，虽然像o1-ioi这样的专用管道显著提高了表现，但扩大规模的通用o3模型在没有依赖手工推理启发式的情况下，能够超越这些成果。整体结果显示，扩大通用强化学习的规模，而不是依赖特定领域的技术，是实现推理领域尖端人工智能的可靠路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:53:19 GMT</pubDate>
</item>
</channel>
</rss>