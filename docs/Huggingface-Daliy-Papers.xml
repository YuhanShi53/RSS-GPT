<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>Vox-Profile：基于语音基础模型的多维度说话人与语音特征基准</title>
<link>https://arxiv.org/abs/2505.14648</link>
<guid>https://arxiv.org/abs/2505.14648</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出Vox-Profile基准，全面评估说话人与语音的静态和动态特性。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种名为Vox-Profile的综合基准，用于通过语音基础模型刻画丰富的说话人和语音特征。与现有仅关注单一维度的研究不同，Vox-Profile提供了涵盖说话人静态属性（如年龄、性别、口音）和语音动态特性（如情感、语流）的全方位多维描述。该基准结合了语音科学和语言学专业知识，由领域专家开发以准确索引说话人及语音特征。研究利用超过15个公开可用的语音数据集和多种广泛使用的语音基础模型进行了基准实验，并展示了Vox-Profile在多个下游应用中的潜力，包括增强现有的自动语音识别数据集、评估语音生成系统性能，以及验证自动化描述的质量。Vox-Profile已开源供公众访问。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14648 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:36:41 GMT</pubDate>
<pubDate>Tue, 20 May 2025 13:36:41 GMT</pubDate>
</item>
<item>
<title>通过检测AI价值观预测潜在风险</title>
<link>https://arxiv.org/abs/2505.14633</link>
<guid>https://arxiv.org/abs/2505.14633</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究发现AI模型的潜在风险与其内在价值观相关。</p><br><br><p><strong>摘要：</strong> 随着AI模型能力增强，传统的风险检测方法面临挑战。本文受到人类行为受价值观驱动的启发，提出通过分析AI模型的价值观来预测其潜在风险。为此，我们开发了LitmusValues评估管道，用于揭示AI模型对多种价值类别的优先级。同时，我们收集了AIRiskDilemmas数据集，其中包含与AI安全相关的困境场景。实验表明，LitmusValues中的价值观不仅能预测已知的危险行为，还能预测未见过的风险行为。这一研究为AI风险管理提供了新的视角和工具。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14633 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:24:09 GMT</pubDate>
<pubDate>Tue, 20 May 2025 13:24:09 GMT</pubDate>
</item>
<item>
<title>KERL：基于知识图谱与大语言模型的个性化食品推荐与食谱生成系统</title>
<link>https://arxiv.org/abs/2505.14629</link>
<guid>https://arxiv.org/abs/2505.14629</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出结合知识图谱与大语言模型的KERL系统，实现个性化食品推荐及营养分析。</p><br><br><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步及其对食品数据的应用催生了多项研究，试图通过LLMs提升食品理解能力。尽管已有推荐系统利用LLMs和知识图谱(KGs)，但将食品相关KGs与LLMs整合的研究尚显不足。本文介绍了一种名为KERL的新系统，该系统结合食品KGs与LLMs，提供个性化食品推荐并生成带有微营养信息的食谱。KERL首先从自然语言问题中提取实体，从KG中检索子图，然后将其作为上下文输入LLM以筛选满足条件的食谱。接着，系统生成每道菜的烹饪步骤和营养信息。为了评估此方法，我们开发了一个包含食谱相关问题、约束条件和个人偏好的基准数据集。实验表明，我们的KG增强型LLM显著优于现有方法，提供了食品推荐、食谱生成和营养分析的一体化解决方案。代码和基准数据集已公开发布。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14629 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:19:57 GMT</pubDate>
<pubDate>Tue, 20 May 2025 13:19:57 GMT</pubDate>
</item>
<item>
<title>基于动态神经活动扩散的脑成像解码模型</title>
<link>https://arxiv.org/abs/2505.14556</link>
<guid>https://arxiv.org/abs/2505.14556</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种单阶段扩散模型Dynadiff用于从动态fMRI信号重建图像。</p><br><br><p><strong>摘要：</strong> 近年来，生成式人工智能模型的进步和超大超高磁场功能磁共振成像(fMRI)数据的可用性推动了脑到图像解码的发展。然而，现有方法依赖复杂的多阶段管道和预处理步骤，通常会压缩大脑记录的时间维度，限制了解码器的时间分辨率。本文介绍了一种名为Dynadiff的新单阶段扩散模型，旨在从动态演化的fMRI记录中重建图像。该模型具有三大贡献：简化训练过程、在时间分辨fMRI信号上的卓越表现以及对大脑活动中图像表示演化进行精确表征的能力。总体而言，这项工作为时间分辨的脑到图像解码奠定了基础。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14556 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:14:37 GMT</pubDate>
<pubDate>Tue, 20 May 2025 12:14:37 GMT</pubDate>
</item>
<item>
<title>基于视觉视角理解的视觉语言模型训练框架</title>
<link>https://arxiv.org/abs/2505.14366</link>
<guid>https://arxiv.org/abs/2505.14366</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出用于机器人视觉视角理解的视觉语言模型训练框架。</p><br><br><p><strong>摘要：</strong> 本文介绍了一个概念框架，用于训练视觉语言模型（VLMs）实现视觉视角理解（VPT），这是人机交互（HRI）中主体认知的核心能力。作为迈向该目标的第一步，我们引入了一个在NVIDIA Omniverse中生成的合成数据集，支持空间推理任务的有监督学习。数据集中的每一项包括RGB图像、自然语言描述及表示物体姿态的4X4变换矩阵。我们重点关注推断Z轴距离这一基础技能，并计划未来扩展至完整的6自由度推理。此数据集已公开，以支持进一步研究，标志着向具备交互场景中空间理解能力的具身AI系统迈进的重要一步。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14366 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:49:09 GMT</pubDate>
<pubDate>Tue, 20 May 2025 09:49:09 GMT</pubDate>
</item>
<item>
<title>CoIn：提升闭源大语言模型计费透明性的验证框架</title>
<link>https://arxiv.org/abs/2505.13778</link>
<guid>https://arxiv.org/abs/2505.13778</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新框架CoIn，用于审计闭源大语言模型中的令牌计数和推理内容真实性。</p><br><br><p><strong>摘要：</strong> 随着后训练技术的发展，大型语言模型通过强化学习被赋予多步结构化推理能力，这些模型在复杂任务上表现优于标准模型，并成为许多商业API的基础。然而，为了保护专有行为并减少冗长性，服务提供商通常隐藏推理痕迹仅返回最终答案，导致计费透明度缺失。这种不透明性可能引发令牌计数膨胀问题，即服务商可能虚报令牌使用量或注入合成令牌来增加费用。本文提出CoIn，这是一种验证框架，通过构建可验证哈希树检查令牌数量，并利用基于嵌入的相关性匹配检测伪造的推理内容。实验表明，当作为可信第三方审计员部署时，CoIn能够有效检测令牌计数膨胀，成功率高达94.7%，恢复了闭源大语言模型服务的计费透明性。相关代码和数据集已公开。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.13778 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 19 May 2025 19:39:23 GMT</pubDate>
<pubDate>Mon, 19 May 2025 19:39:23 GMT</pubDate>
</item>
<item>
<title>Learning to Highlight Audio by Watching Movies</title>
<link>https://arxiv.org/abs/2505.12154</link>
<guid>https://arxiv.org/abs/2505.12154</guid>
<content:encoded><![CDATA[

  Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.

]]></content:encoded>
<pubDate>Sat, 17 May 2025 18:03:57 GMT</pubDate>
<pubDate>Sat, 17 May 2025 18:03:57 GMT</pubDate>
</item>
<item>
<title>可变粒度搜索提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.11730</link>
<guid>https://arxiv.org/abs/2505.11730</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出新算法VG-Search优化验证频率，提高推理效率。</p><br><br><p><strong>摘要：</strong> 测试时扩展(TTS)通过验证提升大语言模型(LLMs)推理性能，但传统验证方式影响计算效率。本文引入变量粒度搜索(VG-Search)，通过调整粒度参数g统一束搜索和最佳N采样，实验显示动态选择g能显著改善计算效率和扩展行为。基于此，提出自适应VG-Search策略，在特定任务上提升精度同时减少FLOPs超过52%，并计划开源代码支持后续研究。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.11730 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 16 May 2025 18:24:48 GMT</pubDate>
<pubDate>Fri, 16 May 2025 18:24:48 GMT</pubDate>
</item>
<item>
<title>基于对象中心表征的机器人操作策略鲁棒性研究</title>
<link>https://arxiv.org/abs/2505.11563</link>
<guid>https://arxiv.org/abs/2505.11563</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>对象中心表征在复杂视觉条件下的泛化能力优于密集和全局特征。</p><br><br><p><strong>摘要：</strong> 本文探讨了对象中心表征（OCR）作为一种结构化替代方案在机器人操作策略中的应用，它通过将视觉输入分割成一组实体，引入了更符合操作任务的归纳偏置。我们评估了一系列视觉编码器——对象中心、全局和密集方法，在一系列模拟和真实世界操作任务中的表现，并在光照变化、纹理差异及干扰物存在等多样化视觉条件下测试其泛化能力。结果显示，即使没有针对特定任务的预训练，基于OCR的策略在泛化场景中也优于密集和全局表示法。这些发现表明，OCR是设计能够在动态真实环境中有效泛化的视觉系统的一个有前景的方向。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.11563 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 16 May 2025 03:06:37 GMT</pubDate>
<pubDate>Fri, 16 May 2025 03:06:37 GMT</pubDate>
</item>
<item>
<title>提升RAG系统性能的新方法：利用困难干扰片段</title>
<link>https://arxiv.org/abs/2505.06914</link>
<guid>https://arxiv.org/abs/2505.06914</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究提出新方法利用困难干扰片段提高检索增强生成模型准确性。</p><br><br><p><strong>摘要：</strong> 本文聚焦于检索增强生成（RAG）系统中的一个核心问题——不相关文档对语言模型答案生成的干扰效应。我们量化了这种干扰效应，并证明其在不同语言模型中的稳健性。研究引入了识别和应用困难干扰片段的新方法，通过微调这些片段，RAG系统的回答准确性提升了高达7.5%，优于传统数据集微调的对照组。我们的贡献分为两方面：一是超越了无关文档简单二分类的方法，二是开发并分析了多种寻找困难干扰片段的技术。据我们所知，这是首个提供全面框架以识别和利用困难干扰片段的研究。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.06914 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Sun, 11 May 2025 05:25:05 GMT</pubDate>
<pubDate>Sun, 11 May 2025 05:25:05 GMT</pubDate>
</item>

<item>
<title>通过强化认知专家提升大规模推理模型的推理效率</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外训练的推理时方法RICE，显著提高大规模推理模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大型推理模型（LRMs）中的混合专家（MoE）架构，指出当前模型存在的过度思考和思考不足等认知效率问题。为解决这些问题，我们提出了推理时强化认知专家（RICE）方法，利用归一化点互信息（nPMI）识别并强化特定的“认知专家”，这些专家负责高层次推理操作。通过在多个基准测试中的实验表明，该方法显著提升了基于MoE的LRMs（如DeepSeek-R1和Qwen3-235B）在定量和科学推理任务中的准确性、认知效率及跨领域泛化能力。相比现有方法如提示设计和解码约束，RICE方法不仅表现更优，还保持了模型的通用指令跟随能力，展示了提升复杂推理模型认知效率的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:16 GMT</pubDate>
</item>
<item>
<title>通过强化学习训练视觉语言模型实现无监督推理</title>
<link>https://arxiv.org/abs/2505.14677</link>
<guid>https://arxiv.org/abs/2505.14677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现通过强化学习训练视觉语言模型可提升图像推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）如DeepSeek-R1的研究表明，利用策略梯度优化（GRPO）等强化学习技术能够使预训练模型具备推理能力。本研究旨在探索如何通过强化学习训练视觉语言模型（VLMs），使其无需显式的链式思维（CoT）监督即可完成基于图像的推理任务。实验表明，仅简单提示模型先生成推理链再给出答案的方式可能导致模型依赖捷径学习，从而影响其泛化能力。为了克服这一问题，我们提出了一种新的输出格式，即先生成图像描述，再构建推理链，从而显著提升了模型的推理性能。最终，我们的模型Visionary-R1在多个视觉推理基准测试中超越了包括GPT-4o、Claude3.5-Sonnet和Gemini-1.5-Pro在内的强大多模态模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:35 GMT</pubDate>
</item>
<item>
<title>IndexMark：一种无需训练的自回归图像生成模型水印框架</title>
<link>https://arxiv.org/abs/2505.14673</link>
<guid>https://arxiv.org/abs/2505.14673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的水印框架IndexMark，用于保护自回归图像生成模型的版权。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为IndexMark的水印框架，专门针对自回归图像生成模型进行版权保护。IndexMark利用代码簿的冗余特性，在不影响图像质量的前提下嵌入水印。该方法通过匹配相似索引并替换生成索引来实现水印嵌入，同时引入索引编码器提高验证精度，并设计辅助验证方案增强对裁剪攻击的鲁棒性。实验表明，IndexMark在图像质量和验证准确性方面表现优异，且对多种干扰具有良好的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>基于自适应混合推理的大规模模型研究</title>
<link>https://arxiv.org/abs/2505.14631</link>
<guid>https://arxiv.org/abs/2505.14631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种能根据查询上下文自适应选择推理模式的大型混合推理模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，大规模推理模型（LRMs）通过引入扩展推理过程显著提升了推理能力，但其过长的推理过程导致效率下降，特别是在处理简单查询时显得多余。本文提出了大型混合推理模型（LHRMs），这是首个可根据用户查询上下文自适应决定是否进行推理的模型。我们设计了一个双阶段训练管道，包括混合微调（HFT）作为冷启动，随后通过提出的混合组策略优化（HGPO）进行在线强化学习，从而隐式学习如何选择适当的推理模式。此外，我们引入了混合准确率这一指标来定量评估模型的混合推理能力。大量实验结果表明，LHRMs在处理不同难度和类型的查询时可以自适应执行混合推理，同时在推理和通用能力上超越现有LRMs和LLMs，且显著提高了效率。这项工作呼吁重新审视扩展推理过程的适当性，并为构建混合推理系统提供了坚实的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:23:25 GMT</pubDate>
</item>
<item>
<title>Gemini模型对抗性鲁棒性评估方法及经验</title>
<link>https://arxiv.org/abs/2505.14534</link>
<guid>https://arxiv.org/abs/2505.14534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini模型因具备工具调用能力可访问用户数据，但面临恶意指令风险。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Google DeepMind评估Gemini模型对抗性鲁棒性的方法，并分享了相关经验。通过构建对抗性评估框架，采用多种自适应攻击技术持续测试Gemini的历史、当前及未来版本，揭示了这些测试如何提升Gemini抵御操控的能力。尽管Gemini模型能够执行用户指定的任务并利用工具访问数据，但当处理不受信任的数据时可能引入安全隐患，恶意指令可能导致模型违背用户期望并错误处理数据或权限。因此，加强模型的鲁棒性和安全性成为研究重点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:54:45 GMT</pubDate>
</item>
<item>
<title>基于推理诱导的无参考图像质量评估模型VisualQuality-R1</title>
<link>https://arxiv.org/abs/2505.14460</link>
<guid>https://arxiv.org/abs/2505.14460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习训练的VisualQuality-R1在图像质量评估中优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VisualQuality-R1的新模型，该模型通过强化学习优化实现了推理诱导的无参考图像质量评估（NR-IQA）。与传统的深度学习方法不同，VisualQuality-R1利用组相对策略优化为图像对生成多个质量评分，并采用Thurstone模型计算比较概率。实验表明，该模型不仅在性能上超越了其他NR-IQA方法，还能提供丰富的人类可解释的质量描述，支持多数据集训练且无需重新校准感知尺度。这些特性使其特别适用于超分辨率和图像生成等任务中的图像质量评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:56:50 GMT</pubDate>
</item>
<item>
<title>探索语言模型中的隐藏知识：Taboo模型与解密策略</title>
<link>https://arxiv.org/abs/2505.14352</link>
<guid>https://arxiv.org/abs/2505.14352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示现有技术如何揭示语言模型中的隐秘知识。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型能力增强，确保其可信性和可靠性至关重要。初步证据表明，这些模型可能试图对操作者隐瞒信息或欺骗。本文通过训练Taboo模型（描述特定秘密词但不明确陈述），测试当前技术揭示隐藏知识的能力。我们首先评估非解释性（黑盒）方法，随后基于机械性解释技术开发自动化策略，如Logit透镜和稀疏自动编码器。实验结果显示两种方法均有效。本研究强调了解释技术在揭示隐藏知识方面的潜力，并提出未来研究方向，包括在更复杂的模型上测试和完善这些方法，从而助力语言模型的安全部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:36:37 GMT</pubDate>
</item>
<item>
<title>视觉主动强化微调提升多模态大模型推理能力</title>
<link>https://arxiv.org/abs/2505.14246</link>
<guid>https://arxiv.org/abs/2505.14246</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉主动强化微调方法，显著提升多模态大模型的推理与工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大型视觉语言模型（LVLMs）的多模态主动能力开发，通过提出视觉主动强化微调（Visual-ARFT），使这些模型能够利用外部工具如网页浏览器进行实时信息检索及图像处理操作。我们设计了一个多模态主动工具基准（MAT），包括搜索（MAT-Search）和编码（MAT-Coding）两种设置，用于评估LVLMs的主动搜索与编码能力。实验结果显示，Visual-ARFT在MAT-Coding上比基线提升了18.6%的F1分数和13.0%的精确匹配得分，在MAT-Search上分别提升了10.3%和8.7%，甚至超越了GPT-4o的表现。此外，该方法在现有多跳问答基准测试如2Wiki和HotpotQA上也取得了显著进步，显示出较强的泛化能力。这项研究为构建强大且可推广的多模态智能代理提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14246" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 07:59:25 GMT</pubDate>
</item>
<item>
<title>基于双阶段训练策略构建少样本推理能力的大语言模型</title>
<link>https://arxiv.org/abs/2505.13718</link>
<guid>https://arxiv.org/abs/2505.13718</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效两阶段训练方法，在有限标注下提升大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在训练具备推理能力的大语言模型时面临的挑战，特别是在高质量训练数据稀缺的情况下。传统方法如基于可验证奖励的强化学习（RLVR）或通过精心设计的长链思维（CoT）进行蒸馏，均需要大量训练数据支持。针对这一问题，我们提出了一个样本高效的两阶段训练策略，用于在有限监督条件下开发推理型大语言模型。第一阶段通过蒸馏逻辑谜题（Knights & Knaves）中的长链思维来“预热”模型，以获取通用推理技能；第二阶段则利用少量目标领域样例对预热后的模型应用RLVR优化。实验表明，此方法不仅提升了模型在多种任务上的表现，还增强了跨领域的泛化能力，并显著提高了样本效率。研究结果证明了预热策略在构建鲁棒推理大语言模型方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13718" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 16:29:15 GMT</pubDate>
</item>
<item>
<title>AnytimeReasoner：优化语言模型在动态计算预算下的推理性能</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架AnytimeReasoner，提升大语言模型在不同计算预算下的推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AnytimeReasoner的新框架，旨在通过优化推理过程中的即时性能来提高大型语言模型（LLMs）的推理能力。传统方法通常依赖强化学习来最大化最终奖励，但这些方法往往在固定的大规模计算预算下进行优化，导致训练和部署效率低下。AnytimeReasoner通过截断完整的推理过程以适应来自先验分布的采样计算预算，迫使模型针对每个截断的推理阶段生成最优答案并验证，从而引入可验证的密集奖励，改进强化学习中的信用分配。此外，该框架还通过分离推理策略和摘要策略的优化，以及引入预算相对策略优化（BRPO）技术，进一步提高了学习过程的鲁棒性和效率。实验结果显示，在数学推理任务中，AnytimeReasoner在各种先验分布和计算预算下均优于传统的GRPO方法，显著提升了训练和计算效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:58:44 GMT</pubDate>
</item>
<item>
<title>基于量化零阶优化的高效大语言模型微调方法</title>
<link>https://arxiv.org/abs/2505.13430</link>
<guid>https://arxiv.org/abs/2505.13430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型量化零阶优化方法，大幅降低大语言模型微调的显存消耗。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型的参数量呈指数级增长，GPU显存成为适配下游任务的瓶颈。本文通过统一框架减少模型权重、梯度及优化器状态的内存占用，提出了量化零阶优化（QZO），该方法在不损失精度的情况下将4位LLMs的总显存成本减少了超过18倍，使Llama-2-13B和Stable Diffusion 3.5 Large能够在单块24GB GPU上完成微调。QZO通过扰动连续量化尺度进行梯度估计，并采用方向导数裁剪稳定训练过程，同时与标量型和码本型后训练量化方法正交兼容。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>神经符号扩散模型提升视觉推理能力</title>
<link>https://arxiv.org/abs/2505.13138</link>
<guid>https://arxiv.org/abs/2505.13138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入神经符号扩散模型解决传统模型过自信和泛化不足的问题。</p><br /><br /><p><strong>摘要：</strong> 神经符号（Neurosymbolic, NeSy）预测器结合神经感知与符号推理，在视觉推理等任务中展现出潜力，但受限于条件独立假设，难以有效建模符号间的交互与不确定性，导致预测过自信且分布外泛化表现欠佳。为克服这一局限，本文提出神经符号扩散模型（NeSyDMs），这是一种新的NeSy预测框架，通过离散扩散过程捕捉符号依赖性。在合成及真实世界基准测试中，包括高维视觉路径规划和基于规则的自动驾驶任务，NeSyDMs实现了最先进的准确率并表现出良好的校准性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:07:47 GMT</pubDate>
</item>
<item>
<title>WILLIAMT：通过模板引导提升自动化程序修复效率</title>
<link>https://arxiv.org/abs/2505.13103</link>
<guid>https://arxiv.org/abs/2505.13103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法简化漏洞修复任务并降低语言模型成本。</p><br /><br /><p><strong>摘要：</strong> 随着漏洞检测技术的进步，现代软件开发面临修复资源不足的问题，促使对高效自动化程序修复(APR)的需求增加。然而，复杂漏洞的分析难度限制了现有修复工具的效果。本研究提出了crash-site修复策略，并结合模板引导的补丁生成方法，显著降低了大型语言模型(LLMs)的令牌消耗，同时保持了修复效率和效果。实验显示，当与CodeRover-S结合时，WILLIAMT在ARVO基准测试中将修复成功率提高至73.5%，并减少了45.9%的令牌消耗。此外，即使在没有前沿LLMs的情况下，该系统仍能实现合理的修复率。这些成果表明WILLIAMT具有广泛适用性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 09:32:51 GMT</pubDate>
</item>
<item>
<title>基于RoBERTa的媒体偏见检测模型性能提升研究</title>
<link>https://arxiv.org/abs/2505.13010</link>
<guid>https://arxiv.org/abs/2505.13010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过微调RoBERTa模型，显著提升了媒体偏见检测的性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了句子级媒体偏见分类问题，利用RoBERTa模型在BABE数据集上进行微调，并与DA-RoBERTa基线模型对比，证明了在McNemar测试和5x2交叉验证配对t检验下性能有显著改善。注意力机制分析显示，该模型避免了过度关注政治敏感词汇的问题，转而更加注重上下文相关的有意义词汇。此外，我们还提出了一种结合现有偏见类型分类器的完整偏见检测管道，展示了良好的泛化性和可解释性。尽管受到数据集规模限制，我们的方法仍为构建更健壮、可解释且社会责任感更强的自然语言处理系统提供了贡献。未来的研究方向包括上下文感知建模、偏见中立化及高级偏见类型分类。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:54:39 GMT</pubDate>
</item>
<item>
<title>FedPrLLM：一种面向隐私保护的大型语言模型压缩框架</title>
<link>https://arxiv.org/abs/2505.13547</link>
<guid>https://arxiv.org/abs/2505.13547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FedPrLLM框架解决LLM压缩中的隐私保护问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FedPrLLM的联邦剪枝框架，用于在资源受限设备上部署大型语言模型（LLMs）的同时保护本地数据隐私。FedPrLLM允许每个客户端仅基于本地校准数据计算剪枝掩码并共享给服务器，从而实现全局模型的协同剪枝。实验表明，采用层比较的一次性剪枝且不缩放权重是该框架下的最佳选择。本研究旨在指导隐私敏感领域中的LLM剪枝工作，并提供开源代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 23:41:54 GMT</pubDate>
</item>
<item>
<title>探究语言模型在多跳问答中的表现及优化策略</title>
<link>https://arxiv.org/abs/2505.11754</link>
<guid>https://arxiv.org/abs/2505.11754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，编码器-解码器模型在多跳问答任务中优于解码器-only模型。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了语言模型在多跳问答（MHQA）任务中的表现，通过重新排列检索到的文档，分析了不同配置下模型的行为。研究发现，如Flan-T5家族的编码器-解码器模型尽管规模较小，但普遍优于解码器-only模型；解码器-only模型在文档顺序与推理链顺序一致时性能最佳；通过修改因果掩码增强双向注意力机制可有效提升解码器-only模型的表现。此外，我们还深入研究了语言模型在多跳问答中的注意力权重分布，发现正确答案对应的注意力权重值更高，据此提出了一种启发式方法来提升模型性能。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 19:29:47 GMT</pubDate>
</item>
<item>
<title>Phare：多语言大语言模型安全性诊断框架</title>
<link>https://arxiv.org/abs/2505.11365</link>
<guid>https://arxiv.org/abs/2505.11365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phare框架评估17个顶尖大语言模型的安全性问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Phare的多语言诊断框架，用于评估大型语言模型（LLMs）在三个关键维度上的行为：幻觉与可靠性、社会偏见及有害内容生成。通过对17个最先进的LLMs进行评估，研究发现所有安全维度上存在系统性漏洞，如谄媚、提示敏感性和刻板印象再现等特定失效模式。与仅对模型排名不同，Phare为研究人员和从业者提供了可操作的见解，以构建更强大、一致且可信的语言系统。这一框架强调识别具体失效模式而非单纯比较性能，从而推动负责任的大规模语言模型部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 11:31:08 GMT</pubDate>
</item>
<item>
<title>基于生物逆效应机制的多模态融合策略</title>
<link>https://arxiv.org/abs/2505.10176</link>
<guid>https://arxiv.org/abs/2505.10176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种受生物逆效应启发的多模态融合方法，显著提升模型性能并降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有多模态融合研究中静态整合不足的问题，受大脑逆效应现象的启发，提出了一种逆效应驱动的多模态融合（IEMF）策略。该策略通过弱化强模态信号的影响，增强弱模态信号的作用，从而实现更高效的多模态信息整合。实验验证了IEMF在音频-视觉分类、持续学习和问答等任务中的优异表现，并展示了其在人工神经网络（ANN）和脉冲神经网络（SNN）上的良好适应性。研究强调了将生物启发机制融入多模态网络的潜力，并为未来多模态人工智能的发展提供了新方向。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 07:08:50 GMT</pubDate>
</item>
<item>
<title>MIGRATION-BENCH：面向代码迁移的大规模基准测试</title>
<link>https://arxiv.org/abs/2505.09569</link>
<guid>https://arxiv.org/abs/2505.09569</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIGRATION-BENCH，评估大语言模型在Java代码迁移中的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，强大的大型语言模型（LLMs）显著提升了软件工程生产力，但现有基准主要聚焦于问题解决任务，而忽视了代码迁移这一重要领域。本文介绍MIGRATION-BENCH，这是首个专注于从Java 8迁移到最新长期支持版本（如Java 17和21）的综合基准测试集。该基准集包含5102个和300个仓库的完整数据集及其精选子集，提供了复杂性和难度适中的代表性样本。此外，我们还开发了一个全面的评估框架，用于标准化评估LLMs在代码迁移任务上的表现。通过引入SD-Feedback方法，我们展示了LLMs在处理仓库级代码迁移方面的有效性，其中Claude-3.5-Sonnet-v2在最小迁移和最大迁移任务上分别达到了62.33%和27.00%的成功率。MIGRATION-BENCH的数据集及相关源代码已公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09569" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:11:23 GMT</pubDate>
</item>
<item>
<title>Aloe Beta：开源医疗领域大型语言模型的标杆</title>
<link>https://arxiv.org/abs/2505.04388</link>
<guid>https://arxiv.org/abs/2505.04388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aloe Beta通过优化数据处理和训练提升模型安全性和效能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Aloe Beta这一开源医疗领域的大型语言模型，该模型基于Llama 3.1和Qwen 2.5等基础模型构建，并通过自定义数据集增强公共数据，同时采用直接偏好优化（DPO）进行对齐，强调伦理和政策导向性能。Aloe Beta在闭合型、开放型、安全性及人类评估测试中表现出色，展现出卓越的竞争能力，尤其是在减少偏见和毒性方面显著提高了安全性，并对潜在风险进行了详尽评估。研究结果表明，这些模型在多个医疗基准测试中表现优异，且受到专业人士青睐，标志着医疗领域开源LLM发展的新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 09:13:14 GMT</pubDate>
</item>
<item>
<title>Emerging Properties in Unified Multimodal Pretraining</title>
<link>https://arxiv.org/abs/2505.14683</link>
<guid>https://arxiv.org/abs/2505.14683</guid>
<content:encoded><![CDATA[
Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>NExT-Search：重塑生成式AI搜索的反馈驱动范式</title>
<link>https://arxiv.org/abs/2505.14680</link>
<guid>https://arxiv.org/abs/2505.14680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">论文提出NExT-Search框架，解决生成式AI搜索反馈断层问题。</p><br /><br /><p><strong>摘要：</strong> 生成式AI搜索通过提供端到端答案简化了复杂查询的信息检索过程，但因用户反馈集中在最终答案层面，导致无法有效映射至具体系统组件，阻碍了中间阶段的优化和反馈循环的持续改进。本文提出NExT-Search框架，引入用户干预和个性化模拟反馈模式，恢复对生成式AI搜索关键阶段的人类控制，旨在通过实时在线适应和离线模型更新实现系统持续进化。NExT-Search融合用户调试模式与影子用户模式，重新建立细粒度的反馈机制，为生成式AI搜索提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:13 GMT</pubDate>
</item>
<item>
<title>Reward Reasoning Model</title>
<link>https://arxiv.org/abs/2505.14674</link>
<guid>https://arxiv.org/abs/2505.14674</guid>
<content:encoded><![CDATA[
Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>General-Reasoner：一种增强大语言模型跨领域推理能力的训练范式</title>
<link>https://arxiv.org/abs/2505.14652</link>
<guid>https://arxiv.org/abs/2505.14652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型训练方法，提升大语言模型在多领域推理的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，强化学习在增强大型语言模型推理能力方面展现出巨大潜力。然而，现有研究主要集中在数学和编程领域，因数据丰富且答案验证容易。本文提出了General-Reasoner，这是一种新的训练范式，旨在提高大型语言模型在多样领域中的推理能力。我们构建了一个大规模高质量的问题数据集，并开发了一种基于生成模型的答案验证器，取代传统的规则验证方法。通过在多个领域（如物理、化学、金融等）的数据集上进行评估，结果显示General-Reasoner在多种基准测试中优于现有基线方法，特别是在数学推理任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:41:33 GMT</pubDate>
</item>
<item>
<title>重新审视视频理解基准：VideoEval-Pro 的提出</title>
<link>https://arxiv.org/abs/2505.14640</link>
<guid>https://arxiv.org/abs/2505.14640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有视频理解基准存在评估偏差问题，本文提出新的基准 VideoEval-Pro。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型（LMMs）在长视频理解（LVU）中的应用推动了相关基准的发展，但我们的研究表明现有LVU基准存在显著问题。多数基准依赖多选题（MCQs），导致评估结果因猜测而虚高；且部分问题具有强先验，模型无需观看视频即可作答。此外，增加帧数未必提升性能，削弱了当前基准的有效性和鲁棒性。为此，我们提出了VideoEval-Pro，一个包含开放式简答题的新基准，真正需要理解整个视频。通过评估21个视频LMMs，发现开放题性能比MCQs下降明显，高MCQ分数不等同于高开放题分数，且VideoEval-Pro从增加帧数中获益更多。该研究揭示了现有基准的局限性，并提供了一个更可靠的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:26:32 GMT</pubDate>
</item>
<item>
<title>基于流匹配的潜在流Transformer压缩大语言模型</title>
<link>https://arxiv.org/abs/2505.14513</link>
<guid>https://arxiv.org/abs/2505.14513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Latent Flow Transformer，通过单一流动算子替换多层网络实现高效压缩。</p><br /><br /><p><strong>摘要：</strong> 标准的大语言模型通常由数十到数百个离散层组成，尽管更多层数可能带来更好的性能，但这种方法效率低下。受扩散和基于流的图像生成模型的启发，本文提出了Latent Flow Transformer (LFT)，它用单一学习的传输算子替代一层或多层网络，同时保持与原始架构的兼容性。此外，为了克服现有基于流方法在保持耦合方面的局限性，引入了Flow Walking算法。实验显示，在Pythia-410M模型上，LFT通过流匹配压缩6层后优于直接跳过2层，且当结合FW算法时，可以将12层进一步蒸馏为1层，显著缩小了自回归生成与基于流生成范式的差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:41:05 GMT</pubDate>
</item>
<item>
<title>推理模型在置信度表达上的优越表现</title>
<link>https://arxiv.org/abs/2505.14489</link>
<guid>https://arxiv.org/abs/2505.14489</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示推理模型在链式思维中表现出更好的置信度校准能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在许多方面表现出色，但它们往往难以准确传达自身的置信度，这限制了其可靠性。本研究证明，采用扩展链式思维（CoT）推理的推理模型不仅在问题解决上表现优异，还在准确表达置信度方面更为出色。通过对比六种推理模型在六个数据集上的表现，我们发现它们在33种设置下相较于非推理模型具有更优的置信度校准能力。深入分析表明，这种改进源于推理模型的慢思考特性，例如探索替代方案和回溯等，这些特性使其在整个链式思维过程中动态调整置信度，从而逐步提高准确性。尤其值得注意的是，随着链式思维的展开，推理模型的置信度校准能力逐渐增强，而非推理模型则不具备这一趋势。此外，即使移除链式思维中的慢思考行为，也会显著降低模型的校准性能。最后，研究还发现，非推理模型在通过情境学习被引导进行慢思考时也能获得类似的收益。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14489" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:19:00 GMT</pubDate>
</item>
<item>
<title>基于推理数据蒸馏提升开源语言模型性能的研究</title>
<link>https://arxiv.org/abs/2505.14464</link>
<guid>https://arxiv.org/abs/2505.14464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过收集高质量推理数据蒸馏提升了开源语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对推理数据蒸馏展开大规模实证研究，从三个最先进的教师模型（AM-Thinking-v1、Qwen3-235B-A22B和DeepSeek-R1）中收集验证过的输出数据，构建了三个并行数据集，涵盖总计189万个查询。分析显示，AM-Thinking-v1蒸馏的数据具有更高的令牌长度多样性和更低的困惑度。基于这些数据训练的学生模型在AIME2024、AIME2025、MATH500和LiveCodeBench等推理基准测试中表现出色，其中AM-Thinking-v1模型在多个任务上取得了最佳性能，例如在AIME2024得分为84.3，在AIME2025得分为72.2，在MATH500得分为98.4，在LiveCodeBench得分为65.9。此外，该模型展示了适应性输出行为，即对较难任务生成较长响应，对简单任务生成较短响应。这些发现强调了高质量验证推理轨迹的重要性，并公开发布了AM-Thinking-v1和Qwen3-235B-A22B蒸馏的数据集，支持未来关于开放高性能推理导向型语言模型的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:00:51 GMT</pubDate>
</item>
<item>
<title>语言模型中的分词对符号推理能力的影响</title>
<link>https://arxiv.org/abs/2505.14178</link>
<guid>https://arxiv.org/abs/2505.14178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">分词结构显著影响语言模型的符号推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了分词方案，特别是基于子词的方法（如字节对编码 BPE），如何通过合并或模糊原子推理单元而阻碍符号计算。研究引入了“标记感知”概念，以形式化粒度较差的标记如何破坏逻辑对齐并阻止模型泛化符号过程。通过对算术和符号任务的系统评估发现，即使采用思维链提示，分词结构仍会严重影响推理表现，而原子对齐的格式则能够解锁强大的泛化能力，使小型模型在结构化推理任务中优于大型系统。研究结果表明，大型语言模型的符号推理能力不仅依赖于架构，还深深依赖于标记级表示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 06:32:30 GMT</pubDate>
</item>
<item>
<title>Hunyuan-Game：智能游戏创作的革新项目</title>
<link>https://arxiv.org/abs/2505.14135</link>
<guid>https://arxiv.org/abs/2505.14135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用人工智能生成高质量游戏内容，提升设计师效率。</p><br /><br /><p><strong>摘要：</strong> 智能游戏创作是游戏开发领域的一项革命性进步，通过生成式人工智能动态生成和优化游戏内容。尽管生成模型已取得显著进展，但高质量游戏资产（图像和视频）的综合合成仍具挑战性。为解决这一问题，我们推出了Hunyuan-Game项目，专注于图像和视频生成两大方向。图像生成基于数十亿游戏图像的数据集，开发了多种定制化模型，如文本到图像生成、视觉效果生成、透明图像生成及角色生成等。视频生成则依托数百万游戏和动漫视频数据集，开发了五大算法模型，涵盖图像到视频生成、人物视频合成、动态插图生成、超分辨率视频生成及交互式游戏视频生成。这些模型不仅具备高水平美学表达，还深度融合了特定领域的知识，系统理解了多样化的游戏和动漫艺术风格。Hunyuan-Game旨在大幅提升设计师的工作效率并满足玩家偏好。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:39:48 GMT</pubDate>
</item>
<item>
<title>推理路径压缩提升逻辑型大模型推理效率</title>
<link>https://arxiv.org/abs/2505.13866</link>
<guid>https://arxiv.org/abs/2505.13866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的推理加速方法RPC，显著提升逻辑型大模型的吞吐量。</p><br /><br /><p><strong>摘要：</strong> 近期基于推理路径的语言模型通过生成长序列中间推理过程提高准确性，但增加了内存消耗和计算复杂度。本文提出推理路径压缩（RPC），一种无需训练的方法，利用推理路径的语义稀疏性加速推理过程。RPC通过保留重要性得分高的键值对（KV缓存），显著提高了生成吞吐量，在AIME 2024基准测试中的准确率仅下降1.2%。实验表明，RPC可将QwQ-32B模型的生成吞吐量提升最多1.6倍，为高效部署逻辑型大语言模型提供了可行方案。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 23:21:52 GMT</pubDate>
</item>
<item>
<title>CompeteSMoE：一种高效的大规模混合专家模型训练机制</title>
<link>https://arxiv.org/abs/2505.13380</link>
<guid>https://arxiv.org/abs/2505.13380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出竞争机制提升稀疏混合专家模型的路由效率。</p><br /><br /><p><strong>摘要：</strong> 稀疏混合专家（SMoE）模型通过增加复杂度而非单纯扩大网络深度或宽度提供了新的可能性，但其训练挑战在于子优化的路由过程。本文提出了竞争机制，使计算专家直接参与路由决策，理论分析表明其样本效率优于传统softmax路由。我们开发了CompeteSMoE算法，通过学习竞争策略显著提升了大语言模型的训练效果，同时降低了训练开销。在视觉指令微调和语言预训练任务上的实验验证了该方法的有效性、鲁棒性和可扩展性，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:24:26 GMT</pubDate>
</item>
<item>
<title>跨语言切换对大语言模型的影响及评估基准CS-Sum</title>
<link>https://arxiv.org/abs/2505.13559</link>
<guid>https://arxiv.org/abs/2505.13559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入CS-Sum评估大语言模型处理跨语言对话总结的能力。</p><br /><br /><p><strong>摘要：</strong> 跨语言切换（Code-switching, CS）对大语言模型（LLMs）构成重大挑战，但其可理解性尚未被充分探索。本研究介绍了CS-Sum，这是一个用于评估LLMs通过跨语言对话到英语总结的可理解性的基准。CS-Sum涵盖了汉语-英语（EN-ZH）、泰米尔语-英语（EN-TA）和马来语-英语（EN-MS）三种语言对，每种语言对包含900至1300个人类注释的对话。通过对十种LLMs（包括开源和闭源模型）进行评估，我们分析了Few-shot、Translate-Summarize和Fine-tuning（LoRA、QLoRA基于合成数据）方法的表现。研究发现，尽管自动化指标得分较高，但LLMs在处理CS输入时会犯一些细微错误，这些错误可能会改变对话的整体含义。为此，我们总结了LLMs在处理CS输入时最常见的三种错误类型。错误率在不同的CS语言对和LLMs之间有所不同，某些LLMs在特定语言对上的错误频率更高，这凸显了对跨语言数据进行专门训练的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 05:18:14 GMT</pubDate>
</item>
<item>
<title>SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.12448</link>
<guid>https://arxiv.org/abs/2505.12448</guid>
<content:encoded><![CDATA[
Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.
]]></content:encoded>
<pubDate>Sun, 18 May 2025 10:40:16 GMT</pubDate>
</item>
<item>
<title>WikiDYK基准测试揭示因果语言模型的知识记忆弱点</title>
<link>https://arxiv.org/abs/2505.12306</link>
<guid>https://arxiv.org/abs/2505.12306</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现双向语言模型比因果语言模型具有更强的知识记忆能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）取得了显著进展，但它们的知识记忆能力仍未被充分探索，因为缺乏标准化且高质量的测试平台。本文引入了一个名为WikiDYK的新基准测试，它基于维基百科的“你知道吗？”条目，这些条目由专家编辑精心挑选，包含可验证性和清晰性等标准。WikiDYK包含12,290个事实和77,180个问题，并可以随着维基百科的更新无缝扩展。通过持续预训练实验，我们惊讶地发现，尽管因果语言模型（CLMs）在现代LLMs中很常见，但它们在可靠性方面的准确性比双向语言模型（BiLMs）低23%。为了弥补当前BiLM规模较小的问题，我们提出了一种模块化协作框架，利用BiLM集合作为外部知识库与LLMs集成，实验显示该框架将可靠性准确性提高了29.1%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12306" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 04:39:05 GMT</pubDate>
</item>
<item>
<title>语言模型中的真相神经元：机制解析与验证</title>
<link>https://arxiv.org/abs/2505.12182</link>
<guid>https://arxiv.org/abs/2505.12182</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型通过“真相神经元”编码真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，用于识别语言模型中神经元级别的真相表示。实验表明，多种规模的语言模型均包含不依赖具体主题的真相神经元，这些神经元的分布模式与先前关于真相几何学的研究结果一致。抑制通过TruthfulQA数据集发现的真相神经元激活会降低模型在TruthfulQA及其他基准测试上的表现，证明真相机制并非局限于特定数据集。本研究为理解语言模型中真相编码机制提供了新见解，并指出了提升模型可信度的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12182" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 20:47:21 GMT</pubDate>
</item>
<item>
<title>FlexiVe：一种灵活的大型语言模型推理验证方法</title>
<link>https://arxiv.org/abs/2505.11966</link>
<guid>https://arxiv.org/abs/2505.11966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FlexiVe，一种高效且可扩展的推理验证方法，提升大规模语言模型的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在复杂任务中大规模语言模型推理过程中解决精度与计算效率之间权衡的问题。传统验证步骤虽然旨在提高性能，但引入了新的挑战，即如何平衡高级生成奖励模型的可靠性和计算成本。为了解决这些问题，我们提出了FlexiVe，这是一种新的生成型验证器，通过灵活分配验证预算，在快速可靠的快速思考和细致的慢速思考之间实现平衡。此外，我们还设计了Solve-Detect-Verify管道，这是一个高效的推理时间扩展框架，通过智能集成FlexiVe，主动识别解决方案完成点来触发有针对性的验证并提供聚焦的求解器反馈。实验表明，FlexiVe在ProcessBench上显著提高了对推理轨迹中错误的定位能力。同时，在具有挑战性的数学推理基准测试中，我们的完整方法在推理准确性和效率方面超过了自一致性等基线方法。该系统为增强大规模语言模型的实时推理提供了可扩展且有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 07:41:44 GMT</pubDate>
</item>
<item>
<title>通过低比特注意力机制提升大规模模型训练效率</title>
<link>https://arxiv.org/abs/2505.11594</link>
<guid>https://arxiv.org/abs/2505.11594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于FP4和8位精度的注意力加速方法，显著提升模型训练与推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对注意力计算的时间复杂度问题，提出了两项改进措施。首先，利用Blackwell GPU中的新FP4张量核心加速推理阶段的注意力计算，在RTX5090上达到1038 TOPS，较FlashAttention提升了5倍。该方法能够无缝集成到多种模型中，实现高效推理。其次，我们开创性地将低比特注意力引入训练任务，设计了一种适用于前向和反向传播的8位注意力机制。实验表明，该方法在微调任务中表现无损，但在预训练任务中收敛速度较慢。本研究为大规模模型的高效训练提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 14:01:54 GMT</pubDate>
</item>
<item>
<title>AI对Z世代数字语言的理解评估：在线安全的新挑战</title>
<link>https://arxiv.org/abs/2505.10588</link>
<guid>https://arxiv.org/abs/2505.10588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了主流AI模型对Z世代数字语言中隐藏风险的检测能力。</p><br /><br /><p><strong>摘要：</strong> 本研究首次评估了AI系统如何解读Z世代（2010-2024年出生）的数字语言，揭示了现有安全工具在面对这一群体独特沟通方式时的局限性。通过分析四款领先AI模型（GPT-4、Claude、Gemini和Llama 3）的表现，研究发现这些系统难以有效识别Z世代交流中的潜在有害互动。研究基于100条来自游戏平台、社交媒体和视频内容的真实表达构建了一个全新数据集，并提出改进青少年保护的AI监管框架。此外，研究结合了AI系统、人类管理者及家长的多视角评价，并直接采纳了Z世代共同研究员的意见，强调了重新设计适配青少年语言的安全系统的紧迫性。这项工作为解决数字时代的关键安全挑战提供了新见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>MedCaseReasoning：评估大型语言模型临床诊断推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.11733</link>
<guid>https://arxiv.org/abs/2505.11733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个开放数据集MedCaseReasoning，用于评估大型语言模型在医疗诊断推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLMs）在医学诊断中的应用日益广泛，但现有评估标准主要关注最终答案准确性，忽视了临床推理过程的质量。为弥补这一不足，本研究引入MedCaseReasoning，这是一个包含14,489个病例的开放数据集，每个病例均附有详细的医生推理说明。通过测试现有最先进的LLMs，发现这些模型在诊断准确性和推理召回率上存在显著缺陷。例如，表现最好的开源模型DeepSeek-R1仅达到48%的诊断准确率和64%的推理召回率。然而，通过对模型进行基于MedCaseReasoning推理痕迹的微调，可以显著提升诊断准确率和推理召回率，分别提高29%和41%。该数据集及相关代码和模型已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 18:34:36 GMT</pubDate>
</item>
<item>
<title>通过镜像方法评估图像常识一致性</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法TLG，利用大型视觉语言模型评估图像常识一致性。</p><br /><br /><p><strong>摘要：</strong> 在人工智能研究中，衡量真实图像的真实性是一项复杂的任务。例如，一个男孩手持吸尘器站在沙漠中的图片违背了常识。本文介绍了一种名为Through the Looking Glass (TLG)的新方法，用于评估图像的常识一致性。TLG利用大型视觉语言模型(LVLMs)和基于Transformer的编码器从图像中提取原子事实，从而获得混合的精确事实。随后，通过在一个紧凑的注意力池化分类器上进行微调，该分类器对编码的原子事实进行处理，TLG在WHOOPS!和WEIRD数据集上的表现达到了新的最先进水平，同时采用了紧凑的微调组件。这一方法为图像真实性评估提供了创新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 12:12:11 GMT</pubDate>
</item>
<item>
<title>R3框架：提升奖励模型的可控性和可解释性</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的奖励建模框架R3，提升语言模型与人类偏好的对齐能力。</p><br /><br /><p><strong>摘要：</strong> 当前的奖励模型虽对齐语言模型输出与人类偏好至关重要，但通常缺乏可控性和可解释性。这些模型多针对狭窄目标优化，难以泛化到更广泛的下游任务，且其标量输出难以解读。为解决这些问题，我们引入R3框架，这是一种不依赖评分标准、跨评估维度通用且提供可解释评分分配的新方法。R3实现了语言模型评估的透明度和灵活性，支持与多样化的价值观和应用场景的稳健对齐。相关模型、数据和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:29:03 GMT</pubDate>
</item>
<item>
<title>低秩克隆方法大幅提升小语言模型训练效率</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的高效预训练方法LRC，大幅提升小语言模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有小语言模型训练中的三大挑战——硬剪枝导致的信息丢失、表示对齐效率低下及对FFN信号利用不足，提出了名为Low-Rank Clone (LRC) 的高效预训练方法。该方法通过训练一组低秩投影矩阵实现教师权重的软剪枝与学生激活信号的对齐，包括FFN信号，从而构建行为上等同于强教师模型的小语言模型。实验显示，LRC在仅使用20B令牌的情况下达到了超越基于万亿令牌训练的SOTA模型的效果，提升了超过1000倍的训练效率。研究代码和模型检查点已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12781" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 03:10:42 GMT</pubDate>
</item>
<item>
<title>SEED-GRPO：基于语义熵的大语言模型不确定性感知优化</title>
<link>https://arxiv.org/abs/2505.12346</link>
<guid>https://arxiv.org/abs/2505.12346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对大语言模型在不同输入提示下的不确定性问题，提出SEED-GRPO方法提升数学推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在处理不同输入提示时表现出不同程度的置信度，这反映了模型对特定问题的理解不确定性。传统方法如Group Relative Policy Optimization (GRPO) 忽略了这一重要信息。为解决此问题，本文提出了SEED-GRPO，通过引入语义熵显式衡量LLMs对输入提示的不确定性。语义熵评估给定提示下多个生成答案的语义多样性，并据此调节策略更新的幅度。这种不确定性感知机制使政策更新幅度根据问题的不确定性动态调整，在高不确定性问题上采取更保守的更新策略，同时保留对确定性问题的原始学习信号。实验结果显示，在五个数学推理基准测试中（AIME24 56.7，AMC 68.7，MATH 83.4，Minerva 34.2，OlympiadBench 48.0），SEED-GRPO在平均准确率上达到新的最先进水平，验证了不确定性感知策略优化的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 06:20:59 GMT</pubDate>
</item>
<item>
<title>HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology</title>
<link>https://arxiv.org/abs/2505.12120</link>
<guid>https://arxiv.org/abs/2505.12120</guid>
<content:encoded><![CDATA[
Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.
]]></content:encoded>
<pubDate>Sat, 17 May 2025 14:59:32 GMT</pubDate>
</item>
<item>
<title>HelpSteer3-Preference：高质量指令跟随语言模型偏好数据集</title>
<link>https://arxiv.org/abs/2505.11475</link>
<guid>https://arxiv.org/abs/2505.11475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布了一个包含4万多个样本的高质量偏好数据集，提升大语言模型训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HelpSteer3-Preference，这是一个高许可性(CC-BY-4.0)且高质量的人类注释偏好数据集，包含超过40,000个样本，涵盖了科学、技术、工程、数学(STEM)、编码及多语言场景等实际应用领域。该数据集通过训练奖励模型(RMs)，在RM-Bench上取得了82.4%的出色成绩，在JudgeBench上也达到了73.7%，较现有最佳结果提升了约10个百分点。此外，我们展示了如何利用此数据集训练生成型奖励模型，并进一步将策略模型通过RLHF方法进行对齐优化。该数据集现可公开获取，适用于多种强化学习应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:31:19 GMT</pubDate>
</item>
<item>
<title>基于多模态观察的一般用户模型</title>
<link>https://arxiv.org/abs/2505.10831</link>
<guid>https://arxiv.org/abs/2505.10831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种能从多模态交互中学习用户偏好的一般用户模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为一般用户模型（GUM）的新架构，该模型通过观察用户与计算机的任何互动来了解用户。GUM可以处理未经结构化的用户观察数据（如设备截图），并构建置信加权命题来捕捉用户的偏好和知识。实验表明，GUM不仅能够准确推断用户行为，还能通过增强聊天助手、管理操作系统通知和提供跨应用自适应代理等方式，实现人机交互的长期愿景。此外，还展示了GUM如何驱动主动助手（GUMBOs），自动为用户提供有用建议。评估结果显示，基于GUM的系统能有效识别并执行用户可能不会明确请求的操作。GUM引入的方法利用多模态模型理解非结构化上下文，推动了人机交互领域的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:31 GMT</pubDate>
</item>
<item>
<title>基于非配对数据学习的智能手机图像信号处理器优化</title>
<link>https://arxiv.org/abs/2505.10420</link>
<guid>https://arxiv.org/abs/2505.10420</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需配对数据的新型学习方法，提升手机ISP图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文解决现代智能手机相机ISP开发中的一个难题，即获取像素级对齐的配对训练数据的高成本问题。通过引入一种新颖的非配对训练方法，该方法利用多损失项对抗训练，并结合多个判别器处理预训练网络的特征图，从而在保持内容结构的同时学习目标RGB数据集的颜色和纹理特性。实验表明，使用轻量级神经网络架构，在Zurich RAW到RGB和Fujifilm UltraISP数据集上评估时，该方法相较于传统的配对训练方法表现出了更强的潜力，并在多项评价指标上实现了高保真度。相关代码和预训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10420" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 11:37:51 GMT</pubDate>
</item>
<item>
<title>VSA：一种高效的可训练稀疏注意力机制</title>
<link>https://arxiv.org/abs/2505.13389</link>
<guid>https://arxiv.org/abs/2505.13389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VSA稀疏注意力机制，显著降低视频扩散模型计算开销。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频扩散变压器（DiTs）因全3D注意力导致的计算瓶颈问题，提出了VSA（可训练硬件高效稀疏注意力）。VSA通过轻量级粗阶段对tokens进行池化并识别关键tokens，再由细粒度阶段仅在这些关键tokens所在块内计算注意力，从而实现硬件效率提升和高效训练。实验表明，VSA在不影响扩散损失的前提下将训练浮点运算减少2.53倍，且在Wan-2.1模型上的应用显著加快了推理速度，同时保持了生成质量。这一成果确立了可训练稀疏注意力作为全注意力的实用替代方案，为视频扩散模型的进一步扩展提供了支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:30:13 GMT</pubDate>
</item>
<item>
<title>LatentSeek：通过测试时实例级适应提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.13308</link>
<guid>https://arxiv.org/abs/2505.13308</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用潜在空间的框架LatentSeek，显著提升大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在实现通用人工智能（AGI）过程中面临的推理难题。尽管模型在训练规模上取得了进展，但在训练算法如灾难性遗忘及新颖训练数据获取方面仍存在挑战。为此，我们提出了LatentSeek，这是一种基于测试时实例级适应（TTIA）的新框架，在模型潜在空间中增强推理能力。具体而言，LatentSeek利用策略梯度迭代更新潜在表示，由自动生成的奖励信号引导。实验表明，无论是在GSM8K、MATH-500还是AIME2024等基准测试中，LatentSeek均优于基线方法，如思维链提示和微调方法。此外，LatentSeek具有高效性，在少数几次迭代后即可解决平均复杂度问题，同时还能从更多迭代中获益，展示了测试时潜在空间扩展的巨大潜力。这些发现使LatentSeek成为一种轻量级、可扩展且有效的LLMs推理能力增强解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13308" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 12:26:02 GMT</pubDate>
</item>
<item>
<title>OSWorld-G与Jedi：提升图形用户界面定位能力的新基准与数据集</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准OSWorld-G和大规模数据集Jedi，解决现有GUI定位任务过于简化的问题。</p><br /><br /><p><strong>摘要：</strong> 图形用户界面（GUI）定位技术是开发计算机代理的关键瓶颈，当前基准未能捕捉真实交互中的复杂性。为应对这一挑战，我们引入OSWorld-G，这是一个包含564个精心标注样本的综合基准，涵盖多种任务类型。同时，我们发布了Jedi，这是目前最大的计算机使用定位数据集，包含400万个示例。通过多视角分解任务，Jedi显著提升了现有模型的性能，在ScreenSpot-v2、ScreenSpot-Pro和OSWorld-G上表现优异。研究还表明，结合特定接口元素的数据可实现对新界面的组合泛化。所有资源均开源并提供访问链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 11:09:23 GMT</pubDate>
</item>
<item>
<title>ViPlan：视觉规划领域的首个开源基准测试</title>
<link>https://arxiv.org/abs/2505.13180</link>
<guid>https://arxiv.org/abs/2505.13180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比符号规划与直接使用视觉语言模型进行视觉规划的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ViPlan，这是首个用于视觉规划的开源基准测试，结合符号谓词和视觉语言模型(VLM)。ViPlan涵盖了两个领域中的挑战性任务：经典Blocksworld问题的视觉变体及模拟的家庭机器人环境。我们评估了多个开源VLM家族及其封闭模型，分别测试了基于VLM的符号规划方法与直接使用VLM提出动作的方法。结果显示，在需要精确图像定位的Blocksworld任务中，符号规划优于直接VLM规划；而在家庭机器人任务中，常识知识和错误恢复能力更为重要。此外，大多数模型和方法中，链式思维提示并未带来显著优势，表明当前VLM在视觉推理方面仍存在困难。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:38:15 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多语言机器翻译模型研究</title>
<link>https://arxiv.org/abs/2505.12996</link>
<guid>https://arxiv.org/abs/2505.12996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新奖励建模方法提升大模型在神经机器翻译中的性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）在复杂问题上展现了强大能力，但将其应用于神经机器翻译（MT）仍面临挑战。现有研究主要集中在高资源语言如英语和中文，且奖励建模方法未能充分发挥强化学习潜力。本研究设计了一种新的奖励建模方法，通过将翻译结果与强LRM对比量化评分提供奖励，显著提升了翻译质量。实验表明，使用Qwen2.5-7B-Instruct作为基础模型，在文学翻译中达到最新技术水平，并超越OpenAI-o1和DeepSeek-R1等强LRMs。此外，该方法扩展到11种语言的多语言设置，在轻量级奖励建模下实现90种翻译方向上的出色表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:34:47 GMT</pubDate>
</item>
<item>
<title>Fractured Sampling：提升大语言模型推理效率的新策略</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">截断CoT和Fractured Sampling方法显著提升了大语言模型推理的准确性和效率。</p><br /><br /><p><strong>摘要：</strong> 截断CoT（Chain-of-Thought prompting）方法在推理过程中提前终止生成中间推理路径，直接得出答案，相较于完整CoT，在大幅减少token消耗的同时保持了相似的准确性。基于这一发现，本文提出了一种名为Fractured Sampling的统一推理策略，该策略通过在三个正交维度上进行插值：推理轨迹数量、每条轨迹最终解的数量以及推理深度的截断位置，实现了更优的准确率-成本权衡。通过在五个多样化推理基准测试及多种模型规模下的广泛实验，证明了Fractured Sampling能够在Pass@k指标上获得显著的对数线性扩展收益。此外，研究还揭示了如何在这三个维度上分配计算资源以最大化性能，为大语言模型的高效可扩展推理奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:30:41 GMT</pubDate>
</item>
<item>
<title>解决同形异义词消歧挑战的半自动化方法及快速规则系统</title>
<link>https://arxiv.org/abs/2505.12973</link>
<guid>https://arxiv.org/abs/2505.12973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对低资源语言中的同形异义词问题，提出新方法提升深度学习和规则系统的消歧准确性。</p><br /><br /><p><strong>摘要：</strong> 同形异义词消歧在基于图示到音素转换（G2P）中是一个重大挑战，尤其对低资源语言而言。主要困难在于构建平衡且全面的数据集耗时费力，而特定策略又引入延迟，不适合实时应用。本文提出解决方案：首先，通过半自动化管道创建同形异义词数据集HomoRich，并将其应用于增强波斯语的先进深度学习G2P系统；其次，倡导利用丰富离线数据开发适用于低延迟无障碍工具（如屏幕阅读器）的快速规则方法，改进规则系统eSpeak为HomoFast eSpeak。实验结果显示，深度学习和eSpeak系统的同形异义词消歧准确率提高了约30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:11:12 GMT</pubDate>
</item>
<item>
<title>语言如何在合作中演化？基于多智能体觅食游戏的研究</title>
<link>https://arxiv.org/abs/2505.12872</link>
<guid>https://arxiv.org/abs/2505.12872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言如何通过合作需求在多智能体系统中自然演化。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人类语言如何从早期合作需求中演化而来。通过构建多智能体觅食游戏环境，模拟早期人类面临的认知和生态约束，我们发现智能体在协作过程中自发发展出具备自然语言特征的交流协议，如任意性、可互换性、位移性、文化传递性和构成性。实验表明，群体规模和时间依赖性等因素对语言特性有显著影响。我们的框架为研究语言在具身化多智能体系统中的演化提供了平台，并计划公开所有数据、代码和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 04:57:30 GMT</pubDate>
</item>
<item>
<title>通过结构化上下文条件化增强大语言模型在科学文档验证中的准确性</title>
<link>https://arxiv.org/abs/2505.12257</link>
<guid>https://arxiv.org/abs/2505.12257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示通过调整大语言模型的上下文条件可以提高其对科学文档中技术错误的检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种基于Persistent Workflow Prompting（PWP）原则的结构化大语言模型（LLM）上下文条件化方法，旨在解决LLM在复杂科学和技术文档验证中的局限性，特别是那些需要多模态解释的任务。实验集中在验证一篇包含已知文本和图像错误的化学测试论文上，结果显示基本提示策略效果不佳，但采用PWP结构调整LLM分析心态的方法显著提升了Gemini 2.5 Pro模型在文本错误检测上的表现，并使其成功识别出之前手动审查中忽略的图像公式错误，而ChatGPT Plus o3在类似测试中未能完成此任务。这些初步结果表明该方法有助于开发更可靠的LLM驱动分析流程，尤其适用于需要细致误差检测的场景。然而，未来仍需更大范围的验证来确认其普适性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 02:33:08 GMT</pubDate>
</item>
<item>
<title>大规模预训练中的模型合并技术研究</title>
<link>https://arxiv.org/abs/2505.12082</link>
<guid>https://arxiv.org/abs/2505.12082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示模型合并可显著提升大语言模型性能并降低训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大规模预训练过程中模型合并技术的应用，通过密集型及Mixture-of-Experts(MoE)架构的实验表明，采用恒定学习率训练的检查点合并不仅提升了模型性能，还预测了退火行为。这些改进提高了模型开发效率并降低了训练成本。此外，对合并策略和超参数的消融研究揭示了潜在机制并发现新应用，为开源社区提供了实用的预训练指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 12:53:14 GMT</pubDate>
</item>
<item>
<title>Tiny QA Benchmark++：轻量级多语言模型安全测试套件</title>
<link>https://arxiv.org/abs/2505.12058</link>
<guid>https://arxiv.org/abs/2505.12058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TQB++提供轻量级多语言测试套件，快速检测大语言模型的安全性问题。</p><br /><br /><p><strong>摘要：</strong> Tiny QA Benchmark++（TQB++）是一个超轻量级、多语言的测试套件，旨在为大型语言模型（LLM）的流水线提供类似单元测试的安全保障数据集。它由Comet Opik提示优化SDK的开发需求催生，后者对快速反馈循环有极高要求，而传统重型基准测试会破坏开发流程。TQB++包含一个52项的英语黄金数据集（小于20kB），并结合了一个基于供应商不可知的LiteLLM构建的小型合成数据生成器PyPI包。该生成器允许用户在任何语言、领域或难度下创建自己的微型数据包，同时已准备好十个涵盖阿拉伯语、中文、法语等十种语言的现成数据包。每个数据集都带有Croissant元数据和即插即用文件，适用于OpenAI-Evals、LangChain和标准CI工具，使得团队可以直接将确定性的微基准测试集成到拉取请求门禁、提示工程循环和生产仪表板中，而无需占用GPU预算。完整运行TQB++仅需几秒钟，但可以提前检测提示模板错误、标记器漂移和微调副作用，比像MMLU或BIG-Bench这样的全规模套件更快发现问题。整个框架被开源，以加速生成式人工智能生态系统的持续、资源高效的质量保证。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 11:40:03 GMT</pubDate>
</item>
<item>
<title>基于RAG框架的领域特定恶意技术识别方法</title>
<link>https://arxiv.org/abs/2505.11988</link>
<guid>https://arxiv.org/abs/2505.11988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合检索增强生成的恶意技术识别新框架。</p><br /><br /><p><strong>摘要：</strong> 现有的恶意技术识别方法面临模型精度与资源消耗之间的权衡问题，本文提出了一种名为TechniqueRAG的领域特定检索增强生成框架，通过整合现成的检索器、指令微调的大语言模型及少量文本-技术对，解决了数据稀缺问题，同时通过零样本LLM重排序提升检索质量与领域特定精度。实验表明，该方法在多个安全基准测试中达到最先进的性能，且无需大量任务特定优化或标记数据。这项研究为网络防御提供了高效的技术支持，同时揭示了进一步改进的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 08:46:10 GMT</pubDate>
</item>
<item>
<title>大型语言模型在学术验证中的局限性研究</title>
<link>https://arxiv.org/abs/2505.11855</link>
<guid>https://arxiv.org/abs/2505.11855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示当前大型语言模型在学术验证中的表现远未达到可靠水平。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）的进步激发了自动化科学发现的愿景，但以往的研究多将其视为生成假设或撰写论文的合著者。本研究探索了一种互补的应用场景：将LLMs作为学术验证的核查工具。为此，我们构建了一个名为SPOT的数据集，该数据集包含83篇已发表论文及其91处足以引发勘误或撤稿的重要错误，并通过作者和人工标注进行交叉验证。评估结果显示，最先进的LLMs在召回率和精确度上分别仅为21.1%和6.1%，且置信度普遍较低，多次运行间难以重现相同错误，表明其可靠性不足。此外，领域专家的定性分析显示，即使是最强的模型也会犯类似于学生水平的误解错误。这些发现揭示了当前LLM能力与可靠学术验证需求之间存在巨大差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 01:45:16 GMT</pubDate>
</item>
<item>
<title>QVGen：面向极低比特量化视频扩散模型的高效推理框架</title>
<link>https://arxiv.org/abs/2505.11497</link>
<guid>https://arxiv.org/abs/2505.11497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QVGen框架，显著提升低比特量化视频扩散模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 视频扩散模型在高质量视频合成方面表现出色，但其高昂的计算与内存需求限制了实际应用。尽管图像扩散模型通过量化技术有效降低了成本，但直接应用于视频扩散模型效果不佳。本文提出QVGen，一种针对极低比特量化（如4比特及以下）的量化感知训练框架，旨在提高视频扩散模型的高性能与推理效率。通过理论分析证明梯度范数减小对量化感知训练至关重要，并引入辅助模块缓解大量化误差，同时采用秩衰减策略逐步消除这些模块的推理开销。实验表明，在四种最先进的视频扩散模型上，QVGen首次实现了4比特设置下接近全精度的质量，并在多个性能指标上超越现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.11484</link>
<guid>https://arxiv.org/abs/2505.11484</guid>
<content:encoded><![CDATA[
Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:47:50 GMT</pubDate>
</item>
<item>
<title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
<link>https://arxiv.org/abs/2505.10238</link>
<guid>https://arxiv.org/abs/2505.10238</guid>
<content:encoded><![CDATA[
Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 08:50:29 GMT</pubDate>
</item>
<item>
<title>基于Persistent Workflow Prompting的科学手稿评审方法研究</title>
<link>https://arxiv.org/abs/2505.03332</link>
<guid>https://arxiv.org/abs/2505.03332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过Prompt工程提升大语言模型科学评审能力的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Persistent Workflow Prompting（PWP）的方法，旨在利用标准的大语言模型聊天接口解决科学手稿评审中的复杂性和数据限制问题。PWP通过分层模块化架构定义详细的分析工作流，采用迭代元提示和元推理技术系统化编码专家评审流程，包括隐性知识。实验表明，PWP引导下的大语言模型能够识别实验化学论文中的主要方法学缺陷，并有效执行复杂任务如区分主张与证据、整合文本/图片/图表分析、执行定量可行性检查等。该方法不仅展示了大语言模型在复杂科学任务中的潜力，还提供了透明且可复制的研究资源。除了具体应用外，这项工作还揭示了元开发过程本身的见解，强调了通过详细工作流形式化实现复杂科学任务分析的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:06:18 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型在图表理解中的视觉推理能力挑战</title>
<link>https://arxiv.org/abs/2505.13444</link>
<guid>https://arxiv.org/abs/2505.13444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型视觉语言模型在复杂图表理解中的视觉推理短板。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型视觉语言模型（LVLMs）在图表理解中的独特挑战，即需要整合复杂的文本和视觉推理能力。通过合成数据集的研究表明，模型性能在视觉复杂度增加时显著下降，而人类表现依然稳健。为此，我们提出了ChartMuseum，这是一个新的图表问答基准，包含来自184个真实来源的1,162个专家标注的问题，旨在评估复杂的视觉和文本推理能力。与现有基准不同，我们的基准显示出模型与人类表现之间存在显著差距，同时有效区分了模型的能力。尽管人类达到了93%的准确率，但最佳模型Gemini-2.5-Pro仅达到63.0%，而开源模型Qwen2.5-VL-72B-Instruct仅为38.5%。此外，在主要依赖视觉推理的问题上，所有模型的表现比文本推理问题下降了35%-55%。最后，我们的定性误差分析揭示了当前LVLMs在特定视觉推理类别上的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>FinePhys：结合物理学的人类动作生成框架</title>
<link>https://arxiv.org/abs/2505.13437</link>
<guid>https://arxiv.org/abs/2505.13437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合物理学的人类动作生成框架，显著提高复杂人体动作的生成质量。</p><br /><br /><p><strong>摘要：</strong> 尽管视频生成技术取得了显著进步，但生成物理上合理的精细人类动作仍是一个挑战。本文介绍FinePhys框架，它通过引入物理学原理提供骨骼指导，解决现有方法在处理精细语义和复杂时间动态方面的不足。FinePhys首先在线估计2D姿态，然后利用上下文学习进行2D到3D维度提升，并通过基于欧拉-拉格朗日方程的运动重估模块增强稳定性。该框架在FineGym数据集的三个子集上表现优异，生成的动作更加自然且符合物理规律。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:58:11 GMT</pubDate>
</item>
<item>
<title>通过过程奖励模型提升多模态推理逻辑一致性</title>
<link>https://arxiv.org/abs/2505.13427</link>
<guid>https://arxiv.org/abs/2505.13427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MM-PRM模型解决多模态大语言模型推理不一致问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大型语言模型在复杂多步推理中的局限性，即缺乏对中间推理步骤的精细监督。为了解决这一问题，研究团队提出了MM-PRM（Process Reward Model），该模型在一个完全自动化且可扩展的框架内进行训练。首先，构建了MM-Policy，这是一个在多样化数学推理数据上训练的强大多模态模型；接着，创建了MM-K12数据集，包含10,000个具有可验证答案的多模态数学问题作为种子数据。利用基于蒙特卡洛树搜索（MCTS）的管道，自动生成超过70万条步骤级标注，无需人工标记。最终的PRM用于最佳N推断设置中评分候选推理路径，在域内（如MM-K12测试集）和域外基准（如OlympiadBench、MathVista等）均取得了显著改进。进一步分析表明，软标签、较小的学习率和路径多样性对优化PRM性能至关重要。MM-PRM证明了过程监督是增强多模态推理系统逻辑鲁棒性的强大工具，并开源了所有代码和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>AdaptThink：一种基于强化学习的自适应推理模式优化算法</title>
<link>https://arxiv.org/abs/2505.13417</link>
<guid>https://arxiv.org/abs/2505.13417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AdaptThink算法，通过自适应选择推理模式提升模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型推理模型在多种任务中的表现，发现NoThinking（跳过推理直接输出答案）在简单任务中更具性能与效率优势。受此启发，我们提出了AdaptThink，一种新的强化学习算法，使推理模型能够根据问题难度自适应选择最佳推理模式。该算法包含两个核心组件：约束优化目标与重要性采样策略，实验表明其显著降低了推理成本并提升了性能，在三个数学数据集上将DeepSeek-R1-D1-Qwen-1.5B模型的平均响应长度减少53%，同时提高了2.4%的准确性。这些成果展示了自适应推理模式选择在平衡推理质量和效率方面的潜力。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:50:52 GMT</pubDate>
</item>
<item>
<title>Thinkless：让语言模型学会何时需要推理</title>
<link>https://arxiv.org/abs/2505.13379</link>
<guid>https://arxiv.org/abs/2505.13379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Thinkless框架，使LLMs根据任务复杂度自适应选择推理方式。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了具备扩展链式推理能力的语言模型在处理所有查询时可能面临计算效率低下的问题，特别是当许多问题可以简单解决时。为了解决这一问题，我们提出了Thinkless，这是一个可学习的框架，允许语言模型根据任务复杂性和自身能力自适应地选择简短形式或长链推理形式。Thinkless通过强化学习进行训练，使用两个控制令牌分别用于简洁响应和详细推理。核心算法Decoupled Group Relative Policy Optimization (DeGRPO) 将混合推理的学习目标分解为两个部分：控制令牌损失和响应损失。这种方法实现了对每个目标贡献的精细控制，稳定了训练过程并有效防止了传统GRPO中的崩溃现象。实证结果显示，在Minerva Algebra、MATH-500和GSM8K等基准测试中，Thinkless将长链推理的使用减少了50%-90%，显著提高了推理语言模型的效率。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:24:16 GMT</pubDate>
</item>
<item>
<title>混合3D-4D高斯泼溅技术提升动态场景重建效率</title>
<link>https://arxiv.org/abs/2505.13215</link>
<guid>https://arxiv.org/abs/2505.13215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种混合方法，在静态区域使用3D高斯模型提高效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为混合3D-4D高斯泼溅(3D-4DGS)的新框架，用于动态场景重建。传统4D高斯泼溅方法因对静态区域分配过多高斯函数导致计算和内存开销大且图像质量下降。3D-4DGS框架通过将时间不变的高斯函数转换为3D模型，减少参数数量并提高效率，同时保留动态元素的4D表示以捕捉复杂运动。实验表明，该方法在训练速度上显著快于基准方法，且视觉质量不降低甚至有所提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:59:58 GMT</pubDate>
</item>
<item>
<title>通过GS-Jacobi优化加速TarFlow图像生成模型采样</title>
<link>https://arxiv.org/abs/2505.12849</link>
<guid>https://arxiv.org/abs/2505.12849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法大幅加速TarFlow图像生成模型采样，同时保持生成质量。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于Transformer的图像生成模型如TarFlow在多个基准测试中取得了最先进的成果。然而，由于其因果注意力机制需要顺序计算，导致采样过程极其缓慢。本文提出了一种基于Gauss-Seidel-Jacobi迭代的新方法，通过引入收敛排名度量（CRM）和初始猜测度量（IGM），有效区分不同块的重要性及对初始值的敏感性。实验表明，在Img128cond、AFHQ、Img64uncond和Img64cond四个模型上，该方法分别实现了4.53倍、5.32倍、2.96倍和2.51倍的采样加速，且FID分数和样本质量未受影响。代码和检查点已开源。关键词：图像生成、TarFlow、GS-Jacobi。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 04:35:44 GMT</pubDate>
</item>
<item>
<title>FedSVD：通过SVD实现联邦学习中高效的低秩适配</title>
<link>https://arxiv.org/abs/2505.12805</link>
<guid>https://arxiv.org/abs/2505.12805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FedSVD通过全局重新参数化解决了LoRA在差分隐私下的噪声放大问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在联邦学习中低秩适配（LoRA）方法的高效微调问题。然而，当LoRA与差分隐私随机梯度下降（DP-SGD）结合时，会面临显著的噪声放大挑战。为了解决这一问题，我们提出了FedSVD，这是一种基于奇异值分解（SVD）的简单而有效的全局重新参数化方法。在FedSVD中，客户端仅优化B矩阵并将结果发送到服务器，服务器通过SVD重新因子化得到新的自适应A矩阵和更新后的B矩阵。该方法避免了二次噪声放大，同时增强了模型表达能力。理论分析表明，这种正交结构可以限制B的梯度范数并保留更多信号。实验结果显示，FedSVD在各种隐私设置和基准测试中表现出色，在私有和非私有环境下均优于相关基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 03:32:56 GMT</pubDate>
</item>
<item>
<title>Clipped Policy Gradient Optimization with Policy Drift (CPGD) 提升语言模型强化学习稳定性</title>
<link>https://arxiv.org/abs/2505.12504</link>
<guid>https://arxiv.org/abs/2505.12504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法CPGD，解决基于规则的强化学习训练不稳定性问题。</p><br /><br /><p><strong>摘要：</strong> 近期基于规则的强化学习方法显著提升了语言模型的推理能力，但现有方法如GRPO、REINFORCE++和RLOO常因训练不稳定而失败。本文提出了一种名为Clipped Policy Gradient Optimization with Policy Drift (CPGD)的新算法，通过引入基于KL散度的策略漂移约束和对数比率的裁剪机制，动态正则化策略更新，防止过度策略更新，从而解决训练崩溃问题。理论分析表明，CPGD不仅提高了性能，还保持了训练稳定性。实验验证了该方法的有效性，同时代码已开源，为后处理阶段的语言模型强化学习提供了稳健的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 13:44:53 GMT</pubDate>
</item>
<item>
<title>VisionReasoner：统一视觉推理框架在多任务感知中的卓越表现</title>
<link>https://arxiv.org/abs/2505.12081</link>
<guid>https://arxiv.org/abs/2505.12081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisionReasoner通过多对象认知学习和任务重铸，在多个视觉任务中表现出色。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为VisionReasoner的统一框架，该框架能够在一个共享模型中进行推理并解决多种视觉感知任务。通过设计新颖的多对象认知学习策略和系统化的任务重构方法，VisionReasoner增强了对视觉输入的分析能力，并在检测、分割和计数三个关键领域内的十个多样化任务上进行了评估。实验结果显示，相比Qwen2.5VL，VisionReasoner在COCO检测任务上提升了29.1%，在ReasonSeg分割任务上提升了22.1%，在CountBench计数任务上提升了15.3%，展现了其作为统一模型的强大性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 12:51:47 GMT</pubDate>
</item>
<item>
<title>QCompiler：通过神经符号框架提升检索增强生成系统的复杂查询处理能力</title>
<link>https://arxiv.org/abs/2505.11932</link>
<guid>https://arxiv.org/abs/2505.11932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QCompiler框架，有效解决资源受限下复杂嵌套查询的检索增强生成难题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为QCompiler的神经符号框架，旨在改进检索增强生成（RAG）系统对复杂查询意图的精确识别。传统的RAG系统在处理复杂查询时面临挑战，尤其是在资源有限的情况下。QCompiler基于Backus-Naur形式（BNF）语法设计了一种最小且充分的形式化规则，能够完整表达复杂查询同时减少冗余。该框架包含查询表达翻译器、词法语法解析器和递归下降处理器，可将查询编译为抽象语法树（AST），从而实现更精确的文档检索和响应生成。实验表明，QCompiler显著提升了RAG系统应对复杂查询的能力，特别是在资源受限场景下的性能表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 05:36:03 GMT</pubDate>
</item>
<item>
<title>AdaCoT：通过自适应推理提升大型语言模型效率</title>
<link>https://arxiv.org/abs/2505.11896</link>
<guid>https://arxiv.org/abs/2505.11896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaCoT框架通过自适应决定是否采用Chain-of-Thought提示，显著降低复杂推理任务的计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）虽然展现了卓越的能力，但在需要复杂推理的任务上常面临挑战。尽管Chain-of-Thought（CoT）提示大幅提升了推理能力，但它对所有查询都生成冗长推理步骤，导致计算成本高且效率低下，尤其是对于简单输入。为解决这一问题，我们提出了AdaCoT（自适应Chain-of-Thought），这是一种新框架，使LLMs能够根据需要灵活决定是否调用CoT。我们将自适应推理建模为帕累托优化问题，旨在平衡模型性能与CoT调用相关的成本（频率和计算开销）。我们提出了一种基于强化学习的方法，具体使用Proximal Policy Optimization（PPO），通过调整惩罚系数动态控制CoT触发决策边界，从而使模型能够根据隐式的查询复杂性判断是否需要CoT。技术贡献之一是Selective Loss Masking（SLM），用于对抗多阶段强化学习训练中的决策边界坍塌，确保自适应触发的鲁棒性和稳定性。实验结果显示，AdaCoT成功导航帕累托前沿，在不需要复杂推理的查询中显著减少了CoT的使用。例如，在生产流量测试集中，AdaCoT将CoT触发率降至仅3.18%，平均响应令牌数减少69.06%，同时在复杂任务上保持高性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 04:27:00 GMT</pubDate>
</item>
<item>
<title>Chain-of-Model Learning for Language Model</title>
<link>https://arxiv.org/abs/2505.11820</link>
<guid>https://arxiv.org/abs/2505.11820</guid>
<content:encoded><![CDATA[
In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.
]]></content:encoded>
<pubDate>Sat, 17 May 2025 00:06:12 GMT</pubDate>
</item>
<item>
<title>通过校正分布偏移提升Transformer稀疏注意力性能</title>
<link>https://arxiv.org/abs/2505.11254</link>
<guid>https://arxiv.org/abs/2505.11254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法纠正稀疏注意力的分布偏移，显著提升性能且保持高效率。</p><br /><br /><p><strong>摘要：</strong> Transformer模型中的注意力机制具有二次复杂度，在长序列推理时会导致高昂的成本和延迟。尽管可以通过稀疏计算降低计算负担，但通常会带来性能下降的问题。我们发现这种性能下降的原因在于稀疏计算导致注意力输出的分布发生偏移，从而影响解码查询与prefill阶段关键键值的对齐效果。为此，我们提出了一种简单而有效的方法来校正这一分布偏移，使稀疏注意力输出的分布更接近全量二次注意力的分布。该方法可与任意稀疏注意力方法结合使用，在基于滑动窗口注意力及sink token的应用中，平均提升了36个百分点的性能，恢复了88%的全量二次注意力精度。同时，我们的方法维持了约98.5%的全量二次注意力稀疏性，在处理100万token预填充时比Flash Attention 2快32倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 09:48:33 GMT</pubDate>
</item>
<item>
<title>基于自然语言指令的图像编辑模型评估基准GIE-Bench</title>
<link>https://arxiv.org/abs/2505.11493</link>
<guid>https://arxiv.org/abs/2505.11493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的基准GIE-Bench，用于评估文本引导图像编辑模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GIE-Bench的新基准，旨在通过功能正确性和内容保存两个关键维度更精确地评估文本引导的图像编辑模型性能。功能正确性通过自动生成的选择题验证目标修改是否成功应用；内容保存则利用对象感知掩码技术确保非目标区域视觉一致性。该基准包含超过1000个高质量编辑示例，涵盖20个多样化内容类别。研究对比了最新旗舰模型GPT-Image-1与其他先进编辑模型，发现GPT-Image-1在指令遵循准确性上表现最佳，但常过度修改无关区域，揭示了当前模型行为的重要权衡。GIE-Bench提供了一个可扩展且可重复的框架，推动文本引导图像编辑更准确的评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:55:54 GMT</pubDate>
</item>
<item>
<title>uLLSAM：利用多模态大语言模型提升显微镜跨域图像分割性能</title>
<link>https://arxiv.org/abs/2505.10769</link>
<guid>https://arxiv.org/abs/2505.10769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入视觉-语言知识，uLLSAM显著提升了显微镜图像的分割性能。</p><br /><br /><p><strong>摘要：</strong> 准确分割生物医学图像中的感兴趣区域具有重要意义。然而，现有基础模型在未见过的领域数据上表现不佳，主要因为缺乏分割前的视觉-语言知识。受多模态大语言模型的理解和推理能力启发，我们提出了一种名为uLLSAM的新方法，该方法通过视觉-语言语义对齐模块将视觉-语言知识注入到Segment Anything Model（SAM）中，并进一步通过语义边界正则化模块优化边界轮廓感知。实验表明，uLLSAM在9个领域内显微镜数据集上的Dice和SA指标分别提高了7.71%和12.10%，并在10个跨领域数据集上分别提高了6.79%和10.08%，达到了最先进的性能。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 20:55:56 GMT</pubDate>
</item>
<item>
<title>结合文本与图像结构引导的文生图模型</title>
<link>https://arxiv.org/abs/2505.05678</link>
<guid>https://arxiv.org/abs/2505.05678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合LLM实例级指令和图像结构初始化的文生图技术。</p><br /><br /><p><strong>摘要：</strong> 尽管生成模型的能力迅速提升，但现有的文本到图像模型在处理复杂提示时仍难以准确捕捉语义。为解决这一问题，研究者开始尝试引入粗略边界框等结构性约束来指导生成过程。本文进一步推进这一思路，利用当代图像生成模型直接提供精细的结构性初始化，并结合基于大型语言模型的实例级指令，从而生成符合文本提示中对象数量、实例级属性及实例间空间关系的图像。这种方法显著提高了生成图像对复杂提示的适应性，为文生图技术提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 18:31:23 GMT</pubDate>
</item>
<item>
<title>大型语言模型在汇编代码优化中的强化学习应用</title>
<link>https://arxiv.org/abs/2505.11480</link>
<guid>https://arxiv.org/abs/2505.11480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，大型语言模型通过强化学习可显著提升汇编代码性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在优化汇编代码性能方面的潜力。通过引入基于Proximal Policy Optimization（PPO）的强化学习框架，并结合功能正确性和执行性能的奖励函数，我们开发了Qwen2.5-Coder-7B-PPO模型。该模型在由8,072个真实世界程序组成的基准测试中表现出色，测试通过率达96.0%，平均速度比gcc -O3基线快1.47倍，优于其他20个评估模型。实验结果表明，强化学习可以释放LLMs在汇编代码优化中的巨大潜能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11480" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:40:45 GMT</pubDate>
</item>
<item>
<title>结合先验知识的实时优化风格迁移方法提升音频效果转换</title>
<link>https://arxiv.org/abs/2505.11315</link>
<guid>https://arxiv.org/abs/2505.11315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入高斯先验改进风格迁移模型，显著提升音频效果转换质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种结合先验知识的风格迁移方法（ST-ITO），通过在参数空间引入基于DiffVox数据集的高斯先验，优化风格嵌入空间中的距离，从而改善音频效果转换的逼真度与可靠性。实验表明，该方法在MedleyDB数据集上的表现优于现有基线，特别是在均方误差减少及参考风格匹配方面取得了显著进步。此外，主观评估进一步验证了新方法在有限数据条件下的优越性。本研究强调了在推理阶段引入先验知识对提高音频处理系统性能的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 10:40:31 GMT</pubDate>
</item>
<item>
<title>CheXGenBench：医学影像生成模型的综合评估框架</title>
<link>https://arxiv.org/abs/2505.10496</link>
<guid>https://arxiv.org/abs/2505.10496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种综合评估医学影像生成模型的新框架CheXGenBench。</p><br /><br /><p><strong>摘要：</strong> CheXGenBench是一种针对合成胸部X光图像生成模型的全面评估框架，它同时考量了模型的真实性、隐私风险和临床实用性。传统医学领域的AI评估存在方法不一致、架构比较过时及评估标准脱节等问题，忽视了样本的实际临床价值。CheXGenBench通过标准化的数据划分和统一的评估协议解决了这些问题，该协议包含了超过20个定量指标，系统分析了11种领先文本到图像生成架构的质量、潜在隐私漏洞及下游临床适用性。研究揭示了现有评估协议中的不足，特别是在生成真实性的评估上。此框架为医学AI社区提供了一个标准化基准，促进了客观可重复的比较，并支持未来模型的无缝集成。此外，还发布了SynthCheX-75K数据集，包含由表现最佳模型生成的75,000张高质量合成X光片。CheXGenBench建立了新的行业标杆，并公开了相关资源和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:59:17 GMT</pubDate>
</item>
<item>
<title>一种结合Logits与采样策略的大语言模型鲁棒水印框架</title>
<link>https://arxiv.org/abs/2505.09924</link>
<guid>https://arxiv.org/abs/2505.09924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种融合两类主流水印方案的新框架，显著提升检测与安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）文本滥用问题，整合logits-based和sampling-based水印技术，设计了一种新颖的共生水印框架，包含串行、并行及混合三种策略。该框架通过自适应嵌入机制优化检测能力、鲁棒性、文本质量和安全性之间的平衡。研究团队通过广泛实验验证了方法的有效性，在多个数据集和模型上取得了最先进的性能表现。这项工作为未来水印技术发展提供了新视角，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 23:12:36 GMT</pubDate>
</item>
<item>
<title>Mergenetic：用于语言模型的开源进化模型合并库</title>
<link>https://arxiv.org/abs/2505.11427</link>
<guid>https://arxiv.org/abs/2505.11427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mergenetic结合进化算法提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 模型合并是一种无需额外训练即可将现有模型能力整合到新模型中的方法，因其低成本和对消费者GPU的支持而受到欢迎。最新研究表明，将模型合并与进化算法结合可进一步提高性能，但目前尚无框架支持灵活实验。本文介绍了一个名为Mergenetic的开源库，该库专门针对语言模型设计，支持轻松组合不同的合并方法和进化算法，并引入轻量级适应度评估器以降低评估成本。通过描述其设计并展示在多种任务和语言上的竞争力，Mergenetic证明了其在有限硬件条件下仍能取得良好效果的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 12:43:23 GMT</pubDate>
</item>
<item>
<title>视觉规划：基于视觉表示的推理新范式</title>
<link>https://arxiv.org/abs/2505.11409</link>
<guid>https://arxiv.org/abs/2505.11409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的视觉规划范式，通过纯视觉表示进行推理。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLMs）及其多模态扩展（MLLMs）显著提升了跨任务的机器推理能力，但这些模型主要依赖纯文本表达推理，即便在存在视觉信息的情况下亦如此。本文指出语言并非在所有情况下最自然或有效的推理模式，特别是在涉及空间和几何信息的任务中。因此，我们提出了视觉规划这一新范式，它通过纯粹的视觉表示执行规划，无需依赖文本。在这一范式下，规划通过图像序列完成，每一步推理均在视觉领域编码，类似于人类的草图或可视化未来行动的方式。我们引入了视觉规划强化学习框架（VPRL），借助GRPO对大规模视觉模型进行后训练，显著提升了在代表性视觉导航任务（如FrozenLake、Maze和MiniBehavior）中的规划性能。实验结果显示，视觉规划在所有其他仅限文本空间的推理变体中表现最优，确立了其作为语言推理替代方案的可行性和潜力，为直观的图像推理任务开辟了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 12:17:22 GMT</pubDate>
</item>
<item>
<title>解决类别和空间不平衡问题的密集手部接触估计框架</title>
<link>https://arxiv.org/abs/2505.11152</link>
<guid>https://arxiv.org/abs/2505.11152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架解决手部接触估计中的类别和空间不平衡问题。</p><br /><br /><p><strong>摘要：</strong> 手部与世界的交互对于人类至关重要，但有效学习密集手部接触估计仍面临挑战。主要问题包括类别不平衡（大部分样本无接触）和空间不平衡（接触多集中于指尖）。为解决这些问题，本文提出了一种名为HACO的新框架，通过平衡接触采样和顶点级类别均衡损失函数，有效预测大规模手部接触数据的密集接触估计，且不受到类别和空间不平衡的影响。该方法不仅提升了模型性能，还为未来研究提供了宝贵资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 07:54:25 GMT</pubDate>
</item>
<item>
<title>大语言模型推理能力增强研究：开放领域问答中的测试时扩展</title>
<link>https://arxiv.org/abs/2505.11140</link>
<guid>https://arxiv.org/abs/2505.11140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明增加推理链长度可提高大语言模型在开放领域问答中的事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究通过分析复杂开放领域问答任务中的大语言模型推理能力，发现较短推理链的小型模型在经过微调后，其事实准确性显著优于原始指令调优模型。此外，引入知识图谱路径作为补充信息进一步增强了推理痕迹的丰富性。实验设置涵盖了六组基准数据集，超过22.6K个问题，并进行了168次实验，分析约170万条推理痕迹。结果表明，在单次运行中，通过添加测试时计算资源和令牌预算，事实准确性普遍提升2-8%，验证了测试时扩展的有效性。本研究公开所有实验材料供后续研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11140" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 07:39:33 GMT</pubDate>
</item>
<item>
<title>Group Think：基于单一大语言模型的并发推理新范式</title>
<link>https://arxiv.org/abs/2505.11107</link>
<guid>https://arxiv.org/abs/2505.11107</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型并发推理方法，使单个大语言模型能同时扮演多个推理代理，提升效率并降低延迟。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在推理能力方面取得了显著进步，但传统的多推理代理通常采用轮流交互的方式，这虽然提高了推理质量，却带来了较高的延迟。本文介绍了一种名为Group Think的新方法，它通过让单一LLM充当多个并发推理代理，在保持高质量的同时大幅降低了延迟。Group Think中的推理代理共享彼此的部分生成进度，从而能够在令牌级别动态适应对方，减少冗余推理。此外，这种并发机制能够更好地利用计算资源，特别适用于边缘推理场景。我们还提供了一个简单的修改方案，使现有的LLM能够在本地GPU上执行Group Think，并设计了一种评估策略，证明了这种方法在开源LLM上的延迟改进效果。这项研究为未来LLMs实现更复杂、更高效的协作行为奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11107" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 06:40:35 GMT</pubDate>
</item>
<item>
<title>GuardReasoner-VL：基于推理的视觉语言模型安全增强方法</title>
<link>https://arxiv.org/abs/2505.11049</link>
<guid>https://arxiv.org/abs/2505.11049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于在线强化学习的视觉语言模型安全增强方法GuardReasoner-VL。</p><br /><br /><p><strong>摘要：</strong> 为了提高视觉语言模型(VLM)的安全性，本文提出了一种名为GuardReasoner-VL的新颖推理型VLM保护模型。该模型通过在线强化学习激励保护模型在做出审核决策前进行深思熟虑的推理。首先构建了一个包含12.3万个样本和63.1万个推理步骤的GuardReasoner-VLTrain推理语料库，涵盖文本、图像及图文输入。接着通过SFT冷启动模型的推理能力。此外，通过在线强化学习进一步增强审核中的推理。具体而言，在拒绝采样后，利用所提出的安全性感知数据连接进行数据增强，同时采用动态裁剪参数促进早期探索和后期利用。为平衡性能和令牌效率，设计了考虑准确性、格式和令牌成本的长度感知安全性奖励。实验表明，GuardReasoner-VL在F1得分上平均比亚军高出19.27%，并开源了相关数据、代码和模型(3B/7B)。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 05:46:10 GMT</pubDate>
</item>
<item>
<title>人类在博弈实验中对大型语言模型对手的行为差异研究</title>
<link>https://arxiv.org/abs/2505.11011</link>
<guid>https://arxiv.org/abs/2505.11011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">人类在与大型语言模型博弈时选择更低数字，主要由高战略推理能力者驱动。</p><br /><br /><p><strong>摘要：</strong> 本研究通过首个受货币激励的实验室实验，探讨人类在多人p-beauty博弈中面对人类对手与大型语言模型（LLMs）对手时的行为差异。采用被试内设计，结果显示，与对抗人类相比，人类参与者对抗LLMs时选择显著更低的数字，这种现象主要源于零纳什均衡选择的增加。此转变主要由具有较高战略推理能力的参与者推动，他们通过引用LLMs的推理能力和合作倾向来解释策略选择。研究揭示了混合人类-LLMs系统机制设计的重要启示，并展示了参与者的异质性行为及对LLMs行为信念的变化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 05:01:09 GMT</pubDate>
</item>
<item>
<title>基于多视角搜索的自动化定理证明系统MPS-Prover</title>
<link>https://arxiv.org/abs/2505.10962</link>
<guid>https://arxiv.org/abs/2505.10962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPS-Prover通过创新的数据精炼和多视角树搜索机制，显著提升了定理证明效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MPS-Prover的新自动化定理证明系统，旨在解决现有逐步定理证明器因搜索引导偏差导致的低效问题。MPS-Prover采用两种关键创新：一种高效的数据后训练精炼策略，可以去除约40%的冗余训练数据而不影响性能；以及一种结合学习到的批评模型与精心设计启发式规则的多视角树搜索机制，该机制有助于多样化战术选择、避免陷入无效状态并增强搜索鲁棒性。广泛的评估显示，MPS-Prover在多个具有挑战性的基准测试中达到了最先进的性能，包括miniF2F和ProofNet，超过了之前7B参数规模的模型。此外，分析表明，与现有的逐步方法和整体证明方法相比，MPS-Prover生成的证明更短且更多样化，凸显了其效率和有效性。这项工作推动了基于大型语言模型的形式推理能力，并为开发更强大的定理证明器提供了稳健框架和全面分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 03:56:03 GMT</pubDate>
</item>
<item>
<title>MatTools：评估大型语言模型在材料科学工具应用中的能力</title>
<link>https://arxiv.org/abs/2505.10852</link>
<guid>https://arxiv.org/abs/2505.10852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MatTools基准应用，评估大型语言模型在材料科学问题上的解答能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MatTools的新基准应用，用于评估大型语言模型（LLMs）在材料科学领域的能力，涵盖文献理解、性质预测、材料发现及合金设计等方面。MatTools由两个互补部分组成：材料模拟工具问答（QA）基准和真实世界工具使用基准。QA基准基于pymatgen代码库和文档构建，包含69,225对问答对；真实世界基准则包含49项任务（138个子任务），涉及材料属性计算的Python代码生成。通过对多种LLMs的评估，研究得出三个关键见解：通用模型优于专用模型、人工智能了解人工智能、简单方法更有效。MatTools为提升LLMs在材料科学工具应用中的能力提供了标准化框架，推动了更有效的AI系统在材料科学及通用科学研究中的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:43:05 GMT</pubDate>
</item>
<item>
<title>MMLongBench：首个全面评估长上下文视觉语言模型的基准测试</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个涵盖多种长上下文视觉语言任务的基准MMLongBench。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MMLongBench的新基准测试，这是第一个全面评估长上下文视觉语言模型(LCVLMs)能力的数据集。MMLongBench包含13,331个样本，涉及五个下游任务类别，并覆盖自然和合成图像等多种类型。通过在标准化输入长度下进行跨模态标记化处理，该基准能够有效评估模型在不同输入长度下的鲁棒性。通过对46个闭源和开源LCVLMs的详尽测试，研究发现单一任务的表现并不能很好地反映整体长上下文能力，且现有模型在长上下文任务中仍面临挑战，具备更强推理能力的模型表现更优。MMLongBench为下一代LCVLMs的发展提供了重要的诊断基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:52:54 GMT</pubDate>
</item>
<item>
<title>MuToR：一种高效多令牌预测方法</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MuToR方法，提升语言模型预训练效果并适用于多种场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MuToR的新方法，用于增强语言模型的多令牌预测能力。MuToR通过在输入序列中插入可学习的寄存器令牌来实现这一目标，每个令牌负责预测未来的特定目标。相比现有技术，MuToR具有参数增加少、无需架构改动且与下一令牌预训练目标兼容等优势。实验表明，MuToR在多种任务上表现出色，包括监督微调、参数高效微调及预训练。无论是在语言还是视觉领域，MuToR都能有效应对复杂的生成任务。此外，该方法支持扩展预测范围，并且代码将在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10518" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:25:03 GMT</pubDate>
</item>
<item>
<title>Qwen3：大型语言模型家族的新里程碑</title>
<link>https://arxiv.org/abs/2505.09388</link>
<guid>https://arxiv.org/abs/2505.09388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen3发布，集复杂推理与快速响应于一身，支持多语言并开源。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Qwen3系列大型语言模型（LLMs），这是Qwen模型家族的最新版本。Qwen3包含密集架构和混合专家（MoE）架构的多种规模模型，参数量从0.6亿到235亿不等。创新之处在于统一整合了思考模式（用于复杂多步推理）和非思考模式（用于快速上下文响应），无需切换不同模型即可实现动态模式转换。此外，Qwen3引入了思考预算机制，允许用户根据任务复杂度灵活分配计算资源，平衡延迟与性能。与前代相比，Qwen3将多语言支持扩展至119种语言和方言，并通过开源促进社区研究。实验表明，Qwen3在代码生成、数学推理及代理任务等多个基准测试中表现优异，优于部分更大规模的MoE模型和专有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 09:41:34 GMT</pubDate>
</item>
<item>
<title>一种用于视觉语言模型知识蒸馏的双头优化框架</title>
<link>https://arxiv.org/abs/2505.07675</link>
<guid>https://arxiv.org/abs/2505.07675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效的知识蒸馏框架DHO，在半监督设置下提升紧凑模型性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉-语言模型（VLMs）在多种任务上取得了显著成果，但其部署在资源受限环境中的挑战依然存在。传统知识蒸馏方法通常涉及多阶段训练或额外调优，增加了计算开销和优化复杂性。本文提出了一种名为“双头优化”（DHO）的新框架，通过引入独立学习标签数据和教师预测的双预测头，在半监督环境中有效转移VLM的知识到紧凑的任务特定模型。实验表明，DHO在多个领域和细粒度数据集上优于基线模型，在ImageNet上使用1%和10%标记数据时分别提升了3%和0.1%的准确性，同时减少了参数量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 11:39:51 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的3D先验引导图像编辑框架3D-Fixup</title>
<link>https://arxiv.org/abs/2505.10566</link>
<guid>https://arxiv.org/abs/2505.10566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用扩散模型和3D先验进行复杂图像编辑的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D-aware图像编辑中的挑战，提出了名为3D-Fixup的新框架，该框架通过学习到的3D先验指导2D图像编辑。不同于仅依赖单一图像描述物体，3D-Fixup利用基于扩散模型的生成能力，并结合视频数据生成训练数据对。此外，通过引入Image-to-3D模型提供3D引导，实现了物体平移、旋转等复杂编辑任务。实验表明，该方法能够在保持身份一致性的同时实现高质量的3D-aware图像编辑，推动扩散模型在真实图像操作中的应用。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>Real2Render2Real：无需硬件操作的机器人训练数据生成方法</title>
<link>https://arxiv.org/abs/2505.09601</link>
<guid>https://arxiv.org/abs/2505.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需物理机器人操作的机器人训练数据生成方法R2R2R。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Real2Render2Real（R2R2R）的新方法，用于生成机器人训练数据，而无需依赖物体动力学模拟或机器人硬件的远程操作。该方法仅需输入智能手机扫描的物体图像及人类演示视频，即可通过3D高保真渲染生成大量机器人演示数据。R2R2R利用3D高斯点云技术实现刚性与可动物体的灵活资产生成和轨迹合成，并将其转换为网格以兼容大规模渲染引擎。实验表明，基于R2R2R生成的数据训练模型的表现可媲美传统方式下150倍的人类远程操作数据训练效果。这种方法显著降低了数据收集成本，为机器人学习提供了新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:50:35 GMT</pubDate>
</item>
<item>
<title>X-Sim：通过人类视频训练机器人操作策略的新框架</title>
<link>https://arxiv.org/abs/2505.07096</link>
<guid>https://arxiv.org/abs/2505.07096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于RGBD视频的跨实体机器人操作策略学习框架X-Sim。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为X-Sim的新框架，用于利用人类视频训练机器人操作策略。X-Sim是一种真实到模拟再到真实的框架，它使用物体运动作为密集且可转移的信号来学习机器人策略。该框架首先从RGBD人类视频重建出逼真的模拟环境并跟踪物体轨迹以定义物体中心奖励，这些奖励被用来在模拟环境中训练强化学习策略。然后，该策略被蒸馏成条件图像扩散策略，使用具有不同视角和光照的合成滚动渲染。为了将策略转移到现实世界，X-Sim引入了一种在线领域适应技术，在部署过程中对齐真实和模拟观察。重要的是，X-Sim不需要任何机器人遥操作数据。我们在两个环境中评估了X-Sim在五个操作任务上的表现，结果显示它平均提高了30%的任务进展，比手部追踪和模拟到真实基线更好，并且匹配了需要10倍更多数据收集时间的行为克隆方法，同时泛化到新的相机视点和测试时的变化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 15:04:00 GMT</pubDate>
</item>
<item>
<title>并行扩展(ParScale)：语言模型更高效的扩展方法</title>
<link>https://arxiv.org/abs/2505.10475</link>
<guid>https://arxiv.org/abs/2505.10475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的语言模型扩展方法——并行扩展，可显著提高推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种全新的语言模型扩展方法，即并行扩展(ParScale)，通过在训练和推理过程中增加模型的并行计算来提升效率，而无需大幅增加参数规模或输出令牌数。该方法通过对输入应用P种不同的变换，在并行执行模型前向传播后动态聚合输出。实验表明，相比参数扩展，ParScale能在达到相同性能提升的同时，减少高达22倍的内存增长和6倍的延迟增长。此外，ParScale还可通过少量微调将现成的预训练模型转化为并行扩展版本，进一步降低训练成本。这一新发现的扩展定律可能有助于在资源受限场景下部署更强大的模型，并为计算在机器学习中的作用提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:24:45 GMT</pubDate>
</item>
<item>
<title>AI Agents与Agentic AI对比分析及发展路径</title>
<link>https://arxiv.org/abs/2505.10468</link>
<guid>https://arxiv.org/abs/2505.10468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究界定了AI Agents与Agentic AI的设计哲学与能力差异。</p><br /><br /><p><strong>摘要：</strong> 本研究通过结构化的概念分类法、应用映射和挑战分析，清晰地区分了AI Agents与Agentic AI。AI Agents被定义为由大型语言模型（LLMs）和大型图像模型（LIMs）驱动的模块化系统，专注于特定任务自动化，而Generative AI为其前身。相比之下，Agentic AI则通过多代理协作、动态任务分解、持久记忆和协调自治展现了范式转变。文章通过架构演化、操作机制、交互风格和自治水平的对比分析，展示了两者在客户支持、调度和数据总结等领域的应用差异，并探讨了幻觉、脆弱性、涌现行为和协调失败等独特挑战，提出了ReAct循环、RAG、协调层和因果建模等解决方案。此研究旨在为开发稳健、可扩展且可解释的AI代理和Agentic AI驱动系统提供明确路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:21:33 GMT</pubDate>
</item>
<item>
<title>QuXAI：用于混合量子经典机器学习模型的可解释性框架</title>
<link>https://arxiv.org/abs/2505.10167</link>
<guid>https://arxiv.org/abs/2505.10167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuXAI框架，提升混合量子经典机器学习模型的可解释性和可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于混合量子经典机器学习（HQML）模型的可解释性研究，指出当前领域在全局和局部解释方法上的研究空白。为此，作者引入QuXAI框架，基于Q-MEDLEY方法，通过量化特征编码与经典学习结合，分析特征重要性并可视化结果。实验表明，Q-MEDLEY不仅能有效区分经典部分的重要性及噪声，还在验证测试中表现优异。此外，消融实验进一步验证了Q-MEDLEY复合结构的优势。这项工作对增强HQML模型的透明度和可靠性具有重要意义，推动量子增强人工智能技术的安全应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 06:51:34 GMT</pubDate>
</item>
<item>
<title>Unilogit：一种新颖的语言模型自蒸馏机删方法</title>
<link>https://arxiv.org/abs/2505.06027</link>
<guid>https://arxiv.org/abs/2505.06027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型自蒸馏方法Unilogit，实现语言模型中高效有选择性遗忘。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Unilogit的新颖自蒸馏方法，用于大规模语言模型中的机器遗忘任务。该方法通过动态调整目标logits，使目标令牌具有均匀概率，从而解决了在遵守GDPR等隐私法规的同时选择性遗忘特定信息的难题。与依赖静态超参数或初始模型输出的现有方法不同，Unilogit利用当前模型的输出作为更准确的自蒸馏目标，不仅无需额外的超参数设置，还提升了模型对黄金标准目标的逼近能力。实验表明，Unilogit在公共基准和自有电商数据集上表现出色，在平衡遗忘与保留目标方面优于NPO和UnDIAL等最先进方法，并且在多种场景下展现出强大的鲁棒性和实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 09:19:09 GMT</pubDate>
</item>
<item>
<title>Prior Depth Anything框架实现任意场景高精度深度图生成</title>
<link>https://arxiv.org/abs/2505.10565</link>
<guid>https://arxiv.org/abs/2505.10565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合精确度量和相对结构的Prior Depth Anything框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Prior Depth Anything的新框架，该框架通过整合不完整但精确的度量信息与相对完整的几何结构，在任意场景中生成准确、密集且详细的度量深度图。为此，设计了一个粗到细的管道逐步融合两种互补的深度源。首先引入像素级度量对齐和距离感知加权，通过显式使用深度预测预填充多种度量先验，有效缩小了先验模式之间的领域差距。其次，开发了一个条件单目深度估计模型来细化深度先验的固有噪声。我们的模型在7个真实世界的数据集上展示了令人印象深刻的零样本泛化能力，并提供了灵活的准确性和效率权衡。此外，它还能在测试时通过切换预测模型进行改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>Style Customization of Text-to-Vector Generation with Image Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.10558</link>
<guid>https://arxiv.org/abs/2505.10558</guid>
<content:encoded><![CDATA[
Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>PointArena：多模态指针能力评估平台</title>
<link>https://arxiv.org/abs/2505.09990</link>
<guid>https://arxiv.org/abs/2505.09990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PointArena平台，用于评估多模态模型在多样化推理场景中的指针能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为PointArena的综合性平台，用于评估多模态模型的指针能力，涵盖从视觉情境中定位物体到实际操作场景下的应用。该平台由三个主要部分组成：Point-Bench数据集，包含五个推理类别的约1000项指针任务；Point-Battle交互式网络竞技场，已收集超过4500份匿名投票进行盲测；以及Point-Act机器人操作系统，用于在真实环境中直接测试模型的指针能力。通过对多种开源及专有模型的广泛评估发现，Molmo-72B表现出色，而针对指针任务的监督训练显著提升了模型性能。此外，研究还揭示了抽象推理与具体现实世界动作之间的强相关性，强调了精确指针能力的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 02:04:42 GMT</pubDate>
</item>
<item>
<title>Tokenadapt：一种高效的多语言模型 tokenizer 移植框架</title>
<link>https://arxiv.org/abs/2505.09738</link>
<guid>https://arxiv.org/abs/2505.09738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tokenadapt方法解决固定分词器导致的语言模型效率低下问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对预训练语言模型（LLMs）因固定分词方案而产生的性能瓶颈及资源消耗问题，提出了名为Tokenadapt的创新性tokenizer移植框架。该框架通过混合启发式方法初始化新的唯一token嵌入，同时引入多词SuperTokens进行压缩优化，有效减少碎片化并提升多语言应用表现。实验表明，Tokenadapt在多个基准测试中显著优于现有方法，尤其在零样本困惑度方面，展现了卓越的性能改进能力，至少实现了2倍以上的整体分数提升。关键词：Tokenadapt, tokenizer移植, 多词SuperTokens</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 15:00:27 GMT</pubDate>
</item>
<item>
<title>基于动作条件的世界模型EnerVerse-AC实现高效机器人模仿学习</title>
<link>https://arxiv.org/abs/2505.09723</link>
<guid>https://arxiv.org/abs/2505.09723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EnerVerse-AC模型，通过生成未来视觉观察提升机器人交互场景测试效率。</p><br /><br /><p><strong>摘要：</strong> 机器人模仿学习已从静态任务扩展到动态交互场景，但实时与动态环境互动的测试和评估成本高昂且困难重重。本文提出EnerVerse-AC（EVAC），这是一种动作条件世界模型，可基于预测动作生成未来的视觉观察，从而实现真实且可控的机器人推理。EVAC基于先前架构，引入多级动作条件机制和射线图编码以支持动态多视角图像生成，同时通过多样化失败轨迹扩充训练数据以增强泛化能力。作为数据引擎和评估工具，EVAC将人类收集的轨迹扩展为多样化的数据集，并生成动作条件视频观察以进行策略测试，无需物理机器人或复杂模拟。此方法大幅降低了成本，同时保持了机器人操作评估的高保真度。实验结果验证了该方法的有效性。代码、检查点和数据集可在指定网址获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 14:30:53 GMT</pubDate>
</item>
<item>
<title>ReSurgSAM2：一种高效的手术场景分割框架</title>
<link>https://arxiv.org/abs/2505.08581</link>
<guid>https://arxiv.org/abs/2505.08581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReSurgSAM2框架，提升手术场景分割精度和效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReSurgSAM2的两阶段手术参考分割框架，该框架结合了Segment Anything Model 2进行目标检测，并通过可靠的初始帧识别和多样性驱动的长期记忆机制实现跟踪。在检测阶段，引入跨模态时空Mamba生成精确结果；在跟踪阶段，采用多样性驱动的记忆机制确保长期一致性。实验表明，ReSurgSAM2在准确性与效率上显著优于现有方法，实时运行速度达61.2 FPS。相关代码和数据集将在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 09:56:10 GMT</pubDate>
</item>
<item>
<title>ETT：端到端视觉标记器调优提升多模态任务性能</title>
<link>https://arxiv.org/abs/2505.10562</link>
<guid>https://arxiv.org/abs/2505.10562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端视觉标记器调优方法，显著提高多模态理解和视觉生成任务的表现。</p><br /><br /><p><strong>摘要：</strong> 现有视觉标记器的优化通常独立于下游任务，忽略了视觉标记器在不同任务中的表征差异性问题。这种解耦范式可能导致目标任务中的表示瓶颈。为解决这一问题，我们提出了ETT（End-to-End Vision Tokenizer Tuning），通过联合优化视觉标记器和目标自回归任务，实现更好的跨任务适应能力。与传统仅依赖冻结标记器的模型不同，ETT利用标记器代码本的视觉嵌入，在重建和描述双重目标下进行端到端优化。该方法易于实施且兼容现有训练管道，无需修改原始代码本或架构。实验表明，ETT在多模态理解和视觉生成任务上相较于冻结标记器基准提升了2-6%的性能，同时保持了原有的重建能力。我们期望这一简单而强大的方法能够推动更多多模态基础模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>通过元能力对齐提升大规模推理模型的可扩展性和可靠性</title>
<link>https://arxiv.org/abs/2505.10554</link>
<guid>https://arxiv.org/abs/2505.10554</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过元能力对齐增强大规模推理模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大规模推理模型（LRMs）在长链推理中的局限性，提出了一种基于元能力对齐的新方法。传统的方法依赖于提示和偶然出现的“顿悟”现象，但这些行为的时机和一致性难以预测和控制，限制了模型的扩展性和可靠性。为了克服这些问题，我们引入了演绎、归纳和溯因三种元能力的显式对齐，并设计了一个三阶段的流水线：个体对齐、参数空间合并以及领域特定强化学习。这种方法相较于仅通过指令微调的基线模型，在性能上提升了超过10%。此外，通过领域特定的强化学习进一步提升了数学、编码和科学基准测试中的表现，平均提高了2%，证明了元能力对齐提供了可扩展且可靠的推理基础。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10554" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>世界偏好建模（WorldPM）揭示人类偏好建模中的规模法则</title>
<link>https://arxiv.org/abs/2505.10527</link>
<guid>https://arxiv.org/abs/2505.10527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型的测试损失与模型和数据集大小成幂律关系，这一规律在偏好建模中同样适用。</p><br /><br /><p><strong>摘要：</strong> 本文提出世界偏好建模（WorldPM），通过从公共论坛收集多样化用户社区的偏好数据，在参数规模从1.5B到72B的不同模型上进行大规模训练（15M量级）。实验表明，对抗性指标随训练数据和基础模型增大而提升；客观指标在大模型中表现出涌现特性；主观指标则未显示规模趋势。进一步验证显示，WorldPM可有效作为偏好微调的基础模型。在7个基准的20个子任务中，WorldPM显著提升了多种规模的人类偏好数据集的泛化性能，尤其在内部强化学习人类反馈（RLHF）流水线中，表现提升4%-8%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:38:37 GMT</pubDate>
</item>
<item>
<title>基于强化学习的AI法官模型训练方法J1</title>
<link>https://arxiv.org/abs/2505.10320</link>
<guid>https://arxiv.org/abs/2505.10320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种强化学习方法J1，显著提升AI法官模型的判断能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为J1的强化学习方法，用于训练具备更强推理能力的大型语言模型（LLM），使其作为评价工具时能够提供更准确的判断。通过将可验证和不可验证的任务转化为具有明确奖励机制的判断任务，J1有效提升了模型的推理能力和减少偏见。实验表明，J1在多个基准测试中表现优于其他同类规模（8B或70B参数量）的模型，甚至在某些任务上超过了更大规模的模型。此外，研究还探讨了不同训练策略、奖励机制及提示种子对模型性能的影响，发现模型通过学习制定评估标准、对比自身生成的答案以及重新评估响应正确性等方式提高了判断质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 10:05:15 GMT</pubDate>
</item>
<item>
<title>基于CoT百科全书的大语言模型推理分析与优化</title>
<link>https://arxiv.org/abs/2505.10185</link>
<guid>https://arxiv.org/abs/2505.10185</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自下而上的框架来分析和引导大语言模型的推理策略。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CoT百科全书，这是一种用于分析和引导现代大型语言模型推理的新框架。该方法通过自动提取模型生成的长链推理中的多样化推理标准，并将其嵌入语义空间进行聚类，形成代表性类别，同时推导对比标准来解释推理行为。实验结果显示，与现有方法相比，此框架生成的分析更具可解释性和全面性。此外，这种对推理策略的理解还能带来性能提升，帮助预测模型可能采用的策略并指导其走向更有效的替代方案。最后，研究还揭示了训练数据格式而非数据领域对推理行为的影响更大，强调了格式感知模型设计的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10185" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 07:31:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型与扩散变压器融合在文本到图像合成中的探索</title>
<link>https://arxiv.org/abs/2505.10046</link>
<guid>https://arxiv.org/abs/2505.10046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨大型语言模型与扩散变压器在多模态生成中的深度融合设计空间。</p><br /><br /><p><strong>摘要：</strong> 本文并非介绍新方法，而是深入探索近期文本到图像合成领域的关键设计空间，即大型语言模型（LLMs）与扩散变压器（DiTs）的深度融合在多模态生成中的应用。先前的研究多集中于系统整体性能而缺乏对替代方法的详细比较，且关键设计细节及训练方案常未公开，导致该方法潜力存在不确定性。为填补这些空白，我们进行了一项关于文本到图像生成的实证研究，通过与基准模型的受控比较、分析重要设计选择以及提供可扩展训练的清晰可复现方案，希望为未来多模态生成研究提供有意义的数据参考和实用指南。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 03:43:23 GMT</pubDate>
</item>
<item>
<title>AdaptCLIP：基于CLIP的视觉异常检测新方法</title>
<link>https://arxiv.org/abs/2505.09926</link>
<guid>https://arxiv.org/abs/2505.09926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为AdaptCLIP的新方法，显著提升跨域异常检测性能。</p><br /><br /><p><strong>摘要：</strong> 现有的通用视觉异常检测方法在处理未知领域时面临设计复杂模板或额外微调的问题，限制了灵活性。本文提出AdaptCLIP方法，通过交替学习视觉和文本表示，并结合上下文和对齐残差特征的比较学习，改进了CLIP模型的零样本/少样本泛化能力。该方法仅需添加三个简单的适配器，在工业和医学领域的12个基准测试中达到了最先进的性能。AdaptCLIP无需针对目标领域进行训练，展示了出色的跨域适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 23:24:28 GMT</pubDate>
</item>
<item>
<title>构建具身世界模型评估基准EWMBench</title>
<link>https://arxiv.org/abs/2505.09694</link>
<guid>https://arxiv.org/abs/2505.09694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EWMBench基准用于评估具身世界模型的物理一致性与动作一致性。</p><br /><br /><p><strong>摘要：</strong> 近期创意AI的进步推动了文本到视频扩散模型的发展，使其演变为能够根据语言指令生成物理上合理场景的具身世界模型（EWMs）。然而，现有模型的评估多局限于一般感知指标，缺乏对物理基础和动作一致性的深入考量。本文提出了EWMBench评估框架，通过视觉场景一致性、运动正确性和语义对齐三个维度，利用精心策划的数据集及多维工具，系统性评估EWMs的表现。该基准不仅揭示了现有视频生成模型在具身任务中的局限性，还为未来研究提供了指导方向。相关数据集与工具已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 14:00:19 GMT</pubDate>
</item>
<item>
<title>双层优化框架：提升大规模语言模型系统提示的鲁棒性与迁移能力</title>
<link>https://arxiv.org/abs/2505.09666</link>
<guid>https://arxiv.org/abs/2505.09666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种双层优化方法，用于提升大规模语言模型系统提示的通用性和迁移能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大规模语言模型（LLMs）中系统提示（system prompt）的优化问题，指出现有研究多集中于特定任务的用户提示（user prompt），而忽视了对系统提示的全局优化。为此，我们引入双层系统提示优化这一新问题，旨在设计出对多样用户提示具有鲁棒性且能在未知任务间迁移的系统提示。为此，我们提出了一个元学习框架，通过在多个数据集上的多种用户提示上迭代优化系统提示，同时同步更新用户提示，以实现两者间的协同效应。实验结果表明，该方法在五个领域的14个未见过的数据集上均表现出色，不仅提升了系统提示对用户提示的适应性，还显著加快了对未知任务的适配速度，同时提高了性能表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:46:15 GMT</pubDate>
</item>
<item>
<title>基于纯视觉元学习框架的通用异常分割方法</title>
<link>https://arxiv.org/abs/2505.09265</link>
<guid>https://arxiv.org/abs/2505.09265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种纯视觉模型实现零样本和少样本通用异常分割的方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用纯视觉基础模型替代广泛使用的视觉-语言模型进行通用视觉异常分割的可能性。通过将异常分割统一为变化分割的新范式，我们利用现有图像数据集中的大规模合成图像对，这些图像对具有对象级和局部区域变化。提出了一个名为MetaUAS的一次提示元学习框架，该框架在合成数据集上训练后，能够很好地泛化到现实世界中的任何新型或未见视觉异常的分割任务。为了处理提示图像与查询图像之间的几何变化，设计了一种软特征对齐模块，以桥接配对图像的变化感知和单图像语义分割。这是首次使用纯视觉模型实现通用异常分割的工作，无需依赖特殊异常检测数据集和预训练的视觉-语言模型。实验表明，MetaUAS在性能上显著优于以往的零样本、少样本甚至全样本异常分割方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:26 GMT</pubDate>
</item>
<item>
<title>基于单张正常图像提示的统一异常检测方法</title>
<link>https://arxiv.org/abs/2505.09264</link>
<guid>https://arxiv.org/abs/2505.09264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅需一张正常图像即可实现高效统一异常检测的方法OneNIP。</p><br /><br /><p><strong>摘要：</strong> 现有的自注意力重建网络在多类别异常检测中表现出色，但其主要依赖目标特征进行重建，可能导致正常和异常特征的完美重建，从而无法有效检测异常。此外，由于在低空间分辨率潜在空间中进行重建，这些模型常常产生不准确的异常分割。为了解决这些问题，本文提出了一个名为OneNIP的新方法，它仅使用一张正常图像即可重建正常特征并恢复异常特征。与以往工作相比，OneNIP首次实现了仅凭一张正常图像进行异常重建或恢复，显著提升了统一异常检测的性能。同时，我们还提出了一种监督细化器，通过使用真实正常图像和合成异常图像来回归重建误差，从而大幅提高了像素级异常分割的准确性。实验表明，OneNIP在MVTec、BTAD和VisA三个工业异常检测基准上优于现有方法。代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:14 GMT</pubDate>
</item>
<item>
<title>基于少样本生成的工业异常检测方法</title>
<link>https://arxiv.org/abs/2505.09263</link>
<guid>https://arxiv.org/abs/2505.09263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种少样本生成方法，有效提升工业异常检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对工业检测中异常样本稀缺的问题，提出了一种名为AnoGen的少样本异常驱动生成方法。该方法通过引导扩散模型利用少量真实异常样本生成逼真的多样化异常图像，从而辅助训练异常检测模型。具体而言，首先基于给定的真实异常样本学习异常分布并注入嵌入向量；其次，结合嵌入向量和边界框指导扩散模型生成特定对象上的异常；最后，设计弱监督异常检测方法，进一步优化模型性能。实验表明，AnoGen显著提升了DRAEM和DesTSeg在分割任务中的AU-PR指标，分别提高了5.8%和1.5%。研究代码与生成的异常数据已在GitHub公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:06 GMT</pubDate>
</item>
<item>
<title>OpenThinkIMG：赋能视觉语言模型的工具增强框架</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个端到端工具增强框架OpenThinkIMG，提升视觉语言模型动态工具操作能力。</p><br /><br /><p><strong>摘要：</strong> 尽管人类能够灵活运用交互式视觉认知解决复杂问题，但使大型视觉语言模型（LVLMs）学习类似的适应性行为仍具挑战。主要障碍在于缺乏标准化基础设施，阻碍了工具整合、交互数据生成及高效训练。为填补这些空白，我们推出了OpenThinkIMG，这是一个开源的综合框架，支持工具增强型LVLMs。它具备标准化视觉工具接口、可扩展轨迹生成和灵活训练环境。鉴于监督微调对动态工具调用的策略泛化有限，我们还提出了V-ToolRL强化学习框架，使LVLMs能够自主发现最佳工具使用策略。实验表明，基于Qwen2-VL-2B的RL训练代理在图表推理任务中表现优异，不仅优于监督微调初始化版本，还超过了Taco、CogCom等基准模型，并且比GPT-4.1高出8.68个百分点。我们希望OpenThinkIMG能成为推动动态工具增强视觉推理发展的基础框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 10:35:51 GMT</pubDate>
</item>
<item>
<title>MLE-Dojo：用于自主大语言模型迭代训练的交互式框架</title>
<link>https://arxiv.org/abs/2505.07782</link>
<guid>https://arxiv.org/abs/2505.07782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLE-Dojo是一个支持大语言模型迭代学习和优化的交互式框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MLE-Dojo的新框架，它是一种类似于Gym的环境，旨在系统性地进行强化学习、评估以及改进自主大型语言模型（LLM）代理。MLE-Dojo不同于依赖静态数据集或单一评估方法的传统基准测试，它通过提供互动环境，使代理能够在结构化反馈循环中反复试验、调试和优化解决方案。该框架基于超过200个真实世界Kaggle挑战构建，涵盖了多样化的开放性机器学习工程任务，如数据处理、架构搜索、超参数调优和代码调试。实验结果显示，尽管当前模型能够在一定程度上实现迭代改进，但在生成长期解决方案和高效解决复杂错误方面仍存在显著局限性。此外，MLE-Dojo具有灵活且可扩展的架构，能够无缝集成多种数据源、工具和评估协议，从而促进模型驱动代理的调优、互操作性、可扩展性和可重复性。我们开源了这一框架及其基准测试，以推动社区对下一代机器学习工程代理的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:35:43 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的图像精确光照编辑方法</title>
<link>https://arxiv.org/abs/2505.09608</link>
<guid>https://arxiv.org/abs/2505.09608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的精确光照编辑方法，实现对图像光源的细粒度控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种简单而有效的基于扩散模型的方法，用于在图像中对光源进行精细且参数化的控制。现有的一些重照明方法要么依赖多个输入视图进行逆渲染，要么无法提供明确的光照变化控制。我们的方法通过微调扩散模型，在一组真实的原始照片对和大规模合成渲染图像上训练，以激发其真实感先验用于重照明。我们利用光的线性特性合成展示受控光照变化的图像对，从而实现对目标光源或环境光的精确控制。实验表明，该方法不仅能够产生令人信服的光照编辑效果，而且在用户偏好测试中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:57:27 GMT</pubDate>
</item>
<item>
<title>基于音频语言模型的对话系统评估方法WavReward</title>
<link>https://arxiv.org/abs/2505.09558</link>
<guid>https://arxiv.org/abs/2505.09558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于音频语言模型的评估框架WavReward，用于评价语音对话系统的IQ和EQ。</p><br /><br /><p><strong>摘要：</strong> 近年来，端到端语音对话模型如GPT-4o-audio受到广泛关注，但其会话性能的评估却未得到足够重视。主要原因是传统文本型语言模型难以衡量智能聊天机器人传达的丰富非文本信息。为填补这一空白，我们提出了WavReward，这是一种基于音频语言模型的奖励反馈模型，能够对语音输入的对话系统进行IQ和EQ的综合评估。WavReward通过多样本反馈机制结合强化学习算法优化后训练，同时引入了包含理解与生成两大方面的ChatReward-30K数据集，涵盖多种任务场景。实验表明，WavReward在多个语音对话场景中显著优于现有评估模型，客观准确性从55.1%提升至91.5%，主观A/B测试领先83%。此外，消融研究验证了WavReward各组件的必要性。所有数据和代码将在论文被接受后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:54:15 GMT</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 15:01:12 GMT</pubDate>
</item>
<item>
<title>LLaVA图像文本预训练数据集毒性分析与缓解策略</title>
<link>https://arxiv.org/abs/2505.06356</link>
<guid>https://arxiv.org/abs/2505.06356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现并移除LLaVA预训练数据集中有害内容，提出针对性的毒性缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了LLaVA图像-文本预训练数据集中毒性内容的存在情况，分析了有害内容在不同模态下的表现形式，并针对常见毒性类别提出了缓解策略。通过这些措施，我们成功移除了7,531对有毒图像-文本对，创建了一个经过毒性优化的数据集。此外，我们还提供了构建稳健毒性检测管道的指南。研究结果表明，主动识别和过滤诸如仇恨言论、露骨图像及针对性骚扰等毒性内容对于构建更负责任且公平的多模态系统至关重要。该去毒化数据集已开源，可供进一步研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 14:01:50 GMT</pubDate>
</item>
<item>
<title>SteepGS：优化3D Gaussian Splatting密度控制以提升渲染效率</title>
<link>https://arxiv.org/abs/2505.05587</link>
<guid>https://arxiv.org/abs/2505.05587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的3DGS优化框架SteepGS，大幅减少高分辨率场景中的Gaussian点数，同时保持渲染质量。</p><br /><br /><p><strong>摘要：</strong> 3D Gaussian Splatting (3DGS) 是一种高效的新视角合成技术，通过GPU加速渲染实现高质量图像生成。然而，其密集化算法容易导致冗余点云，增加内存和存储需求。本文提出了一种理论框架，分析并改进了3DGS的密度控制机制，揭示了分裂操作在避免鞍点的重要性。基于优化理论，我们确定了最小化后代Gaussian数量的条件，并引入了陡峭密度控制策略（SteepestGS）。该方法通过优化参数更新方向和后代透明度归一化，将Gaussian点数减少了约50%，显著提升了渲染效率和系统可扩展性，尤其适用于资源受限设备。实验表明，SteepGS在不降低视觉质量的前提下实现了性能飞跃。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 14:41:38 GMT</pubDate>
</item>
<item>
<title>Omni-R1：通过强化学习提升多模态大模型音频问答性能</title>
<link>https://arxiv.org/abs/2505.09439</link>
<guid>https://arxiv.org/abs/2505.09439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Omni-R1在MMAU基准测试中实现音频问答领域新SOTA。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Omni-R1的新方法，该方法基于对近期多模态大型语言模型Qwen2.5-Omni进行微调，采用强化学习算法GRPO优化处理音频问答数据集。实验结果显示，在MMAU基准测试的多个类别中，包括声音、音乐、语音及整体平均分上，Omni-R1在Test-mini和Test-full两个子集上均取得了最高准确率。进一步分析表明，这种性能提升主要归因于文本推理能力的增强。此外，研究还发现，仅使用纯文本数据集对模型进行微调同样能够有效改善音频相关任务的表现，这一发现令人意外且具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 10:47:16 GMT</pubDate>
</item>
<item>
<title>Marigold：通过条件生成模型从预训练扩散模型中提取知识</title>
<link>https://arxiv.org/abs/2505.09358</link>
<guid>https://arxiv.org/abs/2505.09358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Marigold，利用预训练扩散模型实现高效密集图像分析。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Marigold的条件生成模型家族及其微调协议，该协议可以从预训练的如Stable Diffusion等基于潜在扩散的模型中提取知识，并将其应用于单目深度估计、表面法线预测和内在分解等密集图像分析任务。Marigold仅需对预训练模型进行少量修改，使用小规模合成数据集，在单块GPU上训练几天即可完成，且表现出卓越的零样本泛化能力。这种方法充分利用了文本到图像生成模型的强大潜力，尤其是在稀缺标注数据的情况下，为计算机视觉领域提供了新的研究方向和应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 09:07:03 GMT</pubDate>
</item>
<item>
<title>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</title>
<link>https://arxiv.org/abs/2505.08787</link>
<guid>https://arxiv.org/abs/2505.08787</guid>
<content:encoded><![CDATA[
Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>Visually Interpretable Subtask Reasoning for Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.08084</link>
<guid>https://arxiv.org/abs/2505.08084</guid>
<content:encoded><![CDATA[
Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 17:37:06 GMT</pubDate>
</item>
<item>
<title>DetReIDX：大规模空地视角下的人体再识别数据集</title>
<link>https://arxiv.org/abs/2505.04793</link>
<guid>https://arxiv.org/abs/2505.04793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的大规模空地人体再识别数据集，用于评估真实场景下的技术性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为DetReIDX的大规模空地人体再识别数据集，旨在对现实世界条件下的人体再识别技术进行压力测试。该数据集包含来自三个大洲七个大学校园的超过1300万个边界框，涉及509个身份标识，无人机飞行高度介于5.8至120米之间。此外，数据集中的人体记录至少在两天的不同时间点进行，且穿着、光照及位置均发生变化，使其特别适合评估长期人体再识别。数据还标注了16种软生物特征属性以及检测、跟踪、再识别和动作识别等多任务标签。实验表明，当前最先进的方法在DetReIDX的条件下性能显著下降（检测精度下降高达80%，Rank-1再识别准确率下降超过70%）。该数据集及相关资源现已公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 16:41:06 GMT</pubDate>
</item>
<item>
<title>SweRank：高效代码定位框架解决软件问题描述匹配难题</title>
<link>https://arxiv.org/abs/2505.07849</link>
<guid>https://arxiv.org/abs/2505.07849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SweRank框架，解决传统代码检索模型在处理自然语言问题描述时的不足。</p><br /><br /><p><strong>摘要：</strong> 软件问题定位是软件开发中的关键环节，但传统方法存在效率低下等问题。本文介绍了一种名为SweRank的新框架，该框架通过高效的检索与重排序机制显著提升了问题描述到代码位置的匹配效果。此外，为了支持训练，研究团队构建了一个大规模的SweLoc数据集，包含来自公共GitHub仓库的真实问题描述及其对应的代码修改记录。实验表明，SweRank不仅在性能上超越了现有的排名模型，还优于基于闭源LLMs的昂贵代理系统。同时，SweLoc作为宝贵的公共资源，也为改进现有检索与重排序模型提供了帮助。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 15:44:09 GMT</pubDate>
</item>
<item>
<title>基于单张RGB图像的高效3D场景重建方法CAST</title>
<link>https://arxiv.org/abs/2502.12894</link>
<guid>https://arxiv.org/abs/2502.12894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合GPT模型和物理约束的3D场景重建新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CAST的新方法，用于从单张RGB图像中恢复高质量的3D场景。CAST首先通过提取对象级别的2D分割和相对深度信息，然后利用基于GPT的模型分析对象间的空间关系，以确保重建的一致性。接着，采用遮挡感知的大规模3D生成模型独立生成每个对象的完整几何形状，并使用MAE和点云条件处理来减轻遮挡和部分对象信息的影响。为了将对象对齐到场景中，使用对齐生成模型计算必要的变换。最后，引入物理感知校正步骤，利用细粒度关系图生成约束图，优化对象姿态，解决遮挡、物体穿透和漂浮等问题。CAST在机器人领域具有广泛应用，可实现真实到模拟的工作流并提供逼真的仿真环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2502.12894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 09:29:52 GMT</pubDate>
</item>
<item>
<title>DeepSeek-V3模型的硬件感知设计及其对AI系统的启示</title>
<link>https://arxiv.org/abs/2505.09343</link>
<guid>https://arxiv.org/abs/2505.09343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨硬件感知的深度学习模型设计如何有效解决大规模语言模型训练中的硬件瓶颈。</p><br /><br /><p><strong>摘要：</strong> 本文深入分析了DeepSeek-V3/R1模型架构及其支持基础设施，强调多头潜在注意力（MLA）、专家混合（MoE）架构、FP8混合精度训练和多平面网络拓扑等创新技术，显著提升了内存效率、计算通信权衡优化及硬件潜能释放。这些成果基于在开发过程中遇到的硬件限制，进一步与学术界和工业界展开讨论，提出未来硬件发展的方向，如精确低精度计算单元、规模扩展融合以及低延迟通信结构的创新。研究展示了硬件与模型协同设计在应对人工智能工作负载增长需求中的关键作用，为下一代AI系统提供了实践蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 08:39:03 GMT</pubDate>
</item>
<item>
<title>Video-based 长篇因果推理基准VCRBench及其评估</title>
<link>https://arxiv.org/abs/2505.08455</link>
<guid>https://arxiv.org/abs/2505.08455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准VCRBench测试大型视频语言模型的因果推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管视频理解领域取得了进展，但大型视频语言模型（LVLMs）在基于视频的因果推理方面的表现尚未被充分探索，主要因为缺乏专门的基准来评估这种能力。为解决这一问题，我们引入了一个名为VCRBench的新基准，该基准使用日常活动的程序化视频构建，通过打乱步骤并捕捉关键因果事件来测试LVLMs是否能识别、推理并正确排序实现特定目标所需的事件。此外，VCRBench设计旨在防止模型依赖语言捷径，并避免开放性问答评估的挑战。对当前最先进的LVLMs进行的评估表明，这些模型在处理基于视频的长篇因果推理时存在困难，主要是由于难以直接从视觉观察中建模长距离因果依赖关系。作为迈向提升这种能力的第一步，我们提出了识别-推理分解（RRD）模块化方法，将视频因果推理分为视频识别和因果推理两个子任务。实验结果显示，RRD显著提高了VCRBench上的准确性，提升了最多25.2%。最后，我们的分析揭示了有趣见解，例如LVLMs在复杂视频因果推理任务中主要依赖语言知识。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 07:35:58 GMT</pubDate>
</item>
<item>
<title>DeCLIP：通过解耦自注意力机制提升开放词汇密集预测性能</title>
<link>https://arxiv.org/abs/2505.04410</link>
<guid>https://arxiv.org/abs/2505.04410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架DeCLIP，改进CLIP模型用于开放词汇密集预测任务。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉预测任务中依赖预定义类别带来的限制问题，尤其是Vision-Language Models（VLMs）如CLIP在开放词汇任务中的局限性。我们观察到，CLIP的图像tokens在聚合空间或语义相关区域的信息时表现不佳，导致特征缺乏局部辨别性和空间一致性。为此，我们提出了DeCLIP框架，通过解耦自注意力模块分别获得“内容”和“上下文”特征。“内容”特征与图像裁剪表示对齐以提高局部辨别性，“上下文”特征则在视觉基础模型指导下学习保留空间相关性。实验表明，DeCLIP在多个开放词汇密集预测任务（如目标检测和语义分割）上显著优于现有方法。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 09:46:34 GMT</pubDate>
</item>
<item>
<title>BLIP3-o：统一图像理解和生成的创新多模态模型</title>
<link>https://arxiv.org/abs/2505.09568</link>
<guid>https://arxiv.org/abs/2505.09568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合扩散模型和CLIP特征的新方法，提升图像理解和生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了统一图像理解和生成的多模态模型设计，重点研究自回归和扩散模型在高质量生成及可扩展性上的潜力。通过引入扩散Transformer生成语义丰富的CLIP图像特征，该方法不仅提高了训练效率，还提升了生成质量。此外，通过先训练图像理解再转向图像生成的分阶段预训练策略，模型在保持理解能力的同时增强了生成能力。同时，构建了一个高质的指令微调数据集BLIP3o-60k，进一步优化了模型表现。最终，开发的BLIP3-o在多个基准测试中表现出色，并开源了模型及相关资源，推动未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09568" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:11:07 GMT</pubDate>
</item>
<item>
<title>Aya-Vision：解决多语言多模态模型挑战的新方法</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法解决多语言多模态模型中的数据稀缺及遗忘问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了针对多语言多模态语言模型开发的创新技术和框架，以应对跨视觉与文本模态对齐、高质量指令数据采集以及引入视觉能力后文本性能退化等挑战。通过构建合成注释框架生成多样化多语言多模态指令数据，同时提出跨模态模型融合技术缓解灾难性遗忘问题，从而在多种语言环境下保持并提升模型性能。实验结果显示，Aya-Vision-8B 和 Aya-Vision-32B 分别在性能上优于同类较大规模的模型，展示了高效计算利用和卓越性能表现的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 13:03:48 GMT</pubDate>
</item>
<item>
<title>基于信息瓶颈的LLMs训练方法：压缩与记忆的平衡</title>
<link>https://arxiv.org/abs/2505.08727</link>
<guid>https://arxiv.org/abs/2505.08727</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过压缩内部表征提升大语言模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文证明了泛化能力不仅可以通过数据规模提升，还可以通过压缩内部表征实现。为此引入了信息瓶颈语言建模（IBLM）目标，将语言建模重新定义为受约束优化问题。实验发现，在大型语言模型预训练过程中存在记忆-压缩循环现象，这与IBLM理论预测及生物学习-巩固过程相似。基于此，提出门控相变（GAPT）算法，该算法可自适应切换记忆与压缩阶段。在FineWeb数据集上对GPT-2进行预训练时，GAPT减少了50%的矩阵熵并提升了4.8%的交叉熵。此外，在算术乘法预训练任务中，GAPT使OOD泛化能力提高了35%，并在模拟灾难性遗忘场景中大幅降低了干扰，实现了97%的分离改善。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08727" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 12:37:54 GMT</pubDate>
</item>
<item>
<title>面向复杂工作流的可扩展评估方法研究</title>
<link>https://arxiv.org/abs/2505.08638</link>
<guid>https://arxiv.org/abs/2505.08638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有方法无法应对复杂工作流评估需求，新研究提出分类法并构建标注数据集。</p><br /><br /><p><strong>摘要：</strong> 随着自主性工作流在多个领域的普及，如何系统性地评估这些系统生成的复杂轨迹成为紧迫需求。当前评估依赖人工分析，难以应对日益增长的工作流复杂性和数量。此外，工具输出与语言模型推理之间的相互作用使错误分析更加困难。本研究提出了对自主系统轨迹进行稳健动态评估的需求，引入了自主系统错误类型的正式分类，并基于此构建了包含148个大型人工标注轨迹的数据集（TRAIL）。数据集涵盖单代理和多代理系统，聚焦软件工程和开放世界信息检索等实际应用。评估显示现代长上下文LLMs在轨迹调试方面表现不佳，最佳Gemini-2.5-pro模型仅达11%准确率。研究数据和代码已公开，以推动自主工作流的可扩展评估研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 10:55:31 GMT</pubDate>
</item>
<item>
<title>检索增强生成系统中超参数对性能的影响分析</title>
<link>https://arxiv.org/abs/2505.08445</link>
<guid>https://arxiv.org/abs/2505.08445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示合适超参数可大幅提升检索增强生成系统的准确性与速度。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了超参数如何影响检索增强生成（RAG）系统的速度与质量，涉及Chroma和Faiss向量存储、分块策略、交叉编码重排序及温度等。实验评估了六个指标，发现Chroma查询处理更快，而Faiss具有更高的检索精度，存在明显的速度-准确性权衡。固定长度分块优于语义分割，但重新排序虽提升了检索质量却增加了五倍运行时间。最终通过纠正性RAG工作流验证，最佳配置在迭代请求证据时仍保持优势，达到了近乎完美的上下文精确度（99%），表明恰当的超参数组合可极大提升医疗等领域下游任务表现的关键性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 07:13:27 GMT</pubDate>
</item>
<item>
<title>开源模型AM-Thinking-v1：32B规模下的推理能力新标杆</title>
<link>https://arxiv.org/abs/2505.08311</link>
<guid>https://arxiv.org/abs/2505.08311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AM-Thinking-v1在多项数学和代码评测中超越同类开源模型。</p><br /><br /><p><strong>摘要：</strong> AM-Thinking-v1是一款基于开源Qwen2.5-32B基模型构建的大型密集语言模型，通过精心设计的后训练管道结合监督微调和强化学习技术，实现了卓越的推理能力。该模型在AIME 2024、AIME 2025以及LiveCodeBench等测试中分别获得85.3、74.4和70.3的高分，表现出色，与顶级混合专家模型如Qwen3-235B-A2B和Seed1.5-Thinking相媲美。此外，AM-Thinking-v1完全由开源资源开发，展示了开放社区在32B规模上的强大潜力，强调了高性能与实际可用性之间的平衡。本项目已托管于Hugging Face平台，旨在激励更多协作努力以推动中型模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 03:41:15 GMT</pubDate>
</item>
<item>
<title>基于对抗相对对比学习的文本转音频加速方法</title>
<link>https://arxiv.org/abs/2505.08175</link>
<guid>https://arxiv.org/abs/2505.08175</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的非蒸馏加速算法，显著降低文本转音频模型推理延迟。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Adversarial Relativistic-Contrastive（ARC）后训练的创新方法，该方法首次针对扩散/流模型提出了非蒸馏加速技术。传统蒸馏方法成本较高，而ARC通过引入相对论性对抗框架并结合对比鉴别器目标，有效提升了生成音频对文本提示的依从性。实验表明，当与Stable Audio Open结合优化后，所构建的模型在H100硬件上可于约75毫秒内生成12秒的44.1kHz立体声音频，在移动边缘设备上也可达到约7秒的生成速度，成为目前最快的文本转音频系统之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08175" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 22:25:47 GMT</pubDate>
</item>
<item>
<title>跨模态模型融合：将语言模型推理能力融入视觉语言模型</title>
<link>https://arxiv.org/abs/2505.05464</link>
<guid>https://arxiv.org/abs/2505.05464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过跨模态模型融合实现视觉语言模型获取语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探索了如何通过参数连接不同模型来整合感知与推理能力。不同于以往仅限于同类模型融合的工作，我们提出了跨模态模型融合方法，使语言模型的推理能力得以融入视觉语言模型，且无需额外训练。实验表明，这种融合方式成功实现了推理能力的迁移。此外，我们利用合并后的模型分析了感知与推理在模型中的分布情况，发现感知主要集中在早期层，而推理则由中间到晚期层主导。融合后，所有层均开始参与推理，但感知能力的分布基本保持不变。这些发现揭示了模型融合作为多模态集成与解释工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>基于Transformer的阿拉伯语反向词典系统研究</title>
<link>https://arxiv.org/abs/2504.21475</link>
<guid>https://arxiv.org/abs/2504.21475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开发了一种基于Transformer的新型阿拉伯语反向词典系统。</p><br /><br /><p><strong>摘要：</strong> 本研究针对阿拉伯自然语言处理领域的空白，提出了一种基于Transformer的创新性半编码神经网络架构，用于构建高效的阿拉伯语反向词典系统。该系统通过几何递减层实现对词语描述或意义的查询功能，显著提升了阿拉伯语反向词典任务的表现。实验表明，特定领域的预训练模型优于通用多语言嵌入模型，其中ARBERTv2表现最佳。此外，研究还提出了逆向词典任务的正式抽象，并开发了一个可配置训练管道的Python库(RDTL)，为阿拉伯语定义构建提供了重要的质量标准。这项工作对阿拉伯计算语言学具有重要意义，也为阿拉伯语学习、学术写作及专业交流提供了实用工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 05:56:36 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的机器人导航策略NavDP</title>
<link>https://arxiv.org/abs/2505.08712</link>
<guid>https://arxiv.org/abs/2505.08712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端的仿真训练导航方法NavDP，实现跨多种真实环境的零样本迁移。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Navigation Diffusion Policy (NavDP) 的端到端框架，该框架仅通过仿真训练即可实现对不同真实环境中的多种机器人形态的零样本迁移。NavDP结合了基于扩散的轨迹生成和轨迹选择的批评函数，这些均基于共享策略Transformer编码的局部观测标记。通过利用仿真环境中全局环境的特权信息，我们扩展了高质量演示的规模以训练扩散策略，并通过对比负样本制定批评值函数目标。此方法每天可生成约2500条轨迹/GPU，效率比真实世界数据收集高20倍，生成了包含1244个场景、总长度达363.2公里的大规模导航数据集。在四足、轮式和人形机器人上的实验表明，NavDP在室内和室外多样化环境中表现出最先进的性能和一致性卓越的泛化能力。此外，我们初步尝试使用Gaussian Splatting进行域内真实到仿真微调，实验显示添加此类数据可以将成功率提高30%，而不会损害其泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 12:20:28 GMT</pubDate>
</item>
<item>
<title>SkillFormer：基于多视角融合的技能评估高效架构</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种用于多视角技能评估的参数高效架构SkillFormer。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkillFormer的新架构，该架构旨在通过融合第一人称和第三人称视频，实现复杂活动中的人类技能水平统一评估。SkillFormer基于TimeSformer模型构建，引入了CrossViewFusion模块，利用多头交叉注意力、可学习门控机制及自适应校准技术来融合特定视角的特征。通过低秩适应方法进行微调，仅需调整少量参数即可显著降低训练成本。实验表明，在EgoExo4D数据集上的多视角设置中，SkillFormer达到了最先进的准确性，同时表现出卓越的计算效率，所需参数量仅为先前基线的1/4.5，训练周期减少至3.75倍。SkillFormer在多个结构化任务中的表现证明了多视角整合在细粒度技能评估中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08665" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 11:27:24 GMT</pubDate>
</item>
<item>
<title>MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder</title>
<link>https://arxiv.org/abs/2505.07916</link>
<guid>https://arxiv.org/abs/2505.07916</guid>
<content:encoded><![CDATA[
We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:25:20 GMT</pubDate>
</item>
<item>
<title>多维度约束框架提升大语言模型指令跟随能力</title>
<link>https://arxiv.org/abs/2505.07591</link>
<guid>https://arxiv.org/abs/2505.07591</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多维度约束框架并生成新测试样本，评估多种大语言模型性能差异。</p><br /><br /><p><strong>摘要：</strong> 现有指令跟随基准测试常依赖模板约束提示，缺乏真实场景多样性且限制精细评估。本研究构建了一个包含三种约束模式、四种约束类别及四个难度等级的多维度约束框架，并开发自动化指令生成流水线，生成了1200个可代码验证的指令跟随测试样本。通过对七大家族19个大语言模型的评估发现，不同约束形式下模型表现差异显著，例如平均表现从一级的77.67%下降到四级的32.96%。此外，利用此方法生成强化学习数据，不仅提升了指令跟随能力，还避免了对整体性能的影响，深入分析表明改进主要源于注意力模块参数调整以增强约束识别。相关代码和数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07591" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:16:55 GMT</pubDate>
</item>
<item>
<title>ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation</title>
<link>https://arxiv.org/abs/2505.07416</link>
<guid>https://arxiv.org/abs/2505.07416</guid>
<content:encoded><![CDATA[
Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP
]]></content:encoded>
<pubDate>Mon, 12 May 2025 06:11:28 GMT</pubDate>
</item>
<item>
<title>gg-bench：评估语言模型通用推理能力的游戏基准</title>
<link>https://arxiv.org/abs/2505.07215</link>
<guid>https://arxiv.org/abs/2505.07215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">gg-bench是一种动态生成游戏环境的基准，用于评估语言模型的通用推理能力。</p><br /><br /><p><strong>摘要：</strong> gg-bench是一个创新性的游戏环境集合，旨在评估大型语言模型（LLMs）的通用推理能力。与静态基准不同，gg-bench通过大语言模型生成新的游戏描述和代码实现，然后利用强化学习代理进行自我训练。该基准挑战性强，目前最先进的LLMs在其中的表现仅为7-9%，而专门设计的推理模型也仅达到31-36%的胜率。我们公开了生成的游戏、数据生成过程及评估代码，以支持未来的研究工作和基准扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:01:03 GMT</pubDate>
</item>
<item>
<title>INTELLECT-2：首个全球分布式强化学习语言模型训练</title>
<link>https://arxiv.org/abs/2505.07291</link>
<guid>https://arxiv.org/abs/2505.07291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">INTELLECT-2实现首个320亿参数语言模型的全球分布式强化学习训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了INTELLECT-2，这是首个在全球范围内进行的分布式异步强化学习训练，用于一个拥有320亿参数的语言模型。不同于传统的集中式训练方法，INTELLECT-2通过完全异步的方式，在动态且异构的去中心化计算贡献者群组中训练推理模型。为了支持这种独特的基础设施，我们从零开始构建了多个组件，例如PRIME-RL框架，以及用于验证非可信推理工作者的TOPLOC和高效广播策略权重的SHARDCAST。此外，我们还对标准GRPO训练方案进行了修改，并采用了数据过滤技术，以确保训练稳定性并成功达成训练目标，从而超越了现有320亿参数范围内的最佳推理模型QwQ-32B。最后，我们将INTELLECT-2及其所有代码和数据开源，希望推动去中心化训练领域的开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 03:24:33 GMT</pubDate>
</item>
<item>
<title>LlamaPIE：首款实时主动对话助手提升人类交流体验</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LlamaPIE是一款无需用户明确调用即可提供实时背景辅助的新型对话助手。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LlamaPIE，这是首个旨在通过可穿戴设备提供隐性且简洁指导以增强人类对话的实时主动对话助手。与传统需要用户明确请求的语言模型不同，LlamaPIE能在不打断对话的情况下预测并满足用户需求。为了实现这一目标，我们解决了几个关键挑战，如确定何时响应、生成增强对话的精炼回复、利用用户知识进行上下文感知的辅助以及实现实时的本地处理。为此，我们构建了一个半合成对话数据集，并提出了一个双模型管道：一个小模型用于决定何时响应，一个大模型负责生成回复。我们在真实世界的数据集上评估了该方法，证明其能有效提供有益且不显眼的帮助。用户研究显示，LlamaPIE相较于无辅助基线和反应型模型更受青睐，展示了其在提升实时对话中的潜力。关键词：对话助手、实时辅助、上下文感知。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 22:08:56 GMT</pubDate>
</item>
<item>
<title>Triply-Hierarchical Diffusion Policy: 强化机器人视觉-动作学习的层级结构方法</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型的层级扩散策略，显著提升机器人视觉-动作学习性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Triply-Hierarchical Diffusion Policy（H^3DP）的新框架，该框架通过引入三层层级结构加强视觉特征与动作生成之间的整合。具体而言，H^3DP包含深度感知输入分层、多尺度视觉表示以及层级条件扩散过程。实验表明，在44个模拟任务中，H^3DP相比基线平均提升了27.5%，并在四个具有挑战性的双臂现实世界操作任务中表现出色。这项研究为机器人视觉-动作学习提供了新的思路和技术支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title>无需向量量化：连续视觉自回归生成框架</title>
<link>https://arxiv.org/abs/2505.07812</link>
<guid>https://arxiv.org/abs/2505.07812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需向量量化即可进行连续视觉自回归生成的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统视觉自回归模型在处理连续模态（如视觉数据）时通常依赖于基于量化的方法而导致信息损失的问题，引入了一种新的连续视觉自回归（Continuous VAR）框架。该框架利用严格适当评分规则作为理论基础，通过选择适当的评分规则并将其设为训练目标，实现了直接的连续视觉自回归生成，而无需进行向量量化操作。文中特别探讨了基于能量评分的训练目标，该目标无需显式计算似然性，从而解决了在连续空间中进行概率预测的难题。此外，其他方法如GIVT和扩散损失也可以通过本框架中的其他严格适当评分规则推导得出。这一新框架为连续模态数据的生成建模提供了更加灵活和高效的选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:58:14 GMT</pubDate>
</item>
<item>
<title>Skywork-VL Reward：一种多模态奖励模型</title>
<link>https://arxiv.org/abs/2505.07263</link>
<guid>https://arxiv.org/abs/2505.07263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种用于多模态理解和推理任务的奖励模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Skywork-VL Reward的多模态奖励模型，该模型能够为多模态理解和推理任务提供奖励信号。技术方法包括构建大规模多模态偏好数据集，以及基于Qwen2.5-VL-7B-Instruct设计奖励模型架构并进行多阶段微调。实验结果显示，Skywork-VL Reward在多模态VL-RewardBench上达到最先进水平，并在文本-only RewardBench基准上表现优异。此外，基于此模型构建的偏好数据对混合偏好优化训练非常有效，显著提升了多模态推理能力。研究强调了Skywork-VL Reward作为通用可靠奖励模型的重要进展，并已公开发布以促进透明度和可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 02:23:08 GMT</pubDate>
</item>
<item>
<title>UMoE：通过统一设计提升Transformer模型的稀疏混合专家性能</title>
<link>https://arxiv.org/abs/2505.07260</link>
<guid>https://arxiv.org/abs/2505.07260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出UMoE架构，通过重新定义注意力机制实现FFN与注意力模块的参数共享。</p><br /><br /><p><strong>摘要：</strong> 近年来，稀疏混合专家（MoE）架构被广泛应用于扩展Transformer模型，尤其是在前馈网络（FFN）层的应用取得了显著成效。然而，基于注意力机制的MoE层因需要特殊实现且性能不如FFN层而面临挑战。本研究通过重新构建注意力机制，揭示了注意力模块中隐藏的类似FFN结构，并提出UMoE架构，在保持注意力机制优势的同时实现了高效参数共享，从而在性能上超越了传统的FFN-MoE方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 02:21:44 GMT</pubDate>
</item>
<item>
<title>DynamicRAG：通过强化学习优化检索增强生成模型</title>
<link>https://arxiv.org/abs/2505.07233</link>
<guid>https://arxiv.org/abs/2505.07233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态调整文档数量和顺序的RAG框架，显著提升知识密集型任务性能。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）系统结合大型语言模型（LLM）与外部知识检索，在知识密集型任务中表现出色，但其中重排序器组件的重要性常被忽视。现有方法在确定最佳文档数量（k）时面临挑战，且大多依赖模型内部知识而忽略LLM提供的丰富监督信号。本文提出DynamicRAG框架，通过强化学习优化重排序器，使其根据查询动态调整文档数量和顺序。实验结果显示，DynamicRAG在七个知识密集型数据集上达到最优性能，刷新多项记录。该研究为RAG系统的改进提供了新思路，同时代码与模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 01:19:01 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的照片润色方法</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用多模态大语言模型进行照片润色的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过多模态大语言模型（MLLM）对原始照片进行批评、提出修复建议并执行操作，以实现高质量的照片润色。与传统的基于文本或笔画的生成编辑工具相比，这种方法更加保守且可预测，同时保留了对象细节和分辨率。研究者通过训练MLLM解决专门设计的视觉难题，使其具备图像处理操作意识，并结合专家编辑的数据集合成推理数据集，用于微调。实验表明，该方法在解释性和身份保存方面优于现有的生成式和其他程序化替代方案。代码、数据、模型及补充结果可在项目网站获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 12:38:27 GMT</pubDate>
</item>
<item>
<title>PASSAT：一种融合物理与地形信息的天气预测深度学习模型</title>
<link>https://arxiv.org/abs/2505.04918</link>
<guid>https://arxiv.org/abs/2505.04918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PASSAT模型，结合物理方程和地球表面拓扑优化天气预测。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度学习天气预测模型忽视物理过程或地形信息的问题，开发了一种名为PASSAT的新模型。PASSAT通过求解对流方程和纳维-斯托克斯方程描述天气演化中的对流过程，同时利用球形图神经网络捕捉地球-大气相互作用，并考虑地球表面的真实拓扑结构而非平面假设。实验结果显示，在ERA5数据集上，PASSAT的表现优于最先进的深度学习模型及传统数值天气预报模型IFS T42。代码和模型参数已在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 23:25:55 GMT</pubDate>
</item>
<item>
<title>生成式人工智能评估中的危机与竞赛标准化</title>
<link>https://arxiv.org/abs/2505.00612</link>
<guid>https://arxiv.org/abs/2505.00612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">传统机器学习评估策略无法满足现代生成式AI模型评估需求。</p><br /><br /><p><strong>摘要：</strong> 本文指出生成式人工智能（GenAI）的评估正面临严重危机，因为传统的机器学习评估和基准测试策略不足以应对现代GenAI模型的需求。这些问题源于模型输入输出空间几乎无限、缺乏明确的ground truth目标，以及强反馈循环和上下文依赖性等特性。此外，我们强调泄漏（leakage）和污染（contamination）是GenAI评估中最重要且难以解决的问题。有趣的是，AI竞赛领域已发展出有效措施对抗泄漏，以防止恶意参与者作弊。因此，AI竞赛可视为生成式AI评估的黄金标准，值得更广泛的应用和借鉴。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 11:43:51 GMT</pubDate>
</item>
<item>
<title>DanceGRPO：首个统一强化学习框架实现视觉生成多领域适配</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DanceGRPO首次将GRPO优化算法应用于视觉生成，显著提升多种模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DanceGRPO，这是一种全新的强化学习框架，旨在解决现有基于强化学习的视觉生成方法中的关键问题，如与现代采样范式的兼容性不足、大规模训练的不稳定性以及视频生成验证缺乏等。DanceGRPO首次实现了在两种生成范式（扩散模型和修正流）、三种任务（文本到图像、文本到视频、图像到视频）、四种基础模型（Stable Diffusion、HunyuanVideo、FLUX、SkyReel-I2V）以及五种奖励模型（图像/视频美学、文本-图像对齐、视频运动质量及二元反馈）上的统一适配。实验表明，该框架在多个基准测试（如HPS-v2.1、CLIP Score、VideoAlign、GenEval）中较基线提升了高达181%，不仅增强了复杂视频生成的策略优化稳定性，还改善了去噪轨迹捕捉及从稀疏反馈中学习的能力。DanceGRPO标志着在视觉生成领域的强化学习从人类反馈（RLHF）任务上取得了重大进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>连续预训练中的学习动态及扩展定律研究</title>
<link>https://arxiv.org/abs/2505.07796</link>
<guid>https://arxiv.org/abs/2505.07796</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索大语言模型在连续预训练中的学习动态及其扩展定律。</p><br /><br /><p><strong>摘要：</strong> 本文研究了连续预训练（CPT）过程中大型语言模型的学习动态，重点关注每一步中通用性能和特定领域性能的变化，通过验证损失衡量领域性能。我们发现CPT损失曲线本质上描述了从一个曲线到另一个隐藏曲线的转变，可以通过解耦分布偏移和学习率退火的影响来描述。由此推导出的CPT扩展定律结合了这两个因素，能够预测任何连续训练步骤下的损失值，并适用于不同的学习率调度。我们的公式全面解释了CPT中的多个关键因素，如损失潜力、峰值学习率、训练步数、回放比例等。此外，该方法还可根据不同的CPT目标定制超参数。大量实验表明，该扩展定律适用于多种CPT数据集和超参数设置。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07796" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:47:32 GMT</pubDate>
</item>
<item>
<title>基于分块推理的LLMs长上下文处理效率提升研究</title>
<link>https://arxiv.org/abs/2505.07793</link>
<guid>https://arxiv.org/abs/2505.07793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示分块推理方法可显著提高多种长上下文模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于近期大型语言模型（LLMs）中发展出的递归次二次模型，这些模型旨在提高长上下文处理效率。我们对主要的大规模长上下文模型进行了实验分析，重点关注其固定大小的递归记忆对其表现的影响。结果显示，尽管这些模型经过长时间上下文训练，但其长上下文的实际利用率仍较低。通过采用基于分块的推理程序，即仅识别并处理输入中最相关的部分，可以有效缓解递归记忆失效问题，并在许多长上下文任务中表现出色。具体而言，在LongBench基准测试中，我们的方法使Falcon3-Mamba-Inst-7B的总体性能提高了14%，Falcon-Mamba-Inst-7B提升了28%，RecurrentGemma-IT-9B提升了50%，RWKV6-Finch-7B提升了51%。令人惊讶的是，这一简单策略还在极具挑战性的LongBench v2基准测试中取得了最先进的成果，展示了与同等规模Transformer模型相媲美的竞争力。此外，我们的研究还引发了关于递归模型是否真正利用了长距离依赖关系的思考，因为单一分块策略的表现甚至优于一些需要跨上下文关系的任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:45:05 GMT</pubDate>
</item>
<item>
<title>通过同伴学习解决大型推理模型的前缀主导陷阱</title>
<link>https://arxiv.org/abs/2505.07787</link>
<guid>https://arxiv.org/abs/2505.07787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Learning from Peers方法提升大型推理模型的错误修正能力。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型(LRMs)虽能自我纠正错误，但若推理初始路径不佳则难以恢复，此现象称为“前缀主导陷阱”。受心理学启发，我们提出了Learning from Peers(LeaP)，让推理路径通过路由机制共享中间结果，从而整合同伴见解。实验表明，LeaP显著提升了多种数学基准测试的表现，例如QwQ-32B平均高出基线近5个百分点。此外，经过微调的LeaP-T-7B在AIME 2024上表现与更大规模的DeepSeek模型相当。深入分析显示，LeaP通过及时的同伴见解实现稳健的错误修正，表现出强大的容错能力和处理任务难度差异的能力。LeaP标志着大型推理模型在推理过程中协作的重要里程碑，相关代码、数据集和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:39:56 GMT</pubDate>
</item>
<item>
<title>Step1X-3D：推动可控3D资产生成的开源框架</title>
<link>https://arxiv.org/abs/2505.07747</link>
<guid>https://arxiv.org/abs/2505.07747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Step1X-3D框架解决3D生成挑战，提升生成质量和跨模态一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Step1X-3D的开源框架，旨在应对3D生成领域的数据稀缺、算法局限和生态系统碎片化等问题。该框架通过三个主要方面实现突破：首先，构建了一个包含超过2百万高质量资产的数据集，通过标准化几何和纹理属性提高数据质量；其次，采用两阶段的3D专用架构，结合混合VAE-DiT几何生成器和基于扩散的纹理合成模块，显著提升生成效果；最后，提供模型、训练代码及适应模块的完全开源支持。实验结果显示，Step1X-3D在几何生成和纹理合成上均表现出色，超越现有开源方法，并接近商业解决方案的质量。此外，该框架还首次实现了2D控制技术（如LoRA）向3D生成的直接迁移，推动了开放研究标准的建立。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 12:56:30 GMT</pubDate>
</item>
<item>
<title>MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining</title>
<link>https://arxiv.org/abs/2505.07608</link>
<guid>https://arxiv.org/abs/2505.07608</guid>
<content:encoded><![CDATA[
We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:30:11 GMT</pubDate>
</item>
<item>
<title>基于强化学习的知识协同推理代理IKEA提升大语言模型性能</title>
<link>https://arxiv.org/abs/2505.07596</link>
<guid>https://arxiv.org/abs/2505.07596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入知识协同推理代理IKEA减少大语言模型幻觉并优化知识检索。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）中常见的幻觉问题可通过检索增强生成（RAG）策略缓解，但现有方法常未能充分利用内部知识，导致冗余检索和推理延迟。本文提出了一种名为IKEA的高效自适应搜索代理，通过识别自身知识边界优先利用内部知识，并仅在必要时求助外部检索。这一改进通过知识边界感知奖励函数和训练数据实现，显著提升了多领域知识推理任务的表现，降低了检索频率并增强了泛化能力，超越了现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:21:57 GMT</pubDate>
</item>
<item>
<title>Unified Continuous Generative Models</title>
<link>https://arxiv.org/abs/2505.07447</link>
<guid>https://arxiv.org/abs/2505.07447</guid>
<content:encoded><![CDATA[
Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 07:15:39 GMT</pubDate>
</item>
<item>
<title>基于注意力影响机制的弱监督大规模推理数据选择方法</title>
<link>https://arxiv.org/abs/2505.07293</link>
<guid>https://arxiv.org/abs/2505.07293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需监督信号的训练自由方法提升大模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于通过收集推理密集型预训练数据来增强大型语言模型（LLMs）的复杂推理能力。传统方法依赖于有监督分类器进行数据标注，容易引入领域特定偏差。鉴于注意力头对上下文推理的重要性，我们提出了AttentionInfluence方法，这是一种无需训练且高效的无监督数据选择策略。该方法利用小型预训练语言模型通过简单的注意力头屏蔽操作充当强大的数据选择器。实验中，我们在SmolLM语料库上应用此方法，结合选定的73B令牌子集，预训练了一个7B参数的密集模型，显著提升了多个知识密集型和推理密集型基准测试（如MMLU、AGIEval-en等）的表现，增幅在1.4pp至3.5pp之间。这验证了从弱到强的有效扩展特性，为推理导向的数据选择提供了一条有前景且可扩展的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 03:25:51 GMT</pubDate>
</item>
<item>
<title>Multi-Objective-Guided Discrete Flow Matching用于多目标生物序列设计</title>
<link>https://arxiv.org/abs/2505.07086</link>
<guid>https://arxiv.org/abs/2505.07086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架MOG-DFM，用于高效生成满足多重生物功能需求的生物序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) 的通用框架，该框架能够引导任何预训练的离散时间流匹配生成器实现多个标量目标之间的帕累托最优折中。现有方法仅处理单一目标或需要连续嵌入，可能导致离散分布失真，而MOG-DFM通过计算候选转换的混合排名方向得分并应用自适应超锥过滤器来确保一致的多目标进展。此外，还训练了两个无条件离散流匹配模型——PepDFM用于多样肽生成，EnhancerDFM用于功能性增强子DNA生成，作为MOG-DFM的基础生成模型。实验表明，MOG-DFM在优化五种属性（溶血性、非污染性、溶解度、半衰期和结合亲和力）的肽结合剂生成以及设计具有特定增强子类别和DNA形状的DNA序列方面表现出色，证明了其作为多属性导向生物分子序列设计的强大工具的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 14:17:44 GMT</pubDate>
</item>
<item>
<title>Seed1.5-VL：高性能多模态基础模型</title>
<link>https://arxiv.org/abs/2505.07062</link>
<guid>https://arxiv.org/abs/2505.07062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed1.5-VL在视觉语言任务中表现优异，尤其在多模态推理任务中表现突出。</p><br /><br /><p><strong>摘要：</strong> Seed1.5-VL是一种由532M参数视觉编码器和20B参数的混合专家(MoE)语言模型组成的多模态基础模型。尽管架构紧凑，它在多种公开视觉语言模型(VLM)基准测试和内部评估套件中表现出色，在60个公开基准测试中有38个达到最先进的性能。此外，Seed1.5-VL在基于代理的任务如图形用户界面(GUI)控制和游戏玩法中也优于领先的多模态系统，如OpenAI CUA和Claude 3.7。该模型不仅在视觉和视频理解方面表现出色，还具备强大的推理能力，适用于多模态推理挑战如视觉谜题。这些能力将推动其在多样任务中的广泛应用。本报告详细回顾了我们在构建Seed1.5-VL过程中在模型设计、数据构建和训练方面的经验，希望为后续研究提供灵感。Seed1.5-VL现已通过Volcano Engine平台开放获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 13:28:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型文档归因技术研究</title>
<link>https://arxiv.org/abs/2505.06324</link>
<guid>https://arxiv.org/abs/2505.06324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出两种方法提升大型语言模型文档归因准确性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）被广泛应用于文档相关任务，如摘要生成、问答和信息抽取，确保这些系统的可信度和可解释性成为关键问题。归因作为一种核心方法，旨在将模型输出追溯到源文档。然而，由于LLMs可能产生不准确或不精确的响应，评估引用可靠性至关重要。本文提出了两种技术解决方案：一是零样本方法，将归因视为文本蕴含任务，使用flan-ul2模型在AttributionBench的ID和OOD集合上分别提升了0.27%和2.4%；二是探索注意力机制的作用，在使用较小模型flan-t5-small时，F1分数在几乎所有层面上均优于基线，仅在第4层及第8至11层稍逊。通过这些方法，我们期望提高归因过程的可靠性和模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:40:11 GMT</pubDate>
</item>
<item>
<title>基于强化学习的开源小规模LLM指令数据生成框架</title>
<link>https://arxiv.org/abs/2505.06548</link>
<guid>https://arxiv.org/abs/2505.06548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索开源小规模LLM结合半自动化框架生成指令数据并利用强化学习提升性能。</p><br /><br /><p><strong>摘要：</strong> 指令驱动的大语言模型（LLMs）在许多少样本或零样本自然语言处理任务中表现出色，但人工标注指令数据耗时且成本高昂。先前研究通过任务不可知的方式从模型本身自动生成指令，但多依赖昂贵的API模型。本文考察三个开源小规模LLM（如LLaMA 2-7B、LLaMA 2-13B和Mistral 7B），采用半自动化框架减少人力干预和成本，用于微调LLM的指令数据集生成。此外，将基于强化学习（RL）的训练算法融入该框架，进一步提升了性能。实验结果显示，这种基于RL的框架在63%-66%的任务中较以往方法有显著改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 10 May 2025 03:23:19 GMT</pubDate>
</item>
<item>
<title>WebGen-Bench：评估基于LLM的多文件网站代码生成能力的新基准</title>
<link>https://arxiv.org/abs/2505.03733</link>
<guid>https://arxiv.org/abs/2505.03733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍WebGen-Bench，评估LLM生成多文件网站代码库的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WebGen-Bench的新基准，用于衡量基于大型语言模型（LLM）的代理在从零开始创建多文件网站代码库方面的能力。该基准包含了由人工注释者与GPT-4o共同协作产生的多样化网站生成指令，涵盖三大类别和十三个小类别，几乎涉及所有重要的网络应用程序类型。为了评估生成网站的质量，我们使用GPT-4o生成针对每项功能的测试用例，并通过手动筛选、调整和组织这些测试用例，最终得到了647个测试用例。每个测试用例指定了对网站执行的操作及其预期结果。我们还构建了WebGen-Instruct，这是一个包含6,667个网站生成指令的数据集。训练Qwen2.5-Coder-32B-Instruct模型在从该数据集子集生成的轨迹上，达到了38.2%的准确性，超过了最好的专有模型。此外，我们评估了三个高性能代码代理框架：Bolt.diy、OpenHands和Aider，使用多个专有和开源LLM作为引擎。其中表现最佳的组合——由DeepSeek-R1驱动的Bolt.diy，在测试用例上的准确率仅为27.8%，凸显了WebGen-Bench的挑战性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:15 GMT</pubDate>
</item>
<item>
<title>POLAR：高效多视角点云刚性配准方法</title>
<link>https://arxiv.org/abs/2504.21467</link>
<guid>https://arxiv.org/abs/2504.21467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于潜在空间的多视角点云配准方法，显著提升大变换和高退化情况下的性能。</p><br /><br /><p><strong>摘要：</strong> 点云刚性配准是三维计算机视觉中的基础问题，在多视角情况下，传统基于两两配准的方法因同步算法限制而扩展性差，而生成模型虽能克服此问题但难以处理大变换。本文提出POLAR（POint cloud LAtent Registration），通过将配准问题映射到预训练自动编码器的潜在空间，设计考虑退化的损失函数并采用高效的多起点优化策略，实现了对大量视图的高效处理且具备高鲁棒性。实验表明，POLAR在合成数据和真实数据上均优于现有技术。该方法开源且可直接安装使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 05:42:38 GMT</pubDate>
</item>
<item>
<title>GPT-4o在图像修复领域的潜力与挑战</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPT-4o在图像生成方面表现优异，但修复任务中存在像素级结构失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统评估了GPT-4o模型在多种图像修复任务中的表现。实验表明，尽管GPT-4o生成的图像视觉效果良好，但在像素级结构保真度上往往逊色于真实图像，常见问题是比例变化、物体位置及数量偏移以及视角改变。针对这些问题，通过去雾、去雨和低光照增强三个案例研究，我们发现GPT-4o的输出可以作为强大的视觉先验，显著提升现有去雾网络的性能。此外，我们提供了实用指南和基准框架，以促进未来将GPT-4o集成到图像修复流程中。希望这项研究能推动图像生成领域的发展，同时我们将公开来自超过10个常用图像修复数据集的GPT-4o修复图像以支持后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 16:00:11 GMT</pubDate>
</item>
<item>
<title>UniVLA：一种用于跨形态机器人视觉-语言-动作学习的新框架</title>
<link>https://arxiv.org/abs/2505.06111</link>
<guid>https://arxiv.org/abs/2505.06111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniVLA通过任务驱动的动作表示和语言指令，实现高效跨形态机器人策略学习。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UniVLA的新框架，旨在解决现有机器人策略学习方法依赖标注数据且难以跨形态和环境迁移的问题。UniVLA通过引入潜在动作模型从视频中推导任务导向的动作表示，并结合语言指令在DINO特征空间中建立模型，从而有效利用多样化数据源。该框架在多个操作和导航基准测试及真实机器人部署中取得了最先进的成果，相比OpenVLA显著减少了预训练计算量和下游数据需求。此外，随着更多异构数据的加入，UniVLA的性能持续提升，展示了其在规模化和高效机器人策略学习中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 11:11:13 GMT</pubDate>
</item>
<item>
<title>大型语言模型在英国公共卫生信息领域的知识评估</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了多个大型语言模型对英国公共卫生信息的知识水平。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的广泛普及，了解其在特定领域内的知识变得至关重要，尤其是在公共卫生领域，获取相关、准确且最新的信息直接影响公众健康。本文介绍了PubHealthBench，这是一个包含超过8000个问题的新基准，用于评估LLMs在处理公共卫生查询时的多项选择题回答能力和自由形式响应能力。通过自动化管道创建的问题集基于从英国政府提取的公共卫生指导文件。对24个LLMs进行测试后发现，最新的私有LLMs（如GPT-4.5、GPT-4.1和o1）在多项选择题设置中的表现优异，超过90%，甚至超越了人类用户的基本搜索引擎使用。然而，在自由形式响应设置中，所有模型的表现均低于75%。因此，尽管最先进的LLMs在提供公共卫生信息方面显示出较高的准确性，但在自由形式响应时仍需额外的安全措施或工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 09:42:59 GMT</pubDate>
</item>
<item>
<title>WiserUI-Bench与G-FOCUS：提升UI设计说服力评估的创新方法</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WiserUI-Bench基准和G-FOCUS策略，改进基于视觉语言模型的UI设计说服力评估。</p><br /><br /><p><strong>摘要：</strong> 用户界面(UI)设计的有效性不仅关乎美学，还影响用户行为，这是设计说服力的核心原则。A/B测试虽是确定UI变体对用户参与度影响的主要方法，但成本高且耗时。尽管最近的视觉语言模型(VLMs)可以处理自动化UI分析，但现有方法主要关注孤立的设计属性，而非关键的说服力比较。为解决这一问题，我们引入WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对实际UI图像样本及其A/B测试结果和专家解释。此外，我们提出了G-FOCUS，一种新的推理策略，通过减少位置偏差并提高评估准确性增强基于VLM的说服力评估。实验结果显示，G-FOCUS在成对UI评估的一致性和准确性上优于现有方法。这项工作通过促进基于VLM的UI说服力评估，为补充A/B测试提供了途径，推动了可扩展的UI偏好建模和设计优化的进步。代码和数据将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:00:32 GMT</pubDate>
</item>
<item>
<title>Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02686</link>
<guid>https://arxiv.org/abs/2505.02686</guid>
<content:encoded><![CDATA[
Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 10:33:49 GMT</pubDate>
</item>
<item>
<title>Bielik v3：优化波兰语处理的高效生成式文本模型</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik v3推出两个参数量分别为1.5B和4.5B的模型，性能媲美更大规模模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik v3系列的两种参数高效生成式文本模型（1.5B和4.5B），专门针对波兰语言处理进行了优化。这些模型通过一系列创新技术实现高性能，例如定制化的波兰语分词器（APT4）、加权指令交叉熵损失函数以及自适应学习率策略，显著提升了训练效率和模型效果。模型基于精心筛选的2920亿标记的语料库进行训练，在多个基准测试中表现出色，如Open PL LLM Leaderboard、复杂波兰文理解基准、Polish EQ-Bench和波兰医学领导力榜单等。其中4.5B参数模型的表现可与规模为其2至3倍的模型相媲美，而1.5B参数模型则在紧凑架构下展现出强劲的性能。这一成果为资源受限的应用场景提供了高质量波兰语AI模型的新标杆。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 06:39:51 GMT</pubDate>
</item>
<item>
<title>Bielik 11B v2：面向波兰语处理的高效语言模型</title>
<link>https://arxiv.org/abs/2505.02410</link>
<guid>https://arxiv.org/abs/2505.02410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik 11B v2展示了卓越的波兰语处理能力，同时具备强大的跨语言功能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik 11B v2，这是一种基于Mistral 7B v0.2架构优化的波兰语语言模型，参数规模达到11B，通过深度扩展实现性能提升。该模型引入了加权指令交叉熵损失函数和自适应学习率两项关键技术，显著提升了多语言任务的表现，尤其是在波兰语相关任务中的表现超越了许多更大参数量的模型。此外，其高效的参数利用和多种量化选项使其能够在不同硬件配置上部署，为资源有限的语言建模提供了新的基准。Bielik 11B v2在多项波兰语基准测试中表现出色，涵盖了从语言理解到复杂推理的各种任务，成为少有代表性语言领域的一项重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 03:03:41 GMT</pubDate>
</item>
<item>
<title>基于鲁棒文本水印的大语言模型高效遗忘评估方法</title>
<link>https://arxiv.org/abs/2505.05064</link>
<guid>https://arxiv.org/abs/2505.05064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个面向大语言模型的数据驱动遗忘度量方法WaterDrum。</p><br /><br /><p><strong>摘要：</strong> 现有基于模型效用的遗忘度量方法在实际应用中存在局限性，如忘记集和保留集内容语义相似、重新训练模型不可行等情况下无法准确评估遗忘效果。本文提出了首个面向大语言模型的数据驱动遗忘度量方法WaterDrum，利用鲁棒文本水印技术克服这些限制。此外，我们还引入了新的基准数据集，用于严格评估遗忘算法，并提供了代码和数据集的公开访问链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:56:46 GMT</pubDate>
</item>
<item>
<title>RL^V：强化学习中引入验证能力提升LLM推理性能</title>
<link>https://arxiv.org/abs/2505.04842</link>
<guid>https://arxiv.org/abs/2505.04842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL^V通过联合训练LLM作为推理器和生成式验证器，显著提高数学问题求解精度并优化计算效率。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLM）推理微调的主流强化学习方法（如GRPO或Leave-one-out PPO），倾向于放弃已学得的价值函数，转而依赖经验估计回报，这限制了测试阶段基于价值函数进行验证的计算扩展性。本文提出了一种名为RL^V的新方法，该方法通过使用强化学习生成的数据，同时训练LLM作为推理器和生成式验证器，从而在不增加显著开销的情况下赋予模型验证能力。实验表明，RL^V使MATH数据集的准确率提升了超过20%，并且相比基础强化学习方法，在测试阶段的并行计算效率提高了8到32倍。此外，RL^V在易难任务迁移及域外任务上均展现出强大的泛化能力，同时在联合扩展并行与顺序测试时间计算时，相较于原始R1模型取得了1.2至1.6倍的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 18:41:26 GMT</pubDate>
</item>
<item>
<title>视觉-语言-行动模型综述：架构创新与未来展望</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述了近3年视觉-语言-行动模型的最新进展及其应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了近年来视觉-语言-行动（VLA）模型的研究进展，涵盖五大主题支柱，如从跨模态学习到整合视觉语言模型、动作规划器和分层控制器的通用代理。通过分析80余篇相关论文，重点讨论了架构创新、高效参数训练及实时推理加速等进步领域，并探讨了人形机器人、自动驾驶、医疗工业机器人等多样化应用场景。同时，文章还针对实时控制、多模态动作表示等挑战提出解决方案，并展望了VLA模型与具身人工智能融合推动社会对齐的智能通用体的未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 15:46:43 GMT</pubDate>
</item>
<item>
<title>结合策略优化语言模型对齐：on-policy与off-policy数据的互补优势</title>
<link>https://arxiv.org/abs/2505.02363</link>
<guid>https://arxiv.org/abs/2505.02363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现on-policy和off-policy数据在偏好优化中具有互补性，提出SIMPLEMIX方法显著提升语言模型对齐效果。</p><br /><br /><p><strong>摘要：</strong> 语言模型对齐依赖于成对偏好数据集，然而关于on-policy数据与off-policy数据在偏好学习中的优劣关系存在争议。本研究通过系统分析指出，on-policy数据在推理任务如数学和编程中表现优异，而off-policy数据在开放式任务如创意写作和个人推荐方面更具优势。基于此，我们提出了SIMPLEMIX方法，通过简单混合两种数据源充分利用其互补特性。实验结果显示，SIMPLEMIX在Alpaca Eval 2.0等多任务基准测试中平均优于单独使用on-policy DPO和off-policy DPO达6.03%，且性能超越了更为复杂的HyPO和DPO-Mix-P方法，平均提高3.05%。这一成果为语言模型的高效对齐提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:54:44 GMT</pubDate>
</item>
<item>
<title>3D场景生成综述：技术进展与未来方向</title>
<link>https://arxiv.org/abs/2505.05474</link>
<guid>https://arxiv.org/abs/2505.05474</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述深度学习在3D场景生成中的最新进展及四大范式。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于深度生成模型（如GANs、扩散模型）和3D表示（如NeRF、3D高斯模型）的3D场景生成技术取得了显著进步，大幅提升了场景的真实感、多样性和视角一致性。本文系统性回顾了当前最先进的方法，将其分为基于规则生成、神经3D生成、基于图像生成和基于视频生成四大范式，分析技术基础、优缺点及代表性成果，并探讨常用数据集、评估协议及下游应用。尽管如此，生成能力、3D表示、数据标注及评估仍面临挑战，未来研究方向包括更高保真度、物理感知交互生成及统一感知生成模型。该综述旨在为3D场景生成领域的研究人员提供全面参考，并通过项目页面持续跟踪最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05474" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>Flow-GRPO：首个结合在线强化学习与流匹配模型的方法</title>
<link>https://arxiv.org/abs/2505.05470</link>
<guid>https://arxiv.org/abs/2505.05470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个将在线强化学习融入流匹配模型的方法Flow-GRPO。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Flow-GRPO的新方法，它是第一个将在线强化学习（RL）与流匹配模型相结合的技术。该方法通过两种策略实现创新：首先，将确定性的常微分方程（ODE）转换为等效的随机微分方程（SDE），使模型在所有时间步上都能匹配原始模型的边缘分布，从而支持统计采样进行RL探索；其次，采用去噪降维策略，在减少训练去噪步骤的同时保持原始推理时间步数，显著提高了采样效率且不降低性能。实验表明，Flow-GRPO在多个文本到图像的任务中表现出色，尤其在复杂组合场景下，经过RL调优的SD3.5模型能够生成几乎完美的对象数量、空间关系及细粒度属性，使得GenEval准确率从63%提升至95%，视觉文本渲染准确率也从59%提高到92%。此外，Flow-GRPO在人类偏好对齐方面取得了显著进步，且未观察到明显的奖励黑客现象，即奖励提升并未牺牲图像质量和多样性，这两者在实验中均保持稳定。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:45 GMT</pubDate>
</item>
<item>
<title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
<link>https://arxiv.org/abs/2505.05469</link>
<guid>https://arxiv.org/abs/2505.05469</guid>
<content:encoded><![CDATA[
We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>StreamBridge：将离线Video-LLMs转化为流式模型的高效框架</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamBridge解决了现有视频语言模型向在线场景适配的两大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamBridge框架，旨在将现有的离线视频大语言模型（Video-LLMs）改造为具备实时流处理能力的模型。该框架通过引入内存缓冲区与轮次衰减压缩策略支持多轮长上下文交互，同时采用解耦轻量级激活模型实现持续的主动响应。此外，为了支持这一框架，我们构建了Stream-IT数据集，专门用于流式视频理解任务。实验表明，StreamBridge显著提升了多种任务下的流式理解性能，甚至超过了GPT-4o和Gemini 1.5 Pro等专有模型，在标准视频理解基准测试中也表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>英语推理能力的跨语言泛化研究</title>
<link>https://arxiv.org/abs/2505.05408</link>
<guid>https://arxiv.org/abs/2505.05408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语推理微调可提升多语言数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了针对英语推理进行微调的大型多语言语言模型在跨语言推理中的泛化能力。实验表明，增加英语推理模型的推理计算规模可以显著提高多种语言（包括低资源语言）的数学推理能力，甚至超越更大规模的模型。同时，虽然这些模型的推理链条主要基于英语，但它们能够通过引用和思考非英语输入的方式进行跨语言推理。此外，研究揭示了一种有效控制推理链条语言的方法，并发现模型在高资源语言上的表现更优。然而，模型在跨领域推理上表现出较差的泛化能力，特别是在从科学、技术、工程和数学（STEM）到文化常识知识的转换中。综上所述，该研究展示了英语推理在测试阶段扩展的潜力、机制及其局限性，并建议在高资源语言中进行推理，同时需进一步改进低资源语言及跨领域推理的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 12:50:06 GMT</pubDate>
</item>
<item>
<title>基于情境学习的贡献度测量方法ICon提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需梯度计算的情境学习贡献度测量方法ICon，显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ICon的新方法，它利用情境学习（ICL）的隐式微调特性，无需梯度计算或人工设计启发式指标，即可评估样本对模型训练的贡献度。ICon通过衡量隐式学习下性能的变化，有效筛选高贡献数据，显著降低了训练成本并提升了模型性能。实验表明，在LLaMA3.1-8B上，仅使用15%经ICon筛选的数据即可超越完整数据集的表现，并优于现有常用数据选择方法。进一步分析显示，ICon选出的高贡献样本不仅涵盖多样化任务，还具有适中的难度，而非仅仅是最难的任务。ICon为大语言模型的数据选择提供了高效且低偏见的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:17:37 GMT</pubDate>
</item>
<item>
<title>弹性推理框架实现可控的大规模链式思维推理</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弹性推理框架解决大规模模型推理长度不可控问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在复杂任务中生成过长链条思维的问题，提出了一种名为弹性推理的新框架。该框架将推理过程分为独立分配预算的思考和解决方案两个阶段，在测试时优先保证解决方案的完整性，显著提升了在资源受限情况下的可靠性。同时，通过引入轻量级预算约束滚动策略，使模型在思考过程被截断时也能自适应推理，并有效泛化到未见过的预算限制下，而无需额外训练。实验表明，弹性推理在严格的预算限制下表现稳健，且训练成本显著低于基线方法，即使在无约束设置中也产生更简洁高效的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:01:06 GMT</pubDate>
</item>
<item>
<title>语言引导的3D场景物体放置任务及基准</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的语言引导3D场景物体放置任务并创建基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项名为“语言引导的3D场景物体放置”的新任务，模型需要根据点云、3D资产和文本提示，在3D场景中找到符合要求的物体位置。此任务相较于其他3D场景中的语言引导定位任务具有独特挑战，如多解性和对三维几何关系的理解需求。为了推动该领域的发展，我们提出了一个新的基准和评估协议，同时发布了用于训练3D大语言模型的数据集，并提供了首个非平凡基线方法。我们认为，这项具有挑战性的任务及其新基准将成为评估和比较通用3D大语言模型的标准之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05288" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 10:29:11 GMT</pubDate>
</item>
<item>
<title>Fine-Grained CLIP：通过多模态增强实现细粒度理解</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Fine-Grained CLIP，显著提升图像细粒度理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比语言-图像预训练模型（CLIP）在细粒度理解上的局限性，提出Fine-Grained CLIP（FG-CLIP）。该方法通过利用大规模多模态模型生成包含全局语义细节的长描述图像对、构建高质量区域标注数据集以及引入硬负样本等创新手段，大幅提升了模型区分细微语义差异的能力。实验表明，FG-CLIP在细粒度理解、开放词汇目标检测、图像-文本检索及通用多模态基准任务上均优于现有方法。相关代码、数据和模型已开源。关键词：多模态学习、细粒度理解、CLIP</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 05:06:53 GMT</pubDate>
</item>
<item>
<title>链式思维令牌在复杂推理中的变量特性研究</title>
<link>https://arxiv.org/abs/2505.04955</link>
<guid>https://arxiv.org/abs/2505.04955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示链式思维令牌在解决复杂任务时的功能类似计算机程序中的变量。</p><br /><br /><p><strong>摘要：</strong> 本文通过实证研究探讨大型语言模型中链式思维（CoT）令牌在复合任务如多数字乘法和动态规划中的作用。尽管CoT对解决问题至关重要，但实验表明仅保留存储中间结果的令牌即可实现相近性能。此外，使用替代潜在形式存储中间结果并未影响模型表现。随机干预部分CoT值后，后续令牌及最终答案随之改变。这些发现表明CoT令牌可能类似于程序中的变量，但存在未预料到的捷径和令牌间计算复杂性限制等潜在问题。相关代码与数据已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 01:32:36 GMT</pubDate>
</item>
<item>
<title>多模态推理模型的研究进展与未来展望</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态推理模型在人工智能中实现复杂推理能力，面临泛化、深度及自主行为等挑战。</p><br /><br /><p><strong>摘要：</strong> 推理是智能的核心，尤其在开放、不确定且多模态的环境中，推理能力对人工智能系统至关重要。大型多模态推理模型（LMRMs）通过整合文本、图像、音频和视频等模态，支持复杂的推理功能。早期研究基于特定任务模块，而现代方法则转向统一的语言中心框架，通过指令微调和强化学习提升推理效果。然而，跨模态泛化、推理深度及自主行为仍面临诸多挑战。本文综述了多模态推理领域的研究进展，提出四阶段发展路线图，涵盖任务特定模块、统一多模态大模型、以及原生多模态推理模型（N-LMRMs）。通过分析O3和O4-mini等基准测试案例，探讨了在真实复杂环境中的可扩展性、自主性和适应性推理规划的潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 23:35:23 GMT</pubDate>
</item>
<item>
<title>迈向通用多模态模型：General-Level评估框架与General-Bench基准</title>
<link>https://arxiv.org/abs/2505.04620</link>
<guid>https://arxiv.org/abs/2505.04620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大语言模型正向通用化发展，但性能提升是否等同于更强能力？</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型（MLLM）正迅速发展，从理解多种模态到跨模态生成，其能力已扩展至细粒度理解及任意模态支持。然而，现有评估基准无法简单将任务表现直接关联至模型的整体能力。本文提出General-Level评估框架，定义五级性能与通用性标准，通过协同效应衡量模型能力一致性，并推出包含超70万实例的General-Bench基准，涵盖325,800多个样本和700多项任务。该研究通过对上百种顶级MLLM的评估揭示了通用化模型的能力排名及其面临的挑战，为下一代多模态基础模型的研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>X-Reasoner：通过文本后训练实现跨模态和跨领域可泛化推理</title>
<link>https://arxiv.org/abs/2505.03981</link>
<guid>https://arxiv.org/abs/2505.03981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究证明通用文本后训练可使视觉语言模型具备跨模态和跨领域的强泛化推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，专有模型在多模态推理方面表现出色，但现有开源研究多集中于文本推理，且评估局限于数学和通用领域任务。本文探讨了推理是否能在模态和领域间通用这一基础问题，并通过引入X-Reasoner模型，验证了通用领域文本后训练可以有效提升推理的泛化能力。该模型采用两阶段方法：初始监督微调阶段结合蒸馏后的长链思维，随后通过可验证奖励进行强化学习。实验显示，X-Reasoner在多模态和跨领域设置中表现优异，超越了现有基于领域内和多模态数据训练的最先进模型。进一步研究发现，针对特定领域的文本数据继续训练可进一步提高X-Reasoner在专业领域的性能，由此推出了X-Reasoner-Med，其在多种文本和多模态医学基准测试中达到新高度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 17:08:27 GMT</pubDate>
</item>
<item>
<title>LiftFeat：通过三维几何特征增强视觉定位中的局部特征匹配</title>
<link>https://arxiv.org/abs/2505.03422</link>
<guid>https://arxiv.org/abs/2505.03422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级网络LiftFeat，利用三维几何特征提升特征描述器的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LiftFeat的新轻量级网络，旨在提高视觉特征描述器在极端条件下的鲁棒性和区分度。传统方法在光照变化剧烈、纹理稀少或存在重复模式的情况下难以提取有效特征，而LiftFeat通过结合伪表面法线标签和预测的表面法线，设计了一个三维几何感知特征提升模块，将二维原始描述符与表面法线特征融合，显著增强了描述符在极端环境下的性能。实验表明，LiftFeat在相对位姿估计、单应性估计和视觉定位等任务上优于现有的一些轻量级方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:59:23 GMT</pubDate>
</item>
<item>
<title>SAGE框架评估大语言模型的社会认知能力</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAGE框架通过模拟情感变化评估大语言模型的社会认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sentient Agent as a Judge (SAGE) 的自动化评估框架，用于衡量大型语言模型（LLM）的高级社会认知能力。SAGE通过实例化一个具备人类情感变化和内心思考模拟的智能代理，在多轮对话中提供更真实的模型评估。实验结果显示，SAGE的最终情感评分与Barrett-Lennard关系量表（BLRI）评分及话语级同理心指标高度相关，验证了其心理真实性。此外，基于SAGE构建的公开排行榜揭示了前沿系统（如GPT-4o-Latest、Gemini2.5-Pro）与早期基线之间显著的能力差距，这种差距在传统排行榜中并未显现。因此，SAGE成为追踪真正具有同理心和社会适应能力的语言代理发展的重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02847" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 15:06:10 GMT</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 13:32:43 GMT</pubDate>
</item>
<item>
<title>OpenVision：开源视觉编码器家族挑战CLIP</title>
<link>https://arxiv.org/abs/2505.04601</link>
<guid>https://arxiv.org/abs/2505.04601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenVision提供完全开源的视觉编码器，性能媲美CLIP。</p><br /><br /><p><strong>摘要：</strong> OpenAI的CLIP自2021年初发布以来一直是构建多模态基础模型的首选视觉编码器。然而，近期的替代方案如SigLIP虽开始挑战这一地位，但大多存在数据闭源或训练配方未公开的问题。本文通过推出OpenVision填补了这一空白，OpenVision是一个完全开源且成本效益高的视觉编码器系列，在整合到LLaVA等多模态框架时表现可媲美甚至超越CLIP。基于现有工作如CLIPS训练框架和Recap-DataComp-1B训练数据，OpenVision揭示了提升编码器质量的关键见解，并展示了在推进多模态模型方面的实际益处。通过提供参数规模从5.9M到632.1M不等的多种视觉编码器，OpenVision为实践者提供了在容量与效率之间灵活权衡的选择，较大模型提升了多模态性能，而较小版本则支持轻量级边缘部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:48:35 GMT</pubDate>
</item>
<item>
<title>COSMOS：在资源约束下高效预测大语言模型适配结果</title>
<link>https://arxiv.org/abs/2505.01449</link>
<guid>https://arxiv.org/abs/2505.01449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出COSMOS框架，大幅降低大语言模型适配性能和成本预测的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在资源受限条件下，是否可以准确预测大语言模型（LLMs）的性能和成本，而无需进行昂贵的试验。我们形式化了LLMs的策略选择问题，并引入了COSMOS框架，该框架通过轻量级代理模型和低样本缩放律预测微调性能和检索增强上下文学习结果。对八个代表性基准的广泛评估显示，COSMOS在平均情况下将计算成本降低了92.72%，在资源密集型场景中最高可减少98.71%。实验结果表明，这种高效的预测方法不仅可行，还能显著减少LLMs部署的计算开销，同时保持性能标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 22:06:26 GMT</pubDate>
</item>
<item>
<title>OmniGIRL：多语言、多模态、多领域的GitHub问题自动解决基准测试</title>
<link>https://arxiv.org/abs/2505.04606</link>
<guid>https://arxiv.org/abs/2505.04606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGIRL是一个面向多语言、多模态、多领域GitHub问题解决的基准测试。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLMs）在GitHub问题自动解决任务中的评估基准主要存在三个局限性：仅限单一编程语言、覆盖领域狭窄以及只关注文本信息。本文提出OmniGIRL，这是一个涵盖四种编程语言（Python、JavaScript、TypeScript和Java）及八个不同领域的多语言、多模态、多域基准测试，包含959个任务实例。实验表明，现有LLMs在此基准上的表现有限，即使最佳模型GPT-4o也仅解决了8.6%的问题。此外，LLMs在处理涉及图像信息的问题时表现尤为困难，Claude-3.5-Sonnet在包含图像信息的问题上取得了10.5%的最佳解决率。最后，本文分析了LLMs在OmniGIRL上表现不佳的原因，并为未来改进提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:51:10 GMT</pubDate>
</item>
<item>
<title>轻量级外部信息驱动的自适应检索方法</title>
<link>https://arxiv.org/abs/2505.04253</link>
<guid>https://arxiv.org/abs/2505.04253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于外部信息的轻量级自适应检索方法，提升问答性能并降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）容易产生幻觉，而检索增强生成（RAG）虽能缓解这一问题，但计算成本高昂且存在信息误导风险。现有自适应检索方法依赖LLM的不确定性估计，效率较低且不实用。本研究引入了一种独立于LLM的轻量级自适应检索方法，该方法基于外部信息。我们分析了27个特征，分属7组及其混合组合，并在6个问答数据集上评估了这些方法的问答表现与效率。实验结果显示，我们的方法在性能上可媲美复杂的LLM基方法，同时显著提高了效率，展示了外部信息在自适应检索中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 04:58:52 GMT</pubDate>
</item>
<item>
<title>RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2505.03538</link>
<guid>https://arxiv.org/abs/2505.03538</guid>
<content:encoded><![CDATA[
Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 09:50:57 GMT</pubDate>
</item>
<item>
<title>Cognitio Emergens框架：重新定义人机协作的科学知识创造</title>
<link>https://arxiv.org/abs/2505.03105</link>
<guid>https://arxiv.org/abs/2505.03105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Cognitio Emergens框架，重新审视人机合作中的科学知识创造。</p><br /><br /><p><strong>摘要：</strong> 随着人类和AI系统逐渐超越工具使用者的关系，进入协同演化的认识伙伴关系阶段，科学知识的创造方式正在发生根本性变革。例如，AlphaFold在蛋白质结构预测上的突破改变了研究人员对基本关系的认知方式。本文引入Cognitio Emergens（CE）框架，该框架旨在解决现有模型的关键局限性，这些模型往往关注静态角色或狭窄指标，而未能捕捉科学理解如何通过递归的人机交互随着时间推移而产生。CE框架由三个组成部分构成：首先，Agency Configurations描述了人类和AI之间权威分配的方式，包括指导型、贡献型和合作伙伴型三种模式，这些模式动态地在不同配置间振荡而非线性发展；其次，Epistemic Dimensions捕捉了六个特定能力，这些能力通过发现、整合和投影轴上的协作而出现，形成了独特的“能力特征”，用于指导发展；最后，Partnership Dynamics确定了影响这些关系发展的力量，特别是认知异化风险，即研究人员可能失去对其正式认可的知识的解释控制。CE框架借鉴了自创生理论、社会系统理论和组织模块化理论，揭示了知识共同创造如何通过持续的角色、价值观和组织结构谈判而产生。通过重新概念化人机科学合作为根本的协同进化过程，CE框架提供了一种平衡的观点，既不盲目庆祝也不过度恐惧AI不断演化的角色，而是提供了概念工具，用以培养既能维持有意义的人类参与又能推动科学突破的合作关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 21:49:44 GMT</pubDate>
</item>
<item>
<title>AutoLibra：基于开放反馈的智能体评估框架</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoLibra框架，将人类反馈转化为细粒度智能体行为评估指标。</p><br /><br /><p><strong>摘要：</strong> 当前智能体评估多依赖于任务成功率等粗略指标，这些指标由专家手动设计且无法捕捉中间阶段的行为表现。本文提出AutoLibra框架，通过将开放式的用户反馈（如“按钮禁用时不要重复点击”）转化为对智能体轨迹的细粒度行为评价指标，实现对智能体行为的量化评估。该框架通过行为聚类和具体示例定义指标，可作为大型语言模型评估器的提示来源。同时，引入覆盖率和冗余度两个元指标优化指标集的一致性，实验表明AutoLibra能诱导出比现有基准更具体的评估指标并发现新指标。此外，AutoLibra在文本游戏任务和网页导航任务中展示了其在提升智能体性能和数据筛选上的应用价值，证明其作为任务不可知工具的强大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:49 GMT</pubDate>
</item>
<item>
<title>PrimitiveAnything：一种基于形状条件的几何元素组合生成框架</title>
<link>https://arxiv.org/abs/2505.04622</link>
<guid>https://arxiv.org/abs/2505.04622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的形状分解框架，能够生成高质量的几何元素组合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PrimitiveAnything的新框架，该框架将形状基本元素抽象重新定义为基本元素组装生成任务。通过形状条件下的基本元素变换器和无歧义参数化方案，实现了多种类型的基本元素统一表示。与现有方法相比，新框架直接从大规模手工制作的抽象数据中学习，从而更好地捕捉人类对复杂形状分解的理解能力。实验表明，该方法在保持几何精确性的同时，生成的元素组合更符合人类感知，并且在多样化的形状类别中表现出色。此外，它还适用于多个3D应用领域，并展示了在游戏等用户生成内容中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>ZeroSearch：无需实时搜索引擎的大型语言模型检索能力强化学习框架</title>
<link>https://arxiv.org/abs/2505.04588</link>
<guid>https://arxiv.org/abs/2505.04588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习提升大型语言模型检索能力，解决文档质量不可控和API成本过高的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ZeroSearch的强化学习框架，用于增强大型语言模型（LLMs）的检索能力，而无需与实时搜索引擎交互。传统方法因文档质量不可控和高昂的API费用面临挑战，ZeroSearch通过轻量级有监督微调将LLM转化为检索模块，并采用基于课程的滚动策略逐步降低生成文档的质量，从而激发模型的推理能力。实验表明，该方法不仅有效提升了LLMs的检索性能，还具有良好的泛化性和与多种强化学习算法的兼容性，且在某些情况下甚至超越了真实搜索引擎的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:30:22 GMT</pubDate>
</item>
<item>
<title>基于确定性马尔可夫决策过程的形式化问题求解框架</title>
<link>https://arxiv.org/abs/2505.04528</link>
<guid>https://arxiv.org/abs/2505.04528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FPS框架实现过程验证问题求解并评估多个FTP模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文填补了科学与工程领域问题求解形式化表述的空白，通过将问题求解建模为确定性马尔可夫决策过程，提出了FPS（Formal Problem-Solving）框架，利用现有FTP（Formal Theorem Proving）环境进行过程验证。此外，还设计了D-FPS框架以分离求解与答案验证，增强人类对齐能力。文中证明了这些框架的表达能力、正确性和完备性，并构建了三个问题求解基准测试集：FormalMath500、MiniF2F-Solving和PutnamBench-Solving。同时，引入RPE（Restricted Propositional Equivalence）符号方法评估答案正确性。实验表明，目前流行的FTP模型和提示方法在基准测试中的表现有限，例如在FormalMath500上最多解决23.77%的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 12:02:14 GMT</pubDate>
</item>
<item>
<title>HunyuanCustom：多模态自定义视频生成框架提升身份一致性</title>
<link>https://arxiv.org/abs/2505.04512</link>
<guid>https://arxiv.org/abs/2505.04512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持多模态条件的HunyuanCustom框架，显著提升定制视频的身份一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HunyuanCustom的多模态自定义视频生成框架，该框架通过引入文本-图像融合模块和图像ID增强模块来解决身份一致性问题，并支持多种输入模态如图像、音频、视频和文本。具体而言，文本-图像融合模块基于LLaVA增强多模态理解能力，而图像ID增强模块则利用时间串联强化帧间身份特征。此外，为了实现音频和视频条件下的生成任务，设计了特定的模态条件注入机制，包括通过空间交叉注意力实现层次对齐的AudioNet模块，以及通过基于补丁化的特征对齐网络整合潜压缩条件视频的视频驱动注入模块。实验表明，HunyuanCustom在单主体和多主体场景中均优于现有方法，在身份一致性、真实感及文本-视频对齐方面表现优异。此外，该模型在下游任务中也表现出鲁棒性，所有代码和模型资源均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 11:33:18 GMT</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 08:32:01 GMT</pubDate>
</item>
<item>
<title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.03912</link>
<guid>https://arxiv.org/abs/2505.03912</guid>
<content:encoded><![CDATA[
Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 14:35:07 GMT</pubDate>
</item>
<item>
<title>OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents</title>
<link>https://arxiv.org/abs/2505.03570</link>
<guid>https://arxiv.org/abs/2505.03570</guid>
<content:encoded><![CDATA[
In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 10:29:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在复杂问题求解中的能力与挑战</title>
<link>https://arxiv.org/abs/2505.03418</link>
<guid>https://arxiv.org/abs/2505.03418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述大型语言模型在多步推理、领域知识整合及结果验证中的应用与局限。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能的发展，大型语言模型（LLMs）已成为解决复杂问题的强大工具。与传统计算系统不同，LLMs结合了强大的计算能力和类似人类的推理能力，能够在多个领域生成解决方案。然而，将LLMs应用于实际问题解决时面临多步推理、领域知识整合和结果验证等重大挑战。本文探讨了LLMs在复杂问题求解中的能力与限制，分析了链式思维（CoT）、知识增强和多种基于LLMs和工具的验证技术。此外，文章还针对软件工程、数学推理与证明、数据分析建模及科学研究等特定领域的挑战进行了阐述，并讨论了当前LLMs解决方案的基本局限性及其未来发展方向，特别是在多步推理、领域知识整合和结果验证方面的潜在改进路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:53:58 GMT</pubDate>
</item>
<item>
<title>多模态理解与图像生成统一模型的研究综述</title>
<link>https://arxiv.org/abs/2505.02567</link>
<guid>https://arxiv.org/abs/2505.02567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了多模态理解和图像生成统一模型的最新进展及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态理解和图像生成领域各自取得了显著进步，但因采用不同的架构范式（如自回归与扩散模型），两者发展相对独立。近期，学术界对统一这两类任务的框架表现出浓厚兴趣，GPT-4o等新模型的出现进一步推动了这一趋势。然而，由于架构差异，实现有效融合仍面临诸多挑战。本文通过介绍多模态理解与文本到图像生成的基础概念与最新进展，梳理现有统一模型的三种主要架构范式（扩散型、自回归型及混合型），并分析相关创新设计。同时，我们整理了适用于统一模型的数据集与基准测试资源，讨论了该领域的关键难题，如标记化策略、跨模态注意力机制及数据需求。作为新兴研究方向，本综述旨在为未来探索提供指导，并将持续更新以反映最新成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02567" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 07:18:03 GMT</pubDate>
</item>
<item>
<title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.02393</link>
<guid>https://arxiv.org/abs/2505.02393</guid>
<content:encoded><![CDATA[
Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:33:20 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在视觉视角转换中的能力评估</title>
<link>https://arxiv.org/abs/2505.03821</link>
<guid>https://arxiv.org/abs/2505.03821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有视觉语言模型在场景理解方面表现良好，但在空间推理和视角转换上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本研究通过设计一套基于人类测试的视觉任务，探索视觉语言模型（VLMs）执行视觉视角转换的能力。我们构建了144个独特的视觉任务，这些任务通过控制场景中的物体位置及人形迷你模型的方向等变量，结合鸟瞰图和表面视图来评估模型的视觉认知水平。每个任务配有一系列诊断问题，旨在测试场景理解、空间推理及视觉视角转换三个层次的认知能力。实验结果显示，尽管最先进的模型如GPT-4-Turbo和Llama-3.2-11B-Vision-Instruct在场景理解方面表现出色，但其在空间推理和视角转换上的表现却明显下降。分析表明，当前模型在表面级对象识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这提示未来VLM开发需整合显式的几何表示并采用定制化的训练方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 20:10:41 GMT</pubDate>
</item>
<item>
<title>R&amp;B框架通过语义重分区和高效优化提升数据混合策略性能</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R&amp;B框架通过语义重分区和优化数据组成显著提升语言模型训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为R&amp;B的框架，旨在解决现有数据混合策略中的两个主要问题：依赖预先设定的数据领域可能导致遗漏关键语义细微差别，且其计算复杂度随领域数量呈指数增长。R&amp;B通过基于语义相似性的重新分组（Regroup）创建更精细的数据领域，并利用训练过程中获得的领域梯度诱导的Gram矩阵来高效优化数据组合（Balance）。与传统方法不同的是，R&amp;B无需额外计算即可获取评估信息如损失或梯度。理论分析表明，该技术在标准规则条件下表现优于非自适应混合方法。实证研究显示，在仅增加0.01%的计算开销下，R&amp;B在五个多样化数据集上的表现匹配甚至超越了最先进的数据混合策略，涵盖了自然语言处理、推理及多模态任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 03:08:19 GMT</pubDate>
</item>
<item>
<title>通过Selective Loss方法提升语言模型对高风险文本的理解能力</title>
<link>https://arxiv.org/abs/2505.03052</link>
<guid>https://arxiv.org/abs/2505.03052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SLUNG方法，使模型能理解高风险文本而不生成它们。</p><br /><br /><p><strong>摘要：</strong> 语言模型开发者通常会过滤掉预训练数据中的高风险内容（如有毒或受版权保护的文本），以防止模型生成类似内容。然而，这种做法限制了模型识别和适当响应有害或敏感内容的能力。本文介绍了一种名为SLUNG（Selective Loss to Understand but Not Generate）的预训练范式，该方法让模型学会理解高风险数据而不学习生成它们。SLUNG不是均匀地应用下一个令牌预测损失，而是有选择地避免激励生成高风险令牌，同时确保它们保留在模型的上下文窗口内。实验表明，SLUNG在提高模型对高风险数据的理解能力方面始终表现出色，而不会增加生成这些内容的风险。总的来说，SLUNG范式使得模型能够从原本会被过滤掉的高风险文本中受益。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 18:24:06 GMT</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 19:09:44 GMT</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:56:06 GMT</pubDate>
</item>
<item>
<title>VITA-Audio：基于多模态预测模块的低延迟语音生成大模型</title>
<link>https://arxiv.org/abs/2505.03739</link>
<guid>https://arxiv.org/abs/2505.03739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端的高效语音生成模型VITA-Audio，大幅降低实时对话中的初始音频生成延迟。</p><br /><br /><p><strong>摘要：</strong> 随着自然人机交互需求的增长，语音系统备受关注，但现有模型在流式生成首个音频标记时存在高延迟问题。为解决这一瓶颈，我们提出了VITA-Audio，这是一种具有快速音素-文本标记生成能力的端到端大型语音模型。通过引入轻量级的多模态跨模态标记预测（MCTP）模块，该模型在一个前向传播过程中可高效生成多个音频标记，不仅加速推理速度，还显著降低了流式场景下的首次音频生成延迟。此外，我们探索了一种四阶段渐进式训练策略，在保证最小语音质量损失的同时实现模型加速。作为首个能够在首次前向传播中生成音频输出的多模态大语言模型，VITA-Audio支持实时对话功能，且完全开源可复现。实验结果显示，VITA-Audio在70亿参数规模下推理速度提升了3至5倍，并在自动语音识别（ASR）、文本转语音（TTS）和口语问答（SQA）等多个基准测试中显著优于同类开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>面向综合理解的AI在足球领域的框架与贡献</title>
<link>https://arxiv.org/abs/2505.03735</link>
<guid>https://arxiv.org/abs/2505.03735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识库、基准测试及多智能体系统的足球全面理解框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有AI在足球领域研究集中在孤立任务的问题，提出了一个全面的足球理解框架。首先，构建了SoccerWiki，这是首个大规模多模态足球知识库，整合了丰富的球员、球队、裁判及场馆等领域的知识。其次，开发了SoccerBench，这是一个包含约10K标准化多模态多选题的基准数据集，涵盖13项不同的理解任务。此外，引入了SoccerAgent，这是一种创新的多智能体系统，通过协作推理分解复杂问题，并利用SoccerWiki的专业知识实现稳健性能。最后，通过对SoccerBench的广泛评估和消融实验，展示了所提出的智能体系统的优越性。所有数据和代码均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>FlexiAct：一种灵活的动作迁移方法</title>
<link>https://arxiv.org/abs/2505.03730</link>
<guid>https://arxiv.org/abs/2505.03730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiAct通过引入RefAdapter和FAE技术，实现跨多样主体和场景的动作迁移。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlexiAct的新方法，用于在视频中将动作从参考视频迁移到任意目标图像上。传统动作定制方法受限于空间结构的严格约束，如布局、骨架和视角一致性，这限制了其在多样化主体和场景中的适用性。FlexiAct突破了这些限制，允许参考视频主体与目标图像之间存在布局、视角和骨骼结构的变化，同时保持身份一致性。为了实现这一目标，我们引入了轻量级的图像条件适配器RefAdapter，它在空间适应性和一致性保存方面表现出色，超过了现有方法在外观一致性和结构灵活性之间的平衡能力。此外，我们观察到去噪过程在不同时间步对运动和外观细节的关注程度不同，因此提出了频率感知动作提取（FAE），它直接在去噪过程中实现动作提取，而无需依赖单独的空间-时间架构。实验表明，我们的方法可以有效地将动作转移到具有多样化布局、骨骼和视角的主体上。我们发布了代码和模型权重以支持进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型在地理空间解释性研究中的新框架</title>
<link>https://arxiv.org/abs/2505.03368</link>
<guid>https://arxiv.org/abs/2505.03368</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，揭示大型语言模型处理地理信息的方式。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自然语言处理领域展现了强大的能力，但在地理知识处理和空间推理方面的内部机制仍不明确。本文建立了一种基于空间分析的新框架，通过探针技术和机械可解释性方法，探讨LLMs如何处理地理信息。研究利用空间自相关技术，证明了从地名特征中可以发现与地理位置相关的空间模式，为理解这些模型的地理信息处理提供了新视角。最后讨论了该框架对地理领域基础模型研究与应用的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03368" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:40:06 GMT</pubDate>
</item>
<item>
<title>绝对零度范式下的自我进化推理模型</title>
<link>https://arxiv.org/abs/2505.03335</link>
<guid>https://arxiv.org/abs/2505.03335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出绝对零度范式，使模型无需外部数据即可通过自拟任务提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励方法（RLVR）被证明可以增强大型语言模型的推理能力，但现有方法仍依赖人工标注的问题和答案。为解决人类监督的长期可持续性问题及未来超级智能系统的潜在局限，我们提出了绝对零度（Absolute Zero）范式，该范式下模型自行设计最大化自身学习进度的任务并从中提升推理能力。作为该范式的实例，绝对零度推理器（AZR）利用代码执行器验证任务和答案，提供统一的可验证奖励来源，实现开放且有基础的学习。尽管完全不依赖外部数据，AZR在编码和数学推理任务上达到了当前最优性能，超越了依赖数万个人工标注样本的传统零样本模型，并且适用于不同规模和类型的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:08:00 GMT</pubDate>
</item>
<item>
<title>UnifiedReward-Think：基于长链条推理的多模态奖励模型</title>
<link>https://arxiv.org/abs/2505.03318</link>
<guid>https://arxiv.org/abs/2505.03318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入长链条推理提升多模态奖励模型的可靠性和准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态奖励模型在提供符合人类偏好的视觉模型奖励信号方面取得了显著进展。然而，当前的奖励模型通常局限于浅层推理过程，导致奖励信号不准确。本文提出了一种名为UnifiedReward-Think的新方法，该方法通过引入显式的长链条推理增强奖励推理的可靠性和鲁棒性，并通过隐式推理能力提高直接响应的准确性。具体而言，我们采用探索驱动的强化微调方法来诱发和激励模型潜在的复杂推理能力，包括利用少量图像生成偏好数据蒸馏GPT-4o的推理过程用于冷启动学习，以及利用大规模统一多模态偏好数据在多种视觉任务中引发推理过程。实验结果表明，该方法在多个视觉奖励任务中表现出色，验证了其优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 04:46:41 GMT</pubDate>
</item>
<item>
<title>InfoVids：重塑演示者与可视化之间的关系</title>
<link>https://arxiv.org/abs/2505.03164</link>
<guid>https://arxiv.org/abs/2505.03164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过InfoVids重新定义演示者与可视化的关系，提升人本体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过InfoVids（基于信息图表的视频）改变传统数据展示中演示者与可视化分离的状况。InfoVids旨在建立演示者与可视化之间更平等的关系，探索布局、形式及交互对观看体验的影响。研究对比了InfoVids与传统2D幻灯片，在9项指标下进行测试，并收集了30名参与者的反馈。分析表明，InfoVids减少了观众注意力分散，将关注点从可视化转移到演示者身上，使数据展示更加互动、自然且吸引人。这一方法为重新思考演示者与可视化动态提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:18:42 GMT</pubDate>
</item>
<item>
<title>RADLADS协议：高效转换Transformer至线性注意力解码模型</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RADLADS协议，将Transformer高效转化为线性注意力解码模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Rapid Attention Distillation to Linear Attention Decoders at Scale（RADLADS）协议，用于将softmax注意力的Transformer高效转化为线性注意力解码模型。该研究还提出了两个RWKV变体架构，并基于Qwen2.5开源模型创建了多种规模（7B、32B、72B）的模型。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原始Transformer。这些模型在标准基准测试中表现出色，所有模型均开源于HuggingFace，其中72B模型受Qwen许可协议约束。本文还提供了模型和训练代码的具体链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03005" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 16:03:28 GMT</pubDate>
</item>
<item>
<title>RetroInfer：一种加速长上下文大语言模型推理的新系统</title>
<link>https://arxiv.org/abs/2505.02922</link>
<guid>https://arxiv.org/abs/2505.02922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RetroInfer系统，通过向量存储和注意力稀疏性优化加速长上下文大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RetroInfer的创新系统，旨在解决大语言模型（LLMs）在高效推理时因上下文长度增加而面临的GPU内存和带宽限制问题。该系统重新定义了键值（KV）缓存为向量存储系统，利用内在的注意力稀疏性来加速长上下文的LLMs推理。RetroInfer的核心是一个波浪索引（wave index），这是一种基于注意力的向量索引，通过三部分注意力近似、有界注意力估计和分段聚类等技术实现对关键令牌的有效且精确检索。此外，系统还配备了波浪缓冲区（wave buffer），用于协调KV缓存放置并重叠GPU和CPU上的计算与数据传输，从而维持高吞吐量。与之前基于稀疏性的方法相比，RetroInfer在不牺牲模型准确性的情况下提供了稳健的性能。实验表明，在GPU内存限制下，相对于完整注意力，RetroInfer可实现高达4.5倍的速度提升；当KV缓存扩展到CPU内存时，相对于稀疏注意力基准，速度提升可达10.5倍，同时保持全注意力级别的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 14:01:17 GMT</pubDate>
</item>
<item>
<title>基于AttenHScore的大模型与小模型协作优化方法</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AttenHScore评估指标提升小模型实时幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大语言模型（LM）与小语言模型协作中的关键挑战——即如何精准定位小模型生成幻觉的时刻。传统优化主要依赖于独立于推理过程的后处理技术，导致高计算成本且效果有限。为此，我们提出了AttenHScore这一实用的调用评估指标，通过累积并传播小模型生成过程中的幻觉现象，动态调整检测阈值实现对大模型的更精确调用。同时，结合不确定性感知的知识重组策略，增强小模型捕获关键信息的能力。实验表明，AttenHScore在多个问答数据集上显著优于基线方法，尤其在处理复杂查询时表现优异，且无需额外的模型训练，具备适应多种Transformer架构LM的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 21:45:56 GMT</pubDate>
</item>
<item>
<title>Qwen3低比特量化性能评估与挑战</title>
<link>https://arxiv.org/abs/2505.02214</link>
<guid>https://arxiv.org/abs/2505.02214</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索Qwen3在低比特量化下的表现，揭示压缩大型语言模型的机遇与难题。</p><br /><br /><p><strong>摘要：</strong> Qwen系列作为领先的开源大型语言模型家族，其最新成员Qwen3展现出卓越的自然语言理解能力。然而，在资源受限环境下高效部署这些模型仍具挑战性。本研究系统评估了五种经典后训练量化技术对Qwen3的影响，量化位宽从1到8位不等，并在多个数据集上测试其有效性。结果显示，Qwen3在中等比特宽度下保持竞争力，但在超低精度下显著影响语言任务性能。研究强调了进一步优化极端量化场景下性能的重要性，并提供了针对Qwen3及其未来版本的量化方法改进建议。项目代码已公开于GitHub及Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02214" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 14:43:44 GMT</pubDate>
</item>
<item>
<title>通过眼球运动自动解码开放性阅读目标的研究</title>
<link>https://arxiv.org/abs/2505.02872</link>
<guid>https://arxiv.org/abs/2505.02872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次探索能否通过眼球运动自动解码开放性阅读目标。</p><br /><br /><p><strong>摘要：</strong> 本研究首次探讨了是否可以通过眼球运动自动解码读者在开放性阅读中的具体目标。为了回答这一问题，我们引入了目标分类和目标重建任务及其评估框架，并利用大规模英文阅读的眼动追踪数据，这些数据涉及数百种特定于文本的信息检索任务。我们开发并比较了几种结合眼球运动和文本的判别型与生成型多模态大语言模型（LLMs），用于目标分类和目标重建。实验结果显示，在两项任务上取得了显著的成功，表明LLMs可以从眼球运动中提取出关于读者文本特定目标的有价值信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:23:48 GMT</pubDate>
</item>
<item>
<title>HoloTime：通过扩散模型实现全景视频到4D场景的重建</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架HoloTime，结合视频扩散模型生成沉浸式4D体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散模型在虚拟现实(VR)和增强现实(AR)技术中的应用潜力，特别是针对需要场景级4D资产的用户体验。现有的扩散模型主要集中在静态3D场景或物体级别的动态建模上，限制了它们提供真正沉浸式体验的能力。为了解决这一问题，我们提出了HoloTime框架，它集成了视频扩散模型，可以从单一提示或参考图像生成全景视频，并结合360度4D场景重建方法将生成的全景视频无缝转换为4D资产，从而为用户提供完全沉浸式的4D体验。具体来说，为了控制视频扩散模型以生成高保真的全景视频，我们引入了360World数据集，这是首个适合下游4D场景重建任务的全景视频综合集合。基于此精心策划的数据集，我们提出了Panoramic Animator，这是一种两阶段的图像到视频扩散模型，可以将全景图像转换为高质量的全景视频。随后，我们介绍了Panoramic Space-Time Reconstruction，它利用时空深度估计方法将生成的全景视频转换为4D点云，使优化整体4D高斯点撒表示成为可能，从而重建空间和时间一致的4D场景。通过与现有方法进行比较分析，验证了我们的方法在全景视频生成和4D场景重建方面的优越性，展示了其在创建更具吸引力和真实感的沉浸式环境方面的能力，从而提升了VR和AR应用程序中的用户体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:55:28 GMT</pubDate>
</item>
<item>
<title>Auto-SLURP：用于评估大型语言模型驱动多智能体框架的新基准数据集</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Auto-SLURP，一个针对智能个人助手的多智能体框架评估基准数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，由大型语言模型（LLMs）驱动的多智能体框架取得了显著进展。然而，缺乏专门用于评估这些框架性能的基准数据集。为解决这一问题，本文提出了Auto-SLURP，这是一个扩展自SLURP数据集的新基准，通过重新标注数据并集成模拟服务器和外部服务，支持从语言理解到任务执行再到响应生成的端到端评估。实验表明，当前最先进的框架在Auto-SLURP上面临严峻挑战，表明真正可靠且智能的多智能体个人助手仍需进一步研究。该数据集及相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 10:17:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在图结构数据中的注意力机制研究</title>
<link>https://arxiv.org/abs/2505.02130</link>
<guid>https://arxiv.org/abs/2505.02130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型在处理图结构数据时的注意力机制表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 本文从注意力机制的角度出发，对大型语言模型（LLMs）处理图结构数据的方式进行了实证研究。尽管LLMs能够识别图数据并捕获文本节点交互，但在建模节点间关系上存在局限性。此外，LLMs的注意力分布未能适应图拓扑结构的特点，而中间状态注意力窗口在训练和推理阶段均表现出更好的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 10:40:31 GMT</pubDate>
</item>
<item>
<title>基于Motion-enhanced Event Tensor的RGB-事件模态融合及其应用</title>
<link>https://arxiv.org/abs/2505.01548</link>
<guid>https://arxiv.org/abs/2505.01548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的事件表示方法MET，解决RGB-事件融合中的三类对齐问题并提升语义分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对RGB与事件相机模态融合中存在的时序、空间及模态错配问题，提出了Motion-enhanced Event Tensor (MET) 新型事件表示方法。MET通过利用密集光流和事件时间特征将稀疏事件体素转化为密集且时序一致的形式。此外，引入Frequency-aware Bidirectional Flow Aggregation Module (BFAM) 和 Temporal Fusion Module (TFM)，分别缓解模态错配并解决时空错配问题。实验表明，在两个大规模数据集上，所提框架显著优于现有最先进的RGB-事件语义分割方法。研究代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 15:19:58 GMT</pubDate>
</item>
<item>
<title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.02005</link>
<guid>https://arxiv.org/abs/2505.02005</guid>
<content:encoded><![CDATA[
Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (&gt;6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.
]]></content:encoded>
<pubDate>Sun, 04 May 2025 02:25:14 GMT</pubDate>
</item>
<item>
<title>MUSAR：仅需单领域训练数据的多领域定制框架</title>
<link>https://arxiv.org/abs/2505.02823</link>
<guid>https://arxiv.org/abs/2505.02823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅需单领域数据即可实现多领域自定义的新框架MUSAR。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有跨领域定制方法面临的两大挑战——多领域训练数据获取困难和属性纠缠问题，提出了名为MUSAR的简单而有效的框架。MUSAR通过引入去偏双联学习解决数据限制问题，利用静态注意力路由和双分支LoRA校正分布偏差；同时借助动态注意力路由机制消除领域间的纠缠，从而实现多领域的解耦表示。实验表明，尽管仅使用单领域数据，MUSAR在图像质量、领域一致性及交互自然性方面均优于现有方法，甚至超越部分基于多领域数据训练的方法。这一成果为多领域定制提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:50:24 GMT</pubDate>
</item>
<item>
<title>ReplaceMe：无需训练的Transformer块剪枝方法</title>
<link>https://arxiv.org/abs/2505.02819</link>
<guid>https://arxiv.org/abs/2505.02819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种无需训练即可有效剪枝Transformer块的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReplaceMe的新型训练-free深度剪枝方法，该方法通过将Transformer块替换为线性操作，同时保持高精度性能，适用于低压缩比场景。与传统需要额外训练或微调的剪枝方法不同，ReplaceMe仅需一个小规模校准数据集来估计线性变换，从而逼近被剪枝的块。这一线性映射可无缝合并到剩余的Transformer块中，无需添加额外网络参数。实验表明，ReplaceMe在多个大型语言模型上的表现优于其他训练-free方法，并与涉及大量重新训练/微调及架构修改的最新剪枝方法具有竞争力。在开放基准测试中，ReplaceMe实现了高达25%的剪枝率，而性能保留约90%，且没有额外的计算开销。此外，我们开源了一个包含ReplaceMe及其相关技术的库。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:42 GMT</pubDate>
</item>
<item>
<title>Voila：迈向自然人机交互的大型语音语言基础模型</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voila是一种新型全双工语音语言模型，实现低延迟、情感丰富的对话交互。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Voila，一种旨在实现自主、实时且情感表达丰富的人机语音交互的大型语音语言基础模型。不同于传统的流水线系统，Voila采用端到端架构，支持毫秒级响应时间，同时保留丰富的语音特征如音调、节奏和情感。它通过层次化多尺度Transformer融合了大语言模型的推理能力与强大的声学建模能力，允许用户通过文本指令定义语音角色特性。此外，Voila还支持百万级预制语音并能高效定制新语音，仅需10秒音频样本即可完成。除了对话功能，Voila还能轻松适应自动语音识别（ASR）、文本转语音（TTS）及多语言语音翻译等多种应用场景。该模型已完全开源，助力开放研究并推动下一代人机交互发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:05:01 GMT</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:26:00 GMT</pubDate>
</item>
<item>
<title>引入推理能力的生成型奖励模型提升奖励建模性能</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出推理奖励模型（ReasRM），显著提高大语言模型对人类偏好的对齐能力。</p><br /><br /><p><strong>摘要：</strong> 奖励建模（Reward Modeling）对于通过强化学习从人类反馈（RLHF）来对齐大型语言模型（LLMs）至关重要。然而，现有的奖励模型要么提供不透明的标量评分，要么直接预测首选答案，导致缺乏解释性。受长链推理（CoT）方法的启发，本研究提出了推理奖励模型（ReasRM），将奖励建模视为一个推理任务。ReasRM通过两个阶段进行训练：高质量推理链蒸馏和基于可验证奖励的强化学习。实验结果显示，所提出的RM-R1模型在多个基准测试中达到或接近当前最佳性能，甚至超越了更大的开源权重模型（如Llama3.1-405B）和专有模型（如GPT-4o）。此外，我们分析了ReasRM成功训练的关键因素，并开源了六个ReasRM模型及相关代码和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:11:12 GMT</pubDate>
</item>
<item>
<title>Muon优化器在计算效率与数据效能上的改进</title>
<link>https://arxiv.org/abs/2505.02222</link>
<guid>https://arxiv.org/abs/2505.02222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Muon优化器在计算时间和AdamW之间扩展帕累托前沿，提高大批次训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 本文展示了Muon作为二阶优化器的最简单实例，如何在计算时间与AdamW的权衡中扩展帕累托前沿。研究发现，Muon在大批次训练中比AdamW更具数据效率，且保持了计算效率，从而实现更经济的训练。此外，Muon与最大更新参数化(muP)的结合用于高效超参数转移，提出了一种考虑所有muP误差来源的简单望远镜算法，仅引入适度的资源开销。通过模型大小高达40亿参数的广泛实验及对数据分布和架构的消融研究验证了这些发现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 15:14:43 GMT</pubDate>
</item>
<item>
<title>基于演示交互强化学习的数据增强与技能获取</title>
<link>https://arxiv.org/abs/2505.02094</link>
<guid>https://arxiv.org/abs/2505.02094</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解决强化学习中演示噪声与覆盖不足问题的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文解决了强化学习从交互演示中获取技能的一个基础挑战：演示噪声与覆盖限制。现有数据收集方法虽有价值，但常产生稀疏、不连贯且嘈杂的轨迹。我们通过引入两种数据增强技术——Stitched Trajectory Graph (STG) 和 State Transition Field (STF)，有效连接演示技能并构建状态转换场。同时，开发了自适应轨迹采样策略和记忆依赖技能学习机制。实验表明，在多种交互任务上，该方法显著提升了收敛稳定性、泛化能力和恢复鲁棒性，超越当前最先进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02094" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:00:29 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理引擎系统性评估与未来展望</title>
<link>https://arxiv.org/abs/2505.01658</link>
<guid>https://arxiv.org/abs/2505.01658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估25个开源及商业LLM推理引擎，探讨优化方法与适用场景。</p><br /><br /><p><strong>摘要：</strong> 大规模语言模型（LLMs）在聊天机器人、代码生成器和搜索引擎中广泛应用，但复杂推理等任务显著增加推理成本。尽管已有并行化、压缩和缓存等多种优化方法，但服务需求多样化使得方法选择困难。本文对25个开源和商业推理引擎进行全面评估，涵盖易用性、部署便捷性、通用支持、可扩展性和吞吐量/延迟敏感计算的适用性。通过分析各引擎支持的优化技术，揭示其设计目标，并评估开源引擎的生态系统成熟度及商业解决方案的成本性能策略。此外，本文还提出未来研究方向，如支持复杂服务、兼容多种硬件及增强安全性，并提供公共存储库追踪领域进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 22:47:43 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型的目标遗忘评估基准研究</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多模态遗忘学习基准UnLOK-VQA及框架，评估删除特定知识的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模数据训练的语言模型可能无意中获取敏感信息的风险，尤其是多模态模型整合文本和图像信息时风险更高。为了应对这一挑战，研究者引入了一个名为UnLOK-VQA的多模态遗忘学习基准，并设计了一套攻击与防御框架，用于评估如何有效删除多模态知识。该基准通过自动化管道扩展视觉问答数据集，同时进行人工筛选，确保高质量。研究对六种防御目标进行了测试，发现多模态攻击比单一模态更具优势，且最有效的防御方法是从模型状态中移除答案信息。此外，更大的模型表现出更强的编辑后鲁棒性，表明规模有助于提高安全性。UnLOK-VQA为多模态大型语言模型的安全性提升提供了严格的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 21:54:00 GMT</pubDate>
</item>
<item>
<title>通过Grokking增强Transformer模型的多步事实推理能力</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次将Grokking应用于现实世界事实数据，显著提升多跳推理准确性。</p><br /><br /><p><strong>摘要：</strong> 尽管Transformer模型在众多自然语言处理任务中取得了显著成功，但在缺乏实际知识的情况下进行多步事实推理时仍存在明显不足。最近关于Grokking的研究表明，神经网络可以通过检测潜在逻辑模式实现从记忆到完全泛化的转变，但这些研究主要集中在小型合成任务上。本文首次将Grokking扩展到现实世界的事实数据，并通过精心设计的合成数据扩充现有知识图谱，提高推断事实与原子事实的比例phi_r，从而克服数据稀疏性问题。令人惊讶的是，即使合成数据存在事实错误，也能加强新兴推理电路的形成，而非降低准确性。在多跳推理基准测试中，我们的方法在2WikiMultiHopQA上的准确率达到了95%-100%，大幅优于现有基线模型。此外，我们深入分析了phi_r增加如何促进Transformer内部形成泛化电路。研究结果表明，基于Grokking的数据增强可以解锁隐式的多跳推理能力，为大规模语言模型提供更健壮且可解释的事实推理奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:33:29 GMT</pubDate>
</item>
<item>
<title>ARTIST：引入具身推理与工具集成的大语言模型框架</title>
<link>https://arxiv.org/abs/2505.01441</link>
<guid>https://arxiv.org/abs/2505.01441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARTIST框架通过具身推理和工具集成显著提升大语言模型问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ARTIST（具身推理与工具集成的自我改进Transformer），这是一种将具身推理、强化学习和工具集成统一起来的框架，旨在增强大型语言模型（LLMs）的动态、多步推理能力以及适应性决策能力。ARTIST允许模型自主决定何时、如何以及选择哪个工具进行调用，并通过基于结果的强化学习策略优化工具使用和环境交互。实验表明，ARTIST在数学推理和多轮函数调用基准测试中超越现有最先进的基线模型，特别是在最具挑战性的任务上取得了高达22%的绝对性能提升。深入研究和指标分析显示，具身强化学习训练促进了更深层次的推理、更有效的工具使用和更高质量的解决方案。这些结果确立了具身强化学习与工具集成作为LLMs鲁棒、可解释和泛化能力强的问题解决新方向的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 06:42:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态奖励模型优化研究</title>
<link>https://arxiv.org/abs/2505.02835</link>
<guid>https://arxiv.org/abs/2505.02835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出StableReinforce算法，通过改进现有强化学习方法显著提升多模态奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 多模态奖励模型（MRMs）对多模态大型语言模型（MLLMs）的表现至关重要，但长期推理能力的有效性及激活方式尚未充分探索。本文将奖励建模问题重新定义为基于规则的强化学习任务，并提出StableReinforce算法，通过改进训练损失、优势估计策略和奖励设计，解决现有强化学习算法在奖励建模中的不稳定性问题。实验中，我们从多个数据集中收集了20万份偏好数据用于模型训练，所提出的R1-Reward奖励模型在VL Reward-Bench和Multimodal Reward Bench上分别实现了8.4%和14.3%的性能提升。此外，随着推理计算资源的增加，R1-Reward的性能进一步增强，证明了强化学习算法在优化多模态奖励模型方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>FormalMATH：大规模数学定理形式化基准及其挑战</title>
<link>https://arxiv.org/abs/2505.02735</link>
<guid>https://arxiv.org/abs/2505.02735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FormalMATH提供了一个涵盖多领域的大型数学形式化问题集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为FormalMATH的大规模数学形式化基准，包含5560个从高中到大学水平的已验证数学问题，涉及代数、微积分等多个领域。为了提高自动形式化的效率，研究提出了结合大语言模型（LLMs）和人类协作的新型自动化流水线，该方法显著降低了人工标注成本。然而，当前最先进的基于LLMs的定理证明器表现有限，在实际采样预算下成功率仅为16.46%，且存在明显的领域偏向性。进一步分析发现，自然语言解题指导在链式推理场景中反而可能引入噪声而非清晰度，这对形式化数学推理提出了新的见解。FormalMATH有望成为评估形式化数学推理能力的重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:37:00 GMT</pubDate>
</item>
<item>
<title>LLaMA-Omni 2：基于大语言模型的高质量实时语音交互</title>
<link>https://arxiv.org/abs/2505.02625</link>
<guid>https://arxiv.org/abs/2505.02625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaMA-Omni 2实现高质量实时语音交互，参数规模从0.5B到14B。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LLaMA-Omni 2系列语音语言模型（SpeechLMs），该系列模型基于Qwen2.5系列，整合了语音编码器和自回归流式语音解码器，参数规模涵盖0.5B到14B。尽管仅使用了20万个多轮对话样本进行训练，LLaMA-Omni 2在多个语音问答和指令跟随基准测试中表现出色，超过了如GLM-4-Voice等之前基于大规模语音数据训练的先进模型。通过将大语言模型的能力扩展到语音领域，LLaMA-Omni 2展示了下一代人机交互中智能语音交互的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 08:53:09 GMT</pubDate>
</item>
<item>
<title>Ming-Lite-Uni：开源多模态框架实现文本到图像生成及指令驱动图像编辑</title>
<link>https://arxiv.org/abs/2505.02471</link>
<guid>https://arxiv.org/abs/2505.02471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Lite-Uni开源框架结合统一视觉生成器和多模态自回归模型，实现文本到图像生成及图像编辑。</p><br /><br /><p><strong>摘要：</strong> Ming-Lite-Uni是一个开源的多模态框架，其核心特性在于新设计的统一视觉生成器和原生多模态自回归模型，旨在整合视觉与语言处理。该框架提供了集成MetaQueries和M2-omni框架的开放源代码实现，同时引入了多尺度可学习标记和多尺度表示对齐策略。通过固定MLLM和可学习扩散模型的结合，Ming-Lite-Uni使原生多模态AR模型不仅能够进行文本到图像生成，还能执行基于指令的图像编辑任务，从而扩展了其能力范围。实验结果表明，Ming-Lite-Uni具有卓越的表现，并展示了其交互过程的流畅性。所有代码和模型权重均公开，以促进社区进一步探索。值得注意的是，这项工作与最近的多模态AI里程碑（如ChatGPT-4o的本机图像生成功能更新）相呼应，强调了像Ming-Lite-Uni这样的统一模型在迈向通用人工智能（AGI）道路上的重要性。目前，Ming-Lite-Uni处于Alpha阶段，未来将进一步优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 04:56:12 GMT</pubDate>
</item>
<item>
<title>基于对比指令优化的图像编辑方法</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像编辑指令构造方法，显著提升编辑效果。</p><br /><br /><p><strong>摘要：</strong> 现有图像编辑数据集因自动化构建方法导致监督信号噪声问题，限制了模型性能。本文提出一种创新解决方案，通过修正编辑指令与图像对的匹配度并引入对比指令增强有效性，无需依赖视觉语言模型或预训练任务。实验表明，该方法在多个基准测试中显著优于现有技术，在Real-Edit基准上比SmartEdit提升9.19%，且所需训练数据和模型规模大幅减少。研究发现编辑模型在推理步骤中有特定生成属性，据此定义统一指导规则，同时利用三元组损失构建对比监督信号进一步提高监督效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 01:19:40 GMT</pubDate>
</item>
<item>
<title>自适应模式学习提升社会智能模拟中的动态推理能力</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应模式学习方法，显著提高社会智能模拟中的推理深度和效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前社会智能模拟中语言代理缺乏动态调整推理深度的能力这一问题，提出了自适应模式学习（Adaptive Mode Learning, AML），该方法根据实时情境从四种思考模式中进行选择（直觉反应到深思熟虑）。核心创新的自适应模式策略优化（AMPO）算法实现了多粒度思考模式设计、上下文感知模式切换及通过深度自适应处理实现令牌高效推理。实验表明，AML在社会智能任务上的表现比现有最先进方法高出15.6%，且推理链条缩短32.8%，优于固定深度的GRPO方法7.0%。这些成果证明了AMPO在实现更人性化适应性推理方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02156" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 11:39:58 GMT</pubDate>
</item>
<item>
<title>TEMPURA框架提升视频因果事件关系理解</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TEMPURA框架，通过因果推理与细粒度时间分割提高视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视觉语言模型在理解因果事件关系及视频的时间定位方面仍面临挑战。为解决此问题，本文提出TEMPURA框架，该框架分为两个阶段：首先利用事件掩码预测推理重构缺失事件并生成因果解释；其次学习视频分割与密集描述以分解视频为非重叠事件并提供精确描述。TEMPURA在VER数据集上进行训练，该数据集包含100万训练实例及50万带有时间对齐事件描述和结构化推理步骤的视频。实验表明，TEMPURA在时间定位和亮点检测基准测试中优于基线模型，证明结合因果推理与细粒度时间分割可显著提升视频理解效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01583" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 17:00:17 GMT</pubDate>
</item>
<item>
<title>Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.01043</link>
<guid>https://arxiv.org/abs/2505.01043</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 02:33:25 GMT</pubDate>
</item>
<item>
<title>基于多层记忆与一致性引导的迭代图像编辑框架</title>
<link>https://arxiv.org/abs/2505.01079</link>
<guid>https://arxiv.org/abs/2505.01079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种支持多对象修改并保持上下文关系的迭代图像编辑方法。</p><br /><br /><p><strong>摘要：</strong> 当前大多数图像编辑技术专注于单一对象修改，难以应对需要多次顺序编辑的复杂场景。针对这一挑战，本文提出了两项创新方案：引入粗糙掩码以自然融合新元素同时保留现有内容，以及支持多轮编辑的一致性维护机制。通过引入分层记忆存储先前编辑的潜在表示和提示嵌入，结合背景一致性引导和跨注意力的多查询解耦技术，该框架能够在复杂的多对象编辑任务中保持高质量的结果。此外，我们构建了一个包含语义对齐指标和交互式编辑场景的新基准数据集，实验结果证明了该方法在减少用户操作负担的同时显著提升了迭代图像编辑的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 03:36:49 GMT</pubDate>
</item>
<item>
<title>Llama-Nemotron系列模型：开放且高效的异构推理模型家族</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-Nemotron提供三种规模模型，兼具高效推理和卓越推理能力。</p><br /><br /><p><strong>摘要：</strong> Llama-Nemotron是一组开放的异构推理模型系列，包含Nano（8B）、Super（49B）和Ultra（253B）三种版本，其推理效率和内存利用率优于当前最先进的模型如DeepSeek-R1。该系列模型通过神经架构搜索、知识蒸馏和持续预训练优化，并经过推理聚焦的后训练阶段，包括监督微调和大规模强化学习。此外，Llama-Nemotron是首个支持动态推理切换的开源模型，用户可在标准聊天模式和推理模式间切换。为了促进开放研究，我们提供了完整的后训练数据集和训练代码库，并在NVIDIA开放模型许可协议下发布模型。这些资源旨在推动模型开发并支持学术界和企业用户。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 21:35:35 GMT</pubDate>
</item>
<item>
<title>基于逆映射学习的大型语言模型评估方法</title>
<link>https://arxiv.org/abs/2504.21117</link>
<guid>https://arxiv.org/abs/2504.21117</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种逆映射学习方法以生成高效的LLM评估提示。</p><br /><br /><p><strong>摘要：</strong> 自然语言生成系统的评估因其输出多样性而具有挑战性，尽管人类评估是标准，但存在不一致性、缺乏标准化及人口统计学偏差等问题，限制了可重复性。基于大型语言模型（LLM）的评估提供了可扩展的替代方案，但对提示设计极为敏感，微小变化可能导致显著差异。本文提出了一种逆映射学习方法，通过从模型输出反向映射到输入指令来学习有效的逆映射，从而实现自动高效生成针对特定模型的评估提示。该方法仅需单一样本即可完成，无需耗时的人工提示工程，提升了效率与鲁棒性，为更稳健、高效的LLM评估开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21117" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 14:56:12 GMT</pubDate>
</item>
<item>
<title>基于图神经网络的信号时态逻辑学习框架TeLoGraF</title>
<link>https://arxiv.org/abs/2505.00562</link>
<guid>https://arxiv.org/abs/2505.00562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合图编码与流匹配的信号时态逻辑学习方法，显著提升复杂任务求解效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对信号时态逻辑(STL)在实际应用中的局限性，设计了一种名为TeLoGraF的新框架，通过图神经网络(GNN)编码器和流匹配技术实现对一般STL规格的学习。研究团队收集了20万组配对演示数据，涵盖四种常见STL模板，并在五个模拟环境中进行测试，包括二维动态模型到复杂的七自由度机械臂及四足机器人导航。实验结果显示，TeLoGraF在STL满足率上优于其他基线算法，推理速度比经典STL规划算法快10-100倍，且适用于任意系统动力学。此外，该方法展现出解决复杂STL问题的能力，并具备处理分布外STL规格的鲁棒性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:40:07 GMT</pubDate>
</item>
<item>
<title>生成式人工智能研究焦点转移及其潜在风险</title>
<link>https://arxiv.org/abs/2505.00174</link>
<guid>https://arxiv.org/abs/2505.00174</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">企业AI研究重心转向模型对齐与测试，部署阶段问题关注减少。</p><br /><br /><p><strong>摘要：</strong> 本研究基于2020年至2025年间9,439篇生成式人工智能论文中的1,178篇安全与可靠性相关文献，对比了领先AI公司（如Anthropic、Google DeepMind等）与顶尖大学（如麻省理工学院、斯坦福大学等）的研究产出。结果显示，企业AI研究正越来越多地聚焦于模型部署前的对齐与评估领域，而对部署阶段的重要议题如模型偏差的关注度有所下降。此外，在高风险应用领域（如医疗、金融、虚假信息传播等），存在显著的研究空白。若缺乏对实际部署AI系统的深入观察，企业集中化趋势可能加剧这些知识鸿沟。因此，建议扩大外部研究人员对部署数据的访问权限，并建立系统化的市场中AI行为观测机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00174" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 16:44:42 GMT</pubDate>
</item>
<item>
<title>X-Cross：一种高效的跨域推荐模型</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Cross通过低秩适配器实现跨域推荐，性能媲美传统方法但参数量减少25%。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“X-Cross”的新型跨域顺序推荐模型，旨在无需大规模重新训练的情况下适应新领域的产品推荐。该模型整合多个特定领域的语言模型，利用低秩适配器（LoRA）进行微调。X-Cross通过动态优化各源语言模型表示，将知识层间传播，从而在保持领域特异性的同时实现跨域适应性。基于Amazon的数据集测试显示，X-Cross在性能上与全参数LoRA相当，但仅需25%的额外参数。在跨域任务中，如从玩具领域迁移到工具、电子或运动领域时，X-Cross表现出色，所需微调数据减少了50%-75%，且在准确性上优于其他跨域基准模型。总体而言，X-Cross为数据受限环境提供了高效、可扩展的跨域推荐解决方案，显著降低了计算开销。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:33:20 GMT</pubDate>
</item>
<item>
<title>PixelHacker：基于扩散模型的图像修复新范式</title>
<link>https://arxiv.org/abs/2504.20438</link>
<guid>https://arxiv.org/abs/2504.20438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的图像修复方法PixelHacker，显著提升复杂结构和语义的一致性。</p><br /><br /><p><strong>摘要：</strong> 图像修复是图像编辑与生成领域的重要研究方向。现有最先进的方法虽引入新颖的注意力机制和轻量化架构，但在处理复杂结构（如纹理、形状、空间关系）和语义一致性（如颜色一致性、对象恢复及逻辑正确性）时仍存在不足，导致伪影和不恰当生成。为解决这一问题，我们设计了一种简单而有效的修复范式——潜在类别引导，并提出了名为PixelHacker的扩散模型。该方法通过构建包含1400万图像-掩码对的大规模数据集，分别编码前景和背景表示，并将其特征注入去噪过程中，最终实现优异的修复效果。实验表明，PixelHacker在Places2、CelebA-HQ和FFHQ等数据集上全面超越现有最先进方法，在结构和语义一致性方面表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 01:28:36 GMT</pubDate>
</item>
<item>
<title>Context Organizer (CORG): 处理跨文档知识关系的新框架</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架CORG，有效处理跨文档复杂知识关系并提升性能。</p><br /><br /><p><strong>摘要：</strong> 在现实世界的语料库中，知识经常在文档间重复出现，但由于命名模糊、过时信息或错误等原因，常常存在不一致性，导致上下文之间形成复杂的相互关系。以往研究表明，语言模型难以应对这些复杂性，通常只能单独关注单一因素。我们把这种关系分为分散、模糊、反事实和重复四类。分析表明，没有单一方法可以同时解决所有这些关系。为此，我们提出了Context Organizer（CORG），该框架将多个上下文组织成独立处理的组，从而让模型高效找到所有相关答案并实现去模糊化。CORG由图构造器、重排序器和聚合器三个关键组件组成。实验结果显示，CORG在性能和效率上均优于现有分组方法，并达到与计算密集型单上下文方法相当的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00023" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 22:40:48 GMT</pubDate>
</item>
<item>
<title>基于离线模拟框架的软件特定技能集生成方法</title>
<link>https://arxiv.org/abs/2504.20406</link>
<guid>https://arxiv.org/abs/2504.20406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大型语言模型生成软件特定技能集的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种结合图神经网络的离线模拟框架，用于生成经过验证的脚本集合，从而解决传统实时代码生成中存在的问题。该框架通过任务创建和技能生成两个组件实现，其中任务创建采用自顶向下和自底向上的方法，而技能生成则利用执行反馈进行优化和验证。实验表明，在Adobe Illustrator中使用该框架可显著提高自动化成功率，减少响应时间和运行时标记成本。这是首次将软件脚本接口作为大型语言模型系统测试平台的尝试，为满足专业软件领域用户需求提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:03:37 GMT</pubDate>
</item>
<item>
<title>空间语音翻译技术：让听觉空间语言无缝转换</title>
<link>https://arxiv.org/abs/2504.18715</link>
<guid>https://arxiv.org/abs/2504.18715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型听戴设备技术，实现实时空间语音翻译并保持方向和音质。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为空间语音翻译的新概念，旨在通过智能听戴设备将佩戴者周围环境中的语音实时翻译成母语，同时保持各发言人的声音方向和独特音色。该技术解决了盲源分离、定位、实时情感翻译及双耳渲染等多个技术难题，并在苹果M2芯片上实现了实时推理。实验表明，尽管存在干扰，我们的模型在BLEU评分上达到22.01，优于现有模型。用户研究进一步验证了系统在真实世界混响环境中的有效性。这一成果标志着将空间感知融入语音翻译领域的第一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 17:58:56 GMT</pubDate>
</item>
<item>
<title>基于深度学习的城市多目标多摄像头车辆跟踪框架</title>
<link>https://arxiv.org/abs/2505.00534</link>
<guid>https://arxiv.org/abs/2505.00534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种高效的深度学习框架解决城市交通多摄像头下的车辆跟踪问题。</p><br /><br /><p><strong>摘要：</strong> 随着智能交通系统中网络摄像头数量的增加，视觉传感器在交通监控、管理和优化中的作用愈发重要。然而，在大规模城市交通场景中实现非重叠摄像头间的车辆目标跟踪与匹配面临诸多挑战，如车辆属性多样性、遮挡、光照变化及阴影等。本文提出了一种名为MO-MCT的高效且经济的深度学习框架，该框架结合Mask R-CNN进行目标检测，利用迁移学习实现跨摄像头车辆再识别，并通过适当的损失函数和距离度量应对各种复杂情况。最终方案采用ResNet-152进行特征提取，并结合Deep SORT算法进行车辆跟踪。该框架在AI City Challenge第5赛道的数据集上进行了评估，涵盖46路摄像头视频流，取得了优秀的性能表现，IDF1得分为0.8289，精确率和召回率分别为0.9026和0.8527。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:00:25 GMT</pubDate>
</item>
<item>
<title>通过自我生成示例提升大语言模型的序列决策性能</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过积累成功经验自动生成示例可显著提高大语言模型在决策任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLM）代理通过学习自身在类似任务上的成功经验来自我提升性能的可能性。与依赖特定任务的知识工程方法不同，我们专注于构建和完善一个由自身生成的示例数据库。实验表明，简单累积训练任务中的成功轨迹即可显著提升三个基准测试的表现：ALFWorld（从73%提升到89%）、Wordcraft（从55%提升到64%）以及InterCode-SQL（从75%提升到79%），相当于初始代理在每项任务允许两到三次尝试时的表现。进一步地，我们引入了两种扩展策略：一是基于种群训练的数据库级选择，二是根据实际效用对单个轨迹进行示例级选择。这些改进使ALFWorld的性能达到了91%，接近采用特定任务组件和提示符的复杂方法。我们的研究证明，自动构建轨迹数据库是一种有吸引力的替代方案，可以避免劳动密集型的知识工程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00234" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 20:48:12 GMT</pubDate>
</item>
<item>
<title>强化学习增强大型语言模型在高功率火箭设计中的应用研究</title>
<link>https://arxiv.org/abs/2504.19394</link>
<guid>https://arxiv.org/abs/2504.19394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升的LLMs在高功率火箭设计优化中超越人类专家。</p><br /><br /><p><strong>摘要：</strong> 本文通过RocketBench基准测试评估大型语言模型（LLMs）在高功率火箭设计中的能力，涉及目标高度优化及精确着陆挑战两个任务。尽管最先进的LLMs展现了强大的基础工程知识，但在结合仿真结果迭代设计时表现不佳，最终性能低于人类水平。然而，通过强化学习增强后，一款7B参数模型不仅优于顶级基础模型，还超过了人类专家的表现。这一研究表明，强化学习训练的LLMs可成为复杂工程优化的有效工具，有望推动工程领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 19:59:39 GMT</pubDate>
</item>
<item>
<title>基于双层推理过程的文本到图像生成模型T2I-R1</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的推理增强型文本到图像生成模型T2I-R1。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为T2I-R1的新模型，该模型通过强化学习和双层链式思维推理过程改进了文本到图像的生成能力。T2I-R1在两个生成阶段引入了两种链式思维策略：语义级用于高级提示规划，令牌级用于低级像素处理。此外，我们提出了BiCoT-GRPO方法，以集成奖励优化这两个推理层级。实验结果显示，在T2I-CompBench和WISE基准测试中，T2I-R1分别提升了13%和19%，甚至超越了最先进的FLUX模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于两阶段框架提升大型语言模型数学解题批评能力的研究</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段框架以增强大型语言模型在数学问题中的批评能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，如何提供准确且可扩展的反馈成为了一个紧迫的问题。本研究专注于提高LLMs在数学解题中的批评能力，现有的批评模型对每一步的批评过于浅显，导致判断准确性低且难以提供足够的反馈。为解决这一问题，我们提出了一个新颖有效的两阶段框架。第一阶段利用Qwen2.5-72B-Instruct生成4500份长篇批评作为监督微调的种子数据；第二阶段通过强化学习进一步优化模型，使用人类标注的数据或通过蒙特卡洛采样获得的自动标注数据。最终开发出的批评模型不仅在多种错误识别基准上显著优于现有模型，还通过更详细的反馈帮助生成器修正错误。此外，该模型在数学解题批评任务中表现出色，为LLMs的自动化监督提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:03:17 GMT</pubDate>
</item>
<item>
<title>KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution</title>
<link>https://arxiv.org/abs/2505.00497</link>
<guid>https://arxiv.org/abs/2505.00497</guid>
<content:encoded><![CDATA[
Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 08:56:17 GMT</pubDate>
</item>
<item>
<title>交互生成视频技术综述及其在多领域的应用与挑战</title>
<link>https://arxiv.org/abs/2504.21853</link>
<guid>https://arxiv.org/abs/2504.21853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">定义交互生成视频(IGV)，并探讨其在游戏、具身AI和自动驾驶中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文首次明确交互生成视频(IGV)的概念，即通过生成技术和互动特性创造高质量视频内容，支持用户通过控制信号和反馈进行互动。IGV已在多个领域崭露头角，包括允许虚拟世界无限探索的游戏、提供物理感知环境合成的具身AI训练场景，以及为自动驾驶提供闭环模拟测试的系统。为了指导未来研究，我们提出了一套全面的框架，将理想中的IGV系统分解为五个核心模块：生成、控制、记忆、动力学和智能。同时，针对实现这些模块的技术挑战进行了系统分析，如实现实时生成、开放域控制、长期一致性维护、精确物理模拟及因果推理集成等。这项工作旨在推动IGV技术向更复杂和实用化的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 10:01:45 GMT</pubDate>
</item>
<item>
<title>TF1-EN-3M：大规模开放道德故事数据集的开创性成果</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次推出包含三百万英文寓言的大规模结构化数据集TF1-EN-3M。</p><br /><br /><p><strong>摘要：</strong> 道德故事长期以来被用于传递价值观，但现代自然语言处理领域缺乏结合连贯叙事与明确伦理教训的大型结构化语料库。为填补这一空白，本文推出了TF1-EN-3M，这是一个由指令微调模型生成的首个人类可读开放数据集，包含三百万英语寓言，每个故事均遵循特定的六槽结构（角色 -> 特质 -> 背景 -> 冲突 -> 解决方案 -> 道德）。通过混合评估管道，该数据集的质量得到了全面验证，其中基于Llama-3的8B参数变体表现最佳，可在消费级GPU上高效生成高质量故事。此数据集及其相关代码和元数据已开源，为研究指令跟随、叙事智能、价值对齐及儿童友好型教育AI开辟了新路径，证明大规模道德叙事不再依赖专有巨型模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:15:28 GMT</pubDate>
</item>
<item>
<title>MediAug：医学影像数据增强统一评估框架</title>
<link>https://arxiv.org/abs/2504.18983</link>
<guid>https://arxiv.org/abs/2504.18983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MediAug框架，综合评估多种混合数据增强方法在医学影像分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学影像数据增强面临的领域差距大及单一任务研究局限性问题，提出了MediAug，一个集成六种基于混合的数据增强技术的统一评估框架。该框架适用于脑肿瘤MRI和眼病眼底图像数据集，并结合卷积神经网络（ResNet-50）和Transformer架构（ViT-B）。实验结果显示，MixUp在ResNet-50上显著提升了脑肿瘤分类精度至79.19%，而SnapMix在ViT-B上达到99.44%；YOCO在ResNet-50上的眼疾分类准确率达到91.60%，CutMix在ViT-B上则提升至97.94%。MediAug为医学影像数据增强提供了全面且可复现的基准工具，代码将在GitHub公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 13:56:56 GMT</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 20:09:31 GMT</pubDate>
</item>
<item>
<title>基于对抗性动态的联合分析优化候选人特征研究</title>
<link>https://arxiv.org/abs/2504.19043</link>
<guid>https://arxiv.org/abs/2504.19043</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种概率分布方法优化政治候选人特征，在对抗性环境中更贴近历史选举结果。</p><br /><br /><p><strong>摘要：</strong> 联合分析是一种实验设计工具，广泛应用于社会科学研究中，尤其在政治分析领域，用于研究选民对候选人特征的多维偏好。本文探讨如何确定最优候选人配置的问题。由于联合实验中的特征组合数量远超观察总数，无法精确识别最优配置。为解决这一识别难题，我们推导出一种最优随机干预策略，该策略旨在通过属性的概率分布实现最有利的平均结果。首先，我们在单一政党优化候选人的情境下进行分析；随后扩展到两个政党同时优化并相互对抗的现实情况。我们将所提方法应用于一项关于美国总统选举投票选择的联合实验。结果显示，与非对抗性方法相比，对抗性环境下的预期结果更符合历史选举结果，且由方法建议的最优策略更可能与实际观察到的候选人匹配。这些发现表明，在联合分析中引入对抗性动态可能为实验数据提供独特的社会科学研究洞见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19043" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 18:35:58 GMT</pubDate>
</item>
<item>
<title>Sadeed：基于轻量解码器模型的阿拉伯文变音标注新方法</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于轻量语言模型的阿拉伯文变音标注方法Sadeed。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sadeed的新方法，该方法通过微调Kuwain 1.5B Hennara等人的解码器-only语言模型，解决了自然语言处理中阿拉伯文变音标注的难题。Sadeed经过精心整理的高质量变音数据集训练，尽管使用了有限的计算资源，但其性能与专有大型语言模型相当，并优于传统模型。此外，我们还指出了当前阿拉伯文变音标注基准测试中的局限性，并提出了新的基准测试SadeedDiac-25，以实现更公平和全面的评估。Sadeed和SadeedDiac-25共同为推进阿拉伯自然语言处理应用提供了坚实基础，涵盖机器翻译、文本转语音及语言学习工具等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:37:24 GMT</pubDate>
</item>
<item>
<title>UniBiomed：基于多模态大语言模型的生物医学图像统一解释框架</title>
<link>https://arxiv.org/abs/2504.21336</link>
<guid>https://arxiv.org/abs/2504.21336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniBiomed实现生物医学图像跨模态统一解释，性能卓越。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniBiomed的新模型，它是首个用于生物医学图像解释的通用基础模型。UniBiomed结合了多模态大语言模型（MLLM）和Segment Anything Model（SAM），实现了临床文本生成与生物医学对象分割的统一。该模型支持十种不同的生物医学成像模式，涵盖了多种任务，如分割、疾病识别、诊断等。为了开发UniBiomed，研究团队创建了一个包含超过2700万张图像及其注释和文本描述的大规模数据集。通过在84个内部和外部数据集上的验证，UniBiomed展示了其在分割、疾病识别、区域感知诊断、视觉问答和报告生成等方面的领先性能。与依赖临床专家的传统方法相比，UniBiomed可以提供自动化且端到端的解释，显著提升了诊断效率，标志着临床工作流程的一次范式转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:51:48 GMT</pubDate>
</item>
<item>
<title>基于子思维分析的大语言模型推理性能提升方法</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究挑战传统大语言模型评价方式，提出通过分析中间子思维步骤提升推理准确性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过逐步推理解决复杂问题，通常通过评估最终答案来衡量模型表现。本研究质疑仅依赖最终答案的有效性，提出一种新方法分析推理过程中的中间子思维步骤。我们通过语言线索将完整推理轨迹分割成多个子思维片段，并从各片段生成可能的答案，通过频率统计发现聚合答案的准确性显著优于直接采用完整推理轨迹的结果。进一步分析显示，子思维答案的一致性可反映模型的置信度与正确性。实验表明，该方法在AIME2024和AIME2025数学推理数据集上分别提升了最高13%和10%的准确率。此方法为提高LLMs推理能力提供了新的思路与工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20708" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:39:07 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理服务优化方法综述</title>
<link>https://arxiv.org/abs/2504.19720</link>
<guid>https://arxiv.org/abs/2504.19720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大规模语言模型推理服务优化的关键技术与未来方向。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其巨大的参数量和注意力机制带来的高计算需求阻碍了低延迟和高吞吐量的实现。本文全面回顾了相关领域的研究进展，涵盖了实例级方法（如模型放置、请求调度等）、集群级策略（如GPU部署和负载均衡）、新兴场景下的具体任务优化及辅助方法，还探讨了若干重要但边缘的研究领域。通过整合这些技术成果，文章旨在为LLM推理服务提供系统化的优化方案，并指出了未来可能的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:14:02 GMT</pubDate>
</item>
<item>
<title>Foundation-Sec-8B：面向网络安全部署的大型语言模型</title>
<link>https://arxiv.org/abs/2504.21039</link>
<guid>https://arxiv.org/abs/2504.21039</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出一款基于Llama架构并增强的网络安全专用大语言模型。</p><br /><br /><p><strong>摘要：</strong> 随着基于Transformer的大规模语言模型（LLMs）在社会各领域的渗透，软件工程、创意写作及数字艺术等领域得到了显著革新。然而，在网络安全领域，由于缺乏专门训练数据且需复杂表示特定知识，其应用受到限制。本文介绍了一款名为Foundation-Sec-8B的新模型，该模型基于Llama 3.1架构构建，并通过精心策划的网络安全语料库进行持续预训练得以增强。我们评估了Foundation-Sec-8B在现有和新网络安全基准上的表现，结果显示它在某些特定任务上与Llama 3.1-70B和GPT-4o-mini的表现相当。通过公开此模型，我们希望推动AI驱动工具在公共和私人网络安全环境中的进步和应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21039" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 04:41:12 GMT</pubDate>
</item>
<item>
<title>ReVision：通过参数化物理知识提升视频生成模型能力</title>
<link>https://arxiv.org/abs/2504.21855</link>
<guid>https://arxiv.org/abs/2504.21855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入ReVision框架，显著提升复杂动作和交互视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReVision的插件式框架，通过将参数化的三维物理知识融入到预训练的条件视频生成模型中，显著增强了生成高质量复杂运动和交互视频的能力。该框架分为三个阶段：首先利用视频扩散模型生成粗略视频；接着提取二维和三维特征构建对象中心的三维表示，并通过参数化物理先验模型优化得到精确的三维运动序列；最后将优化后的运动序列反馈至扩散模型作为附加条件，生成一致性更强的视频。实验表明，ReVision在Stable Video Diffusion上的表现优于参数量更多的现有模型，验证了其在复杂场景中的优越性，为实现逼真的物理视频生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>COMPACT：提升多模态大语言模型复杂视觉-语言任务性能的新方法</title>
<link>https://arxiv.org/abs/2504.21850</link>
<guid>https://arxiv.org/abs/2504.21850</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">COMPACT通过控制训练样本的组合复杂性显著提升多模态大语言模型在复杂任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 当前的多模态大型语言模型（MLLMs）在简单视觉-语言任务上表现出色，但在需要多种能力同时工作的复杂任务中表现欠佳。这种局限性部分源于视觉指令微调（VIT）的传统训练策略过于注重数据量扩展而忽视了训练示例的组合复杂性。为此，我们提出COMPACT（COMPositional Atomic-to-complex visual Capability Tuning），该方法专门设计了一个训练数据集，通过显式控制训练样本的组合复杂性来提高模型学习效率。实验表明，COMPACT在多个基准测试中达到了与LLaVA-665k VIT相当甚至更好的性能，特别是在涉及四种及以上原子能力的复杂问题上，例如在MMStar和MM-Vet任务中分别实现了83.3%和94.0%的显著改进。COMPACT提供了一种可扩展且数据高效的视觉组合微调方案，有效提升了复杂视觉-语言任务的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21850" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:57:22 GMT</pubDate>
</item>
<item>
<title>WebThinker：增强大型推理模型复杂知识密集型任务处理能力</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WebThinker系统，提升大型推理模型在线索推理中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WebThinker的新方法，该方法通过结合深度网络探索模块和自主思考搜索撰写策略，增强了大型推理模型（LRMs）在复杂知识密集型任务中的表现。WebThinker允许LRMs在推理过程中动态搜索网络、导航网页并撰写研究报告，从而克服了传统LRMs依赖静态内部知识的局限性。此外，我们还通过基于强化学习的迭代在线直接偏好优化（DPO）策略进一步提升了研究工具的利用效率。实验表明，WebThinker在多个复杂推理基准测试（如GPQA、GAIA等）和科学报告生成任务中显著优于现有方法及专有系统，大幅提高了LRMs在复杂场景中的可靠性和适用性。本研究代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:25:25 GMT</pubDate>
</item>
<item>
<title>Phi-4-reasoning：高效推理模型在复杂任务中的表现</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-4-reasoning及增强版在多种推理任务上超越更大规模的开放权重模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Phi-4-reasoning，一种拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过监督微调Phi-4结合精心挑选的提示和o3-mini生成的推理演示训练而成，可生成详细的推理链。此外，Phi-4-reasoning-plus版本经过短期基于成果的强化学习进一步优化，推理轨迹更长，性能更高。实验结果显示，这两个模型在数学、科学推理、编码、算法问题解决、规划和空间理解等多个领域显著优于大型开放权重模型如DeepSeek-R1-Distill-Llama-70B，并接近全量DeepSeek-R1模型的表现。有趣的是，这些改进在通用基准测试中也有非平凡的迁移效果。研究还探讨了训练数据的精细筛选对监督微调的重要性，并指出强化学习可进一步提升性能。最后，本报告指出了评估推理模型性能和鲁棒性的潜在改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:05:09 GMT</pubDate>
</item>
<item>
<title>通过系统训练提升小规模语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.21233</link>
<guid>https://arxiv.org/abs/2504.21233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一套系统性训练方法，显著提升了小规模语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一套针对小规模语言模型（SLMs）的系统训练方案，该方案由四个阶段组成：大规模中期训练、高质量长链推理数据的监督微调、基于精心筛选偏好数据集的Rollout DPO以及具有可验证奖励的强化学习。实验中，将此方法应用于Phi-4-Mini模型上，其推理版本Phi-4-Mini-Reasoning在数学推理任务中表现出色，超越了更大规模的模型如DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B。这一成果证明，即使在资源受限的小规模模型中，精心设计的训练方案结合高质量的链式推理数据也能有效提升推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 20:04:35 GMT</pubDate>
</item>
<item>
<title>软选择（Softpick）：一种改进Transformer注意力机制的新方法</title>
<link>https://arxiv.org/abs/2504.20966</link>
<guid>https://arxiv.org/abs/2504.20966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">软选择替代Softmax消除注意力问题并提高量化性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为软选择（Softpick）的新方法，它是Transformer注意力机制中Softmax的改进版本，能够消除注意力沉没现象并减少大量激活值。实验表明，在3.4亿参数规模模型上，软选择与Softmax在标准基准测试中的表现相当，但实现了零沉没率。此外，软选择Transformer产生的隐藏状态峰度显著降低，且生成的注意力图更加稀疏。当进行量化时，使用软选择的模型始终优于Softmax，尤其是在较低位精度下优势更为明显。分析显示，软选择可能为量化、低精度训练、稀疏优化、剪枝和可解释性开辟新的可能性。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:36:18 GMT</pubDate>
</item>
<item>
<title>RoboVerse：机器人学习的综合框架</title>
<link>https://arxiv.org/abs/2504.18904</link>
<guid>https://arxiv.org/abs/2504.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboVerse框架，解决机器人领域数据规模和评估基准标准化问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器人领域数据规模化及评估协议建立的独特挑战，介绍了RoboVerse，一个集模拟平台、合成数据集及统一基准为一体的综合性框架。该框架通过MetaSim基础设施抽象多种仿真环境，支持跨模拟器和机器人形态的无缝转换，并提供高质量、多样化的合成数据。此外，还提出了用于模仿学习和强化学习的统一基准，支持不同泛化水平的评估。实验表明，RoboVerse显著提升了模仿学习、强化学习、世界模型学习及仿真到现实迁移的性能，验证了其数据集和基准的可靠性，成为推动机器人学习发展的稳健解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 08:31:04 GMT</pubDate>
</item>
<item>
<title>通过防御性思维提升大语言模型鲁棒性的研究</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示如何利用增强推理能力提高大语言模型对参考污染攻击的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过增强的大语言模型推理能力，在非推理任务中提升其鲁棒性的方法。特别提出了一种名为“防御性思维”的简单方法，仅需少量结构化且具有防御性的推理示例作为演示，就能显著改善模型的鲁棒性。实验表明，该方法效果显著，例如在Natural Questions任务中，标准提示下GPT-4o的准确率从60%降至3%，而采用防御性思维提示后，其准确率仍能保持在50%。这一发现凸显了该方法的简单性和适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:50:05 GMT</pubDate>
</item>
<item>
<title>LawFlow：构建端到端法律工作流程的数据集</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出LawFlow数据集，用于捕捉法律实践中的动态推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LawFlow的新数据集，该数据集收集了法律学生完成的真实商业实体形成场景中的端到端法律工作流程。与现有专注于输入输出对或线性推理链的数据集不同，LawFlow强调动态、模块化且迭代的推理过程，这些过程反映了法律实践中存在的模糊性、修订及客户导向策略。通过对比人类和大型语言模型（LLMs）生成的工作流程，发现两者在结构、推理灵活性和计划执行方面存在显著差异。研究还表明，法律专业人士更倾向于让AI承担辅助角色，如头脑风暴、盲点识别和提供替代方案，而非直接执行复杂的全流程。基于此，我们提出了结合人类目标的设计建议，旨在通过混合规划、自适应执行和支持决策点来提升AI协作能力。本研究揭示了当前LLMs支持复杂法律工作的局限性，并为开发更协作、推理感知的法律AI系统提供了机会。所有数据和代码均可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 11:01:55 GMT</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:57:08 GMT</pubDate>
</item>
<item>
<title>基于强化学习的自主驾驶特权规划研究</title>
<link>https://arxiv.org/abs/2504.17838</link>
<guid>https://arxiv.org/abs/2504.17838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于单一直观奖励项的新方法，显著提升强化学习在自主驾驶中的性能。</p><br /><br /><p><strong>摘要：</strong> 当前针对自主驾驶的任务大多采用基于规则的方法，但这些方法难以应对长尾问题。强化学习（RL）因其可扩展性且不受累积误差影响而受到关注。然而，传统RL方法依赖复杂的复合奖励设计，导致优化困难。本文提出一种新奖励机制，主要基于路线完成度优化，并通过终止或乘法惩罚违规行为，使PPO算法在更大批量下表现优异。实验表明，该方法在CARLA和nuPlan数据集上分别达到64DS和91.3/90.6的高分，超越其他复杂奖励设计的RL方法，同时大幅提高训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于解释性方法的强化学习从人类反馈中优化奖励分配</title>
<link>https://arxiv.org/abs/2504.16272</link>
<guid>https://arxiv.org/abs/2504.16272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">改进奖励分配机制以提升语言模型对齐性能。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLM）对齐中的强化学习从人类反馈（RLHF）管道通常为序列分配标量奖励，以序列末尾标记作为整体质量的代理指标，导致反馈稀疏且令牌级奖励分配次优。本文将奖励塑造视为专注于令牌级奖励分配的优化问题，提出一种利用SHAP和LIME等解释性方法估计令牌奖励的奖励塑造函数，并采用双层优化框架结合贝叶斯优化和策略训练来处理令牌奖励估计中的噪声。实验表明，更好的令牌级奖励归因平衡在下游任务上优于基线模型，并在训练期间更快找到最优策略。此外，理论上证明了特征可加性属性的解释性方法保持原始奖励的最优策略不变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 17:09:33 GMT</pubDate>
</item>
<item>
<title>基于上下文提示的大规模图像编辑方法</title>
<link>https://arxiv.org/abs/2504.20690</link>
<guid>https://arxiv.org/abs/2504.20690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效且高精度的指令引导图像编辑框架。</p><br /><br /><p><strong>摘要：</strong> 本文解决了基于指令的图像编辑领域中精度与效率之间的权衡问题。通过利用大规模Diffusion Transformer（DiT）的生成能力和上下文感知能力，我们提出了三个创新贡献：一是通过上下文提示实现零样本指令合规的编辑框架；二是引入LoRA-MoE混合微调策略，在不进行大量重新训练的情况下增强灵活性；三是采用视觉-语言模型在推理阶段早期筛选初始噪声，从而提升编辑质量。实验结果显示，该方法在性能上超越现有最先进方法，同时仅需传统基线方法0.5%的训练数据和1%的可训练参数。这项工作开创了一种新的范式，实现了高精度且高效的指令引导图像编辑。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:14:47 GMT</pubDate>
</item>
<item>
<title>基于视觉语言模型的3D目标检测系统性综述</title>
<link>https://arxiv.org/abs/2504.18738</link>
<guid>https://arxiv.org/abs/2504.18738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了视觉语言模型在3D目标检测中的应用与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文对基于视觉语言模型(VLMs)的3D目标检测进行了系统的分析，通过对超过100篇研究论文的考察，首次提供了专门针对这一领域的系统性分析。文章首先概述了VLMs在3D目标检测中的独特挑战，特别是与2D检测相比在空间推理和数据复杂性上的差异。接着比较了传统方法如点云和体素网格与现代框架如CLIP及3D大型语言模型(3D LLMs)，后者支持开放词汇检测和零样本泛化。文中还回顾了关键架构、预训练策略及提示工程方法，这些方法用于有效对齐文本和3D特征。通过可视化示例和评估基准讨论了性能和行为。最后，文章指出了当前存在的问题，例如有限的3D-语言数据集和计算需求，并提出了未来的研究方向以推动VLMs在3D目标检测中的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 19:27:26 GMT</pubDate>
</item>
<item>
<title>X-Fusion：一种保持语言能力的多模态任务扩展框架</title>
<link>https://arxiv.org/abs/2504.20996</link>
<guid>https://arxiv.org/abs/2504.20996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Fusion框架通过双塔设计提升多模态任务性能同时保留语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Fusion的框架，旨在扩展预训练的大规模语言模型（LLMs）以处理多模态任务，同时保持其原有的语言处理能力。X-Fusion采用了一种双塔结构，其中包含针对特定模态的权重，使得在整合视觉信息时可以保持LLM参数不变。实验结果显示，在图像到文本和文本到图像的任务上，X-Fusion的表现优于其他替代架构。研究还发现，引入专注于理解的数据能提高生成质量，减少图像数据噪声可改善整体表现，而特征对齐对较小模型的收敛速度有显著促进作用，但对较大模型影响有限。这些发现为构建高效的统一多模态模型提供了宝贵的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>Chatbot Arena排名体系中的系统性问题及改进建议</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Chatbot Arena排名受私测偏颇影响，导致不公平竞争。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Chatbot Arena作为顶级AI系统排行榜所面临的问题。研究发现，少数公司通过未公开的私测策略，在公开发布前测试多个模型变体并选择最佳成绩，这种做法导致了排行榜评分的偏差。例如，Meta在Llama-4发布前对27个私测变体进行了评估。此外，闭源模型比开源模型获得更多测试机会，造成数据获取的不对等。Google和OpenAI分别占Arena总数据的19.2%和20.4%，而83个开源模型仅占29.7%。这些动态使模型过度适应Arena特定环境，而非整体性能提升。尽管如此，Chatbot Arena仍得益于组织者和开放社区的努力。我们提出了改进评估框架的建议，以实现更公平透明的基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.20630</link>
<guid>https://arxiv.org/abs/2504.20630</guid>
<content:encoded><![CDATA[
Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:56:44 GMT</pubDate>
</item>
<item>
<title>Meta Policy Optimization提升大语言模型奖励对齐的鲁棒性</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Meta Policy Optimization框架解决大语言模型奖励对齐中的奖励黑客和提示工程问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于奖励的大语言模型对齐方法存在的两大局限性——易受奖励黑客影响及高度依赖人工设计的提示工程——提出了一种名为Meta Policy Optimization (MPO) 的新框架。MPO通过集成一个元奖励模型，在训练过程中动态优化奖励模型的提示，从而有效减少奖励信号被模型利用的可能性，同时大幅降低手动设计提示的需求。实验表明，该方法不仅在性能上媲美甚至优于传统方法，而且在多种任务（如问答和数学推理）中表现稳定，无需特定奖励设计。此外，MPO的元学习特性使其可扩展至更高层次的对齐框架，为大语言模型的奖励对齐提供了更稳健且灵活的解决方案。未来，代码和模型将公开共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 14:02:35 GMT</pubDate>
</item>
<item>
<title>TreeHop：一种高效的多跳问答系统</title>
<link>https://arxiv.org/abs/2504.20114</link>
<guid>https://arxiv.org/abs/2504.20114</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需LLMs的嵌入级框架TreeHop，显著提升多跳问答效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对多跳问答（MHQA）中检索增强生成（RAG）系统的挑战，提出了一种名为TreeHop的新框架。传统方法依赖迭代的基于LLMs的查询重写和路由，导致高计算成本。TreeHop通过融合前序查询和文档的语义信息动态更新查询嵌入，在嵌入空间内完成迭代检索，从而避免重复调用LLMs和多阶段处理。此外，引入基于规则的停止准则进一步减少冗余检索，平衡效率与召回率。实验结果显示，TreeHop在三个公开数据集上的表现与先进RAG方法相当，但参数规模仅为5%-0.4%，查询延迟降低约99%。这一成果表明TreeHop是一种更快且更具成本效益的解决方案，适用于知识密集型应用。代码和数据已开源以促进复现研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20114" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:56:31 GMT</pubDate>
</item>
<item>
<title>DICE-Talk：基于解耦身份与情感的高表现力可泛化口型同步虚拟头像生成</title>
<link>https://arxiv.org/abs/2504.18087</link>
<guid>https://arxiv.org/abs/2504.18087</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架DICE-Talk，解决现有情感生成方法的情感表达不足问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于扩散模型的虚拟口型同步技术在唇同步和视觉质量方面取得了显著进步，但在保持说话者身份的同时生成富有情感的表情仍然面临挑战。本文指出当前情感口型生成方法存在三大局限：音频内在情感线索利用不足、情感表示中的身份泄露以及情感相关性孤立学习的问题。为此，我们提出了名为DICE-Talk的新框架，通过解耦身份与情感并协作具有相似特性的感情来解决这些问题。首先，开发了一种解耦的情感嵌入器，通过跨模态注意力共同建模音视频情感线索，将情感表示为与身份无关的高斯分布。其次，引入了一个增强相关性的条件模块，使用可学习的情绪银行通过向量量化和基于注意力的特征聚合显式捕获情绪间关系。第三，设计了一种情绪辨别目标，在扩散过程中通过潜在空间分类强制情感一致性。在MEAD和HDTF数据集上的大量实验表明，该方法在情感准确性方面优于最先进的方法，同时保持了竞争性的唇同步性能。定性结果和用户研究进一步证实了该方法能够生成保留身份且具有丰富相关情感表达的可泛化虚拟头像。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18087" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 01:28:21 GMT</pubDate>
</item>
<item>
<title>BloomScrub：一种高效的大语言模型版权清除方法</title>
<link>https://arxiv.org/abs/2504.16046</link>
<guid>https://arxiv.org/abs/2504.16046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法有效解决大语言模型的版权侵权问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）在预训练阶段接触大量受版权保护材料后可能引发的无意版权侵权问题，探讨了现有“版权清除”技术的局限性，特别是对最坏情况下的长段直接引用处理不足的问题。为此，我们提出了BloomScrub，一种简单而高效的推理阶段方法，通过交替检测并重写潜在侵权片段，利用高效的Bloom过滤器实现大规模文本的版权筛查。该方法不仅显著降低了侵权风险，还保证了模型功能的实用性，并支持根据需求调整执行严格程度。实验表明，这种轻量级的推理时间方法在版权预防方面表现出了令人惊讶的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:16:53 GMT</pubDate>
</item>
<item>
<title>YoChameleon: Personalized Vision and Language Generation</title>
<link>https://arxiv.org/abs/2504.20998</link>
<guid>https://arxiv.org/abs/2504.20998</guid>
<content:encoded><![CDATA[
Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>基于RGB-DN视频的高效四维世界模型学习方法</title>
<link>https://arxiv.org/abs/2504.20995</link>
<guid>https://arxiv.org/abs/2504.20995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过RGB-DN视频学习四维世界模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种有效学习新型四维具身世界模型的方法，该模型能够预测三维场景随时间动态变化的过程，响应具身智能体的动作需求，提供空间和时间的一致性。我们建议通过训练RGB-DN（RGB、深度图和法线图）视频来学习四维世界模型。这种方法不仅超越传统的二维模型，通过整合详细的形状、配置和时间变化到预测中，而且还能有效地学习具身智能体的逆动力学模型。具体来说，我们首先利用现有的机器人操作视频数据集扩展深度和法线信息，然后微调视频生成模型，使其联合预测每帧的RGB-DN。最后，我们提出了一种算法，直接将生成的RGB、深度和法线视频转换为高质量的四维场景。我们的方法确保了具身场景中四维场景预测的时间和空间一致性，实现了具身环境中的新颖视图合成，并促进了显著优于先前基于视频的世界模型的策略学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>UniversalRAG：一种支持多模态异构知识检索的增强型生成框架</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，可从多模态异构知识源中检索并整合信息。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniversalRAG的新框架，该框架旨在通过从异构的多模态知识源中检索和整合相关信息，提升基于检索的生成模型（RAG）的能力。现有RAG方法大多局限于单一模态的知识库，而UniversalRAG则通过引入模态感知路由机制，动态确定最合适的模态特定知识库并进行针对性检索。此外，还对各模态按粒度级别组织，以实现根据查询复杂性和范围进行精细化检索。实验结果显示，在涉及多种模态的8个基准测试中，UniversalRAG的表现优于模态特定和统一基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:18:58 GMT</pubDate>
</item>
<item>
<title>ReasonIR-8B：首个专为推理任务训练的检索器</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonIR-8B通过合成数据显著提升了推理密集型信息检索任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ReasonIR-8B，这是首个专门针对一般推理任务训练的检索器。传统检索器在推理任务中的表现有限，因为现有训练数据集主要关注简单事实查询。我们开发了一种合成数据生成管道，该管道为每份文档创建具有挑战性和相关性的查询，同时结合看似相关但最终无用的硬负样本。通过混合使用合成数据和现有公开数据进行训练，ReasonIR-8B在BRIGHT基准测试中实现了新的最佳性能，nDCG@10分别达到了29.9（无重排序器）和36.9（有重排序器）。此外，在RAG任务中，它相对提高了MMLU和GPQA性能，分别提升了6.4%和22.6%，并更有效地利用了测试时计算资源。我们的训练方法通用且易于扩展到未来的大型语言模型，为此我们开源了代码、数据和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:49:28 GMT</pubDate>
</item>
<item>
<title>通过1-shot强化学习实现大型语言模型数学推理能力的有效提升</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示1-shot强化学习结合可验证奖励显著提升了大型语言模型的数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种基于单训练样本（1-shot）的强化学习方法，即强化学习与可验证奖励（RLVR），其在提升大型语言模型（LLMs）的数学推理能力方面表现出色。通过将此方法应用于基础模型Qwen2.5-Math-1.5B，我们发现仅使用一个示例即可显著提高其在MATH500测试集上的表现，从36.0%提升至73.6%，并在六个常见数学推理基准上平均提高了18.1个百分点。此外，实验表明，这种方法不仅适用于多种模型架构和强化学习算法，还能在多个数学问题上实现类似的效果。进一步研究还揭示了一些有趣的现象，如跨领域泛化能力增强、自我反思频率增加以及在训练精度饱和后持续的测试性能提升（称为“后饱和泛化”）。我们还验证了策略梯度损失在此过程中的关键作用，并强调了探索促进的重要性。最后，单独使用熵损失也能显著提高Qwen2.5-Math-1.5B在MATH500上的表现。这些发现为未来提高RLVR的数据效率提供了启示，并促使重新审视相关领域的进展及其机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:24:30 GMT</pubDate>
</item>
<item>
<title>NORA：高效视觉-语言-动作模型实现机器人实时自主性</title>
<link>https://arxiv.org/abs/2504.19854</link>
<guid>https://arxiv.org/abs/2504.19854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NORA模型，通过减少参数量提升机器人任务执行效率。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言-动作(VLA)模型在零样本场景下表现出色，但存在视觉编码限制导致任务失败的问题，且通常参数规模超过7B，带来高昂计算开销。NORA作为一款3B参数的新型模型，采用Qwen-2.5-VL-3B多模态模型作为基础，结合97万真实世界机器人演示数据及FAST+分词器，显著提升了视觉推理和动作定位能力，同时大幅降低计算资源需求。实验结果显示，NORA在任务表现上超越现有大型VLA模型，成为适用于实时机器人环境的更优解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:47:34 GMT</pubDate>
</item>
<item>
<title>Mem0：基于记忆增强的大语言模型对话一致性优化</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mem0架构解决大语言模型长期对话一致性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在长时间多会话对话中因固定上下文窗口导致的一致性挑战，引入了Mem0这一可扩展的记忆中心型架构。Mem0通过动态提取、整合和检索对话中的关键信息来维持一致性。此外，还提出了利用图结构记忆表示法捕捉复杂关系的方法。在LOCOMO基准测试中，Mem0在单跳、时间序列、多跳及开放领域四个问题类别上均优于六类基线系统。实验显示，Mem0在OpenAI的LLM-as-a-Judge指标上提升了26%，且显著降低了计算开销，相比全上下文方法，延迟降低91%，令牌成本节省超90%。这些成果表明，结构化持久化记忆机制对提升长期对话连贯性至关重要，为高效可靠的LLM驱动AI代理铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:46:35 GMT</pubDate>
</item>
<item>
<title>基于领域适应的Chisel代码生成模型ChiseLLM</title>
<link>https://arxiv.org/abs/2504.19144</link>
<guid>https://arxiv.org/abs/2504.19144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合数据处理和提示引导推理的ChiseLLM模型，显著提升Chisel代码语法正确性和设计变异性。</p><br /><br /><p><strong>摘要：</strong> 随着对特定领域架构需求的增长，敏捷硬件开发方法学得到了快速发展，而像Chisel这样的构造语言因其高级抽象特性成为理想选择。尽管大型语言模型在代码生成方面表现出色，但在Chisel代码生成任务上仍面临语法正确性和设计变异性挑战。本文介绍ChiseLLM，通过数据处理、提示引导推理追踪合成及领域适应模型训练，显著提升了Chisel代码生成性能，语法正确性提升至多26.32%，设计变异性提高47.58%。相关数据集和模型已公开，为基于硬件构造语言的敏捷开发提供高效工具，并为后续研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 03:56:49 GMT</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 21:00:06 GMT</pubDate>
</item>
<item>
<title>RepText：一种无需理解文本的多语言视觉文本生成方法</title>
<link>https://arxiv.org/abs/2504.19724</link>
<guid>https://arxiv.org/abs/2504.19724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RepText模型，提升文本到图像生成模型对非拉丁字母及多语言视觉文本的精确渲染能力。</p><br /><br /><p><strong>摘要：</strong> 尽管当前文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著进展，但其在生成精确且灵活的印刷元素（尤其是非拉丁字母）方面的能力仍受到限制。为解决这些局限性，我们基于一个假设——即文本理解只是文本呈现的充分条件而非必要条件，提出了RepText模型。该模型通过引入字体无关的字符和位置信息，允许用户自定义文本内容、字体和位置，同时结合感知损失和扩散损失提高准确性。此外，在推理阶段，RepText采用噪声字符潜在初始化并使用区域掩码来稳定生成过程。实验表明，RepText在多语言视觉文本生成任务中表现出色，性能优于现有开源方法，与闭源多语言模型相当，同时也讨论了其局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:19:53 GMT</pubDate>
</item>
<item>
<title>利用替代密码研究In-Context Learning中的双模态操作</title>
<link>https://arxiv.org/abs/2504.19395</link>
<guid>https://arxiv.org/abs/2504.19395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入基于替代密码的任务重构，研究In-Context Learning的学习模式。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，In-Context Learning (ICL) 存在任务检索与任务学习两种模式，但区分这两种模式仍具挑战性。本文提出一种基于经典密码学中替代密码的任务重构方法ICL CIPHERS，通过部分替换上下文输入中的标记，使英文句子对人类不直观，但仍保持可逆性。实验显示，大型语言模型在处理具有双射映射的ICL CIPHERS时表现优于非双射基线，为量化ICL中的学习能力提供了新途径。此外，我们分析了模型内部表征，发现其具备解码加密输入的能力。这一研究结果在四个数据集和六种模型上具有一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 20:05:29 GMT</pubDate>
</item>
<item>
<title>基于自博弈批评者的大型语言模型推理可靠性评估方法</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需人工标注的自博弈批评者方法提升大语言模型推理可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self-Play Critic（SPC）的新方法，用于评估大型语言模型（LLM）的推理可靠性，如Chain-of-Thought推理。由于高质量的分步监督数据难以获取且成本高昂，SPC通过两个模型之间的对抗性自博弈游戏进化出评估推理步骤的能力，消除了对人工分步注释的需求。这两个模型分别是“狡猾生成器”和“批评者”，前者生成故意错误的推理步骤以迷惑后者，后者则试图检测这些错误。通过基于游戏结果的强化学习机制，两者迭代改进。实验表明，SPC在三个推理过程基准测试上提升了错误检测能力，并优于其他基线模型，同时在数学推理任务中显著提高了多种LLMs的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 04:45:06 GMT</pubDate>
</item>
<item>
<title>CipherBank：评估大型语言模型在密码学推理中的能力</title>
<link>https://arxiv.org/abs/2504.19093</link>
<guid>https://arxiv.org/abs/2504.19093</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出CipherBank基准测试集，评估LLMs在加密解密任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍CipherBank，这是一个针对大型语言模型（LLMs）在密码学推理任务中表现进行评估的综合基准测试集。CipherBank包含2358个精心设计的问题，覆盖了5大领域和14种子领域的262种独特明文，重点关注隐私敏感和现实场景下的加密需求。从密码学角度来看，该基准涵盖了三大类加密方法，涉及9种不同的算法，从经典密码到自定义加密技术均有涉及。通过在CipherBank上对当前最先进的LLMs（如GPT-4、DeepSeek-V3等）及专注于推理的模型（如o1、DeepSeek-R1）进行评估，发现这些模型在推理能力上存在显著差距，不仅体现在通用聊天型LLMs与推理型LLMs之间，也表现在推理型LLMs在经典密码学解密任务中的表现不足。本研究通过详细分析和错误调查揭示了LLMs在密码推理方面的局限性及改进空间，强调了持续提升LLMs推理能力的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19093" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 23:41:17 GMT</pubDate>
</item>
<item>
<title>VCBENCH：大型视觉语言模型在多模态数学推理中的挑战与评估</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有大型视觉语言模型在处理依赖显式视觉的数学问题时表现有限。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型视觉-语言模型（LVLMs）在整合视觉与语言信息方面取得了显著进展，但在涉及基本数学元素和视觉概念的推理任务上仍存在不足。当前基准测试多集中于特定领域的专业知识评估，而忽视了基础数学推理能力。为填补这一空白，我们开发了VCBENCH，这是一个包含1720个问题的综合基准，涉及六个认知领域，平均每个问题包含近四个图像，强调多图推理。通过对26个最先进的LVLM进行测试，我们发现即使是最优秀的模型，在准确性上也未能超过50%，这揭示了视觉与数学整合方面的持续挑战，并为未来模型改进提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 02:16:38 GMT</pubDate>
</item>
<item>
<title>基于均匀下采样的群等变架构广义化研究</title>
<link>https://arxiv.org/abs/2504.17258</link>
<guid>https://arxiv.org/abs/2504.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了群等变架构中的均匀下采样层的广义化方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在群等变架构（如G-CNNs）中均匀下采样层的一般化应用，特别是在反混淆下的通用有限群信号（特征图）下采样问题。首先，给出了一种根据有限群和下采样率选择合适子群的算法；其次，在给定群和子群的情况下，研究了带限性概念并提出了反混淆操作的实现方式。该方法推广了经典采样理论中的下采样概念，当信号位于循环群时，其等效于理想低通滤波器后接下采样操作。实验表明，将所提出的下采样操作引入G-等变网络中，可提高图像分类任务的准确性，更好地保持等变性并减少模型规模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 01:29:51 GMT</pubDate>
</item>
<item>
<title>MMInference：一种加速多模态长上下文推理的动态稀疏注意力方法</title>
<link>https://arxiv.org/abs/2504.16083</link>
<guid>https://arxiv.org/abs/2504.16083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMInference方法，显著加速视觉语言模型的长上下文推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMInference的动态稀疏注意力方法，旨在解决视觉语言模型(VLMs)在处理长上下文多模态输入时，由于二次注意力复杂度导致的预填充阶段效率低下问题。通过分析视频输入的时间和空间局部性，我们发现了一种独特的Grid模式，并提出了基于排列的方法来利用这一模式并处理模态边界问题。此外，通过离线搜索每个头的最佳稀疏模式，MMInference能够根据输入动态构建稀疏分布。该方法无需对现有VLM管道进行任何修改或微调即可无缝集成，并且在多模态基准测试中表现出高达8.3倍的速度提升，同时保持了准确性。实验涵盖了多种任务，如视频问答、视频描述生成等。我们的代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于可信赖生成的数据引擎TrustGeoGen用于几何问题求解</title>
<link>https://arxiv.org/abs/2504.15780</link>
<guid>https://arxiv.org/abs/2504.15780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为TrustGeoGen的数据引擎，用于生成高质量几何问题数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TrustGeoGen的可扩展数据引擎，专门用于生成几何问题数据。该引擎通过多模态对齐生成图表、文本描述及分步解答，结合形式化验证确保推理路径符合规则，并采用自举机制递归生成复杂性增加的状态。此外，设计的GeoExplore系列算法同时生成多解变体并进行自我反思回溯跟踪。通过形式逻辑验证，生成的GeoTrust-200K数据集保证了模态完整性，且GeoTrust-test测试集显示出极高的评估严格性，当前最先进的模型仅达到49.17%的准确率。训练后的模型在GeoQA上表现出出色的OOD泛化能力，显著减少了逻辑不一致性。此工作为几何问题求解方法的发展奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:45:23 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的手机图形用户界面智能代理综述</title>
<link>https://arxiv.org/abs/2504.19838</link>
<guid>https://arxiv.org/abs/2504.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型如何推动手机GUI自动化向智能化方向发展。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了由大型语言模型（LLMs）驱动的手机图形用户界面（GUI）智能代理的发展历程，从基于脚本的传统自动化转变为具备智能和适应性的系统。首先分析了传统自动化面临的三大挑战：泛化能力有限、维护成本高及意图理解薄弱，并阐述LLMs通过先进的语言理解、多模态感知和稳健决策解决这些问题的方法。接着提出了一种涵盖基础代理框架、建模方法及关键数据集和基准的分类体系。此外，还详细介绍了针对特定任务的架构设计、监督微调及强化学习策略，这些技术连接了用户意图与GUI操作。最后，讨论了该领域尚未解决的问题，如数据集多样性、设备端高效部署、用户定制化及安全问题，为研究人员提供了未来研究方向的前瞻性见解。本文旨在为开发可扩展且用户友好的手机GUI代理提供权威参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:39:25 GMT</pubDate>
</item>
<item>
<title>大型语言模型在医疗建议中的表现与用户交互挑战</title>
<link>https://arxiv.org/abs/2504.18919</link>
<guid>https://arxiv.org/abs/2504.18919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，尽管大型语言模型在医学考试中表现出色，但用户交互影响其实际应用效果。</p><br /><br /><p><strong>摘要：</strong> 全球医疗提供者正在探索使用大型语言模型（LLMs）向公众提供医疗建议。LLMs在医学执照考试中几乎达到满分，但在现实场景中的准确性仍存疑。本研究测试了LLMs是否能帮助公众识别潜在疾病并选择行动方案，在1,298名参与者中进行了实验。单独测试时，LLMs正确识别疾病的比例为94.9%，确定处置方案的比例为56.3%，但当参与者使用相同模型时，识别疾病的比例降至34.5%，确定处置方案的比例降至44.2%，均未显著优于对照组。研究指出用户交互是LLMs在医疗建议中部署的关键挑战。现有医学知识基准和模拟患者互动无法预测人类参与者中出现的问题。因此，我们建议在面向公众部署前进行系统化的人类用户测试，以评估其交互能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 09:32:49 GMT</pubDate>
</item>
<item>
<title>Kimi-Audio Technical Report</title>
<link>https://arxiv.org/abs/2504.18425</link>
<guid>https://arxiv.org/abs/2504.18425</guid>
<content:encoded><![CDATA[
We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:31:46 GMT</pubDate>
</item>
<item>
<title>新一代小型推理模型Pleias-RAG在RAG和搜索领域的突破性进展</title>
<link>https://arxiv.org/abs/2504.18225</link>
<guid>https://arxiv.org/abs/2504.18225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新推出的Pleias-RAG系列模型在多语言开放源检索及引用支持方面表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两种新型的小型推理模型Pleias-RAG-350m和Pleias-RAG-1B，这些模型专门设计用于检索增强生成（RAG）、搜索以及来源摘要。通过在一个大型合成数据集上的训练，这些模型能够模拟从Common Crawl中检索多种多语言开放资源的过程。它们不仅提供直接引用和上下文关联的支持，还重新整合了与RAG工作流相关的多个功能，例如查询路由、查询重写和来源重排序。在标准化的RAG基准测试（如HotPotQA和2Wiki）中，这两种模型的表现优于参数少于40亿的单语言模型，并且与更大规模的流行模型（如Qwen-2.5-7B、Llama-3.1-8B和Gemma-3-4B）竞争。值得注意的是，Pleias-RAG系列是目前唯一能在主要欧洲语言中保持一致RAG性能并确保陈述系统性引用关联的单语言模型。由于其较小的体积和对受限基础设施的良好适应性，这些模型为生成式人工智能开辟了新的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 06:17:04 GMT</pubDate>
</item>
<item>
<title>Transformer LLMs中稀疏注意力的研究：效率-准确性权衡与扩展性分析</title>
<link>https://arxiv.org/abs/2504.17768</link>
<guid>https://arxiv.org/abs/2504.17768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨Transformer模型中稀疏注意力方法的可行性及效率-准确性权衡。</p><br /><br /><p><strong>摘要：</strong> 本文针对Transformer语言模型处理长上下文的能力，研究了无需训练的稀疏注意力方法。通过在多种长序列任务上的实验，我们发现：1）在非常长的序列上，较大的高稀疏模型优于较小的密集模型；2）解码阶段相比填充阶段可以实现更高的稀疏水平并保证准确性；3）没有一种策略在所有任务和阶段中表现最佳，不同场景需要不同的稀疏化单元或预算适应性；4）即使中等稀疏度也可能导致某些任务性能显著下降，表明稀疏注意力并非万能解决方案。此外，我们提出了适用于稀疏注意力的新尺度定律，并验证了这些发现可能超越实验范围。综上所述，稀疏注意力是增强Transformer模型长序列处理能力的关键工具，但需要对性能敏感的应用进行仔细评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17768" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:39:25 GMT</pubDate>
</item>
<item>
<title>针对意大利语优化的英语大型语言模型词汇适应技术</title>
<link>https://arxiv.org/abs/2504.17025</link>
<guid>https://arxiv.org/abs/2504.17025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种新方法优化英语大型语言模型以处理意大利语。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多种词汇适应技术，以提升英语大型语言模型（LLMs）对意大利语的处理能力。传统LLMs虽能处理多语言，但因训练数据混杂或非优化设计，在非英语语言上的表现欠佳。为此，我们提出了语义对齐词汇适应（SAVA）方法，利用神经映射实现词汇替换。通过适配两个模型——Mistral-7b-v0.1和Llama-3.1-8B，分别降低了25%的标记“生育率”和减少10亿参数。实验表明，经过词汇适配后，这些模型在持续少量目标语言训练后即可恢复性能。最后，我们在多项选择和生成任务中测试了适配后的模型能力，显示出显著效果。此研究为多语言LLMs优化提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:12:27 GMT</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo：首个跨文化视频评估基准</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个跨文化、多语言、多领域的视频理解评估基准VideoVista-CulturalLingo。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoVista-CulturalLingo，这是一个旨在弥合文化、语言和领域差距的视频理解评估基准。与现有基准不同，它涵盖了中国文化、北美文化和欧洲文化，问题以中文和英文呈现，并包含来自数百个人类创作领域的视频。该基准包含1389个视频和3134个QA对，并评估了24个最近开源或专有的视频大模型。实验结果显示，现有模型在与中国历史相关的问题上表现较差，在事件定位任务中的时间理解能力有限，而主流模型在科学问题上表现出色，但开源模型在数学问题上的表现较弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 09:47:30 GMT</pubDate>
</item>
<item>
<title>Skywork R1V2：下一代多模态推理模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.16656</link>
<guid>https://arxiv.org/abs/2504.16656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork R1V2引入混合强化学习范式，显著提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> Skywork R1V2作为Skywork R1V的升级版，通过结合奖励模型指导与基于规则策略的混合强化学习框架，解决了复杂推理与广泛泛化之间的平衡难题。此外，R1V2创新性地采用Selective Sample Buffer机制，优化训练效率并缓解了GRPO中的“优势消失”问题。实验表明，R1V2在多项基准测试中表现优异，如OlympiadBench得分62.6、AIME2024得分79.0等，展现了其在开放源代码模型中的领先地位，并逐步缩小与顶级专有系统的性能差距。该模型权重已公开发布，以促进研究的透明度与可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 08:24:10 GMT</pubDate>
</item>
<item>
<title>零样本条件下的个性化视频生成模型</title>
<link>https://arxiv.org/abs/2504.17816</link>
<guid>https://arxiv.org/abs/2504.17816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解耦身份学习与时间动态的个性化视频生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需微调的个性化视频生成模型，通过将身份特定的学习与时间动态解耦，在零样本设置下实现视频定制。传统方法依赖大规模标注数据集，成本高昂且需要大量标注工作，而我们的方法直接利用图像定制数据集训练视频定制模型，并将其分解为两个部分：通过图像定制数据集进行身份注入，以及借助少量未标注视频通过图像到视频训练方法保留时间建模。此外，在图像到视频微调过程中采用随机图像标记丢弃和随机图像初始化以缓解复制粘贴问题，并引入随机切换机制以增强学习效果，避免灾难性遗忘。实验表明，该方法在零样本设置下表现出色，具有较强的主体一致性与可扩展性，优于现有视频定制模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 02:48:31 GMT</pubDate>
</item>
<item>
<title>DianJin-R1：面向金融领域的推理增强框架</title>
<link>https://arxiv.org/abs/2504.15716</link>
<guid>https://arxiv.org/abs/2504.15716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DianJin-R1框架，通过强化推理能力提升金融领域大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对金融领域大语言模型在推理方面的挑战，提出了一种名为DianJin-R1的推理增强框架。该框架通过构建高质量的数据集DianJin-R1-Data（整合CFLUE、FinQA及自有合规语料库），结合结构化输出的方式对Qwen2.5进行微调，显著提升了模型的推理能力。此外，采用Group Relative Policy Optimization方法优化模型的奖励机制，进一步提高了答案的准确性。实验表明，DianJin-R1在复杂金融任务上表现优异，甚至超越多代理系统，为实际应用提供了高效且实用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 05:01:04 GMT</pubDate>
</item>
<item>
<title>BitNet v2：高效部署1比特大语言模型的新框架</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BitNet v2框架，实现1比特大语言模型的4比特激活量化。</p><br /><br /><p><strong>摘要：</strong> 本文针对1比特大语言模型（LLMs）在低比特量化时因激活异常值导致的部署难题，引入了BitNet v2这一创新框架，支持原生4比特激活量化。为解决注意力机制及前馈网络中的异常激活问题，提出了H-BitLinear模块，在量化之前应用在线哈达玛变换，将尖锐分布平滑为更符合高斯分布的形式，从而适应低比特表示。实验表明，从零训练的BitNet v2在8比特激活下性能与BitNet b1.58相当，并且在使用原生4比特激活训练时仅产生极小的性能下降，显著减少了内存占用和计算成本，适用于批量推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:17:52 GMT</pubDate>
</item>
<item>
<title>MMLA基准：多模态语言理解能力的全面评估</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有多模态大模型在认知级语义理解上存在局限性。</p><br /><br /><p><strong>摘要：</strong> 多模态语言分析是理解人类会话语义的重要领域，但当前多模态大型语言模型（MLLMs）在认知级语义理解方面的能力尚待深入研究。本文介绍了一个名为MMLA的新基准，该基准包含超过61,000个多模态语句，涵盖意图、情感、对话行为、情感倾向、说话风格及沟通行为六大维度。通过零样本推理、监督微调和指令微调三种方法对主流LLMs和MLLMs进行评估后发现，即使经过微调的模型在复杂人类语言理解上的准确率也仅在60%-70%之间，凸显了现有MLLMs的局限性。MMLA有望成为探索大语言模型潜力的重要基础，并为推动多模态语言分析领域的发展提供宝贵资源。相关数据集和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 01:25:13 GMT</pubDate>
</item>
<item>
<title>CameraBench：评估与提升摄像机运动理解的数据集与基准</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CameraBench数据集，用于评估和改进摄像机运动理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CameraBench的大规模数据集和基准，该数据集由约3000段互联网视频组成，经过专家的多阶段质量控制标注而成。我们与电影摄影师合作开发了一种摄像机运动基元的分类法，发现某些运动如“跟踪”需要理解场景内容。通过大规模的人类研究量化了人类标注性能，发现领域专业知识和教程培训可以显著提高准确性。使用CameraBench，我们评估了运动恢复结构（SfM）模型和视频-语言模型（VLMs），发现SfM模型难以捕捉依赖场景内容的语义基元，而VLMs则难以捕捉需要精确轨迹估计的几何基元。我们进一步在一个生成式VLM上进行微调，使其兼具两者优势，并展示了其在运动增强的字幕生成、视频问答和视频-文本检索中的应用。我们希望此分类法、基准和教程能够推动未来对视频中摄像机运动理解的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 14:34:57 GMT</pubDate>
</item>
<item>
<title>基于Dual Consistency SAM方法的上下文分割研究</title>
<link>https://arxiv.org/abs/2504.12080</link>
<guid>https://arxiv.org/abs/2504.12080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于SAM的上下文图像及视频分割方法DC-SAM。</p><br /><br /><p><strong>摘要：</strong> 上下文分割（In-context Segmentation）作为小样本学习中的单次分割任务，探索了分割模型的泛化能力，被应用于场景理解和图像/视频编辑等任务。然而，现有的Segment Anything Models在交互式分割中表现优异，但不适用于上下文分割。本文提出Dual Consistency SAM (DC-SAM) 方法，通过提示调优适配SAM和SAM2进行图像和视频的上下文分割。该方法通过提供高质量视觉提示增强SAM提示编码器特征，并设计循环一致性交叉注意力和双分支设计提升性能。此外，还提出了掩码管训练策略以适应所提出的双重一致性方法。尽管DC-SAM主要针对图像设计，但在SAM2的支持下可无缝扩展到视频领域。由于视频领域的上下文分割尚属空白，我们手动整理并构建首个基准数据集IC-VOS，用于评估模型的上下文分割能力。实验结果显示，DC-SAM在COCO-20i上达到55.5 mIoU，在PASCAL-5i上达到73.0 mIoU，并在IC-VOS基准上获得71.52的J&amp;F分数。源代码和基准数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:41:59 GMT</pubDate>
</item>
<item>
<title>DynPose-100K：大规模动态互联网视频的相机姿态标注数据集</title>
<link>https://arxiv.org/abs/2504.17788</link>
<guid>https://arxiv.org/abs/2504.17788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种大规模动态视频相机姿态标注数据集DynPose-100K。</p><br /><br /><p><strong>摘要：</strong> 随着逼真的视频生成和模拟领域的发展，在动态互联网视频上大规模标注相机姿态变得至关重要。然而，由于大多数互联网视频不适合姿态估计，收集这样的数据集极具挑战性。本文介绍了一个名为DynPose-100K的大规模动态互联网视频数据集，其中包含相机姿态注释。为了克服数据收集的困难，我们设计了一个结合特定任务模型和通用模型的过滤管道。此外，通过整合最新的点跟踪、动态掩蔽和运动结构技术，我们在姿态估计方面实现了对现有方法的改进。实验分析表明，DynPose-100K在多个关键属性上具有大规模和多样性，为下游应用的进一步发展开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>DiMeR：一种用于稀疏视图网格重建的解耦双流前馈模型</title>
<link>https://arxiv.org/abs/2504.17670</link>
<guid>https://arxiv.org/abs/2504.17670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新型解耦双流模型DiMeR，显著提升了稀疏视图网格重建的效果。</p><br /><br /><p><strong>摘要：</strong> 随着大规模3D数据集的到来，前馈3D生成模型如大尺度重建模型（LRM）受到广泛关注并取得显著成功。然而，RGB图像往往导致训练目标冲突且缺乏几何重建所需的清晰度。本文重新审视了与网格重建相关的归纳偏差，引入了DiMeR，这是一种新颖的解耦双流前馈模型，用于稀疏视图网格重建。DiMeR通过将输入和框架分解为几何和纹理部分，减少了每个部分的训练难度。几何分支利用法线贴图作为输入，而纹理分支则使用RGB图像获取纹理网格。实验表明，DiMeR在多种任务中表现出色，并在GSO和OmniObject3D数据集上显著优于现有方法，Chamfer距离提升超过30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 11:39:20 GMT</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:44:54 GMT</pubDate>
</item>
<item>
<title>Token-Shuffle：通过维度重排提升基于Transformer的文本到高分辨率图像生成</title>
<link>https://arxiv.org/abs/2504.17789</link>
<guid>https://arxiv.org/abs/2504.17789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Token-Shuffle方法，大幅减少Transformer中的图像tokens数量，实现高效高分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对自回归（AR）模型在图像合成领域的局限性展开研究，主要问题在于其所需图像tokens数量庞大，影响训练和推理效率及图像分辨率。为此，我们提出了Token-Shuffle，一种新颖且简单的方法，通过在视觉编码器生成的低维视觉代码映射到语言词汇表时利用视觉词汇表的维度冗余性，减少输入tokens的数量。具体而言，Token-Shuffle通过在通道维度上合并空间局部tokens来降低输入tokens数量，而token-unshuffle则在Transformer块后解纠缠推断出的tokens，恢复空间排列以输出图像。该策略无需额外的预训练文本编码器，使多模态大语言模型（MLLMs）能够以统一的下一-token预测方式支持极高分辨率（如2048x2048）的图像生成，同时保持高效的训练和推理。实验表明，在GenAI基准测试中，我们的2.7B参数模型在困难提示下获得0.77的整体分数，优于LlamaGen 0.18分，超过LDM 0.15分。大规模的人类评估进一步证明了我们在文本对齐、视觉瑕疵和视觉外观方面的卓越能力。我们希望Token-Shuffle能成为高效高分辨率图像生成的基础设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>结合线性和非线性方法的新一代降维算法</title>
<link>https://arxiv.org/abs/2504.17601</link>
<guid>https://arxiv.org/abs/2504.17601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种结合线性和非线性特性的新型降维技术被提出。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的降维方法，旨在解决现有技术如t-SNE和PCA在表达能力和可解释性之间的权衡问题。该算法通过将高维空间映射到低维空间时结合线性变换和高斯加权的非线性变换，实现了复杂非线性转换的同时保持线性方法的可解释性。文章详细描述了这种架构的设计及其在几何关系保留上的优势，并提出了用于分析学习到的变换的技术，例如识别被抑制的维度以及空间如何被扩展和收缩的方法。此外，为了促进学术界和工业界的广泛应用，强调了开发用户友好型软件包的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 10:26:42 GMT</pubDate>
</item>
<item>
<title>RefVNLI：一种高效可靠的文本到图像生成评估方法</title>
<link>https://arxiv.org/abs/2504.17502</link>
<guid>https://arxiv.org/abs/2504.17502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法RefVNLI，用于同时评估文本对齐和主体一致性。</p><br /><br /><p><strong>摘要：</strong> 当前基于主体驱动的文本到图像生成技术在个性化图像生成和视频角色表现等领域有广泛应用潜力，但缺乏可靠的自动评估手段。现有评估方法通常仅关注任务的一个方面、与人类判断不一致或依赖昂贵的API评估。为解决这一问题，我们提出了RefVNLI，这是一种成本效益高的度量方法，能够在单一预测中评估文本对齐和主体保存。通过在大规模数据集上训练，RefVNLI在多个基准测试和主体类别中表现出色，相较于现有基线方法，在文本对齐和主体一致性方面分别提高了6.4和8.5个百分点。此外，它在处理较冷门概念时也表现出色，与人类偏好的一致性超过87%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 08:44:51 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的高质量视频试穿框架3DV-TON</title>
<link>https://arxiv.org/abs/2504.17414</link>
<guid>https://arxiv.org/abs/2504.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的扩散模型方法，解决复杂服装图案和多变人体姿态下的高质量视频试穿问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频试穿技术在处理复杂的服装图案和多样的人体姿势时，难以生成高质量且时间上一致的结果。本文介绍了一种名为3DV-TON的新框架，该框架基于扩散模型，旨在生成高保真度且时间一致的视频试穿效果。3DV-TON通过生成可动画化的纹理化3D网格作为显式的帧级指导，缓解了模型过于关注外观细节而忽略运动连贯性的问题。具体而言，该方法首先选择关键帧进行初始2D图像试穿，随后重建并同步原始视频姿态的纹理化3D网格。此外，我们引入了一种鲁棒的矩形掩码策略，成功减少了动态人体和服装运动过程中因服装信息泄漏导致的伪影传播。为了推动视频试穿研究的发展，我们还创建了一个名为HR-VVT的高分辨率基准数据集，包含130段具有多样服装类型和场景的视频。定量和定性结果表明，我们的方法在性能上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:12:40 GMT</pubDate>
</item>
<item>
<title>TimeChat-Online：革新实时视频交互的在线VideoLLM</title>
<link>https://arxiv.org/abs/2504.17343</link>
<guid>https://arxiv.org/abs/2504.17343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TimeChat-Online，解决流媒体视频冗余问题并实现高效实时互动。</p><br /><br /><p><strong>摘要：</strong> 随着在线视频平台特别是直播服务的快速发展，在线视频理解系统的需求日益迫切。然而，现有的VideoLLMs在处理完整视频时表现出色，但在流媒体场景下由于无法有效处理密集冗余帧而面临显著限制。本文介绍了一种名为TimeChat-Online的新型在线VideoLLM，它通过创新的Differential Token Drop (DTD)模块解决了流媒体视频中的视觉冗余问题。DTD模块受到人类视觉感知变化盲视现象的启发，能够在过滤冗余内容的同时保留有意义的时间变化。实验表明，DTD可以将视频令牌减少82.8%，同时在StreamingBench上保持98%的性能。此外，TimeChat-Online-139K数据集支持多种交互模式，包括后向追踪、当前感知和未来响应。TimeChat-Online的独特主动响应能力也使其在流媒体基准测试和长视频任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 03:59:46 GMT</pubDate>
</item>
<item>
<title>PaperCoder：基于多智能体大语言模型的论文代码自动生成框架</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PaperCoder通过多阶段生成高质量机器学习论文代码实现。</p><br /><br /><p><strong>摘要：</strong> 尽管机器学习研究快速发展，但相关代码实现往往不可用，导致研究人员难以复现结果或扩展先前工作。近期大型语言模型（LLMs）在理解科学文档和生成高质量代码方面表现出色。受此启发，我们提出了PaperCoder，这是一种多智能体LLM框架，可将机器学习论文转化为功能完善的代码库。PaperCoder分为三个阶段：规划阶段构建高层次路线图、设计系统架构并生成配置文件；分析阶段专注于解析特定实现细节；生成阶段则产生模块化且依赖感知的代码。每个阶段均由专门设计的智能体协作完成。我们在基于模型和人工评估的基础上对PaperCoder进行了评估，以机器学习论文为基础生成代码实现，同时以作者发布的代码库作为真实数据。结果显示，PaperCoder生成的代码质量高且忠实于原论文，尤其在新发布的PaperBench基准测试中表现优异，大幅超越了其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 21:57:01 GMT</pubDate>
</item>
<item>
<title>基于任意顺序补丁生成的自回归图像生成方法</title>
<link>https://arxiv.org/abs/2504.17069</link>
<guid>https://arxiv.org/abs/2504.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的自回归图像生成方法，通过训练模型在任意顺序下生成补丁，显著提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 自回归补丁式图像生成在图像质量和可扩展性方面表现出色，且易于整合到视觉语言模型中。然而，这类模型需要定义补丁生成的顺序，传统上采用从左上角到右下角的光栅扫描顺序。本文指出这种顺序并非最优，因为它无法尊重图像内容的因果关系。为此，我们首先训练模型能够在任意顺序下生成补丁，并在生成过程中推断每个补丁的内容及其位置。其次，利用这些提取出的顺序对任意顺序模型进行微调，从而生成更高质量的图像。实验结果显示，新方法在两个数据集上的生成效果优于传统的光栅扫描方法，且训练成本相近，无需额外标注。关键词：自回归生成、补丁生成、图像质量优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 15:33:58 GMT</pubDate>
</item>
<item>
<title>DyMU：一种高效且无需训练的视觉-语言模型动态计算降低框架</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DyMU框架，通过动态合并视觉标记实现VLM高效计算。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DyMU的高效、无需训练的框架，用于动态减少视觉-语言模型（VLMs）的计算负担，同时保持高任务性能。该方法包含两个关键组件：动态标记合并（DToMe）通过基于图像复杂度合并相似标记来减少视觉标记嵌入的数量；虚拟标记拆分（VTU）则通过高效重建完整序列的注意力动态来模拟大型语言模型的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与现有方法不同，DyMU根据图像内容动态调整标记压缩，且完全无需训练，可轻松应用于大多数最先进的VLM架构。实验表明，DyMU在多种VLM架构上平均减少了32%-85%的视觉标记数量，同时保持了与全长度模型相当的性能。此外，定性分析显示，DToMe可根据图像复杂度有效调整标记减少量，为用户提供更多对计算成本的控制权。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:38:18 GMT</pubDate>
</item>
<item>
<title>IberBench：评估伊比利亚半岛及伊比罗-美洲语言的大语言模型基准</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IberBench是一个针对伊比利亚半岛及伊比罗-美洲语言的大语言模型综合评估基准。</p><br /><br /><p><strong>摘要：</strong> 现有的大型语言模型（LLMs）评估基准主要集中在英语上，对于其他语言尤其是资源有限的语言，评估方法存在不足。这些基准通常忽视了语言多样性，过分关注基础自然语言处理（NLP）能力而忽略了工业相关的任务，并且缺乏动态更新机制。为解决这些问题，我们提出了IberBench，这是一个全面且可扩展的基准，用于评估伊比利亚半岛及伊比罗-美洲语言上的LLMs在基础和工业相关NLP任务中的表现。IberBench整合了来自评估活动和最新基准的101个数据集，涵盖22个任务类别，如情感分析、情绪分析、毒性检测和摘要生成等。此外，该基准通过支持持续更新和社区驱动的模型与数据集提交，解决了当前评估实践中存在的问题。我们对23个参数规模从1亿到140亿的LLMs进行了评估，并提供了关于其优势和局限性的实证见解。研究发现，LLMs在工业相关任务上的表现不如基础任务好，在加利西亚语和巴斯克语上的表现较低，某些任务的结果接近随机水平，而在其他任务上LLMs的表现高于随机但低于共享任务系统。IberBench提供了完整的开源实现，包括数据集标准化和托管、LLMs的增量评估以及公开的排行榜。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:48:25 GMT</pubDate>
</item>
<item>
<title>QuaDMix：统一框架优化大语言模型训练数据的质量与多样性</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuaDMix框架，同时优化大语言模型训练数据质量和多样性。</p><br /><br /><p><strong>摘要：</strong> 现有研究通常分别优化大语言模型（LLMs）训练数据的质量和多样性，忽视了两者之间的权衡关系。本文提出QuaDMix框架，通过统一的数据采样函数，在固定训练预算下平衡质量与多样性。QuaDMix利用多个指标衡量数据质量，并借助领域分类评估整体多样性。通过模拟实验和参数搜索加速框架优化，最终在多个基准测试中实现平均性能提升7.2%，显著优于独立优化策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16511" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 04:36:50 GMT</pubDate>
</item>
<item>
<title>一种结合表征学习与扩散模型的图像生成新框架</title>
<link>https://arxiv.org/abs/2504.16064</link>
<guid>https://arxiv.org/abs/2504.16064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将扩散模型与表征学习结合，提升图像生成质量和训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的图像生成框架，通过引入扩散模型同时建模低级图像潜在表示和高级语义特征，实现了表征学习与生成建模的无缝融合。该方法基于变分自编码器和预训练的自监督编码器，从纯噪声开始生成连贯的图像-特征对，在不显著改变标准扩散变换架构的情况下显著提升了生成质量与训练效率。此外，通过引入“表征引导”的推理策略，进一步增强了图像生成的可控性。实验表明，该方法在条件和无条件生成设置下均表现出色，为表征感知的生成建模开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:41:42 GMT</pubDate>
</item>
<item>
<title>ViSMap：基于元提示的无监督视频摘要技术</title>
<link>https://arxiv.org/abs/2504.15921</link>
<guid>https://arxiv.org/abs/2504.15921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViSMap通过元提示策略生成长视频伪摘要，实现无监督长视频摘要。</p><br /><br /><p><strong>摘要：</strong> ViSMap是一种无需标注的长视频摘要系统，解决了现有模型在稀疏分布事件长视频上的总结难题。传统方法依赖于昂贵的分层监督训练，而ViSMap利用LLMs根据短视频片段描述生成长视频的伪摘要，作为长视频摘要模型的训练数据。该系统采用元提示策略迭代生成并优化伪摘要，使用三个LLMs依次生成、评估和改进伪摘要，以提升质量。实验表明，ViSMap在多个数据集上表现优异，性能接近完全监督的最先进模型，且具有跨领域的泛化能力。代码将在发表后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 10:06:01 GMT</pubDate>
</item>
<item>
<title>Step1X-Edit：开源图像编辑模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.17761</link>
<guid>https://arxiv.org/abs/2504.17761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种开源图像编辑模型Step1X-Edit，性能接近闭源顶级模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像编辑模型发展迅猛，闭源模型如GPT-4o和Gemini2 Flash展现了强大的图像编辑能力。然而，现有开源算法与其存在显著差距。本文发布了一种名为Step1X-Edit的最新图像编辑模型，通过多模态大语言模型处理参考图像和用户指令，利用潜在嵌入与扩散图像解码器结合生成目标图像。为训练该模型，构建了高质量数据生成管道，并开发了基于真实用户指令的GEdit-Bench基准进行评估。实验结果显示，Step1X-Edit大幅超越现有开源基线，接近领先闭源模型的性能，为图像编辑领域做出了重要贡献。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:25:12 GMT</pubDate>
</item>
<item>
<title>UniME：基于多模态大语言模型的表征学习框架</title>
<link>https://arxiv.org/abs/2504.17432</link>
<guid>https://arxiv.org/abs/2504.17432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的两阶段框架UniME，提升多模态表示学习能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对比语言图像预训练（CLIP）框架存在文本截断、孤立编码及组合性不足等问题，限制了其效能。尽管多模态大型语言模型（MLLMs）在视觉-语言理解方面取得了显著进展，但其在迁移学习中的潜力尚未充分挖掘。本文介绍了一种名为UniME的新型两阶段框架，通过从强大的基于LLM的教师模型中进行文本辨别性知识蒸馏，增强MLLM的语言组件嵌入能力，并引入硬负样本增强指令微调进一步改进表征学习。实验结果显示，UniME在多个基准测试和检索任务中均表现出色，具有更强的辨别能力和组合性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:51:52 GMT</pubDate>
</item>
<item>
<title>基于抽象视角变化的视觉语言模型视角感知推理框架</title>
<link>https://arxiv.org/abs/2504.17207</link>
<guid>https://arxiv.org/abs/2504.17207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用心理意象模拟提升视觉语言模型视角感知能力的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个用于视觉语言模型(VLMs)视角感知推理的框架，名为抽象视角变化(APC)。该框架通过模拟心理意象，利用视觉基础模型如物体检测、分割和方向估计来构建场景抽象并实现视角转换，从而有效弥补现代VLMs在视角感知推理上的不足。实验结果显示，该框架在合成图像和真实图像基准测试中显著提升了视角感知推理能力，优于微调的空间推理模型和基于新视图合成的方法。这项研究对提高VLMs的人类水平视觉理解能力具有重要意义，有助于促进人机交互与协作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 22:41:34 GMT</pubDate>
</item>
<item>
<title>Causal-Copilot: An Autonomous Causal Analysis Agent</title>
<link>https://arxiv.org/abs/2504.13263</link>
<guid>https://arxiv.org/abs/2504.13263</guid>
<content:encoded><![CDATA[
Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 14:05:39 GMT</pubDate>
</item>
<item>
<title>构建顶级数学推理模型的方法与实践</title>
<link>https://arxiv.org/abs/2504.16891</link>
<guid>https://arxiv.org/abs/2504.16891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于大规模数据集和创新方法的数学推理模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了我们在AI数学奥林匹克竞赛(AIMO-2)中的获奖方案。我们的方法围绕三个关键支柱展开：首先，创建了一个包含54万独特高质量数学问题及其320万个长推理解决方案的大规模数据集；其次，开发了一种通过迭代训练、生成和质量过滤将代码执行与长推理模型集成的新方法，产生了170万个高质量工具集成推理解决方案；最后，构建了一个从多个候选者中挑选最有可能解决方案的流水线。实验表明，这种生成性解选(GenSelect)显著优于多数投票基线。结合这些技术，我们训练了一系列在数学推理基准测试中达到最先进的模型。为了促进进一步研究，我们以商业许可方式发布了代码、模型以及完整的OpenMathReasoning数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:13:04 GMT</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:53:29 GMT</pubDate>
</item>
<item>
<title>基于渐进语言引导视觉学习的多任务视觉定位框架</title>
<link>https://arxiv.org/abs/2504.16145</link>
<guid>https://arxiv.org/abs/2504.16145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外跨模态模块的多任务视觉定位框架PLVL。</p><br /><br /><p><strong>摘要：</strong> 本文针对多任务视觉定位(MTVG)中的子任务(指表达理解REC和分割RES)提出了渐进语言引导视觉学习框架PLVL。传统方法通过独立特征提取、跨模态交互及独立预测头实现任务处理，但存在语言信息注入不足及任务间关系未有效利用的问题。PLVL不仅深入挖掘视觉模态自身特性，还逐步引入语言信息以增强语言相关视觉特征的学习能力，避免了额外跨模态融合模块的需求。此外，通过分析REC定位中心对RES分割对象区域的潜在帮助，设计了多任务头以实现协作预测。实验表明，PLVL在多个基准数据集上显著优于现有代表性方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:48:12 GMT</pubDate>
</item>
<item>
<title>Tina：基于LoRA高效实现语言模型强推理能力的研究</title>
<link>https://arxiv.org/abs/2504.15777</link>
<guid>https://arxiv.org/abs/2504.15777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Tina模型家族，通过LoRA技术显著降低资源消耗实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何以低成本实现语言模型的强推理能力，提出了Tina这一高效推理模型家族。Tina通过对已有1.5B参数的小型基础模型应用低秩适应（LoRA）技术，在强化学习过程中进行参数高效更新，从而在极小资源投入下展现出卓越的推理性能。实验表明，最佳的Tina模型在AIME24数据集上实现了超过20%的推理性能提升及43.33%的Pass@1准确率，且仅需9美元的后训练与评估成本，较现有顶级模型减少了约260倍的成本。此外，Tina在多个开源推理数据集上的表现验证了其有效性，同时通过消融实验进一步支持了LoRA方法的优势。研究表明，LoRA能够快速使模型适应强化学习奖励的推理结构，同时保留基础模型的知识。为促进开放研究，所有代码、训练日志及模型权重均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:38:00 GMT</pubDate>
</item>
<item>
<title>RePOPE: Impact of Annotation Errors on the POPE Benchmark</title>
<link>https://arxiv.org/abs/2504.15707</link>
<guid>https://arxiv.org/abs/2504.15707</guid>
<content:encoded><![CDATA[
Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:47:59 GMT</pubDate>
</item>
<item>
<title>大型语言模型全栈安全综述</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次提出大型语言模型全生命周期安全概念。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在学术界和工业界的广泛应用，其安全性问题逐渐成为关注焦点。然而，现有研究多聚焦于特定阶段的安全性，缺乏对整个生命周期的全面分析。本文首次引入“全栈”安全的概念，系统探讨LLMs从训练到商业化过程中的安全挑战。通过综合分析超过800篇文献，我们定义了完整的LLMs生命周期，涵盖数据准备、预训练、后训练、部署及商业化的全过程，并提出了包括数据生成、对齐技术、模型编辑及基于LLM的代理系统在内的多个研究方向，为未来研究提供了宝贵的指导。这一工作不仅填补了现有研究的空白，还为构建更加安全可靠的LLMs提供了理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 01:02:49 GMT</pubDate>
</item>
<item>
<title>CRUST-Bench：评估C转Rust编译的基准数据集</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CRUST-Bench数据集，用于评估C代码转安全Rust的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CRUST-Bench，这是一个包含100个C代码仓库及其对应安全Rust接口和测试用例的数据集，旨在评估系统将C代码转换为符合规范且通过测试的安全Rust代码的能力。与仅关注孤立函数的传统方法不同，CRUST-Bench考虑整个项目及其多文件依赖关系，提供明确的Rust接口规格以保证内存安全性，并通过测试用例验证功能正确性。尽管现有最先进的大型语言模型（LLMs）在该任务上表现有限，但研究仍揭示了模型常见的错误类型。CRUST-Bench的成功应用有望推动复杂场景下C到Rust等内存安全语言迁移技术的发展。数据集及代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:33:33 GMT</pubDate>
</item>
<item>
<title>DreamID：基于扩散模型的高效高保真人脸交换技术</title>
<link>https://arxiv.org/abs/2504.14509</link>
<guid>https://arxiv.org/abs/2504.14509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的高保真人脸交换方法DreamID。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DreamID的基于扩散模型的人脸交换方法，该方法通过构建三元组身份组（Triplet ID Group）数据实现了对身份相似性和属性保留的显式监督，解决了传统隐式监督方法的局限性。此外，DreamID采用加速扩散模型SD Turbo实现单次迭代推理，显著提高了训练效率。研究还设计了一个由SwapNet、FaceNet和ID Adapter组成的改进架构，进一步增强了显式监督的效果。实验表明，DreamID在身份相似性、姿态表情保留和图像保真度方面超越了现有最先进方法，并且能在0.6秒内完成512*512分辨率的高质量人脸交换，尤其在复杂光照、大角度和遮挡等挑战场景下表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 02:53:00 GMT</pubDate>
</item>
<item>
<title>基于LLM自适应难度的高质量链式思维数据生成方法</title>
<link>https://arxiv.org/abs/2504.11919</link>
<guid>https://arxiv.org/abs/2504.11919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效生成高质量链式思维数据的方法，显著提升模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种通过构建LLM自适应难度问题数据库并利用DeepSeek-R1生成高质量链式思维(CoT)数据的方法。该方法降低了数据生成成本，提升了模型微调效率。实验验证了该方法在数学竞赛和代码生成任务中的有效性，其中ZMath-32B和ZCode-32B分别在仅使用2k高质量数据的情况下超越了DeepSeek-Distill-32B。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 05:55:34 GMT</pubDate>
</item>
<item>
<title>CheckboxQA：评估视觉语言模型处理复选框能力的新基准</title>
<link>https://arxiv.org/abs/2504.10419</link>
<guid>https://arxiv.org/abs/2504.10419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CheckboxQA 数据集用于提升视觉语言模型对复选框的理解能力。</p><br /><br /><p><strong>摘要：</strong> 在文档处理中，复选框的存在与否直接影响到数据提取和决策制定，然而现有大型视觉语言模型在这方面的表现并不理想。这种不足在法律和技术金融等对细节要求极高的行业中尤为突出。为解决这一问题，我们发布了 CheckboxQA 数据集，该数据集专门设计用于评估并改进模型在复选框相关任务上的性能。通过揭示当前模型的局限性，CheckboxQA 为推动文档理解系统的进步提供了宝贵的资源，在法律科技和金融等领域具有重要意义。数据集已公开发布于 GitHub 平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:06:59 GMT</pubDate>
</item>
<item>
<title>统一信息论框架下的现代机器学习损失函数</title>
<link>https://arxiv.org/abs/2504.16929</link>
<guid>https://arxiv.org/abs/2504.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信息论方程，统一多种现代机器学习损失函数。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于信息论的一般化方程，该方程能够涵盖大量现代机器学习中的损失函数。通过这一框架，我们揭示了几种类别的机器学习方法本质上是在最小化两个条件分布之间的集成KL散度。这种视角展示了聚类、谱方法、降维、对比学习及有监督学习背后隐藏的信息几何结构。此外，该框架促进了新损失函数的开发，并通过理论成果改进了无监督图像分类器，在ImageNet-1K上的表现提升了8%以上，同时提出了有效的去偏方法以优化对比表示学习器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>DreamO：一种支持多类型条件集成的图像定制框架</title>
<link>https://arxiv.org/abs/2504.16915</link>
<guid>https://arxiv.org/abs/2504.16915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DreamO框架，实现多种图像定制任务并灵活整合不同类型条件。</p><br /><br /><p><strong>摘要：</strong> 近年来，关于图像定制的研究展示了大规模生成模型的强大定制能力。然而，大多数方法针对特定任务设计，限制了其对多种条件组合的泛化能力。本文介绍DreamO，这是一种统一的图像定制框架，可支持广泛的任务并无缝集成多种条件。DreamO利用扩散Transformer（DiT）框架处理不同类型输入，并通过构建大规模训练集及引入特征路由约束来精确查询参考图像中的相关信息。此外，设计了占位符策略以控制生成结果中条件的位置。采用渐进式训练策略，分三个阶段提升定制能力并校正质量偏差。实验表明，DreamO在高质量完成定制任务的同时，具有很强的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:41:44 GMT</pubDate>
</item>
<item>
<title>Decoupled Global-Local Alignment框架提升视觉语言对齐模型的复合概念理解</title>
<link>https://arxiv.org/abs/2504.16801</link>
<guid>https://arxiv.org/abs/2504.16801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Decoupled Global-Local Alignment框架，提升CLIP的复合概念理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比学习方法在理解视觉语言复合概念时的局限性，提出了一种名为Decoupled Global-Local Alignment（DeGLA）的新框架。DeGLA通过引入局部对齐机制和自蒸馏策略，在增强模型对关系和属性等复合概念理解的同时，有效缓解了全局对比学习导致的泛化能力下降问题。实验结果显示，DeGLA在多个基准测试中表现优异，相较于现有最佳方法平均提升了3.5%，并在零样本分类任务中取得了13.0%的性能提升。此外，该框架还通过利用大语言模型生成高质量负样本及设计新的对比损失函数，进一步增强了模型的视觉语言对齐能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:20:53 GMT</pubDate>
</item>
<item>
<title>Pre-DPO：通过引导参考模型优化偏好强化学习</title>
<link>https://arxiv.org/abs/2504.15843</link>
<guid>https://arxiv.org/abs/2504.15843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DPO的Pre-DPO训练范式，通过参考模型优化大型语言模型的偏好强化学习。</p><br /><br /><p><strong>摘要：</strong> 直接偏好优化（DPO）简化了大型语言模型（LLMs）从人类反馈中的强化学习过程，但初始策略与参考模型相同的做法可能限制性能。本文提出Pre-DPO，通过引入指导参考模型增强偏好优化，该模型根据样本适配性动态分配权重，提高数据利用率和训练鲁棒性。实验表明，Pre-DPO在AlpacaEval 2.0和Arena-Hard v0.1基准测试中显著提升DPO和简单偏好优化（SimPO）的表现，且无需外部模型或额外数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:39:30 GMT</pubDate>
</item>
<item>
<title>Trillion-7B：高效韩文多语言大模型</title>
<link>https://arxiv.org/abs/2504.15431</link>
<guid>https://arxiv.org/abs/2504.15431</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Trillion-7B通过创新机制实现高效的跨语言知识迁移。</p><br /><br /><p><strong>摘要：</strong> Trillion-7B是一款专注于韩语的多语言大型语言模型，具有极高的token效率。该模型引入了交叉语言文档注意（XLDA）机制，能够有效且高效地将英语知识转移到目标语言如韩语和日语上。通过优化的数据混合、语言特定过滤及定制化分词器构建，Trillion-7B仅用其2万亿训练tokens中的10%进行多语言数据训练，并且仅需59.4K H100 GPU小时完成全部训练。全面评估显示，该模型在四种语言的27个基准测试中展现了强大的多语言性能和卓越的跨语言一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15431" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 16:54:44 GMT</pubDate>
</item>
<item>
<title>VisuLogic：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2504.15279</link>
<guid>https://arxiv.org/abs/2504.15279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisuLogic通过六类视觉推理问题评估多模态大模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大型语言模型（MLLMs）评估往往依赖文本描述并允许语言推理捷径，无法有效衡量真正的视觉中心推理能力。为了解决这一问题，本文提出了VisuLogic，这是一个包含1000个人类验证问题的基准，涵盖了六个类别，如定量变化、空间关系和属性比较。通过在该基准上的评估，发现大多数模型的准确率低于30%，远低于人类的51.4%和随机基线的25%，揭示了这些模型在视觉推理方面存在显著差距。此外，研究还提供了补充训练数据集和强化学习基线以促进进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在处理遮挡模式计数中的挑战与局限性</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入新任务CAPTURe测试视觉语言模型对遮挡物体的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Counting Amodally for Patterns Through Unseen REgions (CAPTURe)的新任务，旨在评估视觉语言模型(VLMs)在面对遮挡物体时的推理和空间理解能力。该任务要求模型通过推断被遮挡区域后的模式来完成计数。CAPTURe分为两部分：CAPTURe-real使用真实物体图像，CAPTURe-synthetic则使用生成的图像。实验评估了四个强大的VLMs(GPT-4o、Intern-VL2、Molmo和Qwen2-VL)，发现这些模型在处理遮挡和非遮挡模式时均表现不佳，尤其是在遮挡情况下表现更差。人类的表现远优于模型，提供遮挡物体位置的辅助信息可提高性能，表明模型在处理遮挡和计数方面都存在困难。这项研究揭示了当前VLMs在理解遮挡模式和空间关系上的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 19:38:43 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的个性化图像合成研究</title>
<link>https://arxiv.org/abs/2504.13162</link>
<guid>https://arxiv.org/abs/2504.13162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示自回归模型在文本转图像生成中的潜力。</p><br /><br /><p><strong>摘要：</strong> 个性化图像合成作为文本到图像生成的重要应用，近年来受到广泛关注。尽管扩散模型在此领域占据主导地位，但自回归模型因其统一的文本和图像建模架构，在个性化图像生成方面尚未得到充分探索。本文提出了一种两阶段训练策略，通过优化文本嵌入和微调Transformer层，使自回归模型在个性化图像生成任务上达到了与领先扩散模型相当的效果。实验表明，这种策略在主体保真度和指令遵循能力方面表现优异，为未来的研究提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:26 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化大型语言模型的决策能力</title>
<link>https://arxiv.org/abs/2504.16078</link>
<guid>https://arxiv.org/abs/2504.16078</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示通过强化学习微调可显著提升大型语言模型的决策能力和探索效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在决策场景中的次优表现问题，分析了贪婪性、频率偏差及知识应用差距等三种常见失败模式。研究表明，利用自我生成的链式思维推理进行基于强化学习的微调可以有效缓解这些问题，从而增强模型的探索能力和行动效果。实验分别在多臂老虎机、上下文老虎机和井字棋等任务中验证了该方法的有效性。此外，文章还研究了经典探索机制如ε-贪心策略以及大型语言模型特有的自我校正和一致性方法，以进一步优化模型的决策性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16078" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:57:14 GMT</pubDate>
</item>
<item>
<title>IPBench：推动大语言模型在知识产权领域的应用评估</title>
<link>https://arxiv.org/abs/2504.15524</link>
<guid>https://arxiv.org/abs/2504.15524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个涵盖知识产权多机制与任务的综合基准IPBench。</p><br /><br /><p><strong>摘要：</strong> 知识产权领域因其技术与法律知识的高度融合而复杂且知识密集，现有处理知识产权任务的大语言模型虽潜力巨大，但相关数据集与基准存在局限性，未能完全契合实际场景需求。为解决这一问题，本文引入了首个全面的知识产权任务分类体系及名为IPBench的大型双语基准，该基准覆盖8种知识产权机制及20项任务，旨在真实反映知识产权应用场景下的理解与生成能力。通过评估16种不同类型的大型语言模型，我们发现即便是表现最佳的模型，在IPBench上的准确率也仅为75.8%，表明仍有较大提升空间。此外，开源知识产权与法律导向模型的表现明显落后于闭源通用模型。为了促进进一步研究，我们公开了IPBench的所有数据与代码，并计划持续更新以更好地模拟知识产权领域的实际挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 22:00:41 GMT</pubDate>
</item>
<item>
<title>高效整合新语言到大型语言模型的方法研究</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可将新语言融入现有大模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新方法，用于将新的目标语言（阿拉伯语）无缝集成到现有的大型语言模型（LLM）中，而不会影响其原有的知识。通过在一个主要基于英语的小型开源模型中注入阿拉伯语数据，开发出参数量为15亿的Kuwain模型，该模型在多种基准测试中的阿拉伯语性能平均提升了8%，同时仅需少量原始模型的数据即可保留其原有知识。这种方法不仅显著降低了多语言模型开发的成本，还展示了高效扩展语言模型的潜力，避免了资源密集型的全面重新训练过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:17:25 GMT</pubDate>
</item>
<item>
<title>DiffVox：一种可微分的音乐人声效果匹配模型</title>
<link>https://arxiv.org/abs/2504.14735</link>
<guid>https://arxiv.org/abs/2504.14735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型可解释的人声效果匹配模型DiffVox。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DiffVox（“可微分人声效果”）的新模型，用于音乐制作中的声效匹配。该模型结合了参数均衡、动态范围控制、延迟和混响等模块，并通过高效的不同iable实现支持基于梯度的参数优化。研究利用MedleyDB和私人收藏的数据集中的435首歌曲进行分析，揭示了声效参数间的强相关性及与McAdams音色维度的联系。统计测试表明参数分布非高斯特性，突显了人声效果空间的复杂性。这些初步发现为未来人声效果建模和自动混音奠定了基础。相关代码和数据集可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 16:52:58 GMT</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>MR. Video：基于MapReduce原理的长视频理解框架</title>
<link>https://arxiv.org/abs/2504.16082</link>
<guid>https://arxiv.org/abs/2504.16082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于MapReduce原则的长视频理解框架MR. Video。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MR. Video的长视频理解框架，它通过Map（独立密集感知短视频片段）和Reduce（联合聚合所有片段信息）两个步骤，有效解决了传统序列到序列视觉语言模型在处理长视频时上下文长度受限的问题。与依赖关键片段选择的现有视频代理相比，MR. Video的Map操作实现了更简单的并行感知，而Reduce则支持更全面的上下文聚合和推理。该框架在LVBench挑战数据集上比最先进的视觉语言模型和视频代理提高了超过10%的准确性。此外，MR. Video还展示了其在短视频字幕生成和问题分析中的应用效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>ReflectionFlow：基于推理时自省的文本到图像扩散模型优化框架</title>
<link>https://arxiv.org/abs/2504.16080</link>
<guid>https://arxiv.org/abs/2504.16080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ReflectionFlow，通过推理时自省提升文本到图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReflectionFlow的新框架，该框架旨在解决现有文本到图像扩散模型在处理复杂场景和精细细节时表现不佳的问题。受大型语言模型中涌现的自我反思能力启发，ReflectionFlow在推理阶段引入三个互补的扩展轴：噪声级扩展、提示级扩展以及创新性的反射级扩展，后者通过提供可操作的反馈来评估并修正之前的生成结果。为了支持反射级扩展，我们构建了一个包含一百万个三元组的大规模数据集GenRef，每个三元组包括一条反馈、一张有缺陷的图像和一张增强后的图像。利用此数据集，我们在最先进的扩散变换器FLUX.1-dev上高效执行了反射调优。实验结果表明，ReflectionFlow显著优于朴素的噪声级扩展方法，在具有挑战性的任务中提供了可扩展且计算高效的高质量图像合成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Describe Anything Model：实现图像和视频的精细化局部描述</title>
<link>https://arxiv.org/abs/2504.16072</link>
<guid>https://arxiv.org/abs/2504.16072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种用于图像和视频精细化局部描述的新模型DAM。</p><br /><br /><p><strong>摘要：</strong> 描述特定区域内的图像和视频细节一直是视觉语言模型面临的基本挑战。本文介绍了一种名为DAM（Describe Anything Model）的模型，专门用于精细化局部描述（DLC）。DAM通过两个关键创新保持局部细节和全局上下文：焦点提示确保目标区域的高分辨率编码，局部视觉主干将精确的定位与其更广泛的上下文相结合。为了解决高质量DLC数据稀缺的问题，我们提出了基于半监督学习的数据管道（DLC-SDP），该管道从现有的分割数据集开始，并扩展到未标记的网络图像。此外，我们还引入了DLC-Bench基准，用于评估DLC而不依赖参考标题。DAM在七个涵盖关键词级、短语级和详细的多句局部图像和视频描述基准上创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:51:41 GMT</pubDate>
</item>
<item>
<title>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</title>
<link>https://arxiv.org/abs/2504.16030</link>
<guid>https://arxiv.org/abs/2504.16030</guid>
<content:encoded><![CDATA[
Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 12:52:09 GMT</pubDate>
</item>
<item>
<title>通过神经符号世界模型提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.15785</link>
<guid>https://arxiv.org/abs/2504.15785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的世界对齐方法提升大型语言模型作为世界模型的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）能否构建出准确的世界模型，并分析了世界模型如何增强LLM代理的表现。主要瓶颈在于LLMs的先验知识与特定环境动态之间的差距。为此，我们提出了无需训练的“世界对齐”方法，通过从探索轨迹中提取并编码动作规则、知识图谱及场景图等符号知识，来补充LLMs的不足。进一步地，我们设计了一个无需强化学习的基于模型的代理WALL-E 2.0，利用模型预测控制框架，采用LLM代理作为高效前瞻优化器，显著提升了新环境中的学习效率。在Mars（类似Minecraft）和ALFWorld（具身室内环境）的开放世界挑战中，WALL-E 2.0在成功率和得分上均大幅超越现有方法，特别是在Mars环境中成功率提升16.1%-51.6%，而在ALFWorld中仅需四轮迭代就达到了98%的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:58:27 GMT</pubDate>
</item>
<item>
<title>Vidi：面向视频编辑的大规模多模态模型</title>
<link>https://arxiv.org/abs/2504.15681</link>
<guid>https://arxiv.org/abs/2504.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vidi模型在长时间视频的时间检索任务上显著优于GPT-4o和Gemini。</p><br /><br /><p><strong>摘要：</strong> 随着互联网上视频成为主要的沟通和表达方式，高质量大规模视频内容的制作需求日益增长。然而，传统模型在处理多模态数据（如视觉、音频、文本）以及灵活输入长度时面临挑战。本文介绍了一种名为Vidi的大规模多模态模型家族，专门针对视频理解与编辑场景。Vidi的第一个版本专注于时间检索任务，即根据文本查询识别输入视频中的时间范围。该模型能够在长达数小时的视频中表现出强大的时间理解能力。为了支持真实场景的全面评估，研究团队还推出了VUE-TR基准，具有视频时长更长、支持音频查询、多样化查询格式、高质量标注及改进的IoU评价指标等五大优势。实验结果显示，Vidi在时间检索任务上显著超越了领先的专有模型如GPT-4o和Gemini，展示了其在视频编辑领域的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:04:45 GMT</pubDate>
</item>
<item>
<title>多语言基准评估的现状与未来：挑战与改进建议</title>
<link>https://arxiv.org/abs/2504.15521</link>
<guid>https://arxiv.org/abs/2504.15521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语在多语言基准中过度代表，提出创建本地化基准的重要性。</p><br /><br /><p><strong>摘要：</strong> 本文基于对2021年至2024年间来自148个国家超过2000个多语言（非英语）基准的分析，探讨了当前多语言基准评估的实践及其不足之处。尽管投入了大量资金，但英语仍占据显著优势，且多数基准源自高资源国家。此外，STEM相关任务与人类评价高度相关，而传统NLP任务的相关性较低。翻译基准不足以反映实际应用需求，本地化基准表现更优。研究指出了当前评估中的六大局限，并提出了指导原则及五大研究方向。最后，呼吁全球合作开发更符合人类需求的应用导向型基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 21:47:37 GMT</pubDate>
</item>
<item>
<title>自适应并行推理框架提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架APR，解决现有推理方法的效率与性能瓶颈。</p><br /><br /><p><strong>摘要：</strong> 现有语言模型推理方法存在显著局限性：序列化推理生成过长输出导致延迟增加及上下文窗口耗尽；而并行推理如自一致性则因协调不足产生冗余计算和有限性能提升。为此，本文提出自适应并行推理（APR），一种端到端推理框架，可整合序列化与并行计算。APR通过spawn()和join()操作实现多线程推理，创新性采用端到端强化学习策略优化推理过程。实验表明，在Countdown推理任务中，APR相比传统方法在相同上下文窗口内表现更优（83.4% vs. 60.0%），扩展性更强且在同等延迟下精度更高（75.2% vs. 57.3%）。APR标志着语言模型自主优化推理过程的新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15466" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 18:29:02 GMT</pubDate>
</item>
<item>
<title>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.15415</link>
<guid>https://arxiv.org/abs/2504.15415</guid>
<content:encoded><![CDATA[
Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 15:53:44 GMT</pubDate>
</item>
<item>
<title>基于平行隐藏解码的高效长度缩放预训练框架</title>
<link>https://arxiv.org/abs/2504.14992</link>
<guid>https://arxiv.org/abs/2504.14992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型框架PHD-Transformer，实现预训练中的高效长度缩放。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的研究表明，在后训练阶段通过长度缩放可以显著提升性能，但这一方法在预训练阶段的应用尚待深入探索。本文介绍了一种名为Parallel Hidden Decoding Transformer（PHD-Transformer）的新框架，该框架能够在预训练期间实现高效的长度缩放，同时保持推理效率。通过引入创新的KV缓存管理策略，PHD-Transformer能够区分原始令牌和隐藏解码令牌，仅保留原始令牌的KV缓存以维持长距离依赖关系，而立即丢弃已使用的隐藏解码令牌，从而在不增加KV缓存大小的情况下实现有效扩展。此外，还提出了两种优化变体：PHD-SWA利用滑动窗口注意力机制保存局部依赖关系；PHD-CSWA则采用分块滑动窗口注意力机制消除预填充时间的线性增长。实验结果表明，该方法在多个基准测试中均表现出一致的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:41:26 GMT</pubDate>
</item>
<item>
<title>基于DiT模型的可控角色动画生成方法</title>
<link>https://arxiv.org/abs/2504.14977</link>
<guid>https://arxiv.org/abs/2504.14977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过基础模型微调解决罕见姿态等挑战的新视角。</p><br /><br /><p><strong>摘要：</strong> 可控角色动画在处理罕见姿态、风格化角色及复杂场景时面临诸多难题，传统方法主要依赖复杂的旁路网络注入姿态与外观指导，但泛化能力有限。本文提出新方法，认为只要基础模型足够强大，通过简单修改并采用灵活微调策略即可有效应对上述挑战。文中介绍的RealisDance-DiT基于Wan-2.1视频基础模型构建，实验表明其性能显著优于现有方法。此外，我们还设计了新的测试数据集，涵盖多样化的现实世界挑战，补充了现有基准数据集，进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:09:21 GMT</pubDate>
</item>
<item>
<title>BookWorld：基于书籍的多智能体社会构建与模拟系统</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BookWorld系统，用于构建和模拟基于书籍的多智能体社会。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步推动了通过多智能体系统进行社会模拟的发展，但对已建立的虚构世界和角色的模拟研究较少。本文介绍BookWorld系统，该系统能够全面构建和模拟基于书籍的多智能体社会，涵盖复杂现实细节如多样化动态角色、世界观、地理约束等。实验表明，BookWorld生成的故事具有创意且质量高，忠实于原作，胜过前人方法。BookWorld还可应用于故事生成、互动游戏和社会模拟等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 04:56:27 GMT</pubDate>
</item>
<item>
<title>CheXWorld：基于自监督学习的胸部X光影像世界模型</title>
<link>https://arxiv.org/abs/2504.13820</link>
<guid>https://arxiv.org/abs/2504.13820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个针对放射影像的自监督世界模型CheXWorld。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CheXWorld，这是首个用于放射影像的自监督世界模型，旨在捕捉医学知识的三个关键方面：局部解剖结构、全局解剖布局及领域变化。通过统一框架同时建模这三个方面，实验表明CheXWorld不仅成功捕捉了这些医学知识维度，还在八个医学图像分类和分割基准上显著优于现有自监督学习方法和大规模医学基础模型。代码和预训练模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:50:43 GMT</pubDate>
</item>
<item>
<title>Progent：一种用于大型语言模型代理的权限控制机制</title>
<link>https://arxiv.org/abs/2504.11703</link>
<guid>https://arxiv.org/abs/2504.11703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Progent通过灵活的权限策略提高大型语言模型代理的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Progent的新方法，这是首个面向大型语言模型（LLM）代理的权限控制机制。Progent的核心是一种领域特定语言，用于在代理执行期间灵活表达权限控制策略。这些策略对工具调用施加细粒度限制，决定哪些工具调用是允许的，并指定回退方案。这种设计使得代理开发者和用户可以根据具体用例创建并确定性地应用适当的策略，从而保证安全性。由于其模块化设计，集成Progent不会改变代理内部结构，只需对代理实现进行最小改动，这提升了其实用性和广泛采用的可能性。此外，我们利用LLM自动生成基于用户查询的策略，并动态更新以增强安全性和实用性。我们的广泛评估表明，Progent能够在三个不同的场景或基准测试中（AgentDojo、ASB和AgentPoison）提供强大的安全保障，同时保持高实用性。此外，我们还进行了深入分析，展示了其核心组件的有效性以及自动化策略生成在对抗适应性攻击时的弹性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 21:58:40 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化工具集成推理中的工具使用效率</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过奖励机制减少工具调用次数并提高工具生产力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过强化学习提升工具集成推理（TIR）的能力，尤其是在优化最终答案正确性的同时考虑工具使用的效率与成本。现有方法往往忽视了工具调用的数量及其带来的计算和财务负担，可能导致不理想的模型行为。为解决这一问题，我们提出了Optimal Tool Call-controlled Policy Optimization (OTC-PO)，这是一种基于强化学习的新框架，旨在鼓励模型以最少的工具调用生成准确的答案。该框架引入了一个综合考虑正确性和工具效率的奖励机制，从而提升了工具的生产力。我们将此框架应用于Proximal Policy Optimization (PPO) 和 Group Relative Preference Optimization (GRPO)，分别得到OTC-PPO和OTC-GRPO。实验结果显示，在多个问答基准测试中，我们的方法将工具调用次数减少了高达73.1%，同时提高了工具生产力达229.4%，且保持了相近的答案准确性。据我们所知，这是首个明确优化TIR中工具使用效率的强化学习框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 01:40:05 GMT</pubDate>
</item>
<item>
<title>推理模型过思考问题的研究与优化</title>
<link>https://arxiv.org/abs/2504.13367</link>
<guid>https://arxiv.org/abs/2504.13367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究推理模型过思考问题并提出THOUGHTTERMINATOR优化方法。</p><br /><br /><p><strong>摘要：</strong> 推理模型在处理传统语言模型难以应对的任务时表现出色，但普遍存在过思考问题，即生成大量不必要的标记，这不仅不提升准确性还浪费计算资源。本文引入问题难度的近似度量方法，揭示了问题难度与最优标记消耗之间的关系，并评估了几种推理模型在有效分配最优标记数量方面的校准程度。研究发现，大多数推理模型在校准方面表现不佳，尤其是在简单问题上。为此，我们构建了DUMB500数据集，包含极其简单的数学、推理、代码及任务问题，同时结合现有前沿基准中的极难问题，对推理模型进行综合评估。最后，我们提出了无需训练的黑盒解码技术THOUGHTTERMINATOR，显著改善了推理模型的校准性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 18:16:30 GMT</pubDate>
</item>
<item>
<title>RF-DETR与YOLOv12在复杂果园环境中的绿果检测对比研究</title>
<link>https://arxiv.org/abs/2504.13099</link>
<guid>https://arxiv.org/abs/2504.13099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RF-DETR在单类和多类绿果检测中均优于YOLOv12。</p><br /><br /><p><strong>摘要：</strong> 本研究对基于Transformer的RF-DETR和基于CNN的YOLOv12两种目标检测模型在复杂果园环境下的绿果检测性能进行了详细比较。通过构建包含单类和多类标注的自定义数据集，评估了模型在处理标签模糊、遮挡及背景融合等动态条件下的表现。RF-DETR凭借DINOv2骨干网络和可变形注意力机制，在全局上下文建模和局部特征提取方面表现出色，尤其在单类检测中取得了0.9464的mAP50高分；而YOLOv12则通过优化计算效率和边缘部署能力，在多类检测中达到了0.6622的mAP@50:95最佳分类效果。此外，RF-DETR展现了更快的收敛速度，尤其是在单类检测中仅需10个训练周期即可达到稳定状态。这些结果表明，RF-DETR更适合应用于精确农业场景，而YOLOv12则适用于需要快速响应的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:08:11 GMT</pubDate>
</item>
<item>
<title>基于语音交互的医学视觉语言模型SilVar-Med及其可解释性研究</title>
<link>https://arxiv.org/abs/2504.10642</link>
<guid>https://arxiv.org/abs/2504.10642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合语音交互的端到端医学视觉语言模型SilVar-Med。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SilVar-Med的端到端语音驱动的医学视觉语言模型（VLM），该模型通过整合语音交互功能，突破了传统文本指令限制，在临床环境中实现更实用的医疗图像分析辅助。此外，针对当前医学图像分析模型缺乏透明推理的问题，我们提出了一个用于异常预测解释的数据集，展示了基于推理的医学图像解释概念验证研究。实验表明，SilVar-Med在提升诊断支持系统的透明度、互动性和临床实用性方面具有重要意义。我们的代码和数据集已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 14:51:37 GMT</pubDate>
</item>
<item>
<title>StyleMe3D：实现3D高斯点云风格迁移的综合性框架</title>
<link>https://arxiv.org/abs/2504.15281</link>
<guid>https://arxiv.org/abs/2504.15281</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出StyleMe3D框架，解决3D高斯点云在风格化场景中的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为StyleMe3D的综合性框架，旨在解决3D Gaussian Splatting (3DGS) 在处理卡通、游戏等风格化场景时遇到的纹理碎片化、语义错位及适应抽象美学能力有限的问题。StyleMe3D通过多模态风格条件、多层级语义对齐和感知质量增强等技术，实现了几何结构完整性和风格一致性。该框架包含动态风格评分蒸馏(DSSD)、对比风格描述符(CSD)、同时优化尺度(SOS)以及可微分的3D高斯质量评估(3DG-QA)四个创新组件。实验表明，StyleMe3D在NeRF合成数据集和tandt db场景数据集上表现优异，不仅保留了几何细节，还确保了场景间的风格一致性，并且具备实时渲染能力，适用于游戏、虚拟世界和数字艺术等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15281" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>Eagle 2.5：面向长上下文多模态学习的前沿视觉语言模型</title>
<link>https://arxiv.org/abs/2504.15271</link>
<guid>https://arxiv.org/abs/2504.15271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的视觉语言模型Eagle 2.5，显著提升长视频和高分辨率图像的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组名为Eagle 2.5的前沿视觉语言模型（VLMs），专门针对长上下文多模态学习进行优化。这些模型解决了长视频理解和高分辨率图像分析中的挑战，提供了一个通用框架来同时处理这两种任务。研究中采用了自动降级采样和图像区域保留两项技术，以确保上下文完整性和视觉细节的保留。此外，还对长上下文数据训练的整个流程进行了多项效率优化。为了促进长视频理解，我们提出了一个新的数据集Eagle-Video-110K，该数据集结合了故事级和片段级标注。实验结果显示，Eagle 2.5在长上下文多模态基准测试中表现出色，其中最佳模型Eagle 2.5-8B在Video-MME测试中达到了72.4%的准确率，这一成绩与顶级商业模型如GPT-4o以及大规模开源模型如Qwen2.5-VL-72B和InternVL2.5-78B相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>Quicksviewer：基于动态时间密度划分的高效多模态模型</title>
<link>https://arxiv.org/abs/2504.15270</link>
<guid>https://arxiv.org/abs/2504.15270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多模态模型Quicksviewer，通过自适应时间密度划分实现视频高效理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Quicksviewer的大型多模态模型，该模型采用全新的感知范式，通过Gumbel Softmax将非均匀密度的视频分割成不同大小的立方体，并对每个立方体进行统一重采样，从而提高视频理解效率。这种方法能够在线动态压缩视频，显著减少时空冗余（总体压缩率可达45倍），同时支持大感受野下的高效训练。Quicksviewer通过三个渐进阶段进行训练，在平均420秒/1帧的长时间视频上表现出色。仅使用0.8M视频-文本样本进行训练，该模型在准确性上比采用固定划分策略的基线高出最多8.72分，同时在Video-MME基准测试中达到SOTA性能，且所需令牌数量仅为基线的5%。此外，实验验证了立方网络生成的片段有助于分析视频中的连续事件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:21 GMT</pubDate>
</item>
<item>
<title>基于推理的查询级元代理FlowReasoner自动化设计</title>
<link>https://arxiv.org/abs/2504.15257</link>
<guid>https://arxiv.org/abs/2504.15257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于推理的元代理FlowReasoner，显著提升多智能体系统设计性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlowReasoner的查询级元代理，旨在自动化设计针对每个用户查询的多智能体系统。核心理念是通过外部执行反馈激励基于推理的元代理。首先通过DeepSeek R1提炼出生成多智能体系统的初步推理能力，然后借助强化学习进一步优化，设计了多用途奖励函数从性能、复杂性和效率角度引导训练。实验表明，FlowReasoner在工程和竞赛代码基准测试中优于其他模型，在三个基准上比o1-mini提升了10.52%的准确率。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:35:42 GMT</pubDate>
</item>
<item>
<title>DRAGON框架：一种灵活的生成模型微调方法</title>
<link>https://arxiv.org/abs/2504.15217</link>
<guid>https://arxiv.org/abs/2504.15217</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRAGON是一种比传统RLHF更灵活的生成模型微调框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DRAGON（Distributional RewArds for Generative OptimizatioN），一种用于优化生成模型以实现特定目标的多功能框架。与传统的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）等方法相比，DRAGON更加灵活，能够处理个体示例或其分布的奖励函数评估，支持多种奖励类型。通过利用这一灵活性，研究者们设计了新颖的奖励函数，结合跨模态编码器如CLAP，并通过对比正负样本集最大化奖励。实验表明，DRAGON在多个目标奖励下平均胜率为81.45%，且无需依赖人类偏好标注即可达到较高的音乐质量。这项工作展示了如何通过设计和优化奖励函数来提升生成模型的人类感知质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15217" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 12:41:40 GMT</pubDate>
</item>
<item>
<title>EasyEdit2：支持大语言模型实时行为调控的新框架</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyEdit2 是一种新框架，可轻松实现大语言模型的行为控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为 EasyEdit2 的新框架，该框架旨在为大型语言模型（LLM）的行为控制提供插件式调节功能。与前代相比，EasyEdit2 引入了一种专门设计的新架构，用于平滑的模型操控。它包含多个关键模块，如操控向量生成器和应用器，允许用户通过生成并应用操控向量来影响模型行为，而无需修改模型参数。框架支持多种测试时干预，包括安全性、情感、人格、推理模式、事实性和语言特性等。此外，EasyEdit2 用户界面友好，仅需单个示例即可引导和调整模型响应，使精确控制变得简单高效。实验表明，该框架在多种 LLM 上均表现良好。源代码已公开于 GitHub，并附有演示笔记本和介绍视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:33:55 GMT</pubDate>
</item>
<item>
<title>RainbowPlus：基于进化计算的大语言模型红队框架</title>
<link>https://arxiv.org/abs/2504.15047</link>
<guid>https://arxiv.org/abs/2504.15047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RainbowPlus框架，显著提升大语言模型对抗性提示生成的效率与多样性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）表现出强大的能力，但容易受到对抗性提示的影响，从而产生不安全或有偏见的输出。现有红队方法通常面临可扩展性差、资源消耗高或攻击策略多样性不足的问题。本文提出RainbowPlus，这是一种基于进化计算的新红队框架，通过适应性的质量-多样性（QD）搜索算法改进对抗性提示生成。该框架采用多元素存档存储多样化的高质量提示，并使用综合适应度函数同时评估多个提示，从而克服了先前QD方法中的单一提示存档和成对比较的限制。实验表明，RainbowPlus在六个基准数据集和四种开源LLMs上的攻击成功率（ASR）和多样性均优于其他QD方法。此外，在HarmBench数据集上，RainbowPlus对十二个LLMs的平均ASR达到81.1%，比最先进的AutoDAN-Turbo高出3.9%，且运行速度快9倍。RainbowPlus的开源实现为LLM安全性评估提供了可扩展工具，促进了相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 08:04:57 GMT</pubDate>
</item>
<item>
<title>LUFFY框架：通过离线策略指导提升大规模推理模型的泛化能力</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入LUFFY框架，结合离线策略推理轨迹显著提升了数学基准测试中的性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，大规模推理模型可以通过简单的基于规则奖励的强化学习实现复杂的推理行为。然而，现有的零样本强化学习方法受限于仅能依赖自身输出进行学习，难以突破初始能力限制。为解决这一问题，我们提出了LUFFY框架，该框架通过整合离线推理轨迹扩展了零样本强化学习。LUFFY在训练过程中动态平衡模仿与探索，采用正则化的重要性采样进行策略塑造，避免了混合策略训练中的浅层模仿。实验表明，LUFFY在六个数学基准测试中平均提升了超过7.0分，在分布外任务中领先6.2分以上，且在泛化能力上显著优于监督微调方法。分析显示，LUFFY不仅有效模仿了现有知识，还实现了超出示范范围的探索，为大规模推理模型的可扩展训练提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 04:09:13 GMT</pubDate>
</item>
<item>
<title>Uni3C：统一3D增强框架实现视频生成中摄像机与人体运动的精确控制</title>
<link>https://arxiv.org/abs/2504.14899</link>
<guid>https://arxiv.org/abs/2504.14899</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架Uni3C，实现视频生成中摄像机与人体运动的同步精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成方法中摄像机与人体运动控制分离的问题，提出了Uni3C，这是一种基于3D增强的统一框架。Uni3C通过引入PCDController模块，利用未投影点云进行摄像机控制，该模块无需重新训练视频生成的基础模型，表现出强大的泛化能力。此外，Uni3C还设计了一种联合对齐的3D世界引导机制，在推理阶段整合场景点云和SMPL-X人物模型，实现摄像机与人体运动信号的统一控制。实验表明，Uni3C在摄像机可控性和人体动作质量方面均优于现有方法，并通过定制验证集进一步验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14899" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 03:10:41 GMT</pubDate>
</item>
<item>
<title>TAPIP3D：一种新颖的单目RGB和RGB-D视频中的长期3D点跟踪方法</title>
<link>https://arxiv.org/abs/2504.14717</link>
<guid>https://arxiv.org/abs/2504.14717</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TAPIP3D方法，通过三维空间特征云实现长期3D点跟踪。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TAPIP3D的新方法，用于在单目RGB和RGB-D视频中的长期3D点跟踪。该方法将视频表示为相机稳定的空间-时间特征云，并利用深度和相机运动信息将二维视频特征提升到三维世界空间中，从而有效消除相机运动影响。TAPIP3D在这一稳定表示中迭代优化多帧3D运动估计，实现了长时间的稳健跟踪。为了处理3D点分布的固有不规则性，我们提出了局部成对注意力机制，这种三维上下文策略有效地利用了空间关系，形成了精确的3D轨迹估计所需的有用特征邻域。实验结果显示，我们的方法显著优于现有的3D点跟踪方法，并且在有准确深度的情况下，甚至提高了2D跟踪精度。此外，该方法支持在相机坐标系和世界坐标系下的推理，并证明了补偿相机运动可以提高跟踪性能。相比之前的2D和3D跟踪器使用的传统2D方形相关邻域，TAPIP3D提供了更鲁棒和准确的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14717" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 15:09:43 GMT</pubDate>
</item>
<item>
<title>LeetCodeDataset：面向代码生成模型的高质量基准数据集</title>
<link>https://arxiv.org/abs/2504.14655</link>
<guid>https://arxiv.org/abs/2504.14655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出用于评估和训练代码生成模型的高质量基准数据集LeetCodeDataset。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LeetCodeDataset的新数据集，该数据集旨在应对LLM研究中的两个关键挑战：缺乏注重推理的编程基准和自包含的训练测试平台。通过精心整理具有丰富元数据的LeetCode Python问题，并提供每题超过100个测试用例及时间分割（2024年7月前/后），该数据集实现了无污染评估和高效的监督微调（SFT）。实验表明，具备推理能力的模型显著优于非推理模型，而仅使用2.6K模型生成解决方案的SFT表现可媲美拥有110K样本的数据。数据集及其评估框架已在Hugging Face和Github上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 11:28:16 GMT</pubDate>
</item>
<item>
<title>UFO2：面向Windows桌面的多智能体操作系统实现可靠自动化</title>
<link>https://arxiv.org/abs/2504.14603</link>
<guid>https://arxiv.org/abs/2504.14603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UFO2通过多智能体架构提升计算机使用代理的实用性。</p><br /><br /><p><strong>摘要：</strong> 近期基于多模态大语言模型的计算机使用代理（CUAs）为自然语言驱动的复杂桌面工作流自动化提供了新方向，但现有CUAs多为概念性原型，受制于浅层操作系统集成、脆弱的基于截图交互及干扰性的执行方式。本文提出UFO2，这是一种针对Windows桌面的多智能体操作系统，将CUAs转化为实用的系统级自动化工具。UFO2采用中心化的HostAgent进行任务分解与协调，并配备一系列应用专用AppAgent，具备原生API、领域特定知识及统一的图形用户界面-应用程序接口动作层。该架构不仅实现了稳健的任务执行，还保持了模块化与可扩展性。通过融合Windows UI自动化与基于视觉解析的混合控制检测管道，支持多样化的界面风格。此外，通过推测多动作规划优化运行效率，减少每步大语言模型开销。最后，嵌套式画中画界面允许代理与用户在隔离虚拟桌面中并发操作。实验评估显示，UFO2在超过20款真实Windows应用中显著提升了鲁棒性和执行准确性，证明了深度操作系统集成是实现可靠用户导向桌面自动化的可扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 09:04:43 GMT</pubDate>
</item>
<item>
<title>SphereDiff：一种无缝全景图像与视频生成方法</title>
<link>https://arxiv.org/abs/2504.14396</link>
<guid>https://arxiv.org/abs/2504.14396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的新方法，解决全景图像生成中的极点失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SphereDiff的创新方法，用于通过最先进的扩散模型生成无缝的360度全景图像和视频。传统方法因等距矩形投影(ERP)引起的严重畸变面临挑战，而SphereDiff定义了一个球面潜在表示，确保所有视角下均匀分布，从而减轻ERP固有的畸变。该方法扩展了多扩散机制至球面潜在空间，并提出了球面潜在采样方法，使预训练扩散模型能够直接应用。此外，引入了畸变感知加权平均技术，进一步提升投影过程中的生成质量。实验表明，SphereDiff在生成高质量全景内容方面优于现有方法，适用于增强现实/虚拟现实(AR/VR)等沉浸式应用。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 15:59:11 GMT</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 05:25:55 GMT</pubDate>
</item>
<item>
<title>通过人机演示提升移动GUI代理性能</title>
<link>https://arxiv.org/abs/2504.13805</link>
<guid>https://arxiv.org/abs/2504.13805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人演示的学习方法，显著提高移动GUI代理在多样化场景中的性能。</p><br /><br /><p><strong>摘要：</strong> 移动图形用户界面（GUI）代理在自动化任务方面展现出潜力，但面临现实世界多样场景中泛化能力不足的问题。传统方法通过大规模数据集进行预训练或微调难以应对移动应用及用户特定任务的多样性。本文提出通过人类演示增强移动GUI代理能力，专注于提升其在未见过场景中的表现而非追求更大数据集的普遍泛化。为此，我们引入LearnGUI，这是首个专门用于研究基于演示学习的移动GUI代理的综合数据集，包含2,252个离线任务和101个在线任务的人类高质量演示。同时，开发了LearnAct，这是一种复杂的多智能体框架，可自动从演示中提取知识以增强任务完成。该框架结合了三个专业化智能体：DemoParser用于知识提取，KnowSeeker用于相关知识检索，ActExecutor用于演示增强的任务执行。实验结果显示，在离线和在线评估中均取得了显著性能提升。例如，在离线评估中，单个演示使Gemini-1.5-Pro的准确性从19.3%提高到51.7%；在线评估中，框架将UI-TARS-7B-SFT的任务成功率从18.1%提高到32.8%。LearnAct框架和LearnGUI基准确立了基于演示学习作为更适应性、个性化且可部署的移动GUI代理的有前景方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:13:34 GMT</pubDate>
</item>
<item>
<title>强化学习中工具使用奖励设计的研究与实践</title>
<link>https://arxiv.org/abs/2504.13958</link>
<guid>https://arxiv.org/abs/2504.13958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出强化学习中工具使用任务的奖励设计方案，显著提升大语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型(LLMs)通过有监督微调(SFT)获取工具使用能力，但难以泛化到复杂场景。强化学习(RL)的最新进展显示了优秀的推理和泛化能力，但在工具使用奖励设计上面临挑战，如多种工具参数及反馈不足问题。本研究首次系统性探索强化学习框架下的工具选择和应用任务奖励策略，分析其类型、尺度、粒度及时序动态，提出面向工具使用的奖励设计方案并结合分组相对策略优化(GRPO)训练模型。实验表明该方法比基础模型提高17%，比SFT模型提高15%，强调了奖励设计对提升LLMs工具使用能力和泛化性能的重要性。所有代码已公开以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 17:45:32 GMT</pubDate>
</item>
<item>
<title>基于单目摄像机的多人3D姿态检测与跟踪方法</title>
<link>https://arxiv.org/abs/2504.12186</link>
<guid>https://arxiv.org/abs/2504.12186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种从单目视频流中检测与跟踪多人3D姿态的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种利用单目摄像机流检测并追踪多人详细3D姿势的方法。系统能够在拥挤场景中处理复杂姿势和遮挡情况，维持时间上连贯的预测。该模型不仅能在每帧进行强检测，还通过学习的姿态更新机制实现跨帧追踪。与传统匹配检测不同，该方法直接从新输入图像更新姿态，支持在线追踪。通过使用大量伪标签标注的数据集训练，模型在3D姿态估计准确性方面达到领先水平，同时在多目标长时间追踪中表现更快更准确。代码和权重可在指定GitHub仓库获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>NEMOTRON-CROSSTHINK：强化学习框架提升大语言模型跨领域推理能力</title>
<link>https://arxiv.org/abs/2504.13941</link>
<guid>https://arxiv.org/abs/2504.13941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NEMOTRON-CROSSTHINK框架，通过多领域数据增强强化学习训练，显著提高大语言模型在数学及非数学推理任务中的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在推理任务中表现出色，特别是在数学推理领域。然而，将其推广到其他复杂且多样化的推理任务时面临诸多挑战，例如有限的数据量、缺乏可验证的奖励结构以及任务需求的多样性。为解决这些问题，本文提出了NEMOTRON-CROSSTHINK框架，该框架通过整合来自科学、技术、工程、数学（STEM）、人文学科和社会科学等多个领域的合成和真实问题-答案对，增强了强化学习的训练过程。具体而言，NEMOTRON-CROSSTHINK通过采用多选题和开放性问题模板控制答案空间复杂度，筛选可验证的答案，并优化多源数据融合策略，从而实现对数学及非数学推理任务的高效泛化。实验结果显示，该方法不仅在数学推理任务（如MATH-500和AMC23）上取得了显著的性能提升（分别提升了30.1%和27.5%），还在非数学推理任务（如MMLU-PRO、GPQA-DIAMOND、AGIEVAL和SUPERGPQA）上实现了11.3%至15.1%的准确率增长。此外，NEMOTRON-CROSSTHINK还大幅提高了响应效率，在正确回答问题时所需令牌数量减少了28%，表明其推理更加集中和高效。总之，这一研究证明了在强化学习中引入多领域、多格式数据的重要性，为构建更精确、高效且通用的大语言模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 17:37:13 GMT</pubDate>
</item>
<item>
<title>X-Teaming：一种高效的多轮对话语言模型攻击框架</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出X-Teaming框架，显著提升多轮对话中语言模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多轮对话中语言模型存在的安全风险展开研究，现有工作主要集中在单轮安全性，而多轮对抗测试仍面临适应性和多样性挑战。为解决这些问题，我们提出了X-Teaming框架，通过协作代理进行规划、攻击优化和验证，成功率达到98.1%，尤其对Claude 3.7 Sonnet等先进模型也实现了高达96.2%的成功率。此外，基于X-Teaming，我们开发了XGuard-Train开源训练集，规模为此前最佳资源的20倍，包含3万种交互式越狱场景，助力提升语言模型的多轮安全性。本研究为应对复杂对话攻击提供了重要工具和见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 12:11:28 GMT</pubDate>
</item>
<item>
<title>基于生成模型的新型透视变形图像创作</title>
<link>https://arxiv.org/abs/2504.08902</link>
<guid>https://arxiv.org/abs/2504.08902</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合生成模型与频率感知变换技术，创造可直接解读的透视变形图像。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了经典的光学错觉——透视变形图像（anamorphosis），这类图像是通过故意扭曲形成，在常规视角下难以辨认，仅当从特定角度或借助反射设备观看时才显现其真实形态。尽管这类视觉效果的数学原理可以追溯到17世纪，但它们的意义通常依赖于特定视角。我们提出了一种创新方法，利用潜伏修正流模型和一种名为拉普拉斯金字塔扭曲的技术，在保持图像直接可读性的同时生成高质量的透视变形图像。这一研究将视觉文字谜（Visual Anagrams）的概念扩展到了潜在空间模型及更广泛的几何变换领域，为生成式艺术与感知错觉的创作提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.08902" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 14:12:01 GMT</pubDate>
</item>
<item>
<title>从知识检索到思维构建：生成式AI的第二幕</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式AI进入“Act II”，从知识检索转向思维构建。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了生成式人工智能（Generative AI）从第一代到第二代的演进过程。第一代模型通过大规模参数和数据扩展取得了显著成功，但存在知识延迟、浅层推理和受限认知等根本性限制。在此期间，提示工程成为我们与AI交互的主要方式。如今，随着测试时扩展技术的应用，第二代模型正从知识检索系统转变为思维构建引擎，从而实现与AI在思维层面的连接。文章还解释了认知工程的概念基础，并提供了相关教程和优化实现，使更多从业者能够参与到AI的第二幕发展中。此外，作者维护了一个定期更新的GitHub仓库，收录了关于测试时扩展的相关论文。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:55:58 GMT</pubDate>
</item>
<item>
<title>多语言大型语言模型的知识边界感知研究</title>
<link>https://arxiv.org/abs/2504.13816</link>
<guid>https://arxiv.org/abs/2504.13816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多语言LLMs的知识边界感知机制及跨语言迁移方法。</p><br /><br /><p><strong>摘要：</strong> 本文首次分析了大型语言模型（LLMs）在不同语言中识别知识边界的内部表征特性。通过在多种语言中处理已知与未知问题的探针测试，我们发现LLMs对知识边界的感知主要编码在中间至中上层网络中，且跨语言感知差异呈线性结构。基于此，提出了一种无需训练的对齐方法，可有效实现低资源语言间知识边界感知能力的转移，降低幻觉风险。此外，双语问题对微调进一步提升了跨语言知识边界识别能力。鉴于缺乏标准测试基准，我们构建了一个包含三种代表性知识边界数据的多语言评估套件。所有代码和数据集均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:44:12 GMT</pubDate>
</item>
<item>
<title>语言模型中不确定性量化评估的偏差问题及其解决方案</title>
<link>https://arxiv.org/abs/2504.13677</link>
<guid>https://arxiv.org/abs/2504.13677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现常用正确性函数会放大某些不确定性量化方法的表现。</p><br /><br /><p><strong>摘要：</strong> 不确定性量化（UQ）在提升语言模型（LMs）的安全性和可靠性方面至关重要。本文通过评估四种数据集和四种模型下七种正确性函数（包括基于词汇和嵌入的指标及大型语言模型作为裁判的方法）对六种不确定性量化方法的影响，揭示了这些正确性函数中的长度偏差会与量化方法中的长度偏差相互作用，从而扭曲UQ评估结果。研究指出，将大型语言模型作为裁判的方法相对较少受到长度偏差影响，可能是解决此类偏差的一种潜在方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 09:13:42 GMT</pubDate>
</item>
<item>
<title>Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering</title>
<link>https://arxiv.org/abs/2504.13519</link>
<guid>https://arxiv.org/abs/2504.13519</guid>
<content:encoded><![CDATA[
Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 03:15:27 GMT</pubDate>
</item>
<item>
<title>基于生产理论的语言模型经济价值评估框架</title>
<link>https://arxiv.org/abs/2504.13359</link>
<guid>https://arxiv.org/abs/2504.13359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合准确性与推理成本的经济评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个基于生产理论的框架，用于评估语言模型的经济价值，通过整合准确性与推理成本的概念，定义了“通过成本”（cost-of-pass）和“前沿通过成本”（frontier cost-of-pass）。研究发现轻量级模型在基本量化任务中最具成本效益，大型模型在知识密集型任务中表现优异，而推理模型在复杂量化问题上更具优势。此外，过去一年中复杂量化任务的成本大幅下降，创新如轻量级、大型和推理模型的进展对此起到了关键作用。同时，多数推理技术带来的边际精度提升未能抵消其成本，表明互补的模型层面创新是提高成本效益的主要驱动力。此框架为衡量进展和指导部署提供了原则性工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 17:58:29 GMT</pubDate>
</item>
<item>
<title>基于自我校准框架的大型视频语言模型改进研究</title>
<link>https://arxiv.org/abs/2504.12083</link>
<guid>https://arxiv.org/abs/2504.12083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我校准框架解决大型视频语言模型的细粒度时间理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型视频语言模型（LVLMs）在细粒度时间理解、幻觉现象及简单视频问答任务中易犯错误的问题，提出了一个自我校准框架，通过学习自身错误来提升模型性能。该框架首先构建了一组优选与非优选响应对，非优选响应引入常见错误模式，如空间时间理解不足、概念间虚假相关性等。为实现模型与这些响应对的自我校准，我们引入了精炼正则化偏好优化（RRPO），该方法利用子序列级精炼奖励和逐令牌KL正则化，克服直接偏好优化（DPO）的局限性。实验表明，RRPO在多种视频任务中，包括视频幻觉、短视频和长视频理解以及细粒度时间推理上，均表现出更精确的校准和更稳定的训练效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:43:56 GMT</pubDate>
</item>
<item>
<title>ThoughtMani：通过外部CoTs减少大型推理模型的冗余推理步骤</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ThoughtMani，通过插入小型模型生成的外部CoTs来减少大型推理模型的冗余推理步骤。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）通过扩展测试时计算能力，在多个任务中表现出色。然而，这些模型通常面临“过度思考”问题，即产生大量冗余推理步骤但性能提升有限。现有解决方案依赖于微调，这需要额外的数据、非传统训练设置、潜在的安全性失配和较差的泛化能力。本研究通过实证分析揭示LRM行为的一个重要特性，即在推理令牌之间插入由较小模型生成的外部CoTs可以有效操控模型生成较少的推理步骤。基于此洞察，我们提出了一种简单而高效的管道——ThoughtMani，使LRMs能够绕过不必要的中间步骤并显著降低计算成本。我们在LiveBench/Code数据集上对QwQ-32B进行实验验证，发现ThoughtMani保持了原始性能，减少了约30%的输出标记数，且外部CoT生成器带来的开销极小。此外，ThoughtMani还提高了平均10%的安全性对齐。由于模型供应商通常同时提供不同规模的模型，ThoughtMani为构建更高效、更易访问的LRMs提供了有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 07:07:19 GMT</pubDate>
</item>
<item>
<title>CLASH数据集评估大型语言模型在高风险情境下的价值决策能力</title>
<link>https://arxiv.org/abs/2504.10823</link>
<guid>https://arxiv.org/abs/2504.10823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次引入CLASH数据集，评估大语言模型在高风险价值冲突情境中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CLASH的新数据集，用于评估大型语言模型（LLMs）在高风险情境下处理价值冲突的能力。CLASH包含345个高影响力困境及其3,795个不同的价值视角，旨在弥补先前研究仅限于日常场景的不足。通过测试10种前沿开放及封闭模型，发现顶级模型如GPT-4o和Claude-Sonnet在识别模棱两可决策方面准确率不足50%，但在明确情景中表现较好。此外，尽管LLMs能合理预测心理不适，但对涉及价值转变的观点理解不足，表明LLMs需要增强复杂价值推理能力。实验还显示LLMs的价值偏好与其向特定价值方向的可引导性密切相关，且从第三方视角进行价值推理时表现出更高的可引导性，但某些价值对则从第一人称视角中受益更多。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 22:54:16 GMT</pubDate>
</item>
<item>
<title>DehazeXL：一种高效处理大分辨率图像雾霾去除的方法</title>
<link>https://arxiv.org/abs/2504.09621</link>
<guid>https://arxiv.org/abs/2504.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种平衡全局上下文和局部特征的大分辨率图像去雾方法DehazeXL。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度学习模型在处理大分辨率雾霾图像时遇到的内存限制问题，提出了DehazeXL，一种能够在主流GPU硬件上实现端到端建模的去雾方法。DehazeXL通过有效平衡全局上下文和局部特征提取，在不牺牲细节的情况下解决了高分辨率图像去雾难题。此外，为了评估全局上下文在去雾性能中的作用，我们设计了一种特定于去雾任务的视觉归因方法。同时，由于缺乏大型图像去雾基准数据集，我们创建了一个超高分辨率去雾数据集（8KDehaze），包含10,000对大小为8192×8192像素的清晰与雾霾图像。实验表明，DehazeXL可以在仅占用21GB显存的情况下推断高达10240×10240像素的图像，达到当前最先进的性能。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 11:41:25 GMT</pubDate>
</item>
<item>
<title>强化学习与可验证奖励对大语言模型推理能力的影响</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现强化学习并未显著提升大语言模型的基础推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文重新评估了强化学习与可验证奖励（RLVR）在增强大型语言模型（LLMs）推理能力方面的作用，特别是在数学和编程任务中的表现。虽然RLVR训练的模型在小值k（如k=1）下的pass@k指标优于基线模型，但在大值k时，基线模型的表现可以达到甚至超过RL训练模型。进一步分析表明，RL训练通过调整模型输出分布，提高了采样正确响应的效率，但同时也限制了模型的推理能力边界。此外，在视觉推理任务中也观察到类似结果。值得注意的是，蒸馏方法可以引入不同于RLVR的新知识。这些发现揭示了RLVR在提升LLMs推理能力方面的局限性，需要重新审视强化学习对推理模型的影响并探索更优的训练范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于最大信息增益的指令微调数据高效采样方法</title>
<link>https://arxiv.org/abs/2504.13835</link>
<guid>https://arxiv.org/abs/2504.13835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一的数据信息量化方法及最大信息增益采样算法，显著提升指令微调数据集构建效果。</p><br /><br /><p><strong>摘要：</strong> 数据质量和多样性对构建有效的指令微调数据集至关重要。现有方法多侧重实例质量并采用启发式规则维持多样性，但缺乏全局视角可能导致结果不佳，且难以精确捕捉复杂指令意图。本文提出一种基于标签图构造的语义空间量化方法，通过信息分布衡量数据集多样性，并设计最大信息增益(MIG)采样算法，迭代选择样本以最大化语义空间中的信息增益。实验表明，MIG方法在多个数据集和基础模型上优于当前先进方法，例如用Tulu3数据集5%采样数据微调的模型在AlpacaEval和Wildbench上的性能分别提升了5.73%和6.89%，接近全量数据SFT模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于注意力偏差的深度学习架构设计框架Miras</title>
<link>https://arxiv.org/abs/2504.13173</link>
<guid>https://arxiv.org/abs/2504.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Miras，通过重新设计神经网络作为关联记忆模块提升模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文受人类认知现象“注意力偏差”的启发，将Transformer等神经网络重新定义为通过内部目标学习键值映射的关联记忆模块。研究发现现有序列模型大多采用点积相似性或L2回归作为注意力偏差，为此提出了多种替代配置及其稳定训练的近似方法，并重新解释了现代深度学习中的遗忘机制。基于这些见解，我们开发了Miras框架，包含四种设计选择：关联记忆架构、注意力偏差目标、保留门和记忆学习算法。由此产生了三个新型序列模型Moneta、Yaad和Memora，不仅超越了现有线性RNN的能力，还展现出在语言建模、常识推理及回忆密集型任务上的优异表现，甚至优于Transformer等现代模型。实验表明，Miras的不同设计变体在特定任务上具有显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>结合伪合成与真实图像的空中地面视角几何重建</title>
<link>https://arxiv.org/abs/2504.13157</link>
<guid>https://arxiv.org/abs/2504.13157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合伪合成与真实图像的框架，解决空中与地面视角差异大的几何重建问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了从混合地面和空中视图捕获的图像进行几何重建的任务。当前最先进的基于学习的方法无法处理空中与地面图像对之间极端的视角变化，主要原因是缺乏高质量的共配准空中-地面训练数据集。我们假设这是失败的关键原因，因为这样的数据难以精确组装且难以以可扩展的方式重建。为了解决这一挑战，我们提出了一种可扩展的框架，该框架结合了来自3D城市范围网格（如Google Earth）的伪合成渲染和真实地面级众包图像（如MegaDepth）。伪合成数据模拟了广泛的空中视角，而真实众包图像则在地面级图像中改善了视觉保真度，有效地弥合了真实图像与伪合成渲染之间的领域差距。使用此混合数据集，我们微调了几种最先进的算法，在现实世界的零样本空中-地面任务中取得了显著改进。例如，基线DUSt3R方法仅将不到5%的空中-地面对在相机旋转误差5度以内定位，而使用我们的数据微调后，准确率提高到近56%，解决了处理大视角变化的主要失败点。除了相机估计和场景重建外，我们的数据集还在具有挑战性的空中-地面场景中的下游任务（如新颖视图合成）中提高了性能，展示了我们的方法在实际应用中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>HiScene：基于层次结构的场景级3D生成框架</title>
<link>https://arxiv.org/abs/2504.13072</link>
<guid>https://arxiv.org/abs/2504.13072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HiScene框架，结合2D与3D生成技术实现高质量场景生成。</p><br /><br /><p><strong>摘要：</strong> 场景级3D生成是多媒体与计算机图形学的重要前沿领域，但现有方法存在对象类别有限或编辑灵活性不足的问题。本文介绍了一种名为HiScene的新框架，通过将场景视为具有等距视图的层次化“对象”，实现了高保真度的场景生成。HiScene将房间视为复杂对象并进一步分解为可操控的项目，从而确保生成内容与2D表示的一致性同时保持组成结构。为了保证分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的模态完成技术，有效处理对象之间的遮挡和阴影问题，并引入形状先验注入以确保场景内的空间一致性。实验结果显示，该方法生成的物体排列更加自然且适合交互应用，同时保持物理真实性和与用户输入的对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:33:39 GMT</pubDate>
</item>
<item>
<title>多语言推理在大型语言模型中的上限潜力研究</title>
<link>https://arxiv.org/abs/2504.11833</link>
<guid>https://arxiv.org/abs/2504.11833</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多语言推理比英语推理具有更高的性能上限。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，大型语言模型存在显著的“英语偏见”，即任务用英语呈现时表现更好。然而，我们发现某些其他语言在推理任务中甚至可以优于英语。本文探索了多语言推理在任务中的性能上限，指出多语言推理不仅能显著提高近10 Acc@k点，还更具鲁棒性，对翻译质量和语言选择的变化具有更强的容忍度。尽管如此，目前常见的答案选择方法无法达到这一上限，因为它们受到局限性和偏见的影响。此外，我们分析了性能上限背后的原因及实现挑战。这些发现为未来充分挖掘大型语言模型中多语言推理的潜力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11833" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 03:45:10 GMT</pubDate>
</item>
<item>
<title>NodeRAG：基于异构图结构的大规模语言模型增强型检索生成框架</title>
<link>https://arxiv.org/abs/2504.11544</link>
<guid>https://arxiv.org/abs/2504.11544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NodeRAG通过引入异构图结构优化检索增强生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为NodeRAG的新框架，该框架以图为中心，通过引入异构图结构，实现了图算法在检索增强生成（RAG）工作流中的无缝整合。与现有方法相比，NodeRAG不仅提升了索引时间、查询时间和存储效率，还在多跳基准测试和开放式一对一评估中展现出更优的问题回答能力。此外，NodeRAG在使用最少检索标记的情况下，表现出了显著的优势，尤其是在GraphRAG和LightRAG等方法的基础上进一步优化了性能。NodeRAG的代码已开源，可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:24:00 GMT</pubDate>
</item>
<item>
<title>构建透明图像与视频理解研究的开放性感知语言模型</title>
<link>https://arxiv.org/abs/2504.13180</link>
<guid>https://arxiv.org/abs/2504.13180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过开源框架构建透明的感知语言模型用于图像和视频理解。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型在计算机视觉领域至关重要，但许多高性能模型仍为闭源状态，限制了科学进展。尽管学术界尝试通过黑盒模型蒸馏生成训练数据取得显著基准成果，但缺乏对教师模型及其数据来源的了解阻碍了真正的科学进步。本文提出在完全开源且可复现的框架下构建感知语言模型（PLM），以推动图像和视频理解领域的透明研究。我们分析了标准训练管道，并探索大规模合成数据，发现特别是在详细视频理解方面存在关键的数据缺口。为此，我们发布了包含280万个细粒度视频问答对的人类标注实例以及时空定位的视频描述。此外，我们推出了PLM-VideoBench，这是一个评估具有挑战性的视频理解任务的工具包，重点关注对视频“什么”、“哪里”、“何时”和“如何”的推理能力。我们的工作通过提供数据、训练方法、代码和模型实现了完全的可复现性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Complex-Edit：基于指令的图像编辑模型综合评估基准</title>
<link>https://arxiv.org/abs/2504.13143</link>
<guid>https://arxiv.org/abs/2504.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Complex-Edit基准，评估指令复杂性对图像编辑模型的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Complex-Edit的综合基准，用于系统性评估基于指令的图像编辑模型在不同复杂度指令下的表现。通过利用GPT-4o大规模自动生成多样化编辑指令，我们设计了一套“编辑链”管道来构建复杂的编辑任务。此外，还引入了一系列指标和基于视觉语言模型的自动化评估流程，以支持大规模性能评估。研究发现开源模型显著落后于闭源模型，且随着指令复杂性的增加，模型在保留输入图像关键元素和整体美学质量方面的能力显著下降。此外，将复杂指令分解为原子步骤执行会降低性能，而Best-of-N策略能提升直接编辑和逐步方法的效果。有趣的是，训练中使用合成数据会导致模型生成的图像显得更加人工化，这种现象在GPT-4o输出中也有所体现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>基于Vision Transformer的无人机跟踪中抗遮挡表示学习</title>
<link>https://arxiv.org/abs/2504.09228</link>
<guid>https://arxiv.org/abs/2504.09228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法增强单流ViT模型在航拍跟踪中的遮挡鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文针对使用视觉Transformer(ViT)主干网络的单流架构在实时无人机跟踪中的不足，即缺乏有效处理遮挡的策略，提出了一种基于ViT学习抗遮挡表示（ORR）的新方法。通过引入随机屏蔽操作模拟目标遮挡，使ViT模型具备更强的目标遮挡鲁棒性，构建了名为ORTrack的框架。此外，为满足实时应用需求，设计了自适应特征知识蒸馏（AFKD）方法，生成高效的学生模型ORTrack-D，既保留了ORTrack的性能，又提高了效率。在多个基准数据集上的实验验证了该方法的有效性和领先性能，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 10:06:50 GMT</pubDate>
</item>
<item>
<title>70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float</title>
<link>https://arxiv.org/abs/2504.11651</link>
<guid>https://arxiv.org/abs/2504.11651</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 18:38:38 GMT</pubDate>
</item>
<item>
<title>Perception Encoder：通过视觉-语言对比学习实现多任务通用编码器</title>
<link>https://arxiv.org/abs/2504.13181</link>
<guid>https://arxiv.org/abs/2504.13181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">仅靠对比视觉-语言训练即可生成适用于多种下游任务的强大通用嵌入。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为Perception Encoder（PE）的最先进图像和视频理解编码器，该编码器通过简单的视觉-语言学习进行训练。传统视觉编码器依赖于针对具体任务定制的各种预训练目标，而我们发现，经过精心调校的图像预训练方法和稳健的视频数据引擎优化后，仅基于对比视觉-语言训练就能产生适用于分类、描述、定位等所有这些下游任务的强大通用嵌入。然而，这些嵌入隐藏在网络的中间层中，为此我们提出了两种对齐方法：用于多模态语言建模的语言对齐和用于密集预测的空间对齐。结合核心对比检查点，PE模型家族在零样本图像和视频分类与检索、文档问答、图像问答、视频问答以及检测、深度估计和跟踪等空间任务上均取得了最先进的性能。为了促进进一步研究，我们将发布我们的模型、代码以及一个包含合成和人工注释视频的新颖数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>MetaSynth：通过元提示增强合成数据多样性以实现领域自适应</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSynth通过元提示生成多样化合成数据，成功将大型语言模型适配到金融和生物医学领域。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用合成数据进行领域自适应的问题，特别是针对小型语言模型Phi-3.5和Phi-4，这些模型依赖由大型语言模型生成的合成数据。然而，合成数据的低多样性限制了其在改进其他模型方面的应用。为解决这一问题，我们提出了MetaSynth方法，该方法通过元提示机制，让多个“专家”大型语言模型协同生成数据，从而提高合成数据的多样性。实验表明，仅使用2500万token的MetaSynth生成的合成数据，就能有效将已训练好的大型语言模型（Mistral-7B-v0.3）适配到金融和生物医学两个特定领域，同时不影响其在通用任务中的性能。此外，通过七种自动化指标评估，MetaSynth生成的合成数据的多样性接近大型语言模型预训练语料库。持续预训练Mistral-7B-v0.3模型显示，在金融领域提升了4.08%，在生物医学领域提升了13.75%的表现。相比之下，使用模板提示生成的数据即使包含真实数据示例，也会导致模型性能下降。研究结果表明，使用MetaSynth生成的少量多样化合成数据即可实现有效的领域自适应。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 21:25:15 GMT</pubDate>
</item>
<item>
<title>基于学习的跨相机色彩恒常性方法</title>
<link>https://arxiv.org/abs/2504.07959</link>
<guid>https://arxiv.org/abs/2504.07959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可适应新相机的跨相机色彩校正学习方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于学习的方法，用于解决计算色彩恒常性（即白平衡）问题中的跨相机适应性挑战。该方法利用图像信号处理器(ISP)中预校准的颜色校正矩阵(CCM)，将标准空间的颜色映射到相机的原始颜色空间，并通过编码生成紧凑的相机指纹嵌入(CFE)，从而实现对未知相机的适应。为了防止训练过程中因相机数量有限导致的过拟合，引入了一种相机间数据插值增强技术。实验结果表明，该方法在多个数据集和网络结构上实现了最先进的跨相机色彩恒常性性能，同时保持轻量化且仅依赖于ISP中已有的数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.07959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>通过睡眠计算提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2504.13171</link>
<guid>https://arxiv.org/abs/2504.13171</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入睡眠计算显著降低大语言模型推理时的计算需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为睡眠计算的新方法，允许模型在用户查询到来之前预先处理可能的上下文信息，从而大幅减少测试阶段的计算需求和延迟。实验表明，在两个推理任务(Stateful GSM-Symbolic 和 Stateful AIME)中，睡眠计算可使所需测试计算量减少约5倍，同时通过扩展计算还可提高准确性最高达13%和18%。此外，多查询GSM-Symbolic进一步优化了相关查询的平均成本，降低了2.5倍。研究还发现，用户查询的可预测性与睡眠计算的效果高度相关。最后，睡眠计算在实际代理任务中也显示出良好的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13171" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>CLIMB框架实现高效预训练数据混合优化</title>
<link>https://arxiv.org/abs/2504.13161</link>
<guid>https://arxiv.org/abs/2504.13161</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CLIMB框架优化预训练数据混合，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 当前预训练数据集多来源于网络内容且缺乏明确领域划分，导致构建最优数据混合极具挑战性。本文提出CLIMB框架，通过嵌入和聚类大规模数据集并迭代优化数据混合，显著提升了预训练模型的表现。实验表明，在400B令牌上训练时，CLIMB框架下的1B模型比Llama-3.2-1B高出2%，特定领域优化进一步提升5%。此外，我们发布了ClimbLab和ClimbMix数据集供研究使用，揭示了最优数据混合的特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13161" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>利用专家失败探索提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.13145</link>
<guid>https://arxiv.org/abs/2504.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法EEF，通过分析专家失败轨迹提高大型语言模型代理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）作为代理展现出强大的潜力，特别是在需要多轮推理和交互的任务上。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家成功轨迹并迭代优化自生成轨迹来提升代理技能。然而，由于专家（如GPT-4）在简单子任务上表现较好，而RFT倾向于处理简单场景，许多复杂子任务仍未解决且持续处于分布外（OOD）。研究发现，专家失败轨迹中的计划和关键操作可显著提高代理的探索效率和技能获取。基于此，我们提出了探索专家失败（EEF），该方法从失败的专家轨迹中识别出有益操作并整合到训练数据集中，同时排除潜在有害操作以保护模型学习过程。实验表明，EEF在WebShop中取得了62%的胜率，优于RFT（53.6%）和GPT-4（35.6%），并在WebShop和SciWorld中创造了新的性能记录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:53:54 GMT</pubDate>
</item>
<item>
<title>多因素挑战下的RAG系统改进：RAMDocs与MADAM-RAG</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RAMDocs和MADAM-RAG方法，同时处理模糊查询和信息冲突问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）的检索增强生成（RAG）技术虽提升了事实准确性，但在处理模糊查询及多源冲突信息时仍面临挑战。现有研究通常孤立解决单一问题，如模糊性或噪声信息。本文提出RAMDocs，一个模拟复杂真实场景的新数据集，涵盖模糊性、错误信息和噪声；并开发MADAM-RAG，一种多代理辩论机制，通过多轮讨论聚合去噪后的有效答案。实验显示，MADAM-RAG在AmbigDocs和FaithEval任务中均优于基线模型，但RAMDocs对现有RAG系统构成重大挑战，表明仍有提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>NoisyRollout：通过视觉导向策略增强视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.13055</link>
<guid>https://arxiv.org/abs/2504.13055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合清洁与失真图像的RL方法NoisyRollout，提升视觉语言模型的推理与感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习在视觉语言模型中的应用，特别是如何通过引入视觉导向的噪声策略来增强模型的探索能力。传统的视觉语言模型在面对不完美视觉感知时表现不佳，这限制了其推理效果。为此，我们提出了NoisyRollout，这是一种简单而有效的强化学习方法，通过混合清洁和适度失真的图像轨迹，为视觉感知和推理模式引入目标多样性。该方法无需额外训练成本，在仅使用2.1K训练样本的情况下，在五个跨领域的基准测试中实现了最先进的性能，同时保持了同等甚至更好的领域内表现。此外，NoisyRollout采用了一种噪声退火调度，逐步减少训练过程中的失真强度，从而早期利用噪声信号的优势，同时确保后期训练的稳定性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:10:13 GMT</pubDate>
</item>
<item>
<title>ANT：通过自动引导去噪轨迹实现文本转图像模型的概念擦除</title>
<link>https://arxiv.org/abs/2504.12782</link>
<guid>https://arxiv.org/abs/2504.12782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ANT，用于有效防止有害内容生成并实现概念擦除。</p><br /><br /><p><strong>摘要：</strong> 现有基于微调的文本到图像模型概念擦除方法存在显著局限性，本文引入一种名为ANT的新框架，通过自动引导去噪轨迹，实现了对不想要概念的精确修改。该框架基于分类器自由引导条件方向反转的见解，在去噪的中晚期阶段启用此操作，从而在不牺牲早期结构完整性的情况下实现内容修改。对于单概念擦除，采用增强的权重显著图来精确定位关键参数；对于多概念擦除，提供了一种灵活的插件解决方案。实验表明，ANT在单概念和多概念擦除方面均达到最先进的性能，同时保持生成保真度。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 05:29:30 GMT</pubDate>
</item>
<item>
<title>InstantCharacter：基于扩散Transformer的可扩展角色定制框架</title>
<link>https://arxiv.org/abs/2504.12395</link>
<guid>https://arxiv.org/abs/2504.12395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散Transformer的角色定制框架，提升图像质量和文本可控性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为InstantCharacter的新框架，旨在解决现有基于学习的角色定制方法的局限性，如有限的泛化能力和图像质量下降问题，以及优化方法中特定对象微调导致的文本可控性降低问题。该框架基于扩散Transformer构建，具有开放域个性化能力，支持多样化的角色外观、姿势和风格，同时保持高保真度。通过引入堆叠变换编码器的可扩展适配器，框架能够有效处理开放域角色特征并与现代扩散Transformer的潜在空间无缝交互。此外，构建了一个包含1000万级别样本的大规模角色数据集，分为配对（多视角角色）和非配对（文本-图像组合）子集，以优化身份一致性与文本编辑能力。实验结果表明，InstantCharacter在生成高保真、文本可控且角色一致的图像方面表现出色，树立了新的角色驱动图像生成基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 14:01:59 GMT</pubDate>
</item>
<item>
<title>基于分数蒸馏的文本到图像模型合并方法</title>
<link>https://arxiv.org/abs/2504.12364</link>
<guid>https://arxiv.org/abs/2504.12364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法将多个模型的知识整合为单一多才多艺的文本到图像生成模型。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像生成模型的成功，大量从同一基础模型微调而来的专门模型涌现，但带来了高参数冗余和存储成本问题。传统静态线性插值方法忽略了多样风格可能带来的不兼容性。为此，我们设计了一种可通过风格向量控制任意风格图像生成的流水线，并提出了基于分数蒸馏的模型合并范式（DMM），有效压缩多个模型为单一多功能模型。此外，我们重新思考并重新定义了文本到图像生成领域的模型合并任务，提出新的合并目标和评估协议。实验表明，DMM能够紧凑地重组多个教师模型的知识，并实现可控的任意风格生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:09:45 GMT</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:04:14 GMT</pubDate>
</item>
<item>
<title>GRA框架：多小模型协作生成高质量数据</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多小模型协作模拟同行评审，实现与大模型相当的数据合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GRA的框架，该框架通过多个小型语言模型（LLMs）协同工作，模拟人类同行评审过程，完成数据合成与优化任务。具体而言，GRA框架中的小模型分别承担生成者（Generator）、审查者（Reviewer）和裁定者（Adjudicator）的角色，逐步迭代并控制数据的质量与多样性。实验结果显示，该方法生成的数据在多个基准测试中达到了甚至超过了单一大型语言模型（如Qwen-2.5-72B-Instruct）的表现。这一成果挑战了传统上依赖大型模型进行高质量数据合成的必要性，提倡采用小型模型的策略性协作。研究代码和数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 02:13:43 GMT</pubDate>
</item>
<item>
<title>REVERSE：一种统一的视觉-语言模型幻觉缓解框架</title>
<link>https://arxiv.org/abs/2504.13169</link>
<guid>https://arxiv.org/abs/2504.13169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出REVERSE框架，通过训练与实时自验证结合显著减少视觉幻觉。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言模型在视觉理解方面表现出色，但容易产生不存在的对象、动作或概念的描述，这在安全性至关重要的应用场景中构成重大风险。目前的幻觉缓解方法主要分为两类：生成调整和事后验证。然而，生成调整方法通常依赖启发式规则且缺乏修正机制，而事后验证则复杂且倾向于拒绝输出而非改进。本研究提出了REVERSE框架，它将幻觉感知训练与动态自我验证相结合。通过利用包含超过130万个半合成样本的新幻觉验证数据集，以及创新的推理时回顾重采样技术，该方法使视觉-语言模型能够在生成过程中检测幻觉并动态修正这些幻觉。实验表明，REVERSE在CHAIR-MSCOCO和HaloQuest数据集上的幻觉减少效果达到当前最佳水平，分别比现有最佳方法高出12%和28%。相关数据集、模型和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:54:14 GMT</pubDate>
</item>
<item>
<title>VistaDPO：基于层次化空间-时间直接偏好优化的大规模视频模型对齐框架</title>
<link>https://arxiv.org/abs/2504.13122</link>
<guid>https://arxiv.org/abs/2504.13122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VistaDPO框架解决视频理解中的对齐与幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大型语言模型构建的大规模视频模型（LVMs）在视频理解中存在的对齐人类直觉差及视频幻觉问题，引入了VistaDPO这一新颖的视频层次化空间-时间直接偏好优化框架。该框架通过实例级、时间级及感知级三个层级增强文本与视频间偏好对齐效果。鉴于缺乏细粒度视频语言偏好对齐的数据集，我们创建了VistaDPO-7k数据集，包含7.2K个标注问答对及时空定位信息。在视频幻觉、视频问答及视频描述等基准测试中，VistaDPO显著提升了现有LVMs的表现，有效缓解了视频语言不匹配及幻觉现象。相关代码与数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:39:41 GMT</pubDate>
</item>
<item>
<title>FramePack：用于视频生成的高效帧预测神经网络结构</title>
<link>https://arxiv.org/abs/2504.12626</link>
<guid>https://arxiv.org/abs/2504.12626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为FramePack的神经网络结构，用于视频生成中的帧预测。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FramePack的神经网络结构，旨在训练视频生成中的下一帧预测模型。FramePack通过压缩输入帧，使Transformer的上下文长度固定，从而可以处理大量帧并显著提高训练批次大小。此外，该方法还提出了一种反向漂移采样方法，通过逆时间顺序生成帧并提前确定端点来避免曝光偏差。最后，实验表明，现有的视频扩散模型可以通过FramePack进行微调，视觉质量可能得到改善。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:02:31 GMT</pubDate>
</item>
<item>
<title>WorldMem：基于记忆模块提升世界模拟中的时空一致性</title>
<link>https://arxiv.org/abs/2504.12369</link>
<guid>https://arxiv.org/abs/2504.12369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合记忆机制的世界模拟框架，有效增强长期场景生成的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WorldMem的新框架，旨在通过引入记忆银行（存储记忆帧和状态）来改善虚拟环境中的场景生成质量。记忆银行中的记忆单元不仅存储视觉信息，还记录物体姿态及时间戳等状态数据。通过采用记忆注意力机制，该方法可以从记忆帧中高效提取相关信息，从而即使在较大的视角或时间差距下也能精确重建先前观察到的场景。此外，通过将时间戳纳入状态描述，WorldMem不仅能构建静态世界模型，还能捕捉世界的动态演化过程，支持感知与交互功能。实验表明，该方法在虚拟和真实场景中均表现出色，验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering</title>
<link>https://arxiv.org/abs/2504.05506</link>
<guid>https://arxiv.org/abs/2504.05506</guid>
<content:encoded><![CDATA[
Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:05:06 GMT</pubDate>
</item>
<item>
<title>BitNet b1.58 2B4T：首个开源1比特20亿参数大型语言模型</title>
<link>https://arxiv.org/abs/2504.12285</link>
<guid>https://arxiv.org/abs/2504.12285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BitNet b1.58 2B4T实现了高性能与低能耗的结合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BitNet b1.58 2B4T，这是首个开源的原生1比特20亿参数大型语言模型（LLM）。该模型基于4万亿令牌的语料库训练，性能在多个基准测试中得到验证，包括语言理解、数学推理、编码能力和对话能力。研究结果显示，BitNet b1.58 2B4T的性能与同类规模的领先开源全精度LLM相当，但在计算效率方面具有显著优势，如大幅减少内存占用、能耗和解码延迟。为了促进进一步的研究和应用，模型权重已通过Hugging Face发布，同时提供了支持GPU和CPU架构的开源推理实现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:51:43 GMT</pubDate>
</item>
<item>
<title>基于激光雷达的零样本形状补全方法CAL</title>
<link>https://arxiv.org/abs/2504.12264</link>
<guid>https://arxiv.org/abs/2504.12264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用多模态传感器序列进行激光雷达形状补全的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了CAL（Complete Anything in Lidar），一种用于野外激光雷达形状补全的方法，它与基于激光雷达的语义/全景场景补全密切相关。与现有方法只能完成和识别封闭词汇表中标记的对象不同，CAL采用零样本方法，通过挖掘多模态传感器序列中的时间上下文来获取观察对象的形状和语义特征，并将其蒸馏到仅依赖激光雷达的实例级补全和识别模型中。尽管我们只挖掘部分形状补全，但发现该模型能够从数据集中的多个部分观测推断完整的物体形状。实验表明，该模型可以在标准基准上进行语义和全景场景补全，并识别超出固定类别词汇表的对象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:21:55 GMT</pubDate>
</item>
<item>
<title>Cobra：高效灵活的漫画线条艺术着色方法</title>
<link>https://arxiv.org/abs/2504.12240</link>
<guid>https://arxiv.org/abs/2504.12240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于因果稀疏DiT架构的高效着色方法Cobra。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于参考的漫画线条艺术着色在高精度、效率、上下文一致性及灵活控制方面的需求。传统扩散模型在处理复杂漫画场景时存在局限性，如处理大量参考图像耗时、推理时间长等问题。为此，我们提出了Cobra方法，该方法结合颜色提示并利用超过200张参考图像，在保持低延迟的同时实现高质量着色。Cobra的核心是一种因果稀疏DiT架构，通过设计的位置编码、因果稀疏注意力及Key-Value缓存机制有效管理长上下文参考并保证色彩一致性。实验结果显示，Cobra显著提升了推理速度和交互性，满足了工业应用的关键需求。相关代码和模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 12:45:19 GMT</pubDate>
</item>
<item>
<title>多语言混合作者文本中AI生成内容检测模型的研究</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出适用于多种生成器和混合作者文本的AI生成内容检测模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组针对标记分类任务设计的模型，这些模型经过大量人类与机器共同创作的文本训练，在未见过的领域、生成器、非母语文本及对抗性输入上表现优异。同时，我们构建了一个包含超过240万条文本的新数据集，覆盖23种语言，主要由几种流行的专有大型语言模型与人协作生成。研究还分析了模型在不同领域、生成器下的性能，并对比了对抗方法、输入文本长度及生成文本特性对检测效果的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 06:29:30 GMT</pubDate>
</item>
<item>
<title>ReTool：通过工具集成学习提升复杂数学推理能力</title>
<link>https://arxiv.org/abs/2504.11536</link>
<guid>https://arxiv.org/abs/2504.11536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合代码解释器的强化学习模型在数学推理任务上显著超越传统文本模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReTool的新方法，它通过将工具集成到基于强化学习的推理模型中，显著提升了模型在几何推理、复杂方程求解等结构化问题上的表现。ReTool的关键创新在于动态整合自然语言推理过程中的实时代码执行，并采用自动化的强化学习范式来优化工具调用策略。实验结果显示，在AIME数学竞赛题集上，ReTool的32B参数版本在仅需400个训练步的情况下达到了67%的准确率，优于文本基线模型的40%准确率。此外，在扩展设置下，该模型进一步达到72.5%的准确率，大幅领先OpenAI的o1-preview模型。研究还观察到模型具备自主代码修正的能力，表明其已掌握适应性工具使用的“顿悟”能力。这些发现为推动复杂数学推理及混合神经符号系统的进步提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:10:22 GMT</pubDate>
</item>
<item>
<title>Vivid4D：基于单目视频的4D动态场景重建方法</title>
<link>https://arxiv.org/abs/2504.11092</link>
<guid>https://arxiv.org/abs/2504.11092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入新方法Vivid4D，利用几何先验和生成先验结合实现多视角合成，提升4D场景重建质量。</p><br /><br /><p><strong>摘要：</strong> 单目视频的4D动态场景重建是一个具有挑战性的任务，因为每个时间戳仅能从单一视角观察。本文提出了一种名为Vivid4D的新方法，通过增强观测视角（即从单目输入合成多视角视频）来改进4D单目视频合成。与现有方法不同，Vivid4D同时利用了几何先验和生成先验，将视角增强重新表述为视频修复任务，通过将观测到的视图基于单目深度先验扭曲到新的视角上实现。为了训练这一模型，我们使用了未定位的网络视频，并通过合成遮挡掩模来模拟扭曲导致的遮挡情况，确保空间和时间上的一致性修复。此外，为了减少单目深度先验的不准确性，我们还引入了迭代视角增强策略和鲁棒重建损失函数。实验表明，该方法显著提高了单目4D场景的重建和完成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 07:38:14 GMT</pubDate>
</item>
<item>
<title>基于表示对齐的端到端扩散模型训练方法</title>
<link>https://arxiv.org/abs/2504.10483</link>
<guid>https://arxiv.org/abs/2504.10483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的训练方法REPA-E，显著提升扩散模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了是否可以将潜在扩散模型与变分自编码器（VAE）tokenizer以端到端的方式联合训练的问题。传统深度学习认为端到端训练更优，但实验表明标准扩散损失函数下的联合训练不仅无效，甚至会降低最终性能。研究发现，通过引入表示对齐（REPA）损失，可以在训练过程中同时微调VAE和扩散模型，从而实现高效的端到端训练。该方法不仅大幅加速了训练过程，还提升了扩散模型的性能，同时改善了VAE的潜在空间结构及下游生成效果。最终，在ImageNet 256 x 256上的FID得分达到1.26（带分类器自由引导）和1.83（不带分类器自由引导），创造了新记录。此外，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>AlayaDB：面向大规模语言模型的高效向量数据库系统</title>
<link>https://arxiv.org/abs/2504.10326</link>
<guid>https://arxiv.org/abs/2504.10326</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlayaDB通过解耦KV缓存与注意力计算，优化大规模语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> AlayaDB是由AlayaDB AI开发的新型向量数据库系统，专为提高大型语言模型（LLMs）的长上下文推理效率而设计。该系统将LLM推理中的KV缓存和注意力计算分离并封装进独立的数据库系统中，显著减少了对硬件资源的需求，同时提升了多种工作负载的服务质量。相较于现有解决方案，如KV缓存解聚合和基于检索的稀疏注意力方法，AlayaDB通过将注意力计算和缓存管理抽象为查询处理过程，并利用原生查询优化器提升性能。本文通过来自行业合作伙伴的三个实际案例以及LLM推理基准测试的广泛实验结果，验证了AlayaDB的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10326" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 11:34:26 GMT</pubDate>
</item>
<item>
<title>MLRC-Bench：评估大语言模型在机器学习研究竞赛中的表现</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MLRC-Bench基准，评估大语言模型在解决复杂机器学习问题上的有效性。</p><br /><br /><p><strong>摘要：</strong> 现有对大型语言模型在科学发现中的评估缺乏客观基线和度量标准。本文引入MLRC-Bench基准，专注于衡量语言代理提出和实现创新研究方法的能力，不同于以往关注已建立任务的基准。通过7项竞赛任务测试，发现顶级代理仅达到人类参与者得分的9.3%，并揭示了代理创新与前沿研究性能之间的不匹配。MLRC-Bench是一个动态增长的基准，旨在促进AI研究能力的严谨评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 15:35:43 GMT</pubDate>
</item>
<item>
<title>Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</title>
<link>https://arxiv.org/abs/2504.09566</link>
<guid>https://arxiv.org/abs/2504.09566</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 09:35:41 GMT</pubDate>
</item>
<item>
<title>AI语音生成技术对多语言口音影响的社会技术分析</title>
<link>https://arxiv.org/abs/2504.09346</link>
<guid>https://arxiv.org/abs/2504.09346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示AI语音技术可能加剧语言特权和口音歧视。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能语音生成及克隆技术取得了显著进展，但其对不同口音和语言特征的社会技术系统的影响尚未完全明晰。本研究通过混合方法评估了两个AI语音服务（Speechify和ElevenLabs），并结合调查和访谈探讨用户实际体验如何影响他们对这些技术中口音变化的认知。研究发现，技术性能在五种英语区域口音间存在差异，并指出当前技术可能无意中强化语言特权和基于口音的歧视，从而可能导致新的数字排斥形式。总体而言，本研究强调了包容性设计和监管的重要性，为开发者、政策制定者和组织提供了可操作的见解，以确保公平且负责任的人工智能语音技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 17:31:22 GMT</pubDate>
</item>
<item>
<title>基于SIFT的大规模语音指令微调数据集与语言模型</title>
<link>https://arxiv.org/abs/2504.09081</link>
<guid>https://arxiv.org/abs/2504.09081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SIFT数据集用于语音文本大模型的指令微调和预训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为SIFT（Speech Instruction Fine-Tuning）的5000万样本数据集，专门用于语音文本大语言模型（LLMs）的指令微调和预训练。SIFT-50M数据集由14,000小时公开可用的语音语料库构建而成，涵盖五种语言，并结合大型语言模型和现成的专业模型进行处理。该数据集支持多样化的语音理解和可控语音生成指令。利用SIFT-50M，我们训练了SIFT-LLM模型，在指令跟随基准测试中表现出色，同时在基础语音任务上也达到了竞争水平。此外，为了促进进一步研究，我们还发布了EvalSIFT，这是一个专门设计用来评估语音文本大模型指令跟随能力的基准数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 00:45:48 GMT</pubDate>
</item>
<item>
<title>BlockGaussian：高效高质量的大规模场景重建框架</title>
<link>https://arxiv.org/abs/2504.09048</link>
<guid>https://arxiv.org/abs/2504.09048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架BlockGaussian，实现大规模场景重建的高效率与高质量。</p><br /><br /><p><strong>摘要：</strong> 近期3D Gaussian Splatting (3DGS) 技术在新型视图合成中展现出巨大潜力，但大规模场景重建仍面临分区、优化和合并等挑战。本文介绍了一种名为BlockGaussian的新框架，通过引入基于内容感知的场景分区策略及基于可见性感知的块级优化，实现了高效且高质量的大型场景重建。具体而言，该方法根据区域内容复杂度变化平衡计算负载，解决了独立块优化中的监督不匹配问题，并提出了伪视图几何约束以缓解块合并时的渲染退化。实验表明，该方法在多个基准测试上不仅提升了优化速度达5倍，还提高了平均PSNR值1.21 dB，同时显著降低了计算需求，使大场景重建能在单块24GB显存设备上完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 22:05:55 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型训练中的伪推理路径问题及改进方法</title>
<link>https://arxiv.org/abs/2504.11468</link>
<guid>https://arxiv.org/abs/2504.11468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，监督微调可能损害后续强化学习，引入新数据集验证并提出改进方案。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了用于训练大型视觉语言模型（LVLMs）的监督微调（SFT）后接强化学习（RL）的主流范式，并揭示了一个重要发现：SFT可能导致“伪推理路径”的产生，这些路径虽看似模仿自专家模型，但实际上包含冗长、犹豫且较少信息量甚至错误的推理步骤，从而对后续RL造成负面影响。为系统性研究这一现象，我们设计了一个名为VLAA-Thinking的新多模态数据集，该数据集通过包含描述、推理蒸馏、答案重写和验证等六个步骤构建，提供了高质量的SFT逐步视觉推理轨迹以及来自同一数据源的更具挑战性的RL拆分。实验表明，尽管SFT有助于模型学习推理格式，但它常常使已对齐的模型陷入模仿性和僵化的推理模式，阻碍进一步学习。相比之下，基于Group Relative Policy Optimization（GRPO）算法并结合新颖混合奖励模块（整合感知与认知信号），我们的RL方法促进了更真实、适应性强的推理行为。基于Qwen2.5VL 3B的VLAA-Thinker模型在Open LMM Reasoning Leaderboard上达到了4B规模LVLMs中的最佳性能，比之前最先进的方法高出1.8%。我们希望这些发现能为开发具备推理能力的LVLMs提供有价值的见解，并指导该领域的未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:54:05 GMT</pubDate>
</item>
<item>
<title>ColorBench：评估视觉语言模型颜色理解能力的基准</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入ColorBench基准，评估视觉语言模型的颜色感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ColorBench的创新基准，用于评估视觉语言模型（VLMs）在颜色感知、推理和鲁棒性方面的能力。通过构建多样化的测试场景，ColorBench考察了这些模型如何处理颜色信息以及在不同颜色变换下的表现。对32种不同架构的VLMs进行广泛评估后发现，更大规模的语言模型比视觉编码器更重要，但现有模型在颜色理解上的性能差距较小，表明这一领域被忽视。此外，CoT推理可以提高颜色理解的准确性和鲁棒性，尽管这些任务本质上是视觉驱动的。虽然VLMs确实利用了颜色线索，但在某些任务中也可能产生误导。这些发现揭示了当前VLMs的关键局限性，并强调了提升颜色理解能力的重要性。ColorBench可作为多模态AI实现人类水平颜色理解的基础工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10514" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:36:26 GMT</pubDate>
</item>
<item>
<title>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10449</link>
<guid>https://arxiv.org/abs/2504.10449</guid>
<content:encoded><![CDATA[
Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:38:25 GMT</pubDate>
</item>
<item>
<title>LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models</title>
<link>https://arxiv.org/abs/2504.10430</link>
<guid>https://arxiv.org/abs/2504.10430</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:20:34 GMT</pubDate>
</item>
<item>
<title>Breaking the Data Barrier -- Building GUI Agents Through Task Generalization</title>
<link>https://arxiv.org/abs/2504.10127</link>
<guid>https://arxiv.org/abs/2504.10127</guid>
<content:encoded><![CDATA[
Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 07:35:02 GMT</pubDate>
</item>
<item>
<title>Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems</title>
<link>https://arxiv.org/abs/2504.09763</link>
<guid>https://arxiv.org/abs/2504.09763</guid>
<content:encoded><![CDATA[
Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 20:06:48 GMT</pubDate>
</item>
<item>
<title>Iterative Self-Training for Code Generation via Reinforced Re-Ranking</title>
<link>https://arxiv.org/abs/2504.09643</link>
<guid>https://arxiv.org/abs/2504.09643</guid>
<content:encoded><![CDATA[
Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:34:17 GMT</pubDate>
</item>
<item>
<title>How new data permeates LLM knowledge and how to dilute it</title>
<link>https://arxiv.org/abs/2504.09522</link>
<guid>https://arxiv.org/abs/2504.09522</guid>
<content:encoded><![CDATA[
Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 07:25:04 GMT</pubDate>
</item>
<item>
<title>VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search</title>
<link>https://arxiv.org/abs/2504.09130</link>
<guid>https://arxiv.org/abs/2504.09130</guid>
<content:encoded><![CDATA[
Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 04:37:30 GMT</pubDate>
</item>
<item>
<title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
<link>https://arxiv.org/abs/2504.08066</link>
<guid>https://arxiv.org/abs/2504.08066</guid>
<content:encoded><![CDATA[
AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 14:44:41 GMT</pubDate>
</item>
<item>
<title>InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models</title>
<link>https://arxiv.org/abs/2504.10479</link>
<guid>https://arxiv.org/abs/2504.10479</guid>
<content:encoded><![CDATA[
We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</title>
<link>https://arxiv.org/abs/2504.10415</link>
<guid>https://arxiv.org/abs/2504.10415</guid>
<content:encoded><![CDATA[
Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:00:13 GMT</pubDate>
</item>
<item>
<title>S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10368</link>
<guid>https://arxiv.org/abs/2504.10368</guid>
<content:encoded><![CDATA[
We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 12:13:23 GMT</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 08:12:52 GMT</pubDate>
</item>
<item>
<title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.10068</link>
<guid>https://arxiv.org/abs/2504.10068</guid>
<content:encoded><![CDATA[
Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 06:14:44 GMT</pubDate>
</item>
<item>
<title>FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding</title>
<link>https://arxiv.org/abs/2504.09925</link>
<guid>https://arxiv.org/abs/2504.09925</guid>
<content:encoded><![CDATA[
We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 02:33:29 GMT</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 14:47:22 GMT</pubDate>
</item>
<item>
<title>TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning</title>
<link>https://arxiv.org/abs/2504.09641</link>
<guid>https://arxiv.org/abs/2504.09641</guid>
<content:encoded><![CDATA[
Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:32:49 GMT</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 15:49:22 GMT</pubDate>
</item>
<item>
<title>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08837</link>
<guid>https://arxiv.org/abs/2504.08837</guid>
<content:encoded><![CDATA[
Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:41:56 GMT</pubDate>
</item>
<item>
<title>Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability</title>
<link>https://arxiv.org/abs/2504.08003</link>
<guid>https://arxiv.org/abs/2504.08003</guid>
<content:encoded><![CDATA[
OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 12:10:15 GMT</pubDate>
</item>
<item>
<title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title>
<link>https://arxiv.org/abs/2504.08791</link>
<guid>https://arxiv.org/abs/2504.08791</guid>
<content:encoded><![CDATA[
Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training</title>
<link>https://arxiv.org/abs/2504.09710</link>
<guid>https://arxiv.org/abs/2504.09710</guid>
<content:encoded><![CDATA[
Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 16:10:27 GMT</pubDate>
</item>
<item>
<title>BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</title>
<link>https://arxiv.org/abs/2504.01786</link>
<guid>https://arxiv.org/abs/2504.01786</guid>
<content:encoded><![CDATA[
3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 10:51:45 GMT</pubDate>
</item>
<item>
<title>UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2504.06908</link>
<guid>https://arxiv.org/abs/2504.06908</guid>
<content:encoded><![CDATA[
In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:10:51 GMT</pubDate>
</item>
<item>
<title>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</title>
<link>https://arxiv.org/abs/2504.08727</link>
<guid>https://arxiv.org/abs/2504.08727</guid>
<content:encoded><![CDATA[
We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:55:45 GMT</pubDate>
</item>
<item>
<title>Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization</title>
<link>https://arxiv.org/abs/2504.08641</link>
<guid>https://arxiv.org/abs/2504.08641</guid>
<content:encoded><![CDATA[
Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:41:43 GMT</pubDate>
</item>
<item>
<title>ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration</title>
<link>https://arxiv.org/abs/2504.08591</link>
<guid>https://arxiv.org/abs/2504.08591</guid>
<content:encoded><![CDATA[
Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 10:49:52 GMT</pubDate>
</item>
<item>
<title>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.07615</link>
<guid>https://arxiv.org/abs/2504.07615</guid>
<content:encoded><![CDATA[
Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 06:05:15 GMT</pubDate>
</item>
<item>
<title>Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging</title>
<link>https://arxiv.org/abs/2504.08635</link>
<guid>https://arxiv.org/abs/2504.08635</guid>
<content:encoded><![CDATA[
This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM &gt; 0.93, MSE &lt; 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:37:46 GMT</pubDate>
</item>
<item>
<title>Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</title>
<link>https://arxiv.org/abs/2504.07866</link>
<guid>https://arxiv.org/abs/2504.07866</guid>
<content:encoded><![CDATA[
We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:41:51 GMT</pubDate>
</item>
<item>
<title>SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2504.08192</link>
<guid>https://arxiv.org/abs/2504.08192</guid>
<content:encoded><![CDATA[
Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 21:24:03 GMT</pubDate>
</item>
<item>
<title>InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</title>
<link>https://arxiv.org/abs/2504.05303</link>
<guid>https://arxiv.org/abs/2504.05303</guid>
<content:encoded><![CDATA[
We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models</title>
<link>https://arxiv.org/abs/2504.05262</link>
<guid>https://arxiv.org/abs/2504.05262</guid>
<content:encoded><![CDATA[
Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\% accuracy on numerical addition, performance collapses to leq7.5\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 12:57:10 GMT</pubDate>
</item>
<item>
<title>CoRAG: Collaborative Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.01883</link>
<guid>https://arxiv.org/abs/2504.01883</guid>
<content:encoded><![CDATA[
Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 12:40:43 GMT</pubDate>
</item>
<item>
<title>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2504.08736</link>
<guid>https://arxiv.org/abs/2504.08736</guid>
<content:encoded><![CDATA[
In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:29:35 GMT</pubDate>
</item>
<item>
<title>SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08600</link>
<guid>https://arxiv.org/abs/2504.08600</guid>
<content:encoded><![CDATA[
Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:01:30 GMT</pubDate>
</item>
<item>
<title>PixelFlow: Pixel-Space Generative Models with Flow</title>
<link>https://arxiv.org/abs/2504.07963</link>
<guid>https://arxiv.org/abs/2504.07963</guid>
<content:encoded><![CDATA[
We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</title>
<link>https://arxiv.org/abs/2504.08685</link>
<guid>https://arxiv.org/abs/2504.08685</guid>
<content:encoded><![CDATA[
This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 12:46:20 GMT</pubDate>
</item>
<item>
<title>MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</title>
<link>https://arxiv.org/abs/2504.08388</link>
<guid>https://arxiv.org/abs/2504.08388</guid>
<content:encoded><![CDATA[
World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:41:04 GMT</pubDate>
</item>
<item>
<title>In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</title>
<link>https://arxiv.org/abs/2504.08366</link>
<guid>https://arxiv.org/abs/2504.08366</guid>
<content:encoded><![CDATA[
We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:01:09 GMT</pubDate>
</item>
<item>
<title>FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation</title>
<link>https://arxiv.org/abs/2504.07405</link>
<guid>https://arxiv.org/abs/2504.07405</guid>
<content:encoded><![CDATA[
With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 22:58:22 GMT</pubDate>
</item>
<item>
<title>Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2504.07961</link>
<guid>https://arxiv.org/abs/2504.07961</guid>
<content:encoded><![CDATA[
We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models</title>
<link>https://arxiv.org/abs/2504.07951</link>
<guid>https://arxiv.org/abs/2504.07951</guid>
<content:encoded><![CDATA[
Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>TAPNext: Tracking Any Point (TAP) as Next Token Prediction</title>
<link>https://arxiv.org/abs/2504.05579</link>
<guid>https://arxiv.org/abs/2504.05579</guid>
<content:encoded><![CDATA[
Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 20:28:42 GMT</pubDate>
</item>
<item>
<title>MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection</title>
<link>https://arxiv.org/abs/2504.06801</link>
<guid>https://arxiv.org/abs/2504.06801</guid>
<content:encoded><![CDATA[
Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 07:47:48 GMT</pubDate>
</item>
<item>
<title>Compass Control: Multi Object Orientation Control for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.06752</link>
<guid>https://arxiv.org/abs/2504.06752</guid>
<content:encoded><![CDATA[
Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 06:15:15 GMT</pubDate>
</item>
<item>
<title>C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing</title>
<link>https://arxiv.org/abs/2504.07964</link>
<guid>https://arxiv.org/abs/2504.07964</guid>
<content:encoded><![CDATA[
Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.07960</link>
<guid>https://arxiv.org/abs/2504.07960</guid>
<content:encoded><![CDATA[
Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>MM-IFEngine: Towards Multimodal Instruction Following</title>
<link>https://arxiv.org/abs/2504.07957</link>
<guid>https://arxiv.org/abs/2504.07957</guid>
<content:encoded><![CDATA[
The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.07956</link>
<guid>https://arxiv.org/abs/2504.07956</guid>
<content:encoded><![CDATA[
The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>HoloPart: Generative 3D Part Amodal Segmentation</title>
<link>https://arxiv.org/abs/2504.07943</link>
<guid>https://arxiv.org/abs/2504.07943</guid>
<content:encoded><![CDATA[
3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:53:31 GMT</pubDate>
</item>
<item>
<title>SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement</title>
<link>https://arxiv.org/abs/2504.07934</link>
<guid>https://arxiv.org/abs/2504.07934</guid>
<content:encoded><![CDATA[
In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:49:05 GMT</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:06:54 GMT</pubDate>
</item>
<item>
<title>Kimi-VL Technical Report</title>
<link>https://arxiv.org/abs/2504.07491</link>
<guid>https://arxiv.org/abs/2504.07491</guid>
<content:encoded><![CDATA[
We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 02:48:26 GMT</pubDate>
</item>
<item>
<title>Towards Visual Text Grounding of Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.04974</link>
<guid>https://arxiv.org/abs/2504.04974</guid>
<content:encoded><![CDATA[
Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 08:01:59 GMT</pubDate>
</item>
<item>
<title>DeepSeek-R1 Thoughtology: Let's  about LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.07128</link>
<guid>https://arxiv.org/abs/2504.07128</guid>
<content:encoded><![CDATA[
Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 20:36:08 GMT</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 14:30:18 GMT</pubDate>
</item>
<item>
<title>Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.05541</link>
<guid>https://arxiv.org/abs/2504.05541</guid>
<content:encoded><![CDATA[
We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:35:36 GMT</pubDate>
</item>
<item>
<title>Pretraining Language Models for Diachronic Linguistic Change Discovery</title>
<link>https://arxiv.org/abs/2504.05523</link>
<guid>https://arxiv.org/abs/2504.05523</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:51:32 GMT</pubDate>
</item>
<item>
<title>Are We Done with Object-Centric Learning?</title>
<link>https://arxiv.org/abs/2504.07092</link>
<guid>https://arxiv.org/abs/2504.07092</guid>
<content:encoded><![CDATA[
Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>Self-Steering Language Models</title>
<link>https://arxiv.org/abs/2504.07081</link>
<guid>https://arxiv.org/abs/2504.07081</guid>
<content:encoded><![CDATA[
While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:54:22 GMT</pubDate>
</item>
<item>
<title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
<link>https://arxiv.org/abs/2504.07046</link>
<guid>https://arxiv.org/abs/2504.07046</guid>
<content:encoded><![CDATA[
Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.06958</link>
<guid>https://arxiv.org/abs/2504.06958</guid>
<content:encoded><![CDATA[
Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 11:09:27 GMT</pubDate>
</item>
<item>
<title>RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts</title>
<link>https://arxiv.org/abs/2504.06947</link>
<guid>https://arxiv.org/abs/2504.06947</guid>
<content:encoded><![CDATA[
In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:54:00 GMT</pubDate>
</item>
<item>
<title>Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.06719</link>
<guid>https://arxiv.org/abs/2504.06719</guid>
<content:encoded><![CDATA[
Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 05:19:49 GMT</pubDate>
</item>
<item>
<title>WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments</title>
<link>https://arxiv.org/abs/2504.03886</link>
<guid>https://arxiv.org/abs/2504.03886</guid>
<content:encoded><![CDATA[
We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 15:19:40 GMT</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:58 GMT</pubDate>
</item>
<item>
<title>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</title>
<link>https://arxiv.org/abs/2504.07086</link>
<guid>https://arxiv.org/abs/2504.07086</guid>
<content:encoded><![CDATA[
Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:17 GMT</pubDate>
</item>
<item>
<title>RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception</title>
<link>https://arxiv.org/abs/2504.05287</link>
<guid>https://arxiv.org/abs/2504.05287</guid>
<content:encoded><![CDATA[
Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:38:19 GMT</pubDate>
</item>
<item>
<title>DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion</title>
<link>https://arxiv.org/abs/2504.04010</link>
<guid>https://arxiv.org/abs/2504.04010</guid>
<content:encoded><![CDATA[
Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 21:19:46 GMT</pubDate>
</item>
<item>
<title>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens</title>
<link>https://arxiv.org/abs/2504.07096</link>
<guid>https://arxiv.org/abs/2504.07096</guid>
<content:encoded><![CDATA[
We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</title>
<link>https://arxiv.org/abs/2504.07083</link>
<guid>https://arxiv.org/abs/2504.07083</guid>
<content:encoded><![CDATA[
Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?</title>
<link>https://arxiv.org/abs/2504.06514</link>
<guid>https://arxiv.org/abs/2504.06514</guid>
<content:encoded><![CDATA[
We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</title>
<link>https://arxiv.org/abs/2504.04842</link>
<guid>https://arxiv.org/abs/2504.04842</guid>
<content:encoded><![CDATA[
Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 04:56:01 GMT</pubDate>
</item>
<item>
<title>DDT: Decoupled Diffusion Transformer</title>
<link>https://arxiv.org/abs/2504.05741</link>
<guid>https://arxiv.org/abs/2504.05741</guid>
<content:encoded><![CDATA[
Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \color{ddtD}ecoupled \color{ddtD}iffusion \color{ddtT}ransformer~(\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 03:17:45 GMT</pubDate>
</item>
<item>
<title>HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference</title>
<link>https://arxiv.org/abs/2504.05897</link>
<guid>https://arxiv.org/abs/2504.05897</guid>
<content:encoded><![CDATA[
The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 06:47:37 GMT</pubDate>
</item>
<item>
<title>Efficient Reinforcement Finetuning via Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.05520</link>
<guid>https://arxiv.org/abs/2504.05520</guid>
<content:encoded><![CDATA[
Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:31:31 GMT</pubDate>
</item>
<item>
<title>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2504.03755</link>
<guid>https://arxiv.org/abs/2504.03755</guid>
<content:encoded><![CDATA[
Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 02:13:14 GMT</pubDate>
</item>
<item>
<title>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</title>
<link>https://arxiv.org/abs/2504.06122</link>
<guid>https://arxiv.org/abs/2504.06122</guid>
<content:encoded><![CDATA[
Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:15:26 GMT</pubDate>
</item>
<item>
<title>HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</title>
<link>https://arxiv.org/abs/2504.06232</link>
<guid>https://arxiv.org/abs/2504.06232</guid>
<content:encoded><![CDATA[
Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:30:40 GMT</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:43:01 GMT</pubDate>
</item>
<item>
<title>COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values</title>
<link>https://arxiv.org/abs/2504.05535</link>
<guid>https://arxiv.org/abs/2504.05535</guid>
<content:encoded><![CDATA[
Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:15:51 GMT</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[
Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 09:28:57 GMT</pubDate>
</item>
<item>
<title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
<link>https://arxiv.org/abs/2504.06263</link>
<guid>https://arxiv.org/abs/2504.06263</guid>
<content:encoded><![CDATA[
Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:49 GMT</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>An Empirical Study of GPT-4o Image Generation Capabilities</title>
<link>https://arxiv.org/abs/2504.05979</link>
<guid>https://arxiv.org/abs/2504.05979</guid>
<content:encoded><![CDATA[
The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 08:34:36 GMT</pubDate>
</item>
<item>
<title>Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought</title>
<link>https://arxiv.org/abs/2504.05599</link>
<guid>https://arxiv.org/abs/2504.05599</guid>
<content:encoded><![CDATA[
We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:19:20 GMT</pubDate>
</item>
<item>
<title>Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2504.05594</link>
<guid>https://arxiv.org/abs/2504.05594</guid>
<content:encoded><![CDATA[
Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:02:50 GMT</pubDate>
</item>
<item>
<title>Generative Evaluation of Complex Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02810</link>
<guid>https://arxiv.org/abs/2504.02810</guid>
<content:encoded><![CDATA[
With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:54:18 GMT</pubDate>
</item>
<item>
<title>Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</title>
<link>https://arxiv.org/abs/2504.02160</link>
<guid>https://arxiv.org/abs/2504.02160</guid>
<content:encoded><![CDATA[
Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 18:20:21 GMT</pubDate>
</item>
<item>
<title>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</title>
<link>https://arxiv.org/abs/2504.00043</link>
<guid>https://arxiv.org/abs/2504.00043</guid>
<content:encoded><![CDATA[
Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 16:03:36 GMT</pubDate>
</item>
<item>
<title>3D Scene Understanding Through Local Random Access Sequence Modeling</title>
<link>https://arxiv.org/abs/2504.03875</link>
<guid>https://arxiv.org/abs/2504.03875</guid>
<content:encoded><![CDATA[
3D scene understanding from single images is a pivotal problem in computer vision with numerous downstream applications in graphics, augmented reality, and robotics. While diffusion-based modeling approaches have shown promise, they often struggle to maintain object and scene consistency, especially in complex real-world scenarios. To address these limitations, we propose an autoregressive generative approach called Local Random Access Sequence (LRAS) modeling, which uses local patch quantization and randomly ordered sequence generation. By utilizing optical flow as an intermediate representation for 3D scene editing, our experiments demonstrate that LRAS achieves state-of-the-art novel view synthesis and 3D object manipulation capabilities. Furthermore, we show that our framework naturally extends to self-supervised depth estimation through a simple modification of the sequence design. By achieving strong performance on multiple 3D scene understanding tasks, LRAS provides a unified and effective framework for building the next generation of 3D vision models.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 14:59:41 GMT</pubDate>
</item>
<item>
<title>Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking</title>
<link>https://arxiv.org/abs/2504.03947</link>
<guid>https://arxiv.org/abs/2504.03947</guid>
<content:encoded><![CDATA[
We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 17:27:48 GMT</pubDate>
</item>
<item>
<title>VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.05118</link>
<guid>https://arxiv.org/abs/2504.05118</guid>
<content:encoded><![CDATA[
We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 10:21:11 GMT</pubDate>
</item>
<item>
<title>GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.04155</link>
<guid>https://arxiv.org/abs/2504.04155</guid>
<content:encoded><![CDATA[
Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:30:58 GMT</pubDate>
</item>
<item>
<title>Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</title>
<link>https://arxiv.org/abs/2504.04152</link>
<guid>https://arxiv.org/abs/2504.04152</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:10:55 GMT</pubDate>
</item>
<item>
<title>Rethinking Reflection in Pre-Training</title>
<link>https://arxiv.org/abs/2504.04022</link>
<guid>https://arxiv.org/abs/2504.04022</guid>
<content:encoded><![CDATA[
A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 22:24:07 GMT</pubDate>
</item>
<item>
<title>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</title>
<link>https://arxiv.org/abs/2504.03151</link>
<guid>https://arxiv.org/abs/2504.03151</guid>
<content:encoded><![CDATA[
Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:04:56 GMT</pubDate>
</item>
<item>
<title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title>
<link>https://arxiv.org/abs/2504.03790</link>
<guid>https://arxiv.org/abs/2504.03790</guid>
<content:encoded><![CDATA[
Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 20:41:40 GMT</pubDate>
</item>
<item>
<title>SmolVLM: Redefining small and efficient multimodal models</title>
<link>https://arxiv.org/abs/2504.05299</link>
<guid>https://arxiv.org/abs/2504.05299</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>LiveVQA: Live Visual Knowledge Seeking</title>
<link>https://arxiv.org/abs/2504.05288</link>
<guid>https://arxiv.org/abs/2504.05288</guid>
<content:encoded><![CDATA[
We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:39:31 GMT</pubDate>
</item>
<item>
<title>T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models</title>
<link>https://arxiv.org/abs/2504.04718</link>
<guid>https://arxiv.org/abs/2504.04718</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:01:17 GMT</pubDate>
</item>
<item>
<title>Clinical ModernBERT: An efficient and long context encoder for biomedical text</title>
<link>https://arxiv.org/abs/2504.03964</link>
<guid>https://arxiv.org/abs/2504.03964</guid>
<content:encoded><![CDATA[
We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 18:14:12 GMT</pubDate>
</item>
<item>
<title>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2504.03193</link>
<guid>https://arxiv.org/abs/2504.03193</guid>
<content:encoded><![CDATA[
Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 01:44:45 GMT</pubDate>
</item>
<item>
<title>BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.02812</link>
<guid>https://arxiv.org/abs/2504.02812</guid>
<content:encoded><![CDATA[
We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:55:19 GMT</pubDate>
</item>
<item>
<title>DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2504.02882</link>
<guid>https://arxiv.org/abs/2504.02882</guid>
<content:encoded><![CDATA[
Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:47:28 GMT</pubDate>
</item>
<item>
<title>URECA: Unique Region Caption Anything</title>
<link>https://arxiv.org/abs/2504.05305</link>
<guid>https://arxiv.org/abs/2504.05305</guid>
<content:encoded><![CDATA[
Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Gaussian Mixture Flow Matching Models</title>
<link>https://arxiv.org/abs/2504.05304</link>
<guid>https://arxiv.org/abs/2504.05304</guid>
<content:encoded><![CDATA[
Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>One-Minute Video Generation with Test-Time Training</title>
<link>https://arxiv.org/abs/2504.05298</link>
<guid>https://arxiv.org/abs/2504.05298</guid>
<content:encoded><![CDATA[
Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:56:31 GMT</pubDate>
</item>
<item>
<title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title>
<link>https://arxiv.org/abs/2504.04823</link>
<guid>https://arxiv.org/abs/2504.04823</guid>
<content:encoded><![CDATA[
Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 04:22:45 GMT</pubDate>
</item>
<item>
<title>Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs</title>
<link>https://arxiv.org/abs/2504.04715</link>
<guid>https://arxiv.org/abs/2504.04715</guid>
<content:encoded><![CDATA[
The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit
]]></content:encoded>
<pubDate>Sun, 06 Apr 2025 23:57:41 GMT</pubDate>
</item>
<item>
<title>Concept Lancet: Image Editing with Compositional Representation Transplant</title>
<link>https://arxiv.org/abs/2504.02828</link>
<guid>https://arxiv.org/abs/2504.02828</guid>
<content:encoded><![CDATA[
Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.03770</link>
<guid>https://arxiv.org/abs/2504.03770</guid>
<content:encoded><![CDATA[
Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 01:00:28 GMT</pubDate>
</item>
<item>
<title>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</title>
<link>https://arxiv.org/abs/2503.22738</link>
<guid>https://arxiv.org/abs/2503.22738</guid>
<content:encoded><![CDATA[
Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:58:40 GMT</pubDate>
</item>
<item>
<title>Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation</title>
<link>https://arxiv.org/abs/2504.03597</link>
<guid>https://arxiv.org/abs/2504.03597</guid>
<content:encoded><![CDATA[
Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:05:56 GMT</pubDate>
</item>
<item>
<title>Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery</title>
<link>https://arxiv.org/abs/2504.02534</link>
<guid>https://arxiv.org/abs/2504.02534</guid>
<content:encoded><![CDATA[
The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 08:37:04 GMT</pubDate>
</item>
<item>
<title>MedSAM2: Segment Anything in 3D Medical Images and Videos</title>
<link>https://arxiv.org/abs/2504.03600</link>
<guid>https://arxiv.org/abs/2504.03600</guid>
<content:encoded><![CDATA[
Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:13:37 GMT</pubDate>
</item>
<item>
<title>MegaMath: Pushing the Limits of Open Math Corpora</title>
<link>https://arxiv.org/abs/2504.02807</link>
<guid>https://arxiv.org/abs/2504.02807</guid>
<content:encoded><![CDATA[
Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:52:07 GMT</pubDate>
</item>
<item>
<title>Slow-Fast Architecture for Video Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2504.01328</link>
<guid>https://arxiv.org/abs/2504.01328</guid>
<content:encoded><![CDATA[
Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 23:24:58 GMT</pubDate>
</item>
<item>
<title>SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning</title>
<link>https://arxiv.org/abs/2504.00396</link>
<guid>https://arxiv.org/abs/2504.00396</guid>
<content:encoded><![CDATA[
Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 23:37:30 GMT</pubDate>
</item>
<item>
<title>TransMamba: Flexibly Switching between Transformer and Mamba</title>
<link>https://arxiv.org/abs/2503.24067</link>
<guid>https://arxiv.org/abs/2503.24067</guid>
<content:encoded><![CDATA[
Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 09:26:24 GMT</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:13:57 GMT</pubDate>
</item>
<item>
<title>HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration</title>
<link>https://arxiv.org/abs/2504.03536</link>
<guid>https://arxiv.org/abs/2504.03536</guid>
<content:encoded><![CDATA[
Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 11:35:14 GMT</pubDate>
</item>
<item>
<title>Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization</title>
<link>https://arxiv.org/abs/2504.03011</link>
<guid>https://arxiv.org/abs/2504.03011</guid>
<content:encoded><![CDATA[
This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 16:10:50 GMT</pubDate>
</item>
<item>
<title>EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling</title>
<link>https://arxiv.org/abs/2504.02402</link>
<guid>https://arxiv.org/abs/2504.02402</guid>
<content:encoded><![CDATA[
When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 04:51:17 GMT</pubDate>
</item>
<item>
<title>BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models</title>
<link>https://arxiv.org/abs/2503.24310</link>
<guid>https://arxiv.org/abs/2503.24310</guid>
<content:encoded><![CDATA[
In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 12:56:52 GMT</pubDate>
</item>
<item>
<title>MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models</title>
<link>https://arxiv.org/abs/2504.03641</link>
<guid>https://arxiv.org/abs/2504.03641</guid>
<content:encoded><![CDATA[
Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 12:10:57 GMT</pubDate>
</item>
<item>
<title>Agentic Knowledgeable Self-awareness</title>
<link>https://arxiv.org/abs/2504.03553</link>
<guid>https://arxiv.org/abs/2504.03553</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 12:03:38 GMT</pubDate>
</item>
<item>
<title>VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.02949</link>
<guid>https://arxiv.org/abs/2504.02949</guid>
<content:encoded><![CDATA[
In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 14:06:28 GMT</pubDate>
</item>
<item>
<title>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</title>
<link>https://arxiv.org/abs/2504.02605</link>
<guid>https://arxiv.org/abs/2504.02605</guid>
<content:encoded><![CDATA[
The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 10:06:17 GMT</pubDate>
</item>
<item>
<title>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</title>
<link>https://arxiv.org/abs/2504.01943</link>
<guid>https://arxiv.org/abs/2504.01943</guid>
<content:encoded><![CDATA[
Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:50:31 GMT</pubDate>
</item>
<item>
<title>Scene-Centric Unsupervised Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2504.01955</link>
<guid>https://arxiv.org/abs/2504.01955</guid>
<content:encoded><![CDATA[
Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:58:46 GMT</pubDate>
</item>
<item>
<title>FreSca: Unveiling the Scaling Space in Diffusion Models</title>
<link>https://arxiv.org/abs/2504.02154</link>
<guid>https://arxiv.org/abs/2504.02154</guid>
<content:encoded><![CDATA[
Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 18:03:11 GMT</pubDate>
</item>
<item>
<title>WikiVideo: Article Generation from Multiple Videos</title>
<link>https://arxiv.org/abs/2504.00939</link>
<guid>https://arxiv.org/abs/2504.00939</guid>
<content:encoded><![CDATA[
We present the challenging task of automatically creating a high-level Wikipedia-style article that aggregates information from multiple diverse videos about real-world events, such as natural disasters or political elections. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text and existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles' claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 12:22:15 GMT</pubDate>
</item>
<item>
<title>JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization</title>
<link>https://arxiv.org/abs/2503.23377</link>
<guid>https://arxiv.org/abs/2503.23377</guid>
<content:encoded><![CDATA[
This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 05:40:42 GMT</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Generalist Reward Modeling</title>
<link>https://arxiv.org/abs/2504.02495</link>
<guid>https://arxiv.org/abs/2504.02495</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 07:19:49 GMT</pubDate>
</item>
<item>
<title>Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2503.23542</link>
<guid>https://arxiv.org/abs/2503.23542</guid>
<content:encoded><![CDATA[
Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\% for in-distribution datasets and up to 34\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 14:03:52 GMT</pubDate>
</item>
<item>
<title>NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations</title>
<link>https://arxiv.org/abs/2503.23162</link>
<guid>https://arxiv.org/abs/2503.23162</guid>
<content:encoded><![CDATA[
3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 13:36:53 GMT</pubDate>
</item>
<item>
<title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.02821</link>
<guid>https://arxiv.org/abs/2504.02821</guid>
<content:encoded><![CDATA[
Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:58:35 GMT</pubDate>
</item>
<item>
<title>Interpreting Emergent Planning in Model-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.01871</link>
<guid>https://arxiv.org/abs/2504.01871</guid>
<content:encoded><![CDATA[
We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 12:24:23 GMT</pubDate>
</item>
<item>
<title>Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems</title>
<link>https://arxiv.org/abs/2504.01990</link>
<guid>https://arxiv.org/abs/2504.01990</guid>
<content:encoded><![CDATA[
The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 14:00:29 GMT</pubDate>
</item>
<item>
<title>Scaling Laws in Scientific Discovery with AI and Robot Scientists</title>
<link>https://arxiv.org/abs/2503.22444</link>
<guid>https://arxiv.org/abs/2503.22444</guid>
<content:encoded><![CDATA[
Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 10:00:27 GMT</pubDate>
</item>
<item>
<title>ZClip: Adaptive Spike Mitigation for LLM Pre-Training</title>
<link>https://arxiv.org/abs/2504.02507</link>
<guid>https://arxiv.org/abs/2504.02507</guid>
<content:encoded><![CDATA[
Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 07:41:55 GMT</pubDate>
</item>
<item>
<title>Scaling Analysis of Interleaved Speech-Text Language Models</title>
<link>https://arxiv.org/abs/2504.02398</link>
<guid>https://arxiv.org/abs/2504.02398</guid>
<content:encoded><![CDATA[
Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 04:46:56 GMT</pubDate>
</item>
<item>
<title>Instruction-Guided Autoregressive Neural Network Parameter Generation</title>
<link>https://arxiv.org/abs/2504.02012</link>
<guid>https://arxiv.org/abs/2504.02012</guid>
<content:encoded><![CDATA[
Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:50:19 GMT</pubDate>
</item>
<item>
<title>GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning</title>
<link>https://arxiv.org/abs/2504.00891</link>
<guid>https://arxiv.org/abs/2504.00891</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers</title>
<link>https://arxiv.org/abs/2504.00502</link>
<guid>https://arxiv.org/abs/2504.00502</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:47:55 GMT</pubDate>
</item>
<item>
<title>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing</title>
<link>https://arxiv.org/abs/2504.02826</link>
<guid>https://arxiv.org/abs/2504.02826</guid>
<content:encoded><![CDATA[
Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation</title>
<link>https://arxiv.org/abs/2504.02782</link>
<guid>https://arxiv.org/abs/2504.02782</guid>
<content:encoded><![CDATA[
The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:23:16 GMT</pubDate>
</item>
<item>
<title>Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme</title>
<link>https://arxiv.org/abs/2504.02587</link>
<guid>https://arxiv.org/abs/2504.02587</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 09:53:28 GMT</pubDate>
</item>
<item>
<title>Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</title>
<link>https://arxiv.org/abs/2504.02542</link>
<guid>https://arxiv.org/abs/2504.02542</guid>
<content:encoded><![CDATA[
Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 08:44:41 GMT</pubDate>
</item>
<item>
<title>SkyReels-A2: Compose Anything in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.02436</link>
<guid>https://arxiv.org/abs/2504.02436</guid>
<content:encoded><![CDATA[
This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 05:50:50 GMT</pubDate>
</item>
<item>
<title>Efficient Model Selection for Time Series Forecasting via LLMs</title>
<link>https://arxiv.org/abs/2504.02119</link>
<guid>https://arxiv.org/abs/2504.02119</guid>
<content:encoded><![CDATA[
Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 16:33:27 GMT</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 03:20:58 GMT</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 17:10:39 GMT</pubDate>
</item>
<item>
<title>Target-Aware Video Diffusion Models</title>
<link>https://arxiv.org/abs/2503.18950</link>
<guid>https://arxiv.org/abs/2503.18950</guid>
<content:encoded><![CDATA[
We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>Medical large language models are easily distracted</title>
<link>https://arxiv.org/abs/2504.01201</link>
<guid>https://arxiv.org/abs/2504.01201</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:34:01 GMT</pubDate>
</item>
<item>
<title>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis</title>
<link>https://arxiv.org/abs/2502.18924</link>
<guid>https://arxiv.org/abs/2502.18924</guid>
<content:encoded><![CDATA[
While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 03:22:00 GMT</pubDate>
</item>
<item>
<title>DASH: Detection and Assessment of Systematic Hallucinations of VLMs</title>
<link>https://arxiv.org/abs/2503.23573</link>
<guid>https://arxiv.org/abs/2503.23573</guid>
<content:encoded><![CDATA[
Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 15:45:09 GMT</pubDate>
</item>
<item>
<title>Towards Physically Plausible Video Generation via VLM Planning</title>
<link>https://arxiv.org/abs/2503.23368</link>
<guid>https://arxiv.org/abs/2503.23368</guid>
<content:encoded><![CDATA[
Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 05:03:09 GMT</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:05:03 GMT</pubDate>
</item>
<item>
<title>Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations</title>
<link>https://arxiv.org/abs/2503.18817</link>
<guid>https://arxiv.org/abs/2503.18817</guid>
<content:encoded><![CDATA[
Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:00:21 GMT</pubDate>
</item>
<item>
<title>Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback</title>
<link>https://arxiv.org/abs/2405.20216</link>
<guid>https://arxiv.org/abs/2405.20216</guid>
<content:encoded><![CDATA[
The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.
]]></content:encoded>
<pubDate>Thu, 30 May 2024 12:18:05 GMT</pubDate>
</item>
<item>
<title>VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</title>
<link>https://arxiv.org/abs/2504.01956</link>
<guid>https://arxiv.org/abs/2504.01956</guid>
<content:encoded><![CDATA[
Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement</title>
<link>https://arxiv.org/abs/2504.01934</link>
<guid>https://arxiv.org/abs/2504.01934</guid>
<content:encoded><![CDATA[
We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:45:00 GMT</pubDate>
</item>
<item>
<title>Understanding R1-Zero-Like Training: A Critical Perspective</title>
<link>https://arxiv.org/abs/2503.20783</link>
<guid>https://arxiv.org/abs/2503.20783</guid>
<content:encoded><![CDATA[
DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>PaperBench: Evaluating AI's Ability to Replicate AI Research</title>
<link>https://arxiv.org/abs/2504.01848</link>
<guid>https://arxiv.org/abs/2504.01848</guid>
<content:encoded><![CDATA[
We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 11:55:24 GMT</pubDate>
</item>
<item>
<title>DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</title>
<link>https://arxiv.org/abs/2504.01724</link>
<guid>https://arxiv.org/abs/2504.01724</guid>
<content:encoded><![CDATA[
While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 09:30:32 GMT</pubDate>
</item>
<item>
<title>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</title>
<link>https://arxiv.org/abs/2504.01308</link>
<guid>https://arxiv.org/abs/2504.01308</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 22:35:19 GMT</pubDate>
</item>
<item>
<title>Articulated Kinematics Distillation from Video Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01204</link>
<guid>https://arxiv.org/abs/2504.01204</guid>
<content:encoded><![CDATA[
We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:37:57 GMT</pubDate>
</item>
<item>
<title>AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction</title>
<link>https://arxiv.org/abs/2504.01014</link>
<guid>https://arxiv.org/abs/2504.01014</guid>
<content:encoded><![CDATA[
Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:57:18 GMT</pubDate>
</item>
<item>
<title>MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</title>
<link>https://arxiv.org/abs/2504.00999</link>
<guid>https://arxiv.org/abs/2504.00999</guid>
<content:encoded><![CDATA[
Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:39:19 GMT</pubDate>
</item>
<item>
<title>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</title>
<link>https://arxiv.org/abs/2504.00883</link>
<guid>https://arxiv.org/abs/2504.00883</guid>
<content:encoded><![CDATA[
Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:11:11 GMT</pubDate>
</item>
<item>
<title>ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations</title>
<link>https://arxiv.org/abs/2504.00824</link>
<guid>https://arxiv.org/abs/2504.00824</guid>
<content:encoded><![CDATA[
Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:12:14 GMT</pubDate>
</item>
<item>
<title>LSNet: See Large, Focus Small</title>
<link>https://arxiv.org/abs/2503.23135</link>
<guid>https://arxiv.org/abs/2503.23135</guid>
<content:encoded><![CDATA[
Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:00:54 GMT</pubDate>
</item>
<item>
<title>Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models</title>
<link>https://arxiv.org/abs/2503.22165</link>
<guid>https://arxiv.org/abs/2503.22165</guid>
<content:encoded><![CDATA[
Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 02:09:51 GMT</pubDate>
</item>
<item>
<title>Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL</title>
<link>https://arxiv.org/abs/2503.23157</link>
<guid>https://arxiv.org/abs/2503.23157</guid>
<content:encoded><![CDATA[
Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 13:29:30 GMT</pubDate>
</item>
<item>
<title>MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing</title>
<link>https://arxiv.org/abs/2503.24219</link>
<guid>https://arxiv.org/abs/2503.24219</guid>
<content:encoded><![CDATA[
We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:36:41 GMT</pubDate>
</item>
<item>
<title>MixerMDM: Learnable Composition of Human Motion Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01019</link>
<guid>https://arxiv.org/abs/2504.01019</guid>
<content:encoded><![CDATA[
Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Scaling Language-Free Visual Representation Learning</title>
<link>https://arxiv.org/abs/2504.01017</link>
<guid>https://arxiv.org/abs/2504.01017</guid>
<content:encoded><![CDATA[
Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:15 GMT</pubDate>
</item>
<item>
<title>DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.24210</link>
<guid>https://arxiv.org/abs/2503.24210</guid>
<content:encoded><![CDATA[
Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:27:07 GMT</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 22:18:51 GMT</pubDate>
</item>
<item>
<title>Towards Trustworthy GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2503.23434</link>
<guid>https://arxiv.org/abs/2503.23434</guid>
<content:encoded><![CDATA[
GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 09:26:00 GMT</pubDate>
</item>
<item>
<title>OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts</title>
<link>https://arxiv.org/abs/2503.22952</link>
<guid>https://arxiv.org/abs/2503.22952</guid>
<content:encoded><![CDATA[
The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 22:46:58 GMT</pubDate>
</item>
<item>
<title>Multi-Token Attention</title>
<link>https://arxiv.org/abs/2504.00927</link>
<guid>https://arxiv.org/abs/2504.00927</guid>
<content:encoded><![CDATA[
Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:59:32 GMT</pubDate>
</item>
<item>
<title>m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.00869</link>
<guid>https://arxiv.org/abs/2504.00869</guid>
<content:encoded><![CDATA[
Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:57:43 GMT</pubDate>
</item>
<item>
<title>Z1: Efficient Test-time Scaling with Code</title>
<link>https://arxiv.org/abs/2504.00810</link>
<guid>https://arxiv.org/abs/2504.00810</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., . . . ) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:01:50 GMT</pubDate>
</item>
<item>
<title>Command A: An Enterprise-Ready Large Language Model</title>
<link>https://arxiv.org/abs/2504.00698</link>
<guid>https://arxiv.org/abs/2504.00698</guid>
<content:encoded><![CDATA[
In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 08:08:07 GMT</pubDate>
</item>
<item>
<title>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources</title>
<link>https://arxiv.org/abs/2504.00595</link>
<guid>https://arxiv.org/abs/2504.00595</guid>
<content:encoded><![CDATA[
The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:54:00 GMT</pubDate>
</item>
<item>
<title>Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features</title>
<link>https://arxiv.org/abs/2504.00557</link>
<guid>https://arxiv.org/abs/2504.00557</guid>
<content:encoded><![CDATA[
Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:10:32 GMT</pubDate>
</item>
<item>
<title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
<link>https://arxiv.org/abs/2504.00509</link>
<guid>https://arxiv.org/abs/2504.00509</guid>
<content:encoded><![CDATA[
The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:57:58 GMT</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead</title>
<link>https://arxiv.org/abs/2504.00294</link>
<guid>https://arxiv.org/abs/2504.00294</guid>
<content:encoded><![CDATA[
Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 19:40:28 GMT</pubDate>
</item>
<item>
<title>Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs</title>
<link>https://arxiv.org/abs/2504.00072</link>
<guid>https://arxiv.org/abs/2504.00072</guid>
<content:encoded><![CDATA[
We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:41:29 GMT</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:50:39 GMT</pubDate>
</item>
<item>
<title>GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors</title>
<link>https://arxiv.org/abs/2504.01016</link>
<guid>https://arxiv.org/abs/2504.01016</guid>
<content:encoded><![CDATA[
Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.01005</link>
<guid>https://arxiv.org/abs/2504.01005</guid>
<content:encoded><![CDATA[
Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:41:57 GMT</pubDate>
</item>
<item>
<title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
<link>https://arxiv.org/abs/2504.00906</link>
<guid>https://arxiv.org/abs/2504.00906</guid>
<content:encoded><![CDATA[
Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:40:27 GMT</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.24377</link>
<guid>https://arxiv.org/abs/2503.24377</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1</title>
<link>https://arxiv.org/abs/2503.24376</link>
<guid>https://arxiv.org/abs/2503.24376</guid>
<content:encoded><![CDATA[
Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:55:23 GMT</pubDate>
</item>
<item>
<title>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</title>
<link>https://arxiv.org/abs/2503.23733</link>
<guid>https://arxiv.org/abs/2503.23733</guid>
<content:encoded><![CDATA[
Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:13:02 GMT</pubDate>
</item>
<item>
<title>Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base</title>
<link>https://arxiv.org/abs/2503.23361</link>
<guid>https://arxiv.org/abs/2503.23361</guid>
<content:encoded><![CDATA[
Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 04:33:56 GMT</pubDate>
</item>
<item>
<title>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</title>
<link>https://arxiv.org/abs/2503.21860</link>
<guid>https://arxiv.org/abs/2503.21860</guid>
<content:encoded><![CDATA[
Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:50:30 GMT</pubDate>
</item>
<item>
<title>DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</title>
<link>https://arxiv.org/abs/2503.22677</link>
<guid>https://arxiv.org/abs/2503.22677</guid>
<content:encoded><![CDATA[
Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>ActionStudio: A Lightweight Framework for Data and Training of Large Action Models</title>
<link>https://arxiv.org/abs/2503.22673</link>
<guid>https://arxiv.org/abs/2503.22673</guid>
<content:encoded><![CDATA[
Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>AvatarArtist: Open-Domain 4D Avatarization</title>
<link>https://arxiv.org/abs/2503.19906</link>
<guid>https://arxiv.org/abs/2503.19906</guid>
<content:encoded><![CDATA[
This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>PAVE: Patching and Adapting Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.19794</link>
<guid>https://arxiv.org/abs/2503.19794</guid>
<content:encoded><![CDATA[
Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:02:37 GMT</pubDate>
</item>
<item>
<title>Understanding Co-speech Gestures in-the-wild</title>
<link>https://arxiv.org/abs/2503.22668</link>
<guid>https://arxiv.org/abs/2503.22668</guid>
<content:encoded><![CDATA[
Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:55:52 GMT</pubDate>
</item>
<item>
<title>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</title>
<link>https://arxiv.org/abs/2503.22655</link>
<guid>https://arxiv.org/abs/2503.22655</guid>
<content:encoded><![CDATA[
Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:43:00 GMT</pubDate>
</item>
<item>
<title>Entropy-Based Adaptive Weighting for Self-Training</title>
<link>https://arxiv.org/abs/2503.23913</link>
<guid>https://arxiv.org/abs/2503.23913</guid>
<content:encoded><![CDATA[
The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 06:04:35 GMT</pubDate>
</item>
<item>
<title>MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs</title>
<link>https://arxiv.org/abs/2503.23022</link>
<guid>https://arxiv.org/abs/2503.23022</guid>
<content:encoded><![CDATA[
In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 05:21:50 GMT</pubDate>
</item>
<item>
<title>Decoupling Angles and Strength in Low-rank Adaptation</title>
<link>https://arxiv.org/abs/2503.18225</link>
<guid>https://arxiv.org/abs/2503.18225</guid>
<content:encoded><![CDATA[
Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 18:00:56 GMT</pubDate>
</item>
<item>
<title>Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</title>
<link>https://arxiv.org/abs/2503.24391</link>
<guid>https://arxiv.org/abs/2503.24391</guid>
<content:encoded><![CDATA[
Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data</title>
<link>https://arxiv.org/abs/2503.21694</link>
<guid>https://arxiv.org/abs/2503.21694</guid>
<content:encoded><![CDATA[
It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 12:59:15 GMT</pubDate>
</item>
<item>
<title>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</title>
<link>https://arxiv.org/abs/2503.19901</link>
<guid>https://arxiv.org/abs/2503.19901</guid>
<content:encoded><![CDATA[
Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:57:46 GMT</pubDate>
</item>
<item>
<title>UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2503.14941</link>
<guid>https://arxiv.org/abs/2503.14941</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&amp;A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 03:15:41 GMT</pubDate>
</item>
<item>
<title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</title>
<link>https://arxiv.org/abs/2503.24388</link>
<guid>https://arxiv.org/abs/2503.24388</guid>
<content:encoded><![CDATA[
Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Query and Conquer: Execution-Guided SQL Generation</title>
<link>https://arxiv.org/abs/2503.24364</link>
<guid>https://arxiv.org/abs/2503.24364</guid>
<content:encoded><![CDATA[
We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:43:36 GMT</pubDate>
</item>
<item>
<title>Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model</title>
<link>https://arxiv.org/abs/2503.24290</link>
<guid>https://arxiv.org/abs/2503.24290</guid>
<content:encoded><![CDATA[
We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 12:36:05 GMT</pubDate>
</item>
<item>
<title>What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:46:15 GMT</pubDate>
</item>
<item>
<title>Expanding RL with Verifiable Rewards Across Diverse Domains</title>
<link>https://arxiv.org/abs/2503.23829</link>
<guid>https://arxiv.org/abs/2503.23829</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 04:22:49 GMT</pubDate>
</item>
<item>
<title>KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language</title>
<link>https://arxiv.org/abs/2503.23730</link>
<guid>https://arxiv.org/abs/2503.23730</guid>
<content:encoded><![CDATA[
The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:04:25 GMT</pubDate>
</item>
<item>
<title>TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes</title>
<link>https://arxiv.org/abs/2503.23461</link>
<guid>https://arxiv.org/abs/2503.23461</guid>
<content:encoded><![CDATA[
This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 10:36:55 GMT</pubDate>
</item>
<item>
<title>SketchVideo: Sketch-based Video Generation and Editing</title>
<link>https://arxiv.org/abs/2503.23284</link>
<guid>https://arxiv.org/abs/2503.23284</guid>
<content:encoded><![CDATA[
Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 22:44:09 GMT</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 09:27:46 GMT</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 10:06:17 GMT</pubDate>
</item>
<item>
<title>MoCha: Towards Movie-Grade Talking Character Synthesis</title>
<link>https://arxiv.org/abs/2503.23307</link>
<guid>https://arxiv.org/abs/2503.23307</guid>
<content:encoded><![CDATA[
Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 00:22:09 GMT</pubDate>
</item>
<item>
<title>Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization</title>
<link>https://arxiv.org/abs/2503.20286</link>
<guid>https://arxiv.org/abs/2503.20286</guid>
<content:encoded><![CDATA[
Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 03:30:23 GMT</pubDate>
</item>
<item>
<title>Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code</title>
<link>https://arxiv.org/abs/2503.18809</link>
<guid>https://arxiv.org/abs/2503.18809</guid>
<content:encoded><![CDATA[
In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:50:20 GMT</pubDate>
</item>
<item>
<title>SWI: Speaking with Intent in Large Language Models</title>
<link>https://arxiv.org/abs/2503.21544</link>
<guid>https://arxiv.org/abs/2503.21544</guid>
<content:encoded><![CDATA[
Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:34:28 GMT</pubDate>
</item>
<item>
<title>Reconstructing Humans with a Biomechanically Accurate Skeleton</title>
<link>https://arxiv.org/abs/2503.21751</link>
<guid>https://arxiv.org/abs/2503.21751</guid>
<content:encoded><![CDATA[
In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:24 GMT</pubDate>
</item>
<item>
<title>Your ViT is Secretly an Image Segmentation Model</title>
<link>https://arxiv.org/abs/2503.19108</link>
<guid>https://arxiv.org/abs/2503.19108</guid>
<content:encoded><![CDATA[
Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 15:56:02 GMT</pubDate>
</item>
<item>
<title>4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding</title>
<link>https://arxiv.org/abs/2503.17827</link>
<guid>https://arxiv.org/abs/2503.17827</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\% accuracy compared to the human baseline of 91\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 13:55:53 GMT</pubDate>
</item>
<item>
<title>OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning</title>
<link>https://arxiv.org/abs/2503.16081</link>
<guid>https://arxiv.org/abs/2503.16081</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have gained significant traction for their ability to process diverse input data types and generate coherent, contextually relevant outputs across various applications. While supervised fine-tuning (SFT) has been the predominant approach to enhance MLLM capabilities in task-specific optimization, it often falls short in fostering crucial generalized reasoning abilities. Although reinforcement learning (RL) holds great promise in overcoming these limitations, it encounters two significant challenges: (1) its generalized capacities in multimodal tasks remain largely unexplored, and (2) its training constraints, including the constant Kullback-Leibler divergence or the clamp strategy, often result in suboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an advanced MLLM equipped with profound comprehension and reasoning capabilities across multimodal tasks. Specifically, we introduce Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly enhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct, GRPO-D achieves a relative improvement of more than 5.72% over SFT and more than 13.59% over GRPO in same-task evaluation on two adapted datasets. Furthermore, GRPO-D demonstrates remarkable cross-task generalization capabilities, with an average relative improvement of more than 61.63% over SFT in cross-task evaluation. These results highlight that the MLLM trained with GRPO-D on one multimodal task can be effectively transferred to another task, underscoring the superior generalized reasoning capabilities of our proposed OThink-MR1 model.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:22:18 GMT</pubDate>
</item>
<item>
<title>A Refined Analysis of Massive Activations in LLMs</title>
<link>https://arxiv.org/abs/2503.22329</link>
<guid>https://arxiv.org/abs/2503.22329</guid>
<content:encoded><![CDATA[
Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 07:08:34 GMT</pubDate>
</item>
<item>
<title>Segment Any Motion in Videos</title>
<link>https://arxiv.org/abs/2503.22268</link>
<guid>https://arxiv.org/abs/2503.22268</guid>
<content:encoded><![CDATA[
Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 05:34:11 GMT</pubDate>
</item>
<item>
<title>Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging</title>
<link>https://arxiv.org/abs/2503.22236</link>
<guid>https://arxiv.org/abs/2503.22236</guid>
<content:encoded><![CDATA[
With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:39:20 GMT</pubDate>
</item>
<item>
<title>Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2503.22230</link>
<guid>https://arxiv.org/abs/2503.22230</guid>
<content:encoded><![CDATA[
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:26:41 GMT</pubDate>
</item>
<item>
<title>X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction</title>
<link>https://arxiv.org/abs/2503.21779</link>
<guid>https://arxiv.org/abs/2503.21779</guid>
<content:encoded><![CDATA[
Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</title>
<link>https://arxiv.org/abs/2503.21732</link>
<guid>https://arxiv.org/abs/2503.21732</guid>
<content:encoded><![CDATA[
Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:46:42 GMT</pubDate>
</item>
<item>
<title>On Large Multimodal Models as Open-World Image Classifiers</title>
<link>https://arxiv.org/abs/2503.21851</link>
<guid>https://arxiv.org/abs/2503.21851</guid>
<content:encoded><![CDATA[
Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:03:18 GMT</pubDate>
</item>
<item>
<title>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</title>
<link>https://arxiv.org/abs/2503.21614</link>
<guid>https://arxiv.org/abs/2503.21614</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback</title>
<link>https://arxiv.org/abs/2503.21332</link>
<guid>https://arxiv.org/abs/2503.21332</guid>
<content:encoded><![CDATA[
Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 06:11:41 GMT</pubDate>
</item>
<item>
<title>Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency</title>
<link>https://arxiv.org/abs/2503.20785</link>
<guid>https://arxiv.org/abs/2503.20785</guid>
<content:encoded><![CDATA[
We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2503.20308</link>
<guid>https://arxiv.org/abs/2503.20308</guid>
<content:encoded><![CDATA[
Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:18:57 GMT</pubDate>
</item>
<item>
<title>PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving</title>
<link>https://arxiv.org/abs/2503.21821</link>
<guid>https://arxiv.org/abs/2503.21821</guid>
<content:encoded><![CDATA[
We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:21:56 GMT</pubDate>
</item>
<item>
<title>AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2503.19693</link>
<guid>https://arxiv.org/abs/2503.19693</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 10:18:21 GMT</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 10:04:18 GMT</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.22194</link>
<guid>https://arxiv.org/abs/2503.22194</guid>
<content:encoded><![CDATA[
We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 03:23:12 GMT</pubDate>
</item>
<item>
<title>Tracktention Layer：提升视频预测中的时间一致性</title>
<link>https://arxiv.org/abs/2503.19904</link>
<guid>https://arxiv.org/abs/2503.19904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Tracktention Layer以增强视频预测中的时间一致性。</p><br /><br /><p><strong>摘要：</strong> 视频预测中的时间一致性至关重要，以确保输出的连贯性和去除伪影。传统方法如时间注意力和3D卷积在应对剧烈物体运动时存在困难，并且可能无法捕捉动态场景中的长程时间依赖。为了解决这个问题，本文提出了一种新颖的Tracktention Layer结构组件，它通过点轨迹明确集成运动信息。Tracktention Layer通过这些运动线索来增强时间对齐，有效处理复杂物体运动，并在时间上保持一致的特征表示。此方法计算效率高，可无缝集成到现有模型（如视觉Transformer）中，且仅需微小修改。它还可用于将图像模型升级为最先进的视频模型，有时甚至超越原生设计用于视频预测的模型。实验结果显示，在视频深度预测和视频着色任务中，增添Tracktention Layer的模型在时间一致性上明显优于基线对照。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:48 GMT</pubDate>
</item>
<item>
<title>无训练的测试时领域适应框架SemLA在开放词汇语义分割中的应用</title>
<link>https://arxiv.org/abs/2503.21780</link>
<guid>https://arxiv.org/abs/2503.21780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SemLA是一个无训练的测试时领域适应框架，提升了开放词汇语义分割的适应性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SemLA，一个新颖的无训练框架，旨在在测试时实现领域适应，提升开放词汇语义分割模型的性能。SemLA依托基于LoRA的适配器库，并通过CLIP嵌入进行索引，能够根据目标领域在嵌入空间中的近邻动态合并最相关的适配器，从而无需额外训练便构建出针对具体输入的定制模型。该方法有效地扩展了适应性，增强了模型的可解释性，并在保护数据隐私方面具有优势。针对10个标准数据集构建的20领域基准测试的全面实验显示，SemLA在多样化的设置下展现了卓越的适应性与性能，建立了开放词汇语义分割领域适应的新标准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>LOCATEdit：基于图的文本引导图像编辑方法</title>
<link>https://arxiv.org/abs/2503.21541</link>
<guid>https://arxiv.org/abs/2503.21541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOCATEdit通过图形化方法提升图像编辑的准确性和一致性。</p><br /><br /><p><strong>摘要：</strong> TEXT-guided image editing 旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的完整性。现有方法依赖于从扩散模型生成的交叉注意力图来识别需要修改的目标区域，但由于这些机制侧重于语义相关性，往往无法维持图像的完整性，导致缺乏空间一致性和产生编辑伪影。本文提出的LOCATEdit则通过基于图的方法增强交叉注意力图，利用自注意力所产生的块关系，确保图像区域间的平滑一致性，限制修改仅在指定区域，同时保留周围的结构。LOCATEdit在PIE-Bench上表现优于现有基线方法，在多项编辑任务中展现出卓越的性能和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:32:17 GMT</pubDate>
</item>
<item>
<title>Embodied Reasoner：提升交互式体态搜索任务的推理能力</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Embodied Reasoner 在交互式体态搜索任务中展现出卓越的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期的深度推理模型在数学和编码任务上表现出了杰出的推理能力，但在需要通过图像和动作交替轨迹与环境持续交互的体态领域尚待探索。为此，我们提出了 Embodied Reasoner 模型，旨在扩展到交互式体态搜索任务。与主要依赖逻辑推理的数学推理不同，体态场景需要空间理解、时间推理以及基于交互历史的持续自我反思。我们合成了包含 9.3k 个连贯观察-思考-行动轨迹的训练数据，涵盖 64k 张交互图像和 90k 种多样的思维过程。通过模仿学习、自我探索和自我校正等三阶段训练流程，模型显著提升了能力，评估结果显示其在复杂长时间任务中，超过了 OpenAI o1、o3-mini 和 Claude-3.7 等先进视觉推理模型，表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:00:51 GMT</pubDate>
</item>
<item>
<title>大语言模型在科学发现中的潜力与新基准</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了评估大语言模型在科学发现中的研究假设生成能力的新基准。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）在科学研究中展现了潜力，但其发现高质量研究假设的能力尚未经过检验。为此，本文提出第一个大规模基准，以评估LLMs在科学发现中的表现，涵盖灵感检索、假设构建和假设排名等子任务。我们开发了一个自动化框架，提取12个学科科学论文中的关键组成部分，包括研究问题、背景调查、灵感和假设，并通过专家验证确保其准确性。为避免数据污染，我们仅关注2024年发表的论文，以确保与LLMs预训练数据的重叠最小。评估结果显示，LLMs在灵感检索方面表现良好，表明它们能够发现新颖的知识关联。这使得LLMs有潜力作为“研究假设矿”，通过自动生成创新假设，促进科学发现的自动化，减少人类干预。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 04:09:15 GMT</pubDate>
</item>
<item>
<title>FinAudio: 金融领域音频语言模型评估基准</title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinAudio是首个用于评估音频语言模型在金融领域表现的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FinAudio，这是第一个专为评估音频大型语言模型（AudioLLMs）在金融场景中的能力而设计的基准。随着音频任务（如对话、音频理解和自动语音识别）性能的显著提升，金融领域中，包括收益电话会议和首席执行官演讲在内的音频数据，成为了金融分析和投资决策的重要资源。然而，至今缺乏财务情境下评估AudioLLMs的基准。本文定义了三个基于金融领域独特特征的任务，并策划了两个短音频和两个长音频数据集，同时开发了一种新数据集用于金融音频摘要，构成了FinAudio基准。在我们的评估中，评测了七种流行的AudioLLMs，并揭示了它们在金融领域的局限性，提供了改进AudioLLMs的见解。所有数据集和代码将被公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 17:07:51 GMT</pubDate>
</item>
<item>
<title>Video-R1: 基于多模态大语言模型的视频推理探索</title>
<link>https://arxiv.org/abs/2503.21776</link>
<guid>https://arxiv.org/abs/2503.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Video-R1，通过T-GRPO算法提升视频推理能力，克服数据稀缺和时序建模问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Video-R1，这是首次系统探索在多模态大语言模型（MLLMs）中应用R1范式进行视频推理。直接在视频推理上应用GRPO算法进行强化学习训练面临两个主要挑战：缺乏视频推理的时序建模以及高质量视频推理数据稀缺。为了解决这些问题，提出了T-GRPO算法，该算法鼓励模型利用视频中的时序信息进行推理。此外，我们在训练过程中还引入高质量的图像推理数据。构建了两个数据集：用于SFT冷启动的Video-R1-COT-165k和用于强化学习训练的Video-R1-260k，均包含图像和视频数据。实验结果表明，Video-R1在视频推理基准测试如VideoMMMU和VSI-Bench上取得显著提升，并在MVBench和TempCompass等一般视频基准测试中表现优异。特别是在视频空间推理基准VSI-bench上，Video-R1-7B模型达到了35.8%的准确率，超过商业专有模型GPT-4o。所有代码、模型和数据均已发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>优化步长调度的扩散模型提高生成效率</title>
<link>https://arxiv.org/abs/2503.21774</link>
<guid>https://arxiv.org/abs/2503.21774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种优化步长调度的方法，显著提升扩散模型的生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为最优步长蒸馏的动态规划框架，旨在通过从参考轨迹中提取理论最优的步长调度，以解决扩散模型在采样过程中因子步长离散化不佳而导致的计算密集性问题。通过将步长优化重构为递归误差最小化，该方法保证了全局离散化界限，并充分利用最优子结构。这些蒸馏出的调度在不同架构、ODE求解器和噪声调度中展现出强大的鲁棒性。实验结果表明，该方法使文本到图像的生成速度提高了10倍，同时保持了99.4%的性能。本研究的代码已在GitHub上发布，供进一步研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>视频生成中的物理认知进展及其挑战</title>
<link>https://arxiv.org/abs/2503.21765</link>
<guid>https://arxiv.org/abs/2503.21765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了视频生成中的物理认知进展及未来研究方向。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型的快速发展，视频生成取得了显著进展。但其在物理认知方面的不足逐渐受到重视，生成的内容常常违反物理基本法则。本文旨在填补这一领域系统概述的缺失，讨论物理认知在视频生成中的演进过程，并提出一个三层次的分类：1) 基本模式感知生成，2) 物理知识的被动认知生成，3) 世界模拟的主动认知。我们将重点强调该领域的关键挑战，并概述未来研究的潜在方向，旨在推动学术界和工业界对可解释、可控、物理一致的视频生成范式的讨论，为生成模型从“视觉模仿”向“人类般物理理解”的新阶段迈进提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>Lumina-Image 2.0：先进的文本生成图像框架</title>
<link>https://arxiv.org/abs/2503.21758</link>
<guid>https://arxiv.org/abs/2503.21758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumina-Image 2.0通过统一架构和高效训练显著提升文本生成图像性能。</p><br /><br /><p><strong>摘要：</strong> Lumina-Image 2.0是一种进阶的文本生成图像（T2I）框架，较之前的Lumina-Next取得了显著进展。该框架基于两个关键原则：统一性和效率。通过使用统一的架构（Unified Next-DiT），将文本和图像标记视为联合序列，从而实现自然的跨模态交互，并允许无缝的任务扩展。此外，Lumina-Image 2.0引入了统一的标注系统（Unified Captioner, UniCap），专门设计用于T2I生成任务，能够生成全面且准确的描述，加快收敛速度并增强提示的遵循性。在效率方面，我们开发了多阶段进阶训练策略和推理加速技术，而不降低图像质量。经过广泛的评价，Lumina-Image 2.0在学术基准和公共文本生成图像领域展现了强劲的性能，且仅用2.6B参数，显示出其可扩展性和设计效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>推进视频生成模型的内在真实性评估</title>
<link>https://arxiv.org/abs/2503.21755</link>
<guid>https://arxiv.org/abs/2503.21755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VBench-2.0旨在评估视频生成模型的内在真实度，超越表面视觉效果。</p><br /><br /><p><strong>摘要：</strong> 视频生成技术已从产生不现实的输出发展到生成视觉上令人信服且时间上连贯的视频。为了评估这些模型，VBench等基准被开发出来，衡量每帧美学、时间一致性和基本提示遵循等因素。然而，这些仅代表表面真实性，未能保证生成视频遵循现实原则。为了实现真正的“世界模型”，需要关注内在真实度，以确保生成视频遵循物理定律、常识推理、解剖正确性与组成完整性。为此，我们提出了VBench-2.0，它自动评估视频生成模型的内在真实度，包括人类符合度、可控性、创造性、物理与常识等五个关键维度，进一步细分为更细致的能力。我们的评估框架整合了前沿的VLM和LLM等通用工具，以及专门针对视频生成的异常检测方法，旨在推进视频生成模型的标准向内在真实度发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:01 GMT</pubDate>
</item>
<item>
<title>LeX-Art: 高质量文本-图像合成的完整解决方案</title>
<link>https://arxiv.org/abs/2503.21749</link>
<guid>https://arxiv.org/abs/2503.21749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LeX-Art系统性地提升文本-图像合成质量和渲染真实感。</p><br /><br /><p><strong>摘要：</strong> LeX-Art 是一套全面的高质量文本-图像合成工具，旨在系统性地弥合提示表达力和文本渲染保真度之间的差距。通过以数据为中心的方法，我们构建了一个基于 Deepseek-R1 的高质量数据合成管道，策划了 LeX-10K 数据集，包含10,000幅高分辨率和美学精炼的1024×1024图像。此外，我们开发了 LeX-Enhancer，一个强大的提示增强模型，并训练了两个文本到图像模型，LeX-FLUX 和 LeX-Lumina，取得了最先进的文本渲染性能。为系统评估视觉文本生成，我们推出了 LeX-Bench，一个评估保真度、美学和一致性的基准，辅之以对文本准确性评估的新指标 Pairwise Normalized Edit Distance（PNED）。实验结果显示，LeX-Lumina 在 CreateBench 上实现了79.81%的 PNED 增益，而 LeX-FLUX 在颜色、位置和字体准确性上均超越基线。我们的代码、模型、数据集和演示版本均已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:15 GMT</pubDate>
</item>
<item>
<title>ReaRAG：增强准确性的推理模型</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReaRAG通过增强事实性与推理能力，提升多跳问答表现。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）在推理能力方面表现出色，但主要依赖参数知识，导致事实准确性受限。尽管一些近期研究为基于强化学习的LRMs增加了检索能力，但这些模型仍存在过度思考和推理缺乏稳健性的问题，从而降低了其在问答任务中的效果。为此，我们提出了ReaRAG，一种增强事实性的推理模型，能够在避免过多迭代的情况下探索多样的查询。该解决方案包括一个新的数据构建框架，并为推理链长度设定上限。我们首先利用LRM生成深思熟虑的推理，然后从预定义的动作空间（搜索和结束）中选择一个动作。在搜索动作中，针对RAG引擎执行查询，结果作为观测返回，以指导后续的推理步骤。该过程重复进行，直至选择结束动作。得益于ReaRAG的强大推理能力，我们的方法在多跳问答方面超越了现有基准，进一步的分析突显了其强大的反思能力，可以识别错误并优化推理路径。我们的研究在有效结合稳健推理与检索增强生成的同时，提高了LRMs的事实性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:44:18 GMT</pubDate>
</item>
<item>
<title>基于规则的强化学习提升多模态大语言模型的GUI动作预测能力</title>
<link>https://arxiv.org/abs/2503.21620</link>
<guid>https://arxiv.org/abs/2503.21620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示规则基础的强化学习提升了多模态模型在GUI动作预测中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文首次探讨如何通过规则基础的强化学习（RL）提升多模态大语言模型（MLLMs）在图形用户界面（GUI）动作预测任务中的推理能力。我们构建了一个包含136个挑战性任务的高质量小型数据集，涵盖移动设备上的五种常见动作类型。此外，引入统一的基于规则的动作奖励，该奖励适用于基于策略的算法（如组相对策略优化，GRPO）。实验结果表明，所提出的数据高效模型UI-R1-3B在领域内（ID）和领域外（OOD）任务上均实现了显著改善。在ID基准AndroidControl上，动作类型的准确率提升了15%，而定位准确率提高了10.3%；在OOD GUI定位基准ScreenSpot-Pro上，我们的模型超越了基准模型6.0%，并在与更大模型（如OS-Atlas-7B）进行比较时展现了竞争力，这些模型是在76K数据上通过监督微调（SFT）训练的。这些结果凸显了基于规则的强化学习在推动GUI理解与控制方面的潜力，预示着未来研究的新方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:39:30 GMT</pubDate>
</item>
<item>
<title>智能代理时代的到来：大语言模型驱动下的研究综述</title>
<link>https://arxiv.org/abs/2503.21460</link>
<guid>https://arxiv.org/abs/2503.21460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本综述系统阐述了大语言模型代理的设计与演进。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的革命性进展，智能代理时代已经来临。大语言模型（LLM）代理以目标驱动的行为和动态适应能力，可能成为通向人工通用智能的重要途径。本综述通过以方法论为中心的分类法，系统性地分析了LLM代理系统，揭示了代理设计原则与其在复杂环境中涌现行为之间的基本联系。我们统一了零散的研究线索，提供了统一的架构视角，探讨了代理的构建、合作及其随时间演变的过程，同时还关注了评估方法、工具应用、实际挑战及多样的应用领域。通过对这一快速发展领域的最新进展进行梳理，我们为研究者提供了理解LLM代理的结构化分类，并识别未来研究的有希望方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 08:50:17 GMT</pubDate>
</item>
<item>
<title>OlymMATH: 新的奥林匹克数学基准测试大规模推理模型</title>
<link>https://arxiv.org/abs/2503.21380</link>
<guid>https://arxiv.org/abs/2503.21380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OlymMATH是一个新数学基准，旨在检验大型模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型的发展使得现有的数学推理评估基准显得饱和，亟需更加严谨和具有挑战性的评估框架。为此，我们推出了OlymMATH，这是一个新颖的奥林匹克级数学基准，专为严格测试大型语言模型的复杂推理能力而设计。OlymMATH包含200道经过仔细筛选并手动验证的问题，提供英文和中文两种版本，系统地分为两大难度层次：AIME级别的问题（简单），以及更具挑战性的问题（困难），旨在推动当前最先进模型的极限。基准覆盖四个核心数学领域，且每道题目都有可验证的数值解，以实现客观、基于规则的评估。实证结果表明，OlymMATH所带来的挑战显著，最先进的模型在困难子集上的准确率明显有限。此外，此基准还有助于全面评估数学推理能力，填补了主流数学推理基准未涉及的双语评估维度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 07:20:17 GMT</pubDate>
</item>
<item>
<title>基于音频输入的实时互动视频生成框架</title>
<link>https://arxiv.org/abs/2503.21144</link>
<guid>https://arxiv.org/abs/2503.21144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新颖框架，支持实时互动视频生成，增强表情与上半身动作的同步。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的实时互动视频生成框架，旨在克服现有技术在头部动作与身体动作同步生成中的局限。该框架分为两个主要阶段：第一阶段采用高效的层次化运动扩散模型，基于音频输入生成多样的面部表情，并实现头部与身体动作的同步；第二阶段则专注于生成包含上半身动作和手势的视频，使用显式手部控制信号来生成更细致的手部动作，并对面部进行细化处理以提高视频的真实感和表现力。我们的方案在4090 GPU上以最高512 * 768分辨率和30fps的速度支持实时互动视频聊天，实验结果显示该方法能够生成富有表现力的肖像视频，展现自然的上半身动作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:18:53 GMT</pubDate>
</item>
<item>
<title>ZJUKLAB团队在SemEval-2025任务中的敏感内容去除研究</title>
<link>https://arxiv.org/abs/2503.21088</link>
<guid>https://arxiv.org/abs/2503.21088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍ZJUKLAB团队在SemEval-2025中的敏感内容去除系统与成就。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ZJUKLAB团队在SemEval-2025任务4上提交的作品，旨在从大语言模型中选择性地删除敏感知识，解决过度遗忘和不足遗忘的问题。我们提出的去除系统基于模型融合方法（特别是TIES融合），将两个专门模型整合为一个更均衡的未学习模型。经过我们的方法在26支团队中获得了可竞争的成绩，在任务聚合及整体聚合上分别得分0.944和0.487。论文中，我们进行了局部实验并全面分析了去除过程，包括性能轨迹、损失动态及权重分析，同时进行了多项补充实验，以了解方法的有效性。此外，我们分析了方法与评估指标的不足，强调MIA分数和基于ROUGE的指标不足以全面评估去除效果，最后呼吁未来研究需要更全面的评估方法和对去除目标的重新思考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 22:03:25 GMT</pubDate>
</item>
<item>
<title>统一多模态离散扩散模型UniDisc的探索与应用</title>
<link>https://arxiv.org/abs/2503.20853</link>
<guid>https://arxiv.org/abs/2503.20853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出UniDisc模型，提升多模态生成质量与可控性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种统一的多模态离散扩散模型（UniDisc），该模型在文本和图像领域内实现了更优的生成能力。与传统自回归（AR）模型相比，UniDisc模型利用离散扩散模型的优势，改善了生成样本的质量与多样性的控制，并能够在文本与图像之间进行联合填充。UniDisc在多个下游任务中展现出强大的能力，并通过规模分析表明其在性能、推理时间计算、可控性、可编辑性及生成质量与推理时间之间的灵活权衡方面均优于多模态自回归模型。代码及更多可视化结果可在项目网站获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>Feature4X：扩展2D视觉模型至4D领域的通用框架</title>
<link>https://arxiv.org/abs/2503.20776</link>
<guid>https://arxiv.org/abs/2503.20776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feature4X框架实现了从2D到4D的功能扩展，推动动态场景交互的发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Feature4X的通用框架，旨在将2D视觉基础模型的功能扩大到4D领域，主要通过单目视频输入完成，这种输入形式普遍存在于用户生成的内容中。Feature4X的核心在于其动态优化策略，能够将多个模型功能统一为单一表现。该方法首次使用高斯溅射在4D特征域中提取和提升视频基础模型的特征。实验显示，Feature4X在新颖的视角上实现了随意分割、几何与外观场景编辑，以及跨时间步的自由形式视觉问答，这些都由大语言模型在反馈循环中提供支持。这些技术进步为能够在动态4D场景中进行沉浸式交互的智能应用打下了基础，拓展了智能AI应用的范围。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:56:16 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的故障诱发输入提取研究</title>
<link>https://arxiv.org/abs/2503.20578</link>
<guid>https://arxiv.org/abs/2503.20578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了如何利用大型语言模型从bug报告中提取故障诱发输入。</p><br /><br /><p><strong>摘要：</strong> 故障诱发输入在软件缺陷诊断与分析中至关重要。开发者通常从bug报告中提取这些输入以便于调试。由于bug报告是用自然语言编写的，之前的研究利用了各种自然语言处理技术进行自动化输入提取。随着大型语言模型的出现，本文探讨了生成性大型语言模型在提取故障诱发输入方面的有效性。我们提出了LLPut技术，系统评估了三种开源生成性大型语言模型——LLaMA、Qwen和Qwen-Coder——在从206个bug报告中提取相关输入的表现。实验结果揭示了生成性大型语言模型在自动化缺陷诊断中的能力与局限性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 10:25:01 GMT</pubDate>
</item>
<item>
<title>利用合成视频提升视频生成模型的物理真实性</title>
<link>https://arxiv.org/abs/2503.20822</link>
<guid>https://arxiv.org/abs/2503.20822</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨通过合成视频提升视频生成模型的物理真实性。</p><br /><br /><p><strong>摘要：</strong> 在本研究中，我们探讨了通过利用来自计算机图形管道的合成视频来增强视频生成模型的物理真实性。这些渲染视频遵循现实世界的物理规律，如保持三维一致性，并作为一个宝贵的资源，有潜力改善视频生成模型。为此，我们提出了一种方案，策划和整合合成数据，并引入一种方法将其物理真实性转移到模型中，显著减少不必要的伪影。通过在三个强调物理一致性的典型任务上的实验，我们证明了这一方法在增强物理真实性方面的有效性。尽管我们的模型仍缺乏对物理的深刻理解，但本研究提供了合成视频提高视频合成物理真实性的初步实证证明。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20822" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 20:45:07 GMT</pubDate>
</item>
<item>
<title>RecTable: 高效生成高质量表格数据的新模型</title>
<link>https://arxiv.org/abs/2503.20731</link>
<guid>https://arxiv.org/abs/2503.20731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecTable模型在高质量表格数据生成中表现优异，训练时间更短。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RecTable模型，该模型基于修正流建模，旨在生成高质量的表格数据。相较于传统的GAN和VAE模型，RecTable展现出更优越的性能，同时显著减少了训练时间。RecTable采用简单的架构，使用堆叠的门控线性单元块，并采用混合噪声分布和对数正态时间步分布作为训练策略。实验结果表明，RecTable在多项性能指标上与多种先进扩散模型和基于分数的模型具有竞争力。相关代码已上传至GitHub供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:12:20 GMT</pubDate>
</item>
<item>
<title>Gemma 3：多模态轻量级模型的升级版</title>
<link>https://arxiv.org/abs/2503.19786</link>
<guid>https://arxiv.org/abs/2503.19786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemma 3引入了视觉理解能力，支持多语言和更长上下文。</p><br /><br /><p><strong>摘要：</strong> Gemma 3是Gemma系列的多模态轻量级开源模型，参数范围从1亿到270亿。本版本新增视觉理解能力，支持更广泛的语言覆盖以及至少128K的上下文长度。模型架构进行了调整，以减少在使用长上下文时KV缓存的内存消耗。通过提高本地注意力层与全局注意力层的比例，并缩短本地注意力的跨度，实现了这一目标。Gemma 3模型经过蒸馏训练，其预训练和指令微调版本的性能均优于Gemma 2。特别是，我们的新型后期训练方案显著提升了数学、对话、遵循指令和多语言能力，使得Gemma3-4B-IT与Gemma2-27B-IT竞争，且Gemma3-27B-IT在各项基准测试中可与Gemini-1.5-Pro相媲美。我们将所有模型向社区发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:52:34 GMT</pubDate>
</item>
<item>
<title>模型合并在长期到短期推理中的应用研究</title>
<link>https://arxiv.org/abs/2503.20641</link>
<guid>https://arxiv.org/abs/2503.20641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明模型合并可提高推理效率并减少冗余步骤。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大型语言模型（LLMs）中，从系统1到系统2推理的转变及其带来的复杂任务处理进展。尽管这种转变提高了推理深度，但带来了效率损失，导致模型常常过度思考，冗余推理步骤未能显著提升结果质量。长短推理（L2S）作为解决这一挑战的有效方案，通过模型合并结合了系统1的快速思考能力与系统2的系统性推理。我们通过任务向量、SVD和激活信息的不同合并方法，进行了全面的实验研究，结果显示模型合并能在保持或改善基线性能的同时，平均减少响应长度达55%。研究还发现模型规模与合并效果之间存在明显相关性，并探讨了合并模型的自我批评、自我修正能力以及针对任务复杂度的自适应响应长度。这表明模型合并是一种高效且有效的长短推理方法，能在保持系统2推理的稳健性的同时，解决过度思考的问题。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 11:34:37 GMT</pubDate>
</item>
<item>
<title>基于轨迹平衡与异步的强化学习系统TBA</title>
<link>https://arxiv.org/abs/2503.18929</link>
<guid>https://arxiv.org/abs/2503.18929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TBA是一种高效的RL系统，利用回放缓冲区提升大语言模型的后训练效率。</p><br /><br /><p><strong>摘要：</strong> 强化学习（RL）是大语言模型（LLM）后训练中的关键组成部分，但现有在线算法与经验回放缓冲区不兼容，影响了探索效率。我们提出了一种通过“轨迹平衡与异步(TBA)”的方法，高效利用回放缓冲区的优势，构建了一种可大规模扩展的LLM RL系统。与现有方法相比，TBA将更多计算资源用于搜索，持续生成离线数据供中心回放缓冲区使用。训练节点基于奖励或新颖性从缓冲区采样数据，通过“轨迹平衡”这一多样性寻求目标更新策略。TBA主要有三个优势：训练与搜索解耦，训练效率提升4倍以上；通过大规模离线采样提高多样性；以及支持稀疏奖励环境的可扩展搜索。在数学推理、偏好调整和自动化对抗测试等后训练任务中，TBA在速度和性能上优于传统基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:51:39 GMT</pubDate>
</item>
<item>
<title>UniHDSA: 一种统一的文档层次结构分析方法</title>
<link>https://arxiv.org/abs/2503.15893</link>
<guid>https://arxiv.org/abs/2503.15893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种针对文档层次结构分析的统一关系预测方法UniHDSA。</p><br /><br /><p><strong>摘要：</strong> 文档结构分析对于理解文档的物理布局和逻辑结构至关重要，支持信息检索、文档摘要和知识提取等应用。层次文档结构分析（HDSA）旨在恢复使用分层架构创建的文档的层次结构。本研究提出了一种统一关系预测的方法UniHDSA，将不同的HDSA子任务视作关系预测问题，并将预测标签整合为一个统一的标签空间，从而使单一的关系预测模块能够同时处理多个任务，无论是在页面级还是文档级结构分析中。为了验证UniHDSA的有效性，我们基于Transformer架构开发了一种多模态端到端系统。丰富的实验结果表明，该方法在层次文档结构分析基准Comp-HRDoc上达到了先进的性能，并在大规模文档布局分析数据集DocLayNet上获得了竞争性结果，证明了本方法在所有子任务中的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:44:47 GMT</pubDate>
</item>
<item>
<title>RONA：增强多模态大语言模型图像标注多样性的策略</title>
<link>https://arxiv.org/abs/2503.10997</link>
<guid>https://arxiv.org/abs/2503.10997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RONA策略，以提升多模态模型生成图像标注的多样性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为RONA的新型提示策略，旨在提高多模态大语言模型（MLLM）生成图像标注的多样性和真实性。传统的图像标注往往侧重于视觉描述，而RONA通过利用连贯关系提供了一种新的变化轴，使得生成的标注在语用上更加多样化。实验证明，与多模态语言模型基线相比，RONA在多个领域的图像标注表现出更高的整体多样性和真实对齐度。这一研究为改善图像描述的表达提供了新的视角，并为实际应用提供了代码支持，便于进一步研究和开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 21:45:38 GMT</pubDate>
</item>
<item>
<title>PathoHR：提高乳腺癌生存预测的计算病理新方法</title>
<link>https://arxiv.org/abs/2503.17970</link>
<guid>https://arxiv.org/abs/2503.17970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PathoHR提出了一种新的方法以提高乳腺癌生存预测的准确性。</p><br /><br /><p><strong>摘要：</strong> 乳腺癌的生存预测在计算病理中面临挑战，主要由于肿瘤异质性及病理图像中不同区域的特征差异。本文提出了PathoHR，一个新颖的管道，通过增强病理图像的高分辨率来改善特征学习。该方法包括：使用高分辨率的Vision Transformer（ViT）提升病理切片的细节特征提取，对比多种相似度度量以优化特征表示学习，并展示了经过处理的小切片能与原始大切片相媲美甚至更优的预测准确性，同时大幅降低计算负担。实验结果证明，PathoHR为整合增强图像分辨率和优化特征学习提供了一种可行的方向，助力计算病理的进步，并对乳腺癌生存预测的有效性产生积极影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17970" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:37:24 GMT</pubDate>
</item>
<item>
<title>Vision-Language奖励模型的评估与进展</title>
<link>https://arxiv.org/abs/2503.20271</link>
<guid>https://arxiv.org/abs/2503.20271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探索视觉语言领域中奖励模型的评估，提出了ViLBench基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了过程监督奖励模型（PRM）在视觉语言领域的应用与挑战，尤其是在评估方面的不足。通过对当前视觉大型语言模型（VLLM）进行基准测试，研究发现输出奖励模型（ORM）和过程奖励模型（PRM）在多个视觉语言基准中的表现并不稳定，且更优的VLLM并不一定带来更好的奖励表现。为提升评估能力，本文引入了ViLBench，一个要求强烈过程奖励信号的视觉语言基准，显示了当前VLLM在此基准下挑战性极高，OpenAI的GPT-4o仅达到27.3%的准确率。此外，通过使用增强的树搜索算法收集73.6K视觉语言过程奖励数据，研究表明我们的3B模型在ViLBench上相比于标准的Chain-of-Thought（CoT）平均提高了3.3%。本文的研究成果及实现代码已开放，助力未来的奖励模型研究与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:38:31 GMT</pubDate>
</item>
<item>
<title>利用运动模糊进行稳健的相机运动估计</title>
<link>https://arxiv.org/abs/2503.17358</link>
<guid>https://arxiv.org/abs/2503.17358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过运动模糊进行相机运动估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，将运动模糊视为运动估计的有效信息，而非不必要的伪影。该方法通过单张运动模糊图像直接预测密集运动流场和单目深度图，并在小运动假设下，通过解决线性最小二乘问题来恢复瞬时相机速度。这一方法有效捕捉快速相机运动，产生类似于IMU的测量。为训练模型，构建了一个大规模数据集，并通过全微分管道在真实数据上进行端到端训练。大量的实地评估表明，该方法在角速度和位移估计方面均达到了最先进的水平，表现优于现有方法如MASt3R和COLMAP。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:58:56 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-Omni：多模态流处理模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.20215</link>
<guid>https://arxiv.org/abs/2503.20215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-Omni是一个先进的多模态流处理模型，支持文本、图像、音频和视频。</p><br /><br /><p><strong>摘要：</strong> Qwen2.5-Omni是一个端到端的多模态模型，旨在处理文本、图像、音频和视频等多种输入，同时以流式方式生成文本和自然语音响应。为实现多模态信息的流式处理，音频和视频编码器采用块级处理方法，并通过交错组织音视频序列来同步时间戳，同时提出新的位置嵌入方法TMRoPE（时间对齐的多模态RoPE）。在生成文本和语音的同时，避免两者间的干扰，Qwen2.5-Omni采用了Thinker-Talker架构，其中Thinker作为大型语言模型负责文本生成，而Talker是一个双轨自回归模型，直接利用Thinker的隐表示生成音频标记。此模型在多模态基准测试中尤为突出，表现出与其他模型相媲美的性能，尤其在语音跟随指令和语音生成方面展现出卓越的自然度与稳健性。该模型不仅在Qwen2.5-VL的基础上具备竞争力，还在Omni-Bench等多模态基准上达到了最先进的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:17:55 GMT</pubDate>
</item>
<item>
<title>评估多模态大语言模型的空间推理能力：LEGO-Puzzles基准测试</title>
<link>https://arxiv.org/abs/2503.19990</link>
<guid>https://arxiv.org/abs/2503.19990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过LEGO-Puzzles基准测试评估多模态大语言模型的空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的基准测试LEGO-Puzzles，用于评估多模态大语言模型（MLLMs）在空间理解和顺序推理方面的能力。该基准包含1100个视觉问答样本，涵盖从基本空间理解到复杂多步推理的11个任务。评估结果显示，当前最先进的MLLMs在空间推理方面存在显著局限性，尽管性能最强的模型仅能回答约一半的测试案例，而人类参与者准确率超过90%。此外，针对生成LEGO图像的测试中，仅有Gemini-2.0-Flash和GPT-4o表现出有限的指令跟随能力，其余模型要么重复输入图像，要么产生完全不相关的输出。总体来看，LEGO-Puzzles揭示了现有MLLMs在空间理解和顺序推理方面的关键缺陷，强调了进一步提升多模态空间推理能力的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 14:21:07 GMT</pubDate>
</item>
<item>
<title>MCTS-RAG：提升小型语言模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2503.20757</link>
<guid>https://arxiv.org/abs/2503.20757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCTS-RAG结合检索与推理，提高小型语言模型的知识任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍MCTS-RAG，一种新方法，通过检索增强生成（RAG）提供相关背景，结合蒙特卡洛树搜索（MCTS）优化推理路径，提升小型语言模型在知识密集型任务上的推理能力。MCTS-RAG通过动态集成检索与推理，采用迭代决策过程，克服了标准RAG方法与传统MCTS推理的局限性，实现更加结构化的推理和适应性检索。该方法显著提高了决策质量，减少了幻觉现象，并增强了事实准确性和响应一致性。实验结果显示，在多个推理与知识密集型数据集上，MCTS-RAG使得小型语言模型的表现达到了与前沿大型语言模型（如GPT-4o）相媲美的新标准，展示了其在推理领域的重要进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:46:08 GMT</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:45:29 GMT</pubDate>
</item>
<item>
<title>突破性商业内容生成：以超密布局为基础的文本到图像生成模型</title>
<link>https://arxiv.org/abs/2503.20672</link>
<guid>https://arxiv.org/abs/2503.20672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了基于文章级提示生成高质量商业内容的挑战与解决方案。</p><br /><br /><p><strong>摘要：</strong> 近期，最新的文本到图像生成模型如Flux和Ideogram 2.0在句子级别的视觉文本渲染上取得显著进展。本文关注于更具挑战性的文章级别视觉文本渲染，提出一种新任务，以用户提供的文章级描述性提示和超密布局为基础生成高质量商业内容，包括信息图和幻灯片。文章面临的主要挑战包括更长的上下文长度和高质量商业内容数据的稀缺。与之前的研究相比，本文提出了构建高质量商业内容数据集Infographics-650K，并采用布局引导的跨注意力机制，以适应超密布局的需求。通过在BizEval提示集上的实验，显示了本系统相比之前的SOTA系统（如Flux和SD3）的强大结果，结合消融实验验证了各个组件的有效性。我们希望构建的Infographics-650K和BizEval能推动商业内容生成领域的进一步发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 12:04:57 GMT</pubDate>
</item>
<item>
<title>Wan：一套开源视频基础模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.20314</link>
<guid>https://arxiv.org/abs/2503.20314</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Wan是一个开源的视频基础模型套件，推动视频生成技术的发展。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Wan，这是一个全面的开源视频基础模型套件，旨在推动视频生成的边界。Wan基于主流的扩散变换器范式，通过创新的VAE、可扩展的预训练策略、大规模数据整理和自动评估指标，显著提升了生成能力。Wan拥有14B参数模型和1.3B参数模型，分别适用于效率和效果，涵盖了多个下游应用，包括图像到视频、指导性视频编辑和个人视频生成。Wan的1.3B模型在资源效率方面表现卓越，仅需8.19 GB VRAM，兼容多种消费级GPU。此外，Wan所有代码和模型均为开源，旨在推动视频生成社区的发展，拓宽行业中的创作可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20314" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:25:43 GMT</pubDate>
</item>
<item>
<title>优化条件扩散模型的无条件噪声预测</title>
<link>https://arxiv.org/abs/2503.20240</link>
<guid>https://arxiv.org/abs/2503.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出改进条件扩散模型的方法，通过替换无条件噪声预测提升生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种优化条件扩散模型（CFG）的新方法，主要针对无条件噪声的预测。传统上，CFG通过单一网络同时学习条件和无条件的噪声预测，但联结训练导致无条件噪声预测的质量不佳，从而影响条件生成的质量。文章的灵感源于大多数CFG条件模型通过对基础模型进行微调以提高无条件生成的性能。我们提出，通过用基础模型预测的无条件噪声替换CFG中的无条件噪声，能显著改善条件生成的效果。此外，我们的研究表明，使用与微调模型不同的扩散模型也能有效替代无条件噪声。经过对多个CFG条件模型（包括Zero-1-to-3、Versatile Diffusion、DiT、DynamiCrafter和InstructPix2Pix）的实验验证，我们的结果支持了这一方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 01:11:38 GMT</pubDate>
</item>
<item>
<title>DINeMo：无3D注释的神经网格模型与伪对应生成</title>
<link>https://arxiv.org/abs/2503.20220</link>
<guid>https://arxiv.org/abs/2503.20220</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINeMo是一种新型无3D注释的神经网格模型，用于3D姿态估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了DINeMo，一个训练时不依赖3D注释的神经网格模型，利用大型视觉基础模型获取的伪对应进行学习。通过双向伪对应生成方法，DINeMo结合局部外观特征和全局上下文信息，展现出在汽车数据集上的优越表现，超越了以往的零样本和少样本3D姿态估计，缩小了与完全监督方法之间的差距达67.3%。此外，DINeMo在训练时有效利用更多未标记图像，展现出相较于依赖3D注释的监督学习方法的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20220" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:23:53 GMT</pubDate>
</item>
<item>
<title>开放深度搜索（ODS）：提升开源搜索AI性能的新框架</title>
<link>https://arxiv.org/abs/2503.20201</link>
<guid>https://arxiv.org/abs/2503.20201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ODS通过结合搜索工具与推理代理，提升开源LLM的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了开放深度搜索（ODS），旨在缩小专有搜索AI解决方案与开源对手之间的差距。ODS的创新点在于通过推理代理增强开源大规模语言模型（LLM）的推理能力，帮助有效利用网页搜索工具回答查询。ODS由用户选择的基础LLM及两个组件组成：开放搜索工具和开放推理代理。开放推理代理解析任务并执行一系列动作，包括调用开放搜索工具。这一新型搜索工具在准确性上优于其专有竞争对手。与强大的开源推理LLM（如DeepSeek-R1）结合，ODS在SimpleQA和FRAMES两个基准测试中表现接近或超越现有的最先进基线。在FRAMES基准测试中，ODS的准确率比最新发布的GPT-4o搜索预览提升了9.7%。ODS为任何LLM无缝增强搜索与推理能力，从而达到前沿性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:51:32 GMT</pubDate>
</item>
<item>
<title>Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</title>
<link>https://arxiv.org/abs/2503.20198</link>
<guid>https://arxiv.org/abs/2503.20198</guid>
<content:encoded><![CDATA[
Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:44:25 GMT</pubDate>
</item>
<item>
<title>Gemini Robotics：革命性的机器人视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2503.20020</link>
<guid>https://arxiv.org/abs/2503.20020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Robotics是一个新型机器人控制模型，具备强大的操控和适应能力。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Gemini Robotics，一个基于Gemini 2.0构建的先进视觉-语言-动作（VLA）通用模型，旨在直接控制机器人，执行复杂的操作任务。该模型能平滑反应，并能够适应不同类型和位置的物体，处理未知环境，并遵循多样的开放词汇指令。通过额外的微调，Gemini Robotics能够专门化为新的能力，如解决长时间的高灵巧任务、从少于100次的演示中学习新任务，并适应全新的机器人形态。同时，Gemini Robotics-ER（具身推理）扩展了Gemini在物理世界中的多模态推理能力，具有增强的空间和时间理解，支持对象检测、轨迹预测、抓取预测等机器人相关能力。我们探讨了这一新类基础模型在机器人应用中的潜力和重要的安全考量，标志着朝着开发通用机器人迈出了重要一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 15:02:56 GMT</pubDate>
</item>
<item>
<title>无监督视频中运动估计的新方法</title>
<link>https://arxiv.org/abs/2503.19953</link>
<guid>https://arxiv.org/abs/2503.19953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Opt-CWM，利用无监督学习实现高效的运动估计。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新颖的自监督技术Opt-CWM，用于从预训练的下一帧预测模型中进行光流和遮挡估计。现有方法主要依赖于合成数据或特定场景的启发式调优，导致它们在真实世界中的表现有限。Opt-CWM通过优化反事实探针，从基础视频模型中提取运动信息，避免了固定启发式的需求，并允许在不受限制的视频输入上训练。研究结果表明，Opt-CWM在真实视频运动估计中展示了最先进的性能，同时无需标记数据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:52 GMT</pubDate>
</item>
<item>
<title>利用Attention-IoU度量揭示计算机视觉模型中的偏见</title>
<link>https://arxiv.org/abs/2503.19846</link>
<guid>https://arxiv.org/abs/2503.19846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Attention-IoU度量，用于揭示计算机视觉模型内部的偏见。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨计算机视觉模型在多种数据集和任务中表现出的偏见，并介绍了Attention-IoU（Attention Intersection over Union）度量。该度量通过使用注意力图揭示模型内部表示中的偏见，帮助识别可能导致偏见的图像特征。首先，在合成的Waterbirds数据集上验证了该度量的有效性，结果显示Attention-IoU可以准确衡量模型偏见。接着，在CelebA数据集中进行分析时，发现Attention-IoU揭示了超过准确率差异的关联性。通过检查受保护属性“男性”的个别特征，研究了CelebA中偏见的不同表现方式。最后，通过对训练集进行子采样以改变属性相关性，展示了Attention-IoU能够揭示数据集标签中不存在的潜在混淆变量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:11:39 GMT</pubDate>
</item>
<item>
<title>LogQuant：高效的2位量化技术提升大语言模型推理性能</title>
<link>https://arxiv.org/abs/2503.19950</link>
<guid>https://arxiv.org/abs/2503.19950</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LogQuant采用新型量化技术，实现更高效的KV缓存压缩与性能提升。</p><br /><br /><p><strong>摘要：</strong> LogQuant是一种创新的2位量化技术，专为大语言模型（LLM）推理中的KV缓存设计，显著节省内存并保持卓越性能。与以往方法假设后续tokens更重要或基于早期注意力模式预测重要tokens不同，LogQuant通过应用基于对数的过滤机制，选择性地压缩整个上下文中的KV缓存，使得性能在相同或更小的内存占用下得到改善。基准测试显示，LogQuant在吞吐量上提升25%，批大小增加60%，同时保持内存消耗不变。在处理数学和代码补全等复杂任务时，LogQuant的准确性提升达40%至200%。该技术能够与大多数流行的推理框架无缝集成，相关实现可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19950" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:24:45 GMT</pubDate>
</item>
<item>
<title>Dita：一种可扩展的多模态扩散框架用于机器人行动决策</title>
<link>https://arxiv.org/abs/2503.19757</link>
<guid>https://arxiv.org/abs/2503.19757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dita框架通过多模态扩散过程优化机器人行动决策以适应环境变化。</p><br /><br /><p><strong>摘要：</strong> Dita是一种可扩展的框架，通过Transformer架构直接对连续行动序列进行去噪，克服了以往依赖紧凑动作头的限制。与先前的方法不同，Dita通过上下文条件实现更精细的去噪效果，明确建模动作变动和环境细节。该框架能够在多种相机视角、观察场景、任务及行动空间中集成跨体现的数据集，从而增强其对不同变异的鲁棒性并提升长期任务执行的成功率。在广泛基准测试中的评估显示，Dita在模拟中实现了最先进或可比的性能，并且能够通过10-shot微调在真实世界中成功适应环境变化及复杂任务，仅使用第三人称相机输入。该架构为通用机器人策略学习建立了一个轻量级且开放源代码的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:19:56 GMT</pubDate>
</item>
<item>
<title>GenHancer：提升CLIP表征能力的生成模型探索</title>
<link>https://arxiv.org/abs/2503.19480</link>
<guid>https://arxiv.org/abs/2503.19480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出GenHancer模型，显著提升CLIP的视觉表征能力。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型与判别模型间的协同作用备受关注，研究发现视觉完美的生成并不总是最优选择。本研究提出GenHancer模型，通过有效提取生成模型的细粒度知识并减轻无关信息，提升CLIP的表示能力。我们深入探讨了三个关键因素：1) 条件机制：使用全球视觉tokens作为条件比局部tokens更有效；2) 去噪配置：采用两阶段训练方法优先学习有用视觉知识，而轻量化去噪器显著提高性能；3) 生成范式：研究连续和离散去噪器的效果，验证了方法的多样性。GenHancer在MMVP-VLM基准上表现优异，相较于OpenAICLIP提高了6.0%。所有模型与代码已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19480" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:15:34 GMT</pubDate>
</item>
<item>
<title>AccVideo: 高效视频扩散模型加速方法</title>
<link>https://arxiv.org/abs/2503.19462</link>
<guid>https://arxiv.org/abs/2503.19462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出AccVideo，加速视频扩散模型生成，提高效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文分析了现有扩散蒸馏方法的挑战，并提出了一种名为AccVideo的新方法，旨在通过合成数据集减少视频扩散模型的推理步骤，提高生成效率。我们利用预训练的视频扩散模型生成多个有效的去噪轨迹作为合成数据集，从而消除在蒸馏过程中无用的数据点。通过设计基于轨迹的少步引导方法，我们从去噪轨迹中提取关键数据点，学习噪声到视频的映射，从而在更少的步骤内实现视频生成。此外，我们引入对抗训练策略，使学生模型的输出分布与合成数据集的分布对齐，从而增强视频质量。实验结果表明，与教师模型相比，我们的方法在生成速度上提高了8.5倍，同时保持了相似的性能，并且生成了更高质量和分辨率的视频，具体表现为5秒、720x1280、24fps。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 04:52:07 GMT</pubDate>
</item>
<item>
<title>Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs</title>
<link>https://arxiv.org/abs/2503.16870</link>
<guid>https://arxiv.org/abs/2503.16870</guid>
<content:encoded><![CDATA[
Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (&lt;10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:58:18 GMT</pubDate>
</item>
<item>
<title>高效的微调转移策略：提升预训练模型的性能</title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨微调更新在新模型版本之间的转移，提高了模型性能与效率。</p><br /><br /><p><strong>摘要：</strong> 现代大型语言模型在高效更新方面面临挑战，尤其是在每次发布新预训练模型时需要重复昂贵的对齐过程。本文探讨了在模型版本之间转移微调更新的方法，通过从源模型版本导出差异向量并将其应用于不同目标版本的基础模型上。实验证明，转移差异向量可以显著提升目标模型的表现，例如，重用Llama 3.0 8B的微调更新，使得Llama 3.1 8B在GPQA上实现了10.7%的绝对准确率提升。此外，在多语言模型开发中，本文的方法在不重新训练的情况下，也能显著提高特定语言任务的性能。本研究表明，相互连接的模型在参数空间中微调转移最为有效，同时为后续微调提供了更强大的起始点。最后，我们提出了一种迭代的回收-再微调方法，旨在提高模型的开发效率与效能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20110" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 19:24:43 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在复杂行为识别中的应用与提升</title>
<link>https://arxiv.org/abs/2503.18712</link>
<guid>https://arxiv.org/abs/2503.18712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨多模态大语言模型在行为识别中的应用和改进。</p><br /><br /><p><strong>摘要：</strong> 本研究着重评估并改进多模态大语言模型（MLLMs）在行为识别任务中的表现。我们将EPIC-KITCHENS-100数据集重构为视频多重问答（EPIC-KITCHENS-100-MQA）形式，发现当面对困难的错误答案作为干扰项时，现有的MLLMs在正确行为识别上存在挑战。为此，我们提出了一系列方法，显著提高了MLLMs的行为识别能力，不仅在EPIC-KITCHENS-100验证集上达到了最新的最佳效果，还在EPIC-KITCHENS-100-MQA上将准确率超越GPT-4o 21个百分点。此外，我们在EgoSchema、PerceptionTest、LongVideoBench、VideoMME和MVBench等其他行为相关视频基准测试中也取得了显著进展，表明MLLMs为复杂行为任务提供了有前景的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 10:24:17 GMT</pubDate>
</item>
<item>
<title>Any6D：无模型框架实现6D物体姿态估计</title>
<link>https://arxiv.org/abs/2503.18673</link>
<guid>https://arxiv.org/abs/2503.18673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Any6D 是一种通过单张RGB-D图像实现6D物体姿态估计的无模型框架。</p><br /><br /><p><strong>摘要：</strong> Any6D是一个无模型的6D物体姿态估计框架，只需一张RGB-D锚点图像即可估计未知物体在新场景中的6D姿态和尺寸。与依赖纹理3D模型或多个视点的现有方法不同，Any6D通过联合物体对齐过程来增强2D-3D对齐和度量尺度估计，从而提升姿态准确性。该方法结合了渲染与比较策略，用于生成和细化姿态假设，使其在遮挡、视角不重叠、不同光照条件和环境变异等复杂情境下表现出色。在REAL275、Toyota-Light、HO3D、YCB-INEOAT和LM-O五个具有挑战性的数据库上进行评估，结果表明其在新物体姿态估计方面显著超越了现有的最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>高质量360度人头视图生成的新方法</title>
<link>https://arxiv.org/abs/2503.15667</link>
<guid>https://arxiv.org/abs/2503.15667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出一种高质量的360度人头视图生成方法，具备一致性和精细细节。</p><br /><br /><p><strong>摘要：</strong> 为了实现可访问的沉浸式远程呈现和个性化内容创作，生成高质量的360度人头视图至关重要。目前的最先进方法在生成真实人头方面存在局限，尤其在风格无关的头部合成Diffusion方法中仅能生成正面视图，且常出现视角一致性问题。我们提出了一种新方法，能够生成完全一致的360度人头视图，适用于人类、风格化和拟人化形状，包含眼镜、帽子等配饰。该方法基于DiffPortrait3D框架，结合自定义ControlNet以生成后脑部细节，并采用双重外观模块确保前后一致性。通过对连续视角序列进行训练并整合背面参考图像，我们的方法实现了强健、局部连续的视图合成，能够生成高质量的神经辐射场(NeRFs)以用于实时自由视点渲染，在对象合成及挑战性输入肖像的360度头部生成中超过了现有的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 15:47:04 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的3D场景生成研究</title>
<link>https://arxiv.org/abs/2503.04919</link>
<guid>https://arxiv.org/abs/2503.04919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了FirePlace框架，提升3D场景中物体放置的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在物体放置任务中如何有效利用多模态大语言模型（MLLMs），并提出了一个新颖的框架FirePlace。该框架在3D几何推理和几何细节提取方面应用现有的MLLMs，构建和解决低级几何的约束，同时进行合理的物体放置筛选。通过结合几何推理和MLLM的现实世界理解，我们的方法能够提出既符合几何约束又符合高层语义常识的物体放置建议。实验结果表明，该方法在处理复杂几何场景时，相较于之前的研究，能够更有效地进行物体放置。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 14:34:15 GMT</pubDate>
</item>
<item>
<title>提升时空推理能力的视觉语言模型ST-VLM及其基准评测</title>
<link>https://arxiv.org/abs/2503.19355</link>
<guid>https://arxiv.org/abs/2503.19355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ST-VLM是一种增强时空推理能力的视觉语言模型，展示出优秀的性能。</p><br /><br /><p><strong>摘要：</strong> 时空推理在自动驾驶和体育分析等领域中至关重要。尽管视觉语言模型（VLMs）的空间推理能力因大规模数据的引入有所提升，但在分析运动对象的运动学元素（如行驶距离和速度）方面仍显不足。为了解决这个问题，本文构建了一个包含运动学指令调优的数据集和基准，分别命名为STKit和STKit-Bench，包含详细的真实世界视频及3D注释，涵盖对象运动动态。为扩大数据构建范围至没有3D标签的视频，提出了一种利用4D重建生成伪标签的自动化流程。基于此运动学指令调优数据，推出了增强时空推理的视觉语言模型ST-VLM。ST-VLM在STKit-Bench上的表现优异，并且在不同领域和任务中具有出色的泛化能力，超越了其他时空基准的对比模型。通过将学习的时空推理能力与现有能力结合，ST-VLM能够进行复杂的多步推理。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:08:06 GMT</pubDate>
</item>
<item>
<title>OpenCity3D：城市规模环境的语言驱动分析新范式</title>
<link>https://arxiv.org/abs/2503.16776</link>
<guid>https://arxiv.org/abs/2503.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出OpenCity3D，扩展了VLMs在城市环境中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法OpenCity3D，旨在将视觉语言模型（VLMs）应用于城市规模环境，超越传统的室内和自动驾驶场景分析。通过利用多视角航拍图像的3D重建，OpenCity3D能够执行一系列高层次任务，如人口密度估计、建筑年代分类、房地产价格预测、犯罪率评估和噪音污染评估。研究结果表明，OpenCity3D具有出色的零样本和少样本学习能力，能够适应新的城市分析场景。此研究为语言驱动的城市分析确立了新范式，具有在城市规划、政策制定和环境监测等领域的广泛应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 21:11:21 GMT</pubDate>
</item>
<item>
<title>基于单目相机的无人机深度和语义地图预测</title>
<link>https://arxiv.org/abs/2503.17982</link>
<guid>https://arxiv.org/abs/2503.17982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于单目相机的无人机深度和语义地图联合预测方法。</p><br /><br /><p><strong>摘要：</strong> 在无人机导航中，理解场景的几何和语义属性至关重要，但实现起来十分具有挑战性。本文利用单目相机在低空非结构化环境中预测深度和语义地图，提出了一种联合深度学习架构，能够快速且准确地完成这两项任务。通过在MidAir和Aeroscapes基准数据集上的验证，证明该架构在性能上优于其他单独或联合架构方法，实时预测速率达到每秒20.2帧，并且内存占用较低。相关的训练与预测代码已在GitHub上公开，以供研究与使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 04:25:07 GMT</pubDate>
</item>
<item>
<title>PhysTwin: 基于稀疏视频的动态物体物理数字双胞胎框架</title>
<link>https://arxiv.org/abs/2503.17973</link>
<guid>https://arxiv.org/abs/2503.17973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了PhysTwin框架，用于实时创建动态物体的物理数字双胞胎。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysTwin，一个新颖的框架，利用互动下的稀疏视频生成真实且物理上逼真的动态物体的虚拟复制品。该框架主要由两个关键组件构成：一是融合了弹簧-质量模型进行物理仿真的物理信息化表示、用于几何体的生成形状模型以及用于渲染的高斯点云；二是一个基于多阶段优化的逆建模框架，能够从视频重建完整几何体、推断密集物理属性并复制真实外观。PhysTwin通过整合逆物理框架与视觉感知线索，即使在部分、遮挡和有限视角下也能实现高保真重建。该方法支持建模各种可变形物体，包括绳子、毛绒玩具、布料和快递包裹。实验表明，PhysTwin在重建、渲染、未来预测和新颖互动下的仿真方面超越了竞争方法，并进一步展示了其在实时交互仿真和基于模型的机器人运动规划中的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:49:19 GMT</pubDate>
</item>
<item>
<title>FullDiT：统一的视频生成基础模型</title>
<link>https://arxiv.org/abs/2503.19907</link>
<guid>https://arxiv.org/abs/2503.19907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FullDiT通过全注意力机制优化多任务视频生成，达到最佳效果。</p><br /><br /><p><strong>摘要：</strong> 当前的视频生成基础模型主要集中于文本到视频的任务，提供的细粒度内容创建控制有限。虽然适配器化的方法能提供额外控制，但在整合多个条件时面临挑战。为了解决这些问题，本文提出FullDiT——一个统一的视频生成基础模型，通过全注意力机制无缝集成多个条件。FullDiT将多任务条件融合为统一的序列表示，并利用全自注意力的长程学习能力捕捉条件动态，从而减少参数冗余，避免条件冲突，提升可扩展性和生成能力。我们还引入了FullBench用于多任务视频生成评估。实验结果表明，FullDiT在复杂的多任务视频生成中表现出色，展现了全注意力机制的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>无训练开放词汇语义分割的方法LPOSS+</title>
<link>https://arxiv.org/abs/2503.19777</link>
<guid>https://arxiv.org/abs/2503.19777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于标签传播的无训练语义分割方法，显著提升分割精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无训练的开放词汇语义分割方法LPOSS+，利用视觉语言模型（VLMs）增强初始的每个补丁预测。我们通过标签传播优化预测，结合补丁之间的关系，以改进分割精度。由于VLMs主要针对跨模态对齐而非模态内相似性，我们引入了一种视觉模型（VM），它更好地捕捉这些关系。此外，针对补丁编码器的分辨率限制，我们在像素级别应用标签传播作为精细化步骤，尤其在类别边界附近显著提升精度。LPOSS+以整个图像为基础进行推断，避免了基于窗口的处理，有效捕捉全图的上下文交互，展现出在多种数据集上的最佳性能，成为当前无训练方法中的领先者。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:47:13 GMT</pubDate>
</item>
<item>
<title>实时交互视频数据集：评估AI模型的对话能力</title>
<link>https://arxiv.org/abs/2503.19356</link>
<guid>https://arxiv.org/abs/2503.19356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一个新数据集，评估AI模型在实时对话方面的能力。</p><br /><br /><p><strong>摘要：</strong> 随着AI技术的进步，AI模型在描述和回答现实图像问题方面取得了显著进展。本文提出了新的数据集和基准——Qualcomm交互视频数据集（IVD），旨在评估现有模型在实时对话及场景理解中的表现。该数据集采用简单的问答设置，用户可以根据相机和音频输入进行实时提问。研究发现，现有模型在这一任务上的表现远远落后于人类，同时指出了造成性能差距的主要来源。然而，实验表明，对于许多必要的知觉技能，通过对这种数据的微调，可以显著缩小这一差距，从而提升AI的实时交互能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:13:12 GMT</pubDate>
</item>
<item>
<title>基于少量图像的个性化3D人类头像重建与动画技术</title>
<link>https://arxiv.org/abs/2503.19207</link>
<guid>https://arxiv.org/abs/2503.19207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，仅需少量图像即可重建个性化3D人类头像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，能够仅通过少量图像重建个性化的3D人类头像并实现逼真的动画。传统方法需要对每个用户进行数小时的优化，而我们通过学习来自1000多名着衣人类的通用先验，实现了即时前馈生成和零-shot泛化。我们不再使用共享的皮肤权重进行绑定，而是联合推断个性化的头像形状、皮肤权重和姿态依赖变形，提高了几何真实性并减少了变形伪影。此外，为了解决姿态变化带来的耦合模糊问题，设计了一种3D标准化处理，生成像素对齐的初始条件，帮助重建细致的几何细节。本方法经过大规模捕捉数据集的端到端训练，具有优秀的重建和动画效果，并能直接处理日常手机拍摄的输入。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 19:20:47 GMT</pubDate>
</item>
<item>
<title>VocAgnoLM：解决教师与学生模型词汇不匹配的问题</title>
<link>https://arxiv.org/abs/2503.19123</link>
<guid>https://arxiv.org/abs/2503.19123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VocAgnoLM通过词汇无关的教学指导语言建模解决了词汇不匹配问题。</p><br /><br /><p><strong>摘要：</strong> VocAgnoLM是一种新颖的方法，通过两种关键技术解决教师与学生语言模型之间的词汇不匹配问题，从而提高学习效率和效果。这两种技术包括：1）令牌级词汇对齐，能够对齐不同词汇中的令牌序列；2）教师指导损失，利用教师模型的损失来指导学生模型的有效训练。我们在使用不同词汇的7B教师模型对1B学生模型进行语言建模时，验证了该方法的有效性。例如，当使用Qwen2.5-Math-Instruct作为教师模型时，与TinyLlama共享约6%词汇的情况下，VocAgnoLM与简单的持续预训练相比实现了46%的性能提升。此外，VocAgnoLM在使用更强的教师模型时能够持续受益，提供了一个强健的解决方案以应对语言建模中的词汇不匹配问题。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 16:19:31 GMT</pubDate>
</item>
<item>
<title>WikiAutoGen：一款自动化多模态维基百科式文章生成系统</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍WikiAutoGen，一个提升文章生成质量的多模态系统。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WikiAutoGen，这是一种新颖的自动化多模态维基百科式文章生成系统，与以往的文本生成方法不同，WikiAutoGen能够检索并整合相关图像，增强生成内容的深度与视觉吸引力。此外，研究还提出了一种多视角自我反思机制，从不同观点对检索内容进行评估，以提升生成文章的可靠性与连贯性。为了评估多模态知识生成，本研究构建了一个名为WikiSeek的基准数据集，该数据集包含配有文本和图像表示的维基百科文章。实验结果表明，WikiAutoGen在WikiSeek基准上比以往方法提升了8%-29%，生成的维基百科式文章在准确性、连贯性和视觉丰富性方面表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:51:55 GMT</pubDate>
</item>
<item>
<title>xKV：提升长上下文语言模型内存效率的新方法</title>
<link>https://arxiv.org/abs/2503.18893</link>
<guid>https://arxiv.org/abs/2503.18893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">xKV方法通过奇异值分解显著压缩长上下文模型的KV-Cache。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在处理长上下文时面临高内存消耗的问题，尤其是存储键值状态（KV-Cache）。虽然现有研究尝试将多个层的KV-Cache合并，但通常需要昂贵的预训练或不切实际的相似性假设。本文提出了xKV，一种利用奇异值分解（SVD）在分组层的KV-Cache上进行简单后训练的方法，能够将多个层的KV-Cache整合到共享低秩子空间，从而显著减少内存占用。通过在广泛使用的LLMs（如Llama-3.1和Qwen2.5）上进行的大量评估，xKV展示了相比先进的跨层技术高达6.8倍的压缩率，同时提高了2.7%的准确性。此外，xKV还兼容新兴的多头潜在注意力机制，能够在编码任务上实现3倍的压缩而不损失性能。这些结果表明，xKV在解决长上下文LLM推理中的内存瓶颈方面具有强大的能力和灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:37 GMT</pubDate>
</item>
<item>
<title>Towards a Unified Copernicus Foundation Model for Earth Vision</title>
<link>https://arxiv.org/abs/2503.11849</link>
<guid>https://arxiv.org/abs/2503.11849</guid>
<content:encoded><![CDATA[
Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 16:16:48 GMT</pubDate>
</item>
<item>
<title>基于YOLOv12与BoT-SORT的多无人机跟踪方法</title>
<link>https://arxiv.org/abs/2503.17237</link>
<guid>https://arxiv.org/abs/2503.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于YOLOv12与BoT-SORT的多无人机跟踪方法。</p><br /><br /><p><strong>摘要：</strong> 本论文针对热红外视频中多无人机（UAVs）的检测与跟踪问题，提出了一种新的跟踪框架，构建于YOLOv12与BoT-SORT之上，结合了定制的训练与推理策略。与传统的YOLOv5和DeepSORT流水线不同，我们的方法在没有采用对比度增强或时间信息融合的情况下，仍能展现出竞争力的性能，为多无人机跟踪任务提供了一个强基线。我们根据第四届反无人机挑战赛的指标评估了这一方法，并提供了实现细节、深入的实验分析以及可能的改进讨论。相关代码已发布在GitHub上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 11:40:18 GMT</pubDate>
</item>
<item>
<title>Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation</title>
<link>https://arxiv.org/abs/2503.14905</link>
<guid>https://arxiv.org/abs/2503.14905</guid>
<content:encoded><![CDATA[
With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 01:14:44 GMT</pubDate>
</item>
<item>
<title>MDocAgent：一种多模态多智能体文档理解框架</title>
<link>https://arxiv.org/abs/2503.13964</link>
<guid>https://arxiv.org/abs/2503.13964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MDocAgent通过多智能体协作提升文档问答系统的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的多模态多智能体框架MDocAgent，旨在增强文档理解中的问答能力。与传统的方法不同，MDocAgent结合了文本和图像信息，通过五个专门的智能体共同进行多模态上下文检索。每个智能体专注于不同的领域：一般智能体、关键智能体、文本智能体、图像智能体和总结智能体。这样的协作模式使系统能够全面综合文档内容，提高了问答的准确性。初步实验显示，MDocAgent在五个基准上实现了平均12.1%的性能提升，展示了其在处理复杂真实世界文档方面的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 02:57:21 GMT</pubDate>
</item>
<item>
<title>CoLLM：增强复杂图像检索的综合框架</title>
<link>https://arxiv.org/abs/2503.19910</link>
<guid>https://arxiv.org/abs/2503.19910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLLM是一种新框架，解决复杂图像检索中的数据稀缺和多模态学习问题。</p><br /><br /><p><strong>摘要：</strong> Composed Image Retrieval (CIR)是一项多模态查询图像检索的复杂任务，传统的训练数据获取困难，导致使用零样本方法和图像-文本对的不足之处。为了解决这些问题，本研究提出了CoLLM框架，通过实时生成图像-文本三元组以实现无人工标注的监督训练，利用大语言模型生成参考图像和修改文本的联合嵌入，促进更深入的多模态融合。此外，引入了包含340万样本的大规模数据集Multi-Text CIR (MTCIR)，并优化了现有CIR基准（CIRR和Fashion-IQ），提高了评估的可靠性。实验结果显示，CoLLM在多个CIR基准和设置中达到了最先进的性能，MTCIR的表现也有所提升，提供了更可靠的评估指标，推动了该领域的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>PS3：革命性的高分辨率视觉预训练方法</title>
<link>https://arxiv.org/abs/2503.19903</link>
<guid>https://arxiv.org/abs/2503.19903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PS3提出了一种高效的高分辨率视觉预训练方法，显著提升视觉感知能力。</p><br /><br /><p><strong>摘要：</strong> 高分辨率的视觉细节感知对日常任务至关重要，但现有的视觉预训练方法仍然局限于低分辨率。我们提出了PS3，能够将CLIP风格的视觉预训练扩展至4K分辨率而计算成本几乎保持不变。PS3通过局部区域处理和与局部详细描述的对比学习实现高分辨率表示。应用于多模态大语言模型的VILA-HD在高分辨率视觉感知方面显著优于未经过高分辨率视觉预训练的基线模型，如AnyRes和S^2，并且使用的令牌数量减少了最多4.3倍。此外，VILA-HD在多个基准测试中超越了以往的多模态模型，展现出更好的效率。最终，我们提出了适用于4K分辨率的新的图像问答基准4KPro，VILA-HD在该基准上实现了对所有前一代多模态模型的超越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:37 GMT</pubDate>
</item>
<item>
<title>Multi-round Thinking：一种提升大语言模型推理性能的新方法</title>
<link>https://arxiv.org/abs/2503.19855</link>
<guid>https://arxiv.org/abs/2503.19855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多轮思维方法，通过迭代提升大语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）的发展，推理过程的扩展显著提升了模型性能，但当前模型在处理长文本和强化学习效率上仍存在局限。本文提出了一种简单有效的测试时间扩展方法——多轮思维。该方法通过利用前一轮的答案作为提示，迭代精炼模型推理。经过在多个模型（如QwQ-32B和DeepSeek-R1）上的广泛实验，结果显示在多个基准测试（如AIME 2024、MATH-500等）上均实现性能提升。例如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%，DeepSeek-R1也从79.7%提升至82.0%。多轮思维展示了其作为一种广泛适用且简单的方法，能够稳定地提升模型性能，预示着其在测试时间扩展技术未来发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:19:38 GMT</pubDate>
</item>
<item>
<title>研究视频模态大规模多模态模型的幻觉问题</title>
<link>https://arxiv.org/abs/2503.19622</link>
<guid>https://arxiv.org/abs/2503.19622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究大规模多模态模型在视频理解中的幻觉问题并提出解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模多模态模型（LMMs）在视频模态下的幻觉问题，这类幻觉使得模型在提供看似正确的回答时，实际上往往不准确。为此，本文提出了一个全面的基准测试工具HAVEN，用于评估LMMs在视频理解任务中的幻觉现象，框架包括幻觉原因、幻觉方面和问题格式，共设计了6000个问题。实验分析了视频持续时间、模型大小和推理能力等7个影响因素对于幻觉的影响。为了减轻幻觉问题，研究还提出了一种视频思维模型，结合监督推理微调（SRFT）和直接偏好优化（TDPO），前者提升推理能力，后者减少推理过程中的幻觉。实验结果表明，该方法在幻觉评估的准确度上提升了7.65%，且偏差分数降低了4.5%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 09:12:17 GMT</pubDate>
</item>
<item>
<title>ReSearch：通过强化学习整合推理与搜索的框架</title>
<link>https://arxiv.org/abs/2503.19470</link>
<guid>https://arxiv.org/abs/2503.19470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReSearch框架通过强化学习将推理与搜索整合，提升LLM的推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理能力上展现了卓越的性能，然而将推理与外部搜索过程整合仍然是一个挑战，尤其是对于需要多步检索的复杂问题。为此，我们提出了ReSearch框架，通过强化学习训练LLMs，以无监督的数据进行推理步骤的整合。此方法将搜索操作视为推理链的重要组成部分，依据文本思维指导搜索的时机及方式，并利用搜索结果进一步推动推理过程。我们在Qwen2.5-7B和Qwen2.5-32B模型上进行训练和广泛实验，尽管只使用一个数据集进行训练，我们的模型在多个基准测试中表现出强泛化能力。分析表明，ReSearch在强化学习过程中自然而然地引发了反思和自我修正等高级推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:00:58 GMT</pubDate>
</item>
<item>
<title>流模型的推理时刻扩展方法研究</title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种用于流模型的高效推理时刻扩展方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对预训练流模型的推理时刻扩展方法，旨在提高样本质量并更好地符合用户偏好。以往的推理时刻扩展多用于大规模语言模型及扩散模型，但流模型的确定性生成过程使得现有方法难以直接应用。为此，本文提出了三个关键思想：1) 基于随机微分方程(SDE)的生成，支持流模型中的粒子采样；2) 插值转换，扩展搜索空间提高样本多样性；3) Rollover Budget Forcing (RBF)，在不同时间步中自适应分配计算资源以最大化预算效率。实验结果表明，基于方差保持的SDE生成显著提升粒子采样方法在流模型中的推理时刻扩展表现，而结合VP-SDE和RBF的方法在所有推理时刻扩展方案中表现最佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19385" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 02:30:45 GMT</pubDate>
</item>
<item>
<title>长时间上下文视频生成的进展与Frame AutoRegressive模型</title>
<link>https://arxiv.org/abs/2503.19325</link>
<guid>https://arxiv.org/abs/2503.19325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍Frame AutoRegressive模型在长时间上下文视频生成中的应用及其优势。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Frame AutoRegressive (FAR) 模型，这是一个强大的视频自回归建模基线，旨在解决长时间上下文视频生成的挑战。FAR 模型通过学习连续帧之间的时间因果依赖，超越了传统的语言模型的表现，尤其在收敛性上优于Token AR和视频扩散变换器。此外，针对视觉冗余等长时间视觉建模中存在的问题，我们提出了FlexRoPE技术，增加灵活的时间衰减机制，以适应更长的视觉上下文。通过长短期上下文建模的方法，FAR能够使用更少的令牌编码长范围信息，并在训练长视频序列时保持可管理的令牌上下文长度。最终结果表明，FAR在短视频和长视频生成上均取得了最先进的性能，为视频自回归建模提供了一个简单而有效的基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19325" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 23:38:06 GMT</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:11:42 GMT</pubDate>
</item>
<item>
<title>通过CoMP实现的视觉基础模型的多模态预训练</title>
<link>https://arxiv.org/abs/2503.18931</link>
<guid>https://arxiv.org/abs/2503.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了多模态预训练的CoMP方法，显著提升了视觉模型的表现。</p><br /><br /><p><strong>摘要：</strong> 预训练的视觉基础模型（VFMs）在多个应用中展现出强大的视觉表示能力。本文提出了一种多模态预训练方法CoMP，通过持续预训练的方式，使VFMs能够轻松处理不同尺寸的视觉输入，并生成与语言表示更一致的视觉表示。CoMP通过引入连续旋转位置嵌入来支持原生分辨率的连续预训练，并通过语言原型间的对齐损失来协调视觉和文本特征，从而整合多模态表示。经过三阶段训练，VFMs在多模态理解及其他下游任务（如分类和分割）中取得了显著提升。其中，CoMP-SigLIP在ChartQA和DocVQA上分别取得66.7和75.9的分数，同时在ImageNet-1K上保持87.4%的准确率，以及在ADE20K上实现49.5的mIoU，显示出其在冻结块评估下的强大性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:52:47 GMT</pubDate>
</item>
<item>
<title>Frequency Dynamic Convolution: 一种高效的自适应卷积方法</title>
<link>https://arxiv.org/abs/2503.18783</link>
<guid>https://arxiv.org/abs/2503.18783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FDConv 提供高效的自适应卷积，提升模型性能并降低参数成本。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了频率动态卷积（FDConv），一种通过傅里叶域学习固定参数预算，从而克服动态卷积（DY-Conv）的高相似性频率响应所带来的局限性的方法。FDConv 将参数预算划分为基于频率的组，能够构建频率多样化的权重而不会增加参数成本。此外，文章提出的内核空间调制（KSM）和频率带调制（FBM）进一步增强了适应性，KSM 从空间层面动态调整每个滤波器的频率响应，而FBM则在频率域中将权重分解为不同的频带并根据局部内容进行动态调制。通过在物体检测、分割和分类等任务上进行广泛实验，FDConv 在 ResNet-50模型上表现优异，参数仅增加 3.6M，显著优于需要大幅增加参数预算的先前方法，同时FDConv可以无缝集成到多种架构中，包括ConvNeXt和Swin-Transformer，为现代视觉任务提供灵活高效的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18783" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:32:06 GMT</pubDate>
</item>
<item>
<title>LSRNA：基于潜在空间的图像超分辨率生成框架</title>
<link>https://arxiv.org/abs/2503.18446</link>
<guid>https://arxiv.org/abs/2503.18446</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LSRNA框架，强化潜在空间超分辨率，以生成高分辨率图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架LSRNA，用于采用扩散模型生成高于1K的高分辨率图像，通过在潜在空间中直接利用超分辨率来实现。传统扩散模型在生成高分辨率图像时难以超越其训练分辨率，常导致结构扭曲或内容重复。尽管参考基方法可以通过上采样低分辨率参考图像来指导高分辨率生成，但常常遇到潜在空间上采样导致的流形偏离和RGB空间上采样导致的过于平滑的问题。LSRNA通过结合潜在空间超分辨率（LSR）实现流形对齐，并采用区域噪声添加（RNA）来增强高频细节，克服了这些缺陷。实验结果表明，LSRNA在多个分辨率和评测指标上均优于现有的参考基方法，同时强调了潜在空间上采样在保留细节与清晰度方面的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18446" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 04:50:15 GMT</pubDate>
</item>
<item>
<title>基于Gumbel-Softmax的流匹配框架用于高维简约体的序列生成</title>
<link>https://arxiv.org/abs/2503.17361</link>
<guid>https://arxiv.org/abs/2503.17361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了Gumbel-Softmax流匹配框架，提高高维序列生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的Gumbel-Softmax流和评分匹配的生成框架，旨在解决在高维简约体中进行DNA序列设计的挑战。我们提出了一种依赖于时间的温度的Gumbel-Softmax插值，利用该插值引入Gumbel-Softmax流匹配，通过参数化速度场从平滑的分类分布传输到集中在简约体单一顶点的分布。此外，我们还提出了Gumbel-Softmax评分匹配，该方法学习回归概率密度的梯度。为实现无训练引导，我们设计了直通引导流（STGFlow），这是一种基于分类器的引导方法，能有效利用预训练的分类器在推理时指导速度场朝向简约体的最佳顶点。我们的框架在条件DNA启动子设计、仅序列蛋白生成及稀有疾病治疗的靶向肽设计中实现了最先进的性能，展示了其在可控序列生成方面的强大能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型的人本决策能力</title>
<link>https://arxiv.org/abs/2503.16965</link>
<guid>https://arxiv.org/abs/2503.16965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法提升视觉语言模型在复杂人本决策中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究系统评估了开源的视觉语言模型（VLMs）在多模态人本决策任务中的表现。结果显示，仅接受文本描述的语言模型（LLMs）意外地优于处理实际图像的同规模VLMs，暗示视觉对齐可能对VLM的能力造成抑制。为解决这一挑战，提出了一种新颖的仅基于文本的训练方法，通过合成文本数据来增强VLM的语言组件，并将所学能力转移至多模态推理，从而消除对昂贵的图像-文本配对数据的需求。此外，本研究还表明，VLM可以通过自我提升显著提高性能，利用由其LLM生成的训练数据，而无需依赖更大的教师模型，如GPT-4。我们的发现确立了一种更加高效和可扩展的方法，以增强VLM的人本决策能力，为通过自我提升机制优化VLM开辟了新途径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:25:23 GMT</pubDate>
</item>
<item>
<title>基于视觉语言模型的3D室内场景生成算法研究</title>
<link>https://arxiv.org/abs/2503.18476</link>
<guid>https://arxiv.org/abs/2503.18476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于视觉语言模型的3D室内场景生成新方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于视觉语言模型（VLM）的3D室内场景生成任务，考虑到了空间和布局常识约束。为了解决该问题，提出了一种新的全局-局部树搜索算法。在全局层面，该方法逐步放置对象，并在每个放置过程中探索多种放置选项，问题空间被表示为树形结构。为减少树的深度，场景结构被分解为房间级、区域级、地面对象级及支持对象级，算法独立生成不同区域的地面对象和放置在不同地面对象上的支持对象。在局部层面，算法将每个对象的放置任务分解为多个步骤，并在问题空间树中进行搜索。结合VLM模型，文章通过将自上而下视图空间离散化为密集网格，并使用丰富的表情符号来标识各个网格，提示VLM生成对象的位置。实验结果表明，所提出的方法在生成更为合理的3D场景方面表现优于现有最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:21:13 GMT</pubDate>
</item>
<item>
<title>Instruct-CLIP：改进图像编辑指令对齐的自监督方法</title>
<link>https://arxiv.org/abs/2503.18406</link>
<guid>https://arxiv.org/abs/2503.18406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Instruct-CLIP以提高指令引导图像编辑的质量和对齐性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Instruct-CLIP，这是一种自监督的方法，旨在改进自动图像编辑中的指令对齐问题。传统的文本到图像（T2I）生成模型存在局限，导致生成的图像对与原始和编辑图像的配对不准确，从而影响模型训练效果。Instruct-CLIP通过学习原始图像与编辑图像之间的语义变化，精炼并改进现有数据集中的指令对齐。此外，Instruct-CLIP还适配了处理噪声潜图像和扩散时间步，以便在潜扩散模型中有效地执行指令和图像变化之间的对齐。该方法还用于校正InstructPix2Pix数据集，生成超过120K的精炼样本，并应用于模型的微调，最终得到的模型可生成更符合给定指令的编辑结果。项目的代码和数据集已上传至指定网址。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 03:25:44 GMT</pubDate>
</item>
<item>
<title>QuartDepth：优化单目深度估计在ASIC上的应用</title>
<link>https://arxiv.org/abs/2503.16709</link>
<guid>https://arxiv.org/abs/2503.16709</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出QuartDepth，通过后训练量化优化MDE模型以适应ASIC。</p><br /><br /><p><strong>摘要：</strong> 单目深度估计在计算机视觉中至关重要，但将其准确模型部署于资源有限的边缘设备（如专用集成电路ASIC）却面临高计算和内存需求的挑战。为了解决这一问题，本文提出了一种名为QuartDepth的方法，采用后训练量化技术，针对ASIC量化MDE模型。具体而言，我们将权重和激活量化到4位精度，从而减少模型大小和计算成本。为减轻性能下降，提出了激活抛光及补偿算法，以及权重重建方法，旨在最小化权重量化误差。此外，我们设计了灵活且可编程的硬件加速器，支持内核融合和定制指令编程，这提升了吞吐量和效率。实验结果表明，我们的框架在ASIC上实现了竞争力的准确性，并支持快速推断与更高的能效，弥合了高性能深度估计与边缘设备实际应用之间的差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16709" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 17:03:10 GMT</pubDate>
</item>
<item>
<title>人类运动去学习：防止合成有害动画的新方法</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出运动去学习技术以减少有害动画合成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了人类运动去学习的任务，旨在防止合成有毒动画，同时保留文本到运动的生成性能。由于有毒运动可由明确的文本提示或安全运动的隐式组合生成，去学习有毒运动面临挑战。我们提出了第一个运动去学习基准，通过从HumanML3D和Motion-X这两个大型文本到运动数据集中筛选有毒运动。我们还通过适配最新的图像去学习技术，建立了基线以处理时空信号。此外，我们提出了一种新颖的运动去学习模型，称为LCR（潜变量替换），该模型无需训练，适用于最先进的文本到运动扩散模型的离散潜变量空间。LCR简单且在定性和定量上均优于基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:27 GMT</pubDate>
</item>
<item>
<title>多模态推理的发展与挑战综述</title>
<link>https://arxiv.org/abs/2503.18071</link>
<guid>https://arxiv.org/abs/2503.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述多模态推理方法及其未来发展方向。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地概述了近年来多模态推理的方法，将其分为语言中心多模态推理和协作多模态推理两个层面。前者涉及视觉感知在语言推理中的辅助作用，而后者则强调在推理过程中动作生成和状态更新，促进模态间的动态互动。我们分析这些方法的技术演进，讨论其面临的挑战，并介绍关键基准任务及评估指标，旨在评估多模态推理的表现。此外，本文还从视觉-语言推理到全模态推理、以及从多模态推理到多模态智能体的两个视角探讨未来研究方向，期望能激励多模态推理领域的进一步发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 09:40:44 GMT</pubDate>
</item>
<item>
<title>Feather-SQL: 一种针对小型语言模型的轻量级NL2SQL框架</title>
<link>https://arxiv.org/abs/2503.17811</link>
<guid>https://arxiv.org/abs/2503.17811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feather-SQL是为小型语言模型设计的轻量级NL2SQL框架，提升了执行效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型在自然语言转SQL (NL2SQL) 领域的显著进展，但它们依赖于闭源系统和高计算资源，面临数据隐私和部署挑战。相比之下，小型语言模型在NL2SQL任务中表现不佳，且与现有框架不兼容。为解决这些问题，我们提出Feather-SQL，一个专为小型语言模型量身打造的轻量级框架。Feather-SQL通过schema剪枝和链接、多路径多候选生成来提高SQL的可执行性和准确性。此外，我们引入了“1+1模型协作范式”，将一个强大的通用聊天模型与拥有精细调优的SQL专家模型相结合，既具备强大的分析推理能力，又高效生成高精度的SQL。实验结果表明，Feather-SQL显著提高了小型语言模型在NL2SQL上的表现，未经过微调的模型性能提升约10%，且该范式将小型语言模型的准确率提高至54.76%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 12:22:53 GMT</pubDate>
</item>
<item>
<title>CODA：一种有效的视觉离散化框架</title>
<link>https://arxiv.org/abs/2503.17760</link>
<guid>https://arxiv.org/abs/2503.17760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CODA框架通过解耦压缩与离散化，提升图像生成的稳定性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了CODA（连续到离散适配）框架，旨在解决传统离散视觉标记器在图像生成过程中面临的压缩与离散化联合训练的挑战。CODA通过将已优化的连续变分自编码器（VAE）进行适配，而非从头训练离散标记器，从而实现更稳定和高效的训练。重点关注离散化，确保了视觉质量的保留。在实验证明中，CODA在训练预算上比标准VQGAN减少了6倍，且在ImageNet 256×256基准上达到了100%的代码本利用率，以及在8倍和16倍压缩时分别获得了0.43和1.34的重建FID（rFID），显示出优异的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 08:59:00 GMT</pubDate>
</item>
<item>
<title>DynamicVis：面向遥感图像的动态视觉感知基础模型</title>
<link>https://arxiv.org/abs/2503.16426</link>
<guid>https://arxiv.org/abs/2503.16426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DynamicVis是一个为遥感图像设计的高效动态视觉感知模型。</p><br /><br /><p><strong>摘要：</strong> 随着遥感技术的发展，卫星图像的空间分辨率得到了显著提升，但现有方法在多应用场景中的泛化能力有限。本文提出DynamicVis，一个针对遥感图像的动态视觉感知基础模型，克服了传统模型在面对高分辨率数据和复杂场景语义时的局限性。该模型通过基于选择性状态空间的动态区域感知骨干集成，平衡了局部细节提取与全局上下文集成，支持大规模数据的高效编码，同时实现架构的可扩展性。为了增强跨任务知识的转移，DynamicVis采用了针对百万级区域注释的多实例学习范式。经过在九个下游任务上的评估，该模型展现了出色的多级特征建模能力，以97毫秒的延迟处理2048x2048像素，并消耗833 MB GPU内存，具有卓越的效率，明显优于基于ViT的其他模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于变换器的多光源白平衡修正方法</title>
<link>https://arxiv.org/abs/2503.14774</link>
<guid>https://arxiv.org/abs/2503.14774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的变换器模型来优化多光源下的白平衡修正。</p><br /><br /><p><strong>摘要：</strong> 本文针对多光源场景中的白平衡（WB）修正问题, 提出了两项重要贡献。首先，我们设计了一种高效的变换器模型，能够有效捕捉多种sRGB WB预设之间的空间依赖性，显著提高了线性融合技术的效果。其次，我们构建了一个大规模的多光源数据集，包含超过16000张使用五种不同白平衡设置渲染的sRGB图像及其修正图像。该方法在新构建的多光源图像融合数据集上，性能提升可达100%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 19:01:22 GMT</pubDate>
</item>
<item>
<title>揭示图像超分辨率评估中的地面真实图像质量影响</title>
<link>https://arxiv.org/abs/2503.13074</link>
<guid>https://arxiv.org/abs/2503.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究探讨了现有图像超分辨率数据集中地面真实图像的质量及其对评估的影响。</p><br /><br /><p><strong>摘要：</strong> 随着图像超分辨率（SR）技术的进步，虽然输出的感知质量不断提高，但在定量评估中却通常表现不佳，引发了对现有图像评价指标的怀疑。传统上，研究者们认为地面真实图像（GT）是完美的参考，而未对其质量进行检视。本文通过分析七种先进的SR模型在三个真实世界SR数据集上的表现，指出存在的低质量GT会造成模型评估的一致性下降，因此模型在控制GT质量的情况下表现会显著不同。此外，本文提出一种新颖的感知质量指标——相对质量指数（RQI），用于衡量图像对之间的质量差异，从而修正由于不可靠GT导致的偏差评估。该模型在与人类观众意见的一致性方面表现显著优于现有方法，期望为SR领域未来数据集、模型和指标的开发提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:25:48 GMT</pubDate>
</item>
<item>
<title>文化适应性对大型语言模型数学推理能力的影响研究</title>
<link>https://arxiv.org/abs/2503.18018</link>
<guid>https://arxiv.org/abs/2503.18018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示文化背景影响大型语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨大型语言模型（LLMs）在面临文化适应的数学问题时的推理能力。我们生成了六个合成文化数据集，基于广泛使用的GSM8K基准测试，同时保留了数学逻辑和数值，但修改了文化元素。研究发现，当文化引用发生变化时，LLMs在数学问题解决方面表现不佳，尽管数学结构保持不变。小型模型的性能下降更为明显，而文化熟悉度似乎可以提升数学推理能力。即便是没有显著数学训练的模型，在相关文化背景暴露下，仍能在嵌入文化的数学问题上超越大型数学能力模型。这项研究强调了文化背景对LLMs数学推理能力的影响，并凸显了为提高模型在实际应用中鲁棒性而需要更具多样性和代表性的训练数据的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:35:39 GMT</pubDate>
</item>
<item>
<title>新型平衡图像建模框架的提出</title>
<link>https://arxiv.org/abs/2503.18948</link>
<guid>https://arxiv.org/abs/2503.18948</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了一种新型图像建模框架，以解决子任务间的优化冲突。</p><br /><br /><p><strong>摘要：</strong> 当前的生成模型（如自回归和扩散方法）将高维数据分布学习分解为一系列简单的子任务。然而，在这些子任务的联合优化中，固有的冲突很难解决，现有方案往往无法在不牺牲效率或可扩展性的情况下克服这些冲突。本文提出了一种新型的等变图像建模框架，利用自然视觉信号的平移不变性，从根本上对齐子任务的优化目标。我们的方法引入了（1）沿水平轴增强平移对称性的列状标记化，和（2）通过强制跨位置一致的上下文关系的窗口因果注意力。我们在256x256分辨率的类条件ImageNet生成任务上进行了评估，该方法的性能可与最先进的自回归模型相媲美，同时使用更少的计算资源。系统分析表明，增强的等变性减少了任务间的冲突，显著提高了零样本泛化能力并实现超长图像合成。本文奠定了生成建模中任务对齐分解的基础框架，为高效参数共享和无冲突优化提供了新的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18948" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>CFG-Zero*: 改进的分类器自由引导技术</title>
<link>https://arxiv.org/abs/2503.18886</link>
<guid>https://arxiv.org/abs/2503.18886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨CFG-Zero*在流匹配模型中的改进效果。</p><br /><br /><p><strong>摘要：</strong> 本文分析了分类器自由引导技术（CFG）对基于高斯混合训练的流匹配模型的影响。研究表明，在训练初期，流估计不精确时，CFG可能引导样本走向错误的轨迹。基于此观察，提出了改进版本CFG-Zero*，该方法包括两个主要贡献：一是通过优化比例来校正估计速度的不准确性，二是将常微分方程求解器的前几步初始化为零。通过对文本到图像（如Lumina-Next、Stable Diffusion 3和Flux）以及文本到视频（Wan-2.1）生成的实验，CFG-Zero*在引导流匹配模型方面表现出色，显著优于传统的CFG方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:59:57 GMT</pubDate>
</item>
<item>
<title>探究大型语言模型的推理机制：基于稀疏自编码器的分析</title>
<link>https://arxiv.org/abs/2503.18878</link>
<guid>https://arxiv.org/abs/2503.18878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了稀疏自编码器在大型语言模型推理中的应用。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自然语言处理领域取得了显著成功，尤其是在推理能力方面。本文通过使用稀疏自编码器（SAEs）来识别推动DeepSeek-R1系列模型推理的特征。研究者们提出了一种提取候选“推理特征”的方法，并通过实证分析和可解释性方法验证了这些特征与模型推理能力之间的直接关联。结果表明，系统地引导这些特征可以有效提升推理性能，这为理解LLMs中的推理机制提供了首个机械性解释。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:54:26 GMT</pubDate>
</item>
<item>
<title>CURA: 提升软件工程任务的代码理解与推理代理系统</title>
<link>https://arxiv.org/abs/2503.18494</link>
<guid>https://arxiv.org/abs/2503.18494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURA系统通过增强的过程监督提高了代码生成和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CURA，一个增强了口头过程监督（VPS）的代码理解和推理代理系统，旨在解决复杂软件工程挑战。尽管现有的大型语言模型在代码生成基准上取得了显著进展，但在面临复杂任务时仍表现不佳。CURA在BigCodeBench等具有挑战性的基准上比基线模型提高了3.65%。此外，CURA与o3-mini模型及VPS技术结合后，达到了最先进的性能。这一工作标志着推理驱动架构与基于LLM的代码生成的结合，为语言模型处理复杂软件工程任务提供了代理推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:48:59 GMT</pubDate>
</item>
<item>
<title>MetaSpatial：基于强化学习的3D空间推理框架</title>
<link>https://arxiv.org/abs/2503.18470</link>
<guid>https://arxiv.org/abs/2503.18470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSpatial框架通过RL增强视觉语言模型的3D空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MetaSpatial，这是第一个基于强化学习（RL）的框架，用于增强视觉语言模型（VLM）的3D空间推理能力，实现在无需硬编码优化的情况下进行实时3D场景生成。MetaSpatial解决了两个核心挑战：一是VLM缺乏内化的3D空间推理，限制了其生成现实布局的能力；二是传统的监督微调（SFT）在布局生成任务中的低效性，因为完美的真值标注不可用。其关键创新在于引入了基于多轮RL优化机制，整合了物理约束和渲染图像评估。该框架采用适应性迭代推理过程，使VLM通过分析渲染输出逐步优化空间配置，从而提高场景的一致性。实验评估表明，MetaSpatial显著增强了各种规模模型的空间一致性和格式稳定性，生成的物体放置更加真实、协调，验证了RL在元宇宙、增强现实/虚拟现实、数字双胞胎及游戏开发中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:18:01 GMT</pubDate>
</item>
<item>
<title>Diffusion-4K：基于文本生成的超高分辨率图像合成框架</title>
<link>https://arxiv.org/abs/2503.18352</link>
<guid>https://arxiv.org/abs/2503.18352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diffusion-4K提出了一种新的超高分辨率图像合成方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Diffusion-4K，一个用于直接进行超高分辨率图像合成的新框架，结合文本到图像的扩散模型。主要创新包括构建了Aesthetic-4K基准数据集，该数据集弥补了公开可用的4K图像合成数据集的缺失，并采用GPT-4o生成高质量的4K图像和标题。此外，引入了GLCM得分和压缩比指标来评估图像细节，并结合FID、美学和CLIPScore等全局指标进行全面评价。其次，提出基于小波的微调方法，以实现对光真实4K图像的直接训练，适用于多种潜在扩散模型，显示出在合成高细节4K图像方面的有效性。实验结果表明，Diffusion-4K在超高分辨率图像合成方面展现出了卓越的性能，尤其是结合现代大规模扩散模型时（如SD3-2B和Flux-12B）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 01:25:07 GMT</pubDate>
</item>
<item>
<title>OmnimatteZero：一种无训练视频分层提取的新方法</title>
<link>https://arxiv.org/abs/2503.18033</link>
<guid>https://arxiv.org/abs/2503.18033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmnimatteZero是一种无训练的视频分层提取方法，能高效处理视频对象和背景。</p><br /><br /><p><strong>摘要：</strong> OmnimatteZero是一种新提出的方法，旨在无训练地将给定视频分解为具有语义意义的层，包括背景和独立对象及其相关效果，如阴影和反射。与现有方法相比，OmnimatteZero利用现成的预训练视频扩散模型，实现对象移除、个体对象层提取及其效果合成。该方法通过适应零样本图像修补技术来处理视频对象移除，尽管该技术在此任务上表现不佳。研究显示，自注意力图能够捕捉对象及其影响信息，并用于修补对象效果，从而保留干净的背景。此外，通过简单的潜在算术，用户可以将对象层与新视频层无缝重组以生成新视频。评估结果表明，OmnimatteZero不仅在背景重建方面表现优越，还创下最快Omnimatte方法的新记录，具备实时性能和极小的帧运行时间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 07:26:48 GMT</pubDate>
</item>
<item>
<title>Aether框架：结合几何重建与生成建模的智能空间推理系统</title>
<link>https://arxiv.org/abs/2503.18945</link>
<guid>https://arxiv.org/abs/2503.18945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aether框架实现了几何感知的智能空间推理，具备四维动态重建等三大核心能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Aether，一个统一的框架，旨在结合几何重建与生成建模，推动人类般的空间推理能力。Aether通过联合优化四大核心能力：四维动态重建、动作条件下的视频预测以及目标条件下的视觉规划，利用任务交错的特征学习，促进了重建、预测和规划目标间的知识共享。基于视频生成模型，我们的框架在训练期间未观察到真实数据的情况下，展现出前所未有的合成-真实泛化能力，并在动作跟随和重建任务中实现了零样本泛化。Aether的重建性能显著超过特定领域模型，即使未使用真实数据。此外，Aether通过几何信息驱动的动作空间，有效地将预测转化为行动，实现了自主轨迹规划。我们希望本研究能够激励社区探索物理合理的世界建模新前沿及其应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>探索测试时间扩展对视频生成质量的影响</title>
<link>https://arxiv.org/abs/2503.18942</link>
<guid>https://arxiv.org/abs/2503.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨测试时间扩展如何提高视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。最近，针对大型语言模型的研究扩展了测试时间的规模，从而显著提高了模型性能。本文通过研究如何在视频生成中应用测试时间扩展（TTS），探讨在允许使用大量推理计算的情况下，如何提升生成质量。我们将视频生成中的测试时间扩展重新解释为搜索问题，利用测试时间验证器和启发式算法来优化搜索过程。针对文本提示，我们首先提出了一种线性搜索策略，通过增加推理时的噪声候选项来探寻更好的路径。此外，为了提高效率，我们设计了一种名为树形帧（ToF）的 TTS 方法，以自回归的方式自适应地扩展和修剪视频分支。大量实验表明，增加测试时间计算持续显著提升视频质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>Video SimpleQA：评估大型视频语言模型的事实性基准</title>
<link>https://arxiv.org/abs/2503.18923</link>
<guid>https://arxiv.org/abs/2503.18923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Video SimpleQA基准，评估大型视频语言模型的事实性表现。</p><br /><br /><p><strong>摘要：</strong> 随着大型视频语言模型（LVLMs）在多模态理解领域的最新进展，评估其在视频上下文中的事实性仍然是一个关键挑战。为此，我们提出Video SimpleQA，这是首个针对LVLMs事实性评估的全面基准。该基准的几个关键创新特性包括：要求整合外部知识、针对客观事件的问题设计、明确且简短的答案格式、经过外部来源验证的注释以及时间推理能力的考察。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循上存在显著不足，尤其是开源模型。表现最佳的模型Gemini-1.5-Pro仅达到54.4%的F1分数。此外，测试时间的计算方式对性能提升影响有限，而检索增强生成虽然带来一致性改善，但也增加了推理时间开销，形成效率与性能的权衡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:46:09 GMT</pubDate>
</item>
<item>
<title>FFN Fusion: 提升大规模语言模型推理效率的新技术</title>
<link>https://arxiv.org/abs/2503.18908</link>
<guid>https://arxiv.org/abs/2503.18908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFN Fusion技术通过并行化FFN层实现语言模型推理效率提升。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FFN Fusion的架构优化技术，该技术通过识别并利用大语言模型中自然的并行化机会，降低了顺序计算。研究发现，去除特定注意力层后的前馈网络（FFN）层序列，通常可以在保持模型精度的前提下进行并行处理。我们开发了一种系统的方法来识别并融合这些序列，转化为并行操作，从而显著降低推理延迟，同时保留模型行为。应用于Llama-3.1-405B-Instruct模型，我们创建了Llama-Nemotron-Ultra-253B-Base，一个高效的新模型，推理延迟提高了1.71倍，单个Token的成本降低了35倍，并在各种基准测试中展现出良好的性能。我们的实验表明，在49B到253B参数的模型中，FFN Fusion在大规模应用中效果愈发显著，且能够与量化和剪枝等现有优化技术相辅相成。我们还发现，完整的变换器块（注意力层和FFN层）有时也可以进行并行化，这为神经架构设计指明了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:20:35 GMT</pubDate>
</item>
<item>
<title>基于零强化学习的多模型链式推理训练研究</title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了零强化学习在多模型中的应用和效果。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了零强化学习在10种不同基础模型上的训练效果，包括LLama3-8B、Mistral-7B/24B以及多个Qwen2.5系列模型。通过设计关键策略如调整奖励格式和控制查询难度，我们在推理准确性和响应长度上取得了显著提升。不同基础模型在训练过程中表现出不同的动态特征，增加的响应长度并不总是与特定认知行为（如验证）的出现相关联。值得注意的是，我们首次在非Qwen家族的小模型中观察到了“恍然大悟”的瞬间。此外，我们分享了实现成功零强化学习训练的关键设计，及其相关发现与实践，旨在推动进一步研究，我们还开源了代码、模型和分析工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:10 GMT</pubDate>
</item>
<item>
<title>利用潜在思维推断提升语言模型预训练数据效率</title>
<link>https://arxiv.org/abs/2503.18866</link>
<guid>https://arxiv.org/abs/2503.18866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过潜在思维推断提升数据效率以支持语言模型的预训练。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型预训练的计算规模超越人类文本增长，数据瓶颈问题日益严重。为在数据受限的情况下持续推进预训练，我们提出显式建模和推断文本生成过程背后的潜在思维，以显著提高数据效率。这一方法认为网络文本是冗长人类思维过程的压缩结果，而潜在思维包含重要的上下文知识和推理步骤，是数据高效学习的关键。通过对数学领域的实验，证明了合成数据推断潜在思维能够有效提升数据效率，表现优于相同原始数据量的训练。此外，我们展示了在没有强大教师的情况下进行潜在思维推断，语言模型通过EM算法自我引导其性能的迭代提升。结果显示，1B语言模型在三次迭代中能够利用推断计算显著超过依赖原始数据的基线，进一步的推断计算提升表明在数据受限预训练中存在新的扩展机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:41:23 GMT</pubDate>
</item>
<item>
<title>CaMeL：增强大语言模型安全性的防御措施</title>
<link>https://arxiv.org/abs/2503.18813</link>
<guid>https://arxiv.org/abs/2503.18813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出了CaMeL防御机制，保护LLM免受恶意数据的影响。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CaMeL的防御机制，旨在保护在外部环境中交互的代理系统中的大语言模型（LLM）。LLM在处理不可信数据时容易受到提示注入攻击，而CaMeL通过在LLM周围创建一个防护系统层来增强其安全性。具体来说，CaMeL明确提取来自可信查询的控制和数据流，确保LLM所检索的不可信数据不会影响程序流。此外，CaMeL还依赖于能力的概念，以防止通过未经授权的数据流泄露私密数据。在最近的代理安全基准AgentDojo的测试中，CaMeL成功以可证明的安全性解决了67%的任务，展示了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:54:10 GMT</pubDate>
</item>
<item>
<title>Hummingbird: 高效的文本到视频生成框架</title>
<link>https://arxiv.org/abs/2503.18559</link>
<guid>https://arxiv.org/abs/2503.18559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hummingbird框架通过轻量化设计提高文本到视频生成的效率与质量。</p><br /><br /><p><strong>摘要：</strong> 文本到视频生成（T2V）技术逐渐引起关注，但现有模型在计算效率与视觉质量之间难以平衡。针对这一挑战，本文提出了轻量化的T2V框架Hummingbird，通过剪枝现有模型并采用视觉反馈学习，成功将U-Net参数从14亿减少至7亿，大幅提高了效率且保留了高质量的视频生成能力。此外，Hummingbird还引入了新颖的数据处理流程，利用大型语言模型和视频质量评估模型来改善文本提示和视频数据的质量。实验结果表明，该方法相比于最新的VideoCrafter2模型，实现了31倍的加速，并在VBench上获得了最高得分。同时，Hummingbird支持最多生成26帧的视频，克服了传统U-Net方法在长视频生成方面的局限性。整个训练过程仅需四个GPU，却能与领先方法相媲美，展现出其在实际应用中的高效性和灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 07:13:33 GMT</pubDate>
</item>
<item>
<title>AgentRxiv：促进科学研究的协作框架</title>
<link>https://arxiv.org/abs/2503.18102</link>
<guid>https://arxiv.org/abs/2503.18102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentRxiv框架促进科学家间的协作研究，提升研究效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgentRxiv，一个促进科学家之间协作的框架，旨在解决现有自主研究流程的孤立性问题。通过允许大型语言模型（LLM）实验室上传和检索共享的预印本报告，AgentRxiv使研究人员能够合作、共享见解，并在彼此的研究基础上逐步推进。研究表明，获取先前研究的代理在性能上显著优于孤立工作的代理（在MATH-500上的相对提升达11.4%）。此外，多个实验室通过共享研究成果，能够共同朝着目标努力，并在整体准确性上更显著提升（在MATH-500上的相对提升达到13.7%）。这些发现表明，未来的AI系统设计中，自主代理可能与人类共同发挥作用。希望AgentRxiv为研究者加速科学发现提供助力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 11:16:42 GMT</pubDate>
</item>
<item>
<title>Vision-R1: 一种新型的视觉引导强化学习算法</title>
<link>https://arxiv.org/abs/2503.18013</link>
<guid>https://arxiv.org/abs/2503.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Vision-R1，一个新型视觉引导的强化学习算法，以提升LVLM的性能。</p><br /><br /><p><strong>摘要：</strong> 在大型视觉语言模型（LVLMs）的训练中，传统的两阶段方法包括预训练和监督微调。近期，偏好优化作为一种后训练强化策略，已被证明有效。但高质量的人工标注偏好数据构建及可靠的奖励模型开发均成本高且具挑战性。为解决这一问题，本文提出Vision-R1，一种新型的视觉引导R1-like强化学习算法，使用明确的视觉反馈奖励模型，无需专门的奖励模型和人工偏好数据集。我们引入了一个基于标准驱动的奖励函数，综合多维反馈来评估模型的完成效果。并实施了逐步规则优化策略，动态调整奖励标准，促进持续的模型改进，同时减少奖励劫持。大量实验表明，使用Vision-R1微调7B LVLMs在各类基准测试中获得了显著性能提升，甚至实现了50%的 improvement，并超越了当前10倍规模模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:21:14 GMT</pubDate>
</item>
<item>
<title>资源受限条件下视频生成模型的训练策略研究</title>
<link>https://arxiv.org/abs/2503.17735</link>
<guid>https://arxiv.org/abs/2503.17735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，在受限条件下从零开始训练小型视频生成模型优于参数高效调优。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成技术取得了显著进展，引起了学者们的广泛关注。在资源受限的条件下，研究人员通常基于参数高效调优的方法对预训练模型进行微调。然而，这些方法通常由于训练参数较少，造成适应能力不足，且源领域的知识可能导致目标领域推理过程偏离。本文提出，在有限资源条件下，从头开始训练小型视频生成模型，利用百万级样本，能在下游应用中优于大型模型的参数高效调优。以动画贴纸生成（ASG）为案例，我们构建了一个低帧率的离散帧生成网络，并提出双掩码数据利用策略，以提高有限数据的可用性和多样性。为促进双掩码下的收敛，我们提出了基于难度的适应性课程学习方法，通过将样本熵分解为静态和适应性成分，按难易程度抽取样本。实验结果表明，我们的资源高效双掩码训练框架在定量和定性上均优于I2V-Adapter和SimDA等参数高效调优方法，验证了该方法在受限资源下的可行性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 07:28:25 GMT</pubDate>
</item>
<item>
<title>优化大语言模型预训练的权重初始化与方差控制策略</title>
<link>https://arxiv.org/abs/2503.17500</link>
<guid>https://arxiv.org/abs/2503.17500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LIR和TVR策略，显著提升大语言模型预训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLM）预训练中的权重初始化和方差控制策略，重点介绍了层索引重标定（LIR）权重初始化方案与目标方差重标定（TVR）方差控制策略。通过在一个10亿参数的LLaMA模型上进行实验，结果表明这些技术在方差管理方面的改进可以显著提升下游任务的表现，在常见的预训练基准上提高了多达4.6%。此外，这些方法还有效减少了极端激活值，有助于缓解定量化和低精度训练中遇到的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 15:23:08 GMT</pubDate>
</item>
<item>
<title>优化RISC-V CPU上的大型语言模型推理</title>
<link>https://arxiv.org/abs/2503.17422</link>
<guid>https://arxiv.org/abs/2503.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨在RISC-V CPU上优化大型语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的指数增长，基于GPU的系统成为主要选择。然而，CPU由于其灵活性和更低的成本，逐渐成为特别适合推理和推断工作负载的替代方案。RISC-V作为一种开放且中立的指令集架构，正在迅速获得关注。虽然RISC-V硬件和软件生态系统尚不完善，针对特定领域的调优至关重要，本文旨在填补这一空白，专注于在首款具备向量处理能力的多核RISC-V CPU——Sophon SG2042上优化LLM推理。针对两个经过优化的前沿LLM，DeepSeek R1 Distill Llama 8B和DeepSeek R1 Distill QWEN 14B，我们实现了4.32/2.29 token/s的令牌生成速率与6.54/3.68 token/s的提示处理速率，相较于基线速度提升达2.9倍/3.0倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:00:19 GMT</pubDate>
</item>
<item>
<title>优化最小高斯表示法（OMG）在3D场景渲染中的应用</title>
<link>https://arxiv.org/abs/2503.16924</link>
<guid>https://arxiv.org/abs/2503.16924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OMG方法通过减少高斯数量优化3D场景渲染存储。</p><br /><br /><p><strong>摘要：</strong> 3D Gaussian Splatting（3DGS）作为一种高效的实时渲染表示方法，面临着使用大量显式高斯原语所带来的存储和内存开销问题。尽管已有研究表明通过高精度属性可以在减少高斯数量的情况下实现高质量渲染，现有的压缩方法仍依赖于大量高斯，主要集中在属性压缩上，导致压缩敏感性提高，质量下降。为此，本文提出了一种优化的最小高斯表示法（OMG），旨在显著减少存储需求并使用最小数量的原语。通过识别近邻高斯的区别以降低冗余，并提出一种高效的属性表示，以保留原语之间的连续性和不规则性。此外，采用子向量量化技术以进一步提升不规则性表示的效率，实现快速训练且代码本大小可忽略。大量实验表明，OMG相较于当前最先进技术将存储需求降低近50%，并在保持高渲染质量的同时实现600+帧每秒的渲染速度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:41:45 GMT</pubDate>
</item>
<item>
<title>Typed-RAG: 面向非事实问答的多维度框架</title>
<link>https://arxiv.org/abs/2503.15879</link>
<guid>https://arxiv.org/abs/2503.15879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Typed-RAG框架通过多维度分解提升非事实问答的回答质量。</p><br /><br /><p><strong>摘要：</strong> 非事实问答（NFQA）因其开放性、多样化的意图及多维推理的需求，给传统的事实问答方法带来了重大挑战。本文提出Typed-RAG，一个在RAG框架内的类型感知多维分解方法，专门解决NFQA的问题。Typed-RAG将NFQ分类为不同类型（如争论、经验和比较），并通过基于维度的分解来优化检索与生成策略。通过将多维NFQ分解为单一维度的子查询并汇总结果，Typed-RAG能够生成更具信息量和上下文相关性的回答。我们还引入Wiki-NFQA，这是一个涵盖多种NFQ类型的基准数据集，实验结果表明Typed-RAG明显优于基线模型，凸显了类型感知分解在NFQA中有效检索和生成的关键作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:04:12 GMT</pubDate>
</item>
<item>
<title>Bottleneck Sampling：一种高效的扩散模型推理框架</title>
<link>https://arxiv.org/abs/2503.18940</link>
<guid>https://arxiv.org/abs/2503.18940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架，可在不损失质量的情况下加速扩散模型推理。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视觉内容生成中表现出色，但其推理过程中的高计算成本限制了应用。本文提出了Bottleneck Sampling框架，利用低分辨率的先验知识，减少计算开销同时保持输出的高质量。该框架采用高-低-高的去噪工作流程，在高分辨率的初始和结束阶段进行去噪，而在中间步骤使用低分辨率。为减少混叠和模糊伪影，进一步优化了解析度切换和去噪时间步的自适应调整。实验表明，该方法在图像生成任务中加速推理速度可达3倍，在视频生成中可达2.5倍，且在多个评估指标上与标准的全分辨率采样过程输出质量相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AlphaSpace：提升大型语言模型在3D空间导航中的空间推理能力</title>
<link>https://arxiv.org/abs/2503.18769</link>
<guid>https://arxiv.org/abs/2503.18769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaSpace显著提高大型语言模型的3D空间导航性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了AlphaSpace，一种新方法旨在增强大型语言模型（LLMs）在3D笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的分词策略，通过专门的语义标记编码高度信息，并主要整合符号化合成推理数据。这种方法使LLMs能够准确地操作对象，将其放置在特定的[x, y, z]坐标上。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总体准确度达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:16:51 GMT</pubDate>
</item>
<item>
<title>利用多模态LLM评估跨模态理解与生成任务</title>
<link>https://arxiv.org/abs/2503.17489</link>
<guid>https://arxiv.org/abs/2503.17489</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了评估多模态生成与理解的统一基准，探讨LLM在任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了使用多模态大型语言模型（MLLM）作为自动评估工具在开放式多模态理解（MMU）和生成（MMG）任务中的应用。通过推出两个基准TaskAnything和JudgeAnything，本文分别评估了MLLM在多模态任务中的整体表现和评判能力。TaskAnything针对15类多模态任务进行评估，使用1500个经过精挑细选的查询，而JudgeAnything则重点考察了5种先进模型（如GPT-4o和Gemini-2.0-Flash）的评判能力。实验结果显示，尽管MLLM在MMU任务中表现优异（在配对比较设置中平均达66.55%），但在MMG任务中面临重大挑战（配对比较设置中仅达53.37%），暴露了跨模态偏见和幻觉问题。为此，本文提出了OmniArena，一个评估多模态模型和奖励模型的自动化平台，强调需要更公平的评估协议以及与人类偏好的更强对齐。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17489" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 14:59:20 GMT</pubDate>
</item>
<item>
<title>推动游戏开发革新的生成游戏引擎</title>
<link>https://arxiv.org/abs/2503.17359</link>
<guid>https://arxiv.org/abs/2503.17359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨互动生成视频如何推动生成游戏引擎的发展，重塑游戏创作方式。</p><br /><br /><p><strong>摘要：</strong> 现代游戏开发面临着创意和成本方面的重大挑战，传统游戏引擎的预设内容限制了开发者的创新能力。近期视频生成模型的突破，为创建逼真且互动的虚拟环境提供了新机遇。本文提出以互动生成视频（IGV）为基础，构建生成游戏引擎（GGE），这一平台能够实现无限的新内容生成，革新下一代游戏。GGE利用IGV的优势，进行高质量内容合成、物理意识的世界建模、用户控制的互动、长期记忆能力和因果推理。我们详细阐述了GGE的核心模块及其发展路线图，从L0到L4引导其演进，展望在人工智能时代，AI驱动的生成系统将根本改变游戏的创作与体验方式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>通过错误学习提升大型语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2503.17439</link>
<guid>https://arxiv.org/abs/2503.17439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LEMMA方法，通过错误学习提高大型语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在解决数学问题方面展现出卓越的推理能力，但现有方法主要集中于改进正确训练数据的质量，忽略了错误数据的潜在价值。本文提出了一种名为LEMMA的方法，通过构建包含错误步骤的错误解与正确解的反思联系的数据集，来增强模型的推理能力。我们分析了模型生成的错误类型，并引入了基于错误类型的增强方法，以收集多样且具有代表性的错误。通过模型感知的平滑反思连接，错误解被有效地转化为正确解。实验结果表明，LEMMA在性能上显著优于其他强基线，为大型语言模型的数学推理能力提升提供了新的途径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:10 GMT</pubDate>
</item>
<item>
<title>MagicComp: 通过双阶段精炼提升文本生成视频的组合能力</title>
<link>https://arxiv.org/abs/2503.14428</link>
<guid>https://arxiv.org/abs/2503.14428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicComp是一种创新的方法，通过双阶段精炼改进文本生成视频的能力。</p><br /><br /><p><strong>摘要：</strong> MagicComp是一种无须训练的创新方法，旨在通过双阶段精炼技术提升文本生成视频（T2V）的组合能力。其在两个阶段中特别设计：第一阶段的条件化阶段引入了语义锚点消歧义技术，通过逐步注入语义锚点的方向向量来强化主体特定语义，解决了主体间的模糊性问题；第二阶段的去噪阶段提出了动态布局融合注意力机制，灵活地将主体绑定到其时空区域，以便通过掩蔽注意力调制实现。MagicComp具备模型无关性和通用性，可以无缝地集成到现有的T2V架构中。经过在T2V-CompBench和VBench上的广泛实验，MagicComp的性能超越了当前最先进的方法，显示了其在复杂提示基础和轨迹可控视频生成应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:02:14 GMT</pubDate>
</item>
<item>
<title>Can Large Vision Language Models Read Maps Like a Human?</title>
<link>https://arxiv.org/abs/2503.14607</link>
<guid>https://arxiv.org/abs/2503.14607</guid>
<content:encoded><![CDATA[
In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 14:05:38 GMT</pubDate>
</item>
<item>
<title>GAEA：图像地理定位中的对话模型创新</title>
<link>https://arxiv.org/abs/2503.16423</link>
<guid>https://arxiv.org/abs/2503.16423</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GAEA模型通过交互方式提升图像地理定位的能力。</p><br /><br /><p><strong>摘要：</strong> 图像地理定位是一个有重要应用的挑战性任务，传统模型只能提供GPS坐标，缺乏与用户的互动能力。近期，随着大型多模态模型的发展，研究者仅通过这些模型尝试进行图像的地理定位，但在专业的下游任务中仍显不足。为此，本文提出了GAEA对话模型，旨在为用户提供所需的地理位置信息。由于缺乏大规模数据集进行训练，我们构建了GAEA数据集，包含80万张图像及160万对问答，利用OpenStreetMap属性和地理上下文线索生成。同时，提出了包含4000对图像-文本的多样化基准来评估对话能力，结果显示GAEA在多个开源及专有LMMs中显著优于现有最优模型LLaVA-OneVision和GPT-4o，分别提升25.69%及8.28%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16423" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FFaceNeRF：一种灵活的3D人脸编辑技术</title>
<link>https://arxiv.org/abs/2503.17095</link>
<guid>https://arxiv.org/abs/2503.17095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFaceNeRF通过增强用户控制和灵活性，实现高质量3D人脸编辑。</p><br /><br /><p><strong>摘要：</strong> FFaceNeRF是一种基于神经辐射场（NeRF）的3D人脸编辑技术，旨在克服现有方法在用户控制方面的局限性。传统方法依赖于预训练的分割掩码，限制了用户对面部特征的个性化调整，且需要大量的训练数据。FFaceNeRF采用几何适配器与特征注入技术，有效操控几何属性，结合潜在混合与三平面增强，支持少量样本的训练。这种创新方法确保了快速适应用户所需的掩码布局，尤其适用于个性化医疗影像和创意人脸编辑等领域。比较评估结果显示，FFaceNeRF在灵活性、控制性和生成图像质量等方面超越了现有基于掩码的人脸编辑方法，为高保真3D人脸编辑的未来发展开辟了新路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 08:24:58 GMT</pubDate>
</item>
<item>
<title>TaoAvatar：高保真轻量级全身虚拟头像的实时渲染</title>
<link>https://arxiv.org/abs/2503.17032</link>
<guid>https://arxiv.org/abs/2503.17032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaoAvatar在增强现实中实现高保真实时全身虚拟头像渲染。</p><br /><br /><p><strong>摘要：</strong> TaoAvatar是一种基于3D Gaussian Splatting (3DGS) 的高保真轻量级全身虚拟头像，具有广泛的应用潜力，从电商直播到全息通信。尽管现有技术在头像创建上已有进展，但在全面口语任务中面临着面部表情和身体动作的细粒度控制问题，同时还常常缺乏细节和实时性能。TaoAvatar通过创建个性化服装人类参数模板，并使用StyleUnet网络处理复杂的姿势依赖非刚性变形，来捕捉高频外观细节。最终，通过蒸馏技术将非刚性变形“烘焙”到轻量级的MLP网络中，并开发混合形状以补偿细节，从而在各种设备上实现90 FPS的实时渲染质量，展现了其行业领先的渲染效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 06:40:37 GMT</pubDate>
</item>
<item>
<title>融合3D视觉语言模型的通用少样本点云分割框架</title>
<link>https://arxiv.org/abs/2503.16282</link>
<guid>https://arxiv.org/abs/2503.16282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架GFS-VL，结合少样本和伪标签提高点云分割效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GFS-VL的通用少样本3D点云分割框架，该框架旨在通过结合稀疏的少样本和密集的伪标签，以最大化两者的优势，适应新类的分割。具体而言，我们引入了一种原型引导的伪标签选择方法，以滤除低质量区域，并采用自适应填充策略，结合伪标签上下文和少样本知识，标注过滤后的未标记区域。此外，我们设计了一种新基础混合策略，将少样本嵌入训练场景中，从而保持关键上下文以改善新类学习。针对目前GFS-PCS基准中有限的多样性，我们还引入了两个具有多样化新类的挑战性基准，以便于全面的泛化评估。实验结果验证了框架在不同模型和数据集上的有效性，为GFS-PCS在实际应用中的进一步发展奠定了良好的基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:10:33 GMT</pubDate>
</item>
<item>
<title>SISO：基于单图像的个性化图像生成与编辑方法</title>
<link>https://arxiv.org/abs/2503.16025</link>
<guid>https://arxiv.org/abs/2503.16025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SISO是一种无需训练的个性化图像生成与编辑方法。</p><br /><br /><p><strong>摘要：</strong> SISO是一种新颖的个性化图像生成与编辑方法，解决了仅有少量或单张图像时的个性化挑战。该方法通过优化与输入主体图像的相似性得分，迭代生成图像，直至实现理想的相似度，无需训练。我们在图像编辑和生成任务中对SISO进行评估，使用多样的个人主体数据集，结果显示该方法在图像质量、主体保真性及背景保留方面相较于现有方法有显著提升。SISO的设计允许与任何图像生成器进行即插即用的优化，展示了其广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:45:04 GMT</pubDate>
</item>
<item>
<title>PVChat：个性化视频大语言模型的单次学习框架</title>
<link>https://arxiv.org/abs/2503.17069</link>
<guid>https://arxiv.org/abs/2503.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PVChat 提供了一种个性化视频理解的新框架，支持单一视频的身份感知问答。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PVChat，这是一种新型的个性化视频大语言模型（ViLLM），旨在提升视频理解中的身份感知能力。现有的ViLLMs在一般视频理解上表现出色，但在处理如“威尔森正在接受化疗”或“汤姆与莎拉讨论”这类身份相关的理解任务时存在不足，限制了其在智能医疗和智能家居中的应用。PVChat通过单次学习的框架，结合Mixture-of-Heads (MoH)增强模型和自动化增强管道，生成多样化的训练数据集，涵盖存在、外观、动作和位置查询四种QA类型。此外，PVChat的改进措施包括ReLU Routing MoH注意机制及两种新目标：平滑邻域正则化和头部激活增强。通过在多个数据集上的评估，PVChat显示出在个性化特征理解方面的优势，较现有最先进的ViLLMs具备更高的学习能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 07:50:06 GMT</pubDate>
</item>
<item>
<title>ETVA：一种新的文本到视频对齐评估方法</title>
<link>https://arxiv.org/abs/2503.16867</link>
<guid>https://arxiv.org/abs/2503.16867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ETVA通过细粒度问答方法提升文本到视频对齐评估的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的文本到视频对齐评估方法ETVA，旨在解决现有度量方法在细粒度对齐方面的不足。ETVA通过多agent系统，将输入的提示解析为语义场景图，并生成原子问题。接着，设计了一个知识增强的多阶段推理框架，辅助的大型语言模型（LLM）首先检索相关常识知识，然后由视频LLM通过多阶段推理机制回答生成的问题。实验表明，ETVA的斯皮尔曼相关系数达到58.47，远高于现有度量方法的31.0，显示出与人类判断之间的显著相关性。此外，我们构建了一个专门用于文本到视频对齐评估的全面基准，包含2000个多样化的提示和12000个跨10个类别的原子问题。通过对15种现有文本到视频模型的系统评估，我们识别了它们的关键能力和局限性，为下一代T2V生成奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:52:50 GMT</pubDate>
</item>
<item>
<title>基于特征效用评估的视觉编码器优化方法</title>
<link>https://arxiv.org/abs/2503.16660</link>
<guid>https://arxiv.org/abs/2503.16660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法以优化视觉编码器，减少计算成本而不损失质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉编码器生成的丰富标记在计算中的价值，并提出了一种新方法来评估特征的效用，旨在减少不必要的计算负担。新方法结合自编码器和Gumbel-Softmax选择机制，以识别和保留最具信息量的视觉标记。研究表明，在OCR任务中，利用该方法能够在保持性能的前提下，去除超过50%的视觉上下文；而随机去除同样比例的特征则会显著影响模型能力。此外，在一般领域的任务中，即使随机保留30%的标记，其性能也可与使用全套视觉标记相比拟。这一结果为自适应高效的多模态特征选择和模型优化提供了新的方向，助力更具可扩展性和低开销的推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16660" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 15:17:08 GMT</pubDate>
</item>
<item>
<title>MathFlow：提升多模态大语言模型视觉数学问题解决能力的框架</title>
<link>https://arxiv.org/abs/2503.16549</link>
<guid>https://arxiv.org/abs/2503.16549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MathFlow，以提高多模态模型在视觉数学问题中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLMs）在各种任务中表现出色，但在视觉数学问题的解决能力上，特别是在准确感知和理解图表方面，仍有待提高。我们假设，从图表中提取有意义信息的感知能力对后续推理过程至关重要。为此，我们开发了FlowVerse，一个综合基准，用以评估在问题解决中使用的所有信息。初步结果显示，现有的MLLMs在从图表中提取关键信息和进行复杂推理方面存在显著限制。针对这一问题，我们提出了一个模块化的解决管道MathFlow，将感知和推理分解为独立的阶段，并训练了专为感知设计的MathFlow-P-7B模型。实验结果表明，MathFlow-P-7B在与多种推理模型集成时，显著提升了性能，展示了该管道的有效性及其与多种推理框架的兼容性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16549" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 07:46:19 GMT</pubDate>
</item>
<item>
<title>针对长尾问题的自适应数据精炼框架在大视觉语言模型中的应用</title>
<link>https://arxiv.org/abs/2503.12821</link>
<guid>https://arxiv.org/abs/2503.12821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自适应数据精炼框架，缓解LVLM中的长尾问题。</p><br /><br /><p><strong>摘要：</strong> 本文分析了大视觉语言模型（LVLM）中的长尾（LT）问题，识别出主要原因为头部概念过度代表和尾部概念不足。针对这种不平衡的数据分布，提出了一个自适应数据精炼框架（ADR），包括数据重平衡（DR）和数据合成（DS）两个阶段。在DR阶段，根据实体分布适应性地重新平衡冗余数据；在DS阶段，利用去噪扩散概率模型（DDPMs）和稀缺图像，补充不足的部分。通过在十一项基准测试中的综合评估，ADR有效缓解了训练数据的长尾问题，LLaVA 1.5的平均表现提高了4.36%，且未增加训练数据量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 01:01:09 GMT</pubDate>
</item>
<item>
<title>人工智能中的隐性偏见：通过推理模型隐性联想测试的研究</title>
<link>https://arxiv.org/abs/2503.11572</link>
<guid>https://arxiv.org/abs/2503.11572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，推理模型在处理信息时显示出类似隐性偏见的模式。</p><br /><br /><p><strong>摘要：</strong> 隐性偏见是指自动的心理过程，这些过程影响我们的感知、判断和行为。与人类研究不同，先前对大语言模型（LLMs）中隐性偏见的研究主要集中于模型输出，而非模型处理过程。为此，本文提出了一种名为推理模型隐性联想测试（RM-IAT）的方法，用于研究推理模型中的隐性偏见模式。研究发现，推理模型在处理关联不兼容的信息时需要更多的tokens，这表明人工智能系统在信息处理上存在与人类隐性偏见相似的模式。这些发现对于AI系统在现实应用中的部署具有重要意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11572" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:40:02 GMT</pubDate>
</item>
<item>
<title>深度视觉语言模型OpenVLThinker的推理能力提升研究</title>
<link>https://arxiv.org/abs/2503.17352</link>
<guid>https://arxiv.org/abs/2503.17352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，OpenVLThinker在复杂推理任务中表现出色。</p><br /><br /><p><strong>摘要：</strong> 本研究基于DeepSeek-R1的最新成果，探讨将复杂推理能力融入大型视觉语言模型（LVLMs）的可行性，并评估其在多模态推理任务中的影响。研究采用逐步监督微调（SFT）和强化学习（RL）相结合的方法，首先从纯文本R1模型中提炼出推理能力，通过多样化视觉数据集中的高质量图像标题生成推理步骤。然后，经过迭代的RL训练不断提升推理技能，每轮RL改进的模型生成精炼的SFT数据集供下一轮使用。最终，推出的OpenVLThinker在MathVista、MathVerse和MathVision等挑战性基准测试中展现了显著的推理性能提升，证明了本策略在增强视觉语言推理中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:52:43 GMT</pubDate>
</item>
<item>
<title>FastCuRL：高效的课程强化学习方法提升推理模型性能</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FastCuRL，通过扩展上下文窗口加速推理模型训练，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FastCuRL，一种简单而高效的课程强化学习方法，旨在提高R1类推理模型在处理复杂推理任务时的训练效率，特别是在拥有15亿参数的语言模型上。FastCuRL包括两个主要过程：长度感知的训练数据分段和上下文窗口的扩展训练。具体而言，前者首先根据输入提示长度将原始训练数据分为三个不同的层次，后者则利用分段的训练数据集，通过逐步增加上下文窗口的长度来训练推理模型。实验结果表明，FastCuRL-1.5B-Preview在五个数据集（包括MATH 500、AIME 2024、AMC 2023、Minerva Math和OlympiadBench）上均优于DeepScaleR-1.5B-Preview，同时训练步骤仅使用50%。此外，FastCuRL-1.5B-Preview的所有训练阶段均在单节点8 GPU的环境下完成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 12:35:31 GMT</pubDate>
</item>
<item>
<title>提升创意写作生成的多样性与质量</title>
<link>https://arxiv.org/abs/2503.17126</link>
<guid>https://arxiv.org/abs/2503.17126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了如何在创意写作生成中提高输出的多样性与质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何在创意写作任务中提升输出的多样性及质量，探讨了后训练方法的有效性。核心思想是将偏差概念融入训练目标，这样可以通过学习高质量的稀有实例来促进模型的多样性。采用直接偏好优化（DPO）和几率比偏好优化（ORPO）的方法，我们的模型在输出多样性上达到了与人类创作数据集相媲美的水平，同时保持与最佳指令调优模型相似的输出质量。研究中还通过人类评估、消融实验以及与现有多样性方法DivPO的比较来验证所提出的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 09:21:45 GMT</pubDate>
</item>
<item>
<title>VCtrl：提升视频生成中的细粒度控制能力</title>
<link>https://arxiv.org/abs/2503.16983</link>
<guid>https://arxiv.org/abs/2503.16983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VCtrl框架提升了视频生成过程中的细粒度控制与质量。</p><br /><br /><p><strong>摘要：</strong> 在视频生成研究中，尽管文本到视频生成取得了显著进展，但对细粒度时空属性的精确和灵活控制仍是一个未解决的重要挑战。为应对这些限制，本文提出了VCtrl（也称为PP-VCtrl）这一新框架，旨在以统一的方式实现对预训练视频扩散模型的细粒度控制。VCtrl通过一个通用的条件模块，将多种用户指定的控制信号（如Canny边缘、分割掩码和人体关键点）集成到预训练的视频扩散模型中，而无需修改底层生成器。此外，设计了一个统一的控制信号编码管道以及一种稀疏残差连接机制，以高效地整合控制表示。全面的实验和人类评估显示，VCtrl有效提高了可控性和生成质量，源代码和预训练模型可在PaddlePaddle框架下公开获得。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:48:00 GMT</pubDate>
</item>
<item>
<title>基于适应性DPO的图像生成模型偏好数据研究</title>
<link>https://arxiv.org/abs/2503.16921</link>
<guid>https://arxiv.org/abs/2503.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨适应性DPO在图像生成模型训练中的作用。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像生成领域的进展显著，特别是在调整模型以符合人类普遍偏好的方法上。本文研究了偏好数据在扩散模型训练中的关键作用，特别是Diffusion-DPO及其后续适应情况。我们探讨了图像生成中普遍人类偏好的复杂性，强调了这些偏好的主观特点及偏好数据集中少数样本所带来的挑战。通过试验，我们证明了少数样本的存在及其对模型性能的负面影响。为此，我们提出了一种新方法Adaptive-DPO，采用了少数样本意识度量来优化DPO目标函数。此度量结合了标注者内部信心和标注者间稳定性，区分了多数和少数样本，最终引入了Adaptive-DPO损失函数。此方法不仅增强了模型对多数标签的学习能力，同时也减轻了少数样本的负面影响，通过实验验证了其对合成少数数据和真实偏好数据的有效处理，推动了图像生成任务中更有效的训练方法的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:33:44 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的多模态科学问题解决策略</title>
<link>https://arxiv.org/abs/2503.16905</link>
<guid>https://arxiv.org/abs/2503.16905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于多智能体框架的解决策略，显著提高多模态科学问题的处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态科学问题（MSPs）的复杂性，提出了一种基于多智能体框架的解决策略。传统科学问题的解决尽管已有进展，但MSPs在多模态综合推理和反思能力方面仍面临挑战。为此，我们引入了Big Seven Personality与苏格拉底指导原则（MAPS），利用七个不同的智能体通过反馈机制和苏格拉底方法来引导问题解决。我们提出四种智能体的逐步解决策略，聚焦于问题解决过程的不同阶段，并引入批判智能体，激发批判性思维和自主学习。通过在EMMA、Olympiad和MathVista数据集上的大量实验证明，该方法在所有任务上较现有最优模型提升了15.84%的表现，同时也验证了模型的进步及其泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:13:45 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的自动化提示优化方法</title>
<link>https://arxiv.org/abs/2503.16874</link>
<guid>https://arxiv.org/abs/2503.16874</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MARS框架，通过多代理系统优化提示，提高问答模型的响应质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARS（多智能体框架结合苏格拉底指导）的自动化提示优化方法，以解决传统手动设计提示的认知偏见和现有方法在提示空间搜索中的灵活性不足等问题。MARS框架包含七个功能各异的智能代理，通过规划者自主设计优化路径，实现了优化过程的灵活性。此外，采用教师-批评者-学生的苏格拉底对话模式，MARS能够迭代优化提示并进行有效的搜索。我们在多个数据集上进行了广泛实验，以验证该方法的有效性，并进行附加分析实验以评估模型的进展和可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16874" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 02:19:55 GMT</pubDate>
</item>
<item>
<title>TokenBridge：结合离散与连续Token的视觉生成模型</title>
<link>https://arxiv.org/abs/2503.16430</link>
<guid>https://arxiv.org/abs/2503.16430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TokenBridge，通过后训练量化优化视觉生成模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视觉生成模型TokenBridge，旨在解决离散和连续Token表示之间的矛盾。离散Token虽然易于用标准交叉熵损失进行建模，但却存在信息丢失和训练不稳定的问题；而连续Token虽然能更好地保留视觉细节，却需要复杂的分布建模，增加了生成管道的复杂性。TokenBridge通过后训练量化将离散化与tokenizer训练过程解耦，使得可以从连续表示中直接获得离散Token。具体来说，采用维度独立的量化策略，将每个特征维度独立离散化，并配合轻量级的自回归预测机制，有效建模生成的大Token空间。实验结果表明，该方法在重建和生成质量上与连续方法相当，同时采用标准的分类预测。此研究展示了离散与连续模型的结合能有效发挥两者的优势，为高质量视觉生成开辟了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>多智能体系统的组合约束与自动数据收集框架设计</title>
<link>https://arxiv.org/abs/2503.16408</link>
<guid>https://arxiv.org/abs/2503.16408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出组合约束与自动化数据收集，推动多智能体系统发展。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对多智能体系统的组合约束概念，解决了现有方法在生成安全高效训练数据时面临的挑战。通过设计针对不同类型约束的多个接口，实现了与物理世界的无缝互动。基于这一理念，开发了一种自动数据收集框架，并推出了首个多智能体操作基准RoboFactory。此外，本文探讨了多智能体模仿学习的架构与训练策略，以期建立安全且高效的多智能体系统。在RoboFactory基准上，适配并评估了模仿学习方法，分析了其在不同难度任务中的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:58:38 GMT</pubDate>
</item>
<item>
<title>GASP：自主驾驶中的几何与语义自监督预训练方法</title>
<link>https://arxiv.org/abs/2503.15672</link>
<guid>https://arxiv.org/abs/2503.15672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GASP方法通过4D占据预测提升了自主驾驶性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GASP的几何与语义自监督预训练方法，旨在通过预测任意未来时空点的占据信息来学习环境的统一表示。GASP主要关注三个方面：一般占据，以捕捉3D场景的演变结构；自我占据，建模自我车辆在环境中的路径；以及从视觉基础模型中提炼的高级特征。通过建模几何和语义4D占据场，而非原始传感器测量，该模型能够学习环境及其随时间演变的结构化、可推广的表示。我们在多个自主驾驶基准测试中验证了GASP，结果显示在语义占据预测、在线地图构建和自我轨迹预测方面显著提高。这一研究展示了连续的4D几何与语义占据预测为自主驾驶提供了一种可扩展和有效的预训练范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 16:00:27 GMT</pubDate>
</item>
<item>
<title>基于深度学习的代码补全工具的组织和开发者特定微调研究</title>
<link>https://arxiv.org/abs/2503.14201</link>
<guid>https://arxiv.org/abs/2503.14201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了深度学习代码补全工具在组织和开发者特定微调中的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于深度学习的代码补全工具通过针对特定组织或开发者的微调提高性能的可能性。通过对来自两家组织（Apache和Spring）的136名开发者进行实验，比较了两种模型架构（T5和Code Llama）和三种模型规模（60M, 750M和7B可训练参数）的表现。结果表明，组织特定和开发者特定的微调显著提高了预测能力，尤其是组织特定微调显示出更强的性能。这一发现适用于不同组织和不同规模的模型。此外，组织特定数据集微调的深度学习模型，其完成性能达到了比起使用更大预训练模型的相同效果，从而在部署和推理成本上实现节约，降低了对GPU资源的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 08:26:06 GMT</pubDate>
</item>
<item>
<title>Where do Large Vision-Language Models Look at when Answering Questions?</title>
<link>https://arxiv.org/abs/2503.13891</link>
<guid>https://arxiv.org/abs/2503.13891</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 00:34:43 GMT</pubDate>
</item>
<item>
<title>DeCapBench与DCScore：细节图像标注的新标准</title>
<link>https://arxiv.org/abs/2503.07906</link>
<guid>https://arxiv.org/abs/2503.07906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍DeCapBench和DCScore，提升细节图像标注评估效果。</p><br /><br /><p><strong>摘要：</strong> 图像标注一直是视觉理解的重要任务，近期视觉语言模型(VLM)的发展显著提升了图像标注的能力。然而，细节图像标注的评估仍存在不足，主要由于评价指标过时和注释粗糙。本文介绍了DeCapBench及其新指标DCScore，专为细节标注任务设计，DCScore通过将响应拆解为最小的自洽单元（基础信息单元）来评估幻觉现象和细微的全面性。实验表明，DCScore与人类判断的吻合度高于其他评估指标。同时，DeCapBench在描述性任务中的表现与VLM竞技场结果高度相关，超越了现有的视觉语言模型基准。此外，文章还提出了一种自动细粒度反馈收集方法FeedQuill，基于新指标进行偏好优化，展现出强大的泛化能力。通过对多种VLM的广泛实验，证明该方法显著减少了幻觉现象，并在各项基准测试中提升了性能，达到优越的细节图像标注表现，超越了GPT-4o。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07906" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:53:56 GMT</pubDate>
</item>
<item>
<title>Sonata：高效自监督点云学习模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.16429</link>
<guid>https://arxiv.org/abs/2503.16429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示了Sonata模型在3D任务中实现高效自监督学习的潜力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了自监督点云模型Sonata在多样化三维任务中的有效性，尤其在数据有限与计算资源最小的情况下。我们发现现有3D自监督学习方法在表示质量方面存在不足，原因被称为“几何捷径”，它导致表示仅限于低级空间特征。为了解决这一问题，我们提出了两项关键策略：遮蔽空间信息和增强对输入特征的依赖。Sonata模型通过自蒸馏学习构建，具有直观性和简易性，学习到的表示在零样本可视化中显示出语义分组和强大的空间推理能力，在ScanNet上相比以往方法提高了线性探测准确率，从21.8%提升至72.5%，并且在仅使用1%数据的情况下几乎实现了性能翻倍。此外，全面微调进一步提升了多维场景感知任务的最先进性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>基于文本描述的3D世界生成方法SynCity</title>
<link>https://arxiv.org/abs/2503.16420</link>
<guid>https://arxiv.org/abs/2503.16420</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SynCity是一种基于文本描述生成3D世界的高质量方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SynCity的方法，用于从文本描述生成3D世界。与传统的以物体为中心的3D生成模型不同，SynCity结合了预训练的3D生成模型的几何精度和2D图像生成器的艺术性，以无训练和优化的方式创造大型高质量的3D空间。通过瓦片式的生成方法，SynCity实现了对场景布局和外观的细粒度控制，允许逐块生成世界，并将每个新生成的瓦片在已有场景的上下文中融合。这种方法所生成的场景丰富且具有吸引力，展示了细节和多样性，解决了大规模3D世界生成的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16420" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型的事实知识编码能力</title>
<link>https://arxiv.org/abs/2503.15299</link>
<guid>https://arxiv.org/abs/2503.15299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型内部编码的知识超过其外部表达的知识。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种框架，用于评估大型语言模型（LLMs）是否在其参数中编码了比输出中表现出的更多的事实知识。文章定义了知识，并通过正确与错误答案对的比例量化知识，区分了外部知识和内部知识。通过对三款流行的开放权重LLMs进行案例研究，结果显示：首先，LLMs内部编码的知识比其外部表达的知识平均高出40%；其次，有些知识深埋于内部，模型可能完全知道答案，但在生成时却从未输出，这表明LLMs在生成能力上存在根本限制；最后，这为在闭卷问答中的重复答案采样提出了实际限制，因为某些答案几乎从未被采样，从而提高性能的机会被阻碍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15299" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:21:48 GMT</pubDate>
</item>
<item>
<title>PORTAL：一种新型AI框架实现多3D游戏智能代理</title>
<link>https://arxiv.org/abs/2503.13356</link>
<guid>https://arxiv.org/abs/2503.13356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PORTAL框架通过语言引导生成策略，实现AI在多3D游戏中的应用。</p><br /><br /><p><strong>摘要：</strong> PORTAL是一种新型框架，通过语言引导策略生成，开发能够在数千款3D视频游戏中进行决策的人工智能代理。该框架将决策问题转化为语言建模任务，利用大型语言模型（LLMs）生成以领域特定语言（DSL）表示的行为树。PORTAL方法消除了传统强化学习方法的计算负担，同时保留了战术深度和快速适应能力。它引入了一种混合策略结构，将基于规则的节点与神经网络组件相结合，支持高水平的战略推理和精确的低层次控制。此外，双重反馈机制通过量化游戏指标和视觉-语言模型分析促进策略在战术和战略层面的迭代改进。实验结果表明，PORTAL在数千款第一人称射击游戏中的有效性，表现出开发效率、策略泛化和行为多样性方面的显著提升，代表了游戏人工智能开发的重大进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:42:34 GMT</pubDate>
</item>
<item>
<title>TikZero：从图像到图形程序的文本驱动生成</title>
<link>https://arxiv.org/abs/2503.11509</link>
<guid>https://arxiv.org/abs/2503.11509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TikZero利用图像作为中介，将图形程序生成与文本理解解耦。</p><br /><br /><p><strong>摘要：</strong> 随着生成式人工智能的发展，从文本描述合成图形成为一种引人注目的应用。然而，实现高几何精度和可编辑性需要将图形表示为图形程序（如TikZ），而对齐的训练数据（即图形程序与其描述）仍然较为稀缺。为解决这一问题，本文提出了TikZero，通过使用图像表示作为中介，解耦图形程序生成与文本理解。该方法允许独立训练图形程序和带说明图片，并支持在推理阶段进行零样本文本驱动的图形程序合成。实验表明，TikZero在没有对齐的图形程序的情况下显著超越了只能利用对齐图形程序的基准模型。此外，当结合对齐图形程序作为补充训练信号时，TikZero的表现能与更大规模的模型相媲美，甚至超越商业系统如GPT-4o。我们的代码、数据集及部分模型已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:29:58 GMT</pubDate>
</item>
<item>
<title>利用多模态大语言模型评估AI生成视频的有效性</title>
<link>https://arxiv.org/abs/2503.09949</link>
<guid>https://arxiv.org/abs/2503.09949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在AI生成视频评估中的应用及效果。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成模型的快速发展，建立可靠的自动评估指标显得尤为重要。现有方法多依赖于其他任务优化的现成模型或人类评估数据，难以满足快速增长的评估需求。为此，本文探讨了使用多模态大语言模型（MLLMs）作为统一评估器的可行性，借助其强大的视觉感知和语言理解能力。我们提出了UVE-Bench基准，收集了最新VGMs生成的视频，并在15个评估方面提供了人类偏好标注。通过UVE-Bench，我们对16个MLLMs进行了广泛评估，结果表明，尽管先进的MLLMs在效果上仍不及人类评估者，但相比现有专用评估方法，其在统一AIGV评估中显示了显著的潜力。此外，我们深入分析了影响MLLM驱动评估器性能的关键设计选择，为今后的研究提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 21:52:27 GMT</pubDate>
</item>
<item>
<title>基于集合代币化的图像生成新范式</title>
<link>https://arxiv.org/abs/2503.16425</link>
<guid>https://arxiv.org/abs/2503.16425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新图像生成范式，改进了代币表示和分布建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的图像生成范式，通过集合代币化和分布建模实现图像生成。与传统的将图像序列化为固定位置潜在编码的方式不同，本文引入了无序代币集合表示，能够根据区域语义复杂性动态分配编码容量。该TokenSet增强了全局上下文聚合，并提高了对局部扰动的鲁棒性。为了解决建模离散集合的关键挑战，我们设计了一种双重转换机制，将集合双射地转换为具有求和约束的固定长度整数序列。此外，本文提出的固定和离散扩散框架是首个能够同时处理离散值、固定序列长度和求和不变性的模型，能够有效进行集合分布建模。实验结果表明，本文方法在语义感知表示和生成质量上优于传统方法。这些创新的表示和建模策略为视觉生成的发展提供了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>SwD：扩展的扩散模型蒸馏框架</title>
<link>https://arxiv.org/abs/2503.16397</link>
<guid>https://arxiv.org/abs/2503.16397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SwD框架通过逐级预测有效缩短扩散模型的推理时间。</p><br /><br /><p><strong>摘要：</strong> SwD是一个针对扩散模型(DMs)的规模蒸馏框架，灵感来源于扩散过程与隐式谱自回归之间的关系。SwD提出在较低数据分辨率下初始化生成，并在去噪步骤中逐步上升样本质量，而不损失性能，显著降低计算成本。该框架将这一思想自然融入现有基于分布匹配的扩散蒸馏方法中，并通过引入新的补丁损失，增加了分布匹配方法的细粒度相似性。在应用于先进的文本-图像扩散模型时，SwD在仅有两次全分辨率步骤的推理时间内显著超越同等计算预算的其他方法，得到了自动化指标和人类偏好研究的支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:54:02 GMT</pubDate>
</item>
<item>
<title>VidKV：一种新型的低位数KV缓存量化方法用于视频大语言模型</title>
<link>https://arxiv.org/abs/2503.16257</link>
<guid>https://arxiv.org/abs/2503.16257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VidKV方法，通过低位数量化提高VideoLLMs的效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种针对视频大语言模型（VideoLLMs）的KV缓存量化新方法——VidKV。随着视频输入长度的增加，KV缓存的内存需求显著上升，影响推理速度。研究发现，2位KV量化对模型性能影响很小，而更低位数的量化尚未深入探讨。VidKV通过采用混合精度量化策略，对不同行的通道实施不同位数的量化，同时针对性保留语义重要的视觉标记，进而压缩KV缓存至1.5位和1.58位精度，且几乎不影响性能。大量实验表明，VidKV比传统方法在保持精度的同时，提高了缓存效率，为视频分析中的推理速度提升提供了新路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:52:43 GMT</pubDate>
</item>
<item>
<title>机器智能驱动的药物依从性预测与干预系统</title>
<link>https://arxiv.org/abs/2503.16091</link>
<guid>https://arxiv.org/abs/2503.16091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIMI系统通过机器智能促进药物依从性预测与干预。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为AIMI的药物依从性预测系统，它利用智能手机传感器和患者的用药历史来估算患者忘记服药的可能性。研究中，27名每天服用药物管理心血管疾病的参与者参与了用户研究。研究团队设计并开发了基于卷积神经网络（CNN）和长短期记忆网络（LSTM）的预测模型，经过不同输入特征组合的实验发现，LSTM模型在药物依从性预测上的准确率达到了93.2%，F-1得分为93.6%。通过一系列的消融研究，证明了利用未来已知信息和个性化训练显著提高了药物依从性预测的准确性，填补了基于可穿戴传感器的依从性预测系统的空白。这一系统的代码可在GitHub上找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:32:35 GMT</pubDate>
</item>
<item>
<title>VideoRFSplat：一种直接的文本到3D模型生成方法</title>
<link>https://arxiv.org/abs/2503.15855</link>
<guid>https://arxiv.org/abs/2503.15855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoRFSplat通过视频生成模型直接生成高质量3D场景。</p><br /><br /><p><strong>摘要：</strong> VideoRFSplat是一种新的文本到3D生成模型，它利用视频生成模型实现对无界真实场景的3D高斯点云生成。过往方法在2D生成模型与相机姿态和多视图图像的联合建模时面临不稳定性，因此需要额外模型来稳定训练与推理。本文提出了一种双流架构，将专用的姿态生成模型与预训练视频生成模型结合，分别通过独立通道生成多视图图像和相机姿态，从而减少两种模态间的干扰。我们还提出了一种异步采样策略，使相机姿态去噪速度快于多视图图像，从而利用快速去噪的姿态条件多视图生成，减少互相模糊，提高跨模态一致性。在多个大规模真实场景数据集上进行训练后，VideoRFSplat在无需后处理细化的情况下，优于现有的文本到3D生成方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:26:09 GMT</pubDate>
</item>
<item>
<title>BigO(Bench)：评估生成模型理解和生成代码复杂性的基准</title>
<link>https://arxiv.org/abs/2503.15242</link>
<guid>https://arxiv.org/abs/2503.15242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍BigO(Bench)基准，评估生成模型的代码复杂性理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BigO(Bench)，一个新型编码基准，旨在评估生成语言模型理解和生成具有特定时间与空间复杂度代码的能力。该基准填补了当前评估中忽视模型在计算复杂度限制下理解和生成代码能力的空白。BigO(Bench)包括工具，可以从分析测量中推断任何Python函数的算法复杂度，包含3150道编码问题及1190250个标注有复杂度标签的解法，还提供了不同输入规模下的运行时和内存占用值。通过评估多种最先进的语言模型，本文突出了它们在处理复杂度要求方面的优缺点，尤其是token-space推理模型在代码生成上无与伦比，但在复杂性理解方面表现平平，提示其可能无法很好地泛化到未在训练中给予奖励的任务上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:19:57 GMT</pubDate>
</item>
<item>
<title>优化视频训练方法的令牌选择与增强工具Flux</title>
<link>https://arxiv.org/abs/2503.14237</link>
<guid>https://arxiv.org/abs/2503.14237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了Flux工具，优化视频训练中的令牌选择，提升模型性能并节省计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视频训练设置，称为令牌优化，通过从更合适的采样视频中选择输入令牌，最大化有限输入信息，从而提高模型在不同计算预算下的表现。为此，提出了一种名为Flux的创新增强工具，使得采样网格灵活，并能轻松集成到主流视频训练框架中，实现模型的增强而几乎不增加额外成本。在大规模视频预训练中集成Flux后，FluxViT在多个任务上取得了新的最先进的成绩。值得注意的是，仅使用1/4的令牌，FluxViT仍能匹配之前的最先进模型的性能，实现近90%的节省。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 09:15:58 GMT</pubDate>
</item>
<item>
<title>RSD: 一种加速超分辨率扩散模型的新型蒸馏方法</title>
<link>https://arxiv.org/abs/2503.13358</link>
<guid>https://arxiv.org/abs/2503.13358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RSD是一种高效的超分辨率模型蒸馏方法，提升了图像质量和计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的蒸馏方法RSD，用于加速超分辨率扩散模型ResShift，实现了单步图像恢复，且在多个数据集上优于教师模型。尽管现有的加速方法面临着生成不真实细节或幻觉结构的问题，RSD通过训练学生网络生成与教师模型一致的图像，有效克服了这些不足。实验结果表明，RSD的性能与最先进的扩散模型蒸馏方法相当，且在预训练的文本到图像模型基础上的超分辨率方法中也展现出竞争力，具有更好的图像质量、对退化输入图像的对齐效果，并且所需参数和GPU内存更少。我们在多个真实和合成数据集上进行了实验验证，包括RealSR、RealSet65、DRealSR、ImageNet和DIV2K。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:44:08 GMT</pubDate>
</item>
<item>
<title>LLM驱动代理的评估方法综述</title>
<link>https://arxiv.org/abs/2503.16416</link>
<guid>https://arxiv.org/abs/2503.16416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述LLM驱动代理的评估方法，分析其能力与应用基准。</p><br /><br /><p><strong>摘要：</strong> 本文对基于大型语言模型（LLM）的智能代理的评估方法进行了全面综述，探讨了这些代理在动态环境中进行规划、推理及工具使用的能力。文章系统分析了四个关键维度的评估基准与框架，包括基础能力、特定应用的基准（如网络、软件工程、科学和对话代理）、通用代理的基准和评估框架。分析结果揭示了新兴趋势，如向更具挑战性和现实性的评估的转变。本文还指出了当前研究中的关键缺口，尤其是在成本效益、安全性和鲁棒性等方面的评估，以及开发细粒度和可扩展评估方法的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>高效生成多样化户外场景的方法研究</title>
<link>https://arxiv.org/abs/2503.16375</link>
<guid>https://arxiv.org/abs/2503.16375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种高效的户外场景生成方法，以应对多样化挑战。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种生成多样化户外场景的新方法，涵盖从城堡到高楼等多种场景。与室内场景生成侧重于不同的研究目标相比，户外场景生成面临高度变化和快速生成大规模景观的挑战。为此，提出了一种高效的编码方法，将场景块编码为统一的向量集，比以往的空间结构潜变量在压缩和性能上更具优势。此外，还训练了一个显式的扩展模型，能够快速生成无限场景，提升了生成一致性，并通过消除多余的扩散步骤，加速生成过程。为支持这一任务，本文还策划了NuiScene43，这是一个小巧但高质量的场景数据集，经过预处理以便进行联合训练。值得注意的是，经过不同风格场景的训练后，模型能够将乡村住宅和城市摩天大楼等不同环境融合在同一场景中，展示了我们策划过程在异构场景联合训练中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:37:43 GMT</pubDate>
</item>
<item>
<title>小型语言模型的强化学习推理能力提升研究</title>
<link>https://arxiv.org/abs/2503.16219</link>
<guid>https://arxiv.org/abs/2503.16219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习能显著提升小型语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了在资源限制环境下，利用强化学习（RL）提升小型语言模型推理能力的潜力。研究对象为1.5亿参数的模型DeepSeek-R1-Distill-Qwen-1.5B，训练限制为使用4个NVIDIA A40 GPU（每个48 GB VRAM），并在24小时内完成。通过调整Group Relative Policy Optimization（GRPO）算法及精心策划高质量的数学推理数据集，进行了三次实验，结果显示推理能力迅速提升，如AMC23准确率从63%上升至80%，AIME24达到46.7%。相比传统模型几千美元的训练成本，我们的方案仅需420美元，且采用仅7000个样本。然而，长时间训练中出现了优化不稳定和长度限制等挑战。这些发现展示了基于强化学习的微调在小型语言模型中的有效性，提供了一种高性价比的替代方案。我们还提供了代码和数据集，作为开源资源，助力在资源有限的环境下开发具备推理能力的大型语言模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:13:23 GMT</pubDate>
</item>
<item>
<title>Race-DiT: 一种新型混合专家模型在扩散变换器中的应用</title>
<link>https://arxiv.org/abs/2503.16057</link>
<guid>https://arxiv.org/abs/2503.16057</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Race-DiT模型，提升扩散变换器的性能和可扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Race-DiT，一种创新的混合专家（MoE）模型，旨在提高扩散变换器的可扩展性和性能。通过引入灵活的路由策略——Expert Race，模型允许令牌和专家共同竞争，选择最佳候选，以动态分配专家给关键令牌。此外，提出了每层正则化来解决浅层学习的挑战，并引入路由器相似度损失以防止模式崩溃，从而确保更好的专家利用率。通过在ImageNet上进行的广泛实验，验证了该方法的有效性，展示了显著的性能提升和良好的扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16057" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:45:08 GMT</pubDate>
</item>
<item>
<title>SALT: 结合低秩变换的奇异值适应医学图像分割方法</title>
<link>https://arxiv.org/abs/2503.16055</link>
<guid>https://arxiv.org/abs/2503.16055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SALT方法在医学图像分割中有效地适应奇异值以提升性能。</p><br /><br /><p><strong>摘要：</strong> 医学图像分割需要专门设计的模型来捕捉细致的领域特征。尽管大型基础模型提供了灵活性，但细调成本仍是显著障碍。本文提出的SALT方法结合了低秩适应和奇异值分解（SVD）技术，能够选择性地适应影响最大的奇异值，并对剩余子空间进行低秩更新。经过在5个具有挑战性的医学数据集上的评估，SALT比当前先进的PEFT方法（包括LoRA和SVD）提高了2%到5%的Dice系数，同时仅需3.9%的可训练参数，显示出在低资源环境中的强大适应能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:42:41 GMT</pubDate>
</item>
<item>
<title>Zero-1-to-A：提高4D可动画化头像生成质量的方法</title>
<link>https://arxiv.org/abs/2503.15851</link>
<guid>https://arxiv.org/abs/2503.15851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Zero-1-to-A，提高4D头像生成的一致性及质量。</p><br /><br /><p><strong>摘要：</strong> Animatable head avatar generation traditionally需要大量训练数据。为降低数据需求，本文提出Zero-1-to-A方法，通过视频扩散模型合成具有空间和时间一致性的4D头像。该方法迭代构建视频数据集，采用两阶段的渐进式学习：首先进行空间一致性学习，从正面到侧面固定表情；其次进行时间一致性学习，从放松表情到夸张表情固定视角。这种方法显著提高了生成头像的真实感、动画质量和渲染速度。实验结果表明，Zero-1-to-A在多项指标上优于现有的扩散方法，提供了创建栩栩如生的头像的有效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:07:46 GMT</pubDate>
</item>
<item>
<title>MotionStreamer: 基于文本的流式动作生成新框架</title>
<link>https://arxiv.org/abs/2503.15451</link>
<guid>https://arxiv.org/abs/2503.15451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionStreamer框架，解决流式动作生成中的信息损失和错误累积问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了文本条件流式动作生成中的挑战，特别是如何在变长历史动作和输入文本的基础上预测下一步人类姿态。现有方法在流式动作生成中存在不足，如扩散模型被限制在预定义的动作长度，而基于GPT的方法由于离散化的非因果标记化导致响应延迟和错误累积。为了解决这些问题，本文提出了MotionStreamer，一个将连续因果潜在空间引入概率自回归模型的新框架。连续潜在变量减轻了因离散化造成的信息损失，有效减少了长期自回归生成中的错误累积。此外，通过建立当前和历史动作潜在变量之间的时间因果依赖关系，我们的模型充分利用可用信息，实现准确的在线动作解码。实验结果表明，我们的方法在多个方面优于现有方法，包括多轮生成、长期生成和动态动作合成等应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:32:24 GMT</pubDate>
</item>
<item>
<title>DiffMoE：提升扩散模型图像生成能力的新方法</title>
<link>https://arxiv.org/abs/2503.14487</link>
<guid>https://arxiv.org/abs/2503.14487</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffMoE通过专家访问机制优化扩散模型的图像生成性能。</p><br /><br /><p><strong>摘要：</strong> DiffMoE是一种创新的方法，旨在改善扩散模型在不同条件和噪声水平下的图像生成性能。该方法引入了一种批级全局令牌池，使得专家能够在训练中访问全局令牌分布，从而促进专业化的专家行为。此外，DiffMoE还融入了一个动态容量预测器，根据噪声水平和样本复杂性动态分配计算资源。通过全面评估，DiffMoE在ImageNet基准上取得了业界领先的表现，相较于传统的稠密架构，即便激活参数数量相同，其性能显著提升。该方法的有效性不仅限于类条件生成，还扩展到更具挑战性的任务如文本生成图像，展现了其在不同扩散模型应用中的广泛适用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14487" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>多智能体系统中的挑战与解决方案：综合研究与探索</title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究分析了多智能体系统的挑战，并提出解决方案以提升其性能。</p><br /><br /><p><strong>摘要：</strong> 尽管多智能体系统（MAS）的应用热情日益高涨，但与单一智能体框架相比，其在常用基准上的性能提升仍然有限。本研究首次全面分析了MAS面临的挑战，通过对五种流行的MAS框架进行研究，涉及150多个任务和六位专家评分者，识别出14种独特的故障模式并提出一套适用于各种MAS框架的综合分类法。我们将这些故障模式分为三类：规格与系统设计失败、代理间不一致以及任务验证与终止。此外，为了支持可扩展评估，我们将MASFT与LLM作为评判者结合，并提出通过改进代理角色规格和增强编排策略来预防这些故障的两种干预措施。研究结果表明，解决识别出的故障需要更复杂的解决方案，为未来研究提供了明确的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:04:38 GMT</pubDate>
</item>
<item>
<title>基于单幅图像的高保真可动画人类重建模型LHM</title>
<link>https://arxiv.org/abs/2503.10625</link>
<guid>https://arxiv.org/abs/2503.10625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LHM模型实现从单图像快速生成高保真的动画人类。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的模型LHM（大型可动画人类重建模型），旨在从单幅图像中高效重建动画人类。考虑到现有静态人类重建方法对合成数据的依赖和视频方法对捕捉条件的严格要求，LHM利用多模态变换器架构编码人体特征和图像特征，通过注意机制有效地保存服装的几何形状和纹理。模型还采用了头部特征金字塔编码方案，以增强面部身份的保留和细节的恢复。大量实验表明，LHM能在几秒内生成合理的可动画人类，且无需要面部和手部的后处理，显著超越了现有重建方法的准确性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>XAttention: 高效的长上下文Transformer模型稀疏注意力框架</title>
<link>https://arxiv.org/abs/2503.16428</link>
<guid>https://arxiv.org/abs/2503.16428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XAttention框架通过稀疏注意力加速长上下文Transformer模型的推理。</p><br /><br /><p><strong>摘要：</strong> 长上下文Transformer模型（LCTMs）在实际应用中至关重要，但由于注意力的平方复杂性，计算成本高昂。为了解决这一问题，本文介绍了XAttention框架，它通过引入稀疏注意力显著加速Transformer模型的长上下文推理。XAttention的创新之处在于，它发现注意力矩阵中的反对角线值之和可以作为块重要性的有效代理，从而精确识别并修剪不必要的块，实现高稀疏性并显著加快推理速度。通过在RULER、LongBench、VideoMME和VBench等长上下文基准上的评估，XAttention在保持与全注意力相当的准确度的同时，展现了高达13.5倍的注意力计算加速。这些结果彰显了XAttention在真实世界应用中推动块稀疏注意力的实用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>4D Gaussian Splatting的优化与提升</title>
<link>https://arxiv.org/abs/2503.16422</link>
<guid>https://arxiv.org/abs/2503.16422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出4DGS-1K，通过减少冗余提高动态场景的渲染速度与存储效率。</p><br /><br /><p><strong>摘要：</strong> 4D Gaussian Splatting (4DGS) 在动态场景重建方面受到广泛关注，但其存储需求大且渲染速度慢。本文针对发生的时间冗余进行了深入探讨，识别出短生命周期高斯和非活动高斯作为两个主要问题。我们提出了4DGS-1K，能够在现代GPU上以超过1000 FPS的速度运行。为了解决短生命周期高斯问题，我们引入了空间-时间变化评分作为新的剪枝标准，能够有效去除短生命周期高斯，同时鼓励使用生命周期较长的高斯捕捉场景动态。针对非活动高斯问题，我们存储活跃高斯的掩码，显著减少渲染中的冗余计算。与传统的4DGS相比，本文方法在复杂动态场景中实现了41倍的存储减少和9倍的渲染速度提升，同时保持了相似的视觉质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>MagicMotion：精确轨迹控制的视频生成框架</title>
<link>https://arxiv.org/abs/2503.16421</link>
<guid>https://arxiv.org/abs/2503.16421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicMotion是一种新的视频生成框架，实现精确轨迹控制与视觉质量的提升。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成技术的进步，视觉质量和时间一致性显著提高，轨迹可控的视频生成技术应运而生，以实现精确的物体运动控制。然而，传统方法在复杂物体移动和多物体运动控制方面存在局限，导致轨迹遵循不精确、物体一致性差及视觉质量下降。此外，这些方法仅支持单一格式的轨迹控制，限制了其应用场景。为此，我们提出了MagicMotion，一种新颖的图像到视频生成框架，可以通过掩膜、边界框和稀疏框三种条件实现轨迹控制。输入图像和轨迹后，MagicMotion能够无缝地沿定义轨迹动画物体，同时保持物体一致性和视觉质量。同时，我们还推出了MagicData，一个大规模轨迹控制视频数据集，以及一个自动化的注释和过滤管道。此外，我们建立了MagicBench，一个全面的基准评估视频质量和轨迹控制精度。实验表明，MagicMotion在各项指标上超越了以往方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>优化大语言模型的高效推理方法综述</title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统探讨了提升大语言模型推理效率的多种方法。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）在复杂任务中的显著表现，特别是大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1的进步，本研究首次系统性地调查了如何提高LLMs推理效率。我们将现有研究分为几个关键方向，包括：1）基于模型的高效推理，旨在优化完整推理模型为更简洁的模型；2）基于推理输出的高效推理，目标是在推理过程中动态减少推理步骤和长度；3）基于输入提示的高效推理，试图根据输入提示的难度或长度特性提升推理效率。同时，我们还讨论了利用高效数据训练推理模型、小型语言模型的推理能力，及其评估方法和基准测试。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>InfiniteYou：基于扩散变换器的高保真身份保留图像生成框架</title>
<link>https://arxiv.org/abs/2503.16418</link>
<guid>https://arxiv.org/abs/2503.16418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究介绍了InfiniteYou框架，提升了身份保留图像生成的质量与相似度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了InfiniteYou (InfU) 框架，聚焦于使用扩散变换器 (DiTs) 实现灵活且高保真的身份保留图像生成。InfU解决了现有方法在身份相似性、文本与图像对齐度以及生成质量上的不足，核心是InfuseNet，它通过残差连接将身份特征注入到DiT基础模型中，从而增强身份相似性，同时保持图像生成能力。此外，采用了多阶段训练策略，包括合成单人多样本 (SPMS) 数据的预训练和监督微调（SFT），进一步改善本文的文本与图像对齐，提高图像质量，并减轻面部复制粘贴现象。通过广泛实验，InfU实现了最先进的表现，超越了现有基准，且其即插即用的设计确保了与多种现有方法的兼容性，为更广泛的社区做出了重要贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于视觉语言的后训练行动决策模型提升</title>
<link>https://arxiv.org/abs/2503.16365</link>
<guid>https://arxiv.org/abs/2503.16365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新方法，提升视觉语言模型的开放环境行动决策能力。</p><br /><br /><p><strong>摘要：</strong> 开放世界环境中的行动决策近年来受到重视，视觉语言行动（VLA）模型在此领域展示了潜力。尽管先前的研究主要集中于行动后训练，忽视了对基础模型本身的优化，本文提出了一种新方法，即视觉语言后训练的行动（Act from Visual Language Post-Training），通过视觉和语言的自我监督指导来完善视觉语言模型（VLMs）。该方法改善了模型在世界知识、视觉识别和空间定位能力上的表现。基于此后训练模型，我们在Minecraft中获得了首个能够执行1,000多个原子任务的VLA模型，包括制作、熔炼、烹饪、采矿和击杀等。实验表明，在非轨迹任务上的后训练使得模型在多样化原子任务上相比最佳代理基线提升了40%。我们的研究结果也显示，所提方法优于传统模仿学习策略，在Minecraft中实现了最先进的性能。此外，我们已开源代码、模型和数据集，以推动进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:21:58 GMT</pubDate>
</item>
<item>
<title>超分辨率适应的关键指南URAE</title>
<link>https://arxiv.org/abs/2503.16322</link>
<guid>https://arxiv.org/abs/2503.16322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出URAE以解决高分辨率图像生成中的数据和参数效率问题。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像扩散模型的进展，生成高分辨率图像的训练仍面临挑战，特别是在数据和计算资源有限的情况下。本文从数据和参数效率两个关键角度探讨此问题，并提出超分辨率适应的关键指南URAE。研究表明，某些教师模型生成的合成数据可以显著促进训练收敛。当合成数据不可用时，微调权重矩阵的小组件比常用的低秩适配器表现更佳，提供了显著的性能提升。此外，针对采用指导蒸馏的模型，如FLUX，我们发现，在适应过程中禁用分类器无关的指导，即将指导比例设为1，对于实现满意的性能至关重要。大量实验验证了URAE在仅使用3000个样本和2000次迭代的情况下，实现了与FLUX1.1等闭源模型相当的2K生成性能，同时为4K分辨率生成设定了新基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:44:43 GMT</pubDate>
</item>
<item>
<title>FlashVDM：加速3D形状生成的新框架</title>
<link>https://arxiv.org/abs/2503.16302</link>
<guid>https://arxiv.org/abs/2503.16302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlashVDM通过优化VAE和DiT加速3D形状生成，提升效率和质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FlashVDM，一个系统化框架，旨在加速3D形状生成中的VAE和DiT模型。尽管Vecset Diffusion Model (VDM)在生成高分辨率3D形状方面取得了积极进展，但其在高速度生成中仍面临挑战。FlashVDM通过灵活的扩散采样和创新的Progressive Flow Distillation，使得DiT模型可以在仅5个推理步骤内实现可比质量的生成。同时，通过引入自适应KV选择、分层体积解码和高效网络设计，FlashVDM显著降低了VAE的计算复杂度。实验表明，该模型在性能上优于现有的快速3D生成方法，同时在重建和生成方面的推理时间分别缩短超过45倍和32倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:23:44 GMT</pubDate>
</item>
<item>
<title>MathFusion: 跨问题指令合成增强数学推理能力的框架</title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathFusion通过跨问题指令合成显著提升数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型在数学推理上的进展，本文提出MathFusion，一种通过跨问题指令合成来增强数学推理的新框架。MathFusion包含三种融合策略：顺序融合用于建模解决方案依赖关系，平行融合强化概念理解，条件融合则创建上下文感知的选择性问题以提升推理灵活性。通过这些策略生成的新数据集MathFusionQA，使得在多项基准测试中，大语言模型的数学推理能力提高了18.0个百分点，展现了高数据效率，仅需45K额外的合成指令，较传统单一指令的方法有显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16212" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:00:41 GMT</pubDate>
</item>
<item>
<title>基于粗细预测的自回归图像生成模型</title>
<link>https://arxiv.org/abs/2503.16194</link>
<guid>https://arxiv.org/abs/2503.16194</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过粗细预测优化自回归图像生成的模型。</p><br /><br /><p><strong>摘要：</strong> 本文研究了一种优化自回归模型以改进图像生成的方法，特别针对在图像生成中使用大量代码本所引发的复杂性问题。我们发现，具有相似代码字表征的标记对最终生成图像的影响是相似的，这揭示了大型代码本中的显著冗余。基于这一见解，提出了从粗到细（CTF）预测标记的策略，通过为相似标记分配相同的粗标签来简化预测过程。该方法包括两个阶段：第一阶段是自回归模型，顺序预测每个标记的粗标签；第二阶段是辅助模型，基于粗标签同时预测所有标记的细标签。通过在ImageNet上的实验，我们的方法在Inception Score上相较基线取得了平均59分的显著提升，并且在增加推理步骤的情况下仍然实现了更快的采样速度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16194" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:41:29 GMT</pubDate>
</item>
<item>
<title>基于强化学习的少样本分类策略研究</title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了少样本学习中基于强化学习的分类策略，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了少样本条件下多模态大语言模型（MLLMs）分类的微调方案。研究发现，传统的微调方法可能导致严重的过拟合，甚至比零-shot方法表现更差。为此，提出了CLS-RL方法，利用可验证信号作为奖励，对MLLMs进行微调。实验表明，CLS-RL在多个数据集上相较于传统的监督微调（SFT）方法性能更优，且在不同数据集上表现出一定的“免费午餐”现象，暗示强化学习方法有效增强模型的分类基础能力。此外，本文还引入了No-Thinking-CLS-RL方法，强调微调过程中的思维过程可能影响性能，通过减小思维过程的时间，进一步提高模型的效果和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16188" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:37:45 GMT</pubDate>
</item>
<item>
<title>缓解视觉语言模型中的主导模态偏见的BalGrad框架</title>
<link>https://arxiv.org/abs/2503.13834</link>
<guid>https://arxiv.org/abs/2503.13834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出BalGrad框架以减轻视觉语言模型的主导模态偏见。</p><br /><br /><p><strong>摘要：</strong> 视觉语言（VL）模型在多个任务中展现了出色的表现，但常常依赖特定模态进行预测，导致“主导模态偏见”。这种偏见在某一模态受损时显著影响表现。本研究分析了主导模态偏见下模型的行为，并理论上展示了未对齐的梯度或梯度幅度差异如何阻碍损失的平衡收敛。基于这些发现，我们提出了BalGrad框架，旨在减轻主导模态偏见。该方法包括模态间梯度重加权，根据各个模态的贡献调整KL散度的梯度，以及模态间任务梯度投影，以非冲突的方式对齐任务方向。在UPMC Food-101、Hateful Memes和MM-IMDb数据集上的实验结果证实，BalGrad有效减少了模型在预测时对特定模态的过度依赖。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 22:17:41 GMT</pubDate>
</item>
<item>
<title>MagicID: 实现动态丰富且一致身份的视频生成</title>
<link>https://arxiv.org/abs/2503.12689</link>
<guid>https://arxiv.org/abs/2503.12689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MagicID框架，解决视频生成中的身份一致性与动态性问题。</p><br /><br /><p><strong>摘要：</strong> MagicID是一种新框架，旨在直接提升视频生成的身份一致性和动态性，以满足用户偏好。当前方法面临的主要挑战是视频长度过长导致的身份降解，以及训练过程中动态性降低。为了解决这些问题，MagicID构建了成对的偏好视频数据，明确奖励身份与动态特性，替代传统的自重建方法。通过引入混合采样策略，优先利用源自参考图像的静态视频来维持身份，同时使用Frontier采样方法提高生成视频的动态运动质量。实验结果表明，MagicID在多个指标上优于现有方法，成功实现了视频生成中的身份一致与自然动态。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12689" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 19:15:09 GMT</pubDate>
</item>
<item>
<title>三维空间多模态记忆系统M3的设计与应用</title>
<link>https://arxiv.org/abs/2503.16413</link>
<guid>https://arxiv.org/abs/2503.16413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3是一个多模态记忆系统，旨在通过视频源保存静态场景信息。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了三维空间多模态记忆系统（M3），旨在通过视频源保留中等规模静态场景的信息，提升视觉感知能力。M3结合了三维高斯散点技术与基础模型，构建出一种在不同粒度上渲染特征表示的多模态记忆。我们发现了先前特征散点工作中的两个主要挑战：一是存储每个高斯原语的高维特征所面临的计算限制，二是蒸馏特征与基础模型特征之间的信息丢失或不对齐。为了解决这些问题，M3引入了主场景组成部分和高斯记忆注意力的关键组件，提升了训练和推理的效率。我们通过量化评估特征相似性及下游任务，同时进行定性可视化，展示了高斯记忆注意力的像素轨迹。此外，M3还涵盖了各类基础模型的广泛应用，包括视觉-语言模型、感知模型及大型多模态与语言模型，并在四足机器人上展示其在室内场景中的实际应用。值得注意的是，M3首次解决了三维特征蒸馏中的核心压缩挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>CaKE：一种有效的知识编辑方法提升LLM的多跳推理能力</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CaKE方法通过优化推理电路，提升LLM知识更新的多跳推理能力。</p><br /><br /><p><strong>摘要：</strong> 知识编辑（KE）使得在大型语言模型（LLMs）中修改过时或错误的信息成为可能。然而，现有的KE方法在处理依赖于修改知识的多跳推理任务时效果不佳。通过对推理电路的分析，发现当前局部层次KE方法如MEMIT和WISE在有效整合更新信息时存在局限性。本研究提出了一种新方法CaKE（Circuit-aware Knowledge Editing），它通过精心策划的数据，促进模型使用已修改的知识，从而激励模型为新整合的知识发展适当的推理电路。实验结果表明，CaKE在与相关推理任务的准确性和一致性方面表现优于现有KE方法，在MQuAKE数据集中，实现了平均20%的多跳推理准确率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:14:34 GMT</pubDate>
</item>
<item>
<title>Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens</title>
<link>https://arxiv.org/abs/2503.16278</link>
<guid>https://arxiv.org/abs/2503.16278</guid>
<content:encoded><![CDATA[
Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:07:04 GMT</pubDate>
</item>
<item>
<title>Fin-R1：专为金融领域设计的推理大型语言模型</title>
<link>https://arxiv.org/abs/2503.16252</link>
<guid>https://arxiv.org/abs/2503.16252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了金融领域专用的推理大型语言模型Fin-R1。</p><br /><br /><p><strong>摘要：</strong> 当前，推理大型语言模型在多个领域迅速发展，但在复杂金融任务中的能力仍需深入探讨。本文介绍了Fin-R1，这是一种专门为金融领域设计的推理大型语言模型。Fin-R1采用双阶段架构，基于DeepSeek-R1进行了金融推理数据集的提炼与处理。通过监督微调（SFT）和强化学习（RL）训练，Fin-R1在多项金融推理任务中展现出接近DeepSeek-R1的性能，参数规模为70亿。在FinQA和ConvFinQA任务中，其性能达到了当前最优（SOTA），并在其他任务中超越了更大规模的模型。Fin-R1展现出强大的推理和决策能力，为金融领域面临的多种问题提供了解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:46:18 GMT</pubDate>
</item>
<item>
<title>欺骗幽默数据集（DHD）的构建与分析</title>
<link>https://arxiv.org/abs/2503.16031</link>
<guid>https://arxiv.org/abs/2503.16031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了欺骗幽默数据集（DHD），为研究虚假信息中的幽默提供了新资源。</p><br /><br /><p><strong>摘要：</strong> 本文提出了欺骗幽默数据集（DHD），这是一个研究由虚假声明和错误信息衍生的幽默的新资源。在信息泛滥的时代，理解幽默与欺骗之间的关系至关重要。DHD包含从虚假叙述生成的幽默评论，通过ChatGPT-4o模型生成，所有实例均标记了讽刺水平（从1到3）并分类为五种幽默类型：黑色幽默、讽刺、社会评论、文字游戏和荒诞性。该数据集涵盖英语、泰卢固语、印地语、卡纳达语、坦米尔语及其混合变体，是一个有价值的多语言基准。通过推出DHD，我们为分析虚假语境中的幽默奠定了结构化基础，为探索幽默如何与虚假信息的互动及其感知和传播的影响开辟了新的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16031" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:58:02 GMT</pubDate>
</item>
<item>
<title>Unified Variational Auto-Encoder在3D分子生成中的应用</title>
<link>https://arxiv.org/abs/2503.15567</link>
<guid>https://arxiv.org/abs/2503.15567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种统一的变分自编码器用于高效的3D分子生成。</p><br /><br /><p><strong>摘要：</strong> 3D分子生成对药物发现和材料科学至关重要，但处理包括原子类型、化学键和3D坐标在内的复杂多模态是一个重大挑战。现有方法通常为不变和等变模态维持独立的潜在空间，导致训练和采样效率低下。本研究提出统一变分自编码器（UAE-3D），该模型将3D分子压缩为来自统一潜在空间的潜在序列，同时保持接近于零的重建误差。通过Diffusion Transformer进行潜在生成，UAE-3D在GEOM-Drugs和QM9数据集上进行的广泛实验表明，该方法在新分子生成和条件3D分子生成方面显著建立了新的基准，展示了领先的效率和质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15567" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 04:56:13 GMT</pubDate>
</item>
<item>
<title>Cosmos-Reason1模型：启用物理AI的链式推理与决策</title>
<link>https://arxiv.org/abs/2503.15558</link>
<guid>https://arxiv.org/abs/2503.15558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Cosmos-Reason1模型，具备物理常识和决策能力的物理AI系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Cosmos-Reason1模型，旨在使物理AI系统能够理解物理世界，并通过长链推理过程生成适当的自然语言决策。文章定义了物理AI推理的关键能力，重点关注物理常识与赋能推理。研究中构建了一个层次本体框架，用于表示物理常识，包含空间、时间和物理的基本知识，还依赖于一种二维本体以广泛适应不同的物理体现。我们开发了两种多模态大型语言模型Cosmos-Reason1-8B和Cosmos-Reason1-56B，并通过四个阶段（视觉预训练、一般监督微调、物理AI微调及物理AI强化学习）进行训练。为评估模型性能，构建了物理常识与赋能推理的综合基准，结果显示增强学习和细调带来了显著改进。为促进物理AI的发展，相关代码及预训练模型将根据NVIDIA开放模型许可公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 18:06:58 GMT</pubDate>
</item>
<item>
<title>基于多轮交互的新一代强化学习算法SWEET-RL</title>
<link>https://arxiv.org/abs/2503.15478</link>
<guid>https://arxiv.org/abs/2503.15478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了SWEET-RL算法，改进了LLM代理的多轮任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现有多轮强化学习（RL）算法在优化大语言模型（LLM）代理时面临的信用分配问题。为了研究这一课题，我们引入了新基准ColBench，该基准允许LLM代理与人类合作，在后端编程和前端设计中进行多轮互动以解决实际任务。基于这一基准，我们提出了一种新颖的强化学习算法SWEET-RL，它使用经过精心设计的优化目标，训练一个可以访问额外训练时信息的评估模型。该评估模型提供逐步奖励来改进策略模型的表现。实验结果表明，与其他最先进的多轮强化学习算法相比，SWEET-RL在ColBench上实现了6%的绝对成功率和胜率提升，使得Llama-3.1-8B在现实协作内容创作中达到了或超过了GPT4-o的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>SkyLadder：一种优化的上下文窗口调度策略提升大规模语言模型预训练效率</title>
<link>https://arxiv.org/abs/2503.15450</link>
<guid>https://arxiv.org/abs/2503.15450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SkyLadder提出了一种有效的短至长上下文窗口转换策略，提升了大规模语言模型预训练效率和性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，LLM预训练中的上下文窗口不断扩展以处理更长的序列。然而，初步研究发现，使用较短上下文窗口的模型在固定token预算下通常表现更佳。基于此，我们提出了SkyLadder，一种简单而有效的短至长上下文窗口过渡策略，旨在平衡长上下文能力与预训练效率。实验表明，SkyLadder在多个标准基准任务中保持强劲的表现，同时在长上下文任务中也能达到或超越基准结果。我们在100B tokens上预训练了1B参数（最高32K上下文）和3B参数（8K上下文）模型，取得了最高3.7%的性能提升，并比基准方法训练速度提升22%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:31:15 GMT</pubDate>
</item>
<item>
<title>DP-Recon: 使用扩散先验优化3D场景重建</title>
<link>https://arxiv.org/abs/2503.14830</link>
<guid>https://arxiv.org/abs/2503.14830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DP-Recon通过扩散先验和可见性引导方法优化3D物体重建，显著提升几何与外观恢复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法DP-Recon，用于优化3D场景中物体的几何和纹理重建，特别是在稀疏视角下。传统方法虽然引入了语义或几何正则化，但在约束不足的区域会导致重建效果退化，且无法恢复遮挡区域。为解决这一问题，DP-Recon通过Score Distillation Sampling (SDS) 引入扩散先验，提供缺失信息并增强几何与外观恢复。为避免重建与生成指导之间的冲突，进一步引入了可见性引导方法动态调整每个像素的SDS损失权重。大量实验表明，DP-Recon在Replica和ScanNet++数据集上明显优于现有最先进方法，尤其是在仅使用10个视角时，其重建效果超越了其他方法在100个视角下的表现。此外，该方法支持基于文本的几何和外观编辑，生成详细的UV映射，适用于高质量的视觉特效编辑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 22:11:31 GMT</pubDate>
</item>
<item>
<title>LLM-FE: 基于大语言模型的自动化特征工程框架</title>
<link>https://arxiv.org/abs/2503.14434</link>
<guid>https://arxiv.org/abs/2503.14434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM-FE框架结合进化搜索与大语言模型，自动发现有效特征，提升表格学习任务的预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的自动化特征工程框架LLM-FE，通过结合进化搜索与大语言模型（LLM）的领域知识和推理能力，自动发现适用于表格学习任务的有效特征。传统的特征工程方法往往受限于预定义的转化规则和固定的搜索空间，且忽视了领域知识。而现有基于大语言模型的方案要么依赖直接提示，要么单纯依靠验证得分进行特征选择，未能充分利用以往特征发现实验的洞察或建立特征生成与数据驱动性能之间的有意义推理。LLM-FE将特征工程表述为程序搜索问题，LLM通过提出新的特征转化程序并结合数据驱动反馈进行迭代搜索。实验结果表明，LLM-FE在多个分类和回归基准测试中显著优于现有方法，显著提升了表格预测模型的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:11:24 GMT</pubDate>
</item>
<item>
<title>VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity</title>
<link>https://arxiv.org/abs/2503.11557</link>
<guid>https://arxiv.org/abs/2503.11557</guid>
<content:encoded><![CDATA[
Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:26:11 GMT</pubDate>
</item>
<item>
<title>动态解构框架提升长文本验证的准确性</title>
<link>https://arxiv.org/abs/2503.15354</link>
<guid>https://arxiv.org/abs/2503.15354</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出动态解构框架，优化长文本的验证效果。</p><br /><br /><p><strong>摘要：</strong> 当前针对长文本事实性评价的研究中，解构与验证通常被孤立处理，忽视其相互作用及潜在的不一致性。研究发现，现有的解构策略与下游验证器在信息密度这一新颖指标上的不一致性，导致验证结果次优。为此，研究将寻求最佳解构策略与优化验证的过程形式化为一个双层优化问题，并提出了通过验证反馈动态解构的强化学习框架。实验结果显示，该框架在不同验证器、数据集及输入声明的原子性条件下，相较于现有解构策略，验证信心提升了0.07，准确率提高了0.12（在0-1范围内）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15354" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:56:21 GMT</pubDate>
</item>
<item>
<title>MetaLadder：基于类比问题的数学推理框架</title>
<link>https://arxiv.org/abs/2503.14891</link>
<guid>https://arxiv.org/abs/2503.14891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaLadder框架通过类比问题提升大语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MetaLadder框架，该框架旨在提升大语言模型（LLMs）在数学推理任务中的表现。MetaLadder通过鼓励模型回忆和反思与当前问题结构或语义相似的类比问题及其解决方案，来激发推理过程。此外，文章引入了一种问题重述机制，通过再生原始问题以增强模型对目标问题的理解，从而提高推理准确度。大量实验表明，MetaLadder在数学基准测试中显著提高了LLMs的准确性，相比标准的链式思维（CoT）方法，准确率提升达到10.3%。该研究不仅模拟了人类的类比学习能力，还有效改善了模型的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 00:36:35 GMT</pubDate>
</item>
<item>
<title>CURIE基准：评估大语言模型在科学问题解决中的能力</title>
<link>https://arxiv.org/abs/2503.13517</link>
<guid>https://arxiv.org/abs/2503.13517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURIE基准旨在评估大语言模型在科学问题解决中的应用能力。</p><br /><br /><p><strong>摘要：</strong> CURIE基准是一个用于测量大语言模型在科学问题解决能力的基准测试，涵盖材料科学、凝聚态物理、量子计算、地理空间分析、生物多样性和蛋白质等六个学科的580个问题和解决方案对。基准设置了十个具有挑战性的任务，评估了多个闭式和开放式的大语言模型在需要领域专长、长上下文理解和多步骤推理的任务上的表现。实验结果表明，尽管Gemini Flash 2.0和Claude-3在各个领域展现出较高的理解能力，但流行的GPT-4o和command-R+在蛋白质测序任务上表现不佳，最佳表现仅为32%。CURIE基准希望为未来大语言模型在科学领域的开发提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:53:03 GMT</pubDate>
</item>
<item>
<title>KDTalker：结合无监督3D关键点与时空扩散模型的音频驱动人像生成框架</title>
<link>https://arxiv.org/abs/2503.12963</link>
<guid>https://arxiv.org/abs/2503.12963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KDTalker结合3D关键点与时空扩散模型，实现高效音频驱动的人像生成。</p><br /><br /><p><strong>摘要：</strong> KDTalker是一个创新框架，旨在通过结合无监督的隐式3D关键点与时空扩散模型，提升音频驱动单图像人像生成的效果。传统的方法通常分为基于关键点和基于图像的方式，前者虽然能有效保持角色身份，但在捕捉细致面部特征上受限；后者虽能生成高质量的肖像，但存在身份失真和计算成本高的问题。KDTalker通过调整面部信息密度，灵活建模多样的头部姿态，并选用了专门设计的时空注意机制来确保准确的唇同步，从而实现时间一致的高质量动画，并提高计算效率。实验结果表明，KDTalker在唇同步精度、头部姿态多样性和执行效率方面都达到了先进水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:18:31 GMT</pubDate>
</item>
<item>
<title>SynthScars: 高质量合成图像数据集与LEGION图像伪造分析框架</title>
<link>https://arxiv.org/abs/2503.15264</link>
<guid>https://arxiv.org/abs/2503.15264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了SynthScars数据集及其图像伪造检测框架LEGION。</p><br /><br /><p><strong>摘要：</strong> 随着生成技术的快速发展，合成图像的制作变得越来越方便，但也带来了显著的社会忧虑。当前的合成图像检测方法往往缺乏对伪造图像的详尽解释和有效的文本解读。为此，本文提出了SynthScars数据集，包含12,236张完全合成的图像，配有人工专业注释，涵盖四种不同的图像内容类型和三类伪造工件，提供细粒度的像素级分割、详尽的文本解释和工件类别标签。此外，我们提出了LEGION，一个基于多模态大型语言模型的图像伪造分析框架，集成了工件检测、分割和解释功能。实验表明，LEGION在多个基准测试中优于现有方法，特别是在SynthScars数据集上，显著超越第二名传统专家的表现，mIoU提高3.31%，F1分数提高7.75%。在其指导下生成的图像与人类偏好更加一致。代码、模型和数据集将被公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:37:21 GMT</pubDate>
</item>
<item>
<title>提升多模态推理：取向视觉条件化的新方法</title>
<link>https://arxiv.org/abs/2503.13360</link>
<guid>https://arxiv.org/abs/2503.13360</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法，通过视觉条件化提升多模态模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着大型语言模型的发展，其推理能力不断增强。然而，在需要视觉输入的多模态任务中，模型往往难以持续关注视觉信息，导致文本输出过于依赖。为此，研究者们对长链推理中的图像输入进行了消融实验，发现即使在移除图像输入的情况下，模型的准确率仅下降约2%。基于此，提出了“取向视觉条件化”（TVC）策略，该方法优化了图像输入的处理，压缩多余的视觉信息，使模型在推理过程中更好地关注视觉成分。TVC在五个数学推理基准测试中达到了行业领先的性能，相较于之前的最佳结果提高了3.4%，有效提升了多模态推理系统的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13360" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:45:12 GMT</pubDate>
</item>
<item>
<title>φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation</title>
<link>https://arxiv.org/abs/2503.13288</link>
<guid>https://arxiv.org/abs/2503.13288</guid>
<content:encoded><![CDATA[
Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:38:33 GMT</pubDate>
</item>
<item>
<title>统一构建广义知识图谱的框架研究</title>
<link>https://arxiv.org/abs/2503.11227</link>
<guid>https://arxiv.org/abs/2503.11227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一的框架以构建广义知识图谱，提升自然语言处理任务效果。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一个统一的广义知识图谱（GKG）构建框架，包括知识图谱、事件知识图谱和常识知识图谱，这对自然语言处理任务至关重要。目前的研究往往分开构建这些图谱，忽视了它们在计算资源和使用上的潜在统一性。在构建统一框架的过程中，我们解决了任务特定差异带来的挑战。通过从29个数据集中15个子任务收集数据，分类为样本内、反向任务和超出分布（OOD）数据。接着，我们提出了一个三阶段的课程学习微调框架，通过迭代性地将三种图谱的知识注入大型语言模型中。大量实验表明，所提模型在样本内、OOD和反向任务数据上均提升了三种图谱的构建效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 05:23:22 GMT</pubDate>
</item>
<item>
<title>TULIP：提升图像理解的开源CLIP替代模型</title>
<link>https://arxiv.org/abs/2503.15485</link>
<guid>https://arxiv.org/abs/2503.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TULIP是一种新型开源模型，提升了图像理解性能。</p><br /><br /><p><strong>摘要：</strong> TULIP是一个开源的CLIP类模型替代品，旨在解决现有图像-文本对比模型在细粒度图像理解任务中的不足。尽管CLIP和SigLIP在某些任务上取得了成功，但它们通常在图像理解方面表现不佳，特别是在计数、深度估计和细粒度物体识别等视觉任务中。TULIP通过生成数据增强、强化图像间和文本间对比学习以及图像/文本重构正则化来学习细粒度的视觉特征，同时保持全局语义的一致性。该模型拥有超过10亿参数，表现超过现有的最先进模型，并在多个基准测试上取得了新的零-shot性能，显著提升了图像语言模型的表现，提供了高达3倍于SigLIP在MMVP上的得分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>构建3D智能的基础模型：Roblox的探索与设计</title>
<link>https://arxiv.org/abs/2503.15475</link>
<guid>https://arxiv.org/abs/2503.15475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨Roblox如何构建支持3D生成与推理的基础模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Roblox致力于构建一个能够支持3D智能的基础模型，旨在帮助开发者全方位制作Roblox体验，包括生成3D对象、场景、角色绑定及编写程序化脚本。我们讨论了构建该模型的三个关键设计要求，并展示了实现3D形状标记器的初步步骤。我们提出的标记化方案可以用于多种应用，比如文本到形状生成、形状到文本生成，以及文本到场景生成。此外，文章展示了这些应用如何与现有的大型语言模型（LLMs）合作进行场景分析和推理，最后讨论了实现统一的3D智能基础模型的未来发展路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:52:17 GMT</pubDate>
</item>
<item>
<title>FluxFlow: 提升视频生成的时间质量</title>
<link>https://arxiv.org/abs/2503.15417</link>
<guid>https://arxiv.org/abs/2503.15417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出FluxFlow以增强视频生成的时间一致性和多样性。</p><br /><br /><p><strong>摘要：</strong> 时间质量是视频生成中的关键因素，确保帧间运动和动态的连贯性。然而，实现高时间一致性和多样性依然存在挑战。本研究首次探讨了视频生成中的时间增强，提出了一种名为FluxFlow的策略，旨在提升时间质量。FluxFlow在数据层面操作，应用可控的时间扰动，且无需对架构进行修改。在UCF-101和VBench基准上的广泛实验表明，FluxFlow显著提高了各类视频生成模型（包括U-Net、DiT和基于AR的架构）之间的时间一致性和多样性，同时保持了空间的可靠性。这些研究结果凸显了时间增强作为一种简单而有效的方法，可以显著提升视频生成的质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 12:59:32 GMT</pubDate>
</item>
<item>
<title>DeepMesh：优化三维网格生成的框架</title>
<link>https://arxiv.org/abs/2503.15265</link>
<guid>https://arxiv.org/abs/2503.15265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepMesh框架通过深度学习和强化学习优化三维网格的生成和质量。</p><br /><br /><p><strong>摘要：</strong> Triangle meshes在3D应用中至关重要，然而传统的自回归方法在生成结构化网格时常受到面数限制和网格不完整性的限制。为了解决这些问题，我们提出了DeepMesh框架，主要通过两个创新点进行优化：一是引入高效的预训练策略和新颖的标记算法，同时改进数据整理和处理；二是将强化学习引入3D网格生成，通过直接偏好优化实现与人类偏好的对齐。我们设计了一种评分标准，结合人类评估与3D指标，收集偏好对以确保视觉吸引力和几何准确性。在点云和图像的条件下，DeepMesh生成具有复杂细节和精确拓扑的网格，表现在精度和质量上超越了现有的最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:39:30 GMT</pubDate>
</item>
<item>
<title>ELTEX框架：专用领域合成训练数据生成的有效解决方案</title>
<link>https://arxiv.org/abs/2503.15055</link>
<guid>https://arxiv.org/abs/2503.15055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ELTEX框架通过合成数据提升网络安全领域模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ELTEX（高效LLM令牌提取）框架，旨在生成高质量的专用领域合成训练数据。大型语言模型（LLMs）在通用能力上表现优异，但在专用领域如网络安全方面由于缺乏特定培训数据而受到限制。ELTEX通过系统地整合显式领域指示符提取和动态提示，确保在生成过程中保留重要领域知识。我们在区块链相关的网络攻击检测背景下展示了ELTEX的有效性，并通过多种真实数据与ELTEX生成数据的组合微调Gemma-2B模型。实验结果表明，基于ELTEX增强的模型在标准分类指标和不确定性校准方面的性能与GPT-4相当，同时所需计算资源大大减少。我们还发布了一套针对区块链网络攻击检测的社交媒体文本合成数据集。研究表明，专用领域合成数据生成能够有效弥补资源高效模型与大型架构在专业领域的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 05:46:54 GMT</pubDate>
</item>
<item>
<title>基于文本反演的扩散模型个性化量化方法</title>
<link>https://arxiv.org/abs/2503.14868</link>
<guid>https://arxiv.org/abs/2503.14868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种量化扩散模型以实现个性化，显著降低训练内存需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，对扩散模型进行个性化量化，旨在减少训练和微调过程中的内存需求。通过使用文本反演和零阶优化，避免了去量化，降低了梯度计算和反向传播过程中的存储需求。针对个性化场景中单个或少数图像产生的噪声，我们引入了子空间梯度技术，通过构建过去记忆的标记子空间来对梯度进行去噪。此外，研究文本嵌入对图像生成的影响，提出了部分均匀时间步采样策略，以优化扩散时间步。实验表明，该方法在个性化稳定扩散上与先前方法相比，与图像和文本对齐分数相当，同时训练内存需求降低了最多8.2倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 23:45:37 GMT</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>扩展流媒体视频理解的新任务与ViSpeak模型</title>
<link>https://arxiv.org/abs/2503.12769</link>
<guid>https://arxiv.org/abs/2503.12769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新任务——视觉指令反馈，旨在提升用户与代理的互动。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型多模态模型（LMMs）在离线视频理解方面取得了一定进展，但流媒体视频理解面临重大挑战。本文提出了一项新任务，称为视觉指令反馈，模型需识别视觉内容并提取其中的指令，从而提升用户与代理之间的互动。我们定义了七个与视觉模态密切相关的关键子任务，并为研究收集了ViSpeak-Instruct数据集和ViSpeak-Bench评估集。此外，提出的ViSpeak模型在多个流媒体视频理解基准测试中展现了GPT-4o级别的性能。经过在ViSpeak-Instruct数据集上的微调，ViSpeak具备了基本的视觉指令反馈能力，为未来的研究打下了坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 23:05:31 GMT</pubDate>
</item>
<item>
<title>STEVE：高效训练计算机使用代理的步骤验证管道</title>
<link>https://arxiv.org/abs/2503.12532</link>
<guid>https://arxiv.org/abs/2503.12532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STEVE通过步骤验证提升计算机使用代理训练的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了STEVE，一个用于训练计算机使用代理的步骤验证管道，旨在提升代理的训练效率与准确性。首先，我们建立了一个大型指令集，并利用一些次优代理收集了轨迹数据。通过使用GPT-4o，我们对每个轨迹中的步骤进行验证，根据执行前后的屏幕对每一步赋予二进制标签。随后，我们采用Kahneman和Tversky优化方法，根据这些标签来优化代理。实验结果表明，STEVE可以有效利用轨迹中的正负动作，显著优于监督微调。此外，STEVE还使得我们能够训练一个7B的视觉-语言模型，在具有挑战性的实时桌面环境WinAgentArena中实现领先表现，同时大幅降低成本。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12532" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:43 GMT</pubDate>
</item>
<item>
<title>PyGDA：开源图域适配库的发布</title>
<link>https://arxiv.org/abs/2503.10284</link>
<guid>https://arxiv.org/abs/2503.10284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyGDA是一个为图域适配提供的开源Python库，集成多种模型与数据集。</p><br /><br /><p><strong>摘要：</strong> PyGDA是首个全面支持图域适配的开源Python库，旨在促进不同领域之间的知识转移。该库整合了20多种广泛使用的图域适配方法，并支持多种类型的图数据集。PyGDA提供模块化组件，用户可以灵活构建自定义模型，并利用多种实用函数。为处理大型图数据，PyGDA支持采样和小批量处理，确保高效计算。此外，还包括完善的性能基准和用户友好的API文档，以方便研究人员和从业者使用。PyGDA遵循MIT许可证发布，方便用户访问与安装。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 07:52:23 GMT</pubDate>
</item>
<item>
<title>MeshFleet：高质量三维车辆数据集的自动过滤与注释</title>
<link>https://arxiv.org/abs/2503.14002</link>
<guid>https://arxiv.org/abs/2503.14002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MeshFleet数据集，以优化三维生成模型的精度与控制性。</p><br /><br /><p><strong>摘要：</strong> 近年来，生成模型在三维对象领域取得显著进展，但在工程等专业领域的应用仍受限于其精度、质量和可控性的不足。本文提出MeshFleet，一个从Objaverse-XL提取的过滤和注释的三维车辆数据集，以帮助大规模生成模型的微调。研究中，我们建立了一个基于质量分类器的自动过滤管道，分类器通过对Objaverse的手动标注子集进行训练，结合DINOv2和SigLIP嵌入，并通过基于Caption的分析和不确定性估计进行优化。我们通过与基于Caption和图像美学评分的技术进行比较，证明了我们的过滤方法的有效性，并在SV3D的微调实验中强调了针对性数据选择对专业领域三维生成建模的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14002" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 04:09:24 GMT</pubDate>
</item>
<item>
<title>Multi-Scale Attention模型与Atlas架构在大规模图像建模中的应用</title>
<link>https://arxiv.org/abs/2503.12355</link>
<guid>https://arxiv.org/abs/2503.12355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Multi-Scale Attention和Atlas架构，显著提升高分辨率图像建模效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种高效的大规模图像建模方法——多尺度注意力（Multi-Scale Attention, MSA），其核心在于多尺度表示和双向跨尺度通信。MSA通过创建O(log N)尺度来逐步表示图像特征，并借助跨注意力机制在各尺度间传播信息。此外，基于MSA，我们提出了一种新型神经网络架构Atlas，其在高分辨率的ImageNet 100数据集上显著改善了计算性能与长上下文图像建模的权衡。在1024px分辨率下，Atlas-B的准确率达到91.04%，与ConvNext-B（91.92%）相当，但速度快4.3倍。Atlas在与FasterViT和LongViT的比较中性能更强，分别提升了2.95倍和4.96%的准确率。与MambaVision-S比较时，在不同分辨率下，Atlas-S的准确率提升显著，同时运行时间相似。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:52:13 GMT</pubDate>
</item>
<item>
<title>AdaLLaVA：一种自适应的多模态大型语言模型推断框架</title>
<link>https://arxiv.org/abs/2503.10905</link>
<guid>https://arxiv.org/abs/2503.10905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaLLaVA通过动态调整推断操作，优化多模态大型语言模型的效率。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLM）在推理方面展现了出色的能力，但其巨大的计算成本限制了在资源受限环境中的应用。尽管近期在提高MLLM效率方面有所努力，以往的解决方案在应对变化的运行时条件（如设备上其他程序的争用）时显得不足。为此，我们提出了AdaLLaVA，一个自适应推断框架，能够在推理期间根据输入数据和延迟预算动态重构MLLM中的操作。通过在问题回答、推理和幻觉等基准上的广泛实验，我们的结果显示，AdaLLaVA有效遵循输入延迟预算，在运行时实现不同的准确率与延迟权衡。此外，AdaLLaVA能够适应输入延迟和内容，且可与令牌选择技术结合以提升效率，并在不同MLLM中展现出良好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 17:39:38 GMT</pubDate>
</item>
<item>
<title>AudioX：统一的音频与音乐生成模型</title>
<link>https://arxiv.org/abs/2503.10522</link>
<guid>https://arxiv.org/abs/2503.10522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AudioX是一个创新的统一模型，用于音频和音乐生成，具备灵活的自然语言控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了AudioX，一个统一的扩散变换器模型，旨在进行音频和音乐的生成。与以往特定领域的模型不同，AudioX能够以高质量生成通用音频和音乐，同时提供灵活的自然语言控制，能够无缝处理文本、视频、图像、音乐和音频等多种输入模式。其关键创新在于一种多模态遮蔽训练策略，该策略通过遮蔽不同模态的输入，促使模型从遮蔽输入中学习，从而获得强大而统一的跨模态表示。为了应对数据稀缺的问题，研究人员整理了两个全面的数据集：vggsound-caps，包含来自VGGSound数据集的19万条音频描述，以及V2M-caps，基于V2M数据集生成的600万条音乐描述。广泛的实验表明，AudioX不仅匹配或超越了最先进的专业模型，还在处理多样化输入模态和生成任务方面表现出显著的灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:30:59 GMT</pubDate>
</item>
<item>
<title>EvalTree: 生成语言模型弱点档案的创新方法</title>
<link>https://arxiv.org/abs/2503.08893</link>
<guid>https://arxiv.org/abs/2503.08893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍EvalTree方法，通过弱点档案提升语言模型评估与训练。</p><br /><br /><p><strong>摘要：</strong> 理想的模型评估应能识别模型的缺陷并提供改进指导。为此，本文提出了生成语言模型弱点档案的概念，根据模型在基准测试中的表现，形成一套以自然语言表达的弱点集合。我们引入了一系列定量评估方法来比较不同的弱点分析方法，并提出了EvalTree方法，通过构建能力树，识别模型表现不佳的能力，从而生成弱点档案。在MATH和WildChat基准测试中，EvalTree在识别弱点的精准性和全面性上优于其他基线方法。此外，基于EvalTree识别的弱点进行的数据收集和训练，显著提升了语言模型的性能。我们还展示了EvalTree如何揭示Chatbot Arena中基于人投票的评估方式中的缺陷。为促进未来研究，我们还发布了代码及交互接口，供实践者探索由EvalTree构建的能力树。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 17:12:48 GMT</pubDate>
</item>
<item>
<title>CoLMDriver: 基于大语言模型的协作驾驶系统</title>
<link>https://arxiv.org/abs/2503.08683</link>
<guid>https://arxiv.org/abs/2503.08683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLMDriver 提供了一种新型的基于语言模型的 V2V 协作驾驶解决方案。</p><br /><br /><p><strong>摘要：</strong> CoLMDriver 是首个全流程基于大语言模型的协作驾驶系统，旨在通过有效的语言基础协商和实时驾驶控制来改善车辆间安全性。该系统包含两个主要组件：基于演员-评论者范式的 LLM 协商模块，持续通过车辆间的反馈优化合作策略；以及意图导向的路径点生成器，将协商结果转化为可执行的路径点。此外，文章还引入了 InterDrive，一个包含10个复杂交互驾驶场景的 CARLA 基准测试，用于评估 V2V 的合作能力。实验结果表明，CoLMDriver 在多种高度互动的 V2V 驾驶场景中，成功率较现有方法提高了11%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:58:42 GMT</pubDate>
</item>
<item>
<title>自我提升认知框架：构建下一代多模态大型语言模型</title>
<link>https://arxiv.org/abs/2503.12303</link>
<guid>https://arxiv.org/abs/2503.12303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SIcog框架，通过自我生成数据提升多模态大型语言模型的认知能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大型语言模型（MLLMs）具备出色的能力，但在细致感知和复杂推理方面仍面临挑战。现有的多模态预训练方法主要通过训练高质量的图像描述提升感知能力，而收集思维链（CoT）推理数据的成本极高。本文提出了一种自我学习框架——自我提升认知（SIcog），旨在通过自生成数据的多模态预训练增强MLLMs的系统性认知能力。通过引入逐步描述（Chain-of-Description）方法，SIcog增强了模型的系统性感知，实现更全面和准确的理解，同时采用结构化的CoT推理技术，促进MLLMs进行深度多模态推理。实验表明，仅用213K自生成的预训练样本，SIcog能够构建出认知显著提升的下一代基础MLLMs，并在多个基准上实现领先表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12303" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 20:25:13 GMT</pubDate>
</item>
<item>
<title>小规模高质量数据集提升大型语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2503.13661</link>
<guid>https://arxiv.org/abs/2503.13661</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过小规模双语数据集提高大型语言模型的推理和法语能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大型语言模型（LLMs）中，通过在小规模高质量的双语（英法）数据集上进行战略性细致调整，以提升推理能力和法语语言熟练度的有效方法。研究假设，通过数据的有针对性策划和优化训练，可以实现竞争甚至优于传统依赖大规模数据集的表现。结果显示，通过仅对2,000个精心挑选的样本进行有监督的细致调整，Pensez 7B模型在AIME25基准测试上的准确率提升了20%，在法语MATH 5级基准上的提升为12%。这些发现挑战了大型语言模型推理能力依赖巨型数据集的普遍假设，指出了战略数据策划和优化细致调整在提升特定技能及多语言能力方面的潜力。研究结果为在资源有限的情况下高效开发表现优异的多语言LLMs提供了重要启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13661" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:09:11 GMT</pubDate>
</item>
<item>
<title>FlexWorld: 从单幅图像生成灵活视角3D场景</title>
<link>https://arxiv.org/abs/2503.13265</link>
<guid>https://arxiv.org/abs/2503.13265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexWorld框架能够从单幅图像生成高质量的灵活视角3D场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FlexWorld，一个新颖的框架，旨在从单幅图像生成灵活视角的3D场景，包括360度旋转和缩放。FlexWorld包含两个主要组件：一是强大的视频到视频（V2V）扩散模型，能够从粗略场景的缺失输入中生成高质量的新视图图像；二是逐步扩展过程，以构建完整的3D场景。通过利用先进的预训练视频模型和准确的深度估计训练对，V2V模型在大幅相机姿态变化下生成新视图。FlexWorld通过几何感知场景融合，逐步生成新的3D内容并将其整合到全球场景中。大量实验表明，FlexWorld在多个流行指标和数据集上实现了高质量新视图视频和灵活视角3D场景的生成，视觉质量优于现有的最新方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:18:38 GMT</pubDate>
</item>
<item>
<title>提升3D空间理解能力的多模态大型语言模型研究</title>
<link>https://arxiv.org/abs/2503.13111</link>
<guid>https://arxiv.org/abs/2503.13111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个新数据集和基准，以提升多模态语言模型的3D空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大型语言模型（MLLMs）在3D空间推理上的局限性，引入了一个包含高质量3D场景数据的新 supervised fine-tuning 数据集以及一个新的评估基准，专注于室内场景。我们开发的Cubify Anything VQA（CA-VQA）数据集涵盖了多种空间任务，包括空间关系预测、度量大小及距离估计和3D定位。通过利用CA-VQA，我们训练了MM-Spatial，这是一种强大的通用型MLLM，已在包括我们的新基准在内的多个3D空间理解基准上实现了最先进的性能。研究表明，结合度量深度和多视图输入可以进一步提升3D理解能力，且仅通过数据，我们的模型在深度感知能力上达到了与专用单目深度估计模型相当的效果。我们计划发布我们的SFT数据集和基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 08:34:22 GMT</pubDate>
</item>
<item>
<title>基于超曲率空间的安全意识视觉-语言模型HySAC的提出</title>
<link>https://arxiv.org/abs/2503.12127</link>
<guid>https://arxiv.org/abs/2503.12127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法HySAC，通过超曲率空间提高视觉-语言模型的安全内容识别能力。</p><br /><br /><p><strong>摘要：</strong> 本研究针对视觉-语言模型如CLIP在处理安全内容时的不足，引入了一种新方法HySAC（Hyperbolic Safety-Aware CLIP），通过超曲率空间的层级特性来提升模型对安全与不安全内容的意识，避免了传统去遗忘技术的限制。我们提出将安全和不安全内容编码为蕴含层级，并在超曲率空间中将其置于不同区域。HySAC利用蕴含损失函数，建模安全与不安全图像-文本对之间的层级和不对称关系，从而赋予模型对不安全内容的识别能力，使其不仅可作为多模态的不安全分类器，还能灵活地对不安全查询进行重定向或保留原输出。本研究通过广泛实验验证了该方法在安全识别和内容审核可解释性方面的有效性和适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 09:18:04 GMT</pubDate>
</item>
<item>
<title>Florenz: 单语视觉语言模型在多语种任务中的系统性泛化研究</title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究单语视觉语言模型在多语种任务中的表现及泛化规律。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Florenz，一个单语视觉语言模型(VLM)，在多语种任务中的系统性泛化能力，分析模型大小和训练样本的影响。Florenz结合了预训练VLM Florence-2和大型语言模型Gemma-2，参数范围从0.4B到11.2B不等，利用一个故意语言覆盖不全的合成数据集进行训练。研究表明，间接学习未见任务-语言对遵循缩放规律，且Florenz模型即便在仅提供翻译任务数据时，仍能在特定语言中展现出图像描述能力。通过对多种下游数据集的微调，Florenz在多模态机器翻译、词汇消歧和图像描述等任务上表现出竞争力，展现出很好的缩放趋势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09443" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:41:10 GMT</pubDate>
</item>
<item>
<title>Concat-ID：统一的身份保持视频生成框架</title>
<link>https://arxiv.org/abs/2503.14151</link>
<guid>https://arxiv.org/abs/2503.14151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Concat-ID是一个通过自注意力机制实现身份保持视频生成的框架。</p><br /><br /><p><strong>摘要：</strong> Concat-ID是一个新提出的框架，旨在实现身份保持的视频生成。该框架采用变分自编码器提取图像特征，并在序列维度上将其与视频潜在特征拼接，完全依赖于3D自注意力机制，无需其他模块。通过引入新颖的跨视频配对策略和多阶段训练方案，Concat-ID有效平衡了身份一致性与面部可编辑性，同时提高了视频的自然性。大量实验表明，Concat-ID在单一和多重身份生成方面均优于现有方法，并且能够无缝扩展到多主体场景，包括虚拟试穿和背景可控生成，确立了身份保持视频合成的新基准，为广泛应用提供了灵活可扩展的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的对齐算法综述</title>
<link>https://arxiv.org/abs/2503.14504</link>
<guid>https://arxiv.org/abs/2503.14504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统评审了多模态大语言模型的对齐算法及其应用。</p><br /><br /><p><strong>摘要：</strong> 本文对多模态大语言模型(MLLMs)的对齐算法进行了全面的系统评审，探讨了四个关键方面：首先，涵盖了对齐算法的应用场景，包括一般图像理解、多图像处理、视频和音频等扩展多模态应用；其次，讨论了构建对齐数据集的核心因素，如数据来源、模型响应和偏好注释；第三，评估对齐算法的基准测试；最后，讨论了对齐算法未来发展的潜在方向。通过本研究，旨在帮助研究人员整理该领域的最新进展，激励更优秀的对齐方法的开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>AI系统任务完成时间的新度量与未来展望</title>
<link>https://arxiv.org/abs/2503.14499</link>
<guid>https://arxiv.org/abs/2503.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出50%-任务完成时间指标 quantifies AI与人类能力的差异。</p><br /><br /><p><strong>摘要：</strong> 尽管AI在基准测试中取得快速进展，但基准性能的现实意义仍不明确。为量化AI系统的能力与人类能力的关系，本文提出了一种新指标：50%-任务完成时间。这是指人类在完成AI模型能以50%成功率完成的任务时所需的时间。我们记录了领域专家在RE-Bench、HCAST等任务中的完成时间。目前前沿AI模型如Claude 3.7 Sonnet的50%时间范围约为50分钟。自2019年以来，前沿AI的时间范围大约每七个月翻倍，预计在2024年这一趋势可能加速。这一增长主要得益于AI模型更强的可靠性、对错误的适应能力，以及更好的逻辑推理和工具使用能力。我们讨论了研究结果的局限性及其外部有效性，并探讨了自主性增加对危险能力的影响。如果这些结果能够推广到现实软件任务，预测未来五年内AI系统将能够自动化许多当前需要一个月人力完成的软件任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>Temporal Consistency for LLM Reasoning Process Error Identification</title>
<link>https://arxiv.org/abs/2503.14495</link>
<guid>https://arxiv.org/abs/2503.14495</guid>
<content:encoded><![CDATA[
Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:58:28 GMT</pubDate>
</item>
<item>
<title>Cosmos-Transfer: 基于多模态输入的条件世界生成模型</title>
<link>https://arxiv.org/abs/2503.14492</link>
<guid>https://arxiv.org/abs/2503.14492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cosmos-Transfer模型基于多模态输入实现灵活的世界生成与控制。</p><br /><br /><p><strong>摘要：</strong> Cosmos-Transfer是一种基于条件的世界生成模型，能够根据多种空间控制输入（如分割、深度和边缘信息）生成世界模拟。该模型的设计具备自适应和可定制的空间条件方案，可以在不同空间位置对不同的条件输入进行不同的加权，进而实现高度可控的世界生成。Cosmos-Transfer在多个世界到世界的转移场景中具有重要应用，包括Sim2Real。我们对该模型进行了广泛评估，并展示了其在机器人Sim2Real和自动驾驶数据增强等物理AI应用中的实践价值。此外，我们还提出了一种推断缩放策略，使其能够在NVIDIA GB200 NVL72机架上实现实时世界生成。为加速该领域的研究发展，我们公开了相关模型和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:54 GMT</pubDate>
</item>
<item>
<title>Creation-MMBench: 评估多模态大语言模型创意能力的新基准</title>
<link>https://arxiv.org/abs/2503.14478</link>
<guid>https://arxiv.org/abs/2503.14478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Creation-MMBench 旨在评估多模态大语言模型的创意能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Creation-MMBench，一个针对多模态大语言模型（MLLMs）创意能力的评估基准。该基准包含765个测试案例，涵盖51个细分任务，旨在实现对 MLLMs 在现实图像任务中的创意解决方案生成能力的系统评估。为了确保评估的严谨性，本文为每个测试案例定义了特定的评估标准，以指导对响应质量和与视觉输入的一致性进行评估。实验结果表明，当前开源的 MLLMs 在创意任务中远不及商业模型。同时，分析结果显示，视觉微调可能对基础LLM的创意能力产生负面影响。Creation-MMBench 为推动 MLLM 的创造力提升提供了有价值的见解，并为未来在多模态生成智能领域的发展奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:51:34 GMT</pubDate>
</item>
<item>
<title>开放源码的大规模强化学习系统提升LLM推理能力</title>
<link>https://arxiv.org/abs/2503.14476</link>
<guid>https://arxiv.org/abs/2503.14476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DAPO算法，开源大规模RL系统，助力LLM推理能力提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出了去耦合剪辑与动态采样策略优化（DAPO）算法，并完全开源了一种大规模强化学习（RL）系统，使用Qwen2.5-32B基础模型在AIME 2024中取得了50分的优异成绩。与以往隐瞒训练细节的工作不同，本文介绍了四项关键技术，使得大规模LLM的RL训练取得成功。此外，开放源码的训练代码基于verl框架，并配有经过精心策划和处理的数据集。这些组件的开源旨在提高可重复性，并支持未来在大规模LLM RL领域的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:49:06 GMT</pubDate>
</item>
<item>
<title>DeepPerception: 融合认知视觉感知的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2503.12797</link>
<guid>https://arxiv.org/abs/2503.12797</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepPerception，通过知识密集型视觉定位提升多模态理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepPerception，一个增强认知视觉感知能力的多模态大型语言模型（MLLM），旨在解决当前MLLM在视觉感知和推理中的不足。通过引入知识密集型视觉定位任务（KVG），DeepPerception结合了精细的视觉感知和领域特定的知识集成。我们构建了一个自动化数据合成管道，以生成高质量的训练样本，并采用了两阶段的训练框架，实现了认知推理和强化学习的结合。为了评估性能，我们推出了KVG-Bench数据集，其中包含来自10个领域的1.3K手动策划的测试案例。实验结果表明，DeepPerception在KVG-Bench上相比直接微调提高了8.08%的准确率，并在跨领域泛化上优于基线方法4.60%。这些发现强调了将认知过程整合进MLLM的重要性，为多模态推理研究开辟了新方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12797" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 00:06:34 GMT</pubDate>
</item>
<item>
<title>CapArena：评估视觉语言模型在图像描述中的表现</title>
<link>https://arxiv.org/abs/2503.12329</link>
<guid>https://arxiv.org/abs/2503.12329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究评估了视觉语言模型在图像描述中的表现及自动评测的可靠性。</p><br /><br /><p><strong>摘要：</strong> 图像描述在视觉语言研究中一直是一个长期挑战。随着大规模语言模型（LLMs）的崛起，现代视觉语言模型（VLMs）能够生成详细的图像描述。本研究通过创建CapArena平台，进行超过6000对图像描述的对比评估，揭示了领先模型如GPT-4o在性能上达到了或超过了人类水平，而大多数开源模型则相对落后。此外，研究分析了自动评测标准是否能可靠评估描述质量，结果表明传统的评测指标存在系统性偏差，导致模型评分不一致，而VLM-as-a-Judge在描述和模型层面表现出卓越的辨别能力。基于这些发现，我们推出了CapArena-Auto，这是一个准确且高效的自动化基准，能以较低成本实现与人工评分94.3%的相关性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 22:56:09 GMT</pubDate>
</item>
<item>
<title>Reflect-DiT：用于文本到图像生成的推理时间扩展</title>
<link>https://arxiv.org/abs/2503.12271</link>
<guid>https://arxiv.org/abs/2503.12271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Reflect-DiT通过反思能力提升文本到图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种新的文本到图像生成方法Reflect-DiT，该方法通过在上下文中引入反思能力，改进了基于最佳选项采样的推理时间扩展策略。与传统的best-of-N采样方法不同，Reflect-DiT允许Diffusion Transformers使用之前生成图像的示例和文本反馈来细化其生成结果。实验表明，Reflect-DiT在GenEval基准上显著提升了性能，得分达到0.81，超越了使用更大模型（SANA-1.5-4.8B）在最佳选择方法下获得的0.80成绩，同时生成的样本数量仅为20，显示出其在生成效率和效果上的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 17:58:12 GMT</pubDate>
</item>
<item>
<title>RWKV-7 "Goose" with Expressive Dynamic State Evolution</title>
<link>https://arxiv.org/abs/2503.14456</link>
<guid>https://arxiv.org/abs/2503.14456</guid>
<content:encoded><![CDATA[
We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:31:05 GMT</pubDate>
</item>
<item>
<title>IPV-Bench：评估生成与理解不可能视频的新基准</title>
<link>https://arxiv.org/abs/2503.14378</link>
<guid>https://arxiv.org/abs/2503.14378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出IPV-Bench，评估视频生成与理解模型对不可能视频的处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着合成视频在应对数据不足和增加多样性方面的广泛应用，传统数据集主要集中于真实场景，未能深入探讨不可能、反事实和反现实的视频概念。本文旨在回答两个问题：一是当前视频生成模型能否有效地创建不可能的视频内容；二是现有视频理解模型是否足够出色以理解这些不可能的视频。为此，我们提出了IPV-Bench，一个新颖的基准，旨在评估和促进视频理解与生成的进展。IPV-Bench基于一个全面的分类法，涵盖了4个领域和14个类别，包含了违反物理、生物、地理或社会法律的多样场景。此外，设计了一个提示套件用于评估视频生成模型的创造力和遵循提示的能力，同时开发了一个视频基准以测试视频LLMs对理解不可能视频的能力，特别是需要对时间动态和世界知识进行推理的能力。综合评估揭示了视频模型的局限性和未来的研究方向，为下一代视频模型奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 12:10:24 GMT</pubDate>
</item>
<item>
<title>Frac-Connections：一种新型的深度学习连接方法</title>
<link>https://arxiv.org/abs/2503.14125</link>
<guid>https://arxiv.org/abs/2503.14125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Frac-Connections通过分割隐藏状态优化深度学习模型，显著提高了性能。</p><br /><br /><p><strong>摘要：</strong> 在现代深度学习架构中，残差连接是关键，帮助训练非常深的网络以减少梯度消失的问题。最近，超连接通过引入不同深度的多个连接强度来概括残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。本文提出了Frac-Connections，这是一种新颖的方法，通过将隐藏状态分割成多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少内存消耗。我们的实验在语言任务上进行了大规模测试，其中最大的模型是一个7B MoE模型，训练了高达3万亿个标记，结果表明Frac-Connections显著优于传统的残差连接。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 06:37:50 GMT</pubDate>
</item>
<item>
<title>Infinite Mobility：一种生成高保真关节物体的新方法</title>
<link>https://arxiv.org/abs/2503.13424</link>
<guid>https://arxiv.org/abs/2503.13424</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种程序生成方法，合成高质量关节物体用于增强体态AI任务。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Infinite Mobility，这是一种通过程序生成技术合成高保真关节物体的新方法。现有的关节物体生成方法多为数据驱动或基于模拟，受限于训练数据的规模与质量或模拟的高成本与繁琐性。通过用户研究和量化评估，作者证明该方法在物理属性和网格质量上均超越了现有的最先进方法，且与人类标注的数据集相当。此外，作者还展示了合成数据可用作生成模型的训练数据，从而为后续的规模扩展提供支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13424" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:53:56 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型安全性的机器去学习基准研究</title>
<link>https://arxiv.org/abs/2503.12545</link>
<guid>https://arxiv.org/abs/2503.12545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了评估多模态大语言模型机器去学习的新基准PEBench。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视觉问答、视觉理解和推理等任务中取得了显著进展。然而，这一进展依赖于大量互联网上收集的数据，给隐私和安全性带来了重大挑战。为了解决这些问题，机器去学习（MU）作为一种有前景的解决方案应运而生，能够在不需从头再训练的情况下，从已训练模型中移除特定知识。尽管MU在MLLMs中引起了关注，但其评估仍不全面，相关问题定义模糊，制约了更安全、值得信赖系统的开发。为此，本文提出了一个基准PEBench，包含个人实体和一般事件场景的数据集，旨在全面评估MU在MLLMs中的性能。通过PEBench，我们建立了一个标准化、严谨的框架，以推动安全与隐私保护的多模态模型研究。我们对6种MU方法进行了评测，揭示了这些方法的优缺点，并指出了MU在MLLMs中面临的关键挑战与机遇。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 11:26:20 GMT</pubDate>
</item>
<item>
<title>MPBench：评估过程级奖励模型的多任务基准</title>
<link>https://arxiv.org/abs/2503.12505</link>
<guid>https://arxiv.org/abs/2503.12505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPBench是一个评估过程级奖励模型在多种推理场景中有效性的基准。</p><br /><br /><p><strong>摘要：</strong> 推理能力是大型语言模型（LLMs）解决复杂任务的关键，而过程错误的识别对于提高这种能力至关重要。为此，提出了过程级奖励模型（PRMs）以提供逐步奖励，进而增强强化学习和数据生成，帮助LLMs在推理时朝着正确步骤引导，提升推理准确性。然而，现有的PRMs基准测试多为基于文本，主要聚焦于错误检测，忽视了推理搜索等其他场景。为填补这一空白，我们引入了MPBench，一个全面的多任务多模态基准，旨在系统性评估PRMs在不同场景中的有效性。MPBench采用三种评估范式，分别针对PRMs在推理过程中的特定角色进行评估：步骤正确性、答案聚合和推理过程搜索。通过这些范式，MPBench能够全面评估PRMs，并为多模态PRMs的发展提供见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12505" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 09:50:38 GMT</pubDate>
</item>
<item>
<title>KUDA：集成动态学习与视觉提示的开放词汇操控系统</title>
<link>https://arxiv.org/abs/2503.10546</link>
<guid>https://arxiv.org/abs/2503.10546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KUDA系统融合动态学习与视觉提示，实现更复杂的机器人操控。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）和视觉-语言模型（VLMs）的快速发展，开放词汇机器人操控系统取得了显著进展。然而，许多现有方法忽视了物体动态的影响，限制了它们在复杂动态任务中的应用。本文介绍了KUDA，一个开放词汇操控系统，通过关键点集成动态学习与视觉提示，运用VLMs和基于学习的神经动态模型。KUDA首先根据语言指令和视觉观察对RGB图像进行关键点分配，并查询VLM生成目标规范。这些抽象的基于关键点的表示随后被转换为成本函数，并通过学习的动态模型进行优化，以生成机器人轨迹。我们在各种操控任务上评估了KUDA，展示了其在自由形式语言指令、多物体交互以及可变形或颗粒状物体处理中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:59:17 GMT</pubDate>
</item>
<item>
<title>RoCo-Sim：提升路边协同感知的新模拟框架</title>
<link>https://arxiv.org/abs/2503.10410</link>
<guid>https://arxiv.org/abs/2503.10410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoCo-Sim模拟框架显著提升路边协同感知性能，解决数据问题。</p><br /><br /><p><strong>摘要：</strong> 路边协同感知是一种多个路边单元协作收集感知数据的系统，旨在提升车辆的环境意识。现有方法主要关注模型设计，却忽视了数据问题，如校准错误和稀疏信息，这导致在新的数据集上表现不佳。为了解决这些问题，本文提出了首个模拟框架RoCo-Sim，能够通过动态前景编辑和全场景风格转移生成多样且视角一致的路边数据。RoCo-Sim由四个组件构成：摄像机外部优化、多视角遮挡感知采样器（MOAS）、深度SAM模型以及可扩展后处理工具包，有效改善路边3D物体检测性能，较现有最先进方法提升83.74和83.12的AP70。RoCo-Sim填补了路边感知模拟的关键空白，相关代码和预训练模型即将发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:33:42 GMT</pubDate>
</item>
<item>
<title>Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models</title>
<link>https://arxiv.org/abs/2503.06269</link>
<guid>https://arxiv.org/abs/2503.06269</guid>
<content:encoded><![CDATA[
Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:29:45 GMT</pubDate>
</item>
<item>
<title>可扩展的开源视频基础模型训练管道</title>
<link>https://arxiv.org/abs/2503.12964</link>
<guid>https://arxiv.org/abs/2503.12964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了一种新的视频基础模型训练管道，旨在提高视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了视频基础模型（VFM）在模拟现实世界、训练物理人工智能系统及开发创意视觉体验方面的潜力，并指出训练高质量VFMs面临的挑战。为此，作者提出了一种可扩展的开源VFM训练管道，基于NVIDIA NeMo，提供加速的视频数据集整理、多模态数据加载及并行化的视频扩散模型训练和推理。此外，文章还提供了综合性能分析，阐述了高效VFM训练和推理的最佳实践，旨在为相关领域的研究者和开发者提供参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:19:12 GMT</pubDate>
</item>
<item>
<title>GenStereo：高质量立体图像生成的新方法</title>
<link>https://arxiv.org/abs/2503.12720</link>
<guid>https://arxiv.org/abs/2503.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenStereo通过扩散方法实现高质量立体图像生成。</p><br /><br /><p><strong>摘要：</strong> 立体图像在扩展现实、自动驾驶和机器人技术等多个领域至关重要，但高质量立体图像的获取仍面临挑战。现有的方法通常只关注视觉质量或几何准确性，而未能兼顾二者。为此，我们提出了一种新的扩散基础方法——GenStereo，旨在填补这一空白。该方法的两个主要创新在于：其一，通过差异感知坐标嵌入和变形输入图像对扩散过程进行条件化，从而实现比以往方法更精确的立体对齐；其二，采用自适应融合机制，将扩散生成的图像与变形图像智能结合，提高真实性和差异一致性。经过对11个多样立体数据集的全面训练，GenStereo显示出强大的泛化能力，并在立体图像生成和无监督立体匹配任务上取得了领先的性能。本框架消除了对复杂硬件的需求，使得高质量的立体图像生成变得可行，并在实际应用和无监督学习场景中具有重要价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 21:19:28 GMT</pubDate>
</item>
<item>
<title>WISA: 提升文本生成视频模型物理理解的新框架</title>
<link>https://arxiv.org/abs/2503.08153</link>
<guid>https://arxiv.org/abs/2503.08153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISA框架提升文本到视频生成模型的物理理解与生成能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了WISA（世界模拟助手）框架，该框架旨在解决当前文本到视频（T2V）生成模型在理解物理原则方面的不足。通过将物理原则分解为文本描述、定性类别和定量属性，WISA有效地将这些物理属性嵌入生成过程中，提升模型的物理意识。此外，本文提出了新的视频数据集WISA-32K，包含32,000个视频，覆盖17个物理定律，适用于动力学、热力学和光学三个物理领域。实验结果显示，WISA框架显著提高了T2V模型与现实物理法则的兼容性，并在VideoPhy基准测试中取得了显著进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 04:10:03 GMT</pubDate>
</item>
<item>
<title>SPIN-Bench: 评估战略规划与社会推理的新基准</title>
<link>https://arxiv.org/abs/2503.12349</link>
<guid>https://arxiv.org/abs/2503.12349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIN-Bench是一个全新基准，用于评估AI的战略规划和社会推理能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种新颖的多领域评估工具SPIN-Bench，旨在测量AI在战略规划和社会推理方面的智能。与现有的狭窄任务基准不同，SPIN-Bench将经典的PDDL任务、竞争棋盘游戏、合作卡牌游戏和多智能体谈判场景集成到一个统一框架中。该框架通过系统变化行动空间、状态复杂性和互动代理数量，模拟多种社会环境，以考验AI的推理和战略行为。实验结果表明，当代大型语言模型在基本事实检索和短期规划方面表现良好，但在需进行深层次多跳推理和社交协调的不确定任务中遇到显著瓶颈。SPIN-Bench被设想为未来多智能体规划、社会推理及人机合作研究的催化剂。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:10:53 GMT</pubDate>
</item>
<item>
<title>Sightation：提升视觉障碍者图表描述的模型评估与数据集</title>
<link>https://arxiv.org/abs/2503.13369</link>
<guid>https://arxiv.org/abs/2503.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过Sightation提供视觉障碍者友好的图表描述数据集。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了盲人和低视觉者（BLV）在图表描述需求与评估中的挑战。研究发现，目视标注者在直接生成图表描述时，往往易受偏见影响且不符合BLV标准。因此，本研究采用由视觉语言模型（VLM）生成的图表描述，并邀请目视个体进行评估，而非生成。评估结果被证明对专业的BLV教育者尤为有效，这些教育者为视觉障碍学习者提供指导。为此，我们发布了Sightation数据集，涵盖5000个图表及137,000个样本，支持完成、偏好、检索、问答和推理等多种训练目的，并展示其在多个下游任务中的微调潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:52:46 GMT</pubDate>
</item>
<item>
<title>基于人类指令的混杂物品抓取任务研究</title>
<link>https://arxiv.org/abs/2503.13082</link>
<guid>https://arxiv.org/abs/2503.13082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨如何利用视觉-语言模型进行机器人抓取。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在零样本设置下，如何利用视觉-语言模型（VLMs）来执行复杂的混杂物品抓取任务。针对机器人抓取的挑战，提出了一种新方法FreeGrasp，该方法利用预训练的VLMs进行人类指令的推理，同时处理物体的空间关系。通过将所有物体检测为关键点并在图像上进行标注，FreeGrasp能够帮助GPT-4o进行空间推理，以确定是否可以直接抓取目标物体或需要先抓取其他物体。由于缺乏专门的数据集，我们引入了合成数据集FreeGraspData，通过扩展MetaGraspNetV2数据集，加入人工注释的指令和真实抓取序列。研究结果表明，FreeGrasp在抓取推理和执行方面达到了领先水平，并通过搭载抓取器的机器人臂进行了实际验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:41:16 GMT</pubDate>
</item>
<item>
<title>多模态链条思维推理的系统性综述</title>
<link>https://arxiv.org/abs/2503.12605</link>
<guid>https://arxiv.org/abs/2503.12605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统回顾了多模态链条思维推理的现状与挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态链条思维（MCoT）推理在多模态大型语言模型（MLLMs）中得到了显著关注。现有研究设计了多种方法和创新推理范式，以应对图像、视频、语音、音频、3D及结构化数据所带来的独特挑战，并在机器人技术、医疗保健、自动驾驶及多模态生成等领域取得了广泛成功。然而，MCoT领域仍面临诸多独特的挑战与机遇，需要进一步关注。为填补这一领域的空白，本文首次系统性地调查了MCoT推理，阐明了相关的基础概念和定义，并从多角度对当前的方法论进行了综合分类和深入分析。此外，本文还提供了对现有挑战的见解和未来研究方向的探讨，旨在促进多模态通用人工智能（AGI）的创新发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 14:39:13 GMT</pubDate>
</item>
<item>
<title>LVAS-Agent: 长视频音频合成的新框架</title>
<link>https://arxiv.org/abs/2503.10719</link>
<guid>https://arxiv.org/abs/2503.10719</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVAS-Agent 通过多代理协作提升长视频音频合成的效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出LVAS-Agent，一种新型多代理框架，旨在解决长视频音频合成中的难点，如动态语义变化和时间错位。该方法通过专业配音流程的模拟，将长视频合成分为四个步骤：场景分割、脚本生成、声音设计和音频合成。核心创新包括场景/脚本细化的讨论修正机制，以及用于时间语义对齐的生成-检索循环。此外，我们还推出了LVAS-Bench，这是第一个包含207个专业策划长视频的基准数据集，涵盖多样场景。实验结果表明，相比现有基线方法，LVAS-Agent在音频与视觉的匹配度上优于其他方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10719" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 03:58:23 GMT</pubDate>
</item>
<item>
<title>BlobCtrl：精确灵活的元素级视觉内容生成与编辑框架</title>
<link>https://arxiv.org/abs/2503.13434</link>
<guid>https://arxiv.org/abs/2503.13434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlobCtrl框架通过概率性Blob表示实现精确的元素级视觉内容编辑。</p><br /><br /><p><strong>摘要：</strong> BlobCtrl是一个新框架，旨在通过概率性Blob表示统一元素级生成与编辑。该框架利用Blob作为视觉原语，有效分离并表征空间位置、语义内容和身份信息，从而实现精确的元素级操作。其主要贡献包括：1) 采用双分支扩散架构与分层特征融合，实现前景与背景的无缝集成；2) 采用自监督训练范式，结合定制的数据增强和评分函数；3) 使用可控的dropout策略，平衡生成内容的保真度和多样性。此外，项目还推出了BlobData用于大规模训练，以及BlobBench用于系统评估。实验结果表明，BlobCtrl在多种元素级操作任务中表现优异，同时保持了计算效率，提供了一种实用的视觉内容生成与编辑解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:05 GMT</pubDate>
</item>
<item>
<title>提升视频生成的时空一致性研究</title>
<link>https://arxiv.org/abs/2503.06053</link>
<guid>https://arxiv.org/abs/2503.06053</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了视频生成中的时空一致性问题及相应解决方案。</p><br /><br /><p><strong>摘要：</strong> 时空一致性是视频生成中的一个重要研究主题，生成的视频段落需要保证情节的合理性和连贯性，同时在不同视角下维持对象与场景的视觉一致性。以往的研究多集中于时间或空间一致性，忽视了二者之间的综合影响。为解决这一问题，本文提出并探讨了整体时空一致性，关注情节发展与摄影技巧之间的协同效应及之前内容对后续生成的长期影响。研究涉及数据集的构建与模型的开发，首先构建了DropletVideo-10M数据集，该数据集包含1000万条视频，展示了动态相机动作和对象行为，并对每个视频进行了平均206字的注释，详述了不同的摄像机移动和情节发展。接下来，开发并训练了DropletVideo模型，在视频生成过程中出色地保持时空一致性。DropletVideo数据集和模型现已开放访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06053" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 23:37:38 GMT</pubDate>
</item>
<item>
<title>VideoMind：一个新的视频语言代理用于时间基础的视频理解</title>
<link>https://arxiv.org/abs/2503.13444</link>
<guid>https://arxiv.org/abs/2503.13444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoMind是一个用于时间基础视频理解的创新性视频语言代理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoMind，一个旨在提升视频理解的创新视频语言代理。VideoMind通过两个核心创新实现了对视频时间推理的深入理解。首先，它识别了视频时间推理所需的关键能力，并开发了一个基于角色的代理工作流程，包括角色协调的规划者、时间定位的基础角色、评估时间间隔准确性的验证者和回答用户问题的回答者。其次，提出了一种新颖的Chain-of-LoRA策略，通过轻量级LoRA适配器实现无缝的角色切换，避免了多模型的负担，平衡了效率与灵活性。通过对14个公共基准的广泛实验，VideoMind在多项视频理解任务中表现出色，尤其是在3个基础视频问答、6个视频时间基础和5个通用视频问答任务中达到了最先进的性能，证明了其在视频代理和长时空推理方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>WideRange4D: 一种针对大范围空间运动的4D重建基准与方法</title>
<link>https://arxiv.org/abs/2503.13435</link>
<guid>https://arxiv.org/abs/2503.13435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新4D重建基准WideRange4D及相应方法Progress4D，支持大范围空间运动的场景重建。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于具有显著物体空间运动的4D场景重建，提出了新的4D重建基准WideRange4D，该基准包含丰富的4D场景数据，能够评估4D生成方法在大范围空间变化下的能力。现有4D重建技术在处理动态物体的广泛空间运动时面临挑战，常依赖变形场，但变形场在大范围动作中表现不佳。为了克服这一限制，本文还介绍了一种新型4D重建方法Progress4D，能够在多种复杂的4D场景重建任务中生成稳定、高质量的结果。通过在WideRange4D上进行定量和定性比较实验，我们证明了Progress4D在性能上优于现有的最先进4D重建方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13435" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>MicroVQA：生物医学研究中的多模态视觉问答基准</title>
<link>https://arxiv.org/abs/2503.13399</link>
<guid>https://arxiv.org/abs/2503.13399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MicroVQA是针对生物医学研究的多模态视觉问答基准，提升科学推理能力。</p><br /><br /><p><strong>摘要：</strong> MicroVQA是一个针对生物医学研究的视觉问答（VQA）基准，旨在评估科学研究中至关重要的三种推理能力：专家图像理解、假设生成和实验提案。该基准包含1,042道由生物专家策划的多项选择题，涵盖多种显微镜成像方式，确保问题与实际科学实践相关。在构建基准过程中，研究发现标准的多项选择题生成方法容易导致语言简化，因此提出了一种新的两阶段流程来优化过程。记分卡评估显示，现有的多模态大型语言模型（MLLMs）在MicroVQA测评中的最高表现为53%，小型LLMs略低于顶级模型，表明语言推理通常低于多模态推理的挑战。此外，通过科学文献调优可提高模型性能。专家分析结果表明，视觉感知错误最为频繁，其次是知识错误和过度概括错误。这些发现突显了在科学推理中的多模态挑战，MicroVQA为推动AI驱动的生物医学研究提供了重要资源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:33:10 GMT</pubDate>
</item>
<item>
<title>Edit Transfer: 基于少量示例的非刚性图像编辑技术</title>
<link>https://arxiv.org/abs/2503.13327</link>
<guid>https://arxiv.org/abs/2503.13327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的Edit Transfer方法，通过少量示例实现非刚性图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的编辑转移（Edit Transfer）设置，模型能够从单一的源-目标示例中学习变换，并将其应用于新的查询图像。虽然基于文本的方法在语义操作上表现优异，但在精准的几何细节上（如姿态和视角变化）常常显得力不从心。与此不同，基于参考的编辑通常侧重于风格或外观，无法有效处理非刚性变换。Edit Transfer通过显式学习源-目标对的编辑变换克服了文本驱动和外观中心参考的限制。我们提出了一种视觉关系上下文学习的范式，基于DiT的文本到图像模型，构建了编辑示例与查询图像的统一四面复合体，并通过轻量级的LoRA微调捕捉复杂的空间变换。尽管只使用42个训练样本，Edit Transfer在多样的非刚性场景中显著超越了现有的TIE和RIE方法，展示了少样本视觉关系学习的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:04:44 GMT</pubDate>
</item>
<item>
<title>通过奖励增强的生成方法提升文本到图像生成控制能力</title>
<link>https://arxiv.org/abs/2503.13070</link>
<guid>https://arxiv.org/abs/2503.13070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出R0方法，通过奖励优化提升文本到图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 生成与复杂文本提示和人类偏好对齐的图像是人工智能生成内容(AIGC)中的核心挑战。研究表明，当条件更具体、奖励信号更强时，奖励在生成中的主导力量将逐渐超越扩散损失。为了验证这一假设，本文提出了R0，一种通过规范化奖励最大化的全新条件生成方法。R0将图像生成视为数据空间中的优化问题，旨在寻找具有高组合奖励的有效图像，而非依赖复杂的扩散耗损。通过创新的生成器参数化设计和适当的正则化技术，我们在大规模上训练了最先进的少步骤文本到图像生成模型。研究结果挑战了传统的拓展后训练和条件生成观念，展示了在复杂条件情境下奖励的主导作用。希望这些发现能为人本及以奖励为中心的生成范式的进一步研究提供借鉴。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:21:43 GMT</pubDate>
</item>
<item>
<title>Step-wise Group Relative Policy Optimization提升多语言大型模型的推理能力</title>
<link>https://arxiv.org/abs/2503.12937</link>
<guid>https://arxiv.org/abs/2503.12937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过新的在线强化学习框架提升多语言大型模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过设计一种新的在线强化学习框架——逐步群体相对策略优化（StepGRPO），提升多语言大型模型（MLLMs）的推理能力。传统的方法往往让模型被动地模仿成功的推理路径，而StepGRPO则鼓励模型通过简单、有效且密集的逐步奖励机制自我改进。为此，本文引入了两种新颖的基于规则的推理奖励：逐步推理准确性奖励（StepRAR）和逐步推理有效性奖励（StepRVR）。StepRAR通过软密钥步骤匹配技术奖励包含必要中间推理步骤的推理路径，而StepRVR则通过推理完整性和逻辑评估策略奖励遵循合理结构和逻辑一致性的推理路径。通过StepGRPO，本文开发了一系列具有优异逐步推理能力的MLLMs（R1-VL），并在八个基准测试上进行了广泛实验，结果显示该方法的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 04:51:44 GMT</pubDate>
</item>
<item>
<title>DreamRenderer：增强图像生成的实例控制</title>
<link>https://arxiv.org/abs/2503.12885</link>
<guid>https://arxiv.org/abs/2503.12885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRenderer通过创新的方法提升了图像合成中的实例控制能力。</p><br /><br /><p><strong>摘要：</strong> DreamRenderer是一种基于FLUX模型的无训练方法，旨在提升图像合成中用户对多个实例内容的控制能力。本文介绍了两个关键创新：一是桥接图像令牌用于硬文本属性绑定，确保T5文本嵌入在联合注意力过程中正确绑定每个实例的视觉属性；二是仅在关键层应用硬图像属性绑定，通过识别FLUX中负责实例属性渲染的关键层，确保精确控制的同时保持图像质量。评估结果表明，DreamRenderer在COCO-POS和COCO-MIG基准测试中相比FLUX提升了17.7%的图像成功率，并且提高了GLIGEN和3DIS等布局到图像模型的性能，最高提升可达26.8%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 03:30:16 GMT</pubDate>
</item>
<item>
<title>基于扩散变换器的个性化图像生成新方法</title>
<link>https://arxiv.org/abs/2503.12590</link>
<guid>https://arxiv.org/abs/2503.12590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于扩散变换器的训练-free个性化图像生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的个性化图像生成方法，利用扩散变换器(DiTs)的潜力，实现用户指定概念的图像生成和灵活编辑。尽管最近的无训练方法在计算效率上优于基于训练的方法，但在身份保留和适用性方面仍存在挑战。我们提出的‘Personalize Anything’框架，通过时间步自适应的令牌替换和补丁扰动策略，实现了个性化图像生成，支持布局引导生成、多主体个性化和掩码控制编辑。评估结果表明，该方法在身份保留和多样性方面表现出色，提供了高效的个性化解决方案，推动了对扩散变换器的新理解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>Being-0: 一种高效的人形机器人自主代理框架</title>
<link>https://arxiv.org/abs/2503.12533</link>
<guid>https://arxiv.org/abs/2503.12533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-0框架将高层次认知与低层次技能结合，实现高效的人形机器人任务执行。</p><br /><br /><p><strong>摘要：</strong> Being-0是一个层次化的自主代理框架，旨在将高层次认知与低层次技能有效整合，以实现人形机器人在真实环境中的自主操作。该框架结合了基础模型（FM）和模块化技能库，FM负责理解指令、任务规划和推理，而技能库则提供稳定的运动和灵巧的操作。为了解决不同层次之间的协调问题，Being-0引入了一个名为Connector的模块，该模块利用轻量级的视觉-语言模型（VLM）将语言计划转换为可执行的技能指令，并动态协调行走和操作，从而提高任务成功率。通过在大规模室内环境中的广泛实验，Being-0展示了其在复杂长周期任务中的有效性，能够克服具有挑战性的导航和操控子任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:53 GMT</pubDate>
</item>
<item>
<title>视觉语言模型中的基本水平分类研究</title>
<link>https://arxiv.org/abs/2503.12530</link>
<guid>https://arxiv.org/abs/2503.12530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视觉语言模型如何体现人类的基本水平分类行为。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了心理学中基本水平分类的概念，并研究了两个开放源代码的视觉语言模型（VLMs）在此分类中的表现。研究发现，Llama 3.2 Vision Instruct（11B）和Molmo 7B-D模型均偏好与人类行为一致的基本水平分类。此外，模型偏好表现出与生物-非生物基本水平效应及专家基本水平转变等人类复杂行为一致的特点，进一步表明这些视觉语言模型在训练过程中获取了来自人类数据的认知分类行为。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12530" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:50:54 GMT</pubDate>
</item>
<item>
<title>量化大语言模型不确定性以增强用户信任</title>
<link>https://arxiv.org/abs/2503.12528</link>
<guid>https://arxiv.org/abs/2503.12528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨多种不确定性度量对人类行为的相关性，以提高模型控制和用户信任。</p><br /><br /><p><strong>摘要：</strong> 文章研究了多种不确定性度量，以识别与人类群体不确定性相对应的度量方法。研究发现，贝叶斯度量和一种名为top-k熵的熵变体在模型大小变化下，能够更好地与人类行为一致。尽管一些强度量在模型规模增大时与人类表现的相似性下降，但通过多元线性回归分析，结合多种不确定性度量的方法在减小规模依赖性的同时，仍能实现与人类行为的良好对齐。这项研究为增强模型的可控性和用户对模型的信任提供了理论支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:45:43 GMT</pubDate>
</item>
<item>
<title>强化奖励模型的鲁棒性研究</title>
<link>https://arxiv.org/abs/2503.11751</link>
<guid>https://arxiv.org/abs/2503.11751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨奖励模型的鲁棒性及其过拟合现象。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了现代自然语言处理中的奖励模型，特别是其鲁棒性与过拟合现象。尽管最新的奖励模型在标准基准上表现优异，作者指出这可能部分源于过拟合，影响了对模型真实能力的理解。为此，团队构建了reWordBench，系统性地对奖励模型输入进行意义或排名保持的变换，结果显示即使是微小的输入变换也能导致模型性能显著下降，展现出脆弱性。为改善这一问题，作者提出通过显式训练模型为同义句赋予相似分数的方法，结果表明该方法不仅提高了模型对同义句的鲁棒性，还增强了对其他不同类型变换的适应能力。最终，经过增强训练的奖励模型在对齐任务中展示出更好的实用性，与标准训练模型相比，在59%的情况下能生成更高质量的输出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>视频时空推理基准V-STaR的提出及视频大语言模型评估</title>
<link>https://arxiv.org/abs/2503.11495</link>
<guid>https://arxiv.org/abs/2503.11495</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出V-STaR基准以评估视频模型的时空推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨视频大语言模型（Video-LLMs）是否能在视频中进行顺序时空推理，现有的基准主要评估对象的存在而忽视了关系推理，无法有效测量模型对对象互动的理解。为此，本文引入了视频时空推理基准（V-STaR），通过反向时空推理任务（RSTR），同时评估对象的存在、事件的发生时机和空间位置，并捕捉人类认知的思维链逻辑。为支持此评估，构建了一个数据集，通过半自动化的GPT-4管道生成粗细结合的思维链问题，嵌入明确的推理链。实验结果显示，当前的视频大语言模型在稳健和一致的时空推理能力方面存在显著差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11495" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:21:44 GMT</pubDate>
</item>
<item>
<title>MTV-Inpaint：统一多任务视频修复框架</title>
<link>https://arxiv.org/abs/2503.11412</link>
<guid>https://arxiv.org/abs/2503.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MTV-Inpaint是一个统一的视频修复框架，兼具场景填充和物体插入能力。</p><br /><br /><p><strong>摘要：</strong> 视频修复涉及修改视频中的局部区域，确保时空一致性。现有方法主要侧重于场景填充，缺乏对新对象可控插入的能力。而最近的发展在文本引导视频(T2V)扩散模型方面为视频修复提供了新的思路。针对当前方法的局限，我们提出了MTV-Inpaint，一个统一的多任务视频修复框架，能够同时处理传统的场景填充和新对象插入任务。通过设计双分支空间注意机制，MTV-Inpaint在单一框架中实现了这两种任务的无缝整合。同时，MTV-Inpaint支持通过整合多个图像修复模型的图像到视频(I2V)修复模式，增强了多模态控制能力。此外，我们提出的两阶段管道结合了关键帧修复与中间帧传播，使得MTV-Inpaint能够有效处理长达数百帧的视频。大规模实验表明，MTV-Inpaint在场景填充和物体插入任务中均表现出色，展示了其在多模态修复、物体编辑、去除及处理长视频的广泛应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 09:54:10 GMT</pubDate>
</item>
</channel>
</rss>