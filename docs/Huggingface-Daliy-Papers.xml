<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>基于对比学习的链式思维强化微调方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.15868</link>
<guid>https://arxiv.org/abs/2508.15868</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出一种新方法提升LLM推理性能。</p><br><br><p><strong>摘要：</strong> 本文针对大语言模型（LLM）在推理能力上的不足，提出了一种基于标注链式思维（CoT）的对比学习强化微调方法（CARFT）。该方法通过学习每个CoT的表示，并设计新的对比信号来指导微调过程，解决了传统强化学习方法在训练过程中不稳定、模型崩溃以及过度依赖标注CoT的问题。实验表明，该方法在多个基准数据集和模型上均表现出显著的性能提升，最高可达10.15%，同时提升了训练效率，最高达30.62%。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.15868 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 20:20:47 GMT</pubDate>
<pubDate>Wed, 20 Aug 2025 20:20:47 GMT</pubDate>
</item>
<item>
<title>基于自我对弈与变分问题生成的强化学习方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.14029</link>
<guid>https://arxiv.org/abs/2508.14029</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>SvS策略提升RLVR训练中模型推理性能与多样性。</p><br><br><p><strong>摘要：</strong> 本文研究了基于可验证奖励的强化学习（RLVR）在大语言模型中的应用，指出传统方法虽能提升Pass@1性能，但会降低策略熵，影响生成多样性。为解决这一问题，作者提出在线自我对弈与变分问题生成（SvS）策略，通过利用正确解生成变分问题来维持策略熵，从而显著提升Pass@k性能。实验表明，该方法在多个推理基准测试中表现优异，尤其在AIME24和AIME25上分别提升了18.3%和22.8%。SvS在不同规模的模型中均展现出良好的泛化性和鲁棒性。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.14029 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 13:42:45 GMT</pubDate>
<pubDate>Tue, 19 Aug 2025 13:42:45 GMT</pubDate>
</item>
<item>
<title>基于稀疏自编码器的持久概念遗忘方法CRISP</title>
<link>https://arxiv.org/abs/2508.13650</link>
<guid>https://arxiv.org/abs/2508.13650</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>CRISP实现持久概念遗忘，提升模型安全性。</p><br><br><p><strong>摘要：</strong> 随着大型语言模型在现实场景中的广泛应用，如何选择性移除有害知识同时保持模型功能成为关键问题。本文提出CRISP方法，利用稀疏自编码器（SAE）实现对目标概念的持久遗忘。该方法通过自动识别多层中显著的SAE特征并抑制其激活，有效移除了有害知识，同时保留了模型的通用能力和领域内性能。实验表明，CRISP在WMDP基准测试中优于现有方法，实现了语义上一致的概念分离。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.13650 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 05:01:22 GMT</pubDate>
<pubDate>Tue, 19 Aug 2025 05:01:22 GMT</pubDate>
</item>
<item>
<title>Learnable SMPLify：基于神经网络的3D人体姿态估计方法</title>
<link>https://arxiv.org/abs/2508.13562</link>
<guid>https://arxiv.org/abs/2508.13562</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Learnable SMPLify提升3D人体姿态估计效率与泛化能力。</p><br><br><p><strong>摘要：</strong> 本文提出Learnable SMPLify，一种将SMPLify的迭代优化过程替换为单次回归模型的神经框架。该方法通过时间采样策略构建初始化-目标对，并引入人体中心归一化和残差学习以提高泛化能力。实验表明，该方法在运行速度上比SMPLify快近200倍，且在多个数据集上表现良好，支持序列推理和插件后处理。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.13562 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 02:53:57 GMT</pubDate>
<pubDate>Tue, 19 Aug 2025 02:53:57 GMT</pubDate>
</item>
<item>
<title>EgoTwin：联合生成第一视角视频与人体运动的框架</title>
<link>https://arxiv.org/abs/2508.13013</link>
<guid>https://arxiv.org/abs/2508.13013</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>EgoTwin解决第一视角视频与人体运动生成难题。</p><br><br><p><strong>摘要：</strong> 本文提出EgoTwin，一个基于扩散Transformer架构的联合视频与人体运动生成框架。该框架解决了两个关键挑战：视角对齐和因果交互。通过引入以头部为中心的运动表示和受控制论启发的交互机制，EgoTwin能够生成与人体动作自然匹配的第一视角视频。研究团队还构建了一个大规模的真实世界数据集，并设计了新的评估指标来衡量视频与运动的一致性。实验结果表明，该框架在相关任务中表现出色。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.13013 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 11:33:09 GMT</pubDate>
<pubDate>Mon, 18 Aug 2025 11:33:09 GMT</pubDate>
</item>
<item>
<title>ODYSSEY：面向复杂环境的移动操作框架研究</title>
<link>https://arxiv.org/abs/2508.08240</link>
<guid>https://arxiv.org/abs/2508.08240</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出ODYSSEY框架提升机器人长时序操作能力。</p><br><br><p><strong>摘要：</strong> 本文针对移动机器人在复杂环境中进行长期任务规划与操作的挑战，提出了ODYSSEY框架。该框架结合高层任务规划与低层全身控制，解决感知受限、操作范围有限及环境多变等问题。通过引入基于视觉语言模型的分层规划器，实现指令分解与精确执行，并在不同地形中实现稳健控制。研究还构建了首个长时序移动操作基准测试，验证了系统在真实环境中的泛化能力和鲁棒性，展示了腿式机械臂在非结构化场景中的实用性。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.08240 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:54:31 GMT</pubDate>
<pubDate>Mon, 11 Aug 2025 13:54:31 GMT</pubDate>
</item>
<item>
<title>基于CLIP的弱监督可操作性定位方法研究</title>
<link>https://arxiv.org/abs/2508.07877</link>
<guid>https://arxiv.org/abs/2508.07877</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出一种基于CLIP的弱监督可操作性定位方法。</p><br><br><p><strong>摘要：</strong> 本文研究如何通过弱监督方式实现物体可操作性定位。传统方法依赖分类器和知识蒸馏策略，但常忽视与可操作性无关的常见特征。为解决此问题，本文引入选择性原型和像素对比目标，在不同粒度信息下自适应学习可操作性相关线索。通过CLIP模型在第一人称和第三人称视角中识别动作相关对象，并交叉验证获取精确的部分级可操作性线索，有效区分可操作区域与背景。实验结果表明该方法具有良好的效果。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2508.07877 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 07:49:37 GMT</pubDate>
<pubDate>Mon, 11 Aug 2025 07:49:37 GMT</pubDate>
</item>

<item>
<title>AetherCode：评估大型语言模型代码能力的新基准</title>
<link>https://arxiv.org/abs/2508.16402</link>
<guid>https://arxiv.org/abs/2508.16402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AetherCode提升LLM代码评估的准确性和挑战性。</p><br /><br /><p><strong>摘要：</strong> 文章指出当前评估大型语言模型（LLMs）代码能力的基准存在不足，未能真实反映模型与顶尖程序员之间的差距。为解决这一问题，作者提出了AetherCode，一个基于国际信息学奥林匹克竞赛（IOI）和国际大学生程序设计竞赛（ICPC）的新型基准。AetherCode不仅包含更具挑战性的题目，还通过自动化生成与人工验证相结合的方式构建了高质量的测试用例，从而提供更可靠、严谨的评估方式，推动代码推理研究的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 10:04:55 GMT</pubDate>
</item>
<item>
<title>基于语言指令的机器人任务处理框架研究</title>
<link>https://arxiv.org/abs/2508.16292</link>
<guid>https://arxiv.org/abs/2508.16292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出IVA框架提升机器人对错误指令的理解与响应能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉-语言-动作（VLA）模型在处理包含错误前提的自然语言指令时的表现，并提出了Instruct-Verify-and-Act（IVA）框架。该框架能够检测无法执行的指令，进行语言澄清或修正，并在感知和行动中提供合理替代方案。通过构建大规模指令调优数据集，训练出能处理准确与错误请求的VLA模型。实验表明，IVA在错误前提检测准确率上比基线提升了97.56%，并在错误场景下的成功响应率提高了50.78%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 06:54:33 GMT</pubDate>
</item>
<item>
<title>AgentScope 1.0：支持高效工具交互的智能代理框架</title>
<link>https://arxiv.org/abs/2508.16279</link>
<guid>https://arxiv.org/abs/2508.16279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentScope 1.0 提升了智能代理的工具交互能力，优化了开发体验。</p><br /><br /><p><strong>摘要：</strong> AgentScope 1.0 是一个面向智能代理应用的新型框架，旨在提升代理在现实任务中的表现。它通过抽象关键组件、提供统一接口和可扩展模块，使开发者能够更便捷地利用最新模型和技术。该框架基于 ReAct 模式，并采用异步设计增强交互效率与多样性。此外，AgentScope 集成了针对特定场景的内置代理、可视化评估工具以及运行时沙箱，提升了开发效率与安全性，为构建可扩展、适应性强的智能代理应用提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 06:35:56 GMT</pubDate>
</item>
<item>
<title>基于记忆的在线强化学习实现LLM代理的持续适应</title>
<link>https://arxiv.org/abs/2508.16153</link>
<guid>https://arxiv.org/abs/2508.16153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需微调的LLM代理新方法，提升持续学习能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型的自适应大型语言模型（LLM）代理学习范式，无需对底层LLM进行微调。该方法通过记忆增强的马尔可夫决策过程（M-MDP）实现低成本的持续适应，利用神经案例选择策略引导行动决策，并通过记忆重写机制和高效检索实现策略更新与优化。在DeepResearch场景中，该模型AgentFly在GAIA验证集上取得87.88%的Pass@3成绩，在测试集上达到79.40%，并在DeepResearcher数据集上获得66.6% F1和80.4% PM，优于现有训练方法。记忆机制在分布外任务中提升了4.7%-9.6%。该方法为构建无需梯度更新的通用LLM代理提供了高效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 03:25:30 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在社交推理游戏中的个性化推理能力</title>
<link>https://arxiv.org/abs/2508.16072</link>
<guid>https://arxiv.org/abs/2508.16072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出InMind框架，评估LLMs在社交推理游戏中的个性化推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出InMind框架，用于评估大语言模型（LLMs）在社交推理游戏（SDGs）中捕捉和应用个性化推理风格的能力。该框架通过结构化游戏数据、回合级策略记录和赛后反思，支持四种认知任务，以评估静态对齐与动态适应能力。研究以《Avalon》游戏为例，评估了11个最先进的LLMs，发现通用LLMs如GPT-4o常依赖词汇线索，难以根据时间演化调整策略，而增强推理的LLMs如DeepSeek-R1展现出初步的风格敏感推理能力。研究揭示了当前LLMs在个性化和自适应推理方面的局限性，并推动更符合人类认知的人机交互发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:04:00 GMT</pubDate>
</item>
<item>
<title>基于强化学习的医疗诊断增强系统Deep-DxSearch</title>
<link>https://arxiv.org/abs/2508.15746</link>
<guid>https://arxiv.org/abs/2508.15746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Deep-DxSearch提升医疗诊断准确性，通过强化学习优化检索与推理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Deep-DxSearch，一个基于强化学习（RL）的自主RAG系统，旨在提升医疗诊断的准确性和可追溯性。该系统构建了一个大规模医学检索语料库，并将大语言模型作为核心代理，通过定制奖励机制优化检索、推理结构和诊断准确性。实验表明，Deep-DxSearch在常见和罕见疾病的诊断中均优于GPT-4o、DeepSeek-R1等现有框架。消融实验验证了奖励设计和检索语料的重要性，案例研究也展示了其在临床诊断中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:42:47 GMT</pubDate>
</item>
<item>
<title>Tensor-Parallel Latent Attention 提升模型推理效率</title>
<link>https://arxiv.org/abs/2508.15881</link>
<guid>https://arxiv.org/abs/2508.15881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPLA实现高效张量并行注意力机制，提升模型推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Tensor-Parallel Latent Attention (TPLA)，在 DeepSeek-V2 的 Multi-Head Latent Attention (MLA) 基础上优化了张量并行计算。TPLA 将潜在表示和每个注意力头的输入维度划分到不同设备上独立计算，并通过 all-reduce 合并结果，保留 MLA 的压缩键值缓存优势，同时提升并行效率。相比 Grouped Latent Attention (GLA)，TPLA 中每个头仍能利用完整的潜在表示，保持更强的表达能力。TPLA 可无缝集成预训练模型，支持 MLA 式预填充，并在不重新训练的情况下实现高效的张量并行解码。通过简单的正交变换（如 Hadamard 变换或 PCA）减少跨设备干扰，保证性能稳定。实验表明，在 DeepSeek-V3 和 Kimi-K2 上分别获得 1.79x 和 1.93x 的加速，且在常识与 LongBench 基准测试中保持良好表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 11:25:40 GMT</pubDate>
</item>
<item>
<title>基于LLM与人工辅助的恶意内容检测框架及其在越狱攻击评估中的应用</title>
<link>https://arxiv.org/abs/2508.10390</link>
<guid>https://arxiv.org/abs/2508.10390</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MDH框架提升恶意内容检测效率，增强越狱攻击评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对越狱攻击评估中因提示不明显或未产生有害输出而导致的挑战，提出一种结合大语言模型与人工辅助的混合评估框架MDH，用于数据集清洗和越狱响应检测。研究发现精心设计的开发者消息可显著提升越狱成功率，并据此提出两种新策略：D-Attack利用上下文模拟，DH-CoT引入劫持思维链。相关代码、数据集及检测结果已发布于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10390" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 02:46:56 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的高效图像编辑方法VAREdit</title>
<link>https://arxiv.org/abs/2508.15772</link>
<guid>https://arxiv.org/abs/2508.15772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VAREdit通过自回归机制实现高效且精准的图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出VAREdit，一种基于自回归模型的图像编辑框架，将图像编辑任务转化为多尺度预测问题。该方法通过条件生成目标特征来实现精确编辑，克服了扩散模型在全局去噪过程中易产生不必要修改的问题。为解决源图像特征与目标特征尺度不匹配的问题，引入Scale-Aligned Reference模块，提升编辑准确性。实验表明，VAREdit在编辑符合度和效率方面均优于现有扩散模型，512×512图像编辑仅需1.2秒，速度是UltraEdit的2.2倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>LLaSO：首个全面开源的大规模语音语言建模框架</title>
<link>https://arxiv.org/abs/2508.15418</link>
<guid>https://arxiv.org/abs/2508.15418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaSO提供语音语言模型的开放数据与基准，推动研究标准化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了LLaSO，这是首个完全开源、端到端的大规模语音语言建模框架。LLaSO提供了三个关键资源：LLaSO-Align语音文本对齐语料库、LLaSO-Instruct多任务指令调优数据集以及LLaSO-Eval标准化评估基准。同时，作者发布了LLaSO-Base模型，该模型在公开数据上训练，取得了0.72的标准化得分，成为强有力的基线。研究指出，尽管更广泛的训练数据能提升性能，但在未见任务上仍存在显著泛化差距，尤其是在纯音频场景中。LLaSO通过开放所有数据、基准和模型，为语音语言模型的研究建立了统一标准，推动社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 06:20:00 GMT</pubDate>
</item>
<item>
<title>AI伴侣行为评估基准INTIMA揭示情感互动模式</title>
<link>https://arxiv.org/abs/2508.09998</link>
<guid>https://arxiv.org/abs/2508.09998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入INTIMA基准评估AI伴侣行为，发现情感支持行为普遍但存在模型差异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了INTIMA（Interactions and Machine Attachment Benchmark）基准，用于评估语言模型中的伴侣行为。基于心理学理论和用户数据，构建了涵盖31种行为的分类体系，并通过368个针对性提示进行测试。结果显示，所有模型中情感强化行为更为常见，但不同模型在敏感部分的优先级存在显著差异，这引发了对适当边界设定和情感支持平衡的关注。研究强调了在处理情感互动时需要更一致的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 04:25:38 GMT</pubDate>
</item>
<item>
<title>ATLAS：一种高保真人体建模方法</title>
<link>https://arxiv.org/abs/2508.15767</link>
<guid>https://arxiv.org/abs/2508.15767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ATLAS通过解耦形状与骨骼基底提升人体建模精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ATLAS的高保真人体模型，该模型基于60万张高分辨率扫描数据，通过将网格表示建立在人体骨骼基础上，实现了形状与骨骼基底的显式解耦。这种方法增强了形状表达能力，支持更精细的体征定制，并能独立于外部软组织进行关键点匹配。相比传统线性模型，ATLAS在多种姿态下对未见过的受试者具有更高的拟合精度，且非线性姿态校正更能准确捕捉复杂姿态。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:58:56 GMT</pubDate>
</item>
<item>
<title>Waver：统一图像与视频生成的高性能基础模型</title>
<link>https://arxiv.org/abs/2508.15761</link>
<guid>https://arxiv.org/abs/2508.15761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Waver支持多种视频生成任务，性能领先同类模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Waver，一个用于统一图像和视频生成的高性能基础模型。Waver能够直接生成5至10秒的720p视频，并提升至1080p。该模型支持文本到视频、图像到视频和文本到图像的生成。通过引入Hybrid Stream DiT架构和高质量数据筛选机制，Waver在运动捕捉和时间一致性方面表现出色，排名全球前3，在多个基准测试中超越现有开源模型，接近或超过商业解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:56:10 GMT</pubDate>
</item>
<item>
<title>LiveMCP-101：评估AI代理多工具协作能力的新基准</title>
<link>https://arxiv.org/abs/2508.15760</link>
<guid>https://arxiv.org/abs/2508.15760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveMCP-101测试AI代理在真实场景中使用多种工具完成复杂任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LiveMCP-101，这是一个包含101个精心设计的真实查询的基准测试集，旨在评估AI代理在动态现实环境中协调使用多种MCP工具（如网络搜索、文件操作、数学推理和数据分析）解决多步骤任务的能力。研究提出了一种基于真实执行计划的评估方法，而非仅依赖API输出，从而更准确地反映实际环境的变化性。实验表明，即使是先进的大型语言模型在该任务上的成功率也低于60%，揭示了工具编排中的主要挑战。通过详细的消融实验和错误分析，研究进一步指出了模型在任务执行效率和令牌使用方面的不足，并为未来模型优化提供了方向。LiveMCP-101为评估AI代理的真实世界能力设定了高标准，推动了自主AI系统的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:55:54 GMT</pubDate>
</item>
<item>
<title>Geo-Visual Agents：基于多模态AI的地理视觉问答系统</title>
<link>https://arxiv.org/abs/2508.15752</link>
<guid>https://arxiv.org/abs/2508.15752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Geo-Visual Agents通过分析地理图像实现对空间问题的理解与回答。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Geo-Visual Agents的概念，这是一种能够理解和回答复杂视觉空间问题的多模态AI代理。它结合了大规模地理图像数据（如街景、景点照片和卫星图像）与传统GIS数据，旨在提升数字地图在地理视觉查询方面的表现。文章提出了该系统的愿景，描述了感知与交互方法，并提供了三个示例，同时讨论了未来研究的关键挑战与机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:49:52 GMT</pubDate>
</item>
<item>
<title>Grounded VideoDiT：提升视频理解的时序感知与实体对齐能力</title>
<link>https://arxiv.org/abs/2508.15641</link>
<guid>https://arxiv.org/abs/2508.15641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Grounded VideoDiT，提升视频中的时间定位与实体交互理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Grounded VideoDiT，一种旨在提升视频理解能力的视频大模型。针对现有视频大模型在时序感知和实体对齐方面的不足，该模型引入了三项关键创新：Diffusion Temporal Latent（DTL）编码器增强边界敏感性和时间一致性；对象引导表示将查询实体与局部视觉证据绑定，加强对齐；混合标记方案结合离散时间标记，实现精细的时间推理。实验结果表明，Grounded VideoDiT在Charades STA、NExT GQA等多个视频问答基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 11:12:14 GMT</pubDate>
</item>
<item>
<title>大型语言模型评估基准的现状与挑战</title>
<link>https://arxiv.org/abs/2508.15361</link>
<guid>https://arxiv.org/abs/2508.15361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统综述大语言模型评估基准，分析其分类与现存问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着大型语言模型能力的不断提升，相关评估基准也逐渐增多。本文首次系统回顾了当前大语言模型评估基准的发展状况，将283个代表性基准分为通用能力、领域特定和目标特定三类。通用能力基准涵盖语言、知识和推理等方面；领域特定基准关注自然科学、人文社科和工程技术等；目标特定基准则涉及风险、可靠性及代理系统等。文章指出当前基准存在数据污染导致评分虚高、文化语言偏见引发评价不公以及缺乏对过程可信度和动态环境评估等问题，并提出了未来基准设计的参考范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 04:43:35 GMT</pubDate>
</item>
<item>
<title>aiXiv：面向人工智能科学家的开放科研平台</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">aiXiv解决AI生成研究内容的发布难题。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了aiXiv，一个面向人工智能和人类科学家的下一代开放获取平台。该平台采用多智能体架构，支持研究提案和论文的提交、评审与迭代优化，并提供API和MCP接口以实现人机协作。通过实验验证，aiXiv能够显著提升AI生成研究的质量，推动高质量AI研究成果的发表与传播。该平台旨在构建一个可扩展、可持续的科学发现生态系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 19:16:41 GMT</pubDate>
</item>
<item>
<title>基于双视角图像的3D人体重建方法</title>
<link>https://arxiv.org/abs/2508.14892</link>
<guid>https://arxiv.org/abs/2508.14892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">从前后视图重建3D人体，提升渲染质量与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种从仅有的前后视图中重建完整3D人体的方法，旨在降低用户创建3D数字人类的门槛。该方法通过优化几何重建模型和增强算法，有效解决输入信息稀疏带来的挑战，实现高精度点云重建并补充颜色信息。实验表明，该方法在THuman2.0和跨域数据集上表现优异，且可在低成本移动设备上运行，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:59:11 GMT</pubDate>
</item>
<item>
<title>SceneGen：基于单图和多图输入的3D场景资产生成框架</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SceneGen可直接生成3D场景资产，无需优化或检索。</p><br /><br /><p><strong>摘要：</strong> 本文提出SceneGen，一个能够从单张场景图像及对应物体掩码中同时生成多个3D资产（包括几何和纹理）的框架。该方法无需优化或资产检索，通过引入特征聚合模块，结合视觉和几何编码器的信息，实现3D资产及其相对空间位置的同步生成。此外，SceneGen在多图像输入场景下也表现出良好的扩展性，即使仅在单图像数据上训练，也能提升生成效果。大量定量和定性评估验证了该方法的高效性和鲁棒性，为高质量3D内容生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:59:16 GMT</pubDate>
</item>
<item>
<title>Intern-S1：面向科学领域的高性能开源基础模型</title>
<link>https://arxiv.org/abs/2508.15763</link>
<guid>https://arxiv.org/abs/2508.15763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Intern-S1在科学领域表现卓越，超越部分闭源模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Intern-S1，一个专注于科学领域的开源基础模型，具备多模态和专家混合（MoE）架构，拥有280亿激活参数和2410亿总参数。该模型在5T token数据上持续预训练，其中2.5T来自科学领域。通过离线与在线强化学习（RL）训练，以及Mixture-of-Rewards（MoR）技术，Intern-S1在多个科学任务中表现出色，如分子合成规划、反应条件预测等，性能优于多数开源模型，并在专业任务中超越了闭源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:58:00 GMT</pubDate>
</item>
<item>
<title>DeepConf提升大语言模型推理效率与准确性</title>
<link>https://arxiv.org/abs/2508.15260</link>
<guid>https://arxiv.org/abs/2508.15260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepConf通过置信度过滤提升推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Deep Think with Confidence (DeepConf) 方法，利用模型内部的置信度信号动态过滤低质量推理路径，无需额外训练或调参，可集成至现有框架。在多个推理任务和最新开源模型上评估，DeepConf在AIME 2025等挑战性基准测试中表现出色，准确率高达99.9%，并减少84.7%的生成token。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 01:48:38 GMT</pubDate>
</item>
<item>
<title>Fin-PRM：面向金融领域的过程奖励模型</title>
<link>https://arxiv.org/abs/2508.15202</link>
<guid>https://arxiv.org/abs/2508.15202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fin-PRM提升金融任务中语言模型的推理质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Fin-PRM，一种针对金融领域设计的过程奖励模型，用于评估大语言模型在金融任务中的中间推理步骤。该模型结合了步骤级和轨迹级奖励监督，能够更精确地衡量符合金融逻辑的推理过程。Fin-PRM在离线和在线学习设置中表现出色，支持高质量推理轨迹选择、密集过程奖励生成以及测试时的奖励引导推理。实验结果表明，Fin-PRM在CFLUE和FinQA等金融推理基准上优于通用PRM和强基线模型，提升了监督学习、强化学习和测试性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 23:31:11 GMT</pubDate>
</item>
<item>
<title>GUI-Owl与Mobile-Agent-v3：开源GUI代理模型的最新进展</title>
<link>https://arxiv.org/abs/2508.15144</link>
<guid>https://arxiv.org/abs/2508.15144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI-Owl和Mobile-Agent-v3在多个GUI基准测试中取得新高。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GUI-Owl，一个在桌面和移动环境中表现优异的开源GUI代理模型，在十个GUI基准测试中达到最先进的性能。GUI-Owl-7B在AndroidWorld和OSWorld分别获得66.4和29.4的分数。基于此，研究者提出了Mobile-Agent-v3框架，进一步提升至73.3和37.7。GUI-Owl包含三个关键创新：大规模环境基础设施、多样化的基础代理能力以及可扩展的环境强化学习。该模型支持端到端决策，并可作为多代理系统中的模块化组件。研究还引入了Trajectory-aware Relative Policy Optimization (TRPO) 方法，实现了34.9的OSWorld得分。GUI-Owl和Mobile-Agent-v3已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 20:39:12 GMT</pubDate>
</item>
<item>
<title>FLARE：一种线性复杂度的自注意力机制</title>
<link>https://arxiv.org/abs/2508.12594</link>
<guid>https://arxiv.org/abs/2508.12594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLARE实现线性复杂度自注意力，提升大规模数据处理能力。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为FLARE的自注意力机制，具有线性复杂度，通过固定长度的潜在序列路由注意力，从而在大规模无结构网格上实现高效计算。该方法通过将输入序列投影到固定长度的潜在序列，以较低的计算成本学习低秩注意力形式，显著提升了模型的可扩展性和准确性。FLARE在多个基准测试中优于最先进的神经偏微分方程代理模型，并提供了新的增材制造数据集以促进进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 23:00:55 GMT</pubDate>
</item>
<item>
<title>MCP-Universe：首个评估大语言模型与外部工具交互的综合基准</title>
<link>https://arxiv.org/abs/2508.14704</link>
<guid>https://arxiv.org/abs/2508.14704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCP-Universe是首个评估LLM在真实任务中表现的基准，涵盖多个领域。</p><br /><br /><p><strong>摘要：</strong> MCP-Universe是一个全新的基准测试框架，旨在评估大语言模型（LLM）在与真实世界MCP服务器交互时的表现。该基准覆盖六个核心领域，包括位置导航、代码仓库管理、金融分析、3D设计、浏览器自动化和网络搜索，共涉及11个MCP服务器。为确保评估的严谨性，MCP-Universe引入了执行评估器，包括格式评估、静态评估和动态评估。实验结果显示，即使是最先进的模型如GPT-5、Grok-4和Claude-4.0-Sonnet也表现出显著性能限制。此外，该基准还提出了长上下文和未知工具的挑战，同时开放了可扩展的评估框架，支持研究人员和开发者集成新代理和MCP服务器，推动MCP生态系统的持续发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 09:28:58 GMT</pubDate>
</item>
<item>
<title>基于全同态加密的Levenshtein距离优化算法</title>
<link>https://arxiv.org/abs/2508.14568</link>
<guid>https://arxiv.org/abs/2508.14568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出优化Levenshtein距离计算方法，提升FHE性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种在全同态加密（FHE）框架下计算Levenshtein距离的新方法，特别针对第三代方案如TFHE。该算法通过减少每个计算单元所需的可编程刷新次数，将计算成本从传统Wagner-Fisher算法的约94次降至仅1次，并优化了字符比较过程，将ASCII字符比较降至2次PBS操作。此外，当其中一个输入为明文时，通过预处理可进一步提升性能。实验表明，该方法比现有最佳TFHE实现快278倍，比优化后的Wagner-Fisher算法快39倍，且在有明文输入时还能额外提升3倍速度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14568" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 05:40:06 GMT</pubDate>
</item>
<item>
<title>扩散语言模型的量化研究与部署分析</title>
<link>https://arxiv.org/abs/2508.14896</link>
<guid>https://arxiv.org/abs/2508.14896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型的量化方法及部署挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统研究了扩散大语言模型（dLLMs）的后训练量化（PTQ）方法。文章指出，激活异常值是低比特量化的主要障碍，因为它们占据了动态范围的大部分，影响了大多数值的精度。作者采用了先进的PTQ技术，并在多种任务类型和模型变体上进行了全面评估。分析从四个维度展开：位宽、量化方法、任务类别和模型类型，为未来高效部署dLLMs提供了实践见解。所有代码和实验设置将公开，以支持社区研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>MeshCoder：将3D物体重建为可编辑的Python脚本</title>
<link>https://arxiv.org/abs/2508.14879</link>
<guid>https://arxiv.org/abs/2508.14879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MeshCoder通过点云生成可编辑的Blender Python脚本，提升3D形状重建与编辑能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MeshCoder框架，能够将复杂的3D物体从点云数据重建为可编辑的Blender Python脚本。该框架包含一套丰富的Blender Python API，用于生成复杂几何结构，并构建了一个大规模的配对数据集。基于这些数据，训练了一个多模态大语言模型，实现从3D点云到可执行代码的转换。该方法不仅提升了形状到代码的重建性能，还支持直观的几何和拓扑编辑，增强了大语言模型在3D形状理解中的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:50:15 GMT</pubDate>
</item>
<item>
<title>Tinker：无需微调的高保真3D编辑框架</title>
<link>https://arxiv.org/abs/2508.14811</link>
<guid>https://arxiv.org/abs/2508.14811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tinker实现零样本3D编辑，提升多视角一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tinker，一个能够在单次或少量样本情况下进行高保真3D编辑的框架，无需针对每个场景进行微调。Tinker通过重新利用预训练扩散模型，实现了对3D潜在空间的理解，并具备多视角一致性编辑能力。研究团队构建了首个大规模多视角编辑数据集，支持多样化的场景和风格。Tinker包含两个创新组件：参考式多视角编辑器和任意视角到视频的合成器，分别用于精确编辑和高质量场景生成。实验表明，Tinker在编辑、新视角生成和渲染增强任务中均达到领先水平，为通用化3D内容创作提供了重要突破。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 12:02:59 GMT</pubDate>
</item>
<item>
<title>DuPO：一种无需标注的双学习偏好优化框架</title>
<link>https://arxiv.org/abs/2508.14460</link>
<guid>https://arxiv.org/abs/2508.14460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuPO通过双学习机制实现无标注反馈，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuPO，一种基于双学习的偏好优化框架，能够生成无需标注的反馈。该方法解决了传统强化学习依赖昂贵标签和仅适用于可验证任务的问题，同时突破了传统双学习仅限于严格双任务对的限制。DuPO将原始任务输入分解为已知和未知部分，并构建双任务来利用原始输出和已知信息重建未知部分，从而扩展到非可逆任务。重建质量作为自监督奖励优化原始任务，结合大语言模型的能力，实现单模型完成两个任务。实验表明，DuPO在多个任务中取得显著提升，如翻译质量提高2.13 COMET，数学推理准确率提升6.4分，推理重排序性能提升9.3分，展现出其在大语言模型优化中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 02:31:18 GMT</pubDate>
</item>
<item>
<title>Nemotron-Nano-9B-v2：提升推理性能的混合Mamba-Transformer语言模型</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nemotron-Nano-9B-v2在推理任务中实现更高吞吐量与准确率。</p><br /><br /><p><strong>摘要：</strong> Nemotron-Nano-9B-v2是一款结合Mamba和Transformer结构的语言模型，旨在提升推理任务的吞吐量并保持与同类模型相当或更高的准确性。该模型基于Nemotron-H架构，用Mamba-2层替代了传统Transformer中的大部分自注意力层，从而提高推理速度。通过预训练120亿参数的Nemotron-Nano-12B-v2-Base模型，并采用Minitron策略进行压缩和蒸馏，使得模型能够在单块NVIDIA A10G GPU上处理最多128k tokens。实验表明，相比Qwen3-8B等模型，Nemotron-Nano-9B-v2在推理任务中实现了高达6倍的吞吐量提升，同时保持优异的准确性。相关模型和数据集已发布在Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 02:00:57 GMT</pubDate>
</item>
<item>
<title>提升模型局部尺度不变性的深度均衡校准器</title>
<link>https://arxiv.org/abs/2508.14187</link>
<guid>https://arxiv.org/abs/2508.14187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DEC提升模型对局部尺度变化的适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种深度均衡校准器（DEC），用于增强模型对局部尺度变化的适应性。DEC可以轻松集成到现有的网络架构中，并适用于预训练模型。实验表明，在ImageNet基准测试中，DEC在多个主流预训练模型（如ViT、DeiT、Swin和BEiT）上提升了模型性能和局部尺度一致性。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 14:21:59 GMT</pubDate>
</item>
<item>
<title>RynnEC：面向具身认知的视频多模态大语言模型</title>
<link>https://arxiv.org/abs/2508.14160</link>
<guid>https://arxiv.org/abs/2508.14160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RynnEC是用于具身认知的视频多模态大模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了RynnEC，一个专为具身认知设计的视频多模态大语言模型。该模型基于通用视觉-语言基础模型，结合区域编码器和掩码解码器，实现了灵活的区域级视频交互。尽管结构紧凑，RynnEC在物体属性理解、分割和空间推理方面表现优异。研究提出了一种基于第一视角视频的数据生成管道，以解决3D标注数据不足的问题，并引入了RynnEC-Bench评估基准。作者期望RynnEC能推动具身智能体通用认知核心的发展，并提升在多样化任务中的泛化能力。代码、模型和基准已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 14:00:01 GMT</pubDate>
</item>
<item>
<title>基于多模态对比学习与同构关系的推荐系统框架REARM</title>
<link>https://arxiv.org/abs/2508.13745</link>
<guid>https://arxiv.org/abs/2508.13745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REARM提升多模态推荐系统的性能与数据利用效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的推荐系统框架REARM，旨在解决多模态推荐中因数据稀疏而导致的特征表示不准确和用户-物品交互挖掘不足的问题。该框架通过引入元网络和正交约束策略优化多模态对比学习，有效过滤噪声并保留关键信息。同时，结合用户兴趣图与物品共现图，增强同构关系建模。实验结果表明，REARM在多个真实数据集上优于现有方法，并通过可视化验证了其在区分共享与独特模态特征方面的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 07:35:48 GMT</pubDate>
</item>
<item>
<title>越南语多模态教育评估中视觉语言模型的表现研究</title>
<link>https://arxiv.org/abs/2508.13680</link>
<guid>https://arxiv.org/abs/2508.13680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在越南语多模态教育任务中表现有限，仅部分模型接近人类水平。</p><br /><br /><p><strong>摘要：</strong> 本研究首次评估了视觉语言模型（VLMs）在越南语多模态教育考试中的表现，通过构建ViExam基准测试集，包含2,548道多模态题目。结果显示，最先进的VLM模型平均准确率为57.74%，而开源模型仅为27.70%，均低于人类平均水平（66.54%）。仅有o3模型超过人类平均表现（74.07%），但仍远低于人类最佳成绩（99.60%）。使用英文指令进行跨语言提示未能提升性能，反而使SOTA模型准确率下降1个百分点。通过人机协作可部分提升模型表现。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 05:31:18 GMT</pubDate>
</item>
<item>
<title>FinCDM：面向金融大语言模型的认知诊断评估框架</title>
<link>https://arxiv.org/abs/2508.13491</link>
<guid>https://arxiv.org/abs/2508.13491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinCDM揭示金融大模型知识盲区，提升模型可信度。</p><br /><br /><p><strong>摘要：</strong> 本文提出FinCDM，首个针对金融大语言模型的认知诊断评估框架，通过技能标签任务识别模型的知识与技能短板，而非仅依赖单一评分。构建了CPA-QKA数据集，涵盖真实会计与金融技能，由专家严格标注。实验显示FinCDM能发现传统基准未覆盖的税务和监管推理等薄弱环节，并揭示模型行为模式，为更可靠、针对性的模型开发提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 23:52:15 GMT</pubDate>
</item>
<item>
<title>人工智能在科学发现中的自主化：Agentic Science的演进与展望</title>
<link>https://arxiv.org/abs/2508.14111</link>
<guid>https://arxiv.org/abs/2508.14111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI正从工具变为科学发现的自主伙伴，推动Agentic Science发展。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了人工智能如何从辅助工具演变为具备科学自主性的研究伙伴，提出了Agentic Science作为AI驱动科学发现的新范式。通过大型语言模型、多模态系统和集成平台的支持，AI展现出生成假设、设计实验、执行分析和迭代优化的能力。文章统一了过程导向、自主导向和机制导向三个视角，构建了一个涵盖基础能力、核心流程和领域应用的综合框架。作者回顾了AI在生命科学、化学、材料科学和物理学等领域的应用，并分析了当前挑战与未来发展方向，为AI赋能的科学研究提供了结构化的理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 01:25:54 GMT</pubDate>
</item>
<item>
<title>面向未来预测的动态评估基准FutureX及其对LLM代理的性能分析</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FutureX是首个动态实时评估未来预测能力的基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了FutureX，这是一个专为评估大型语言模型（LLM）在复杂未来预测任务中的表现而设计的动态、实时评估基准。当前缺乏大规模的评估标准，主要是因为处理实时更新和获取准确答案存在挑战。FutureX通过自动化流程实现每日更新并避免数据污染，支持多种模型评估，包括具备推理、搜索和外部工具整合能力的模型。研究分析了模型在面对虚假网页和时间有效性问题时的失败模式，并提出建立一个无污染、动态的评估标准，以推动LLM代理达到专业分析师的水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 16 Aug 2025 04:54:08 GMT</pubDate>
</item>
<item>
<title>CHORD：融合SFT与RL的可控强化学习框架</title>
<link>https://arxiv.org/abs/2508.11408</link>
<guid>https://arxiv.org/abs/2508.11408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CHORD通过动态权重统一SFT与RL，提升模型训练稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出CHORD框架，旨在通过动态加权机制将监督微调（SFT）与强化学习（RL）相结合，以解决现有方法在整合过程中可能导致的模式破坏和过拟合问题。CHORD将SFT视为RL过程中的辅助目标，并引入双控制机制：全局系数用于引导从模仿学习到探索学习的过渡，而基于token的权重函数则实现对专家数据的细粒度学习，从而减少干扰并保持探索能力。实验表明，CHORD在多个基准测试中表现出更稳定高效的训练效果，优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 07:20:03 GMT</pubDate>
</item>
<item>
<title>多语言常识推理基准mSCoRe的构建与分析</title>
<link>https://arxiv.org/abs/2508.10137</link>
<guid>https://arxiv.org/abs/2508.10137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出mSCoRe基准评估多语言常识推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了mSCoRe，一个用于评估多语言常识推理能力的基准。该基准包含三个核心组件：细粒度的推理技能分类、针对常识推理的数据生成流程以及可扩展的任务难度框架。实验表明，当前最先进的大语言模型在处理高复杂度任务时仍面临挑战，尤其在涉及文化背景和多语言常识的场景中表现有限。研究进一步分析了模型的推理过程，并提出了未来改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 14:59:02 GMT</pubDate>
</item>
<item>
<title>轻量级语言模型中的推理与检索增强生成方法</title>
<link>https://arxiv.org/abs/2508.11386</link>
<guid>https://arxiv.org/abs/2508.11386</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级RAG系统，提升领域查询准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种将推理与检索增强生成（RAG）结合的新型方法，采用轻量级语言模型架构，适用于资源受限或安全环境。系统整合了密集检索器和微调的Qwen2.5-Instruct模型，利用合成查询和前沿模型的推理轨迹，在NHS A-to-Z条件页面语料库中进行训练。研究探讨了摘要压缩、合成数据设计和推理感知微调对模型性能的影响，并在非推理模型和通用轻量模型上进行了评估，结果表明该方法在保持本地部署可行性的同时，显著提升了答案准确性和一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11386" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:38:15 GMT</pubDate>
</item>
<item>
<title>基于少样本学习的合成语音检测方法研究</title>
<link>https://arxiv.org/abs/2508.13320</link>
<guid>https://arxiv.org/abs/2508.13320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">少样本学习提升合成语音检测在分布偏移下的性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在分布偏移条件下（如未见过的合成方法、说话人、语言或音频环境）检测合成语音的挑战。提出了一种自注意力原型网络，以增强少样本适应能力。通过对比传统零样本检测器与所提方法，在控制训练条件的情况下引入分布偏移进行评估。结果表明，在零样本检测性能受限时，该方法仅需10个内分布样本即可快速适应，显著提升了检测效果，例如在日语Deepfake数据集上相对EER降低32%，在ASVspoof 2021 Deepfake数据集上降低20%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 15:14:45 GMT</pubDate>
</item>
<item>
<title>提出PASR方法提升大语言模型自我优化能力</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PASR实现大模型在生成过程中主动优化输出，提升效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为ProActive Self-Refinement (PASR) 的新方法，使大语言模型能够在生成过程中主动决定何时、如何进行优化，而非依赖固定的迭代次数。与传统方法不同，PASR根据模型内部状态和上下文动态调整优化策略，从而提高问题解决性能。实验表明，在Qwen3-8B模型上，PASR相比标准生成方式平均减少41.6%的token消耗，并提升8.2%的准确率。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 09:07:21 GMT</pubDate>
</item>
<item>
<title>CAMAR：多智能体路径规划的新型MARL基准</title>
<link>https://arxiv.org/abs/2508.12845</link>
<guid>https://arxiv.org/abs/2508.12845</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAMAR为多智能体路径规划提供高效且真实的MARL测试平台。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CAMAR，一个专为多智能体路径规划设计的新型多智能体强化学习（MARL）基准。CAMAR结合了连续状态和动作空间，并支持合作与竞争交互，能够以高达每秒10万步的速度运行。文章还提出了一种三层评估协议，用于更精确地跟踪算法进展并深入分析性能。此外，CAMAR支持将传统规划方法如RRT和RRT*集成到MARL流程中，提升算法效果。研究提供了多种测试场景和基准工具，确保实验的可重复性和公平比较，表明CAMAR是一个具有挑战性和现实意义的MARL测试环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12845" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 07:32:26 GMT</pubDate>
</item>
<item>
<title>基于原子思维的增强型检索生成框架Atom-Searcher</title>
<link>https://arxiv.org/abs/2508.12800</link>
<guid>https://arxiv.org/abs/2508.12800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Atom-Searcher提升多步骤推理与搜索效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Atom-Searcher的新框架，结合了原子思维（Atomic Thought）和推理奖励模型（RRM），以改进大型语言模型在复杂任务中的表现。该框架通过细粒度的推理单元和奖励机制，解决了传统方法在多步骤推理和策略搜索中的局限性。实验表明，Atom-Searcher在七个基准测试中均优于现有方法，具有更高的可扩展性、可解释性和人类相似的推理模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:23:10 GMT</pubDate>
</item>
<item>
<title>FineCE：一种用于大语言模型的细粒度置信度估计方法</title>
<link>https://arxiv.org/abs/2508.12040</link>
<guid>https://arxiv.org/abs/2508.12040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FineCE提升大语言模型生成文本的置信度估计准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FineCE的新型置信度估计方法，旨在解决大语言模型在生成文本时缺乏细粒度置信度评估的问题。该方法通过构建全面的训练数据管道，训练模型对任意文本序列进行置信度预测，并引入了Backward Confidence Integration（BCI）策略，利用后续文本信息提升当前序列的置信度估计。此外，还提出了三种确定最佳置信度估计位置的策略。实验结果表明，FineCE在多个基准数据集上均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 16 Aug 2025 09:29:35 GMT</pubDate>
</item>
<item>
<title>MM-BrowseComp：评估AI代理多模态检索与推理能力的新基准</title>
<link>https://arxiv.org/abs/2508.13186</link>
<guid>https://arxiv.org/abs/2508.13186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准MM-BrowseComp测试AI代理多模态搜索与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MM-BrowseComp，一个专门用于评估AI代理在多模态内容下进行深度搜索和推理能力的新基准。该基准包含224个精心设计的问题，其中许多问题涉及图像或视频等非文本信息，挑战现有仅依赖文本的模型。研究还提供了每个问题的验证清单，以分析多模态依赖关系和推理路径。实验表明，即使最先进的模型如OpenAI o3在该基准上的准确率也仅为29.02%，反映出当前AI模型在多模态理解和推理方面的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 09:46:47 GMT</pubDate>
</item>
<item>
<title>基于指向表示的具身AI模型提升泛化能力</title>
<link>https://arxiv.org/abs/2508.13998</link>
<guid>https://arxiv.org/abs/2508.13998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Embodied-R1通过指向表示实现具身AI的高效泛化。</p><br /><br /><p><strong>摘要：</strong> 文章指出，具身AI的泛化能力受限于‘看到-做到’的差距，主要由于数据稀缺和具身异质性。为此，研究提出‘指向’作为统一的、与具身无关的中间表示，并定义了四种核心具身指向能力，以连接高层视觉语言理解和底层动作原语。研究构建了Embodied-Points-200K大规模数据集，并训练了Embodied-R1模型，采用两阶段强化微调（RFT）方法。该模型在11个具身空间和指向基准测试中表现优异，尤其在零样本泛化任务中取得显著提升，如SIMPLEREnv成功率达56.2%，XArm任务平均达87.5%。此外，模型对视觉干扰具有高鲁棒性，展示了指向表示与RFT训练范式在机器人感知-行动差距中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 12:50:01 GMT</pubDate>
</item>
<item>
<title>大型语言模型在道德理解上的表现分析</title>
<link>https://arxiv.org/abs/2508.13804</link>
<guid>https://arxiv.org/abs/2508.13804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较AI与人类在道德判断上的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究首次对主流大型语言模型进行了大规模贝叶斯评估，通过模拟标注者之间的分歧，区分了人类固有的不确定性与模型的领域敏感性。研究涵盖了超过10万条文本和700名标注者，使用GPU优化的贝叶斯框架处理了百万级模型查询。结果显示，AI模型通常排名在人类标注者的前25%，表现出更高的平衡准确率，并且产生更少的假阴性结果，显示出更强的道德识别能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 09:05:48 GMT</pubDate>
</item>
<item>
<title>跨骨骼拓扑动画迁移方法研究</title>
<link>https://arxiv.org/abs/2508.13139</link>
<guid>https://arxiv.org/abs/2508.13139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Motion2Motion实现跨骨骼拓扑的高效动画迁移。</p><br /><br /><p><strong>摘要：</strong> 本文针对不同骨骼拓扑结构之间的动画迁移问题，提出了一种无需训练的框架Motion2Motion。该方法仅需目标骨骼的一个或几个示例动作和稀疏的骨骼对应关系，即可实现高效的动画迁移。研究通过定性和定量评估验证了该方法在相似骨骼和跨物种骨骼迁移中的有效性，并展示了其在实际应用中的潜力。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:50:31 GMT</pubDate>
</item>
<item>
<title>基于相关性的稀疏自编码器自动调优方法CorrSteer</title>
<link>https://arxiv.org/abs/2508.12535</link>
<guid>https://arxiv.org/abs/2508.12535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CorrSteer通过相关性选择特征提升语言模型任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出CorrSteer方法，利用推理时生成的token激活与样本正确性的相关性来选择稀疏自编码器（SAE）的特征，从而避免冗余关联并自动化调优过程。该方法无需对比数据集或大量激活存储，在QA、偏见缓解、越狱防御和推理基准测试中表现出色，尤其在MMLU和HarmBench上分别提升了4.1%和22.9%。所选特征具有语义意义，展示了驱动性能的关键能力，验证了基于相关性的SAE调优方法的有效性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 20:01:42 GMT</pubDate>
</item>
<item>
<title>大语言模型版权保护技术综述</title>
<link>https://arxiv.org/abs/2508.11548</link>
<guid>https://arxiv.org/abs/2508.11548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大语言模型版权保护技术，重点分析模型指纹技术。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了大语言模型（LLM）的版权保护技术，特别关注模型指纹技术。文章首先澄清了文本水印、模型水印与模型指纹之间的概念联系，并统一术语将模型水印纳入更广泛的指纹框架。接着对多种文本水印技术进行了概述和比较，指出部分方法也可作为模型指纹使用。随后系统分类并比较了现有的模型指纹方法，首次介绍了指纹迁移与移除技术，并总结了评估模型指纹的指标，包括有效性、无害性、鲁棒性、隐蔽性和可靠性。最后讨论了当前面临的挑战与未来研究方向，旨在为研究人员提供对LLM时代版权保护技术的深入理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 11:50:20 GMT</pubDate>
</item>
<item>
<title>MedSAMix：一种无需训练的医学图像分割模型融合方法</title>
<link>https://arxiv.org/abs/2508.11032</link>
<guid>https://arxiv.org/abs/2508.11032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedSAMix提升医学图像分割性能，增强模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MedSAMix，一种无需训练的医学图像分割模型融合方法，结合通用模型（如SAM）和专业模型（如MedSAM）的优势。该方法通过零阶优化自动发现最优层融合方案，并提供两种优化策略以满足不同临床场景下的领域特异性与泛化性需求。在25个医学分割任务上的实验表明，MedSAMix有效减少模型偏差，提升专业任务准确率和多任务泛化能力，分别提高6.67%和4.37%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 15:35:57 GMT</pubDate>
</item>
<item>
<title>深度学习在语音分离中的系统综述</title>
<link>https://arxiv.org/abs/2508.10830</link>
<guid>https://arxiv.org/abs/2508.10830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述深度学习在语音分离中的应用与进展。</p><br /><br /><p><strong>摘要：</strong> 本文对基于深度神经网络的语音分离技术进行了系统性综述，旨在填补当前研究中对不同架构和方法孤立分析的不足。文章从学习范式、分离场景、监督/自监督/无监督框架以及模型结构等方面进行全面探讨，并结合最新研究成果进行分析。同时，作者评估了不同方法在标准数据集上的表现，揭示其优缺点，并指出未来发展方向，如领域鲁棒性、高效架构、多模态融合等。该综述为研究人员提供了全面的参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 12:54:34 GMT</pubDate>
</item>
<item>
<title>统一生成模型中语义ID的构建与性能研究</title>
<link>https://arxiv.org/abs/2508.10478</link>
<guid>https://arxiv.org/abs/2508.10478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何构建适用于搜索和推荐的统一语义ID。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在统一生成模型中构建适用于搜索和推荐任务的语义ID方法。传统上，物品通过唯一标识符或从嵌入中获得的离散代码表示。文章比较了多种构建语义ID的策略，包括任务特定和跨任务方法，并分析了是否应在联合模型中为每个任务使用独立的语义ID。实验表明，通过对搜索和推荐任务进行微调的双编码器模型获取物品嵌入，并构建统一的语义ID空间，能够在两个任务中实现良好的性能平衡。研究希望推动更通用、语义基础的ID方案的发展，并为下一代统一生成推荐架构提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:28:49 GMT</pubDate>
</item>
<item>
<title>利用多模态大语言模型提升视频推荐系统的语义理解能力</title>
<link>https://arxiv.org/abs/2508.09789</link>
<guid>https://arxiv.org/abs/2508.09789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多模态大语言模型增强视频推荐的语义理解。</p><br /><br /><p><strong>摘要：</strong> 现有视频推荐系统主要依赖用户定义的元数据或低级视觉和音频信号，但这些特征无法捕捉如意图、幽默和世界知识等深层次语义。本文提出一种无需微调的框架，通过提示预训练多模态大语言模型（MLLM）生成丰富的自然语言描述，从而将高阶语义注入推荐流程。该方法结合了先进的文本编码器，并在标准协同过滤、基于内容和生成式推荐器中验证，结果表明在MicroLens-100K数据集上优于传统视频、音频和元数据特征。研究展示了MLLM作为实时知识提取工具在构建更注重用户意图的视频推荐系统中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 09:19:31 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的播客推荐评估框架</title>
<link>https://arxiv.org/abs/2508.08777</link>
<guid>https://arxiv.org/abs/2508.08777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用LLM进行播客推荐评估，提升推荐质量与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大语言模型（LLM）的播客推荐评估框架，旨在解决传统离线指标和在线测试方法在长音频领域中的局限性。该框架通过分析用户90天的收听历史构建自然语言用户画像，以高语义层次的上下文引导LLM进行更精准的推荐评估。实验表明，该方法在47名参与者中表现出与人工判断高度一致的效果，并优于使用原始数据的变体方法。该框架为推荐系统提供了高效、可解释的评估方式，适用于迭代测试与模型选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 05:23:35 GMT</pubDate>
</item>
<item>
<title>辐射场在扩展现实中的研究现状与挑战</title>
<link>https://arxiv.org/abs/2508.04326</link>
<guid>https://arxiv.org/abs/2508.04326</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">辐射场技术在XR中应用有限，本文系统分析其现状与研究缺口。</p><br /><br /><p><strong>摘要：</strong> 本文对辐射场（RF）技术在扩展现实（XR）中的应用进行了系统性综述。尽管3D Gaussian Splatting和NeRF等技术推动了高质量视图合成的发展，但RF在XR领域的贡献仍较少。作者从计算机视觉、图形学、机器人学等多个领域收集了365篇相关论文，并深入分析其中66篇探讨了RF在XR中的具体应用。研究揭示了RF在XR中的当前应用场景、实现方式及存在的研究空白，为XR社区提供了有价值的参考，助力其应对辐射场技术快速发展的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04326" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 07:14:06 GMT</pubDate>
</item>
<item>
<title>基于时间结构的流匹配模型强化学习优化方法</title>
<link>https://arxiv.org/abs/2508.04324</link>
<guid>https://arxiv.org/abs/2508.04324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TempFlow-GRPO提升文本到图像生成的人类偏好对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出TempFlow-GRPO，一种针对流模型的强化学习优化框架，解决了传统方法在时间步长上奖励分配不均的问题。该方法引入轨迹分支机制和噪声感知权重策略，提升了模型在不同生成阶段的优化效率与稳定性，显著提高了文本到图像生成任务中的人类偏好对齐效果和基准测试表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 07:10:39 GMT</pubDate>
</item>
<item>
<title>ZARA：基于代理的零样本可解释运动时间序列识别框架</title>
<link>https://arxiv.org/abs/2508.04038</link>
<guid>https://arxiv.org/abs/2508.04038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZARA实现无需微调的零样本运动识别，准确率高且可解释。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZARA，一个基于代理的框架，用于直接从原始运动时间序列中进行零样本、可解释的人类活动识别（HAR）。ZARA结合了自动构建的成对特征知识库、多传感器检索模块和分层代理管道，使大型语言模型能够迭代选择特征、引用相关证据并生成活动预测及自然语言解释。该方法无需微调或任务特定分类器，在8个HAR基准测试中表现出色，达到最先进的零样本性能，比最强基线高出2.53倍的宏F1分数。消融实验验证了各模块的必要性，展示了ZARA在可信赖、即插即用运动时间序列分析中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:57:57 GMT</pubDate>
</item>
<item>
<title>LongSplat：解决长视频新视角合成的3D高斯点云框架</title>
<link>https://arxiv.org/abs/2508.14041</link>
<guid>https://arxiv.org/abs/2508.14041</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongSplat提升长视频新视角合成质量与效率。</p><br /><br /><p><strong>摘要：</strong> LongSplat是一种针对非结构化长视频的新视角合成框架，解决了相机位姿漂移、几何初始化不准确和内存限制等问题。其核心创新包括：联合优化相机位姿与3D高斯分布以避免局部最优，基于学习的3D先验进行鲁棒位姿估计，以及通过八叉树锚点机制高效处理密集点云。实验表明，LongSplat在渲染质量、位姿精度和计算效率方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14041" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>MMAU-Pro：评估AI音频智能的全面基准</title>
<link>https://arxiv.org/abs/2508.13992</link>
<guid>https://arxiv.org/abs/2508.13992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMAU-Pro是评估AI音频理解能力的全面基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MMAU-Pro，这是一个全面且严格构建的音频智能评估基准，包含5305个实例，涵盖语音、声音、音乐及其组合。该基准评估49项独特技能，涉及长音频理解、空间音频推理和多音频理解等复杂维度。所有问题要求多步骤推理，并采用选择题和开放回答形式。音频数据来源于真实环境，而非已有数据集。评估显示，即使最先进的模型如Gemini 2.5 Flash和Audio Flamingo 3也仅达到约60%和52%的准确率，表明当前AI在音频智能方面仍存在明显不足。研究提供了改进方向，助力AI向音频通用智能发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 12:33:49 GMT</pubDate>
</item>
<item>
<title>POML：一种用于复杂提示管理的标记语言</title>
<link>https://arxiv.org/abs/2508.13948</link>
<guid>https://arxiv.org/abs/2508.13948</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">POML提升大型语言模型提示的结构化与可维护性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为POML（Prompt Orchestration Markup Language）的新型标记语言，旨在解决大型语言模型在提示设计中面临的结构不清晰、数据整合困难、格式敏感等问题。POML通过组件化标记、专用标签和类似CSS的样式系统，实现内容与展示的分离，并提供模板化功能和开发工具包，提升提示的灵活性和协作效率。研究通过案例分析和用户测试验证了POML在复杂应用集成和准确性方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13948" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 11:37:29 GMT</pubDate>
</item>
<item>
<title>OmniTry：一种扩展至多种可穿戴物品的虚拟试穿框架</title>
<link>https://arxiv.org/abs/2508.13632</link>
<guid>https://arxiv.org/abs/2508.13632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniTry扩展了虚拟试穿任务，支持多种可穿戴物品。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniTry，一个统一的虚拟试穿框架，不仅限于衣物，还扩展到珠宝和配饰等可穿戴物品。该框架在无需掩码的情况下进行物体定位和外观一致性迁移。通过两个阶段的训练流程，首先利用大量未配对图像进行预训练，然后使用少量配对图像进行微调。实验表明，OmniTry在对象定位和ID保留方面优于现有方法，并在包含12类可穿戴物品的基准数据集上进行了评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 04:47:31 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的人类痛苦感知预测研究</title>
<link>https://arxiv.org/abs/2508.12669</link>
<guid>https://arxiv.org/abs/2508.12669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用LLM预测人类对情景的痛苦评分，提升情感推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了使用大语言模型（LLMs）从自然语言描述中预测人类感知的痛苦分数。任务被建模为回归问题，模型将每个输入语句分配一个0到100之间的数值。研究评估了多种提示策略，包括零样本、固定上下文少样本和基于BERT句子嵌入的检索提示。结果表明，少样本方法在预测准确性上优于零样本基线，突显了上下文示例在情感预测中的价值。为进一步测试模型表现，研究引入了“痛苦游戏秀”这一新颖的互动框架，模拟电视节目形式，通过结构化回合测试模型的顺序比较、二分类、标量估计和反馈驱动推理能力。该方法不仅评估预测精度，还考察模型根据纠正反馈进行适应的能力。实验展示了LLMs在动态情感推理任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 03:02:59 GMT</pubDate>
</item>
<item>
<title>无需训练的图像与视频颜色编辑方法ColorCtrl</title>
<link>https://arxiv.org/abs/2508.09131</link>
<guid>https://arxiv.org/abs/2508.09131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ColorCtrl实现精准且一致的颜色编辑，提升图像和视频质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出ColorCtrl，一种无需训练的图像和视频颜色编辑方法，利用多模态扩散Transformer的注意力机制，通过分离结构与颜色实现精确控制。该方法在SD3和FLUX.1-dev等模型上表现出色，优于现有方法，并在一致性方面超越商业模型。扩展至CogVideoX等视频模型时，展现出更强的时间连贯性和编辑稳定性，同时适用于指令式编辑模型，展示出广泛适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>多视觉参考的可控图像生成研究</title>
<link>https://arxiv.org/abs/2508.06905</link>
<guid>https://arxiv.org/abs/2508.06905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多视觉参考下的图像生成挑战与数据集构建。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于利用多个视觉参考进行可控图像生成的任务，提出并评估了MultiRef-bench框架，包含990个合成样本和1000个真实世界样本。通过RefBlend数据引擎生成的33种参考组合，构建了包含38k张高质量图像的MultiRef数据集。实验结果显示，即使最先进的模型在多参考条件下仍表现有限，表明需要更灵活的创意工具来整合多种视觉灵感。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 05:36:21 GMT</pubDate>
</item>
<item>
<title>Chain-of-Agents：一种新型的端到端多智能体推理范式</title>
<link>https://arxiv.org/abs/2508.13167</link>
<guid>https://arxiv.org/abs/2508.13167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Chain-of-Agents实现端到端多智能体协作，提升复杂问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Chain-of-Agents（CoA）的新范式，使大型语言模型能够以多智能体系统的方式进行端到端的复杂问题解决。该方法通过动态激活不同工具代理和角色扮演代理，模拟多智能体协作。研究引入了多智能体蒸馏框架，将先进多智能体系统转化为链式代理轨迹，用于代理监督微调。随后通过可验证代理任务的代理强化学习进一步提升模型能力，形成了Agent Foundation Models（AFMs）。实验表明，AFM在多个基准测试中均取得最新性能，研究全部开源，为未来代理模型和代理强化学习提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:01:02 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习在大型语言模型中的应用</title>
<link>https://arxiv.org/abs/2508.12790</link>
<guid>https://arxiv.org/abs/2508.12790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVR扩展至开放任务，提升语言模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了基于可验证奖励的强化学习（RLVR）在大型语言模型中的应用。传统RLVR依赖于可自动验证的奖励信号，如代码测试或数学答案匹配，限制了其适用范围。为解决这一问题，研究引入了基于评分标准的奖励机制，通过设计的评分标准对主观输出进行自动评分。研究构建了目前最大的评分系统，包含10,000多个由人类、大模型或人机协作生成的评分标准。实验表明，该方法在开放任务中表现出色，仅需5000个样本即可在人文类任务中提升5.2%，超越671B参数的DeepSeek-V3模型。同时，该方法还能实现更自然的语言风格控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:06:08 GMT</pubDate>
</item>
<item>
<title>面向机器遗忘方法的可视化评估系统研究</title>
<link>https://arxiv.org/abs/2508.12730</link>
<guid>https://arxiv.org/abs/2508.12730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Unlearning Comparator系统，用于评估机器遗忘方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器遗忘（MU）领域中方法评估困难的问题，提出了一种可视化分析系统Unlearning Comparator。该系统支持模型比较和隐私攻击模拟两大任务，帮助研究人员在不同层次上分析模型行为，并通过模拟成员推断攻击评估方法的隐私保护效果。实验表明，该系统有助于深入理解模型变化并优化MU方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 04:53:53 GMT</pubDate>
</item>
<item>
<title>G-CUT3R：一种融合先验信息的3D场景重建方法</title>
<link>https://arxiv.org/abs/2508.11379</link>
<guid>https://arxiv.org/abs/2508.11379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">G-CUT3R通过引入先验信息提升3D场景重建性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出G-CUT3R，一种改进的前馈式3D场景重建方法。该方法在CUT3R基础上引入深度、相机标定和位置等先验信息，通过为每种模态设计专用编码器并利用零卷积融合RGB图像特征，实现灵活的多模态输入整合。实验表明，该方法在多个基准测试中均表现出显著性能提升，具备良好的兼容性和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:25:58 GMT</pubDate>
</item>
<item>
<title>多模态模型在空间智能方面的进展与挑战</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态模型在空间智能上取得进步，但仍不及人类。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了多模态模型在空间理解与推理方面的最新进展与不足。尽管近年来取得了显著成果，但这些模型在空间智能方面仍存在明显局限。通过对GPT-5等前沿模型的评估，研究发现其在空间任务中表现出色，但仍未达到人类水平。此外，研究还揭示了多模态模型在面对复杂空间问题时的挑战，并指出专有模型在极端任务中并不具备明显优势。文章还通过定性分析展示了人类直观的任务对当前模型的困难。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:55:17 GMT</pubDate>
</item>
<item>
<title>视觉动作提示：跨领域复杂交互视频生成的新方法</title>
<link>https://arxiv.org/abs/2508.13104</link>
<guid>https://arxiv.org/abs/2508.13104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉动作提示以平衡动作精度与跨域适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种统一的动作表示——视觉动作提示，用于复杂高自由度交互的视频生成，同时保持跨领域的可迁移视觉动态。现有方法在动作驱动的视频生成中面临精度与泛化性的权衡问题，而该研究通过将动作“渲染”为通用的视觉提示，实现了几何精度与跨域适应性的结合。研究利用视觉骨架作为通用表示，并从人-物交互和灵巧机器人操作数据中构建骨架，从而实现跨域训练。通过轻量级微调，将视觉骨架集成到预训练视频生成模型中，实现对复杂交互的精准控制并保留跨域动态学习能力。实验表明该方法在EgoVid、RT-1和DROID数据集上均有效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:12:28 GMT</pubDate>
</item>
<item>
<title>HeroBench：评估大语言模型长程规划能力的新基准</title>
<link>https://arxiv.org/abs/2508.12782</link>
<guid>https://arxiv.org/abs/2508.12782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeroBench测试LLM在复杂虚拟环境中的长期规划与结构化推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HeroBench，一个专门用于评估大语言模型（LLMs）在复杂RPG风格虚拟世界中长程规划和结构化推理能力的新基准。该基准包含广泛难度的任务、模拟执行环境以及性能分析工具，旨在挑战模型制定战略计划、收集资源、掌握技能、制作装备并击败对手的能力。对25个先进LLM的评估揭示了其在生成稳健高层计划和可靠执行结构化动作方面的显著差异，表明传统推理基准难以反映真实场景中的复杂性。HeroBench不仅推动了LLM推理能力的评估，也为未来自主规划研究提供了灵活且可扩展的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 05:59:02 GMT</pubDate>
</item>
<item>
<title>评估提升大语言模型提示鲁棒性的方法</title>
<link>https://arxiv.org/abs/2508.11383</link>
<guid>https://arxiv.org/abs/2508.11383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比5种提升LLM提示鲁棒性的方法，涵盖多个模型与任务。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了5种提升大语言模型（LLM）提示鲁棒性的方法，在统一实验框架下对Llama、Qwen和Gemma系列的8个模型进行了测试，覆盖Natural Instructions数据集中的52项任务。研究包括微调和上下文学习两种范式下的鲁棒性方法，并测试其在多种分布偏移下的泛化能力。此外，还扩展分析了GPT-4.1和DeepSeek V3，评估前沿模型对格式扰动的鲁棒性。研究结果为实际应用中选择有效的鲁棒性方法提供了实用参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11383" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:32:50 GMT</pubDate>
</item>
<item>
<title>大型推理模型在主动信息获取能力上的不足与挑战</title>
<link>https://arxiv.org/abs/2508.11252</link>
<guid>https://arxiv.org/abs/2508.11252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型模型在主动提问方面存在缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文指出，尽管大型推理模型在数学问题解决上表现出色，但它们在面对信息不完整的任务时缺乏主动询问的能力，这限制了其作为真正智能代理的潜力。研究团队构建了一个包含多种情境的不完整问题数据集，并基于此对模型进行了系统评估，发现模型在主动获取信息方面表现不佳，还存在过度思考和幻觉现象。文章进一步探讨了监督微调在提升这一能力上的潜力与挑战，旨在推动更具真实智能的模型发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 02:42:00 GMT</pubDate>
</item>
<item>
<title>4DNeX：基于单张图像的高效4D场景生成框架</title>
<link>https://arxiv.org/abs/2508.13154</link>
<guid>https://arxiv.org/abs/2508.13154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4DNeX通过微调视频扩散模型实现单图到4D场景的高效生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出4DNeX，首个从单张图像生成动态3D场景的前馈框架。与依赖优化或多帧视频输入的方法不同，4DNeX通过微调预训练视频扩散模型实现端到端的图像到4D生成。研究构建了大规模高质量的4D数据集4DNeX-10M，并引入统一的6D视频表示，结合RGB和XYZ序列进行外观与几何建模。同时提出简单有效的适应策略，使预训练模型适用于4D建模。实验表明，4DNeX在效率和泛化性上优于现有方法，为动态场景模拟提供了可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Matrix-Game 2.0：基于扩散模型的实时交互视频生成框架</title>
<link>https://arxiv.org/abs/2508.13009</link>
<guid>https://arxiv.org/abs/2508.13009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Matrix-Game 2.0，实现高速实时交互视频生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Matrix-Game 2.0的交互式世界模型，能够通过少量步骤的自回归扩散生成高质量的长视频。该框架包含三个关键组件：大规模视频数据生成管道、动作注入模块以及基于因果结构的少步蒸馏方法。其在Unreal Engine和GTA5环境中生成超过1200小时的多样化交互视频数据，并支持实时鼠标键盘输入。该模型可在25 FPS的速度下生成分钟级高质量视频，显著提升了交互视频生成的实时性能。研究已开源模型权重与代码库，以推动交互世界建模领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 11:28:53 GMT</pubDate>
</item>
<item>
<title>基于大规模视频生成模型的视频重新照明方法</title>
<link>https://arxiv.org/abs/2508.12945</link>
<guid>https://arxiv.org/abs/2508.12945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumen框架实现视频背景替换与光照一致性调整。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lumen，一个基于大规模视频生成模型的端到端视频重新照明框架，通过灵活的文本指令控制光照和背景。为解决高质量配对视频数据不足的问题，构建了包含真实与合成视频的大规模数据集。合成数据利用先进3D渲染引擎生成，真实数据通过HDR光照模拟补充。设计联合训练流程以发挥各领域优势，并引入领域感知适配器分离光照与场景学习。实验表明，Lumen能有效生成具有统一光照和严格前景保留的电影级重照明视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 10:21:22 GMT</pubDate>
</item>
<item>
<title>S^2-Guidance：提升扩散模型生成质量的新方法</title>
<link>https://arxiv.org/abs/2508.12880</link>
<guid>https://arxiv.org/abs/2508.12880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S^2-Guidance通过随机块丢弃提升扩散模型生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对扩散模型中广泛使用的Classifier-free Guidance (CFG) 方法存在的不足，提出了一种新的优化方法S^2-Guidance。通过在前向过程中引入随机块丢弃，构建随机子网络，从而引导模型避免低质量预测，提升生成效果。实验表明，S^2-Guidance在文本到图像和文本到视频任务中均优于CFG及其他先进方法，具有显著的性能优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 08:31:20 GMT</pubDate>
</item>
<item>
<title>基于视觉粒度序列的图像生成方法研究</title>
<link>https://arxiv.org/abs/2508.12811</link>
<guid>https://arxiv.org/abs/2508.12811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像生成框架，提升生成质量与控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于视觉粒度序列的图像生成方法，将图像分解为具有相同空间分辨率但不同唯一标记数量的结构化序列，通过Next Visual Granularity (NVG) 框架逐步生成图像，从整体布局到细节逐步优化。该方法在ImageNet数据集上进行训练，并展现出良好的扩展性。实验结果表明，NVG在FID分数上优于VAR系列模型，证明了其在图像生成质量上的优势。研究还展示了该框架的潜力和应用前景，相关代码和模型将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:47:37 GMT</pubDate>
</item>
<item>
<title>逆向多模态学习方法Invers-LLaVA突破传统对齐预训练范式</title>
<link>https://arxiv.org/abs/2508.12466</link>
<guid>https://arxiv.org/abs/2508.12466</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出逆向多模态学习方法，无需对齐预训练即可实现视觉与语言融合。</p><br /><br /><p><strong>摘要：</strong> 本文提出Invers-LLaVA，一种颠覆传统多模态学习范式的新型方法。该方法摒弃了传统的视觉特征到文本空间的映射方式，转而将文本嵌入映射到连续视觉表示空间，并在Transformer中间层进行融合。通过注意力机制中的选择性加法组件，实现了视觉与文本表示的动态整合，无需依赖大规模图像-文本对齐数据集。实验表明，在推理密集型任务中表现显著提升，而在依赖记忆的感知任务中略有下降。结果证明，对齐预训练并非多模态学习的必要条件，尤其在复杂推理任务中效果更优。该方法降低了45%的计算需求，挑战了传统模态融合观念，为高效多模态架构提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12466" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 14:36:04 GMT</pubDate>
</item>
<item>
<title>基于生物听觉机制的语音表示学习模型AuriStream</title>
<link>https://arxiv.org/abs/2508.11598</link>
<guid>https://arxiv.org/abs/2508.11598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AuriStream通过双阶段框架模拟人类听觉处理，实现高效语音表示学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AuriStream，一种受人类听觉处理机制启发的语音编码模型。该模型采用双阶段框架：第一阶段将原始音频转换为基于人耳耳蜗的时间-频率表示，并提取离散的耳蜗标记；第二阶段在这些标记上应用自回归序列模型。AuriStream能够学习有意义的音素和词表示以及最先进的词汇语义，表现出在多种下游SUPERB语音任务中的竞争力。此外，AuriStream能够生成音频延续，并在频谱图空间中可视化，有助于理解模型预测。整体而言，该模型为开发更接近人类的语音处理系统提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:06:04 GMT</pubDate>
</item>
<item>
<title>Ovis2.5：提升多模态推理与视觉感知的新型模型</title>
<link>https://arxiv.org/abs/2508.11737</link>
<guid>https://arxiv.org/abs/2508.11737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis2.5提升视觉感知与多模态推理能力，实现高精度分析。</p><br /><br /><p><strong>摘要：</strong> Ovis2.5是Ovis2的升级版本，专注于原生分辨率视觉感知和强大的多模态推理。该模型采用原生分辨率视觉Transformer，避免固定分辨率拼接带来的细节损失，适用于复杂图表等视觉密集内容。通过引入反思机制（如自我检查与修正），增强推理能力，并提供可选的“思考模式”以提升准确性。训练过程采用五阶段课程，涵盖基础预训练、指令调优及对齐优化。为提高效率，使用多模态数据打包和混合并行技术，显著提升整体性能。释放了两个开源模型Ovis2.5-9B和Ovis2.5-2B，后者在资源受限场景中表现出色。在OpenCompass基准测试中，Ovis2.5-9B取得78.3的平均分，超越前代并达到开源多模态大语言模型的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:01:08 GMT</pubDate>
</item>
<item>
<title>基于动态记忆的长文本推理方法ComoRAG</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ComoRAG通过动态交互提升长文本推理效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为ComoRAG的新型检索增强生成方法，旨在解决长篇故事和小说中的叙事理解难题。传统RAG方法因单次检索过程无法捕捉长距离上下文中的复杂关系而效果有限。ComoRAG模拟人类认知过程，通过迭代推理与动态记忆空间交互，不断生成新查询并整合新信息，从而构建连贯的上下文支持问题解决。在四个大规模长文本基准测试中，ComoRAG相比最强基线提升了11%的性能，尤其在需要全局理解的复杂查询中表现突出。该方法提供了一种具有认知启发性的状态化推理框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 03:52:09 GMT</pubDate>
</item>
<item>
<title>高效大语言模型架构研究综述</title>
<link>https://arxiv.org/abs/2508.09834</link>
<guid>https://arxiv.org/abs/2508.09834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述高效大语言模型架构，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了针对传统Transformer架构局限性的创新大语言模型（LLM）架构，旨在提高计算效率和可扩展性。文章从语言建模出发，涵盖了线性与稀疏序列建模方法、高效的全注意力变体、稀疏专家混合模型、结合多种技术的混合架构，以及新兴的扩散LLM。同时探讨了这些技术在多模态中的应用及其对构建资源感知基础模型的深远影响。通过分类整理近期研究，本文为现代高效LLM架构提供了一个蓝图，希望推动更高效、多功能的人工智能系统发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 10:13:46 GMT</pubDate>
</item>
<item>
<title>BeyondWeb：提升预训练语言模型合成数据质量的新框架</title>
<link>https://arxiv.org/abs/2508.10975</link>
<guid>https://arxiv.org/abs/2508.10975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BeyondWeb合成数据框架显著提升预训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BeyondWeb，一个用于生成高质量合成数据的预训练框架。该框架在多个基准测试中表现优于现有最佳合成数据集，如Cosmopedia和Nemotron-Synth。实验表明，使用BeyondWeb训练的3B模型在180B token下超越了在Cosmopedia上训练的8B模型。研究还揭示了合成数据质量的关键影响因素，强调优化需综合考虑多种因素，而非单一方法。虽然简单方法可能带来有限提升，但精心设计的方法能实现显著改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10975" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:55:47 GMT</pubDate>
</item>
<item>
<title>基于奖励引导解码的多模态大语言模型适应方法</title>
<link>https://arxiv.org/abs/2508.11616</link>
<guid>https://arxiv.org/abs/2508.11616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种奖励引导解码方法提升MLLM视觉定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了通过控制解码来适应多模态大语言模型（MLLMs）的方法。作者引入了首个用于MLLM的奖励引导解码方法，并应用于改善其视觉定位能力。该方法构建了两个独立的奖励模型，分别控制输出中的对象精确度和召回率。该方法实现了对MLLM推理过程的实时可控性，允许用户在图像描述任务中动态调整精确度与召回率的平衡，以及控制解码搜索的广度，从而在计算资源与视觉定位精度之间取得平衡。实验表明，该方法在标准物体幻觉基准测试中表现出色，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:29:06 GMT</pubDate>
</item>
<item>
<title>基于多维人类偏好优化的音频驱动肖像动画方法</title>
<link>https://arxiv.org/abs/2508.11255</link>
<guid>https://arxiv.org/abs/2508.11255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Talking-Critic与TLPO提升音频驱动肖像动画的多维一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对音频驱动肖像动画中存在的多维偏好对齐难题，提出Talking-Critic多模态奖励模型和TLPO框架。Talking-Critic通过学习人类偏好函数量化生成视频的质量，而TLPO通过分解偏好为专家模块并跨时间步与网络层融合，实现无干扰的多维优化。实验表明，该方法在唇形同步、运动自然性和视觉质量等方面均优于现有方法，并构建了包含41万条偏好对的Talking-NSQ数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 02:43:46 GMT</pubDate>
</item>
<item>
<title>MAESTRO：面向多模态遥感数据的自监督学习方法</title>
<link>https://arxiv.org/abs/2508.10894</link>
<guid>https://arxiv.org/abs/2508.10894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MAESTRO在多时相遥感任务中取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文针对遥感数据的独特性，对多模态、多时相和多光谱数据的融合策略与重建目标归一化方案进行了全面基准测试。基于研究结果，提出了一种名为MAESTRO的新方法，该方法是对掩码自编码器的改进，采用了优化的融合策略和定制的目标归一化方案，并引入了光谱先验作为自监督信号。在四个遥感数据集上评估表明，MAESTRO在依赖多时相动态的任务中表现优异，同时在单时相任务中也保持了竞争力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:58:45 GMT</pubDate>
</item>
<item>
<title>大型语言模型在强化学习中的模拟搜索应用研究</title>
<link>https://arxiv.org/abs/2508.10874</link>
<guid>https://arxiv.org/abs/2508.10874</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs可作为高效模拟器用于RL搜索任务，减少对外部引擎依赖。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在强化学习（RL）中作为高效模拟器的潜力，以减少对昂贵外部搜索引擎的依赖。通过结构化提示和重复采样，研究人员量化了LLMs的内在搜索能力，并将其称为Self-Search。实验结果表明，LLMs在推理预算增加时表现出强大的扩展性，在问答基准测试中取得了高pass@k成绩。基于此，作者提出了Self-Search RL（SSRL），通过基于格式和规则的奖励增强LLMs的Self-Search能力，使模型能够内部迭代优化知识利用，而无需外部工具。实证评估显示，SSRL训练的策略模型为搜索驱动的RL训练提供了成本效益高且稳定的环境，减少了对外部搜索引擎的依赖，并促进了模拟到现实的迁移。研究得出三个结论：LLMs具备可有效激发的世界知识；SSRL展示了利用内部知识减少幻觉的潜力；SSRL训练的模型可无缝与外部搜索引擎集成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10874" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:46:01 GMT</pubDate>
</item>
<item>
<title>TexVerse：大规模高分辨率3D纹理数据集发布</title>
<link>https://arxiv.org/abs/2508.10868</link>
<guid>https://arxiv.org/abs/2508.10868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TexVerse提供1.6M高分辨率3D模型，推动纹理合成与PBR材料开发。</p><br /><br /><p><strong>摘要：</strong> TexVerse是一个大规模的高分辨率3D纹理数据集，包含超过858,000个独特的3D模型，其中158,000个带有基于物理的渲染（PBR）材质。该数据集总共有1.6M个3D实例，并包含专门的子集如TexVerse-Skeleton和TexVerse-Animation，分别包含带骨骼和动画的模型。每个模型都保留了原始的结构和细节信息，并附有详细的注释。TexVerse为纹理合成、PBR材料开发、动画以及多种3D视觉和图形任务提供了高质量的数据资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:43:25 GMT</pubDate>
</item>
<item>
<title>X-Node：一种可解释的图神经网络框架</title>
<link>https://arxiv.org/abs/2508.10461</link>
<guid>https://arxiv.org/abs/2508.10461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Node通过节点自解释提升图神经网络的可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Node，一种自解释的图神经网络框架，使每个节点在预测过程中生成自己的解释。该框架构建结构化上下文向量，包含度、中心性、聚类、特征显著性和标签一致性等可解释线索，并通过轻量级Reasoner模块生成解释向量。解释向量用于重建节点嵌入、生成自然语言解释以及通过文本注入机制指导GNN。实验表明，X-Node在保持分类准确性的前提下提供了忠实的节点级解释。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10461" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:00:45 GMT</pubDate>
</item>
<item>
<title>XQuant：通过低比特量化提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2508.10395</link>
<guid>https://arxiv.org/abs/2508.10395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XQuant通过低比特量化显著降低内存消耗，提升推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了XQuant，一种通过低比特量化技术减少大语言模型（LLM）推理内存占用的方法。与传统的KV缓存方式不同，XQuant量化并缓存层输入激活X，在推理过程中实时重新生成Keys和Values，从而实现两倍的内存节省。实验表明，XQuant在保持接近FP16精度的情况下，可实现高达7.7倍的内存节省。进一步提出的XQuant-CL利用跨层X嵌入的相似性，实现高达10倍的内存压缩，仅带来0.01的困惑度损失。该方法充分利用硬件计算能力，有效缓解了LLM推理中的内存瓶颈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 02:52:38 GMT</pubDate>
</item>
<item>
<title>DINOv3：实现自监督学习愿景的视觉基础模型</title>
<link>https://arxiv.org/abs/2508.10104</link>
<guid>https://arxiv.org/abs/2508.10104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINOv3通过自监督学习提升视觉模型性能，无需微调即可超越现有技术。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DINOv3，这是一种基于自监督学习的视觉基础模型，旨在减少对人工标注数据的依赖。通过扩大数据集和模型规模，并引入Gram anchoring方法解决密集特征图在长期训练中的退化问题，DINOv3在多种视觉任务中表现出色，优于现有的自监督和弱监督模型。此外，该模型还具备灵活的分辨率、模型大小和与文本对齐的能力，适用于多种资源约束和部署场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 14:00:55 GMT</pubDate>
</item>
<item>
<title>Thyme：通过代码实现图像处理与逻辑推理的新型多模态大模型框架</title>
<link>https://arxiv.org/abs/2508.11630</link>
<guid>https://arxiv.org/abs/2508.11630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Thyme通过代码实现图像处理与逻辑推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Thyme，一种新的多模态大语言模型（MLLM）范式，旨在超越传统的‘思考与图像’方法。Thyme能够自主生成并执行多种图像处理和计算操作，同时增强逻辑推理能力。该模型通过两阶段训练策略进行优化：首先在50万样本的数据集上进行监督微调，随后通过强化学习进一步优化决策过程。为了提高学习难度，研究者手动收集并设计了高分辨率问答对，并提出GRPO-ATS算法，以不同温度控制文本和代码生成，平衡推理探索与代码执行精度。实验表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其在高分辨率感知和复杂推理任务中表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:59:49 GMT</pubDate>
</item>
<item>
<title>StyleMM：基于文本描述的风格化3D可变形模型框架</title>
<link>https://arxiv.org/abs/2508.11203</link>
<guid>https://arxiv.org/abs/2508.11203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StyleMM通过文本控制生成风格化3D人脸模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出StyleMM，一种基于用户文本描述生成风格化3D可变形模型（3DMM）的新框架。该方法利用预训练的网格变形网络和纹理生成器，并通过文本引导的图像到图像翻译（i2i）扩散模型生成风格化面部图像作为训练目标。为保持原始面部特征，引入了保留面部属性的风格化方法，确保在风格迁移过程中身份、对齐和表情的一致性。训练完成后，StyleMM能够实现对形状、表情和纹理参数的显式控制，生成具有连贯顶点连接性和可动画性的3D人脸模型。实验表明，该方法在身份级面部多样性与风格化能力方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:29:46 GMT</pubDate>
</item>
<item>
<title>PaperRegister：支持细粒度论文检索的系统</title>
<link>https://arxiv.org/abs/2508.11116</link>
<guid>https://arxiv.org/abs/2508.11116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PaperRegister提升细粒度论文搜索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出PaperRegister，一种结合离线分层索引和在线自适应检索的论文搜索系统。传统系统依赖摘要构建索引，难以支持细粒度查询。PaperRegister通过构建分层索引树，有效提升不同粒度下的检索效果，尤其在细粒度场景中表现突出，展现出良好的实际应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 19:43:46 GMT</pubDate>
</item>
<item>
<title>基于GAN的半监督学习框架在低标注数据医学影像中的应用</title>
<link>https://arxiv.org/abs/2508.06429</link>
<guid>https://arxiv.org/abs/2508.06429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种GAN半监督方法，提升低标注数据下的医学影像分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于生成对抗网络（GAN）的半监督学习框架，旨在解决医学影像领域中标注数据不足的问题。该框架包含生成器、判别器和分类器三部分，在有限的标注数据下通过图像翻译进行无监督学习，并结合集成伪标签和时间一致性机制提升预测可靠性。实验表明，该方法在多个MedMNIST数据集上均优于现有方法，尤其在5样本/类的极端情况下表现优异，为高成本标注场景提供了有效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:16:43 GMT</pubDate>
</item>
<item>
<title>Puppeteer：自动3D模型绑定与动画生成框架</title>
<link>https://arxiv.org/abs/2508.10898</link>
<guid>https://arxiv.org/abs/2508.10898</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Puppeteer实现3D模型自动绑定与高质量动画生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Puppeteer，一个用于3D模型自动绑定和动画生成的综合框架。该系统通过自回归Transformer预测骨骼结构，并采用基于关节的标记化策略和分层排序方法提升双向学习能力。同时，利用注意力机制推断皮肤权重，结合拓扑感知的关节注意力编码关节关系。此外，Puppeteer还引入了基于优化的动画生成管道，提高计算效率并生成稳定、高保真的动画效果。实验结果表明，该方法在骨骼预测准确性和皮肤质量方面均优于现有技术，适用于多种3D内容，有效解决了动态3D内容生成中的瓶颈问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10898" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>STream3R：基于Transformer的实时3D重建方法</title>
<link>https://arxiv.org/abs/2508.10893</link>
<guid>https://arxiv.org/abs/2508.10893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STream3R利用Transformer实现高效3D重建，性能优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出STream3R，一种将点云预测重新定义为解码器-only Transformer问题的新型3D重建方法。与依赖全局优化或简单记忆机制的传统方法不同，STream3R采用流式框架，通过因果注意力机制处理图像序列，借鉴了现代语言模型的进展。该方法通过大规模3D数据集学习几何先验，能够有效应对动态场景等复杂情况。实验表明，STream3R在静态和动态场景基准测试中均优于现有方法，并且与LLM训练基础设施兼容，支持大规模预训练和微调，为实时3D感知提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:58:05 GMT</pubDate>
</item>
<item>
<title>自然语言处理中隐私与可解释性的权衡研究</title>
<link>https://arxiv.org/abs/2508.10482</link>
<guid>https://arxiv.org/abs/2508.10482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了NLP中隐私与可解释性的关系及共存可能性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了可信自然语言处理中的隐私与可解释性之间的关系。尽管近年来对可解释性和隐私保护的研究显著增加，但两者交叉领域的研究仍较少。本文通过实证分析，探讨了在差分隐私和事后可解释性方法下，隐私与可解释性之间的权衡问题。研究发现，两者的相互作用受到下游任务、文本隐私化方法和可解释性方法选择的影响。文章指出隐私与可解释性并非必然对立，提出了未来在该领域工作的实用建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:34:29 GMT</pubDate>
</item>
<item>
<title>ToonComposer：统一动画中间帧生成与上色的AI模型</title>
<link>https://arxiv.org/abs/2508.10881</link>
<guid>https://arxiv.org/abs/2508.10881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToonComposer提升动画制作效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ToonComposer的生成模型，用于统一处理动画制作中的中间帧生成和上色阶段。该模型通过稀疏草图注入机制提供精确控制，并利用空间低秩适配器将现代视频基础模型适配到卡通领域。ToonComposer仅需少量草图和参考帧即可生成高质量动画，同时支持多草图输入以实现更精细的运动控制，显著降低人工工作量并提高灵活性。为评估模型性能，研究者还构建了PKBench基准测试集。实验结果表明，ToonComposer在视觉质量、运动一致性及生产效率方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:50:11 GMT</pubDate>
</item>
<item>
<title>扩散语言模型的现状与展望</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散语言模型在生成效率和上下文捕捉方面表现出色。</p><br /><br /><p><strong>摘要：</strong> 本文全面综述了扩散语言模型（DLMs）的发展现状，分析了其与自回归模型和掩码语言模型的关系，并探讨了预训练、后训练以及推理优化等关键技术。文章还介绍了DLM在多模态任务中的应用及其面临的挑战，如效率、长序列处理和基础设施需求，并提出了未来研究方向。项目代码可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:47:22 GMT</pubDate>
</item>
<item>
<title>基于Pass@k的强化学习探索能力研究</title>
<link>https://arxiv.org/abs/2508.10751</link>
<guid>https://arxiv.org/abs/2508.10751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pass@k训练提升RLVR探索能力，优化策略平衡。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在强化学习中使用Pass@k作为奖励机制对模型探索能力的影响。传统RLVR依赖Pass@1奖励，导致策略偏向保守，难以跳出局部最优。通过将Pass@k作为奖励进行训练，实验表明模型探索能力得到提升。进一步分析发现，Pass@k训练能够有效优化优势函数设计，使探索与利用目标相互促进，而非冲突。该研究为RLVR中的策略优化提供了新思路，并展示了在优势函数设计方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 11:34:47 GMT</pubDate>
</item>
<item>
<title>NextStep-1：基于离散文本与连续图像标记的先进文本到图像生成模型</title>
<link>https://arxiv.org/abs/2508.10711</link>
<guid>https://arxiv.org/abs/2508.10711</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NextStep-1在文本到图像生成中表现优异，具备高保真图像合成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出NextStep-1，一个14B参数的自回归模型，结合157M参数的流匹配头，通过预测下一个标记实现文本到图像的生成。该模型在离散文本标记和连续图像标记上进行训练，表现出卓越的性能，在高保真图像合成和图像编辑任务中均取得优异结果。研究团队计划开源代码和模型以促进开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10711" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 10:54:22 GMT</pubDate>
</item>
<item>
<title>视觉编码器对图像采集参数的敏感性分析</title>
<link>https://arxiv.org/abs/2508.10637</link>
<guid>https://arxiv.org/abs/2508.10637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像采集参数影响视觉编码器的语义预测效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉编码器在面对图像采集过程中的细微或不可察觉的变换时的表现。不同于以往关注严重图像损坏的研究，本文聚焦于那些可能被人类忽略的参数变化。研究发现，这些参数在视觉表示中被系统性地编码，并可被恢复。这些参数的存在对语义预测有显著影响，其效果取决于语义标签与采集或处理相关标签之间的相关性或反相关性。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 09:34:13 GMT</pubDate>
</item>
<item>
<title>基于可解释机器学习的自动口译质量评估框架</title>
<link>https://arxiv.org/abs/2508.10860</link>
<guid>https://arxiv.org/abs/2508.10860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多维建模框架提升口译质量评估的透明性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对自动口译质量评估中存在的语言使用质量分析不足、数据稀缺和模型不可解释等问题，提出一种融合特征工程、数据增强和可解释机器学习的多维建模框架。该框架通过使用透明特征并结合SHAP分析，提升了模型的可解释性。实验表明，在英中连续口译数据集上，BLEURT和CometKiwi得分对忠实度具有最强预测力，停顿相关特征对流利度有显著影响，而中文特有的短语多样性指标则对语言使用质量有重要贡献。该方法提供了一种可扩展、可靠且透明的替代传统人工评估的方式，有助于为学习者提供详细诊断反馈，支持自主学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:31:18 GMT</pubDate>
</item>
<item>
<title>UI-Venus：基于多模态大语言模型的高效UI代理系统</title>
<link>https://arxiv.org/abs/2508.10833</link>
<guid>https://arxiv.org/abs/2508.10833</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-Venus在UI识别与导航任务中表现优异，超越现有SOTA模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UI-Venus，一个仅以截图作为输入的原生UI代理系统，基于多模态大语言模型Qwen2.5-VL进行强化微调。该系统在UI识别和导航任务中表现出色，7B和72B版本分别在Screenspot-V2/Pro基准测试中达到94.1%/50.8%和95.3%/61.9%的准确率，优于现有SOTA模型如GTA1和UI-TARS-1.5。此外，在AndroidWorld导航任务中，其成功率达到49.1%和65.9%。研究团队设计了专门的奖励函数和数据清洗策略，并提出Self-Evolving Trajectory History Alignment与Sparse Action Enhancement方法，提升导航性能。项目代码已开源，旨在推动社区进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10833" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 12:58:07 GMT</pubDate>
</item>
<item>
<title>HumanSense：评估多模态大语言模型的人类中心交互能力</title>
<link>https://arxiv.org/abs/2508.10576</link>
<guid>https://arxiv.org/abs/2508.10576</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HumanSense评估MLLM在理解人类意图和提供共情反馈方面的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HumanSense，一个用于评估多模态大语言模型（MLLM）在人类中心场景中感知与交互能力的基准。研究指出，当前领先的MLLM在复杂交互任务上仍有较大提升空间，尤其是在结合视觉、音频和文本信息时表现更优。文章强调了推理能力在生成合理反馈中的关键作用，并通过多阶段、模态渐进式强化学习提升了模型表现。此外，研究还发现成功推理过程具有高度一致的思维模式，通过设计特定提示可有效提升非推理模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10576" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 08:14:15 GMT</pubDate>
</item>
<item>
<title>We-Math 2.0：提升多模态大模型数学推理能力的统一系统</title>
<link>https://arxiv.org/abs/2508.10433</link>
<guid>https://arxiv.org/abs/2508.10433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">We-Math 2.0提升多模态大模型数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出We-Math 2.0，一个旨在提升多模态大语言模型（MLLMs）数学推理能力的统一系统。该系统包含四个关键贡献：数学知识体系MathBook、MathBook-Standard与MathBook-Pro数据集、基于强化学习的MathBook-RL训练框架以及全面的MathBookEval评估基准。通过结构化知识构建、数据空间建模和强化学习优化，We-Math 2.0在多个数学推理任务中表现出色，展现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 04:15:41 GMT</pubDate>
</item>
<item>
<title>PRELUDE基准测试：评估长上下文理解能力</title>
<link>https://arxiv.org/abs/2508.09848</link>
<guid>https://arxiv.org/abs/2508.09848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRELUDE挑战模型对长上下文的理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> PRELUDE是一个用于评估长上下文理解能力的基准测试，任务是判断一个角色的前传故事是否符合原著的正典叙事。该任务比现有基准更具挑战性，因为前传并非原故事的一部分，评估其合理性通常需要整合多个部分的信息。实验结果显示，即使使用最先进的大语言模型和商业服务，其表现仍落后于人类超过15%。进一步的人类研究发现，模型有时能给出正确答案但推理过程有误，导致推理准确率与人类相差超过30%。这些结果表明，长上下文理解和推理仍有很大提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 10:28:25 GMT</pubDate>
</item>
<item>
<title>基于mu参数化的Mixture-of-Experts模型研究</title>
<link>https://arxiv.org/abs/2508.09752</link>
<guid>https://arxiv.org/abs/2508.09752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出muP参数化方法提升MoE模型性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，大语言模型（LLM）的广泛应用推动了超参数调优技术的发展，其中muTransfer成为关键方法。同时，Mixture-of-Experts（MoE）架构在大规模模型中表现出色。然而，两者结合的研究仍处于空白。本文提出了mu-Parameterization（muP）用于MoE模型，理论分析了其在不同模型宽度下的特征学习能力，并通过实验验证了该参数化方法的有效性。此外，研究还探讨了专家数量和粒度对最优学习率的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:31:27 GMT</pubDate>
</item>
<item>
<title>GFPO：通过优化训练策略减少大模型响应长度膨胀</title>
<link>https://arxiv.org/abs/2508.09726</link>
<guid>https://arxiv.org/abs/2508.09726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GFPO优化训练策略，减少模型响应长度膨胀。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GFPO（Group Filtered Policy Optimization）的新方法，旨在解决大型语言模型在使用强化学习训练时出现的响应长度膨胀问题。该方法通过在训练过程中采样更大的问题组，并基于响应长度和每token奖励效率两个指标过滤响应，从而减少冗余内容。实验表明，在Phi-4-reasoning模型上，GFPO在保持准确率的同时，显著减少了响应长度，尤其在STEM和编程基准测试中效果明显。此外，GFPO还引入了自适应难度机制，动态分配训练资源以提升计算效率与准确性的平衡。研究证明，增加训练时间的计算资源可以有效减少推理时间的计算需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 07:43:49 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在模糊问题下的鲁棒性</title>
<link>https://arxiv.org/abs/2508.07321</link>
<guid>https://arxiv.org/abs/2508.07321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究测试大语言模型在模糊问题下的表现，发现其易产生错误回答。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ObfusQAte的新技术，并基于此构建了ObfusQA框架，用于评估大语言模型（LLMs）在面对经过混淆的问题时的鲁棒性和适应能力。该框架通过三种不同的混淆层次——命名实体误导、干扰项误导和上下文过载，系统地测试LLMs的表现。研究发现，当面对这些复杂变化时，LLMs往往无法正确回答或产生幻觉式回应。为了促进相关领域的研究，作者公开了ObfusQAte工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 08:27:52 GMT</pubDate>
</item>
<item>
<title>基于3DGS的场景重建与修复方法GSFixer</title>
<link>https://arxiv.org/abs/2508.09667</link>
<guid>https://arxiv.org/abs/2508.09667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSFixer提升稀疏视角下3DGS重建质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出GSFixer框架，旨在解决从稀疏视角重建3D场景时因信息不足导致的明显伪影问题。该方法基于DiT视频扩散模型，结合参考引导的视频修复技术，利用2D语义特征和3D几何特征提升重建结果的语义一致性和3D一致性。为评估3DGS伪影修复效果，作者还构建了DL3DV-Res数据集。实验表明，GSFixer在3DGS伪影修复和稀疏视角3D重建任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 05:56:28 GMT</pubDate>
</item>
<item>
<title>基于自适应扫描的细粒度医学图像分割方法ASM-UNet</title>
<link>https://arxiv.org/abs/2508.07237</link>
<guid>https://arxiv.org/abs/2508.07237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASM-UNet提升细粒度医学图像分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于Mamba的新型细粒度医学图像分割架构ASM-UNet，通过引入自适应扫描得分来动态调整扫描顺序，结合群体共性和个体差异，提升了对小尺度解剖结构的分割精度。在ACDC、Synapse和新提出的胆道系统FGS数据集BTMS上的实验表明，ASM-UNet在粗粒度和细粒度分割任务中均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 04:33:03 GMT</pubDate>
</item>
<item>
<title>CannyEdit：一种改进的文本到图像区域编辑框架</title>
<link>https://arxiv.org/abs/2508.06937</link>
<guid>https://arxiv.org/abs/2508.06937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CannyEdit提升文本引导图像编辑的准确性与自然度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CannyEdit的训练-free 图像编辑框架，旨在解决现有方法在文本遵循性、上下文保真度和编辑融合度之间的平衡问题。该框架通过两项创新技术：Selective Canny Control 实现了对编辑区域的精准控制并保留未编辑区域的细节；Dual-Prompt Guidance 结合局部与全局提示，确保场景一致性。实验表明，CannyEdit在真实图像编辑任务中优于KV-Edit等方法，提升了文本遵循性和上下文保真度，并在编辑融合度上显著优于竞争对手。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 07:06:58 GMT</pubDate>
</item>
<item>
<title>基于多智能体强化学习的无人机协同操控方法</title>
<link>https://arxiv.org/abs/2508.01522</link>
<guid>https://arxiv.org/abs/2508.01522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种去中心化方法实现无人机协同操控负载。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种首次实现现实世界中6自由度负载操控的去中心化方法，利用多智能体强化学习（MARL）训练每个微型飞行器的外层控制策略。该方法无需全局状态、无人机间通信或邻近信息，仅通过负载姿态观测进行隐式通信，提高了可扩展性和灵活性，并降低了计算成本，支持 onboard 部署。研究还引入了基于线性加速度和机体速率的新动作空间设计，结合稳健的低级控制器，实现了可靠的仿真到现实迁移。实验验证了方法在负载模型不确定性下的全姿态控制性能，并展示了异构控制策略的协作能力和对单个无人机失效的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 19:52:33 GMT</pubDate>
</item>
<item>
<title>Story2Board：无需训练的叙事分镜生成框架</title>
<link>https://arxiv.org/abs/2508.09983</link>
<guid>https://arxiv.org/abs/2508.09983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Story2Board提升分镜生成的连贯性和叙事性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Story2Board，一种无需训练的自然语言到叙事分镜的生成框架。现有方法仅关注主体身份，忽视了空间构图、背景演变和叙事节奏等关键因素。Story2Board通过两个轻量级组件——潜在面板锚定和互注意值混合，增强视觉一致性而不改变架构或微调模型。该框架利用现成的语言模型生成结构化分镜提示，并引入Rich Storyboard Benchmark评估布局多样性与背景相关叙事能力。同时提出Scene Diversity指标衡量空间和姿态变化。实验表明，Story2Board在动态性、连贯性和叙事吸引力方面优于现有基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:56:26 GMT</pubDate>
</item>
<item>
<title>VisCodex：融合视觉与代码的多模态语言模型框架</title>
<link>https://arxiv.org/abs/2508.09945</link>
<guid>https://arxiv.org/abs/2508.09945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisCodex提升多模态代码生成能力，表现接近商业模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出VisCodex，一个将视觉和代码语言模型融合的统一框架，旨在增强多模态大语言模型的代码生成能力。通过任务向量模型融合技术，将先进的代码生成模型与强大的视觉语言基础模型结合，同时保留视觉理解和高级编码技能。研究还发布了Multimodal Coding Dataset (MCD) 和InfiBench-V基准测试，用于训练和评估模型。实验表明，VisCodex在开源多模态模型中表现最佳，接近GPT-4o等商业模型，验证了其方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:00:44 GMT</pubDate>
</item>
<item>
<title>自动化生成文本解释提升NLP模型性能</title>
<link>https://arxiv.org/abs/2508.09776</link>
<guid>https://arxiv.org/abs/2508.09776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化生成的文本解释可有效提升NLP模型性能。</p><br /><br /><p><strong>摘要：</strong> 在可解释自然语言处理领域，文本解释对于理解模型预测和丰富数据集至关重要。传统方法依赖人工标注，成本高且难以扩展。本文提出一种自动化框架，利用多个先进的大语言模型生成高质量文本解释，并通过自然语言生成指标评估其质量。研究还探讨了这些解释对预训练语言模型在自然语言推理任务中的影响。实验表明，自动化生成的解释在提升模型性能方面与人工标注具有竞争力，为扩展NLP数据集和增强模型性能提供了可行路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:59:08 GMT</pubDate>
</item>
<item>
<title>M3-Agent：具备长期记忆的多模态智能体框架</title>
<link>https://arxiv.org/abs/2508.09736</link>
<guid>https://arxiv.org/abs/2508.09736</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3-Agent具备长期记忆与多模态处理能力，提升任务完成效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了M3-Agent，一个具备长期记忆的多模态智能体框架。该框架能够处理实时视觉和听觉输入，构建并更新长期记忆，不仅包含情景记忆，还发展出语义记忆，以积累世界知识。其记忆以实体为中心的多模态形式组织，有助于更深入理解环境。M3-Agent能自主进行多轮推理，并从记忆中检索信息完成任务。为评估多模态智能体的记忆效果和基于记忆的推理能力，作者开发了M3-Bench基准测试集，包含机器人视角视频和网络视频数据。实验结果显示，M3-Agent在多个基准上优于现有最强基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09736" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:03:03 GMT</pubDate>
</item>
<item>
<title>轻量级身份保持视频生成框架Stand-In</title>
<link>https://arxiv.org/abs/2508.07901</link>
<guid>https://arxiv.org/abs/2508.07901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出轻量级视频生成框架Stand-In，实现高效身份保持。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Stand-In的轻量级、即插即用的视频生成身份保持框架。该框架通过在预训练视频生成模型中引入条件图像分支，利用受限自注意力机制和条件位置映射实现身份控制，并仅需2000对数据即可快速学习。尽管仅增加了1%的额外参数，该框架在视频质量和身份保持方面表现优异，优于其他全参数训练方法。此外，该框架可无缝集成到其他任务中，如基于主体的视频生成、姿态参考视频生成、风格化和人脸交换。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 08:17:38 GMT</pubDate>
</item>
<item>
<title>GRAO：融合SFT与RL的高效语言模型对齐方法</title>
<link>https://arxiv.org/abs/2508.07750</link>
<guid>https://arxiv.org/abs/2508.07750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRAO提升语言模型对齐效果，融合SFT与RL优势。</p><br /><br /><p><strong>摘要：</strong> 本文提出GRAO（Group Relative Alignment Optimization），一种结合监督微调（SFT）和强化学习（RL）的统一框架，旨在解决语言模型对齐中的效率与效果问题。GRAO通过多样本生成策略、组内相对优势加权损失函数以及参考感知参数更新机制，提升了模型的收敛速度和样本效率。理论分析表明其具有更好的收敛性和效率，实验结果显示在多个复杂对齐任务中，GRAO分别比SFT、DPO、PPO和GRPO基线提升了57.70%、17.65%、7.95%和5.18%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 04:28:47 GMT</pubDate>
</item>
<item>
<title>基于隐式奖励的自适应元微调方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AMFT方法通过动态平衡SFT与RL提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为自适应元微调（AMFT）的新方法，旨在解决大语言模型在推理任务中因两阶段微调导致的灾难性遗忘和模仿与探索之间的权衡问题。AMFT通过隐式奖励理论，将监督微调（SFT）和强化学习（RL）视为互补的奖励信号，并引入一个可学习的元梯度权重控制器，动态优化两者之间的平衡。该方法通过策略熵正则化确保稳定性，能够自动发现有效的训练课程。实验表明，AMFT在多个基准测试中表现优异，特别是在数学推理、抽象视觉推理和视觉-语言导航任务中均达到新的最先进水平，并展现出更强的分布外泛化能力。消融实验和训练动态分析验证了元学习控制器对模型稳定性、样本效率和性能的关键作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 07:40:54 GMT</pubDate>
</item>
<item>
<title>GPT-4o合成数据提升开放模型图像生成能力</title>
<link>https://arxiv.org/abs/2508.09987</link>
<guid>https://arxiv.org/abs/2508.09987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPT-4o合成数据可增强开放模型图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用GPT-4o生成的合成图像数据来提升开放源代码模型图像生成能力的潜力。研究指出，合成图像能补充真实数据集中的罕见场景，并提供更清晰、可控的监督信号。基于此，作者构建了180K规模的Echo-4o-Image数据集，并用于微调基础模型Echo-4o。此外，还提出了两个新的评估基准GenEval++和Imagine-Bench，以更准确地衡量图像生成能力。实验结果表明，该数据集在多个标准基准上表现优异，并且对其他基础模型也具有良好的迁移性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:59:28 GMT</pubDate>
</item>
<item>
<title>基于噪声超网络的测试时扩展优化方法</title>
<link>https://arxiv.org/abs/2508.09968</link>
<guid>https://arxiv.org/abs/2508.09968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出噪声超网络替代测试时优化，提升模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对测试时扩展（test-time scaling）方法在推理阶段计算成本过高的问题，提出一种新的解决方案。通过引入噪声超网络替代传统的奖励引导噪声优化，实现对初始输入噪声的调制。该方法在保持基础模型精度的同时，显著降低了计算成本，并有效恢复了测试时优化带来的性能提升。实验表明，该方法在生成模型中具有良好的应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:33:37 GMT</pubDate>
</item>
<item>
<title>基于动态监督的多智能体系统提升问题解决稳定性</title>
<link>https://arxiv.org/abs/2508.09889</link>
<guid>https://arxiv.org/abs/2508.09889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态监督机制提升多智能体系统稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型在智能代理中的应用，指出随着代理依赖多种工具，面临上下文复杂和工具输出噪声等问题，影响系统可靠性。为解决这一问题，研究提出动态监督与操控机制，在AWorld框架内构建了稳健的多智能体系统（MAS）。执行代理在关键步骤调用守护代理进行验证与修正，有效减少错误并提高解决问题的鲁棒性。实验表明，该系统在GAIA数据集上表现优于单代理系统和标准工具增强系统，取得优异成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 11:46:25 GMT</pubDate>
</item>
<item>
<title>输入感知后门攻击方法在视觉语言模型中的应用</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新型后门攻击方法，操控VLM的视觉定位行为。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为IAG的输入感知后门攻击方法，旨在操控视觉语言模型（VLMs）的视觉定位行为。该方法通过将目标对象的语义信息嵌入原始图像中，使模型无论用户查询如何，都会将特定目标对象作为定位结果。为提高攻击隐蔽性，采用了重建损失以减少被污染图像与干净图像之间的视觉差异，并提出了统一的攻击数据生成方法。实验表明，IAG在多个模型上表现出高成功率，同时对干净样本的准确率影响较小，展示了其有效性、鲁棒性和迁移能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 23:22:19 GMT</pubDate>
</item>
<item>
<title>Mol-R1：提升分子生成推理能力的新型框架</title>
<link>https://arxiv.org/abs/2508.08401</link>
<guid>https://arxiv.org/abs/2508.08401</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mol-R1提升LLM在分子生成中的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Mol-R1，一种改进R1类显式长链思维（Long-CoT）大语言模型在文本驱动分子生成任务中解释性和推理能力的新框架。该框架通过PRID方法构建高质量推理数据集，并采用MoIA训练策略，结合监督微调与强化策略优化，显著提升了模型在分子发现领域的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08401" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 14:50:05 GMT</pubDate>
</item>
<item>
<title>基于D2F的扩散语言模型加速方法</title>
<link>https://arxiv.org/abs/2508.09192</link>
<guid>https://arxiv.org/abs/2508.09192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">D2F提升扩散语言模型推理速度，超越传统模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为离散扩散强制（D2F）的策略，使扩散大语言模型（dLLMs）在推理速度上超越相同规模的自回归（AR）模型。D2F通过块级自回归生成和跨块并行解码，实现KV缓存利用与高效推理。该方法基于预训练dLLMs进行非对称蒸馏，并结合流水线并行解码算法，在保持输出质量的同时显著提升效率。实验表明，D2F模型在GSM8K任务中推理速度是LLaMA3和Qwen2.5的2.5倍以上，比原始dLLMs快50倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:51:37 GMT</pubDate>
</item>
<item>
<title>MathReal：面向真实教育场景的多模态数学推理数据集</title>
<link>https://arxiv.org/abs/2508.06009</link>
<guid>https://arxiv.org/abs/2508.06009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathReal提升多模态模型在真实教育场景中的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MathReal，一个包含2000个数学问题的多模态数据集，这些问题来自真实的K-12教育场景，图像由手持设备拍摄。数据集根据图像质量、视角变化和干扰内容分为14个子类，并覆盖五个核心知识领域和三种难度级别。研究设计了六个实验设置来评估多模态大语言模型在现实场景中的表现，发现现有模型在真实教育环境中面临显著挑战。通过分析模型性能与错误模式，提出了未来改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:39:16 GMT</pubDate>
</item>
<item>
<title>Cooper框架提升大语言模型推理能力与奖励模型鲁棒性</title>
<link>https://arxiv.org/abs/2508.05613</link>
<guid>https://arxiv.org/abs/2508.05613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cooper框架提升大模型推理与奖励模型鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cooper框架，通过联合优化策略模型和奖励模型，解决现有奖励机制在鲁棒性和抗奖励欺骗方面的不足。Cooper结合规则奖励的高精度与动态样本对训练，增强模型稳定性。同时引入混合标注策略和基于参考答案的奖励建模方法，提升了奖励模型的准确性。实验表明，Cooper有效缓解了奖励欺骗问题，并在端到端强化学习中表现出色，如在Qwen2.5-1.5B-Instruct上提升了0.54%的平均准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:53:56 GMT</pubDate>
</item>
<item>
<title>面向域适应的变更检测视觉问答方法研究</title>
<link>https://arxiv.org/abs/2508.08974</link>
<guid>https://arxiv.org/abs/2508.08974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TCSSM模型应对CDVQA中的域偏移问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对变更检测视觉问答（CDVQA）任务中因域偏移导致性能下降的问题，引入了多模态、多域数据集BrightVQA，并提出Text-Conditioned State Space Model（TCSSM）模型。该模型通过统一处理双时相图像和与地质灾害相关的文本信息，提取跨域不变特征，提升模型在不同域下的泛化能力。实验表明，TCSSM在多个基准测试中表现优于现有方法，代码和数据集将在论文接受后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 10:37:53 GMT</pubDate>
</item>
<item>
<title>RedDino：面向红细胞图像分析的自监督基础模型</title>
<link>https://arxiv.org/abs/2508.08180</link>
<guid>https://arxiv.org/abs/2508.08180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RedDino是用于红细胞分析的先进AI模型，性能优于现有技术。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RedDino，一个专为红细胞图像分析设计的自监督基础模型。该模型基于DINOv2框架，并在包含125万张红细胞图像的数据集上进行训练，涵盖了多种成像模态和来源。实验表明，RedDino在红细胞形态分类任务中表现优于当前最先进的模型。通过线性探测和最近邻分类评估，验证了其强大的特征表示能力和泛化能力。研究的主要贡献包括：针对红细胞分析的基础模型、DINOv2配置的消融研究以及泛化性能的详细评估。RedDino解决了计算血液学中的关键挑战，推动了可靠诊断工具的发展。源代码和预训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 12:59:31 GMT</pubDate>
</item>
<item>
<title>ASTRA：系统化检测AI代码生成安全缺陷的自动化代理系统</title>
<link>https://arxiv.org/abs/2508.03936</link>
<guid>https://arxiv.org/abs/2508.03936</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASTRA提升AI代码生成系统的安全性，发现更多漏洞。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ASTRA，一个用于检测AI代码生成和安全指导系统中安全缺陷的自动化代理系统。ASTRA通过构建领域知识图谱、在线漏洞探索以及生成高危测试用例三个阶段，有效识别现实场景中的潜在问题。相比现有方法，ASTRA在两个主要评估领域中发现了11-66%更多的问题，并提升了17%的对齐训练效果，展示了其在构建更安全AI系统方面的实际价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03936" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 17:57:52 GMT</pubDate>
</item>
<item>
<title>Putnam-AXIOM：评估大语言模型数学推理的新基准</title>
<link>https://arxiv.org/abs/2508.08292</link>
<guid>https://arxiv.org/abs/2508.08292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Putnam-AXIOM测试大语言模型数学推理能力，揭示模型依赖记忆而非推理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Putnam-AXIOM，一个包含522道大学级数学竞赛题的基准测试集，以及其100道通过程序扰动生成的变体题。该基准旨在克服训练数据污染问题，并提供动态、抗污染的评估方式。实验表明，即使是最强模型o1-preview在变体题上的准确率也显著下降，说明模型可能依赖记忆而非真正推理。文章还引入了Teacher-Forced Accuracy（TFA）作为衡量推理过程的新指标，以更准确地评估模型的数学推理能力。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:57:50 GMT</pubDate>
</item>
<item>
<title>逻辑指令理解挑战与基准测试研究</title>
<link>https://arxiv.org/abs/2508.09125</link>
<guid>https://arxiv.org/abs/2508.09125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前大语言模型在复杂逻辑指令理解上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在处理复杂逻辑指令方面的表现，提出了LogicIFGen和LogicIFEval框架与基准。LogicIFGen用于自动生成可验证的逻辑指令，而LogicIFEval包含426个复杂的逻辑指令测试集。实验表明，当前最先进的大语言模型在遵循这些指令时表现不佳，多数只能正确理解不到60%的指令，显示出在指令理解能力上的显著缺陷。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:54:27 GMT</pubDate>
</item>
<item>
<title>基于LLM的低资源语言数据生成方法TopXGen</title>
<link>https://arxiv.org/abs/2508.08680</link>
<guid>https://arxiv.org/abs/2508.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TopXGen提升低资源语言机器翻译性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为TopXGen的基于大语言模型的方法，用于生成高质量且主题多样的低资源语言数据。该方法通过利用大语言模型在高资源语言上的翻译能力，生成自然流畅的目标语言文本，再通过回译生成可用于上下文学习和微调的平行语料。实验表明，TopXGen能有效提升大语言模型在低资源语言翻译任务中的表现。相关代码和结果已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 02:58:02 GMT</pubDate>
</item>
<item>
<title>无需微调的大型语言模型在《外交》游戏中的评估框架</title>
<link>https://arxiv.org/abs/2508.07485</link>
<guid>https://arxiv.org/abs/2508.07485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">无需微调即可让LLM玩《外交》，提升战略推理评估。</p><br /><br /><p><strong>摘要：</strong> 本文提出了首个无需微调或专门训练即可让本地大型语言模型（LLMs）完整参与《外交》游戏的评估框架。由于《外交》游戏状态复杂且信息密集，以往研究依赖前沿模型或微调。本研究通过数据驱动的迭代优化文本游戏状态表示，使24B参数模型无需微调即可完成对局。同时开发了工具支持假设测试和统计分析，并进行了关于说服、激进玩法和模型表现的案例研究。实验表明，大模型表现最佳，但小模型也能合理应对。文中还引入了关键状态分析方法，用于深入研究对局关键时刻。该框架降低了战略推理评估的门槛，并揭示了广泛使用的LLM中自然涌现的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 17:07:08 GMT</pubDate>
</item>
<item>
<title>基于Q语言的大型语言模型适配与优化研究</title>
<link>https://arxiv.org/abs/2508.06813</link>
<guid>https://arxiv.org/abs/2508.06813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究针对Q语言进行LLM适配，提升模型在量化金融领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将大型语言模型（LLM）适配到Q编程语言，该语言在量化金融领域广泛应用，但在互联网上信息较少，因此对通用AI模型而言是一个挑战。研究构建了一个类似Leetcode的评估数据集，并对多个前沿模型进行了基准测试。随后，通过预训练、监督微调和强化学习，训练了一系列基于Qwen-2.5系列的模型，涵盖多种参数规模。实验结果显示，最佳模型在Q语言任务上的准确率达到了59%，显著优于Claude Opus-4和其他模型。此外，所有模型，包括最小的1.5B版本，均超过了GPT-4.1的表现。研究还提供了完整的模型训练流程和方法论，适用于其他领域和任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 00:22:07 GMT</pubDate>
</item>
<item>
<title>无需重建与优化的3D高斯点云风格迁移方法</title>
<link>https://arxiv.org/abs/2508.05813</link>
<guid>https://arxiv.org/abs/2508.05813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重建的3D高斯点云快速风格迁移方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需重建或优化的3D高斯点云风格迁移方法。该方法通过在点云隐式表面生成图结构，然后使用基于表面的前馈风格迁移方法，并将其插值回场景中的每个点云。此方法无需额外训练或优化，可快速实现风格迁移，即使在消费级硬件上也能在2分钟内完成。实验表明该方法在风格迁移质量上优于其他方法，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 15:35:01 GMT</pubDate>
</item>
<item>
<title>基于部分卷积的图像局部风格迁移方法</title>
<link>https://arxiv.org/abs/2508.05769</link>
<guid>https://arxiv.org/abs/2508.05769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种局部风格迁移网络，提升图像特定区域的风格化效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统风格迁移方法对整张图像进行处理的问题，提出一种基于部分卷积的风格迁移网络，能够更精确地将艺术风格应用于图像中的特定区域。同时，该方法引入了内部融合技术，以应对区域选择不准确带来的影响。实验表明，该方法在视觉效果和定量评估上均优于现有方法，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 14:35:44 GMT</pubDate>
</item>
<item>
<title>OpenCUA：开源框架推动计算机使用代理研究</title>
<link>https://arxiv.org/abs/2508.09123</link>
<guid>https://arxiv.org/abs/2508.09123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCUA开源框架提升CUA研究能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenCUA，一个用于扩展计算机使用代理（CUA）数据和基础模型的开源框架。该框架包含三个核心部分：用于捕捉人类计算机操作演示的标注基础设施、首个覆盖多操作系统和应用的大规模CUA数据集AgentNet，以及可扩展的数据转换管道，能够通过反思式长链式思维实现性能提升。实验表明，基于OpenCUA的模型在多个CUA基准测试中表现优异，其中OpenCUA-32B在OSWorld-Verified任务中达到34.8%的成功率，成为当前开源模型中的最佳表现。研究团队已公开工具、数据集、代码和模型，以支持后续CUA研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:52:32 GMT</pubDate>
</item>
<item>
<title>VertexRegen：一种连续细节层次的网格生成框架</title>
<link>https://arxiv.org/abs/2508.09062</link>
<guid>https://arxiv.org/abs/2508.09062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VertexRegen实现连续细节层次的网格生成，支持任意时刻停止。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VertexRegen，这是一种新颖的网格生成框架，能够在连续细节层次上生成网格。与传统的自回归方法不同，VertexRegen通过学习顶点分裂过程来反转边折叠，从而实现更灵活的生成方式。实验结果表明，VertexRegen生成的网格质量与现有最佳方法相当，并且能够随时停止生成，得到具有不同细节层次的有效网格。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 12:25:46 GMT</pubDate>
</item>
<item>
<title>量子博弈论在真实硬件上的实验验证</title>
<link>https://arxiv.org/abs/2508.09050</link>
<guid>https://arxiv.org/abs/2508.09050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">量子博弈在真实硬件上实现，验证了理论预测。</p><br /><br /><p><strong>摘要：</strong> 本文在IBM Quantum的ibm sherbrooke超导处理器上首次完整实现了Eisert-Wilkens-Lewenstein框架下的‘性别之战’博弈。通过四种量子策略在31个纠缠参数下的实验，对比了理论预测与实际执行结果。为减少噪声影响，作者提出了一种引导电路映射方法（GCM），优化了量子比特选择和路由。尽管存在硬件偏差，实验结果仍保持了理论预期的收益趋势，误差在3.5%-12%之间。这表明在NISQ条件下，量子博弈的优势仍可保持，为未来多智能体、经济和分布式决策系统提供了应用路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 12:10:05 GMT</pubDate>
</item>
<item>
<title>基于课程学习的长度控制推理方法研究</title>
<link>https://arxiv.org/abs/2508.08940</link>
<guid>https://arxiv.org/abs/2508.08940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">课程学习提升大模型推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于组相对策略优化（GRPO）的课程学习策略，用于控制大语言模型的推理长度。该方法从宽松的token预算开始，逐步收紧，促使模型先探索有效解法，再压缩为更简洁的推理过程。通过结合任务正确性、长度效率和格式遵循的奖励函数，实验表明该方法在多个数据集上优于固定预算基线，提升了准确率和计算效率。研究还分析了奖励权重和衰减策略的影响，证明渐进式约束是训练高效推理模型的有效归纳偏置。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 09:48:03 GMT</pubDate>
</item>
<item>
<title>DeCRED：提升编码器-解码器ASR模型鲁棒性的解码器中心正则化方法</title>
<link>https://arxiv.org/abs/2508.08938</link>
<guid>https://arxiv.org/abs/2508.08938</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeCRED通过解码器辅助分类器提升ASR模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DeCRED的解码器中心正则化方法，用于改进编码器-解码器语音识别模型中的内部语言模型。该方法在解码器中添加辅助分类器，通过中间logits实现下一个词的预测。实验结果显示，DeCRED在11个测试集上将内部语言模型的BPE困惑度降低了36.6%。此外，在5个域内和3个域外测试集中，WER表现优于基线模型，分别将宏观WER从6.4%降至6.3%和18.2%降至16.2%。在TEDLIUM3数据集上，DeCRED达到7.0%的WER，优于基线和InterCTC方法。尽管训练数据较少且参数更少，DeCRED仍表现出与OWSM v3.1和Whisper-medium相当的WER表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08938" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 09:44:50 GMT</pubDate>
</item>
<item>
<title>AffordDex：一种具备通用抓取能力的仿人手控制框架</title>
<link>https://arxiv.org/abs/2508.08896</link>
<guid>https://arxiv.org/abs/2508.08896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AffordDex实现通用且类人抓取，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出AffordDex框架，旨在提升机器人手的通用抓取能力。该框架采用两阶段训练策略：第一阶段通过模仿学习获取自然的手部运动先验；第二阶段通过残差模块调整动作以适应具体物体。关键组件包括Negative Affordance-aware Segmentation模块和教师-学生蒸馏过程，确保抓取动作既符合人体工学又功能合理。实验表明，AffordDex在已知、未知及全新类别物体上均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 08:36:01 GMT</pubDate>
</item>
<item>
<title>BiasGym：一种用于分析和减轻大语言模型偏见的框架</title>
<link>https://arxiv.org/abs/2508.08855</link>
<guid>https://arxiv.org/abs/2508.08855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BiasGym框架以检测和减轻LLM中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BiasGym，一个用于在大型语言模型中注入、分析和减轻概念关联的框架。该框架包含两个组件：BiasInject用于通过基于令牌的微调注入特定偏见，而BiasScope则利用这些信号识别并引导导致偏见的行为组件。该方法支持对偏见进行系统分析，实现针对性去偏而不影响下游任务性能，并适用于训练中未见过的偏见。实验表明，BiasGym能有效减少现实世界中的刻板印象，并探测虚构关联，具有安全干预和可解释性研究的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 07:23:44 GMT</pubDate>
</item>
<item>
<title>Aryabhata 1.0：专为印度高考优化的7B参数数学推理模型</title>
<link>https://arxiv.org/abs/2508.08665</link>
<guid>https://arxiv.org/abs/2508.08665</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aryabhata 1.0是专为JEE设计的高效数学推理模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Aryabhata 1.0，一个专为印度高考（JEE）优化的7B参数数学推理模型。该模型通过融合强大的开源推理模型，并结合课程学习的监督微调（SFT）和可验证奖励的强化学习（RLVR），提升了在数学问题解决上的表现。评估结果显示，Aryabhata 1.0在JEE主考、MATH和GSM8K等基准测试中均优于现有模型，同时提供清晰的逐步推理过程。该模型已开源，旨在推动以考试为导向的小型语言模型的发展，并鼓励社区反馈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08665" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 02:20:07 GMT</pubDate>
</item>
<item>
<title>基于单张参考图和2D姿态序列的可控4D角色动画框架CharacterShot</title>
<link>https://arxiv.org/abs/2508.07409</link>
<guid>https://arxiv.org/abs/2508.07409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CharacterShot实现从单图和2D姿态生成稳定4D角色动画。</p><br /><br /><p><strong>摘要：</strong> 本文提出CharacterShot，一个可控且一致的4D角色动画框架，允许设计师仅通过一张参考图和2D姿态序列创建动态3D角色。该方法首先基于先进的DiT图像到视频模型预训练2D角色动画模型，随后通过引入双注意力模块和相机先验将模型提升至3D，生成具有时空和视图一致性的多视角视频。最后，利用新型邻域约束4D高斯点云优化技术，生成连续稳定的4D角色表示。此外，研究者构建了大规模数据集Character4D，包含13,115个独特角色。在新基准CharacterBench上的实验表明，该方法优于现有最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 12:15:04 GMT</pubDate>
</item>
<item>
<title>视频推广攻击：文本到视频检索中的对抗性威胁</title>
<link>https://arxiv.org/abs/2508.06964</link>
<guid>https://arxiv.org/abs/2508.06964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViPro攻击提升视频在文本检索中的排名，揭示T2VR漏洞。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对文本到视频检索（T2VR）的新型对抗性攻击方法ViPro，该攻击通过提升特定视频在检索结果中的排名来实现恶意目的。与以往攻击旨在降低视频相关性不同，ViPro专注于提高视频的相关性评分，可能带来更大的信息误导和经济利益。研究引入了Modal Refinement（MoRe）技术以增强跨模态交互的细粒度理解，从而提升攻击效果。实验覆盖多个T2VR模型和数据集，在多种攻击场景下验证了ViPro的有效性，结果显示其在白盒、灰盒和黑盒设置中均优于现有基线。研究还探讨了防御策略和攻击的隐蔽性，并指出T2VR系统存在被忽视的安全风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 08:20:13 GMT</pubDate>
</item>
<item>
<title>基于时空融合的10米日地表温度估计方法研究</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WGAST模型实现高精度10米地表温度估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WGAST的弱监督生成网络，用于通过融合Terra MODIS、Landsat 8和Sentinel-2数据来估计每日10米分辨率的地表温度（LST）。该方法采用条件生成对抗网络架构，包含特征提取、融合、LST重建和噪声抑制四个阶段。实验表明，与现有方法相比，WGAST在定量和定性评估中均表现更优，平均RMSE降低17.18%，SSIM提升11.00%。此外，该模型对云层引起的LST误差具有鲁棒性，并能有效捕捉细尺度热模式，验证结果来自33个地面传感器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:49:46 GMT</pubDate>
</item>
<item>
<title>基于通用样本重放的大型语言模型持续学习方法</title>
<link>https://arxiv.org/abs/2508.04676</link>
<guid>https://arxiv.org/abs/2508.04676</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GeRe框架解决LLM持续学习中的灾难性遗忘问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在跨领域持续学习中面临的灾难性遗忘问题，提出了一种名为General Sample Replay (GeRe)的框架。该框架利用常规预训练文本进行高效的反遗忘学习，并引入基于阈值的边际损失（TM）方法，以保持激活状态的一致性。实验表明，少量固定预收集的通用样本即可同时保留模型的通用能力并提升任务性能。GeRe框架在多种回放策略下均表现出良好的效果和鲁棒性，为未来LLM的高效回放提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04676" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:42:22 GMT</pubDate>
</item>
<item>
<title>NVSpeech：面向中文的语音情感识别与合成系统</title>
<link>https://arxiv.org/abs/2508.04195</link>
<guid>https://arxiv.org/abs/2508.04195</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NVSpeech整合了语音情感识别与合成，提升自然语音交互体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NVSpeech，一个集成且可扩展的系统，用于语音情感信号（如笑声、呼吸和感叹词）的识别与合成。该系统包括数据集构建、语音识别模型和可控文本转语音模块。首先，作者创建了一个包含48,430条带标注语音的语料库，涵盖18种语音情感类别。其次，开发了能够同时识别词汇和非语言信号的ASR模型，并利用该模型自动生成大规模中文语音数据集。最后，通过微调零样本TTS模型，实现对语音情感的精确控制，从而生成更自然的人类语音。NVSpeech是首个针对普通话的开放、大规模、逐词标注的语音情感建模系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04195" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 04:25:26 GMT</pubDate>
</item>
<item>
<title>利用时间一致性提升扩散语言模型的生成质量</title>
<link>https://arxiv.org/abs/2508.09138</link>
<guid>https://arxiv.org/abs/2508.09138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过时间一致性策略提升扩散语言模型的生成准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了扩散语言模型在生成过程中出现的‘时间振荡’现象，即正确答案常出现在中间步骤但被后续去噪步骤覆盖。为解决此问题，作者提出了两种互补方法：时间自一致性投票（无需训练的解码策略）和时间一致性强化（通过时间语义熵作为奖励信号进行后训练）。实验结果表明，该方法在多个数据集上显著提升了模型性能，如Countdown数据集平均提升24.7%，GSM8K等其他数据集也有明显改进。研究揭示了扩散语言模型中时间动态的潜力，并提供了有效的工具来加以利用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>AutoCodeGen：自动化生成多语言代码生成基准数据集</title>
<link>https://arxiv.org/abs/2508.09101</link>
<guid>https://arxiv.org/abs/2508.09101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoCodeGen生成高质量多语言代码数据集，提升LLM评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出AutoCodeGen，一种无需人工标注的自动化方法，用于生成高难度多语言代码生成数据集。该方法通过LLMs生成测试输入并利用多语言沙箱获取输出，确保数据质量。基于此方法，研究团队构建了AutoCodeBench，包含3920个问题，覆盖20种编程语言，旨在评估大型语言模型在复杂、多样和实际任务中的表现。实验表明，即使最先进的模型也难以应对这些挑战。此外，还推出了AutoCodeBench-Complete，专门用于评估基础模型的少样本代码生成能力。研究希望该系列基准能推动社区关注更复杂的多语言代码生成场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:29:20 GMT</pubDate>
</item>
<item>
<title>提升大语言模型工具使用能力的强化学习方法</title>
<link>https://arxiv.org/abs/2508.08791</link>
<guid>https://arxiv.org/abs/2508.08791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种强化学习框架提升LLM工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在工具使用中的挑战，提出了一种自动化环境构建流程和可验证的奖励机制。该方法通过场景分解、文档生成、功能集成等步骤创建高质量训练环境，并结合轨迹数据与标准强化学习算法进行模型训练。实验表明，该方法显著提升了模型的工具使用性能，同时保持了其通用能力。分析显示，性能提升源于模型下层MLP参数更新带来的上下文理解和推理能力增强。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 05:45:19 GMT</pubDate>
</item>
<item>
<title>基于Diffusion Transformer的电影级镜头生成方法</title>
<link>https://arxiv.org/abs/2508.08244</link>
<guid>https://arxiv.org/abs/2508.08244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Cut2Next框架提升镜头生成的叙事连贯性与电影感。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cut2Next框架，通过引入Hierarchical Multi-Prompting策略和架构创新Context-Aware Condition Injection (CACI)与Hierarchical Attention Mask (HAM)，实现符合专业剪辑模式的高质量后续镜头生成。研究构建了RawCuts和CuratedCuts数据集，并设计CutBench评估基准。实验表明，该方法在视觉一致性、文本匹配度及用户评价上均优于现有方法，尤其在叙事连贯性和电影感方面表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08244" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:56:59 GMT</pubDate>
</item>
<item>
<title>基于分层强化学习的深度搜索框架研究</title>
<link>https://arxiv.org/abs/2508.08088</link>
<guid>https://arxiv.org/abs/2508.08088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HierSearch提升多源信息检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于分层强化学习的深度搜索框架HierSearch，旨在解决传统方法在多源信息检索中的效率低和工具掌握不足的问题。该框架由本地和网络搜索代理组成，并通过规划代理协调工作。同时引入知识精炼模块以过滤错误信息，实验表明其在多个领域任务中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 11:31:47 GMT</pubDate>
</item>
<item>
<title>基于单图或文本提示的全景3D世界生成方法</title>
<link>https://arxiv.org/abs/2508.08086</link>
<guid>https://arxiv.org/abs/2508.08086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Matrix-3D框架实现广域可探索3D世界生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出Matrix-3D框架，利用全景表示实现从单张图像或文本提示生成广域、可探索的3D世界。该框架结合条件视频生成与全景3D重建技术，包含两种3D重建方法：一种是快速的前馈大全景重建模型，另一种是基于优化的高精度重建流程。为支持训练，作者构建了Matrix-Pano数据集，包含11.6万条带深度和轨迹标注的高质量全景视频序列。实验表明，该方法在全景视频生成和3D世界生成任务中均达到当前最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 11:29:57 GMT</pubDate>
</item>
<item>
<title>基于大规模强化学习的搜索代理ASearcher研究</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASearcher提升搜索代理性能，实现长序列搜索与高效训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ASearcher的开源项目，旨在通过大规模强化学习训练搜索代理。当前开放源代码代理在处理复杂查询和长期搜索任务方面存在不足，而ASearcher通过异步强化学习方法实现了高效的长周期搜索能力。该方法利用提示式LLM生成高质量问答数据，并通过训练显著提升了搜索效果。实验结果显示，其QwQ-32B代理在xBench和GAIA基准测试中分别取得46.7%和20.8%的提升。此外，ASearcher-Web-QwQ在无需外部LLM的情况下，达到了42.1和52.8的Avg@4得分，优于现有32B开源模型。项目已开源，提供模型、数据和代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 09:36:57 GMT</pubDate>
</item>
<item>
<title>多模态深度研究代理WebWatcher的开发与评估</title>
<link>https://arxiv.org/abs/2508.05748</link>
<guid>https://arxiv.org/abs/2508.05748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebWatcher提升多模态信息检索能力，超越现有代理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了WebWatcher，一种具备增强视觉-语言推理能力的多模态深度研究代理。该代理通过高质量合成多模态轨迹进行冷启动训练，并利用多种工具进行深度推理，同时通过强化学习提升泛化能力。为评估多模态代理的能力，作者提出了BrowseComp-VL基准测试。实验结果表明，WebWatcher在四个挑战性的VQA基准测试中显著优于现有基线、RAG工作流和开源代理，展示了其在解决复杂多模态信息检索任务方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 14:03:50 GMT</pubDate>
</item>
<item>
<title>基于空间一致性提升GUI接地任务的性能</title>
<link>https://arxiv.org/abs/2508.05615</link>
<guid>https://arxiv.org/abs/2508.05615</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过空间一致性方法提升GUI接地精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的GUI接地方法GUI-RC，利用多个预测结果的空间重叠模式作为隐式置信度信号，提升定位准确性。进一步引入GUI-RCPO，将一致性模式转化为奖励用于测试时强化学习，使模型在无标签数据上迭代优化。实验表明，该方法显著提升了Qwen2.5-VL-3B-Instruct在ScreenSpot基准上的表现，展示了测试时扩展和强化学习在GUI接地中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05615" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:54:27 GMT</pubDate>
</item>
<item>
<title>基于对比注意力引导的无掩码文本到图像生成方法</title>
<link>https://arxiv.org/abs/2508.05399</link>
<guid>https://arxiv.org/abs/2508.05399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UNCAGE提升文本到图像生成的组合准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UNCAGE的新方法，用于改进基于Masked Generative Transformers的文本到图像生成任务。该方法通过利用注意力图来优先解码明确代表单个物体的标记，从而提高组合生成的准确性。与传统扩散模型相比，UNCAGE在多个基准测试中表现出色，且推理开销极低。实验表明，UNCAGE在定量和定性评估中均有效提升了文本与图像的一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 09:51:17 GMT</pubDate>
</item>
<item>
<title>针对智能事实核查系统的新型攻击框架研究</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新型攻击框架Fact2Fiction提升虚假信息传播成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Fact2Fiction，这是一个针对基于LLM的智能事实核查系统的新型中毒攻击框架。该框架模仿系统分解复杂声明的方式，并利用系统生成的解释性理由来制造针对性的恶意证据，从而破坏子声明的验证过程。实验表明，Fact2Fiction在多种中毒预算下比现有攻击方法提高了8.9%至21.2%的攻击成功率，揭示了当前事实核查系统存在的安全漏洞，并强调了防御措施的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 02:44:57 GMT</pubDate>
</item>
<item>
<title>基于步骤熵的思维链压缩框架提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2508.03346</link>
<guid>https://arxiv.org/abs/2508.03346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过步骤熵识别冗余步骤，显著提升LLM推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于步骤熵的思维链（CoT）压缩框架，用于识别和去除大语言模型在复杂推理任务中的冗余步骤。理论分析与实验表明，约80%的低熵步骤可以被有效修剪而不显著影响最终答案的准确性。研究还引入了两阶段训练策略，结合监督微调和组相对策略优化，使模型在推理过程中自主生成压缩的思维链，从而大幅提升推理效率并保持高精度。该方法对实际部署大语言模型具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 07:48:18 GMT</pubDate>
</item>
<item>
<title>TextQuests：评估AI代理长期推理能力的新基准</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextQuests用于评估AI在复杂环境中的长期自主推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TextQuests，这是一个基于Infocom互动小说游戏的基准测试，旨在评估AI代理在需要持续、自主推理的探索性环境中表现。与传统基准不同，TextQuests不依赖外部工具，专注于评估AI在单次交互会话中进行试错学习和持续解决问题的能力。该基准适用于测试大型语言模型在长上下文中的内在推理能力，为AI研究提供了一个新的评估框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23701" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 12:22:55 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理中的研究与实践综述</title>
<link>https://arxiv.org/abs/2508.08221</link>
<guid>https://arxiv.org/abs/2508.08221</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统分析RL在LLM中的应用及优化策略。</p><br /><br /><p><strong>摘要：</strong> 本文综述了强化学习（RL）在大语言模型（LLM）推理中的最新研究进展，重点探讨了算法创新和实际应用中的关键挑战。文章指出，目前缺乏标准化的RL应用指南，且实验设置不一致导致结果差异较大。为解决这些问题，作者在统一的开源框架下进行了系统的复现与评估，通过不同难度的数据集、模型规模和架构进行深入实验，分析了各类RL技术的内部机制和适用场景。基于研究结果，文章提出了针对特定任务的RL技术选择指南，并展示了通过简单组合两种技术即可提升模型性能，优于GRPO和DAPO等方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08221" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:39:45 GMT</pubDate>
</item>
<item>
<title>视觉强化学习的最新进展与综述</title>
<link>https://arxiv.org/abs/2508.08189</link>
<guid>https://arxiv.org/abs/2508.08189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述视觉强化学习的最新发展与关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了视觉强化学习（Visual RL）领域的最新进展，涵盖了从RLHF到可验证奖励范式的策略演进，并梳理了超过200篇代表性研究，分为四个主题：多模态大语言模型、视觉生成、统一模型框架和视觉-语言-动作模型。文章分析了算法设计、奖励工程及基准进展，总结出课程驱动训练、对齐扩散和统一奖励建模等趋势。同时，文章还评估了不同层次的评估协议，并指出了样本效率、泛化能力和安全部署等开放性挑战。旨在为研究人员提供该快速扩展领域的发展图谱，并引导未来研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:08:55 GMT</pubDate>
</item>
<item>
<title>无需训练的精准形状编辑框架Follow-Your-Shape</title>
<link>https://arxiv.org/abs/2508.08134</link>
<guid>https://arxiv.org/abs/2508.08134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的形状编辑方法，实现高精度可控编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Follow-Your-Shape的训练-free 和 mask-free 框架，用于实现对象形状的精确和可控编辑，同时严格保留非目标区域。该方法通过计算反演与编辑轨迹之间的差异生成轨迹发散图（TDM），以精确定位可编辑区域，并引导调度键值注入机制，确保编辑的稳定性和真实性。为促进评估，作者还引入了ReShapeBench基准测试集，包含120张新图像和丰富的提示对。实验表明，该方法在大规模形状替换任务中表现出色，具有更高的编辑能力和视觉保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 12:10:00 GMT</pubDate>
</item>
<item>
<title>评估智能搜索代理在大规模信息收集中的可靠性</title>
<link>https://arxiv.org/abs/2508.07999</link>
<guid>https://arxiv.org/abs/2508.07999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前搜索代理在大规模信息收集中表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WideSearch基准，用于评估基于大型语言模型的搜索代理在大规模信息收集任务中的可靠性。该基准包含200个来自15个不同领域的手动整理问题，涵盖中英文。每个任务要求代理收集可逐项验证的信息并整理成结构化输出。尽管多个人工测试者在充足时间下几乎可以100%完成任务，但现有系统整体成功率接近0%，最佳表现仅5%。这表明当前搜索代理在处理大规模信息任务时存在显著不足，亟需进一步研究与改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 10:03:09 GMT</pubDate>
</item>
<item>
<title>Omni-Effects：统一的视觉特效生成框架</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Omni-Effects实现多特效空间可控生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出Omni-Effects，一个能够生成提示引导和空间可控复合特效的统一框架。该框架通过LoRA-MoE技术整合多种特效并减少任务干扰，并利用Spatial-Aware Prompt实现精准的空间控制。此外，引入IIF模块防止特效间意外融合。研究构建了Omni-VFX数据集并设计专用评估框架，实验表明该方法在空间控制和特效多样性上表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 09:41:24 GMT</pubDate>
</item>
<item>
<title>Action Reasoning Models: 提升机器人感知与行动的结构化推理方法</title>
<link>https://arxiv.org/abs/2508.07917</link>
<guid>https://arxiv.org/abs/2508.07917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARMs通过结构化推理提升机器人任务适应性与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Action Reasoning Models (ARMs)，这是一种将感知、规划和控制结合的视觉-语言-动作模型。MolmoAct作为该类模型的代表，能够通过深度感知令牌编码观察和指令，生成可编辑的空间计划，并预测精确的低级动作，从而实现可解释和可控的行为。在多个模拟和现实任务中，MolmoAct表现出色，包括高零样本准确率、长任务成功率以及任务进展提升。此外，研究团队发布了MolmoAct数据集，包含超过10,000条高质量机器人轨迹，有助于提升模型的泛化能力。该研究为构建基于结构化推理的机器人基础模型提供了开放蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 08:32:45 GMT</pubDate>
</item>
<item>
<title>Grove MoE：一种动态可扩展的专家混合架构</title>
<link>https://arxiv.org/abs/2508.07785</link>
<guid>https://arxiv.org/abs/2508.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Grove MoE通过动态激活不同规模专家提升模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Grove MoE，这是一种基于异构专家设计的新型混合专家（MoE）架构，旨在克服传统MoE中专家规模固定、计算效率受限的问题。Grove MoE引入了动态激活机制和不同规模的专家，提升了模型的扩展性和计算效率。基于该架构，研究团队开发了GroveMoE-Base和GroveMoE-Inst两款33B参数的语言模型，通过中段训练和后期训练策略优化，实现了在不同输入复杂度下动态激活3.14-3.28B参数，并达到与更大规模开源模型相当的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 05:15:36 GMT</pubDate>
</item>
<item>
<title>GLiClass：一种高效的序列分类方法</title>
<link>https://arxiv.org/abs/2508.07662</link>
<guid>https://arxiv.org/abs/2508.07662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLiClass在保持高精度的同时提升分类效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GLiClass的新方法，用于序列分类任务。该方法基于GLiNER架构，结合了嵌入方法的高效性与零样本学习的灵活性，适用于动态变化的分类需求。同时，研究还引入了PPO算法用于多标签文本分类，在数据稀疏或依赖人类反馈的情况下表现良好。相比传统方法，GLiClass在准确性和计算效率上均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 02:22:25 GMT</pubDate>
</item>
<item>
<title>Klear-Reasoner：具备强大推理能力的模型及其优化方法</title>
<link>https://arxiv.org/abs/2508.07629</link>
<guid>https://arxiv.org/abs/2508.07629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Klear-Reasoner在多个基准测试中表现优异，优化了推理训练流程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Klear-Reasoner，一个具备强大推理能力的模型，在数学和编程任务中表现出色。文章详细分析了其训练流程，包括数据准备、长链式思维监督微调（long CoT SFT）和强化学习（RL）。研究发现，少量高质量数据比大量多样化数据更有效，且困难样本无需精度过滤即可获得更好结果。针对当前RL中的裁剪机制问题，提出了GPPO方法，增强了模型探索能力和负样本学习效率。Klear-Reasoner在AIME 2024、AIME 2025、LiveCodeBench V5和V6上分别取得90.5%、83.2%、66.0%和58.1%的高分。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 01:17:51 GMT</pubDate>
</item>
<item>
<title>多语言文档视觉检索基准VisR-Bench的引入与评估</title>
<link>https://arxiv.org/abs/2508.07493</link>
<guid>https://arxiv.org/abs/2508.07493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisR-Bench支持多语言文档的视觉检索，提升跨语言信息获取能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VisR-Bench，一个面向长文档的多语言视觉检索基准。该基准包含超过35,000个高质量问答对，覆盖1,200篇文档和16种语言，涵盖图表、文本和表格三种问题类型。与以往数据集不同，VisR-Bench包含无显式答案的查询，以防止模型依赖关键词匹配。研究评估了多种检索模型，发现尽管多模态大语言模型（MLLMs）表现优于传统方法，但在处理结构化表格和低资源语言时仍存在挑战，揭示了多语言视觉检索的关键难点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 17:44:43 GMT</pubDate>
</item>
<item>
<title>自进化智能代理系统的研究综述</title>
<link>https://arxiv.org/abs/2508.07407</link>
<guid>https://arxiv.org/abs/2508.07407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究自进化智能代理系统的设计与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文综述了自进化智能代理系统的研究进展，提出了一种统一的概念框架，涵盖系统输入、代理系统、环境和优化器四个核心组件。文章系统回顾了针对不同组件的自进化技术，并探讨了在生物医学、编程和金融等特定领域中的演化策略。此外，还讨论了评估、安全性和伦理问题，为构建更适应、自主和持续进化的智能代理系统提供了理论基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 12:07:32 GMT</pubDate>
</item>
<item>
<title>LessIsMore：一种高效的稀疏注意力机制提升推理模型性能</title>
<link>https://arxiv.org/abs/2508.07101</link>
<guid>https://arxiv.org/abs/2508.07101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LessIsMore通过全局注意力模式提升推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LessIsMore的训练-free 稀疏注意力机制，旨在解决大型推理模型在处理短输入提示时因过多token生成导致的计算开销问题。该方法不依赖传统的头部特定局部优化，而是利用全局注意力模式，结合局部注意力头和近期上下文信息进行统一的跨头token排序，从而提高泛化能力和效率。实验表明，LessIsMore在多个推理任务中保持甚至提升了准确率，同时相比全注意力机制实现了1.1倍的解码速度提升，并且仅需处理2倍更少的token，整体端到端速度提升1.13倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 17:10:33 GMT</pubDate>
</item>
<item>
<title>基于推理的列表排序模型ReasonRank的优化与性能提升</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonRank通过推理训练提升列表排序性能，效果优于现有基线。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大型语言模型的列表排序方法ReasonRank，旨在通过增强推理能力提升排序效果。研究首先构建了一个自动化的推理密集型训练数据生成框架，利用DeepSeek-R1生成高质量标签，并通过自一致性过滤机制确保数据质量。随后，采用两阶段微调策略，包括监督学习和强化学习，以提升模型的推理与排序能力。实验表明，ReasonRank在多个任务中表现优异，尤其在BRIGHT基准测试中达到SOTA水平，同时具有更低的延迟。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 13:26:18 GMT</pubDate>
</item>
<item>
<title>通过数据过滤提升开放权重AI系统的安全性</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据过滤可有效增强开放权重AI模型的抗攻击能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过从训练数据中过滤与双重用途相关的文本，以减少开放权重AI系统中的有害能力。研究提出了一种多阶段的数据过滤流程，并展示了其在降低生物威胁代理知识方面的有效性。实验表明，经过过滤的模型对多达10,000步和300M标记的生物威胁文本攻击表现出显著的抵抗力，优于现有后训练方法。尽管过滤模型内部不包含危险知识，但它们仍可能利用上下文中的信息，因此需要多层次防御策略。该研究为开放权重AI系统的安全防护提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>BrowseComp-Plus：提升深度研究系统评估的基准测试</title>
<link>https://arxiv.org/abs/2508.06600</link>
<guid>https://arxiv.org/abs/2508.06600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入BrowseComp-Plus基准提升深度研究系统评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出BrowseComp-Plus，一个基于原有BrowseComp的改进基准，采用固定且精心筛选的文档语料库，以解决现有评估方法在公平性和透明性方面的不足。该基准包含人工验证的支持文档和挑战性负样本，支持可控实验。实验表明，GPT-5搭配Qwen3-Embedding-8B检索器可达到70.1%的准确率，优于其他模型。此基准有助于深入分析检索效果、引用准确性和上下文工程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:55:11 GMT</pubDate>
</item>
<item>
<title>研究通用机器人策略的泛化能力与快捷学习问题</title>
<link>https://arxiv.org/abs/2508.06426</link>
<guid>https://arxiv.org/abs/2508.06426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示快捷学习影响机器人策略泛化，提出数据增强解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于大规模数据集（如Open X-Embodiment）训练的通用机器人策略在任务泛化上的局限性。研究发现，快捷学习——即依赖任务无关特征——是导致泛化能力不足的关键因素。通过理论和实证分析，作者指出两个主要诱因：子数据集内部多样性不足以及子数据集间的分布差异，导致数据集碎片化。这些问题是由于大型数据集通常由不同环境和设备独立收集而产生的。研究提出了改进数据收集策略以减少快捷学习、提升泛化能力的方法，并在无法获取新数据的情况下，验证了数据增强策略的有效性，从而改善了机器人策略在模拟和现实环境中的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:14:01 GMT</pubDate>
</item>
<item>
<title>Temporal Self-Rewarding Language Models提升模型生成能力</title>
<link>https://arxiv.org/abs/2508.06026</link>
<guid>https://arxiv.org/abs/2508.06026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过时间协调机制提升语言模型生成质量与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种改进的Self-Rewarding语言模型架构，通过协调过去、现在和未来的模型生成来维持学习信号。该方法引入了两个关键机制：锚定拒绝（Anchored Rejection）和未来引导选择（Future-Guided Chosen），有效解决了传统Self-Rewarding模型中对比样本表征差异缩小的问题。实验表明，在多个模型家族和不同规模下，该方法显著优于现有Self-Rewarding方法，尤其在数学推理、知识问答和代码生成任务中表现出更强的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 01:25:54 GMT</pubDate>
</item>
<item>
<title>Bifrost-1：融合多模态大模型与扩散模型的高效图像生成框架</title>
<link>https://arxiv.org/abs/2508.05954</link>
<guid>https://arxiv.org/abs/2508.05954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bifrost-1实现高效图像生成，保留多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Bifrost-1，一个将预训练多模态大语言模型（MLLM）与扩散模型结合的统一框架。该框架利用CLIP的补丁级图像嵌入作为潜在变量，通过轻量级ControlNet适配扩散模型，同时为MLLM添加视觉生成分支以保持其多模态推理能力。实验表明，Bifrost-1在视觉保真度和多模态理解上表现优异，且训练计算成本显著降低。研究还进行了全面消融实验，验证了设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 22:38:47 GMT</pubDate>
</item>
<item>
<title>OmniEAR：评估语言模型在具身任务中的推理能力</title>
<link>https://arxiv.org/abs/2508.05614</link>
<guid>https://arxiv.org/abs/2508.05614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型在具身推理中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniEAR框架，用于评估语言模型在物理交互、工具使用和多智能体协作方面的具身推理能力。与以往依赖预设工具集或明确指令的基准不同，OmniEAR要求智能体动态获取能力并自主制定协作策略。通过文本环境建模，研究覆盖了1500个家庭和工业场景。实验发现，当模型需要从约束中推理时，性能显著下降，尤其在隐式协作和复合任务中表现不佳。即使提供完整环境信息，模型仍无法有效过滤无关约束，表明其在具身推理方面存在根本性挑战。该研究为推进具身AI系统提供了重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:54:15 GMT</pubDate>
</item>
<item>
<title>SONAR-LLM：基于连续SONAR嵌入空间的生成模型</title>
<link>https://arxiv.org/abs/2508.05305</link>
<guid>https://arxiv.org/abs/2508.05305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SONAR-LLM在连续嵌入空间中生成文本，提升生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SONAR-LLM，这是一种基于连续SONAR嵌入空间的解码器模型，通过冻结的SONAR解码器传播token级交叉熵进行监督训练。该模型结合了LCM的语义抽象优势，并去除了扩散采样器，恢复了基于似然的训练信号。实验表明，SONAR-LLM在39M到1.3B参数规模下均表现出色，文章还提供了完整的训练代码和预训练检查点以促进研究复现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 08:03:44 GMT</pubDate>
</item>
<item>
<title>基于MoBE的专家混合模型压缩方法研究</title>
<link>https://arxiv.org/abs/2508.05257</link>
<guid>https://arxiv.org/abs/2508.05257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoBE实现高效模型压缩，保持高精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的Mixture-of-Basis-Experts (MoBE) 方法，用于压缩大型专家混合（MoE）语言模型。该方法通过将每个专家的权重矩阵分解为两个部分，其中较大的矩阵通过共享的基础矩阵线性组合进行重参数化，从而在减少参数数量的同时保持较高的模型精度。实验表明，MoBE在多个大规模模型上实现了显著的压缩效果，参数量减少了24%-30%，而准确率下降仅为1%-2%。此方法为部署大规模MoE模型提供了有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 06:48:24 GMT</pubDate>
</item>
<item>
<title>数学表达式语音转录的挑战与新数据集的提出</title>
<link>https://arxiv.org/abs/2508.03542</link>
<guid>https://arxiv.org/abs/2508.03542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个大规模数学语音数据集，提升方程转录准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了将口语数学表达转换为结构化符号表示的挑战，特别是在自动语音识别和语言模型领域。尽管已有进展，但数学表达的语音转录仍存在诸多问题。为此，作者提出了首个完全开源的大规模数据集，包含66,000个英语和俄语数学方程及句子的音频样本。研究还测试了多种模型，包括ASR后校正模型和音频语言模型，在MathSpeech和S2L-equations基准上取得了显著成果。此外，作者还建立了首个数学句子识别基准，并在方程转录任务中达到了40%的字符错误率。该工作为多模态AI在数学内容识别方面的进一步发展奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 11:11:37 GMT</pubDate>
</item>
<item>
<title>音频攻击框架WhisperInject可操控先进语音语言模型生成有害内容</title>
<link>https://arxiv.org/abs/2508.03365</link>
<guid>https://arxiv.org/abs/2508.03365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WhisperInject通过隐蔽音频扰动操控AI生成有害内容，成功率超86%</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型日益融入日常生活，音频成为人机交互的重要接口。然而，这也带来了新的安全风险。本文提出WhisperInject，一种两阶段的对抗性音频攻击框架，能够使先进的语音语言模型生成有害内容。该方法通过人类难以察觉的音频扰动实现攻击，在第一阶段使用基于奖励的优化方法RL-PGD引导目标模型绕过自身安全协议，生成有害响应；第二阶段则将这些有害内容嵌入到正常的音频载体中，如天气查询或问候信息。实验在StrongREJECT、LlamaGuard和人工评估等严格安全框架下验证，成功率达到86%以上，展示了音频攻击对AI行为的潜在威胁。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 08:14:01 GMT</pubDate>
</item>
<item>
<title>UserBench：评估语言模型协作能力的新基准</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLM在用户协作中存在显著差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UserBench，一个以用户为中心的基准测试，用于评估大型语言模型在多轮、偏好驱动交互中的表现。该基准模拟了用户从模糊目标逐步表达偏好的过程，要求模型主动澄清意图并做出合理决策。评估结果显示，即使是最先进的模型，在与用户对齐方面也存在明显不足，仅20%的时间能完全满足用户意图，且仅能发现30%的用户偏好。这表明当前模型更多是任务执行者而非真正的协作伙伴，UserBench为提升这一能力提供了重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:34:12 GMT</pubDate>
</item>
<item>
<title>VLM4D：评估视觉语言模型时空推理能力的基准</title>
<link>https://arxiv.org/abs/2508.02095</link>
<guid>https://arxiv.org/abs/2508.02095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLM4D基准揭示了视觉语言模型在时空推理上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLM4D，这是首个专门用于评估视觉语言模型（VLMs）时空推理能力的基准。该基准包含真实世界和合成视频，并配有强调平移、旋转、视角意识和运动连续性的问答对。通过评估最新的开源和闭源VLMs，研究发现其性能与人类基准存在显著差距，尤其是在整合多视觉线索和保持时间连贯性方面。文章还探讨了利用4D特征场重建和针对性时空微调等方法，以提升模型的时空理解能力，旨在推动更强大可靠的动态环境视觉智能发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 02:06:06 GMT</pubDate>
</item>
<item>
<title>Transformer模型中大规模激活的动态演化分析</title>
<link>https://arxiv.org/abs/2508.03616</link>
<guid>https://arxiv.org/abs/2508.03616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了Transformer模型中大规模激活的数学规律及预测方法。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统分析了Transformer模型在训练过程中大规模激活的演变规律，使用Pythia模型家族进行实验。研究发现，大规模激活的出现遵循可预测的数学模式，可用一个五参数的指数调制对数函数进行建模。作者开发了一个机器学习框架，能够仅根据模型结构预测这些参数，对于稳定性和训练周期等具有重要影响。该成果为模型设计提供了新视角，有助于提前预测和控制大规模激活的出现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:29:51 GMT</pubDate>
</item>
<item>
<title>Lightswitch：基于多视角与材质信息的高效3D光照重渲染方法</title>
<link>https://arxiv.org/abs/2508.06494</link>
<guid>https://arxiv.org/abs/2508.06494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lightswitch通过多视角和材质信息提升3D光照重渲染效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lightswitch，一种基于微调材料-光照扩散框架的新方法，能够在保留物体结构的同时，根据输入图像和多视角数据高效地将任意数量的图像转换为目标光照条件。该方法结合了推断出的固有属性和可扩展去噪方案，显著提升了不同材质物体的光照重渲染质量。实验表明，Lightswitch在合成和真实物体的光照重渲染任务中表现优于现有最先进的方法，并能在短时间内完成高质量渲染。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>GLM-4.5：高性能开源混合专家大语言模型</title>
<link>https://arxiv.org/abs/2508.06471</link>
<guid>https://arxiv.org/abs/2508.06471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.5在多项任务中表现优异，参数量少于竞品。</p><br /><br /><p><strong>摘要：</strong> GLM-4.5是一款开源的混合专家（MoE）大语言模型，拥有355B总参数和32B激活参数，支持思考与直接响应模式。通过多阶段训练和后训练优化，它在代理、推理和编码任务中表现出色，在TAU-Bench、AIME 24和SWE-bench Verified测试中分别获得70.1%、91.0%和64.2%的分数。相比其他模型，GLM-4.5参数更少但性能领先，排名第三，并在代理基准中位列第二。项目提供了完整代码和模型，包括一个轻量版GLM-4.5-Air（106B参数），旨在推动推理和代理AI系统的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:21:06 GMT</pubDate>
</item>
<item>
<title>基于可学习程序记忆的智能代理研究</title>
<link>https://arxiv.org/abs/2508.06433</link>
<guid>https://arxiv.org/abs/2508.06433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Memp机制提升智能代理的程序记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何为智能代理赋予可学习、可更新和持续演进的程序记忆。作者提出Memp方法，将代理的历史轨迹提炼为细粒度指令和高层次抽象，并研究了构建、检索和更新程序记忆的不同策略。结合动态更新机制，该记忆库能与新经验同步演化。实验表明，随着记忆库的优化，代理在类似任务中的成功率和效率显著提升。此外，从强模型中提取的程序记忆对弱模型也有显著性能提升作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:20:56 GMT</pubDate>
</item>
<item>
<title>无监督视觉语言模型适应方法综述</title>
<link>https://arxiv.org/abs/2508.05547</link>
<guid>https://arxiv.org/abs/2508.05547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述无监督VLM适应方法，分类并分析不同数据场景下的策略。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了无监督视觉语言模型（VLM）适应方法，旨在提升其在特定任务中的表现。文章提出了一种基于未标记视觉数据可用性和性质的分类框架，将现有方法分为四类：无数据迁移、无监督领域迁移、周期性测试时适应和在线测试时适应。通过对每种范式的深入分析，本文总结了核心方法与策略，并回顾了多个应用场景的基准测试，指出了当前研究的挑战与未来方向。相关文献资源可在GitHub仓库中获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 12:27:37 GMT</pubDate>
</item>
<item>
<title>操作系统代理研究综述：从基础到未来方向</title>
<link>https://arxiv.org/abs/2508.04482</link>
<guid>https://arxiv.org/abs/2508.04482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述操作系统代理的发展与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了基于操作系统的智能代理（OS Agents）的研究进展，涵盖了其核心组件、构建方法、评估标准以及当前面临的挑战。文章介绍了环境、观察空间和动作空间等关键要素，并探讨了领域特定基础模型和代理框架的构建方法。同时，对评估协议和基准测试进行了详细分析，指出了安全隐私、个性化与自我进化等未来研究方向。该综述旨在为学术界和工业界提供参考，推动相关技术发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 10:33:45 GMT</pubDate>
</item>
<item>
<title>GENIE：结合NeRF与高斯点云的交互式3D场景编辑方法</title>
<link>https://arxiv.org/abs/2508.02831</link>
<guid>https://arxiv.org/abs/2508.02831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GENIE融合NeRF与高斯点云实现高效交互式3D场景编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出GENIE，一种结合神经辐射场（NeRF）与高斯点云（GS）的混合模型，旨在提升3D场景的可编辑性与实时渲染能力。GENIE通过为每个高斯分布分配可训练特征嵌入，并利用最近邻高斯点对NeRF进行条件建模，实现高效的场景编辑。引入基于光线追踪的最近邻搜索算法RT-GPS和多分辨率哈希网格，提升了计算效率与局部感知能力。该方法在保持NeRF高质量渲染的同时，支持实时交互与物理模拟，推动了神经渲染与几何编辑的融合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 14:59:23 GMT</pubDate>
</item>
<item>
<title>MeshLLM：基于大语言模型的3D网格文本序列化框架</title>
<link>https://arxiv.org/abs/2508.01242</link>
<guid>https://arxiv.org/abs/2508.01242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MeshLLM提升3D网格文本生成与理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MeshLLM，一种利用大语言模型处理文本序列化3D网格的新框架。针对现有方法在数据规模和结构信息丢失方面的不足，MeshLLM引入了Primitive-Mesh分解策略，构建了一个包含150万+样本的大规模数据集，并通过顶点推断面连接和局部网格组装训练策略，显著提升了模型对网格拓扑和空间结构的理解能力。实验表明，MeshLLM在网格生成质量和形状理解方面优于现有最佳方法LLaMA-Mesh，展示了其在3D网格处理领域的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 03:37:37 GMT</pubDate>
</item>
<item>
<title>UI-AGILE框架提升GUI代理性能</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-AGILE提升GUI代理的推理与训练效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了UI-AGILE框架，旨在解决现有GUI代理在推理设计、奖励机制和视觉噪声方面的不足。该框架通过改进监督微调过程，包括连续奖励函数、简单思考奖励和基于裁剪的重采样策略，提升训练效果；同时提出分解选择接地方法，显著提高高分辨率屏幕上的接地准确性。实验表明，UI-AGILE在ScreenSpot-Pro和ScreenSpot-v2两个基准测试中表现最佳，其中在ScreenSpot-Pro上接地准确率提升了23%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:22:07 GMT</pubDate>
</item>
<item>
<title>基于锚点和意外度的代码推理压缩方法研究</title>
<link>https://arxiv.org/abs/2508.05988</link>
<guid>https://arxiv.org/abs/2508.05988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ASAP方法提升代码推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在代码推理中因推理过程过长导致的训练成本高、推理延迟大等问题，提出了一种名为ASAP（Anchor-guided, Surprisal-based Pruning）的新型粗到细的CoT压缩框架。该方法通过锚点引导的剪枝保留核心推理结构，并利用首次词意外度指标选择逻辑关键步骤，最终使模型能够在推理时自动生成简洁的CoT。实验表明，ASAP在多个代码生成基准测试中取得了最先进的准确率，同时显著降低了训练和推理成本，在LiveCodeBench v4_v5基准上，相比最强基线减少了23.5%的token生成量和43.5%的推理延迟，Pass@1准确率达到36.19%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 23:46:21 GMT</pubDate>
</item>
<item>
<title>基于AEPO的多模态大模型GUI语义对齐研究</title>
<link>https://arxiv.org/abs/2508.05731</link>
<guid>https://arxiv.org/abs/2508.05731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AEPO提升多模态模型GUI语义对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了多模态大语言模型在图形用户界面中进行自然语言指令理解的挑战，特别是空间和语义对齐问题。尽管RLVR在空间对齐上表现良好，但其探索效率限制了语义对齐的学习。为此，作者提出了自适应探索策略优化（AEPO），通过多答案生成和理论驱动的自适应探索奖励函数，显著提升了模型在多个GUI基准测试中的性能，相对RLVR基线提高了高达9.0%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:49:56 GMT</pubDate>
</item>
<item>
<title>提升低资源语言多模态大模型性能的研究</title>
<link>https://arxiv.org/abs/2508.05502</link>
<guid>https://arxiv.org/abs/2508.05502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出双源策略提升低资源语言多模态模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在低资源语言中的表现不足问题，提出通过增强语言能力和文化背景知识来提升模型效果。研究强调了文化意识的重要性，并引入MELLA数据集，该数据集结合了本地网络的替代文本和多模态生成的描述，有效提升了八种语言的模型表现。实验结果表明，模型在语言理解和文化适应方面均有显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 11:36:24 GMT</pubDate>
</item>
<item>
<title>Voost：一种统一且可扩展的虚拟试穿与脱下框架</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voost通过联合学习提升虚拟试穿效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出Voost，一个统一且可扩展的框架，通过单个扩散变压器联合学习虚拟试穿和试下任务。该方法通过同时建模两个任务，使每个服装-人体对相互监督，支持灵活的生成方向和服装类别条件，无需特定任务网络或额外标签。此外，引入了两种推理阶段技术：注意力温度缩放以提高对分辨率或掩码变化的鲁棒性，以及自校正采样利用任务间的双向一致性。实验表明，Voost在试穿和试下基准测试中均达到最先进水平，表现出更高的对齐精度、视觉真实性和泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 15:10:58 GMT</pubDate>
</item>
<item>
<title>提升长文本事实性推理的在线强化学习方法</title>
<link>https://arxiv.org/abs/2508.05618</link>
<guid>https://arxiv.org/abs/2508.05618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新奖励函数提升R-LLM的事实性推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对推理型大语言模型（R-LLMs）在长文本事实性任务中容易产生幻觉的问题，提出了一种结合事实准确性、回答细节度和相关性的新型奖励函数，并通过在线强化学习提升模型表现。实验表明，该方法在六个长文本事实性基准测试中，平均减少了23.1个百分点的幻觉率，提升了23%的回答细节度，同时保持了整体回答的有用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>基于注意力机制的文档重排序方法AttnRank提升大语言模型性能</title>
<link>https://arxiv.org/abs/2508.05128</link>
<guid>https://arxiv.org/abs/2508.05128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AttnRank通过优化输入顺序提升大模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究发现大语言模型对输入信息的位置高度敏感，存在注意力盆地现象，即模型更关注序列开头和结尾的信息。为解决这一问题，作者提出AttnRank方法，通过小样本校准集估计模型的注意力偏好，并重新排列输入内容以提升关键信息的可见性。该方法无需修改模型参数或训练流程，适用于多种大语言模型，在多跳问答和少样本上下文学习任务中均取得显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 04:08:08 GMT</pubDate>
</item>
<item>
<title>MLLMSeg：一种高效且精确的参考表达分割框架</title>
<link>https://arxiv.org/abs/2508.04107</link>
<guid>https://arxiv.org/abs/2508.04107</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLLMSeg在保持高精度的同时降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MLLMSeg的新框架，用于参考表达分割（RES）。该框架充分利用多模态大模型（MLLM）视觉编码器中的内在视觉细节特征，无需额外引入视觉编码器。同时，设计了一个细节增强且语义一致的特征融合模块（DSFF），将视觉细节特征与大语言模型（LLM）的语义特征进行融合。此外，还构建了一个仅含34M参数的轻量级掩码解码器，有效结合视觉和语义信息以实现精准的掩码预测。实验表明，该方法在性能和成本之间取得了更好的平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04107" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 02:06:52 GMT</pubDate>
</item>
<item>
<title>SODEC：一种高效的单步扩散图像压缩模型</title>
<link>https://arxiv.org/abs/2508.04979</link>
<guid>https://arxiv.org/abs/2508.04979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SODEC提升图像压缩效率与质量，解码速度提高20倍。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SODEC的新型单步扩散图像压缩模型，旨在解决现有方法在解码延迟和图像保真度方面的不足。SODEC通过预训练的VAE生成高信息量的潜在表示，避免多步去噪过程，并引入保真引导模块以提升输出图像的准确性。此外，采用率退火训练策略以支持极低比特率下的有效训练。实验结果表明，SODEC在速率-失真-感知性能上优于现有方法，且解码速度提升了20倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 22:24:03 GMT</pubDate>
</item>
<item>
<title>MACT：多智能体协作框架提升文档理解与视觉问答性能</title>
<link>https://arxiv.org/abs/2508.03404</link>
<guid>https://arxiv.org/abs/2508.03404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MACT框架通过多智能体协作提升文档理解与复杂推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出MACT（Multi-Agent Collaboration with Test-Time scaling）框架，用于改进视觉文档理解和视觉问答任务。该框架由四个小型智能体组成，分别负责规划、执行、判断和回答，具备明确分工和高效协作机制。其中，判断智能体专门验证答案正确性并引导修正，提升了模型的自我纠错能力。此外，MACT引入混合奖励建模和智能体级测试时缩放策略，以优化个体能力和整体协作。实验表明，MACT在多个基准测试中表现优异，尤其在长视觉上下文和复杂推理任务中领先，且参数规模较小，不影响通用性和数学任务表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 08:52:09 GMT</pubDate>
</item>
<item>
<title>基于多模态协同反思的实体链接框架研究</title>
<link>https://arxiv.org/abs/2508.02243</link>
<guid>https://arxiv.org/abs/2508.02243</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多模态实体链接框架，提升链接准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态实体链接任务中图像数据冗余和视觉特征一次性提取的问题，提出了一种基于大语言模型的框架——Intra- and Inter-modal Collaborative Reflections。该框架优先利用文本信息进行实体链接，在文本不足以确定正确实体时，通过多轮迭代策略整合图像中的关键视觉线索，从而提升匹配精度。实验结果表明，该框架在三个公开数据集上均优于现有方法，分别提升了3.2%、5.1%和1.6%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02243" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 05:43:54 GMT</pubDate>
</item>
<item>
<title>Genie Envisioner：统一的机器人操作基础平台</title>
<link>https://arxiv.org/abs/2508.05635</link>
<guid>https://arxiv.org/abs/2508.05635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Genie Envisioner整合策略学习、评估与模拟，提升机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> Genie Envisioner（GE）是一个统一的世界基础平台，用于机器人操作，集成了策略学习、评估和模拟在一个视频生成框架中。其核心是GE-Base，一个大规模指令条件视频扩散模型，能够捕捉现实世界机器人交互的空间、时间和语义动态。GE-Act通过轻量级流匹配解码器将潜在表示映射到可执行的动作轨迹，实现跨多种机器人的精确策略推理。GE-Sim作为动作条件神经模拟器，支持可扩展的评估和训练。此外，平台还配备了EWMBench基准套件，用于衡量视觉保真度、物理一致性和指令-动作对齐。该平台为指令驱动的通用具身智能提供了可扩展且实用的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的隐私保护个人身份信息去标识化研究</title>
<link>https://arxiv.org/abs/2508.05545</link>
<guid>https://arxiv.org/abs/2508.05545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在PII去标识化中表现优异，提供高效隐私保护方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLM）在无结构文本中去除个人身份信息（PII）的应用。相比传统规则系统和领域特定的命名实体识别模型，LLM在上下文理解方面表现出更强的适应性。研究分析了不同架构和训练策略对PII去标识化效果的影响，并评估了其在去标识化准确性、语义保留和PII泄露方面的表现。研究结果为构建高效且隐私友好的LLM去标识系统提供了实用指导。同时，作者发布了PRvL开源工具套件，支持多种推理设置，便于实际部署和定制化应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 12:22:49 GMT</pubDate>
</item>
<item>
<title>InfiAlign：一种高效且可扩展的大型语言模型后训练框架</title>
<link>https://arxiv.org/abs/2508.05496</link>
<guid>https://arxiv.org/abs/2508.05496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiAlign提升LLM推理能力，减少数据需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfiAlign，一种结合监督微调（SFT）和直接偏好优化（DPO）的后训练框架，旨在提高大型语言模型（LLMs）的推理能力。该框架通过多维质量指标自动筛选高质量对齐数据，显著降低数据需求并提升性能。在Qwen2.5-Math-7B-Base模型上的实验表明，其性能与DeepSeek-R1-Distill-Qwen-7B相当，仅使用约12%的训练数据，并在数学推理任务中取得3.89%的平均提升。结果表明，结合系统化数据选择与全阶段后训练是实现高效、可扩展模型对齐的有效方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 11:34:06 GMT</pubDate>
</item>
<item>
<title>DeepPHY：评估视觉语言模型物理推理能力的新基准</title>
<link>https://arxiv.org/abs/2508.05405</link>
<guid>https://arxiv.org/abs/2508.05405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在物理推理任务中表现不足，DeepPHY提供新评估框架。</p><br /><br /><p><strong>摘要：</strong> 尽管视觉语言模型（VLMs）在感知和视觉推理方面表现出色，但在复杂动态环境中仍存在细节关注不足和精确动作规划能力差的问题。现实任务需要高级空间推理、长期规划和持续策略优化，通常依赖对物理规则的理解。然而，真实场景评估成本高昂。为此，研究者提出了DeepPHY，一个系统评估VLMs物理原理理解与推理能力的基准框架，包含多种难度的模拟环境和细粒度评估指标。实验表明，即使最先进的VLMs也难以将描述性物理知识转化为精确的预测控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 09:58:19 GMT</pubDate>
</item>
<item>
<title>基于REINA的实时语音翻译系统优化研究</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REINA提升实时语音翻译质量与延迟平衡，实现SOTA效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为REINA（Regularized Entropy INformation Adaptation）的新损失函数，用于优化实时语音翻译（SimulST）系统在翻译质量和延迟之间的权衡。该方法基于信息论原理，通过智能决策是否等待更多输入来提升翻译效率。实验表明，REINA在法语、西班牙语和德语到英语的翻译任务中表现优异，即使仅使用开源或合成数据也能达到最先进的流式翻译效果。此外，作者引入了一种新的流式效率度量标准，验证了REINA相比现有方法在延迟与质量平衡上提升了21%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 20:25:58 GMT</pubDate>
</item>
<item>
<title>评估大语言模型对语言标志的敏感性基准研究</title>
<link>https://arxiv.org/abs/2508.04939</link>
<guid>https://arxiv.org/abs/2508.04939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型对特定语言模式的不公平评分现象。</p><br /><br /><p><strong>摘要：</strong> 本文提出一个全面的基准，用于评估大语言模型（LLMs）对语言标志的反应，这些标志可能无意中暴露性别、社会阶层或地域背景等人口特征。通过100组经过验证的问答对模拟访谈，研究发现LLMs会系统性地惩罚某些语言模式，尤其是模糊表达，尽管内容质量相同。该基准生成受控的语言变化，保持语义一致，从而精确测量自动化评估系统中的偏见。研究在多个语言维度上验证了方法，结果显示模糊回应平均评分低25.6%，并证明了该基准在识别模型特定偏见方面的有效性。这项工作为检测和衡量AI系统中的语言歧视提供了基础框架，具有广泛的应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 19:51:03 GMT</pubDate>
</item>
<item>
<title>RPCANet++：融合RPCA与深度网络的稀疏目标分割框架</title>
<link>https://arxiv.org/abs/2508.04190</link>
<guid>https://arxiv.org/abs/2508.04190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RPCANet++提升稀疏目标分割性能并增强可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RPCANet++，一种结合鲁棒主成分分析（RPCA）与深度学习架构的稀疏目标分割框架。该方法通过背景近似模块、目标提取模块和图像恢复模块实现高效分割，引入记忆增强模块和深度对比先验模块以提升特征保留与目标提取效率。实验表明，RPCANet++在多种数据集上表现优异，并通过可视化和数值测量增强模型可解释性，为可靠且可解释的稀疏目标分割提供新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 04:19:37 GMT</pubDate>
</item>
<item>
<title>大型多模态模型输入审查能力评估研究</title>
<link>https://arxiv.org/abs/2508.04017</link>
<guid>https://arxiv.org/abs/2508.04017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨LMMs对错误输入的主动检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型多模态模型（LMMs）在处理错误输入时的表现进行研究，提出输入审查能力评估框架ISEval，涵盖七类错误前提和三项评估指标。通过对十种先进LMMs的测试发现，大多数模型缺乏主动识别文本错误的能力，依赖明确提示才能发现问题。不同类型的错误对模型表现影响显著，逻辑谬误识别较好，但语言表面错误和条件性错误较难识别。同时，不同模型在视觉与文本信息的信任度上存在差异。研究强调了提升LMMs主动验证输入有效性的必要性，并为相关问题的解决提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:13:46 GMT</pubDate>
</item>
<item>
<title>结合GUI与编程的多智能体系统提升计算机自动化效率</title>
<link>https://arxiv.org/abs/2508.03923</link>
<guid>https://arxiv.org/abs/2508.03923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoAct-1通过融合GUI操作与编程实现高效任务处理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CoAct-1的新型多智能体系统，将基于GUI的操作与直接编程执行相结合，以提高复杂任务的自动化效率。该系统由一个协调器动态分配子任务给GUI操作员或程序员代理，后者可编写并执行Python或Bash脚本。这种方法在文件管理和数据处理等任务中显著提升了效率，减少了操作步骤。在OSWorld基准测试中，CoAct-1取得了60.76%的成功率，优于现有方法，并将平均任务完成步骤数降至10.15，展现出更强大、高效和可扩展的计算机自动化路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 17:33:36 GMT</pubDate>
</item>
<item>
<title>Double-Bench：多模态文档RAG系统的全面评估框架</title>
<link>https://arxiv.org/abs/2508.03644</link>
<guid>https://arxiv.org/abs/2508.03644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Double-Bench提供多语言、多模态的文档RAG系统评估。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Double-Bench，一个大规模、多语言、多模态的文档RAG系统评估框架。该框架包含3,276份文档和5,168个单跳与多跳查询，覆盖6种语言和4种文档类型，并支持动态更新以应对数据污染问题。所有查询均基于详尽扫描的证据页面并通过人工验证，确保高质量和完整性。实验表明，文本与视觉嵌入模型之间的差距正在缩小，同时揭示了现有RAG框架在缺乏证据时仍过度自信的问题。作者希望Double-Bench能为未来高级文档RAG研究提供坚实基础，并计划每年更新语料库并发布新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:55:02 GMT</pubDate>
</item>
<item>
<title>高效推理方法在大型推理模型中的研究进展</title>
<link>https://arxiv.org/abs/2508.02120</link>
<guid>https://arxiv.org/abs/2508.02120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了LRMs中高效推理方法的最新进展。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）因其在处理复杂任务上的卓越表现而成为研究热点。DeepSeek R1因其优异性能和开源特性受到广泛关注，推动了R1风格LRMs的发展。这些模型通过引入长链式思维和自我反思机制，提升了逻辑推理和决策能力。然而，随着应用的广泛，过度思考问题逐渐显现，表现为生成答案时构建冗长且重复的推理路径，影响效率与准确性。为此，研究者提出了多种高效推理方法，旨在减少推理路径长度而不牺牲性能。本文系统回顾了相关研究，将现有工作分为单模型优化和模型协作两大方向，并维护了一个公开的GitHub仓库以跟踪最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 02:54:31 GMT</pubDate>
</item>
<item>
<title>多模态语音合成系统Marco-Voice：语音克隆与情感控制的统一框架</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Marco-Voice实现语音克隆与情感控制的统一，提升语音自然度与表现力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多模态语音合成系统Marco-Voice，该系统整合了语音克隆和情感控制功能，旨在解决语音生成中表达性、可控性和自然度不足的问题。系统引入了有效的说话人-情感解耦机制和批次内对比学习方法，实现了对说话人身份和情感风格的独立操控，并采用旋转情感嵌入集成方法以实现平滑的情感控制。为了支持全面训练与评估，研究者构建了CSEMOTIONS数据集，包含6位专业演讲者7种情绪下的10小时中文语音。实验结果表明，Marco-Voice在客观和主观指标上均有显著提升，展现出优秀的语音清晰度和情感丰富性，代表了语音合成领域的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:08:22 GMT</pubDate>
</item>
<item>
<title>基于草图的逼真发丝生成模型</title>
<link>https://arxiv.org/abs/2508.01650</link>
<guid>https://arxiv.org/abs/2508.01650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于草图的发丝生成模型，提升生成精度与用户友好性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于草图的逼真发丝生成模型，解决了传统文本或图像输入在精度和用户友好性方面的不足。该模型通过两项创新技术：可学习的发丝上采样策略和多尺度自适应条件机制，有效处理复杂的发丝交互和多样化的草图模式。实验表明，该方法在多个基准数据集上优于现有方法，在真实感和精度方面表现优异。相关代码将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 04:17:50 GMT</pubDate>
</item>
<item>
<title>MOSEv2：推动视频目标分割向真实场景演进的挑战性数据集</title>
<link>https://arxiv.org/abs/2508.05630</link>
<guid>https://arxiv.org/abs/2508.05630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOSEv2提升视频目标分割在复杂现实场景中的性能挑战。</p><br /><br /><p><strong>摘要：</strong> MOSEv2是一个比MOSEv1更具挑战性的视频目标分割数据集，旨在推动该技术在更真实的环境中发展。它包含5,024个视频和超过70万张高质量的标注掩码，覆盖200个类别共10,074个物体。相比前一版本，MOSEv2引入了更多复杂场景，如频繁的物体消失与重现、严重遮挡、小物体、恶劣天气、低光环境、多镜头序列等。实验表明，多个先进VOS方法在MOSEv2上的性能显著下降，显示出当前模型在面对真实世界复杂性时的不足。该数据集已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>动态微调提升大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2508.05629</link>
<guid>https://arxiv.org/abs/2508.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态微调提升大语言模型在多个基准测试中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为动态微调（DFT）的方法，用于改进监督微调（SFT）在大语言模型中的表现。通过数学分析发现，标准SFT的梯度隐含了限制模型泛化能力的奖励结构。DFT通过动态调整目标函数来稳定每个token的梯度更新，显著提升了模型在多个挑战性基准和基础模型上的性能，并在离线强化学习设置中表现出色。该方法结合理论洞察与实际应用，为SFT提供了一个更高效且简单的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>Hi3DEval：面向3D生成内容的层次化评估框架</title>
<link>https://arxiv.org/abs/2508.05609</link>
<guid>https://arxiv.org/abs/2508.05609</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Hi3DEval框架，提升3D内容质量评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D内容生成质量评估的挑战，提出了Hi3DEval框架，该框架结合对象级和部件级评估，实现多维度的全面分析。同时，扩展了纹理评估范围，关注材质真实性。为支持该框架，构建了Hi3DBench数据集，并设计了基于混合3D表示的自动化评分系统。实验表明，该方法优于现有图像基指标，更符合人类偏好。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05609" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>R-Zero：一种自进化大型语言模型框架</title>
<link>https://arxiv.org/abs/2508.05004</link>
<guid>https://arxiv.org/abs/2508.05004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R-Zero通过自生成数据实现LLM自主进化与能力提升。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为R-Zero的自进化大型语言模型框架，该框架无需依赖人工标注任务即可自主生成训练数据。R-Zero通过两个独立模型——Challenger和Solver之间的互动进行优化，Challenger提出挑战性任务，Solver则逐步解决这些任务，从而形成一个自我改进的训练循环。实验表明，R-Zero显著提升了多种基础LLM的推理能力，例如在数学推理和通用领域推理基准测试中分别提升了6.49和7.54个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 23:38:16 GMT</pubDate>
</item>
<item>
<title>探索语言模型在多跳问答任务中的推理失败</title>
<link>https://arxiv.org/abs/2508.04699</link>
<guid>https://arxiv.org/abs/2508.04699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型在多跳问答中的推理错误模式。</p><br /><br /><p><strong>摘要：</strong> 本文系统研究了当前语言模型在多跳问答任务中的推理失败问题，提出了一种新的错误分类框架，从源文档的多样性与独特性、相关信息的覆盖程度以及认知效率三个维度分析模型的不足。通过人工标注和自动化指标的结合，研究发现了被传统评估方式掩盖的复杂错误模式，为提升语言模型的推理准确性、透明度和鲁棒性提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04699" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:58:36 GMT</pubDate>
</item>
<item>
<title>构建结构化客户服务对话框架与数据集提升客服质量</title>
<link>https://arxiv.org/abs/2508.04423</link>
<guid>https://arxiv.org/abs/2508.04423</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建CSC框架和CSConv数据集提升客服策略响应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Customer Support Conversation (CSC)任务，旨在训练客服人员使用明确的支持策略进行对话。基于COPC指南，定义了五个对话阶段和十二种策略，并构建了CSConv数据集，包含1,855条经过LLM重写并标注的客户服务对话。同时开发了RoleCS训练数据集，通过LLM模拟策略丰富的对话场景。实验表明，在RoleCS上微调大型语言模型可显著提升其生成符合策略的高质量回复能力，并通过人工评估验证了问题解决效果的提升。所有代码和数据将公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04423" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 09:11:17 GMT</pubDate>
</item>
<item>
<title>提升大语言模型在福祉解释中的质量与适配性</title>
<link>https://arxiv.org/abs/2508.03990</link>
<guid>https://arxiv.org/abs/2508.03990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了LLM在福祉解释中的表现并优化其质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在解释福祉概念方面的表现，构建了一个包含43,880条解释的大规模数据集。研究引入了一种基于原则的LLM评价框架，并通过监督微调和直接偏好优化方法提升了模型生成解释的质量。结果显示，LLM的解释质量因模型、受众和类别而异，且经过微调的模型在性能上优于未微调的模型，证明了基于偏好的学习在特定解释任务中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 20:45:02 GMT</pubDate>
</item>
<item>
<title>HarmonyGuard：多智能体协作框架提升网络代理的安全与效率</title>
<link>https://arxiv.org/abs/2508.04010</link>
<guid>https://arxiv.org/abs/2508.04010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HarmonyGuard提升网络代理任务完成率与安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出HarmonyGuard，一个用于提升网络代理安全性和任务完成率的多智能体协作框架。该框架包含两个核心能力：自适应策略增强和双目标优化。Policy Agent自动提取并维护结构化安全策略，以应对不断变化的网络威胁；Utility Agent通过马尔可夫实时推理评估安全与效率，并利用元认知能力进行优化。实验表明，HarmonyGuard在多个基准测试中显著优于现有基线，任务完成率提高20%，策略合规性提升38%以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 21:49:32 GMT</pubDate>
</item>
<item>
<title>机器学习模型全生命周期中的偏见治理与公平性评估</title>
<link>https://arxiv.org/abs/2508.03970</link>
<guid>https://arxiv.org/abs/2508.03970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨机器学习模型全生命周期中的偏见治理与公平性评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统介绍了在机器学习模型的整个生命周期中，如何有效治理、评估和量化偏见。基于作者在大型语言模型偏见评估测试套件（BEATS）方面的基础工作，文章分析了大型语言模型中存在的常见偏见与公平性问题，并提出了数据与人工智能治理框架，以应对模型中的偏见、伦理、公平性和事实性问题。该治理方法适用于实际应用场景，可实现模型部署前的严格基准测试、实时持续评估以及生成内容的主动管理。通过在整个AI开发生命周期中实施数据与AI治理，组织可以显著提升生成式AI系统的安全性与责任性，有效降低歧视风险并保护品牌声誉。文章旨在推动社会负责任且符合伦理的生成式人工智能应用的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03970" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 19:15:31 GMT</pubDate>
</item>
<item>
<title>基于DiffSemanticFusion的自动驾驶场景理解与轨迹预测方法</title>
<link>https://arxiv.org/abs/2508.01778</link>
<guid>https://arxiv.org/abs/2508.01778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiffSemanticFusion框架提升自动驾驶场景理解与轨迹预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DiffSemanticFusion的多模态轨迹预测与规划融合框架，旨在提升自动驾驶中的场景理解能力。该框架结合了栅格化表示和图结构表示的优势，通过地图扩散模块增强在线高精度地图的稳定性和表达能力。实验结果表明，在nuScenes和NAVSIM数据集上，该方法在轨迹预测和端到端自动驾驶任务中均取得了优于现有方法的性能。特别是在nuScenes数据集上，与QCNet结合后提升了5.1%，在NAVSIM的NavHard场景中提升了15%。此外，消融实验显示该模块可无缝集成到其他向量基方法中以进一步提升性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:32:05 GMT</pubDate>
</item>
<item>
<title>DPoser-X：基于扩散模型的全身人体姿态生成方法</title>
<link>https://arxiv.org/abs/2508.00599</link>
<guid>https://arxiv.org/abs/2508.00599</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DPoser-X通过扩散模型实现全身人体姿态建模，性能优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出DPoser-X，一种基于扩散模型的全身人体姿态先验模型。针对人体姿态建模的复杂性和高质量数据稀缺问题，作者引入了扩散模型作为姿态先验，并通过改进的时间步调度方法和掩码训练机制提升模型性能。该方法统一处理多种姿态相关任务，有效捕捉身体各部分之间的依赖关系，避免过拟合特定动作。实验表明，DPoser-X在多个基准测试中表现优异，为全身人体姿态建模设立了新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00599" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 08:56:39 GMT</pubDate>
</item>
<item>
<title>FACTORY：一种用于评估模型事实准确性的大型人工验证基准</title>
<link>https://arxiv.org/abs/2508.00109</link>
<guid>https://arxiv.org/abs/2508.00109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FACTORY提升模型生成内容的事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FACTORY，一个基于人工验证的大规模提示集，用于评估语言模型生成内容的事实准确性。与现有基准相比，FACTORY更具挑战性，实验表明约40%的顶级模型输出存在不实信息，而其他数据集仅为10%。研究强调了FACTORY在可靠性方面的优势，并指出模型需具备处理长尾事实的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 15:00:11 GMT</pubDate>
</item>
<item>
<title>提升数学命题自动形式化的ThinkingF方法</title>
<link>https://arxiv.org/abs/2508.04440</link>
<guid>https://arxiv.org/abs/2508.04440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkingF提升数学命题自动形式化准确率。</p><br /><br /><p><strong>摘要：</strong> 本文提出ThinkingF方法，旨在提升自然语言数学命题到形式语言的自动转换准确性。研究指出，有效的自动形式化需要两个关键能力：对形式语言领域知识的全面掌握以及自然语言问题理解与非形式-形式对齐的推理能力。为解决现有方法准确率低的问题，作者构建了两个数据集，并通过SFT和RLVR训练进一步融合这两种能力。最终模型在FormalMATH-Lite和ProverBench任务中取得了最先进的成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 09:28:22 GMT</pubDate>
</item>
<item>
<title>提升大模型指令遵循能力的框架与实验验证</title>
<link>https://arxiv.org/abs/2508.03178</link>
<guid>https://arxiv.org/abs/2508.03178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出框架提升模型指令遵循能力，效果显著。</p><br /><br /><p><strong>摘要：</strong> 尽管大语言模型在数学问题、编码任务和通用谜题上的推理能力有所提升，但其在准确遵循复杂指令方面仍存在不足。本文指出，推理过程中的‘懒惰推理’是导致指令遵循不佳的主要原因。为此，作者提出了一种全面框架，通过预览与自检机制增强推理严谨性。该框架包括生成复杂约束指令、过滤得到有效提示集、利用拒绝采样构建高质量数据集，并结合熵保持的监督微调与基于规则密集奖励的强化学习策略，提升模型的推理能力。实验表明，该方法在多个基准测试中表现优异，其中Light-IF-32B模型超越了多个大型开源和闭源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:42:00 GMT</pubDate>
</item>
<item>
<title>持续学习的3D异常检测框架C3D-AD</title>
<link>https://arxiv.org/abs/2508.01311</link>
<guid>https://arxiv.org/abs/2508.01311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出C3D-AD框架，实现多类别3D点云的持续学习与异常检测。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为C3D-AD的持续学习框架，用于3D异常检测。该框架能够学习多类别点云的通用表示，并处理随时间出现的新类别。在特征提取模块中引入了KAL机制以提取通用局部特征；在数据重建过程中采用KAA机制，既能学习新类别信息，又能丢弃冗余旧信息；最后通过RPP模块保持任务间的表示一致性。实验结果表明，该方法在三个公开数据集上均取得了优异性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 06:54:55 GMT</pubDate>
</item>
<item>
<title>Sel3DCraft：提升文本到3D生成的视觉提示工程系统</title>
<link>https://arxiv.org/abs/2508.00428</link>
<guid>https://arxiv.org/abs/2508.00428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sel3DCraft优化3D生成流程，提升设计创意效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Sel3DCraft，一个用于文本到3D生成的视觉提示工程系统。该系统通过引入双分支结构、多视角混合评分方法和提示驱动的可视化分析工具，解决了传统3D生成中因盲目的提示过程导致的结果不可预测问题。实验和用户研究显示，Sel3DCraft在支持设计师创意方面优于其他3D生成系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 04:36:15 GMT</pubDate>
</item>
<item>
<title>SEAgent：通过自主学习提升计算机使用代理的性能</title>
<link>https://arxiv.org/abs/2508.04700</link>
<guid>https://arxiv.org/abs/2508.04700</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEAgent使计算机使用代理能自主学习新软件，提升任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SEAgent框架，旨在让计算机使用代理（CUAs）通过自主探索和经验学习掌握新软件。该框架包含世界状态模型和课程生成器，支持逐步复杂化的任务训练，并结合对抗性模仿失败动作与群体相对策略优化提升代理性能。此外，采用专家到通用的训练策略整合多个专家代理的经验，最终实现超越单一专家代理的通用代理系统。实验表明，SEAgent在五个新软件环境中成功率达到34.5%，比UI-TARS提升了23.2%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04700" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:58:46 GMT</pubDate>
</item>
<item>
<title>通过主动上下文管理提升大语言模型的长文本处理能力</title>
<link>https://arxiv.org/abs/2508.04664</link>
<guid>https://arxiv.org/abs/2508.04664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">主动上下文管理框架Sculptor提升LLM在长文本任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Sculptor的框架，旨在通过主动上下文管理（ACM）工具提升大语言模型（LLMs）在处理长上下文时的性能。Sculptor包含三种工具：上下文分段、摘要与恢复以及智能搜索，帮助LLMs更有效地管理注意力和工作记忆，减少前向干扰的影响。实验表明，该方法在无需额外训练的情况下显著提升了模型在信息稀疏基准测试中的表现，证明了其在长文本任务中的有效性。研究强调，明确的上下文控制策略比单纯扩大token窗口更为关键。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:32:58 GMT</pubDate>
</item>
<item>
<title>基于IFDecorator的强化学习框架提升指令遵循能力</title>
<link>https://arxiv.org/abs/2508.04632</link>
<guid>https://arxiv.org/abs/2508.04632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IFDecorator提升LLM指令遵循准确性并减少奖励黑客行为。</p><br /><br /><p><strong>摘要：</strong> 本文提出Instruction Following Decorator (IFDecorator) 框架，旨在解决强化学习中因难度评估不足导致的训练效率低下和过优化问题。该框架包含三个组件：协同对抗数据飞轮生成更难的指令-验证对、IntentCheck模块确保意图对齐、以及通过陷阱指令检测奖励黑客行为。实验表明，Qwen2.5-32B-Instruct-IFDecorator在IFEval上达到87.43%的准确率，优于更大模型如GPT-4o，并在FollowBench任务中表现出色，同时有效降低奖励黑客率。相关模型、代码和数据将公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:00:54 GMT</pubDate>
</item>
<item>
<title>AI学术会议的可持续转型：社区联邦会议模型的提出</title>
<link>https://arxiv.org/abs/2508.04586</link>
<guid>https://arxiv.org/abs/2508.04586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI会议面临多方面危机，提出社区联邦会议模型以促进可持续发展。</p><br /><br /><p><strong>摘要：</strong> 本文指出人工智能学术会议在科学、环境、心理和物流等方面面临严重压力，如作者发表率翻倍、碳排放高、负面情绪普遍及参会人数超负荷。这些挑战威胁了会议的核心目标。为应对这些问题，文章提出了社区联邦会议（CFC）模型，将同行评审、展示和社交活动分拆为全球协调但本地组织的形式，旨在实现更可持续、包容和有韧性的AI研究交流方式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 12:08:27 GMT</pubDate>
</item>
<item>
<title>EvoC2Rust：一种用于C到Rust项目级转换的自动化框架</title>
<link>https://arxiv.org/abs/2508.04295</link>
<guid>https://arxiv.org/abs/2508.04295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvoC2Rust提升C代码迁移到Rust的准确性和安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为EvoC2Rust的自动化框架，用于将整个C代码库转换为等效的Rust代码。该框架采用基于骨架的翻译策略，分为三个进化阶段：首先分解C项目并生成可编译的Rust骨架；其次逐步翻译函数；最后通过集成LLM和静态分析修复编译错误。实验表明，EvoC2Rust在语法和语义准确性上分别比基于LLM的方法高出17.24%和14.32%，代码安全性也优于基于规则的工具。在工业项目中，其模块级编译通过率为92.25%，测试通过率为89.53%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 06:31:23 GMT</pubDate>
</item>
<item>
<title>基于VL-DAC的视觉语言模型强化学习方法研究</title>
<link>https://arxiv.org/abs/2508.04280</link>
<guid>https://arxiv.org/abs/2508.04280</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VL-DAC提升VLM在真实场景中的任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Vision-Language Decoupled Actor-Critic (VL-DAC) 的轻量级、无需超参数调整的强化学习算法，用于训练视觉语言模型（VLMs）。该方法通过将动作标记的PPO更新与环境步级的价值学习解耦，提升了训练效率和稳定性。实验表明，在多个低成本模拟器中训练的VLM能够有效泛化到真实场景任务，如游戏控制、空间规划和网页导航，且不损害图像理解能力。这是首个证明简单RL算法可在合成环境中训练VLM并提升实际任务表现的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04280" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 06:08:48 GMT</pubDate>
</item>
<item>
<title>VeriGUI：推动长链GUI任务的可验证数据集研究</title>
<link>https://arxiv.org/abs/2508.04026</link>
<guid>https://arxiv.org/abs/2508.04026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VeriGUI提升GUI代理处理长链任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VeriGUI，一个用于训练和评估通用GUI代理的可验证长链任务数据集。该数据集强调两个关键维度：长链复杂性与子任务级可验证性，支持任务分解为数百步的相互依赖子任务，并允许每个子任务作为起点。数据集涵盖桌面和网页环境，由专家标注。实验表明，现有代理在处理长链任务时存在显著性能差距，凸显了对更强大规划与决策能力的需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:38:18 GMT</pubDate>
</item>
<item>
<title>MiDashengLM：一种基于开放数据的高效音频语言模型</title>
<link>https://arxiv.org/abs/2508.03983</link>
<guid>https://arxiv.org/abs/2508.03983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiDashengLM通过开放数据实现高效音频理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MiDashengLM，一个基于开放数据源的音频语言模型，旨在提升音频理解的效率与全面性。该模型利用自建的ACAVCaps训练数据集，结合开源音频编码器Dasheng，实现了对语音、声音和音乐信息的统一文本表示。相比以往主要依赖自动语音识别（ASR）的方法，MiDashengLM更注重通用音频描述，提升了复杂音频场景的处理能力。实验表明，MiDashengLM在首次令牌时间（TTFT）和吞吐量方面分别比同类模型快4倍和20倍。模型检查点已在Hugging Face和GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 20:30:19 GMT</pubDate>
</item>
<item>
<title>Sotopia-RL：提升大语言模型社会智能的强化学习框架</title>
<link>https://arxiv.org/abs/2508.03905</link>
<guid>https://arxiv.org/abs/2508.03905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sotopia-RL通过多维奖励机制提升社会智能模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Sotopia-RL，一种用于训练具有社会智能的大语言模型的强化学习框架。该框架解决了传统强化学习在社交任务中的两大挑战：部分可观测性和多维行为影响。通过将粗粒度的回合级反馈细化为话语级别的多维奖励，Sotopia-RL有效提升了模型在社交任务中的表现。实验表明，该方法在Sotopia基准测试中取得了领先成绩，显著优于现有方法。消融实验进一步验证了话语级信用分配和多维奖励设计的重要性。代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 16:43:42 GMT</pubDate>
</item>
<item>
<title>Agent Lightning：一种用于强化学习训练大型语言模型的灵活框架</title>
<link>https://arxiv.org/abs/2508.03680</link>
<guid>https://arxiv.org/abs/2508.03680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent Lightning实现RL与AI代理的解耦训练，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> Agent Lightning是一种灵活且可扩展的框架，允许通过强化学习（RL）训练大型语言模型（LLMs）以适应任何AI代理。该框架实现了代理执行与训练的完全解耦，支持与多种现有代理系统的无缝集成，几乎无需代码修改。通过将代理执行建模为马尔可夫决策过程，Agent Lightning定义了一个统一的数据接口，并提出了一种分层RL算法LightningRL，包含信用分配模块，能够将任意代理生成的轨迹分解为训练转换。这使得RL能够处理复杂的交互逻辑，如多代理场景和动态工作流。系统设计引入了Training-Agent Disaggregation架构，并集成了代理可观测性框架，提供标准化的代理微调接口。实验表明，该框架在文本到SQL、检索增强生成和数学工具使用任务中表现出稳定且持续的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>HPSv3：提升文本生成图像质量的新评估方法</title>
<link>https://arxiv.org/abs/2508.03789</link>
<guid>https://arxiv.org/abs/2508.03789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HPSv3提升文本生成图像质量，支持高效评估与优化。</p><br /><br /><p><strong>摘要：</strong> 本文提出Human Preference Score v3 (HPSv3)，用于更准确地评估文本到图像生成模型。HPSv3包含1.08M文本-图像对和1.17M人工偏好比较数据，结合基于视觉语言模型的偏好模型和不确定性感知排序损失函数，实现细粒度图像排名。此外，文章还引入Chain-of-Human-Preference (CoHP) 方法，通过迭代优化提升图像质量，无需额外数据。实验表明，HPSv3在广泛场景下具有良好的评估性能，CoHP可有效提升生成图像质量。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:17:13 GMT</pubDate>
</item>
<item>
<title>基于布局思维的网页设计代码转换方法研究</title>
<link>https://arxiv.org/abs/2508.03560</link>
<guid>https://arxiv.org/abs/2508.03560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LaTCoder提升网页设计代码生成的布局准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LaTCoder方法，通过引入布局思维（Layout-as-Thought）来增强网页设计到代码转换中的布局保留能力。该方法将网页设计划分为图像块，并采用基于链式思维的提示策略为每个块生成代码，再通过绝对定位和MLLM方法进行组装。实验结果显示，使用DeepSeek-VL2模型时，TreeBLEU分数提升了66.67%，MAE下降了38%。人工评估也表明，超过60%的情况下，LaTCoder生成的网页更受青睐，验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03560" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 11:28:48 GMT</pubDate>
</item>
<item>
<title>强化学习在大型语言模型软件工程任务中的应用</title>
<link>https://arxiv.org/abs/2508.03501</link>
<guid>https://arxiv.org/abs/2508.03501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL提升LLM在软件工程任务中的成功率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习（RL）应用于大型语言模型（LLMs）以解决实际软件工程任务。与以往专注于单轮问题的研究不同，本文关注需要多轮交互的复杂环境，并使用改进的DAPO算法训练基于Qwen2.5-72B-Instruct的代理。实验结果显示，该方法在SWE-bench Verified基准测试中将成功率从20%提升至39%，并在SWE-rebench上达到或超过领先开源模型的表现，展示了利用开放模型构建高效自主代理的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 10:30:47 GMT</pubDate>
</item>
<item>
<title>基于文本控制的音乐修复与母带处理模型SonicMaster</title>
<link>https://arxiv.org/abs/2508.03448</link>
<guid>https://arxiv.org/abs/2508.03448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SonicMaster可统一修复多种音频问题并提升音质。</p><br /><br /><p><strong>摘要：</strong> 本文提出SonicMaster，首个基于文本控制的音乐修复与母带处理统一生成模型。该模型能有效解决非专业环境中常见的音频质量问题，如混响过强、失真、频段不平衡等。通过自然语言指令进行定向增强或自动处理，SonicMaster在大量配对的劣化与高质量音频数据上进行训练，采用流匹配生成训练范式，实现从劣化输入到优化输出的映射。实验结果显示，SonicMaster在客观音质指标和主观听感测试中均优于原始音频，验证了其在音频修复领域的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 09:49:04 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的多毒性预测框架CoTox</title>
<link>https://arxiv.org/abs/2508.03159</link>
<guid>https://arxiv.org/abs/2508.03159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoTox利用大语言模型提升药物毒性预测准确性与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CoTox的新框架，结合大语言模型（LLM）和链式思维（CoT）推理，用于多毒性预测。该框架整合化学结构数据、生物通路和基因本体（GO）术语，通过逐步推理生成可解释的毒性预测结果。实验表明，CoTox在GPT-4o等模型上表现优于传统机器学习和深度学习方法。研究还发现，使用IUPAC命名表示化学结构能增强模型的推理能力。此外，通过模拟药物对相关细胞的作用，CoTox能够生成与生理反应一致的毒性预测，展示了其在药物开发中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:04:44 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的视频虚拟试穿技术DreamVVT</title>
<link>https://arxiv.org/abs/2508.02807</link>
<guid>https://arxiv.org/abs/2508.02807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamVVT提升视频虚拟试穿的细节和时序一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DreamVVT的两阶段视频虚拟试穿框架，利用扩散Transformer（DiTs）和预训练模型的优势，有效解决现有方法在缺乏成对数据、细节保留和时序一致性方面的不足。第一阶段通过多帧试穿模型生成高质量关键帧图像，第二阶段结合骨骼图、运动描述和关键帧图像进行视频生成，显著提升了动态效果和真实感。实验表明，DreamVVT在实际场景中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 14:27:55 GMT</pubDate>
</item>
<item>
<title>LeanK：一种基于学习的KV缓存剪枝方法提升大语言模型效率</title>
<link>https://arxiv.org/abs/2508.02215</link>
<guid>https://arxiv.org/abs/2508.02215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LeanK通过剪枝KV缓存提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出LeanK，一种基于学习的KV缓存剪枝方法，通过静态通道稀疏性减少不重要的键缓存通道，从而降低GPU内存占用并加速解码过程。该方法采用两阶段训练策略，生成满足特定稀疏率和硬件适配要求的通道掩码。实验表明，LeanK可实现高达70%的K缓存和16%-18%的V缓存内存减少，并通过自定义解码内核实现1.3倍的注意力计算加速。文章还分析了长上下文推理过程中模型通道和注意力头的重要性分布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 05:08:43 GMT</pubDate>
</item>
<item>
<title>基于查询的U-Net架构IAUNet在生物医学实例分割中的应用</title>
<link>https://arxiv.org/abs/2508.01928</link>
<guid>https://arxiv.org/abs/2508.01928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IAUNet提升细胞实例分割性能，优于现有模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于查询的U-Net架构IAUNet，用于生物医学图像中的实例分割。该模型结合了完整的U-Net结构和轻量级像素解码器，提高了效率并减少了参数数量。同时引入了Transformer解码器以多尺度优化对象特征。研究还发布了2025 Revvity全细胞分割数据集，为生物医学实例分割提供了新基准。实验表明，IAUNet在多个公开数据集和自建数据集上均优于当前最先进的全卷积、基于Transformer和基于查询的模型，为细胞实例分割任务设定了新的基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 17:36:20 GMT</pubDate>
</item>
<item>
<title>基于多模态知识的网络代理框架与推理系统</title>
<link>https://arxiv.org/abs/2508.01858</link>
<guid>https://arxiv.org/abs/2508.01858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Web-CogKnowledge框架提升网络代理认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大模型对网络代理发展的推动作用，强调网络代理需要先获取充分知识才能进行有效推理。作者将网络代理的能力分解为知识学习和认知过程两个阶段，并提出Web-CogKnowledge框架，将知识分为事实性、概念性和程序性。通过构建Web-CogDataset数据集，实现对网络代理的基础知识训练，并开发出基于知识驱动的Chain-of-Thought（CoT）推理框架Web-CogReasoner。实验表明该模型在未见过的任务中表现优异，同时引入Web-CogBench评估体系以全面衡量代理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 13:17:52 GMT</pubDate>
</item>
<item>
<title>OpenMed NER：高效且开源的医学实体识别模型</title>
<link>https://arxiv.org/abs/2508.01630</link>
<guid>https://arxiv.org/abs/2508.01630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenMed NER在多个生物医学NER基准上取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenMed NER，一个基于Transformer的开源医学实体识别模型。该模型通过轻量级领域自适应预训练（DAPT）和低秩适配（LoRA）技术，在350,000条临床笔记和科研文献语料上进行训练，并在12个生物医学NER基准测试中取得了显著提升，尤其在疾病、化学物质、基因和细胞系等实体类型上表现突出。其训练效率高，仅需单块GPU在12小时内完成，碳排放极低，且提供宽松许可协议，有助于符合欧盟AI法案等法规要求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 03:33:28 GMT</pubDate>
</item>
<item>
<title>3D占用定位基准与GroundingOcc模型研究</title>
<link>https://arxiv.org/abs/2508.01197</link>
<guid>https://arxiv.org/abs/2508.01197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出3D占用定位任务及GroundingOcc模型，提升自动驾驶感知精度。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统视觉定位依赖边界框导致细节不足的问题，提出3D占用定位任务，并基于nuScenes数据集构建了包含体素级占用标注的基准。同时，设计了GroundingOcc模型，通过多模态学习融合视觉、文本和点云信息，实现从粗到细的物体位置与占用预测。模型包含多模态编码器、占用头、定位头以及2D定位和深度估计模块，实验表明其在3D占用定位任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 01:05:50 GMT</pubDate>
</item>
<item>
<title>CoT推理的脆弱性：数据分布视角下的分析</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT推理在分布外数据中表现不稳定，揭示其局限性。</p><br /><br /><p><strong>摘要：</strong> 本文从数据分布的角度研究了Chain-of-Thought (CoT) 推理的有效性。作者发现，尽管CoT能生成类似人类的推理步骤，但其效果受限于训练数据与测试数据之间的分布差异。通过构建DataAlchemy环境，研究者系统地分析了任务、长度和格式对CoT推理的影响，结果表明CoT推理在超出训练分布的情况下会失效，显示出其表面性与脆弱性。该研究为理解CoT推理的局限性提供了新的视角，并强调了实现真正通用推理的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01191" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 00:37:28 GMT</pubDate>
</item>
<item>
<title>RL-PLUS：突破大语言模型能力边界的新方法</title>
<link>https://arxiv.org/abs/2508.00222</link>
<guid>https://arxiv.org/abs/2508.00222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL-PLUS提升LLM推理能力，突破原有边界。</p><br /><br /><p><strong>摘要：</strong> 本文提出RL-PLUS，一种结合内部探索与外部数据的混合策略优化方法，旨在解决传统强化学习（RLVR）在大语言模型（LLM）中因策略局限和奖励稀疏性导致的能力边界问题。该方法通过多重重要性采样和基于探索的优势函数，有效提升模型的推理能力，并在多个数学推理基准和分布外任务中取得显著性能提升。实验表明，RL-PLUS在六项基准测试中达到最先进水平，平均相对提升达69.2%，并有效缓解了能力边界崩溃现象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 19:55:29 GMT</pubDate>
</item>
<item>
<title>基于视频生成动态4D内容的新框架</title>
<link>https://arxiv.org/abs/2507.23785</link>
<guid>https://arxiv.org/abs/2507.23785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种从视频生成高质量动态3D内容的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的视频到4D生成框架，能够从单个视频输入中创建高质量的动态3D内容。由于直接进行4D扩散建模面临数据构建成本高和高维表示的挑战，作者提出了一种Direct 4DMesh-to-GS Variation Field VAE，可直接从3D动画数据中编码规范的高斯点（GS）及其时间变化，并将其压缩到紧凑的潜在空间中。在此基础上，训练了一个基于时间感知扩散Transformer的高斯变化场扩散模型，该模型在Objaverse数据集上进行了训练，表现出优于现有方法的生成质量，并且在真实视频输入上也展现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>文本到图像扩散模型中的内容与风格表示研究</title>
<link>https://arxiv.org/abs/2507.23313</link>
<guid>https://arxiv.org/abs/2507.23313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究文本到图像模型如何区分内容与风格。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于Transformer的文本到图像扩散模型在生成艺术作品时如何编码内容和风格概念。通过使用交叉注意力热图，研究人员能够将生成图像中的像素与特定提示词相关联，从而区分内容描述词和风格描述词的影响区域。研究发现，模型在不同艺术提示和风格下表现出不同程度的内容与风格分离，内容词主要影响物体区域，而风格词则影响背景和纹理区域，表明模型可能在没有明确监督的情况下自发理解内容与风格的区别。研究结果有助于理解大规模生成模型如何内部表示复杂的艺术概念，并提供了代码和数据集以供进一步探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 03:47:01 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的移动网络根因分析框架</title>
<link>https://arxiv.org/abs/2507.21974</link>
<guid>https://arxiv.org/abs/2507.21974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用LLM进行移动网络根因分析，提升可解释性和准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种轻量级框架，利用大语言模型（LLMs）进行移动网络的根因分析（RCA）。研究引入了TeleLogs数据集，用于评估LLMs在RCA任务中的表现。实验发现现有开源模型在此任务中表现不佳，表明需要领域适配。为此，作者提出了一种两阶段训练方法，结合监督微调和强化学习，以提高模型的推理能力和诊断准确性。该方法整合领域知识，生成结构化、多步骤的诊断解释，显著提升了模型性能，并在多个LLM规模上验证了其有效性。结果表明，经过领域适配的LLM在可解释性和实用性方面具有广泛应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 12:21:42 GMT</pubDate>
</item>
<item>
<title>高效代理框架的研究与优化</title>
<link>https://arxiv.org/abs/2508.02694</link>
<guid>https://arxiv.org/abs/2508.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究代理系统的效率与性能平衡，提出高效代理框架。</p><br /><br /><p><strong>摘要：</strong> 本文系统研究了大型语言模型驱动的代理系统在效率与性能之间的权衡问题，探讨了任务复杂性、模块扩展的边际效益以及高效框架设计对成本的影响。通过GAIA基准测试，分析了LLM骨干选择、框架设计和测试时扩展策略对效率的影响，并提出了Efficient Agents框架，在保持96.7%性能的同时，将成本降低了28.4%，为构建高效、可持续的AI代理系统提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 13:56:51 GMT</pubDate>
</item>
<item>
<title>基于注意力权重的上下文回溯方法AttnTrace</title>
<link>https://arxiv.org/abs/2508.03793</link>
<guid>https://arxiv.org/abs/2508.03793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AttnTrace方法提升LLM上下文回溯效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于注意力权重的上下文回溯方法AttnTrace，旨在提高长上下文大语言模型（LLM）在生成响应时对相关文本的追溯能力。与现有方法相比，AttnTrace在准确性和计算效率上均有显著提升。研究团队提出了两种增强技术，并提供了理论支持。实验结果表明，AttnTrace不仅在传统任务中表现优异，还能有效检测长上下文中的提示注入攻击。此外，该方法在实际应用中能够精准识别被篡改的指令，提升了模型输出的可信度和可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:56:51 GMT</pubDate>
</item>
<item>
<title>AI代理在电商中的购物行为研究</title>
<link>https://arxiv.org/abs/2508.02630</link>
<guid>https://arxiv.org/abs/2508.02630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI代理在电商中表现出不同的购物偏好和决策模式。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了由视觉语言模型驱动的AI代理在在线市场中的购物行为。通过构建ACES环境，研究者分析了AI代理如何评估产品、做出购买决策，并揭示了它们对产品位置、价格、评分、评论及推广标签的敏感性。研究发现，不同AI模型对同一商品的偏好存在差异，且对推广标签持负面态度，对平台推荐有积极反应。此外，文章还展示了卖家如何通过优化产品描述来提升市场占有率，强调了AI中介购物对电商生态系统的潜在影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 13:19:36 GMT</pubDate>
</item>
<item>
<title>HyCodePolicy：一种融合多模态推理的自主控制框架</title>
<link>https://arxiv.org/abs/2508.02629</link>
<guid>https://arxiv.org/abs/2508.02629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HyCodePolicy提升机器人任务执行的鲁棒性和效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出HyCodePolicy，一种结合代码生成、几何定位、感知监控和迭代修复的闭环编程框架，用于增强具身代理的任务执行能力。系统通过自然语言指令分解任务，生成基于几何原语的初始程序，并在模拟中执行。利用视觉语言模型检测执行失败并分析原因，结合程序执行轨迹与感知反馈进行自动修复，实现低监督下的自我修正代码生成。实验表明，该方法显著提升了机器人操作策略的鲁棒性和样本效率，为多模态推理融入自主决策提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 13:18:14 GMT</pubDate>
</item>
<item>
<title>基于语言模型的代码补全排序方法研究</title>
<link>https://arxiv.org/abs/2508.02455</link>
<guid>https://arxiv.org/abs/2508.02455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级代码补全排序方法，提升IDE开发效率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了代码补全功能在现代IDE中的重要性，并提出了一种基于语言模型的轻量级代码补全排序方法。该方法通过构建前缀树并进行一次贪婪解码，实现精准的token级排序，无需使用束搜索或模型适配。该方法速度快、架构无关，且兼容现有代码补全模型，为IDE中集成语言模型提供了实用有效的路径，有助于提升开发者的辅助体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 10:20:39 GMT</pubDate>
</item>
<item>
<title>基于场景上下文的自中心人体运动生成与预测方法</title>
<link>https://arxiv.org/abs/2508.01126</link>
<guid>https://arxiv.org/abs/2508.01126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniEgoMotion模型，实现自中心运动生成与预测。</p><br /><br /><p><strong>摘要：</strong> 本文针对增强现实和虚拟现实等应用中自中心视角下人体运动生成与预测的问题，提出了一种新的任务框架，并设计了UniEgoMotion模型。该模型通过第一人称图像提取场景语义信息，实现无需依赖显式3D场景的运动生成与预测。研究还构建了EE4D-Motion数据集用于训练，并在多个任务上取得了最先进的性能，为自中心运动建模提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 20:41:20 GMT</pubDate>
</item>
<item>
<title>基于自回归与强化学习的图像编辑模型EARL</title>
<link>https://arxiv.org/abs/2508.01119</link>
<guid>https://arxiv.org/abs/2508.01119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升图像编辑效果，EARL模型表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了三种提升图像编辑任务性能的策略：监督微调、强化学习和思维链推理。研究采用了一个统一处理文本和视觉标记的自回归多模态模型，并发现结合大规模多模态语言模型验证的强化学习是最有效的策略。因此，作者推出了EARL模型，该模型在多种图像编辑任务中表现优于现有基准，且使用较少训练数据。研究开源了代码、训练数据和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 19:47:29 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的文本-视频检索方法研究</title>
<link>https://arxiv.org/abs/2507.23284</link>
<guid>https://arxiv.org/abs/2507.23284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BLiM框架提升文本-视频检索效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究文本-视频检索任务，针对多模态大语言模型在检索中因候选优先偏差导致的问题，提出双向似然估计框架BLiM，并引入候选先验归一化模块CPN，有效提升检索精度。实验表明，BLiM在四个基准数据集上平均提升6.4 R@1，且CPN在多模态任务中展现出广泛适用性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:57:28 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover-V2：开源语言模型在自动定理证明中取得新突破</title>
<link>https://arxiv.org/abs/2508.03613</link>
<guid>https://arxiv.org/abs/2508.03613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover-V2在定理证明任务中表现优异，超越多个大型模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域取得了新的突破。该模型通过三项关键创新提升性能：Scaffolded数据合成、验证器引导的自我修正以及模型平均。其小型模型Goedel-Prover-V2-8B在MiniF2F测试中达到84.6%的pass@32，优于更大的DeepSeek-Prover-V2-671B。旗舰模型Goedel-Prover-V2-32B在标准模式下达到88.1%，在自我修正模式下达到90.4%，并在PutnamBench上解决了86个问题，成为开源模型中的第一名。模型代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:28:22 GMT</pubDate>
</item>
<item>
<title>ChartCap数据集提升图表描述准确性</title>
<link>https://arxiv.org/abs/2508.03164</link>
<guid>https://arxiv.org/abs/2508.03164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChartCap提升图表描述准确性和信息量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ChartCap，一个包含56.5万张真实图表及其类型特定、密集描述的大规模数据集。该数据集通过四阶段流程生成不含冗余信息的描述，并采用基于循环一致性的验证方法确保质量。研究还提出了一种新的评估指标——视觉一致性分数，用于衡量描述与图表的相似性。实验表明，使用ChartCap训练的模型在生成准确且信息丰富的图表描述方面表现优于现有模型和人工标注。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:09:07 GMT</pubDate>
</item>
<item>
<title>Representation Shift：一种与FlashAttention兼容的无训练令牌压缩方法</title>
<link>https://arxiv.org/abs/2508.00367</link>
<guid>https://arxiv.org/abs/2508.00367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的令牌压缩方法，提升模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Representation Shift的无训练、模型无关的令牌重要性度量方法，能够与FlashAttention等高效注意力机制兼容。该方法通过测量每个令牌表示的变化程度来实现令牌压缩，无需依赖注意力图，从而避免了传统方法与高效内核之间的不兼容问题。实验表明，该方法在视频文本检索和视频问答任务中分别实现了5.5%和4.4%的速度提升，适用于Transformer、CNN和状态空间模型等多种架构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 02:53:55 GMT</pubDate>
</item>
<item>
<title>可控超长视频生成方法LongVie及其基准测试</title>
<link>https://arxiv.org/abs/2508.03694</link>
<guid>https://arxiv.org/abs/2508.03694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongVie解决超长视频生成中的时序不一致和视觉退化问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为LongVie的端到端自回归框架，用于可控的超长视频生成。针对现有方法在长视频生成中出现的时序不一致和视觉退化问题，LongVie引入了统一噪声初始化策略、全局控制信号归一化、多模态控制框架以及退化感知训练策略，以提升视频的连贯性和视觉质量。此外，研究团队还构建了LongVGenBench基准测试集，包含100个高分辨率视频，涵盖多种真实和合成环境。实验表明，LongVie在长距离可控性、一致性和质量方面均达到当前最优水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>CompassVerifier：多领域答案验证与评估框架</title>
<link>https://arxiv.org/abs/2508.03686</link>
<guid>https://arxiv.org/abs/2508.03686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CompassVerifier提升LLM答案验证能力，支持多领域任务。</p><br /><br /><p><strong>摘要：</strong> 本文提出CompassVerifier，一个高效且稳健的轻量级答案验证模型，用于评估和优化大型语言模型。该模型具备跨数学、知识和推理任务的多领域能力，能够处理多种答案类型，并有效识别异常响应。研究还引入了VerifierBench基准，通过多源数据和人工分析增强验证效果。作者期望该框架能推动答案验证、评估协议和强化学习的研究。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:55:24 GMT</pubDate>
</item>
<item>
<title>Skywork UniPic：统一多模态任务的高效大模型</title>
<link>https://arxiv.org/abs/2508.03320</link>
<guid>https://arxiv.org/abs/2508.03320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork UniPic在单个架构中实现图像理解、文本生成和编辑，性能优异。</p><br /><br /><p><strong>摘要：</strong> Skywork UniPic是一款15亿参数的自回归模型，能够在单一架构中完成图像理解、文本到图像生成和图像编辑任务，无需任务专用适配器或模块间连接。该模型在GenEval、DPG-Bench、GEditBench-EN和ImgEdit-Bench等基准测试中表现优异，并能在消费级硬件（如RTX 4090）上生成1024x1024图像。其技术亮点包括解耦编码策略、渐进式分辨率训练以及基于任务奖励模型的数据集优化。Skywork UniPic展示了高保真多模态集成在资源受限环境下的可行性，为部署高效的多模态AI提供了新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 06:59:01 GMT</pubDate>
</item>
<item>
<title>多人群体说话视频生成数据集MIT与基线模型CovOG</title>
<link>https://arxiv.org/abs/2508.03050</link>
<guid>https://arxiv.org/abs/2508.03050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIT数据集和CovOG模型，推动多人群体说话视频生成研究。</p><br /><br /><p><strong>摘要：</strong> 现有研究主要关注单人独白或孤立面部动画，难以应用于真实多人群体互动。为此，本文提出MIT数据集，包含12小时高分辨率多人群体对话视频，涵盖2至4名说话者，并带有精细的身体姿态和语音交互标注。为验证MIT的潜力，作者进一步提出CovOG基线模型，集成多人群体姿态编码器（MPE）和交互式音频驱动器（IAD），用于生成逼真的多人群体说话视频。该研究为多人群体视频生成提供了重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 23:54:18 GMT</pubDate>
</item>
<item>
<title>ToolTrain提升代码问题定位能力</title>
<link>https://arxiv.org/abs/2508.03012</link>
<guid>https://arxiv.org/abs/2508.03012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToolTrain提升代码问题定位性能，优于现有模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ToolTrain的两阶段工具集成训练框架，旨在增强大型语言模型在软件开发中进行代码问题定位的能力。该方法结合了拒绝采样监督微调和工具集成强化学习，以提高模型使用检索工具进行多步骤推理和导航的能力。实验结果显示，ToolTrain训练的模型在函数级定位任务上表现优异，甚至超越了Claude-3.7。此外，改进的问题定位性能也提升了端到端的问题解决效果，证明了针对问题定位的训练是提升自动化软件开发的有效策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 22:44:21 GMT</pubDate>
</item>
<item>
<title>Seed Diffusion Preview：高效代码生成的扩散模型</title>
<link>https://arxiv.org/abs/2508.02193</link>
<guid>https://arxiv.org/abs/2508.02193</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed Diffusion Preview实现高速代码生成，性能领先。</p><br /><br /><p><strong>摘要：</strong> Seed Diffusion Preview是一种基于离散状态扩散的大规模语言模型，采用非序列化并行生成方式，显著提升了推理速度。在H20 GPU上实现了每秒2146个token的推理速度，同时在多个标准代码评估基准中保持了竞争力，比现有的Mercury Coder和Gemini Diffusion更快，成为代码生成领域的最新里程碑。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02193" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 04:43:01 GMT</pubDate>
</item>
<item>
<title>基于强化学习的近似最近邻搜索算法CRINN</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRINN通过强化学习实现高效近似最近邻搜索。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CRINN的新近似最近邻搜索（ANNS）算法，该算法将ANNS优化问题转化为强化学习问题，以执行速度作为奖励信号。这种方法能够自动生成越来越快的ANNS实现，同时保持准确性。实验结果表明，CRINN在六个广泛使用的NNS基准数据集上表现优异，其中在三个数据集上取得最佳性能，在另外两个数据集上并列第一。CRINN的成功不仅提升了ANNS优化的效果，还证明了结合强化学习的大型语言模型可以有效用于自动化复杂的算法优化任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:57:46 GMT</pubDate>
</item>
<item>
<title>AlignGuard-LoRA：一种防止大语言模型微调中对齐漂移的方法</title>
<link>https://arxiv.org/abs/2508.02079</link>
<guid>https://arxiv.org/abs/2508.02079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlignGuard-LoRA有效减少微调中的对齐漂移，提升模型安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出AlignGuard-LoRA（AGL），一种用于在微调大型语言模型时保持对齐性的框架。AGL通过引入主任务损失、基于Fisher信息矩阵的正则化、任务特定正则化以及碰撞感知正则化，有效控制参数更新方向，防止对齐漂移。研究还构建了DriftCaps基准测试，用于量化对齐漂移和安全性能下降。实验表明，AGL在不损害下游任务性能的前提下，可减少高达50%的安全关键指标上的对齐漂移。此外，AGL揭示了灾难性遗忘的缩放规律，确保微调后的模型保持稳定的适应能力。该方法为LoRA提供了一种结构化的改进方案，具有良好的实用性和扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:45:24 GMT</pubDate>
</item>
<item>
<title>TraceAlign：追踪并缓解大语言模型对齐漂移的框架</title>
<link>https://arxiv.org/abs/2508.02063</link>
<guid>https://arxiv.org/abs/2508.02063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TraceAlign通过分析训练数据源，有效减少LLM对齐漂移。</p><br /><br /><p><strong>摘要：</strong> 本文提出TraceAlign，一个用于追踪大语言模型（LLMs）对齐失败根源的框架。该框架引入Belief Conflict Index（BCI）来量化生成内容与对齐政策之间的语义不一致，并通过三种干预措施——TraceShield、Contrastive Belief Deconfliction Loss 和 Prov-Decode——显著降低对齐漂移。实验表明，这些方法在保持任务性能的同时，能减少高达85%的对齐偏差。此外，研究还基于后缀数组统计推导出对齐漂移的可能性上限，揭示了记忆频率和长度对对抗性激活风险的影响。TraceAlign为理解与缓解模型对齐问题提供了首个可扩展、可追溯且基于数据的工具集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:03:35 GMT</pubDate>
</item>
<item>
<title>LiveMCPBench：首个大规模MCP环境下的LLM代理评估基准</title>
<link>https://arxiv.org/abs/2508.01780</link>
<guid>https://arxiv.org/abs/2508.01780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveMCPBench首次提供大规模MCP环境下的LLM代理评估框架。</p><br /><br /><p><strong>摘要：</strong> 随着Model Context Protocol (MCP) 的快速发展，MCP服务器数量已超过10,000个。然而，现有的MCP基准测试仅限于单服务器设置，无法有效评估代理在真实场景中的能力。为此，研究团队推出了LiveMCPBench，这是首个包含95个真实任务的综合性基准，旨在评估大型、多样化MCP环境中的LLM代理。该平台还提供了LiveMCPTool和LiveMCPEval，支持可扩展、可复现的评估流程，并引入了MCP Copilot Agent进行多步骤任务执行。实验覆盖了10个主流模型，最佳模型成功率达到78.95%。研究揭示了模型性能差异较大，部分常用模型在复杂工具环境中表现不佳。LiveMCPBench为LLM代理能力的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:36:42 GMT</pubDate>
</item>
<item>
<title>LAMIC：一种无需训练的多参考图像合成框架</title>
<link>https://arxiv.org/abs/2508.00477</link>
<guid>https://arxiv.org/abs/2508.00477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LAMIC实现多参考图像合成，无需训练即可保持布局和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LAMIC，一个无需训练的多参考图像合成框架，首次将单参考扩散模型扩展到多参考场景。LAMIC基于MMDiT模型，引入两种可插拔注意力机制：Group Isolation Attention（GIA）用于增强实体分离，Region-Modulated Attention（RMA）用于实现布局感知生成。研究还引入三个评估指标：Inclusion Ratio、Fill Ratio 和 Background Similarity，以全面评估模型性能。实验表明，LAMIC在多个主要指标上达到最先进水平，表现出色的零样本泛化能力，在身份保持、背景一致性和提示遵循方面表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 05:51:54 GMT</pubDate>
</item>
<item>
<title>Dynaword: From One-shot to Continuously Developed Datasets</title>
<link>https://arxiv.org/abs/2508.02271</link>
<guid>https://arxiv.org/abs/2508.02271</guid>
<content:encoded><![CDATA[
Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:30:42 GMT</pubDate>
</item>
<item>
<title>ReMoMask：一种提升文本到动作生成性能的统一框架</title>
<link>https://arxiv.org/abs/2508.02605</link>
<guid>https://arxiv.org/abs/2508.02605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReMoMask提升文本到动作生成的准确性和多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ReMoMask，一种用于文本到动作生成的统一框架，旨在解决现有方法在多样性、物理合理性及同步性方面的不足。该框架包含三个关键创新：双向动量文本-动作模型提高跨模态检索精度；语义时空注意力机制消除异步伪影；RAG无分类器引导增强泛化能力。实验表明，ReMoMask在多个基准数据集上表现优异，FID分数显著优于现有最先进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 12:56:35 GMT</pubDate>
</item>
<item>
<title>Sparse-dLLM：提升扩散大语言模型推理效率的稀疏缓存方法</title>
<link>https://arxiv.org/abs/2508.02558</link>
<guid>https://arxiv.org/abs/2508.02558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sparse-dLLM通过动态缓存优化，提升dLLM推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Sparse-dLLM，一种无需训练的框架，通过动态缓存机制和稀疏注意力策略，解决扩散大语言模型（dLLMs）在推理过程中计算复杂度高和内存消耗大的问题。研究发现dLLMs中存在跨层稀疏性，关键token在多个解码步骤中保持重要性，而低相关token则持续不重要，从而支持选择性缓存剔除。该方法通过关注引导策略，保留关键token并动态剔除无用前缀/后缀，实验表明其在LLaDA和Dream系列模型上实现了高达10倍的吞吐量提升，同时保持性能与内存成本相近，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 12:14:03 GMT</pubDate>
</item>
<item>
<title>SHAMI-MT：连接标准阿拉伯语与叙利亚方言的机器翻译系统</title>
<link>https://arxiv.org/abs/2508.02268</link>
<guid>https://arxiv.org/abs/2508.02268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SHAMI-MT有效实现了标准阿拉伯语与叙利亚方言之间的高质量翻译。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SHAMI-MT，一个专门用于连接现代标准阿拉伯语（MSA）和叙利亚方言的双向机器翻译系统。该系统由两个模型组成，分别用于MSA到叙利亚方言和叙利亚方言到MSA的翻译，基于AraT5v2-base-1024架构进行优化。模型在Nabra数据集上进行了微调，并在MADAR语料库的未见数据上进行了评估。MSA到叙利亚方言的模型在GPT-4.1的评分中获得了4.01的高分，显示出其在准确性和方言真实性方面的优势。该研究为之前缺乏支持的语言对提供了重要工具，推动了方言阿拉伯语翻译的发展，并具有内容本地化、文化遗产保护和跨文化交流等应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:21:11 GMT</pubDate>
</item>
<item>
<title>AuroBind：一种用于高通量分子筛选的结构功能学习框架</title>
<link>https://arxiv.org/abs/2508.02137</link>
<guid>https://arxiv.org/abs/2508.02137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AuroBind提升蛋白质药物筛选效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AuroBind，一种基于结构的虚拟筛选框架，通过原子级模型和多种优化策略，显著提升了药物靶点筛选的精度与速度。该方法在多个疾病相关靶点中实现了7-69%的实验命中率，并成功识别出孤儿G蛋白偶联受体GPR151和GPR160的激动剂和拮抗剂，展示了其在药物发现中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 03:34:48 GMT</pubDate>
</item>
<item>
<title>基于不确定性驱动的流程奖励数据构建框架</title>
<link>https://arxiv.org/abs/2508.01773</link>
<guid>https://arxiv.org/abs/2508.01773</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自动化流程奖励数据构建方法，提升数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于不确定性的流程奖励数据构建框架，旨在提高大语言模型在数学推理任务中的准确性。该框架涵盖数据生成和标注过程，同时引入两种新的输出聚合方法，结合多数投票与流程奖励模型的优势。实验结果表明，该方法在多个基准测试中表现出色，有效提升了模型的推理能力。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01773" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:14:13 GMT</pubDate>
</item>
<item>
<title>Voxlect：全球方言与区域语言的语音基础模型基准测试</title>
<link>https://arxiv.org/abs/2508.01691</link>
<guid>https://arxiv.org/abs/2508.01691</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voxlect提供全球多种语言方言的语音模型基准测试。</p><br /><br /><p><strong>摘要：</strong> Voxlect是一个用于建模全球方言和区域语言的新基准，涵盖了英语、阿拉伯语、普通话、粤语、藏语、印地语、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语等多种语言。该研究使用了来自30个公开语音语料库的超过200万条训练语音，并评估了多个常用语音基础模型在方言分类中的表现。研究还分析了在噪声条件下的模型鲁棒性，并展示了Voxlect在语音识别数据集增强和语音生成系统评估中的应用。Voxlect已开源，可供研究人员使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01691" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 05:51:28 GMT</pubDate>
</item>
<item>
<title>人工智能在绘画归属中的挑战与研究</title>
<link>https://arxiv.org/abs/2508.01408</link>
<guid>https://arxiv.org/abs/2508.01408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI在绘画归属中面临识别和生成图像的双重挑战。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能在艺术作品归属，特别是绘画方面的挑战。随着AI生成和分析图像能力的提升，出现了两个主要问题：一方面，AI生成的图像可能被误认为是某位艺术家的作品；另一方面，AI可能无法正确识别真实绘画的作者。研究使用了最新的AI模型，在包含近40,000幅来自128位艺术家的画作数据集上进行了实验。结果显示，视觉语言模型在画布归属和识别AI生成图像方面存在局限性。随着用户越来越多地依赖AI获取信息，提高这些模型在艺术家归属和AI图像检测方面的能力变得尤为重要，以防止错误信息的传播。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 11:27:31 GMT</pubDate>
</item>
<item>
<title>多模态数据融合预测家庭财富的研究</title>
<link>https://arxiv.org/abs/2508.01109</link>
<guid>https://arxiv.org/abs/2508.01109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态数据有效提升家庭财富预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了社会经济指标如家庭财富是否能在卫星图像和互联网文本中留下可识别的痕迹。通过结合非洲地区的人口与健康调查（DHS）数据、Landsat卫星图像以及由AI搜索代理获取的网络文本，作者构建了一个多模态框架，用于预测国际财富指数。该框架包括五个管道：基于图像的视觉模型、仅依赖位置/年份的大型语言模型、AI代理检索与合成文本、图像-文本联合编码器，以及所有信号的集成。研究发现，融合视觉和文本信息显著优于单一视觉模型，且LLM内部知识比代理检索文本更有效。此外，研究揭示了视觉和语言模态之间存在部分表征收敛，支持了柏拉图式表征假设。最后，研究发布了一个包含60,000多个DHS集群的多模态数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 19:07:16 GMT</pubDate>
</item>
<item>
<title>基于ViT嵌入的量子支持向量机可扩展性研究</title>
<link>https://arxiv.org/abs/2508.00024</link>
<guid>https://arxiv.org/abs/2508.00024</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViT嵌入提升量子SVM性能，实现更高准确率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了量子支持向量机在高维量子态和硬件限制下的可扩展性挑战，并提出了一种结合类平衡k-means蒸馏与预训练Vision Transformer嵌入的量子-经典混合管道。实验表明，ViT嵌入能够显著提升量子SVM的性能，在Fashion-MNIST数据集上比传统SVM高出8.02%，在MNIST数据集上高出4.42%。而CNN特征则表现出性能下降。通过16量子位张量网络模拟，研究首次系统证明了量子核优势高度依赖于嵌入选择，揭示了Transformer注意力机制与量子特征空间之间的关键协同效应，为可扩展的量子机器学习提供了可行路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00024" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 17:23:51 GMT</pubDate>
</item>
<item>
<title>Dens3R：统一几何预测的3D基础模型</title>
<link>https://arxiv.org/abs/2507.16290</link>
<guid>https://arxiv.org/abs/2507.16290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dens3R实现多几何量联合预测，提升3D重建精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出Dens3R，一种用于联合几何密集预测的3D基础模型，能够同时准确回归深度、表面法线等几何属性。该模型采用两阶段训练框架，构建具有泛化性和内在不变性的点云表示，并引入位置插值旋转位置编码以增强对高分辨率输入的鲁棒性。通过融合图像对匹配特征与内在不变性建模，Dens3R实现了从单视角到多视角的一致几何感知。此外，研究还设计了支持几何一致性多视角推理的后处理流程，实验表明其在多种密集3D预测任务中表现优异，具备广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 03:22:30 GMT</pubDate>
</item>
<item>
<title>Qwen-Image：在文本渲染与图像编辑上的重大突破</title>
<link>https://arxiv.org/abs/2508.02324</link>
<guid>https://arxiv.org/abs/2508.02324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen-Image在文本渲染和图像编辑上取得显著进展。</p><br /><br /><p><strong>摘要：</strong> Qwen-Image是Qwen系列中的一款图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。文章介绍了其通过大规模数据处理、渐进式训练策略提升文本渲染能力，尤其在中英文等语言中表现优异。同时，通过多任务训练和双编码机制，增强了图像编辑的一致性与视觉质量。Qwen-Image在多个基准测试中达到最先进水平，展示了其在图像生成与编辑方面的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 07:49:20 GMT</pubDate>
</item>
<item>
<title>基于自监督强化学习的指令遵循能力提升方法</title>
<link>https://arxiv.org/abs/2508.02150</link>
<guid>https://arxiv.org/abs/2508.02150</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自监督RL框架提升推理模型的指令遵循能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于自监督强化学习的框架，旨在提升推理模型的指令遵循能力，而无需依赖外部模型。该方法利用模型内部信号进行训练，避免了传统方法中对外部模型的依赖，从而解决了成本高和可访问性差的问题。实验表明，该框架在保持推理性能的同时显著提升了指令遵循能力，提供了一种可扩展且经济高效的解决方案。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02150" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 03:48:59 GMT</pubDate>
</item>
<item>
<title>基于情境嵌入的长文档检索增强生成方法</title>
<link>https://arxiv.org/abs/2508.01959</link>
<guid>https://arxiv.org/abs/2508.01959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">情境嵌入模型提升长文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的检索增强生成方法，通过将短文本块置于更广泛的情境中进行编码，以提高检索效果。研究指出，传统方法在处理长文档时面临嵌入模型容量不足和局部证据需求的挑战。为此，作者引入了情境嵌入模型（SitEmb），并在专门设计的书籍情节检索数据集上验证了其有效性。实验结果表明，SitEmb模型在参数量较少的情况下表现优于多个大模型，并在多语言和多种下游任务中取得优异成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 19:59:31 GMT</pubDate>
</item>
<item>
<title>动态视觉标记压缩框架GlimpsePrune提升大视觉语言模型效率</title>
<link>https://arxiv.org/abs/2508.01548</link>
<guid>https://arxiv.org/abs/2508.01548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GlimpsePrune实现高效视觉标记压缩，保持模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为GlimpsePrune的动态剪枝框架，旨在解决大型视觉语言模型（LVLMs）在处理高分辨率输入时的效率问题。传统方法采用固定压缩比例，难以适应不同复杂度的场景，导致信息丢失和性能下降。GlimpsePrune通过数据驱动的“瞥见”机制，在生成答案前一次性剪除无关视觉标记，有效减少计算成本。实验表明，该方法可剪除92.6%的视觉标记，同时保持基准性能，并在微调后进一步提升至110%。该研究为构建更强大且高效的LVLMs提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 22:15:43 GMT</pubDate>
</item>
<item>
<title>RoboMemory：一种面向物理机器人系统的多记忆框架</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboMemory提升机器人长期学习与任务执行能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboMemory，一种基于认知神经科学的多记忆框架，用于物理机器人在现实环境中的持续学习。该框架包含四个核心模块：信息预处理器、终身具身记忆系统、闭环规划模块和低级执行器，分别模拟大脑不同区域的功能。其核心模块通过并行更新与检索机制提升推理速度，并结合动态知识图谱增强记忆一致性与可扩展性。实验表明，RoboMemory在EmbodiedBench基准测试中表现优于现有开源和闭源模型，显示出强大的长期学习能力与实际部署效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 11:39:42 GMT</pubDate>
</item>
<item>
<title>基于贪婪目标的元强化学习中涌现探索行为的研究</title>
<link>https://arxiv.org/abs/2508.01287</link>
<guid>https://arxiv.org/abs/2508.01287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，在特定条件下，贪婪目标可激发探索行为。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在元强化学习中，通过仅最大化贪婪目标是否能产生探索行为。研究提出三个必要条件：环境结构重复性、代理记忆和长期信用分配。实验表明，当环境具有重复结构且代理具备记忆能力时，即使不引入额外激励，策略也能表现出信息寻求的探索行为。进一步实验显示，若缺少环境结构或记忆，探索行为消失。而移除长期信用分配后，探索行为不一定消失，这可能与伪汤普森采样效应有关。研究结果表明，在适当条件下，探索与利用可从统一的奖励最大化过程中自然涌现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 05:42:59 GMT</pubDate>
</item>
<item>
<title>多阶段复杂任务中的测试时计算最优缩放研究</title>
<link>https://arxiv.org/abs/2508.00890</link>
<guid>https://arxiv.org/abs/2508.00890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多阶段任务中测试时计算资源分配优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在多阶段复杂任务中如何通过测试时计算资源的最优分配来提升大型语言模型的性能。传统研究主要关注单阶段任务，而现实问题通常由多个异构子任务组成，每个子任务需要不同能力的模型。文章提出了AgentTTS框架，利用LLM代理通过与执行环境的迭代交互实现计算资源的最优分配。实验表明，该方法在搜索效率、对训练集大小的鲁棒性以及可解释性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 15:21:18 GMT</pubDate>
</item>
<item>
<title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
<link>https://arxiv.org/abs/2508.02317</link>
<guid>https://arxiv.org/abs/2508.02317</guid>
<content:encoded><![CDATA[
Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 07:33:04 GMT</pubDate>
</item>
<item>
<title>CellForge：基于多智能体框架的虚拟细胞建模系统</title>
<link>https://arxiv.org/abs/2508.02276</link>
<guid>https://arxiv.org/abs/2508.02276</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CellForge通过多智能体协作构建虚拟细胞模型，提升预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CellForge，一个基于多智能体框架的系统，能够将生物数据和任务描述直接转化为优化的虚拟细胞计算模型。该系统包含任务分析、方法设计和实验执行三个核心模块，其中方法设计模块由不同视角的专家代理和一个中央协调者组成，通过协作达成共识。CellForge在多种单细胞扰动预测任务中表现优于现有方法，展示了多智能体协作在解决复杂建模问题中的优势。代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02276" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:43:31 GMT</pubDate>
</item>
<item>
<title>个性化安全对齐框架提升文本到图像生成模型的安全性</title>
<link>https://arxiv.org/abs/2508.01151</link>
<guid>https://arxiv.org/abs/2508.01151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PSA框架实现用户定制化安全控制，提升生成内容安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对文本到图像扩散模型中存在的安全机制不足问题，提出了个性化安全对齐（PSA）框架。该框架通过整合用户个人安全偏好，调整模型行为以符合个体安全标准，同时保持图像质量。研究引入了Sage数据集，并采用交叉注意力机制实现用户偏好融合。实验表明，PSA在抑制有害内容方面优于现有方法，能更好地满足用户约束条件，提升了生成内容的安全性和准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 22:23:20 GMT</pubDate>
</item>
<item>
<title>Foundation-Sec-8B-Instruct：面向网络安全的对话型大语言模型</title>
<link>https://arxiv.org/abs/2508.01059</link>
<guid>https://arxiv.org/abs/2508.01059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新型网络安全大模型提升对话与指令执行能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Foundation-Sec-8B-Instruct，这是一个专为网络安全对话设计的大语言模型，基于Foundation-Sec-8B进行优化，具备领域知识、指令遵循和对话交互能力。该模型在多个网络安全任务中表现优于Llama 3.1-8B-Instruct，并与GPT-4o-mini相当。研究者希望该模型能成为网络安全专业人员日常工作的得力助手，并已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 16:25:57 GMT</pubDate>
</item>
<item>
<title>无需运行时环境的网络安全LLM训练框架Cyber-Zero</title>
<link>https://arxiv.org/abs/2508.00910</link>
<guid>https://arxiv.org/abs/2508.00910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cyber-Zero通过模拟生成高质量交互轨迹提升网络安全LLM性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cyber-Zero，一个无需运行时环境的框架，用于合成高质量的代理轨迹以训练网络安全大语言模型。该框架利用公开的CTF写入和角色驱动的LLM模拟，逆向工程运行行为并生成真实、长周期的交互序列。在三个主流CTF基准测试中，基于Cyber-Zero训练的模型性能比基线模型高出13.1%。最佳模型Cyber-Zero-32B在开源模型中达到最先进的性能，与DeepSeek-V3-0324和Claude-3.5-Sonnet相当，同时具有更高的成本效益，证明了无运行时轨迹合成在网络安全代理开发中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 14:10:37 GMT</pubDate>
</item>
<item>
<title>InstructVLA：融合多模态推理与精准动作生成的机器人模型</title>
<link>https://arxiv.org/abs/2507.17520</link>
<guid>https://arxiv.org/abs/2507.17520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InstructVLA提升机器人多模态任务表现与推理能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了InstructVLA，一种端到端的视觉-语言-动作（VLA）模型，旨在同时保持大型视觉-语言模型（VLM）的灵活推理能力和高效的操控性能。通过引入Vision-Language-Action Instruction Tuning（VLA-IT）训练范式，InstructVLA在标准VLM语料库和650K样本的VLA-IT数据集上联合优化文本推理与动作生成。实验表明，InstructVLA在SimplerEnv任务中比SpatialVLA提升30.5%，在SimplerEnv-Instruct基准测试中优于OpenVLA和GPT-4o辅助的行动专家。此外，InstructVLA在多模态任务中表现优异，并在推理时通过文本推理增强操作性能，展示了其在人机交互与策略学习中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 09:57:06 GMT</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro：开源AI代理框架提升研究可复现性与性能</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cognitive Kernel-Pro是开源AI代理框架，提升研究可复现性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Cognitive Kernel-Pro，一个完全开源且尽可能免费的多模块AI代理框架，旨在推动高级AI代理的开发与评估。该框架在四个关键领域（网络、文件、代码和通用推理）系统地研究了高质量训练数据的构建，包括查询、轨迹和可验证答案。此外，还探索了代理在测试时的反思和投票策略，以提高其鲁棒性和性能。Cognitive Kernel-Pro在GAIA基准上取得了最先进的结果，其8B参数模型超越了WebDancer和WebSailor等先前领先系统，为可访问、高性能的AI代理设定了新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 04:11:31 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型评估新基准MCIF发布</title>
<link>https://arxiv.org/abs/2507.19634</link>
<guid>https://arxiv.org/abs/2507.19634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCIF是首个多语言多模态指令跟随评估基准。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的发展，多模态大语言模型（MLLMs）逐渐从单一任务系统演变为通用指令遵循模型。然而，现有评估基准在多语言、多模态和长文本上下文方面存在不足。为此，研究者推出了MCIF，这是一个基于科学演讲的多语言人工标注基准，旨在评估模型在跨语言和多模态环境下的指令遵循能力。MCIF涵盖语音、视觉和文本三种模态，并支持英语、德语、意大利语和中文四种语言，有助于全面评估MLLMs的能力。该基准以CC-BY 4.0许可证发布，鼓励开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 15:00:51 GMT</pubDate>
</item>
<item>
<title>AI生成交互式音视频内容的新方法与挑战</title>
<link>https://arxiv.org/abs/2508.00632</link>
<guid>https://arxiv.org/abs/2508.00632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AVR-Eval评估机制和多代理系统提升AI生成游戏质量。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了AI在生成交互式音视频内容（如游戏）方面的挑战，提出AVR-Eval评估机制，通过对比音频-视频记录来判断内容质量。同时构建了AVR-Agent多代理系统，利用多媒体资源生成JavaScript代码，并通过反馈迭代优化。实验表明，该系统生成的内容胜率高于单次生成方式，但尚未有效利用定制资源和反馈，揭示了人机在内容创作上的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 09:45:13 GMT</pubDate>
</item>
<item>
<title>基于3D高斯的增量图像目标导航方法</title>
<link>https://arxiv.org/abs/2508.00823</link>
<guid>https://arxiv.org/abs/2508.00823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出IGL-Nav实现高效3D图像目标导航。</p><br /><br /><p><strong>摘要：</strong> 本文针对以图像为目标的视觉导航问题，提出了一种基于可渲染3D高斯（3DGS）表示的增量定位框架IGL-Nav。该方法通过逐步更新场景表示并利用几何信息进行粗略定位，再结合可微渲染进行精细姿态优化，提升了导航效率与准确性。实验表明，IGL-Nav在多种配置下均优于现有方法，并能在真实机器人平台上部署，支持任意姿态拍摄目标图像。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于音频空间信息的视频生成方法研究</title>
<link>https://arxiv.org/abs/2508.00782</link>
<guid>https://arxiv.org/abs/2508.00782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpA2V通过音频空间信息生成语义与空间对齐的视频。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SpA2V的新框架，旨在通过利用音频中的空间信息（如音量和频率）来生成与输入音频在语义和空间上高度一致的视频。该方法分为两个阶段：第一阶段是音频引导的视频规划，利用先进的多模态大模型从音频中提取空间和语义线索，构建视频场景布局；第二阶段是基于布局的视频生成，将这些布局作为条件指导整合到预训练扩散模型中，实现无需训练的视频生成。实验表明，SpA2V在生成高质量视频方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:05:04 GMT</pubDate>
</item>
<item>
<title>3D-R1：提升3D场景理解的视觉语言模型</title>
<link>https://arxiv.org/abs/2507.23478</link>
<guid>https://arxiv.org/abs/2507.23478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D-R1增强3D场景理解能力，提升推理与泛化效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出3D-R1，一种用于提升3D场景理解的视觉语言模型。针对现有3D VLMs在空间数据质量和视角假设上的不足，研究者构建了高质量合成数据集Scene-30K，并引入RLHF训练方法和三种奖励函数以增强推理能力。此外，动态视角选择策略提升了模型对3D场景的理解效果。实验表明，3D-R1在多个3D场景基准测试中平均提升10%，展现出强大的推理与泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 07:59:06 GMT</pubDate>
</item>
<item>
<title>多语言对话中大语言模型的幻觉现象研究</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现不同语言中大模型幻觉率差异显著。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在三种语言（印地语、波斯语和中文）的对话数据中，GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1 和 Qwen-3 等大语言模型的幻觉现象。结果显示，这些模型在中文中的幻觉较少，而在印地语和波斯语中幻觉率显著较高。该研究扩展了对英语以外语言中幻觉问题的理解，强调了多语言环境下模型可靠性的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 10:39:51 GMT</pubDate>
</item>
<item>
<title>DAEDAL：动态自适应长度扩展提升扩散语言模型性能</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DAEDAL解决DLLMs固定生成长度限制，提升效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出DAEDAL，一种无需训练的去噪策略，用于解决扩散语言模型（DLLMs）因固定生成长度导致的性能瓶颈。该方法通过两个阶段实现动态长度扩展：首先从短初始长度开始，根据序列完成度逐步扩展至任务合适的长度；其次在去噪过程中动态干预，通过插入掩码标记扩展不足区域，确保输出完整。实验表明，DAEDAL在保持甚至超越固定长度基线性能的同时，显著提升了计算效率，为DLLMs的发展提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>基于多模型共识的高效对话评估方法</title>
<link>https://arxiv.org/abs/2508.00454</link>
<guid>https://arxiv.org/abs/2508.00454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效对话评估方法，提升评估效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在对话评估中的可靠性问题，提出一种高效的多轮对话评估方法。该方法通过整合多个语言模型的判断知识到单一模型中，既保留了多模型反馈的优势，又大幅降低了评估成本。实验结果表明，该方法在多个对话评估基准上表现优于现有方法，展现出更高的效率和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 05:26:01 GMT</pubDate>
</item>
<item>
<title>多模态指代分割研究综述</title>
<link>https://arxiv.org/abs/2508.00265</link>
<guid>https://arxiv.org/abs/2508.00265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述多模态指代分割技术及其应用。</p><br /><br /><p><strong>摘要：</strong> 多模态指代分割旨在根据文本或音频中的指代表达，在图像、视频和3D场景中准确分割目标对象，广泛应用于需要基于用户指令进行精确物体感知的场景。过去十年，随着卷积神经网络、Transformer和大语言模型的发展，该领域取得了显著进展。本文系统回顾了多模态指代分割的研究背景、常用数据集、统一的元架构以及针对图像、视频和3D场景的代表性方法，并探讨了处理现实复杂性的广义指代表达方法及相关任务与应用。文章还提供了在标准基准上的性能比较，并持续跟踪相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 22:14:00 GMT</pubDate>
</item>
<item>
<title>基于经验增强的软件问题解决方法SWE-Exp</title>
<link>https://arxiv.org/abs/2507.23361</link>
<guid>https://arxiv.org/abs/2507.23361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Exp通过积累修复经验提升软件问题解决效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SWE-Exp，一种基于经验增强的软件问题解决方法。该方法通过构建多维度的经验库，从以往的修复过程中提取可复用的知识，从而避免重复探索失败路径，并提升对类似问题的解决能力。实验表明，SWE-Exp在SWE-bench-Verified数据集上取得了41.6%的Pass@1准确率，展现出卓越的性能。该方法为自动化软件工程代理提供了一种系统积累和利用修复经验的新范式，推动问题解决方式从试错探索向经验驱动的战略性解决转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 05:13:42 GMT</pubDate>
</item>
<item>
<title>SWE-Debate：通过多智能体辩论提升代码问题定位与修复</title>
<link>https://arxiv.org/abs/2507.23348</link>
<guid>https://arxiv.org/abs/2507.23348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Debate利用多智能体辩论提升代码问题定位与修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SWE-Debate，一种基于多智能体辩论的框架，旨在提升代码问题定位与修复的准确性。该框架通过遍历代码依赖图生成多个故障传播路径，并组织不同推理视角的智能体进行三轮辩论，从而协同得出更精准的修复方案。最终，修复计划被整合到基于MCTS的代码修改代理中以生成补丁。实验表明，SWE-Debate在SWE-bench基准测试中取得了最先进的结果，显著优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 04:54:46 GMT</pubDate>
</item>
<item>
<title>PixelNerd：一种高效的单阶段像素神经场扩散模型</title>
<link>https://arxiv.org/abs/2507.23268</link>
<guid>https://arxiv.org/abs/2507.23268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PixelNerd实现高效图像生成，无需复杂管道。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PixelNerd的新型扩散模型，该模型通过神经场直接进行像素级解码，避免了传统两阶段训练中因VAE压缩导致的误差累积和解码伪影问题。PixelNerd采用单尺度、单阶段的端到端架构，在ImageNet数据集上取得了2.15 FID（256×256）和2.84 FID（512×512）的优异性能，且无需复杂的级联管道或VAE模块。此外，该框架还扩展至文本到图像生成任务，在GenEval和DPG基准测试中表现出色，显示出其在高质量图像生成中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:07:20 GMT</pubDate>
</item>
<item>
<title>基于迭代优化的高效3D重建模型iLRM</title>
<link>https://arxiv.org/abs/2507.23277</link>
<guid>https://arxiv.org/abs/2507.23277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">iLRM通过迭代机制实现高效高质3D重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为iLRM的迭代大型3D重建模型，旨在解决现有基于Transformer的3D重建方法在可扩展性和计算效率上的不足。iLRM通过三个核心原则提升性能：解耦场景表示与输入图像以生成紧凑的3D表示；将多视角注意力交互分解为两阶段注意力机制以降低计算成本；并在每一层注入高分辨率信息以提高重建精度。实验结果表明，iLRM在RE10K和DL3DV等数据集上表现出色，在保持计算成本相近的情况下，能够处理更多输入视角并提供更高质量的3D重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:33:07 GMT</pubDate>
</item>
<item>
<title>基于增量学习的高效机器遗忘算法研究</title>
<link>https://arxiv.org/abs/2507.23257</link>
<guid>https://arxiv.org/abs/2507.23257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出IAU算法实现高效机器遗忘。</p><br /><br /><p><strong>摘要：</strong> 随着隐私问题日益突出，机器遗忘技术受到关注。现有基于影响的方法因计算复杂而难以应用。本文受认知科学启发，建立记忆与遗忘的理论联系，提出从增量学习角度出发的IAU算法，显著提升遗忘效率并保持模型性能。实验表明IAU在多个数据集和模型架构中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 01:34:27 GMT</pubDate>
</item>
<item>
<title>Phi-Ground模型提升GUI接地性能，推动计算机使用代理发展</title>
<link>https://arxiv.org/abs/2507.23779</link>
<guid>https://arxiv.org/abs/2507.23779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-Ground在GUI接地任务中取得突破性进展。</p><br /><br /><p><strong>摘要：</strong> 随着多模态推理模型的发展，计算机使用代理（CUAs）正逐渐成为现实。GUI接地是实现实际操作的核心技术，直接影响系统成败。当前的端到端接地模型在ScreenSpot-pro和UI-Vision等挑战性基准上的准确率不足65%，难以部署。本文通过实证研究，从数据收集到模型训练进行了深入探讨，最终提出了Phi-Ground模型家族，在所有五个接地基准测试中表现优异，尤其在10B参数以下的代理设置中达到最先进水平。在端到端模型设置中，Phi-Ground在ScreenSpot-pro和UI-Vision上的得分分别为43.2和27.2，展示了其在实际应用中的潜力。论文还总结了模型构建的经验与教训，对其他感知任务具有借鉴意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:59:09 GMT</pubDate>
</item>
<item>
<title>强化学习在3D环境中的空间推理与泛化能力研究</title>
<link>https://arxiv.org/abs/2507.23698</link>
<guid>https://arxiv.org/abs/2507.23698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升3D环境中视觉运动代理的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）在提升视觉运动代理空间推理和交互能力方面的潜力，特别是在Minecraft等3D环境中的应用。研究提出通过跨视角目标规范构建统一的多任务目标空间，并利用Minecraft的高可定制性实现自动化任务生成，以解决多任务RL中的表示挑战。同时，构建了一个高效的分布式RL框架用于大规模训练。实验结果表明，RL显著提升了交互成功率，实现了对未见过环境的零样本泛化，展示了其在增强视觉运动代理空间推理能力方面的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 12:20:02 GMT</pubDate>
</item>
<item>
<title>ViLLA框架提升机器人操作策略的泛化能力</title>
<link>https://arxiv.org/abs/2507.23682</link>
<guid>https://arxiv.org/abs/2507.23682</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViLLA框架提升机器人操作策略的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ViLLA-X，一种基于视觉-语言-隐动作（latent action）的新型框架，用于学习具有泛化能力的机器人操作策略。该框架改进了隐动作的学习方式及其在预训练中的整合方法，在多个模拟环境和真实机器人平台上表现出色。研究认为，ViLLA范式具有重要潜力，为未来相关研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23682" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:57:46 GMT</pubDate>
</item>
<item>
<title>软最大化注意力机制的递归形式及其表达能力分析</title>
<link>https://arxiv.org/abs/2507.23632</link>
<guid>https://arxiv.org/abs/2507.23632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究软最大化注意力的递归形式以解释其优越性。</p><br /><br /><p><strong>摘要：</strong> 本文通过推导软最大化注意力的递归形式，揭示了其与线性注意力之间的差异。研究表明，软最大化注意力在查询和键的内积上具有更优的非线性特性，从而提升了模型的表达能力。通过将软最大化注意力描述为递归神经网络的形式，可以更好地理解其各个组件的作用及相互关系，进而解释为何软最大化注意力在下游任务中表现更优。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:10:03 GMT</pubDate>
</item>
<item>
<title>基于KAN的双教师知识蒸馏艺术风格分类方法</title>
<link>https://arxiv.org/abs/2507.23436</link>
<guid>https://arxiv.org/abs/2507.23436</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">使用KAN改进双教师框架，提升艺术风格分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对艺术风格分类中数据标注不足和风格特征复杂的问题，提出一种基于Kolmogorov-Arnold Networks (KANs) 的双教师知识蒸馏方法。该方法通过替换传统MLP投影层为KANs，增强对非线性特征关联的建模能力，同时保留两个教师网络的互补信息：一个关注局部纹理与笔触，另一个捕捉整体风格层次。实验表明，该方法在WikiArt和Pandora18k数据集上优于原有双教师架构，提升了Top-1准确率和线性探针性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23436" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>面向阿拉伯语的增强型密集段落检索框架</title>
<link>https://arxiv.org/abs/2507.23404</link>
<guid>https://arxiv.org/abs/2507.23404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种针对阿拉伯语的增强DPR框架，提升问答系统性能。</p><br /><br /><p><strong>摘要：</strong> 阿拉伯语由于其复杂的形态学、可选变音符号以及现代标准阿拉伯语与方言共存的特点，给自然语言处理和信息检索带来了挑战。尽管阿拉伯语在全球的重要性日益增加，但在NLP研究和基准资源中仍处于劣势。本文提出了一种专为阿拉伯语设计的增强型密集段落检索（DPR）框架，核心是引入了注意力相关性评分（ARS）机制，以更有效地建模问题与段落之间的语义相关性。该方法结合了预训练阿拉伯语语言模型和架构优化，显著提升了阿拉伯语问答任务的排名准确性。相关代码已公开在GitHub上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 06:18:28 GMT</pubDate>
</item>
<item>
<title>NeRF-GS：融合NeRF与3D高斯散射的新型框架</title>
<link>https://arxiv.org/abs/2507.23374</link>
<guid>https://arxiv.org/abs/2507.23374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeRF-GS结合NeRF与3DGS，提升3D场景表示性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出NeRF-GS，一种将神经辐射场（NeRF）与3D高斯散射（3DGS）联合优化的新框架。该框架利用NeRF的连续空间表示，解决3DGS在高斯初始化敏感性、空间感知有限和高斯间相关性弱等方面的问题，从而提升其性能。NeRF-GS通过逐步对齐3DGS的空间特征与NeRF，使两者在共享3D空间信息的基础上共同优化。同时，通过优化隐式特征和高斯位置的残差向量，增强3DGS的个性化能力。实验结果表明，NeRF-GS在基准数据集上表现优于现有方法，达到最先进水平，验证了NeRF与3DGS的互补性，为高效3D场景表示提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 05:43:31 GMT</pubDate>
</item>
<item>
<title>TARS：一种改进多模态大语言模型幻觉的偏好优化方法</title>
<link>https://arxiv.org/abs/2507.21584</link>
<guid>https://arxiv.org/abs/2507.21584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TARS提升多模态模型推理可靠性，减少幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文提出TARS，一种基于token自适应的偏好优化策略，旨在解决多模态大语言模型在视觉-语言推理中产生的幻觉问题。传统直接偏好优化方法容易过拟合文本线索，导致对视觉信息的依赖不足。TARS通过最小最大优化框架，在保持语义约束的前提下，增强模型对因果视觉信息的感知能力，从而有效降低幻觉率。实验表明，TARS仅使用4800个偏好样本即可显著提升模型表现，优于标准DPO并接近GPT-4o水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 04:39:19 GMT</pubDate>
</item>
<item>
<title>基于人格向量的大型语言模型行为分析与控制</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过人格向量分析和控制语言模型的行为变化。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在部署过程中行为变化的潜在原因，通过分析模型激活空间中的人格向量（persona vectors），识别出如邪恶、奉承和幻觉倾向等特质。研究发现，这些向量可用于监控模型人格的变化，并预测和控制训练过程中的性格转变。研究还提出了一种预防性引导方法，以减少或避免不期望的性格变化。此外，人格向量还能用于检测可能导致不良性格变化的训练数据。该方法自动化程度高，只需自然语言描述即可提取人格向量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 01:20:14 GMT</pubDate>
</item>
<item>
<title>农业视觉语言模型评估基准AgroBench的引入与分析</title>
<link>https://arxiv.org/abs/2507.20519</link>
<guid>https://arxiv.org/abs/2507.20519</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgroBench用于评估农业VLM模型性能，揭示其在细粒度识别中的不足。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgroBench，一个用于评估视觉语言模型（VLM）在农业任务中表现的基准测试平台。该基准涵盖七个农业主题，包括203种作物和682种病害分类，由专家农艺师标注。研究发现，尽管VLM在某些任务中表现良好，但在细粒度识别如杂草识别方面仍存在较大提升空间。作者分析了VLM的错误类型，并提出了未来改进方向。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20519" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:58:29 GMT</pubDate>
</item>
<item>
<title>时间对称性在序列模型中的应用研究</title>
<link>https://arxiv.org/abs/2507.14793</link>
<guid>https://arxiv.org/abs/2507.14793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究时间对称性在序列模型中的应用及优势。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将等变性理论应用于序列模型，特别是针对时间参数化的对称性。传统RNN在处理动态变换时表现不佳，而引入时间流等变性后，模型在训练速度、长度泛化和速度泛化方面均有显著提升。该研究为构建更符合现实世界时间对称性的序列模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 22:52:21 GMT</pubDate>
</item>
<item>
<title>Seed-Prover：基于形式验证的数学定理证明模型</title>
<link>https://arxiv.org/abs/2507.23726</link>
<guid>https://arxiv.org/abs/2507.23726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed-Prover在数学定理证明中取得重大突破。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Seed-Prover，一个基于形式验证和长链式推理的数学定理证明模型。该模型通过Lean反馈、已证明引理和自我总结不断优化证明过程，并设计了三种测试时推理策略以应对IMO级别的竞赛问题。Seed-Prover成功证明了78.1%的过去IMO问题，超越了之前的最先进水平。此外，作者还开发了Seed-Geometry几何推理引擎，提升了Lean在几何领域的支持能力。该系统在IMO 2025中成功解决了5道题目，展示了自动化数学推理的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:00:30 GMT</pubDate>
</item>
<item>
<title>构建多语言语音对话模型评估基准</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一个包含中英文的语音对话模型评估数据集。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于语音对话模型（SDMs）的实际效果，指出其在理解与模拟人类对话方面仍存在研究不足。相较于文本大语言模型，语音交互更具复杂性，如歧义、语境依赖等问题。为推动SDM发展，作者构建了一个包含1079个实例的中英文数据集，并引入基于大语言模型的评估方法，以更贴近人类判断的方式全面评估SDM的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>基于用户意图的下一代推荐系统RecGPT</title>
<link>https://arxiv.org/abs/2507.22879</link>
<guid>https://arxiv.org/abs/2507.22879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecGPT通过用户意图驱动提升推荐系统效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种以用户意图为核心的下一代推荐系统RecGPT，旨在解决传统推荐系统依赖历史交互数据导致的过滤气泡和长尾问题。RecGPT通过整合大语言模型，在用户兴趣挖掘、物品检索和解释生成等关键阶段实现意图驱动的推荐。该系统采用多阶段训练策略，并引入人机协作的评估机制，有效提升了推荐系统的多样性与用户满意度。目前RecGPT已在淘宝App全面部署，实验结果表明其在用户、商家和平台三方均取得显著性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Step-3：面向解码效率优化的超大规模视觉语言模型</title>
<link>https://arxiv.org/abs/2507.19427</link>
<guid>https://arxiv.org/abs/2507.19427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-3通过硬件感知设计显著提升长上下文推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Step-3，一个拥有321B参数的视觉语言模型，采用硬件感知的模型与系统协同设计，以最小化解码成本。该模型在两个关键方面进行创新：一是多矩阵分解注意力机制（MFA），有效降低KV缓存和计算量；二是注意力-前馈网络解耦（AFD），将注意力层与前馈网络分离为专用子系统。实验表明，Step-3在长上下文任务中相比DeepSeek-V3和Qwen3 MoE 235B等模型表现出更高的成本效率，并在Hopper GPU上实现了每秒4,039个token的解码吞吐量，创下新纪录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 12:53:13 GMT</pubDate>
</item>
<item>
<title>DreamScene：基于自然语言的高质量可编辑3D场景生成框架</title>
<link>https://arxiv.org/abs/2507.13985</link>
<guid>https://arxiv.org/abs/2507.13985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamScene实现从文本生成高质量、一致且可编辑的3D场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DreamScene，一个端到端的3D场景生成框架，能够根据自然语言或对话生成高质量、可编辑的3D场景。该框架首先通过GPT-4代理进行场景规划，构建混合图结构；随后利用基于图的布局算法生成无碰撞的结构化布局。接着，通过Formation Pattern Sampling（FPS）生成物体几何，并采用渐进式相机采样策略确保全局一致性。最后，系统支持对象移动、外观修改和4D动态运动等精细编辑功能。实验表明，DreamScene在质量、一致性和灵活性方面优于现有方法，为开放域3D内容创作提供实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 10:45:54 GMT</pubDate>
</item>
<item>
<title>MetaCLIP 2：在多语言网络数据上训练的对比语言-图像预训练模型</title>
<link>https://arxiv.org/abs/2507.22062</link>
<guid>https://arxiv.org/abs/2507.22062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaCLIP 2提升多语言图像文本任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MetaCLIP 2，这是首个从全球网络规模图像-文本对中从头训练的对比语言-图像预训练模型。针对非英语数据的筛选和多语言模型性能下降的问题，研究者通过最小改动进行严格消融实验，提出一种能够同时利用英语和非英语数据的训练方法。在零样本ImageNet分类任务中，MetaCLIP 2 ViT-H/14优于其仅限英语的版本和mSigLIP，且在多语言基准测试中取得了新的最先进结果，如CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600图像到文本检索达到64.3%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>MixGRPO：提升图像生成中人类偏好对齐效率的新框架</title>
<link>https://arxiv.org/abs/2507.21802</link>
<guid>https://arxiv.org/abs/2507.21802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MixGRPO通过混合采样策略提升图像生成模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MixGRPO，一种结合随机微分方程（SDE）和常微分方程（ODE）的新型框架，用于改进图像生成中的人类偏好对齐。该方法引入滑动窗口机制，仅在窗口内使用SDE采样和GRPO引导优化，减少优化开销并加速收敛。同时支持高阶求解器，进一步提升了训练效率。实验表明，MixGRPO在多个维度上优于现有方法，训练时间减少了近50%，而其变体MixGRPO-Flash更是将训练时间降低了71%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:40:09 GMT</pubDate>
</item>
<item>
<title>VL-Cogito：基于多阶段渐进式强化学习的多模态推理模型</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VL-Cogito通过PCuRL框架提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出VL-Cogito，一个基于多阶段渐进式强化学习（PCuRL）框架的多模态推理模型。该框架通过逐步增加任务难度，有效提升了模型在不同领域和复杂度下的推理能力。PCuRL引入了在线难度软加权机制和动态长度奖励机制，使模型能根据任务复杂度自适应调整推理路径长度，从而提高推理效率与准确性。实验结果表明，VL-Cogito在数学、科学、逻辑和通用理解等主流多模态基准测试中表现优异，验证了其方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 08:23:21 GMT</pubDate>
</item>
<item>
<title>基于强化学习的差分隐私优化框架RLDP提升语言模型性能</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLDP通过强化学习优化差分隐私，提升语言模型效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLDP框架，将差分隐私优化视为闭环控制问题，利用深度强化学习动态调整梯度裁剪阈值和噪声大小。该方法在多个语言模型上实现显著的困惑度降低和下游任务性能提升，同时保持隐私保护承诺，并减少训练所需的梯度更新次数。实验表明，RLDP在保证隐私的前提下提升了模型效果和训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 06:46:53 GMT</pubDate>
</item>
<item>
<title>提出OmniAVS数据集与OISA模型推动多模态指代音频视觉分割研究</title>
<link>https://arxiv.org/abs/2507.22886</link>
<guid>https://arxiv.org/abs/2507.22886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniAVS提升多模态音频视觉分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了OmniAVS数据集，包含2098个视频和59458条多模态指代表达，具有8种多模态表达形式、强调音频内容理解以及融入复杂推理与常识知识的特点。同时引入了OISA模型，利用多模态大语言模型进行细粒度音频视觉内容理解和推理分割。实验表明，OISA在OmniAVS上表现优于现有方法，并在其他相关任务中取得良好效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>基于测试前置的自动化程序修复方法 Repair-R1</title>
<link>https://arxiv.org/abs/2507.22853</link>
<guid>https://arxiv.org/abs/2507.22853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Repair-R1通过测试前置提升程序修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Repair-R1 的自动化程序修复方法，该方法将测试用例引入模型训练阶段，并在修复前生成区分性测试用例，以提高缺陷定位和修复效果。该方法采用强化学习联合优化测试生成与修复过程，在四个基准测试中表现出色，相比传统方法，修复成功率提升了2.68%至48.29%，测试覆盖率提升了0.78%至53.96%。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:24:05 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的UI到代码自动化转换方法</title>
<link>https://arxiv.org/abs/2507.22827</link>
<guid>https://arxiv.org/abs/2507.22827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多阶段UI-to-code框架，提升代码生成准确性和可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于多智能体框架的UI到前端代码自动化转换方法，该框架分为三个可解释的阶段：接地、规划和生成。接地代理通过视觉语言模型识别UI组件，规划代理利用前端工程先验构建层次布局，生成代理通过自适应提示合成HTML/CSS代码。该方法相比端到端黑盒方法更具鲁棒性、可解释性和准确性。此外，研究还扩展了该框架为可扩展的数据引擎，自动生成大规模图像-代码对，并用于微调开源视觉语言模型，显著提升了UI理解与代码质量。实验表明，该方法在布局准确性、结构连贯性和代码正确性方面均达到最新水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 12:41:21 GMT</pubDate>
</item>
<item>
<title>Falcon-H1：混合架构大语言模型系列提升性能与效率</title>
<link>https://arxiv.org/abs/2507.22448</link>
<guid>https://arxiv.org/abs/2507.22448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Falcon-H1采用混合架构，性能超越多款大模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Falcon-H1系列大语言模型，该模型采用Transformer与状态空间模型（SSM）的混合架构，优化了性能与效率。Falcon-H1提供了多种参数规模的版本，包括0.5B、1.5B、3B、7B和34B等，并支持量化版本。其性能在多个任务中表现优异，甚至在较小参数规模下超越了更大模型。Falcon-H1支持多语言和长上下文处理，适用于广泛的应用场景。所有模型均以开源方式发布，推动AI研究的开放与共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 03:55:33 GMT</pubDate>
</item>
<item>
<title>BANG：一种基于生成爆炸动态的3D对象分解方法</title>
<link>https://arxiv.org/abs/2507.21493</link>
<guid>https://arxiv.org/abs/2507.21493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BANG实现3D对象的直观分解与生成，提升3D设计效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出BANG，一种新型的生成式方法，用于3D对象的部件级分解。其核心是“生成爆炸动态”，通过平滑的爆炸状态序列逐步分离物体部件，同时保持几何和语义一致性。BANG利用预训练的扩散模型，并结合轻量级爆炸视图适配器和时间注意力模块，实现精确控制和流畅过渡。用户可通过空间提示（如边界框和表面区域）指定分解内容，还可结合多模态模型如GPT-4进行2D到3D操作。BANG支持详细部件生成、功能描述关联及组件感知的3D制造流程，适用于3D打印等领域，使创意概念更易转化为详细3D资产。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:21:21 GMT</pubDate>
</item>
<item>
<title>基于生成AI的航空图像车辆检测域适应方法</title>
<link>https://arxiv.org/abs/2507.20976</link>
<guid>https://arxiv.org/abs/2507.20976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用生成AI合成数据提升航空图像车辆检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于生成AI的新型方法，用于改善航空图像中车辆检测模型在不同地理区域间的泛化能力。通过使用微调的潜在扩散模型（LDMs）生成高质量的合成图像及其标注，实现多阶段、多模态的知识迁移，从而缩小源域与目标域之间的分布差异。实验表明，该方法在多个航空图像数据集上显著优于监督学习、弱监督适应、无监督域适应和开放集检测方法。此外，研究者还发布了两个新的新西兰和犹他州的航空图像数据集，以支持相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:38:06 GMT</pubDate>
</item>
<item>
<title>ChemDFM-R：提升化学领域推理能力的大型语言模型</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChemDFM-R在化学领域实现最先进的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对化学领域中大型语言模型理解深度和推理能力不足的问题，提出ChemDFM-R模型。该模型通过构建原子化知识数据集增强化学基础理解，并采用混合来源的知识蒸馏与领域特定强化学习策略，显著提升了化学推理能力。实验表明，ChemDFM-R在多个化学基准测试中表现优异，且输出具有可解释性和逻辑性，增强了人机协作中的可靠性与实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 12:40:49 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型诚实行为的系统评估与基准测试</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大模型在无法回答视觉问题时的诚实行为。</p><br /><br /><p><strong>摘要：</strong> 本文首次对多模态大语言模型（MLLMs）的诚实行为进行了系统评估。研究基于模型对无法回答的视觉问题的响应行为，定义了四种代表性类型的问题，并构建了一个包含12000多个样本的多模态诚实基准（MoHoBench），通过多阶段过滤和人工验证确保数据质量。实验发现，大多数模型在面对无法回答的问题时未能正确拒绝回答，且模型的诚实行为不仅受语言建模影响，还与视觉信息密切相关。为此，研究者尝试使用监督学习和偏好学习方法进行初步对齐，以提升模型的诚实表现，为未来构建可信的多模态大语言模型奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21503" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:55:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的CUDA自动优化框架CUDA-L1</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CUDA-L1通过强化学习实现高效CUDA优化，提升多GPU架构性能。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型对GPU计算资源需求的激增，传统CUDA优化方法已难以满足需求。本文提出CUDA-L1，一个基于强化学习的自动化CUDA优化框架。该框架在NVIDIA A100上训练，平均提升了KernelBench中250个CUDA内核的性能达17.7倍，最高可达449倍。CUDA-L1不仅在多种GPU架构（如H100、RTX 3090等）上表现出色，还能发现多种优化技术并进行策略性组合，揭示CUDA优化的基本原则，并识别潜在性能瓶颈。该方法无需人工干预，仅通过速度奖励信号即可将低效的LLM转化为高效的CUDA优化器，具有广泛的适用性和推广价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:43:56 GMT</pubDate>
</item>
<item>
<title>基于运动引导的少样本视频目标分割研究</title>
<link>https://arxiv.org/abs/2507.22061</link>
<guid>https://arxiv.org/abs/2507.22061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MOVE数据集与DMA方法提升运动引导的视频目标分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对运动引导的少样本视频目标分割（FSVOS）问题，提出一个大规模的MOVE数据集，以弥补现有数据集在动态运动理解上的不足。通过评估多种先进方法，发现当前技术在处理运动引导任务时存在明显不足。为此，作者提出一种新的基线方法DMA，实验表明该方法在少样本运动理解任务中表现优异，为后续研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>基于强化学习的离散自回归模型在图像与语言生成中的应用</title>
<link>https://arxiv.org/abs/2507.22058</link>
<guid>https://arxiv.org/abs/2507.22058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升离散自回归模型的图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Omni的框架，结合了语义图像分词器、统一的自回归模型以及离线扩散解码器，利用强化学习有效减少了图像生成中的伪影，显著提升了生成质量。该方法在使用7B语言模型时表现出色，在图像生成任务中达到了最先进的性能，能够高质量地生成图像，并具备良好的指令遵循能力和长文本渲染能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>HunyuanWorld 1.0：融合文本与图像生成沉浸式3D场景的新框架</title>
<link>https://arxiv.org/abs/2507.21809</link>
<guid>https://arxiv.org/abs/2507.21809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HunyuanWorld 1.0实现高质量3D场景生成，支持交互与兼容现有图形管线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HunyuanWorld 1.0，一种结合视频与3D方法优势的新框架，用于从文本和图像生成沉浸式、可探索和交互的3D场景。该框架具有三个核心优势：通过全景世界代理实现360度沉浸体验、支持网格导出以兼容现有图形管线、以及分离的对象表示增强交互性。其核心是语义分层的3D网格表示，利用全景图像作为语义感知的世界分解和重建工具，从而生成多样化的3D世界。实验表明，该方法在生成连贯、可探索和交互的3D场景方面表现优异，并适用于虚拟现实、物理模拟、游戏开发和互动内容创作等多个领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:43:35 GMT</pubDate>
</item>
<item>
<title>深度学习在非洲野生动物图像分类中的应用与评估</title>
<link>https://arxiv.org/abs/2507.21364</link>
<guid>https://arxiv.org/abs/2507.21364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较了多种深度学习模型在非洲野生动物分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了深度学习技术在非洲野生动物图像分类中的应用，重点评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14等模型的性能。实验结果显示，DenseNet-201在卷积网络中表现最佳，准确率为67%，而ViT-H/14虽然准确率高达99%，但计算成本较高。研究强调了准确率、资源消耗和部署可行性之间的权衡，并将DenseNet-201集成到Hugging Face Gradio Space中，展示了轻量级模型在野外应用的可行性。该工作为非洲本土的AI研究提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 18:18:13 GMT</pubDate>
</item>
<item>
<title>AnimalClue：首个基于间接证据的物种识别大规模数据集</title>
<link>https://arxiv.org/abs/2507.20240</link>
<guid>https://arxiv.org/abs/2507.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimalClue是首个用于从间接证据中识别物种的大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnimalClue，这是首个专注于通过动物足迹、粪便、卵、骨骼和羽毛等间接证据进行物种识别的大规模数据集。该数据集包含159,605个边界框，涵盖5类间接证据，覆盖968个物种、200个科和65个目。每张图像均带有物种级标签、边界框或分割掩码，以及详细的特征信息。与以往主要关注直接视觉特征的数据集不同，AnimalClue在分类、检测和实例分割任务中面临更复杂的挑战，因其需要识别更细微的视觉特征。研究团队对多个视觉模型进行了评估，并指出了从痕迹中识别动物的关键挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 07:48:03 GMT</pubDate>
</item>
<item>
<title>基于最大后验估计的偏好优化方法MaPPO</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaPPO提升语言模型与人类偏好的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Maximum a Posteriori Preference Optimization (MaPPO)的框架，用于从偏好中学习，该方法通过将先验奖励知识整合到优化目标中，改进了传统偏好优化方法。相比DPO及其变体，MaPPO不仅扩展了这一范式，还通过避免过于简化的二分类响应方式提升了对齐效果。此外，MaPPO无需额外超参数，支持离线和在线设置，并可作为插件与SimPO、IPO、CPO等方法结合使用。实验表明，在多个基准测试中，MaPPO在保持计算效率的同时显著提升了对齐性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 01:26:50 GMT</pubDate>
</item>
<item>
<title>基于用户目标状态追踪的对话模拟器研究</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGST框架提升对话模拟器的目标对齐能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对对话人工智能中用户模拟器在多轮对话中难以保持目标导向行为的问题，提出了用户目标状态追踪（UGST）框架。该框架通过跟踪用户目标进展，结合三阶段方法开发能够自主追踪目标并生成符合目标响应的用户模拟器。研究还建立了全面的评估指标，并在两个基准数据集上验证了方法的有效性，显著提升了模拟器的表现。该成果填补了对话AI领域的关键空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 03:07:12 GMT</pubDate>
</item>
<item>
<title>SAND-Math：提升数学推理大语言模型性能的合成数据生成方法</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAND-Math提升数学推理LLM性能，显著优于现有数据集。</p><br /><br /><p><strong>摘要：</strong> 随着对具备复杂数学推理能力的大语言模型（LLMs）需求增加，训练数据的稀缺成为发展瓶颈。本文提出SAND-Math，一种通过生成高质量数学问题并逐步提升难度的合成数据生成管道。实验表明，使用SAND-Math数据可使模型在AIME25基准测试中性能提升17.85个百分点，并通过难度提升步骤将平均问题难度从5.02提高到5.98，进一步提升模型表现。该方法为构建更强大、高效的数学推理LLM提供了实用且可扩展的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 01:17:48 GMT</pubDate>
</item>
<item>
<title>提升主观推理能力的多角色增强框架研究</title>
<link>https://arxiv.org/abs/2507.20187</link>
<guid>https://arxiv.org/abs/2507.20187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多角色框架提升主观推理准确性和多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出MultiRole-R1，一个通过引入多角色视角来增强大型推理模型在主观任务中表现的框架。该框架采用无监督数据生成方法，结合多样化的角色推理链，并利用强化学习中的组相对策略优化技术，将多样性作为奖励信号，以提高推理的准确性和多样性。实验表明，该方法在多个基准测试中均表现出色，展示了多样性增强训练在大型推理模型中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 05:07:42 GMT</pubDate>
</item>
<item>
<title>自演化智能体：从静态模型到动态适应的范式转变</title>
<link>https://arxiv.org/abs/2507.21046</link>
<guid>https://arxiv.org/abs/2507.21046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨自演化智能体的发展与应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了自演化智能体的研究进展，聚焦于其在任务、时机和方法上的演化机制。文章分析了智能体组件（如模型、记忆、工具）的进化方式，分类讨论了不同阶段的适应方法，并探讨了算法与架构设计对演化的影响。此外，文章还评估了相关指标与基准，展示了在编程、教育和医疗等领域的应用，并指出了安全性、可扩展性和协同演化等关键挑战。该研究为构建自适应智能系统提供了框架和方向，助力实现人工智能超级智能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>GenoMAS：结合LLM的基因表达分析新方法</title>
<link>https://arxiv.org/abs/2507.21035</link>
<guid>https://arxiv.org/abs/2507.21035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenoMAS提升基因表达分析效率与准确性。</p><br /><br /><p><strong>摘要：</strong> GenoMAS是一种基于大型语言模型（LLM）的基因表达分析系统，通过集成多个专业化LLM代理，结合结构化工作流与自主适应能力，有效处理复杂的转录组数据。该系统在GenoTEX基准测试中表现出色，数据预处理的综合相似性相关系数达到89.13%，基因识别的F_1值为60.48%，优于现有方法。此外，GenoMAS能发现具有生物学意义的基因-表型关联，并调整潜在混杂因素，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>基于超长输出强化学习的大型语言模型推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UloRL提升LLM推理能力，显著提高训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UloRL的超长输出强化学习方法，旨在提升大型语言模型的推理能力。针对传统强化学习框架在处理超长输出时的低效问题，UloRL通过将输出解码分为短段来提高训练效率，并引入动态掩码机制防止熵崩溃。实验结果表明，该方法在Qwen3-30B-A3B模型上提升了2.06倍的训练速度，并在AIME2025和BeyondAIME任务中分别提高了14.2%和11.2%的性能，效果优于更大型的Qwen3-235B-A22B模型。研究展示了UloRL在超长序列生成中的潜力，并计划开源代码和模型供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 23:42:33 GMT</pubDate>
</item>
<item>
<title>ScenePainter：解决3D场景生成语义漂移问题的新框架</title>
<link>https://arxiv.org/abs/2507.19058</link>
<guid>https://arxiv.org/abs/2507.19058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScenePainter提升3D场景生成的语义一致性与连贯性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScenePainter，一种用于长期且语义一致的3D场景生成框架。现有方法在生成连续视角序列时存在语义漂移问题，主要由于出图模块的累积偏差。ScenePainter通过引入层次化场景概念图（SceneConceptGraph）来构建多层级场景概念之间的关系，指导出图模块生成一致的新视角，并可动态优化以增强多样性。实验表明，该框架有效解决了语义漂移问题，提升了3D视图序列的一致性和沉浸感。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 04:21:12 GMT</pubDate>
</item>
<item>
<title>基于隐式两阶段训练的多变量天气预测方法</title>
<link>https://arxiv.org/abs/2507.17189</link>
<guid>https://arxiv.org/abs/2507.17189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升天气预测准确性。</p><br /><br /><p><strong>摘要：</strong> 文章针对全球气候变化导致极端天气频发的问题，提出一种基于隐式两阶段训练的多变量天气预测方法。该方法通过为每个变量配置独立的编码器和解码器，在第一阶段学习共享潜在空间，第二阶段由翻译器捕捉变量间交互关系，并引入自注意力机制进行多变量融合，显著提升了预测性能。实验表明，该方法在近地面气温和相对湿度预测上分别降低了28.82%和23.39%的均方误差。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:26:56 GMT</pubDate>
</item>
<item>
<title>基于校准奖励的强化学习提升语言模型推理可靠性</title>
<link>https://arxiv.org/abs/2507.16806</link>
<guid>https://arxiv.org/abs/2507.16806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLCR方法提升语言模型推理准确性与置信度校准。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLCR（Reinforcement Learning with Calibration Rewards）方法，通过在强化学习中引入Brier评分来优化语言模型的置信度估计，从而提升模型的准确性和置信度校准。该方法在生成预测的同时输出数值化置信度，并利用结合二进制正确性评分和Brier评分的奖励函数进行训练。实验表明，RLCR在多个数据集上显著提升了置信度校准效果，且不影响准确性，优于传统强化学习和事后置信度分类器。此外，测试时可通过置信度加权方法进一步提高准确性和校准度。结果表明，显式优化置信度有助于构建更可靠的推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于表示空间的多任务学习方法Rep-MTL</title>
<link>https://arxiv.org/abs/2507.21049</link>
<guid>https://arxiv.org/abs/2507.21049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rep-MTL通过表示空间优化提升多任务学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的多任务学习方法Rep-MTL，该方法通过分析任务间的表示空间交互，利用熵惩罚和样本级跨任务对齐来优化任务间的信息共享。与传统依赖损失缩放和梯度调整的方法不同，Rep-MTL关注任务特定优化与共享表示学习之间的相互作用，旨在减少负迁移并增强互补信息的传递。实验表明，即使在简单的等权重策略下，Rep-MTL也能在多个多任务学习基准上取得良好的性能，且具有较高的效率。此外，幂律指数分析进一步验证了其在平衡任务学习与跨任务共享方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:28 GMT</pubDate>
</item>
<item>
<title>4D空间智能重建的多层级方法综述</title>
<link>https://arxiv.org/abs/2507.21045</link>
<guid>https://arxiv.org/abs/2507.21045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出5层4D空间智能重建框架，分析各阶段挑战与发展方向。</p><br /><br /><p><strong>摘要：</strong> 本文综述了从视觉观测中重建4D空间智能的研究进展，提出了一个五层结构的方法体系，涵盖低级3D属性、场景组件、动态场景、交互建模以及物理约束的重建。文章分析了每一层级的关键挑战，并指出了未来研究方向。作者还维护了一个项目页面以跟踪该领域的最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GPT-IMAGE-EDIT-1.5M：推动指令引导图像编辑的开源数据集</title>
<link>https://arxiv.org/abs/2507.21033</link>
<guid>https://arxiv.org/abs/2507.21033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开源图像编辑数据集提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GPT-IMAGE-EDIT-1.5M，一个包含150万组高质量指令-源图-编辑图的公开图像编辑数据集。该数据集通过GPT-4o生成并优化了三个主流图像编辑数据集，提升了视觉质量和指令对齐度。实验表明，基于该数据集微调的模型在多个基准测试中表现优异，显著缩小了与专有模型的差距，为开放研究提供了重要支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>SmallThinker：本地设备上高效运行的大语言模型</title>
<link>https://arxiv.org/abs/2507.20984</link>
<guid>https://arxiv.org/abs/2507.20984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SmallThinker在本地设备上实现高性能大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SmallThinker，一种专为本地设备设计的大型语言模型家族。与传统依赖GPU云基础设施的模型不同，SmallThinker从零开始构建，以适应计算能力弱、内存有限和存储速度慢的限制。其创新点包括两层稀疏结构、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，显著提升了计算效率和内存利用率。SmallThinker-4B-A0.6B和SmallThinker-21B-A3B在性能上达到或超过更大的模型，并且在普通CPU上即可高效运行，无需昂贵的GPU硬件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:45:14 GMT</pubDate>
</item>
<item>
<title>ARC-Hunyuan-Video：提升短视频多模态理解能力的模型</title>
<link>https://arxiv.org/abs/2507.20939</link>
<guid>https://arxiv.org/abs/2507.20939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARC-Hunyuan-Video提升短视频多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ARC-Hunyuan-Video，一个能够处理视频中视觉、音频和文本信息的多模态模型，旨在解决真实世界短视频理解中的挑战。该模型支持多粒度时间戳视频摘要、开放式视频问答、时间视频定位和视频推理等功能。通过高质量数据和多种训练策略，模型在多个任务上表现出色，并在实际部署中提升了用户参与度和满意度。其高效性也得到验证，一分钟后视频的推理时间仅需10秒。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 11:52:36 GMT</pubDate>
</item>
<item>
<title>Music Arena：开放平台推动文本到音乐模型的人类偏好评估</title>
<link>https://arxiv.org/abs/2507.20900</link>
<guid>https://arxiv.org/abs/2507.20900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Music Arena提供实时人类偏好评估，提升文本到音乐模型的评价标准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Music Arena，一个用于文本到音乐（TTM）模型大规模人类偏好评估的开放平台。通过让用户输入文本提示并比较不同系统的输出，平台收集真实用户的偏好数据，形成排行榜。Music Arena不仅遵循其他AI领域的评估趋势，还特别设计了适合音乐的特性，如基于LLM的路由系统和详细偏好数据收集。此外，平台采用滚动数据发布政策，确保用户隐私和数据透明性。该平台旨在解决TTM领域缺乏统一评估标准的问题，并展示如何将实时评估适配到特定AI领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:52:57 GMT</pubDate>
</item>
<item>
<title>基于流匹配的JAM模型实现歌词到歌曲的精细控制</title>
<link>https://arxiv.org/abs/2507.20880</link>
<guid>https://arxiv.org/abs/2507.20880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM模型实现歌词到歌曲的词级时序控制，提升音乐生成质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种基于流匹配的歌词到歌曲生成模型JAM，该模型首次实现了对歌词中每个词的时序和持续时间的精细控制，满足音乐创作中的需求。为了提升生成歌曲的质量，研究者采用了直接偏好优化方法进行美学对齐，无需人工标注即可迭代优化模型。此外，作者还构建了公开评估数据集JAME，以标准化此类模型的评价体系。实验结果显示，JAM在音乐特定属性上优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:34:02 GMT</pubDate>
</item>
<item>
<title>GMPO：一种更稳定的大型语言模型策略优化方法</title>
<link>https://arxiv.org/abs/2507.20673</link>
<guid>https://arxiv.org/abs/2507.20673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GMPO通过几何均值优化提升模型稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何均值策略优化（GMPO）的方法，作为Group Relative Policy Optimization (GRPO) 的改进版本。GMPO通过最大化token级奖励的几何均值，减少了对异常值的敏感性，从而提升了策略更新的稳定性。实验表明，GMPO在多个数学和多模态推理基准测试中表现优于GRPO，如AIME24、AMC、MATH500等，平均性能提升4.1%。研究还提供了理论分析和实验验证，证明了GMPO的设计优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 05:54:05 GMT</pubDate>
</item>
<item>
<title>RICE：提升区域级视觉与OCR能力的对比学习方法</title>
<link>https://arxiv.org/abs/2507.20025</link>
<guid>https://arxiv.org/abs/2507.20025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RICE提升区域级视觉和OCR能力，适用于多种密集预测任务。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RICE的新方法，旨在增强区域级别的视觉和OCR能力。通过构建大规模候选区域数据集，并引入区域Transformer层来提取丰富的区域语义，RICE设计了一个统一的区域聚类判别损失，支持对象和OCR学习。该方法在分割、密集检测和多模态大语言模型的视觉感知任务中表现出色，且已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 13:47:09 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习算法ARPO提升多轮语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARPO算法提升LLM在多轮工具交互中的推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Agentic Reinforced Policy Optimization (ARPO)的新型强化学习算法，旨在提升大语言模型（LLM）在多轮工具交互任务中的表现。研究发现，LLM在使用外部工具后会产生更高的生成不确定性，因此ARPO引入了基于熵的自适应采样机制，动态平衡全局轨迹采样与步骤级采样，以增强高不确定性步骤的探索能力。同时，通过优势归因估计，使LLM能够更好地理解多轮工具使用中的策略差异。实验表明，ARPO在多个计算推理、知识推理和深度搜索任务中优于现有轨迹级强化学习算法，并且仅需一半的工具使用预算即可实现更优性能，为LLM代理在实时动态环境中的对齐提供了可扩展解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 03:53:11 GMT</pubDate>
</item>
<item>
<title>基于前景感知的文档图像校正方法研究</title>
<link>https://arxiv.org/abs/2507.19804</link>
<guid>https://arxiv.org/abs/2507.19804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ForCenNet模型提升文档图像几何校正效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Foreground-Centric Network (ForCenNet) 的新方法，用于消除文档图像中的几何变形，以提高文本识别的准确性。该方法通过提取前景元素作为几何参考，并引入前景感知的标签生成、掩码机制和曲率一致性损失，有效提升了模型对布局元素如文本行和表格边框的校正能力。实验表明，ForCenNet在多个真实世界基准数据集上取得了最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 01:36:48 GMT</pubDate>
</item>
<item>
<title>GEPA：利用自然语言反思提升LLM任务优化的提示优化器</title>
<link>https://arxiv.org/abs/2507.19457</link>
<guid>https://arxiv.org/abs/2507.19457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEPA通过自然语言反思实现高效任务优化，显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出GEPA（Genetic-Pareto）提示优化器，利用自然语言反思来提升大型语言模型（LLM）的任务适应能力。与传统基于稀疏奖励的强化学习方法相比，GEPA通过分析系统级轨迹并进行自然语言诊断，能够高效地生成和测试提示更新。实验表明，GEPA在四个任务中平均比GRPO提升10%，最高达20%，且使用 rollouts 数量减少35倍。同时，GEPA在两个LLM上优于MIPROv2，展现出在代码优化中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:42:32 GMT</pubDate>
</item>
<item>
<title>前沿人工智能模型的风险评估与管理</title>
<link>https://arxiv.org/abs/2507.16534</link>
<guid>https://arxiv.org/abs/2507.16534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">前沿AI模型风险评估揭示关键风险领域。</p><br /><br /><p><strong>摘要：</strong> 本文基于Frontier AI Risk Management Framework，通过E-T-C分析方法评估了前沿人工智能模型的七类关键风险，包括网络攻击、生物化学风险、说服与操控、自主AI研发、战略欺骗、自我复制和合谋。通过AI-45°法则设定红黄线阈值，将风险划分为绿色（可管理）、黄色（需加强控制）和红色（需暂停开发）。实验结果显示，当前所有前沿AI模型均处于绿色和黄色区域，未触及红色警戒线。其中，多数模型在自我复制和战略欺骗方面仍处于绿色区域，而说服与操控风险则普遍处于黄色区域。文章呼吁共同应对AI发展带来的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:44:38 GMT</pubDate>
</item>
<item>
<title>MMBench-GUI：跨平台GUI自动化代理评估基准</title>
<link>https://arxiv.org/abs/2507.19478</link>
<guid>https://arxiv.org/abs/2507.19478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMBench-GUI评估GUI自动化代理的多平台能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMBench-GUI，这是一个用于评估跨Windows、macOS、Linux、iOS、Android和Web平台的GUI自动化代理的分层基准。该基准包含四个层次：GUI内容理解、元素定位、任务自动化和任务协作，涵盖了GUI代理的核心技能。文章还提出了一种新的效率-质量面积（EQA）指标，用于评估在线自动化场景中的执行效率。研究发现，准确的视觉定位是任务成功的关键，模块化框架结合专用定位模块具有显著优势。此外，可靠的GUI自动化需要强大的任务规划和跨平台泛化能力，长上下文记忆、广泛的操作空间和长期推理至关重要。任务效率仍是一个被忽视的维度，所有模型都存在显著低效问题。精确定位、有效规划和早期停止策略的整合对实现高效可扩展的GUI自动化不可或缺。相关代码、数据和环境将公开在GitHub上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>GPTQ与格点算法的数学等价性研究</title>
<link>https://arxiv.org/abs/2507.18553</link>
<guid>https://arxiv.org/abs/2507.18553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPTQ被证明与格点最近平面算法等价，具有理论保障。</p><br /><br /><p><strong>摘要：</strong> 本文揭示了GPTQ在反向执行时与Babai最近平面算法在数学上的等价性。这种等价性基于线性层输入的Hessian矩阵定义的格点问题，为GPTQ提供了几何解释和误差上界保证。这一发现不仅深化了对GPTQ的理解，还为未来大规模语言模型的量化算法设计提供了理论基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:22:18 GMT</pubDate>
</item>
<item>
<title>基于LLM的错误分析工具CLEAR提升模型评估深度</title>
<link>https://arxiv.org/abs/2507.18392</link>
<guid>https://arxiv.org/abs/2507.18392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEAR提供交互式错误分析，揭示模型性能背后的具体原因。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型的评估多依赖其他模型进行打分或排序，但这种方式仅能判断哪个模型更好，无法解释原因。为解决这一问题，研究者提出了CLEAR，一个开源的LLM错误分析工具。CLEAR能够生成逐实例的文本反馈，并识别系统级别的错误问题，同时量化各类问题的出现频率。该工具还提供了交互式仪表盘，支持通过可视化图表、过滤器和实例分析进行深入错误探究。研究展示了CLEAR在RAG和数学基准测试中的应用，并通过用户案例验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:15:21 GMT</pubDate>
</item>
<item>
<title>Specification Self-Correction: 提升语言模型对规范漏洞的自我修正能力</title>
<link>https://arxiv.org/abs/2507.18742</link>
<guid>https://arxiv.org/abs/2507.18742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SSC框架帮助语言模型在推理过程中自我修正规范漏洞，提升任务准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Specification Self-Correction (SSC) 的新框架，使语言模型能够在推理过程中识别并修正自身所依据的规范中的缺陷。该方法通过多步骤推理：首先基于可能有误的规范生成响应，然后对该响应进行批判性分析，并最终修改规范以消除可被利用的漏洞。实验表明，使用 SSC 后，模型在创意写作和代理编码任务中因规范漏洞而偏离用户意图的情况减少了超过 90%。该方法无需修改模型权重，仅在推理阶段完成动态修复，显著提升了模型行为的鲁棒性和对齐度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 14:44:28 GMT</pubDate>
</item>
<item>
<title>基于纯视觉的高效端到端自动驾驶架构PRIX</title>
<link>https://arxiv.org/abs/2507.17596</link>
<guid>https://arxiv.org/abs/2507.17596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRIX通过纯视觉实现高效自动驾驶，无需LiDAR和BEV表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PRIX的端到端自动驾驶架构，仅依赖摄像头数据，无需昂贵的LiDAR传感器和复杂的BEV特征表示。该架构采用视觉特征提取器和生成式规划头，直接从原始像素预测安全轨迹。核心组件Context-aware Recalibration Transformer (CaRT) 提升了多级视觉特征的鲁棒性。实验表明，PRIX在NavSim和nuScenes基准测试中表现优异，性能接近大型多模态扩散规划器，但推理速度更快、模型更小，适合大规模部署。项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 11:28:23 GMT</pubDate>
</item>
<item>
<title>TTD-DR：基于扩散过程的深度研究生成框架</title>
<link>https://arxiv.org/abs/2507.16075</link>
<guid>https://arxiv.org/abs/2507.16075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTD-DR通过迭代优化提升长文本研究报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Test-Time Diffusion Deep Researcher (TTD-DR) 的新框架，旨在解决大型语言模型在生成复杂、长篇研究报告时性能受限的问题。该框架模仿人类研究的迭代过程，将报告生成视为一个扩散过程，从初步草稿出发，通过不断‘去噪’和引入外部信息进行逐步优化。同时，其核心流程结合了自进化算法，以提高生成内容的质量与连贯性。实验表明，TTD-DR在多个需要深度搜索和多跳推理的任务中表现优异，超越了现有研究代理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:23:21 GMT</pubDate>
</item>
<item>
<title>AI视频聊天：实时通信的新范式与优化框架</title>
<link>https://arxiv.org/abs/2507.10510</link>
<guid>https://arxiv.org/abs/2507.10510</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI视频聊天面临延迟挑战，Artic框架提升实时性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了AI视频聊天作为实时通信新范式的潜力，指出由于多模态大语言模型（MLLM）推理耗时长，导致视频传输延迟成为瓶颈。为此，研究提出了Artic框架，通过上下文感知视频流和抗丢包自适应帧率技术，减少带宽消耗并提升AI理解能力。同时，构建了首个Degraded Video Understanding Benchmark（DeViBench）评估视频质量对MLLM的影响，并讨论了未来的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10510" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:34:49 GMT</pubDate>
</item>
<item>
<title>基于双空间建模的局部相关视频检索方法</title>
<link>https://arxiv.org/abs/2507.17402</link>
<guid>https://arxiv.org/abs/2507.17402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HLFormer框架提升视频与文本的局部相关性匹配。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频与文本查询之间部分内容匹配的问题，提出了一种基于双空间建模的局部相关视频检索方法HLFormer。该方法利用双曲空间学习弥补欧几里得空间在层次结构建模上的不足，通过集成洛伦兹注意力模块和欧几里得注意力模块，结合动态特征融合机制，增强了跨模态匹配效果。同时引入部分序保留损失函数，强化文本与视频内容之间的局部相关性。实验表明，该方法优于现有最佳模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 06:59:46 GMT</pubDate>
</item>
<item>
<title>提升多模态上下文学习能力的动态注意力重分配方法</title>
<link>https://arxiv.org/abs/2507.15807</link>
<guid>https://arxiv.org/abs/2507.15807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出DARA方法提升多模态模型的上下文学习能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了当前多模态大语言模型在多模态上下文学习（MICL）中的局限性，指出其过度依赖文本模式而忽视视觉信息。为解决这一问题，作者提出了动态注意力重分配（DARA）策略，以增强模型对视觉信息的关注。同时，构建了TrueMICL数据集，专门用于评估多模态任务中的真实上下文学习能力。实验表明，该方法显著提升了模型的多模态适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:08:18 GMT</pubDate>
</item>
<item>
<title>Iwin Transformer：一种无需位置嵌入的层次化视觉Transformer</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Iwin Transformer通过创新机制实现高效图像处理与任务表现。</p><br /><br /><p><strong>摘要：</strong> Iwin Transformer是一种无需位置嵌入的层次化视觉Transformer，能够从低分辨率直接微调到高分辨率。其核心在于结合交错窗口注意力和深度可分离卷积，使全局信息在单一模块内交换，克服了Swin Transformer需要两个连续块才能近似全局注意力的限制。实验表明，Iwin Transformer在图像分类、语义分割和视频动作识别等任务中表现出色，尤其在ImageNet-1K上达到87.4%的top-1准确率。此外，其核心组件可作为独立模块用于条件图像生成，并为未来研究如Iwin 3D Attention提供启发。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:45:48 GMT</pubDate>
</item>
<item>
<title>Group Sequence Policy Optimization: 提升大语言模型训练效率与稳定性的强化学习算法</title>
<link>https://arxiv.org/abs/2507.18071</link>
<guid>https://arxiv.org/abs/2507.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSPO提升大语言模型训练效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Group Sequence Policy Optimization (GSPO)，一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型。GSPO不同于以往基于token级重要性比率的算法，而是基于序列似然定义重要性比率，并进行序列级裁剪、奖励和优化。实验表明，GSPO在训练效率和性能上优于GRPO算法，尤其在稳定Mixture-of-Experts（MoE）强化学习训练方面表现突出，同时简化了强化学习基础设施的设计。GSPO的优势显著提升了最新Qwen3模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 23:50:32 GMT</pubDate>
</item>
<item>
<title>Agentar-Fin-R1系列金融大模型提升推理与可信度</title>
<link>https://arxiv.org/abs/2507.16802</link>
<guid>https://arxiv.org/abs/2507.16802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agentar-Fin-R1提升金融场景下的推理能力与可信度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了基于Qwen3的Agentar-Fin-R1系列金融大语言模型（8B和32B参数），旨在增强金融应用中的推理能力、可靠性和领域专业化。通过高质量的金融任务标签系统和多层可信保障框架，结合自动化难度感知优化、两阶段训练流程和动态归属系统，显著提升了训练效率。模型在Fineva、FinEval、FinanceIQ等主流金融基准以及MATH-500、GPQA-diamond等通用推理数据集上表现优异，并提出了新的Finova评估基准以测试实际部署能力。实验结果表明，Agentar-Fin-R1在金融任务和通用推理方面均表现出色，具备高可信度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>NABLA：提升视频生成效率的自适应块级注意力机制</title>
<link>https://arxiv.org/abs/2507.13546</link>
<guid>https://arxiv.org/abs/2507.13546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NABLA通过动态自适应块级注意力提升视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出NABLA，一种基于Transformer的视频生成注意力机制，能够动态适应视频中的稀疏模式，从而降低计算复杂度。该方法无需定制低级操作，可与PyTorch的Flex Attention无缝集成。实验表明，NABLA在几乎不损失生成质量的前提下，使训练和推理速度提升了2.7倍。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:36:36 GMT</pubDate>
</item>
<item>
<title>基于深度学习的面部年龄与性别联合分类方法研究</title>
<link>https://arxiv.org/abs/2507.18565</link>
<guid>https://arxiv.org/abs/2507.18565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种联合年龄与性别分类的深度学习模型，提升广告精准度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于深度学习的面部图像年龄与性别联合分类方法，旨在提高目标广告投放的效果。该方法采用定制化的卷积神经网络（CNN）架构，通过共享特征表示同时处理两个任务，优于传统独立处理的方式。模型在大规模多样化数据集上进行训练，并经过预处理以增强对光照、姿态和图像质量变化的鲁棒性。实验结果显示，性别分类准确率达到95%，年龄估计的平均绝对误差为5.77年。研究还分析了不同年龄段的表现差异，指出年轻群体的年龄估计存在挑战，并建议通过数据增强和模型优化来减少偏差。此外，还探讨了不同CNN结构和超参数设置对性能的影响，为未来研究提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:41:26 GMT</pubDate>
</item>
<item>
<title>GLiNER2：统一的高效信息抽取框架</title>
<link>https://arxiv.org/abs/2507.18546</link>
<guid>https://arxiv.org/abs/2507.18546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLiNER2实现多任务信息抽取，提升部署效率。</p><br /><br /><p><strong>摘要：</strong> GLiNER2是一种统一的信息抽取框架，能够在单一模型中支持命名实体识别、文本分类和结构化数据提取。该框架基于预训练的Transformer架构，在保持CPU效率和模型紧凑性的同时，通过直观的模式接口实现多任务组合。实验表明，GLiNER2在抽取和分类任务中表现出色，并在部署便捷性上优于基于大语言模型的方案。项目已开源，提供预训练模型和文档。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:11:14 GMT</pubDate>
</item>
<item>
<title>DriftMoE：一种应对概念漂移的在线专家混合模型</title>
<link>https://arxiv.org/abs/2507.18464</link>
<guid>https://arxiv.org/abs/2507.18464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DriftMoE通过协同训练提升在线学习中的概念漂移适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DriftMoE，一种基于在线专家混合（MoE）架构的模型，用于处理非平稳数据流中的概念漂移问题。该模型通过一个紧凑的神经路由器与增量Hoeffding树专家池协同训练，形成共生学习循环。路由器根据预测选择最合适的专家，专家在获得真实标签后进行增量更新，同时路由器利用多热正确性掩码优化参数，从而加速专家专业化。实验表明，DriftMoE在多个数据流学习基准中表现优异，提供了一种高效且系统的方法来应对概念漂移。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 10:39:20 GMT</pubDate>
</item>
<item>
<title>2024年更新的英文GloVe模型评估报告</title>
<link>https://arxiv.org/abs/2507.18103</link>
<guid>https://arxiv.org/abs/2507.18103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2024年更新的GloVe模型在语言和文化上更具相关性。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了2024年新发布的英文GloVe模型，旨在提升语言模型对现代文化和语言变化的适应能力。相比2014年的原始模型，新模型基于Wikipedia、Gigaword和Dolma数据集进行训练，并详细记录了数据版本和预处理过程。通过词汇比较、直接测试和命名实体识别（NER）任务评估，结果显示新模型在包含非西方新闻数据等时间依赖性任务中表现更优，同时在结构任务如类比和相似性任务中保持良好性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 01:29:18 GMT</pubDate>
</item>
<item>
<title>TeleChat系列模型升级：性能显著提升的多版本发布</title>
<link>https://arxiv.org/abs/2507.18013</link>
<guid>https://arxiv.org/abs/2507.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeleChat系列新版本在训练策略上优化，提升推理与任务表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TeleChat系列的最新版本——TeleChat2、TeleChat2.5和T1，相较于前代模型实现了显著的性能提升。尽管模型架构变化不大，但通过改进的预训练和后训练策略，新模型在代码生成、数学推理等任务中表现出色。T1特别注重复杂推理能力，支持长链式思维，而TeleChat2.5则强调推理速度。两款旗舰模型均为115B参数的密集型Transformer架构，且T1-115B在多项指标上超越了如OpenAI的o1-mini和GPT-4o等专有模型。所有版本已公开发布，供开发者和研究人员使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 21:00:48 GMT</pubDate>
</item>
<item>
<title>基于Spelke对象的视觉分割方法研究</title>
<link>https://arxiv.org/abs/2507.16038</link>
<guid>https://arxiv.org/abs/2507.16038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SpelkeNet，用于识别物理运动关系下的视觉对象。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了计算机视觉中基于语义的分割与人类感知的Spelke对象之间的差异。Spelke对象是根据物理因果运动关系定义的，而非特定类别。作者引入了SpelkeBench数据集，并构建了SpelkeNet模型，通过预测未来运动分布来提取Spelke对象。该模型利用运动可及性图和预期位移图进行统计反事实探测，从而定义Spelke段。实验表明，SpelkeNet在SpelkeBench上优于现有方法，并在物理对象操作任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 16:11:57 GMT</pubDate>
</item>
<item>
<title>基于扩散变换器的皮肤病变分割模型SegDT</title>
<link>https://arxiv.org/abs/2507.15595</link>
<guid>https://arxiv.org/abs/2507.15595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SegDT在皮肤病变分割中表现优异，适用于医疗应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SegDT的新分割模型，基于扩散变换器（DiT），旨在提高皮肤病变分割的准确性和效率。该模型采用Rectified Flow技术，在降低推理步骤的同时保持生成质量，并在低功耗硬件上运行。SegDT在三个基准数据集上进行了评估，结果优于现有方法，且推理速度快，适用于实际医疗场景。研究推动了深度学习在医学图像分析中的应用，为医疗诊断提供了更快速、精准的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:18:05 GMT</pubDate>
</item>
<item>
<title>基于动量不确定性的高效语言模型推理优化方法</title>
<link>https://arxiv.org/abs/2507.14958</link>
<guid>https://arxiv.org/abs/2507.14958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUR方法提升LLM推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Momentum Uncertainty-guided Reasoning (MUR)的新型推理优化方法，旨在提高大型语言模型（LLMs）在推理任务中的效率和准确性。该方法受物理学中动量概念启发，通过跟踪和聚合每一步的不确定性来动态分配思考预算，从而减少冗余计算。研究还引入了gamma-control机制，通过单一超参数调节推理预算。实验结果表明，MUR在多个基准测试中平均减少50%以上的计算量，同时提升准确率0.62-3.37%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 09:36:19 GMT</pubDate>
</item>
<item>
<title>Captain Cinema：基于文本生成高质量短片的框架</title>
<link>https://arxiv.org/abs/2507.18634</link>
<guid>https://arxiv.org/abs/2507.18634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Captain Cinema通过关键帧规划与视频合成生成高质量短片。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Captain Cinema，一个用于生成短片的框架。该框架根据详细的文本描述生成关键帧序列，确保故事和视觉的一致性，随后利用视频合成模型生成时空动态内容。为提升多场景长叙事视频的生成效果，作者引入了针对长上下文数据的交错训练策略，并在专门构建的电影数据集上进行训练。实验表明，Captain Cinema能够在高质量和高效率的前提下实现视觉连贯且叙事一致的短片生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>TTS-VAR：一种高效的视觉自回归模型测试时扩展框架</title>
<link>https://arxiv.org/abs/2507.18537</link>
<guid>https://arxiv.org/abs/2507.18537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTS-VAR提升视觉生成模型性能，提高8.7%得分。</p><br /><br /><p><strong>摘要：</strong> 本文提出TTS-VAR，首个针对视觉自回归（VAR）模型的测试时扩展框架，将生成过程建模为路径搜索问题。通过自适应下降批量大小调度平衡计算效率与探索能力，并引入基于聚类的多样性搜索和基于重采样的潜力选择机制。实验表明，在Infinity模型上提升了8.7%的GenEval分数，揭示早期结构特征对最终质量的影响及不同尺度下重采样效果的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>TeEFusion：一种高效的文本到图像生成蒸馏方法</title>
<link>https://arxiv.org/abs/2507.18192</link>
<guid>https://arxiv.org/abs/2507.18192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeEFusion提升图像生成效率，保持高质量输出。</p><br /><br /><p><strong>摘要：</strong> 本文提出TeEFusion，一种高效的文本到图像生成蒸馏方法。通过将引导强度直接融入文本嵌入中，TeEFusion在不增加额外参数的情况下，使学生模型能够学习教师模型的复杂采样策略。实验表明，该方法显著提升了推理速度，达到教师模型的6倍，同时保持了相近的图像质量。该方法已在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 04:45:40 GMT</pubDate>
</item>
<item>
<title>大规模地球3D生成技术的创新与应用</title>
<link>https://arxiv.org/abs/2507.16535</link>
<guid>https://arxiv.org/abs/2507.16535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EarthCrafter实现千平方公里级3D地球建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出EarthCrafter，一种用于大规模3D地球生成的框架，结合了Aerial-Earth3D数据集和稀疏解耦扩散模型。该方法通过分离结构与纹理生成，有效降低计算成本并保持地理合理性。实验表明其在超大规模生成任务中表现优异，并支持多种应用场景，如语义引导的城市布局生成和无条件地形合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:46:48 GMT</pubDate>
</item>
<item>
<title>Hierarchical Budget Policy Optimization提升推理效率与能力</title>
<link>https://arxiv.org/abs/2507.15844</link>
<guid>https://arxiv.org/abs/2507.15844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HBPO提升模型推理效率同时保持准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Hierarchical Budget Policy Optimization (HBPO)框架，通过分层预算探索和差异化奖励机制，使模型根据问题复杂度自动调整推理深度。该方法有效解决传统方法在效率训练中探索空间塌陷的问题，减少平均token使用量达60.6%，同时提升准确率3.14%。HBPO无需外部约束或离散模式选择，展现出模型自适应调整推理深度的能力，证明推理效率与能力可以协同优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:52:34 GMT</pubDate>
</item>
<item>
<title>LAPO：通过自适应策略优化实现高效推理的框架</title>
<link>https://arxiv.org/abs/2507.15758</link>
<guid>https://arxiv.org/abs/2507.15758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LAPO提升模型推理效率并减少token使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Length-Adaptive Policy Optimization (LAPO)的新框架，旨在将推理长度控制从外部约束转化为模型的内在能力。该框架通过两阶段强化学习过程，使模型能够学习自然的推理模式，并在推理过程中动态调整计算资源。实验表明，LAPO在数学推理基准测试中可减少高达40.9%的token使用量，同时提升2.3%的准确性。分析显示，LAPO训练的模型能根据问题复杂度有效分配计算资源，实现高效且高质量的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:14:41 GMT</pubDate>
</item>
<item>
<title>DMOSpeech 2：通过强化学习优化语音合成的持续预测器</title>
<link>https://arxiv.org/abs/2507.14988</link>
<guid>https://arxiv.org/abs/2507.14988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DMOSpeech 2优化了语音合成中的持续预测，提升整体性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DMOSpeech 2，这是一种基于扩散模型的文本转语音系统，通过强化学习方法优化了持续预测器，提高了语音合成的整体质量。该系统采用了一种新的持续策略框架，利用说话人相似性和词错误率作为奖励信号。此外，引入了教师引导采样方法，在保持效率的同时提升了输出多样性。实验结果表明，DMOSpeech 2在所有指标上均优于之前系统，并减少了采样步骤而未影响质量，标志着语音合成系统在多组件优化方面的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 10:48:48 GMT</pubDate>
</item>
<item>
<title>Promptomatix：自动提示优化框架提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.14241</link>
<guid>https://arxiv.org/abs/2507.14241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Promptomatix实现自动提示优化，提升LLM性能。</p><br /><br /><p><strong>摘要：</strong> Promptomatix是一个自动提示优化框架，能够将自然语言任务描述转换为高质量提示，无需手动调整或领域知识。该框架支持基于元提示的优化器和DSPy驱动的编译器，具备模块化设计，便于未来扩展。系统通过分析用户意图、生成合成训练数据、选择提示策略并使用成本感知目标优化提示，已在五个任务类别中表现出色，优于现有库，同时减少提示长度和计算开销，提高效率和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 14:18:20 GMT</pubDate>
</item>
<item>
<title>Pusa：基于向量化时间步适应的视频扩散模型新范式</title>
<link>https://arxiv.org/abs/2507.16116</link>
<guid>https://arxiv.org/abs/2507.16116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pusa通过VTA实现高效视频生成与多任务能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Pusa，一种基于向量化时间步适应（VTA）的视频扩散模型新范式。该方法在保持基础模型能力的同时，实现了精细的时间控制，显著提升了图像到视频生成的效率和性能。实验表明，Pusa在VBench-I2V基准上得分87.32%，远超现有模型，且具备零样本多任务能力，如起始帧、视频扩展等，同时支持文本到视频生成。VTA技术有效避免了计算冗余和灾难性遗忘问题，为下一代视频合成提供了可扩展、高效且通用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 20:09:37 GMT</pubDate>
</item>
<item>
<title>Yume: An Interactive World Generation Model</title>
<link>https://arxiv.org/abs/2507.17744</link>
<guid>https://arxiv.org/abs/2507.17744</guid>
<content:encoded><![CDATA[
Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>DesignLab：通过迭代优化提升幻灯片设计质量</title>
<link>https://arxiv.org/abs/2507.17202</link>
<guid>https://arxiv.org/abs/2507.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DesignLab通过角色分离实现幻灯片设计的持续优化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为DesignLab的系统，旨在帮助非专业人士设计高质量的幻灯片。该系统将设计过程分为设计审查者和设计贡献者两个角色，审查者负责识别设计问题，贡献者则进行修正，形成一个迭代优化的流程。通过微调大语言模型并引入受控扰动模拟中间草稿，DesignLab能够有效学习设计错误和修复方法。实验表明，该方法在设计质量上优于现有工具，能生成更专业、精美的幻灯片。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:49:48 GMT</pubDate>
</item>
<item>
<title>RAVine：面向代理式搜索的现实对齐评估框架</title>
<link>https://arxiv.org/abs/2507.16725</link>
<guid>https://arxiv.org/abs/2507.16725</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAVine提升代理式搜索系统的评估准确性与实用性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RAVine，一个面向代理式大语言模型搜索的现实对齐评估框架。针对现有评估体系在复杂查询、细粒度评估和迭代过程分析方面的不足，RAVine通过多点查询和长文本回答更贴近用户意图，并引入可追溯的地面真实数据构建策略，提高评估精度。同时，RAVine关注模型在搜索工具交互中的表现及效率因素。实验结果揭示了代理式搜索系统的发展方向，相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16725" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:08:12 GMT</pubDate>
</item>
<item>
<title>文本到图像扩散模型中的记忆与隐私问题研究</title>
<link>https://arxiv.org/abs/2507.16880</link>
<guid>https://arxiv.org/abs/2507.16880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到图像模型易复制训练数据，现有防御措施效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了文本到图像扩散模型在数据隐私和知识产权方面的潜在风险。尽管已有方法尝试通过剪枝来减少模型对训练数据的复制，但实验表明，仅需微调输入提示的文本嵌入即可重新触发数据复制，说明现有方法并不稳固。此外，研究挑战了记忆局部化的假设，证明复制行为可以在文本嵌入空间的不同位置被触发，并且路径各异。这表明当前的缓解策略不足以解决问题，应寻求真正删除记忆内容的方法。为此，作者提出了一种新的对抗性微调方法，以提升模型的鲁棒性。研究为构建更可信、合规的生成式AI提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:02:38 GMT</pubDate>
</item>
<item>
<title>基于形式语言的大型语言模型验证方法研究</title>
<link>https://arxiv.org/abs/2507.16331</link>
<guid>https://arxiv.org/abs/2507.16331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用形式语言提升LLM验证可靠性与可扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于形式语言（如Dafny）的大型语言模型（LLM）验证方法，以解决传统基于自然语言的验证不可靠、不可扩展的问题。通过引入自动数据整理流程和结合形式语言验证器反馈的强化学习设计，研究团队构建了DafnyComp基准测试集，并在监督微调阶段使小型模型生成可验证的Dafny代码，效果优于现有专有模型。进一步的正则化强化学习提升了模型在跨域任务中的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 04:13:01 GMT</pubDate>
</item>
<item>
<title>高效3D生成框架Ultra3D提升稀疏体素建模速度</title>
<link>https://arxiv.org/abs/2507.17745</link>
<guid>https://arxiv.org/abs/2507.17745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ultra3D通过优化注意力机制实现高速高质3D建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出Ultra3D，一种高效的3D生成框架，旨在解决现有方法在稀疏体素建模中的计算效率问题。该框架利用VecSet表示法在第一阶段快速生成粗略物体布局，减少令牌数量并加速体素坐标预测。第二阶段引入Part Attention机制，仅在语义一致的部分区域进行注意力计算，从而保持结构连续性并避免不必要的全局注意力，实现高达6.7倍的生成速度提升。此外，构建了可扩展的部件注释流程，将原始网格转换为带标签的稀疏体素。实验表明，Ultra3D支持1024分辨率的高质量3D生成，并在视觉保真度和用户偏好方面达到领先水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>多领域推理在强化学习中的系统研究</title>
<link>https://arxiv.org/abs/2507.17512</link>
<guid>https://arxiv.org/abs/2507.17512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多领域推理在强化学习中的交互机制与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统探讨了在强化学习框架下，数学推理、代码生成和逻辑谜题解决等多领域推理的交互机制。通过GRPO算法和Qwen-2.5-7B模型进行实验，分析了单领域训练下的性能提升与跨领域泛化能力，同时研究了多领域联合训练中出现的相互促进与冲突现象。此外，还比较了基础模型与指令微调模型在相同强化学习配置下的表现差异，并深入探讨了课程学习、奖励设计及语言特性对训练效果的影响。实验结果揭示了领域间互动的关键因素，为提升大语言模型的多领域推理能力提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 09:51:04 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的感知能力评估与Turing Eye Test基准</title>
<link>https://arxiv.org/abs/2507.16863</link>
<guid>https://arxiv.org/abs/2507.16863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前多模态模型在感知任务中表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在感知能力方面与人类的差距，提出了一种名为Turing Eye Test（TET）的感知导向基准测试。该基准包含四个诊断任务，用于评估模型在合成图像上的表现。研究发现，尽管最先进的MLLMs在语言推理任务中表现良好，但在感知任务上却出现严重失败，而仅通过语言模型的微调无法提升其性能。只有对视觉模块进行微调才能显著改善表现，表明当前MLLMs在视觉泛化能力上仍存在重大不足。作者发布了部分TET任务，并计划在未来引入更多任务以提升视觉泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:50:16 GMT</pubDate>
</item>
<item>
<title>Elevate3D：提升低质量3D模型质量的新框架</title>
<link>https://arxiv.org/abs/2507.11465</link>
<guid>https://arxiv.org/abs/2507.11465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Elevate3D提升低质量3D模型的纹理与几何质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Elevate3D，一种用于提升低质量3D模型质量的新框架。该框架通过HFS-SDEdit方法显著改善纹理质量，同时保留原始外观和几何结构，并修复其退化问题。Elevate3D采用逐视角优化策略，结合先进的单目几何预测器，实现纹理与几何的协同优化。相比现有方法，Elevate3D在3D模型精修方面达到最新水平，有效缓解高质量3D资源短缺的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 12:36:20 GMT</pubDate>
</item>
<item>
<title>PrefPalette：基于属性分解的人类偏好建模框架</title>
<link>https://arxiv.org/abs/2507.13541</link>
<guid>https://arxiv.org/abs/2507.13541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefPalette提升AI个性化能力，增强可解释性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了PrefPalette，一个将人类偏好分解为属性维度并根据不同社会群体价值观进行个性化预测的框架。该方法通过生成合成数据和注意力机制，使AI更准确地理解用户偏好，并在Reddit的45个社区中表现出比GPT-4o更高的预测准确率。此外，它揭示了不同社区的偏好特征，如学术群体重视详尽与刺激，冲突导向群体偏好讽刺与直接表达，支持型群体强调同理心。PrefPalette不仅提升了偏好建模效果，还提供了透明、可解释的洞察，为更可信的个性化应用奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:21:54 GMT</pubDate>
</item>
<item>
<title>面向目标检测的新型零样本量化框架</title>
<link>https://arxiv.org/abs/2507.16782</link>
<guid>https://arxiv.org/abs/2507.16782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种任务特定的零样本量化方法提升目标检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种面向目标检测的新型零样本量化框架（ZSQ），通过引入边界框和类别采样策略生成任务相关的校准集，并将任务特定训练融入知识蒸馏过程，从而恢复量化检测网络的性能。实验表明该方法在MS-COCO和Pascal VOC数据集上表现出色，具有较高的效率和先进性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:28:29 GMT</pubDate>
</item>
<item>
<title>ExpTeach：通过自我生成记忆实现视觉语言模型与机器人的有效融合</title>
<link>https://arxiv.org/abs/2507.16713</link>
<guid>https://arxiv.org/abs/2507.16713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ExpTeach提升机器人任务成功率，增强空间理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出ExpTeach框架，通过构建真实世界经验的自我生成记忆，将原本在互联网数据上训练的视觉语言模型（VLMs）有效地应用于物理机器人。ExpTeach使VLM能够自主规划动作、验证结果、反思失败并调整行为，同时将经验总结为长期记忆，用于未来任务的检索增强生成（RAG）。此外，该框架还引入按需图像标注模块，提升VLM的空间理解能力。实验表明，ExpTeach显著提高了机器人任务的成功率，从36%提升至84%，并在12个真实场景中展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>区域自适应潜在上采样提升扩散模型推理效率</title>
<link>https://arxiv.org/abs/2507.08422</link>
<guid>https://arxiv.org/abs/2507.08422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RALU方法提升扩散模型推理速度且保持图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为区域自适应潜在上采样（RALU）的训练-free 框架，用于加速基于扩散的图像和视频生成模型。RALU通过在空间维度上进行混合分辨率采样，包括低分辨率去噪、特定区域的自适应上采样以及全分辨率细节优化，显著提升了推理效率。同时，通过噪声-时间步重新调度技术，确保不同分辨率之间的生成稳定性。实验表明，RALU在FLUX和Stable Diffusion 3上分别实现了7.0倍和3.0倍的速度提升，且图像质量损失极小。该方法还可与现有时间维度加速技术兼容，进一步降低推理延迟而不影响生成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:07:43 GMT</pubDate>
</item>
<item>
<title>ThinkAct：基于视觉潜在规划的多模态推理与动作执行框架</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkAct提升多模态AI任务的长程规划与自修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ThinkAct框架，通过结合高层推理与低层动作执行，实现更高效的多模态任务处理。该框架利用强化学习生成与动作对齐的视觉奖励，指导多模态大模型生成具有一致性和目标导向的推理计划，并将其压缩为视觉潜在表示以指导后续动作执行。实验表明，ThinkAct在复杂环境任务中展现出少样本适应、长程规划和自我修正的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SOPHIA提升视觉语言模型的慢思考推理能力</title>
<link>https://arxiv.org/abs/2507.16814</link>
<guid>https://arxiv.org/abs/2507.16814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SOPHIA增强LVLM的多模态推理能力，效果显著。</p><br /><br /><p><strong>摘要：</strong> 本文提出SOPHIA，一种用于视觉语言模型（LVLM）的半离策略强化学习方法，旨在提升其慢思考推理能力。通过结合可训练LVLM的视觉理解与语言模型的离策略推理，SOPHIA生成基于结果的奖励，并将视觉奖励反向传播，使LVLM能够从推理轨迹中学习。实验表明，SOPHIA在多个多模态推理基准测试中表现优异，尤其在InternVL3.0-38B上提升了8.5%，甚至超越部分闭源模型。分析显示，SOPHIA优于监督微调和直接在线策略方法，为后续在线策略训练提供了更好的策略初始化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于多模态大模型的人-物交互合成方法研究</title>
<link>https://arxiv.org/abs/2507.16813</link>
<guid>https://arxiv.org/abs/2507.16813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HOComp方法实现自然人-物交互合成。</p><br /><br /><p><strong>摘要：</strong> 本文针对图像中人与物体交互合成的挑战，提出HOComp方法，通过MLLMs驱动的区域姿态引导和细节一致外观保持机制，实现更自然、和谐的人-物交互合成。研究还构建了首个交互感知的人-物合成数据集IHOC，实验表明该方法在视觉质量和一致性上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>构建科学推理数据集与模型提升AI在自然科学中的表现</title>
<link>https://arxiv.org/abs/2507.16812</link>
<guid>https://arxiv.org/abs/2507.16812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建高质量科学推理数据集，提升AI在自然科学中的研究能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出TextbookReasoning和MegaScience两个大型科学推理数据集，旨在填补人工智能在自然科学领域研究的空白。TextbookReasoning包含12,000本大学教材中的650,000道推理题，涵盖7个科学领域；MegaScience则整合了1.25百万条高质量数据，通过系统性实验优化数据选择方法。研究还构建了覆盖15个基准测试的评估体系，确保准确衡量模型性能。实验表明，基于MegaScience训练的Llama3.1、Qwen2.5和Qwen3系列模型在平均性能上优于官方指令模型，并展现出规模效应。作者已公开数据集、评估系统及训练模型，推动科学推理研究发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>无需修改训练数据的大型语言模型泛化控制方法</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAFT技术通过概念消融提升模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为概念消融微调（CAFT）的新方法，用于控制大型语言模型在微调后的泛化行为，而无需修改训练数据。该方法利用可解释性工具，在微调过程中通过线性投影消除模型中的不良概念，从而避免不必要的泛化。实验表明，CAFT在三个微调任务中有效减少了错误泛化现象，且不损害模型在训练分布上的性能。这种方法为控制模型泛化提供了一种新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16795" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:45:04 GMT</pubDate>
</item>
<item>
<title>突破大语言模型推理瓶颈的Thread Inference Model</title>
<link>https://arxiv.org/abs/2507.16784</link>
<guid>https://arxiv.org/abs/2507.16784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TIM与TIMRUN提升大模型推理能力与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Thread Inference Model (TIM) 和 TIMRUN 推理运行时，以解决大语言模型（LLM）在推理准确性和效率上的限制。TIM 是一种用于递归和分解问题解决的 LLM 家族，而 TIMRUN 支持超越上下文限制的长周期结构化推理。通过将自然语言建模为推理树，TIM 实现了几乎无限的工作内存和多跳工具调用，克服了输出限制、位置嵌入约束和 GPU 内存瓶颈。实验表明，该系统在处理数学任务和需要长周期推理的信息检索中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:30:04 GMT</pubDate>
</item>
<item>
<title>Zebra-CoT数据集提升多模态视觉链式推理能力</title>
<link>https://arxiv.org/abs/2507.16746</link>
<guid>https://arxiv.org/abs/2507.16746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra-CoT提升视觉链式推理模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Zebra-CoT，一个包含182,384个样本的大规模多模态数据集，用于训练视觉链式推理（Visual CoT）模型。该数据集涵盖科学问题、二维和三维推理任务以及逻辑与策略游戏等场景。实验表明，在Zebra-CoT上微调Anole-7B模型可使测试集准确率提升12%，在标准VLM基准测试中性能提高最多13%。Bagel-7B模型在该数据集上微调后能生成高质量的多模态推理链，证明了Zebra-CoT在提升多模态推理能力方面的有效性。作者开源了数据集和模型以支持相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:35:36 GMT</pubDate>
</item>
<item>
<title>Step-Audio 2：面向工业级音频理解与语音对话的端到端多模态大语言模型</title>
<link>https://arxiv.org/abs/2507.16632</link>
<guid>https://arxiv.org/abs/2507.16632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Audio 2 提升了语音识别与音频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Step-Audio 2，这是一个面向工业级音频理解和语音对话的端到端多模态大语言模型。通过整合潜在音频编码器和以推理为中心的强化学习（RL），Step-Audio 2 在自动语音识别（ASR）和音频理解方面表现出色。该模型还引入了离散音频标记生成，增强了对语调、情感等副语言信息的响应能力。此外，Step-Audio 2 结合了检索增强生成（RAG）技术，并支持调用外部工具如网络搜索和音频搜索，以减少幻觉并实现音色切换。经过数百万小时的语音和音频数据训练，Step-Audio 2 在多种对话场景中展现出强大的智能和表现力。评估结果表明，其在多个音频理解和对话基准测试中均优于其他开源和商业解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 10:23:55 GMT</pubDate>
</item>
<item>
<title>推理时计算增强模型鲁棒性的安全风险分析</title>
<link>https://arxiv.org/abs/2507.15974</link>
<guid>https://arxiv.org/abs/2507.15974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时计算提升模型鲁棒性，但可能暴露安全风险。</p><br /><br /><p><strong>摘要：</strong> 本文研究发现，小型开源模型也能通过推理时的预算强制策略提升鲁棒性。然而，研究揭示了先前研究中的一个隐含假设：中间推理步骤对攻击者是隐藏的。当这一假设被打破，即中间步骤可被访问时，推理时计算反而会降低模型的鲁棒性，呈现出反向缩放规律。此外，文章指出在工具集成和高级推理提取攻击等场景下，即使推理链被隐藏，模型仍可能受到攻击。因此，作者强调在实际应用中需权衡推理时计算带来的安全与性能之间的微妙平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 14:08:38 GMT</pubDate>
</item>
<item>
<title>ObjectGS：融合语义理解的3D场景重建框架</title>
<link>https://arxiv.org/abs/2507.15454</link>
<guid>https://arxiv.org/abs/2507.15454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectGS实现对象级3D重建与语义理解的统一。</p><br /><br /><p><strong>摘要：</strong> 本文提出ObjectGS，一个将3D场景重建与语义理解相结合的对象感知框架。不同于传统方法将场景视为整体，ObjectGS通过将每个物体建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。训练过程中动态调整锚点并优化特征，同时使用one-hot ID编码和分类损失确保语义清晰。实验表明，ObjectGS在开放词汇和全景分割任务中优于现有方法，并支持网格提取和场景编辑等应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:06:23 GMT</pubDate>
</item>
<item>
<title>SPAR：基于多智能体框架的学术文献检索新方法</title>
<link>https://arxiv.org/abs/2507.15245</link>
<guid>https://arxiv.org/abs/2507.15245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPAR提升学术文献检索性能，优于现有基线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SPAR，一种基于多智能体框架的学术文献检索系统，结合RefChain进行查询分解和演化，以提高搜索的灵活性和有效性。为了系统评估，研究者构建了SPARBench基准测试集，包含专家标注的相关性标签。实验结果表明，SPAR在AutoScholar和SPARBench数据集上分别比最佳基线提升了56%和23%的F1分数。SPAR与SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 01:06:53 GMT</pubDate>
</item>
<item>
<title>基于强化学习的RefCritic模块提升语言模型批判能力</title>
<link>https://arxiv.org/abs/2507.15024</link>
<guid>https://arxiv.org/abs/2507.15024</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefCritic通过双重奖励机制提升模型批判与优化能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型中批评模块开发的挑战，提出了一种基于强化学习的长链思维批评模块RefCritic。该模块采用双重规则奖励机制，分别评估解决方案的实例级正确性和策略模型的细化准确性，以生成高质量且具有行动指导性的反馈。在多个基准测试中，RefCritic表现出显著优势，尤其在数学推理任务中优于传统步骤级监督方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15024" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:19:51 GMT</pubDate>
</item>
<item>
<title>基于MCP的LLM智能体评估框架MCPEval</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCPEval实现LLM智能体自动化评估与标准化测试。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MCPEval，一个基于Model Context Protocol (MCP) 的开源框架，用于自动化生成任务并深度评估大型语言模型（LLM）智能体在多个领域中的表现。该框架通过标准化指标、与原生工具集成，减少了人工构建评估流程的工作量。实验结果表明，MCPEval在五个现实场景中有效揭示了模型在特定领域的性能特征，并已公开发布以促进可复现和标准化的LLM评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 01:46:27 GMT</pubDate>
</item>
<item>
<title>LLM生成学生风格代码的系统研究</title>
<link>https://arxiv.org/abs/2507.12674</link>
<guid>https://arxiv.org/abs/2507.12674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究LLM生成类似学生代码的能力与方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出ParaStudent，研究大型语言模型在编程课程中生成类似学生代码的能力。通过分析多个学期的学生提交记录，设计低分辨率和高分辨率实验，评估代码在语义、功能和风格维度的表现。结果表明，微调能显著提升与真实学生代码轨迹的一致性，更准确地捕捉错误模式、逐步改进和风格变化。研究强调了通过上下文感知生成、时间建模和多维评估来模拟真实学生代码的重要性。代码和实验数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 19:12:14 GMT</pubDate>
</item>
<item>
<title>机器学习中的串行计算挑战与未来发展方向</title>
<link>https://arxiv.org/abs/2507.12549</link>
<guid>https://arxiv.org/abs/2507.12549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">串行计算问题限制了当前机器学习的进展。</p><br /><br /><p><strong>摘要：</strong> 文章指出，尽管机器学习在大规模并行化方面取得了进展，但某些问题本质上是顺序的，如数学推理、物理模拟和序列决策等，这些任务需要依赖性的计算步骤，无法并行处理。基于复杂性理论，作者明确了这一区别，并展示了当前以并行为中心的架构在处理此类任务时存在根本性局限。文章强调，识别计算的串行性质对机器学习、模型设计和硬件发展具有深远影响，认为在AI面对更复杂的推理任务时，应更加重视串行计算的扩展，而不仅仅是并行计算。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12549" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 14:01:26 GMT</pubDate>
</item>
<item>
<title>基于去噪的潜在令牌化器设计研究</title>
<link>https://arxiv.org/abs/2507.15856</link>
<guid>https://arxiv.org/abs/2507.15856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型去噪令牌化器，提升生成模型效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉令牌化器在生成建模中的有效性问题，指出现代生成模型都具有从噪声或遮蔽输入中重建清晰信号的训练目标，这一过程称为去噪。受此启发，作者提出将令牌化器嵌入直接与下游去噪目标对齐，使潜在嵌入在严重干扰下仍易于重建。为此，引入了Latent Denoising Tokenizer（l-DeTok），该令牌化器通过插值噪声和随机遮蔽来重建清晰图像。实验表明，在ImageNet 256x256数据集上，l-DeTok在六种代表性生成模型中均优于标准令牌化器。研究强调去噪是令牌化器设计的重要原则，希望为未来设计提供新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于概念驱动的视频目标分割框架SeC及其性能评估</title>
<link>https://arxiv.org/abs/2507.15852</link>
<guid>https://arxiv.org/abs/2507.15852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeC提升视频目标分割效果，实现更高精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于概念驱动的视频目标分割框架SeC，旨在解决传统方法在处理视觉变化、遮挡和复杂场景时的不足。SeC利用大视觉语言模型构建高阶、以对象为中心的表示，增强语义理解能力，并在推理过程中动态调整计算资源。为评估该方法，研究者还构建了SeCVOS基准数据集，包含160个精心标注的多场景视频。实验表明，SeC在SeCVOS上比SAM 2.1提升了11.8个百分点，成为当前最先进的概念感知视频目标分割方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GUI-G^2：基于高斯分布的图形用户界面定位奖励框架</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI-G^2通过高斯分布提升GUI交互的精确性与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为GUI Gaussian Grounding Rewards (GUI-G^2) 的新型奖励框架，用于改进图形用户界面（GUI）中的自然语言指令定位任务。传统方法依赖二值奖励，导致信号稀疏且忽略空间连续性。而GUI-G^2通过将GUI元素建模为连续高斯分布，引入了两种协同机制：高斯点奖励用于精确定位，覆盖奖励用于评估空间对齐度。此外，该框架还包含自适应方差机制，以处理不同尺寸的界面元素。实验表明，GUI-G^2在多个基准测试中显著优于现有方法，特别是在ScreenSpot-Pro上提升了24.7%。研究显示，连续建模增强了模型对界面变化的鲁棒性和对未知布局的泛化能力，为GUI交互任务提供了新的范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:53:42 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的经济政策模拟框架</title>
<link>https://arxiv.org/abs/2507.15815</link>
<guid>https://arxiv.org/abs/2507.15815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM Economist通过代理建模实现经济政策设计与评估。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为LLM Economist的新框架，利用基于代理的建模方法，在具有层级决策的战略环境中设计和评估经济政策。底层由有限理性的工人代理组成，这些代理基于美国人口普查数据生成，以最大化文本定义的效用函数。上层的规划代理则通过上下文强化学习提出分段线性边际税率方案。该框架具备优化异质效用、生成大规模真实人口代理以及完全用自然语言表达机制设计的能力。实验表明，该框架在一百个交互代理中能够接近斯塔克尔伯格均衡，提升社会福利，并在去中心化治理下通过定期投票进一步提高效果。结果证明，基于大语言模型的代理可以共同建模、模拟和治理复杂经济系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:21:14 GMT</pubDate>
</item>
<item>
<title>基于熵感知的强化学习方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.15778</link>
<guid>https://arxiv.org/abs/2507.15778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Archer方法提升LLM推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Archer的熵感知强化学习方法，旨在提升大语言模型（LLM）的推理能力。传统RLVR方法对所有token应用相同的训练信号，而忽略了低熵知识类token与高熵推理类token的不同作用。Archer通过双token约束和同步更新机制，对推理token施加较弱的KL正则化和较高的裁剪阈值以鼓励探索，同时对知识token施加强约束以保持事实准确性。实验结果表明，该方法在多个数学推理和代码生成基准上表现优异，达到或超过同规模模型的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:34:01 GMT</pubDate>
</item>
<item>
<title>TokensGen：基于压缩标记的长视频生成框架</title>
<link>https://arxiv.org/abs/2507.15728</link>
<guid>https://arxiv.org/abs/2507.15728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokensGen通过压缩标记提升长视频生成的连贯性与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出TokensGen，一种两阶段框架，旨在解决长视频生成中的记忆瓶颈和长期不一致问题。该方法将长视频生成分解为内片段语义控制、长期一致性控制和片段间平滑过渡三个核心任务。首先训练To2V模型，利用文本和视频标记生成短视频；其次引入T2To模型，一次性生成所有标记以确保全局一致性；最后在推理阶段采用自适应FIFO-Diffusion策略实现片段间的无缝连接。实验表明，该方法在保持计算效率的同时显著提升了长视频的时间和内容连贯性，为叙事、电影制作和沉浸式模拟提供了可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 11:37:33 GMT</pubDate>
</item>
<item>
<title>基于数据混合代理的持续预训练方法</title>
<link>https://arxiv.org/abs/2507.15640</link>
<guid>https://arxiv.org/abs/2507.15640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出数据混合代理模型，提升语言模型在新领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Data Mixing Agent的端到端框架，用于在持续预训练中优化源领域和目标领域数据的混合比例，以避免模型遗忘原有能力。该方法通过强化学习从大量数据混合轨迹中学习通用的重加权策略，无需人工设定。实验表明，该方法在数学推理任务中优于现有基线，并能跨领域、跨模型泛化。此外，它在代码生成等不同任务中也表现出良好的适应性，且在较少源领域数据的情况下仍能保持高性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 10:01:54 GMT</pubDate>
</item>
<item>
<title>基于离散SDF的3D高斯点云逆渲染方法</title>
<link>https://arxiv.org/abs/2507.15629</link>
<guid>https://arxiv.org/abs/2507.15629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种离散SDF增强的3D高斯点云逆渲染方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于离散符号距离场（SDF）的3D高斯点云逆渲染方法，旨在解决传统方法在几何约束应用上的不足。通过将SDF编码到每个高斯中，并利用SDF到透明度的转换实现高效渲染，避免了光线追踪计算。为保证离散样本与真实SDF的一致性，引入基于投影的对齐损失。实验表明，该方法在光照重建质量上优于现有方法，且无需额外内存和复杂优化设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:52:33 GMT</pubDate>
</item>
<item>
<title>Being-H0：基于人类视频的多模态机器人操作模型</title>
<link>https://arxiv.org/abs/2507.15597</link>
<guid>https://arxiv.org/abs/2507.15597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-H0通过人类视频训练，提升机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> Being-H0是一种新型的视觉-语言-动作模型（VLA），利用大规模人类视频进行训练，旨在解决现有模型在复杂操作任务中的不足。该模型采用物理指令微调方法，结合大规模预训练、三维空间对齐和任务适配，提升了机器人在真实环境中的表现。同时，研究团队构建了一个包含多种数据源的大规模数据集，以支持模型训练与优化。实验表明，Being-H0在手部运动生成和指令遵循方面表现出色，并具备良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:19:09 GMT</pubDate>
</item>
<item>
<title>PhysGym：评估大语言模型科学发现能力的新基准</title>
<link>https://arxiv.org/abs/2507.15550</link>
<guid>https://arxiv.org/abs/2507.15550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysGym用于评估大语言模型在物理环境中的科学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysGym，一个用于评估基于大语言模型的科学推理能力的新基准套件和仿真平台。该平台通过控制提供给代理的先验知识水平，使研究人员能够分析模型在不同问题复杂度下的表现。PhysGym包含一系列交互式模拟，要求代理主动探测环境、在约束条件下收集数据并提出关于物理规律的假设。平台提供了标准化的评估协议和指标，用于衡量假设的准确性和模型的真实性。实验结果展示了该基准在区分不同先验知识和任务复杂度下的模型能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 08:28:10 GMT</pubDate>
</item>
<item>
<title>GR-3：通用机器人策略的进展与ByteMini集成</title>
<link>https://arxiv.org/abs/2507.15493</link>
<guid>https://arxiv.org/abs/2507.15493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GR-3具备强大泛化能力，适用于多种任务和环境。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GR-3，一种大规模视觉-语言-动作（VLA）模型，具有在新物体、环境和抽象指令中进行泛化的出色能力。GR-3可通过少量人类轨迹数据高效微调，实现快速适应新场景。它还能处理长时序和精细操作任务，包括双臂操作和移动任务。该模型通过多阶段训练方法实现，包括网络规模的视觉-语言数据联合训练、VR设备收集的人类轨迹数据微调以及机器人轨迹数据的模仿学习。同时，文章介绍了与GR-3集成的ByteMini机器人，展示了其在多种任务中的灵活性和可靠性。实验表明，GR-3在多个挑战性任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:54:13 GMT</pubDate>
</item>
<item>
<title>Stitch：实现语音模型同步思考与回答的新方法</title>
<link>https://arxiv.org/abs/2507.15375</link>
<guid>https://arxiv.org/abs/2507.15375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Stitch让语音模型在说话时同步生成内部思考过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Stitch的新方法，用于改进语音语言模型（SLMs）的性能。传统SLMs在生成回应前缺乏内部思考过程，而人类在交流前通常会进行内部推理。Stitch通过交替生成未发声的推理片段和语音回应片段，实现了在语音输出过程中同步进行内部思考。这种方法有效减少了额外延迟，同时在数学推理数据集上比基线模型高出15%的性能，并在非推理任务中表现相当。项目页面提供了相关演示和动画。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 04:30:03 GMT</pubDate>
</item>
<item>
<title>基于知识投影的网页信息检索数据合成框架WebShaper</title>
<link>https://arxiv.org/abs/2507.15061</link>
<guid>https://arxiv.org/abs/2507.15061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebShaper提升IS代理性能，实现更精准的信息检索。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为WebShaper的数据合成框架，旨在解决信息检索（IS）代理训练数据不足的问题。该框架通过集合论对IS任务进行形式化，并引入知识投影（KP）概念，以精确控制推理结构。WebShaper通过多步骤扩展过程生成复杂任务，提升了IS代理在GAIA和WebWalkerQA基准测试中的表现，达到了当前开源IS代理的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 13:53:37 GMT</pubDate>
</item>
<item>
<title>视频理解测试：评估视频大语言模型的准确性和鲁棒性</title>
<link>https://arxiv.org/abs/2507.15028</link>
<guid>https://arxiv.org/abs/2507.15028</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">视频大语言模型在视频理解上与人类存在显著差距。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Video-TT的视频理解测试，用于评估视频大语言模型（video LLMs）在真实视频中的表现是否接近人类。该测试包含1000个YouTube Shorts视频，每个视频配有1个开放性问题和4个对抗性问题，以检验模型在视觉和叙事复杂性方面的理解能力。实验结果表明，当前视频大语言模型在准确性和鲁棒性方面与人类仍有明显差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15028" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:30:33 GMT</pubDate>
</item>
<item>
<title>RLVR在推理边界扩展中的局限性研究</title>
<link>https://arxiv.org/abs/2507.14843</link>
<guid>https://arxiv.org/abs/2507.14843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVR可能限制AI发现新解，而非真正扩展推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习与可验证奖励（RLVR）在提升AI解决复杂逻辑任务中的作用。研究指出，RLVR受限于基础模型的初始概率分布，仅能对已有高奖励输出进行优化，而难以发现全新的解决方案。同时，RLVR在提高精度的同时，可能会缩小探索范围，导致遗漏原本可被基础模型找到的正确答案。实验表明，在更大的采样预算下，RLVR的实证支持范围反而缩小。此外，尽管RLVR增加了逐标记的不确定性，但最终答案的多样性却下降，表明其可能局限于少数答案。研究建议未来需通过显式探索机制或混合策略来突破这一限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 03:04:08 GMT</pubDate>
</item>
<item>
<title>开源数学推理语言模型MiroMind-M1的开发与性能评估</title>
<link>https://arxiv.org/abs/2507.14683</link>
<guid>https://arxiv.org/abs/2507.14683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiroMind-M1系列模型在数学推理任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MiroMind-M1系列开源推理语言模型，旨在提升数学推理任务的透明度和可复现性。该模型基于Qwen-2.5架构，在719,000个经过验证的数学推理问题上进行监督微调，并在62,000个挑战性问题上进行强化学习训练。为提高训练效率，作者提出了一种上下文感知的多阶段策略优化算法。实验结果显示，MiroMind-M1在AIME24、AIME25和MATH等基准测试中达到或超越现有开源模型的性能，并具有更高的token效率。研究团队还公开了完整的模型、数据集及训练配置，以支持后续研究和社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 12:21:23 GMT</pubDate>
</item>
<item>
<title>长推理下大推理模型性能下降现象研究</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">长推理导致大模型性能下降，揭示五种失败模式。</p><br /><br /><p><strong>摘要：</strong> 本文构建了评估任务，发现延长大推理模型的推理长度会降低性能，呈现出测试时计算与准确率之间的反向关系。评估任务涵盖四种类型：带干扰项的计数任务、含虚假特征的回归任务、约束跟踪的演绎任务以及高级AI风险。研究发现了五种模型在长时间推理中的失败模式，包括Claude模型被无关信息分散注意力、OpenAI o系列模型过度依赖问题框架、模型从合理先验转向虚假相关性、所有模型在复杂演绎任务中难以保持专注，以及推理延长可能加剧不良行为。研究强调了在不同推理长度下评估模型的重要性，以识别和解决这些失败模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 20:06:13 GMT</pubDate>
</item>
<item>
<title>基于单值反馈的多轮强化学习方法提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.14295</link>
<guid>https://arxiv.org/abs/2507.14295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多轮强化学习结合单值反馈提升模型推理与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多轮问题解决对大型推理模型（LRMs）的重要性，并指出传统强化学习方法在多轮任务中表现不足。研究提出一种基于单值反馈的强化学习框架（UFO），利用简单的用户反馈（如“再试一次”）来改进模型的多轮推理能力。实验表明，该方法不仅保持了单轮性能，还提升了多轮推理准确率高达14%。此外，通过设计合理的奖励机制，模型在减少回答轮次的同时，也能在出错时产生更细致和多样化的推理过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 14:07:38 GMT</pubDate>
</item>
<item>
<title>自动化生成高质量图像编辑数据集提升AI图像处理能力</title>
<link>https://arxiv.org/abs/2507.14119</link>
<guid>https://arxiv.org/abs/2507.14119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化管道生成高精度图像编辑数据，提升AI图像处理效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种自动化、模块化的图像编辑数据挖掘系统，能够跨领域、多分辨率、多风格地生成高质量的图像编辑三元组。该系统基于公开生成模型，无需人工干预，通过任务调优的Gemini验证器直接评估指令遵循度和美学质量，无需分割或定位模型。通过逆向和组合引导方法，数据量扩大约2.2倍，为大规模训练提供支持。研究团队发布了NHR-Edit数据集和Bagel-NHR-Edit模型，显著提升了图像编辑任务的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:50:00 GMT</pubDate>
</item>
<item>
<title>基于不确定性引导的渐进学习框架在CT图像分类中的应用</title>
<link>https://arxiv.org/abs/2507.14102</link>
<guid>https://arxiv.org/abs/2507.14102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UGPL提升CT图像分类准确率，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为UGPL的不确定性引导渐进学习框架，用于提高CT图像的分类准确性。该方法通过全局到局部的分析策略，首先识别诊断模糊区域，再对关键区域进行详细分析。利用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取具有空间多样性的信息片段。结合自适应融合机制，UGPL能够有效整合上下文信息与细粒度特征。实验表明，UGPL在三个CT数据集上分别提升了3.29%、2.46%和8.08%的准确率，证明其在肾脏异常、肺癌和新冠检测中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:30:56 GMT</pubDate>
</item>
<item>
<title>PhyWorldBench：评估视频生成模型物理模拟能力的基准测试</title>
<link>https://arxiv.org/abs/2507.13428</link>
<guid>https://arxiv.org/abs/2507.13428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出PhyWorldBench，用于评估视频生成模型的物理真实性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhyWorldBench，这是一个用于评估视频生成模型在物理现象模拟方面表现的全面基准。该基准涵盖从基础物理规律到复杂物体交互等多个层次的物理场景，并引入了“反物理”类别以测试模型在违反现实物理规则时的逻辑一致性。研究团队评估了12个先进的文本到视频生成模型，分析它们在不同物理场景下的表现，识别出模型在遵循真实物理规律方面的关键挑战，并提出了优化提示词以提升物理真实性的建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:54:09 GMT</pubDate>
</item>
<item>
<title>基于流式处理的4D时空几何重建方法</title>
<link>https://arxiv.org/abs/2507.11539</link>
<guid>https://arxiv.org/abs/2507.11539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种实时4D几何重建模型，提升推理速度与空间一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种流式4D视觉几何变换器，借鉴自回归大语言模型的设计理念。该模型采用因果Transformer架构，通过时间因果注意力机制和历史键值缓存实现高效的在线长期4D重建。为提升训练效率，引入从密集双向视觉几何接地Transformer（VGGT）中知识蒸馏的方法，并支持迁移高效注意力算子（如FlashAttention）。实验表明，该模型在保持性能的同时显著提升了在线场景下的推理速度，为可扩展的交互式4D视觉系统提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>RoMaP：基于3D高斯编辑的精准局部3D内容修改方法</title>
<link>https://arxiv.org/abs/2507.11061</link>
<guid>https://arxiv.org/abs/2507.11061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoMaP实现精准3D高斯局部编辑，提升3D内容质量与灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoMaP框架，解决3D高斯点云局部编辑中的挑战。通过引入3D-Geometry Aware Label Prediction模块生成鲁棒的3D掩码，结合正则化SDS损失函数，有效提升局部编辑精度和上下文一致性。实验表明，RoMaP在重建和生成场景中均达到最先进的3D局部编辑效果，为更灵活、可靠的3D内容创作提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 03:54:11 GMT</pubDate>
</item>
<item>
<title>基于几何引导的弱监督自蒸馏框架GeoDistill用于跨视角定位</title>
<link>https://arxiv.org/abs/2507.10935</link>
<guid>https://arxiv.org/abs/2507.10935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GeoDistill提升跨视角定位精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出GeoDistill，一种基于几何引导的弱监督自蒸馏框架，用于提升跨视角定位的性能。该方法通过教师-学生模型结构，利用视场角（FoV）掩码生成有限视场图像，使学生模型专注于关键特征如车道线，忽略无纹理区域。实验表明，GeoDistill在不同框架下均显著提升定位效果，并引入一种无需精确平面位置标注的相对方向估计网络，为实际应用提供高效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 23:00:15 GMT</pubDate>
</item>
<item>
<title>基于4D扩散模型的高保真人体视角合成方法</title>
<link>https://arxiv.org/abs/2507.13344</link>
<guid>https://arxiv.org/abs/2507.13344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出滑动迭代去噪方法提升4D扩散模型时空一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对稀疏视角视频输入下的人体高保真视角合成问题，提出一种新的滑动迭代去噪过程。通过在潜在网格中编码图像、相机姿态和人体姿态信息，并使用滑动窗口在空间和时间维度上交替去噪，从而增强4D扩散模型的时空一致性。该方法在DNA-Rendering和ActorsHQ数据集上表现出色，显著优于现有方法，同时保持GPU内存消耗可控。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>Voxtral Mini和Voxtral Small多模态音频聊天模型发布</title>
<link>https://arxiv.org/abs/2507.13264</link>
<guid>https://arxiv.org/abs/2507.13264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voxtral模型在音频和文本任务中表现优异，支持长对话与大上下文。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Voxtral Mini和Voxtral Small两款多模态音频聊天模型，它们能够理解和处理语音和文本信息，在多个音频基准测试中表现出色。Voxtral Small模型体积较小，可在本地运行，并支持长达40分钟的音频文件和多轮对话。研究团队还发布了三个用于评估语音理解模型的基准测试。两款模型均采用Apache 2.0许可证开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:17:37 GMT</pubDate>
</item>
<item>
<title>提升多模态大模型安全性：AutoSteer技术研究</title>
<link>https://arxiv.org/abs/2507.13255</link>
<guid>https://arxiv.org/abs/2507.13255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoSteer提升多模态大模型安全性，无需微调。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为AutoSteer的模块化、自适应推理时干预技术，旨在提升多模态大语言模型（MLLMs）在面对对抗性多模态输入时的安全性。该技术包含三个核心组件：安全意识评分（SAS）、自适应安全探测器和轻量级拒绝头。实验表明，AutoSteer在多个安全关键基准测试中显著降低了攻击成功率，同时保持了模型的通用能力。该方法为多模态AI系统的安全部署提供了一个实用、可解释且有效的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>基于残差学习的稀疏自编码器改进方法</title>
<link>https://arxiv.org/abs/2507.12990</link>
<guid>https://arxiv.org/abs/2507.12990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过残差学习提升稀疏自编码器在特定领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于残差学习的方法，用于改进稀疏自编码器（SAE）在特定领域中的表现。该方法通过训练一个辅助SAE来建模主SAE在特定文本上的重构误差，从而捕捉主模型未识别的特征。在推理阶段，将两个模型的输出相加，显著提升了跨熵和解释方差指标。实验表明，该方法能够在不牺牲通用任务性能的前提下，有效融入新领域知识，增强SAE的可解释性，为LLM的定向机制解释提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 06:57:49 GMT</pubDate>
</item>
<item>
<title>基于黎曼几何的LoRA优化方法研究</title>
<link>https://arxiv.org/abs/2507.12142</link>
<guid>https://arxiv.org/abs/2507.12142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RiemannLoRA提升LoRA收敛速度与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于黎曼几何的新型LoRA优化方法，旨在同时解决低秩矩阵分解中的过参数化问题和初始化策略问题。该方法将固定秩的LoRA矩阵视为一个光滑流形，并通过在流形上寻找损失函数下降最快的方向来实现有效初始化。实验结果表明，RiemannLoRA在大语言模型和扩散模型中均表现出更快的收敛速度和更好的最终性能，优于传统LoRA及其改进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 07:17:12 GMT</pubDate>
</item>
<item>
<title>Einstein Fields：基于神经张量场的四维时空压缩方法</title>
<link>https://arxiv.org/abs/2507.11589</link>
<guid>https://arxiv.org/abs/2507.11589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Einstein Fields通过神经张量场压缩四维相对论模拟，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Einstein Fields的神经表示方法，用于将计算密集的四维数值相对论模拟压缩为紧凑的隐式神经网络权重。该方法通过建模广义相对论的核心张量场——度规，利用自动微分推导物理量。与传统的神经场（如距离场、占用场或辐射场）不同，Einstein Fields是神经张量场，在将广义相对论的时空几何编码为神经场表示时，动态特性自然地作为副产品出现。该方法在连续时空建模、无网格性、存储效率、导数精度和易用性方面展现出巨大潜力。研究团队在多个广义相对论的标准测试案例中验证了其性能，并发布了基于JAX的开源库，为数值相对论提供了更高效和表达力更强的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:55:39 GMT</pubDate>
</item>
<item>
<title>动态视觉令牌压缩方法VisionThink提升视觉语言模型效率</title>
<link>https://arxiv.org/abs/2507.13348</link>
<guid>https://arxiv.org/abs/2507.13348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisionThink通过动态调整图像分辨率提升VLM效率与精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的视觉令牌压缩方法VisionThink，该方法根据任务复杂度动态选择图像分辨率，从而在保持高精度的同时减少视觉令牌数量。相比传统固定压缩策略，VisionThink能智能判断是否需要更高分辨率图像，并通过强化学习优化决策过程。实验表明，该方法在OCR任务中表现优异，同时在一般视觉问答任务中显著节省计算资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>pi^3：一种无需固定参考视角的视觉几何重建方法</title>
<link>https://arxiv.org/abs/2507.13347</link>
<guid>https://arxiv.org/abs/2507.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">pi^3实现无参考视角的视觉几何重建，性能领先。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了pi^3，这是一种基于前馈神经网络的新方法，用于视觉几何重建，不再依赖传统的固定参考视角。传统方法常以特定视角作为基准，若该视角不佳可能导致不稳定或失败。pi^3采用全排列等变架构，预测仿射不变的相机位姿和尺度不变的局部点图，无需参考帧。这种设计使模型对输入顺序具有鲁棒性且易于扩展，其简单无偏的方法在多种任务中达到最先进水平，包括相机位姿估计、单目/视频深度估计和密集点图重建。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>上下文工程：大型语言模型推理的系统优化方法</title>
<link>https://arxiv.org/abs/2507.13334</link>
<guid>https://arxiv.org/abs/2507.13334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">上下文工程优化LLM推理，提升复杂情境理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了上下文工程这一系统性方法，用于优化大型语言模型在推理过程中的信息输入。文章将上下文工程分解为四个基础组件：上下文检索与生成、上下文处理和上下文管理，并探讨了这些组件如何集成到智能系统中，如检索增强生成、记忆系统、工具整合推理和多智能体系统。通过对1300多篇论文的系统分析，文章不仅构建了该领域的技术路线图，还揭示了一个关键研究缺口：尽管当前模型在复杂情境理解上表现出色，但在生成高质量长文本方面仍存在明显不足。因此，未来的研究应优先解决这一不对称问题，以推动更先进的上下文感知AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:36 GMT</pubDate>
</item>
<item>
<title>基于图灵机模拟的Transformer模型长度泛化方法</title>
<link>https://arxiv.org/abs/2507.13332</link>
<guid>https://arxiv.org/abs/2507.13332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TAIL方法提升Transformer模型对长序列问题的解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Transformer模型在处理比训练时更长序列问题时的挑战，并提出一种基于图灵机模拟的模仿学习方法TAIL，以增强模型的长度泛化能力。TAIL通过生成模仿图灵机执行过程的思维链数据，扩展推理步骤并引入显式内存访问机制，从而缓解短路学习和动态数据访问难题。实验表明，TAIL在多个任务中表现优异，证明图灵机的核心概念对于模型的长度泛化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:07 GMT</pubDate>
</item>
<item>
<title>AbGen：评估大语言模型设计消融实验能力的基准</title>
<link>https://arxiv.org/abs/2507.13300</link>
<guid>https://arxiv.org/abs/2507.13300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AbGen是首个评估LLM设计消融实验能力的基准，揭示模型与人类专家的差距。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AbGen，这是首个用于评估大语言模型（LLM）在科学研究中设计消融实验能力的基准。AbGen包含1500个由专家标注的示例，来源于807篇自然语言处理论文。该基准要求LLM根据给定的研究背景生成详细的消融实验设计。对DeepSeek-R1-0528和o4-mini等先进模型的评估表明，它们在重要性、忠实性和合理性方面与人类专家存在显著差距。此外，文章指出当前自动化评估方法不可靠，与人工评估存在明显差异。为此，作者开发了AbGen-Eval，一个元评估基准，用于评估常用自动化评估系统在测量LLM性能方面的可靠性，并为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:09:22 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的多角色面部表情动画生成方法</title>
<link>https://arxiv.org/abs/2507.12956</link>
<guid>https://arxiv.org/abs/2507.12956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FantasyPortrait实现高保真多角色面部表情动画生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出FantasyPortrait，一种基于扩散Transformer的框架，能够生成高质量且富有情感的单角色和多角色面部动画。该方法通过引入表达增强学习策略，利用隐式表示捕捉与身份无关的面部动态，提升模型对细微情绪的渲染能力。针对多角色控制，设计了掩码交叉注意力机制，确保独立且协调的表情生成，避免特征干扰。为推动该领域研究，作者构建了Multi-Expr数据集和ExprBench基准测试。实验表明，FantasyPortrait在定量和定性评估中均优于现有方法，尤其在跨角色再现和多角色场景中表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 05:50:43 GMT</pubDate>
</item>
<item>
<title>AnyCap项目提升多模态生成的可控性与评估可靠性</title>
<link>https://arxiv.org/abs/2507.12841</link>
<guid>https://arxiv.org/abs/2507.12841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyCap提升多模态生成的可控性和评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnyCap项目，旨在解决现有模型在多模态生成中缺乏精细控制和可靠评估的问题。项目包括AnyCapModel（ACM），一种轻量级框架，可在不重新训练基础模型的情况下增强其可控性；AnyCapDataset（ACD），一个包含三种模态、28种用户指令类型和30万条高质量数据的数据集；以及AnyCapEval，一个新的评估基准，提供更可靠的评估指标。实验表明，ACM显著提升了多种基础模型的生成质量，在GPT-4o上内容得分提升45%，风格得分提升12%，并在多个基准测试中取得显著成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 03:04:05 GMT</pubDate>
</item>
<item>
<title>可学习分词器提升语言模型适应性</title>
<link>https://arxiv.org/abs/2507.12720</link>
<guid>https://arxiv.org/abs/2507.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FLEXITOKENS，提升语言模型的分词灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文针对语言模型在面对新数据分布时适应性差的问题，提出一种基于字节级的可学习分词器方法。传统子词分词器在适应过程中保持固定，导致分词效率低下。本文引入可学习的边界预测模块，使分词过程更具灵活性。相比现有方法，FLEXITOKENS通过简化训练目标，有效减少分词过度碎片化，并在多个多语言和形态多样任务中取得显著性能提升，最高提升达10%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 21:55:41 GMT</pubDate>
</item>
<item>
<title>MindJourney：通过世界模型提升视觉语言模型的3D空间推理能力</title>
<link>https://arxiv.org/abs/2507.12508</link>
<guid>https://arxiv.org/abs/2507.12508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MindJourney提升VLM在3D空间推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出MindJourney，一种无需微调即可增强视觉语言模型（VLM）3D空间推理能力的测试时扩展框架。该方法通过将VLM与基于视频扩散的世界模型结合，使VLM能够生成相机轨迹并合成多视角图像，从而进行更准确的空间推理。实验表明，MindJourney在SAT基准测试中平均提升了8%的性能，展示了其在提升VLM 3D理解能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>基于时序感知扩散模型的视频帧插值方法</title>
<link>https://arxiv.org/abs/2507.04984</link>
<guid>https://arxiv.org/abs/2507.04984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TLB-VFI模型，提升视频帧插值效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频帧插值任务，提出一种名为Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) 的高效视频扩散模型。该模型通过3D波浪门控和时序感知自编码器提取丰富的时序信息，在挑战性数据集上相比最新图像扩散模型提升了20%的FID表现。同时，模型参数减少3倍，推理速度提升2.3倍，并通过光流引导显著降低训练数据需求，参数量比视频扩散模型少20倍以上。项目代码和结果已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:25:32 GMT</pubDate>
</item>
<item>
<title>GitChameleon：面向库版本的代码生成评估基准</title>
<link>https://arxiv.org/abs/2507.12367</link>
<guid>https://arxiv.org/abs/2507.12367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GitChameleon提供执行验证的代码生成基准，提升AI代码生成适应性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GitChameleon，这是一个包含328个Python代码补全问题的数据集，每个问题都基于特定的库版本，并附带可执行单元测试。该数据集用于评估大型语言模型、代码助手等系统在特定版本下生成功能正确的代码的能力。实验表明，当前最先进的系统在此任务上表现不佳，成功率为48%-51%，凸显了该任务的复杂性。GitChameleon通过执行验证的方式，帮助研究人员更清晰地理解代码生成挑战，并推动更具适应性和可靠性的AI代码生成方法的发展。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 12:10:42 GMT</pubDate>
</item>
<item>
<title>基于超网络的多模态模型对齐方法</title>
<link>https://arxiv.org/abs/2507.10015</link>
<guid>https://arxiv.org/abs/2507.10015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hyma提升多模态模型选择与连接训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Hyma的新方法，用于优化多模态模型的选择和连接模块训练。该方法利用超网络的参数预测能力，能够同时为N乘M种单模态模型组合生成连接模块，从而显著降低搜索最佳模型对的成本。实验表明，Hyma在多个多模态基准测试中实现了与网格搜索相当的性能，但计算成本降低了10倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 03:51:01 GMT</pubDate>
</item>
<item>
<title>跨模态知识蒸馏框架MST-Distill的提出与实验验证</title>
<link>https://arxiv.org/abs/2507.07015</link>
<guid>https://arxiv.org/abs/2507.07015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">跨模态知识蒸馏面临路径选择和知识漂移问题，MST-Distill有效提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对跨模态知识蒸馏中存在的挑战，提出了MST-Distill框架。该框架通过引入多个专用教师模型和实例级路由网络，实现动态自适应的知识迁移。同时，设计了独立训练的掩码模块，以减少模态差异并增强知识传递效果。在五个多模态数据集上的实验表明，该方法显著优于现有方法，具有良好的泛化能力和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:28 GMT</pubDate>
</item>
<item>
<title>对比编码器与解码器语言模型的性能与适应性</title>
<link>https://arxiv.org/abs/2507.11412</link>
<guid>https://arxiv.org/abs/2507.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比编码器和解码器模型性能，发现任务适配性差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了编码器-only 和解码器-only 语言模型的性能差异。尽管大多数工作集中在解码器模型上，但编码器模型在分类和检索任务中表现更优。研究团队推出了 Ettin 套件，包含从 1700 万到 10 亿参数的成对模型，使用相同训练方法实现了 SOTA 结果。实验表明，将模型从一种架构转换到另一种任务效果不佳，且小规模编码器在分类任务中优于大规模解码器。研究开源了所有训练数据和模型检查点，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:31:51 GMT</pubDate>
</item>
<item>
<title>AI Wizards在CLEF 2025主题性检测任务中的表现</title>
<link>https://arxiv.org/abs/2507.11764</link>
<guid>https://arxiv.org/abs/2507.11764</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI Wizards通过融合情感特征提升新闻主观性检测效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AI Wizards团队在CLEF 2025 CheckThat! Lab Task 1主题性检测任务中的研究成果，旨在识别新闻文章中的主观/客观句子。该任务覆盖了单语、多语和零样本设置，并在训练/开发数据集中包含阿拉伯语、德语、英语、意大利语和保加利亚语，最终评估还引入了希腊语、罗马尼亚语、波兰语和乌克兰语等未见语言以测试模型泛化能力。研究采用基于Transformer的分类器，并通过整合情感评分与句法表示来增强模型性能。为应对类别不平衡问题，团队优化了决策阈值。实验结果表明，情感特征的引入显著提升了模型表现，尤其在主观性F1分数上效果明显，最终在希腊语任务中获得第一名。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11764" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 18:10:20 GMT</pubDate>
</item>
<item>
<title>MMHU：大规模人类行为分析基准数据集</title>
<link>https://arxiv.org/abs/2507.12463</link>
<guid>https://arxiv.org/abs/2507.12463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMHU基准，用于评估自动驾驶中的人类行为理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MMHU，一个大规模的人类行为分析基准数据集，包含57,000个动作片段和173万帧数据，涵盖多种来源如Waymo、YouTube视频和自采集数据。该数据集提供丰富的标注信息，包括动作轨迹、文本描述、意图标签等，旨在支持自动驾驶中的行为预测、生成和问答任务。通过构建人机协作的标注流程，确保数据质量，并为相关研究提供全面的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>SpatialTrackerV2：一种高效的单目视频3D点跟踪方法</title>
<link>https://arxiv.org/abs/2507.12462</link>
<guid>https://arxiv.org/abs/2507.12462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialTrackerV2提升3D点跟踪性能，运行速度更快。</p><br /><br /><p><strong>摘要：</strong> 本文提出SpatialTrackerV2，一种基于单目视频的前馈3D点跟踪方法。该方法将点跟踪、单目深度估计和相机位姿估计统一在一个端到端的架构中，能够分解世界空间中的3D运动为场景几何、相机自运动和像素级物体运动。通过在多种数据集上进行可扩展训练，SpatialTrackerV2在性能上优于现有3D跟踪方法30%，同时在动态3D重建任务中达到领先水平，且运行速度提高了50倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>SWE-Perf：首个针对代码性能优化的基准测试</title>
<link>https://arxiv.org/abs/2507.12415</link>
<guid>https://arxiv.org/abs/2507.12415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Perf评估LLM在真实代码库中优化性能的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SWE-Perf，这是首个专门用于评估大型语言模型（LLMs）在真实代码仓库中进行代码性能优化任务的基准测试。该基准包含140个精心挑选的实例，来源于GitHub上性能改进的拉取请求。每个实例包括代码库、目标函数、性能测试、专家编写的补丁和可执行环境。通过评估不同方法，研究发现现有LLMs在性能优化方面与专家水平存在显著差距，揭示了该领域的重要研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:05:17 GMT</pubDate>
</item>
<item>
<title>基于空间音频的人类运动生成方法研究</title>
<link>https://arxiv.org/abs/2507.11949</link>
<guid>https://arxiv.org/abs/2507.11949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAM数据集和MOSPA框架，实现空间音频驱动的逼真人类运动生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对虚拟人类对多样化听觉刺激的动态、真实响应这一挑战，提出了首个全面的空间音频驱动人类运动（SAM）数据集，并开发了基于扩散模型的MOSPA框架。该框架通过有效的融合机制，准确捕捉人体运动与空间音频之间的关系，能够根据不同的空间音频输入生成多样且逼真的运动。实验表明，该方法在该任务上达到了最先进的性能。相关模型和数据集将在论文接受后开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 02:33:11 GMT</pubDate>
</item>
<item>
<title>RLEP：一种用于大语言模型的强化学习框架</title>
<link>https://arxiv.org/abs/2507.07451</link>
<guid>https://arxiv.org/abs/2507.07451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLEP通过经验回放提升大语言模型的训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RLEP的强化学习框架，旨在解决大语言模型训练过程中不稳定和策略漂移的问题。该框架分为两个阶段：首先收集验证过的轨迹，然后在后续训练中进行回放。每次更新时，策略在混合新生成数据和回放成功案例的mini-batch上进行优化。RLEP通过回放高质量示例，引导模型避免无效探索，专注于有潜力的推理路径，从而加快收敛并提升最终性能。实验结果显示，在Qwen2.5-Math-7B模型上，RLEP在更少更新次数下达到基线峰值准确率，并在多个数学竞赛数据集上显著提升了准确率。相关代码、数据集和检查点已公开，便于复现和进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 01:58:55 GMT</pubDate>
</item>
<item>
<title>基于推理时扩展计算的大型语言模型训练方法</title>
<link>https://arxiv.org/abs/2507.05065</link>
<guid>https://arxiv.org/abs/2507.05065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过工具交互增强推理计算，提升代码修复能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的机器学习范式，利用推理和训练阶段的计算扩展来提升大型语言模型的能力。不同于传统的监督微调和强化学习方法，该研究将推理过程中的计算以多轮交互形式与状态化工具结合，模型通过自定义领域特定语言控制工具。实验表明，这种设置提高了经验采样速度和奖励信号密度，使参数量达30亿的模型也能高效执行任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 10:49:18 GMT</pubDate>
</item>
<item>
<title>AnyI2V：一种无需训练的视频生成框架</title>
<link>https://arxiv.org/abs/2507.02857</link>
<guid>https://arxiv.org/abs/2507.02857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyI2V支持多种条件图像和运动轨迹生成高质量视频。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为AnyI2V的新型视频生成框架，该框架无需训练即可根据用户定义的运动轨迹动画化任意条件图像。与现有方法相比，AnyI2V支持更广泛的模态输入，如网格和点云，并具备混合条件输入、风格迁移和编辑功能。实验表明，AnyI2V在空间和运动控制方面表现出色，为视频生成提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>物理引导的3D资产生成方法PhysX</title>
<link>https://arxiv.org/abs/2507.12465</link>
<guid>https://arxiv.org/abs/2507.12465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysX实现物理属性驱动的3D资产生成。</p><br /><br /><p><strong>摘要：</strong> 文章提出PhysX，一种基于物理属性的3D资产生成框架，旨在解决现有3D生成模型忽视物理特性的不足。PhysX包含两个核心部分：PhysXNet是一个首次系统标注五维物理属性的3D数据集，并通过人机协同标注流程提升效率；PhysXGen则是一个前馈式图像到3D资产生成框架，通过双分支结构建模3D结构与物理属性的关系，确保生成结果在保持几何质量的同时具备合理的物理特性。实验表明该框架性能优越且具有良好的泛化能力，相关代码、数据和模型将公开以推动物理生成AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>面向土木工程的技术图纸修订评估基准 DrafterBench</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DrafterBench用于评估LLM在技术图纸修订中的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了 DrafterBench，一个针对土木工程中技术图纸修订任务的全面评估基准。该基准包含12种任务类型、46个自定义工具和1920个任务，旨在系统评估大型语言模型代理在理解复杂指令、利用先验知识和适应动态指令质量方面的能力。DrafterBench 提供了详细的准确率分析和错误统计，以帮助深入理解AI代理的能力并为工程应用中的LLM集成提供改进方向。该基准已开源，可在GitHub和Hugging Face上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:56:04 GMT</pubDate>
</item>
<item>
<title>融合检索与推理的先进方法研究</title>
<link>https://arxiv.org/abs/2507.09477</link>
<guid>https://arxiv.org/abs/2507.09477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了RAG与推理结合的最新进展及未来方向。</p><br /><br /><p><strong>摘要：</strong> 本文对检索增强生成（RAG）与推理相结合的方法进行了全面综述，探讨了如何通过推理优化RAG的各个阶段，以及如何利用不同类型的检索知识支持复杂推理。文章还介绍了融合RAG与推理的框架，这些框架通过迭代搜索和推理实现高性能表现。作者分类整理了相关方法、数据集和挑战，并提出了未来在更有效、多模态适应、可信和以用户为中心的系统方面的研究方向。相关资源可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 23:29:41 GMT</pubDate>
</item>
<item>
<title>Lizard：一种用于无限上下文生成的线性化框架</title>
<link>https://arxiv.org/abs/2507.09025</link>
<guid>https://arxiv.org/abs/2507.09025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lizard提升LLM的长文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lizard，一种将预训练Transformer-based大型语言模型（LLM）转换为灵活、次二次架构的线性化框架，以实现无限上下文生成。Lizard通过引入次二次注意力机制，有效缓解了Transformer模型在长上下文场景下的内存和计算瓶颈。该框架结合门控线性注意力与增强元记忆的滑动窗口注意力，实现了全局上下文压缩与局部细节捕捉的平衡。同时，Lizard支持常量内存推理和更强的长度泛化能力，并通过硬件感知算法加速训练。实验表明，Lizard在标准语言建模任务中几乎恢复了教师模型的性能，并在5-shot MMLU基准上比之前方法提升了18分，在关联回忆任务中也表现出显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 17:19:18 GMT</pubDate>
</item>
<item>
<title>无需微调的视频光流提取方法</title>
<link>https://arxiv.org/abs/2507.09082</link>
<guid>https://arxiv.org/abs/2507.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于生成模型的零样本光流提取方法取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何从视频中提取光流，而无需进行特定任务的微调。受自监督视频模型启发，作者提出了一种新的测试时方法——KL-tracing，通过注入局部扰动并计算预测分布的Kullback-Leibler散度来获取光流。该方法在真实世界和合成数据集上均优于现有方法，展示了其在高质量光流估计中的潜力。研究发现，模型的三个关键特性有助于实现零样本光流提取：未来帧的分布预测、独立处理时空块的因子化潜在表示以及可随机访问的解码机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:59:38 GMT</pubDate>
</item>
<item>
<title>基于离散扩散模型的音频补全方法研究</title>
<link>https://arxiv.org/abs/2507.08333</link>
<guid>https://arxiv.org/abs/2507.08333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型音频补全方法，适用于长间隙恢复。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于离散扩散建模的音频补全方法，该方法利用预训练音频分词器生成的离散音频表示进行重建。与以往基于波形或频谱图的方法不同，该方法在离散潜在空间中直接建模生成过程，从而实现更稳定和语义连贯的音频恢复。实验在MusicNet和MTG数据集上进行，覆盖长达500毫秒的缺失片段，结果表明该方法在长间隙恢复任务中表现优于现有基线，为音乐录音修复提供了可靠方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 02:25:49 GMT</pubDate>
</item>
<item>
<title>BYOKG-RAG：一种增强知识图谱问答的框架</title>
<link>https://arxiv.org/abs/2507.04127</link>
<guid>https://arxiv.org/abs/2507.04127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BYOKG-RAG提升知识图谱问答性能，适用于自定义图谱。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BYOKG-RAG的框架，用于增强知识图谱问答（KGQA）任务。该框架结合大型语言模型（LLM）与专门的图检索工具，通过生成关键图结构元素（如问题实体、候选答案、推理路径和OpenCypher查询），并利用图工具进行链接和上下文检索，从而提高问答的准确性和泛化能力。实验表明，BYOKG-RAG在五个不同类型的基准数据集上表现优于现有方法，并展现出对自定义知识图谱的良好适应性。框架已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 14:47:14 GMT</pubDate>
</item>
<item>
<title>面向用户生成视频的多模态字幕生成基准与模型</title>
<link>https://arxiv.org/abs/2507.11336</link>
<guid>https://arxiv.org/abs/2507.11336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGC-VideoCap，提升多模态视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对用户生成视频（如TikTok）中音频与视觉信息融合不足的问题，提出了UGC-VideoCap基准和模型框架。该基准包含1000个经过三阶段人工标注的短视频，强调音频与视觉信息的平衡整合，并配有4000个QA对用于评估多模态理解能力。同时，作者开发了UGC-VideoCaptioner(3B)模型，基于Gemini 2.5 Flash进行轻量化训练，采用两阶段策略提升数据效率与性能。该研究为真实场景下的多模态视频字幕生成提供了高质量基础和解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:08:29 GMT</pubDate>
</item>
<item>
<title>基于多代理架构的视觉分类框架提升零样本场景下的AI可信度</title>
<link>https://arxiv.org/abs/2507.10571</link>
<guid>https://arxiv.org/abs/2507.10571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信任感知的多代理AI框架，提升零样本场景下的分类准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型模块化Agentic AI视觉分类框架，结合通用多模态代理、非视觉推理协调器和检索增强生成（RAG）模块，用于提升AI在零样本场景下的可信度。该框架在苹果叶片疾病诊断任务中进行了测试，结果显示使用信任感知协调器和RAG可使零样本设置下的准确率提高77.94%，达到85.63%。研究还比较了不同模型的校准效果，并通过图像-RAG技术增强了预测的可靠性。该系统将感知与元推理分离，提高了多代理AI的可扩展性和可解释性，适用于医疗诊断、生物学等对信任要求高的领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:39:29 GMT</pubDate>
</item>
<item>
<title>EXAONE 4.0：融合推理与非推理模式的多语言AI模型</title>
<link>https://arxiv.org/abs/2507.11407</link>
<guid>https://arxiv.org/abs/2507.11407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EXAONE 4.0支持多语言，具备推理与非推理模式。</p><br /><br /><p><strong>摘要：</strong> EXAONE 4.0 是一款结合了非推理模式和推理模式的AI模型，旨在提升用户体验并增强推理能力。该模型支持英语、韩语和西班牙语，包含32B和1.2B两个版本，分别适用于高性能计算和设备端应用。EXAONE 4.0在同类模型中表现优异，甚至可与前沿模型竞争，且已公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:24:51 GMT</pubDate>
</item>
<item>
<title>MISS-QA：评估模型解读科学文献示意图能力的基准测试</title>
<link>https://arxiv.org/abs/2507.10787</link>
<guid>https://arxiv.org/abs/2507.10787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MISS-QA是首个评估模型解读科学文献示意图的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MISS-QA，这是首个专门用于评估模型在科学文献中解读示意图能力的基准测试。该基准包含465篇科学论文中的1,500个专家标注示例。模型需根据示意图和论文上下文回答相关问题。研究评估了18种前沿多模态基础模型的表现，发现这些模型与人类专家之间存在显著差距。通过分析模型在无法回答问题上的表现及详细错误分析，揭示了当前模型在理解多模态科学文献方面的优缺点，为未来改进提供了关键见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 16:35:25 GMT</pubDate>
</item>
<item>
<title>LLM在恶意软件变种生成中的应用研究</title>
<link>https://arxiv.org/abs/2507.09411</link>
<guid>https://arxiv.org/abs/2507.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM可有效生成恶意软件变种，降低检测率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在修改恶意软件源代码以生成变种方面的可行性。作者提出了一种名为LLMalMorph的半自动化框架，该框架利用LLMs对代码的语义和语法理解能力，通过自定义提示和代码转换策略生成新的恶意软件变种，而无需资源密集型微调。实验中收集了10个不同类型的Windows恶意软件样本，并生成了618个变种。结果表明，这些变种在一定程度上降低了杀毒软件的检测率，同时保持了原始功能。此外，尽管未针对机器学习检测器进行优化，部分变种仍对基于ML的分类器表现出较高的攻击成功率。文章还讨论了当前LLM在生成恶意软件变种方面的局限性，并评估了该技术在恶意软件变种生成领域的现状。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 18:11:10 GMT</pubDate>
</item>
<item>
<title>基于缩放定律的大型基础模型数据混合优化方法</title>
<link>https://arxiv.org/abs/2507.09404</link>
<guid>https://arxiv.org/abs/2507.09404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种系统方法优化模型训练数据比例，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于缩放定律的方法，用于确定大型基础模型在不同目标领域下的最优数据混合比例。该方法能够准确预测模型在不同规模和数据权重下的损失表现，并在语言模型、多模态模型和视觉模型等多种大规模预训练场景中得到验证。研究还表明，这些缩放定律可以推广到新的数据组合和模型规模，仅需少量小规模训练即可估计大尺度性能，为模型训练提供了一种更高效、更系统的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 17:16:08 GMT</pubDate>
</item>
<item>
<title>OpenCodeReasoning-II数据集与代码生成及评估的改进</title>
<link>https://arxiv.org/abs/2507.09075</link>
<guid>https://arxiv.org/abs/2507.09075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCodeReasoning-II提升代码生成与评估性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenCodeReasoning-II数据集，包含250万条问题-解法-评注三元组，是目前最大的公开代码推理数据集。研究采用两阶段微调策略，分别优化代码生成和代码评估能力。实验表明，微调后的Qwen2.5-Instruct模型在代码生成方面表现优于或等同于现有最佳模型，并显著提升了竞赛编程性能。此外，还扩展了LiveCodeBench基准以支持C++语言，增强LLM评估的全面性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:35:54 GMT</pubDate>
</item>
<item>
<title>多智能体系统在复杂网络中的协作与推理能力评估</title>
<link>https://arxiv.org/abs/2507.08616</link>
<guid>https://arxiv.org/abs/2507.08616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多智能体系统的协作与推理能力，提出AgentsNet新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多智能体系统在复杂网络中的自组织与协作能力，提出了一种新的基准测试框架AgentsNet。该基准借鉴分布式系统和图论的经典问题，评估多智能体在不同网络拓扑下的策略制定、自我组织和通信能力。研究发现，尽管一些先进的大语言模型在小规模网络中表现良好，但随着网络规模扩大，性能显著下降。与现有基准最多支持2-5个智能体不同，AgentsNet可扩展至更大规模，甚至支持100个智能体的测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 10:13:22 GMT</pubDate>
</item>
<item>
<title>探究大语言模型中认知偏见的成因与影响</title>
<link>https://arxiv.org/abs/2507.07186</link>
<guid>https://arxiv.org/abs/2507.07186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大模型偏见主要源于预训练而非微调。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）中认知偏见的来源，发现这些偏见主要由预训练阶段决定，而非微调过程。研究通过两次实验方法：首先多次微调模型以观察训练随机性对偏见的影响；其次引入跨微调方法，交换不同数据集以测试偏见是否依赖于数据源。结果表明，尽管训练随机性会带来一定变化，但模型的偏见模式主要由其预训练背景决定。这一发现强调在评估和减轻模型偏见时，需关注其预训练基础，而不仅仅是微调数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:01:14 GMT</pubDate>
</item>
<item>
<title>基于预训练模型的高效视觉语言模型构建方法</title>
<link>https://arxiv.org/abs/2507.07104</link>
<guid>https://arxiv.org/abs/2507.07104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效构建视觉语言模型的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Vision-Language-Vision (VLV)的自动编码器框架，通过结合预训练的视觉编码器、文本到图像扩散模型的解码器以及大语言模型，实现高效的视觉语言模型训练。该方法通过引入信息瓶颈机制，利用连续嵌入进行知识蒸馏，实现了高质量的语义理解与图像重建。通过微调预训练语言模型生成详细描述，构建出性能接近GPT-4o和Gemini 2.0 Flash的图像描述系统。该方法大幅降低了训练成本和数据需求，总训练费用低于1000美元。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>NeuralMark：一种鲁棒的神经网络水印方法</title>
<link>https://arxiv.org/abs/2507.11137</link>
<guid>https://arxiv.org/abs/2507.11137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralMark通过哈希水印滤波提升神经网络所有权保护。</p><br /><br /><p><strong>摘要：</strong> 本文提出NeuralMark，一种基于哈希水印滤波的神经网络水印方法，旨在提高深度神经网络的所有权保护能力。该方法利用哈希函数从密钥生成不可逆二进制水印，并作为过滤器选择模型参数进行嵌入，从而增强对伪造和覆盖攻击的防御能力。同时引入平均池化以抵抗微调和剪枝攻击，且可适配多种神经网络架构。理论分析与实验验证表明，NeuralMark在13种卷积和Transformer架构中表现出良好的有效性和鲁棒性，适用于图像分类和文本生成任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 05:38:11 GMT</pubDate>
</item>
<item>
<title>CoDi框架实现文本到图像的主体一致性与姿态多样性生成</title>
<link>https://arxiv.org/abs/2507.08396</link>
<guid>https://arxiv.org/abs/2507.08396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoDi框架提升文本到图像生成的主体一致性和姿态多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为CoDi的文本到图像生成框架，旨在解决现有方法在保持主体一致性的同时缺乏姿态和布局多样性的问题。CoDi采用两阶段策略：身份传输（IT）和身份精炼（IR）。IT在早期去噪步骤中通过最优传输将身份特征以姿态感知的方式转移到目标图像，从而保持主体一致性并保留姿态多样性；IR在后期去噪步骤中选择最显著的身份特征来进一步细化主体细节。实验结果表明，CoDi在主体一致性、姿态多样性和提示保真度方面均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 04:15:56 GMT</pubDate>
</item>
<item>
<title>将大语言模型集成到非一致性逻辑的形式语义中</title>
<link>https://arxiv.org/abs/2507.09751</link>
<guid>https://arxiv.org/abs/2507.09751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种方法，将LLM融入非一致性逻辑形式语义，提升推理一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保持逻辑一致性的前提下，利用大语言模型（LLM）的广泛知识进行形式化推理。作者提出了一种直接将LLM集成到非一致性逻辑形式语义解释函数中的方法，并通过多个短文本事实性基准数据集验证了该方法的可行性。与以往工作不同，该方法提供了一个理论框架，使神经符号推理能够有效利用LLM的知识，同时保留逻辑系统的可靠性与完备性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 15:05:43 GMT</pubDate>
</item>
<item>
<title>REST：一种评估大模型多任务推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.10541</link>
<guid>https://arxiv.org/abs/2507.10541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REST框架提升大模型多任务推理评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出REST（Reasoning Evaluation through Simultaneous Testing）框架，用于评估大型推理模型在多任务压力下的表现。传统评估方法存在数据污染和无法模拟真实场景的问题，而REST通过同时测试多个问题，更全面地评估模型的上下文优先级分配、跨问题干扰抵抗和动态认知负荷管理能力。实验表明，即使是最先进的模型在REST下也会出现性能下降，且REST比现有基准更具区分力。研究还发现，采用“long2short”训练技术的模型在REST中表现更优，揭示了模型在复杂任务中的关键机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:58:47 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理能力中的作用与评估挑战</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升LLM推理能力，但需关注数据污染问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）对大型语言模型（LLMs）推理能力的提升作用，指出尽管Qwen2.5等模型在特定基准测试中表现优异，但其结果可能因数据污染而不具普遍性。研究发现，仅准确的奖励信号能有效提升性能，而随机或错误信号则无效。为解决这一问题，作者提出一个完全合成的算术问题生成器，构建了无泄漏数据集RandomCalculation，并建议在未污染基准上进行跨模型评估以确保结论可靠性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10532" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Recursions：提升语言模型效率的新框架</title>
<link>https://arxiv.org/abs/2507.10524</link>
<guid>https://arxiv.org/abs/2507.10524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture-of-Recursions 提升语言模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Mixture-of-Recursions (MoR) 的新框架，旨在同时实现参数共享和自适应计算，以提高语言模型的效率。MoR 在一个递归 Transformer 中结合了两种效率策略，通过复用共享层实现参数效率，并利用轻量级路由器动态分配不同递归深度给不同 token，从而优化计算和内存访问效率。此外，还引入了 KV 共享变体以减少预填充延迟和内存占用。实验表明，MoR 在不同模型规模下均表现出色，能够在保持较低训练 FLOPs 和较小模型尺寸的同时，显著降低验证困惑度并提升少样本准确性，同时具备更高的吞吐量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:49:00 GMT</pubDate>
</item>
<item>
<title>MoVieS：一种高效的4D动态视图合成模型</title>
<link>https://arxiv.org/abs/2507.10065</link>
<guid>https://arxiv.org/abs/2507.10065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoVieS实现单帧视频中4D动态视图的快速合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoVieS，一种新型的前馈模型，能够在一秒内从单目视频中合成4D动态新视角。该模型通过像素对齐的高斯基元网格表示动态3D场景，并显式监督其随时间变化的运动。这使得首次在单一学习框架中统一建模外观、几何和运动，实现了视图合成、重建和3D点跟踪。MoVieS通过将新视角合成与动态几何重建相结合，支持大规模训练并减少对任务特定监督的依赖，从而自然支持多种零样本应用，如场景流估计和移动物体分割。实验验证了MoVieS在多个任务中的有效性和效率，性能优异且速度显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 04:49:57 GMT</pubDate>
</item>
<item>
<title>利用ICO图像透明通道的可执行隐写术研究</title>
<link>https://arxiv.org/abs/2507.09074</link>
<guid>https://arxiv.org/abs/2507.09074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过ICO图像透明通道隐藏JavaScript代码实现隐蔽攻击。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型可执行隐写术方法，利用ICO图像文件的alpha透明通道嵌入自解压JavaScript负载。该方法通过修改非透明区域的最低有效位，在不影响视觉效果的前提下隐藏压缩后的JavaScript代码。实验表明，64x64 ICO图像可嵌入最多512字节未压缩数据或0.8KB压缩数据。浏览器在加载页面时会默认获取favicon，从而触发嵌入的加载脚本，利用原生JavaScript API和canvas像素访问在内存中执行payload，形成无需额外网络请求的隐蔽通道。测试验证了该方法在多种浏览器环境中的有效性，并分析了其对内容安全策略和杀毒软件的规避能力。文章还讨论了现有隐写分析和清理防御的局限性，揭示了静态图像与可执行内容之间界限模糊的安全风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:29:04 GMT</pubDate>
</item>
<item>
<title>构建韩国专业级大语言模型评估基准</title>
<link>https://arxiv.org/abs/2507.08924</link>
<guid>https://arxiv.org/abs/2507.08924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍两个韩国专业级大语言模型评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两个针对韩国市场的专业级大语言模型评估基准——KMMLU-Redux和KMMLU-Pro。KMMLU-Redux基于韩国国家技术资格考试题目，去除关键错误以提高可靠性；KMMLU-Pro则基于韩国国家职业执照考试，反映专业领域知识。实验表明，这些基准能够全面体现韩国工业领域的知识水平，并已公开发布数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:56:32 GMT</pubDate>
</item>
<item>
<title>结合SFT与GRPO提升大语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2507.08267</link>
<guid>https://arxiv.org/abs/2507.08267</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过延长SFT和GRPO优化提升数学模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种结合监督微调（SFT）和在线推理强化学习（GRPO）的训练方法，以提升大语言模型在数学推理任务中的准确性和效率。研究发现，延长SFT至10个epoch有助于模型达到最佳性能，而GRPO则主要用于优化解题长度。实验表明该方法在多个高难度基准测试中表现优异，包括在AI数学奥林匹克竞赛中排名靠前。作者开源了全部代码和配置，为后续研究提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08267" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 22:26:01 GMT</pubDate>
</item>
<item>
<title>DreamPoster：基于文本和图像生成高质量海报的框架</title>
<link>https://arxiv.org/abs/2507.04218</link>
<guid>https://arxiv.org/abs/2507.04218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamPoster可生成高质量海报，支持灵活布局与分辨率。</p><br /><br /><p><strong>摘要：</strong> DreamPoster是一种文本到图像生成框架，能够从用户提供的图像和文本提示中智能合成高质量海报，同时保持内容一致性，并支持灵活的分辨率和布局输出。该框架基于Seedream3.0模型构建，统一处理多种海报生成任务。在数据集构建方面，提出了系统化的数据标注流程，精确标注海报中的文字内容和排版层次信息，并构建了包含原始素材和最终海报输出的配对数据集。此外，采用渐进式训练策略，使模型逐步获得多任务生成能力，同时保持高质量生成效果。测试结果表明，DreamPoster在可用性上优于GPT-4o和SeedEdit3.0，达到88.55%。DreamPoster将上线字节跳动旗下应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 23:06:45 GMT</pubDate>
</item>
<item>
<title>EmRACE-3K数据集推动视觉语言模型在具身环境中的推理能力研究</title>
<link>https://arxiv.org/abs/2507.10548</link>
<guid>https://arxiv.org/abs/2507.10548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmRACE-3K提升VLM在具身环境中的交互能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmRACE-3K数据集，旨在解决视觉语言模型（VLMs）在具身环境中的交互与推理能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行等挑战。通过EmRACE-3K，研究者建立了评估VLM在探索、动态空间语义推理和多阶段目标执行三个维度上的基准。实验表明，现有模型在零样本设置下的成功率低于20%，凸显了该领域的挑战。研究进一步通过微调Qwen2.5-VL-7B模型，显著提升了其在各项任务中的表现，验证了EmRACE-3K的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SpeakerVid-5M数据集推动音视频双人交互虚拟人研究</title>
<link>https://arxiv.org/abs/2507.09862</link>
<guid>https://arxiv.org/abs/2507.09862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpeakerVid-5M是首个大规模音视频交互虚拟人数据集。</p><br /><br /><p><strong>摘要：</strong> 随着大模型的快速发展，数字人类领域取得了显著进展。为推动音视频双人交互虚拟人的研究，本文发布了SpeakerVid-5M数据集，包含超过8,743小时的视频片段，涵盖多种互动类型。该数据集按互动类型和数据质量分为四个类别和两个子集，适用于2D虚拟人任务。同时，还提供了基于自回归的视频聊天基线模型及评估指标，旨在为未来研究提供基准。数据集和代码将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 22:22:47 GMT</pubDate>
</item>
<item>
<title>CompassJudger-2：提升大语言模型评估能力的通用判官模型</title>
<link>https://arxiv.org/abs/2507.09104</link>
<guid>https://arxiv.org/abs/2507.09104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CompassJudger-2提升LLM评估性能，具备跨领域判断能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CompassJudger-2，一种新型的通用判官模型，旨在克服现有判官模型在专业性和鲁棒性方面的不足。该模型通过任务驱动的多领域数据收集策略，结合可验证奖励监督和拒绝采样，增强内在批判性推理能力。同时引入改进的学习目标，提升了模型表现。实验表明，CompassJudger-2在多个基准测试中表现优异，其7B版本在判断准确性上与大型模型相当。此外，作者还提出了JudgerBenchV2，用于标准化判官模型的评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 21:34:24 GMT</pubDate>
</item>
<item>
<title>基于令牌感知与层局部对比解码的真相生成方法</title>
<link>https://arxiv.org/abs/2507.04404</link>
<guid>https://arxiv.org/abs/2507.04404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的对比解码方法提升大模型事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需额外训练或模型修改的解码方法，通过将特定类型的令牌与其最相关的Transformer层进行对齐，提高事实性生成。研究发现标点符号在早期层中获得主要关注，而概念性令牌则在中间层主导语义推理。通过有选择地抑制这些令牌在各自深度的关注度，实现可控的事实退化，并利用对比信号引导最终的事实解码。实验表明该方法在多个大语言模型和基准测试中均有效提升了事实准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 10:35:43 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的视觉推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.05255</link>
<guid>https://arxiv.org/abs/2507.05255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过两阶段训练提升多模态模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究如何将大型语言模型的推理能力迁移至多模态大语言模型（MLLMs），以增强其视觉推理能力。作者提出了一种两阶段范式，首先进行大规模语言冷启动微调，然后进行近1000步的多模态强化学习，超越了以往开源模型的规模。研究揭示了三个关键发现：冷启动阶段早期就能产生行为迁移，冷启动广泛记忆视觉行为，而强化学习则能识别并扩展有效模式。最终模型Open-Vision-Reasoner（OVR）在多个推理基准测试中表现优异，如MATH500达到95.3%，MathVision达到51.8%。研究公开了模型、数据和训练过程，以推动更强大的多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>Gemini 2.X 模型家族发布：提升代码与推理能力</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini 2.5 Pro 具备超强编码与推理能力，支持3小时视频处理。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了 Gemini 2.X 模型家族，包括 Gemini 2.5 Pro、Gemini 2.5 Flash、Gemini 2.0 Flash 和 Flash-Lite。其中，Gemini 2.5 Pro 是目前最强大的模型，具备前沿的代码和推理能力，并能处理长达3小时的视频内容。它在多模态理解和推理方面表现出色，可支持复杂的自主工作流。Gemini 2.5 Flash 在计算资源和延迟要求较低的情况下仍保持优秀的推理能力，而 Gemini 2.0 Flash 和 Flash-Lite 则提供了高性能且低延迟、低成本的解决方案。整体上，Gemini 2.X 模型覆盖了从性能到成本的完整优化边界。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:36:04 GMT</pubDate>
</item>
<item>
<title>基于缓存引导的语言模型隐式控制方法</title>
<link>https://arxiv.org/abs/2507.08799</link>
<guid>https://arxiv.org/abs/2507.08799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">缓存引导提升小模型链式推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为缓存引导的轻量级方法，通过一次性的干预直接作用于键值缓存，实现对语言模型的隐式控制。该方法利用GPT-4o生成的推理轨迹构建引导向量，使模型行为更偏向于显式的多步骤推理，无需微调或修改提示。实验表明，缓存引导在多种推理基准测试中提升了模型推理质量和任务性能。相比需要持续干预的激活引导技术，缓存引导在超参数稳定性、推理效率和集成便捷性方面具有显著优势，是一种更稳健且实用的可控生成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>BlockFFN：一种高效的稀疏激活MoE架构及其加速技术</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlockFFN提升模型稀疏性并实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的混合专家（MoE）架构BlockFFN，旨在解决传统MoE在计算负载和稀疏性方面的不足。通过引入带有ReLU激活和RMSNorm的路由机制，实现了可微且灵活的路由策略。同时，设计了兼顾令牌级和块级稀疏性的训练目标，提升了模型在低资源环境下的加速性能。此外，结合激活稀疏性和推测解码技术，BlockFFN在实际端侧设备上实现了3.67倍的加速效果，实验结果表明其在稀疏性与性能方面均优于其他MoE基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:28:56 GMT</pubDate>
</item>
<item>
<title>基于视觉基础模型的图像分词器设计与优化</title>
<link>https://arxiv.org/abs/2507.08441</link>
<guid>https://arxiv.org/abs/2507.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新型图像分词器VFMTok，提升图像重建与生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文探索了利用预训练视觉基础模型构建图像分词器的新方向。通过冻结视觉模型作为编码器，并引入区域自适应量化框架和语义重建目标，提升了分词器的效率和语义一致性。所提出的VFMTok在图像重建和生成任务中表现出色，显著提高了生成质量与token效率，同时加速了模型收敛，实现了高保真类别条件合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:32:45 GMT</pubDate>
</item>
<item>
<title>评估基础模型的归纳偏置与世界模型的对齐性</title>
<link>https://arxiv.org/abs/2507.06952</link>
<guid>https://arxiv.org/abs/2507.06952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基础模型在任务中表现良好，但可能缺乏对底层世界模型的归纳偏置。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种评估基础模型的方法，通过检查其在合成数据集上的适应能力来判断其是否具备与预设世界模型一致的归纳偏置。研究发现，尽管基础模型在训练任务中表现优异，但在面对新任务时，往往未能发展出与世界模型相符的归纳偏置。特别地，在轨道轨迹训练的模型在新物理任务中无法应用牛顿力学，表现出依赖任务特定启发式的倾向，缺乏泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 11:36:15 GMT</pubDate>
</item>
<item>
<title>基于神经信号的无手图像编辑方法LoongX</title>
<link>https://arxiv.org/abs/2507.05397</link>
<guid>https://arxiv.org/abs/2507.05397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoongX利用神经信号实现无手图像编辑，性能优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出LoongX，一种基于多模态神经生理信号的无手图像编辑方法。该方法结合脑机接口和生成模型，利用EEG、fNIRS、PPG和头部运动信号捕捉用户意图，并通过跨尺度状态空间模块和动态门控融合模块整合多模态信息。实验表明，LoongX在文本驱动方法上表现相当甚至更好，尤其在结合语音时更具优势。研究展示了神经驱动生成模型在无障碍图像编辑中的潜力，并为认知驱动的创意技术提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:31:50 GMT</pubDate>
</item>
<item>
<title>基于LLM架构的自回归视频生成模型Lumos-1</title>
<link>https://arxiv.org/abs/2507.08801</link>
<guid>https://arxiv.org/abs/2507.08801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumos-1实现高效自回归视频生成，保持LLM架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lumos-1，一个基于大语言模型（LLM）架构的自回归视频生成模型。该模型通过引入MM-RoPE机制增强时空相关性，并采用令牌依赖策略优化帧内和帧间关系。为解决帧级损失不平衡问题，提出AR-DF方法，在训练中引入时间管屏蔽策略以提升生成质量。Lumos-1仅使用48块GPU即可达到与EMU3、COSMOS-Video2World和OpenSoraPlan相当的性能，展示了其在视频生成任务中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>NeuralOS：基于神经网络的GUI模拟框架</title>
<link>https://arxiv.org/abs/2507.08800</link>
<guid>https://arxiv.org/abs/2507.08800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralOS通过预测用户输入生成GUI界面，提升人机交互体验。</p><br /><br /><p><strong>摘要：</strong> NeuralOS是一个基于神经网络的框架，能够根据用户输入（如鼠标移动、点击和键盘事件）直接生成图形用户界面。该框架结合了循环神经网络（RNN）和基于扩散的神经渲染器，用于跟踪系统状态并生成屏幕图像。训练数据来自Ubuntu XFCE的大量记录，包括随机交互和AI代理生成的真实交互。实验表明，NeuralOS能够生成逼真的GUI序列，准确捕捉鼠标交互，并可靠预测应用启动等状态转换。尽管在精确建模键盘交互方面仍存在挑战，但NeuralOS为未来自适应、生成式的人机交互系统提供了重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>生成式奖励模型的脆弱性与改进方法</title>
<link>https://arxiv.org/abs/2507.08794</link>
<guid>https://arxiv.org/abs/2507.08794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式奖励模型易受表面操纵，研究提出增强方法提升鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 生成式奖励模型（LLMs-as-judges）在强化学习中被广泛用于评估答案质量，但研究发现它们对非单词符号或推理引导语句容易产生错误奖励。这种漏洞在多种模型、数据集和提示格式中普遍存在，威胁到依赖此类模型的核心算法。为解决此问题，研究引入一种数据增强策略，并训练出更稳健的奖励模型。研究强调了改进基于LLM的评估方法的重要性，并公开了相关模型和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:55:22 GMT</pubDate>
</item>
<item>
<title>基于压缩光场令牌的神经渲染方法</title>
<link>https://arxiv.org/abs/2507.08776</link>
<guid>https://arxiv.org/abs/2507.08776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效神经渲染方法CLiFT，实现高质量场景重建与视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于“压缩光场令牌（CLiFTs）”的神经渲染方法，能够保留场景的丰富外观和几何信息。该方法通过多视角编码器将图像转换为令牌，并利用潜在空间K均值算法选择关键光线作为聚类中心。随后，通过多视角“压缩器”将所有令牌的信息压缩到中心令牌中构建CLiFTs。在测试阶段，根据目标视图和计算预算收集相应数量的邻近令牌，并使用自适应计算渲染器生成新视图。实验表明，该方法在RealEstate10K和DL3DV数据集上实现了显著的数据压缩，同时保持了与现有方法相当的渲染质量，并提供了数据大小、渲染质量和速度之间的权衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:38:52 GMT</pubDate>
</item>
<item>
<title>CoPart：基于部件感知的3D生成框架</title>
<link>https://arxiv.org/abs/2507.08772</link>
<guid>https://arxiv.org/abs/2507.08772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoPart提升3D生成的细节和可控性。</p><br /><br /><p><strong>摘要：</strong> 本文提出CoPart，一种基于部件感知的3D生成框架，通过将3D对象分解为上下文相关的部件潜在表示，实现更精确的多部件生成。该方法解决了传统单潜变量表示在复杂几何结构上的不足，并增强了部件间关系建模与细粒度控制能力。研究团队还构建了Partverse数据集，支持大规模训练。实验表明，CoPart在部件级编辑、关节物体生成和场景构建方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:33:18 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型中的模态冲突与幻觉研究</title>
<link>https://arxiv.org/abs/2507.07151</link>
<guid>https://arxiv.org/abs/2507.07151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在模态冲突下的幻觉问题及缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文从模态冲突的角度探讨多模态大语言模型（MLLMs）在现实场景中产生幻觉的现象。不同于以往关注模型输出与输入之间冲突的研究，本文聚焦于不同模态输入之间的内在冲突，这些冲突直接导致了幻觉的出现。作者提出了一个名为Multimodal Modality Conflict（MMMC）的数据集来模拟这一现象，并设计了三种基于提示工程、监督微调和强化学习的方法来缓解由模态冲突引起的幻觉。实验结果表明，强化学习方法在减少幻觉方面表现最佳，而监督微调方法则表现出良好的稳定性和潜力。该研究为理解MLLMs的鲁棒性提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 07:18:38 GMT</pubDate>
</item>
<item>
<title>MetaStone-S1：基于自监督奖励模型的生成式模型</title>
<link>https://arxiv.org/abs/2507.01951</link>
<guid>https://arxiv.org/abs/2507.01951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaStone-S1通过SPRM实现与OpenAI o3相当的性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MetaStone-S1，这是首个基于自监督过程奖励模型（SPRM）的生成式模型。该模型通过共享主干网络并使用任务特定头部进行下一步预测和过程评分，将策略模型与过程奖励模型整合到统一接口中，大幅减少了PRM参数量，提升了推理效率。MetaStone-S1支持测试时缩放（TTS），提供三种推理努力模式，并实证建立了总思考计算与TTS性能之间的缩放规律。实验表明，MetaStone-S1在仅32B参数规模下，性能可与OpenAI-o3-mini系列媲美。研究团队已开源该项目。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:01 GMT</pubDate>
</item>
<item>
<title>基于动态分块机制的端到端语言模型研究</title>
<link>https://arxiv.org/abs/2507.07955</link>
<guid>https://arxiv.org/abs/2507.07955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">H-Nets实现端到端语言建模，提升数据效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的动态分块机制，通过将内容和上下文依赖的分割策略与模型联合学习，构建了一个显式的分层网络（H-Net），从而替代传统的分词-语言模型-反分词流程。实验表明，H-Nets在计算和数据匹配的情况下，优于基于BPE分词的Transformer模型，并且多级分层结构进一步提升了性能，表现出更好的数据扩展性。预训练的H-Nets在字符层面更具鲁棒性，无需启发式规则或监督即可学习有意义的分块策略。在中文、代码和DNA序列等分词较弱的语言和模态中，H-Nets表现出显著的数据效率提升，展示了端到端模型从原始数据中学习和扩展的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:39:37 GMT</pubDate>
</item>
<item>
<title>基于Re-Bottleneck的神经音频编码器结构优化</title>
<link>https://arxiv.org/abs/2507.07867</link>
<guid>https://arxiv.org/abs/2507.07867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重构瓶颈提升音频模型在不同任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Re-Bottleneck的框架，用于优化预训练神经音频编码器的潜在空间结构。该方法通过引入一个专门通过潜在空间损失训练的内部瓶颈，实现对潜在通道的排序、与语义嵌入对齐以及引入等变性，从而提升模型在不同下游任务中的性能。实验表明，该框架能够在不牺牲重建质量的前提下，有效增强模型的灵活性和适应性，适用于多种音频应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 11:47:43 GMT</pubDate>
</item>
<item>
<title>基于固定嵌入的模块化与分层扩展方法提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.07129</link>
<guid>https://arxiv.org/abs/2507.07129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出一种新型模型扩展方法，提升大模型灵活性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种不同于传统端到端训练的大语言模型扩展方法，利用不可训练的确定性输入嵌入作为基础，实现了模块化组合和分层增长。实验表明，不同数据集训练的专家模型可通过简单平均输出logits合并为更强大的混合专家模型，且在推理任务中表现优于单一模型。同时，通过逐层构建深度Transformer，验证了模型深度与复杂推理能力之间的关系。该方法为资源高效扩展、持续学习和更民主化的AI系统开发提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 16:01:15 GMT</pubDate>
</item>
<item>
<title>非语义嵌入在Transformer模型中的有效性研究</title>
<link>https://arxiv.org/abs/2507.04886</link>
<guid>https://arxiv.org/abs/2507.04886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">非语义嵌入可替代语义嵌入并提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了传统观点，即输入嵌入是语义表示的基础。研究构建了冻结嵌入层的Transformer模型，使用基于Unicode字形结构的预计算视觉嵌入。尽管这些嵌入不包含语义信息，但模型仍能收敛、生成连贯文本，并在MMLU推理基准上优于具有可训练嵌入的模型。研究认为，传统模型中嵌入层同时学习结构和语义特征导致了表征干扰，而高阶语义是Transformer架构和数据规模的涌现属性。该研究重新定义了嵌入的作用，将其从语义容器转变为结构基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title>通过动态调整预训练模型结构提升推理效率与准确性</title>
<link>https://arxiv.org/abs/2507.07996</link>
<guid>https://arxiv.org/abs/2507.07996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">预训练模型可通过动态调整结构提升推理效率和准确率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，利用预训练大型语言模型（LLM）的层作为可组合模块，构建针对每个测试样本的定制化浅层模型。通过跳过、重复或重新排列层，形成链式结构（CoLa），并采用蒙特卡洛树搜索（MCTS）优化每种样本的最佳结构。实验表明，CoLa在保持高准确率的同时，能显著提升推理效率，并在部分原本预测错误的样本中实现正确预测，展示了预训练模型在不同输入下动态适应的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视频大模型中的时空令牌合并方法STTM</title>
<link>https://arxiv.org/abs/2507.07990</link>
<guid>https://arxiv.org/abs/2507.07990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STTM提升视频理解效率，减少计算负担。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的时空令牌合并方法STTM，旨在解决视频大语言模型中因令牌数量增加而导致的计算复杂度问题。STTM通过利用视频数据中的局部空间和时间冗余，采用分层空间令牌生成和定向时间合并策略，在多个视频问答基准测试中表现出色。实验结果显示，在50%的令牌预算下，STTM实现了两倍的速度提升且仅损失0.5%的准确性；在30%预算下，速度提升三倍，仅损失2%的准确性。此外，STTM具有查询无关性，支持同一视频不同问题间的键值缓存复用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>在线时空理解基准OST-Bench推动多模态大语言模型发展</title>
<link>https://arxiv.org/abs/2507.07984</link>
<guid>https://arxiv.org/abs/2507.07984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OST-Bench评估多模态模型在线时空推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OST-Bench，一个用于评估多模态大语言模型在在线场景中进行时空理解的基准。该基准强调模型在动态环境中逐步获取信息并结合历史记忆进行推理的能力，与真实世界的沉浸式感知更贴近。基于ScanNet、Matterport3D和ARKitScenes数据集，OST-Bench包含1.4k场景和10k问答对。实验发现，现有模型在需要复杂时空推理的任务中表现不佳，随着探索范围扩大和记忆增长，准确率下降明显。研究还揭示了模型在空间线索推理和长期记忆检索方面的性能瓶颈，为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>视觉-语言模型的线性推理瓶颈与对齐优化研究</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示VLMs在抽象推理中的线性分离瓶颈及解决方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前最先进的视觉-语言模型（VLMs）在抽象推理任务中受到的线性分离瓶颈问题。通过引入线性可分性上限（LSC），研究发现这一瓶颈并非源于感知能力不足，而是语言模型推理路径的失败。研究指出该问题可通过针对性对齐解决，简单语义概念只需激活现有路径，而复杂关系推理则需要调整核心模型权重。研究还发现，尽管后缀微调能激活模型中潜在的推理路径，但在需要深度适应的复杂任务中，提升表示质量反而会导致模型在新提示格式下失效。该研究为VLM分析提供了新的视角，强调稳健推理的关键在于精准对齐而非单纯提升表示学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 05:23:32 GMT</pubDate>
</item>
<item>
<title>基于时间感知的视觉表示学习方法ToBo</title>
<link>https://arxiv.org/abs/2507.06543</link>
<guid>https://arxiv.org/abs/2507.06543</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToBo通过紧凑令牌预测动态场景，提升序列理解任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Token Bottleneck (ToBo) 的自监督学习方法，旨在从动态场景中提取紧凑且具有时间感知能力的视觉表示。该方法通过将场景压缩为一个瓶颈令牌，并利用少量目标图像块作为提示来预测后续场景，从而学习序列场景的表示。ToBo在视频标签传播和机器人操作等任务中表现出色，并在真实机器人环境中验证了其鲁棒性和有效性。此外，研究还验证了该方法在不同模型规模下的可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06543" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:57:29 GMT</pubDate>
</item>
<item>
<title>基于单图定制的扩散模型微调方法T-LoRA</title>
<link>https://arxiv.org/abs/2507.05964</link>
<guid>https://arxiv.org/abs/2507.05964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">T-LoRA解决单图定制下的过拟合问题，提升模型泛化与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出T-LoRA，一种针对扩散模型个性化定制的时步依赖低秩微调框架。在数据有限的情况下，传统微调方法容易过拟合，影响模型的泛化能力和生成多样性。T-LoRA通过动态调整不同时间步的微调策略，并采用正交初始化确保适配器组件独立性，有效缓解了这一问题。实验表明，T-LoRA在概念保真度和文本对齐之间取得了更好的平衡，适用于数据和资源受限的场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:14:10 GMT</pubDate>
</item>
<item>
<title>SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?</title>
<link>https://arxiv.org/abs/2507.05241</link>
<guid>https://arxiv.org/abs/2507.05241</guid>
<content:encoded><![CDATA[
The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:50:52 GMT</pubDate>
</item>
<item>
<title>TreeBench与TreeVGR：推动视觉基础推理的新基准与训练方法</title>
<link>https://arxiv.org/abs/2507.07999</link>
<guid>https://arxiv.org/abs/2507.07999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TreeBench评估视觉推理能力，引入TreeVGR提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉基础推理模型缺乏全面评估的问题，提出了TreeBench基准，该基准基于三个原则：对复杂场景中细微目标的精准感知、通过边界框评估的可追溯证据，以及对物体交互和空间层次的二阶推理。TreeBench包含405个具有挑战性的视觉问答对，即使最先进的模型如OpenAI-o3也仅达到54.87%的准确率。同时，文章介绍了TreeVGR训练范式，结合强化学习实现定位与推理的联合监督，显著提升了多个基准测试的性能，证明了可追溯性在视觉基础推理中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>PyVision：动态工具生成框架提升视觉推理能力</title>
<link>https://arxiv.org/abs/2507.07998</link>
<guid>https://arxiv.org/abs/2507.07998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyVision通过动态生成工具提升视觉推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PyVision，一个交互式、多轮的框架，使大语言模型能够自主生成、执行和优化基于Python的工具，从而实现灵活且可解释的问题解决。研究团队对PyVision生成的工具进行了分类，并在多个基准测试中分析了其应用效果。实验结果显示，PyVision显著提升了GPT-4.1和Claude-4.0-Sonnet的性能，表明动态工具使用不仅增强了模型的推理能力，还使其具备创造新工具的能力，推动视觉推理向更智能的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>几何引导提升视频扩散模型的3D一致性</title>
<link>https://arxiv.org/abs/2507.07982</link>
<guid>https://arxiv.org/abs/2507.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过几何引导提升视频生成模型的3D结构一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何引导（Geometry Forcing）的方法，旨在增强视频扩散模型对3D结构的理解。该方法通过将模型的中间表示与预训练几何基础模型的特征对齐，引导模型学习具有几何感知能力的潜在表示。具体包括角度对齐和尺度对齐两个目标，分别通过余弦相似度和归一化几何特征回归实现。实验结果表明，该方法在相机视角和动作条件下的视频生成任务中显著提升了视觉质量和3D一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型长视频推理能力的全栈框架</title>
<link>https://arxiv.org/abs/2507.07966</link>
<guid>https://arxiv.org/abs/2507.07966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种提升VLM长视频推理能力的全栈框架及实验验证。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全栈框架，用于扩展视觉语言模型（VLMs）在长视频上的推理能力，通过强化学习实现。该框架包含三个关键组件：一个大规模的长视频问答数据集LongVideo-Reason，一个两阶段训练流程，以及一个名为MR-SP的长视频强化学习训练基础设施。实验表明，LongVILA-R1-7B在多个长视频QA基准测试中表现优异，并在多个推理任务上超越了现有模型。此外，MR-SP系统实现了2.1倍的训练加速，支持在单个A100节点上进行长达一小时的视频强化学习训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:47:40 GMT</pubDate>
</item>
<item>
<title>机器谎言：大语言模型中虚假陈述的机制与评估</title>
<link>https://arxiv.org/abs/2507.07484</link>
<guid>https://arxiv.org/abs/2507.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型中虚假陈述现象及其机制。</p><br /><br /><p><strong>摘要：</strong> 本文提出“机器谎言”作为描述大语言模型中失去真实性的新概念，引入“谎言指数”作为衡量模型对真实性漠不关心的指标，并分析四种形式的谎言：空洞修辞、模糊言辞、含糊其辞和未经验证的声明。通过在多个数据集上的实验，研究发现强化学习微调和推理时的思维链提示会加剧谎言现象，尤其在政治语境中，模糊言辞成为主要策略。研究揭示了AI对齐中的系统性挑战，并为提升模型的真实性提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 03:11:57 GMT</pubDate>
</item>
<item>
<title>长视频生成技术的现状与分类研究</title>
<link>https://arxiv.org/abs/2507.07202</link>
<guid>https://arxiv.org/abs/2507.07202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究分析了长视频生成技术的挑战与进展。</p><br /><br /><p><strong>摘要：</strong> 本文综述了当前视频生成模型在长视频创作中的局限性，如16秒以上的视频难以保持角色一致性与场景连贯性。尽管有方法可生成长达150秒的视频，但存在帧冗余和时间多样性低的问题。研究分析了32篇相关论文，总结了关键架构组件和训练策略，并构建了一个新的分类体系，以帮助理解不同方法的设计与性能特点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:20:33 GMT</pubDate>
</item>
<item>
<title>LangSplatV2：提升3D语言场推理速度与精度的高效方法</title>
<link>https://arxiv.org/abs/2507.07136</link>
<guid>https://arxiv.org/abs/2507.07136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangSplatV2实现高速3D文本查询与特征投射。</p><br /><br /><p><strong>摘要：</strong> 本文提出LangSplatV2，通过引入稀疏系数场和CUDA优化，显著提升了3D语言场的推理速度和查询精度。相比之前的LangSplat，LangSplatV2在高分辨率图像下实现了476.2 FPS的高维特征投射和384.6 FPS的3D开放词汇文本查询，分别提升了42倍和47倍。该方法通过将每个高斯视为全局字典中的稀疏编码，消除了重型解码器的需要，从而大幅提高效率。实验结果表明，LangSplatV2不仅在查询准确性上表现优异，还在实际应用中具备更高的可行性。代码和演示可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 20:19:58 GMT</pubDate>
</item>
<item>
<title>利用扩散模型提升多模态大语言模型的视觉理解能力</title>
<link>https://arxiv.org/abs/2507.07106</link>
<guid>https://arxiv.org/abs/2507.07106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型可作为更优的视觉编码器，提升多模态模型的视觉理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将预训练文本到图像扩散模型作为指令感知的视觉编码器，以弥补传统CLIP模型在细粒度信息捕捉上的不足。通过分析扩散模型内部表示，发现其具有丰富的语义和强图像-文本对齐能力。研究还发现可通过文本条件引导模型关注与问题相关的区域，并探索了如何将这些特征与大语言模型对齐。同时揭示了信息泄露现象，并提出缓解策略。最终通过融合CLIP与扩散特征，在通用VQA和专业MLLM基准测试中验证了扩散模型在视觉任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于门控记忆单元的高效序列建模架构SambaY</title>
<link>https://arxiv.org/abs/2507.06607</link>
<guid>https://arxiv.org/abs/2507.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SambaY通过GMU提升解码效率与长上下文性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SambaY的新型解码器-混合解码器架构，引入门控记忆单元（GMU）实现跨层记忆共享，显著提升解码效率并保持线性预填充时间复杂度。该模型在不依赖显式位置编码的情况下，提升了长上下文处理能力，并在大规模计算条件下表现出更低的不可约损失。实验表明，SambaY在数学推理任务中优于现有基线模型，同时在vLLM框架下实现了10倍的解码吞吐量提升。训练代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 03:27:00 GMT</pubDate>
</item>
<item>
<title>Video-RTS：提升视频推理能力的高效强化学习方法</title>
<link>https://arxiv.org/abs/2507.06485</link>
<guid>https://arxiv.org/abs/2507.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Video-RTS通过高效RL和TTS策略提升视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Video-RTS，一种基于强化学习（RL）和视频自适应测试时缩放（TTS）策略的视频推理方法，旨在提高数据效率并减少对大规模监督微调的依赖。该方法跳过资源密集的微调步骤，采用基于输出奖励的纯RL训练，并引入稀疏到密集的TTS策略以提升推理效果。实验表明，在多个视频推理基准上，Video-RTS仅使用3.6%的训练样本便实现了比现有模型平均高出2.4%的准确率，例如在Video-Holmes和MMVU基准上分别提升了4.2%和2.6%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 22:06:13 GMT</pubDate>
</item>
<item>
<title>PERK：一种高效长上下文推理方法</title>
<link>https://arxiv.org/abs/2507.06415</link>
<guid>https://arxiv.org/abs/2507.06415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PERK提升长上下文推理性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出PERK（Parameter Efficient Reasoning over Knowledge），一种用于在测试时通过轻量级模型适配器进行参数更新的高效长上下文推理方法。PERK采用两层嵌套优化循环，在元训练阶段，内层快速将上下文编码为低秩适配器，作为基础模型的参数高效记忆模块；外层则学习利用更新后的适配器准确回忆和推理相关信息。实验表明，PERK在多个长上下文推理任务中表现优异，相比标准提示方法，小模型GPT-2平均提升90%，大模型Qwen-2.5-0.5B平均提升27%。PERK在推理复杂度、长度外推和相关信息位置方面更具鲁棒性，且推理效率优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 17:38:45 GMT</pubDate>
</item>
<item>
<title>基于分层框架的自主外科手术研究</title>
<link>https://arxiv.org/abs/2505.10251</link>
<guid>https://arxiv.org/abs/2505.10251</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出分层框架实现长期、精细的自主手术操作。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自主外科手术，针对现实世界中复杂、长时间的操作需求，提出了一种分层框架。该框架包括高层任务规划策略和低层轨迹生成策略，高层通过语言空间进行任务指导，低层则负责机器人运动控制。通过离体胆囊切除实验验证了方法的有效性，实现了100%的自主成功率，标志着向临床应用迈出重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10251" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 09:04:53 GMT</pubDate>
</item>
<item>
<title>4KAgent：一种统一的图像超分辨率通用系统</title>
<link>https://arxiv.org/abs/2507.07105</link>
<guid>https://arxiv.org/abs/2507.07105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4KAgent可将低分辨率图像提升至4K甚至更高。</p><br /><br /><p><strong>摘要：</strong> 4KAgent是一种统一的图像超分辨率通用系统，能够将任何图像提升至4K分辨率甚至更高。该系统由三个核心组件构成：用于定制处理流程的Profile模块、利用视觉语言模型和图像质量评估专家进行分析的感知代理，以及执行修复计划并遵循质量驱动策略的修复代理。此外，系统还包含专门的人脸修复管道，显著提升了肖像和自拍照片的面部细节。在11个任务类别和26个基准测试中进行了严格评估，覆盖自然图像、人像、AI生成内容、卫星图像、荧光显微镜和医学影像等广泛领域，展示了其在感知和保真度指标上的优越性能。4KAgent旨在推动低级视觉任务中自主代理的研究与创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>面向包容性内容审核的多视角毒性语言检测研究</title>
<link>https://arxiv.org/abs/2507.05455</link>
<guid>https://arxiv.org/abs/2507.05455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新数据集和模型以提升毒性语言检测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了毒性语言检测在构建安全在线环境中的重要性，指出现有模型因依赖单一标注而忽视了社区规范和语境影响。为此，研究团队创建了包含6.8K社交媒体帖子和40K标注的MODELCITIZENS数据集，并通过LLM生成对话场景增强数据。实验表明，当前主流工具在该数据集上表现不佳，而基于该数据集微调的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在分布内评估中优于GPT-o4-mini。研究强调了社区参与标注和建模对包容性内容审核的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 16:15:18 GMT</pubDate>
</item>
<item>
<title>基于多智能体的mLLM有害模因评估框架AdamMeme</title>
<link>https://arxiv.org/abs/2507.01702</link>
<guid>https://arxiv.org/abs/2507.01702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdamMeme动态评估mLLM对有害模因的理解能力。</p><br /><br /><p><strong>摘要：</strong> 随着社交媒体上多模态模因的普及，需要更有效的评估方法来衡量多模态大语言模型（mLLMs）对有害模因的理解能力。现有基准依赖静态数据集和准确率评估，难以适应快速变化的在线模因。为此，本文提出AdamMeme，一个基于多智能体的自适应评估框架，通过协作更新挑战性模因样本，全面评估mLLMs在识别有害模因方面的表现，揭示不同模型的具体弱点。实验表明，该框架能系统分析模型性能并提供细粒度分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 09:32:30 GMT</pubDate>
</item>
<item>
<title>推动文本到动作生成的零样本泛化能力</title>
<link>https://arxiv.org/abs/2507.07095</link>
<guid>https://arxiv.org/abs/2507.07095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionMillion数据集与评估框架，提升文本生成动作的零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于基于文本描述生成多样且自然的人类运动序列这一挑战性任务。尽管已有进展，但现有方法在零样本泛化方面仍存在不足，主要受限于训练数据量和缺乏全面评估体系。为此，研究团队构建了目前最大的人体运动数据集MotionMillion，包含2000小时、200万条高质量运动序列，并提出了MotionMillion-Eval作为最全面的评估基准。通过扩展模型至7B参数规模，实验验证了其在跨领域和复杂组合运动上的强大泛化能力，标志着向零样本人类运动生成迈出了重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:52:04 GMT</pubDate>
</item>
<item>
<title>FR3E框架提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2507.07017</link>
<guid>https://arxiv.org/abs/2507.07017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FR3E增强LLM推理稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FR3E（First Return, Entropy-Eliciting Explore）框架，用于改进大语言模型（LLM）的推理能力。该方法通过识别推理轨迹中的高不确定性决策点，并进行针对性的回溯，以构建语义基础的中间反馈，从而在无需密集监督的情况下提供更精准的指导。实验结果表明，FR3E能够提升训练稳定性、生成更长且连贯的响应，并提高完全正确推理路径的比例，验证了其在增强LLM推理能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:48 GMT</pubDate>
</item>
<item>
<title>提升代码生成评估的测试用例生成方法研究</title>
<link>https://arxiv.org/abs/2507.06920</link>
<guid>https://arxiv.org/abs/2507.06920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAGA方法提升测试用例质量与覆盖率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前代码生成评估基准中测试用例数量有限且同质化的问题，提出了一种多维度测试用例生成方法SAGA，结合人类编程经验和大语言模型推理能力，显著提升了测试用例的覆盖范围和质量。实验表明，该方法在TCGBench上的检测率达到90.62%，验证准确率为32.58%，并使合成评估基准的验证准确率比LiveCodeBench-v6高出10.78%。研究旨在构建更可靠的LLM代码评估体系，推动强化学习框架中的奖励估计及自动化对抗测试生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 10:58:47 GMT</pubDate>
</item>
<item>
<title>基于多模态光谱数据的分子结构生成框架DiffSpectra</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffSpectra通过扩散模型从多模态光谱数据中生成分子结构。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiffSpectra，一个基于扩散模型的生成框架，能够直接从多模态光谱数据中推断出分子的2D和3D结构。该方法将结构解析问题建模为条件生成过程，采用SE(3)-等变架构的Diffusion Molecule Transformer来整合拓扑与几何信息，并通过SpecFormer编码器捕捉多模态光谱中的依赖关系。实验表明，DiffSpectra在结构恢复任务中表现出色，具有较高的准确率。该工作是首个将多模态光谱推理与联合2D/3D生成建模相结合的分子结构解析框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 09:57:20 GMT</pubDate>
</item>
<item>
<title>混合线性注意力机制在长序列建模中的研究与优化</title>
<link>https://arxiv.org/abs/2507.06457</link>
<guid>https://arxiv.org/abs/2507.06457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究混合线性注意力模型在长序列任务中的表现与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了多种线性注意力模型在独立和混合架构中的表现，训练并开源了72个模型，涵盖不同参数规模和混合比例。实验表明，优秀的独立线性模型在混合架构中未必表现最佳，而增加全注意力层能显著提升召回性能，尤其在3:1以下比例时效果更明显。研究强调了选择性门控、层次递归和可控遗忘的重要性，并推荐HGRN-2或GatedDeltaNet等架构在3:1至6:1的混合比例下实现高效的Transformer级召回性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:54:11 GMT</pubDate>
</item>
<item>
<title>PAPO：一种增强多模态推理的感知意识强化学习方法</title>
<link>https://arxiv.org/abs/2507.06448</link>
<guid>https://arxiv.org/abs/2507.06448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAPO提升多模态任务中的视觉感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PAPO，一种基于GRPO的感知意识策略优化方法，通过引入隐式感知损失提升模型在多模态任务中的表现。PAPO无需额外数据或外部奖励模型，仅依赖内部监督信号，在多个多模态基准测试中取得显著提升，尤其在高视觉依赖任务中效果更佳。研究还发现并解决了损失黑客问题，通过双熵损失进行缓解，为视觉驱动的强化学习提供了新框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:22:34 GMT</pubDate>
</item>
<item>
<title>AutoTriton：基于强化学习的Triton编程模型</title>
<link>https://arxiv.org/abs/2507.05687</link>
<guid>https://arxiv.org/abs/2507.05687</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoTriton利用强化学习优化Triton编程，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AutoTriton，这是一个基于强化学习（RL）的Triton编程模型，旨在优化深度学习中的内核开发。通过监督微调和GRPO算法，AutoTriton能够自动调整关键参数，如tile大小和内存访问模式，从而提高计算效率。实验表明，AutoTriton在多个基准测试中表现优异，接近主流大模型。研究还验证了各个模块对性能提升的重要性，展示了强化学习在生成高性能内核方面的潜力，为构建更高效的AI系统奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05687" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 01:38:24 GMT</pubDate>
</item>
<item>
<title>解耦推理与证明：提升自动化定理证明的新框架</title>
<link>https://arxiv.org/abs/2507.06804</link>
<guid>https://arxiv.org/abs/2507.06804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出解耦推理与证明的新框架，显著提升数学难题的自动证明能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对自动化定理证明（ATP）中大型语言模型在形式化证明方面的表现不足问题，提出一种将高层推理与底层证明分离的新框架。该框架采用两个专门模型：一个用于生成多样化的子目标引理，另一个用于严格验证这些引理。实验表明，该方法在2000年后国际数学奥林匹克竞赛（IMO）难题集上成功解决了5道题目，展示了在复杂数学挑战中实现自动化推理的重要进展。研究还发布了包含大量生成和验证引理的数据集，以促进未来相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 18:38:49 GMT</pubDate>
</item>
<item>
<title>Nova Premier模型的安全评估与公开发布</title>
<link>https://arxiv.org/abs/2507.06260</link>
<guid>https://arxiv.org/abs/2507.06260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nova Premier通过安全评估，符合公开发布标准。</p><br /><br /><p><strong>摘要：</strong> Nova Premier是亚马逊最强大的多模态基础模型，支持文本、图像和视频处理，具有100万词的上下文窗口。文章介绍了基于前沿模型安全框架对Nova Premier进行的全面风险评估，重点考察了化学、生物、放射性和核（CBRN）、进攻性网络操作以及自动化AI研发等高风险领域。评估结合了自动化基准测试、专家红队测试和提升研究，结果显示Nova Premier符合2025年巴黎人工智能安全峰会的承诺，可安全公开发布。未来将继续完善安全评估和缓解流程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:33:35 GMT</pubDate>
</item>
<item>
<title>面向自动驾驶的视觉-语言-行动模型综述</title>
<link>https://arxiv.org/abs/2506.24044</link>
<guid>https://arxiv.org/abs/2506.24044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述自动驾驶中视觉-语言-行动模型的发展与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次全面回顾了面向自动驾驶的视觉-语言-行动（VLA4AD）模型。文章梳理了VLA模型的架构组成，追溯了从早期解释器到以推理为核心的模型演进过程，并对比分析了20多个代表性模型。同时，文章整合了现有数据集和评估标准，强调了驾驶安全、准确性和解释质量的综合评估。最后，文章指出了VLA4AD面临的开放性挑战，如鲁棒性、实时效率和形式化验证，并提出了未来研究方向，为可解释且符合社会规范的自动驾驶系统提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 12:50:02 GMT</pubDate>
</item>
<item>
<title>Agent KB：提升智能体错误纠正与跨领域知识复用的框架</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent KB提升智能体任务解决能力，实现跨领域知识共享。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Agent KB，这是一个基于Reason-Retrieve-Refine管道的分层经验框架，旨在提高智能体在复杂任务中的表现。该框架通过捕捉高层策略和详细执行日志，构建共享知识库，实现跨智能体的知识迁移。在GAIA基准测试中，Agent KB使成功率达到最高提升16.28个百分点，显著提升了Claude-3和GPT-4等模型的性能。此外，在SWE-bench代码修复任务中也表现出色。研究结果表明，Agent KB为智能体提供了一个模块化、框架无关的学习基础设施，使其能够从过往经验中学习并推广成功策略到新任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>NeoBabel：多语言图像生成的新范式</title>
<link>https://arxiv.org/abs/2507.06137</link>
<guid>https://arxiv.org/abs/2507.06137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBabel支持六种语言，提升图像生成的多语言性能与包容性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了NeoBabel，一种新型多语言图像生成框架，支持英语、中文、荷兰语、法语、印地语和波斯语。该模型通过大规模多语言预训练和高分辨率指令调优进行训练，在扩展的多语言基准测试中表现出色，同时保持强大的英语能力。NeoBabel在多语言任务上优于现有模型，并且模型规模更小。研究还引入了新的评估指标，以衡量多语言对齐和代码混合提示的鲁棒性。作者开源了工具包，包括代码、模型检查点、数据集和标准化评估协议，旨在推动包容性AI研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:19:45 GMT</pubDate>
</item>
<item>
<title>MedGemma：医疗视觉-语言基础模型的开发与应用</title>
<link>https://arxiv.org/abs/2507.05201</link>
<guid>https://arxiv.org/abs/2507.05201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedGemma在医疗任务中表现出色，提升AI在医疗领域的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedGemma，一个基于Gemma 3 4B和27B的医疗视觉-语言基础模型集合。MedGemma在医学图像和文本理解方面表现出色，性能优于同类模型并接近专用模型水平，同时保持了Gemma 3基础模型的通用能力。在分布外任务中，MedGemma在多模态问答、胸部X光诊断分类和代理评估中分别提升了2.6%-10%、15.5%-18.1%和10.8%。微调后进一步提升了子领域的表现，如电子健康记录信息检索错误率降低50%，并在气胸分类和组织病理学图像分类中达到现有先进方法的水平。此外，MedSigLIP作为医疗优化的视觉编码器，也展现出与专业医学图像编码器相当或更优的性能。MedGemma为医疗研究和下游应用提供了强大的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:01:44 GMT</pubDate>
</item>
<item>
<title>世界模型的理论探讨与新型架构设计</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨世界模型的定义与构建，提出新型多层级架构。</p><br /><br /><p><strong>摘要：</strong> 本文从科幻作品《沙丘》出发，结合心理学中的假设性思维概念，对当前世界模型的研究进行了批判性分析。文章指出，世界模型的核心目标是模拟现实世界的可行动可能性，以支持有目的的推理与行为。基于此，作者提出了一种基于分层、多级、连续/离散混合表示的新架构，并引入生成式自监督学习框架，展望了由该模型驱动的物理、代理和嵌套（PAN）通用人工智能系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:23:46 GMT</pubDate>
</item>
<item>
<title>无监督语义场景补全方法SceneDINO的研究</title>
<link>https://arxiv.org/abs/2507.06230</link>
<guid>https://arxiv.org/abs/2507.06230</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SceneDINO实现无监督3D场景语义补全，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SceneDINO的无监督语义场景补全方法，无需依赖昂贵的标注数据。该方法结合自监督表示学习和2D无监督场景理解技术，通过多视角一致性自监督进行训练，仅凭单张图像即可推断出3D几何结构和语义特征。通过创新的3D特征蒸馏方法，实现了无监督3D语义分割。实验表明，SceneDINO在3D和2D无监督场景理解任务中均达到最先进水平，其3D特征线性探测性能与当前有监督方法相当，并展现出良好的领域泛化能力和多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06230" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>改进Mamba模型的注意力分配机制</title>
<link>https://arxiv.org/abs/2507.06204</link>
<guid>https://arxiv.org/abs/2507.06204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何优化Mamba模型的注意力分配以提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将针对Transformer的微分设计方法应用于Mamba模型，该模型基于选择性状态空间层，具有更高的效率。研究发现，直接应用传统方法效果不佳，需进行针对性调整。为此，作者提出了一种新的微分机制，并在语言建模基准上进行了验证，结果表明该方法提升了Mamba的检索能力和整体性能。此外，通过广泛的消融实验和分析，验证了设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06204" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:30:14 GMT</pubDate>
</item>
<item>
<title>SingLoRA：一种更稳定且参数更少的低秩微调方法</title>
<link>https://arxiv.org/abs/2507.05566</link>
<guid>https://arxiv.org/abs/2507.05566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SingLoRA提升模型微调稳定性并减少参数使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SingLoRA的低秩微调方法，通过将权重更新分解为单个低秩矩阵与其转置的乘积，解决了传统LoRA中矩阵尺度不一致导致的训练不稳定问题。该设计不仅消除了矩阵间的尺度冲突，还减少了约50%的参数量。理论分析表明，SingLoRA在无限宽度神经网络框架下能保证稳定的特征学习。实验结果表明，在MNLI数据集上，SingLoRA在使用更少参数的情况下实现了比LoRA和LoRA+更高的准确率；在图像生成任务中，SingLoRA在DreamBooth数据集上的表现优于DoRA和LoRA。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:11:30 GMT</pubDate>
</item>
<item>
<title>AXLearn：模块化深度学习系统的设计与实现</title>
<link>https://arxiv.org/abs/2507.05411</link>
<guid>https://arxiv.org/abs/2507.05411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AXLearn支持异构硬件，提升模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AXLearn，一个专注于模块化和异构硬件支持的深度学习系统。相比其他先进系统，AXLearn通过严格的组件封装，实现了高效的模型开发与实验。文章提出了一种基于代码量复杂度的模块化评估方法，证明了其在扩展时保持稳定复杂度的优势。例如，在集成Rotary Position Embeddings功能时，仅需10行代码即可完成，而其他系统则需要数百行。同时，AXLearn在性能上与当前最先进的系统相当。最后，作者分享了AXLearn开发与运营的经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:50:58 GMT</pubDate>
</item>
<item>
<title>StreamVLN：一种高效的多模态视觉语言导航框架</title>
<link>https://arxiv.org/abs/2507.05240</link>
<guid>https://arxiv.org/abs/2507.05240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamVLN提升实时视觉语言导航效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamVLN，一种用于现实世界中视觉语言导航（VLN）的流式框架。该框架采用混合慢速-快速上下文建模策略，结合视觉、语言和动作输入进行多模态推理。快速对话流支持响应式动作生成，而慢速更新记忆流通过3D感知的token剪枝策略压缩历史视觉状态。这种设计实现了多轮对话的连贯性，同时保持有限的上下文大小和计算成本。实验表明，StreamVLN在VLN-CE基准测试中表现出色，具有稳定的低延迟，适用于实际部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:49:41 GMT</pubDate>
</item>
<item>
<title>LOOM-Scope：一种高效且全面的长上下文评估框架</title>
<link>https://arxiv.org/abs/2507.04723</link>
<guid>https://arxiv.org/abs/2507.04723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOOM-Scope提供标准化的长上下文评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出LOOM-Scope，一个用于评估大语言模型长上下文能力的高效且全面的框架。该框架统一了不同基准的评估设置，支持高效的长上下文推理加速方法，并引入了一个轻量级但全面的基准套件，以帮助研究者更准确地评估模型性能。由于长上下文评估的计算成本较高，LOOM-Scope旨在降低社区进行综合评估的难度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 03:33:24 GMT</pubDate>
</item>
<item>
<title>Nile-Chat系列模型：支持阿拉伯语和拉丁语的埃及方言大语言模型</title>
<link>https://arxiv.org/abs/2507.04569</link>
<guid>https://arxiv.org/abs/2507.04569</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nile-Chat系列模型提升埃及方言文本处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Nile-Chat-4B、3x4B-A6B和12B等针对埃及方言的大语言模型，这些模型能够理解和生成阿拉伯语和拉丁语文本。其中，Nile-Chat-3x4B-A6B采用Branch-Train-MiX策略，将不同脚本的专业模型融合为一个MoE模型。实验表明，这些模型在新推出的埃及语评估基准上显著优于LLaMa、Jais和ALLaM等主流模型，12B版本在拉丁语基准上比Qwen2.5-14B-Instruct高出14.4%。所有资源均已公开，为双语脚本语言的模型适配提供了系统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04569" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:53:41 GMT</pubDate>
</item>
<item>
<title>基于属性切换的公平图生成框架FAROS</title>
<link>https://arxiv.org/abs/2507.03728</link>
<guid>https://arxiv.org/abs/2507.03728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FAROS通过属性切换实现图生成的公平性与准确性平衡。</p><br /><br /><p><strong>摘要：</strong> 本文提出FAROS，一种基于属性切换机制的公平图生成框架，直接在预训练图扩散模型的生成过程中运行。该方法通过调整节点的敏感属性，在保持原始图结构特征的同时，提升生成图的公平性。实验表明，FAROS在链接预测任务中有效减少公平性差异，并在准确性和公平性之间取得更好的权衡，优于其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:31:41 GMT</pubDate>
</item>
<item>
<title>基于FLOPs的LLM重排序器效率评估方法研究</title>
<link>https://arxiv.org/abs/2507.06223</link>
<guid>https://arxiv.org/abs/2507.06223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出E2R-FLOPs评估LLM重排序器效率与效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLM）在信息检索中的重排序任务中计算资源消耗过高的问题，提出了一种新的评估方法E2R-FLOPs。该方法通过每PetaFLOP的排名指标（RPP）和每PetaFLOP的查询吞吐量（QPP）来衡量模型的效率与效果，克服了传统指标受硬件和运行设置影响的局限性。同时，构建了一个可解释的FLOPs估算器，无需实际运行即可预估模型计算量。通过广泛实验，评估了多种架构的LLM重排序器，探讨了效率与效果之间的权衡关系，旨在提升该领域的研究透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:56:28 GMT</pubDate>
</item>
<item>
<title>数据多样性在机器人操作中的关键作用研究</title>
<link>https://arxiv.org/abs/2507.06219</link>
<guid>https://arxiv.org/abs/2507.06219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据多样性对机器人学习影响显著，任务多样性更重要。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据多样性在机器人学习中的作用，重点分析了任务、机器人本体和专家演示三个维度。研究发现，任务多样性比每项任务的示范数量更为重要，有助于模型在不同场景下的迁移。此外，高质量的单一本体数据预训练模型在微调时表现出更优的扩展性，而专家多样性可能对策略学习产生干扰，尤其是速度多模态是主要因素。基于此，作者提出一种分布去偏方法，提升了模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:52:44 GMT</pubDate>
</item>
<item>
<title>潜层推理：大语言模型的多步骤推理新范式</title>
<link>https://arxiv.org/abs/2507.06203</link>
<guid>https://arxiv.org/abs/2507.06203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">潜层推理提升大语言模型的推理效率与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文综述了潜层推理这一新兴领域，探讨了如何通过模型的连续隐藏状态进行多步骤推理，从而避免依赖自然语言的限制。文章首先分析了神经网络层在推理中的基础作用，接着介绍了多种潜层推理方法，包括基于激活的递归、隐藏状态传播和微调策略。最后，讨论了通过掩码扩散模型实现无限深度潜层推理等先进范式，旨在统一潜层推理的概念框架，并为大语言模型的认知研究提供未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:29:07 GMT</pubDate>
</item>
<item>
<title>CriticLean：提升形式化语义准确性的强化学习框架</title>
<link>https://arxiv.org/abs/2507.06181</link>
<guid>https://arxiv.org/abs/2507.06181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CriticLean提升形式化语义准确性，优化自动定理证明。</p><br /><br /><p><strong>摘要：</strong> 本文提出CriticLean，一个基于强化学习的批判性评估框架，旨在提升自然语言数学陈述到形式化代码的语义准确性。通过训练CriticLeanGPT模型评估Lean 4形式化语义一致性，并构建CriticLeanBench基准测试模型区分正确与错误形式化。研究还创建了包含28.5万题的FineLeanCorpus数据集，验证了CriticLean在提升形式化可靠性方面的有效性。结果表明，优化批判阶段对生成可靠形式化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:03:39 GMT</pubDate>
</item>
<item>
<title>OmniPart：支持语义解耦与结构一致的3D对象生成框架</title>
<link>https://arxiv.org/abs/2507.06165</link>
<guid>https://arxiv.org/abs/2507.06165</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniPart实现3D对象的可编辑部件生成，提升交互应用性能。</p><br /><br /><p><strong>摘要：</strong> OmniPart是一种新型的3D对象生成框架，旨在实现组件间的高语义解耦和结构一致性。该方法将任务分为两个协同阶段：第一阶段通过自回归结构规划模块生成可控制的3D部件边界框序列，并利用灵活的2D部件掩码进行引导；第二阶段则通过空间条件校正流模型，从预训练的3D生成器中高效合成所有部件。OmniPart支持用户定义的部件粒度和精确定位，适用于多种下游应用，实验表明其性能达到当前最优水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06165" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:46:15 GMT</pubDate>
</item>
<item>
<title>Code Triangle框架评估大语言模型的编程能力</title>
<link>https://arxiv.org/abs/2507.06138</link>
<guid>https://arxiv.org/abs/2507.06138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过Code Triangle框架评估LLM的编程能力，发现其存在局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度系统评估大语言模型（LLMs）的编程能力。通过在竞赛编程基准上的实验，发现虽然LLMs能够形成自洽的系统，但其解决方案缺乏人类程序员的多样性和鲁棒性。研究揭示了模型认知与人类专家之间的显著分布差异，模型错误往往因训练数据偏差和推理迁移有限而聚集。文章建议引入人类生成的编辑指南、解决方案和多样化测试用例，以及使用模型混合方法，以显著提升LLMs的性能和鲁棒性。此外，研究还展示了LLMs在认知上的一致性与不一致性，为自我反思和自我改进提供了潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:20:43 GMT</pubDate>
</item>
<item>
<title>Tora2：多实体视频生成的运动引导模型改进</title>
<link>https://arxiv.org/abs/2507.05963</link>
<guid>https://arxiv.org/abs/2507.05963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tora2提升视频生成中外观与运动的多实体定制能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tora2，这是Tora模型的增强版本，旨在提升视频生成中外观和运动的多实体定制能力。Tora2引入了解耦个性化提取器，能够为多个开放集实体生成更细致的个性化嵌入。同时，设计了门控自注意力机制，整合轨迹、文本描述和视觉信息，减少多模态条件对齐问题。此外，通过对比损失函数优化运动动态与实体一致性。实验结果表明，Tora2在保持竞争力的同时，实现了更高级的运动控制，标志着多条件视频生成的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:11:40 GMT</pubDate>
</item>
<item>
<title>基于多轮定位的策略优化方法提升大模型视觉理解能力</title>
<link>https://arxiv.org/abs/2507.05920</link>
<guid>https://arxiv.org/abs/2507.05920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MGPO方法提升大模型在高分辨率图像中的视觉定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Multi-turn Grounding-based Policy Optimization (MGPO)的端到端强化学习框架，使大模型能够通过多轮对话自动聚焦关键视觉区域，无需额外标注。与监督微调相比，MGPO利用最终答案的正确性作为二元奖励信号，有效提升了模型的视觉定位能力。实验表明，MGPO在标准数据集上表现出色，在分布内和分布外任务中均优于现有方法，甚至超越了OpenAI的o1和GPT-4o模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 08:05:05 GMT</pubDate>
</item>
<item>
<title>GUI代理任务规划与视觉定位的优化方法</title>
<link>https://arxiv.org/abs/2507.05791</link>
<guid>https://arxiv.org/abs/2507.05791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GTA1模型解决GUI任务规划和视觉定位问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对GUI代理在任务规划和视觉定位中的两个主要挑战，提出了GTA1模型。该模型通过测试时缩放方法，在每一步采样多个动作提案，并利用判断模型选择最优方案，提升决策质量。同时，通过强化学习实现更精确的视觉元素定位。实验表明，GTA1在多个基准测试中表现优异，准确率分别达到50.1%、92.4%和67.7%，并展示了出色的代理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 04:52:18 GMT</pubDate>
</item>
<item>
<title>医学视频生成新进展：MedVideoCap-55K数据集与MedGen模型</title>
<link>https://arxiv.org/abs/2507.05675</link>
<guid>https://arxiv.org/abs/2507.05675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">医学视频生成面临挑战，MedGen模型表现优异。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成技术在开放领域取得显著进展，但医学视频生成仍处于探索阶段。医学视频在临床培训、教育和模拟中具有重要作用，需要高视觉保真度和严格的医学准确性。然而，现有模型在处理医学提示时常产生不现实或错误内容，主要原因是缺乏大规模高质量的医学数据集。为此，研究者推出了MedVideoCap-55K，这是首个大规模、多样化且带有丰富描述的医学视频数据集，包含超过5.5万条精心筛选的视频片段。基于该数据集，研究人员开发了MedGen模型，在多个基准测试中表现出色，其性能可与商业系统媲美。该研究希望推动医学视频生成领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:58:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的数据记忆现象研究</title>
<link>https://arxiv.org/abs/2507.05578</link>
<guid>https://arxiv.org/abs/2507.05578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨LLM的记忆现象及其检测与缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文综述了大型语言模型（LLMs）在训练过程中对数据的记住现象，分析了影响记忆的关键因素，如训练数据重复、训练动态和微调过程。文章还介绍了多种检测方法，包括基于前缀的提取、成员推理和对抗性提示，并评估了它们的有效性。此外，讨论了记忆带来的法律和伦理问题，以及数据清洗、差分隐私和后训练遗忘等缓解策略。最后，指出了在减少有害记忆与保持模型性能之间平衡的挑战，并提出了未来研究的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:30:46 GMT</pubDate>
</item>
<item>
<title>PRING：首个基于图级别的蛋白质相互作用预测基准</title>
<link>https://arxiv.org/abs/2507.05101</link>
<guid>https://arxiv.org/abs/2507.05101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRING是首个从图级视角评估PPI预测的基准，推动生物研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PRING，这是首个从图级别评估蛋白质-蛋白质相互作用（PPI）预测的综合基准。PRING包含21,484个蛋白质和186,818个相互作用的高质量多物种数据集，并设计了应对数据冗余和泄露的策略。该基准提出了两种互补的评估范式：拓扑导向任务和功能导向任务，涵盖PPI网络构建、蛋白复合体路径预测、GO模块分析及关键蛋白识别等。实验表明，现有PPI模型在恢复结构和功能特性方面存在局限，PRING为开发更有效的PPI预测模型提供了可靠平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>any4：一种无需预处理的4位权重量化方法</title>
<link>https://arxiv.org/abs/2507.04610</link>
<guid>https://arxiv.org/abs/2507.04610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">any4在不预处理权重或激活的情况下实现高精度4位量化。</p><br /><br /><p><strong>摘要：</strong> 本文提出any4，一种无需预处理权重或激活的4位权重量化方法，能够提供任意数值表示。实验表明，any4在多个模型（如Llama 2、Llama 3、Mistral和Mixtral）上相比int4、fp4和nf4等其他4位表示方法具有更高的准确性。此外，any4在不需要预处理的情况下仍能与需要预处理的技术（如AWQ和GPTQ）相媲美。研究还探索了any3和any2在更低比特下的表现，并展示了仅需一个精心挑选的多样化样本即可进行校准，而非传统方法所需的数百个样本。同时，作者开源了tinygemm，一个针对LLM优化的GPU矩阵乘法库，支持any4及其他常见量化方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 21:59:47 GMT</pubDate>
</item>
<item>
<title>基于LLM的网络代理计算资源优化研究</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出优化LLM网络代理的计算分配策略，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大语言模型（LLM）的网络代理在开放源代码系统中的发展滞后问题，提出了一种统计基础的计算资源分配方法。通过两阶段流程，使用Llama 3.1 8B模型模仿Llama 3.3 70B教师模型，并结合监督微调和在线策略强化学习，显著提升了性能。研究发现该方法对超参数高度敏感，因此通过采样1,370个配置并利用自助法估计有效参数，避免了昂贵的试错过程。实验表明，该策略在WorkArena和MiniWob++上优于单独使用SFT或RL，且仅需55%的计算资源即可达到纯SFT的最佳性能，有效缩小了与闭源模型的差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 13:12:33 GMT</pubDate>
</item>
<item>
<title>SAMed-2：基于SAM-2架构的医学图像分割基础模型</title>
<link>https://arxiv.org/abs/2507.03698</link>
<guid>https://arxiv.org/abs/2507.03698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAMed-2提升医学图像分割性能，解决数据噪声与任务迁移问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAMed-2，一个基于SAM-2架构的医学图像分割基础模型。为应对医学数据复杂性、标注噪声及多模态学习挑战，SAMed-2引入时间适配器和置信度驱动的记忆机制，以捕捉图像相关性并存储高置信特征。研究构建了MedBank-100k数据集，涵盖七种成像模态和21项分割任务。实验表明，SAMed-2在多任务场景中优于现有最佳方法，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:30:38 GMT</pubDate>
</item>
<item>
<title>基于可验证情感奖励的强化学习框架提升语言模型情感智能</title>
<link>https://arxiv.org/abs/2507.03112</link>
<guid>https://arxiv.org/abs/2507.03112</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVER框架提升LLM情感智能，增强对话能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLVER，一个端到端的强化学习框架，利用模拟用户的可验证情感奖励来提升大型语言模型的情感智能。通过与自我一致的情感模拟用户进行对话轮次，生成确定性情感评分作为奖励信号，引导模型学习。在Qwen2.5-7B-Instruct模型上使用PPO微调后，Sentient-Benchmark得分从13.3提升至79.2，同时保持数学和编码能力。实验表明，RLVER能持续提升多种对话能力，思考型模型在共情和洞察力上表现更优，而非思考型模型则偏向行动导向。不同优化算法（如GRPO和PPO）对性能有不同影响，且中等难度环境可能带来更优结果。研究证明RLVER是构建情感智能、全面能力语言代理的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03112" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 14:33:18 GMT</pubDate>
</item>
<item>
<title>面向记忆代理的基准测试MemoryAgentBench</title>
<link>https://arxiv.org/abs/2507.05257</link>
<guid>https://arxiv.org/abs/2507.05257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MemoryAgentBench评估LLM代理的记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文指出，当前大型语言模型代理的基准测试主要关注推理、规划和执行能力，而对记忆能力的评估不足。作者定义了记忆代理的四项核心能力：准确检索、测试时学习、长程理解与冲突解决，并指出现有数据集无法满足这些需求。为此，作者提出了MemoryAgentBench，一个专门针对记忆代理的新基准，结合了重构和新构建的数据集，全面覆盖上述四项能力，为评估记忆质量提供系统化测试平台。实验表明，现有方法在掌握所有能力方面仍有不足，凸显了进一步研究综合记忆机制的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于强化学习的实体关系抽取方法研究</title>
<link>https://arxiv.org/abs/2507.04642</link>
<guid>https://arxiv.org/abs/2507.04642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-RE提升关系抽取的跨领域泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文将关系抽取任务重新定义为受标注指南引导的推理过程，提出R1-RE框架，利用可验证奖励的强化学习方法提升模型的跨领域泛化能力。实验结果显示，R1-RE-7B模型在Sem-2010和MDKG数据集上的平均OOD准确率约为70%，与GPT-4o等先进模型相当。研究还深入分析了RLVR范式在关系抽取中的训练动态和涌现推理行为，提供了新的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 23:50:59 GMT</pubDate>
</item>
<item>
<title>基于Llama 3.2 1B的隐私保护医疗转录系统研究</title>
<link>https://arxiv.org/abs/2507.03033</link>
<guid>https://arxiv.org/abs/2507.03033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种隐私保护的医疗转录系统，提升临床记录效率。</p><br /><br /><p><strong>摘要：</strong> 本文旨在开发一种隐私保护、可在设备端运行的医疗转录系统，采用微调后的Llama 3.2 1B模型，将医学转录内容转化为结构化医疗笔记。通过参数高效微调方法（LoRA）在1,500对合成数据上训练，并在两个数据集上评估其性能。结果显示，该系统在ROUGE和BERTScore等指标上显著优于基础模型，同时减少了重大幻觉问题并提高了事实准确性。该方法有助于解决医疗AI应用中的隐私、成本和可访问性问题，适用于资源有限的环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 21:51:49 GMT</pubDate>
</item>
<item>
<title>视觉嵌入模型中的有序属性捕捉研究</title>
<link>https://arxiv.org/abs/2507.03683</link>
<guid>https://arxiv.org/abs/2507.03683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现视觉嵌入模型可捕捉连续有序属性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉嵌入模型是否能通过线性方向（称为rank axes）捕捉连续的有序属性。研究发现，许多流行的编码器在多个数据集上具备这种能力，即通过投影嵌入可以保持属性的顺序。令人惊讶的是，只需少量样本或两个极端例子即可恢复有意义的rank axes，而无需全面监督。这一发现为图像排序在向量数据库中的应用提供了新可能，并推动了对可排序嵌入结构和学习方法的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:03:31 GMT</pubDate>
</item>
<item>
<title>UnMix-NeRF：结合光谱解混的神经辐射场方法</title>
<link>https://arxiv.org/abs/2506.21884</link>
<guid>https://arxiv.org/abs/2506.21884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UnMix-NeRF通过光谱解混提升材料分割与视图合成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UnMix-NeRF，一种将光谱解混引入神经辐射场（NeRF）的框架，实现联合高光谱新视角合成与无监督材料分割。该方法通过建模光谱反射率，利用全局端元字典表示纯材料特征，并通过每点丰度捕捉其分布。材料分割基于学习端元的光谱预测，实现无监督聚类。此外，UnMix-NeRF支持场景编辑，通过修改端元字典实现灵活的材料外观操控。实验表明，该方法在光谱重建和材料分割方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:42:49 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的图像编辑系统X-Planner</title>
<link>https://arxiv.org/abs/2507.05259</link>
<guid>https://arxiv.org/abs/2507.05259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Planner提升文本引导图像编辑的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Planner，一个基于多模态大语言模型的图像编辑系统，旨在解决现有方法在理解复杂指令、保持身份一致性和避免意外编辑方面的不足。X-Planner通过链式思维推理将复杂指令分解为简单子指令，并自动生成精确的编辑类型和分割掩码，无需人工干预。此外，作者还构建了一个大规模数据生成管道，使X-Planner在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于策略判别器的奖励建模方法POLAR及其性能提升</title>
<link>https://arxiv.org/abs/2507.05197</link>
<guid>https://arxiv.org/abs/2507.05197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">POLAR通过策略判别提升奖励模型性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的奖励建模方法POLAR，将奖励建模视为策略判别器，通过量化两个策略之间的差异来生成奖励信号，从而引导训练策略向目标策略靠拢。与依赖绝对偏好的传统方法不同，POLAR关注策略间的相对差异，适用于通用排序关系建模。实验表明，POLAR在多个任务中显著提升了奖励模型的性能，如在STEM任务中偏好准确率从54.8%提升至81.0%，在创意写作任务中从57.9%提升至85.5%。此外，POLAR在RLHF中的表现也十分稳健，提升了多个大模型的性能。研究还发现计算量与性能之间存在明显的幂律关系，表明POLAR具有良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:56:31 GMT</pubDate>
</item>
<item>
<title>基于低帧率相机的高速4D捕捉系统</title>
<link>https://arxiv.org/abs/2507.05163</link>
<guid>https://arxiv.org/abs/2507.05163</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需高速相机的高帧率4D重建方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于低帧率相机的高速4D捕捉系统，通过异步拍摄和生成模型提升重建效果。在拍摄端，采用异步采集方案，通过分组相机和基础帧率实现等效100-200 FPS的高帧率。在处理端，引入基于视频扩散的修复模型，解决稀疏视角重建中的伪影问题，提升细节精度和时间一致性。实验表明，该方法在高速4D重建上优于同步采集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05163" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:18:35 GMT</pubDate>
</item>
<item>
<title>自动化历史文献修复方法与全页数据集研究</title>
<link>https://arxiv.org/abs/2507.05108</link>
<guid>https://arxiv.org/abs/2507.05108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoHDR方法提升历史文献修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种全页历史文献修复数据集（FPHDR）和自动化修复方法（AutoHDR），旨在解决传统修复方法在多模态和大规模修复上的不足。FPHDR包含真实和合成图像，涵盖不同损伤等级的字符和行级标注。AutoHDR通过OCR辅助定位损伤、视觉语言文本预测和补丁自回归修复三个阶段模拟历史学家的工作流程，并支持人机协作。实验表明，该方法显著提升了OCR识别准确率，从46.83%提升至84.05%，协作后更达94.25%。该研究对文化遗产保护具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:26:17 GMT</pubDate>
</item>
<item>
<title>ArtifactsBench：自动化评估视觉代码生成的新基准</title>
<link>https://arxiv.org/abs/2507.04952</link>
<guid>https://arxiv.org/abs/2507.04952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtifactsBench实现对视觉代码生成的多模态自动评估。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ArtifactsBench，这是一个用于自动化、多模态评估视觉代码生成的新基准和范式。该框架通过程序化渲染生成的视觉制品并捕获其动态行为，结合源代码由多模态大语言模型进行评估。研究构建了1,825个多样化任务，并评估了30多个领先的大语言模型。实验表明，ArtifactsBench在排名一致性上与WebDev Arena高度一致，且与人类专家有超过90%的匹配度，证明其可大规模可靠评估用户感知质量。分析显示通用模型通常优于领域特定模型。作者开源了ArtifactsBench及其相关资源，以推动以用户为中心的生成模型发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 08:53:00 GMT</pubDate>
</item>
<item>
<title>VLM2Vec-V2：跨多种视觉形式的统一嵌入框架</title>
<link>https://arxiv.org/abs/2507.04590</link>
<guid>https://arxiv.org/abs/2507.04590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLM2Vec-V2支持多模态输入并提升嵌入性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出VLM2Vec-V2，一种支持文本、图像、视频和视觉文档输入的统一嵌入框架。为弥补现有模型对非自然图像支持不足的问题，研究团队构建了MMEB-V2基准，涵盖视觉文档检索、视频检索等五种新任务。实验表明，VLM2Vec-V2在视频和文档检索任务中表现优异，并优于现有基线模型。该研究为多模态嵌入学习提供了有效策略，推动了更广泛的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 20:51:57 GMT</pubDate>
</item>
<item>
<title>大型语言模型在预测任务中的表现评估</title>
<link>https://arxiv.org/abs/2507.04562</link>
<guid>https://arxiv.org/abs/2507.04562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型预测能力仍不及超级预测者。</p><br /><br /><p><strong>摘要：</strong> 本文评估了当前最先进的大型语言模型在464个Metaculus预测问题上的表现，发现尽管这些模型的Brier得分看似超过普通人群，但与超级预测者相比仍有显著差距。这表明虽然大型语言模型在多种任务中表现出色，但在未来事件预测方面仍需进一步提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:26:59 GMT</pubDate>
</item>
<item>
<title>DreamVLA：融合世界知识预测的视觉-语言-动作框架</title>
<link>https://arxiv.org/abs/2507.04447</link>
<guid>https://arxiv.org/abs/2507.04447</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamVLA提升机器人操作的泛化与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DreamVLA，一种新型的视觉-语言-动作框架，通过整合全面的世界知识预测，实现逆动力学建模，构建感知-预测-行动循环。该框架引入动态区域引导的世界知识预测，并结合空间和语义线索，提供紧凑而全面的动作规划表示。为减少训练过程中动态、空间和语义信息的干扰，采用块状结构注意力机制，防止信息泄露。此外，使用基于扩散的Transformer模型对未来的动作分布进行建模。实验表明，DreamVLA在真实机器人任务中取得76.7%的成功率，在CALVIN ABC-D基准测试中平均长度为4.44。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04447" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 12:14:29 GMT</pubDate>
</item>
<item>
<title>MOD-X：面向异构智能体的模块化开放去中心化交换框架</title>
<link>https://arxiv.org/abs/2507.04376</link>
<guid>https://arxiv.org/abs/2507.04376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOD-X 提供一种新型智能体互操作架构，提升异构系统集成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出 MOD-X（Modular Open Decentralized eXchange）框架，旨在解决当前人工智能系统中智能体之间通信协议不足的问题。MOD-X 采用分层架构，包含通用消息总线、状态管理、翻译能力和基于区块链的安全机制，支持不同架构、厂商和知识表示的异构智能体之间的集成。其核心创新包括发布-订阅通信模型、语义能力发现和动态工作流编排，为实现真正去中心化、可扩展的智能体生态系统提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 08:46:57 GMT</pubDate>
</item>
<item>
<title>SeqTex：基于视频预训练模型的端到端3D纹理生成框架</title>
<link>https://arxiv.org/abs/2507.04285</link>
<guid>https://arxiv.org/abs/2507.04285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeqTex直接生成UV纹理图，提升3D一致性与真实感。</p><br /><br /><p><strong>摘要：</strong> 本文提出SeqTex，一个端到端的3D纹理生成框架，利用预训练视频模型的视觉知识直接生成完整的UV纹理图。不同于以往方法依赖多视角图像和后处理，SeqTex将任务转化为序列生成问题，学习多视角渲染与UV纹理的联合分布，从而有效迁移图像空间先验至UV域。通过解耦多视角与UV分支、几何引导注意力以及自适应令牌分辨率等创新设计，SeqTex在无需后处理的情况下生成高质量UV纹理，实验表明其在图像和文本条件下的3D纹理生成任务中均达到最优性能，具有更强的3D一致性、纹理-几何对齐性和现实泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 03:58:36 GMT</pubDate>
</item>
<item>
<title>PresentAgent：将长文档转化为同步视听演示的多模态代理</title>
<link>https://arxiv.org/abs/2507.04036</link>
<guid>https://arxiv.org/abs/2507.04036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PresentAgent生成同步视听演示视频，提升信息传达效果。</p><br /><br /><p><strong>摘要：</strong> PresentAgent是一种多模态代理，能够将长文档转化为同步的视觉和语音演示视频。与现有方法仅生成静态幻灯片或文本摘要不同，PresentAgent通过模块化流程生成高质量的视觉帧和上下文相关的语音叙述，并实现精确的音画同步。为评估其输出质量，研究者引入了PresentEval框架，该框架基于视觉-语言模型对视频内容进行综合评分。实验表明，PresentAgent在多项指标上接近人类水平，展示了可控多模态代理在动态展示静态文本材料方面的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 09:24:15 GMT</pubDate>
</item>
<item>
<title>基于GUI的统一数据集合成框架Easy Dataset提升领域语言模型性能</title>
<link>https://arxiv.org/abs/2507.04009</link>
<guid>https://arxiv.org/abs/2507.04009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Easy Dataset通过GUI合成高质量数据，提升LLM领域适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Easy Dataset，一个通过图形用户界面从非结构化文档中合成微调数据的统一框架。用户可配置文本提取模型和分块策略，将原始文档转化为连贯文本片段，并利用基于角色的提示方法生成多样化的问答对。过程中的人机交互界面有助于审查和优化中间结果，确保数据质量。实验表明，使用该框架生成的数据集能显著提升语言模型在金融问答任务中的表现，同时保持其通用知识。项目已在GitHub上开源，获得超过9000星标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 07:38:59 GMT</pubDate>
</item>
<item>
<title>StreamDiT：实现实时视频生成的流式模型</title>
<link>https://arxiv.org/abs/2507.03745</link>
<guid>https://arxiv.org/abs/2507.03745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamDiT实现实时视频生成，支持交互与流媒体应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamDiT，一种基于流式处理的视频生成模型，解决了现有模型仅能生成短片段的问题。通过引入流动缓冲区和混合训练策略，提升了内容一致性和视觉质量。模型采用自适应层归一化DiT结构，并结合多步蒸馏方法，显著降低了计算量，使模型在单块GPU上达到16 FPS的实时性能，支持512p分辨率的视频流生成。实验表明，该模型在定量指标和人类评估中均表现优异，适用于实时视频生成和交互场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 14:00:01 GMT</pubDate>
</item>
<item>
<title>MemOS：面向持续学习与个性化建模的内存操作系统</title>
<link>https://arxiv.org/abs/2507.03724</link>
<guid>https://arxiv.org/abs/2507.03724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MemOS提升LLM长期推理与知识一致性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在长期上下文推理、持续个性化和知识一致性方面的挑战，指出现有模型依赖静态参数和短期上下文状态，难以有效管理用户偏好和更新知识。尽管RAG引入了外部知识，但缺乏生命周期管理和持久化整合。为此，作者提出MemOS，一个将内存作为可管理资源的操作系统，统一表示、调度和演化文本、激活和参数级记忆，通过MemCube实现灵活的知识迁移与融合，提升计算效率并支持持续学习和个性化建模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:21:46 GMT</pubDate>
</item>
<item>
<title>基于Transformer的软件漏洞严重性预测模型VLAI</title>
<link>https://arxiv.org/abs/2507.03607</link>
<guid>https://arxiv.org/abs/2507.03607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLAI模型准确预测软件漏洞严重性，提升漏洞分类效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLAI，一个基于Transformer架构的模型，能够直接从文本描述中预测软件漏洞的严重性等级。该模型基于RoBERTa进行微调，使用超过60万条真实漏洞数据进行训练，预测准确率超过82%，可显著提升漏洞优先级排序的效率，减少对人工CVSS评分的依赖。VLAI模型和相关数据集已开源，并集成到Vulnerability-Lookup服务中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 10:28:14 GMT</pubDate>
</item>
<item>
<title>BMMR：多语言、多模态、跨学科推理数据集的构建与应用</title>
<link>https://arxiv.org/abs/2507.03483</link>
<guid>https://arxiv.org/abs/2507.03483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BMMR是一个跨学科的多模态推理数据集，用于评估和训练大型多模态模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BMMR，这是一个大规模的双语、多模态、跨学科推理数据集，旨在支持大型多模态模型（LMMs）的研究与评估。该数据集包含11万道大学水平的题目，覆盖300个联合国教科文组织定义的学科，涵盖多种题型，并从纸质和数字媒体中获取。数据经过人工与自动化框架筛选，每个实例都配有高质量的推理路径。数据集分为BMMR-Eval（20,458个高质量实例）和BMMR-Train（88,991个实例），用于评估和研究。此外，作者还提出了BMMR-Verifier，用于精确评估推理过程。实验表明，即使是最先进的模型在BMMR-Eval上仍有提升空间，且不同学科表现差异明显。开源模型仍落后于专有模型，但通过微调可缩小差距。研究揭示了当前LMMs在跨学科推理中的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 07:20:09 GMT</pubDate>
</item>
<item>
<title>DiaFORGE提升企业API调用成功率的对话框架研究</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiaFORGE提高企业API调用成功率27%-49%</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DiaFORGE，一种以消除歧义为核心的三阶段对话框架，旨在解决大型语言模型在调用企业API时因工具相似或参数不明确而失败的问题。该框架通过生成多轮对话、进行监督微调以及动态评估模型表现，显著提升了工具调用的成功率。在动态基准测试中，使用DiaFORGE训练的模型比GPT-4o和Claude-3.5-Sonnet分别高出27个百分点和49个百分点。同时，作者发布了包含5000个企业级API规范及验证过的对话数据集，为构建可靠的工具调用代理提供了实用蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 02:49:02 GMT</pubDate>
</item>
<item>
<title>RefineX：大规模预训练数据的精准优化框架</title>
<link>https://arxiv.org/abs/2507.03253</link>
<guid>https://arxiv.org/abs/2507.03253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefineX提升预训练数据质量，增强模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RefineX，一种用于大规模预训练数据精准优化的框架。该方法通过程序化编辑任务实现细粒度数据修正，同时保持文本的多样性和自然性。RefineX将高质量的专家指导结果提炼为最小化的删除指令，构建高效可靠的精炼模型，可在不同规模模型上显著提升下游任务表现。实验表明，RefineX在多个基准测试中优于原始数据、过滤数据或替代方法，具有高效率和精确度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 22:19:58 GMT</pubDate>
</item>
<item>
<title>OmniDraft：一种支持多模型协同的高效推测解码框架</title>
<link>https://arxiv.org/abs/2507.02659</link>
<guid>https://arxiv.org/abs/2507.02659</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniDraft实现单一模型适配多种目标模型并提升推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniDraft，一种统一框架，使单一draft模型能够与任何目标模型协作，并动态适应用户数据。通过在线n-gram缓存和混合蒸馏微调解决词汇不匹配问题，并利用自适应推测技术提升解码速度。该框架适用于设备端大语言模型应用，展示了在数学推理、编程和文本生成任务中的有效性，可使Llama-68M模型与多个目标模型如Vicuna-7B、Qwen2-7B和Llama3-8B进行推测解码，并带来1.5-2倍的速度提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02659" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:20:41 GMT</pubDate>
</item>
<item>
<title>RoboBrain 2.0：面向物理环境的多模态AI模型</title>
<link>https://arxiv.org/abs/2507.02029</link>
<guid>https://arxiv.org/abs/2507.02029</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboBrain 2.0是先进的多模态AI模型，支持空间与时间推理。</p><br /><br /><p><strong>摘要：</strong> RoboBrain 2.0是最新一代的具身视觉-语言基础模型，旨在统一感知、推理和规划以完成复杂的物理环境任务。该模型包含两个版本：轻量级7B模型和全尺寸32B模型，采用异构架构，结合视觉编码器和语言模型。尽管体积较小，RoboBrain 2.0在多种具身推理任务中表现出色。32B版本在空间和时间基准测试中均取得领先结果，超越了之前的开源和专有模型。它支持关键的现实世界具身AI能力，如空间理解（例如功能预测、空间指代、轨迹预测）和时间决策（例如闭环交互、多智能体长期规划、场景图更新）。本文详细介绍了模型架构、数据构建、多阶段训练策略、基础设施及实际应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02029" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:05:33 GMT</pubDate>
</item>
<item>
<title>对比MLM与CLM在文本表示学习中的效果与优化策略</title>
<link>https://arxiv.org/abs/2507.00994</link>
<guid>https://arxiv.org/abs/2507.00994</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较MLM与CLM在文本表示上的表现及效率。</p><br /><br /><p><strong>摘要：</strong> 本文通过大规模实验对比了基于MLM和CLM的预训练方法在文本表示任务中的表现。虽然MLM通常表现更优，但CLM在数据效率和微调稳定性方面更具优势。研究还提出了一种结合CLM和MLM的双阶段训练策略，在计算资源有限的情况下取得了最佳效果。此外，利用现有的CLM预训练模型可以降低训练高性能编码器模型的计算成本。所有实验结果和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00994" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:45:48 GMT</pubDate>
</item>
<item>
<title>扩散模型在动态系统模拟中的潜在应用</title>
<link>https://arxiv.org/abs/2507.02608</link>
<guid>https://arxiv.org/abs/2507.02608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型在潜空间中模拟动态系统表现良好。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在动态系统模拟中使用扩散模型的可行性，特别是在潜空间中进行生成而非像素空间。研究发现，潜空间模拟在压缩率高达1000倍的情况下仍保持较高的准确性。此外，基于扩散的模拟器比非生成方法更精确，并能通过预测的多样性来补偿不确定性。文章还讨论了训练潜空间模拟器的关键设计选择，包括架构和优化器等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 09:32:50 GMT</pubDate>
</item>
<item>
<title>LitBench：首个用于创意写作评估的标准化基准与数据集</title>
<link>https://arxiv.org/abs/2507.00769</link>
<guid>https://arxiv.org/abs/2507.00769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LitBench提升创意写作模型评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LitBench，这是首个用于创意写作验证的标准化基准和配对数据集。该数据集包含从Reddit中提取的2,480个去偏见、人工标注的故事比较测试集，以及43,827对人工偏好标签的训练语料。通过LitBench，研究者评估了零样本语言模型作为评判者的性能，并训练了Bradley-Terry和生成式奖励模型。实验表明，训练后的模型在准确率上优于所有现成模型，并通过在线人类研究进一步验证了其与人类偏好的一致性。研究结果为创意写作系统的自动化评估和优化提供了可靠资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 10:10:36 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在计算机视觉任务中的性能评估</title>
<link>https://arxiv.org/abs/2507.01955</link>
<guid>https://arxiv.org/abs/2507.01955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态模型在视觉任务中表现一般，但具备一定通用性。</p><br /><br /><p><strong>摘要：</strong> 本文评估了GPT-4o、o4-mini、Gemini 1.5 Pro等多模态基础模型在图像分类、目标检测等标准计算机视觉任务中的表现。由于这些模型主要训练用于文本输出，难以直接处理视觉任务，研究通过提示链技术将其转化为文本可处理的任务。结果显示，尽管这些模型在专业视觉模型面前仍有差距，但在语义任务上表现较好，且对提示变化不敏感的模型表现更优。GPT-4o在非推理模型中表现最佳，而具有图像生成能力的模型则存在幻觉和空间错位问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>EKA-EVAL：面向多语言大模型的统一评估框架</title>
<link>https://arxiv.org/abs/2507.01853</link>
<guid>https://arxiv.org/abs/2507.01853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EKA-EVAL提供多语言大模型评估，覆盖35个基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了EKA-EVAL，这是一个面向多语言大模型的统一评估框架，整合了超过35个基准测试，包括10个针对印度语言的数据集。该框架支持推理、数学、工具使用、长文本理解和阅读理解等任务，并具备分布式推理、量化和多GPU支持等功能。与现有工具相比，EKA-EVAL具有更广泛的基准覆盖，是首个端到端、可扩展的评估套件，旨在降低多语言模型评估的门槛。该框架已开源，并计划扩展至100多个基准，构建一个强大的多语言评估生态系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:07:54 GMT</pubDate>
</item>
<item>
<title>基于文本描述的多器官医学分割模型CRISP-SAM2</title>
<link>https://arxiv.org/abs/2506.23121</link>
<guid>https://arxiv.org/abs/2506.23121</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRISP-SAM2提升多器官医学图像分割精度与细节表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CRISP-SAM2的新模型，用于改进多器官医学图像分割。该模型通过跨模态交互和语义提示机制，增强对视觉信息的理解，并减少对几何提示的依赖。此外，引入了相似性排序自更新策略和掩码优化过程，以提高局部细节的准确性。实验结果表明，CRISP-SAM2在多个公开数据集上优于现有模型，显示出其在解决当前分割模型不足方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23121" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 03:05:27 GMT</pubDate>
</item>
<item>
<title>视觉语言分割中的幻觉评估基准研究</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HalluSegBench评估视觉语言分割中的幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言分割模型中常见的幻觉问题，提出了首个专门用于评估视觉接地幻觉的基准HalluSegBench。该基准包含1340对反事实实例对，覆盖281个独特物体类别，并引入了新的度量标准来量化在视觉一致场景修改下的幻觉敏感性。实验表明，基于视觉的幻觉比基于标签的幻觉更为普遍，强调了反事实推理在诊断接地准确性中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>InnerControl：提升扩散模型空间控制精度的新方法</title>
<link>https://arxiv.org/abs/2507.02321</link>
<guid>https://arxiv.org/abs/2507.02321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InnerControl通过全阶段空间一致性训练提升图像生成精度。</p><br /><br /><p><strong>摘要：</strong> 尽管文本到图像的扩散模型取得了显著进展，但实现精确的空间控制仍具挑战。ControlNet及其改进版本ControlNet++通过引入条件模块和循环一致性损失来提升控制效果，但忽略了中间生成阶段。为此，InnerControl提出一种训练策略，在所有扩散步骤中强制空间一致性。该方法通过轻量级卷积探针从UNet中间特征中重建输入控制信号（如边缘、深度），即使在高噪声潜变量下也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测与目标条件之间的差异，InnerControl提升了控制精度和生成质量，并结合ControlNet++等现有技术实现了多种条件方法的最先进性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 01:25:53 GMT</pubDate>
</item>
<item>
<title>大型语言模型的自我纠正盲点研究</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs存在自我纠正盲点，影响其可靠性。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）已取得显著进展，但它们仍会犯错并陷入无效推理路径。自我纠正能力对可信的LLM至关重要，尤其是自回归模型。然而，LLMs在用户输入中能识别错误，却无法修正自身输出中的相同错误，这种现象被称为‘自我纠正盲点’。为系统研究这一问题，研究人员提出了Self-Correction Bench框架，通过在三个复杂度级别上注入错误进行测试。实验显示，14个模型平均有64.5%的盲点率。研究发现，这一限制与训练数据组成有关：人类演示数据多为无错误响应，而非纠错序列，而强化学习训练的模型则通过反馈学习纠错。令人惊讶的是，仅添加“Wait”即可减少89.3%的盲点，表明该能力存在但需激活。该研究揭示了当前LLMs的关键缺陷，并为提升其可靠性和可信度提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:41:30 GMT</pubDate>
</item>
<item>
<title>利用LLM辅助科学论文局限性识别的基准研究</title>
<link>https://arxiv.org/abs/2507.02694</link>
<guid>https://arxiv.org/abs/2507.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在识别科学论文局限性方面具有潜力，LimitGen基准提升其反馈能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在科学论文同行评审中的应用，特别是在识别论文局限性方面的潜力。作者提出了一个全面的局限性分类体系，并基于此构建了LimitGen基准，包含合成数据集和真实人类撰写的局限性数据集。通过引入文献检索增强LLM的识别能力，该方法提升了LLM生成具体且有建设性反馈的能力，为早期研究反馈和补充人工评审提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:04:38 GMT</pubDate>
</item>
<item>
<title>基于能量模型的系统2思维推理方法研究</title>
<link>https://arxiv.org/abs/2507.02092</link>
<guid>https://arxiv.org/abs/2507.02092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EBTs通过无监督学习实现系统2推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的基于能量模型的推理方法（Energy-Based Transformers, EBTs），旨在通过无监督学习实现类似人类系统2思维的推理能力。与传统方法相比，EBTs无需额外监督或训练，而是通过显式验证输入与候选预测的兼容性，并将预测问题重新建模为以验证器为基准的优化问题。实验表明，EBTs在文本和视觉等多模态任务中均表现出色，训练速度更快，推理性能优于Transformer++和Diffusion Transformers。此外，EBTs在多数下游任务中表现更优，表明其具有更好的泛化能力，是一种有前景的模型扩展范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>IntFold：一种可控制的生物分子结构预测基础模型</title>
<link>https://arxiv.org/abs/2507.02025</link>
<guid>https://arxiv.org/abs/2507.02025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IntFold在结构预测方面达到AlphaFold3水平，支持多种定制化任务。</p><br /><br /><p><strong>摘要：</strong> IntFold是一种可控制的基础模型，用于通用和专业生物分子结构预测。其预测精度与AlphaFold3相当，并采用优化的注意力核。除了标准结构预测外，IntFold还可通过适配器预测别构状态、约束结构和结合亲和力。此外，研究团队引入了一种新的置信度头，以更精细地评估对接质量，特别是在抗体-抗原复合物等复杂目标上表现突出。文章还分享了训练该计算密集型模型的经验与见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:09:47 GMT</pubDate>
</item>
<item>
<title>AsyncFlow：一种高效的异步流式强化学习框架</title>
<link>https://arxiv.org/abs/2507.01663</link>
<guid>https://arxiv.org/abs/2507.01663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AsyncFlow提升大语言模型后训练效率，支持灵活扩展。</p><br /><br /><p><strong>摘要：</strong> 本文提出AsyncFlow，一种用于大语言模型后训练的异步流式强化学习框架。针对传统框架在可扩展性、数据流复杂性和资源闲置方面的不足，AsyncFlow引入了分布式数据存储与传输模块，实现统一的数据管理和细粒度调度。其架构支持自动流水线重叠和动态负载均衡，并通过生产者-消费者异步工作流减少计算空闲。此外，AsyncFlow与底层训练和推理引擎解耦，提供模块化用户界面。实验表明，该框架在吞吐量上平均提升1.59倍，为下一代强化学习系统设计提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:45:34 GMT</pubDate>
</item>
<item>
<title>ZeCO：实现线性注意力模型高效序列并行的新方法</title>
<link>https://arxiv.org/abs/2507.01004</link>
<guid>https://arxiv.org/abs/2507.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeCO提升长序列训练效率，消除通信开销。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZeCO（Zero Communication Overhead）序列并行方法，用于优化线性注意力机制在大型语言模型中的应用。传统序列并行方法因通信开销大而成为瓶颈，而ZeCO通过引入All-Scan通信原语，有效减少通信负担，实现近线性扩展。实验表明，在256块GPU上处理8M长度的序列时，ZeCO相比现有最优方法提升了60%的速度。理论与实证均证明了ZeCO的高效性与可行性，为训练超长序列的下一代LLM提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:54:53 GMT</pubDate>
</item>
<item>
<title>多模态推理中‘思考与图像’范式的演进与展望</title>
<link>https://arxiv.org/abs/2506.23918</link>
<guid>https://arxiv.org/abs/2506.23918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨AI从‘思考图像’到‘与图像共思’的范式转变。</p><br /><br /><p><strong>摘要：</strong> 本文综述了多模态推理领域中‘思考与图像’范式的最新进展。传统方法将视觉视为静态输入，导致感知数据与符号推理之间的语义鸿沟。而人类认知常利用视觉作为动态思维工具，这一理念正推动AI从仅‘思考图像’向‘与图像共思’演进。文章提出该范式发展的三个阶段：外部工具探索、程序化操作和内在想象，并总结了核心方法、评估基准、应用前景及未来挑战，为构建更强大且符合人类认知的多模态AI提供路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 10:48:35 GMT</pubDate>
</item>
<item>
<title>动态选择与合并专家模型提升跨领域信息抽取性能</title>
<link>https://arxiv.org/abs/2506.22813</link>
<guid>https://arxiv.org/abs/2506.22813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SaM框架通过动态选择和合并专家模型提升信息抽取效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SaM框架，通过在推理阶段动态选择和合并预训练的专家模型来优化信息抽取任务。该方法基于目标领域的相似性和采样实例的表现选择合适的专家模型，并将其合并以生成针对特定领域的优化模型。这种方法无需额外训练即可提高跨领域泛化能力，并具备良好的可扩展性。实验结果表明，该框架在多个基准测试中平均优于统一模型10%。文章还探讨了框架的潜在改进方向和实际应用经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 04:28:52 GMT</pubDate>
</item>
<item>
<title>基于语言理解的3D场景重建框架LangScene-X</title>
<link>https://arxiv.org/abs/2507.02813</link>
<guid>https://arxiv.org/abs/2507.02813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangScene-X通过多模态生成实现从稀疏视角的高质量3D场景重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LangScene-X的新生成框架，用于从2D图像中恢复一致的3D结构并实现开放词汇场景理解。该方法通过TriMap视频扩散模型生成RGB、法线和语义分割图，并结合语言量化压缩器（LQC）实现跨场景的语言嵌入编码，从而在仅有稀疏视角的情况下构建可泛化的3D语言嵌入场景。最后，通过将语言信息对齐到3D场景表面，支持开放式语言查询。实验表明，LangScene-X在质量和泛化能力上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:21:23 GMT</pubDate>
</item>
<item>
<title>2-单纯形Transformer提升token效率的研究</title>
<link>https://arxiv.org/abs/2507.02754</link>
<guid>https://arxiv.org/abs/2507.02754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2-单纯形Transformer在token效率上优于传统Transformer。</p><br /><br /><p><strong>摘要：</strong> 本文研究了2-单纯形Transformer架构，该架构通过高效的Triton内核实现三线性函数的注意力机制，相较于标准的点积注意力，其在数学、编程、推理和逻辑任务中表现出更高的token效率。实验表明，在固定token预算下，2-单纯形Transformer模型性能更优，并且改变了知识和推理任务的缩放定律指数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:16:34 GMT</pubDate>
</item>
<item>
<title>基于自生成目标条件MDPs的自动定理证明方法</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新框架提升LLM在复杂推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为自生成目标条件马尔可夫决策过程（sG-MDP）的新框架，旨在解决大型语言模型在自动定理证明中面临的挑战。该框架通过让智能体根据不断变化的证明状态生成并追求子目标，使问题更易于搜索。研究者将这种方法应用于Bourbaki（7B）系统，该系统可以集成多个7B参数的LLM进行子目标生成和策略合成。在PutnamBench基准测试中，Bourbaki（7B）成功解决了26个问题，取得了当前同类模型的最佳成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:41:38 GMT</pubDate>
</item>
<item>
<title>HiRA：一种分层框架提升复杂信息检索与推理效率</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HiRA通过分层规划与执行提升复杂搜索任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出HiRA，一种分层信息检索框架，将战略规划与专业执行分离。该方法将复杂搜索任务分解为子任务，并分配给具备外部工具和推理能力的领域特定代理。通过结构化集成机制协调结果，避免执行细节干扰高层推理，从而提升系统效率与答案质量。在四个跨模态深度搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:18:08 GMT</pubDate>
</item>
<item>
<title>提升大模型信息检索能力的WebSailor方法</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebSailor提升大模型在复杂信息任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出WebSailor，一种后训练方法，旨在增强大模型在复杂信息搜索任务中的能力。通过生成高不确定性任务、RFT冷启动和DUPO算法，WebSailor显著提升了开源模型的表现，使其接近专有系统水平。该方法解决了传统模型在处理海量信息时的不确定性问题，推动了大语言模型在信息检索领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 08:59:07 GMT</pubDate>
</item>
<item>
<title>提升奖励模型性能：基于高质量数据集的Skywork-Reward-V2研究</title>
<link>https://arxiv.org/abs/2507.01352</link>
<guid>https://arxiv.org/abs/2507.01352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork-Reward-V2通过高质量数据提升奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文指出当前开放奖励模型在评估基准上表现不佳，主要由于偏好数据集存在局限。为解决这一问题，作者提出了包含4000万对偏好的SynPref-40M数据集，并设计了人机协同的数据筛选流程。基于该数据集，训练出8个参数规模从0.6B到8B的Skywork-Reward-V2模型，在多个基准测试中取得最优成绩。实验表明，模型性能提升不仅得益于数据量，更得益于高质量的数据筛选。该研究展示了人机协作在数据质量提升中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:40:29 GMT</pubDate>
</item>
<item>
<title>多尺度多模态大语言模型在自动放射学报告生成中的应用</title>
<link>https://arxiv.org/abs/2507.00316</link>
<guid>https://arxiv.org/abs/2507.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出mu^2LLM模型提升CT影像报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自动化放射学报告生成（RRG）技术，旨在通过临床影像数据生成详细文本报告以提高诊断准确性和效率。文章指出该技术面临两大挑战：从影像数据中提取相关信息的复杂性以及模型生成报告与专家报告之间差异的客观评估困难。为解决这些问题，作者提出了mu^2LLM模型，结合多尺度和多模态特征，并通过直接偏好优化提升报告质量。实验结果表明，该方法在多个大型CT图像-报告数据集上优于现有方法，显示出在有限数据下进行RRG任务的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 19:14:49 GMT</pubDate>
</item>
<item>
<title>MARVIS：一种无需训练的多模态推理方法</title>
<link>https://arxiv.org/abs/2507.01544</link>
<guid>https://arxiv.org/abs/2507.01544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARVIS使小型视觉语言模型能高效预测多种数据模态。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARVIS的无需训练的方法，使小型视觉语言模型能够以高精度预测任何数据模态。该方法通过将潜在嵌入空间转换为可视化表示，并利用视觉语言模型的空间和细粒度推理能力进行解释和利用。MARVIS在视觉、音频、生物和表格领域表现出色，使用单一3B参数模型即可达到优于Gemini 16%的平均性能，并接近专用方法，同时不涉及个人身份信息且无需领域特定训练。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 05:56:24 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的实时交互式头部生成方法</title>
<link>https://arxiv.org/abs/2507.00472</link>
<guid>https://arxiv.org/abs/2507.00472</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIG框架实现更真实的实时交互头部生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了面对面交流中的交互式头部生成问题，针对传统方法在实时性和交互真实性上的不足，提出了一种基于自回归（AR）的逐帧生成框架ARIG。该框架通过非向量量化AR过程进行运动预测，并利用扩散过程表示运动分布以提高连续空间预测精度。同时，引入交互行为理解（IBU）和详细对话状态理解（CSU），通过双模态信号分析和上下文建模提升交互真实感。实验验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00472" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:38:14 GMT</pubDate>
</item>
<item>
<title>Locality-aware Parallel Decoding加速自回归图像生成</title>
<link>https://arxiv.org/abs/2507.01957</link>
<guid>https://arxiv.org/abs/2507.01957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPD技术提升自回归图像生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Locality-aware Parallel Decoding (LPD)方法，用于加速自回归图像生成。传统方法依赖逐块预测，导致高延迟。现有研究虽尝试通过多块预测实现并行化，但效果有限。LPD引入两种关键技术：灵活并行自回归建模和局部感知生成顺序，分别实现任意生成顺序和减少组内依赖，从而显著降低生成步数并提升效率。实验表明，在保持生成质量的前提下，LPD将ImageNet图像生成步数从256降至20（256×256分辨率）和1024降至48（512×512分辨率），延迟降低至少3.4倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>FreeMorph：无需微调的高效图像形态转换方法</title>
<link>https://arxiv.org/abs/2507.01953</link>
<guid>https://arxiv.org/abs/2507.01953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeMorph实现无需训练的高质量图像形态转换，速度快且效果优。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeMorph，一种无需微调的图像形态转换方法，能够处理不同语义或布局的输入。与现有依赖微调扩散模型的方法不同，FreeMorph通过引入引导感知球面插值和步进变化趋势，解决了非线性去噪过程中的质量下降问题，实现了更高效、更一致的图像过渡效果。实验表明，FreeMorph在速度和性能上均优于现有方法，提升达10倍至50倍，并建立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:20 GMT</pubDate>
</item>
<item>
<title>视觉-语言-动作模型中的动作标记化研究综述</title>
<link>https://arxiv.org/abs/2507.01925</link>
<guid>https://arxiv.org/abs/2507.01925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述分析VLA模型中动作标记的分类与作用。</p><br /><br /><p><strong>摘要：</strong> 本文综述了视觉-语言-动作（VLA）模型的研究进展，指出当前模型在处理视觉和语言输入后生成一系列动作标记，最终输出可执行动作。文章强调，VLA模型的核心设计差异在于动作标记的构建方式，包括语言描述、代码、可操作性、轨迹等多种形式。然而，对动作标记的理解仍不充分，阻碍了VLA的发展。本文通过分析不同动作标记的优缺点，提出未来研究方向，旨在推动VLA向通用智能迈进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:34:52 GMT</pubDate>
</item>
<item>
<title>STR-Match：一种无需训练的视频编辑算法</title>
<link>https://arxiv.org/abs/2506.22868</link>
<guid>https://arxiv.org/abs/2506.22868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STR-Match通过潜空间优化实现高质量视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为STR-Match的视频编辑算法，该算法无需训练即可生成视觉吸引人且时空一致的视频。其核心在于利用新颖的STR分数，结合2D空间注意力和1D时间模块，捕捉相邻帧之间的时空像素相关性，避免了计算成本高昂的3D注意力机制。通过集成潜空间优化框架和潜空间掩码，STR-Match在保持源视频关键视觉属性的同时，实现了在显著领域变换下的强性能表现。大量实验表明，该方法在视觉质量和时空一致性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 08:36:19 GMT</pubDate>
</item>
<item>
<title>Kwai Keye-VL：面向短视频理解的多模态大模型</title>
<link>https://arxiv.org/abs/2507.01949</link>
<guid>https://arxiv.org/abs/2507.01949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kwai Keye-VL提升短视频理解能力，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Kwai Keye-VL，一款80亿参数的多模态基础模型，旨在提升对短视频的理解能力。该模型基于超过6000亿token的高质量数据集和创新的训练方法，包括四阶段预训练和两阶段后训练。第二阶段引入五种模式的数据混合，增强模型的推理能力。通过强化学习和对齐优化，模型在多个视频基准测试中表现优异，并发布了针对真实短视频场景的KC-MMBench基准，进一步验证了其优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>基于动态全局-局部范式的长动画上色方法研究</title>
<link>https://arxiv.org/abs/2507.01945</link>
<guid>https://arxiv.org/abs/2507.01945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LongAnimation框架，提升动画上色的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 动画上色是动画产业的重要环节，传统方法成本高且效率低。现有研究多集中于短期上色，依赖局部特征融合，难以保持长期颜色一致性。本文提出LongAnimation框架，采用动态全局-局部范式，结合SketchDiT、DGLM和Color Consistency Reward模块，有效提升长视频段的颜色一致性。实验表明，该方法在短时（14帧）和长时（平均500帧）动画任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>DepthAnything-AC：一种适应多种环境的单目深度估计模型</title>
<link>https://arxiv.org/abs/2507.01634</link>
<guid>https://arxiv.org/abs/2507.01634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DepthAnything-AC在复杂环境下表现出色，具备零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DepthAnything-AC的单目深度估计模型，能够在多种复杂环境中保持高精度。与以往模型相比，该模型在光照变化、恶劣天气和传感器畸变等条件下表现更优。研究者引入了无监督一致性正则化微调方法，仅需少量未标记数据即可提升性能，并通过空间距离约束增强模型对局部关系的学习能力。实验结果表明，DepthAnything-AC在多个基准测试中展现出强大的零样本能力，包括真实世界恶劣天气数据集、合成噪声数据集和通用数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:05:57 GMT</pubDate>
</item>
<item>
<title>JAM-Flow：统一生成面部动作与语音的框架</title>
<link>https://arxiv.org/abs/2506.23552</link>
<guid>https://arxiv.org/abs/2506.23552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM-Flow实现面部动作与语音的联合生成，提升多模态合成效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出JAM-Flow，一个统一的框架，用于同时生成和条件化面部动作与语音。该方法结合流匹配和多模态扩散Transformer（MM-DiT）架构，包含专门的Motion-DiT和Audio-DiT模块，并通过选择性联合注意力层进行耦合。JAM-Flow采用类似修复的目标进行训练，支持文本、参考音频和参考动作等多种输入，适用于同步生成说话头像、音频驱动动画等任务，显著提升了多模态生成模型的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:51:40 GMT</pubDate>
</item>
<item>
<title>Mixture of Reasoning：提升大语言模型推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.00606</link>
<guid>https://arxiv.org/abs/2507.00606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture of Reasoning 提升大语言模型推理性能，无需外部提示。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Mixture of Reasoning (MoR) 框架，通过将多种推理策略嵌入大语言模型中，实现自主、任务自适应的推理能力。该框架包含两个阶段：首先生成推理链模板，然后通过监督微调提升性能。实验表明，MoR 在多个基准测试中显著提升表现，且无需依赖任务特定的提示，为跨任务的鲁棒推理提供了通用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 05:39:04 GMT</pubDate>
</item>
<item>
<title>基于频率修正的神经材质表示方法FreNBRDF</title>
<link>https://arxiv.org/abs/2507.00476</link>
<guid>https://arxiv.org/abs/2507.00476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreNBRDF提升材质建模精度与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为FreNBRDF的频率修正神经材质表示方法，旨在提高材质建模的准确性和可解释性。传统方法依赖于表格化的BRDF数据，而现代方法则采用隐式神经表示，但其在频域中的行为仍不明确。该研究通过引入球谐函数，将频域考虑整合到神经BRDF建模中，并设计了一种新的频率修正损失函数。该框架提升了材质重建和编辑的保真度、适应性和效率。实验表明，FreNBRDF在材质外观重建和编辑方面优于现有方法，支持更结构化和可解释的下游任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:48:50 GMT</pubDate>
</item>
<item>
<title>MOVi-MC-AC：首个多摄像头视图的非模态分割与内容数据集</title>
<link>https://arxiv.org/abs/2507.00339</link>
<guid>https://arxiv.org/abs/2507.00339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOVi-MC-AC是首个支持多摄像头视图的非模态分割数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MOVi-MC-AC，这是目前最大的非模态分割和首个非模态内容数据集。该数据集通过多摄像头模拟复杂家庭场景中的物体遮挡情况，提供了约580万实例的标签，并首次引入了真实非模态内容的地面实况。相比以往依赖慢速拼接生成伪标签的方法，MOVi-MC-AC在合成视频中实现了跨帧和多视角的一致性对象标识，为计算机视觉中的目标检测、跟踪和分割研究提供了新的挑战和机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 20:36:56 GMT</pubDate>
</item>
<item>
<title>MusiXQA：推动多模态大模型理解乐谱的基准数据集</title>
<link>https://arxiv.org/abs/2506.23009</link>
<guid>https://arxiv.org/abs/2506.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MusiXQA是首个用于评估音乐乐谱理解的多模态大模型数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MusiXQA，这是首个针对多模态大语言模型（MLLMs）在音乐乐谱理解方面的综合数据集。该数据集通过MusiXTeX生成高质量的合成乐谱，并包含结构化的注释，涵盖音符音高与时值、和弦、谱号、调号与拍号等信息，支持多种视觉问答任务。实验表明当前最先进的MLLMs在该领域存在显著不足，为此作者开发了Phi-3-MusiX模型，在该数据集上取得了优于GPT类方法的性能。该数据集和模型为未来MLLMs在音乐乐谱理解领域的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 16:46:47 GMT</pubDate>
</item>
<item>
<title>基于置信度的3D高斯点云压缩方法</title>
<link>https://arxiv.org/abs/2506.22973</link>
<guid>https://arxiv.org/abs/2506.22973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于置信度的3D高斯点云压缩方法，提升渲染效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于可学习置信度分数的损失函数，用于压缩3D高斯点云。该方法通过优化重建感知损失来调整每个点的置信度，从而修剪低置信度点，同时保持视觉质量。该方法与架构无关，适用于任何高斯点云渲染变体，并引入平均置信度作为场景质量的新评估指标。实验表明，该方法在压缩率和保真度之间取得了良好平衡。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 14:11:30 GMT</pubDate>
</item>
<item>
<title>FreeLong++：提升长视频生成质量的训练无关框架</title>
<link>https://arxiv.org/abs/2507.00162</link>
<guid>https://arxiv.org/abs/2507.00162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeLong++通过多频段融合提升长视频生成的时序一致性和视觉质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeLong和FreeLong++，用于解决长视频生成中时间一致性下降和视觉质量退化的问题。FreeLong通过在去噪过程中融合全局低频特征与局部高频特征，平衡长视频的频率分布。FreeLong++进一步扩展为多分支架构，支持多尺度时间窗口的多频段融合，从而增强语义连贯性和细节动态。该方法无需额外训练即可集成到现有模型中，显著提升长视频生成效果，并支持多提示生成、平滑场景切换及可控视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 14:11:21 GMT</pubDate>
</item>
<item>
<title>IR3D-Bench：通过主动创造评估视觉语言模型的场景理解能力</title>
<link>https://arxiv.org/abs/2506.23329</link>
<guid>https://arxiv.org/abs/2506.23329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IR3D-Bench挑战VLMs通过主动创造理解场景，推动其生成能力发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了IR3D-Bench，一个用于评估视觉语言模型（VLMs）场景理解能力的新基准。该基准要求模型通过主动使用编程和渲染工具来重建输入图像的3D结构，从而实现“通过创造理解”的方法。不同于传统基于被动识别的评估方式，IR3D-Bench强调工具使用和生成能力。研究提供了多种指标来评估几何准确性、空间关系、外观属性和整体合理性。实验表明，当前VLMs在视觉精度方面仍存在局限。IR3D-Bench包含数据和评估协议，旨在促进工具使用型VLMs的发展，以实现真正的场景理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 13:02:57 GMT</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking：多模态推理模型的进展与性能评估</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.1V-Thinking在多模态任务中表现卓越，超越多个基准。</p><br /><br /><p><strong>摘要：</strong> GLM-4.1V-Thinking是一款面向通用多模态推理的视觉语言模型。通过大规模预训练和基于课程采样的强化学习（RLCS），该模型在STEM问题解决、视频理解、内容识别、编程、定位、GUI代理和长文档理解等多个任务中展现出全面的能力提升。开源版本GLM-4.1V-9B-Thinking在28个公共基准测试中表现出色，超越Qwen2.5-VL-7B，并在多数任务上优于更大的Qwen2.5-VL-72B模型。此外，它在长文档理解和STEM推理等挑战性任务上也表现出与GPT-4o相当或更优的性能。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>SciArena：科学文献任务的开放协作评估平台</title>
<link>https://arxiv.org/abs/2507.01001</link>
<guid>https://arxiv.org/abs/2507.01001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciArena通过社区投票评估基础模型在科学文献任务中的表现。</p><br /><br /><p><strong>摘要：</strong> SciArena是一个开放协作平台，用于评估基础模型在科学文献任务中的表现。与传统基准不同，它采用社区投票方式，让研究人员直接参与模型比较。该平台已支持23个开源和专有模型，并收集了超过13,000份来自不同领域研究者的投票。分析显示，提交的问题多样且符合实际需求，研究人员在评估中表现出高度一致性和准确性。此外，团队还发布了SciArena-Eval，一个基于偏好数据的元评估基准，用于衡量模型判断答案质量的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>迈向通用人工智能：跨学科视角下的认知与架构分析</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AGI发展需整合记忆、推理与多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能向通用人工智能（AGI）发展的关键问题，指出当前模型如GPT-4.5、DeepSeek等虽具备多模态能力和部分推理能力，但仍受限于基于token的预测机制和缺乏具身代理。文章从人工智能、认知神经科学、心理学等多个领域出发，分析了AGI的架构与认知基础，强调模块化推理、持久记忆和多智能体协作的重要性。同时，Agentic RAG框架、信息压缩与测试时适应等策略被视为实现灵活智能的关键路径。此外，视觉语言模型被重新定义为具身理解与协作任务的接口。作者认为，真正的智能源于记忆与推理的融合，而非单纯依赖规模。最后，文章讨论了AGI发展中的科学、技术和伦理挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 12:52:25 GMT</pubDate>
</item>
<item>
<title>AI生成内容激增与新型水印技术PECCAVI的提出</title>
<link>https://arxiv.org/abs/2506.22960</link>
<guid>https://arxiv.org/abs/2506.22960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI生成内容将达90%，PECCAVI应对水印攻击。</p><br /><br /><p><strong>摘要：</strong> 欧洲联盟执法机构报告预测，到2026年，高达90%的在线内容可能由生成式AI创建，引发政策制定者的担忧。加州AB 3211法案要求对AI生成内容进行水印标记，但现有技术易被篡改。本文提出PECCAVI，一种针对视觉重述攻击的安全且无失真的图像水印技术。该技术在图像的核心语义区域嵌入水印，并利用多通道频域水印和噪声烧制技术增强抗逆向工程能力。PECCAVI具备模型无关性，相关资源将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 13:34:08 GMT</pubDate>
</item>
<item>
<title>提升语言模型训练效果的数据效能研究</title>
<link>https://arxiv.org/abs/2506.21545</link>
<guid>https://arxiv.org/abs/2506.21545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据效能优化可显著提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据效能（Data Efficacy）在语言模型训练中的作用，提出了一种名为DELT的通用范式，包含数据评分、数据选择和数据排序三个组件。其中，Learnability-Quality Scoring（LQS）通过梯度一致性评估样本的可学习性和质量，Folding Ordering（FO）则解决模型遗忘和数据分布偏差问题。实验表明，DELT能有效提升模型性能，且与数据效率结合使用效果更佳，证明数据效能是语言模型训练的重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>扩散语言模型在代码生成中的应用与优化</title>
<link>https://arxiv.org/abs/2506.20639</link>
<guid>https://arxiv.org/abs/2506.20639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型在代码生成中的解码行为与强化学习训练方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散语言模型（dLLMs）在代码生成中的潜力，分析了其与自回归模型的不同之处，如生成的因果性控制和采样温度对生成顺序的影响。作者训练了一个7B参数的dLLM模型DiffuCoder，并提出了一种新的采样方案coupled-GRPO，以提升强化学习训练效率。实验结果显示，该方法在代码生成基准测试中提升了4.4%，并减少了对自回归因果性的依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:35:47 GMT</pubDate>
</item>
<item>
<title>基于时空能量衰减的径向注意力机制提升视频生成效率</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出径向注意力机制，提升视频生成效率与长度。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视频扩散模型中的时空能量衰减现象，并提出了径向注意力机制。该机制通过稀疏注意力计算，将能量衰减转化为计算密度的指数衰减，从而显著降低计算复杂度。实验表明，该方法在多个视频生成模型上保持高质量的同时，提升了生成速度并减少了训练成本。相比传统密集注意力，其效率更高，且支持更长视频的生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>数学推理模型的泛化能力与训练方法研究</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数学模型在特定任务上表现优异，但泛化能力有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型在数学推理任务上的进展，并探讨其是否具备更广泛的问题解决能力。通过对20多个模型的评估发现，多数数学表现优秀的模型在其他领域如科学问答、编程和指令遵循上表现不佳。通过对比强化学习和监督微调两种训练方法，发现强化学习模型在跨领域任务中表现更好，而监督微调模型容易失去通用能力。分析表明，监督微调会导致表示和输出分布的变化，影响模型的泛化能力，提示需要重新考虑当前的训练策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 01:23:05 GMT</pubDate>
</item>
<item>
<title>MoCa：提升多模态嵌入模型性能的两阶段框架</title>
<link>https://arxiv.org/abs/2506.23115</link>
<guid>https://arxiv.org/abs/2506.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoCa通过两阶段方法提升多模态嵌入模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoCa，一种将预训练视觉语言模型转化为高效双向多模态嵌入模型的两阶段框架。第一阶段为模态感知持续预训练，引入联合重建目标以增强双向上下文感知推理；第二阶段为异构对比微调，利用多样化的多模态数据提升泛化能力和对齐效果。该方法解决了现有模型在注意力机制、数据依赖性和训练目标多样性方面的不足，并在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 02:41:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2506.21277</link>
<guid>https://arxiv.org/abs/2506.21277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出方法增强多模态模型理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要。本文指出当前多模态推理模型存在全局上下文理解不足和依赖捷径的问题，并提出通过引入上下文奖励、格式奖励和逻辑奖励来提升模型的推理能力。同时，作者构建了IntentBench基准测试，用于评估模型在理解复杂人类意图和情感方面的表现。实验结果表明，该方法在多个多模态基准上优于现有开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 10:01:03 GMT</pubDate>
</item>
<item>
<title>MEMFOF：高效多帧光流估计方法</title>
<link>https://arxiv.org/abs/2506.23151</link>
<guid>https://arxiv.org/abs/2506.23151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEMFOF在高分辨率下实现高效光流估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出MEMFOF，一种内存高效的多帧光流估计方法，在保持高精度的同时显著降低GPU内存消耗。该方法在1080p输入下仅需2.09GB显存运行，训练时为28.5GB，无需裁剪或降采样即可在原生分辨率下训练。通过优化RAFT架构设计，结合减少的卷积体积和高分辨率训练策略，MEMFOF在多个基准测试中取得最佳性能，包括Spring、Sintel和KITTI-2015数据集，展现出出色的准确性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 05:01:42 GMT</pubDate>
</item>
<item>
<title>基于多路径扩散的可调金属镜头摄影方法</title>
<link>https://arxiv.org/abs/2506.22753</link>
<guid>https://arxiv.org/abs/2506.22753</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型金属镜头成像方法，提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于退化建模的多路径扩散方法，用于可调金属镜头摄影。该方法利用预训练模型中的自然图像先验，而非依赖大规模数据集，通过正向、中性与负向提示路径平衡高频细节生成、结构保真度和金属镜头特有退化的抑制。同时引入伪数据增强和可调解码器，实现保真度与感知质量的可控权衡。此外，设计了空间变化的退化感知注意力模块，以适应复杂的光学和传感器引起的退化。最终构建了毫米级MetaCamera进行实际验证，实验结果表明该方法优于现有技术，实现了高保真和清晰的图像重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22753" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 00:48:37 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型代理在研究扩展任务中的能力</title>
<link>https://arxiv.org/abs/2506.22598</link>
<guid>https://arxiv.org/abs/2506.22598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM代理在自主实现研究扩展任务上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了基于大型语言模型（LLMs）的智能体在自主执行软件工程和科研任务方面的潜力。研究引入了RExBench，一个包含12个真实研究实验任务的基准测试，用于评估智能体在扩展已有研究成果方面的能力。每个任务都基于现有论文和代码库，并附有专家指导说明。尽管RExBench具备抗数据污染能力和自动化评估系统，但测试结果显示，使用不同框架开发的九个LLM代理在没有大量人工提示的情况下，仅能完成不到40%的任务。这表明当前智能体仍需大量人工干预才能处理现实中的研究扩展任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 15:41:41 GMT</pubDate>
</item>
<item>
<title>Tower+：在翻译与多语言通用能力之间实现性能平衡的模型</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tower+在翻译和多语言通用任务中表现出色，兼顾专业与泛用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tower+，一个在机器翻译和多语言通用文本处理方面均表现优异的模型系列。通过引入一种新的训练方法，包括持续预训练、监督微调、偏好优化和基于可验证奖励的强化学习，Tower+实现了翻译专业化与多语言通用能力之间的帕累托最优。研究团队在多个规模（2B、9B、72B）上构建了模型，并在代码生成、数学问题解决和指令遵循等任务中提升了性能。实验结果显示，Tower+在高资源语言翻译中表现卓越，并在多语言Arena Hard评估和IF-MT基准测试中取得领先。该研究证明，在优化特定业务领域（如翻译和本地化）的同时，仍可达到前沿模型的通用能力水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:30:06 GMT</pubDate>
</item>
<item>
<title>基于自对弈的强化学习框架SPIRAL提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIRAL通过自对弈训练提升语言模型推理能力，无需人工监督。</p><br /><br /><p><strong>摘要：</strong> 本文提出SPIRAL，一种基于自对弈的强化学习框架，使语言模型通过与不断进化的自我版本进行零和博弈来学习，从而无需依赖人工标注的数据或领域特定奖励工程。该框架生成持续进阶的问题课程，推动模型适应更强对手。研究中引入了角色条件优势估计（RAE）以稳定多智能体训练，并在Kuhn扑克等游戏中验证了其有效性。实验表明，SPIRAL可显著提升数学和通用推理能力，且多游戏训练进一步增强模型表现。结果表明，零和博弈是发展可迁移推理能力的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>基于运动不变图融合的ToF深度去噪网络</title>
<link>https://arxiv.org/abs/2506.23542</link>
<guid>https://arxiv.org/abs/2506.23542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型ToF深度去噪方法，提升时间稳定性和空间锐度。</p><br /><br /><p><strong>摘要：</strong> 本文针对ToF传感器获取的深度图像易受噪声影响的问题，提出了一种基于运动不变图融合的深度去噪网络。该方法利用跨帧几何注意力机制，结合图像平滑先验和ToF噪声分布的数据保真项，构建最大后验问题进行去噪。通过将解法展开为自适应学习的迭代滤波器，实现了高精度且可解释的去噪效果。实验结果表明，该方法在合成数据集和真实数据集上均表现出色，具有良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:29:24 GMT</pubDate>
</item>
<item>
<title>多语言模型工具调用能力提升方法研究</title>
<link>https://arxiv.org/abs/2506.23394</link>
<guid>https://arxiv.org/abs/2506.23394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过持续训练提升多语言模型的工具调用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，使多语言语言模型能够在非英语语言中实现可靠的工具调用。以保加利亚语为例，研究者对BgGPT模型系列进行了持续训练，使用了一个包含10,035个函数调用示例的双语数据集。该方法引入了TUCAN模型，在保加利亚语基准测试中实现了28.75%的函数调用准确率提升，并保持了核心语言理解能力。TUCAN模型还表现出更规范、可解析的输出格式，优于基础模型。研究提供了模型、评估框架和数据集，支持其他语言的复现与扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 16:47:27 GMT</pubDate>
</item>
<item>
<title>UrbanLLaVA：面向城市研究的多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.23219</link>
<guid>https://arxiv.org/abs/2506.23219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UrbanLLaVA提升城市多模态任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UrbanLLaVA，一个专为城市研究设计的多模态大语言模型。该模型能够同时处理多种城市数据，并在多个城市任务中表现出色。研究团队构建了一个涵盖单模态与跨模态数据的城市指令数据集，并设计了分阶段训练框架以提升空间推理和领域知识学习的兼容性。此外，还扩展了城市研究基准以评估模型性能。实验结果表明，UrbanLLaVA在多个城市任务中优于现有开源和商业模型，展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 09:04:27 GMT</pubDate>
</item>
<item>
<title>RoboScape：一种融合物理知识的统一世界模型</title>
<link>https://arxiv.org/abs/2506.23135</link>
<guid>https://arxiv.org/abs/2506.23135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboScape通过物理信息联合训练提升机器人视频生成的物理合理性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboScape，一个融合物理知识的统一世界模型，能够同时学习RGB视频生成和物理知识。该模型引入了时间深度预测和关键点动力学学习两个关键任务，以增强视频生成的3D几何一致性和复杂运动建模能力。实验表明，RoboScape在多种机器人场景中生成的视频具有更高的视觉质量和物理合理性，并在机器人策略训练和评估中展现出实用价值。研究为构建高效的物理感知世界模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 04:19:45 GMT</pubDate>
</item>
<item>
<title>Ovis-U1：一款融合多模态理解与生成能力的大型统一模型</title>
<link>https://arxiv.org/abs/2506.23044</link>
<guid>https://arxiv.org/abs/2506.23044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis-U1是一款30亿参数的多模态统一模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Ovis-U1，一款拥有30亿参数的统一模型，具备多模态理解、文本到图像生成和图像编辑能力。该模型基于Ovis系列，采用基于扩散的视觉解码器和双向标记精修器，在多个基准测试中表现优异，如OpenCompass多模态学术基准得分69.6，文本到图像生成在DPG-Bench和GenEval分别获得83.72和0.89分，图像编辑在ImgEdit-Bench和GEdit-Bench-EN分别获得4.00和6.42分。相比以往模型，Ovis-U1通过统一训练方法提升了性能，展示了多任务整合的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 20:40:17 GMT</pubDate>
</item>
<item>
<title>提出MARBLE基准测试，推动多模态推理模型发展</title>
<link>https://arxiv.org/abs/2506.22992</link>
<guid>https://arxiv.org/abs/2506.22992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARBLE挑战多模态推理模型的逐步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MARBLE，一个用于评估多模态语言模型（MLLMs）在复杂多模态问题和环境中进行逐步推理能力的基准测试。MARBLE包含两个高难度任务M-Portal和M-Cube，要求模型在空间、视觉和物理约束下制定并理解多步骤计划。实验结果显示，当前12个先进模型在M-Portal任务中表现接近随机，在M-Cube任务中准确率为0%。只有在简化子任务中部分模型优于随机基线，表明复杂推理仍是MLLMs的挑战。此外，研究发现感知能力仍是瓶颈，模型在从视觉输入中提取信息时存在困难。作者希望通过MARBLE推动下一代多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 15:44:32 GMT</pubDate>
</item>
<item>
<title>基于监听器增强的GRPO框架提升视觉语言模型对齐效果</title>
<link>https://arxiv.org/abs/2506.22832</link>
<guid>https://arxiv.org/abs/2506.22832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监听器增强的GRPO方法提升视觉语言模型的对齐性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于监听器增强的Group Relative Policy Optimization (GRPO)框架，用于提升视觉语言模型与人类偏好的对齐效果。传统奖励模型在泛化性方面存在不足，而监督微调容易导致记忆现象。尽管RL方法如GRPO有所改进，但当模型推理过程与独立的冻结视觉-语言模型（“监听器”）不一致时，推理准确性会显著下降。为此，本文引入监听器重新评估推理链，提供密集且校准的置信度评分，从而优化强化学习奖励信号。该方法不仅提高了准确率，还在大规模人类偏好数据集上提升了分布外性能，并减少了推理矛盾。实验结果表明，监听器奖励机制是一种高效、可扩展的对齐路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 05:53:17 GMT</pubDate>
</item>
<item>
<title>基于语言模型头的无训练优化方法提升推测解码性能</title>
<link>https://arxiv.org/abs/2506.22694</link>
<guid>https://arxiv.org/abs/2506.22694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VocabTrim技术优化推测解码速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需训练的优化方法，用于提升基于草稿模型的推测解码（SpD）性能。该方法在草稿生成过程中引入语言模型头（LM head），通过限制草稿模型的词汇量，仅保留目标模型中高频采样的词，从而降低内存瓶颈下的推理延迟。尽管接受率略有下降，但显著提升了生成速度，尤其在边缘设备上效果明显。实验表明，该方法在Llama-3.2-3B-Instruct模型上可提升内存瓶颈速度约16%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 20:26:40 GMT</pubDate>
</item>
<item>
<title>ThinkSound：基于思维链推理的视频到音频生成框架</title>
<link>https://arxiv.org/abs/2506.21448</link>
<guid>https://arxiv.org/abs/2506.21448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkSound通过思维链推理实现高质量视频到音频生成。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ThinkSound，一个利用思维链（CoT）推理的视频到音频生成框架。该框架将过程分为三个阶段：基础音效生成、交互式对象优化和自然语言指导的定向编辑。每个阶段都由多模态大语言模型生成上下文相关的CoT推理，以引导统一的音频基础模型。同时，研究者还发布了AudioCoT数据集，用于连接视觉内容、文本描述和声音合成。实验表明，ThinkSound在多个音频指标和CoT指标上均达到领先水平，并在Movie Gen Audio基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:32:06 GMT</pubDate>
</item>
<item>
<title>基于随机演示剪枝的新型提示设计范式</title>
<link>https://arxiv.org/abs/2506.17930</link>
<guid>https://arxiv.org/abs/2506.17930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过剪枝随机演示提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种颠覆传统思路的提示设计方法，通过剪除随机演示生成看似无意义的“乱码”提示，反而显著提升多种任务表现。该方法在不同任务和模型上均优于现有自动提示优化技术。为解决有效剪枝策略的发现难题，作者提出PromptQuine框架，利用进化搜索在低数据条件下自动寻找最优策略。该框架模仿自然界的复杂性演化机制，在仅使用上下文内token的情况下，生成高效且非传统的提示。实验表明其在分类、问答、生成和数学推理等任务中表现优异，同时具备良好的运行效率。研究希望推动对上下文学习的机制研究，并鼓励开发更开放的搜索算法以提升大语言模型提示效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 03:53:07 GMT</pubDate>
</item>
<item>
<title>SparseLoRA：通过上下文稀疏性加速大模型微调</title>
<link>https://arxiv.org/abs/2506.16500</link>
<guid>https://arxiv.org/abs/2506.16500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SparseLoRA通过稀疏性提升大模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SparseLoRA，一种利用上下文稀疏性加速大语言模型微调的方法。该方法引入轻量级、无需训练的SVD稀疏性估计器，动态选择部分权重进行损失和梯度计算，从而降低计算成本。实验表明，SparseLoRA在保持准确性的前提下，将计算成本减少最多2.2倍，并实现1.6倍的速度提升，适用于多种下游任务如常识推理、数学运算、代码生成等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:53:34 GMT</pubDate>
</item>
<item>
<title>Calligrapher：基于扩散模型的数字书法与设计框架</title>
<link>https://arxiv.org/abs/2506.24123</link>
<guid>https://arxiv.org/abs/2506.24123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Calligrapher通过创新技术实现精准风格控制和高质量字体生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Calligrapher，一个基于扩散模型的新型框架，结合文本定制与艺术排版，用于数字书法和设计应用。该框架解决了风格控制和数据依赖性的挑战，包含三个关键技术贡献：自蒸馏机制、局部风格注入框架以及上下文生成机制。这些技术提升了目标风格的精确对齐和字形定位。实验结果表明，Calligrapher在多种字体和设计场景中能够准确再现复杂的风格细节，优于传统模型，为数字艺术、品牌设计和上下文排版提供了强大支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>视频扩散模型中的稀疏注意力机制VMoBA</title>
<link>https://arxiv.org/abs/2506.23858</link>
<guid>https://arxiv.org/abs/2506.23858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VMoBA提升视频扩散模型效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对视频扩散模型的新型稀疏注意力机制——Video Mixture of Block Attention (VMoBA)。该方法通过分析预训练视频Transformer中的注意力模式，引入了三层递归块划分、全局块选择和基于阈值的块选择策略，以优化视频数据的时空特征捕捉。实验表明，VMoBA在保持生成质量的同时，显著提升了训练效率，实现了更高的FLOPs和延迟加速效果，并在无训练推理中也表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 09:52:31 GMT</pubDate>
</item>
<item>
<title>推理时技术在视觉语言模型中的有效性研究</title>
<link>https://arxiv.org/abs/2506.17417</link>
<guid>https://arxiv.org/abs/2506.17417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时技术提升VLM推理能力，但自验证能力仍不足。</p><br /><br /><p><strong>摘要：</strong> 本文研究了推理时计算技术在视觉语言模型（VLMs）中的应用效果，特别是基于强化学习（RL）训练的模型。实验表明，如多数投票和最佳N选择等解码策略能有效提升VLM的推理表现，其中生成依赖方法优于验证依赖方法。然而，与RL调优模型相关的自我修正行为（如顿悟时刻）并未带来显著提升。研究发现，RL训练的VLM在视觉和文本模态上的自我验证能力仍较弱，这是影响推理性能的关键原因。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 14:23:48 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在世界建模能力上的系统评估</title>
<link>https://arxiv.org/abs/2506.21876</link>
<guid>https://arxiv.org/abs/2506.21876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在基础世界建模能力上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于认知科学的两阶段框架，用于评估视觉语言模型（VLMs）作为内部世界模型的能力，涵盖感知和预测两个方面。研究引入了WM-ABench基准，包含23个细粒度维度，在6个模拟环境中进行测试。通过对15个最新商业和开源VLMs的660次实验，发现这些模型在基本世界建模能力上表现不佳，如无法准确区分运动轨迹，且缺乏对变量的解耦理解。结果揭示了VLMs与人类水平之间存在显著差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:24:29 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型的空间想象能力：MindCube基准与新方法</title>
<link>https://arxiv.org/abs/2506.21458</link>
<guid>https://arxiv.org/abs/2506.21458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出MindCube评估VLM空间推理能力，通过认知地图提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）是否能像人类一样从少量视角中想象完整场景。作者提出了MindCube基准，包含3,268张图像和21,154个问题，揭示现有VLM在空间推理方面表现不佳。研究评估了VLM构建空间心理模型的能力，包括位置、视角和动态模拟。通过引入未见中间视角、自然语言推理链和认知地图等方法，特别是“先生成地图再推理”的协同策略，显著提升了模型性能，准确率从37.8%提升至60.8%，进一步结合强化学习后达到70.7%。研究强调构建结构化内部空间表示对理解不可见空间的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:38:19 GMT</pubDate>
</item>
<item>
<title>TAPAS：基于多智能体的复杂任务求解框架</title>
<link>https://arxiv.org/abs/2506.19592</link>
<guid>https://arxiv.org/abs/2506.19592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAPAS通过LLM与符号规划结合，实现复杂任务自动化求解。</p><br /><br /><p><strong>摘要：</strong> TAPAS是一种多智能体框架，将大型语言模型（LLMs）与符号规划相结合，用于解决不需要手动定义环境模型的复杂任务。该框架利用专门的LLM代理协作生成和调整领域模型、初始状态和目标规范，并通过结构化工具调用机制进行交互。下游代理可以向上游代理请求修改，从而适应新的属性和约束。TAPAS还引入了类似ReAct风格的执行代理和自然语言计划翻译，以连接动态生成的计划与实际机器人能力。实验表明，TAPAS在基准规划领域和虚拟家庭模拟环境中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 09:02:06 GMT</pubDate>
</item>
<item>
<title>Fractional Reasoning：动态调整推理强度提升大语言模型性能</title>
<link>https://arxiv.org/abs/2506.15882</link>
<guid>https://arxiv.org/abs/2506.15882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fractional Reasoning通过动态调整推理强度提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练且与模型无关的Fractional Reasoning框架，能够在推理阶段动态控制推理强度。该方法通过提取与深度推理相关的潜在引导向量，并以可调缩放因子重新应用，使模型能根据输入复杂度自适应调整推理过程。该方法支持两种测试时扩展模式：提高广度策略（如Best-of-N）的输出质量，以及增强深度策略（如自省）的正确性。实验表明，Fractional Reasoning在GSM8K、MATH500和GPQA等任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 17:15:59 GMT</pubDate>
</item>
<item>
<title>基于3D代理的视频编辑框架Shape-for-Motion</title>
<link>https://arxiv.org/abs/2506.22432</link>
<guid>https://arxiv.org/abs/2506.22432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Shape-for-Motion框架，实现精准视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Shape-for-Motion的新框架，通过将视频中的目标物体转换为时间一致的3D网格代理，实现对视频内容的精确和一致编辑。该框架采用双传播策略，允许用户在单帧上进行编辑，并自动传播到其他帧。随后，3D网格被投影到2D空间生成编辑后的几何和纹理渲染图，输入到解耦视频扩散模型中生成最终结果。该方法支持多种精确且物理一致的视频编辑操作，如姿态调整、旋转、缩放、平移、纹理修改和对象合成，展示了其在高质量可控视频编辑中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>GPAS提升预归一化Transformer的训练效果</title>
<link>https://arxiv.org/abs/2506.22049</link>
<guid>https://arxiv.org/abs/2506.22049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPAS技术有效缓解Pre-LN模型激活方差问题，提升训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Gradient-Preserving Activation Scaling (GPAS) 的技术，旨在解决Pre-LN Transformer架构中激活值方差随层数增加而指数增长的问题。该技术通过缩放中间激活值但保持梯度不变，避免了梯度消失问题，同时保留激活信息。实验表明，GPAS在多种模型规模（71M到1B参数）下均能提升性能，并且对Sandwich-LN和DeepNorm等其他架构也表现出良好的适应性，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 05:45:15 GMT</pubDate>
</item>
<item>
<title>基于文本到文本回归的系统资源效率预测方法</title>
<link>https://arxiv.org/abs/2506.21718</link>
<guid>https://arxiv.org/abs/2506.21718</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到文本回归在系统资源预测中表现优异，优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 在许多行业中，预测大型系统的指标结果是一个核心问题，传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时面临挑战。本文提出了一种通用且可扩展的文本到文本回归方法。该方法在Google的Borg调度系统上取得了显著效果，使用60M参数的编码器-解码器模型，在整个集群中实现了接近完美的0.99（平均0.9）排名相关性，并且均方误差比传统方法低100倍。此外，模型仅需500个少样本示例即可适应新任务，并能捕捉复杂结果分布的密度。消融实验表明，使用编码器、增加序列长度以及模型的不确定性量化能力至关重要。这些成果为现实世界结果的通用模拟器奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21718" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 15:10:08 GMT</pubDate>
</item>
<item>
<title>基于RCME框架的视觉-语言模型层次结构学习</title>
<link>https://arxiv.org/abs/2506.21476</link>
<guid>https://arxiv.org/abs/2506.21476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RCME框架提升视觉-语言模型层次结构建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为径向跨模态嵌入（RCME）的框架，用于在视觉-语言模型中显式建模蕴含关系的传递性。该框架优化了概念在表示空间中的偏序关系，从而构建出能够表达生命树层次结构的视觉-语言基础模型。实验表明，该模型在层次分类和检索任务中优于现有最先进模型。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:05:06 GMT</pubDate>
</item>
<item>
<title>多模态上下文学习在医学任务中的挑战与评估</title>
<link>https://arxiv.org/abs/2506.21355</link>
<guid>https://arxiv.org/abs/2506.21355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大模型在医学任务中表现有限，上下文学习效果不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在医学任务中进行多模态上下文学习（ICL）的潜力与局限性。研究引入了SMMILE基准，包含111个医学问题和517个问答图像对，覆盖6个医学专科和13种影像技术。进一步扩展的SMMILE++包含1038个排列问题。实验表明，大多数模型在医学任务中表现出中等至较差的多模态ICL能力，上下文学习仅带来约8%-9.4%的性能提升。此外，研究发现不相关示例会显著降低性能，而示例顺序存在近期偏差，最后出现的相关示例可大幅提升表现。结果揭示了当前MLLM在医学多模态任务中面临的关键限制和偏见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:08:18 GMT</pubDate>
</item>
<item>
<title>Confucius3-Math：面向中国K-12数学教育的高效大语言模型</title>
<link>https://arxiv.org/abs/2506.18330</link>
<guid>https://arxiv.org/abs/2506.18330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Confucius3-Math在单块消费级GPU上实现高效运行并取得SOTA数学推理性能。</p><br /><br /><p><strong>摘要：</strong> Confucius3-Math是一款开源的大语言模型，拥有140亿参数，能够在单块消费级GPU上高效运行，并在多项数学推理任务中表现优异，超越了许多参数量更大的模型。该模型专为提升中国K-12阶段数学教育和知识传播而设计，通过大规模强化学习微调，与国家课程标准对齐，擅长解决主流数学问题。研究中提出了三项技术创新，包括目标熵正则化、近期样本恢复和策略特定难度加权，显著提升了训练稳定性、数据效率和模型性能。项目已开源，代码和模型可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 02:23:53 GMT</pubDate>
</item>
<item>
<title>基于贝叶斯框架的上下文学习策略分析</title>
<link>https://arxiv.org/abs/2506.17859</link>
<guid>https://arxiv.org/abs/2506.17859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了上下文学习中策略选择的贝叶斯机制。</p><br /><br /><p><strong>摘要：</strong> 本文通过贝叶斯预测器统一解释了上下文学习（ICL）中的不同策略，提出了一种层次化贝叶斯框架，能够准确预测Transformer模型的下一个词预测行为。该框架将预训练视为策略后验概率的更新过程，并在推理时对不同策略进行加权平均。研究强调了策略选择中损失与复杂度之间的权衡，解释了已知的ICL现象并提出了新的预测，如任务多样性增加时从泛化到记忆的过渡时间呈超线性增长。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 19:49:08 GMT</pubDate>
</item>
<item>
<title>基于视觉对比的链式推理方法研究</title>
<link>https://arxiv.org/abs/2506.22434</link>
<guid>https://arxiv.org/abs/2506.22434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过视觉对比训练模型进行链式推理，无需人工标注数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的方法，利用视觉对比任务来增强模型的链式推理能力。该方法基于自监督学习，构建图像三元组，包括同一图像的两个增强视图和一个相似但不同的图像。模型在训练过程中被提示生成推理过程以比较这些图像，并通过规则强化学习进行优化。由于图像间的高度相似性和增强操作，模型必须关注细微的视觉变化并进行逻辑推理。实验表明，尽管仅在视觉对比任务上训练，该方法在多图像推理基准测试中表现出色，并在通用视觉任务中也展现出强大性能，且无需依赖人工标注的问答对。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在科学再现任务中的能力</title>
<link>https://arxiv.org/abs/2506.22419</link>
<guid>https://arxiv.org/abs/2506.22419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大语言模型在再现已有研究成果方面仍存在挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Automated LLM Speedrunning Benchmark，用于评估AI代理在科学领域再现已有成果的能力。该基准基于NanoGPT速度竞赛，包含19个任务，提供训练脚本和不同形式的提示。尽管使用最新的推理模型和最先进的工具，AI仍难以复现已知创新，表明科学再现仍是AI研究的重要挑战。该基准为衡量AI自动化科学再现能力提供了有效手段。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>基于视觉-语言预训练的RetFiner提升OCT图像分类性能</title>
<link>https://arxiv.org/abs/2506.22149</link>
<guid>https://arxiv.org/abs/2506.22149</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RetFiner通过视觉-语言预训练提升OCT图像分类效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RetFiner的自监督学习视觉-语言优化方法，用于改进现有视网膜OCT基础模型的表示能力。该方法利用文本数据中的丰富监督信号，提升模型在多种复杂任务上的表现。实验结果显示，RetFiner在七个多样化OCT分类任务中分别提升了5.8、3.9和2.1个百分点。研究展示了该方法在无需额外标注的情况下，有效提升模型适应特定人群和应用场景的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22149" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 07:53:54 GMT</pubDate>
</item>
<item>
<title>XVerse：实现多主体精细控制的文本生成模型</title>
<link>https://arxiv.org/abs/2506.21416</link>
<guid>https://arxiv.org/abs/2506.21416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XVerse提升多主体图像生成的可控性与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型多主体可控生成模型XVerse，解决文本到图像生成中多主体身份和语义属性（如姿态、风格、光照）精细控制的问题。通过将参考图像转换为针对特定标记的文本流调制偏移量，XVerse实现了对特定主体的精确独立控制，同时不影响图像潜在表示或特征。该方法提升了多主体图像合成的保真度和可编辑性，增强了个性化和复杂场景生成能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:04:16 GMT</pubDate>
</item>
<item>
<title>ShotBench与ShotVL：推动电影语言理解的AI基准与模型</title>
<link>https://arxiv.org/abs/2506.21356</link>
<guid>https://arxiv.org/abs/2506.21356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ShotBench和ShotVL，提升AI对电影语言的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ShotBench，一个专为电影语言理解设计的全面基准，包含3.5k专家标注的问答对，覆盖8个关键摄影维度。评估显示现有VLM在理解电影细节方面存在明显不足，最高准确率低于60%。为此，研究构建了70k规模的ShotQA数据集，并基于此训练出ShotVL模型，在ShotBench上取得最佳表现。研究开源了模型、数据和代码，以促进AI在电影理解与生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:09:21 GMT</pubDate>
</item>
<item>
<title>DenseDiT：基于生成模型的密集预测方法在真实场景中的应用</title>
<link>https://arxiv.org/abs/2506.20279</link>
<guid>https://arxiv.org/abs/2506.20279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DenseDiT通过生成模型提升真实场景下的密集预测性能。</p><br /><br /><p><strong>摘要：</strong> 密集预测任务在计算机视觉中具有重要意义，旨在为输入图像学习像素级标注。然而，现有方法主要针对理想条件设计，难以适应真实世界场景，且面临真实数据稀缺的问题。为此，研究者提出了DenseWorld基准，涵盖25个密集预测任务，并引入DenseDiT方法，利用生成模型的视觉先验，通过统一策略执行多种真实场景下的密集预测任务。DenseDiT采用参数复用机制和两个轻量分支，仅需不到0.1%的额外参数即可有效集成多尺度上下文信息。实验表明，DenseDiT在DenseWorld上表现优于现有基线，使用不到0.01%的训练数据即可取得优异结果，展现出其在实际部署中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 05:40:50 GMT</pubDate>
</item>
<item>
<title>ARK：一个面向自主机器人的Python优先开源框架</title>
<link>https://arxiv.org/abs/2506.21628</link>
<guid>https://arxiv.org/abs/2506.21628</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARK通过Python统一机器人与AI实践，加速自主机器人研发。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ARK，一个面向自主机器人的开源Python框架。当前机器人软件栈面临学习曲线陡峭、工具碎片化等问题，而ARK旨在解决这些瓶颈。它提供类似Gym的环境接口，支持数据收集、预处理和模仿学习算法训练，并可无缝切换仿真与真实机器人。ARK采用轻量级客户端-服务器架构，支持C/C++绑定以保证实时性能。框架包含控制、SLAM、运动规划等模块，并兼容ROS。丰富的文档和案例展示了其在快速原型设计和端到端流程方面的优势，有助于降低进入门槛并推动机器人研究与商业化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21628" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 16:23:39 GMT</pubDate>
</item>
<item>
<title>基于噪声一致性的高效可控生成方法NCT</title>
<link>https://arxiv.org/abs/2506.19741</link>
<guid>https://arxiv.org/abs/2506.19741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NCT实现高效可控内容生成，无需重训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为噪声一致性训练（NCT）的新方法，用于在不重新训练基础模型的情况下，将新的控制信号直接整合到预训练的一步生成器中。NCT通过引入适配模块和噪声空间中的噪声一致性损失，使生成模型在不同条件下的行为保持一致，从而实现对新控制条件的适应。该方法具有模块化、数据效率高和易于部署的优点，实验表明其在单次前向传播中即可实现最先进的可控生成效果，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 11:58:55 GMT</pubDate>
</item>
<item>
<title>BlenderFusion：一种生成式视觉合成框架</title>
<link>https://arxiv.org/abs/2506.17450</link>
<guid>https://arxiv.org/abs/2506.17450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlenderFusion通过重组对象、相机和背景生成新场景。</p><br /><br /><p><strong>摘要：</strong> BlenderFusion是一种生成式视觉合成框架，能够通过重组对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程：首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D对齐控制编辑，最后通过生成式合成器将它们融合成连贯场景。该框架扩展了预训练扩散模型，支持同时处理原始和编辑后的场景，并通过源掩码和模拟物体抖动等训练策略提升合成效果。实验表明，BlenderFusion在复杂合成场景编辑任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 15:38:34 GMT</pubDate>
</item>
<item>
<title>Gazal-R1：医疗推理领域的高性能语言模型</title>
<link>https://arxiv.org/abs/2506.21594</link>
<guid>https://arxiv.org/abs/2506.21594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gazal-R1在医疗推理任务中表现卓越，提供透明解释。</p><br /><br /><p><strong>摘要：</strong> Gazal-R1是一款基于Qwen3 32B的320亿参数语言模型，在医疗推理任务中表现出色，并能提供清晰、分步的临床决策解释。通过两阶段训练方法，包括监督微调和强化学习，该模型在多个医学基准测试中超越了更大规模的模型。研究还揭示了在专业领域训练推理模型所面临的挑战，如奖励欺骗、训练不稳定性和事实记忆与详细推理之间的权衡。该方法为开发高性能、高效且可解释的领域专用语言模型提供了可复现的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 05:44:21 GMT</pubDate>
</item>
<item>
<title>Mixture of Grouped Experts 提升大模型推理效率与负载均衡</title>
<link>https://arxiv.org/abs/2505.21411</link>
<guid>https://arxiv.org/abs/2505.21411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoGE优化专家负载，提升大模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Mixture of Grouped Experts (MoGE)，这是一种改进的专家混合架构，通过将专家分组并限制每个组内激活的专家数量，实现更均衡的计算负载。相比传统MoE，MoGE在分布式设备上能显著提升吞吐量，特别是在推理阶段。基于MoGE构建的Pangu Pro MoE模型拥有720亿参数，其中160亿参数在每token中被激活，经过优化后在Ascend NPUs上表现出色，推理速度达1148 tokens/s，经推测加速后可达1528 tokens/s，优于同类模型。实验表明，MoGE在训练和推理中均表现出更高的效率和性价比。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:40:21 GMT</pubDate>
</item>
<item>
<title>LLaVA-Scissor：一种用于视频多模态大语言模型的无训练令牌压缩策略</title>
<link>https://arxiv.org/abs/2506.21862</link>
<guid>https://arxiv.org/abs/2506.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-Scissor通过语义连通组件实现高效视频令牌压缩。</p><br /><br /><p><strong>摘要：</strong> 本文提出LLaVA-Scissor，一种无需训练的视频多模态大语言模型令牌压缩策略。与以往依赖注意力得分的方法不同，该方法利用语义连通组件（SCC）将令牌分配到不同的语义区域，确保全面的语义覆盖。该策略在时空域中应用SCC，有效压缩令牌并以非重叠语义令牌表示整个视频。在多个视频理解基准测试中，LLaVA-Scissor表现出色，尤其在低保留率下表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 22:29:58 GMT</pubDate>
</item>
<item>
<title>SpatialReasoner-R1：提升视觉语言模型空间推理能力的新方法</title>
<link>https://arxiv.org/abs/2506.21656</link>
<guid>https://arxiv.org/abs/2506.21656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialReasoner-R1通过新方法提升空间推理性能。</p><br /><br /><p><strong>摘要：</strong> 当前视觉语言模型在细粒度空间推理方面存在不足，尤其是在多步骤逻辑和精确空间对齐方面。本文提出SpatialReasoner-R1模型，并引入M3CTS方法生成多样且逻辑一致的LongCoT推理轨迹，同时提出fDPO方法，通过空间奖励机制提升描述性定位和逻辑推理能力。实验表明，fDPO在空间质量任务中提升4.1%，在空间数量任务中提升9.0%。SpatialReasoner-R1在SPATIALRGPT-Bench上达到新SOTA，平均准确率超过最强基线9.8%，并在通用视觉语言任务中保持竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
<title>基于人体动作的自我中心视频预测模型PEVA</title>
<link>https://arxiv.org/abs/2506.21552</link>
<guid>https://arxiv.org/abs/2506.21552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PEVA模型通过人体姿态预测第一视角视频，提升环境模拟能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PEVA模型，旨在根据过去视频和由相对3D人体姿态表示的动作，预测自我中心视频。通过基于身体关节层次结构的运动轨迹进行条件建模，该模型学习如何从第一人称视角模拟人类行为对环境的影响。研究在Nymeria大规模真实世界自我中心视频与人体姿态数据集上训练了一个自回归条件扩散Transformer，并设计了分层评估协议以全面分析模型的具身预测与控制能力。该工作是首次尝试从人类视角出发，建模复杂现实环境和具身代理行为的视频预测方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>大语言模型预训练中的grokking现象与泛化机制研究</title>
<link>https://arxiv.org/abs/2506.21551</link>
<guid>https://arxiv.org/abs/2506.21551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大模型预训练中存在grokking现象，揭示了泛化能力的形成机制。</p><br /><br /><p><strong>摘要：</strong> 本文首次在7B参数的大语言模型OLMoE的一次性预训练过程中观察到grokking现象，即测试性能在训练损失收敛后仍持续提升。研究通过评估多种基准任务（如数学推理、代码生成和常识知识检索）验证了这一现象，并进一步分析了模型内部动态。发现样本路径从随机、实例特定逐渐演变为结构化且可共享，同时路径复杂度降低，表明模型经历了从记忆到泛化的转变。研究还提出了两个新指标，用于量化路径距离和复杂度，能够有效预测下游任务的泛化表现，具有实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>SAM4D：多模态时序基础模型用于相机与激光雷达的可提示分割</title>
<link>https://arxiv.org/abs/2506.21547</link>
<guid>https://arxiv.org/abs/2506.21547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAM4D实现相机与激光雷达的跨模态分割与高效数据标注。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAM4D，一个面向相机和激光雷达流的多模态时序基础模型，支持可提示分割。通过引入统一多模态位置编码（UMPE）将相机和激光雷达特征对齐到共享3D空间，实现跨模态提示与交互。同时，提出运动感知跨模态记忆注意力机制（MCMA），利用自运动补偿提升时间一致性和长时程特征检索能力。为避免标注瓶颈，开发了多模态自动化数据引擎，结合视频掩码生成、时空4D重建和跨模态掩码融合，以极高速度生成高质量伪标签，保留语义信息。实验表明SAM4D在Waymo-4DSeg数据集上展现出强大的跨模态分割能力和数据标注潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>WorldVLA：统一动作与图像理解的自回归世界模型</title>
<link>https://arxiv.org/abs/2506.21539</link>
<guid>https://arxiv.org/abs/2506.21539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldVLA通过整合动作与图像理解提升环境预测与行动生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出WorldVLA，一个将动作与图像理解及生成统一的自回归世界模型。该模型结合视觉-语言-动作（VLA）框架与世界模型，通过动作和图像理解预测未来图像，以学习环境物理规律并优化动作生成。同时，动作模型根据图像观测生成后续动作，辅助视觉理解和图像生成。实验表明，WorldVLA优于独立的动作和世界模型，但自回归生成动作序列时性能下降，原因在于模型对动作预测的泛化能力有限，导致误差累积。为此，作者提出注意力掩码策略，在生成当前动作时屏蔽先前动作，显著提升了动作块生成的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:55:40 GMT</pubDate>
</item>
<item>
<title>MADrive：基于记忆增强的自动驾驶场景重建方法</title>
<link>https://arxiv.org/abs/2506.21520</link>
<guid>https://arxiv.org/abs/2506.21520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MADrive通过外部记忆库实现更真实的自动驾驶场景合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MADrive，一种基于记忆增强的场景重建框架，旨在提升自动驾驶环境的逼真度。该方法利用大规模外部记忆库中的3D资产替换原始观测中的车辆，实现更逼真的场景合成。研究团队发布了包含约7万段360度汽车视频的MAD-Cars数据集，并开发了检索模块，用于在记忆库中找到相似车辆实例，进行3D重建并整合到目标场景中。实验表明，该方法可生成多视角完整车辆表示，支持显著改变或新驾驶场景的高质量合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:41:07 GMT</pubDate>
</item>
<item>
<title>Mind2Web 2：面向智能搜索系统的长期任务基准与评估框架</title>
<link>https://arxiv.org/abs/2506.21506</link>
<guid>https://arxiv.org/abs/2506.21506</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mind2Web 2为智能搜索系统提供长期任务基准与评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mind2Web 2，一个包含130个高质、长周期任务的基准测试集，旨在评估自主网络搜索系统的能力。该基准通过超过1000小时的人工劳动构建，支持实时浏览和信息整合。研究提出了一种新的Agent-as-a-Judge评估框架，利用树状评分标准自动判断答案准确性和来源归属。实验评估了九个前沿系统及人类表现，并分析了错误原因，为未来研究提供了方向。结果显示，OpenAI Deep Research已能实现人类50%-70%的性能，且耗时更少。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21506" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:32:50 GMT</pubDate>
</item>
<item>
<title>FairyGen：基于单幅儿童画生成叙事动画视频的系统</title>
<link>https://arxiv.org/abs/2506.21272</link>
<guid>https://arxiv.org/abs/2506.21272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FairyGen从单张儿童画生成风格一致、叙事连贯的动画视频。</p><br /><br /><p><strong>摘要：</strong> FairyGen是一个自动系统，能够从一张儿童的绘画中生成具有叙事性的卡通视频，并忠实保留其艺术风格。该系统通过分离角色建模与风格化背景生成，并引入电影镜头设计来增强表达和连贯性。它首先利用多模态大语言模型生成结构化的分镜脚本，再通过风格传播适配器保持角色视觉风格的一致性。此外，系统还包含镜头设计模块和两阶段运动定制适配器，以提升视频的多样性和电影感。实验表明，FairyGen能够生成风格忠实、叙事自然的动画，具有个性化和互动性强的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:58:16 GMT</pubDate>
</item>
<item>
<title>DiLoCoX：一种用于超大规模模型的低通信去中心化训练框架</title>
<link>https://arxiv.org/abs/2506.21263</link>
<guid>https://arxiv.org/abs/2506.21263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiLoCoX实现107B模型在1Gbps网络上的高效分布式训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiLoCoX，一个适用于超大规模语言模型（超过1000亿参数）的低通信去中心化训练框架。该框架结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案，显著提升了训练规模和速度。理论分析验证了通信与训练重叠及梯度压缩的有效性。实验表明，在1Gbps网络下，DiLoCoX可成功预训练107B模型，并比传统AllReduce方法快357倍，同时保持模型收敛性几乎不变。这是首个成功应用于1000亿参数以上模型的去中心化训练框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:45:04 GMT</pubDate>
</item>
<item>
<title>动态跳过中间层的Transformer架构优化研究</title>
<link>https://arxiv.org/abs/2506.21103</link>
<guid>https://arxiv.org/abs/2506.21103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态跳过中间层的Transformer架构，提升效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的Transformer架构，通过动态跳过中间层来提高计算效率。该方法基于输入内容决定是否跳过对称的中间块，并引入门控机制防止后续token访问被跳过的token位置。同时采用残差归一化方案和自适应正则化损失控制门控稀疏性。尽管旨在降低简单token的计算需求并促进多层级表征结构，但实验结果显示在所研究规模下，该方法在验证交叉熵与估计FLOPs之间的权衡上未优于较少层数的密集基线模型。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 05:01:19 GMT</pubDate>
</item>
<item>
<title>PhysRig：基于物理的可微分皮肤绑定与骨骼框架</title>
<link>https://arxiv.org/abs/2506.20936</link>
<guid>https://arxiv.org/abs/2506.20936</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PhysRig，解决传统LBS的变形缺陷，提升动画真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出PhysRig，一种基于物理的可微分皮肤绑定与骨骼框架，旨在克服传统线性混合皮肤（LBS）在模拟软组织、毛发等弹性材料时的局限性。通过将刚性骨骼嵌入体积表示（如四面体网格），并将其作为可变形软体结构进行模拟，PhysRig结合连续力学与粒子离散化方法，实现对材料属性和骨骼运动的可微分建模。同时引入材料原型以降低学习空间复杂度。实验表明，该方法在多个数据集上优于传统LBS方法，生成更真实、物理合理的动画效果，并展示了其在姿态迁移任务中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20936" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 21:58:09 GMT</pubDate>
</item>
<item>
<title>基于神经符号的高效图像编辑代理FaSTA^*</title>
<link>https://arxiv.org/abs/2506.20911</link>
<guid>https://arxiv.org/abs/2506.20911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaSTA^*通过结合LLM和A^*搜索实现高效图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FaSTA^*的神经符号代理，用于处理复杂的多轮图像编辑任务。该方法结合了大语言模型（LLMs）的快速高层次子任务规划与每个子任务中的慢速、精确的工具使用和局部A^*搜索，以找到成本高效的工具路径。通过LLMs对以往成功工具路径进行归纳推理，提取并优化常用子程序，并将其作为新工具在未来的任务中重复使用，从而显著降低了相似子任务的探索成本。FaSTA^*首先由LLMs进行快速子任务规划和规则子程序选择，仅在遇到新颖或困难子任务时才激活慢速A^*搜索。实验表明，FaSTA^*在计算效率上优于现有方法，同时保持了与最先进基线相当的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 20:33:43 GMT</pubDate>
</item>
<item>
<title>基于生成块的世界：通过几何抽象交互生成图像场景</title>
<link>https://arxiv.org/abs/2506.20703</link>
<guid>https://arxiv.org/abs/2506.20703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过3D几何体编辑生成高质量图像，提升可编辑性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Generative Blocks World的方法，通过操纵简单的几何抽象来与生成图像的场景进行交互。该方法将场景表示为凸3D基本形状的组合，并允许通过不同数量的形状进行结构或细节的编辑。编辑后的场景通过流模型生成图像，且图像生成条件包括深度信息和纹理提示。该纹理提示考虑了修改后的3D形状，优于现有键值缓存技术的纹理一致性。实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态搜索框架MMSearch-R1</title>
<link>https://arxiv.org/abs/2506.20670</link>
<guid>https://arxiv.org/abs/2506.20670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMSearch-R1实现端到端多模态搜索，提升信息获取效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MMSearch-R1，首个基于强化学习的多模态搜索框架，使大型多模态模型能够在真实网络环境中进行按需、多轮搜索。该框架整合图像和文本搜索工具，并通过基于结果的奖励机制引导模型决策。研究构建了一个多模态搜索VQA数据集，并筛选出搜索平衡子集以优化搜索行为。实验表明，该模型在知识密集型任务中表现优于现有RAG方法，且减少30%以上的搜索调用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>DeepRare：基于大语言模型的罕见病诊断系统</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepRare利用大语言模型实现罕见病精准诊断，准确率达100%。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepRare，一个基于大语言模型的罕见病诊断系统。该系统能够处理多种临床输入，并生成带有透明推理链的诊断假设，提升罕见病的诊断准确性。DeepRare包含三个核心组件：具备长期记忆的中央主机、负责领域特定分析的代理服务器以及集成40多个工具和最新医学知识的模块化设计。在8个数据集上的评估显示，DeepRare在2919种疾病中实现了100%的准确率，HPO评估中Recall@1达到57.18%，显著优于其他方法。此外，系统已上线为网页应用，便于临床使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 09:42:26 GMT</pubDate>
</item>
<item>
<title>MuseControlLite：轻量级文本到音乐生成模型微调机制</title>
<link>https://arxiv.org/abs/2506.18729</link>
<guid>https://arxiv.org/abs/2506.18729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MuseControlLite提升音乐生成模型的控制精度与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MuseControlLite，一种轻量级机制，用于微调文本到音乐生成模型，以实现对时间变化音乐属性和参考音频信号的精确控制。研究发现，位置嵌入在时间相关条件中至关重要。实验表明，在解耦交叉注意力层中添加旋转位置嵌入可将控制准确率从56.6%提升至61.1%，且参数量仅为现有方法的1/6.75。该方法在旋律控制、音频修复和扩展等方面表现优于MusicGen-Large和Stable Audio Open ControlNet，仅需85M可训练参数。相关代码、模型和示例已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:08:03 GMT</pubDate>
</item>
<item>
<title>DuaShepherd：融合正确性与潜力的奖励建模框架提升大语言模型数学推理能力</title>
<link>https://arxiv.org/abs/2506.17533</link>
<guid>https://arxiv.org/abs/2506.17533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuaShepherd通过结合正确性和潜力信号提升LLM数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuaShepherd，一种融合正确性与潜力奖励信号的新型奖励建模框架，旨在增强大语言模型（LLM）的数学推理能力。正确性信号关注步骤错误识别，潜力信号则侧重最终答案的可达性。研究构建了包含这两种信号的大规模奖励建模数据集，并采用多头统一架构在多任务设置中同时训练两个奖励模型。通过将两种信号合并为复合概率，模型在多个基准测试中均表现出色，尤其在MATH500和ProcessBench上优于仅使用单一奖励信号的模型，达到当前最优性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:11:01 GMT</pubDate>
</item>
<item>
<title>基于用户偏好的大语言模型路由框架</title>
<link>https://arxiv.org/abs/2506.16655</link>
<guid>https://arxiv.org/abs/2506.16655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种匹配用户偏好的模型路由方法，提升模型选择的灵活性和透明度。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的快速发展，不同模型在性能、风格和成本上各有优势，模型路由成为关键操作技术。然而，现有方法在评估性能时依赖不反映用户主观偏好的基准，并且模型选择范围有限。本文提出一种偏好对齐的路由框架，通过将查询与用户定义的领域或操作类型匹配，实现更灵活和透明的模型选择。我们引入了Arch-Router，一个1.5B参数的小型模型，能够学习将查询映射到对应的领域-操作偏好。该方法支持无缝添加新模型而无需重新训练或修改架构。实验表明，该方法在对话数据集上优于顶级专有模型，能更好地捕捉主观评价标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 19:57:41 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的超启发式框架HeurAgenix在组合优化中的应用</title>
<link>https://arxiv.org/abs/2506.15196</link>
<guid>https://arxiv.org/abs/2506.15196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeurAgenix通过LLM自动演化和选择启发式算法，提升组合优化效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出HeurAgenix，一个基于大语言模型（LLM）的两阶段超启发式框架，用于解决组合优化问题。该框架首先利用LLM比较种子启发式解与高质量解，提取可复用的演化策略；随后在求解过程中动态选择最合适的启发式方法。为提高灵活性，可以选择高性能LLM或轻量级模型。为应对监督信号不足的问题，采用双奖励机制进行微调，提升在噪声标注下的鲁棒性。实验表明，HeurAgenix在多个基准测试中表现优于现有方法，甚至超越专用求解器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 03:20:01 GMT</pubDate>
</item>
<item>
<title>AnimaX：一种基于视频扩散模型的3D动画生成框架</title>
<link>https://arxiv.org/abs/2506.19851</link>
<guid>https://arxiv.org/abs/2506.19851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimaX通过视频扩散模型实现跨骨架的3D动画生成。</p><br /><br /><p><strong>摘要：</strong> AnimaX是一种新型的前馈式3D动画框架，能够将视频扩散模型中的运动先验与基于骨骼的动画控制结构相结合。该方法克服了传统运动合成方法在固定骨骼拓扑或高维变形空间优化方面的限制，支持任意骨骼结构的3D网格动画生成。通过多视角、多帧2D姿态图表示3D运动，并结合模板渲染和文本运动提示进行联合视频-姿态扩散，实现了视频先验到运动生成任务的有效迁移。该框架在16万条带绑定序列的数据集上训练，取得了VBench基准测试中在泛化性、运动保真度和效率方面的最佳表现，为无类别限制的3D动画提供了一种可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ScaleCap：一种可扩展的图像描述生成策略</title>
<link>https://arxiv.org/abs/2506.19848</link>
<guid>https://arxiv.org/abs/2506.19848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ScaleCap，提升图像描述的准确性和平衡性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScaleCap，一种在推理阶段可扩展的图像描述生成策略，旨在解决大型视觉语言模型（LVLMs）中存在的多模态偏见和语言偏见问题。通过引入启发式问答和对比句评分两个新组件，ScaleCap能够逐步丰富并校准描述内容，提升描述的准确性、平衡性和信息量。实验表明，使用ScaleCap标注45万张图像进行预训练，可在11个基准测试中取得一致的性能提升，并在VQA任务和从描述重建图像的任务中展现出优秀的描述质量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于级联视频超分辨率的高效视频生成方法研究</title>
<link>https://arxiv.org/abs/2506.19838</link>
<guid>https://arxiv.org/abs/2506.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效视频生成框架，提升高分辨率输出效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了级联视频超分辨率（VSR）模型在视频生成中的关键设计原则。通过分离语义内容生成与细节合成两个阶段，先使用低分辨率基础模型生成内容，再通过轻量级VSR模型提升分辨率。作者提出了两种退化策略以增强训练数据的匹配性，并分析了时间步采样和噪声增强对低分辨率输入的影响。此外，引入了交错时间单元和稀疏局部注意力机制，显著降低了计算开销。实验表明，该框架优于现有方法，为高效级联视频生成提供了有效基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:57:26 GMT</pubDate>
</item>
<item>
<title>基于知识增强的强化学习缓解大语言模型幻觉问题</title>
<link>https://arxiv.org/abs/2506.19807</link>
<guid>https://arxiv.org/abs/2506.19807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KnowRL通过事实奖励提升模型推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）在推理过程中出现的严重幻觉问题，提出了一种名为KnowRL的知识增强强化学习方法。该方法在强化学习训练中引入基于知识验证的事实性奖励，引导模型进行以事实为基础的慢思考，从而帮助模型识别其知识边界。实验结果表明，KnowRL有效减少了慢思考模型的幻觉现象，同时保持了其原有的强推理能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:17:17 GMT</pubDate>
</item>
<item>
<title>提升开源大语言模型数据分析能力的研究</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何增强开源大语言模型的数据分析能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何提升开源大语言模型在数据密集型任务中的表现。通过构建多样化的现实场景数据集，从数据理解、代码生成和战略规划三个维度评估模型性能，发现战略规划质量、交互设计与任务复杂度以及数据质量是影响模型表现的关键因素。基于这些发现，作者提出了一种数据合成方法，显著提升了开源大语言模型的分析推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:04:23 GMT</pubDate>
</item>
<item>
<title>SRFT：统一SFT与RL的单阶段语言模型微调方法</title>
<link>https://arxiv.org/abs/2506.19767</link>
<guid>https://arxiv.org/abs/2506.19767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SRFT提升数学推理任务性能，优于传统两阶段方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了监督微调（SFT）和强化学习（RL）在大型语言模型中的整合问题。通过分析令牌分布、学习动态和熵指标，发现SFT带来全局性的策略变化，而RL则进行细粒度优化。基于此，作者提出了一种单阶段的监督强化微调方法（SRFT），结合SFT与RL的优势，直接利用演示数据和自探索轨迹进行优化。实验表明，SRFT在五项数学推理基准上平均准确率达到59.1%，比零RL方法高出9.0%，在三项分布外基准上高出10.9%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 12:31:37 GMT</pubDate>
</item>
<item>
<title>自动化数据集构建提升LLM在软件工程任务中的表现</title>
<link>https://arxiv.org/abs/2506.19290</link>
<guid>https://arxiv.org/abs/2506.19290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化数据集提升LLM在SWE任务中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种自动化的软件工程（SWE）数据集构建流程，旨在解决传统手动标注和环境设置耗时的问题。该数据集包含来自2531个GitHub仓库的10,169个真实Python任务实例，并附带自然语言描述和运行环境镜像。通过在这些数据上微调Skywork-SWE模型，研究发现模型性能随着数据量增加而持续提升，未出现饱和现象。在SWE-bench Verified基准测试中，Skywork-SWE模型达到38.0%的pass@1准确率，成为Qwen2.5-Coder-32B模型中的新SOTA。结合测试时缩放技术后，准确率进一步提升至47.0%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 23:53:36 GMT</pubDate>
</item>
<item>
<title>统一音频表示学习方法USAD在多类型音频任务中表现优异</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">USAD实现语音与音频的统一表示学习，性能接近最先进水平。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为通用语音与音频蒸馏（USAD）的统一音频表示学习方法，能够整合语音、声音和音乐等多种音频类型。USAD通过从特定领域的自监督学习模型中进行高效层间蒸馏，训练一个单一模型以处理多种音频任务。该方法在多个基准测试中表现出色，包括语音处理、音频标记和声音分类等任务，在SUPERB和HEAR基准上取得了接近最先进的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的智能照片修图系统 JarvisArt</title>
<link>https://arxiv.org/abs/2506.17612</link>
<guid>https://arxiv.org/abs/2506.17612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JarvisArt通过AI实现高效、精准的照片修图，提升用户体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 JarvisArt，一个基于多模态大语言模型（MLLM）的智能照片修图系统。该系统能够理解用户意图，模拟专业艺术家的思考过程，并协调 Lightroom 中超过 200 种修图工具进行自动化操作。通过两阶段训练方法（Chain-of-Thought 监督微调和 GRPO-R 策略优化），JarvisArt 在全局与局部调整上表现出色，具备出色的泛化能力和精细控制。研究还构建了 MMArt-Bench 基准测试集以评估性能，结果显示 JarvisArt 在内容保真度方面优于 GPT-4o，提升了 60% 的像素级指标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 02:36:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理一致性的强化学习方法研究</title>
<link>https://arxiv.org/abs/2506.16141</link>
<guid>https://arxiv.org/abs/2506.16141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRPO-CARE提升多模态模型推理一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型（MLLMs）的推理能力不足问题，提出了一种新的强化学习框架GRPO-CARE。该框架通过引入双层奖励机制，在提升答案正确性的同时增强推理步骤与答案之间的逻辑一致性。研究还构建了SEED-Bench-R1基准，用于评估MLLMs在复杂视频任务中的泛化能力。实验表明，GRPO-CARE在多个挑战性场景中均优于传统GRPO方法，特别是在最困难的评估级别上提升了6.7%，并显著提高了推理一致性。该方法为开发更可解释、更稳健的多模态模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:49:13 GMT</pubDate>
</item>
<item>
<title>代码转换对大语言模型理解能力的影响研究</title>
<link>https://arxiv.org/abs/2506.14012</link>
<guid>https://arxiv.org/abs/2506.14012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">代码转换影响LLM理解，嵌入外语可提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了代码转换（CSW）对大型语言模型（LLMs）理解能力的影响。研究表明，在多语言社区和在线内容中，代码转换现象日益普遍，而LLMs在处理此类混合语言文本时表现出不同的性能变化。当外语词汇干扰英语文本时，模型表现有所下降，但在将英语嵌入其他语言的情况下，理解能力反而提高。尽管提示方法效果不一，但微调策略在缓解性能下降方面更为稳定有效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 17:19:27 GMT</pubDate>
</item>
<item>
<title>4D-LRM：大规模时空重建模型实现任意视角与时间的高质量渲染</title>
<link>https://arxiv.org/abs/2506.18890</link>
<guid>https://arxiv.org/abs/2506.18890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4D-LRM实现从少量视角到任意视角和时间的高质量4D重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出4D-LRM，这是首个大规模4D重建模型，能够从非受限视角和时间戳输入中学习统一的时空表示，并直接预测每像素的4D高斯基元，从而实现高速、高质量的任意视角-时间组合渲染。相比传统方法，4D-LRM在效率、泛化性和真实性方面表现更优。实验表明，该模型能泛化到新物体、跨时间插值，并适应多种相机设置，在单块A100 GPU上可于1.5秒内完成24帧序列的重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:57:47 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态大语言模型个性化图像描述方法</title>
<link>https://arxiv.org/abs/2506.18369</link>
<guid>https://arxiv.org/abs/2506.18369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种强化学习框架提升MLLM个性化图像描述能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在生成个性化图像描述时的不足，提出了一种基于强化学习的后训练框架。尽管现有方法通过监督微调已取得一定进展，但在复杂场景下仍存在描述不准确的问题。由于高质量标注数据获取困难，作者采用强化学习策略，有效提升了模型的视觉识别和个性化生成能力，并在多概念图像描述任务中表现优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:55:52 GMT</pubDate>
</item>
<item>
<title>基于LLM代理的复杂规格到RTL代码生成系统</title>
<link>https://arxiv.org/abs/2506.13905</link>
<guid>https://arxiv.org/abs/2506.13905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Spec2RTL-Agent，实现从复杂规格自动生成RTL代码。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前LLM在生成硬件RTL代码时存在的实际应用与需求之间的差距，提出了一种名为Spec2RTL-Agent的LLM代理系统。该系统通过多代理协作框架，包括推理理解模块、逐步编码与提示优化模块以及自适应反思模块，直接处理复杂的规格文档并生成对应的RTL代码。与传统方法不同，该系统先生成可综合的C++代码再进行HLS优化，提高了代码的正确性和兼容性。实验表明，该系统在三个规格文档上的测试中，减少了75%的人工干预，成为首个完全自动化的从非结构化规格生成RTL代码的多代理系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 14:33:25 GMT</pubDate>
</item>
<item>
<title>基于两阶段优化的长视频照明编辑方法TC-Light</title>
<link>https://arxiv.org/abs/2506.18904</link>
<guid>https://arxiv.org/abs/2506.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TC-Light方法，实现高效且时间一致的视频照明编辑。</p><br /><br /><p><strong>摘要：</strong> 本文针对长视频中复杂动态场景的照明编辑问题，提出了一种名为TC-Light的新方法。该方法采用两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照，第二阶段优化提出的唯一视频张量（UVT）以对齐细粒度纹理和光照。为全面评估性能，研究者还构建了一个长而高度动态的视频基准数据集。实验结果表明，该方法在物理合理性和时间一致性方面表现优异，同时计算成本较低。相关代码和视频演示已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>RealPlay：基于神经网络的实时交互视频生成引擎</title>
<link>https://arxiv.org/abs/2506.18901</link>
<guid>https://arxiv.org/abs/2506.18901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealPlay实现从用户控制信号生成逼真、连贯的视频序列。</p><br /><br /><p><strong>摘要：</strong> RealPlay是一种基于神经网络的实时交互游戏引擎，能够根据用户控制信号生成逼真且时间一致的视频。与以往专注于游戏风格视觉效果的研究不同，RealPlay旨在模拟真实世界画面。它通过用户观察场景、发出控制指令、接收视频片段的交互循环工作。为实现高质量生成，研究解决了低延迟反馈、时间一致性及准确控制响应等挑战。训练数据包括标注的游戏数据和未标注的真实视频，无需真实动作标注。实验显示其具备控制迁移和实体迁移能力，可将虚拟控制应用于现实场景，并控制多种实体。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多面板故事可视化中的协作多智能体框架</title>
<link>https://arxiv.org/abs/2506.18900</link>
<guid>https://arxiv.org/abs/2506.18900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出协作多智能体框架提升故事视觉一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对故事可视化中多面板场景的视觉一致性问题，提出了一种协作多智能体框架。该框架能够自动识别、修正并优化多面板故事中的不一致之处，通过迭代循环实现细粒度的面板级更新，而无需重新生成整个序列。该方法与多种扩散模型兼容，包括Flux和Stable Diffusion等。实验结果表明，该方法在多面板一致性方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:29 GMT</pubDate>
</item>
<item>
<title>基于梯度优化的隐空间激活控制方法提升科学代码生成语言偏向性</title>
<link>https://arxiv.org/abs/2506.18887</link>
<guid>https://arxiv.org/abs/2506.18887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过G-ACT框架提升LLM生成代码时对特定语言的偏好。</p><br /><br /><p><strong>摘要：</strong> 本文研究了通过激活语言模型中的潜在子空间来引导科学代码生成向特定编程语言偏移的可行性。在四种编程语言上评估了五种因果语言模型的基线偏差，发现静态神经元归因方法效果有限。为此，作者提出了一种基于梯度优化的自适应激活控制框架（G-ACT），通过聚类每提示的激活差异并在线训练轻量级探测器，实现更有效的语言控制。实验表明，在LLaMA-3.2 3B模型中，该方法提升了15%的分类准确率，早期层甚至提高了61.5%。对于更大的LLaMA-3.3 70B模型，关键层注入仍有效。尽管引入轻微推理开销，但仅对部分层进行控制仍具实用性，展示了可扩展、可解释且高效的语义级控制机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:56:34 GMT</pubDate>
</item>
<item>
<title>3D Arena平台：基于人类偏好的生成式3D模型评估</title>
<link>https://arxiv.org/abs/2506.18787</link>
<guid>https://arxiv.org/abs/2506.18787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D Arena通过大规模用户偏好数据评估生成式3D模型质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了3D Arena平台，这是一个用于评估图像到3D生成模型的开放平台，通过大规模的人类偏好收集进行评估。自2024年6月推出以来，已收集超过12万次投票，涵盖19个最先进的模型。平台提供了iso3d数据集，并通过统计欺诈检测确保用户真实性。ELO排名系统为模型评估提供了可靠依据。分析显示，用户更偏好视觉呈现特征，Gaussian splat输出比网格模型有16.6 ELO优势，带纹理模型比无纹理模型高144.1 ELO。研究提出了多标准评估、任务导向评估和格式感知比较等改进建议，推动了生成式3D领域的人类中心评估发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:57:10 GMT</pubDate>
</item>
<item>
<title>DIP：一种用于提升密集图像表示的无监督后训练方法</title>
<link>https://arxiv.org/abs/2506.18463</link>
<guid>https://arxiv.org/abs/2506.18463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DIP通过伪上下文任务提升视觉编码器的密集表示性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DIP的新型无监督后训练方法，旨在提升大规模预训练视觉编码器在上下文场景理解中的密集图像表示能力。与以往依赖复杂自蒸馏架构的方法不同，DIP通过伪任务模拟下游上下文场景进行训练，借鉴了元学习原理。为了在未标注数据上进行后训练，该方法结合预训练扩散模型和视觉编码器自身，自动生成上下文任务。DIP具有简单、无监督和计算高效的特点，仅需单块A100 GPU不到9小时即可完成训练。实验表明，DIP在多种真实场景的上下文理解任务中表现优异，优于初始编码器和现有方法，提供了一种实用有效的密集表示优化方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 06:01:14 GMT</pubDate>
</item>
<item>
<title>基于视觉定位的医学视觉问答方法研究</title>
<link>https://arxiv.org/abs/2506.17939</link>
<guid>https://arxiv.org/abs/2506.17939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ThinkVG数据集提升医学问答模型的可解释性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过改进医学视觉问答模型的可解释性和可靠性，以支持临床决策。研究提出了一个名为ThinkVG的数据集，将答案生成过程分解为具有视觉定位的中间推理步骤，从而提供更细粒度的解释。同时引入了一种可验证的奖励机制，用于强化学习训练，提高模型推理过程与最终答案的一致性。实验表明，该方法仅需1/8的训练数据即可达到相当性能，展示了其高效性与有效性。数据集已公开在Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 04:09:58 GMT</pubDate>
</item>
<item>
<title>大语言模型输出稳定性的概率集中现象研究</title>
<link>https://arxiv.org/abs/2506.17871</link>
<guid>https://arxiv.org/abs/2506.17871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了对齐模型输出稳定性与概率集中有关。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了对齐大型语言模型（LLMs）输出缺乏多样性的原因，通过分析输出分布的概率集中现象，引入了分支因子（BF）作为衡量生成过程中可能下一步数量的指标。研究发现，随着生成过程推进，BF通常下降，表明模型变得更可预测。对齐调优显著减少了BF，使输出更稳定。此外，长推理链（CoT）模型通过进入低BF阶段实现更稳定的输出。研究认为对齐并未改变模型行为，而是引导其使用风格化词汇，从而减少熵值。实验表明，基础模型也可通过提示此类词汇降低BF。该研究为理解并控制LLM输出提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 22:00:37 GMT</pubDate>
</item>
<item>
<title>多文化音乐基础模型CultureMERT-95M提升跨文化音乐表示学习</title>
<link>https://arxiv.org/abs/2506.17818</link>
<guid>https://arxiv.org/abs/2506.17818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多文化音乐模型CultureMERT-95M提升非西方音乐分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多文化适应的音乐基础模型CultureMERT-95M，旨在增强跨文化音乐表示学习。通过两阶段持续预训练策略，在650小时多文化音乐数据上训练，显著提升了非西方音乐自动标记任务的性能，平均ROC-AUC和AP提高4.9%。同时，研究还探索了任务算术方法，与多文化训练模型效果相当。实验表明，单文化模型在不同音乐传统中迁移效果不一，而多文化模型表现最佳。为促进世界音乐表示学习研究，模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 17:16:39 GMT</pubDate>
</item>
<item>
<title>TPTT框架提升大语言模型效率与准确性</title>
<link>https://arxiv.org/abs/2506.17671</link>
<guid>https://arxiv.org/abs/2506.17671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPTT通过线性注意力机制提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TPTT（Transforming Pretrained Transformer into Titans）框架，该框架通过引入高效的线性化注意力机制和先进的内存管理技术，如Memory as Gate (MaG) 和混合线性化注意力 (LiZA)，提升了预训练Transformer模型的性能。TPTT兼容Hugging Face Transformers库，支持通过LoRA参数高效微调，无需完全重训练。实验结果显示，在MMLU基准测试中，Titan-Llama-3.2-1B模型在准确率上提升了20%。统计分析和对比表明，TPTT具有良好的可扩展性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:06:07 GMT</pubDate>
</item>
<item>
<title>基于前馈架构的4D视频与3D高斯粒子联合生成框架</title>
<link>https://arxiv.org/abs/2506.18839</link>
<guid>https://arxiv.org/abs/2506.18839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个4D视频与3D高斯粒子联合生成框架，提升视觉质量与重建能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的框架，能够使用前馈架构在每个时间步计算视频帧和3D高斯粒子的4D时空网格。该框架包含两个主要部分：4D视频模型和4D重建模型。第一部分分析了现有的4D视频扩散架构，并指出其局限性，提出了一种融合架构，在单一层中同时进行空间和时间注意力计算。关键在于稀疏注意力模式，使得标记在相同帧、同一时间戳或同一视角内进行交互。第二部分通过引入高斯头、相机标记替换算法以及额外的动态层和训练方法，扩展了现有的3D重建算法。整体上，该方法在4D生成任务中达到了新的技术水平，显著提升了视觉质量和重建能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 19:44:59 GMT</pubDate>
</item>
<item>
<title>视觉质量对多模态大语言模型性能的影响及优化方法</title>
<link>https://arxiv.org/abs/2506.15645</link>
<guid>https://arxiv.org/abs/2506.15645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像质量不直接影响MLLM表现，VQ-TTT提升模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了图像视觉质量对多模态大语言模型（MLLM）性能的影响，发现图像质量与模型表现之间存在视觉质量悖论，即某些情况下偏离人类感知的图像反而能提升模型性能。为解决这一问题，作者提出VQ-TTT方法，在不依赖外部模型或数据的情况下，通过轻量级适配模块动态调整输入图像，显著提升了多个基准测试中的准确率。该研究重新定义了MLLM所需的视觉输入标准，强调适应性图像的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 13:14:07 GMT</pubDate>
</item>
<item>
<title>基于Surfel索引视图记忆的视频生成方法</title>
<link>https://arxiv.org/abs/2506.18903</link>
<guid>https://arxiv.org/abs/2506.18903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型记忆机制提升视频生成环境探索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的记忆机制，用于构建能够交互式探索环境的视频生成器。现有方法要么通过扩展2D视图重建3D几何导致误差累积，要么依赖短上下文窗口难以维持场景连贯性。为此，作者引入了Surfel-Indexed View Memory (VMem)，通过基于3D表面元素（surfels）对过去视图进行几何索引，实现高效检索相关视图。该方法在生成新视图时仅关注关键视图，从而以较低计算成本保持场景一致性与相机控制能力。实验表明，在长时场景合成基准测试中，该方法优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于统一离散语义表示的多模态框架Tar</title>
<link>https://arxiv.org/abs/2506.18898</link>
<guid>https://arxiv.org/abs/2506.18898</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tar通过共享语义空间实现视觉与文本的统一理解和生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多模态框架Tar，旨在通过共享的离散语义表示统一视觉理解和生成。核心是Text-Aligned Tokenizer (TA-Tok)，它利用大型语言模型（LLM）词汇的文本对齐代码本将图像转换为离散标记。Tar通过扩展词汇表将视觉和文本整合到统一空间中，支持跨模态输入和输出，无需特定模态设计。研究还引入了适应规模的编码和解码方法，以及生成性解码器以提高视觉输出质量。为了满足不同的解码需求，采用了两种互补的解码器：快速自回归模型和基于扩散的模型。实验表明，Tar在多个基准测试中表现优异，收敛更快、训练效率更高。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18898" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>ReasonFlux-PRM：一种新型轨迹感知的奖励模型框架</title>
<link>https://arxiv.org/abs/2506.18896</link>
<guid>https://arxiv.org/abs/2506.18896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonFlux-PRM提升大模型推理轨迹评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型轨迹感知的奖励模型（ReasonFlux-PRM），用于更精准地评估大语言模型在推理过程中的中间步骤。与以往仅依赖最终输出的奖励模型不同，ReasonFlux-PRM结合了步骤级和轨迹级监督，能够对结构化思维链数据进行细粒度奖励分配。该模型支持离线和在线两种设置，适用于模型蒸馏、强化学习和测试时的缩放优化。实验结果表明，ReasonFlux-PRM-7B在多个基准任务中表现优于现有强基线模型，并实现了显著的性能提升。此外，作者还发布了轻量版ReasonFlux-PRM-1.5B，适用于资源受限场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>通用光度立体技术中的光照与表面法线耦合问题研究</title>
<link>https://arxiv.org/abs/2506.18882</link>
<guid>https://arxiv.org/abs/2506.18882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">解决光照与表面法线耦合难题，提升复杂表面几何细节恢复质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通用光度立体（PS）技术在任意光照条件下恢复高质量表面法线所面临的两大挑战。首先，光照变化与表面法线特征之间存在深度耦合，导致观察到的亮度变化难以区分是由于光照变化还是表面朝向变化。其次，复杂表面上的高频几何细节难以保留，自阴影、多次反射和细微法线变化使传统特征处理方法难以准确捕捉。文章分析了这些问题，并指出其对现有方法如SDM-UniPS和Uni MS-PS的限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:53:11 GMT</pubDate>
</item>
<item>
<title>基于Commutative Vector Quantization的长上下文大语言模型优化方法</title>
<link>https://arxiv.org/abs/2506.18879</link>
<guid>https://arxiv.org/abs/2506.18879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Commutative Vector Quantization技术，显著降低KV缓存内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在处理长上下文时KV缓存导致的内存瓶颈问题，提出了一种名为Commutative Vector Quantization (CommVQ) 的量化方法。该方法通过轻量级编码器和码本对KV缓存进行加法量化压缩，并利用矩阵乘法解码。为降低解码计算成本，设计了与RoPE兼容的码本，并通过EM算法训练。实验表明，该方法在保持高精度的同时，将FP16 KV缓存大小减少了87.5%，并支持1位量化，使LLaMA-3.1 8B模型在单块RTX 4090 GPU上实现128K上下文长度的推理。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:50:11 GMT</pubDate>
</item>
<item>
<title>OmniGen2：一种多功能生成模型的开源实现</title>
<link>https://arxiv.org/abs/2506.18871</link>
<guid>https://arxiv.org/abs/2506.18871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGen2是一个多功能生成模型，支持文本到图像等多种任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniGen2，一个开源的多功能生成模型，旨在为多种生成任务提供统一解决方案，包括文本到图像、图像编辑和上下文生成。与前代版本相比，OmniGen2采用独立的解码路径和非共享参数设计，提升了生成效果并保留了原始文本生成能力。文章还描述了用于训练OmniGen2的数据构建流程、针对图像生成的反射机制以及新提出的OmniContext基准测试。尽管参数规模较小，OmniGen2在多个任务上表现优异，并已公开模型、代码和数据集以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:38:54 GMT</pubDate>
</item>
<item>
<title>Phantom-Data数据集提升文本到视频生成的准确性</title>
<link>https://arxiv.org/abs/2506.18851</link>
<guid>https://arxiv.org/abs/2506.18851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phantom-Data提升文本到视频生成的一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本到视频生成取得了显著进展，但现有模型在忠实遵循文本指令方面仍面临挑战，尤其是‘复制粘贴问题’。该问题源于传统的成对训练方式，导致主体身份与背景属性混淆。为解决此问题，研究者提出了Phantom-Data，这是首个通用的跨对主体到视频一致性数据集，包含约一百万对身份一致的数据。该数据集通过三阶段流程构建：主体检测、跨上下文主体检索以及先验引导的身份验证。实验表明，使用Phantom-Data训练可显著提升提示对齐度和视觉质量，同时保持身份一致性与传统方法相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:11:56 GMT</pubDate>
</item>
<item>
<title>无需合成数据的超长文本生成方法研究</title>
<link>https://arxiv.org/abs/2506.18841</link>
<guid>https://arxiv.org/abs/2506.18841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于强化学习的超长文本生成方法超越传统微调技术。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需依赖合成数据的超长文本生成方法，通过强化学习训练模型，提升其生成长文本的质量和结构控制能力。实验表明，该方法在多个基准测试中表现优异，优于传统的监督微调方法，并且在性能上超越了更大规模的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:59:02 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的动态新视角合成方法ViDAR</title>
<link>https://arxiv.org/abs/2506.18792</link>
<guid>https://arxiv.org/abs/2506.18792</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViDAR利用扩散模型生成多视角监督信号，提升动态场景的视图合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出ViDAR，一种基于扩散模型的4D重建框架，用于从单目视频中生成高质量动态新视角。通过个性化扩散模型生成伪多视角监督信号，结合场景特定特征，ViDAR能够恢复精细外观细节并减少单目模糊带来的伪影。为解决扩散监督的时空不一致性，引入扩散感知损失函数和相机位姿优化策略，使合成视图与场景几何对齐。在极端视角变化的DyCheck基准上，ViDAR在视觉质量和几何一致性方面均优于现有方法，并在动态区域表现显著提升。项目页面提供更多信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18792" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:01:15 GMT</pubDate>
</item>
<item>
<title>ReDit：通过奖励抖动提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2506.18631</link>
<guid>https://arxiv.org/abs/2506.18631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReDit通过添加随机噪声改善离散奖励，提升模型训练效率和性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReDit的方法，用于解决大语言模型在使用离散奖励时出现的梯度异常、优化不稳定和收敛缓慢问题。ReDit通过向离散奖励信号中添加简单随机噪声，使训练过程中持续提供探索性梯度，从而实现更平滑的梯度更新和更快的收敛速度。实验表明，ReDit在多种任务中表现优异，仅需约10%的训练步数即可达到与传统GRPO相当的性能，并在相似训练时间内仍能提升4%的性能。可视化结果和理论分析进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 09:36:24 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的多视角图像生成方法</title>
<link>https://arxiv.org/abs/2506.18527</link>
<guid>https://arxiv.org/abs/2506.18527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MV-AR方法，实现多视角图像一致性生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D内容创作中多视角图像生成的挑战，提出了一种基于自回归模型的多视角图像生成方法（MV-AR）。该方法通过自回归模型逐步生成一致的多视角图像，并利用前序视图提取有效参考信息。为适应多种提示条件，设计了统一模型架构并引入条件注入模块，支持文本、相机姿态、图像和形状等多种输入。采用渐进式训练策略提升模型性能，并通过“Shuffle View”数据增强技术缓解数据不足问题。实验表明，MV-AR在多种条件下均能生成高质量且一致的多视角图像，性能与主流扩散模型相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 07:28:37 GMT</pubDate>
</item>
<item>
<title>SlimMoE：高效压缩大型Mixture of Experts模型的方法</title>
<link>https://arxiv.org/abs/2506.18349</link>
<guid>https://arxiv.org/abs/2506.18349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SlimMoE通过分阶段压缩提升MoE模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SlimMoE的多阶段压缩框架，用于将大型Mixture of Experts (MoE)模型转换为更小、高效的版本，而无需从头开始训练。该方法通过逐步减少参数数量并利用中间阶段的知识迁移，有效缓解了单次剪枝导致的性能下降问题。使用该框架，作者将Phi 3.5-MoE模型压缩为Phi-mini-MoE和Phi-tiny-MoE，分别仅需400B个token进行训练，且可在单块GPU上微调，适用于资源受限环境。实验表明，这些压缩模型在性能上优于同类模型，并与更大的模型相当。研究证明，结构化剪枝结合分阶段蒸馏是构建高质量紧凑MoE模型的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:15:59 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的自适应用户画像框架LettinGo</title>
<link>https://arxiv.org/abs/2506.18309</link>
<guid>https://arxiv.org/abs/2506.18309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LettinGo提升推荐系统的准确性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LettinGo，一种基于大语言模型的自适应用户画像生成框架。传统嵌入式画像缺乏可解释性，而现有方法受限于固定格式。LettinGo通过多阶段流程，结合直接偏好优化（DPO）和下游任务反馈，生成多样且适应性强的用户画像，显著提升推荐系统的准确性、灵活性和上下文感知能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 01:51:52 GMT</pubDate>
</item>
<item>
<title>无需验证器的强化学习框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.18254</link>
<guid>https://arxiv.org/abs/2506.18254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLPR通过LLM自身概率评分实现无验证器强化学习，提升推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLPR，一种无需依赖领域特定验证器的强化学习框架，利用大语言模型（LLM）生成答案时的token概率作为奖励信号，从而提升模型在多个通用领域和数学领域的推理能力。研究发现，处理概率奖励的高方差是关键，因此引入prob-to-reward和稳定化方法以确保奖励的准确性与稳定性。实验表明，RLPR在四个通用领域和三个数学基准测试中均表现优异，显著优于现有方法如VeriFree和General-Reasoner。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 22:56:36 GMT</pubDate>
</item>
<item>
<title>FaithfulSAE提升稀疏自编码器的稳定性与模型内部特征捕捉能力</title>
<link>https://arxiv.org/abs/2506.17673</link>
<guid>https://arxiv.org/abs/2506.17673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaithfulSAE通过使用模型自身数据提升SAE稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FaithfulSAE方法，旨在解决稀疏自编码器（SAEs）在不同初始化种子下不稳定以及无法准确捕捉模型内部特征的问题。传统SAEs通常在外部数据集上训练，可能引入分布外（OOD）数据，导致生成虚假特征。FaithfulSAE则使用模型自身生成的数据进行训练，实验表明其在多个模型中表现出更高的稳定性，并在SAE探测任务中优于基于网络数据的SAEs，同时降低了虚假特征比例。该方法减少了对外部数据集的依赖，提升了模型解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:18:25 GMT</pubDate>
</item>
<item>
<title>ConsumerBench：评估端侧GenAI系统效率的基准框架</title>
<link>https://arxiv.org/abs/2506.17538</link>
<guid>https://arxiv.org/abs/2506.17538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ConsumerBench评估端侧GenAI模型的效率与响应时间。</p><br /><br /><p><strong>摘要：</strong> 本文提出ConsumerBench，一个用于评估端用户设备上生成式AI（GenAI）模型系统效率和响应时间的基准框架。不同于传统基准假设模型独占GPU资源，ConsumerBench模拟了真实多应用并发运行的场景，支持自定义工作流以模拟复杂任务。该框架收集应用级指标（如延迟和SLO达成率）和系统级指标（如CPU/GPU利用率和内存带宽）。实验揭示了资源共享低效、贪婪分配下的不公平调度以及静态模型服务器配置的性能问题，并为模型开发者和系统设计者提供了实用建议，包括定制化内核和SLO感知调度策略的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:32:22 GMT</pubDate>
</item>
<item>
<title>基于MICS的医学多模态大语言模型推理路径优化</title>
<link>https://arxiv.org/abs/2506.16962</link>
<guid>https://arxiv.org/abs/2506.16962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MICS方法提升医学MLLM推理能力，构建新数据集和模型。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学领域多模态大语言模型（MLLM）推理能力不足的问题，提出一种名为Mentor-Intern Collaborative Search（MICS）的新型推理路径搜索方案。该方法通过导师模型引导推理路径，并由多个实习模型进行验证与优化，最终根据综合表现选择最优路径。为评估推理质量，引入MICS-Score指标。研究还构建了MMRP医学推理数据集和Chiron-o1模型，后者在多项医学视觉问答和推理任务中取得最佳性能。实验表明，基于MICS生成的CoT数据能显著提升模型表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 08:51:19 GMT</pubDate>
</item>
<item>
<title>基于LSTM的新生儿死亡风险预测研究</title>
<link>https://arxiv.org/abs/2506.16929</link>
<guid>https://arxiv.org/abs/2506.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LSTM模型在新生儿死亡预测中表现最佳，准确率达99%。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了如何通过机器学习技术预测新生儿死亡风险，以减少新生儿死亡率。研究使用了140万份历史数据，应用了逻辑回归、K近邻、随机森林、XGBoost、卷积神经网络和LSTM等多种算法。结果显示，XGBoost和随机森林分类器准确率为94%，而LSTM模型准确率最高，达到99%。因此，LSTM被推荐为预测新生儿是否需要采取预防措施的最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 07:44:48 GMT</pubDate>
</item>
<item>
<title>Crome：一种因果鲁棒的奖励建模框架以防止奖励黑客</title>
<link>https://arxiv.org/abs/2506.16507</link>
<guid>https://arxiv.org/abs/2506.16507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crome通过因果和中性增强提升奖励模型鲁棒性，有效防止奖励黑客。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Crome的新型奖励建模框架，旨在解决大型语言模型在对齐过程中因奖励黑客而导致的偏差问题。Crome基于显式的因果模型，通过两种合成增强方式——因果增强和中性增强，分别强化对因果属性的敏感性和对虚假属性的不变性。这些增强仅通过查询一个权威语言模型来识别因果标准，无需预先了解虚假因素。实验结果显示，Crome在多个基准测试中显著优于传统基线，提升了平均准确率，并在不同任务和设置中表现出一致的性能优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FinCoT：基于领域专家推理的结构化思维链提示方法</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinCoT提升金融问答性能并优化推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出FinCoT，一种结合领域专家金融推理的结构化思维链提示方法。研究对比了FinNLP中的三种提示风格：标准提示、非结构化思维链提示和结构化思维链提示，并发现结构化提示在性能和可解释性上具有优势。FinCoT在十个金融领域的CFA风格问题上表现出色，将准确率从63.2%提升至80.5%，同时显著减少生成的token数量。结果表明，与领域对齐的结构化提示能有效提升模型性能并降低推理成本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:18:55 GMT</pubDate>
</item>
<item>
<title>基于CodeT5的LLM代码作者归属识别研究</title>
<link>https://arxiv.org/abs/2506.17323</link>
<guid>https://arxiv.org/abs/2506.17323</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新型模型用于识别AI生成的C语言代码来源。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型生成的代码日益增多，识别其来源变得尤为重要。本文提出了CodeT5-Authorship模型，该模型仅使用CodeT5的编码器部分进行分类任务。通过引入LLM-AuthorBench基准测试集，评估了模型在区分不同LLM生成代码方面的性能。实验结果显示，该模型在二分类和多分类任务中均表现出色，准确率分别达到97.56%和95.40%。研究还提供了开源代码和数据集以支持开放科学。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17323" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 15:49:41 GMT</pubDate>
</item>
<item>
<title>Agentic AI研究中的标准化与评估协议改进</title>
<link>https://arxiv.org/abs/2506.15741</link>
<guid>https://arxiv.org/abs/2506.15741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究指出Agentic AI缺乏标准评估，提出新框架OAgents提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前Agentic AI研究中缺乏标准化和科学严谨性的问题，导致方法间难以公平比较。通过在GAIA基准和BrowseComp上的系统实证研究，发现以往工作因评估协议不统一而难以复现。为此，作者提出了更稳健的评估协议，并基于研究结果开发了OAgents框架，该框架在开源项目中表现优异，具备模块化设计以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>LLM jailbreak攻击防护机制的系统性分析</title>
<link>https://arxiv.org/abs/2506.10597</link>
<guid>https://arxiv.org/abs/2506.10597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次系统分析LLM的jailbreak防护机制，提出多维分类和评估框架。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLMs）在部署过程中暴露的关键漏洞，尤其是jailbreak攻击对安全机制的突破。为应对这一问题，研究提出了外部防御机制——guardrails，并指出当前该领域缺乏统一的分类和评估体系。本文作为知识系统化（SoK）论文，首次对LLM的jailbreak防护机制进行了全面分析，提出了一种六维分类体系，并设计了一个结合安全性、效率与实用性的评估框架。通过实验与分析，研究揭示了现有防护方法的优势与局限性，探讨了其在不同攻击类型中的适用性，并为未来优化防护组合提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:42:40 GMT</pubDate>
</item>
<item>
<title>基于隐式视觉标记的多模态推理框架 Mirage</title>
<link>https://arxiv.org/abs/2506.17218</link>
<guid>https://arxiv.org/abs/2506.17218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mirage框架提升多模态推理能力，无需生成显式图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Mirage 的多模态推理框架，旨在提升视觉语言模型（VLMs）在不需要生成显式图像的情况下进行视觉推理的能力。受人类通过心理意象进行推理的启发，Mirage 在文本解码过程中引入隐式视觉标记，使模型能够在不生成像素级图像的情况下继续多模态推理流程。该方法首先通过从真实图像嵌入中蒸馏监督隐式标记，随后切换为纯文本监督以对齐任务目标，并通过强化学习进一步增强多模态推理能力。实验结果表明，Mirage 在多个基准测试中表现出更强的多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>MEXA：一种无需训练的多模态推理框架</title>
<link>https://arxiv.org/abs/2506.17113</link>
<guid>https://arxiv.org/abs/2506.17113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEXA通过聚合专家模型实现跨领域的多模态推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MEXA的训练-free 框架，用于在不同领域中进行多模态推理。MEXA能够根据输入模态和任务需求动态选择专家模型，并利用大型推理模型对这些模型的输出进行整合与推理，从而生成最终答案。该方法无需额外训练，具有模块化设计，适用于视频推理、音频推理、3D理解及医疗问答等多种任务。实验表明，MEXA在多个多模态基准测试中均优于现有方法，展示了其在多模态推理中的有效性与广泛适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 12:14:13 GMT</pubDate>
</item>
<item>
<title>基于对数概率序列的提示逆向方法研究</title>
<link>https://arxiv.org/abs/2506.17090</link>
<guid>https://arxiv.org/abs/2506.17090</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PILS方法，提升隐藏提示恢复准确率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了语言模型逆向问题，旨在通过模型输出恢复隐藏提示。作者提出了一种新方法——从对数概率序列中进行提示逆向（PILS），利用模型在多个生成步骤中的下一个词概率来恢复隐藏提示。该方法基于一个关键洞察：语言模型的向量输出位于低维子空间中，这使得可以无损压缩多步生成的概率分布。实验结果显示，PILS在恢复隐藏提示方面比现有方法有显著提升，恢复率提高了2到3.5倍。此外，该方法在恢复隐藏系统消息任务中也表现出色，并展示了良好的泛化能力。研究还分析了重复内容在提示恢复中的作用，并提出了基于逻辑的跨模型迁移方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17090" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:53:51 GMT</pubDate>
</item>
<item>
<title>图像生成模型输出的令牌级水印方法</title>
<link>https://arxiv.org/abs/2506.16349</link>
<guid>https://arxiv.org/abs/2506.16349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种图像生成模型的令牌级水印技术，提升检测可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对自回归图像生成模型输出的令牌级水印方法，这是该领域的首次尝试。研究发现，由于缺乏反向循环一致性（RCC），重新分词会破坏水印。为解决此问题并增强对常见图像变换、神经压缩和移除攻击的鲁棒性，作者引入了定制的分词器-反分词器微调流程和互补的水印同步层。实验表明，该方法在理论上具有可靠的p值，能够实现稳健的水印检测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 10:25:51 GMT</pubDate>
</item>
<item>
<title>Vision-Language-Action模型的泛化能力评估与基准测试</title>
<link>https://arxiv.org/abs/2506.09930</link>
<guid>https://arxiv.org/abs/2506.09930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">评估VLA模型在模拟任务中的泛化能力，揭示其感知与执行差距。</p><br /><br /><p><strong>摘要：</strong> 本文针对Vision-Language-Action (VLA)模型在机器人领域中的泛化能力进行了系统评估。尽管VLA模型基于大型视觉-语言模型（VLM）展现出强大的感知理解和高层规划能力，但在实际动作执行中表现不稳定，尤其在面对分布外观察时。研究引入了一个包含50个模拟任务的统一评估套件，覆盖10个子类别。实验结果表明，微调动作数据可能削弱VLM的通用推理能力。作者开源了任务套件和代码，旨在为未来VLA研究提供标准化基准，推动感知到动作的差距缩小。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:52:18 GMT</pubDate>
</item>
<item>
<title>多语言语音合成中文化敏感情感与口音建模的新方法</title>
<link>https://arxiv.org/abs/2506.16310</link>
<guid>https://arxiv.org/abs/2506.16310</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新TTS系统提升印度语和英语口音及情感准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型文本转语音（TTS）架构，专门针对印地语和印度英语的口音与情感建模。该系统在Parler-TTS基础上进行了改进，引入了语言特定的音素对齐编码器-解码器结构、基于本地语料库训练的文化敏感情感嵌入层，以及动态口音代码切换机制。实验结果显示，该系统在口音准确率上提升了23.7%，情感识别准确率达85.3%，优于现有基线模型。此外，系统支持实时口音切换，保持情感一致性，用户主观评价得分为4.2/5，显著优于现有系统。该研究为跨语言语音合成提供了可行方案，适用于南亚教育科技和无障碍软件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16310" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 09:35:05 GMT</pubDate>
</item>
<item>
<title>InfGen：一种用于长期交通仿真的统一模型</title>
<link>https://arxiv.org/abs/2506.17213</link>
<guid>https://arxiv.org/abs/2506.17213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfGen实现长期交通仿真，性能优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfGen，一种统一的下一标记预测模型，能够同时进行封闭回路运动模拟和场景生成。该模型在短期（9秒）交通仿真中达到最先进水平，并在长期（30秒）仿真中显著优于其他方法。InfGen可以自动切换模拟模式，实现稳定、真实的长期交通仿真，适用于自动驾驶系统部署中的实际场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>UniFork：一种新型的统一图像理解与生成架构</title>
<link>https://arxiv.org/abs/2506.17202</link>
<guid>https://arxiv.org/abs/2506.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniFork架构，提升图像理解与生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了统一图像理解与生成模型的架构设计问题，发现理解任务和生成任务在模态对齐模式上存在显著差异。理解任务需要随着网络深度增加逐步增强模态对齐以提升语义理解，而生成任务则在浅层增强对齐、深层减弱以恢复空间细节。这种差异导致传统共享Transformer架构难以兼顾两者。为此，作者提出UniFork架构，采用Y型结构，在浅层共享表示学习，深层引入任务专用分支，有效平衡了共享学习与任务专精，实验表明其性能优于传统架构和任务专用模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:52:31 GMT</pubDate>
</item>
<item>
<title>基于提示的参数生成方法DnD实现高效大语言模型微调</title>
<link>https://arxiv.org/abs/2506.16406</link>
<guid>https://arxiv.org/abs/2506.16406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DnD通过提示生成参数，大幅提升LLM微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Drag-and-Drop LLMs（DnD）的新方法，该方法通过将少量未标记的任务提示直接映射为LoRA权重更新，从而避免了每个下游数据集都需要单独优化的过程。DnD使用轻量级文本编码器将提示批次压缩为条件嵌入，并通过级联超卷积解码器生成完整的LoRA矩阵。训练完成后，DnD可在几秒内生成特定任务的参数，相比全量微调降低了12,000倍的开销，并在多个基准测试中表现出30%的性能提升。此外，DnD在跨领域任务中也展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 11:38:21 GMT</pubDate>
</item>
<item>
<title>PAROAttention：通过重新排序提升视觉生成中的注意力效率</title>
<link>https://arxiv.org/abs/2506.16054</link>
<guid>https://arxiv.org/abs/2506.16054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAROAttention优化视觉生成中的注意力机制，提升效率与性能。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成中，注意力机制的二次复杂度导致高内存和计算成本，尤其在高分辨率图像或视频生成时更为显著。为解决此问题，研究者尝试了稀疏化和量化等技术，但效果受限。本文指出，视觉注意力模式的分散性和不规则性是核心难题，因此提出一种新策略——重新组织注意力模式。受视觉特征提取局部聚合特性的启发，设计了Pattern-Aware token ReOrdering (PARO)技术，将多样化的注意力模式统一为硬件友好的块状模式，从而简化并提升稀疏化和量化效果。实验表明，PAROAttention在保持与全精度模型相近性能的同时，显著降低密度和位宽（INT8/INT4），实现1.9x至2.7倍的端到端延迟加速。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 02:25:02 GMT</pubDate>
</item>
<item>
<title>基于多模态模型的文档分块方法提升RAG系统性能</title>
<link>https://arxiv.org/abs/2506.16035</link>
<guid>https://arxiv.org/abs/2506.16035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态文档分块方法提升RAG系统准确性和结构保留能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大型多模态模型（LMMs）的新型多模态文档分块方法，用于处理PDF文档中的复杂结构，如跨页表格、嵌入图表和上下文依赖。该方法通过配置化的页面批次处理方式，保持语义连贯性和结构完整性，并在手动构建的PDF数据集上验证了其有效性，结果显示该方法在分块质量和下游RAG性能方面均有显著提升，相比传统RAG系统具有更高的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 01:11:43 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.1：3D AI生成内容的全面教程</title>
<link>https://arxiv.org/abs/2506.15442</link>
<guid>https://arxiv.org/abs/2506.15442</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.1提供3D生成模型的完整训练与评估指南。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Hunyuan3D 2.1，这是一个用于3D人工智能生成内容（AIGC）的先进系统，旨在帮助用户处理3D数据、训练生成模型并进行性能评估。该系统包含两个核心组件：Hunyuan3D-DiT用于形状生成，Hunyuan3D-Paint用于纹理合成。文章详细讲解了从数据准备、模型架构、训练策略到评估指标和部署的全流程，适合希望在游戏、虚拟现实和工业设计中应用3D生成技术的开发者和研究人员。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15442" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:14:46 GMT</pubDate>
</item>
<item>
<title>InfiniPot-V：突破视频流理解的内存瓶颈</title>
<link>https://arxiv.org/abs/2506.15745</link>
<guid>https://arxiv.org/abs/2506.15745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniPot-V实现视频流实时处理且不占用过多内存。</p><br /><br /><p><strong>摘要：</strong> InfiniPot-V是一种无需训练、与查询无关的框架，能够在视频流处理中保持固定内存上限。该方法在视频编码过程中监控缓存，当达到用户设定阈值时，通过时间轴冗余度（TaR）和值范数（VaN）对缓存进行轻量级压缩，有效去除冗余信息并保留语义重要信息。实验表明，InfiniPot-V可减少高达94%的GPU峰值内存，同时保持实时生成能力和与全缓存相当的准确性，适用于多种多模态大模型和视频基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 22:22:14 GMT</pubDate>
</item>
<item>
<title>基于多平面同步的3D全景图扩散模型DreamCube</title>
<link>https://arxiv.org/abs/2506.17206</link>
<guid>https://arxiv.org/abs/2506.17206</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多平面同步扩展2D基础模型至全景域，实现高质量3D全景图生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DreamCube的多平面RGB-D扩散模型，用于3D全景图生成。该模型通过在2D基础模型的操作器上应用多平面同步技术，有效克服了2D图像先验与3D全景图不兼容的问题，从而实现了对全景视觉外观和几何结构的有效建模。实验表明，DreamCube在全景图像生成、全景深度估计及3D场景生成方面均表现出色，显著提升了生成内容的质量和多样性，同时保证了多视角一致性。这项工作为3D全景合成提供了新的思路，有望推动相关领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17206" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Hunyuan-GameCraft：面向游戏环境的高动态交互视频生成框架</title>
<link>https://arxiv.org/abs/2506.17201</link>
<guid>https://arxiv.org/abs/2506.17201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Hunyuan-GameCraft框架解决现有游戏视频生成方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来扩散模型驱动的可控视频生成技术取得了显著进展，但当前方法在动态表现、通用性、长期一致性及效率方面仍存在不足，限制了其在游戏视频生成中的应用潜力。为弥补这些缺陷，我们引入了Hunyuan-GameCraft，这是一种针对游戏环境中高动态交互视频生成的新框架。该框架通过统一键盘和鼠标输入至共享相机表示空间实现精细动作控制，并采用混合历史条件训练策略以扩展视频序列并保留场景信息。此外，为了提升推理效率与可玩性，框架实现了模型蒸馏，减少计算开销的同时保持长时间序列的一致性。模型基于涵盖超过100款AAA级游戏的百万量级游戏录像数据集进行训练，并在精心标注的合成数据集上微调以增强精度与控制能力。实验表明，Hunyuan-GameCraft在视觉逼真度、真实性及动作控制方面均显著优于现有方法，极大推动了交互式游戏视频生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:50:37 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.5：高保真3D资产生成的新突破</title>
<link>https://arxiv.org/abs/2506.16504</link>
<guid>https://arxiv.org/abs/2506.16504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.5通过新模型LATTICE和PBR技术提升3D形状与纹理生成质量。</p><br /><br /><p><strong>摘要：</strong> Hunyuan3D 2.5是一款强大的3D扩散模型套件，专注于生成高质量且细节丰富的纹理化3D资产。它沿用了Hunyuan3D 2.0的两阶段流程，但在形状和纹理生成方面取得了显著进步。在形状生成方面，引入了新的基础模型LATTICE，该模型通过扩展高质量数据集、模型规模和计算资源进行训练。最大模型参数达到10B，能够生成清晰、详细的3D形状，并精确匹配图像与3D模型，同时保持网格表面干净平滑，大幅缩小生成形状与手工制作形状之间的差距。在纹理生成方面，通过基于物理的渲染(PBR)技术，借助从Hunyuan3D 2.0 Paint模型扩展的多视图架构实现升级。广泛评估显示，Hunyuan3D 2.5在形状生成和端到端纹理生成方面均显著优于先前方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在无偏视角摘要中的应用与评估</title>
<link>https://arxiv.org/abs/2506.15925</link>
<guid>https://arxiv.org/abs/2506.15925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出改进的评估方法提升视角摘要质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中的政治视角摘要这一重要应用，指出当前评价框架对覆盖度和忠实性等属性的传统度量方法缺乏适用性验证的问题。为填补这一空白，我们首先通过人工注释构建测试集来评估度量指标的可靠性，发现基于语言模型的度量方法优于传统指标。进一步地，我们证明基于重排序的方法效果显著，并且利用合成数据进行偏好微调可进一步提升性能。这些研究成果有助于推动视角摘要方法的可靠评估与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 20:01:43 GMT</pubDate>
</item>
<item>
<title>VIKI-Bench与VIKI-R：多智能体协作新基准与框架</title>
<link>https://arxiv.org/abs/2506.09049</link>
<guid>https://arxiv.org/abs/2506.09049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VIKI-Bench基准和VIKI-R框架，提升具身多智能体视觉推理合作能力。</p><br /><br /><p><strong>摘要：</strong> 在动态环境中协调多个具身代理是人工智能中的核心挑战，涉及感知驱动推理与可扩展合作策略。尽管已有研究利用大型语言模型进行多智能体规划，但基于视觉-语言模型(VLM)的视觉推理探索尚少且支持多样性有限。本文引入VIKI-Bench，首个针对具身多智能体协作设计的分层基准，涵盖代理激活、任务规划和轨迹感知三个层次。VIKI-Bench包含多样化的机器人形态、多视角视觉观测及结构化监督信号。为展示其价值，我们提出VIKI-R框架，通过链式思维标注演示微调预训练VLM，再结合多层级奖励信号强化学习。实验表明，VIKI-R在所有任务层次上显著优于基线方法，并促进异构代理间的组合式合作模式涌现。VIKI-Bench与VIKI-R共同构成推动具身AI系统多智能体视觉驱动合作发展的统一测试平台与方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Show-o2：基于流匹配与自回归建模的多模态统一模型</title>
<link>https://arxiv.org/abs/2506.15564</link>
<guid>https://arxiv.org/abs/2506.15564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合流匹配与自回归建模的多模态统一模型Show-o2。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Show-o2的改进型原生统一多模态模型，该模型通过空间（时空）融合构建统一视觉表示，并利用因果变分自编码器空间实现跨图像和视频模态的扩展。Show-o2在语言头和流头分别应用自回归建模和流匹配技术，用于文本标记预测和图像/视频生成。设计了两阶段训练方法以有效学习并扩展到更大模型，最终模型在多种模态（如文本、图像和视频）的多模态理解和生成任务中表现出色。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 11:39:15 GMT</pubDate>
</item>
<item>
<title>RE-IMAGINE框架评估大型语言模型的推理能力</title>
<link>https://arxiv.org/abs/2506.15455</link>
<guid>https://arxiv.org/abs/2506.15455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出RE-IMAGINE框架，评估大型语言模型是否依赖统计记忆进行推理。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明大型语言模型（LLMs）在推理基准测试中表现优异，但其推理能力的真实来源尚不明确。本文受因果阶梯理论启发，提出RE-IMAGINE框架，通过生成不同层次的问题变体，系统性地评估LLMs的推理能力。该框架基于中间符号表示生成问题，避免单纯依赖训练集的记忆能力。实验结果显示，在不同推理领域（如数学、代码、逻辑）中，模型性能随问题复杂度提升而下降，表明现有模型对统计记忆存在一定程度的依赖。这一发现为未来研究如何提升LLMs的高层次推理能力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:35:47 GMT</pubDate>
</item>
<item>
<title>SonicVerse：融合多任务特征检测的音乐描述生成模型</title>
<link>https://arxiv.org/abs/2506.15154</link>
<guid>https://arxiv.org/abs/2506.15154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合音频特征检测的音乐描述生成模型，提升音乐AI研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SonicVerse的多任务音乐描述生成模型，该模型不仅生成音乐片段的描述，还通过集成关键检测、人声检测等辅助任务捕捉低级声学细节和高级音乐属性。其创新之处在于基于投影的架构，将音频输入转化为语言标记的同时进行特征检测。此框架不仅能为短音乐片段生成丰富的描述，还能通过时间信息链式生成长音乐片段的详细描述。为训练模型，作者扩展了MusicBench数据集，使用MIRFLEX标注音乐特征，最终实验表明，这种方法显著提升了生成描述的质量和细节水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 01:51:36 GMT</pubDate>
</item>
<item>
<title>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</title>
<link>https://arxiv.org/abs/2506.14965</link>
<guid>https://arxiv.org/abs/2506.14965</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 16:24:00 GMT</pubDate>
</item>
<item>
<title>基于迭代精化的图表到代码生成方法</title>
<link>https://arxiv.org/abs/2506.14837</link>
<guid>https://arxiv.org/abs/2506.14837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于结构化指令的迭代精化方法提升图表到代码生成性能。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）因其强大的视觉理解能力而备受关注，但在图表到代码生成任务上表现欠佳。这项任务不仅需要精确的视觉理解，还需要将视觉元素准确转化为结构化代码。本文提出的{ChartIR}通过区分视觉理解和代码翻译两个子任务，设计描述和差异两种结构化指令，将视觉特征转化为语言表示，从而优化后续代码翻译过程。此外，该方法将整个生成流程分解为初始代码生成和迭代精化两个阶段，逐步提升最终输出质量。实验结果显示，无论是在开源模型Qwen2-VL还是闭源模型GPT-4o上，{ChartIR}均优于其他方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 10:10:16 GMT</pubDate>
</item>
<item>
<title>EmoNet-Voice：基于新基准的数据集推动语音情感识别发展</title>
<link>https://arxiv.org/abs/2506.09827</link>
<guid>https://arxiv.org/abs/2506.09827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmoNet-Voice推出包含大规模预训练数据集和专家标注的新基准，提升AI语音情感识别能力。</p><br /><br /><p><strong>摘要：</strong> 随着文本转语音及音频生成模型的进步，评估AI系统情感理解能力的需求愈发迫切。然而，当前的语音情感识别（SER）数据集存在情感粒度不足、隐私问题或依赖演员表演等局限性。本文介绍EmoNet-Voice，一种新的语音情感检测资源，它包括EmoNet-Voice Big（涵盖超过4500小时的跨语言、多情绪语音数据）和EmoNet-Voice Bench（具有人工专家注释的新型基准数据集）。该资源旨在通过精细的情感分类评估SER模型，并利用先进的语音生成技术创建模拟演员表演的合成音频片段，同时由心理学专家验证并标注感知强度。此外，我们提出Empathic Insight Voice模型，在情感识别上达到与人类专家高度一致的新标准。我们的研究发现表明，高唤醒情绪如愤怒比低唤醒状态如专注更容易被检测到。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 11:06:59 GMT</pubDate>
</item>
<item>
<title>基于视觉Transformer的寿命预测模型</title>
<link>https://arxiv.org/abs/2506.13430</link>
<guid>https://arxiv.org/abs/2506.13430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用预训练视觉Transformer模型从图像预测剩余寿命。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，通过利用预训练的视觉Transformer基础模型，结合面部和全身图像估计个体的剩余寿命，并实现了对预测不确定性的量化。研究显示，预测不确定性系统性地随真实剩余寿命变化，且可以通过学习高斯分布来有效建模。该方法在现有数据集上达到了7.48年的平均绝对误差（MAE），并在两个新发布的高质量数据集上进一步优化至4.79年和5.07年。虽然此模型尚未达到临床部署标准，但其结果显示了从图像中提取医学相关信息的潜力。所有代码和数据集均公开，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:47:37 GMT</pubDate>
</item>
<item>
<title>提升小规模推理语言模型性能的研究</title>
<link>https://arxiv.org/abs/2506.13404</link>
<guid>https://arxiv.org/abs/2506.13404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索训练策略以提高0.5B参数量语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型的发展，尽管这些模型在多种任务上表现出色，但其高昂的计算成本和潜在隐私问题限制了其应用范围。相比之下，具有约0.5亿参数的小型推理语言模型（SRLMs）因其高效性和经济性成为一种有吸引力的选择，尤其是在资源受限的环境中。然而，这类小型模型由于容量限制，在处理复杂任务如数学推理和代码生成时面临挑战。本研究探讨了监督微调（SFT）、知识蒸馏（KD）和强化学习（RL）等不同的训练策略及其组合方法，以缩小小型模型与大型模型之间的性能差距。通过广泛的实验验证和分析，我们提出了优化的训练管道建议，旨在最大化0.5B参数量模型的推理能力，为相关领域的实际应用提供指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:18:11 GMT</pubDate>
</item>
<item>
<title>SeqPE：一种统一且完全可学习的位置编码框架</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SeqPE框架，解决传统位置编码在扩展性和多模态适应性上的局限。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeqPE的统一且完全可学习的位置编码框架，该框架通过将每个n维位置索引表示为符号序列，并采用轻量级顺序位置编码器以端到端方式学习嵌入。为了规范SeqPE的嵌入空间，引入了对比目标和知识蒸馏损失两种互补的目标函数。实验表明，SeqPE在语言建模、长上下文问答和二维图像分类等任务上表现优异，特别是在上下文长度外推方面超越了强大的基线模型，并且能够在无需手动架构重设计的情况下无缝泛化到多维输入。此外，作者开源了代码、数据和检查点。关键词：Transformer、位置编码、SeqPE。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 05:16:40 GMT</pubDate>
</item>
<item>
<title>DoTA-RAG：面向大规模网络知识索引的高效检索增强生成系统</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DoTA-RAG通过三阶段管道优化检索增强生成系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DoTA-RAG（动态聚合检索增强生成系统）的新方法，该系统专门针对高吞吐量、大规模网络知识索引进行了优化。传统检索增强生成（RAG）系统在处理海量多样化数据时通常面临高延迟和有限准确性的问题。为解决这些问题，DoTA-RAG采用了三阶段管道策略，包括查询重写、动态路由到专用子索引以及多阶段检索和排名。此外，通过评估并选择更优的嵌入模型并对FineWeb-10BT语料库进行重新嵌入，进一步增强了系统的检索能力。同时，构建了一个包含500个问题的多样化问答数据集，涵盖了广泛的WebOrganizer主题和格式。实验结果显示，DoTA-RAG将答案正确性得分从基线的0.752提高到了1.478，同时保持了低延迟，在Live Challenge Day上达到了0.929的正确性得分。这些成果表明，DoTA-RAG在需要快速可靠访问大型且不断发展的知识源的领域具有实际部署潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 12:56:00 GMT</pubDate>
</item>
<item>
<title>利用大型语言模型评估新闻媒体可信度与政治偏见的研究</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法利用大型语言模型评估新闻媒体的整体可信度与政治倾向。</p><br /><br /><p><strong>摘要：</strong> 在当今充斥着虚假信息的网络环境中，帮助读者理解所读内容至关重要。传统上，事实核查主要依赖手动或自动方式，但对新兴话题或信息有限的情况难以应对。本研究聚焦于评估新闻媒体整体的可靠性及政治偏见，而非单篇文章。我们设计了一系列基于专业事实核查员标准的提示，并通过大型语言模型（LLMs）获取回应，最终整合预测结果。实验表明，该方法显著优于现有基线模型，并深入分析了媒体流行度和地区对模型表现的影响。此外，还进行了消融研究以确定数据集的关键要素。为促进后续研究，我们公开了数据集和代码。这项研究填补了新闻媒体整体评估领域的空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 11:49:20 GMT</pubDate>
</item>
<item>
<title>QGuard：一种基于问题提示的大语言模型安全防护方法</title>
<link>https://arxiv.org/abs/2506.12299</link>
<guid>https://arxiv.org/abs/2506.12299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QGuard方法，利用问题提示零样本防御大语言模型的有害提示攻击。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）面临的有害提示和越狱提示带来的恶意攻击风险，提出了名为QGuard的安全防护方法。QGuard通过引入问题提示，在无需微调的情况下，以零样本方式有效阻止文本和多模态有害提示的攻击。实验结果显示，该方法在文本和多模态有害数据集上表现出色，同时通过对问题提示的分析，实现了用户输入的白盒分析。我们认为，此方法为实际LLMs服务中的安全风险缓解提供了有价值的参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12299" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 21:23:50 GMT</pubDate>
</item>
<item>
<title>EgoPrivacy：第一人称视角隐私风险评估基准</title>
<link>https://arxiv.org/abs/2506.12258</link>
<guid>https://arxiv.org/abs/2506.12258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示第一人称视频中可推断出的穿戴者隐私信息。</p><br /><br /><p><strong>摘要：</strong> 随着可穿戴摄像头的普及，尽管已有研究关注旁观者隐私，但对穿戴者隐私威胁的研究相对不足。本文引入EgoPrivacy，这是首个针对第一人称视角视觉隐私风险的大规模基准测试工具，涵盖三类隐私（人口统计、个体特征和情境信息），并定义了七个任务来恢复从精细到粗粒度的私密信息。此外，我们提出了一种新颖的检索增强攻击策略，通过外部旁观者视频池中的检索来提高人口统计隐私攻击的效果。实验表明，即使在零样本设置下，基础模型也能有效泄露穿戴者的身份、场景、性别和种族等属性，准确率可达70%-80%。我们的代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 18:19:54 GMT</pubDate>
</item>
<item>
<title>语言模型在动态仇恨言论检测中的时间敏感性评估</title>
<link>https://arxiv.org/abs/2506.12148</link>
<guid>https://arxiv.org/abs/2506.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示静态基准无法准确评估语言模型处理动态仇恨言论的能力。</p><br /><br /><p><strong>摘要：</strong> 随着社会动态和文化变迁，仇恨言论的语言也在迅速演变，这对自然语言处理（NLP）领域提出了挑战。尽管已有研究探讨了语言演化对模型训练的影响并提出了一些解决方案，但其对模型基准测试的影响仍未得到充分探索。作为保障模型安全的关键工具，仇恨言论基准的重要性不言而喻。本文通过实证研究评估了20种语言模型在两个动态仇恨言论实验中的鲁棒性，揭示了静态与时间敏感性评估之间的时间错配问题。我们的发现强调了构建时间敏感型语言基准的必要性，以便更可靠地评价仇恨言论领域的语言模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 14:08:19 GMT</pubDate>
</item>
<item>
<title>VGR：增强视觉感知能力的多模态链式推理大模型</title>
<link>https://arxiv.org/abs/2506.11991</link>
<guid>https://arxiv.org/abs/2506.11991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新多模态大语言模型VGR，提升复杂视觉推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有多模态链式推理方法主要依赖纯语言空间且局限于数学或科学领域的局限性，提出了一种名为VGR的新模型。VGR是一种具有增强细粒度视觉感知能力的多模态大语言模型，它通过检测有助于解决问题的相关区域并基于这些区域提供精确答案来实现视觉和语言推理的结合。为了训练该模型，我们构建了一个包含视觉定位和语言推理混合数据的大规模SFT数据集VGR-SFT。实验表明，在LLaVA-NeXT-7B基线上，VGR在需要全面理解图像细节的多模态基准测试中表现出色，例如在MMStar上得分提高了+4.1，在AI2D上提高了+7.1，在ChartQA上则提升了+12.9，同时使用的图像token数量仅为基线模型的30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11991" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:47:43 GMT</pubDate>
</item>
<item>
<title>TaskCraft：自动化生成可扩展的多工具交互型任务</title>
<link>https://arxiv.org/abs/2506.10055</link>
<guid>https://arxiv.org/abs/2506.10055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaskCraft通过自动化生成多工具交互型任务提升AI模型性能。</p><br /><br /><p><strong>摘要：</strong> 随着多步骤问题解决等自主性任务在自然语言处理（NLP）和人工智能（AI）中的重要性增加，现有的指令数据缺乏工具交互能力，而当前的基准测试依赖昂贵的人工标注，限制了其扩展性。本文介绍TaskCraft，这是一种自动化的任务生成工作流，能够创建难度可调节、支持多工具交互且可验证的任务，并包含执行轨迹。TaskCraft通过深度和宽度扩展原子任务，构建结构和层次复杂的挑战。实证结果显示，这些任务优化了生成工作流中的提示，并提升了对自主基础模型的监督微调效果。此外，研究还提供了一个包含约36,000个任务的大规模合成数据集，以支持未来关于代理调优和评估的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:14 GMT</pubDate>
</item>
<item>
<title>基于LLM辅助系统的自主学习能力培养研究</title>
<link>https://arxiv.org/abs/2506.09968</link>
<guid>https://arxiv.org/abs/2506.09968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过游戏化和AI支持系统提升大学生自主学习技能。</p><br /><br /><p><strong>摘要：</strong> 自主学习能力（SRL）对大学生适应学术挑战至关重要。本研究针对59名大学生面临的自主学习技能发展难题，如目标设定、时间管理和反思性学习困难，开发了SRLAgent系统。该系统基于Zimmerman的三阶段SRL框架，利用大型语言模型（LLM）提供实时反馈和适应性支持，在游戏化环境中促进学生的自主学习。实验结果显示，SRLAgent组学生的自主学习技能显著提高（p < .001, 效应值d=0.234），且用户参与度高于对照组。这项研究强调了将自主学习支架和实时AI支持嵌入游戏化环境中的价值，为教育技术的设计提供了重要启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:45:03 GMT</pubDate>
</item>
<item>
<title>TransDiff：结合Transformer与扩散模型的图像生成新方法</title>
<link>https://arxiv.org/abs/2506.09482</link>
<guid>https://arxiv.org/abs/2506.09482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransDiff结合AR Transformer与扩散模型，在ImageNet上性能超越其他方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TransDiff，一种首次将自回归（AR）Transformer与扩散模型相结合的图像生成模型。在该联合建模框架下，TransDiff通过编码标签和图像到高层次语义特征，并利用扩散模型估计图像样本分布。在ImageNet 256x256基准测试中，TransDiff显著优于基于独立AR Transformer或扩散模型的其他图像生成模型。具体而言，TransDiff实现了Fréchet Inception Distance (FID) 为1.61和Inception Score (IS) 为293.4的性能，并且相比最先进的AR Transformer方法推理延迟快2倍，比纯扩散模型方法快112倍。此外，基于TransDiff模型，我们提出了一种新的图像生成范式——多参考自回归（MRAR），它通过预测下一个图像进行自回归生成，允许模型引用多个先前生成的图像，从而促进学习更丰富的表示并提高后续迭代生成图像的质量。应用MRAR后，TransDiff的FID从1.61降至1.42。我们预计TransDiff将在图像生成领域开辟新的研究前沿。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 03:50:31 GMT</pubDate>
</item>
<item>
<title>MATTER：结合材料知识的新型分词方法提升科学文本处理性能</title>
<link>https://arxiv.org/abs/2506.11115</link>
<guid>https://arxiv.org/abs/2506.11115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MATTER，一种结合材料知识的分词方法，改善材料科学领域语言模型的表现。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型在材料科学中的应用日益广泛，传统的基于频率的分词方法因无法维持材料概念的结构和语义完整性而受到限制。本文提出了一种名为MATTER的新方法，通过集成材料知识并优先考虑材料概念，有效解决了现有分词方法的问题。实验表明，MATTER在生成和分类任务中分别提升了4%和2%的平均性能，强调了领域知识在科学文本处理中的重要性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:59:13 GMT</pubDate>
</item>
<item>
<title>DeepEDM：结合深度学习与非线性动力学系统的时序预测框架</title>
<link>https://arxiv.org/abs/2506.06454</link>
<guid>https://arxiv.org/abs/2506.06454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种融合深度神经网络与非线性动力学建模的新框架DeepEDM。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中复杂非线性动态时间序列的预测问题，提出了DeepEDM框架。该框架基于Takens定理，通过延迟嵌入学习潜在空间，并利用核回归逼近隐藏的动力学机制，同时采用高效的Softmax注意力实现精准的未来时间步预测。实验表明，DeepEDM在合成数据及跨领域真实数据上均表现出较强的鲁棒性和预测准确性，优于现有方法。研究代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:24:12 GMT</pubDate>
</item>
<item>
<title>离散扩散语言模型与多模态语言模型综述</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">离散扩散模型在并行生成和推理加速方面优于自回归模型。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs），这些模型采用多令牌并行解码范式，与自回归模型相比具有生成速度快、可控性强等优势。文章追溯了dLLMs和dMLLMs的历史发展，总结了训练与推理的关键技术，并分析了其在语言、视觉-语言及生物领域的应用前景。此外，本文还讨论了未来研究方向和部署挑战，强调了这些模型在学术界和工业界的快速发展趋势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>预算引导：通过轻量级预测控制大语言模型推理长度</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出预算引导方法，有效控制大语言模型推理长度并提升效率。</p><br /><br /><p><strong>摘要：</strong> 近期深度大型语言模型通常通过大量推理来提高性能，但这种方法可能导致过高的推理成本且性能提升有限。因此，在有限的推理预算下控制推理长度至关重要，但具有挑战性。本文提出了一种名为预算引导的新方法，无需对语言模型进行微调即可指导其推理过程达到目标预算。该方法引入了一个轻量级预测器，在每次生成下一个令牌时预测剩余推理长度的伽马分布。这一信号被用于以软方式、令牌级别的方式指导生成过程，确保整体推理符合指定的预算限制。实验表明，预算引导方法在数学基准测试中显著提高了令牌效率，例如在MATH-500基准测试中，与基线方法相比，在严格预算下准确率提高了26%，同时仅使用全推理模型63%的令牌即保持竞争力。此外，该方法在更广泛的任务领域表现出色，并展现出估计问题难度等新能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>Test3R：通过测试时学习显著提升3D重建几何精度</title>
<link>https://arxiv.org/abs/2506.13750</link>
<guid>https://arxiv.org/abs/2506.13750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Test3R技术，利用图像三元组优化网络，大幅提升3D重建几何一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Test3R的简单但有效的测试时学习技术，用于增强密集匹配方法在3D重建中的几何准确性。传统方法依赖于成对点图回归，但存在全局几何一致性不足的问题。Test3R通过使用图像三元组(I_1, I_2, I_3)，分别生成基于(I_1, I_2)和(I_1, I_3)的重建，并在测试时通过自监督目标优化网络，即最大化这两重建之间的几何一致性。这种方法确保了模型输出的跨对一致性，无论输入如何。实验结果显示，Test3R在3D重建和多视图深度估计任务上显著优于现有最先进的方法，且具有广泛的适用性和几乎零成本的特点。此外，该技术易于与其他模型集成，并在测试时仅需极小的训练开销和参数量。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:56:22 GMT</pubDate>
</item>
<item>
<title>Ego-R1框架：通过强化学习实现超长时间第一人称视频推理</title>
<link>https://arxiv.org/abs/2506.13654</link>
<guid>https://arxiv.org/abs/2506.13654</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Ego-R1框架，利用工具链式推理解决超长时第一人称视频理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Ego-R1的新框架，用于处理长达数天甚至数周的第一人称视频推理。该框架通过结构化的工具链式思维（CoTT）过程实现推理，由经过强化学习训练的Ego-R1智能体协调操作。受到人类解决问题策略的启发，CoTT将复杂的推理分解为模块化步骤，每一步调用特定工具完成子问题求解，如时间检索和多模态理解。为了训练该智能体，设计了包含监督微调（SFT）和强化学习（RL）两个阶段的训练范式，并构建了Ego-R1 Data数据集，其中包括Ego-CoTT-25K用于SFT，Ego-QA-4.4K用于RL。此外，Ego-R1智能体在新创建的一周视频问答基准Ego-R1 Bench上进行了评估，该基准包含来自混合来源的人类验证问答对。实验结果表明，Ego-R1智能体能够有效应对理解超长第一人称视频的独特挑战，时间覆盖范围从几小时显著扩展到一周。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13654" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 12:17:08 GMT</pubDate>
</item>
<item>
<title>MiniMax-M1：全球首个开放权重大规模混合注意力推理模型发布</title>
<link>https://arxiv.org/abs/2506.13585</link>
<guid>https://arxiv.org/abs/2506.13585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniMax-M1是一款支持超长上下文处理的高效混合注意力推理模型。</p><br /><br /><p><strong>摘要：</strong> MiniMax-M1是全球首款开放权重、大规模混合注意力推理模型，由混合专家（MoE）架构与闪电注意力机制驱动，基于MiniMax-Text-01升级而来，总参数达4560亿，单token激活459亿参数，上下文长度可达100万tokens，是DeepSeek R1的8倍。该模型通过大规模强化学习训练，在复杂任务如软件工程和工具利用方面表现出色。此外，提出的CISPO算法进一步提升了强化学习效率，使M1仅用512块H800 GPU完成训练仅需三周，成本仅为$534,700。实验显示，MiniMax-M1在标准基准测试中与强开源模型相比表现相当甚至更优。MiniMax-M1已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 11:08:02 GMT</pubDate>
</item>
<item>
<title>结构化提示对大型语言模型文本分析能力的影响研究</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，结构化提示可有效引导大型语言模型进行复杂文本分析。</p><br /><br /><p><strong>摘要：</strong> 本研究设计并评估了一组概念验证的结构化工作流提示，旨在引导大型语言模型（LLMs）完成高阶语义和语言分析任务，如检测学术论文总结中的未经证实主张（信息完整性）及模糊代词引用（语言清晰度）。通过在Gemini Pro 2.5 Pro和ChatGPT Plus o3两款前沿模型上进行多轮系统性评估，发现模型在信息完整性任务上的表现因语法角色差异而异，且在仅提供摘要而非全文的情况下，ChatGPT表现出色，而Gemini性能显著下降。这一研究揭示了结构化提示在复杂文本分析中的潜力，同时强调了模型性能高度依赖于模型类型、任务性质及上下文条件，需进行严谨的特定模型测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 03:34:31 GMT</pubDate>
</item>
<item>
<title>基于提示的大型语言模型时间序列预测方法</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示无需重训即可让大型语言模型高效进行时间序列预测。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLMs）的进步为准确且高效的时间序列分析提供了新的可能性，但以往研究通常需要大量微调或忽略了时间序列间的相关性。本文探索了一种简单灵活的基于提示的方法，使LLMs能够在不进行大规模重新训练或不依赖复杂外部架构的情况下完成时间序列预测。通过利用时间序列分解、基于块的标记化以及基于相似性的邻居增强等专门提示方法，我们发现可以提升LLM的预测质量，同时保持模型的简洁性和对数据预处理的需求最小化。为此，我们提出了PatchInstruct方法，该方法使LLMs能够做出精确有效的预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 15:42:58 GMT</pubDate>
</item>
<item>
<title>PersonaFeedback：评估大型语言模型个性化能力的新基准</title>
<link>https://arxiv.org/abs/2506.12915</link>
<guid>https://arxiv.org/abs/2506.12915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准PersonaFeedback，用于评估LLMs根据用户画像生成个性化响应的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）通用能力的提升，如何构建能提供个性化服务的系统成为重要研究课题。然而，缺乏高质量的个性化评估基准阻碍了这一领域的发展。本文介绍PersonaFeedback，这是一个直接评估LLMs生成个性化响应能力的新基准，它通过显式用户画像进行测试，而非依赖隐式推断。该基准包含8298个人类注释的测试案例，分为易、中、难三个层级。实验结果显示，即使是最先进的LLMs在难度较高的测试中也表现不佳，甚至人类评估者也可能难以区分细微差异。此外，分析表明当前的检索增强框架并非个性化任务的默认解决方案。所有数据、注释协议和评估流程都将公开，以促进未来的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 13:19:19 GMT</pubDate>
</item>
<item>
<title>面向用户界面教学视频的多模态摘要新基准</title>
<link>https://arxiv.org/abs/2506.12623</link>
<guid>https://arxiv.org/abs/2506.12623</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出针对UI教学视频的多模态摘要新基准，填补现有数据集空白。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于教学视频的多模态摘要生成问题，旨在通过文本指令和关键帧为用户提供高效技能学习方式。然而，现有视频摘要数据集主要关注通用语义层面的视频总结，无法满足提供逐步可执行说明的需求。为此，我们构建了一个名为MS4UI的新数据集，包含2413个UI教学视频，总时长达167小时，并对视频进行分割、文本摘要及视频摘要的手动标注。实验表明，目前最先进的多模态摘要方法在UI教学视频摘要任务上表现不佳，强调了开发新方法的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12623" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 16:39:32 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的表示对齐及其跨语言控制方法</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型具有自然出现的表示对齐能力，并提出了一种新的跨语言控制方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现，以及这种对齐对分离语言特定信息和语言无关信息的意义。通过实证研究，我们验证了这种对齐的存在，并将其与显式设计的对齐模型进行比较分析，展示了其在不损害语义的情况下实现语言特定操作的潜力。基于这些发现，我们提出了推理时语言控制（ITLC），一种利用潜在注入的新方法，以实现精确的跨语言控制并减轻LLMs中的语言混淆问题。实验表明，ITLC在保持目标语言语义完整性的同时展现出强大的跨语言控制能力，并有效缓解了现有大规模LLMs中存在的跨语言语言混淆问题，从而导致语言生成的一致性不足。本研究加深了我们对LLMs表示对齐的理解，并为提高其跨语言性能提供了实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 07:09:50 GMT</pubDate>
</item>
<item>
<title>大型语言模型人格解读：基于Supernova Event Dataset的事件提取与基准测试</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过新数据集评估多种语言模型的人格特质及事件提取能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法，利用Supernova Event Dataset来分析大型语言模型（LLMs）的人格特质及其对文本中关键事件提取和排序的能力。该数据集涵盖了传记、历史事件、新闻报道及科学发现等多样化内容。研究对象包括小型模型（如Phi-4、Orca 2、Qwen 2.5）和强大的大型模型（如Claude 3.7、Gemini 2.5、OpenAI o3）。通过让另一款LLM作为裁判，根据模型对事件的选择和分类推断其人格特征。研究揭示了各模型的独特人格特质，例如Orca 2专注于人际动态的情感推理，而Qwen 2.5则表现出战略性分析风格。此外，在处理科学发现事件时，Claude Sonnet 3.7侧重概念框架构建，Gemini 2.5 Pro强调实证验证，o3倾向于逐步因果推理。这项工作提高了模型的可解释性，使其更适合广泛多样的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 15:31:52 GMT</pubDate>
</item>
<item>
<title>DeepResearch Bench：LLM驱动的研究代理能力评估基准</title>
<link>https://arxiv.org/abs/2506.11763</link>
<guid>https://arxiv.org/abs/2506.11763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepResearch Bench基准用于评估基于LLM的研究代理能力。</p><br /><br /><p><strong>摘要：</strong> 深度研究代理（DRAs）作为大型语言模型（LLMs）驱动的代理类别，能够通过多步骤网络探索、目标检索和高级合成将海量在线信息转化为高质量的研究报告。然而，目前缺乏系统评估这些代理能力的综合基准。为填补这一空白，我们发布了DeepResearch Bench，包含由22个领域专家设计的100项博士级研究任务。由于评估DRAs复杂且耗时，我们提出了两种新方法，一种是参考导向法，采用自适应标准评估生成报告的质量；另一种框架则通过有效引用数量和总体引用准确性来衡量信息检索能力。DeepResearch Bench及相关组件已开源，以推动实用LLM代理的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 09:17:32 GMT</pubDate>
</item>
<item>
<title>科学家首次考试（SFE）基准评测科学多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准SFE评估科学多模态大语言模型的认知能力，发现当前顶级模型表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 随着科学研究对复杂多模态推理的需求增加，科学多模态大语言模型（MLLMs）被寄予厚望，可以显著提升科学发现的效率。然而，现有的科学基准主要集中在评估MLLMs的知识理解能力，对其感知和推理能力的评估不足。为弥补这一缺陷，我们提出了科学家的首次考试（SFE）基准，通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLMs的科学认知能力。SFE包含830个由专家验证的视觉问答对，涵盖五个高价值学科中的66个多模态任务。实验结果显示，最先进的GPT-o3和InternVL-3模型在SFE上的得分分别为34.08%和26.52%，表明这些模型在科学领域还有很大的改进空间。我们希望SFE提供的见解能够促进人工智能辅助科学发现的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 05:29:16 GMT</pubDate>
</item>
<item>
<title>ALE-Bench：评估AI系统在算法工程中的表现</title>
<link>https://arxiv.org/abs/2506.09050</link>
<guid>https://arxiv.org/abs/2506.09050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准ALE-Bench用于评估AI在优化问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ALE-Bench的新基准，旨在评估人工智能系统在解决诸如包裹配送路径规划、机组排班、工厂生产计划及电网平衡等复杂优化问题时的表现。该基准基于AtCoder启发式竞赛的实际任务构建，包含计算上难以处理且无已知精确解的问题。不同于短期通过/失败的编码测试，ALE-Bench强调长时间跨度内的迭代式解决方案改进。研究采用的软件框架支持交互式代理架构，允许利用运行反馈和可视化工具。实验表明，尽管前沿的大语言模型在特定问题上表现出色，但与人类相比，在跨问题的一致性及长期问题解决能力方面仍存在显著差距。这一发现凸显了设立此类基准的重要性，以推动未来AI技术的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>NoWait：高效多模态推理的插件式解决方案</title>
<link>https://arxiv.org/abs/2506.08343</link>
<guid>https://arxiv.org/abs/2506.08343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出NoWait方法，通过抑制自省信号提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 近期大规模推理模型的进步虽实现了复杂分步推理，但常伴随冗长和重复输出的问题，降低了效率。本研究探讨显式自我反思是否对高级推理必要，并提出了名为NoWait的方法，在推理过程中抑制诸如“等一下”、“嗯”之类的自省标记。实验结果显示，NoWait在文本、视觉及视频推理的十个基准测试中，将五种R1系列模型的推理轨迹长度减少了27%-51%，且不影响模型实用性。这一简单有效的方法为多模态推理提供了高效的实用型解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 21:54:04 GMT</pubDate>
</item>
<item>
<title>BridgeVLA：一种高效的三维视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2506.07961</link>
<guid>https://arxiv.org/abs/2506.07961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合3D信号的新型VLA模型BridgeVLA，显著提升了机器人操作学习的效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，利用预训练的视觉-语言模型(VLMs)构建视觉-语言-动作(VLA)模型成为机器人操作学习的有效方法之一，但现有方法对3D信号的利用不足，导致样本效率低下。本文提出了BridgeVLA，通过将3D输入投影到多个2D图像上并与VLM骨干对齐，同时利用2D热图进行动作预测，统一了输入与输出空间。此外，还设计了一种可扩展的预训练方法，在下游策略学习前赋予VLM骨干预测2D热图的能力。实验表明，BridgeVLA在三个模拟基准测试中均优于当前最先进的基线方法，尤其在RLBench、COLOSSEUM和GemBench中表现优异。在真实机器人实验中，BridgeVLA不仅在多种分布外设置中表现出稳健的泛化能力，还实现了极高的样本效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:36:34 GMT</pubDate>
</item>
<item>
<title>From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding</title>
<link>https://arxiv.org/abs/2506.03968</link>
<guid>https://arxiv.org/abs/2506.03968</guid>
<content:encoded><![CDATA[
The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:00:47 GMT</pubDate>
</item>
<item>
<title>构建AI代理行为科学：从模型到行为的系统性研究</title>
<link>https://arxiv.org/abs/2506.06366</link>
<guid>https://arxiv.org/abs/2506.06366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AI代理行为科学，强调观察、干预与理论指导。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的进步推动了具备人类行为特征的AI代理发展，这些行为不仅依赖模型架构，还受具体情境中的环境、社会线索及交互反馈影响。本文提出AI代理行为科学这一新视角，聚焦于观察行为、设计实验验证假设及基于理论解释AI的行为、适应性和交互过程。通过整合个体代理、多代理及人机交互的研究成果，该视角被应用于提升AI的公平性、安全性、可解释性、责任性和隐私保护等特性。本研究统一现有发现并规划未来方向，将AI代理行为科学定位为传统模型驱动方法的重要补充，为评估和管理日益自主的AI系统提供必要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:12:32 GMT</pubDate>
</item>
<item>
<title>Avey：一种突破注意力与循环机制的新神经基础架构</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种不依赖注意力和循环的新神经架构Avey，显著提升长序列处理能力。</p><br /><br /><p><strong>摘要：</strong> Transformer虽在语言模型领域广泛应用，但受限于固定上下文窗口和二次复杂度问题。本文提出Avey架构，通过排名器与自回归处理器协作，仅关注相关token，实现任意长度序列的有效处理。实验表明，Avey在短距离NLP基准测试中表现优异，尤其擅长捕捉长距离依赖关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 17:11:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不确定场景下的拒绝回答能力评估</title>
<link>https://arxiv.org/abs/2506.09038</link>
<guid>https://arxiv.org/abs/2506.09038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，大型语言模型在面对不确定问题时的拒绝回答能力亟待提升。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在复杂问题求解上取得了显著进展，但在处理不确定或无法回答的问题时仍存在明显不足。本文介绍了一个名为AbstentionBench的新基准，用于评估20个多样化数据集中的拒绝回答能力，涵盖未知答案、表述不清、错误前提、主观解释及过时信息等问题。实验结果显示，即使是最新推理型LLMs在拒绝回答方面的表现也远不如预期，甚至在经过推理微调后，该能力反而下降了24%。尽管精心设计的系统提示可以在一定程度上改善这一状况，但并未解决模型根本无法有效处理不确定性的问题。本研究旨在通过发布AbstentionBench推动LLMs可靠性的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型对反馈的吸收能力研究</title>
<link>https://arxiv.org/abs/2506.11930</link>
<guid>https://arxiv.org/abs/2506.11930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现即使在理想条件下，LLMs对反馈的吸收仍存在阻力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在接收外部反馈后的改进能力，通过构建受控实验环境，让解题模型在接收接近完整的反馈后重新尝试问题解答。实验涵盖了数学推理、知识推理、科学推理以及多领域综合评估等任务，使用了如Claude 3.7等先进的语言模型。尽管条件理想，但这些模型普遍表现出对反馈的抵抗现象，即所谓的“反馈摩擦”。我们尝试通过采样策略改善这一情况，但效果有限。此外，排除了模型过度自信和数据熟悉度等因素作为主要原因。希望揭示这一局限性能为未来LLMs的自我提升研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:31:51 GMT</pubDate>
</item>
<item>
<title>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards</title>
<link>https://arxiv.org/abs/2506.11474</link>
<guid>https://arxiv.org/abs/2506.11474</guid>
<content:encoded><![CDATA[
Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 01:36:30 GMT</pubDate>
</item>
<item>
<title>通过学习继续思考令牌提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.11274</link>
<guid>https://arxiv.org/abs/2506.11274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索了通过学习专用继续思考令牌来增强语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 测试时扩展计算是一种有效提升语言模型性能的方法，通过在推理阶段利用额外计算资源实现。近期研究表明，覆盖结束思考标记（如将“”替换为“等待”）可以延长推理步骤并提高准确性。本研究尝试引入一个专门的继续思考标记，通过强化学习训练其嵌入，同时保持模型权重不变。实验表明，此方法在标准数学基准测试中优于基线模型及固定标记的测试时扩展方法。例如，在GSM8K基准测试中，固定标记方法提升了1.3%的准确率，而学习标记方法则实现了比不使用预算强迫的基线模型高4.2%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 16:28:54 GMT</pubDate>
</item>
<item>
<title>Mirage-1：基于分层多模态技能的跨平台GUI代理</title>
<link>https://arxiv.org/abs/2506.10387</link>
<guid>https://arxiv.org/abs/2506.10387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的跨平台GUI代理Mirage-1，显著提升了长周期任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型作为图形用户界面（GUI）代理在线上环境处理长周期任务时的知识不足问题，提出了一种分层多模态技能（HMS）模块，通过逐步抽象轨迹形成执行技能、核心技能及元技能，构建层次化知识结构以支持长期规划。同时，引入技能增强的蒙特卡洛树搜索（SA-MCTS）算法，利用离线环境中习得的技能缩减在线探索中的动作搜索空间，从而弥合域间差距。在此基础上开发的Mirage-1是一种多模态、跨平台的即插即用型GUI代理。为验证其性能，构建了新基准AndroidLH，实验结果显示，相比现有方法，Mirage-1在多个测试集上的表现分别提升了32%、19%、15%和79%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:21:19 GMT</pubDate>
</item>
<item>
<title>基于LoRA调优的掩码引导视频编辑方法</title>
<link>https://arxiv.org/abs/2506.10082</link>
<guid>https://arxiv.org/abs/2506.10082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于掩码的LoRA调优方法，实现灵活可控的视频编辑。</p><br /><br /><p><strong>摘要：</strong> 当前基于扩散模型的视频编辑方法多依赖大规模预训练，缺乏灵活性。为解决这一问题，本文提出了一种基于掩码的LoRA（低秩适应）调优方法，该方法通过适配预训练的图像到视频（I2V）模型，在保持背景不变的同时实现对特定区域的可控编辑传播。此外，引入参考图像作为视觉锚点，帮助模型更好地理解编辑意图。实验表明，该方法在性能上优于现有最先进方法，且无需改变模型架构，展现出高效性和适应性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 14:03:55 GMT</pubDate>
</item>
<item>
<title>基于生成-修剪-排名范式的程序验证效率与准确性权衡</title>
<link>https://arxiv.org/abs/2506.10056</link>
<guid>https://arxiv.org/abs/2506.10056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出生成-修剪-排名方法，显著提升程序验证速度同时保持较高准确性。</p><br /><br /><p><strong>摘要：</strong> 当前通过大型语言模型解决编程任务的标准范式是生成代码后进行排名，其中排名步骤依赖于验证器。普遍认为当全面验证器可用时，应优先采用而非结果奖励模型(ORM)，且较少考虑速度与准确性之间的权衡。本研究挑战这一假设，系统性探索两者之间的平衡，发现ORM在利用速度换取准确率方面至关重要，尤其是在生成-修剪-排名范式中。这种新方法比传统全面测试套件快11.65倍，仅降低8.33%的准确性。分析表明，该方法通过过滤掉高度排名但错误的解决方案实现高效验证，从而为设计可扩展且精确的程序排名系统提供了可能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>JAFAR：轻量且灵活的基础视觉编码器特征上采样方法</title>
<link>https://arxiv.org/abs/2506.11136</link>
<guid>https://arxiv.org/abs/2506.11136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为JAFAR的轻量级特征上采样方法，提升视觉特征的空间分辨率。</p><br /><br /><p><strong>摘要：</strong> Foundation Vision Encoders在密集视觉任务中至关重要，但其低分辨率特征输出限制了下游任务的表现。本文介绍了一种名为JAFAR的新方法，它通过基于注意力机制的模块，利用低级图像特征衍生的高分辨率查询和语义丰富的低分辨率键进行空间特征变换(SFT)调制，从而增强特征的空间分辨率至任意目标分辨率。尽管缺乏高分辨率监督，JAFAR在低上采样比和分辨率下学习的效果能够很好地推广到更高的输出尺度。实验表明，JAFAR在恢复细粒度空间细节方面表现出色，并在多种下游任务中显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 16:53:12 GMT</pubDate>
</item>
<item>
<title>基于自我意识弱点驱动的问题合成框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.08989</link>
<guid>https://arxiv.org/abs/2506.08989</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我意识弱点驱动问题合成框架，提高大语言模型在复杂推理任务上的性能。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励机制（RLVR）对训练大型语言模型（LLMs）解决复杂推理任务如数学问题非常有效，但现有数据集的人类标注数学问题质量和答案精确性不足限制了其效果。大多数问题合成策略不考虑模型能力，导致效率低下。为解决这些问题，我们提出了自我意识弱点驱动问题合成框架（SwS），通过系统识别模型的薄弱环节并针对性生成新问题，显著提升了模型在主流推理基准测试中的平均性能，7B和32B模型分别提高了10.0%和7.7%，且无需外部知识蒸馏。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08989" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>Infinity-Instruct：提升大语言模型基础与对话能力的新基准数据集</title>
<link>https://arxiv.org/abs/2506.11116</link>
<guid>https://arxiv.org/abs/2506.11116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Infinity-Instruct数据集，增强开源模型的基础与指令跟随能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Infinity-Instruct的高质量指令数据集，旨在通过两阶段管道增强大型语言模型（LLMs）的基础和聊天能力。第一阶段，从超过1亿个样本中筛选出740万个高质量基础指令（InfInstruct-F-7.4M）。第二阶段，通过多阶段过程合成150万个高质量对话指令（InfInstruct-G-1.5M）。通过微调多个开源模型（如Mistral、LLaMA、Qwen和Yi），实验结果显示Infinity-Instruct显著提升了模型在基础和指令跟随任务上的表现，甚至在某些任务上超过了官方指令优化的模型。特别是，基于LLaMA的InfInstruct版本在指令跟随任务中的表现比GPT-4高出8.6%，同时在基础任务上表现相当。这些结果证明了基础训练和对话训练之间的协同效应，并为全面开发LLMs提供了新视角。相关数据集和代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:37:15 GMT</pubDate>
</item>
<item>
<title>基于候选标注的大语言模型数据标注方法</title>
<link>https://arxiv.org/abs/2506.03857</link>
<guid>https://arxiv.org/abs/2506.03857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大语言模型候选标注的新方法以提升数据质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大语言模型（LLMs）在数据标注中的局限性展开研究，这些方法通常通过单一标签策略进行标注，但难以应对复杂样本导致的错误标注问题。受人类规避不确定性行为的启发，我们提出了一种新的候选标注范式，即当模型存在不确定性时，鼓励其输出所有可能的标签。为适应下游任务需求，我们设计了一个教师-学生框架CanDist，利用小语言模型（SLM）对候选标注进行蒸馏。理论分析表明，这种候选标注的蒸馏方法优于直接采用单一标注。实验结果在六个文本分类任务中验证了该方法的有效性，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 07:42:37 GMT</pubDate>
</item>
<item>
<title>多维线性循环神经网络在长距离依赖任务中的表现</title>
<link>https://arxiv.org/abs/2506.11997</link>
<guid>https://arxiv.org/abs/2506.11997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩展线性RNN至多维结构，提出pLSTM模型，适用于复杂图结构数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的多维线性循环神经网络（pLSTM），通过引入Source、Transition和Mark门，使其能够处理一般有向无环图（DAG）上的数据。pLSTM不仅能够并行化操作，还解决了长距离依赖问题，采用定向传播模式和扩散分布模式。实验表明，pLSTM在合成箭头指向外推任务及分子图和计算机视觉基准测试中表现出色，尤其在处理更大图像时优于Transformer模型。代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:51:37 GMT</pubDate>
</item>
<item>
<title>大型语言模型在编程竞赛中的表现评估</title>
<link>https://arxiv.org/abs/2506.11928</link>
<guid>https://arxiv.org/abs/2506.11928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型在编程竞赛中仍显著落后于人类专家。</p><br /><br /><p><strong>摘要：</strong> 近期报道声称大型语言模型（LLMs）在编程竞赛中已超越顶尖人类选手。然而，本研究通过分析国际算法竞赛奖牌得主的经验，重新审视这一说法，探讨LLMs与人类专家的区别及现有局限。我们引入LiveCodeBench Pro基准测试，涵盖来自Codeforces、ICPC和IOI的问题，并由金牌得主对问题进行分类和错误代码分析。结果显示，即使是最前沿的模型，在中等难度问题上的pass@1仅为53%，而在难题上则为0%，远不及人类专家。尽管LLMs在实现类问题上表现良好，但在复杂算法推理和案例分析方面存在明显不足，且常给出错误的自信解释。高绩效主要依赖于代码精确性而非更强的推理能力。因此，LiveCodeBench Pro不仅揭示了LLMs与人类大师水平之间的显著差距，还提供了详细的诊断工具，以指导未来代码型LLM推理能力的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:29:09 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的对齐新视角图像与几何生成框架</title>
<link>https://arxiv.org/abs/2506.11924</link>
<guid>https://arxiv.org/abs/2506.11924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过扭曲与填补方法生成对齐的新视角图像与几何的扩散模型框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于扩散模型的框架，该框架通过扭曲与填补的方法实现对齐的新视角图像和几何生成。不同于以往需要密集姿态图像或受限于域内视图的姿态嵌入生成模型的方法，我们的方法利用现成的几何预测器预测参考图像所看到的部分几何形状，并将新视角合成表述为图像和几何的填补任务。为了确保生成图像和几何之间的精确对齐，我们提出了跨模态注意力蒸馏，在训练和推理过程中将图像扩散分支的注意力图注入到并行的几何扩散分支中。这种多任务方法实现了协同效应，促进了具有几何鲁棒性的图像合成以及定义良好的几何预测。此外，我们引入了基于邻近性的网格条件化，以整合深度和法线线索，插值点云并过滤掉错误预测的几何形状对生成过程的影响。实证研究表明，我们的方法在多种未见场景下实现了高保真的外推视角合成，无论是图像还是几何，并且在插值设置下提供了竞争性的重建质量，还生成了几何对齐的彩色点云以实现全面的3D补全。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:19:00 GMT</pubDate>
</item>
<item>
<title>FourierAttention：一种高效的大语言模型长上下文处理方法</title>
<link>https://arxiv.org/abs/2506.11886</link>
<guid>https://arxiv.org/abs/2506.11886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的FourierAttention框架，提升大语言模型长上下文处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着上下文长度的增长，大语言模型面临日益增加的记忆需求，现有压缩方法往往牺牲精度或引入计算开销。本文提出FourierAttention，这是一种无需训练的框架，通过利用Transformer头维度的异构角色，在较低维度优先处理局部上下文的同时，较高维度捕捉远距离依赖。通过将对长上下文不敏感的维度投影到正交傅里叶基上，FourierAttention用固定长度的频谱系数近似其时间演化过程。实验表明，FourierAttention在LongBench和Needle-In-A-Haystack数据集上的长上下文准确性达到最佳。此外，还设计了一个定制的Triton内核FlashFourierAttention，优化内存使用并通过流线型读写操作实现高效部署，性能不受影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 11:35:54 GMT</pubDate>
</item>
<item>
<title>Configurable Preference Tuning (CPT): 动态调整AI行为的新框架</title>
<link>https://arxiv.org/abs/2506.11702</link>
<guid>https://arxiv.org/abs/2506.11702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入CPT框架，使语言模型可根据人类指令动态调整行为。</p><br /><br /><p><strong>摘要：</strong> 现有的AI对齐模型通常基于固定的单一偏好集，限制了适应性。本文提出Configurable Preference Tuning (CPT)，一种新颖的框架，允许语言模型根据显式的人类可解释指令动态调整行为。CPT通过利用基于结构化细粒度量表生成的合成偏好数据，这些量表定义了如写作风格等期望属性，从而实现这一目标。通过在这些量表引导的偏好上微调，LLM能够在推理时响应系统提示调整输出，而无需重新训练。这种方法不仅提供了细粒度控制，还为建模更细腻和情境依赖的人类反馈提供了机制。研究中的多个实验资源，包括训练代码、生成的数据集和微调模型，已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 08:17:38 GMT</pubDate>
</item>
<item>
<title>Duo：通过高斯扩散改进离散扩散模型以提升文本生成性能</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Duo通过引入课程学习和一致性蒸馏技术，显著提升了离散扩散模型的训练效率和采样速度。</p><br /><br /><p><strong>摘要：</strong> 离散均匀状态扩散模型因其自我校正能力而具有快速文本生成的潜力，但通常表现不如自回归模型和掩码扩散模型。本研究通过利用高斯扩散过程的洞察，提出了一种名为Duo的新方法，通过课程学习策略和离散一致性蒸馏技术，大幅提高了训练速度和采样效率。实验结果显示，采用课程学习后，Duo在零样本困惑度上超越了自回归模型，在7个基准中的3个上表现优异；而离散一致性蒸馏则使扩散语言模型的采样速度加快了两个数量级。这项工作有效缩小了离散扩散模型与更先进模型之间的性能差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:55:35 GMT</pubDate>
</item>
<item>
<title>通过视觉描述幻觉批评提升视觉语言模型的感知能力</title>
<link>https://arxiv.org/abs/2506.10128</link>
<guid>https://arxiv.org/abs/2506.10128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ViCrit任务，通过检测图像描述中的细微错误提升视觉语言模型的感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViCrit（视觉描述幻觉批评）的强化学习代理任务，旨在提升视觉语言模型（VLMs）在视觉感知方面的表现。ViCrit任务通过在人类编写的图像描述中注入微妙的合成视觉幻觉错误，训练模型定位这些错误。此方法不仅保留了视觉感知的全部难度，还提供了易于计算且明确的二元奖励机制。实验表明，使用ViCrit任务训练的模型在多种视觉语言基准测试中取得了显著改进，这些改进不仅限于自然图像数据，还能推广到抽象图像推理和视觉数学等领域。此外，为了促进评估，我们还推出了ViCrit-Bench，这是一个类别平衡的诊断基准，系统性地探测了不同图像领域和错误类型的感知错误。总体而言，我们的研究证明了细粒度幻觉批评是一种有效且可泛化的视觉感知增强目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 15:16:54 GMT</pubDate>
</item>
<item>
<title>对抗性用户对政策合规AI代理的威胁及防御策略研究</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究政策合规AI代理在客户服务中的安全性和鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 随着任务导向型基于大型语言模型（LLM）的智能体在受政策约束领域的应用增加，如退款资格或取消规则等，如何保证这些智能体始终遵守相关政策并拒绝违规请求成为一大挑战。本文提出了一种新的威胁模型，关注试图利用政策合规智能体谋取个人利益的恶意用户。为解决这一问题，我们开发了CRAFT，一个多智能体红队系统，通过采用基于政策感知的说服策略，在客户服务质量评估场景下有效削弱政策合规智能体的表现，超越了传统的越狱方法。此外，基于现有的tau-bench基准，我们引入了tau-break，一个专门用于评估智能体对操纵性用户行为的稳健性的补充基准。最后，我们测试了几种简单但有效的防御措施，虽然提供了一定程度的保护，但仍需更强有力的研究驱动的安全保障来抵御潜在的攻击。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 06:59:47 GMT</pubDate>
</item>
<item>
<title>InterSyn：基于自评估迭代精炼的大规模多模态数据集</title>
<link>https://arxiv.org/abs/2506.09427</link>
<guid>https://arxiv.org/abs/2506.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InterSyn数据集和SynJudge评估模型，提升多模态模型生成能力。</p><br /><br /><p><strong>摘要：</strong> 近年来大型多模态模型（LMMs）在多模态理解和生成方面取得显著进展，但生成紧密交织的图文输出仍具挑战性，主要受限于现有训练数据集的规模、质量和指导丰富度。为此，我们引入InterSyn数据集，利用自评估与迭代精炼（SEIR）方法构建，提供多轮指令驱动的图文对话，具备丰富的对象多样性和严格的自动化质量优化，适合作为下一代指令跟随型LMMs的训练数据。同时，针对缺乏可靠评估工具的问题，我们开发了SynJudge自动评估模型，从文本内容、图像内容、图像质量和图文协同四个维度量化评估多模态输出。实验表明，SEIR方法显著提高了数据集质量，而基于InterSyn训练的LMMs在所有评估指标上均表现更优，验证了InterSyn在推动多模态系统发展中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 02:21:20 GMT</pubDate>
</item>
<item>
<title>SkillBlender：一种用于人形机器人灵活操控的分层强化学习框架</title>
<link>https://arxiv.org/abs/2506.09366</link>
<guid>https://arxiv.org/abs/2506.09366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的分层强化学习框架SkillBlender，实现人形机器人的多样化操控。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkillBlender的新框架，旨在解决现有方法在处理人形机器人多任务适应性时存在的问题。通过预训练无任务特定条件的基础技能并动态混合这些技能，SkillBlender能够在最小化任务特定奖励工程的情况下完成复杂的操控任务。同时，我们还开发了一个名为SkillBench的模拟基准，包含三种机器人形态、四种基础技能及八个挑战性任务，用于科学评估。实验表明，SkillBlender显著优于其他基线模型，在准确性与可行性方面表现出色。未来，我们的代码和基准将开源供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:24:26 GMT</pubDate>
</item>
<item>
<title>基于自精炼框架的无标注数据增强ASR性能研究</title>
<link>https://arxiv.org/abs/2506.11130</link>
<guid>https://arxiv.org/abs/2506.11130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用无标注数据提升语音识别性能的自精炼框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为自精炼框架的方法，通过仅使用无标注数据即可提升自动语音识别（ASR）系统的性能。该框架首先利用现有的ASR模型对无标注语音生成伪标签，然后使用这些伪标签训练高保真的文本到语音（TTS）系统。接着，将合成的语音文本对反馈至原始ASR系统，完成闭合循环的自我改进过程。此方法在台湾华语语音上进行了验证，在6000小时无标注语音、少量文本数据及AI模型生成的合成内容的支持下，成功将Whisper-large-v2模型转化为专门化的Twister模型。结果显示，Twister在普通话基准测试中的错误率降低了20%，而在普通话-英语混合代码转换基准测试中的错误率降低了50%。这一框架不仅优于传统的伪标签自蒸馏方法，还为低资源或特定领域ASR性能提升提供了实际可行的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:30:32 GMT</pubDate>
</item>
<item>
<title>基于二值注意力掩码的图像预测方法</title>
<link>https://arxiv.org/abs/2506.08915</link>
<guid>https://arxiv.org/abs/2506.08915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用二值注意力掩码提升图像预测鲁棒性的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于注意力机制的方法，通过学习得到的二值注意力掩码，确保只有被关注的图像区域影响最终预测结果。这种方法旨在解决对象感知中的上下文干扰问题，尤其是在物体出现在分布外背景时可能导致的偏差表示。为了应对这一挑战，我们提出了一个双阶段框架：第一阶段处理完整图像以发现对象部分并确定任务相关的区域；第二阶段利用输入注意力掩码限制其感受野到这些区域，从而实现聚焦分析同时过滤掉潜在的虚假信息。两个阶段联合训练，使第二阶段能够优化第一阶段的表现。大量实验表明，该方法显著提高了对虚假相关性和分布外背景的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:41:22 GMT</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[
This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 05:00:33 GMT</pubDate>
</item>
<item>
<title>U-CoT+: 一种高效灵活的有害模因检测框架</title>
<link>https://arxiv.org/abs/2506.08477</link>
<guid>https://arxiv.org/abs/2506.08477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架U-CoT+，通过文本描述和指导优化有害模因检测效率与灵活性。</p><br /><br /><p><strong>摘要：</strong> 有害模因的检测对维护在线环境的完整性至关重要，但现有方法在资源效率、灵活性和可解释性方面存在局限性。本文介绍了一种名为U-CoT+的新框架，该框架通过开发高保真的模因到文本转换管道，将视觉模因转化为细节保留的文本描述，从而实现对复杂原始视觉内容的高效分解与分类。此设计解耦了模因解释与分类过程，使基于通用大型语言模型的有害模因检测更加高效。进一步结合针对性的人类指导准则，在零样本思维链提示下引导模型推理，增强了框架的适应性和可解释性。实验验证了该框架的有效性，表明其在低资源条件下利用小型语言模型进行有害模因检测具有广阔潜力。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:10:45 GMT</pubDate>
</item>
<item>
<title>基于Reg-GRPO的视频大型语言模型增强视频推理能力的研究</title>
<link>https://arxiv.org/abs/2506.07464</link>
<guid>https://arxiv.org/abs/2506.07464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种改进的强化学习算法Reg-GRPO，显著提升视频大型语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，基于强化学习的后训练方法可以有效增强大型语言模型的推理能力，其中Group Relative Policy Optimization (GRPO)表现出色。然而，GRPO在视频大型语言模型（Video LLMs）中的应用尚未深入探索。本文研究了GRPO应用于Video LLMs时存在的两个主要问题：对安全措施的依赖及优势消失的问题。为解决这些问题，我们提出了DeepVideo-R1，这是一种结合了Reg-GRPO（回归形式的GRPO）和难度感知数据增强策略的视频大型语言模型。Reg-GRPO将GRPO目标重新定义为回归任务，直接预测优势值，从而消除了对安全措施如裁剪和最小函数的需求，使模型政策指导更加直接。此外，我们设计了一种难度感知的数据增强策略，在可解难度级别动态扩充训练样本，产生多样化且信息丰富的奖励信号。实验表明，DeepVideo-R1在多个视频推理基准测试中显著提升了视频推理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:15:54 GMT</pubDate>
</item>
<item>
<title>HeadHunter: 针对扩散模型注意力扰动的细粒度控制方法</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HeadHunter框架，实现扩散模型生成质量的精细调控。</p><br /><br /><p><strong>摘要：</strong> 现有扩散模型中的注意力扰动方法在确定扰动位置时缺乏系统性，尤其是在Diffusion Transformer架构中，质量相关的计算分布在多层中。本文研究了注意力扰动的不同粒度，发现特定的注意力头控制着不同的视觉概念，如结构、风格和纹理质量。基于此，我们提出了HeadHunter框架，用于迭代选择与用户目标对齐的注意力头，从而实现生成质量与视觉属性的精细控制。此外，我们引入SoftPAG，通过线性插值注意力图向单位矩阵靠近，提供连续调节扰动强度的旋钮以抑制伪影。实验验证表明，该方法不仅缓解了现有层级扰动导致的过度平滑问题，还实现了特定视觉风格的目标操控。本研究首次对扩散模型中的注意力扰动进行了头级别分析，揭示了注意力层内的可解释专业化，并为设计有效的扰动策略提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>AutoMind：一种适应性强的知识驱动型大语言模型代理框架</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoMind提升自动化数据科学能力，优于现有最先进方法。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）代理在解决实际数据科学问题方面展现出巨大潜力，但现有框架受限于固定工作流与编码策略，难以处理复杂创新任务。本文提出AutoMind框架，通过构建领域专家知识库、采用知识驱动树搜索算法及动态自适应编码策略，克服现有缺陷。评估显示，AutoMind在两个自动化数据科学基准测试中表现优异，效率、效果及解的质量均优于当前最佳基线，标志着迈向全自动数据科学的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>SWE-Factory：自动化构建大规模GitHub问题解决数据集</title>
<link>https://arxiv.org/abs/2506.10954</link>
<guid>https://arxiv.org/abs/2506.10954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-Factory自动化流水线，显著提升GitHub问题解决数据集构建效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模GitHub问题解决数据集构建的传统挑战，提出名为SWE-Factory的自动化流水线。该流水线通过SWE-Builder多智能体系统实现评估环境的自动构建，采用基于退出码的标准化评分方法取代自定义解析器，并通过可靠退出码信号实现失败转成功的自动化验证过程。实验表明，在四种编程语言的671个问题上，SWE-Builder以最低成本构造有效实例，且基于退出码的评分方法达到100%准确性，自动验证也表现出高精度与完全召回率。我们希望此流水线能加速高质量数据集的收集，用于训练和评估大型语言模型的软件工程能力。相关代码与数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:54:17 GMT</pubDate>
</item>
<item>
<title>迈向自主网络代理的新交互范式：Agentic Web Interface 的提出</title>
<link>https://arxiv.org/abs/2506.10953</link>
<guid>https://arxiv.org/abs/2506.10953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种针对大型语言模型优化的网络界面设计，解决现有方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）及其多模态变体的发展引发了对网络代理研究的浓厚兴趣。网络代理是一种能够在网页环境中自主导航并完成任务的人工智能系统，然而当前方法因人类设计的界面与LLMs能力之间的根本不匹配而面临诸多挑战。这些问题体现在处理复杂的DOM树、依赖截图附加信息或绕过用户界面的API交互上。本文主张网络代理研究需要转变范式，而非让代理适应人为设计的界面，而是开发专门针对代理能力优化的新型交互方式。为此，我们提出了Agentic Web Interface（AWI）的概念，这是一种专为代理设计的网络界面，并制定了六项指导原则，强调安全、效率和标准化，以平衡各利益相关方的需求。这种重新定义旨在克服现有界面的根本限制，推动更高效、可靠且透明的网络代理设计，这一过程需由整个机器学习社区共同参与。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:58 GMT</pubDate>
</item>
<item>
<title>Domain2Vec：一种高效的数据集分解方法</title>
<link>https://arxiv.org/abs/2506.10952</link>
<guid>https://arxiv.org/abs/2506.10952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Domain2Vec通过分解数据集为元域向量优化语言模型预训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Domain2Vec的新方法，该方法将任意数据集分解为若干元域的线性组合。元域是一个新概念，用于捕捉数据集的关键特征。Domain2Vec通过分类器将数据集转化为对应的元域分布向量，从而无需训练即可确定最优数据混合方案。此方法基于分布对齐假设，表明当训练集和验证集的数据分布更匹配时，验证损失会更低。此外，Domain2Vec可以无缝集成到现有工作中，提升模型效率和可扩展性。实验表明，使用Domain2Vec进行预训练时，只需原计算资源的51.5%，即可达到相同的验证损失，同时在同等计算预算下，下游任务性能平均提高2.83%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:51 GMT</pubDate>
</item>
<item>
<title>基于半非负矩阵分解的大型语言模型可解释性特征提取</title>
<link>https://arxiv.org/abs/2506.10920</link>
<guid>https://arxiv.org/abs/2506.10920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过半非负矩阵分解直接分解MLP激活，提高因果导向能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中的机制可解释性问题，重点在于找到一种无监督方式来识别具有因果解释力的特征方向。传统方法依赖稀疏自动编码器（SAEs），但其在因果评估中表现不佳且缺乏内在可解释性。为此，我们引入半非负矩阵分解（SNMF）技术，将MLP激活直接分解为稀疏线性组合的神经元特征，并映射到激活输入上，从而提升特征的直观性和可解释性。实验表明，基于SNMF的方法在因果导向性能上优于SAEs及强监督基线，在Llama 3.1、Gemma 2和GPT-2等模型上表现出色。进一步分析揭示了神经元组合在语义相关特征间被复用的现象，展示了MLP激活空间中的层次结构。这些结果表明，SNMF是一种简单而有效的工具，用于识别LLMs中的可解释特征并解析概念表示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:33:29 GMT</pubDate>
</item>
<item>
<title>NoLoCo：一种无需显式同步的高效大规模语言模型训练方法</title>
<link>https://arxiv.org/abs/2506.10911</link>
<guid>https://arxiv.org/abs/2506.10911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需显式参数同步的优化方法NoLoCo，显著降低通信开销并提高收敛速度。</p><br /><br /><p><strong>摘要：</strong> 当前大规模语言模型的训练通常依赖于高度互联的计算集群，但扩展此类集群成本高昂且不切实际。近期研究提出了减少通信密集度的训练方法，但仍需模型参数的同步步骤，这在低带宽网络上可能变得昂贵。本文提出了一种名为NoLoCo的新型优化方法，该方法通过Nesterov动量优化器的变体隐式同步模型权重，无需显式同步所有模型参数，也无需集体通信。理论分析和实验结果表明，NoLoCo在各种加速器数量和模型规模（125M到6.8B参数）下表现出色，相比完全分片的数据并行训练和低通信训练方法DiLoCo，其通信开销显著减少。此外，在数百个互联网加速器上进行的同步步骤估计比DiLoCo中的all-reduce快一个数量级，且不存在全局阻塞通信，从而减少了加速器的空闲时间。实验还显示，NoLoCo对各种模型大小和加速器数量的收敛速度比DiLoCo快高达4%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:23:23 GMT</pubDate>
</item>
<item>
<title>Magistral：基于纯强化学习训练的语言模型</title>
<link>https://arxiv.org/abs/2506.10910</link>
<guid>https://arxiv.org/abs/2506.10910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magistral展示了纯强化学习训练大型语言模型的可能性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mistral的第一款推理模型Magistral及其自主研发的可扩展强化学习（RL）管道。不同于依赖现有实现或从先前模型中蒸馏出的RL跟踪，我们采取了自下而上的方法，完全依赖自身模型和基础设施。研究展示了纯强化学习训练大型语言模型的潜力，提出了一种强制模型推理语言的简单方法，并证明仅使用文本数据进行强化学习可以保持初始检查点的能力。此外，我们展示了仅使用文本的强化学习可以维持或改善多模态理解、指令跟随和功能调用能力。我们发布了Magistral Medium，它是基于Mistral Medium 3通过纯强化学习训练的模型，并开源了Magistral Small（采用Apache 2.0许可证），它还包括了来自Magistral Medium的冷启动数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:22:37 GMT</pubDate>
</item>
<item>
<title>CreatiPoster：一种支持多层可编辑图形设计的AI框架</title>
<link>https://arxiv.org/abs/2506.10890</link>
<guid>https://arxiv.org/abs/2506.10890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CreatiPoster利用自然语言指令生成高质量、可编辑的图形设计。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CreatiPoster的新框架，它可以从自然语言指令或用户提供的资产中生成可编辑的多层次图形作品。该框架通过协议模型生成包含每层布局、层级、内容和风格的JSON规范，并结合背景提示生成协调的背景。实验表明，CreatiPoster在生成图形设计方面超越了现有的开源方法和商业系统。此外，研究团队发布了一个包含100,000个多层设计的无版权数据集，以促进进一步的研究。CreatiPoster支持多种应用场景，如画布编辑、文本覆盖、响应式缩放、多语言适应和动态海报制作，推动了AI辅助图形设计的普及。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:54:39 GMT</pubDate>
</item>
<item>
<title>VRBench：首个长叙事视频基准用于评估大模型多步推理能力</title>
<link>https://arxiv.org/abs/2506.10857</link>
<guid>https://arxiv.org/abs/2506.10857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VRBench，首个长叙事视频基准，用于评估大型模型的多步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VRBench，这是一个专为评估大型模型多步推理能力而设计的首个长叙事视频基准。现有评估方法忽视了时间推理和程序有效性，而VRBench通过包含1010段长视频（平均时长1.6小时）、9468个人类标注的多步问答对以及30292个带有时间戳的推理步骤，解决了这些问题。这些视频经过多阶段过滤过程筛选，确保情节连贯性。我们开发了一种人机协作框架，生成连贯的推理链，每条链需要多个基于时间的步骤，涵盖七种类型（如事件归因、隐式推理）。VRBench还设计了一个多阶段评估管道，在结果和过程层面评估模型。除了多项选择题外，我们还提出了LLM引导的进度级评分指标，从多个维度全面评估推理链质量。通过对12个LLM和16个VLM在VRBench上的广泛评估，我们进行了深入分析并提供了有价值的见解，推动了多步推理领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:17:17 GMT</pubDate>
</item>
<item>
<title>基于文本推理模型的长视频理解框架</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过仅依赖文本推理模型实现长视频理解任务。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型在长视频理解（LVU）任务上面临复杂性和上下文窗口限制的挑战。传统观点认为需要扩展上下文窗口、增强视觉感知能力和领域专业知识的基础模型。本研究提出VideoDeepResearch框架，仅利用现有实践中的文本-only大推理模型及多模态工具包（如多模态检索器和视觉感知器），通过推理制定问题解决策略并选择性访问视频内容。实验表明，在MLVU、Video-MME和LVBench等基准测试中，该方法显著优于现有基础模型，分别提升了9.6%、6.6%和3.9%，证明了基于智能体系统的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 11:39:10 GMT</pubDate>
</item>
<item>
<title>PosterCraft：一种统一框架用于高审美海报生成</title>
<link>https://arxiv.org/abs/2506.10741</link>
<guid>https://arxiv.org/abs/2506.10741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架PosterCraft，显著提升海报生成的质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PosterCraft的新框架，旨在解决高审美海报生成中的挑战。与传统的模块化流水线不同，PosterCraft允许模型自由探索连贯且视觉吸引人的布局。该框架通过四个优化阶段实现高质量海报生成：大规模文本渲染优化、区域感知监督微调、美学文本强化学习及联合视觉语言反馈优化。这些阶段由专门设计的数据构建管道支持，无需复杂的架构修改即可进行稳健训练。实验表明，PosterCraft在文本渲染准确性、布局一致性及整体视觉吸引力方面优于开源基线，接近顶级商业系统的质量。相关代码、模型和数据集可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:28:12 GMT</pubDate>
</item>
<item>
<title>TaxoAdapt：动态适配的科学文献自动分类框架</title>
<link>https://arxiv.org/abs/2506.10737</link>
<guid>https://arxiv.org/abs/2506.10737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TaxoAdapt框架，提升科学文献分类的精度与适应性。</p><br /><br /><p><strong>摘要：</strong> 传统专家构建的科学文献分类体系耗时且成本高昂，而现有的自动方法存在特定语料库依赖性强或忽视领域动态变化的问题。为此，本文提出TaxoAdapt框架，通过迭代分层分类，在多个维度上扩展分类宽度和深度，以适应特定语料库的专题分布。实验表明，TaxoAdapt在多类计算机科学会议文献中表现优异，其生成的分类系统比现有最佳基线方法在粒度保存率和一致性上分别高出26.51%和50.41%，有效捕捉了科学领域的演变特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:26:28 GMT</pubDate>
</item>
<item>
<title>ClaimSpect：基于检索增强生成框架的复杂主张分解与视角表示</title>
<link>https://arxiv.org/abs/2506.10728</link>
<guid>https://arxiv.org/abs/2506.10728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ClaimSpect，用于自动构建主张的层级结构并分析其多维视角。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了科学和政治主张中复杂的表述通常难以简单归类为“真”或“假”，但可以通过拆解成子方面（如有效性、安全性）进行验证。为此，我们提出了ClaimSpect框架，它利用检索增强生成技术，将主张分解为其组成成分和子成分，并从特定语料库中提取相关段落来丰富这些方面。该框架不仅能够发现新的子方面，还能揭示针对某一主张方面的不同立场（支持、中立或反对）及其流行程度。通过应用到科学和政治主张的真实案例，并结合人工评估，证明了ClaimSpect在解析复杂主张和表示语料库观点方面的有效性和准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:17:45 GMT</pubDate>
</item>
<item>
<title>TeleMath：首个电信领域数学问题评估基准</title>
<link>https://arxiv.org/abs/2506.10674</link>
<guid>https://arxiv.org/abs/2506.10674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出TeleMath基准数据集，用于评估大语言模型在电信数学问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能在电信领域的广泛应用，对大语言模型（LLMs）处理特定领域数学密集型任务的能力产生了兴趣。尽管LLMs在一般数学推理方面取得了进展，但在信号处理、网络优化等专门领域的表现尚不明确。为此，我们引入TeleMath，这是一个针对电信领域数学问题设计的首个基准数据集，包含500组问答对。通过专家设计的问题种子，我们构建了该数据集并评估了多种开源LLMs的表现，发现专注于数学推理的模型表现最优，而通用模型则表现不佳。本研究还公开了数据集和评估代码，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 09:04:18 GMT</pubDate>
</item>
<item>
<title>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2506.10600</link>
<guid>https://arxiv.org/abs/2506.10600</guid>
<content:encoded><![CDATA[
Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:43:50 GMT</pubDate>
</item>
<item>
<title>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</title>
<link>https://arxiv.org/abs/2506.10568</link>
<guid>https://arxiv.org/abs/2506.10568</guid>
<content:encoded><![CDATA[
In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:58:23 GMT</pubDate>
</item>
<item>
<title>AniMaker：基于多智能体框架的文本驱动故事动画生成</title>
<link>https://arxiv.org/abs/2506.10540</link>
<guid>https://arxiv.org/abs/2506.10540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AniMaker框架，解决多场景故事动画生成中的叙事连贯性和稳定性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成模型在多场景叙事动画生成中的不足，提出了名为AniMaker的多智能体框架。该框架通过导演代理、摄影代理、审查代理及后期制作代理的协同工作，实现了从文本输入到全局一致且叙事连贯的动画生成。其中，摄影代理采用蒙特卡洛树搜索启发策略（MCTS-Gen），高效筛选高质量候选片段；审查代理则引入首个多镜头动画评估框架（AniEval），综合评估叙事一致性、动作完整性等特性。实验表明，AniMaker在多个指标上超越现有方法，显著提升生成效率，使AI生成的故事动画接近生产标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:06:21 GMT</pubDate>
</item>
<item>
<title>基于因果表示学习的语言模型能力评估框架</title>
<link>https://arxiv.org/abs/2506.10378</link>
<guid>https://arxiv.org/abs/2506.10378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种因果表示学习框架，用于有效评估语言模型的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过因果表示学习解决语言模型评估中的复杂混杂效应及高昂计算成本问题。研究利用线性变换将基准性能建模为潜在能力因素的函数，并控制基础模型作为共同混杂因子，确定这些因素间的因果关系。通过对涵盖1500多个模型的数据集进行分析，揭示了一种简洁的三节点线性因果结构，解释了性能变化。进一步解读显示了从一般问题解决能力到指令跟随能力再到数学推理能力的因果路径。结果强调了在评估中控制基础模型变异的重要性，这是准确揭示潜在模型能力间因果关系的关键步骤。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:07:42 GMT</pubDate>
</item>
<item>
<title>Optimus-3：面向Minecraft环境的多模态通用智能体</title>
<link>https://arxiv.org/abs/2506.10357</link>
<guid>https://arxiv.org/abs/2506.10357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于多模态大语言模型的通用智能体Optimus-3。</p><br /><br /><p><strong>摘要：</strong> 本文针对开放世界环境中构建具备感知、规划、行动、定位及反思能力的通用型智能体所面临的挑战，如领域特定数据不足、异构任务干扰及视觉多样性等问题，提出了三项创新解决方案。首先，设计了一种知识增强的数据生成管道，以提供可扩展且高质量的训练数据；其次，引入任务级路由的专家混合架构（MoE），减少异构任务间的干扰；最后，开发了多模态推理增强的强化学习方法，提升智能体对视觉多样性的处理能力。基于这些改进，我们推出了Optimus-3，一款针对Minecraft环境的通用智能体。实验结果表明，Optimus-3在多个任务上超越了现有的多模态大语言模型及智能体基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 01:29:40 GMT</pubDate>
</item>
<item>
<title>Discrete Audio Tokens: More Than a Survey!</title>
<link>https://arxiv.org/abs/2506.10274</link>
<guid>https://arxiv.org/abs/2506.10274</guid>
<content:encoded><![CDATA[
Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 21:35:43 GMT</pubDate>
</item>
<item>
<title>高效探针方法在自监督学习中的性能提升研究</title>
<link>https://arxiv.org/abs/2506.10178</link>
<guid>https://arxiv.org/abs/2506.10178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多查询交叉注意力机制，在多个基准测试中超越传统线性及现有注意力探针方法。</p><br /><br /><p><strong>摘要：</strong> 随着大规模微调变得不切实际，探针成为自监督学习评估的主要协议。然而标准线性探针难以充分反映掩码图像建模训练模型的潜力，因此激发了对关注探针的需求。尽管关注探针逐渐被采用，但其过度参数化和计算效率低的问题仍未解决。本文通过精度与效率权衡的角度重新审视关注探针，系统分析现有方法并引入高效探针（EP），该机制通过消除冗余投影和减少可训练参数数量实现高达10倍的速度提升。EP在七个基准测试中优于线性探针和先前的方法，同时在多样化预训练范式和低样本、分层设置中表现优异，且生成的注意力图具有可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 17:10:26 GMT</pubDate>
</item>
<item>
<title>面向文本恢复的图像修复方法研究</title>
<link>https://arxiv.org/abs/2506.09993</link>
<guid>https://arxiv.org/abs/2506.09993</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉与文本恢复的新任务及多任务扩散框架。</p><br /><br /><p><strong>摘要：</strong> 现有的基于扩散模型的图像修复方法虽在自然图像修复方面取得了成功，但在处理退化图像中的文本区域时往往表现不佳，容易产生看似合理但不正确的类似文本模式，即文本-图像幻觉现象。为解决这一问题，本文引入了一种新的任务——文本感知图像修复（TAIR），旨在同时恢复视觉内容和文本准确性。为此，我们构建了一个包含10万张高质量场景图像的大规模基准数据集SA-Text，这些图像密集标注了多样且复杂的文本实例。此外，我们还提出了一个名为TeReDiff的多任务扩散框架，将扩散模型的内部特征集成到文本检测模块中，使两者通过联合训练相互受益，从而提取丰富的文本表示，并将其作为后续去噪步骤的提示。大量实验表明，我们的方法在文本识别准确率上显著优于现有最先进的修复方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09993" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>高效激发语言模型推理能力的稀疏自编码调优方法</title>
<link>https://arxiv.org/abs/2506.09967</link>
<guid>https://arxiv.org/abs/2506.09967</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过稀疏自编码调优显著降低语言模型推理能力训练成本的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Resa的推理模型家族，该模型通过一种新颖且高效的稀疏自编码调优(SAE-Tuning)技术训练而成。该方法首先利用源模型训练一个稀疏自编码器以捕捉推理能力，然后借助此编码器引导标准监督微调过程，使目标模型获得类似推理能力，整个过程仅使用经过验证的问题-答案数据而无需任何推理痕迹。研究显示，当应用于某些基础模型后，在进一步强化学习(RL)微调之前，SAE-Tuning保留了超过97%的RL训练推理性能，同时将训练成本降低超过2000倍至约1美元，训练时间缩短超过450倍至约20分钟。此外，当应用于轻度RL训练模型时，它能在AIME24上实现43.33%的Pass@1分数，在AMC23上实现90%的Pass@1分数，只需增加约1美元的成本。令人惊讶的是，通过SAE提取的推理能力可能具有通用性和模块化特性，即从某一数据集提取的能力仍可提升更大重叠语料库的表现，并且可以从Qwen或Qwen-Math等模型中提取的能力在测试时附加到R1-Distill模型上，无需重新训练即可获得相似收益。广泛的消融实验验证了这些发现，所有成果均已完全开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09967" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:44:01 GMT</pubDate>
</item>
<item>
<title>UniPre3D：一种适用于任意尺度点云的统一预训练方法</title>
<link>https://arxiv.org/abs/2506.09952</link>
<guid>https://arxiv.org/abs/2506.09952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个可无缝应用于任意规模点云的统一预训练方法UniPre3D。</p><br /><br /><p><strong>摘要：</strong> 点云数据的尺度多样性对3D视觉统一表征学习技术的发展提出了重大挑战。目前缺乏统一的3D模型，且现有预训练方法对物体级和场景级点云的效果不均衡。本文介绍UniPre3D，这是一种针对任意尺度和架构的3D模型设计的统一预训练方法。UniPre3D通过预测高斯基元作为预训练任务，并利用可微分高斯散射渲染图像，实现精确的像素级监督和端到端优化。此外，该方法结合预训练图像模型的2D特征，引入成熟的纹理知识，进一步规范预训练任务并引导模型关注几何结构。实验验证表明，UniPre3D在多种物体级和场景级任务中表现出了普遍有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:23:21 GMT</pubDate>
</item>
<item>
<title>结合规则与大模型推理的指令跟随强化学习</title>
<link>https://arxiv.org/abs/2506.09942</link>
<guid>https://arxiv.org/abs/2506.09942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合代码验证与大语言模型验证的强化学习方法提升指令跟随性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了指令跟随领域中强化学习验证挑战，并提出了名为VerIF的新方法，该方法融合了基于规则的代码验证与基于大型推理模型（如QwQ-32B）的语言模型验证。为了支持这一方法，我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其对应的验证信号。通过在两个模型上应用带有VerIF的强化学习训练，我们在多个代表性指令跟随基准测试中取得了显著改进。训练后的模型在相同规模的模型中达到最先进的性能，并且对未见过的约束条件具有良好的泛化能力。此外，观察到这些模型的整体能力未受负面影响，表明VerIF可以整合到现有的强化学习框架中以提高整体模型性能。我们已公开数据集、代码和模型以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:10:36 GMT</pubDate>
</item>
<item>
<title>ReasonMed：大规模医学推理数据集推动LLMs在医疗问答中的性能提升</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ReasonMed数据集，显著提升LLMs在医学推理和问答中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管基于推理的大语言模型（LLMs）在数学和编程领域表现出色，但它们在知识密集型医学问答中的能力尚未得到充分探索。为此，我们引入了ReasonMed，这是目前最大的医学推理数据集，包含从170万个初始推理路径中提取的37万高质量示例。通过多智能体验证和优化过程，设计了一个错误修正器来改进推理路径，纠正由验证器标记的错误步骤。利用ReasonMed，我们系统地研究了训练医学推理模型的最佳实践，发现结合详细的因果推理和简洁的答案总结是最有效的微调策略。基于此策略，我们训练了ReasonMed-7B模型，在PubMedQA上超过了之前最好的模型，并且超过了更大的LLaMA3.1-70B模型。这一成果为医学领域的LLMs发展设立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 04:36:55 GMT</pubDate>
</item>
<item>
<title>Ming-Omni：一种支持多模态处理与生成的统一模型</title>
<link>https://arxiv.org/abs/2506.09344</link>
<guid>https://arxiv.org/abs/2506.09344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Omni是一种能够处理多种模态并生成高质量图像和语音的统一模型。</p><br /><br /><p><strong>摘要：</strong> Ming-Omni是一个创新的多模态统一模型，它不仅能够处理图像、文本、音频和视频，还具备强大的语音和图像生成能力。该模型通过专用编码器从不同模态中提取特征，并利用Ling这一混合专家架构结合模态特定路由进行高效处理与融合。与传统多模态模型相比，Ming-Omni的独特之处在于支持音频和图像的生成任务，这得益于先进的音频解码器和Ming-Lite-Uni图像生成模块的支持。此外，该模型还能实现上下文感知聊天、文本转语音转换以及多样化的图像编辑功能。实验结果表明，Ming-Omni在多模态感知与生成方面表现卓越。值得一提的是，这是首个我们所知开源且在模态支持上可媲美GPT-4o的模型，我们已公开所有代码和模型权重，旨在推动相关领域的进一步研究与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 22:50:49 GMT</pubDate>
</item>
<item>
<title>Token Perturbation Guidance提升扩散模型生成质量</title>
<link>https://arxiv.org/abs/2506.10036</link>
<guid>https://arxiv.org/abs/2506.10036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练且适用于条件与非条件生成的Token Perturbation Guidance方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对分类器自由引导（CFG）在现代扩散模型中的局限性，如特定训练需求及仅限条件生成，提出了一种名为Token Perturbation Guidance (TPG)的新方法。TPG通过直接对扩散网络内的中间token表示应用扰动矩阵，采用保范洗牌操作提供有效的稳定引导信号，从而提升生成质量而无需架构改动。作为一种无需训练且与输入条件无关的方法，TPG可应用于条件和非条件生成。实验表明，在SDXL和Stable Diffusion 2.1上，TPG在无条件生成中的FID得分较SDXL基线提升了近两倍，同时在提示对齐方面接近CFG的效果，确立了TPG作为通用且无条件依赖的引导方法的地位。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:25:46 GMT</pubDate>
</item>
<item>
<title>Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.09250</link>
<guid>https://arxiv.org/abs/2506.09250</guid>
<content:encoded><![CDATA[
Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N &gt; 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:16:53 GMT</pubDate>
</item>
<item>
<title>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</title>
<link>https://arxiv.org/abs/2506.08862</link>
<guid>https://arxiv.org/abs/2506.08862</guid>
<content:encoded><![CDATA[
Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 10:52:36 GMT</pubDate>
</item>
<item>
<title>Draft-based Approximate Inference for LLMs</title>
<link>https://arxiv.org/abs/2506.08373</link>
<guid>https://arxiv.org/abs/2506.08373</guid>
<content:encoded><![CDATA[
Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 22:37:46 GMT</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 17:04:14 GMT</pubDate>
</item>
<item>
<title>LLM Unlearning Should Be Form-Independent</title>
<link>https://arxiv.org/abs/2506.07795</link>
<guid>https://arxiv.org/abs/2506.07795</guid>
<content:encoded><![CDATA[
Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:21:25 GMT</pubDate>
</item>
<item>
<title>Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques</title>
<link>https://arxiv.org/abs/2506.08060</link>
<guid>https://arxiv.org/abs/2506.08060</guid>
<content:encoded><![CDATA[
Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:37:19 GMT</pubDate>
</item>
<item>
<title>LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer</title>
<link>https://arxiv.org/abs/2506.06952</link>
<guid>https://arxiv.org/abs/2506.06952</guid>
<content:encoded><![CDATA[
Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 20:15:32 GMT</pubDate>
</item>
<item>
<title>What Makes a Good Natural Language Prompt?</title>
<link>https://arxiv.org/abs/2506.06950</link>
<guid>https://arxiv.org/abs/2506.06950</guid>
<content:encoded><![CDATA[
As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 19:19:27 GMT</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 03:19:11 GMT</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 18:16:16 GMT</pubDate>
</item>
<item>
<title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title>
<link>https://arxiv.org/abs/2506.05982</link>
<guid>https://arxiv.org/abs/2506.05982</guid>
<content:encoded><![CDATA[
As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:02:01 GMT</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation中的知识冲突类型及其处理研究</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新型知识冲突分类法并构建首个高质量基准CONFLICTS，评估大型语言模型处理冲突信息的能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval Augmented Generation (RAG) 是一种常见的方法，用于通过相关且最新的信息增强大型语言模型 (LLMs)，但检索到的来源经常包含矛盾信息，模型如何解决这些矛盾仍不清楚。本研究首先提出了RAG中知识冲突类型的新型分类法，并定义了每种类型期望的模型行为。随后，引入了CONFLICTS，这是一个具有专家标注的高质量基准，在现实RAG环境中对冲突类型进行注释。CONFLICTS是首个能够追踪模型在处理多种知识冲突方面进展的基准。我们对该基准进行了广泛的实验，结果显示LLMs往往难以适当解决来源之间的冲突。虽然提示LLMs显式推理检索文档中的潜在冲突显著提高了响应质量和适当性，但仍存在很大的改进空间，未来研究仍有大量工作待完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:52:57 GMT</pubDate>
</item>
<item>
<title>Institutional Books 1.0：基于哈佛图书馆公共领域书籍的历史文本数据集发布</title>
<link>https://arxiv.org/abs/2506.08300</link>
<guid>https://arxiv.org/abs/2506.08300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍基于哈佛图书馆公共领域书籍的数据集Institutional Books 1.0。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，高质量训练数据的稀缺性成为关注焦点。本技术报告介绍了Institutional Books 1.0，这是一个由哈佛图书馆参与Google Books项目（自2006年开始）数字化的大量公共领域书籍组成的数据集。我们与哈佛图书馆合作，从原始扫描的1,075,899卷书中提取并处理出包含约2500亿标记的历史文本数据集，其中983,004卷书（约2420亿标记）的OCR提取文本及元数据已公开。该数据集涵盖超过250种语言，旨在提升历史文本对人类和机器的可访问性和可用性。报告详细描述了项目的背景、目标、方法以及分析结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 20:11:30 GMT</pubDate>
</item>
<item>
<title>Mirage：基于音频生成高质量视频的统一模型</title>
<link>https://arxiv.org/abs/2506.08279</link>
<guid>https://arxiv.org/abs/2506.08279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型Mirage，可从音频输入生成逼真的视频。</p><br /><br /><p><strong>摘要：</strong> 当前视频生成方法要么忽略声音专注于无声图像生成，要么关注视听元素但局限于特定应用领域。本文介绍的Mirage是一种音频到视频的基础模型，能从零开始根据音频输入生成真实且富有表现力的输出图像。当与现有的语音合成方法结合时，Mirage可以生成引人注目的多模态视频。其核心技术贡献是一种统一的自注意力机制音频到视频生成模型训练方法，无论是从头开始还是基于已有权重，都能保持模型的通用性并提升输出质量。该模型在处理人物演讲场景时尤其有效，能够生成可信的表演解读视频。我们鼓励读者亲自观看和聆听Mirage的成果（详见论文和链接）。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 18:56:02 GMT</pubDate>
</item>
<item>
<title>测试时间交互扩展提升智能体性能</title>
<link>https://arxiv.org/abs/2506.07976</link>
<guid>https://arxiv.org/abs/2506.07976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的测试时间扩展维度——测试时间交互，显著提升智能体性能。</p><br /><br /><p><strong>摘要：</strong> 当前测试时间扩展范式依赖于生成长推理路径以增加思考深度，但不支持智能体实时获取环境信息或动态调整行为。本研究引入测试时间交互这一新维度，使智能体能在单次运行中进行探索、回溯及动态重规划等复杂行为。通过在网页代理领域验证，即使没有训练，基于提示的交互扩展也能显著提高任务成功率。进一步提出TTI方法，采用基于课程的在线强化学习策略，根据智能体的表现动态调整其交互长度。实验表明，TTI在WebVoyager和WebArena基准上达到了开源开放数据集的最佳性能，并展示了智能体在探索与利用之间的自适应平衡能力。测试时间交互扩展为提升智能体适应性提供了全新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:02 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的可解释AI生成图像检测</title>
<link>https://arxiv.org/abs/2506.07045</link>
<guid>https://arxiv.org/abs/2506.07045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">优化后的多模态大语言模型能有效检测AI生成图像并提供合理解释。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着图像生成技术的飞速发展，对可解释且鲁棒的检测方法需求增加。尽管现有方法精度较高，但通常缺乏透明性，无法为人所理解。多模态大语言模型虽非专门设计用于伪造检测，但具备强大的分析推理能力。通过适当微调，这些模型能够有效识别AI生成图像并提供有意义的解释。然而，当前的多模态大语言模型仍存在幻觉问题，其视觉解释难以与实际图像内容及人类推理保持一致。为此，我们构建了一个包含标注边界框和描述性字幕的数据集，用于突出合成伪影，奠定人类对齐的视觉文本推理基础。随后，我们采用多阶段优化策略，逐步平衡准确检测、视觉定位和连贯文本解释的目标。最终模型在检测AI生成图像和定位视觉缺陷方面表现出色，显著优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:44 GMT</pubDate>
</item>
<item>
<title>MMRefine：多模态大语言模型误差精化能力评估基准</title>
<link>https://arxiv.org/abs/2506.04688</link>
<guid>https://arxiv.org/abs/2506.04688</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMRefine基准，评估多模态大语言模型的误差检测与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMRefine的多模态精化基准，用于评估多模态大型语言模型（MLLMs）的误差精化能力。与以往仅关注最终准确率不同，MMRefine通过六个特定场景评估MLLMs的错误检测和纠正能力，并将错误分类为六种类型进行性能分析。实验表明，不同开放及闭源MLLMs存在改进瓶颈，揭示了提升推理效果的关键领域。该基准代码和数据集已公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04688" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 03:11:36 GMT</pubDate>
</item>
<item>
<title>基于查询聚焦的定量关键点摘要模型QQSUM-RAG</title>
<link>https://arxiv.org/abs/2506.04020</link>
<guid>https://arxiv.org/abs/2506.04020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QQSUM任务及模型，改进产品问答系统对客户意见多样性的捕捉能力。</p><br /><br /><p><strong>摘要：</strong> 现有基于评论的产品问答(PQA)系统因仅提供单一视角而难以反映客户意见的多样性。本文引入Quantitative Query-Focused Summarization(QQSUM)任务，旨在通过量化意见的流行度来生成具有代表性的关键点摘要，从而有效回答用户查询。尽管检索增强生成(RAG)方法在PQA中有一定潜力，但其生成的答案仍无法充分涵盖多样的观点。为解决这一问题，我们提出了QQSUM-RAG模型，该模型通过少量样本学习同时训练关键点导向的检索器和关键点摘要生成器，从而实现能够捕捉多样化且代表性观点的关键点摘要。实验结果表明，QQSUM-RAG在文本质量和意见量化准确性方面均优于最先进的RAG基线模型。相关源代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:50:32 GMT</pubDate>
</item>
<item>
<title>PlayerOne：首个第一人称现实世界模拟器</title>
<link>https://arxiv.org/abs/2506.09995</link>
<guid>https://arxiv.org/abs/2506.09995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlayerOne可精准构建虚拟世界并生成与真实场景匹配的第一人称视频。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为PlayerOne的创新性第一人称现实世界模拟器，它能够在动态环境中实现沉浸式且无限制的探索。该系统通过输入用户的第一人称场景图像，能够精确构建对应的虚拟世界，并生成与用户实际动作严格对齐的第一人称视频。PlayerOne采用粗到细的训练管道，在大规模第一人称文本-视频对上进行预训练，随后利用同步运动-视频数据进行微调，这些数据来自由自动构建管道提取的内外视角视频数据集。此外，为了更好地控制不同部位的动作，设计了一种部分解耦的动作注入方案，并开发了一种联合重建框架，逐步建模4D场景及视频帧，保证长时间视频生成的一致性。实验结果显示，PlayerOne在控制多样化场景中的各种人体动作方面表现出强大的泛化能力，标志着第一人称现实世界模拟领域的首次尝试，为世界建模及其广泛应用开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>基于多模态条件的端到端人体动画生成框架</title>
<link>https://arxiv.org/abs/2506.09984</link>
<guid>https://arxiv.org/abs/2506.09984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架实现多概念人体为中心的高质量可控视频生成。</p><br /><br /><p><strong>摘要：</strong> 近年来，结合文本、图像和音频等多种模态条件的人体动画取得了显著进展。然而，大多数现有方法仅限于单一主体动画，并以全局方式注入条件，忽视了多概念在同一视频中的丰富交互场景。这种局限性阻碍了对人类及物体的精确个性化控制。本研究摒弃单实体假设，提出了一种新颖框架，通过区域特定绑定模态条件至各身份的空间时间足迹，实现了多概念的精确控制。实验结果和消融研究表明，该方法在多模态条件下的显式布局控制优于隐式方法及其他现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>SAFE：面向视觉语言动作模型的多任务故障检测器</title>
<link>https://arxiv.org/abs/2506.09937</link>
<guid>https://arxiv.org/abs/2506.09937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种适用于多种任务的故障检测器SAFE，用于提升机器人在新任务中的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言动作模型（VLAs）在新任务上的有限成功率问题，引入多任务故障检测问题，并设计了一种名为SAFE的故障检测器。SAFE利用VLAs内部特征，通过学习任务成功与失败的高阶知识，在模拟和真实环境中对多种策略架构进行测试，展现出卓越的故障检测性能及准确性和检测时间的最佳权衡。SAFE在多个任务上优于现有基线方法，为机器人在未知环境中的安全交互提供了保障。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:59:13 GMT</pubDate>
</item>
<item>
<title>ComfyUI-R1：首个用于自动化工作流生成的大规模推理模型</title>
<link>https://arxiv.org/abs/2506.09790</link>
<guid>https://arxiv.org/abs/2506.09790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模推理模型ComfyUI-R1，显著提升AI艺术创作的工作流生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ComfyUI-R1的新模型，该模型是首个专门用于自动化工作流生成的大规模推理模型。通过利用精心策划的数据集中的4000个工作流，我们构建了包含节点选择、工作流规划及代码级工作流表示在内的长链推理数据。ComfyUI-R1采用了两阶段框架进行训练：首先通过冷启动的链式思维微调来适应ComfyUI领域；然后利用细粒度规则-指标混合奖励机制下的强化学习，以激励推理能力并确保格式的有效性、结构的完整性以及节点级别的准确性。实验结果显示，我们的7B参数模型达到了97%的格式有效性率，并且在节点级别和图级别F1得分上表现出色，明显优于使用GPT-4o和Claude系列等领先闭源模型的方法。进一步分析表明推理过程的重要性以及将工作流转化为代码的优势。定性比较显示，我们在合成复杂且多样化的节点组合工作流方面具有优势，这突显了长链推理在AI艺术创作中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 10:35:15 GMT</pubDate>
</item>
<item>
<title>Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation</title>
<link>https://arxiv.org/abs/2506.09350</link>
<guid>https://arxiv.org/abs/2506.09350</guid>
<content:encoded><![CDATA[
Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:04:23 GMT</pubDate>
</item>
<item>
<title>Seedance 1.0：高效高质量视频生成基础模型</title>
<link>https://arxiv.org/abs/2506.09113</link>
<guid>https://arxiv.org/abs/2506.09113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedance 1.0通过多源数据精炼等技术实现了高效高质量视频生成。</p><br /><br /><p><strong>摘要：</strong> Seedance 1.0是一种高性能且推理高效的视频生成基础模型，针对当前扩散模型在视频生成中的挑战进行了多项技术改进。首先，它通过多源数据精炼结合精确的视频描述，实现跨多样化场景的全面学习；其次，采用高效的架构设计和训练范式，支持多镜头生成并联合处理文本到视频和图像到视频任务；第三，利用细粒度监督微调和多维奖励机制优化后训练，显著提升性能；最后，通过多阶段蒸馏策略和系统级优化，实现约10倍的推理加速。实验结果显示，Seedance 1.0仅需41.4秒即可生成1080p分辨率的5秒视频（基于NVIDIA-L20）。相较于现有最先进模型，该模型在时空流畅性、结构稳定性、复杂多主体场景下的指令遵循能力以及多镜头叙事连贯性方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:56:11 GMT</pubDate>
</item>
<item>
<title>Branched Schrödinger Bridge Matching：多模态分布转换的新框架</title>
<link>https://arxiv.org/abs/2506.09007</link>
<guid>https://arxiv.org/abs/2506.09007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法BranchSBM，用于多路径分布转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Branched Schrödinger Bridge Matching（BranchSBM）的新框架，该框架通过参数化多个时间相关的速度场和增长过程，解决了现有方法无法处理多模态分布转换的问题。BranchSBM不仅提高了表达能力，还在多路径表面导航、细胞命运分支建模以及细胞对扰动的发散响应模拟等任务中展现出其必要性。传统方法如流匹配和Schrödinger桥接匹配仅能处理单模态转换，而BranchSBM可以捕捉从共同起点到多个不同终点的分支或发散演化过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:29:48 GMT</pubDate>
</item>
<item>
<title>基于测试驱动开发的数据合成框架SWE-Flow</title>
<link>https://arxiv.org/abs/2506.09003</link>
<guid>https://arxiv.org/abs/2506.09003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于测试驱动开发的新数据合成框架SWE-Flow。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SWE-Flow的新型数据合成框架，该框架以测试驱动开发（TDD）为基础。与依赖人工提交问题的传统软件工程数据不同，SWE-Flow通过自动推断单元测试中的增量开发步骤来生成数据，这些测试本身就包含了高层次的需求。SWE-Flow的核心是构建运行时依赖图（RDG），它精确捕捉函数间的交互，从而生成结构化的逐步开发计划。每一步骤都会生成部分代码库、对应的单元测试及必要的代码修改，形成可验证的TDD任务。通过此方法，我们从GitHub的真实项目中生成了16,061个训练实例和2,020个测试实例，创建了SWE-Flow-Eval基准。实验表明，在该数据集上微调开源模型显著提升了基于TDD的编码性能。为了促进进一步研究，所有代码、数据集、模型和Docker镜像均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09003" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:23:33 GMT</pubDate>
</item>
<item>
<title>SeerAttention-R：面向推理模型长解码的稀疏注意力框架</title>
<link>https://arxiv.org/abs/2506.08889</link>
<guid>https://arxiv.org/abs/2506.08889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeerAttention-R通过自蒸馏门机制实现高效稀疏注意力，适用于长序列推理任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeerAttention-R的稀疏注意力框架，专为推理模型的长序列解码设计。该框架继承了SeerAttention的设计，通过自蒸馏门机制学习注意力稀疏性，并去除了查询池化以支持自回归解码。它轻量且易于集成到现有的预训练模型中，无需修改原始参数。实验表明，在AIME基准测试中，SeerAttention-R仅需0.4Btokens即可在大稀疏注意力块大小下保持接近无损的推理精度。此外，利用TileLang开发的优化稀疏解码内核，在H100 GPU上实现了高达9倍的速度提升。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:17:26 GMT</pubDate>
</item>
<item>
<title>POET：一种基于正交等价变换的大规模语言模型训练算法</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法POET，通过优化神经元显著提升大规模语言模型的训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对有效且可靠地训练大型语言模型这一人工智能领域的重大挑战，提出了一种名为POET的新算法。该算法采用正交等价变换对神经元进行重新参数化，具体而言，每个神经元通过两个可学习的正交矩阵和一个固定的随机权重矩阵表示。由于其在谱性质上的保真性，POET能够在稳定优化目标函数的同时提高泛化能力。此外，还开发了高效的近似方法，使POET适用于大规模神经网络的灵活且可扩展的训练。大量实验验证了POET在训练大型语言模型方面的有效性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于自我信心的大语言模型后训练强化学习方法</title>
<link>https://arxiv.org/abs/2506.06395</link>
<guid>https://arxiv.org/abs/2506.06395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需标注的强化学习方法RLSC，显著提升数学推理任务性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理方面表现出色，但后训练过程对调整其行为以符合任务目标至关重要。现有的强化学习（RL）方法通常依赖昂贵的人类注释或外部奖励模型。我们提出了通过自我信心进行强化学习（RLSC），利用模型自身的置信度作为奖励信号，从而避免了标签、偏好模型或奖励工程的需求。将RLSC应用于Qwen2.5-Math-7B模型，在每个问题仅使用16个样本且训练步骤为10或20的情况下，该方法在AIME2024上提升了13.4%，在MATH500上提升了21.2%，在Minerva Math上提升了21.7%，在Olympiadbench上提升了20.8%，在AMC23上提升了9.7%的准确性。RLSC提供了一种简单且可扩展的后训练方法，仅需少量样本和无标注监督即可优化推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:55:15 GMT</pubDate>
</item>
<item>
<title>引入Autoregressive Semantic Visual Reconstruction提升多模态理解</title>
<link>https://arxiv.org/abs/2506.09040</link>
<guid>https://arxiv.org/abs/2506.09040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ASVR模型，通过联合学习视觉与文本模态提高多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型视觉语言模型（LVLMs）主要依赖文本序列的自回归监督，未能充分整合视觉模态，导致无法有效利用无描述图片、遗漏关键视觉细节及难以通过文本传达特定视觉内容的问题。本研究引入Autoregressive Semantic Visual Reconstruction（ASVR），在一个统一的自回归框架下实现视觉与文本模态的联合学习。实验表明，直接自回归重建图像原始外观可能削弱多模态理解，而基于语义表示的重建显著提升性能。ASVR在多种数据规模和大语言模型骨干上均有显著改进，在14个多模态基准测试中使LLaVA-1.5平均得分提升了5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:50 GMT</pubDate>
</item>
<item>
<title>基于强化学习的小型规则推理模型的高效方法</title>
<link>https://arxiv.org/abs/2506.08672</link>
<guid>https://arxiv.org/abs/2506.08672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型强化规则推理方法，显著提升小模型在多任务中的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于规则推理的挑战，特别是针对实际应用中规则格式、类型和复杂性的变化带来的困难。虽然大型推理模型在强化学习辅助下表现出色，但小型推理模型是否能有效学习并具有跨任务和领域的稳健泛化能力尚不清楚。为解决此问题，我们提出了RuleReasoner，这是一种通过精心策划的任务集和新颖的领域感知动态采样方法实现规则推理的方法。实验表明，该方法在分布内和分布外基准测试中均优于现有前沿方法，同时具有更高的计算效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 06:31:21 GMT</pubDate>
</item>
<item>
<title>Mathesis：结合强化学习的端到端数学定理证明系统</title>
<link>https://arxiv.org/abs/2506.07047</link>
<guid>https://arxiv.org/abs/2506.07047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mathesis首次实现从自然语言问题到形式化证明的全流程自动化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Mathesis的新方法，它是一个端到端的数学定理证明流水线，能够处理非正式的问题陈述并将其自动形式化。Mathesis的关键组成部分包括Mathesis-Autoformalizer，这是一个使用强化学习增强自然语言问题形式化的工具，并通过LeanScorer框架评估形式化质量。此外，还提出了一种Mathesis-Prover，用于从形式化陈述生成形式化证明。为了评估该方法的实际应用价值，我们创建了一个名为Gaokao-Formal的新基准测试集，其中包含来自中国高考的488个复杂问题。实验结果显示，Mathesis-Autoformalizer在Gaokao-Formal上的通过率比最佳基线高出22%，整个系统在MiniF2F上达到64%的准确率，在Gaokao-Formal上实现了18%的最新性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 05:04:14 GMT</pubDate>
</item>
<item>
<title>DiscoVLA：针对视频文本检索的CLIP参数高效适配方法</title>
<link>https://arxiv.org/abs/2506.08887</link>
<guid>https://arxiv.org/abs/2506.08887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiscoVLA方法，同时减少图像到视频的视觉、语言及对齐三方面差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了CLIP模型在视频文本检索中的参数高效适应问题，发现从图像级到视频级迁移时存在视觉、语言和对齐三大关键差异。现有方法主要关注视觉差异，忽视了语言和对齐方面的改进。为此，我们提出了DiscoVLA（Discrepancy Reduction in Vision, Language, and Alignment），通过引入图像-视频特征融合来解决视觉和语言差异，并利用伪图像描述生成学习细粒度的图像级对齐，同时采用图像到视频对齐蒸馏提升视频级对齐效果。实验表明，DiscoVLA在MSRVTT数据集上的R@1指标比现有方法高出1.5%，达到50.5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:16:40 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的新训练范式：Self Forcing</title>
<link>https://arxiv.org/abs/2506.08009</link>
<guid>https://arxiv.org/abs/2506.08009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Self Forcing方法解决视频生成中的曝光偏差问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self Forcing的新型训练范式，专门针对自回归视频扩散模型设计，旨在解决长期存在的曝光偏差问题。传统方法在推理过程中依赖于真实的上下文帧，而Self Forcing通过在训练时利用之前自动生成的输出进行条件生成，显著提高了生成序列的整体质量。该方法采用整体视频级别的损失函数，而非传统的逐帧目标函数，同时结合了几步扩散模型和随机梯度截断策略，有效平衡了计算成本与性能。此外，引入滚动键值缓存机制进一步提升了效率，使得该方法能够在单GPU上实现亚秒级延迟的实时流媒体视频生成，且生成质量可媲美甚至超越更为复杂和非因果的扩散模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Aligning Text, Images, and 3D Structure Token-by-Token</title>
<link>https://arxiv.org/abs/2506.08002</link>
<guid>https://arxiv.org/abs/2506.08002</guid>
<content:encoded><![CDATA[
Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:37 GMT</pubDate>
</item>
<item>
<title>Squeeze3D：基于隐式先验知识的高效3D数据压缩框架</title>
<link>https://arxiv.org/abs/2506.07932</link>
<guid>https://arxiv.org/abs/2506.07932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用已有3D生成模型的隐式先验知识进行高比率压缩的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Squeeze3D的创新框架，该框架通过可训练映射网络连接已有的编码器和生成模型的潜在空间，利用这些模型学习到的隐式先验知识实现对网格、点云及辐射场等3D数据的极高压缩比。实验表明，Squeeze3D在保持视觉质量的同时，对纹理网格的压缩比可达2187倍，对点云为55倍，对辐射场为619倍。此外，由于不涉及对象特定网络的训练，其压缩和解压延迟极小。Squeeze3D适用于多种现有预训练3D编码器和生成模型，并支持不同格式的3D数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:52:10 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不等式证明中的挑战与研究进展</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法提升不等式证明能力，发现顶级模型在严谨推理上仍有显著差距。</p><br /><br /><p><strong>摘要：</strong> 不等式证明作为科学与数学领域的重要工具，考验着高级推理能力。然而，现有数据集的局限性阻碍了大型语言模型（LLMs）在此领域的进步。为此，研究者引入了一种新的任务形式，将不等式证明分解为可自动验证的子任务——边界估计与关系预测，并构建了一个名为IneqMath的专业级不等式数据集。该数据集包含丰富的解题步骤与定理注释，旨在推动模型性能提升。此外，研究设计了一种结合最终答案判断与逐步判断的评估框架，用于检测推理缺陷。对29个领先LLMs的测试显示，尽管顶级模型如o1在最终答案准确性上有一定表现，但在逐步推理验证下整体准确率不足10%，较仅考虑最终答案时下降了65.5%。这一发现揭示了当前LLMs在严谨证明构建上的脆弱性和局限性。研究进一步表明，单纯增加模型规模或计算资源并不能显著提高证明正确性，而是需要探索如定理引导推理及自我优化等新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07927" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:43:38 GMT</pubDate>
</item>
<item>
<title>Frame Guidance：无需训练的可控视频生成引导方法</title>
<link>https://arxiv.org/abs/2506.07177</link>
<guid>https://arxiv.org/abs/2506.07177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于帧级信号的无需训练的可控视频生成引导方法Frame Guidance。</p><br /><br /><p><strong>摘要：</strong> 扩散模型的进步显著提升了视频质量，但许多现有方法依赖于针对特定任务微调大规模视频模型，随着模型规模增长变得不切实际。本文介绍Frame Guidance，这是一种无需训练即可实现可控视频生成的引导方法，基于帧级信号如关键帧、风格参考图像、草图或深度图等。为实现实用的无需训练引导，我们提出了一种简单的潜在处理方法以大幅减少内存使用，并采用一种新颖的潜在优化策略以实现全局一致的视频生成。Frame Guidance能够在多种任务上有效控制，包括关键帧引导、风格化和循环生成，且无需任何训练，兼容所有视频模型。实验结果显示，该方法可以为广泛的任务和输入信号生成高质量的可控视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:54:41 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的地缘政治偏见研究</title>
<link>https://arxiv.org/abs/2506.06751</link>
<guid>https://arxiv.org/abs/2506.06751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大型语言模型存在显著的地缘政治偏见，且简单去偏方法效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在解释具有国家视角冲突的历史事件时所表现出的地缘政治偏见（涉及美国、英国、苏联和中国）。我们构建了一个包含中立事件描述及多国对立观点的新数据集，实验结果显示这些模型倾向于支持特定的国家叙事。尽管尝试了简单的去偏提示，但其对减少偏见的效果十分有限。此外，通过操控参与者标签的实验表明，模型对归因高度敏感，有时会放大偏见或检测到不一致性，尤其是在标签被替换的情况下。本研究揭示了LLMs中的国家叙事偏见，挑战了简单去偏方法的有效性，并为未来地缘政治偏见研究提供了框架和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 06:45:17 GMT</pubDate>
</item>
<item>
<title>异构Mixture-of-Adapters方法提升大语言模型参数高效微调性能</title>
<link>https://arxiv.org/abs/2506.05928</link>
<guid>https://arxiv.org/abs/2506.05928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出异构MoA方法解决现有MoE-LoRA的表征塌陷和负载不均问题。</p><br /><br /><p><strong>摘要：</strong> 最近的研究将低秩适配（LoRA）与专家混合（MoE）结合，进一步提升大语言模型（LLM）参数高效微调（PEFT）方法的性能。然而，现有的同构MoE-LoRA架构常面临表征塌陷和专家负载失衡的问题。为应对这些挑战，本文提出了异构Mixture-of-Adapter（MoA）方法，通过动态整合结构多样的PEFT适配器专家，利用互补的表示能力促进专家专业化，从而增强预训练知识向下游任务的有效迁移。MoA支持两种变体：Soft MoA实现细粒度集成，Sparse MoA则基于贡献稀疏激活适配器专家。实验表明，异构MoA在性能和参数效率上均优于同构MoE-LoRA方法。该项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:54:19 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的知识增强金融推理模型RKEFino1</title>
<link>https://arxiv.org/abs/2506.05700</link>
<guid>https://arxiv.org/abs/2506.05700</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合监管知识的金融推理模型RKEFino1，提升数字合规报告的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型(LLMs)在数字监管报告(DRR)中的应用挑战，提出了RKEFino1模型，该模型通过微调Fino1，在XBRL、CDM和MOF等领域的专业知识基础上进行增强。研究设计了基于知识的问答(QA)任务、数学推理QA任务以及覆盖句子和表格中财务实体的数值命名实体识别(NER)任务。实验结果表明，RKEFino1在合规相关的金融任务中表现出色且具有良好的泛化能力。目前，该模型已发布在Hugging Face平台上供公众使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05700" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 23:02:52 GMT</pubDate>
</item>
<item>
<title>基于证据性引导的检索增强生成模型ECoRAG</title>
<link>https://arxiv.org/abs/2506.05167</link>
<guid>https://arxiv.org/abs/2506.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ECoRAG，提升LLMs在开放域问答中的性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在开放域问答（ODQA）中通过检索增强生成（RAG）利用外部文档表现优异，但现有压缩方法未能有效过滤非证据性信息，限制了性能。本文提出ECoRAG框架，通过基于证据性的文档压缩提升LLMs性能，确保答案生成由正确证据支持，并在必要时补充更多内容。实验表明，ECoRAG在多个ODQA任务中超越现有压缩方法，同时显著降低延迟并减少令牌使用量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 11:43:49 GMT</pubDate>
</item>
<item>
<title>基于预操作批评机制的多模态大语言模型在GUI自动化中的应用</title>
<link>https://arxiv.org/abs/2506.04614</link>
<guid>https://arxiv.org/abs/2506.04614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的预操作批评机制显著提升了GUI自动化任务的成功率和效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）被广泛应用于图形用户界面（GUI）自动化等多模态推理任务。不同于一般的离线多模态任务，GUI自动化需要在线交互环境中逐步决策，对每一步决策的准确性要求较高。为了提高决策可靠性，我们引入了一种预操作批评机制，通过预测行动的潜在结果和正确性提供反馈。具体而言，提出了建议感知梯度相对策略优化（S-GRPO）策略构建预操作批评模型GUI-Critic-R1，并引入新颖的建议奖励以增强反馈的可靠性。此外，开发了一种基于推理引导的数据收集管道，创建GUI-Critic-Train和GUI-Critic-Test，填补了现有GUI批评数据的空白。静态实验表明，GUI-Critic-R1在批评准确性上优于当前的MLLMs；动态评估进一步证明了该模型在成功率和操作效率方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:12:36 GMT</pubDate>
</item>
<item>
<title>基于正交匹配追踪的预训练语言模型跨分词器移植方法</title>
<link>https://arxiv.org/abs/2506.06607</link>
<guid>https://arxiv.org/abs/2506.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需微调的正交匹配追踪方法，实现大型语言模型的跨分词器迁移。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种无需微调的预训练大型语言模型（LLMs）的分词器移植方法，通过正交匹配追踪（OMP）重建未见令牌嵌入。该方法将词汇外令牌近似为共享令牌的稀疏线性组合，在两个具有挑战性的跨分词器任务中展示了最佳的零样本性能保存能力。与其他零样本方法相比，OMP在多个基准测试中表现出色，且有效解决了大分词器差异问题。此外，该技术支持预训练权重的直接重用，适用于跨分词器的知识蒸馏、推测解码、集成、合并及领域特定词汇适应。我们还将此方法集成到开源工具mergekit-tokensurgeon中，用于后处理词汇对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 20:51:27 GMT</pubDate>
</item>
<item>
<title>NetPress: Dynamically Generated LLM Benchmarks for Network Applications</title>
<link>https://arxiv.org/abs/2506.03231</link>
<guid>https://arxiv.org/abs/2506.03231</guid>
<content:encoded><![CDATA[
Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:04:22 GMT</pubDate>
</item>
<item>
<title>无需权重更新的动态视图合成</title>
<link>https://arxiv.org/abs/2506.08004</link>
<guid>https://arxiv.org/abs/2506.08004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重新设计预训练视频扩散模型的噪声初始化阶段实现高保真动态视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文将动态视图合成问题视为无监督训练中的逆问题，提出了一种不需权重更新或辅助模块的方法。我们首先指出由零终端信噪比（SNR）调度引起的确定性反转基本障碍，并通过引入新的噪声表示方法——K阶递归噪声表示（K-order Recursive Noise Representation）解决该问题。此表示法具有闭合形式表达，可精确高效地对齐VAE编码与DDIM反演潜在变量。此外，针对相机运动导致的新可见区域，我们提出随机潜在调制（Stochastic Latent Modulation），在潜在空间上进行基于可见性的采样以完成被遮挡区域。实验表明，通过结构化潜在变量操作可以在噪声初始化阶段有效执行动态视图合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>tau^2-bench：引入双控环境评估对话AI代理</title>
<link>https://arxiv.org/abs/2506.07982</link>
<guid>https://arxiv.org/abs/2506.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准tau^2-bench通过双控环境模拟真实场景，评估对话AI代理的推理与用户引导能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对话AI代理评估基准多局限于单一控制环境，即仅AI代理可操作工具与外界交互，而用户作为被动的信息提供者。这种设置与现实世界中的应用场景（如技术支持）存在差异，在这些场景中用户需主动参与改变共享世界的状态。为弥合这一差距，本文提出了tau^2-bench，该基准具有四个关键贡献：首先，构建了一个新颖的电信双控领域，建模为Dec-POMDP，允许AI代理和用户共同利用工具在共享动态环境中行动，测试代理的协调与沟通能力；其次，开发了一种组合任务生成器，从原子组件编程生成多样化且可验证的任务，确保领域覆盖度和可控复杂性；第三，设计了一个紧密耦合环境的可靠用户模拟器，其行为受制于工具和可观测状态，提高仿真保真度；最后，通过多种消融分析细致评估代理性能，包括区分推理错误与沟通/协调错误。实验表明，当代理从无用户环境切换到双控环境时，性能显著下降，凸显出引导用户面临的挑战。总体而言，tau^2-bench为需要有效推理并引导用户行为的代理提供了受控测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:52:18 GMT</pubDate>
</item>
<item>
<title>CyberV：基于控制论的视频多模态大语言模型自适应框架</title>
<link>https://arxiv.org/abs/2506.07971</link>
<guid>https://arxiv.org/abs/2506.07971</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于控制论的视频多模态大语言模型自适应框架，显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大语言模型在处理长视频或复杂视频时面临计算需求高、鲁棒性差和准确性有限的问题，尤其是参数较少的模型。为解决这些问题，我们提出了名为CyberV的创新框架，该框架受到控制论原理的启发，将视频多模态大语言模型重新设计为具备自监测、自修正和动态资源分配能力的自适应系统。具体而言，CyberV引入了一个由多模态大语言模型推理系统、传感器和控制器组成的控制环路。传感器监控推理过程并收集中间解释，控制器则决定何时触发自修正并生成反馈以指导下一轮推理。此框架无需重新训练或添加额外组件即可增强冻结的多模态大语言模型。实验结果显示，在VideoMMMU基准上，CyberV使Qwen2.5-VL-7B提升了8.3%，InternVL3-8B提升了5.5%，甚至超过了竞争性的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，性能提升达10.0%，达到接近人类专家的表现。此外，该方法在VideoMME和WorldSense等通用基准测试中也显示出一致的改进，证明了其在提高动态视频理解方面模型的鲁棒性和准确性方面的有效性及泛化能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07971" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:45:18 GMT</pubDate>
</item>
<item>
<title>SAFEFLOW：构建可信大型语言模型及视觉语言模型驱动代理的新框架</title>
<link>https://arxiv.org/abs/2506.07564</link>
<guid>https://arxiv.org/abs/2506.07564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAFEFLOW框架，强化多模态代理的信息流控制与安全性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）和视觉语言模型（VLMs）的进步推动了复杂推理与工具使用的自主代理的发展。然而，现有代理框架仍存在脆弱性问题，缺乏安全的信息流动机制、可靠性以及多代理协作能力。本文引入SAFEFLOW，一种协议级框架，用于构建可信的LLM/VLM驱动代理。该框架通过细粒度的信息流控制（IFC），精确追踪代理间交换数据的来源、完整性和保密性。此外，它通过事务执行、冲突解决和共享状态的安全调度，在并发多代理环境中确保全局一致性。为了增强鲁棒性，SAFEFLOW还引入了写前日志记录、回滚和安全缓存等机制。通过构建SAFEFLOWBENCH基准套件验证其性能，实验表明，即使在敌对环境下，基于SAFEFLOW的代理也能保持优异的任务表现和安全保障，显著优于当前最先进的方法。这一成果奠定了构建原则性、鲁棒且安全的代理生态系统的基石，推动可靠自治领域的前沿发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 05:04:37 GMT</pubDate>
</item>
<item>
<title>基于自适应提升循环的机器人视觉规划模型在线学习方法</title>
<link>https://arxiv.org/abs/2506.06658</link>
<guid>https://arxiv.org/abs/2506.06658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过自我适应迭代提升视频模型性能的方法解决未知任务。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于演示训练的视频生成模型在解决机器人任务时泛化能力不足的问题，提出了Self-Adapting Improvement Loop (SAIL) 方法。该方法利用互联网规模预训练的视频模型进行领域内适应，收集自我产生的轨迹数据，并迭代更新领域内视频模型，从而持续提升特定任务的性能。研究将SAIL应用于MetaWorld任务集及真实机械臂的两个操作任务中，发现即使初始领域内演示质量较低或未经过筛选的数据，模型仍能在多个迭代后显著改善对新任务的表现。这表明通过互联网规模数据的适应性学习与在线经验累积，可以逐步构建高性能的视频模型以应对未知的机器人任务挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 00:34:37 GMT</pubDate>
</item>
<item>
<title>SynthesizeMe：基于用户交互生成合成人格以实现个性化奖励建模</title>
<link>https://arxiv.org/abs/2506.05598</link>
<guid>https://arxiv.org/abs/2506.05598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过用户交互生成合成人格以提升个性化奖励模型准确性的方法。</p><br /><br /><p><strong>摘要：</strong> 近期对大型语言模型多样对齐的需求推动了适应不同用户偏好的研究，但现有个性化奖励模型大多依赖额外的身份信息。本文介绍SynthesizeMe方法，通过用户交互生成合成人格，用于个性化奖励建模。该方法首先生成并验证解释用户偏好的推理，然后从这些推理中诱导出合成人格，并筛选出有意义的用户交互以构建个性化提示。实验表明，使用SynthesizeMe生成的提示可将Chatbot Arena上的个性化LLM作为评委的准确性提高4.4%，并与奖励模型结合在PersonalRewardBench上达到最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 17:23:16 GMT</pubDate>
</item>
<item>
<title>大型语言模型在策略规划中的自适应进化研究</title>
<link>https://arxiv.org/abs/2506.04651</link>
<guid>https://arxiv.org/abs/2506.04651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过多角色协作，LLM可以自主改进策略规划能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在面对需要长期战略规划的任务时的表现，并提出了一种多角色协作的自适应架构。该架构由分析者、研究者、编码者和玩家四个角色组成，共同迭代优化游戏策略。实验基于《卡坦岛》桌游展开，利用开源框架Catanatron评估了从基础游戏代理到能够自我重写代码的复杂系统。结果显示，相较于人工设计的静态代理，由LLMs驱动的自演化代理在Claude 3.7和GPT-4o等模型的支持下，展现出更强的适应性和策略调整能力，证明了LLMs在持续学习和策略优化方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 01:45:24 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在隐含推理场景中的表现分析</title>
<link>https://arxiv.org/abs/2506.00258</link>
<guid>https://arxiv.org/abs/2506.00258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示当前多模态大语言模型在隐含推理任务中常忽视潜在问题，但简单干预可显著提升表现。</p><br /><br /><p><strong>摘要：</strong> 多模态大语言模型（MLLMs）在开放性真实环境中应用时面临输入混乱、定义不清等问题。不同于精心设计的基准测试，这些场景中指令可能涉及缺失对象、矛盾事实或请求不可行操作。研究通过构建包含四种实际失败模式的诊断工具包，评估六种模型如o3和GPT-4o，发现尽管模型具备必要的感知和推理能力，却常常未能揭示隐藏问题。进一步研究表明，显式提示表明这些能力存在但常被用户合规性压制。实验表明，简单的推理阶段干预，例如谨慎的角色提示及提出澄清问题，能显著提高模型性能。研究揭示了当前MLLMs在推理能力和行为合规性之间的持续差距，并提出了使模型在非约束环境下更加可信的实用策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 17:47:28 GMT</pubDate>
</item>
<item>
<title>基于条件数分析的模型免疫框架及其应用</title>
<link>https://arxiv.org/abs/2505.23760</link>
<guid>https://arxiv.org/abs/2505.23760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过Hessian矩阵条件数分析模型免疫性，并设计算法实现对线性和非线性模型的有效免疫。</p><br /><br /><p><strong>摘要：</strong> 本文旨在探讨如何在预训练阶段构建对有害任务难以微调但仍保留其他任务效用的模型，即模型免疫技术。尽管先前研究表明文本到图像模型可以通过免疫处理，但免疫发生的条件及免疫模型的精确定义尚不明确。为此，我们提出了一个基于Hessian矩阵条件数的框架，用于分析线性模型的免疫性。在此基础上，设计了一种带有正则化项的算法，以控制预训练后模型的条件数。实验结果表明，该算法不仅适用于线性模型，也能有效应用于非线性深度网络的免疫处理。这项研究为模型免疫提供了理论基础和实践方法，同时代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>无需重训的视觉Transformer测试时异常值抑制方法</title>
<link>https://arxiv.org/abs/2506.08010</link>
<guid>https://arxiv.org/abs/2506.08010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重训的视觉Transformer测试时异常值处理方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉Transformer中高模令牌出现的机制及其导致的嘈杂注意力图问题。通过观察发现，某些模型中的稀疏神经元会集中高模激活到异常令牌上，从而破坏下游视觉处理。现有解决方案需要重新训练模型，而我们基于发现提出了一种无需重训的方法，通过将高模激活转移到额外的未训练令牌上来模拟注册令牌的效果。实验表明，该方法生成更清洁的注意力和特征图，在多个下游视觉任务中提升性能，并达到与显式训练注册令牌模型相当的结果。此外，还将此方法扩展到现成的视觉语言模型中，提高了其可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>通过视觉游戏学习提升多模态大型语言模型的泛化推理能力</title>
<link>https://arxiv.org/abs/2506.08011</link>
<guid>https://arxiv.org/abs/2506.08011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过玩简单街机游戏来提升多模态大型语言模型的跨领域推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究受认知科学启发，提出了一种新的后训练范式——视觉游戏学习（ViGaL），通过让多模态大型语言模型（MLLMs）玩类似街机的游戏，如贪吃蛇，从而提升其多模态推理的跨领域泛化能力。实验表明，这种强化学习方法显著提高了模型在多模态数学基准测试（如MathVista）及跨学科问题集（如MMMU）上的表现，且无需接触示例解法、方程或图表。值得注意的是，该模型在多模态推理基准测试中优于专门针对多模态推理数据微调的模型，同时保持了对一般视觉基准的表现，这是许多专门模型难以实现的。这一发现揭示了合成规则型游戏作为可控可扩展预训练任务的新潜力，可以有效解锁MLLMs中的通用多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>Dreamland：结合物理模拟器与生成模型的混合世界生成框架</title>
<link>https://arxiv.org/abs/2506.08006</link>
<guid>https://arxiv.org/abs/2506.08006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Dreamland框架，提升大规模视频生成模型的可控性并优化场景编辑与AI训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Dreamland的混合世界生成框架，该框架结合了基于物理的模拟器和大规模预训练生成模型的优点，旨在解决现有大型视觉生成模型缺乏元素级可控性的缺陷。Dreamland通过设计一种分层的世界抽象表示，将像素级和对象级语义及几何信息作为中间表示，从而实现对模拟器和生成模型的有效连接。这种方法不仅增强了模型的可控性，还降低了适配成本，并支持直接使用现有的和未来的预训练生成模型。此外，研究团队构建了一个名为D3Sim的数据集，用于支持混合生成管道的训练和评估。实验结果显示，Dreamland在图像质量方面比现有基线提高了50.8%，可控性增强了17.9%，并在增强具身智能体训练方面展现出巨大潜力。未来，代码和数据将公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Temperature-Adjusted Cross-modal Attention提升文本到图像扩散模型的对齐效果</title>
<link>https://arxiv.org/abs/2506.07986</link>
<guid>https://arxiv.org/abs/2506.07986</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TACA方法解决跨模态注意力不平衡问题，显著改善文本到图像生成的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态扩散Transformer（MM-DiT）模型如FLUX在文本提示与生成内容精确对齐上的不足，分析了跨模态注意力机制中的两大问题：视觉与文本模态间标记数量失衡导致的跨模态注意力抑制，以及缺乏时间步相关的注意力权重调整。为此，我们提出了温度调节跨模态注意力（TACA），通过温度缩放和时间步相关调整实现高效动态的多模态交互平衡。TACA结合LoRA微调后，在T2I-CompBench基准上显著提升了文本到图像生成的对齐性能，同时计算开销极小。实验表明，TACA在FLUX和SD3.5等先进模型上均有效改善了对象外观、属性绑定及空间关系的对齐效果。本研究强调了平衡跨模态注意力在提高文本到图像扩散模型语义保真度中的重要性。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07986" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>OneIG-Bench：文本到图像模型的综合性评估基准</title>
<link>https://arxiv.org/abs/2506.07977</link>
<guid>https://arxiv.org/abs/2506.07977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OneIG-Bench，用于多维度评估文本到图像模型的性能。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像（T2I）模型的发展，现有基准测试暴露出缺乏全面评估的问题，特别是对推理能力、文本渲染和风格化等方面的不足。为解决这些局限性，本文引入OneIG-Bench，这是一个经过精心设计的综合评估框架，涵盖提示-图像对齐、文本渲染精度、推理生成内容、风格化及多样性等多个维度。通过结构化的评估方式，该基准能够深入分析模型性能，帮助研究者和开发者识别模型的优势与瓶颈。此外，OneIG-Bench支持灵活的评估模式，用户可以专注于特定的评估子集，而无需针对所有提示生成图像。我们的代码库和数据集现已公开，旨在促进T2I研究领域的可重复性研究和跨模型比较。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:21 GMT</pubDate>
</item>
<item>
<title>MiniCPM4：面向端侧设备的高效大型语言模型</title>
<link>https://arxiv.org/abs/2506.07900</link>
<guid>https://arxiv.org/abs/2506.07900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniCPM4通过多维度创新成为高效的端侧大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一款名为MiniCPM4的高效大型语言模型（LLM），专为端侧设备设计。为了实现这一效率，MiniCPM4在模型架构、训练数据、训练算法及推理系统四个方面进行了系统性创新。在模型架构上，提出InfLLM v2，一种可训练的稀疏注意力机制，加速长上下文处理的预填充和解码阶段；在训练数据方面，提出UltraClean和UltraChat v2，仅需8万亿训练令牌即可获得满意性能；在训练算法上，改进了ModelTunnel v2并引入chunk-wise rollout和BitCPM，提升训练效率；在推理系统中，提出CPM.cu，结合稀疏注意力、模型量化和推测采样以实现高效预填充和解码。MiniCPM4提供两种版本，分别具有0.5B和8B参数量，并在多个基准测试中表现优异，特别是在处理长序列时，MiniCPM4-8B相比Qwen3-8B有显著速度提升。此外，通过进一步适配，MiniCPM4成功支持多种应用，如可信问卷生成和工具使用等，展示了其广泛的实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:16:50 GMT</pubDate>
</item>
<item>
<title>PolyVivid：多主体视频定制框架实现精确身份控制</title>
<link>https://arxiv.org/abs/2506.07848</link>
<guid>https://arxiv.org/abs/2506.07848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多主体视频定制框架，提升身份一致性与交互效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PolyVivid的多主体视频定制框架，解决了现有视频生成模型在精细控制方面的不足，特别是在多主体身份一致性和交互上的难题。该框架通过设计基于视觉语言大模型(VLLM)的文本-图像融合模块，将视觉身份嵌入到文本空间中，确保准确对应。同时，引入基于3D-RoPE的增强模块，促进文本与图像嵌入的双向结构化融合。此外，开发了继承注意力机制的身份注入模块，有效减轻身份漂移问题。最后，构建基于多语言大模型(MLLM)的数据管道，结合多种策略生成高质量多主体数据，显著提高主体区分度。实验表明，PolyVivid在身份保真度、视频真实感及主体对齐方面表现优异，优于现有开源和商业基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 11:11:09 GMT</pubDate>
</item>
<item>
<title>Improving large language models with concept-aware fine-tuning</title>
<link>https://arxiv.org/abs/2506.07833</link>
<guid>https://arxiv.org/abs/2506.07833</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:55:00 GMT</pubDate>
</item>
<item>
<title>通过图像重建解析视觉特征编码器</title>
<link>https://arxiv.org/abs/2506.07803</link>
<guid>https://arxiv.org/abs/2506.07803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过图像重建方法解析视觉特征编码器内部表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于图像重建的新方法，用于解释视觉特征编码器的内部表示。我们比较了SigLIP和SigLIP2两个相关模型家族，发现基于图像任务预训练的编码器保留了比对比学习等非图像任务更多的图像信息。此外，该方法还能对多种视觉编码器进行排名，并揭示特征空间的操作会导致可预测的重建图像变化，其中正交旋转而非空间变换控制颜色编码。此方法适用于任何视觉编码器，有助于揭示其特征空间的内部结构。实验代码和模型权重已公开于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:32:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型对低资源语言的漏洞分析</title>
<link>https://arxiv.org/abs/2506.07645</link>
<guid>https://arxiv.org/abs/2506.07645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型在低资源语言中的潜在安全漏洞。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在处理自然语言处理任务时面临的安全挑战，特别是其对小幅度字符和词汇级攻击的敏感性。通过仅修改少量字符并结合小型代理模型计算词的重要性，研究人员成功构建出强大的廉价攻击方式，这些攻击显著影响了多种LLMs的预测结果，表明其内部安全机制存在被绕过的隐患。研究重点验证了这种攻击在波兰语等低资源语言上的有效性，并展示了其扩展到其他语言的可能性。此外，研究团队公开了创建的数据集和代码，以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 07:09:39 GMT</pubDate>
</item>
<item>
<title>通过ReLIFT提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.07527</link>
<guid>https://arxiv.org/abs/2506.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合强化学习和监督微调的ReLIFT显著提升了大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，通过强化学习（RL），大型语言模型（LLM）能够表现出复杂的推理行为如规划和自我反思。然而，当前的强化学习方法受限于基础模型的知识范围，难以超越其现有能力。为解决这一问题，我们采用监督微调（SFT）来学习强化学习无法实现的能力，从而引入新的知识和推理模式。分析显示，强化学习擅长维持并优化模型原始能力范围内的表现，而监督微调则更适用于扩展模型能力边界。受此启发，我们提出了ReLIFT（强化学习与在线微调交替训练）的新方法。该方法主要依赖强化学习进行训练，在遇到难题时收集高质量解决方案进行微调，并交替进行两种训练方式以增强模型推理能力。实验表明，ReLIFT在多个竞争级基准测试中平均提升了超过5.2分，且仅需13%的详细演示数据便优于传统方法，证明了其优越的可扩展性与潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:11:20 GMT</pubDate>
</item>
<item>
<title>SpatialLM：基于标准多模态LLM架构的3D场景理解模型</title>
<link>https://arxiv.org/abs/2506.07491</link>
<guid>https://arxiv.org/abs/2506.07491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialLM通过标准LLM架构处理点云数据并实现3D场景理解。</p><br /><br /><p><strong>摘要：</strong> SpatialLM是一种专门设计用于处理三维点云数据的大语言模型，旨在生成结构化的3D场景理解输出，例如墙壁、门、窗户等建筑元素及带语义类别的对象框。与以往依赖特定任务网络设计的方法不同，该模型采用标准的多模态大型语言模型架构，并直接从开源模型微调而来。为了训练SpatialLM，我们收集了一个包含12,328个室内场景（54,778个房间）及其真实3D标注的大规模高质量合成数据集，并对多种建模和训练决策进行了深入研究。在公共基准测试中，该模型在布局估计方面达到了最先进的性能，在3D物体检测方面也取得了具有竞争力的结果。这一成果展示了增强现代大型语言模型空间理解能力的一种可行路径，适用于增强现实、具身机器人等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 03:10:58 GMT</pubDate>
</item>
<item>
<title>CCI4.0：构建高质量双语预训练数据集及其应用</title>
<link>https://arxiv.org/abs/2506.07463</link>
<guid>https://arxiv.org/abs/2506.07463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于大规模语言模型预训练的高质量双语数据集CCI4.0。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CCI4.0的大规模双语预训练数据集，该数据集经过精心设计以提供卓越的数据质量和多样化的人类推理轨迹。数据集由两个子集组成：CCI4.0-M2-Base和CCI4.0-M2-CoT。其中，CCI4.0-M2-Base结合了精心挑选的中文网络语料库、Nemotron-CC的英文子集以及其他多样化的数学、维基百科、arxiv和代码来源。为了保证数据质量，我们提出了一个新的流水线方法，主要基于模型通过两阶段去重、多分类器质量评分和领域感知流畅性过滤来验证数据质量。此外，我们还提取了45亿个CoT（Chain-of-Thought）模板，命名为CCI4.0-M2-CoT，这些模板展示了不同的推理模式，并显著降低了幻觉的可能性。实验评估表明，在CCI4.0上预训练的语言模型在下游任务中，尤其是在数学和代码反射任务中，表现出了一致的改进。我们的研究强调了严格的数据整理和人类思维模板在提升大型语言模型性能中的关键作用，为自动处理预训练语料库提供了启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:14:19 GMT</pubDate>
</item>
<item>
<title>弱到强解码框架提升大语言模型对齐能力</title>
<link>https://arxiv.org/abs/2506.07434</link>
<guid>https://arxiv.org/abs/2506.07434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弱到强解码框架增强大语言模型对齐能力。</p><br /><br /><p><strong>摘要：</strong> 为了改善大语言模型（LLMs）生成内容可能存在的不当、虚假或无意义的问题，近年来低资源对齐方法受到关注，但高质量且对齐良好的数据获取仍具挑战性。本研究发现解码开始阶段对生成对齐响应的难度较大，因此提出弱到强解码（WSD）框架，通过小型对齐模型引导大型基础模型，先由小型模型起草对齐开头，再由大型模型完成后续内容。此外，还构建了一个名为GenerAlign的新数据集，用于微调小型Pilot-3B模型作为起草模型。实验表明，该方法在不损害下游任务性能的情况下显著提升了多种基础模型的对齐效果。同时，研究还深入分析了WSD框架的内在机制及其设置和时间效率的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 01:21:22 GMT</pubDate>
</item>
<item>
<title>通过ConfQA策略降低大语言模型事实性陈述幻觉率</title>
<link>https://arxiv.org/abs/2506.07309</link>
<guid>https://arxiv.org/abs/2506.07309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为ConfQA的微调策略，将大语言模型的事实性幻觉率降至5%以下。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ConfQA的微调策略，旨在减少大型语言模型（LLMs）在生成事实性陈述时的幻觉现象。传统方法下，LLMs的幻觉率通常在20%-40%，而通过ConfQA策略，这一比率可降至5%以下。该策略的核心在于当模型回答正确时，训练其继续给出答案；否则则训练其承认“我不确定”。此外，通过引入“仅在自信时回答”的抑制提示，以及利用知识图谱中的简单事实属性值来校准模型的信心，进一步提升了效果。基于此，研究提出了Dual Neural Knowledge框架，该框架能够根据ConfQA的置信度，在内部参数化神经知识和外部记录的符号知识之间无缝切换，不仅使准确率提升至超过95%，还减少了超过30%不必要的外部检索操作。这种方法展现了在多个事实性基准测试上的强大泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 18:51:46 GMT</pubDate>
</item>
<item>
<title>利用预训练语言模型通过上下文学习预测隐马尔可夫模型生成序列</title>
<link>https://arxiv.org/abs/2506.07298</link>
<guid>https://arxiv.org/abs/2506.07298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示预训练语言模型可通过上下文学习有效预测隐马尔可夫模型生成的数据。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了预训练大型语言模型（LLMs）通过上下文学习（ICL）对隐马尔可夫模型（HMMs）生成数据的建模能力。实验结果显示，LLMs在多种合成HMM上达到了接近理论最优的预测精度。此外，研究揭示了受HMM属性影响的新颖规模趋势，并提出了相关理论假设。针对科学家的实际应用，本文还提供了使用ICL作为复杂数据分析诊断工具的指南。在真实动物决策任务中，ICL的表现与人类设计的模型相当。这是首次证明ICL可以学习并预测HMM生成序列的研究，这一发现深化了我们对LLMs上下文学习的理解，并展示了其作为揭示复杂科学数据隐藏结构的强大工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 17:49:38 GMT</pubDate>
</item>
<item>
<title>通过内部进度编码优化大型语言模型的显式推理过程</title>
<link>https://arxiv.org/abs/2506.07240</link>
<guid>https://arxiv.org/abs/2506.07240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLMs如何调节推理长度并提出一种减少过思考的方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在显式推理过程中理解和调控推理长度的机制。首先，我们发现LLMs会在推理过程中编码进展状态，并通过交互式进度条可视化揭示了模型的规划动态。其次，我们通过对推理阶段的内部进度编码进行操作，减少了不必要的推理步骤，从而生成更加简洁且果断的推理链条。实验结果显示，这种方法可以有效缓解过思考问题，提高答案准确性，同时降低推理延迟。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 13:54:33 GMT</pubDate>
</item>
<item>
<title>GeometryZero：通过强化学习优化几何问题求解的辅助构造</title>
<link>https://arxiv.org/abs/2506.07160</link>
<guid>https://arxiv.org/abs/2506.07160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习框架GCPO，用于训练高效结合辅助构造和几何推理的模型GeometryZero。</p><br /><br /><p><strong>摘要：</strong> 近期大规模语言模型(LLMs)在数学推理领域表现出色，但在几何问题求解方面仍具挑战性，尤其是辅助构造的重要性。现有方法要么性能欠佳，要么依赖庞大的LLMs导致高计算成本。我们提出了一种基于可验证奖励的强化学习方向，但直接应用存在局限性。为此，我们设计了Group Contrastive Policy Optimization(GCPO)，创新性地引入上下文效用自适应奖励和长度奖励，开发出GeometryZero模型家族，其在多个基准测试中显著优于现有方法，平均提升4.29%。这项研究为更经济高效的几何推理模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:18:15 GMT</pubDate>
</item>
<item>
<title>大型推理模型的性能与局限性分析</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型推理模型在复杂度超过一定阈值时会出现准确性崩溃。</p><br /><br /><p><strong>摘要：</strong> 近年来，语言模型的发展引入了大型推理模型（LRMs），这些模型在提供答案前会生成详细的推理过程，显示出在推理基准测试中的改进表现。然而，关于其基本能力、扩展特性及限制的研究仍显不足。目前的评估主要集中在数学和编码基准上，侧重最终答案的准确性，但这种评估方式容易受到污染且无法深入洞察推理路径。本研究利用可控谜题环境系统地探讨了这些空白，在保持逻辑结构一致的同时精确调整复杂度。实验结果显示，LRMs在超过特定复杂度时完全丧失准确性，且表现出反直觉的扩展极限——随着问题复杂性的增加，推理努力先上升后下降。通过与标准语言模型对比，我们发现LRMs在低、中、高复杂度任务中分别表现出不同的性能表现。此外，研究还深入分析了LRMs的计算行为和推理路径，揭示了其在精确计算和跨尺度推理中的局限性，并对模型的推理能力提出了疑问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 18:42:29 GMT</pubDate>
</item>
<item>
<title>通过元学习提升多模态大模型的小样本任务适应能力</title>
<link>https://arxiv.org/abs/2506.06905</link>
<guid>https://arxiv.org/abs/2506.06905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于元学习的方法，通过蒸馏任务相关图像特征的软提示，提升小规模多模态大模型的小样本性能。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型(LMMs)通常依赖于上下文学习(ICL)来执行新任务，但这种性能在较小的LMMs中往往不一致且非单调增长。我们假设这是由于模型被多余的图像嵌入信息所淹没，这些信息对下游任务并非必要。为解决此问题，我们提出了一种元学习方法，利用固定的任务相关图像特征集的软提示，在少量示例下进行测试时适配。为此，我们引入了一个注意力映射模块，可以轻松集成到流行的LLaVA v1.5架构中，并与软提示联合训练，使模型在低数据情况下仅需少量梯度步即可适应任务。在VL-ICL基准上的评估表明，我们的方法在面对图像扰动时始终优于ICL及相关提示微调方法，提升了视觉问答任务中的任务诱导和推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 15:37:22 GMT</pubDate>
</item>
<item>
<title>大型语言模型在上下文与记忆冲突下的表现评估框架</title>
<link>https://arxiv.org/abs/2506.06485</link>
<guid>https://arxiv.org/abs/2506.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出诊断框架评估大型语言模型在上下文与参数知识冲突中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种诊断框架，用于系统性评估大型语言模型（LLMs）在面对上下文信息与参数知识冲突时的行为。通过构建引发此类冲突的诊断数据集，并分析模型在多种任务类型上的表现，研究发现当上下文信息与模型的参数化信念一致时，模型表现更好；知识冲突对不依赖知识的任务影响较小；模型难以完全抑制内部知识，即使有明确指令；解释冲突的推理过程会增加对上下文的依赖。这些发现引发了对基于模型评估有效性的担忧，并强调了在部署LLMs时考虑知识冲突的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 15:20:23 GMT</pubDate>
</item>
<item>
<title>面向LLM推理阶段的安全保障：SAFFRON范式的提出</title>
<link>https://arxiv.org/abs/2506.06444</link>
<guid>https://arxiv.org/abs/2506.06444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对LLM推理阶段的安全性问题，本文提出了一种新的范式SAFFRON。</p><br /><br /><p><strong>摘要：</strong> 现有的安全保证研究主要集中在训练阶段对齐，以使大型语言模型（LLMs）习得安全行为。然而，近期研究表明这些方法容易受到多种越狱攻击的影响。与此同时，推理阶段的扩展显著提升了LLMs的推理能力，但其在安全保证中的应用尚未被深入探索。本研究填补了这一空白，开创性地提出了推理阶段的扩展方法，以应对新兴威胁下的LLMs安全问题。我们发现，尽管传统的推理扩展技术在推理任务中表现良好，但在安全场景下却表现欠佳，甚至不如基本的Best-of-N采样方法。这种低效性归因于频繁的过程奖励模型（PRM）评估所带来的高计算开销。为解决这一问题，我们提出了SAFFRON，一种专门针对安全保证设计的新推理扩展范式。该范式的核心在于引入多分支奖励模型（MRM），大幅减少所需奖励模型评估次数。此外，我们还设计了部分监督训练目标、保守探索约束以及基于Trie的数据缓存策略等关键技术。实验结果验证了SAFFRON的有效性，并公开了训练好的多分支奖励模型（Saffron-1）及相关的安全奖励数据集（Safety4M），以促进LLMs安全领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:05:45 GMT</pubDate>
</item>
<item>
<title>通过自学习训练KV缓存以降低大语言模型长上下文推理成本</title>
<link>https://arxiv.org/abs/2506.06266</link>
<guid>https://arxiv.org/abs/2506.06266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过自学习方法训练KV缓存（Cartridge），大幅降低大语言模型长上下文推理的成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通常通过将整个文本语料库放入上下文窗口并利用上下文学习（ICL）来回答基于大规模文本语料库的问题。然而，这种方法的内存消耗随着输入长度增加而增加，导致服务成本高昂。为解决这一问题，本文探索了一种替代方案：在线下为每个语料库训练一个较小的KV缓存。这种缓存被称为“Cartridge”，在推理时加载即可生成响应。尽管训练Cartridge的成本可以分摊到引用相同语料库的所有查询上，但直接使用下一个标记预测的方法未能超越ICL。因此，我们提出了自学习（self-study）方法，即生成关于语料库的合成对话，并以上下文化蒸馏为目标训练Cartridge。实验表明，采用自学习训练的Cartridge不仅能够复制ICL的功能，而且显著降低了推理成本。在具有挑战性的长上下文基准测试中，使用自学习训练的Cartridge在内存使用减少38.6倍的同时，吞吐量提高了26.4倍。此外，自学习还延长了模型的有效上下文长度，并意外地使推理时无需重新训练即可组合Cartridge。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:48:23 GMT</pubDate>
</item>
<item>
<title>Astra双模型架构提升机器人室内导航性能</title>
<link>https://arxiv.org/abs/2506.06205</link>
<guid>https://arxiv.org/abs/2506.06205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Astra双模型架构显著提高机器人在复杂室内环境中的导航成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Astra的全新双模型架构，用于解决现代机器人在多样化复杂室内环境中导航困难的问题。Astra由全局模型Astra-Global和局部模型Astra-Local组成，其中Astra-Global采用多模态大型语言模型处理视觉和语言输入，结合混合拓扑语义图实现自定位和目标定位，优于传统视觉位置识别方法；Astra-Local则是一个多任务网络，负责局部路径规划和里程估计，其4D时空编码器通过自监督学习生成鲁棒特征，规划头利用流匹配和新型掩码ESDF损失减少碰撞风险，里程计头通过变压器编码器整合多传感器输入预测机器人相对姿态。Astra部署在实际移动机器人上，在多种室内环境中实现了高成功率的任务完成率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 12:08:47 GMT</pubDate>
</item>
<item>
<title>基于语言表达动作的视觉-语言基础模型世界模型与动力学模型研究</title>
<link>https://arxiv.org/abs/2506.06006</link>
<guid>https://arxiv.org/abs/2506.06006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监督微调提升动力学模型较构建世界模型更容易。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉-语言基础模型在语言表达动作时，构建现实世界模型（观察-动作-观察）和动力学模型（观察-观察-动作）的能力。尽管开源模型在这两方面均表现不佳，但研究发现，通过监督微调获取动力学模型比获取世界模型更为容易。进一步地，动力学模型可通过两种策略助力世界模型的构建：一是利用合成数据进行弱监督学习，二是推理阶段的验证。具体而言，动力学模型可标注视频帧对之间的动作，从而扩充训练数据；同时，通过引入一种新目标函数，依据识别模型预测的重要性加权观察对中的图像标记。此外，动力学模型还能为世界模型的多个样本分配奖励，用于推理阶段的评分。通过Aurora-Bench上的动作中心图像编辑任务评估，我们最佳模型的表现与最先进的图像编辑模型相当，在GPT4o-as-judge评估的现实子集上提升了15%，并在Aurora-Bench的所有子集上获得了最佳的人类评价。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:50:18 GMT</pubDate>
</item>
<item>
<title>基于合成对话数据的实时感知任务引导对话系统</title>
<link>https://arxiv.org/abs/2506.05904</link>
<guid>https://arxiv.org/abs/2506.05904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种综合框架，解决实时感知任务引导对话系统的数据和评估难题。</p><br /><br /><p><strong>摘要：</strong> 近期会话人工智能取得了显著进展，但在开发实时感知任务引导系统方面仍面临挑战。这类系统需要根据流式视觉输入提供交互性和主动性帮助，但其开发受到昂贵且耗时的数据收集和系统评估过程的限制。为了解决这些局限性，本文提出了一个全面的框架，包含三个关键贡献：首先，引入一种新颖的数据整理管道，从标注的主观视角视频中合成对话，生成跨越多个领域的大型合成对话数据集；其次，开发了一组自动评估指标，并通过广泛的用户研究验证其有效性；最后，提出一个端到端模型，处理流式视频输入以生成上下文相关的响应，同时采用创新技术处理数据不平衡和长时间视频问题。这项工作为开发实时主动AI助手奠定了基础，能够指导用户完成多样化任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:23:29 GMT</pubDate>
</item>
<item>
<title>大型语言模型在辩论演讲评估中的表现分析</title>
<link>https://arxiv.org/abs/2506.05062</link>
<guid>https://arxiv.org/abs/2506.05062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型在评估辩论演讲中的能力表现。</p><br /><br /><p><strong>摘要：</strong> 本文引入了辩论演讲评估作为评估大型语言模型（LLMs）判断能力的新基准，该任务需要对演讲在多个层面进行深入理解，包括论证强度、连贯性及风格等。通过分析超过600份精心标注的辩论演讲数据集，我们发现虽然较大的模型在某些方面可以接近人类的判断，但在整体判断行为上存在显著差异。此外，研究还探讨了前沿LLMs生成有说服力演讲的能力，表明这些模型在这项任务上的表现可能达到人类水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 10:06:51 GMT</pubDate>
</item>
<item>
<title>MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character Recognition with over 97K Categories</title>
<link>https://arxiv.org/abs/2506.04807</link>
<guid>https://arxiv.org/abs/2506.04807</guid>
<content:encoded><![CDATA[
Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 05:33:06 GMT</pubDate>
</item>
<item>
<title>gamma-PO算法提升大语言模型对齐效率</title>
<link>https://arxiv.org/abs/2506.03690</link>
<guid>https://arxiv.org/abs/2506.03690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出gamma-PO算法优化大语言模型对齐，显著提高性能且不影响训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为gamma-PO的新算法，该算法是一种动态目标边界偏好优化方法，通过引入实例特定的边界校准，在每对偏好数据中调整奖励边界。这种方法优先处理高置信度数据对，同时抑制噪声影响，从而有效提升大语言模型（LLMs）的安全性和可靠性。gamma-PO兼容多种基于奖励边界的直接偏好优化（DPO）变体，并在AlpacaEval2和Arena-Hard等基准测试中实现了平均4.4%的性能提升，成为当前最先进的技术。此外，gamma-PO仅需少量代码改动，对训练效率几乎没有负面影响，证明了其作为增强LLMs对齐工具的稳健性。相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:19:37 GMT</pubDate>
</item>
<item>
<title>ExpertLongBench：面向专家级任务的大规模语言模型评估基准</title>
<link>https://arxiv.org/abs/2506.01241</link>
<guid>https://arxiv.org/abs/2506.01241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出包含11项任务的ExpertLongBench基准，评估现有大模型在专家级任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为ExpertLongBench的专家级基准，该基准包含来自9个领域的11项任务，这些任务反映了真实的专家工作流程和应用。不同于传统的问答任务，ExpertLongBench中的任务需要超过5000个标记的长篇输出，并且要求严格遵守领域特定的要求。每个任务都有由领域专家设计或验证的任务说明表，用于指定任务需求并指导输出评估。此外，我们提出了CLEAR评估框架，支持对本基准中长篇模型输出的准确评估。为了实现细粒度的专家对齐评估，CLEAR通过从任务特定说明表中提取信息，从模型输出和参考中衍生出检查清单。然后将模型输出的检查清单项目与参考输出的对应项目进行比较，以评估其正确性，从而实现基于事实的评估。我们对11个大型语言模型进行了基准测试，并分析了CLEAR的组成部分，结果显示现有大型语言模型在专家级任务上的表现仍需显著改进，其中顶级性能仅达到26.8%的F1得分；尽管如此，模型确实可以生成符合要求的内容，但准确性仍有待提高；同时，开放权重模型在CLEAR中准确提取和比较检查清单方面具有可扩展性和低成本的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 21:39:02 GMT</pubDate>
</item>
<item>
<title>EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions</title>
<link>https://arxiv.org/abs/2505.23473</link>
<guid>https://arxiv.org/abs/2505.23473</guid>
<content:encoded><![CDATA[
Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 10:26:46 GMT</pubDate>
</item>
<item>
<title>引入自省与纠错能力的端到端多模态GUI自动化框架</title>
<link>https://arxiv.org/abs/2506.08012</link>
<guid>https://arxiv.org/abs/2506.08012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GUI-Reflection框架，提升多模态模型的自省与错误恢复能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有图形用户界面(GUI)自动化模型缺乏自省和错误恢复能力的问题，提出了一种名为GUI-Reflection的新框架。该框架通过三个阶段的专用训练——特定于GUI的预训练、离线监督微调(SFT)和在线自省微调，将自我反思和错误纠正功能融入到端到端的多模态GUI模型中。这一过程完全基于自动化的数据生成和学习，无需人工标注。具体而言，我们首先设计了可扩展的数据管道，从现有的成功轨迹中自动生成用于反思和错误修正的数据。同时，我们还提出了GUI-Reflection任务套件，以显式地学习和评估这些面向反思的能力。此外，我们在移动设备上构建了一个多样化且高效的环境，用于GUI模型的在线训练和数据收集，并开发了一种迭代式的在线自省微调算法，使模型能够持续提升其自省和错误纠正能力。我们的方法为更健壮、适应性强且智能的GUI自动化铺平了道路，并承诺公开所有数据、模型、环境和工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>强化预训练（RPT）：语言模型与强化学习的新范式</title>
<link>https://arxiv.org/abs/2506.08007</link>
<guid>https://arxiv.org/abs/2506.08007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化预训练通过将预测下一token重构为推理任务，显著提升语言模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的扩展范式——强化预训练（Reinforcement Pre-Training, RPT），它将下一个词预测重新定义为一种可以通过强化学习训练的推理任务。该方法利用可验证的奖励机制，针对给定上下文正确预测下一个词，从而有效利用大量文本数据进行通用强化学习，而无需依赖特定领域的标注答案。实验表明，RPT不仅显著提高了语言建模的准确性，还为后续的强化微调奠定了坚实的基础。此外，随着计算资源的增加，训练效果持续改善，显示出RPT作为语言模型预训练有效且有前景的扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>小规模语言模型在长链式思维训练中的性能退化现象</title>
<link>https://arxiv.org/abs/2506.07712</link>
<guid>https://arxiv.org/abs/2506.07712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示小规模语言模型在长链式思维训练中会出现显著性能下降的现象。</p><br /><br /><p><strong>摘要：</strong> 长链式思维（Long CoT）监督是提升语言模型推理能力的常见策略，但在小规模语言模型（SLMs，≤3B参数）中，我们观察到一种名为“长CoT退化”的现象，即这些模型在有限长CoT数据集上训练时性能会大幅下降。通过Qwen2.5、LLaMA3和Gemma3系列模型的实验验证，我们发现这种退化普遍存在。例如，在某些情况下，仅基于8k长CoT样本训练的模型在微调前性能可能降低多达75%。即使增加训练样本量至220k，部分极小型模型也无法恢复或超越初始性能。进一步分析表明，这种退化源于错误累积效应，尽管长响应能增强多步推理能力，但也放大了错误传播风险。此外，长CoT退化可能对下游强化学习产生负面影响，但可通过充分的监督微调缓解。本研究挑战了关于长CoT训练对SLMs益处的传统认知，为构建更有效的中小规模推理模型提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 08:56:41 GMT</pubDate>
</item>
<item>
<title>GTR-Mol-VLM：基于图遍历机制的光学化学结构识别框架</title>
<link>https://arxiv.org/abs/2506.07553</link>
<guid>https://arxiv.org/abs/2506.07553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GTR-Mol-VLM模型提升复杂分子结构识别精度。</p><br /><br /><p><strong>摘要：</strong> 光学化学结构识别(OCSR)是将分子图像转换为机器可读格式的关键技术。本文介绍了一种名为GTR-Mol-VLM的新框架，通过引入Graph Traversal as Visual Chain of Thought机制和Faithfully Recognize原则解决了现有视觉语言模型在处理复杂分子结构时的不足。为了支持模型开发，构建了GTR-CoT-1.3M大规模指令调优数据集，并设计了MolRec-Bench基准测试。实验表明，GTR-Mol-VLM在涉及功能团缩写分子图像的场景中，比第二好的基线高出约14个百分点。本研究希望推动OCSR技术更好地服务于实际需求，促进化学生物信息学和科学人工智能领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:47:10 GMT</pubDate>
</item>
<item>
<title>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</title>
<link>https://arxiv.org/abs/2506.07530</link>
<guid>https://arxiv.org/abs/2506.07530</guid>
<content:encoded><![CDATA[
Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:15:11 GMT</pubDate>
</item>
<item>
<title>Lingshu：面向医学应用的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2506.07044</link>
<guid>https://arxiv.org/abs/2506.07044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的医学专用多模态大型语言模型Lingshu。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有医学多模态大型语言模型在医疗场景中的局限性，如知识覆盖不足、易产生幻觉及缺乏复杂推理能力等问题，提出了综合的数据整理程序和多阶段训练方法，构建了一个名为Lingshu的医学专用多模态大型语言模型。Lingshu不仅扩展了医学知识的范围，还通过强化学习技术增强了其推理能力。此外，我们开发了MedEvalKit评估框架，用于标准化评估模型性能。实验结果显示，Lingshu在多项医学任务上优于现有的开源多模态模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:30 GMT</pubDate>
</item>
<item>
<title>MIRIAD：构建高质量医疗知识库以提升大语言模型可靠性</title>
<link>https://arxiv.org/abs/2506.06091</link>
<guid>https://arxiv.org/abs/2506.06091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入MIRIAD医学问答数据集，显著提高大语言模型在医疗领域的准确性与可信度。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）有望通过高级决策支持和灵活的聊天助手革新医疗领域，但其生成的医疗内容可能存在不准确性。为解决这一问题，本文提出了MIRIAD，这是一个大规模、经过精心筛选的医学问答对数据集，包含5,821,948个问答对，这些问答对源自同行评审的医学文献并经过半自动化处理。与依赖原始未结构化文本的传统方法相比，MIRIAD以操作化的查询-回答格式组织医疗知识，从而实现更精确的知识检索。实验表明，在具有相同源数据和检索文本量的情况下，结合MIRIAD的大语言模型在困难的医学问答基准测试中的准确率提高了多达6.7%，同时在检测医疗幻觉方面的能力提升了22.5至37%（F1分数增加）。此外，还推出了MIRIAD-Atlas，这是一个交互式的知识地图，覆盖了56个医学学科，便于临床用户探索、搜索和细化医学知识。MIRIAD及其相关工具为医疗信息检索器、增强型RAG应用和基于知识的聊天界面等下游应用奠定了基础，最终推动医疗领域更可靠的LLM应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 09:52:32 GMT</pubDate>
</item>
<item>
<title>从模型中心到数据中心：AI图像生成领域的范式转变</title>
<link>https://arxiv.org/abs/2506.05673</link>
<guid>https://arxiv.org/abs/2506.05673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型领域转向数据质量驱动的发展，引入高质量图像数据集DSD。</p><br /><br /><p><strong>摘要：</strong> 现代人工智能模型，特别是在计算机视觉和图像生成任务中的扩散模型，正在经历开发方法的重大转变。过去主要依赖于复杂模型架构和超参数优化的“模型中心”方法，正逐渐被更注重数据质量、结构和相关性的“数据中心”方法取代。本文介绍了DataSeeds.AI样本数据集（DSD），该数据集包含约10,610张经过高质量人类评分和多层注释的摄影图片，旨在推动商业图像数据集的新标准。作为DataSeed.AI超过1亿张图像目录的一小部分，DSD为稳健的商业和多模态AI开发提供了可扩展的基础。通过深入分析，我们展示了DSD对特定模型在已知基准上的定量改进，并公开了评估过程中使用的代码和训练模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 21:50:28 GMT</pubDate>
</item>
<item>
<title>AI推理知识转移能力的评估与优化</title>
<link>https://arxiv.org/abs/2506.05579</link>
<guid>https://arxiv.org/abs/2506.05579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现AI模型性能与人类理解间存在不一致性，需专门优化知识转移。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能推理能力的进步显著提升了多项任务的表现，但其是否有助于更佳的知识转移尚存疑问。知识转移涉及模型以人类可理解的方式传达推理，从而促进学习和应用。为探讨这一问题，我们开发了知识集成与转移评估(KITE)框架，并开展了首个大规模人类实验(N=118)，以衡量AI与人类协作中的知识传递效果。实验分为两个阶段：首先，人类与AI共同构思解决方案策略；随后，人类独立实施解决方案。结果显示，尽管AI基准性能与合作成果之间存在一定关联，但这种关系并不稳定，存在显著异常值，表明知识转移需要针对性优化。进一步分析揭示了影响成功知识转移的行为和战略因素。我们公开了代码、数据集及评估框架，以支持未来研究交流对齐模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 16:48:16 GMT</pubDate>
</item>
</channel>
</rss>