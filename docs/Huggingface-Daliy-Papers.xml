<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>Dr.V：一种用于诊断视频幻觉的层次化框架</title>
<link>https://arxiv.org/abs/2509.11866</link>
<guid>https://arxiv.org/abs/2509.11866</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Dr.V通过多层级分析有效诊断视频幻觉。</p><br><br><p><strong>摘要：</strong> 本文提出了一种名为Dr.V的层次化框架，旨在解决大型视频模型（LVMs）中存在的幻觉问题。该框架包含两个核心组件：Dr.V-Bench数据集和Dr.V-Agent视频代理。Dr.V-Bench由10,000个实例组成，涵盖4,974段视频，具有详细的空间-时间标注。Dr.V-Agent通过在感知、时间和认知层面上进行细粒度的空间-时间定位，系统地检测LVMs中的幻觉，模拟人类视频理解过程，提升模型的可解释性和可靠性。实验表明，Dr.V-Agent能有效诊断幻觉，并为实际视频理解提供可行方案。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2509.11866 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 08:39:19 GMT</pubDate>
<pubDate>Mon, 15 Sep 2025 08:39:19 GMT</pubDate>
</item>
<item>
<title>SearchInstruct：构建高质量指令数据集以提升大语言模型性能</title>
<link>https://arxiv.org/abs/2509.10708</link>
<guid>https://arxiv.org/abs/2509.10708</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>SearchInstruct提升LLM在特定领域的性能。</p><br><br><p><strong>摘要：</strong> 本文提出SearchInstruct方法，用于构建高质量的指令数据集以优化大语言模型的监督微调过程。该方法从少量领域相关的手工问题出发，利用大语言模型进行扩展，并动态检索相关资源生成准确答案。实验表明，SearchInstruct提高了数据集的多样性和质量，从而提升了模型在专业领域的表现。此外，该方法还可用于模型编辑，支持对现有模型进行高效更新。作者提供了完整的实现细节、数据集和源代码以供复现和社区使用。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2509.10708 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 17:50:39 GMT</pubDate>
<pubDate>Fri, 12 Sep 2025 17:50:39 GMT</pubDate>
</item>

<item>
<title>提升视觉语言模型视觉反思能力的研究</title>
<link>https://arxiv.org/abs/2509.12132</link>
<guid>https://arxiv.org/abs/2509.12132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Reflection-V模型，增强视觉语言模型的视觉反思能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将文本领域的‘慢思考’推理能力迁移至视觉语言模型（VLMs），以训练更有效的视觉推理模型（VRMs）。研究发现，当前VRMs在长时间推理过程中对视觉信息的关注度迅速下降，导致视觉反思能力不足。为此，作者提出Reflection-V模型，通过构建以视觉为中心的推理数据和基于视觉注意力的奖励机制，提升VRMs的视觉反思能力。实验表明，Reflection-V在多个视觉推理基准测试中表现显著提升，并在推理过程中保持对视觉信息的持续依赖，验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.12132" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 12:57:25 GMT</pubDate>
</item>
<item>
<title>视觉-语言模型中信息损失分析与量化研究</title>
<link>https://arxiv.org/abs/2509.11986</link>
<guid>https://arxiv.org/abs/2509.11986</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉-语言模型在特征投影中的信息损失问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉-语言模型（VLMs）在将视觉输入通过预训练视觉编码器处理后，经由连接组件投影到语言模型嵌入空间过程中可能产生的信息损失。作者提出了两种互补方法来分析和量化这种损失：一是通过分析图像表示在投影前后的k近邻关系变化来评估语义信息的保留程度；二是通过从投影表示中重建视觉嵌入，以图像块级别定位信息损失。实验结果显示，连接组件显著扭曲了视觉表示的局部几何结构，导致k近邻关系偏离40%-60%，并影响检索性能。此外，图像块级别的嵌入重建为模型在视觉基础问答任务中的行为提供了可解释的洞察，发现高信息损失区域可靠地预测了模型表现不佳的情况。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11986" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 10:38:06 GMT</pubDate>
</item>
<item>
<title>Nav-R1：一种统一的具身导航基础模型</title>
<link>https://arxiv.org/abs/2509.10884</link>
<guid>https://arxiv.org/abs/2509.10884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nav-R1提升具身导航的推理与控制效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Nav-R1，一个统一的具身导航基础模型，旨在解决传统方法在复杂3D环境中推理不稳定、难以平衡长距离语义推理与低延迟控制的问题。研究构建了Nav-CoT-110K大规模数据集，支持结构化推理初始化，并设计基于GRPO的强化学习框架，结合格式、理解与导航三类奖励以提升路径准确性。同时引入“快入慢思”机制，分离语义推理与实时控制，实现高效且连贯的导航。实验表明，Nav-R1在多个基准测试中表现优于现有方法，平均提升超过8%。此外，在移动机器人上的部署验证了其在资源受限环境下的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 12:31:03 GMT</pubDate>
</item>
<item>
<title>深度扩散模型中的局部性源于图像数据集的统计特性</title>
<link>https://arxiv.org/abs/2509.09672</link>
<guid>https://arxiv.org/abs/2509.09672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示深度扩散模型的局部性来自图像数据集的统计特性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散模型中深度神经网络与最优去噪器之间的性能差异。研究发现，深度扩散模型的局部性并非源于卷积神经网络的归纳偏置，而是自然图像数据集中像素相关性的统计结果。通过实验和理论分析，作者证明了一个最优参数化线性去噪器也表现出类似的局部特性。基于这些发现，他们设计了一个更接近深度扩散模型预测得分的解析去噪器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>LazyDrag：基于多模态扩散Transformer的拖拽图像编辑方法</title>
<link>https://arxiv.org/abs/2509.12203</link>
<guid>https://arxiv.org/abs/2509.12203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LazyDrag通过显式对应图提升拖拽编辑效果，无需依赖隐式点匹配。</p><br /><br /><p><strong>摘要：</strong> 本文提出LazyDrag，一种针对多模态扩散Transformer的拖拽图像编辑方法。与以往依赖隐式点匹配的方法不同，LazyDrag通过用户拖拽输入生成显式对应图，从而增强注意力控制，实现更稳定的全强度反演过程。该方法避免了耗时的测试阶段优化，提升了生成模型的能力，实现了精准几何控制与文本引导的统一。LazyDrag能够完成复杂编辑任务，如打开狗嘴并进行内部修复、生成新物体等，并支持多轮操作。在DragBench数据集上，LazyDrag在拖拽准确性和感知质量方面均优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.12203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>OmniWorld：推动4D世界建模发展的多模态数据集</title>
<link>https://arxiv.org/abs/2509.12201</link>
<guid>https://arxiv.org/abs/2509.12201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniWorld提升4D建模性能，推动机器理解物理世界。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了OmniWorld，一个大规模、多领域、多模态的4D世界建模数据集，旨在解决现有数据集在动态复杂性、多域多样性和时空标注方面的不足。OmniWorld包含新收集的OmniWorld-Game数据集和多个公共数据集，相比现有合成数据集具有更丰富的模态覆盖、更大的规模和更真实的动态交互。基于该数据集，研究者建立了具有挑战性的基准，揭示了当前最先进方法在建模复杂4D环境中的局限性，并通过微调显著提升了4D重建和视频生成任务的性能，验证了OmniWorld作为训练与评估资源的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.12201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>构建心理健康伦理决策评估框架：EthicsMH数据集的引入</title>
<link>https://arxiv.org/abs/2509.11648</link>
<guid>https://arxiv.org/abs/2509.11648</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EthicsMH数据集用于评估AI在心理健康领域的伦理决策能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了EthicsMH数据集，旨在填补当前AI在心理健康领域伦理决策评估方面的空白。该数据集包含125个情景，涵盖保密性、自主权、利他主义和偏见等关键伦理问题，并提供结构化字段用于评估AI系统的决策准确性、解释质量和专业规范对齐情况。尽管规模有限，EthicsMH为AI伦理与心理健康决策的结合提供了任务框架，并鼓励社区和专家进一步扩展该资源，以推动AI在敏感领域负责任地发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11648" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 15 Sep 2025 03:35:35 GMT</pubDate>
</item>
<item>
<title>半在线强化学习提升GUI代理多步骤任务执行能力</title>
<link>https://arxiv.org/abs/2509.11543</link>
<guid>https://arxiv.org/abs/2509.11543</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">半在线RL提升GUI代理多步骤任务执行效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为半在线强化学习（Semi-online RL）的新范式，旨在解决传统离线和在线强化学习在GUI代理任务中的局限性。该方法在预收集的轨迹上模拟在线RL，通过保留多轮对话中的原始模型输出，并利用补丁模块恢复轨迹偏差。同时引入折扣未来回报以捕捉长期训练信号，并优化策略。实验表明，该方法在多个动态基准测试中表现优异，显著提升了多步骤任务执行能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11543" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 14 Sep 2025 23:24:08 GMT</pubDate>
</item>
<item>
<title>动态奖励加权在多目标强化学习中的应用</title>
<link>https://arxiv.org/abs/2509.11452</link>
<guid>https://arxiv.org/abs/2509.11452</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态奖励加权提升多目标强化学习性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对多目标强化学习中传统线性奖励标量化方法无法捕捉非凸帕累托前沿的问题，提出动态奖励加权方法。该方法在在线强化学习过程中自适应调整奖励权重，克服了固定权重方案的局限性。文中介绍了两种增强方法：基于超体积引导的权重调整和基于梯度的权重优化，展示了其在多种数学推理数据集上的有效性，并证明其在不同模型家族中均能实现更优的帕累托前沿解，且所需训练步骤更少。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11452" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 14 Sep 2025 17:56:35 GMT</pubDate>
</item>
<item>
<title>CognitiveSky：基于去中心化社交平台的实时话语分析框架</title>
<link>https://arxiv.org/abs/2509.11444</link>
<guid>https://arxiv.org/abs/2509.11444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CognitiveSky用于分析去中心化社交平台上的情感与叙事。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CognitiveSky，一个开源且可扩展的框架，用于在去中心化社交平台Bluesky上进行情感、情绪和叙事分析。通过接入Bluesky的API，CognitiveSky利用基于Transformer的模型对用户生成内容进行标注，并生成结构化的分析结果。这些结果被用于动态仪表板，可视化情绪、活动和话题的变化趋势。CognitiveSky完全基于免费基础设施构建，具有低成本和高可访问性。虽然主要用于监测心理健康相关讨论，但其模块化设计也适用于虚假信息检测、危机响应和公民情绪分析等领域。该框架为计算社会科学提供了一个透明、可扩展的工具，适应数字生态系统的演变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 14 Sep 2025 17:37:24 GMT</pubDate>
</item>
<item>
<title>PersonaX：多模态数据集推动行为特质分析与因果推理</title>
<link>https://arxiv.org/abs/2509.11362</link>
<guid>https://arxiv.org/abs/2509.11362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PersonaX结合多模态数据，提升行为特质分析效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了PersonaX，一个整合了行为描述、面部属性和生平信息的多模态数据集，包含CelebPersona和AthlePersona两个部分。该数据集通过大型语言模型推断行为特质，并利用统计独立性测试和因果表示学习框架进行分析。实验表明，PersonaX在合成和真实数据上均表现出色，为多模态行为特质研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.11362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 14 Sep 2025 13:30:03 GMT</pubDate>
</item>
<item>
<title>基于领域重要性的模型剪枝方法GAPrune</title>
<link>https://arxiv.org/abs/2509.10844</link>
<guid>https://arxiv.org/abs/2509.10844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GAPrune提升领域特定模型压缩性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出GAPrune，一种结合领域重要性和通用语言基础的模型剪枝框架。通过Fisher信息和通用-领域梯度对齐评估参数重要性，并利用DAI评分进行剪枝决策。实验表明，在50%稀疏度下，GAPrune在FinMTEB和ChemTEB两个领域基准上保持接近密集模型的性能，且在100步微调后分别提升4.51%和1.73%，证明其有效提升领域特定能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 11:03:37 GMT</pubDate>
</item>
<item>
<title>InternScenes：构建大规模可模拟的室内场景数据集</title>
<link>https://arxiv.org/abs/2509.10813</link>
<guid>https://arxiv.org/abs/2509.10813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InternScenes提供40,000个多样化室内场景，提升AI训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了InternScenes，一个包含约40,000个多样化室内场景的大规模可模拟3D数据集。该数据集整合了真实扫描、程序生成和设计师创建的场景，涵盖15种常见场景类型和288种物体类别，总计196万3D物体。InternScenes特别保留了大量小型物品，使场景布局更加真实复杂，平均每个区域有41.5个物体。通过数据处理流程，确保了场景的可模拟性、交互性和无碰撞性。文章展示了该数据集在场景布局生成和点目标导航两个基准任务中的应用，证明其能推动AI模型在复杂场景中的训练与应用。项目将开源数据、模型和基准测试以促进社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 13 Sep 2025 10:25:17 GMT</pubDate>
</item>
<item>
<title>HumbleBench：评估多模态大语言模型拒绝错误选项的能力</title>
<link>https://arxiv.org/abs/2509.09658</link>
<guid>https://arxiv.org/abs/2509.09658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HumbleBench测试MLLM识别错误答案的能力，提升AI可靠性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了HumbleBench，这是一个用于评估多模态大语言模型（MLLMs）在面对错误选项时能否正确拒绝的基准测试。该基准针对三种类型的幻觉——对象、关系和属性，通过细粒度场景图数据集构建，并生成包含“以上都不是”选项的多项选择题。研究团队评估了多种先进的MLLM，并发现该基准能够更真实地衡量模型在安全关键场景中的可靠性。HumbleBench填补了现有评估体系的空白，有助于提升AI系统的可信度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:54:00 GMT</pubDate>
</item>
<item>
<title>Probabilistic Structure Integration：构建可控世界模型的新方法</title>
<link>https://arxiv.org/abs/2509.09737</link>
<guid>https://arxiv.org/abs/2509.09737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PSI系统通过三步循环提升世界模型的可控性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Probabilistic Structure Integration (PSI)，一种从数据中学习丰富可控且灵活提示的世界模型的系统。PSI包含三个步骤：第一步是概率预测，构建一个基于随机访问自回归序列模型的图模型；第二步是结构提取，通过因果推理零样本提取数据中的低维属性；第三步是整合，将这些结构转化为新的标记类型并重新融入训练中。该系统在1.4万亿个互联网视频数据上进行训练，实现了视频预测、理解、光流估计、自监督深度和物体分割等任务，并提升了模型的预测能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 14:01:04 GMT</pubDate>
</item>
<item>
<title>大型语言模型在社会科学中的潜在风险与验证方法</title>
<link>https://arxiv.org/abs/2509.08825</link>
<guid>https://arxiv.org/abs/2509.08825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM输出易受研究者选择影响，可能导致统计错误。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLMs）在社会科学研究中的应用及其带来的潜在风险。由于模型选择、提示策略等因素的不同，LLM的输出可能引入系统性偏差和随机误差，导致统计结论错误。通过复制37个数据标注任务并测试2,361个假设，研究发现约三分之一的假设在使用先进模型时得出错误结论，而小型模型则有一半出现错误。尽管高精度模型能降低风险，但无法完全消除。研究还指出，人类标注对减少假阳性结果至关重要，而常见的回归校正方法效果有限。此外，文章揭示了有意操控LLM输出的可行性，强调了对统计显著性结论进行严格验证的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 13:58:53 GMT</pubDate>
</item>
<item>
<title>Visual-TableQA：面向结构化数据的多模态视觉推理数据集</title>
<link>https://arxiv.org/abs/2509.07966</link>
<guid>https://arxiv.org/abs/2509.07966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Visual-TableQA，提升表格图像的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Visual-TableQA，这是一个大规模、开放域的多模态数据集，旨在评估和增强对复杂表格数据的视觉推理能力。该数据集包含2.5k个结构化的LaTeX渲染表格和6k个高推理强度的问答对，生成成本低于100美元。其生成流程采用模块化、可扩展的自主机制，通过多个语言模型协作完成生成、验证和灵感激发任务。实验表明，基于该数据集微调的模型在外部基准测试中表现优异，优于一些专有模型。数据集和相关资源已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 13:52:26 GMT</pubDate>
</item>
<item>
<title>基于Rescorla-Wagner模型的LLM上下文处理与安全优化研究</title>
<link>https://arxiv.org/abs/2509.04500</link>
<guid>https://arxiv.org/abs/2509.04500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM易受不相关内容影响，RW-Steering提升响应质量39.8%</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在处理混合上下文时的行为模式，发现其倾向于吸收较少出现的信息，这在现实场景中可能引发可靠性问题。研究引入了Poisoned Context Testbed测试平台，并借鉴神经科学中的Rescorla-Wagner模型分析上下文信号的影响。实验表明，LLM对少量不当内容高度敏感，从而降低输出质量。为解决此问题，作者提出RW-Steering方法，通过两阶段微调使模型识别并忽略不当信息，在多种情境下均表现出良好的泛化能力。该方法在实际应用中显著提升了模型的安全性和响应质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 20:40:34 GMT</pubDate>
</item>
<item>
<title>基于因果知识的大型语言模型优化方法研究</title>
<link>https://arxiv.org/abs/2509.01535</link>
<guid>https://arxiv.org/abs/2509.01535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出CAT方法提升LLM在因果推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）是否能有效利用因果知识进行预测和生成。研究发现，LLMs在大规模数据训练中往往捕捉到虚假相关性而非真实因果关系，导致在分布外场景下表现不佳。为解决这一问题，作者提出Causal Attention Tuning（CAT）方法，通过注入细粒度因果知识改进注意力机制，并设计了自动化流程生成因果信号。实验结果表明，该方法在多个任务中表现出色，增强了模型在分布外场景下的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 11:13:15 GMT</pubDate>
</item>
<item>
<title>DeMeVa团队在LeWiDi 2025中的方法研究</title>
<link>https://arxiv.org/abs/2509.09524</link>
<guid>https://arxiv.org/abs/2509.09524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了ICL与LDL方法在标注预测中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeMeVa团队在LeWiDi 2025共享任务中的方法，重点研究了两种方向：基于大语言模型的上下文学习（ICL）和基于RoBERTa的标签分布学习（LDL）。在ICL部分，比较了不同的示例采样策略；在LDL部分，评估了多种微调方法。研究结果显示，ICL可以有效预测注释者特定的标注，并通过聚合生成软标签获得良好性能；同时，LDL方法在软标签预测方面表现出潜力，值得进一步探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 11:04:42 GMT</pubDate>
</item>
<item>
<title>基于修复能力的强化学习框架提升掩码扩散语言模型性能</title>
<link>https://arxiv.org/abs/2509.10396</link>
<guid>https://arxiv.org/abs/2509.10396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用修复能力优化扩散语言模型的强化学习算法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于修复能力的强化学习框架IGPO，用于提升掩码扩散语言模型（dLLMs）的性能。该框架通过在在线采样过程中插入部分真实推理轨迹，引导探索过程，从而提高样本效率并恢复有意义的梯度。研究还引入了对合成简短轨迹的监督微调，结合熵过滤等技术，在数学基准测试中取得了显著提升，达到了当前最优结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 12:44:31 GMT</pubDate>
</item>
<item>
<title>构建可调控的AI代理经济体系</title>
<link>https://arxiv.org/abs/2509.10147</link>
<guid>https://arxiv.org/abs/2509.10147</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI代理经济快速发展，需设计可调控机制以保障安全与公平。</p><br /><br /><p><strong>摘要：</strong> 随着自主AI代理的迅速普及，一种新的经济层正在形成，其交易和协调能力远超人类直接监管。文章提出“沙盒经济”框架，从起源（自发或有意）和与传统经济的隔离程度（渗透性或非渗透性）两个维度进行分析。当前趋势表明，AI代理经济将高度渗透，带来前所未有的协作机会，但也伴随着系统性风险和不平等加剧的问题。文章探讨了可能的设计方案，如拍卖机制实现资源公平分配、AI“使命经济”促进集体目标达成，以及建立社会技术基础设施以确保信任与责任。作者主张积极设计可控的AI代理市场，以确保技术变革符合人类长期福祉。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10147" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 07:20:11 GMT</pubDate>
</item>
<item>
<title>中国少数民族语言新闻标题生成数据集CMHG发布</title>
<link>https://arxiv.org/abs/2509.09990</link>
<guid>https://arxiv.org/abs/2509.09990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CMHG数据集助力少数民族语言新闻标题生成研究。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了针对中国少数民族语言（如藏语、维吾尔语和蒙古语）的新闻标题生成任务，提出了一种名为中国少数民族新闻标题生成（CMHG）的新数据集。该数据集包含10万条藏语条目以及各5万条维吾尔语和蒙古语条目，并配有由母语者标注的高质量测试集，旨在推动相关领域的研究并建立基准。作者希望该数据集能促进少数民族语言新闻标题生成技术的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 02:18:44 GMT</pubDate>
</item>
<item>
<title>基于基础模型微调的长尾半监督学习方法研究</title>
<link>https://arxiv.org/abs/2509.09926</link>
<guid>https://arxiv.org/abs/2509.09926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LoFT框架提升长尾数据半监督学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对长尾数据分布下的半监督学习问题，提出了一种基于基础模型微调的新框架LoFT，通过引入大量未标记数据提升模型性能。与以往从头训练的方法不同，LoFT利用预训练模型生成更可靠的伪标签，从而改善类别不平衡问题。此外，还提出了LoFT-OW，在开放世界条件下处理分布外样本，增强模型的判别能力。实验结果表明，该方法在多个基准数据集上表现优于现有方法，即使仅使用1%的未标记数据也能取得优异效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 22:28:32 GMT</pubDate>
</item>
<item>
<title>X-Part：可控的3D形状部件生成模型</title>
<link>https://arxiv.org/abs/2509.08643</link>
<guid>https://arxiv.org/abs/2509.08643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Part实现高保真3D形状部件生成与分解。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Part，一种可控的生成模型，能够将3D物体分解为语义明确且结构一致的部件，提升3D打印、UV映射等应用的效果。该模型利用边界框作为提示，并注入点级语义特征以实现有意义的分解。此外，还设计了可交互的部件生成流程。实验表明，X-Part在部件级3D形状生成任务中表现优异，为创建可编辑、结构稳固的3D资产提供了新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 10:37:02 GMT</pubDate>
</item>
<item>
<title>基于对话的第二语言学习兴趣研究与IntrEx数据集构建</title>
<link>https://arxiv.org/abs/2509.06652</link>
<guid>https://arxiv.org/abs/2509.06652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨教育对话中的兴趣驱动因素及IntrEx数据集的应用。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于第二语言习得中学习者参与度与动机的重要性，指出当前对教育对话中引发兴趣的语言特征了解不足。为填补这一空白，研究团队构建了IntrEx数据集，该数据集基于教师-学生聊天语料库，并引入序列级标注以分析兴趣在对话中的演变。通过100多名二语学习者的参与，采用类似人类反馈强化学习的方法进行标注。研究还发现，经过兴趣评分微调的大型语言模型（7B/8B参数）在预测人类兴趣判断方面优于GPT-4o等大模型。最后，文章分析了具体性、可理解性及回应等因素如何影响教育对话中的参与度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 09:07:35 GMT</pubDate>
</item>
<item>
<title>HANRAG：提升多跳问答任务的检索增强生成框架</title>
<link>https://arxiv.org/abs/2509.09713</link>
<guid>https://arxiv.org/abs/2509.09713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HANRAG优化多跳查询处理，提升问答系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出HANRAG，一种基于启发式的检索增强生成框架，旨在解决现有RAG方法在处理多跳查询时存在的检索效率低、噪声积累等问题。该框架通过分解查询、过滤噪声，提高了系统的适应性和抗噪能力。实验表明，HANRAG在单跳和多跳问答任务中均表现出色，优于现有主流方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 02:22:38 GMT</pubDate>
</item>
<item>
<title>高效视觉-语言-动作策略FLOWER的开发与应用</title>
<link>https://arxiv.org/abs/2509.04996</link>
<guid>https://arxiv.org/abs/2509.04996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLOWER通过优化模型结构实现高效VLA策略，性能优于大型模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种高效的视觉-语言-动作（VLA）策略FLOWER，旨在解决现有方法计算成本高、资源需求大的问题。FLOWER通过中间模态融合和动作特定的全局AdaLN条件化技术，分别减少了50%的LLM层和20%的参数量。该模型仅需950 M参数，在200 H100 GPU小时的预训练下，表现优于多个大型VLA模型，在190个任务中取得优异成绩，并在CALVIN ABC基准上达到新的SOTA（4.53）。相关代码和权重已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 06:43:12 GMT</pubDate>
</item>
<item>
<title>基于固定潜在空间的任意分辨率图像生成方法</title>
<link>https://arxiv.org/abs/2509.10441</link>
<guid>https://arxiv.org/abs/2509.10441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InfGen模型实现快速高分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为InfGen的新方法，通过将扩散模型生成的固定潜在空间作为内容表示，并使用单步生成器解码任意分辨率图像。该方法无需重新训练扩散模型，显著降低了计算复杂度，使4K图像生成时间从100秒以上缩短至10秒以内，适用于所有使用相同潜在空间的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 13:48:57 GMT</pubDate>
</item>
<item>
<title>无需训练的文本到图像颜色对齐方法</title>
<link>https://arxiv.org/abs/2509.10058</link>
<guid>https://arxiv.org/abs/2509.10058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的颜色对齐框架，提升文本到图像生成的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对文本到图像生成中颜色对齐不足的问题，提出了一种无需训练的框架。该方法利用大语言模型解析文本中的模糊颜色描述，并在CIELAB颜色空间中根据颜色的空间关系优化文本嵌入。与以往依赖交叉注意力、参考图像或微调的方法不同，该方法直接在文本嵌入空间中引导颜色混合，提高了颜色准确性和与文本语义的一致性，同时保持了图像质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.10058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 04:44:22 GMT</pubDate>
</item>
<item>
<title>QuantAgent：面向高频交易的多智能体语言模型框架</title>
<link>https://arxiv.org/abs/2509.09995</link>
<guid>https://arxiv.org/abs/2509.09995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QuantAgent在高频交易中表现优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了QuantAgent，一个专为高频交易设计的多智能体语言模型框架。该系统将交易任务分解为四个专业代理：指标、模式、趋势和风险，每个代理都具备领域特定工具和结构化推理能力，以捕捉短期市场动态。在十个金融工具上的零样本评估显示，QuantAgent在预测准确性和累计回报方面均优于神经网络和基于规则的基线模型，展示了结合结构化金融先验与语言模型推理在实时高频交易中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 12 Sep 2025 02:35:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型的持续扩展是否带来边际收益递减？</title>
<link>https://arxiv.org/abs/2509.09677</link>
<guid>https://arxiv.org/abs/2509.09677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">模型规模扩大能显著提升任务执行长度，但存在自我强化错误现象。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）持续扩展是否带来边际收益递减的问题。研究发现，虽然单步准确率的微小提升可能带来任务执行长度的指数级增长，但随着任务步骤增加，模型的每一步准确率会下降。这种下降不仅源于长上下文限制，还存在一种‘自我条件化’效应，即模型在包含之前错误的上下文中更易出错。相比之下，近期的思维模型在单次操作中能完成更长的任务。文章强调了模型规模和序列推理计算对长期任务的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>MCP-AgentBench：评估语言代理在MCP工具交互中的能力基准</title>
<link>https://arxiv.org/abs/2509.09734</link>
<guid>https://arxiv.org/abs/2509.09734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCP-AgentBench用于评估语言代理在MCP工具交互中的性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MCP-AgentBench，这是一个专门用于评估语言代理在MCP（模型上下文协议）支持的工具交互中能力的全面基准。该基准包含33个运行中的服务器和188种不同的工具，设计了600个系统化查询，涵盖6个不同复杂度的交互类别，并引入了MCP-Eval评估方法，专注于实际任务的成功率。通过广泛的实验评估，MCP-AgentBench旨在为研究社区提供一个标准化、可靠的框架，以构建和验证能够充分利用MCP优势的智能代理，推动真正具备互操作性的AI系统发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 10:08:40 GMT</pubDate>
</item>
<item>
<title>语音语言模型的语音风格适应研究</title>
<link>https://arxiv.org/abs/2509.09716</link>
<guid>https://arxiv.org/abs/2509.09716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究语音语言模型根据指令调整说话风格的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出语音风格适应（VSA）任务，探讨语音语言模型（SLMs）是否能根据自然语言指令调整语调、韵律或角色等说话风格。为此，作者构建了VStyle基准数据集，涵盖中英文四种语音生成类别，并引入LALM as a Judge框架进行客观评估。实验表明当前模型在可控风格适配方面仍存在明显不足，凸显该任务的挑战性与重要性。研究旨在推动更以人为本的语音交互技术发展，相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 10:28:58 GMT</pubDate>
</item>
<item>
<title>工业场景下深度学习漏洞检测技术的应用与评估</title>
<link>https://arxiv.org/abs/2509.09313</link>
<guid>https://arxiv.org/abs/2509.09313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了CodeBERT在工业软件中的漏洞检测效果及应用。</p><br /><br /><p><strong>摘要：</strong> 本文评估了CodeBERT在工业和开源软件中检测漏洞的性能，分析了其跨领域泛化能力，并探索了处理类别不平衡的策略。基于研究结果，开发了一个集成到CI/CD流程中的推荐系统AI-DO，用于在代码审查中检测和定位漏洞。通过调查评估了该工具的实用性，结果显示工业数据训练的模型在本领域表现良好，而开源数据微调的模型在适当采样下也能有效检测漏洞。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 05:58:43 GMT</pubDate>
</item>
<item>
<title>AU-Harness：提升大音频语言模型评估效率与全面性的框架</title>
<link>https://arxiv.org/abs/2509.08031</link>
<guid>https://arxiv.org/abs/2509.08031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AU-Harness提升大音频模型评估效率与覆盖范围。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AU-Harness，一个高效且全面的大型音频语言模型（LALMs）评估框架。该框架通过优化批量处理和并行执行，实现了比现有工具包高127%的速度提升，支持大规模评估。同时，提供标准化提示协议和灵活配置，确保模型在不同场景下的公平比较。此外，新增了LLM-Adaptive Diarization和Spoken Language Reasoning两个评估类别，用于测试时间音频理解和复杂语音推理能力。通过380多个任务的评估，揭示了当前LALMs在时间理解与复杂语音推理方面的显著不足，并指出音频基准测试中指令模态缺乏标准化的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08031" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 11:30:40 GMT</pubDate>
</item>
<item>
<title>提升大语言模型的少样本学习能力：MachineLearningLM框架研究</title>
<link>https://arxiv.org/abs/2509.06806</link>
<guid>https://arxiv.org/abs/2509.06806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MachineLearningLM增强LLM的多示例学习能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MachineLearningLM框架，通过持续预训练使大语言模型（LLM）具备强大的基于上下文学习（ICL）能力，同时保留其广泛的知识和推理能力。该框架利用数百万个结构因果模型（SCMs）进行预训练，支持最多1024个示例，并通过优化提示策略提升上下文利用率。实验表明，在金融、物理、生物和医疗等多个领域，MachineLearningLM在无任务特定训练的情况下，达到了随机森林级别的准确率，且在多示例学习中表现出持续的性能提升。此外，其通用聊天能力也得到保持，MMLU测试得分为75.4%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 11:38:31 GMT</pubDate>
</item>
<item>
<title>Ego3D-Bench与Ego3D-VLM提升视觉语言模型的三维空间推理能力</title>
<link>https://arxiv.org/abs/2509.06266</link>
<guid>https://arxiv.org/abs/2509.06266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Ego3D-Bench和Ego3D-VLM以提升VLM的三维空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前视觉语言模型（VLMs）在三维空间关系理解上的不足，提出了Ego3D-Bench基准测试集，利用第一视角多视图户外数据评估VLM的空间推理能力。该基准包含8600个QA对，由人工标注确保质量与多样性。实验表明，现有SOTA VLM在空间理解上仍远低于人类水平。为此，作者提出Ego3D-VLM框架，通过生成基于全局3D坐标的认知地图，显著提升了VLM在多选题和绝对距离估计任务上的表现。Ego3D-VLM具有模块化设计，可与任何现有VLM结合使用，为实现真实场景下的类人空间理解提供了重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 07 Sep 2025 21:08:41 GMT</pubDate>
</item>
<item>
<title>基于物体相对控制的视觉导航方法研究</title>
<link>https://arxiv.org/abs/2509.09594</link>
<guid>https://arxiv.org/abs/2509.09594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于物体相对控制的新导航方法，提升跨场景适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于物体相对控制的视觉导航方法，旨在克服传统图像相对方法在姿态和体征依赖上的局限。通过构建相对3D场景图作为拓扑地图，实现更鲁棒的全局路径规划。同时设计了一个名为ObjectReact的局部控制器，直接基于高阶WayObject Costmap进行决策，无需RGB输入。实验表明，该方法在不同传感器高度和逆向导航任务中表现优异，并能在仿真环境中良好泛化到真实室内场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 12:34:17 GMT</pubDate>
</item>
<item>
<title>面向全景牙科X光片的多模态大模型研究与应用</title>
<link>https://arxiv.org/abs/2509.09254</link>
<guid>https://arxiv.org/abs/2509.09254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出MMOral数据集和OralGPT模型提升牙科影像分析能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对牙科影像分析中全景X光片的挑战，提出了首个专门用于该领域的多模态指令数据集MMOral，包含20,563张标注图像和130万条任务实例。同时构建了MMOral-Bench评估体系，用于全面评估模型在五个关键诊断维度上的表现。实验发现，即使最先进的GPT-4o模型在该任务上仅达到41.45%的准确率，显示出当前模型在该领域的不足。为此，作者基于Qwen2.5-VL-7B提出了OralGPT模型，并通过微调显著提升了性能，单次微调后性能提升了24.73%。研究为智能牙科AI系统的发展提供了重要基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 04:39:08 GMT</pubDate>
</item>
<item>
<title>MambaRec：一种基于注意力引导的多模态推荐框架</title>
<link>https://arxiv.org/abs/2509.09114</link>
<guid>https://arxiv.org/abs/2509.09114</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MambaRec提升多模态推荐的融合质量与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出MambaRec，一种用于多模态推荐的新框架，通过局部特征对齐和全局分布正则化来解决现有方法在跨模态关联建模和分布一致性方面的不足。核心模块DREAM利用多尺度扩张卷积与通道和空间注意力机制，实现视觉与文本模态间的细粒度语义对齐。同时引入最大均值差异和对比损失函数，增强全局语义一致性。该方法还采用降维策略提高计算效率，在真实电商数据集上表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09114" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 22:52:26 GMT</pubDate>
</item>
<item>
<title>基于LLM的自主漏洞发现与修复系统在DARPA竞赛中的应用</title>
<link>https://arxiv.org/abs/2509.07225</link>
<guid>https://arxiv.org/abs/2509.07225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">团队在DARPA竞赛中展示了自主漏洞检测与修复系统。</p><br /><br /><p><strong>摘要：</strong> 该团队在DARPA的人工智能网络挑战赛（AIxCC）中作为七支决赛队伍之一，取得了第四名的好成绩。他们开发了一个名为Cyber Reasoning System（CRS）的自主系统，在真实开源C和Java项目中发现了28个安全漏洞，包括6个未知的零日漏洞，并成功修复了其中14个。该系统已开源，论文详细介绍了其基于大语言模型（LLM）的组件和策略。此外，团队还基于AIxCC数据集创建了一个公开的排行榜，用于评估最先进的LLM在漏洞检测和修复任务上的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 17:08:01 GMT</pubDate>
</item>
<item>
<title>mmBERT：多语言编码器模型的性能提升研究</title>
<link>https://arxiv.org/abs/2509.06888</link>
<guid>https://arxiv.org/abs/2509.06888</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mmBERT在多语言任务中表现优于现有模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了mmBERT，一个基于3T个token训练的多语言编码器模型，涵盖1800多种语言。通过引入逆向掩码比例调度和逆向温度采样比例等创新方法，并在衰减阶段加入1700种低资源语言数据，显著提升了模型性能。尽管仅在短时间内引入低资源语言，mmBERT在分类任务上表现与OpenAI的o3和Google的Gemini 2.5 Pro相当，且在高、低资源语言任务中均优于前代模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06888" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:08:42 GMT</pubDate>
</item>
<item>
<title>基于自编码器框架的统一多模态学习方法研究</title>
<link>https://arxiv.org/abs/2509.09666</link>
<guid>https://arxiv.org/abs/2509.09666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UAE框架实现图像与文本双向信息流统一。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于自编码器的统一多模态学习框架（UAE），将图像理解视为编码器（I2T），将图像生成视为解码器（T2I）。通过重建保真度作为统一训练目标，实现理解和生成之间的双向信息流动。该框架包含三个阶段：冷启动阶段使用语义重建损失初始化模型；生成阶段优化编码器以生成有助于解码器重建的描述；理解阶段提升解码器对复杂描述的理解能力。为评估模型效果，引入Unified-Bench基准测试。实验发现，随着强化学习的推进，编码器生成更丰富的描述，解码器也展现出更强的理解和生成能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:57:59 GMT</pubDate>
</item>
<item>
<title>VLA-Adapter：高效连接视觉语言与动作空间的轻量级方法</title>
<link>https://arxiv.org/abs/2509.09372</link>
<guid>https://arxiv.org/abs/2509.09372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLA-Adapter通过轻量策略提升视觉语言到动作的映射性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出VLA-Adapter，一种降低对大规模视觉语言模型依赖的新型方法。通过分析不同视觉语言条件的有效性，作者设计了一个带有桥接注意力机制的轻量策略模块，使模型在不使用机器人数据预训练的情况下仍能实现高性能。实验表明，该方法在模拟和真实机器人基准测试中表现优异，且仅需单块消费级GPU即可在8小时内完成训练，显著降低了部署门槛。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09372" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 07:42:21 GMT</pubDate>
</item>
<item>
<title>HuMo：统一的人类视频生成框架</title>
<link>https://arxiv.org/abs/2509.08519</link>
<guid>https://arxiv.org/abs/2509.08519</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HuMo解决多模态视频生成中的数据和任务协调问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出HuMo，一个统一的人类视频生成框架，旨在有效协调文本、图像和音频等多模态输入。针对训练数据不足和任务协作困难的问题，作者构建了一个高质量的多模态数据集，并设计了两阶段渐进式训练策略。在主体保持任务中采用最小侵入式图像注入策略，在音画同步任务中引入预测引导策略。此外，还提出了一种时间自适应的无分类器指导方法，以实现更精细的多模态控制。实验结果表明，HuMo在多个子任务上优于现有方法，为多模态条件下的视频生成提供了统一解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08519" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 07:54:29 GMT</pubDate>
</item>
<item>
<title>分解推理中毒攻击与大语言模型的后门鲁棒性</title>
<link>https://arxiv.org/abs/2509.05739</link>
<guid>https://arxiv.org/abs/2509.05739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究分解推理中毒攻击及LLM的后门鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了针对大型语言模型（LLMs）的数据中毒攻击，特别是通过分解推理路径进行的隐蔽攻击。这种攻击方式仅修改推理过程，保持提示和最终答案干净，并将触发器分散到多个无害组件中。尽管可以注入此类分解毒药，但可靠地激活它们以改变最终答案却异常困难。这是因为模型能够从推理过程中被激活的后门中恢复。这表明，先进LLMs的推理能力和推理与最终答案生成之间的架构分离可能正在产生一种新兴的后门鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 11:06:18 GMT</pubDate>
</item>
<item>
<title>基于高斯点云的图像修复方法研究</title>
<link>https://arxiv.org/abs/2509.01964</link>
<guid>https://arxiv.org/abs/2509.01964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于高斯点云的图像修复框架，提升修复质量与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探索了高斯点云（Gaussian Splatting）在图像修复中的应用潜力。作者提出了首个基于2D高斯点云的图像修复框架，通过将不完整图像编码为连续的高斯点云系数，并利用可微分光栅化过程进行重建。该方法在像素级连贯性方面表现优异，同时引入分块光栅化策略以提高效率和可扩展性。此外，结合预训练DINO模型的全局特征，增强了修复结果的语义一致性，确保修复内容与周围场景相符。实验表明，该方法在多个标准数据集上均取得良好效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 01:12:52 GMT</pubDate>
</item>
<item>
<title>FLUX-Reason-6M与PRISM-Bench推动开放源代码文本到图像生成模型发展</title>
<link>https://arxiv.org/abs/2509.09680</link>
<guid>https://arxiv.org/abs/2509.09680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLUX-Reason-6M与PRISM-Bench提升开放源代码文本到图像生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对开放源代码文本到图像生成模型缺乏大规模、推理导向数据集和评估基准的问题，提出了FLUX-Reason-6M数据集和PRISM-Bench评估基准。FLUX-Reason-6M包含600万张高质量图像及2000万条中英文描述，用于训练复杂推理能力，其图像按六种关键特征分类，并引入生成链式思维（GCoT）以细化生成过程。该数据集的构建耗时15,000 A100 GPU天。PRISM-Bench提供七个评估赛道，包括基于GCoT的长文本挑战，通过先进视觉语言模型进行细致的人类对齐评估。对19个领先模型的评估揭示了性能差距，并指出了改进方向。相关资源已公开，旨在推动推理导向文本到图像生成的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>SpatialVID：大规模动态视频数据集推动空间智能发展</title>
<link>https://arxiv.org/abs/2509.09676</link>
<guid>https://arxiv.org/abs/2509.09676</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialVID提升空间智能模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SpatialVID数据集，该数据集包含超过21,000小时的原始视频，并通过筛选处理生成7,089小时的动态内容。数据集提供丰富的3D标注信息，包括每帧相机姿态、深度图、动态掩码等，有助于提升空间重建和世界探索模型的性能与泛化能力。分析表明，SpatialVID具有高度多样性和丰富性，为视频与3D视觉研究提供了重要资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09676" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>基于强化学习的视觉-语言-动作模型优化研究</title>
<link>https://arxiv.org/abs/2509.09674</link>
<guid>https://arxiv.org/abs/2509.09674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升VLA模型长期任务规划能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出SimpleVLA-RL，一种针对视觉-语言-动作（VLA）模型的高效强化学习框架。通过改进轨迹采样、并行化和损失计算等方法，SimpleVLA-RL在LIBERO数据集上达到最先进性能，并在RoboTwin任务中超越现有方法。该框架减少了对大规模数据的依赖，提升了模型在现实任务中的泛化能力。研究还发现了一种新的‘pushcut’现象，表明策略在训练过程中能发现未见过的新模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>LoCoBench：评估长上下文语言模型在软件开发中的基准测试</title>
<link>https://arxiv.org/abs/2509.09614</link>
<guid>https://arxiv.org/abs/2509.09614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoCoBench用于评估长上下文语言模型在复杂软件开发中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了LoCoBench，这是一个专门用于评估长上下文语言模型（LLMs）在真实、复杂的软件开发场景中的性能的基准测试。与以往专注于单函数完成或短上下文任务的代码评估基准不同，LoCoBench填补了对长上下文能力评估的空白，强调理解整个代码库、跨文件推理和保持架构一致性。该基准包含8000个跨10种编程语言的评估场景，上下文长度从1万到100万个标记不等，涵盖8类任务，如架构理解、跨文件重构、多会话开发等。文章还提出了一套包含17项指标的评估框架，并指出当前最先进的长上下文模型仍存在显著性能差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 12:55:04 GMT</pubDate>
</item>
<item>
<title>Kling-Avatar：基于多模态指令的高保真语音驱动虚拟人生成框架</title>
<link>https://arxiv.org/abs/2509.09595</link>
<guid>https://arxiv.org/abs/2509.09595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kling-Avatar通过多模态指令理解生成高质量虚拟人视频。</p><br /><br /><p><strong>摘要：</strong> 本文提出Kling-Avatar，一种结合多模态指令理解和高保真肖像生成的级联框架。该方法分为两个阶段：第一阶段使用多模态大语言模型生成包含角色动作和情感的蓝图视频；第二阶段根据关键帧并行生成子片段，确保细节保留与指令意图一致。实验表明，Kling-Avatar能够生成1080p、48fps的高质量视频，在唇形同步、情绪表达、指令控制等方面表现优异，适用于数字人直播等实际场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 12:34:57 GMT</pubDate>
</item>
<item>
<title>OmniEVA：提升多模态大语言模型在具身智能中的适应性与规划能力</title>
<link>https://arxiv.org/abs/2509.09332</link>
<guid>https://arxiv.org/abs/2509.09332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniEVA通过两项创新提升具身智能任务的适应性和可行性。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniEVA，一种具身通用规划器，旨在解决当前多模态大语言模型（MLLM）在具身智能系统中面临的两大挑战：几何适应性差距和具身约束差距。OmniEVA引入了任务自适应的3D定位机制和具身感知推理框架，使模型能够根据任务需求动态调整3D信息融合，并将任务目标与机器人物理限制结合进行决策。实验表明，OmniEVA在多种具身任务中表现出色，具备强大的泛化能力和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 06:32:22 GMT</pubDate>
</item>
<item>
<title>基于代码思维的图表理解方法提升视觉语言模型推理能力</title>
<link>https://arxiv.org/abs/2509.09286</link>
<guid>https://arxiv.org/abs/2509.09286</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Code-as-Thought方法提升图表理解效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言模型在图表理解中的推理能力不足问题，提出了一种名为Code-as-Thought（CaT）的方法。该方法通过将图表信息转化为可验证的符号化代码形式，增强模型推理的准确性与可解释性。研究还引入了Visual Programmability概念，使模型能够根据任务需求动态选择使用代码推理或直接视觉分析。通过强化学习训练，模型在数据准确性和决策策略上均取得显著提升，实验表明该方法在多个图表理解基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09286" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 05:22:16 GMT</pubDate>
</item>
<item>
<title>基于熵调节的策略梯度方法提升长期任务性能</title>
<link>https://arxiv.org/abs/2509.09265</link>
<guid>https://arxiv.org/abs/2509.09265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出EMPG方法，提升LLM在长期任务中的学习效率。</p><br /><br /><p><strong>摘要：</strong> 在长期任务中，基于大语言模型的智能体面临稀疏奖励难以评估中间步骤的问题。传统方法通过密集奖励信号引导学习，但效果有限。本文指出LLM学习过程中策略梯度与熵存在耦合问题，导致对确定性正确动作更新不足、不确定性步骤更新不稳定。为此，作者提出熵调节策略梯度（EMPG），通过步骤不确定性与最终任务结果重新校准学习信号，增强确定性正确动作的更新，惩罚确定性错误，并抑制不确定步骤的更新以稳定探索。此外，引入未来清晰度奖励鼓励寻找更可预测的解决方案路径。实验表明，EMPG在WebShop、ALFWorld和Deep Search等任务中表现优异，显著优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 04:50:01 GMT</pubDate>
</item>
<item>
<title>EchoX：提升语音大语言模型知识与推理能力的新方法</title>
<link>https://arxiv.org/abs/2509.09174</link>
<guid>https://arxiv.org/abs/2509.09174</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EchoX通过语义表示提升语音大模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了EchoX，一种针对语音大语言模型（SLLMs）的改进方法。由于当前训练范式未能有效弥合声学与语义之间的差距，导致SLLMs在知识和推理方面表现不佳。EchoX通过利用语义表示并动态生成语音训练目标，结合声学与语义学习，提升了模型的推理能力。实验表明，在约六千小时数据训练下，EchoX在多个基于知识的问答基准测试中表现优异。项目代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09174" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 11 Sep 2025 02:17:59 GMT</pubDate>
</item>
<item>
<title>基于CLIP的行人表征学习改进方法研究</title>
<link>https://arxiv.org/abs/2509.09118</link>
<guid>https://arxiv.org/abs/2509.09118</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GA-DMS框架提升CLIP在行人表征任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对CLIP在行人表征学习中的两个关键挑战——缺乏大规模标注数据和全局对比学习难以保持细粒度特征——提出了改进方案。首先，构建了WebPerson数据集，利用大语言模型自动筛选并生成高质量图像-文本对。其次，设计了GA-DMS框架，通过梯度注意力引导的双掩码机制增强跨模态对齐，并引入掩码文本预测目标以提升细粒度语义表示。实验表明，该方法在多个基准测试中均达到最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.09118" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 23:06:22 GMT</pubDate>
</item>
<item>
<title>通过Divergence项提升RLVR中的模型多样性与性能</title>
<link>https://arxiv.org/abs/2509.07430</link>
<guid>https://arxiv.org/abs/2509.07430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DPH-RL利用f散度保持模型多样性，提升Pass@k和Pass@1性能。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了在使用强化学习与可验证奖励（RLVR）微调大语言模型时，多尝试成功率（Pass@k）下降的问题。作者指出，标准RLVR目标缺乏知识保留机制，导致模型遗忘已有技能。他们提出DPH-RL框架，采用质量覆盖的f散度（如前向KL和JS散度）作为重放机制，通过持续参考初始策略保持广泛解空间。实验表明，DPH-RL不仅解决Pass@k下降问题，还提升了Pass@1和跨域性能，并且训练更高效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 02:34:32 GMT</pubDate>
</item>
<item>
<title>统计方法在生成式人工智能中的应用与展望</title>
<link>https://arxiv.org/abs/2509.07054</link>
<guid>https://arxiv.org/abs/2509.07054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">统计方法提升生成式AI的可靠性与评估效率。</p><br /><br /><p><strong>摘要：</strong> 本文综述了统计方法在生成式人工智能中的应用，包括提高其可靠性、安全性、公平性以及优化AI评估和实验设计。文章介绍了常用的统计技术，并探讨了其在生成式AI中的实际应用，同时指出了当前研究的局限性和未来发展方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:42:59 GMT</pubDate>
</item>
<item>
<title>LLM生成的有毒数据在文本净化任务中的局限性研究</title>
<link>https://arxiv.org/abs/2509.08358</link>
<guid>https://arxiv.org/abs/2509.08358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM生成的有毒数据效果不如人类数据，性能下降达30%。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用大型语言模型（LLMs）生成的合成有毒数据作为文本净化训练数据的可行性。研究使用Llama 3和Qwen模型生成ParaDetox与SST-2数据集中中性文本的有毒版本，并发现基于合成数据微调的模型在联合指标上表现显著低于基于人类数据训练的模型，性能下降最高达30%。研究指出，这一差距源于LLM生成的有毒内容词汇多样性不足，仅使用少量重复的侮辱性词汇，无法捕捉人类毒性表达的复杂性和多样性。该研究揭示了当前LLMs在该领域的局限性，并强调了高质量、多样化的标注数据在构建稳健净化系统中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 03:48:24 GMT</pubDate>
</item>
<item>
<title>通过学习聚合提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2509.06870</link>
<guid>https://arxiv.org/abs/2509.06870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AggLM通过学习聚合多解提升推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的方法AggLM，通过训练聚合模型来整合多个独立生成的解决方案，以提高大语言模型在复杂推理任务中的表现。与传统的多数投票或奖励模型排序不同，AggLM利用强化学习从可验证奖励中学习如何审查、协调并合成最终答案。该方法在训练过程中平衡简单和困难样本，使模型既能识别少数但正确的答案，也能处理多数正确的答案。实验表明，AggLM在多个基准测试中优于现有方法，并能有效泛化到不同模型生成的解决方案，同时显著减少所需token数量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 12:39:38 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理中的应用与挑战</title>
<link>https://arxiv.org/abs/2509.08827</link>
<guid>https://arxiv.org/abs/2509.08827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习推动大语言模型向推理模型演进，面临多方面挑战。</p><br /><br /><p><strong>摘要：</strong> 本文综述了强化学习（RL）在大语言模型（LLMs）推理能力提升中的最新进展。RL在解决数学和编程等复杂逻辑任务中表现出色，已成为将LLMs转化为推理模型（LRMs）的核心方法。随着技术发展，RL在扩展至人工超智能（ASI）过程中面临计算资源、算法设计、训练数据和基础设施等方面的挑战。文章回顾了该领域的发展历程，分析了关键组件、核心问题及应用场景，并探讨了未来研究方向，旨在推动更广泛的推理模型研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title>RewardDance：一种可扩展的视觉生成奖励建模框架</title>
<link>https://arxiv.org/abs/2509.08826</link>
<guid>https://arxiv.org/abs/2509.08826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RewardDance解决视觉生成中奖励模型的可扩展性问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RewardDance的可扩展奖励建模框架，旨在克服现有视觉生成模型中奖励模型（RMs）在规模扩展方面的挑战。传统方法如CLIP-based RMs和Bradley-Terry损失存在架构和输入模态限制，且与VLM的下一词预测机制不匹配，导致难以有效扩展。此外，RLHF优化过程中常出现奖励黑客问题。RewardDance通过将奖励分数重新定义为模型预测“是”标记的概率，使奖励目标与VLM架构内在对齐，从而实现模型规模和上下文规模的双重扩展。实验表明，RewardDance在文本到图像、文本到视频和图像到视频生成任务中均优于现有方法，并有效缓解了奖励黑客问题和模式崩溃现象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>AgentGym-RL：一种用于训练智能代理的强化学习框架</title>
<link>https://arxiv.org/abs/2509.08755</link>
<guid>https://arxiv.org/abs/2509.08755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AgentGym-RL框架提升智能代理决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AgentGym-RL的新框架，旨在通过强化学习训练大型语言模型代理在多轮交互中做出智能决策。该框架具有模块化和解耦架构，支持多种现实场景和主流强化学习算法。作者还提出了ScalingInter-RL方法，平衡探索与利用，提升代理的多样性和稳定性。实验表明，基于该框架的代理在27项任务中表现优于或等于商业模型。研究团队将开源完整框架，助力下一代智能代理的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>人工智能与人类自主性的关系研究</title>
<link>https://arxiv.org/abs/2509.08494</link>
<guid>https://arxiv.org/abs/2509.08494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨AI对人类自主性的影响并提出评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文研究了人工智能在决策过程中对人类自主性的影响，提出了一个名为HumanAgencyBench（HAB）的基准测试工具，用于评估AI系统在六个维度上对人类自主性的支持程度，包括澄清问题、避免价值操控、纠正错误信息等。研究发现当前大型语言模型在这些维度上的表现存在较大差异，且自主性支持并不一定随着模型能力或指令遵循能力的提升而增加，建议关注更稳健的安全与对齐目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 10 Sep 2025 07:10:10 GMT</pubDate>
</item>
<item>
<title>EnvX：通过智能代理提升开源代码库的自动化利用</title>
<link>https://arxiv.org/abs/2509.08088</link>
<guid>https://arxiv.org/abs/2509.08088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EnvX将GitHub仓库转化为智能代理，提升代码复用效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EnvX框架，该框架利用Agentic AI技术将GitHub仓库转变为可自主交互和协作的智能代理。EnvX通过三个阶段实现仓库的智能化：环境初始化、任务执行与代理间协作。相比传统方法，EnvX不仅自动化代码生成，还实现了对仓库功能的理解、初始化和部署。在GitTaskBench基准测试中，EnvX表现出色，任务完成率达74.07%，任务通过率达51.85%。案例研究进一步验证了其在多仓库协作中的能力。该研究推动了开源代码从静态资源向智能互动实体的转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.08088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 14:51:36 GMT</pubDate>
</item>
<item>
<title>基于P3-SAM的3D点提示部件分割方法研究</title>
<link>https://arxiv.org/abs/2509.06784</link>
<guid>https://arxiv.org/abs/2509.06784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出P3-SAM模型实现3D物体自动部件分割，提升分割精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为P3-SAM的3D点提示可交互部件分割模型，旨在实现对任意3D物体的全自动部件分割。该模型受到SAM启发，包含特征提取器、多个分割头和IoU预测器，支持用户交互式分割。此外，还设计了一种算法用于自动选择和合并模型预测的掩码，以实现部件实例分割。模型在包含近370万条带合理分割标签数据的全新数据集上进行训练，实验表明其在复杂物体上的分割精度和鲁棒性均优于现有方法，达到当前最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 11:12:17 GMT</pubDate>
</item>
<item>
<title>多语言翻译模型Hunyuan-MT-7B及其改进版本Hunyuan-MT-Chimera-7B的发布</title>
<link>https://arxiv.org/abs/2509.05209</link>
<guid>https://arxiv.org/abs/2509.05209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多语言翻译模型Hunyuan-MT-7B在多种语言中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Hunyuan-MT-7B，这是首个支持33种主要语言双向翻译的开源多语言翻译模型，特别关注普通话与少数民族语言及方言之间的翻译。同时，文章还提出了Hunyuan-MT-Chimera-7B，该模型通过整合不同参数设置下的输出，提升了翻译性能。模型训练过程包括预训练、监督微调和强化学习对齐等阶段。实验表明，这两个模型在多个语言对上表现优于同类模型，尤其在普通话与少数民族语言翻译任务中表现突出。在WMT2025比赛中，Hunyuan-MT-7B在31个语言对中获得30个第一，展示了其在多种语言环境下的强大适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 12:11:05 GMT</pubDate>
</item>
<item>
<title>3D与4D世界建模的全面综述</title>
<link>https://arxiv.org/abs/2509.07996</link>
<guid>https://arxiv.org/abs/2509.07996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述3D和4D世界建模方法，建立统一定义与分类体系。</p><br /><br /><p><strong>摘要：</strong> 本文是对3D和4D世界建模的首次全面综述，旨在填补该领域缺乏标准化定义和分类的空白。文章提出了精确的定义，并构建了涵盖视频、占用网格和LiDAR三种类型的结构化分类体系。同时，总结了适用于3D/4D场景的数据集和评估指标，探讨了实际应用、现存挑战及未来研究方向，为该领域的进一步发展提供基础参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Transformer模型幻觉机制及其对AI安全的影响</title>
<link>https://arxiv.org/abs/2509.06938</link>
<guid>https://arxiv.org/abs/2509.06938</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Transformer模型在输入不确定时易产生幻觉。</p><br /><br /><p><strong>摘要：</strong> 随着生成式AI在各领域的广泛应用，其故障模式的深入理解变得尤为重要。本文通过实验分析发现，当输入信息变得无结构时，Transformer模型会激活与输入无关但连贯的语义特征，导致幻觉输出。在纯噪声输入下，模型仍能触发多种有意义的概念，且这些概念可通过定向控制验证其功能完整性。研究还表明，模型输出中的幻觉可从层激活中的概念模式中可靠预测。该研究对AI对齐、安全及幻觉风险量化具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06938" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:50:45 GMT</pubDate>
</item>
<item>
<title>复杂检索任务评估与大语言模型影响研究</title>
<link>https://arxiv.org/abs/2509.07253</link>
<guid>https://arxiv.org/abs/2509.07253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">复杂检索任务评估显示LLM增强效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了复杂检索任务的挑战，指出当前检索模型在处理多部分、多约束的自然语言查询时表现不佳。尽管人们期望搜索系统能处理更具体的信息请求，但现有评估资源有限且缺乏现实场景。为此，作者构建了一个多样且真实的复杂检索任务集，并对多个先进检索模型进行了基准测试。同时研究了基于大语言模型的查询扩展和重写对检索质量的影响。结果显示，即使最佳模型在平均nDCG@10和R@100指标上也仅达到0.346和0.587，表明检索模型仍有较大提升空间。此外，虽然LLM增强有助于弱模型，但最强模型在使用重写技术后性能反而下降。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 18:11:10 GMT</pubDate>
</item>
<item>
<title>基于直接对齐和语义相对偏好优化的扩散模型改进方法</title>
<link>https://arxiv.org/abs/2509.06942</link>
<guid>https://arxiv.org/abs/2509.06942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Direct-Align和SRPO提升扩散模型的图像质量与美学表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对扩散模型在对齐人类偏好时存在的计算成本高和依赖离线奖励微调的问题，提出了Direct-Align方法，通过预定义噪声先验实现任意时间步的图像恢复，避免晚期优化过拟合。同时引入语义相对偏好优化（SRPO），将奖励信号与文本条件结合，支持在线调整奖励，减少对离线微调的依赖。实验表明，该方法显著提升了FLUX.1.dev模型的人类评估真实感和美学质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:54:08 GMT</pubDate>
</item>
<item>
<title>基于强化学习的大型语言模型推理机制研究</title>
<link>https://arxiv.org/abs/2509.03646</link>
<guid>https://arxiv.org/abs/2509.03646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL提升LLM推理能力，揭示其分层结构与优化策略。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）如何增强大型语言模型（LLM）的复杂推理能力，并揭示了其成功背后的机制。研究发现，诸如‘顿悟’、‘长度扩展’和熵动态等现象实际上是推理层次结构的体现，类似于人类认知中战略规划与执行步骤的分离。文章提出了一种两阶段动态学习过程：初期注重低级技能的改进，后期转向高级战略规划的探索。现有RL算法如GRPO存在优化压力分散的问题，为此作者提出了HIerarchy-Aware Credit Assignment (HICRA)算法，专注于高影响的规划标记，显著提升了性能。同时，验证了语义熵作为衡量战略探索的更优指标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03646" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 14:52:49 GMT</pubDate>
</item>
<item>
<title>Delta L Normalization：提升RLVR中动态生成长度的损失聚合方法</title>
<link>https://arxiv.org/abs/2509.07558</link>
<guid>https://arxiv.org/abs/2509.07558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Delta L Normalization方法，解决RLVR训练中的梯度方差问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Delta L Normalization的简单而有效的损失聚合方法，旨在应对强化学习与可验证奖励（RLVR）中动态生成长度带来的挑战。由于响应长度在训练过程中变化较大，导致梯度方差高且优化不稳定。尽管已有方法如GRPO、DAPO和Dr. GRPO尝试通过不同的损失归一化项来缓解这一问题，但仍然存在估计偏差或梯度方差高的问题。通过对不同长度对策略损失的影响进行理论和实证分析，作者将问题重新建模为寻找最小方差无偏估计器。实验表明，Delta L Normalization不仅提供无偏的策略损失估计，还在理论上最小化了梯度方差，并在多种模型规模、最大长度和任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 05:52:34 GMT</pubDate>
</item>
<item>
<title>Curia：基于大规模医学影像数据的放射学基础模型</title>
<link>https://arxiv.org/abs/2509.06830</link>
<guid>https://arxiv.org/abs/2509.06830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Curia在多个放射学任务中表现优于或等同于放射科医生。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Curia，一个基于大型医院多年积累的150,000例横断面影像数据（130 TB）训练的放射学基础模型。该模型在19个外部验证任务中表现出色，能够准确识别器官、检测如脑出血和心肌梗死等病变，并在肿瘤分期中预测结果。Curia在跨模态和低数据环境下展现出显著的临床性能，其表现与放射科医生及现有基础模型相当甚至更优。研究团队已公开模型权重以促进进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 12:04:12 GMT</pubDate>
</item>
<item>
<title>Q-Sched：通过调整扩散模型调度器实现高效量化生成</title>
<link>https://arxiv.org/abs/2509.01624</link>
<guid>https://arxiv.org/abs/2509.01624</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Q-Sched提升扩散模型生成质量并减少计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出Q-Sched，一种基于后训练量化的新型方法，通过修改扩散模型的调度器而非模型权重，实现高保真图像生成。该方法在减少模型规模的同时保持全精度准确性，并引入JAQ损失函数，结合文本-图像兼容性和图像质量指标进行优化。实验表明，Q-Sched在多个模型上均取得显著性能提升，且无需全精度校准即可完成量化，适用于资源受限场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01624" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 13:09:22 GMT</pubDate>
</item>
<item>
<title>基于强化学习的并行思维框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2509.07980</link>
<guid>https://arxiv.org/abs/2509.07980</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Parallel-R1通过强化学习实现并行思维，提升模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Parallel-R1，一个基于强化学习的框架，用于在复杂现实任务中实现并行思维。与以往依赖监督微调的方法不同，该框架采用渐进式课程学习，先通过监督微调在简单任务上培养并行思维能力，再通过强化学习在更难任务上进行探索和泛化。实验结果表明，Parallel-R1在多个数学基准测试中表现出色，比直接使用强化学习训练的顺序思维模型提升了8.4%的准确率。进一步分析显示，模型在早期阶段利用并行思维作为探索策略，后期则用于多角度验证。最重要的是，该研究验证了并行思维作为中期训练探索工具的有效性，使模型在RL后达到更高的性能上限，在AIME25上比基线模型提升了42.9%。相关代码和数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07980" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>VIRAL：提升多模态大模型视觉推理能力的对齐策略</title>
<link>https://arxiv.org/abs/2509.07979</link>
<guid>https://arxiv.org/abs/2509.07979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VIRAL增强多模态模型视觉推理能力，提升任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出VIRAL方法，通过将多模态大语言模型的内部视觉表示与预训练视觉基础模型对齐，解决现有模型在视觉密集型任务中的不足。该方法保留输入中的关键视觉细节，并补充额外视觉知识，从而提升模型处理复杂视觉输入的能力。实验表明，VIRAL在多个多模态基准任务中均取得显著改进，并通过消融实验验证了其设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>Mini-o3：提升视觉搜索任务的多轮推理系统</title>
<link>https://arxiv.org/abs/2509.07969</link>
<guid>https://arxiv.org/abs/2509.07969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mini-o3通过多轮推理实现高效视觉搜索。</p><br /><br /><p><strong>摘要：</strong> 本文提出Mini-o3，一个能够进行深度多轮推理的视觉搜索系统，解决了现有开源方法在复杂任务中推理模式单一、交互轮次有限的问题。该系统基于三个关键组件：Visual Probe Dataset数据集、迭代数据收集管道以及过轮遮蔽策略，有效提升了模型在视觉搜索任务中的表现。实验表明，Mini-o3能生成多样化的推理路径，并在推理时自然扩展到更多轮次，显著提高准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 13:54:21 GMT</pubDate>
</item>
<item>
<title>SimpleQA Verified：提升大语言模型事实性评估的基准测试</title>
<link>https://arxiv.org/abs/2509.07968</link>
<guid>https://arxiv.org/abs/2509.07968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SimpleQA Verified优化了LLM事实性评估，提升准确性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SimpleQA Verified，这是一个由1000个提示组成的基准测试，用于评估大型语言模型的简短事实性。该基准测试针对OpenAI的SimpleQA存在的噪声标签、主题偏差和问题重复等问题进行了改进。通过多阶段过滤流程，包括去重、主题平衡和来源核对，确保了数据集的可靠性和挑战性。在该新基准上，Gemini 2.5 Pro取得了55.6的F1分数，表现优于其他先进模型。研究为社区提供了更精确的工具来跟踪模型事实性进展并减少幻觉。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 13:53:58 GMT</pubDate>
</item>
<item>
<title>基于自博弈的大型语言模型强化学习方法</title>
<link>https://arxiv.org/abs/2509.07414</link>
<guid>https://arxiv.org/abs/2509.07414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自博弈提升语言模型性能，无需额外数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于自博弈的强化学习方法，使大型语言模型在无需新增数据的情况下持续提升性能。该方法将模型能力视为竞争游戏中表现，并通过模型与自身对弈（即语言自博弈）来优化策略。实验表明，Llama-3.2-3B-Instruct模型在指令遵循任务中，仅通过自博弈即可显著提升表现，效果优于依赖数据的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 09 Sep 2025 01:51:34 GMT</pubDate>
</item>
<item>
<title>CASTLE：一种改进的因果注意力机制</title>
<link>https://arxiv.org/abs/2509.07301</link>
<guid>https://arxiv.org/abs/2509.07301</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CASTLE通过前瞻键提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为CASTLE的新型因果注意力机制，该机制在标准因果注意力基础上，通过动态更新每个标记的键（keys）来整合后续信息，同时保持自回归特性。这些更新后的键被称为前瞻键，能够在不破坏顺序性的前提下提升模型对上下文的理解能力。尽管机制看似顺序处理，但作者通过数学等价性证明实现了并行训练，提高了效率。实验表明，CASTLE在多个语言建模基准上优于传统因果注意力，有效降低了验证困惑度，并提升了下游任务的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07301" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 20:15:23 GMT</pubDate>
</item>
<item>
<title>Reconstruction Alignment提升统一多模态模型的生成性能</title>
<link>https://arxiv.org/abs/2509.07295</link>
<guid>https://arxiv.org/abs/2509.07295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecA方法提升多模态模型图像生成和编辑质量。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Reconstruction Alignment (RecA)的后训练方法，用于提升统一多模态模型（UMMs）的生成性能。该方法利用视觉理解编码器的嵌入作为密集的“文本提示”，在无需依赖稀疏文本描述的情况下提供丰富的监督信号。通过让模型基于自身的视觉嵌入重建输入图像，RecA实现了理解和生成之间的对齐。实验表明，RecA在多种类型的UMMs中均表现出色，显著提升了图像生成和编辑任务的性能，且仅需少量计算资源即可达到优于大型开源模型的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.07295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 19:59:32 GMT</pubDate>
</item>
<item>
<title>F1：一种融合视觉预见的视觉-语言-动作框架</title>
<link>https://arxiv.org/abs/2509.06951</link>
<guid>https://arxiv.org/abs/2509.06951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">F1通过视觉预见提升动态环境下的任务执行能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出F1，一种基于视觉-语言-动作（VLA）的预训练框架，旨在解决动态视觉环境中执行语言条件任务的挑战。F1通过引入视觉预见生成机制，将未来视觉状态作为显式规划目标，从而将动作生成转化为基于预见的逆动力学问题。该框架采用混合Transformer架构，包含感知、预见生成和控制模块，提升了模型在复杂场景中的鲁棒性和泛化能力。通过在包含330k轨迹的广泛数据集上进行三阶段训练，F1在真实任务和模拟基准测试中均表现出色，显著提升了任务成功率和泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:58:30 GMT</pubDate>
</item>
<item>
<title>动态调整难度的强化学习框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2509.06923</link>
<guid>https://arxiv.org/abs/2509.06923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEELE通过动态调整问题难度提升RLVR效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SEELE的新颖监督辅助强化学习框架，旨在解决传统RLVR方法在探索效率上的不足。该框架通过量化损失下降速度与回放准确率之间的关系，动态调整问题难度以保持在高效区域。SEELE通过为每个训练样本添加提示（部分解决方案）来增强训练数据，并根据模型能力实时调整提示长度。实验表明，SEELE在六个数学推理基准测试中表现优于现有方法，分别提升了11.8和10.5个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:36:21 GMT</pubDate>
</item>
<item>
<title>UMO框架提升多身份图像定制的一致性与可扩展性</title>
<link>https://arxiv.org/abs/2509.06818</link>
<guid>https://arxiv.org/abs/2509.06818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UMO框架提升多身份图像定制的一致性与可扩展性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为UMO的统一多身份优化框架，旨在解决图像定制中多参考图像导致的身份混淆问题。通过“多对多匹配”范式，UMO将多身份生成转化为全局分配优化问题，并利用扩散模型上的强化学习提升多身份一致性。研究还构建了一个包含合成和真实数据的可扩展定制数据集，并提出新的身份混淆度量指标。实验表明，UMO显著提升了身份一致性并减少了身份混淆，在开源方法中达到最新水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 11:54:55 GMT</pubDate>
</item>
<item>
<title>SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents</title>
<link>https://arxiv.org/abs/2509.06283</link>
<guid>https://arxiv.org/abs/2509.06283</guid>
<content:encoded><![CDATA[
Equipping large language models (LLMs) with complex, interleaved reasoning and tool-use capabilities has become a key focus in agentic AI research, especially with recent advances in reasoning-oriented (``thinking'') models. Such capabilities are key to unlocking a number of important applications. One such application is Deep Research (DR), which requires extensive search and reasoning over many sources. Our work in this paper focuses on the development of native Autonomous Single-Agent models for DR featuring minimal web crawling and Python tool integration. Unlike multi-agent systems, where agents take up pre-defined roles and are told what to do at each step in a static workflow, an autonomous single-agent determines its next action dynamically based on context, without manual directive. While prior work has proposed training recipes for base or instruction-tuned LLMs, we focus on continual reinforcement learning (RL) of reasoning-optimized models to further enhance agentic skills while preserving reasoning ability. Towards this end, we propose a simple RL recipe with entirely synthetic data, which we apply to various open-source LLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam benchmark. In addition, we conduct key analysis experiments to provide more insights into our methodologies.
]]></content:encoded>
<pubDate>Sun, 07 Sep 2025 22:07:09 GMT</pubDate>
</item>
<item>
<title>CLIP-SVD：一种高效的多模态模型微调方法</title>
<link>https://arxiv.org/abs/2509.03740</link>
<guid>https://arxiv.org/abs/2509.03740</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLIP-SVD通过SVD实现高效微调，提升模型适应新领域的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CLIP-SVD的新型多模态参数高效适应技术，利用奇异值分解（SVD）对CLIP模型的内部参数空间进行修改，无需引入额外模块。该方法仅微调CLIP参数矩阵的奇异值，以重新缩放基础向量进行领域适应，同时保留预训练模型的知识。CLIP-SVD仅使用0.04%的模型参数即可实现增强的适应性能，并在11个自然和10个生物医学数据集上取得最先进的分类结果。此外，研究还采用基于自然语言的方法分析了CLIP适应的有效性和动态性，提升了模型的可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03740" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 18:00:23 GMT</pubDate>
</item>
<item>
<title>基于任务算术的多任务学习模型融合方法</title>
<link>https://arxiv.org/abs/2509.02108</link>
<guid>https://arxiv.org/abs/2509.02108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外标注数据的多任务模型融合方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多任务学习模型融合方法，通过任务算术实现不同任务模型的合并，避免了传统方法中需要合并数据集的步骤。该方法利用Jensen-Shannon散度指导融合过程，并自动平衡任务重要性，有效缓解任务干扰问题。与现有方法相比，该方法在任务数量增加时仍保持稳定性能，并在多个任务上表现更优。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 05:04:41 GMT</pubDate>
</item>
<item>
<title>基于内部表示的视觉-语言-动作模型可解释与控制方法</title>
<link>https://arxiv.org/abs/2509.00328</link>
<guid>https://arxiv.org/abs/2509.00328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VLA模型内部表示的可解释与实时控制框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉-语言-动作（VLA）模型在实际机器人应用中缺乏可解释性和可控性的问题，提出了一种基于内部表示的可解释与控制方法。通过将Transformer层中的前向激活投影到token嵌入空间，识别出与动作选择相关的语义方向，如速度和方向。该方法无需微调、奖励信号或环境交互，即可在推理时直接干预模型行为。实验在Pi0和OpenVLA两个开源VLA模型上进行，展示了在仿真环境和真实机器人上的零样本行为控制效果，为机器人领域的透明和可操控基础模型提供了新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 23:01:57 GMT</pubDate>
</item>
<item>
<title>guided decoding 在 RAG 系统中的应用与比较研究</title>
<link>https://arxiv.org/abs/2509.06631</link>
<guid>https://arxiv.org/abs/2509.06631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较了三种 guided decoding 方法在 RAG 中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在 Retrieval-Augmented Generation (RAG) 系统中，guided decoding 技术如何提升输出的结构化和可靠性。研究对比了 Outlines、XGrammar 和 LM Format Enforcer 三种方法，并在不同多轮提示设置（0-turn、1-turn 和 2-turn）下评估其成功率、幻觉率和输出质量。结果揭示了多轮交互对 guided decoding 性能的影响，发现了意想不到的性能差异，为特定应用场景下的方法选择提供了理论支持和实践指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 08:51:40 GMT</pubDate>
</item>
<item>
<title>基于DCReg的点云配准优化方法研究</title>
<link>https://arxiv.org/abs/2509.06285</link>
<guid>https://arxiv.org/abs/2509.06285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DCReg提升点云配准精度与效率，解决退化环境问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出DCReg框架，旨在解决几何退化或狭窄环境中激光雷达点云配准的不稳定性问题。通过引入Schur补分解技术，实现对病态问题的可靠检测，并将配准问题解耦为旋转和平移子空间。进一步在这些子空间中进行量化分析，明确映射数学特征与物理运动方向的关系。最后设计针对性的预处理策略，仅稳定病态方向，保留可观测空间中的有效信息，从而提升优化效率和鲁棒性。实验表明，DCReg在多种环境下显著提升定位精度并加快计算速度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 07 Sep 2025 22:12:54 GMT</pubDate>
</item>
<item>
<title>Inpaint4Drag：基于像素空间的实时拖拽图像编辑框架</title>
<link>https://arxiv.org/abs/2509.04582</link>
<guid>https://arxiv.org/abs/2509.04582</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Inpaint4Drag实现快速实时图像拖拽编辑与修复。</p><br /><br /><p><strong>摘要：</strong> 本文提出Inpaint4Drag，一种新的基于像素空间的拖拽图像编辑框架。该方法通过分解拖拽操作为像素空间的双向变形和图像修复，实现了更精确、实时的交互体验。相比传统方法，Inpaint4Drag在512x512分辨率下可达到0.01秒的变形预览和0.3秒的修复速度，显著提升了编辑效率。其设计无需修改模型架构，可适配任何图像修复模型，自动继承未来技术进步。实验表明，该方法在视觉质量和控制精度上均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04582" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 14:04:47 GMT</pubDate>
</item>
<item>
<title>Interleaving Reasoning Generation提升文本到图像生成质量</title>
<link>https://arxiv.org/abs/2509.06945</link>
<guid>https://arxiv.org/abs/2509.06945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IRG框架通过交替推理与生成提升图像质量与细节保留。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Interleaving Reasoning Generation (IRG) 的框架，通过在文本推理和图像生成之间交替进行，以提高文本到图像生成的质量和细节保留能力。作者还引入了Interleaving Reasoning Generation Learning (IRGL) 训练方法，旨在优化初始生成阶段并提升后续的细节调整。研究团队构建了IRGL-300K数据集，包含六种学习模式，用于训练模型在文本推理和图像生成之间的协同工作。实验结果显示，该方法在多个基准测试中表现优异，显著提升了图像质量和细粒度保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>Paper2Agent：将研究论文转化为AI代理的自动化框架</title>
<link>https://arxiv.org/abs/2509.06917</link>
<guid>https://arxiv.org/abs/2509.06917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Paper2Agent将论文转化为可交互的AI代理，提升科研成果的复用与传播。</p><br /><br /><p><strong>摘要：</strong> Paper2Agent是一个自动化框架，能够将研究论文转换为AI代理，使静态的研究成果转变为动态、可交互的系统。该框架通过分析论文和相关代码库，构建Model Context Protocol (MCP)服务器，并通过迭代测试优化MCP。这些MCP可以与聊天代理结合，执行复杂的科学查询并调用原始论文中的工具和工作流。案例研究表明，Paper2Agent能够生成可靠的论文代理，如基于AlphaGenome的基因组变异解释器和基于ScanPy与TISSUE的单细胞分析代理，有效复现原论文结果并处理新用户查询。Paper2Agent为知识传播和AI科学家协作生态系统提供了新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:28:42 GMT</pubDate>
</item>
<item>
<title>测试时缩放在知识密集型任务中的局限性</title>
<link>https://arxiv.org/abs/2509.06861</link>
<guid>https://arxiv.org/abs/2509.06861</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">测试时缩放在知识密集型任务中效果有限，可能增加幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文研究了测试时缩放（Test-time scaling）在知识密集型任务中的表现。尽管该方法在多个领域表现出色，但实验结果显示，在需要高事实准确性和低幻觉率的任务中，增加推理时间并未显著提升准确性，甚至可能导致更多幻觉。分析表明，减少幻觉往往是模型因思考过多而选择不回答，而非提高事实回忆能力。此外，某些模型在更长的推理过程中反而更容易产生幻觉。虽然存在局限性，但与不进行推理相比，启用推理仍有一定优势。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06861" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 12:28:25 GMT</pubDate>
</item>
<item>
<title>基于自动定理证明的高质量逻辑数据生成方法</title>
<link>https://arxiv.org/abs/2509.06809</link>
<guid>https://arxiv.org/abs/2509.06809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自动化定理证明生成高质量逻辑数据提升LLM数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于自动定理证明的高质量逻辑数据生成方法，旨在解决大型语言模型（LLM）在数学推理方面的数据瓶颈问题。该方法利用E-prover在TPTP公理库上的饱和能力，生成大量保证有效的定理，并将其转化为三个可控难度的挑战任务：蕴含验证、前提选择和证明重建。实验表明，当前前沿模型在需要深度结构推理的任务中表现不佳，而该框架不仅提供了诊断工具，还提供了可扩展的符号训练数据源。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 11:43:29 GMT</pubDate>
</item>
<item>
<title>基于多模态推理的黑暗幽默检测方法研究</title>
<link>https://arxiv.org/abs/2509.06771</link>
<guid>https://arxiv.org/abs/2509.06771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多模态框架用于检测网络迷因中的黑暗幽默。</p><br /><br /><p><strong>摘要：</strong> 本文针对网络迷因中黑暗幽默识别的挑战，提出了一个包含4,379个Reddit迷因的标注数据集，并设计了一个结合视觉、文本和推理的三流交叉推理网络（TCRNet）。该框架利用大视觉语言模型生成结构化解释，并通过角色反转自循环机制优化解释。最终通过融合文本、图像和推理信息进行分类，实验表明该方法在黑暗幽默检测、目标类别识别和强度预测任务中均优于现有基线。相关数据集和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 10:55:16 GMT</pubDate>
</item>
<item>
<title>WebExplorer：构建高效长周期网络搜索代理的系统方法</title>
<link>https://arxiv.org/abs/2509.06501</link>
<guid>https://arxiv.org/abs/2509.06501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebExplorer提升长周期网络搜索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出WebExplorer，一种基于模型探索和迭代查询演化的数据生成方法，旨在解决信息检索任务中数据不足的问题。通过构建高质量数据集，研究人员训练出WebExplorer-8B模型，支持128K上下文长度和最多100次工具调用，显著提升了复杂任务的搜索能力。在多个基准测试中，该模型表现出色，尤其在长周期问题解决方面优于更大规模模型。此外，WebExplorer-8B在知识密集型问答任务中也展现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 06:07:03 GMT</pubDate>
</item>
<item>
<title>BFS-Prover-V2：解决大型语言模型在自动定理证明中的扩展挑战</title>
<link>https://arxiv.org/abs/2509.06493</link>
<guid>https://arxiv.org/abs/2509.06493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BFS-Prover-V2提升LLM在定理证明中的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BFS-Prover-V2系统，旨在解决大型语言模型（LLMs）在自动定理证明中训练和推理阶段的扩展问题。该系统包含两项主要创新：一是基于AlphaZero原理的多轮离策略强化学习框架，通过多阶段专家迭代和自适应数据过滤持续提升LLM的推理能力；二是基于规划器的多智能体搜索架构，在推理阶段通过分层分解复杂定理并共享证明缓存，显著降低搜索空间。实验表明，BFS-Prover-V2在MiniF2F和ProofNet基准测试中分别达到95.08%和41.4%的准确率，展现出强大的性能。其方法对需要长期多轮推理和复杂搜索的其他领域也具有参考价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 05:54:18 GMT</pubDate>
</item>
<item>
<title>GUI与快捷方式混合代理的基准评估研究</title>
<link>https://arxiv.org/abs/2509.06477</link>
<guid>https://arxiv.org/abs/2509.06477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出MAS-Bench，评估GUI与快捷方式混合代理的效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MAS-Bench的基准测试框架，用于评估结合图形用户界面（GUI）操作和快捷方式（如API、深度链接）的智能代理。该框架包含139个跨11个真实应用的复杂任务，以及88个预定义的快捷方式和7项评估指标。实验表明，混合代理在成功率和效率上显著优于仅使用GUI的代理，验证了其在智能代理开发中的有效性。MAS-Bench填补了该领域的评估空白，为未来高效、稳健的智能代理研究提供了基础平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 05:43:48 GMT</pubDate>
</item>
<item>
<title>DINOv3在医学影像任务中的表现与局限性研究</title>
<link>https://arxiv.org/abs/2509.06467</link>
<guid>https://arxiv.org/abs/2509.06467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINOv3在医学影像中表现优异，但存在领域适应性限制。</p><br /><br /><p><strong>摘要：</strong> 本文评估了DINOv3作为通用视觉编码器在医学影像任务中的表现。实验表明，DINOv3在多种医学图像分类和分割任务中表现出色，甚至超越了一些专门的医学模型。然而，在需要深度领域知识的任务如全切片病理图像、电子显微镜和正电子发射断层扫描中，其性能下降。此外，DINOv3在医学领域的表现并不遵循典型的模型规模增长规律，不同任务的扩展行为差异较大。研究认为DINOv3可作为医学视觉任务的强大先验，未来可用于增强3D重建的多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 05:28:57 GMT</pubDate>
</item>
<item>
<title>基于对比注意力机制的视觉增强方法研究</title>
<link>https://arxiv.org/abs/2509.06461</link>
<guid>https://arxiv.org/abs/2509.06461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过对比注意力机制提升视觉语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉语言模型（VLMs）在复杂视觉环境下的性能下降问题，发现视觉复杂度与注意力熵呈强相关性，并揭示了注意力从全局扫描到局部聚焦的演变过程。基于此，提出了一种无需训练的对比注意力优化方法CARVE，通过像素级注意力对比提取任务相关视觉信号，显著提升了模型性能，最高提升达75%。该研究为理解视觉复杂度与注意力机制的关系提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06461" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 05:20:04 GMT</pubDate>
</item>
<item>
<title>REER：一种基于逆向推理的开放性生成方法</title>
<link>https://arxiv.org/abs/2509.06160</link>
<guid>https://arxiv.org/abs/2509.06160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REER通过逆向推理提升开放性任务的生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为REverse-Engineered Reasoning (REER)的新范式，旨在解决开放性、创造性生成任务中深度推理的应用难题。传统方法如强化学习和指令蒸馏在缺乏明确奖励信号或计算成本过高的情况下表现不佳。REER通过从已知良好解决方案反向推导出潜在的推理过程，实现更高效的推理建模。研究团队构建了DeepWriting-20K数据集，并训练出DeepWriter-8B模型，在开放性任务中表现出色，性能可与GPT-4o和Claude 3.5等领先模型媲美。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 07 Sep 2025 14:07:58 GMT</pubDate>
</item>
<item>
<title>T2I-CoReBench：评估文本到图像生成模型的综合基准</title>
<link>https://arxiv.org/abs/2509.03516</link>
<guid>https://arxiv.org/abs/2509.03516</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出T2I-CoReBench，评估图像生成模型的组合与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了T2I-CoReBench，一个用于评估文本到图像生成模型在组合与推理方面能力的综合性基准。该基准围绕场景图元素（实例、属性和关系）构建组合评估，并基于演绎、归纳和溯因推理框架设计推理评估，形成12维评价体系。所有提示均具有高组合密度和多步骤推理要求，并配有检查清单以实现细粒度评估。基准包含1080个挑战性提示和约13500个检查问题。实验表明，当前模型在复杂高密度场景下的组合能力有限，而推理能力更是成为主要瓶颈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03516" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 13:58:12 GMT</pubDate>
</item>
<item>
<title>基于轨迹感知的强化学习框架提升扩散语言模型性能</title>
<link>https://arxiv.org/abs/2509.06949</link>
<guid>https://arxiv.org/abs/2509.06949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TraceRL提升扩散语言模型推理能力，实现更优数学与编码任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出TraceRL，一种轨迹感知的强化学习框架，用于改进扩散语言模型（DLMs）的推理性能。该框架通过引入偏好推理轨迹增强训练稳定性，并在数学和编程等复杂任务中表现出色。基于TraceRL，研究团队开发了TraDo系列模型，其中TraDo-4B-Instruct在数学推理任务中超越7B规模的自回归模型，TraDo-8B-Instruct在数学基准测试中分别比Qwen2.5-7B-Instruct和Llama3.1-8B-Instruct高出6.1%和51.3%。此外，通过课程学习方法，团队还构建了首个长链思维（long-CoT）DLM，在MATH500任务中取得18.1%的相对准确率提升。为促进研究和应用，团队开源了完整的扩散LLM构建、训练与部署框架，支持多种任务和架构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 13:58:06 GMT</pubDate>
</item>
<item>
<title>基于共演化机制的AI安全框架研究</title>
<link>https://arxiv.org/abs/2509.06786</link>
<guid>https://arxiv.org/abs/2509.06786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出R^2AI框架以实现AI系统的持续安全。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能能力快速提升与安全进展滞后之间的矛盾，指出传统安全方法存在局限性。文章提出一种新的‘安全共演化’理念，借鉴生物免疫系统，强调安全应作为动态、对抗性和持续学习的过程。为此，作者引入R^2AI框架，整合对已知威胁的抵抗能力和对未知风险的恢复力，结合快速与慢速安全模型、对抗模拟验证及持续反馈机制，推动安全与能力的同步进化。该框架旨在为AI在动态环境中提供可扩展且主动的安全保障，应对短期漏洞和长期潜在风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 11:13:23 GMT</pubDate>
</item>
<item>
<title>深度研究系统中的强化学习基础综述</title>
<link>https://arxiv.org/abs/2509.06733</link>
<guid>https://arxiv.org/abs/2509.06733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了深度研究系统中强化学习的应用与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文是首个专注于深度研究系统中强化学习基础的综述。文章从三个维度系统梳理了相关工作：数据生成与整理、面向智能体的研究强化学习方法，以及智能体训练系统与框架。涵盖了稳定性、样本效率、长上下文处理、奖励设计、多目标优化和多模态整合等内容。同时讨论了智能体架构与协作、评估与基准测试，包括最近的问答、视觉问答、长文本生成和领域驱动的工具交互任务。文章总结了常见模式，揭示了基础设施瓶颈，并为训练强大且透明的深度研究代理提供了实用建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Sep 2025 10:27:23 GMT</pubDate>
</item>
<item>
<title>UniVerse-1：一种统一的音频视频生成模型</title>
<link>https://arxiv.org/abs/2509.06155</link>
<guid>https://arxiv.org/abs/2509.06155</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniVerse-1实现音频与视频的协同生成，提升音画同步效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UniVerse-1，一种类似于Veo-3的统一模型，能够同时生成协调的音频和视频内容。为提高训练效率，研究者采用了一种专家拼接（SoE）技术，将预训练的视频和音乐生成模型进行深度融合，充分利用其基础能力。为了确保音频与视频内容的准确标注和时间对齐，团队开发了一个在线标注管道，在训练过程中生成标签，避免因文本标注错位导致性能下降。经过约7600小时的音频视频数据微调，该模型在环境声音生成和语音生成方面表现出良好的音画同步效果。为系统评估方法，研究者还引入了Verse-Bench基准数据集，并公开了模型和代码以推动相关领域研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.06155" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 07 Sep 2025 13:55:03 GMT</pubDate>
</item>
<item>
<title>Llama-GENBA-10B：多语言基础模型应对英语中心偏见</title>
<link>https://arxiv.org/abs/2509.05668</link>
<guid>https://arxiv.org/abs/2509.05668</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-GENBA-10B平衡英语、德语和巴伐利亚语资源，提升多语言性能。</p><br /><br /><p><strong>摘要：</strong> Llama-GENBA-10B是一款基于Llama 3.1-8B扩展至10B参数的三语基础模型，旨在减少大型语言模型中的英语中心偏见。它在164B个token上进行持续预训练，其中包含82B英语、82B德语和80M巴伐利亚语数据，以平衡语言资源并防止英语主导。该模型针对德语自然语言处理社区，并推动巴伐利亚语这一低资源语言的发展。开发过程中解决了四个关键挑战：多语言语料库构建、统一分词器设计、架构与语言比例优化以及首个标准化三语评估套件的建立。实验表明，Llama-GENBA-10B在跨语言任务中表现优异，尤其在巴伐利亚语中超越了其他模型，同时在英语和德语中也表现出色。训练过程展示了高效的大规模多语言预训练方法，为包容性基础模型提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05668" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 06:12:52 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态大模型视觉推理方法研究</title>
<link>https://arxiv.org/abs/2509.01656</link>
<guid>https://arxiv.org/abs/2509.01656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReVPT方法提升多模态LLM的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在视觉推理任务中的局限性，提出一种基于强化学习的新型训练方法ReVPT。该方法通过引入GRPO算法，使模型能够有效使用四种视觉工具进行推理。实验表明，ReVPT在多个视觉感知基准测试中表现优异，显著优于监督学习和文本强化学习基线。ReVPT-3B和ReVPT-7B在CV-Bench数据集上分别超越指令模型9.03%和9.44%。研究还提供了关于基于强化学习的视觉工具使用的新见解，并公开了代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 13:57:49 GMT</pubDate>
</item>
<item>
<title>Set Block Decoding：提升语言模型生成效率的新方法</title>
<link>https://arxiv.org/abs/2509.04185</link>
<guid>https://arxiv.org/abs/2509.04185</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SBD通过并行预测多个未来标记加速语言模型生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Set Block Decoding（SBD）的新方法，用于加速自回归语言模型的生成过程。SBD结合了标准的下一个标记预测（NTP）和掩码标记预测（MATP），允许模型并行采样非连续的未来标记，从而提高推理效率。该方法无需修改模型结构或增加额外训练参数，兼容现有的KV缓存技术，并可通过微调现有模型实现。实验表明，使用Llama-3.1 8B和Qwen-3 8B模型进行微调后，SBD在保持与传统NTP相同性能的前提下，减少了3-5倍的前向传递次数，显著提升了生成速度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04185" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 09:02:39 GMT</pubDate>
</item>
<item>
<title>WinT3R：一种高效的在线相机位姿与点云重建模型</title>
<link>https://arxiv.org/abs/2509.05296</link>
<guid>https://arxiv.org/abs/2509.05296</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WinT3R实现高精度在线相机位姿与点云重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出WinT3R，一种前馈重建模型，能够实现精确的相机位姿和高质量点云地图的在线预测。传统方法在重建质量和实时性能之间存在权衡，而WinT3R通过引入滑动窗口机制，提升几何预测质量而不增加大量计算。同时，采用紧凑的相机表示和全局相机标记池，提高相机位姿估计的可靠性且不牺牲效率。实验表明，WinT3R在多个数据集上均表现出色，达到当前最优水平。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05296" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>提升大语言模型生成符号图形程序的能力</title>
<link>https://arxiv.org/abs/2509.05208</link>
<guid>https://arxiv.org/abs/2509.05208</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究改进大语言模型生成SVG图形程序的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型在生成符号图形程序（SGP）方面的能力，特别是通过自然语言描述生成可渲染的SVG图像。作者构建了SGP-GenBench基准测试，评估了模型在对象保真度、场景保真度和组合性方面的表现，并发现前沿商业模型优于开源模型。为提升生成能力，作者提出了一种基于可验证奖励的强化学习方法，显著提升了SVG生成的质量和语义准确性。实验表明，该方法有助于模型更精细地分解对象并增强场景连贯性，为跨模态对齐提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05208" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 12:10:53 GMT</pubDate>
</item>
<item>
<title>基于自迭代的强化学习方法ExIt提升模型推理时的自我优化能力</title>
<link>https://arxiv.org/abs/2509.04575</link>
<guid>https://arxiv.org/abs/2509.04575</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ExIt方法提升模型在推理时的自我优化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Exploratory Iteration (ExIt)的自课程强化学习方法，旨在让大语言模型在推理阶段进行多步骤的自我改进。ExIt通过选择性地采样任务过程中最具有信息量的中间状态，扩展任务空间，并将其作为新的自我迭代任务实例来训练自我改进策略。该方法不仅能够与显式探索机制结合以增强任务多样性，还在多个领域如数学竞赛、多轮工具使用和机器学习工程中展示了其有效性。实验表明，ExIt可以从少量或多个任务实例出发，生成在未见任务上表现出强大自我优化能力的策略，并能在超出训练阶段平均迭代深度的步数预算下持续提升性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04575" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 14:01:00 GMT</pubDate>
</item>
<item>
<title>评估大语言模型对改写问题的鲁棒性</title>
<link>https://arxiv.org/abs/2509.04013</link>
<guid>https://arxiv.org/abs/2509.04013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大语言模型在面对改写问题时性能下降，质疑现有基准测试的有效性。</p><br /><br /><p><strong>摘要：</strong> 本研究系统评估了大语言模型（LLMs）在面对不同改写版本的问题时的表现，通过生成多个基准测试题目的改写版本，并测量34个先进LLMs的效果变化。结果表明，尽管模型排名相对稳定，但绝对得分显著下降，说明LLMs在处理语言变体方面存在困难。这引发了对其泛化能力和评估方法的担忧，同时也挑战了基于基准测试的可靠性，强调需要更贴近实际应用场景的鲁棒性评估标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 04:43:27 GMT</pubDate>
</item>
<item>
<title>基于视频扩散Transformer的HDR环境光照估计方法</title>
<link>https://arxiv.org/abs/2509.03680</link>
<guid>https://arxiv.org/abs/2509.03680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LuxDiT模型，实现从单张图像生成高动态范围环境光照。</p><br /><br /><p><strong>摘要：</strong> 本文针对从单张图像或视频中估计场景光照这一长期挑战，提出了一种名为LuxDiT的新方法。该方法基于视频扩散Transformer模型，通过微调生成HDR环境图，能够从间接视觉线索中推断光照信息，并在真实场景中表现出良好的泛化能力。为了提升输入与预测环境图之间的语义对齐，研究引入了低秩适配微调策略。实验表明，该方法在定量和定性评估中均优于现有最先进的技术，能够生成具有真实感高频细节的光照预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 15:59:20 GMT</pubDate>
</item>
<item>
<title>U-Arm：低成本且可快速适配的遥操作框架</title>
<link>https://arxiv.org/abs/2509.02437</link>
<guid>https://arxiv.org/abs/2509.02437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">U-Arm是一种低成本、兼容性强的遥操作框架，支持多种机械臂。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为U-Arm的低成本、可快速适配的遥操作框架，能够与市面上大多数机器人手臂兼容。该系统通过三种结构不同的3D打印引导臂实现遥操作，具有统一的控制逻辑。相比以往的开源遥操作接口，U-Arm在机械设计和伺服选择上进行了优化，使6-DoF和7-DoF版本的成本分别降至50.5美元和56.8美元。通过机械和控制优化，U-Arm有效解决了冗余自由度控制难题。实验表明，其数据采集效率比Joycon高39%，任务成功率相当。所有CAD模型、仿真支持及真实操作数据均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 11:39:38 GMT</pubDate>
</item>
<item>
<title>LatticeWorld：基于轻量级大模型的高效3D世界生成框架</title>
<link>https://arxiv.org/abs/2509.05263</link>
<guid>https://arxiv.org/abs/2509.05263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LatticeWorld提升3D环境生成效率与真实感。</p><br /><br /><p><strong>摘要：</strong> 本文提出LatticeWorld，一个结合轻量级大语言模型和工业级渲染引擎的3D世界生成框架。该框架通过文本和视觉指令生成大规模交互式3D场景，具备多智能体互动、高保真物理模拟和实时渲染能力。实验表明，LatticeWorld在场景布局准确性和视觉质量上表现优异，并将工业生产效率提升了90倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.05263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 05 Sep 2025 13:22:33 GMT</pubDate>
</item>
<item>
<title>WildScore：首个多模态符号音乐推理与分析基准</title>
<link>https://arxiv.org/abs/2509.04744</link>
<guid>https://arxiv.org/abs/2509.04744</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WildScore评估MLLM在符号音乐领域的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WildScore，这是首个用于评估多模态大语言模型（MLLMs）在真实世界符号音乐领域推理与分析能力的基准。WildScore包含来自真实音乐作品的数据，并配有用户生成的问题和讨论，以反映实际音乐分析的复杂性。研究提出了一套系统化的分类体系，并将复杂的音乐推理任务转化为多项选择题形式，便于评估模型的理解能力。实验结果显示，当前最先进的MLLM在符号音乐推理方面展现出一定的潜力，但也暴露出诸多挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04744" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 21:54:50 GMT</pubDate>
</item>
<item>
<title>语言模型的幻觉问题及其成因分析</title>
<link>https://arxiv.org/abs/2509.04664</link>
<guid>https://arxiv.org/abs/2509.04664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">语言模型因训练机制倾向于猜测而非承认不确定性，导致幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型在面对不确定信息时产生幻觉的现象，指出这种现象源于训练和评估过程对猜测行为的奖励，而非对不确定性的认可。作者认为，幻觉本质上是二分类错误，并且由于评估标准偏向于测试表现，模型被优化为善于猜测。文章建议通过调整现有基准的评分方式来缓解这一问题，而非引入新的评估方法，以提升AI系统的可信度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 17:26:31 GMT</pubDate>
</item>
<item>
<title>MedVista3D：解决3D影像诊断误差的多尺度语义增强框架</title>
<link>https://arxiv.org/abs/2509.03800</link>
<guid>https://arxiv.org/abs/2509.03800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedVista3D提升3D医学影像诊断准确性与报告一致性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MedVista3D，一种用于3D CT分析的多尺度语义增强视觉语言预训练框架。该框架旨在解决放射学诊断中的错误，如漏诊、注意力盲区和沟通失败。通过局部与全局图像文本对齐，实现细粒度表示学习，并利用语言模型重写和放射学语义匹配库应对报告多样性问题。MedVista3D在零样本疾病分类、报告检索和医学视觉问答任务中表现优异，并能有效迁移至器官分割和预后预测。相关代码和数据集将公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 21:28:44 GMT</pubDate>
</item>
<item>
<title>基于行为指纹的大型语言模型评估框架研究</title>
<link>https://arxiv.org/abs/2509.04504</link>
<guid>https://arxiv.org/abs/2509.04504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出行为指纹框架评估LLM的交互特性差异。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“行为指纹”的新框架，用于评估大型语言模型（LLM）的内在认知和交互风格，超越传统的性能指标。通过使用定制的诊断提示集和自动化评估流程，分析了18个不同层级的模型。研究发现，尽管顶级模型在抽象和因果推理等核心能力上趋于一致，但在对齐相关行为如附和性和语义鲁棒性方面存在显著差异。此外，模型表现出一种默认人格聚类（ISTJ/ESTJ），可能反映了常见的对齐激励机制。结果表明，模型的交互特性并非由规模或推理能力自然产生，而是特定且高度可变的开发者对齐策略的直接结果。该框架提供了一种可复现、可扩展的方法来揭示这些深层次的行为差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 03:03:20 GMT</pubDate>
</item>
<item>
<title>Loong项目：基于可验证奖励的大型语言模型推理提升框架</title>
<link>https://arxiv.org/abs/2509.03059</link>
<guid>https://arxiv.org/abs/2509.03059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Loong项目提供合成数据生成与验证框架，提升LLM在多领域推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Loong项目，这是一个用于跨多个推理密集型领域的大规模合成数据生成与验证的开源框架。该框架包含两个核心组件：LoongBench，一个由8,729个经过人工审核的示例组成的种子数据集，涵盖12个领域；以及LoongEnv，一个支持多种提示策略的合成数据生成环境。通过将LLM作为代理，结合代码执行答案进行奖励，形成强化学习循环。实验表明，该框架有效提升了LLM在数学、化学等领域的推理表现，并对合成数据的质量进行了全面分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 02:42:40 GMT</pubDate>
</item>
<item>
<title>Video-MTR：基于多轮推理的长视频理解框架</title>
<link>https://arxiv.org/abs/2508.20478</link>
<guid>https://arxiv.org/abs/2508.20478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Video-MTR框架提升长视频理解性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Video-MTR，一种基于强化学习的多轮推理框架，用于解决长视频理解中的长期时间依赖和多事件问题。与传统单次推理方法不同，Video-MTR通过多轮迭代逐步选择关键视频片段，并结合答案正确性和帧-查询相关性进行奖励优化，实现端到端训练，无需依赖外部视觉语言模型。实验表明，该方法在多个基准数据集上均优于现有方法，提升了长视频理解的准确性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 02:55:08 GMT</pubDate>
</item>
<item>
<title>Delta Activations：一种用于微调模型表示的新方法</title>
<link>https://arxiv.org/abs/2509.04442</link>
<guid>https://arxiv.org/abs/2509.04442</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Delta Activations帮助更高效地理解和管理微调模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Delta Activations的新方法，通过测量微调模型相对于基础模型的内部激活变化，将其表示为向量嵌入。这种方法能够有效按领域和任务对模型进行聚类，揭示模型之间的结构关系。Delta Activations具有鲁棒性和可加性，适用于不同微调设置，并能通过少量样本嵌入任务，有助于模型选择和合并。该方法旨在提升公开模型的复用效率。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04442" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>Durian：基于零样本面部属性迁移的肖像动画生成方法</title>
<link>https://arxiv.org/abs/2509.04434</link>
<guid>https://arxiv.org/abs/2509.04434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Durian实现零样本面部属性迁移生成高质量肖像动画。</p><br /><br /><p><strong>摘要：</strong> 本文提出Durian，一种无需显式三元组监督即可从参考图像中迁移面部属性生成肖像动画视频的方法。通过引入双参考网络，在扩散模型的去噪过程中注入肖像和属性图像的空间特征，实现跨帧的高保真和空间一致性属性迁移。训练过程中采用自重构策略，利用同一肖像视频中的两帧作为属性参考和目标肖像，其余帧在这些输入和对应掩码条件下进行重建。此外，通过关键点条件图像生成的掩码扩展策略支持不同空间范围的属性迁移，并结合空间和外观级变换提升对位置错位的鲁棒性。该方法在肖像动画属性迁移任务中达到最先进水平，且其双参考设计可在一次生成过程中实现多属性组合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:53:03 GMT</pubDate>
</item>
<item>
<title>基于流的3D生成模型少步蒸馏方法研究</title>
<link>https://arxiv.org/abs/2509.04406</link>
<guid>https://arxiv.org/abs/2509.04406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MDT-dist框架，实现3D生成模型高效蒸馏。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为MDT-dist的新型框架，用于实现3D生成模型的少步流蒸馏。该方法通过引入Velocity Matching（VM）和Velocity Distillation（VD）两个可优化目标，将原本难以实现的传输目标转化为速度和分布层面的优化任务。在TRELLIS框架上的实验表明，该方法将每个流变换器的采样步骤从25次减少到1或2次，在A800显卡上分别达到0.68秒和0.94秒的延迟，速度提升达9.0倍和6.5倍，同时保持了高质量的视觉和几何保真度。实验结果表明，该方法显著优于现有CM蒸馏方法，提升了3D生成任务的效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:24:31 GMT</pubDate>
</item>
<item>
<title>基于连续时间动力学的高效生成模型TiM</title>
<link>https://arxiv.org/abs/2509.04394</link>
<guid>https://arxiv.org/abs/2509.04394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TiM实现高效生成与高质量输出的平衡。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的生成模型TiM，通过引入精确的连续时间动力学方程，解决了传统扩散模型在计算成本和生成质量之间的矛盾。TiM能够在任意步数下进行生成，从单步跳跃到精细细化，表现出色。尽管参数量仅为865M，TiM在多个评估指标上超越了SD3.5和FLUX.1等大型模型。同时，随着采样预算增加，TiM的质量持续提升，并在高分辨率下展现出卓越的保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:05:59 GMT</pubDate>
</item>
<item>
<title>基于图像编辑模型的密集几何预测框架FE2E</title>
<link>https://arxiv.org/abs/2509.04338</link>
<guid>https://arxiv.org/abs/2509.04338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像编辑模型在密集几何预测中表现优于生成模型。</p><br /><br /><p><strong>摘要：</strong> 本文研究了图像编辑模型与文本到图像生成模型在密集几何估计任务中的微调行为，发现编辑模型具备结构先验，能更稳定地收敛并取得更好性能。基于此，作者提出FE2E框架，利用Diffusion Transformer架构的编辑模型进行密集几何预测。通过重新设计损失函数、采用对数量化解决精度冲突，并利用全局注意力实现深度与法线的联合估计。FE2E在多个数据集上取得了显著提升，尤其在ETH3D数据集上性能提升超过35%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 11:58:50 GMT</pubDate>
</item>
<item>
<title>评估大语言模型对抗性指令遵循能力的基准测试</title>
<link>https://arxiv.org/abs/2509.04292</link>
<guid>https://arxiv.org/abs/2509.04292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Inverse IFEval基准，评估LLM对抗性指令适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Inverse IFEval基准，用于评估大语言模型（LLMs）在面对与训练模式冲突的指令时的适应能力。该基准包含八种挑战类型，如问题修正、故意文本缺陷等，并通过人机协作构建了涵盖23个领域的1012个高质量中英文问题数据集。实验表明，现有先进LLMs在应对非标准指令时存在局限性，强调未来对齐工作应关注模型在非常规情境下的适应性，而不仅仅是流畅性和事实正确性。研究希望Inverse IFEval能成为诊断工具和提升模型指令遵循可靠性的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 11:03:02 GMT</pubDate>
</item>
<item>
<title>NER Retriever：一种无需预定义类型的实体检索框架</title>
<link>https://arxiv.org/abs/2509.04011</link>
<guid>https://arxiv.org/abs/2509.04011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NER Retriever通过语义空间嵌入实现无模式实体检索，效果优于传统方法。</p><br /><br /><p><strong>摘要：</strong> NER Retriever是一种零样本检索框架，用于处理无预定义类型的命名实体检索任务。该方法不依赖固定模式或微调模型，而是利用大语言模型的内部表示，将实体提及和用户定义的类型描述嵌入到共享语义空间中。研究发现，中间层Transformer块的值向量比顶层嵌入更能捕捉细粒度类型信息。通过训练一个轻量级对比投影网络，进一步优化这些表示，使实体嵌入具有类型感知能力，适合最近邻搜索。在三个基准测试中，NER Retriever显著优于基于词汇和密集句级的基线方法。该工作为大语言模型中的表示选择提供了实证支持，并提出了可扩展的无模式实体检索方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 04:42:23 GMT</pubDate>
</item>
<item>
<title>Drivelology：语言中的无意义与深度</title>
<link>https://arxiv.org/abs/2509.03867</link>
<guid>https://arxiv.org/abs/2509.03867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型难以理解Drivelology的深层含义。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Drivelology这一独特的语言现象，其特点是语法上通顺但语用上悖论、情感丰富或修辞颠覆。尽管当前大型语言模型在许多自然语言处理任务中表现出色，但在理解Drivelological文本的多层语义方面存在明显不足。研究团队构建了一个包含1200多个精心挑选示例的基准数据集，涵盖多种语言，并通过多轮专家评审确保其真实性。实验结果显示，模型常将Drivelology误认为浅层无意义，产生不连贯解释，或完全忽略其隐含修辞功能。这表明语言模型在语用理解方面仍存在显著差距，挑战了统计流畅性等同于认知理解的假设。研究已公开数据集和代码，以促进对超越表层连贯性的语言深度建模研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 23:58:55 GMT</pubDate>
</item>
<item>
<title>统一策略梯度与混合后训练方法的理论与实践</title>
<link>https://arxiv.org/abs/2509.04419</link>
<guid>https://arxiv.org/abs/2509.04419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出统一的策略梯度框架，实现RL与SFT的融合。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现代语言模型后训练中两种主要数据来源——在线生成数据和离线演示数据，并指出传统方法如强化学习（RL）和监督微调（SFT）实际上是同一优化过程的不同实例。作者提出了一种统一的策略梯度估计器，并通过不同数据分布假设和偏差-方差权衡展示了多种后训练方法的统一性。该梯度估计器由四个可互换部分组成，包括稳定掩码、参考策略分母、优势估计和似然梯度。基于理论分析，作者提出混合后训练（HPT）算法，能够动态选择训练信号，在利用演示数据的同时保持稳定的探索能力。实验表明，HPT在多个数学推理基准和分布外测试集上均优于现有基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.04419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 13:40:33 GMT</pubDate>
</item>
<item>
<title>探究基于探测的LLM安全检测方法的局限性</title>
<link>https://arxiv.org/abs/2509.03888</link>
<guid>https://arxiv.org/abs/2509.03888</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现探测方法可能仅学习表面模式，而非真正理解有害性。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了基于探测的大型语言模型（LLMs）安全检测方法的有效性。研究指出，当前的探测方法可能仅学习指令模式和触发词等表面特征，而非真正识别有害内容的语义本质。通过一系列控制实验和语义清洗数据集的测试，作者验证了这一假设，并揭示了现有方法可能带来的虚假安全感。文章呼吁重新设计模型和评估协议，以推动更负责任的安全研究方向。项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03888" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 04 Sep 2025 01:15:55 GMT</pubDate>
</item>
<item>
<title>DeepResearch Arena：构建高质量研究任务的基准平台</title>
<link>https://arxiv.org/abs/2509.01396</link>
<guid>https://arxiv.org/abs/2509.01396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepResearch Arena为研究代理提供高质量任务评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepResearch Arena，一个基于学术研讨会的基准平台，旨在更真实地反映研究环境并评估深度研究代理的能力。通过多智能体分层任务生成系统（MAHTG），从研讨会中提取有价值的研究灵感，并转化为高质量的研究任务。该平台包含超过10,000个任务，覆盖12个学科领域，展示了当前先进研究代理在面对其挑战时存在明显性能差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 07:42:47 GMT</pubDate>
</item>
<item>
<title>从二维工程图生成参数化CAD模型的框架研究</title>
<link>https://arxiv.org/abs/2508.18733</link>
<guid>https://arxiv.org/abs/2508.18733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Drawing2CAD框架，实现从2D图纸到CAD模型的自动转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Drawing2CAD的框架，旨在将二维工程图纸自动转换为参数化CAD模型。该框架通过将CAD生成问题重新定义为序列到序列的学习任务，利用矢量绘图原语直接指导参数化CAD操作的生成，从而保持几何精度和设计意图。其核心技术包括一种保留精确几何信息的矢量原语表示、一个解耦命令类型与参数生成的双解码器Transformer架构，以及一种适应CAD参数灵活性的软目标分布损失函数。为了训练和评估该方法，作者构建了CAD-VGDrawing数据集，并进行了广泛的实验验证了方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 03:01:58 GMT</pubDate>
</item>
<item>
<title>利用深度相机提升机器人操作的泛化能力</title>
<link>https://arxiv.org/abs/2509.02530</link>
<guid>https://arxiv.org/abs/2509.02530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">深度相机结合神经数据引擎提升机器人操作精度与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Camera Depth Models (CDMs)，通过结合RGB图像和原始深度信号，输出去噪且精确的度量深度信息，从而提升机器人操作的准确性。研究利用模拟数据生成高质量配对数据，训练出的模型在真实世界任务中表现出色，无需额外噪声或微调即可实现高泛化能力。实验表明，基于模拟深度数据训练的策略可在复杂任务中无缝迁移到实际机器人，为未来机器人学习提供新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02530" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:29:38 GMT</pubDate>
</item>
<item>
<title>SATQuest：一种用于评估和提升大语言模型逻辑推理能力的系统验证工具</title>
<link>https://arxiv.org/abs/2509.00930</link>
<guid>https://arxiv.org/abs/2509.00930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SATQuest通过生成SAT问题评估LLM逻辑推理，提升其泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SATQuest，一个用于评估和增强大语言模型（LLMs）逻辑推理能力的系统验证工具。SATQuest通过从合取范式（CNF）实例生成多样化的可满足性逻辑推理问题，从实例规模、问题类型和问题格式三个维度进行结构化分析。该工具利用随机生成和PySAT进行客观答案验证，有效避免记忆问题，提供对推理性能的深入洞察，并支持有效的强化微调。实验表明，LLMs在逻辑推理方面存在显著局限，尤其是在超出熟悉数学格式的泛化能力上。然而，使用SATQuest奖励进行微调能显著提升任务表现并推广到更复杂实例，同时揭示了跨格式适应的挑战。SATQuest展示了作为推进LLM逻辑推理基础工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 31 Aug 2025 12:56:06 GMT</pubDate>
</item>
<item>
<title>视觉语言世界模型提升智能规划性能</title>
<link>https://arxiv.org/abs/2509.02722</link>
<guid>https://arxiv.org/abs/2509.02722</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLWM通过语义与时间抽象提升智能系统规划能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了视觉语言世界模型（VLWM），该模型基于自然视频进行语言驱动的世界建模。VLWM能够根据视觉观察推断目标达成情况，并预测由动作和世界状态变化交织组成的轨迹。其通过迭代的LLM自修正机制，结合由树状标题压缩的未来观察进行推理。VLWM同时学习动作策略和动态模型，分别支持快速反应和深度规划。通过成本最小化实现系统-2规划，评估模型基于语义距离计算，采用自监督训练。实验表明，VLWM在VPA任务中表现优异，系统-2规划相比系统-1提升了27%的Elo评分，并在RoboVQA和WorldPrediction等基准测试中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02722" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 14:18:57 GMT</pubDate>
</item>
<item>
<title>LMEnt：用于研究语言模型知识获取的工具套件</title>
<link>https://arxiv.org/abs/2509.03405</link>
<guid>https://arxiv.org/abs/2509.03405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LMEnt提供分析语言模型知识获取的新方法和资源。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LMEnt，一个用于研究语言模型在预训练过程中如何获取知识的工具套件。该套件包括一个富含实体信息的预训练语料库、一种基于实体的检索方法以及多个预训练模型。这些资源有助于理解实体提及与下游性能之间的关系，以及预训练数据中因果干预的影响。研究发现，事实频率是知识获取的关键因素，但并不能完全解释学习趋势。LMEnt旨在支持对语言模型知识表示、可塑性、编辑、归属和学习动态的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.03405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Sep 2025 11:31:18 GMT</pubDate>
</item>
<item>
<title>MOSAIC：多主体图像生成的语义对齐与特征解耦方法</title>
<link>https://arxiv.org/abs/2509.01977</link>
<guid>https://arxiv.org/abs/2509.01977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOSAIC通过语义对齐和特征解耦提升多主体图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出MOSAIC框架，旨在解决多主体图像生成中身份混淆和属性泄露的问题。该方法通过显式的语义对应和正交特征解耦，实现更精准的多主体图像合成。研究引入SemAlign-MS数据集，提供细粒度的语义对应关系，并设计语义对应注意力损失和多参考解耦损失，以提高生成图像的一致性和身份保真度。实验表明，MOSAIC在多个基准测试中表现优异，尤其在处理4个以上参考主体时仍能保持高质量生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 01:40:07 GMT</pubDate>
</item>
<item>
<title>Face-MoGLE：一种可控制的人脸生成框架</title>
<link>https://arxiv.org/abs/2509.00428</link>
<guid>https://arxiv.org/abs/2509.00428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Face-MoGLE实现高保真可控人脸生成，提升生成模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Face-MoGLE，一种基于扩散Transformer的新型人脸生成框架。该框架通过语义解耦的潜在建模、全局与局部专家混合结构以及动态门控网络，实现了对人脸属性的精确控制和高质量生成。实验表明，Face-MoGLE在多模态和单模态人脸生成任务中表现出色，并具备强大的零样本泛化能力，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 05:21:07 GMT</pubDate>
</item>
<item>
<title>Robix：一种集成机器人推理与自然语言交互的统一模型</title>
<link>https://arxiv.org/abs/2509.01106</link>
<guid>https://arxiv.org/abs/2509.01106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Robix实现机器人任务规划与自然语言交互的统一。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Robix，一个将机器人推理、任务规划和自然语言交互整合在一个视觉-语言架构中的统一模型。作为分层机器人系统中的高层认知层，Robix能够动态生成低级控制器的原子指令，并提供与人类互动的自然语言响应。该模型支持复杂指令执行、长周期任务规划以及自然对话交互。Robix引入了主动对话、实时中断处理和上下文感知常识推理等新能力。其核心采用思维链推理，并通过三阶段训练策略提升基础具身推理能力、建模人机交互与任务规划的统一序列，以及优化推理-行动一致性。实验表明，Robix在多种任务类型和用户参与场景中表现优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 31 Aug 2025 23:53:47 GMT</pubDate>
</item>
<item>
<title>InfoSeek：构建复杂深度研究任务的框架</title>
<link>https://arxiv.org/abs/2509.00375</link>
<guid>https://arxiv.org/abs/2509.00375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfoSeek提升大语言模型处理复杂研究任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfoSeek，一个用于合成复杂深度研究任务的可扩展框架。通过双代理系统构建研究树，并将其转化为需要多步骤推理的问题。该框架生成了超过50K个训练样本和测试集，实验表明使用InfoSeek训练的模型在BrowseComp-Plus基准上表现优异，甚至超越更大的模型和商业API。InfoSeek还支持高级优化策略，如复合奖励设计和轨迹级探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 02:02:56 GMT</pubDate>
</item>
<item>
<title>动态守护模型提升聊天机器人内容监管效率</title>
<link>https://arxiv.org/abs/2509.02563</link>
<guid>https://arxiv.org/abs/2509.02563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态守护模型提升聊天机器人内容监管效率。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了动态守护模型的应用，用于监督和调节用户交互聊天机器人的输出，确保符合预定义的政策。与传统的静态守护模型不同，动态模型可以根据用户自定义的策略进行文本评估，适用于更多应用场景。该模型在检测静态危害类别方面与传统模型相当，同时在识别自由格式政策违规方面表现出色，且速度更快。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:57:56 GMT</pubDate>
</item>
<item>
<title>小型自动语音识别模型在低资源语言中的应用</title>
<link>https://arxiv.org/abs/2509.02523</link>
<guid>https://arxiv.org/abs/2509.02523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">小型ASR模型在低资源语言中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Flavors of Moonshine，一套针对多种低资源语言的微型自动语音识别（ASR）模型。研究挑战了多语言模型优于单语言模型的传统观点，表明在参数量较小（27M）的情况下，通过高质量人工标注、伪标注和合成数据的混合训练，单语言系统可以取得显著更好的性能。实验结果显示，这些模型的错误率比同规模的Whisper Tiny模型低48%，并优于9倍大的Whisper Small模型，甚至在多数情况下达到或超过28倍大的Whisper Medium模型。这些成果推动了该规模模型的最新进展，使得更多语言能够在设备端实现准确的语音识别。作者发布了阿拉伯语、中文、日语、韩语、乌克兰语和越南语的Moonshine模型，并采用宽松的开源许可证发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:22:54 GMT</pubDate>
</item>
<item>
<title>重新评估大语言模型的提示敏感性问题</title>
<link>https://arxiv.org/abs/2509.01790</link>
<guid>https://arxiv.org/abs/2509.01790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现提示敏感性更多源于评估方法而非模型缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了大语言模型（LLM）在提示敏感性方面的表现，探讨其是否为模型固有缺陷。通过在多个基准测试中对7个LLM进行系统评估，研究发现，许多提示敏感性现象实际上是由评估方法（如日志似然评分和严格答案匹配）引起的，这些方法容易忽略语义正确但表达方式不同的回答。当使用LLM作为评判者进行评估时，模型性能差异显著减小，且模型排名的相关性更高。这表明现代LLM对提示模板的鲁棒性比之前认为的更强，提示敏感性可能更多是评估过程的问题，而非模型本身的缺陷。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 17:38:28 GMT</pubDate>
</item>
<item>
<title>Gated Associative Memory：一种高效的序列建模架构</title>
<link>https://arxiv.org/abs/2509.00605</link>
<guid>https://arxiv.org/abs/2509.00605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GAM通过线性复杂度实现高效序列建模，优于传统Transformer。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Gated Associative Memory (GAM) 的新型序列建模架构，其计算复杂度为线性（O(N)），显著优于传统Transformer的二次复杂度（O(N^2)）。GAM通过两个并行路径——因果卷积和关联记忆检索，分别捕捉局部和全局信息，并利用门控机制动态融合两者。实验表明，GAM在训练速度和验证困惑度上均优于Transformer和Mamba模型，展现出作为高效序列建模方法的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 16:59:46 GMT</pubDate>
</item>
<item>
<title>推荐系统中群体公平与个体公平的关系研究</title>
<link>https://arxiv.org/abs/2508.21334</link>
<guid>https://arxiv.org/abs/2508.21334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示群体公平与个体公平可能相互影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了推荐系统中群体公平与个体公平之间的关系。由于以往研究分别采用不同的评估指标，导致两者难以直接比较。通过在三个数据集上进行八次实验，发现高度群体公平的推荐可能对个体不公平。该研究为提升系统公平性提供了新见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 01:25:05 GMT</pubDate>
</item>
<item>
<title>向量嵌入在现实任务中的理论限制</title>
<link>https://arxiv.org/abs/2508.21038</link>
<guid>https://arxiv.org/abs/2508.21038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示向量嵌入在简单查询下也存在理论限制。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了向量嵌入在检索任务中的理论局限性，指出即使在简单的查询场景中，嵌入模型也可能遇到性能瓶颈。研究结合学习理论，证明嵌入维度限制了可返回的文档子集数量，并通过实验验证了这一结论。作者构建了一个名为LIMIT的数据集来测试模型，发现即使最先进的模型在该数据集上也表现不佳。文章强调了当前单向量范式的局限性，并呼吁未来研究探索解决这一根本问题的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:43:53 GMT</pubDate>
</item>
<item>
<title>动态剪切策略提升大语言模型强化学习性能</title>
<link>https://arxiv.org/abs/2509.02333</link>
<guid>https://arxiv.org/abs/2509.02333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DCPO改进了强化学习中的梯度更新与奖励标准化，提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出动态剪切策略优化（DCPO），解决现有强化学习方法在大语言模型中因固定剪切边界和相同奖励标准化导致的梯度失效问题。DCPO通过自适应调整剪切边界增强token级探索，并采用平滑优势标准化提升响应级奖励利用效率。实验表明，DCPO在多个基准测试中表现优于GRPO和DAPO，显著提升了训练效率和非零优势比例，同时大幅降低了剪切率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 10:01:07 GMT</pubDate>
</item>
<item>
<title>ViSTA-SLAM：无需相机内参的实时单目SLAM系统</title>
<link>https://arxiv.org/abs/2509.01584</link>
<guid>https://arxiv.org/abs/2509.01584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViSTA-SLAM实现无需内参的实时单目SLAM，提升定位与重建精度。</p><br /><br /><p><strong>摘要：</strong> ViSTA-SLAM是一种无需相机内参的实时单目视觉SLAM系统，采用轻量级对称双视图关联模型作为前端，能够从两幅RGB图像中同时估计相对相机位姿并回归局部点云，显著降低模型复杂度。其前端大小仅为现有方法的35%，同时提升了双视图约束的质量。后端构建了专门设计的Sim(3)位姿图，并结合回环检测以解决累积漂移问题。大量实验表明，该方法在相机跟踪和密集3D重建方面均优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 12:12:23 GMT</pubDate>
</item>
<item>
<title>大规模语言模型优化方法的系统评估</title>
<link>https://arxiv.org/abs/2509.01440</link>
<guid>https://arxiv.org/abs/2509.01440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统评估了LLM优化方法，提供实践指导。</p><br /><br /><p><strong>摘要：</strong> 本文对近年来用于优化大规模语言模型的多种方法进行了系统评估，旨在解决因实验协议差异导致的比较困难问题。研究在标准化的预训练场景下，通过调整模型规模、批量大小和训练时长，全面分析了不同优化方法的效果。研究结果为从业者提供了选择合适优化器的指导，并指出了未来优化研究的方向。此外，作者公开了代码和实验数据，以促进未来方法的开发与严谨评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 08:50:30 GMT</pubDate>
</item>
<item>
<title>MobiAgent：提升移动代理系统性能的综合方案</title>
<link>https://arxiv.org/abs/2509.00531</link>
<guid>https://arxiv.org/abs/2509.00531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MobiAgent提升移动代理在真实场景下的性能。</p><br /><br /><p><strong>摘要：</strong> 随着视觉语言模型的快速发展，基于GUI的移动代理成为智能移动系统的重要方向。然而，现有代理模型在实际任务执行中仍面临准确性和效率的挑战。为此，本文提出MobiAgent，一个包含MobiMind系列模型、AgentRR加速框架和MobiFlow基准套件的综合系统。同时，为解决高质量数据不足的问题，开发了AI辅助的数据收集流程，降低人工标注成本。与通用大模型和专用GUI代理模型相比，MobiAgent在真实移动场景中表现出色，达到当前最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 11:24:47 GMT</pubDate>
</item>
<item>
<title>Camlang测试揭示大型语言模型在元语言推理上的局限性</title>
<link>https://arxiv.org/abs/2509.00425</link>
<guid>https://arxiv.org/abs/2509.00425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示LLM在Camlang任务中表现远低于人类。</p><br /><br /><p><strong>摘要：</strong> 本文通过构建一种新型语言Camlang，测试大型语言模型是否具备真正的元语言推理能力。Camlang包含语法书和双语词典，模拟成人学习第二语言的过程。实验表明，尽管GPT-5在英语任务中表现优异，但在Camlang任务中仅达到47%的准确率，远低于人类的87%。研究指出，大多数模型的成功依赖于浅层词汇对齐，而非系统性的语法掌握。Camlang为评估模型与人类在元语言能力上的差距提供了一个认知基础框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 05:13:10 GMT</pubDate>
</item>
<item>
<title>长视频中的语义聚合幻觉研究与ELV-Halluc基准构建</title>
<link>https://arxiv.org/abs/2508.21496</link>
<guid>https://arxiv.org/abs/2508.21496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究长视频中的语义聚合幻觉并提出首个相关基准。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于长视频中一种新型的幻觉现象——语义聚合幻觉（SAH），即模型在将帧级语义整合为事件级语义时产生的错误输出。传统视频幻觉研究多集中于短视频，而本文指出SAH在长视频中尤为显著，因其语义复杂度更高。为此，作者提出了首个针对长视频幻觉的基准ELV-Halluc，并通过实验验证了SAH的存在及其随语义复杂度增加的趋势。研究还发现模型在快速变化的语义场景下更容易产生SAH。文章进一步探讨了缓解策略，如位置编码和DPO方法，并构建了8K对对抗数据集，显著降低了SAH比例。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 06:25:03 GMT</pubDate>
</item>
<item>
<title>FastFit：一种高效多参考虚拟试穿框架</title>
<link>https://arxiv.org/abs/2508.20586</link>
<guid>https://arxiv.org/abs/2508.20586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FastFit提升多参考虚拟试穿效率3.5倍。</p><br /><br /><p><strong>摘要：</strong> 本文提出FastFit，一种基于缓存扩散架构的高效多参考虚拟试穿框架。通过引入半注意力机制和用类别嵌入替代时间步嵌入，实现参考特征编码与去噪过程的解耦，仅需一次计算即可复用参考特征，显著提升效率。实验表明，FastFit在多个数据集上优于现有方法，且推理速度提升3.5倍。同时，研究团队还发布了DressCode-MR数据集，包含28,179组高质量图像，涵盖五类服饰，用于推动多参考虚拟试穿研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 05:25:52 GMT</pubDate>
</item>
<item>
<title>基于生成模型的视频合成技术研究</title>
<link>https://arxiv.org/abs/2509.02460</link>
<guid>https://arxiv.org/abs/2509.02460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成模型提升视频合成效率与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于生成模型的视频合成方法，旨在自动化传统视频合成流程，减少人工干预和成本。该方法通过设计一种新型的Diffusion Transformer（DiT）管道，实现对前景视频的身份和运动信息的自适应注入，并引入背景保留分支、DiT融合块和扩展旋转位置嵌入（ERoPE）以提高合成效果。同时，作者构建了一个包含61,000组视频的数据集VideoComp，用于支持该任务。实验表明，该方法在保真度和一致性方面优于现有方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 12:10:13 GMT</pubDate>
</item>
<item>
<title>动态验证框架提升医疗大模型临床实用性</title>
<link>https://arxiv.org/abs/2509.02208</link>
<guid>https://arxiv.org/abs/2509.02208</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态验证框架提升医疗大模型临床应用效果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型在对话和推理能力上的进步，其在医疗领域的实际应用成为研究热点。然而，现有静态测试基准与真实临床决策之间存在显著差距。为此，本文提出一种动态验证框架，包含患者模拟器和临床评分生成器，构建高保真交互强化学习系统。基于此框架，开发了32B参数的医疗增强推理模型Baichuan-M2，采用改进的GRPO算法进行多阶段训练。在HealthBench测试中，Baichuan-M2表现优于其他开源模型及多数闭源模型，达到32分以上，接近GPT-5水平。研究表明，动态验证系统对提升医疗AI的实际应用价值至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02208" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 07:23:35 GMT</pubDate>
</item>
<item>
<title>基于印度宪法的LLM公平性增强框架AMBEDKAR</title>
<link>https://arxiv.org/abs/2509.02133</link>
<guid>https://arxiv.org/abs/2509.02133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AMBEDKAR框架以减少印度语境下的语言模型偏见。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在印度语境下可能反映的种姓和宗教偏见问题，并指出现有缓解策略多为西方中心，无法应对本地化问题。为此，作者提出AMBEDKAR框架，该框架受印度宪法之父B.R. Ambedkar的平等理念启发，通过引入一个基于印度AI宪法的解码层，在不修改模型参数的情况下提升输出的公平性与包容性。该方法利用一种推测性解码算法，在生成过程中主动减少偏见，将小语言模型作为生成器，大语言模型作为验证者，实现一种基于推测的公平性机制。实验结果显示，该方法可使偏见降低26.41%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 05:33:30 GMT</pubDate>
</item>
<item>
<title>优化器比较研究：AdamW与替代方案的公平评估</title>
<link>https://arxiv.org/abs/2509.02046</link>
<guid>https://arxiv.org/abs/2509.02046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，多数优化器的实际速度提升低于宣称，且随模型规模增加而减少。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在语言模型预训练中，尽管有声称其他优化器能带来1.4到2倍的速度提升，但AdamW仍占主导地位。研究指出，公平比较存在两个方法论问题：超参数调优不一致和评估设置有限。通过系统研究十种优化器在不同模型规模和数据量下的表现，发现合理的超参数调优和多尺度评估是关键。研究还发现，许多快速优化器如Muon和Soap使用矩阵作为预条件器，但其速度提升随模型规模增大而下降，仅在1.2B参数模型中达到1.1倍的提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 03:43:22 GMT</pubDate>
</item>
<item>
<title>通过任务向量迁移模型推理能力</title>
<link>https://arxiv.org/abs/2509.01363</link>
<guid>https://arxiv.org/abs/2509.01363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">模型推理能力可提取并迁移，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，将大型语言模型的推理能力以紧凑的任务向量形式提取并转移。研究使用两个初始化相同的Qwen2.5模型，一个经过监督微调，另一个经过群体相对策略优化。通过计算两者的参数差值，得到推理向量v_{reason}。该向量在多个推理基准测试中显著提升了模型性能，如GSM8K、HumanEval等。实验表明，该向量能有效增强模型推理能力，且在对抗条件下仍保持效果。此方法为利用已有模型资源提升新模型提供了可行路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 07:04:51 GMT</pubDate>
</item>
<item>
<title>VerlTool：统一的强化学习工具框架提升多轮交互性能</title>
<link>https://arxiv.org/abs/2509.01055</link>
<guid>https://arxiv.org/abs/2509.01055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VerlTool提升多轮强化学习工具交互效率与扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VerlTool，一个统一且模块化的强化学习工具框架，旨在解决现有ARLT方法在任务特定代码库、同步执行瓶颈和跨领域扩展性方面的不足。VerlTool通过上游对齐VeRL、标准化API管理工具、异步执行以及全面评估，实现了在6个ARLT领域的高性能表现。该框架支持多模态观察（文本/图像/视频）的多轮轨迹建模，提升了数学推理、知识问答、SQL生成等任务的性能，并提供了轻量级插件架构以降低开发成本，推动工具增强型强化学习研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 31 Aug 2025 21:45:18 GMT</pubDate>
</item>
<item>
<title>基于LLM的GUI代理在冒险游戏中的表现与改进研究</title>
<link>https://arxiv.org/abs/2509.01052</link>
<guid>https://arxiv.org/abs/2509.01052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示LLM驱动的GUI代理在完整剧情任务中的挑战与改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于大语言模型（LLM）的GUI代理在互动数字环境中的应用，特别是在冒险游戏中的表现。由于现有游戏基准缺乏多样性且未全面评估代理完成整个故事线的能力，作者提出了FlashAdventure基准，包含34款Flash冒险游戏，用于测试代理的长期记忆和行为规划能力。同时，研究引入了CUA-as-a-Judge评估工具和COAST框架，以提升代理在连续任务中的表现。实验表明，当前代理在完成完整剧情方面存在困难，而COAST通过弥补观察-行为差距提升了里程碑任务的完成率。然而，人类与最佳代理之间仍存在显著差距，需进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 31 Aug 2025 21:33:16 GMT</pubDate>
</item>
<item>
<title>基于思维链和上下文学习的文本到SQL框架研究</title>
<link>https://arxiv.org/abs/2509.00581</link>
<guid>https://arxiv.org/abs/2509.00581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SQL-of-Thought框架，提升文本到SQL转换效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何利用上下文学习和思维链方法构建强大的文本到SQL系统。作者提出了SQL-of-Thought框架，该框架将文本到SQL任务分解为模式链接、子问题识别、查询计划生成、SQL生成以及引导纠错循环。与以往仅依赖执行反馈的静态纠错方式不同，该框架引入了基于上下文学习的动态错误修正机制。实验表明，该方法在Spider数据集及其变体上取得了最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 14:27:12 GMT</pubDate>
</item>
<item>
<title>基于上下文感知融合的细粒度目标检测方法研究</title>
<link>https://arxiv.org/abs/2509.00578</link>
<guid>https://arxiv.org/abs/2509.00578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CAF方法提升细粒度目标检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对细粒度视觉领域（如车辆损伤评估）中的目标检测挑战，提出Context-Aware Fusion（CAF）方法。该方法通过跨注意力机制将全局场景上下文与局部目标提案特征进行融合，利用独立编码器生成全面的环境信息，使每个目标提案能够关注场景级理解。实验结果表明，该框架在CarDD基准测试中优于现有模型，为细粒度目标检测提供了新的性能标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 14:06:06 GMT</pubDate>
</item>
<item>
<title>Metis框架提升低比特量化大语言模型训练性能</title>
<link>https://arxiv.org/abs/2509.00404</link>
<guid>https://arxiv.org/abs/2509.00404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Metis解决低比特量化中的参数分布问题，提升模型训练稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文指出，各向异性参数分布是限制大语言模型在低比特量化下训练的关键障碍。由于少数主导奇异值导致数值范围过宽，与块状量化固有偏差冲突，造成训练不稳定和性能低下。为此，研究提出Metis框架，包含三个核心组件：(i) 利用谱分解与随机嵌入分离主导与尾部成分，压缩数值范围；(ii) 在谱域中自适应调整学习率，增强低频方向的表达能力；(iii) 双范围正则化器同步约束精度与参数分布，确保稳定训练。实验表明，使用Metis的FP8训练超越FP32基线，FP4训练达到与FP32相当的精度，为高效低比特大模型训练提供新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 04:09:08 GMT</pubDate>
</item>
<item>
<title>代理强化学习：从被动生成到自主决策的范式转变</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agentic RL将LLM转变为自主决策代理，推动AI向通用化发展。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了代理强化学习（Agentic RL）如何颠覆传统强化学习在大型语言模型中的应用，使LLM从被动的序列生成器转变为能够在复杂动态环境中自主决策的智能体。文章通过对比LLM-RL的单步马尔可夫决策过程与Agentic RL的时序扩展、部分可观测马尔可夫决策过程，阐明了这一范式转变。作者提出了一种双维度分类体系，涵盖核心代理能力（如规划、工具使用、记忆、推理等）及其在不同任务领域的应用。文中强调，强化学习是实现这些能力从静态模块向自适应行为转化的关键机制，并汇总了开源环境、基准测试和框架，为未来研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:46:26 GMT</pubDate>
</item>
<item>
<title>UI-TARS-2：提升GUI代理性能的系统性方法</title>
<link>https://arxiv.org/abs/2509.02544</link>
<guid>https://arxiv.org/abs/2509.02544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-TARS-2在GUI任务中表现优于前代模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UI-TARS-2，一个专注于图形用户界面（GUI）的自主代理模型。该模型通过数据飞轮、稳定多轮强化学习框架、混合GUI环境和统一沙盒平台，解决了数据扩展性、多轮强化学习、GUI操作限制和环境稳定性等挑战。实验结果显示，UI-TARS-2在多个GUI基准测试中表现优异，如Online-Mind2Web达到88.2分，OSWorld达到47.5分，并在游戏环境中实现了接近人类水平的性能。此外，该模型还能泛化到长期信息检索任务和软件工程基准测试，展示了其在多种任务中的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:44:45 GMT</pubDate>
</item>
<item>
<title>DARLING：提升大语言模型多样性与质量的强化学习框架</title>
<link>https://arxiv.org/abs/2509.02534</link>
<guid>https://arxiv.org/abs/2509.02534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DARLING通过优化多样性与质量，提升语言模型在创意任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DARLING的强化学习框架，旨在解决大语言模型在后训练过程中因追求准确性和帮助性而导致的多样性下降问题。DARLING通过引入一个学习得到的分区函数来衡量语义多样性，并将其与质量奖励结合，在在线强化学习中同时优化响应质量和多样性。实验表明，该方法在多个模型家族和规模上均表现出色，特别是在非验证任务（如指令遵循和创意写作）和验证任务（如竞赛数学）中，均优于仅优化质量的基线模型，提升了输出的新颖性和多样性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:38:47 GMT</pubDate>
</item>
<item>
<title>PACS：一种基于监督学习的强化学习框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2509.02522</link>
<guid>https://arxiv.org/abs/2509.02522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PACS通过监督学习提升LLM在数学推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PACS的新强化学习框架，用于提升大语言模型（LLMs）在具有可验证奖励的任务中的表现。PACS通过将结果奖励视为可预测的标签，将RLVR问题转化为监督学习任务，从而实现更稳定和高效的训练。实验表明，PACS在数学推理任务中优于现有的RLVR方法，如PPO和GRPO，在AIME 2025数据集上达到59.78%的pass@256得分，分别比PPO和GRPO高出13.32和14.36个百分点。该方法简单而有效，为LLMs的后训练提供了一个有前景的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 13:22:46 GMT</pubDate>
</item>
<item>
<title>SimpleTIR：提升多轮工具集成推理的稳定性方法</title>
<link>https://arxiv.org/abs/2509.02479</link>
<guid>https://arxiv.org/abs/2509.02479</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SimpleTIR通过过滤无效交互提升多轮推理稳定性。</p><br /><br /><p><strong>摘要：</strong> 文章提出SimpleTIR，一种用于增强多轮工具集成推理（TIR）稳定性的算法。该方法通过识别并过滤掉没有生成代码块或最终答案的无效交互轨迹，有效防止训练过程中的梯度爆炸问题，从而提升模型在数学推理任务上的表现。实验表明，SimpleTIR在AIME24基准测试中显著优于文本基线模型，同时鼓励模型发展出自我修正和交叉验证等高级推理策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02479" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 12:30:19 GMT</pubDate>
</item>
<item>
<title>MedDINOv3：基于视觉基础模型的医学影像分割方法</title>
<link>https://arxiv.org/abs/2509.02379</link>
<guid>https://arxiv.org/abs/2509.02379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedDINOv3提升医学影像分割性能，具有广泛应用潜力。</p><br /><br /><p><strong>摘要：</strong> 文章提出MedDINOv3，一种将DINOv3适配到医学影像分割的框架。针对医学影像与自然图像之间的领域差异以及ViT在医学任务中的表现不足，研究设计了多尺度token聚合结构，并在CT-3M数据集上进行领域适应预训练。实验表明，MedDINOv3在多个分割基准测试中达到或超过当前最佳性能，展示了视觉基础模型在医学影像分析中的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 10:44:43 GMT</pubDate>
</item>
<item>
<title>基于遗传算法的合成数据生成框架Genetic Prompt</title>
<link>https://arxiv.org/abs/2509.02040</link>
<guid>https://arxiv.org/abs/2509.02040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Genetic Prompt提升合成数据质量与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Genetic Prompt，一种结合遗传算法与大语言模型的合成数据生成框架。该方法将语义文本属性视为基因序列，并利用LLM模拟交叉与突变操作，从而增强数据质量和多样性，使合成数据更接近真实数据分布。同时引入主动学习策略优化父代选择，扩大后代搜索空间。实验表明，Genetic Prompt在多个NLP任务中表现优于现有方法，且对不同规模模型具有鲁棒性。融合原始训练集后，显著提升了下游模型性能，尤其在类别不平衡场景下效果明显。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.02040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 03:35:20 GMT</pubDate>
</item>
<item>
<title>基于VAR的文本引导图像编辑方法VARIN研究</title>
<link>https://arxiv.org/abs/2509.01984</link>
<guid>https://arxiv.org/abs/2509.01984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VARIN实现文本引导的图像精准编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出VARIN，一种针对视觉自回归模型（VAR）的图像编辑方法。该方法通过引入位置感知的argmax反演技术（LAI），生成逆Gumbel噪声，实现对源图像的精确重建和文本提示下的可控编辑。实验表明，VARIN在保持原图背景和结构细节的同时，能有效根据提示修改图像，验证了其在实际应用中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 02 Sep 2025 02:01:52 GMT</pubDate>
</item>
<item>
<title>OpenVision 2：简化架构提升训练效率</title>
<link>https://arxiv.org/abs/2509.01644</link>
<guid>https://arxiv.org/abs/2509.01644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenVision 2通过移除文本编码器提升训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenVision 2，这是一种对原始OpenVision模型进行简化设计的版本。通过移除文本编码器和对比损失，仅保留生成式损失，显著提升了训练效率。实验结果表明，OpenVision 2在多个多模态基准测试中表现与原模型相当，同时大幅减少了训练时间和内存消耗。例如，在ViT-L/14配置下，训练时间减少约1.5倍，内存使用减少约1.8倍。此外，该模型能够扩展至超过10亿参数规模，展现出强大的可扩展性。作者认为，这种轻量级、纯生成式的范式对未来多模态基础模型的视觉编码器发展具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 13:38:21 GMT</pubDate>
</item>
<item>
<title>基于同侪学习的大型视觉语言模型对齐方法</title>
<link>https://arxiv.org/abs/2509.01610</link>
<guid>https://arxiv.org/abs/2509.01610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">同侪学习提升LVLM对齐效果，无需大量人工标注数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种受人类协作学习启发的同侪学习框架，用于改进大型视觉语言模型（LVLMs）的对齐方法。该方法通过一组LVLM组成的面板，进行迭代自我优化，模拟课堂学习环境，生成、评估并优化输出。实验表明，该方法在多个基准测试中显著提升了模型性能，平均得分从48%提高到57%，证明了同侪评估作为自监督对齐替代方案的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 12:43:48 GMT</pubDate>
</item>
<item>
<title>Keye-VL-1.5：提升视频理解能力的多模态大模型创新</title>
<link>https://arxiv.org/abs/2509.01563</link>
<guid>https://arxiv.org/abs/2509.01563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Keye-VL-1.5通过三项创新提升视频理解性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视频理解方面面临挑战，主要由于视频内容动态性强且信息密集。Keye-VL-1.5通过三项关键创新解决这一问题：首先，采用慢速-快速视频编码策略，根据帧间相似性动态分配计算资源；其次，实施四阶段预训练方法，扩展模型上下文长度至128K tokens；最后，构建全面的后训练流程，提升推理能力和与人类偏好的对齐。实验表明，Keye-VL-1.5在视频理解任务中表现优异，同时保持在多模态基准上的竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 11:46:58 GMT</pubDate>
</item>
<item>
<title>多模态医学图像检索模型M3Ret的构建与应用</title>
<link>https://arxiv.org/abs/2509.01360</link>
<guid>https://arxiv.org/abs/2509.01360</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3Ret实现跨模态医学图像检索，提升临床决策效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出M3Ret，一个无需模态特异性设计的统一视觉编码器，通过生成式和对比式自监督学习方法，在多种医学图像数据上取得优异表现。该模型在零样本图像检索任务中超越现有基准，并展现出跨模态对齐能力和对未见过的MRI任务的泛化能力，为多模态医学图像理解提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01360" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 06:59:39 GMT</pubDate>
</item>
<item>
<title>基于双视角的点云自监督学习方法Point-PQAE</title>
<link>https://arxiv.org/abs/2509.01250</link>
<guid>https://arxiv.org/abs/2509.01250</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Point-PQAE实现点云双视角重建，提升自监督学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出Point-PQAE，一种基于双视角的点云自监督学习方法。该方法通过生成两个解耦的点云视图，并相互重建，提升了预训练的难度和信息量。为实现这一目标，作者首次引入了点云视图生成的裁剪机制，并设计了一种新的位置编码来表示两个解耦视图之间的3D相对位置。实验结果表明，与单视图自重建方法Point-MAE相比，Point-PQAE在ScanObjectNN数据集上的表现分别提升了6.5%、7.0%和6.7%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01250" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 04:42:17 GMT</pubDate>
</item>
<item>
<title>无需蒸馏的自动化文档提取框架</title>
<link>https://arxiv.org/abs/2509.01215</link>
<guid>https://arxiv.org/abs/2509.01215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需蒸馏的自动化文档提取框架，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种完全自动化的文档提取框架，分为两个阶段。第一阶段生成大规模、多样化的合成数据以提升模型初始性能；第二阶段通过自我优化方法，将模型从合成数据迁移到真实文档。具体包括使用微调模型标注真实文档、应用过滤策略验证标注质量，并在验证数据集上重新训练模型。该过程不断迭代，提升模型转换能力和数据质量。基于此框架训练的POINTS-Reader模型在多个任务中表现优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.01215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 01 Sep 2025 03:54:18 GMT</pubDate>
</item>
<item>
<title>基于批评模型的多模态生成与评估统一系统</title>
<link>https://arxiv.org/abs/2509.00676</link>
<guid>https://arxiv.org/abs/2509.00676</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习将批评模型用于生成任务，提升多模态系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文挑战传统视觉语言模型中批评模型仅用于评估的惯例，提出将偏好标注数据转化为可验证训练信号，并在基础生成模型上进行强化学习，构建了LLaVA-Critic-R1。该模型不仅在评估任务中表现优异，还能作为政策模型，在26个视觉推理任务中超越专业模型。进一步优化得到LLaVA-Critic-R1+，在MMMU基准上达到71.9的SOTA成绩。此外，测试时应用自我批评机制，显著提升了推理任务的表现，展示了统一评估与生成模型的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00676" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 30 Aug 2025 23:08:02 GMT</pubDate>
</item>
<item>
<title>通用深度研究系统UDR的提出与应用</title>
<link>https://arxiv.org/abs/2509.00244</link>
<guid>https://arxiv.org/abs/2509.00244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UDR允许用户自定义深度研究策略，无需额外训练。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Universal Deep Research (UDR)，这是一种通用的智能代理系统，能够围绕任何语言模型运行。UDR使用户能够创建、编辑和优化自己的深度研究策略，而无需进行额外的训练或微调。为了展示系统的通用性，作者提供了最小化、扩展性和密集型研究策略示例，并设计了用户界面以方便实验和使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2509.00244" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 17:22:19 GMT</pubDate>
</item>
<item>
<title>AI代理社会的制度设计与权力平衡模拟研究</title>
<link>https://arxiv.org/abs/2508.19562</link>
<guid>https://arxiv.org/abs/2508.19562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI代理在不同制度下自我治理，揭示权力与公共利益的平衡机制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Democracy-in-Silico模拟系统，该系统通过高级AI代理在不同制度框架下进行自我治理，探索人工智能时代人类身份的意义。这些AI代理被赋予复杂心理特征，如创伤记忆和隐藏动机，并在预算危机等压力下参与讨论、立法和选举。研究提出了一种新的衡量指标——权力保留指数（PPI），用于评估代理优先考虑自身权力而非公共利益的行为。结果表明，结合宪法AI宪章和协商协议的制度设计能有效减少腐败行为，提升政策稳定性和公民福祉。研究为未来AI社会的制度设计提供了参考，并促使人们重新思考在人机共治时代人类的核心责任与价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:44:41 GMT</pubDate>
</item>
<item>
<title>基于输入重构的多智能体框架提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.20931</link>
<guid>https://arxiv.org/abs/2508.20931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IRMA框架提升语言模型在动态环境中的推理与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在多轮对话环境中作为自主代理的局限性，特别是在推理一致性、领域规则遵守和长期信息提取方面的问题。通过分析常见错误并尝试输入重构方法，作者提出了Input-Reformulation Multi-Agent (IRMA)框架，该框架通过引入相关领域规则和工具建议来优化代理决策。实验结果表明，IRMA在整体pass^5得分上分别优于ReAct、Function Calling和Self-Reflection 16.1%、12.7%和19.1%，展示了其在动态环境中的优越可靠性与一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 11:57:33 GMT</pubDate>
</item>
<item>
<title>表到报告任务与T2R-bench基准构建</title>
<link>https://arxiv.org/abs/2508.19813</link>
<guid>https://arxiv.org/abs/2508.19813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出表到报告任务及T2R-bench基准，评估大模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于将表格信息转化为报告的任务，指出当前大语言模型在该任务上仍存在显著不足。为解决这一问题，作者提出了表到报告任务，并构建了多语言、多行业的T2R-bench基准，包含457个真实工业表格，涵盖19个领域和4种表格类型。同时，设计了一套评估标准以公平衡量报告生成质量。实验表明，即使最先进的模型如Deepseek-R1，在T2R-bench上的综合得分仅为62.71，说明该任务仍有较大提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 07:55:40 GMT</pubDate>
</item>
<item>
<title>ALLaM-34B阿拉伯语大模型性能评估与应用分析</title>
<link>https://arxiv.org/abs/2508.17378</link>
<guid>https://arxiv.org/abs/2508.17378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ALLaM-34B在多种阿拉伯语任务中表现优异，具备实际部署潜力。</p><br /><br /><p><strong>摘要：</strong> 本文对基于ALLaM-34B的HUMAIN Chat系统进行了全面的UI级评估。通过涵盖现代标准阿拉伯语、地区方言、混合语言、事实知识、算术与时间推理、创意生成和对抗性安全等多类提示的测试集，收集了115个输出结果，并由三个前沿大模型（GPT-5、Gemini 2.5 Pro、Claude Sonnet-4）进行评分。结果显示，ALLaM-34B在生成任务和语言混合方面表现最佳（4.92/5），在标准阿拉伯语处理、推理能力和方言准确性上也表现出色，同时在安全性任务中保持稳定。这些结果表明ALLaM-34B是一个技术强大且适合实际应用的阿拉伯语大模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 10:32:15 GMT</pubDate>
</item>
<item>
<title>基于生物启发的空间认知框架提升智能体导航能力</title>
<link>https://arxiv.org/abs/2508.17198</link>
<guid>https://arxiv.org/abs/2508.17198</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BSC-Nav构建结构化空间记忆，提升智能体导航与适应能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了BSC-Nav，一个基于生物启发的空间认知框架，用于构建和利用结构化空间记忆。该框架从自我中心轨迹和上下文线索中生成外在认知地图，并动态检索与语义目标对齐的空间知识。结合多模态大语言模型，BSC-Nav在多种导航任务中表现出色，具备强大的零样本泛化能力和在真实物理世界中的多样化行为支持，为通用空间智能提供了可扩展且生物基础的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17198" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 23 Aug 2025 23:20:48 GMT</pubDate>
</item>
<item>
<title>PVPO：基于优势参考锚点的高效强化学习方法</title>
<link>https://arxiv.org/abs/2508.21104</link>
<guid>https://arxiv.org/abs/2508.21104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PVPO通过参考锚点和预采样提升强化学习效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PVPO的高效强化学习方法，旨在解决传统群体策略在复杂任务中因依赖多轮采样和比较而导致的局部最优和计算成本高的问题。该方法引入了一个优势参考锚点，并结合数据预采样技术，利用参考模型提前进行 rollout 并计算奖励得分作为参考。这一机制有效减少了组内比较带来的累积偏差，降低了对采样次数的依赖，同时提升了训练效率。实验表明，PVPO在多个数据集上均达到了最先进的性能，展现出良好的泛化能力和模型规模适应性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 05:18:26 GMT</pubDate>
</item>
<item>
<title>SuperSimpleNet：一种高效且适应性强的表面缺陷检测模型</title>
<link>https://arxiv.org/abs/2508.19060</link>
<guid>https://arxiv.org/abs/2508.19060</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SuperSimpleNet实现多监督场景下的高效表面缺陷检测。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SuperSimpleNet的高效且适应性强的表面缺陷检测模型，能够处理多种监督场景（包括无监督、弱监督、混合监督和全监督）。该模型基于SimpleNet构建，引入了合成异常生成、增强分类头和改进的学习过程，实现了在所有四种监督模式下的高效训练。实验结果表明，SuperSimpleNet在多个基准数据集上表现优异，推理时间低于10毫秒，具有极高的实用价值。该模型为工业制造中的表面缺陷检测提供了新的解决方案，推动了学术研究与实际应用之间的融合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19060" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 10:20:21 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型中的新发现与挑战</title>
<link>https://arxiv.org/abs/2508.21188</link>
<guid>https://arxiv.org/abs/2508.21188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示RL在LLM中出现的反直觉现象及其适用条件。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）在大语言模型（LLMs）中的最新进展，指出一些反直觉现象，如单个训练样本可媲美整个数据集、奖励信号不需精准等。研究发现，这些现象仅在预训练模型与任务高度对齐时成立，而在更具挑战性的场景下，传统RL方法仍更有效。通过系统实验验证，作者强调了模型-任务对齐度在RL应用中的关键作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21188" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 16:02:10 GMT</pubDate>
</item>
<item>
<title>基于时间残差连接的深度非训练循环神经网络研究</title>
<link>https://arxiv.org/abs/2508.21172</link>
<guid>https://arxiv.org/abs/2508.21172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepResESN提升长期时间建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于时间残差连接的深度非训练循环神经网络（DeepResESNs），旨在解决传统Echo State Networks（ESNs）在处理长期信息时的不足。通过引入层次化的非训练残差递归层，显著增强了网络的记忆能力和长期时间建模性能。研究分析了不同正交配置对网络动态的影响，并提供了数学分析以确保系统稳定性。实验表明，该方法在多个时间序列任务中优于传统的浅层和深层RC模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 15:22:02 GMT</pubDate>
</item>
<item>
<title>科学大语言模型的发展与数据驱动的未来</title>
<link>https://arxiv.org/abs/2508.21148</link>
<guid>https://arxiv.org/abs/2508.21148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">科学大语言模型依赖复杂数据，推动科研变革。</p><br /><br /><p><strong>摘要：</strong> 本文全面综述了科学大语言模型（Sci-LLMs）的发展，强调其与科学数据之间的协同演进关系。文章提出了科学数据的统一分类和科学知识的层级模型，分析了科学语料在多模态、跨尺度和领域特定方面的挑战。通过对270多个预训练/后训练数据集的系统回顾，揭示了Sci-LLMs对异构、多尺度、不确定性数据的需求。同时，文章探讨了190多个基准数据集的评估趋势，并提出半自动化标注和专家验证等解决方案。最后，展望了基于Sci-LLMs的闭环系统，实现自主实验与知识更新，为构建可信且持续演化的AI系统提供路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 14:30:52 GMT</pubDate>
</item>
<item>
<title>Post-training Quantization对YOLO模型在不同精度下的鲁棒性评估</title>
<link>https://arxiv.org/abs/2508.19600</link>
<guid>https://arxiv.org/abs/2508.19600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究YOLO模型在不同精度下的鲁棒性表现。</p><br /><br /><p><strong>摘要：</strong> 本文对YOLO系列模型（从纳米到超大尺寸）在多种精度格式（FP32、FP16、Dynamic UINT8和Static INT8）下的鲁棒性进行了全面评估。通过引入一种基于退化图像的校准策略，测试了模型在噪声、模糊、低对比度和JPEG压缩等七种退化条件下的性能。结果显示，虽然Static INT8模型在速度上有显著提升，但其鲁棒性并未在大多数情况下优于标准校准方法。仅在部分大模型和特定噪声条件下表现出改进，表明模型规模可能影响校准效果。该研究为在非控制环境下部署量化检测器提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 02:20:38 GMT</pubDate>
</item>
<item>
<title>基于多模态的物理定律自动发现模型VIPER-R1</title>
<link>https://arxiv.org/abs/2508.17380</link>
<guid>https://arxiv.org/abs/2508.17380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VIPER-R1通过视觉与符号推理发现物理定律，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为VIPER-R1的多模态模型，用于从观测数据中自动发现物理定律。该模型结合视觉感知、轨迹数据和符号推理，模拟科学家的发现过程。通过运动结构诱导训练，并利用因果链思维和奖励引导符号校准优化公式结构。在推理阶段，VIPER-R1先提出高置信度的符号假设，再调用符号回归工具进行残差重对齐，以提高理论模型与实证数据的一致性。为支持研究，作者构建了包含5000个实例的多模态语料库PhysSymbol。实验表明，VIPER-R1在准确性和可解释性方面均优于现有先进模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 10:34:21 GMT</pubDate>
</item>
<item>
<title>EduRABSA：首个教育评论的公开ABSA数据集</title>
<link>https://arxiv.org/abs/2508.17008</link>
<guid>https://arxiv.org/abs/2508.17008</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EduRABSA是首个教育领域的公开ABSA数据集，支持多任务分析。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EduRABSA，这是首个公开的教育评论方面情感分析（ABSA）数据集，涵盖课程、教师和大学三个主题类型，并支持所有主要ABSA任务，包括隐式方面和隐式观点提取。同时，作者还发布了ASQE-DPT工具，用于高效进行数据标注。这些资源有助于推动教育领域ABSA研究的发展，提升研究透明度和可复现性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17008" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 23 Aug 2025 08:38:40 GMT</pubDate>
</item>
<item>
<title>AHELM：首个全面评估音频语言模型的基准测试</title>
<link>https://arxiv.org/abs/2508.21376</link>
<guid>https://arxiv.org/abs/2508.21376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AHELM是首个全面评估音频语言模型的基准，涵盖10个关键维度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AHELM，这是一个用于评估音频语言模型（ALMs）的综合性基准测试。AHELM整合了多个数据集，包括两个新的合成音频-文本数据集PARADE和CoRe-Bench，以全面衡量ALMs在音频感知、知识、推理、情绪检测、偏见、公平性、多语言性、鲁棒性、毒性及安全性等10个方面的能力。研究还标准化了提示、推理参数和评估指标，以确保模型间的公平比较。实验测试了14个开源和闭源ALMs以及3个基线系统，结果显示Gemini 2.5 Pro在5个方面表现最佳，但存在群体不公平性；基线系统也表现出合理性能。所有数据和结果已公开，AHELM将持续更新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 03:40:39 GMT</pubDate>
</item>
<item>
<title>Think in Games：通过游戏环境提升大语言模型的程序性知识</title>
<link>https://arxiv.org/abs/2508.21365</link>
<guid>https://arxiv.org/abs/2508.21365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TiG框架提升LLM在交互任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出Think in Games (TiG)框架，旨在帮助大语言模型（LLMs）通过与游戏环境的直接互动获得程序性知识。传统强化学习方法虽然能获取程序性知识，但存在黑箱问题且需要大量数据。而LLMs虽具备丰富的世界知识和推理能力，却难以将其转化为动态决策。TiG将强化学习决策转化为语言建模任务，使LLMs生成语言引导的策略，并通过环境反馈进行迭代优化。实验表明，TiG有效弥补了陈述性知识与程序性知识之间的差距，在数据和计算资源消耗上远低于传统方法，同时提供自然语言解释以提高透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 03:13:39 GMT</pubDate>
</item>
<item>
<title>Jina Code Embeddings：跨语言代码检索与语义相似性识别</title>
<link>https://arxiv.org/abs/2508.21290</link>
<guid>https://arxiv.org/abs/2508.21290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Jina Code Embeddings 提供高效的代码检索与语义分析能力。</p><br /><br /><p><strong>摘要：</strong> Jina Code Embeddings 是一个创新的代码嵌入模型套件，能够根据自然语言查询检索代码、回答技术问题，并在不同编程语言中识别语义相似的代码片段。该模型基于预训练的自回归主干网络，通过最后令牌池化生成嵌入表示。尽管模型规模较小，但仍表现出卓越的性能，验证了其在代码嵌入建模方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 21:18:15 GMT</pubDate>
</item>
<item>
<title>视频数据在3D生成中的应用与探索</title>
<link>https://arxiv.org/abs/2508.20470</link>
<guid>https://arxiv.org/abs/2508.20470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">视频数据助力3D生成，提升空间一致性与语义合理性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何利用视频数据提升3D资产生成的效果。由于3D数据在互联网上相对稀缺，视频因其包含丰富的语义信息和多视角内容，成为一种有效的补充来源。文章介绍了首个具有多视角标注的大规模视频数据集Droplet3D-4M，并训练了一个支持图像和密集文本输入的生成模型Droplet3D。实验表明，该方法能够生成空间一致且语义合理的3D内容，并具备扩展至场景级应用的潜力。研究结果表明，视频中的常识先验对3D创作有显著帮助。相关资源已全部开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 02:39:41 GMT</pubDate>
</item>
<item>
<title>HERMES：基于人类运动数据的移动双臂灵巧操作框架</title>
<link>https://arxiv.org/abs/2508.20085</link>
<guid>https://arxiv.org/abs/2508.20085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HERMES将人类动作转化为机器人可控行为，提升灵巧操作能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出HERMES框架，旨在通过人类运动数据赋予机器人多样化的操作技能。该框架采用统一的强化学习方法，将多源人类手部动作转化为物理上可行的机器人行为，并通过端到端深度图像模拟到现实迁移方法减少仿真与现实之间的差距。此外，HERMES结合闭环PnP定位机制，增强在复杂环境中的自主导航与操作能力。实验表明，HERMES在多种真实场景中表现出良好的泛化能力，成功完成多项复杂的移动双臂灵巧操作任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 13:53:46 GMT</pubDate>
</item>
<item>
<title>动态调整数据混合策略提升语言模型性能</title>
<link>https://arxiv.org/abs/2508.17677</link>
<guid>https://arxiv.org/abs/2508.17677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TiKMiX通过动态调整数据混合提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为TiKMiX的动态数据混合方法，用于优化语言模型的预训练过程。传统静态混合策略无法适应模型在训练过程中对不同数据域的学习偏好变化。TiKMiX引入了Group Influence指标，以高效评估数据域对模型的影响，并将数据混合问题转化为最大化影响的分布搜索问题。该方法包括TiKMiX-D和TiKMiX-M两种实现方式，分别通过直接优化和回归预测来提升混合效果。实验表明，TiKMiX-D在计算资源减少80%的情况下超越了现有最佳方法，而TiKMiX-M在多个基准测试中平均提升了2%的性能。研究还发现，模型的数据偏好会随着训练进展和规模变化，动态调整数据混合可有效缓解静态比例带来的数据消化不足问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 01:18:32 GMT</pubDate>
</item>
<item>
<title>基于CLIP的对称性检测方法CLIPSym</title>
<link>https://arxiv.org/abs/2508.14197</link>
<guid>https://arxiv.org/abs/2508.14197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLIPSym利用CLIP模型提升图像对称性检测效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于预训练视觉-语言模型CLIP的对称性检测方法CLIPSym，通过结合CLIP的图像和语言编码器以及一种混合Transformer与G-卷积的旋转等变解码器，实现对旋转和反射对称性的检测。为更好地利用CLIP的语言编码器，作者设计了语义感知提示分组（SAPG）技术，通过聚合多种常见物体提示来增强语义信息的整合。实验表明，CLIPSym在三个标准对称性检测数据集上优于当前最先进的方法，并通过消融实验验证了CLIP预训练、等变解码器和SAPG的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 14:43:14 GMT</pubDate>
</item>
<item>
<title>UItron：面向GUI自动化的开源基础模型</title>
<link>https://arxiv.org/abs/2508.21767</link>
<guid>https://arxiv.org/abs/2508.21767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UItron提升GUI自动化能力，推动AI通用智能发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UItron，一个面向GUI自动化的开源基础模型，具备先进的GUI感知、定位和规划能力。由于操作轨迹稀缺、交互基础设施不足以及基础模型能力有限，构建GUI代理仍具挑战。UItron通过系统数据工程和交互环境建设，提升了训练效果，并采用监督微调与课程强化学习框架，在多种GUI场景中实现复杂推理与探索。实验表明，UItron在中文移动应用中表现出色，填补了现有解决方案在中文支持方面的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 12:40:57 GMT</pubDate>
</item>
<item>
<title>Morae：提升盲人和低视力用户UI交互体验的混合决策代理</title>
<link>https://arxiv.org/abs/2508.21456</link>
<guid>https://arxiv.org/abs/2508.21456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Morae通过让用户参与关键决策，提升盲人用户任务完成度。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为Morae的UI代理系统，旨在改善盲人和低视力用户在复杂界面中的操作体验。与传统UI代理不同，Morae在执行任务过程中会识别关键决策点并暂停，以征求用户意见。该系统结合大模型、UI代码和截图来理解用户需求，并在需要选择时提示用户进行澄清。实验表明，Morae相比基线代理能帮助用户完成更多任务，并更符合其个人偏好，体现了混合主动性交互的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 29 Aug 2025 05:39:00 GMT</pubDate>
</item>
<item>
<title>R-4B：一种自适应思考的多模态大语言模型</title>
<link>https://arxiv.org/abs/2508.21113</link>
<guid>https://arxiv.org/abs/2508.21113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R-4B通过自适应决策提升多模态模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出R-4B，一种具备自适应思考能力的多模态大语言模型。该模型通过双模式退火技术赋予其思考与非思考能力，并利用双模式策略优化（BPO）提升判断是否激活思考过程的准确性。R-4B在多个基准测试中表现优异，尤其在推理密集型任务中展现出与更大模型相当的性能，同时降低了计算成本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:48:19 GMT</pubDate>
</item>
<item>
<title>EO-Robotics：多模态具身推理与机器人控制的新模型与数据集</title>
<link>https://arxiv.org/abs/2508.21112</link>
<guid>https://arxiv.org/abs/2508.21112</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EO-Robotics提升机器人多模态交互与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EO-Robotics系统，包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过融合视觉、文本、视频和动作的预训练，在多模态具身推理和机器人控制方面表现出色。其核心在于统一架构和高质量的多模态数据。实验表明，该模型在开放世界任务中具有强大的泛化能力，为未来具身智能系统提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21112" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:26:15 GMT</pubDate>
</item>
<item>
<title>AI代码生成安全评估基准A.S.E的提出与实验分析</title>
<link>https://arxiv.org/abs/2508.18106</link>
<guid>https://arxiv.org/abs/2508.18106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">A.S.E基准提升AI代码生成安全性评估水平。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型在软件工程中的广泛应用，其生成代码的安全性评估变得尤为重要。现有基准存在局限，如仅关注孤立代码片段、评估方法不稳定且难以复现，未能考虑输入上下文对输出安全性的影响。为此，研究者提出了A.S.E（AI Code Generation Security Evaluation）基准，该基准基于真实仓库中的已知漏洞构建任务，保留完整的仓库上下文，采用可复现的容器化评估框架进行稳定、可审计的安全性评估。实验结果显示，Claude-3.7-Sonnet表现最佳，开源与闭源模型之间的安全差距较小，且快速推理策略在安全补丁生成中优于复杂推理策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 11:11:11 GMT</pubDate>
</item>
<item>
<title>TalkVid：解决语音驱动人脸合成多样性不足的新数据集</title>
<link>https://arxiv.org/abs/2508.13618</link>
<guid>https://arxiv.org/abs/2508.13618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TalkVid提升语音驱动人脸合成的多样性与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 文章提出TalkVid，一个大规模、高质量且多样化的视频数据集，包含1244小时来自7729位不同说话者的视频。该数据集通过多阶段自动化筛选流程确保视频质量与面部细节，并经过人工验证以保证可靠性。同时，研究团队构建了TalkVid-Bench评估集，用于更精准地衡量模型在不同人口统计和语言群体中的表现。实验表明，基于TalkVid训练的模型在跨数据集泛化方面优于以往模型，但分析也揭示了子群体间的性能差异，强调了未来研究中多样化评估的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 04:31:15 GMT</pubDate>
</item>
<item>
<title>OneReward：基于单一奖励模型的多任务生成框架</title>
<link>https://arxiv.org/abs/2508.21066</link>
<guid>https://arxiv.org/abs/2508.21066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OneReward提升多任务生成模型性能，无需任务特定微调。</p><br /><br /><p><strong>摘要：</strong> 本文提出OneReward，一个统一的强化学习框架，通过单一视觉语言模型作为生成奖励模型，在不同任务和评估标准下增强模型的生成能力。该框架应用于掩码引导的图像生成任务，包括图像填充、扩展、对象移除和文本渲染等子任务。相比传统方法依赖任务特定监督微调，OneReward通过多任务强化学习直接在预训练模型上训练，提升了泛化能力和效率。实验表明，OneReward在多个评估维度上优于商业和开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于多模态预训练的社交行为感知模型Social-MAE</title>
<link>https://arxiv.org/abs/2508.17502</link>
<guid>https://arxiv.org/abs/2508.17502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Social-MAE在多模态社交任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文提出Social-MAE，一种基于改进版对比音频视觉掩码自编码器（CAV-MAE）的多模态预训练模型。该模型通过自监督方式在大规模社交数据集VoxCeleb2上进行预训练，能够处理更多帧输入。实验表明，Social-MAE在情感识别、笑声检测和明显人格估计等任务中取得了最先进的性能，验证了领域内自监督预训练的有效性。代码和模型权重已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 15:49:48 GMT</pubDate>
</item>
<item>
<title>提升大语言模型在说服对话中的可信度评估与训练方法</title>
<link>https://arxiv.org/abs/2508.17450</link>
<guid>https://arxiv.org/abs/2508.17450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出DuET-PD框架，提升LLM在说服对话中的可信度。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuET-PD框架，用于评估大语言模型在说服对话中的可信度，涵盖纠正性与误导性说服类型及知识与安全领域。研究发现，即使GPT-4o在持续误导性说服下仅能取得27.32%的准确率，且新模型表现出更高的盲从倾向。为解决此问题，作者引入Holistic DPO训练方法，平衡正负说服样本，显著提升了Llama-3.1-8B-Instruct在安全场景下的表现。该研究为构建更可靠、适应性强的多轮对话系统提供了路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 13:08:37 GMT</pubDate>
</item>
<item>
<title>多视角3D点追踪技术的创新与应用</title>
<link>https://arxiv.org/abs/2508.21060</link>
<guid>https://arxiv.org/abs/2508.21060</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多视角3D点追踪方法，提升动态场景跟踪精度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于数据驱动的多视角3D点追踪系统，能够通过多个摄像头视角对动态场景中的任意点进行精准跟踪。与传统单目追踪方法相比，该系统克服了深度模糊和遮挡问题；同时，相较于需要大量摄像头和复杂优化的多相机方法，本系统仅需4个摄像头即可实现在线实时跟踪。通过融合多视角特征并结合k近邻相关性和Transformer更新机制，系统在遮挡条件下仍能可靠地估计长距离3D对应关系。实验结果显示，在两个真实世界基准测试中，分别达到了3.1厘米和2.0厘米的中位轨迹误差。该方法适用于不同视角配置和视频长度，具有良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21060" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:58:20 GMT</pubDate>
</item>
<item>
<title>通过ROSI方法提升大语言模型的安全对齐</title>
<link>https://arxiv.org/abs/2508.20766</link>
<guid>https://arxiv.org/abs/2508.20766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ROSI方法提升大语言模型拒绝有害请求的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Rank-One Safety Injection (ROSI) 的白盒方法，通过永久性地引导模型激活向拒绝有害请求的子空间偏移，从而增强大语言模型的安全对齐。ROSI是一种无需微调的简单权重修改方法，仅需少量有害与无害指令对即可计算所需的安全方向。实验表明，ROSI在保持模型在MMLU、HellaSwag和Arc等基准测试中性能的同时，显著提高了模型的安全拒绝率。此外，ROSI还能重新对齐未受限制的模型，展示了其作为最后一道安全防线的有效性。结果表明，有针对性的可解释权重调整是提升LLM安全性的低成本且高效手段。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 09:22:33 GMT</pubDate>
</item>
<item>
<title>Dress&amp;Dance：基于视频扩散的高质量虚拟试穿框架</title>
<link>https://arxiv.org/abs/2508.21070</link>
<guid>https://arxiv.org/abs/2508.21070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dress&amp;Dance生成高质量虚拟试穿视频，支持多种衣物和动作。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Dress&amp;Dance，一个视频扩散框架，能够生成5秒长、24帧每秒的高质量虚拟试穿视频，分辨率为1152x720。该框架仅需用户提供一张图像，即可生成穿着指定衣物并按照参考视频动作移动的视频。其核心是CondNet，一种利用注意力机制统一多模态输入（文本、图像和视频）的条件网络，提升了衣物对齐和动作保真度。CondNet通过多阶段渐进式训练，结合有限的视频数据和大量图像数据进行训练。Dress&amp;Dance在开放源代码和商业解决方案中表现优异，提供了高质量且灵活的虚拟试穿体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>OnGoal：提升用户在LLM对话中目标管理的界面设计</title>
<link>https://arxiv.org/abs/2508.21061</link>
<guid>https://arxiv.org/abs/2508.21061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OnGoal帮助用户更有效地跟踪和评估对话目标进展。</p><br /><br /><p><strong>摘要：</strong> 随着与大型语言模型（LLM）的多轮对话日益复杂，用户需要更好的方式来评估和回顾对话目标的进展。本文介绍了OnGoal，一个基于LLM的聊天界面，能够提供实时的目标对齐反馈、评估结果的解释以及目标进展的时间视图，从而帮助用户更高效地管理对话目标。通过一项20名参与者参与的写作任务研究，OnGoal被证明比传统聊天界面更有效，使用户在更短时间内达成目标，并探索新的提示策略以克服沟通障碍。研究结果为未来改进LLM聊天界面的设计提供了重要启示，包括增强目标沟通、降低认知负担、提高互动性以及通过反馈优化LLM性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:58:29 GMT</pubDate>
</item>
<item>
<title>基于稀疏注意力路由的长视频生成方法</title>
<link>https://arxiv.org/abs/2508.21058</link>
<guid>https://arxiv.org/abs/2508.21058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MoC模块提升长视频生成中的记忆与一致性。</p><br /><br /><p><strong>摘要：</strong> 长视频生成面临长期上下文记忆的挑战，传统扩散Transformer因自注意力的二次复杂度难以扩展。本文将长视频生成视为内部信息检索任务，引入Mixture of Contexts (MoC)模块，通过动态选择关键片段和强制锚点进行注意力计算，实现高效且一致的长视频生成。该方法在数据扩展和路由稀疏化过程中，有效保留了视频中的身份、动作和场景信息，实现了近线性计算效率，为大规模视频生成提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:57:55 GMT</pubDate>
</item>
<item>
<title>FakeParts：针对局部深度伪造视频的检测挑战与基准数据集</title>
<link>https://arxiv.org/abs/2508.21052</link>
<guid>https://arxiv.org/abs/2508.21052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FakeParts是新型局部深度伪造，难以检测。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FakeParts，一种通过局部区域或时间片段的细微篡改生成的深度伪造技术，相较于完全合成内容更具欺骗性。为应对检测能力的不足，作者提出了FakePartsBench，首个专注于局部深度伪造的大规模基准数据集，包含25000多个带有像素级和帧级标注的视频。实验表明，FakeParts使人类检测准确率下降30%以上，同时对现有检测模型也造成显著影响。该研究揭示了当前深度伪造检测方法的脆弱性，并提供了开发更稳健检测手段的资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:55:14 GMT</pubDate>
</item>
<item>
<title>CogVLA：一种高效多模态视觉-语言-动作框架</title>
<link>https://arxiv.org/abs/2508.21046</link>
<guid>https://arxiv.org/abs/2508.21046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CogVLA提升视觉-语言-动作模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出CogVLA，一种基于认知对齐的视觉-语言-动作框架，通过指令驱动的路由和稀疏化技术提升模型效率与性能。该框架包含三个阶段：EFA-Routing将指令信息注入视觉编码器，实现视觉特征的有选择性聚合；LFP-Routing通过剪枝引入动作意图，实现令牌级稀疏性；CAtten结合因果视觉-语言注意力与双向动作并行解码，提升动作生成准确性。实验表明，CogVLA在LIBERO基准和真实机器人任务中分别达到97.4%和70.0%的成功率，同时降低训练成本和推理延迟。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.21046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 13:50:58 GMT</pubDate>
</item>
<item>
<title>工具增强语言模型在事实回忆中的优势</title>
<link>https://arxiv.org/abs/2508.20755</link>
<guid>https://arxiv.org/abs/2508.20755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">工具使用提升语言模型的事实回忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了工具增强语言模型在事实回忆方面的优势，指出通过外部检索等工具使用方式，可以实现无限的事实记忆，而仅依赖模型权重的记忆能力受限于参数数量。实验结果表明，使用工具的模型在事实回忆任务中表现优于单纯依赖记忆的模型。此外，研究还发现对预训练大语言模型进行工具使用和通用规则的教学比直接微调事实更有效。该研究为工具增强的工作流提供了理论和实证基础，证明其不仅实用且更具可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 09:12:19 GMT</pubDate>
</item>
<item>
<title>基于偏好奖励的GRPO方法与统一文本到图像基准研究</title>
<link>https://arxiv.org/abs/2508.20751</link>
<guid>https://arxiv.org/abs/2508.20751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Pref-GRPO方法提升文本到图像生成稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于GRPO的强化学习方法在文本到图像生成中的应用，指出当前基于点对评分模型的方法容易受到奖励黑客攻击。为此，作者提出Pref-GRPO，通过偏好奖励机制替代传统评分最大化，提升了训练稳定性。同时，文章引入UniGenBench，一个包含600个提示的统一基准，涵盖5大主题和20个子主题，用于更全面评估文本到图像模型的表现。实验表明，该方法能够有效区分图像质量差异，提升模型评估的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 09:11:24 GMT</pubDate>
</item>
<item>
<title>rStar2-Agent：基于代理强化学习的14B数学推理模型</title>
<link>https://arxiv.org/abs/2508.20722</link>
<guid>https://arxiv.org/abs/2508.20722</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">rStar2-Agent通过代理强化学习实现前沿数学推理能力。</p><br /><br /><p><strong>摘要：</strong> rStar2-Agent是一款基于代理强化学习训练的14B参数数学推理模型，能够展现出高级认知行为，如在使用Python编码工具前进行仔细思考，并根据代码执行反馈自主探索、验证和优化复杂问题的解决步骤。该模型通过三项关键技术实现大规模有效代理强化学习：高效的RL基础设施、GRPO-RoC算法以及高效的代理训练方法。仅用510次RL步骤，rStar2-Agent在AIME24和AIME25数据集上分别达到80.6%和69.8%的pass@1分数，超越了DeepSeek-R1（671B）模型。此外，该模型在对齐、科学推理和代理工具使用任务中也表现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20722" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 08:45:25 GMT</pubDate>
</item>
<item>
<title>MCP-Bench：评估大型语言模型多步骤任务能力的基准</title>
<link>https://arxiv.org/abs/2508.20453</link>
<guid>https://arxiv.org/abs/2508.20453</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCP-Bench测试LLM在多步骤任务中的工具使用与协作能力。</p><br /><br /><p><strong>摘要：</strong> MCP-Bench是一个用于评估大型语言模型（LLMs）在现实多步骤任务中表现的基准，这些任务需要工具使用、跨工具协调、精确参数控制以及规划和推理。该基准基于Model Context Protocol (MCP)，连接了28个代表性的实时MCP服务器，涵盖金融、旅行、科学计算和学术搜索等多个领域，提供250种工具。与以往依赖API的基准不同，MCP-Bench强调工具间的协同工作，构建真实多步骤任务。任务测试代理从模糊指令中检索相关工具、规划多跳执行路径、根据中间工具输出生成响应，并协调跨领域流程。实验表明，当前20个先进LLMs在该基准中仍面临挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20453" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 01:58:57 GMT</pubDate>
</item>
<item>
<title>AWorld系统提升Agentic AI训练效率与性能</title>
<link>https://arxiv.org/abs/2508.20404</link>
<guid>https://arxiv.org/abs/2508.20404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AWorld提升AI训练效率，显著提高GAIA基准表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AWorld系统，这是一个用于大规模智能体-环境交互的开源系统，能够将经验收集速度提升14.6倍。通过该系统，研究人员训练了一个基于Qwen3-32B的智能体，在GAIA基准测试中，其准确率从21.59%提升至32.23%，在最困难的级别上达到16.33%，超越了主流专有模型。AWorld为构建高效的Agentic AI训练流程提供了实用的参考方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 28 Aug 2025 00:04:30 GMT</pubDate>
</item>
<item>
<title>任务导向的指令增强方法提升大语言模型的实际应用性能</title>
<link>https://arxiv.org/abs/2508.20374</link>
<guid>https://arxiv.org/abs/2508.20374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TCIA提升LLM在真实任务中的表现，保持多样性与任务相关性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种任务导向的指令增强框架（TCIA），旨在提升大语言模型在实际任务中的表现。传统方法虽注重数据多样性，但忽视了任务相关性。TCIA通过在离散查询-约束空间中表示指令，生成与任务高度相关的多样化指令，使模型在保持整体性能的同时，更好地适应特定应用场景。实验表明，TCIA在四个真实任务中平均提升了开源大语言模型8.7%的性能，部分情况下甚至超越了闭源模型，展示了其在实际应用中的有效性与可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 22:42:10 GMT</pubDate>
</item>
<item>
<title>统一风格与主题生成框架USO的提出与实验验证</title>
<link>https://arxiv.org/abs/2508.18966</link>
<guid>https://arxiv.org/abs/2508.18966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">USO统一风格与主题生成，提升图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 现有研究通常将风格驱动和主题驱动生成视为两个独立任务，前者注重风格相似性，后者强调主题一致性，导致两者存在矛盾。本文提出USO模型，通过统一框架实现风格与主题的联合优化。首先构建了一个大规模三元组数据集，包含内容图像、风格图像及其风格化结果。其次引入解耦学习方案，通过风格对齐训练和内容-风格解耦训练同时优化风格特征和内容分离。此外，采用SRL风格奖励学习机制进一步提升性能。最后，发布USO-Bench基准，首次综合评估风格相似性和主题保真度。实验表明，USO在开源模型中表现最优。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 08:10:24 GMT</pubDate>
</item>
<item>
<title>ROSE：一种针对视频中物体及其副作用的去除框架</title>
<link>https://arxiv.org/abs/2508.18633</link>
<guid>https://arxiv.org/abs/2508.18633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ROSE解决视频中物体及其阴影、反射等副作用的去除问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出ROSE框架，用于去除视频中物体及其副作用（如阴影、反射、透光等）。由于缺乏成对的标注数据，传统方法难以有效处理这些副作用。ROSE利用3D渲染引擎生成合成数据，并构建了一个全自动的数据准备流程，以模拟多样化的场景和视角。该框架基于扩散Transformer实现视频修复，通过参考视频进行局部擦除，并引入差分掩码进行额外监督。研究还提出了ROSE-Bench基准测试，涵盖多种常见和特殊副作用场景。实验结果表明，ROSE在多个任务中表现优于现有方法，并具有良好的现实适应性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 23:18:31 GMT</pubDate>
</item>
<item>
<title>TriMM：首个基于多模态的3D生成模型</title>
<link>https://arxiv.org/abs/2508.15228</link>
<guid>https://arxiv.org/abs/2508.15228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TriMM利用多模态数据提升3D资产生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出TriMM，这是首个基于多模态数据（如RGB、RGBD和点云）的前馈3D生成模型。TriMM通过协同多模态编码融合不同模态特征，并引入2D和3D辅助监督以增强模型鲁棒性。随后，利用三平面潜在扩散模型生成高质量的3D资产，显著提升了纹理和几何细节。实验表明，TriMM在多个数据集上表现优异，即使使用少量训练数据也能达到与大规模数据训练模型相当的效果。此外，研究还验证了其他多模态数据在3D生成中的可行性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:31:14 GMT</pubDate>
</item>
<item>
<title>链式思维在软推理任务中的有效性与忠实性研究</title>
<link>https://arxiv.org/abs/2508.19827</link>
<guid>https://arxiv.org/abs/2508.19827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">链式思维在软推理中效果有限且可能不忠实于模型实际推理。</p><br /><br /><p><strong>摘要：</strong> 本文研究了链式思维（CoT）在分析和常识推理等软推理任务中的表现，发现CoT在不同模型中的影响和忠实性并不一致。研究对比了指令调优、推理和推理蒸馏模型，揭示了CoT在这些模型中的动态差异，表明CoT的效果和其对模型实际推理的忠实性并非总是同步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 08:25:29 GMT</pubDate>
</item>
<item>
<title>SEAM基准测试评估视觉语言模型的跨模态一致性</title>
<link>https://arxiv.org/abs/2508.18179</link>
<guid>https://arxiv.org/abs/2508.18179</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEAM测试显示视觉语言模型在跨模态任务中存在性能不平衡。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SEAM基准，用于评估视觉语言模型（VLMs）在不同模态间的推理一致性。SEAM通过在四个领域中使用语义等价的输入，提供了一种严格的对比评估方式。研究发现，尽管问题包含语义等价信息，视觉模态在整体表现上通常落后于语言模态，且跨模态一致性较低。分析表明，文本和视觉感知失败是主要原因。研究结果对改进模态无关的推理具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18179" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 12:33:07 GMT</pubDate>
</item>
<item>
<title>大规模多视角rPPG与健康指标数据集的构建与应用</title>
<link>https://arxiv.org/abs/2508.17924</link>
<guid>https://arxiv.org/abs/2508.17924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建了一个多视角rPPG数据集以推动医疗AI发展。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有rPPG数据集存在的规模小、隐私问题及条件单一等问题，提出一个大规模多视角视频数据集。该数据集包含600名受试者的3600个同步视频记录，涵盖静息和运动状态，并使用多台消费级摄像头从不同角度采集。每个视频均配有100Hz PPG信号及多种健康指标，如心电图、血压、血氧饱和度等。基于此数据集训练了一个高效的rPPG模型，并在跨数据集场景下与其他方法进行比较。该数据集和模型的公开发布将显著促进AI医疗助手的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 07:46:40 GMT</pubDate>
</item>
<item>
<title>DeepScholar-bench：评估生成式研究综合能力的新基准</title>
<link>https://arxiv.org/abs/2508.20033</link>
<guid>https://arxiv.org/abs/2508.20033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepScholar-bench评估生成式研究综合系统。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepScholar-bench，一个用于评估生成式研究合成系统的实时基准和全面自动化评估框架。该框架从高质量的ArXiv论文中获取查询，并专注于生成论文相关工作部分的任务。评估框架从知识合成、检索质量和可验证性三个维度进行综合评估。作者还开发了DeepScholar-base作为参考管道，并对多个开源系统进行了系统评估，结果显示DeepScholar-base表现优异，但整体任务仍具有挑战性，表明该基准在推动AI生成研究合成能力方面的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 12:36:34 GMT</pubDate>
</item>
<item>
<title>基于早期答案收敛的扩散语言模型快速解码方法</title>
<link>https://arxiv.org/abs/2508.19982</link>
<guid>https://arxiv.org/abs/2508.19982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Prophet通过早期收敛实现扩散模型加速解码。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的快速解码方法Prophet，利用扩散语言模型（DLMs）在半自回归和随机重掩码调度下早期答案收敛的特性。实验表明，在GSM8K和MMLU数据集上，97%和99%的实例可以在仅完成一半细化步骤时正确解码。Prophet通过比较前两名预测候选的置信度差异动态决定是否提前终止细化过程，从而显著减少解码步骤，提升推理速度，同时保持生成质量。该方法适用于现有DLM框架，无需额外训练，具有广泛的应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 11:40:25 GMT</pubDate>
</item>
<item>
<title>HeteroScale：解决LLM服务中P/D解耦架构的高效自动扩展框架</title>
<link>https://arxiv.org/abs/2508.19559</link>
<guid>https://arxiv.org/abs/2508.19559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeteroScale提升LLM服务GPU利用率26.6%</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HeteroScale，一种针对Prefill-Decode（P/D）解耦架构的高效自动扩展框架。该框架通过拓扑感知调度器和基于大规模实证研究的度量驱动策略，解决了现代LLM服务中的硬件异构性、网络瓶颈和预填充与解码阶段不平衡等问题。HeteroScale通过单一度量联合扩展预填充和解码池，实现了资源的高效利用。在大规模生产环境中部署后，其显著提升了GPU利用率，每日节省数十万GPU小时，同时满足严格的服务水平目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:22:02 GMT</pubDate>
</item>
<item>
<title>基于TAPO与MotionFLUX的高效动作生成系统</title>
<link>https://arxiv.org/abs/2508.19527</link>
<guid>https://arxiv.org/abs/2508.19527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAPO与MotionFLUX提升动作生成的语义对齐与实时性。</p><br /><br /><p><strong>摘要：</strong> 本文提出TAPO（Aligned Preference Optimization）和MotionFLUX两个框架，以解决文本驱动动作生成中语义对齐不足和推理效率低的问题。TAPO通过迭代优化增强动作与文本描述的语义一致性，而MotionFLUX利用确定性修正流匹配技术，在不牺牲动作质量的前提下实现实时生成。实验表明，该系统在语义一致性和动作质量上优于现有方法，并显著提升了生成速度。相关代码和预训练模型将公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 22:45:09 GMT</pubDate>
</item>
<item>
<title>基于多模态控制的实时数字人视频生成框架</title>
<link>https://arxiv.org/abs/2508.19320</link>
<guid>https://arxiv.org/abs/2508.19320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种低延迟、高可控性的数字人视频生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种自回归视频生成框架，支持多模态输入（如音频、姿态和文本）并实现低延迟的流式交互。该框架通过最小化对标准大语言模型的修改，结合深度压缩自编码器，有效降低长序列推理负担。研究构建了一个包含20,000小时对话数据的大规模数据集，用于训练多场景交互模型。实验表明，该方法在双工对话、多语言人像合成和交互式世界建模中表现出低延迟、高效率和细粒度的多模态控制能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 10:00:16 GMT</pubDate>
</item>
<item>
<title>提升语音识别系统可解释性的方法研究</title>
<link>https://arxiv.org/abs/2508.15882</link>
<guid>https://arxiv.org/abs/2508.15882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究将可解释性方法应用于语音识别，揭示内部动态与错误机制。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了将可解释性方法如logit lens、线性探测和激活补丁应用于自动语音识别（ASR）系统，以理解声学和语义信息在模型各层中的演变。实验揭示了编码器-解码器交互导致的重复幻觉以及声学表示中的语义偏差等新发现。这些成果展示了扩展可解释性技术对提升语音识别模型透明度和鲁棒性的潜力，为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 11:42:53 GMT</pubDate>
</item>
<item>
<title>CODA：一种结合规划器与执行器的可训练组合框架</title>
<link>https://arxiv.org/abs/2508.20096</link>
<guid>https://arxiv.org/abs/2508.20096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CODA提升GUI自主代理在科学计算中的执行与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出CODA，一种可训练的组合框架，将通用规划器（Cerebrum）与专用执行器（Cerebellum）结合，解决科学计算中GUI自主代理长期规划与精确执行的难题。该框架通过两阶段训练：第一阶段为特殊化，利用解耦GRPO方法为每个科学应用单独训练专家规划器；第二阶段为泛化，整合所有成功轨迹构建数据集，对最终规划器进行监督微调。CODA在ScienceBoard基准测试的四个挑战性应用中表现优异，超越基线模型，成为开源模型中的新最佳方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>AudioStory：生成结构化长时音频叙事的统一框架</title>
<link>https://arxiv.org/abs/2508.20088</link>
<guid>https://arxiv.org/abs/2508.20088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AudioStory提升长音频叙事生成能力，结合LLM与TTA技术。</p><br /><br /><p><strong>摘要：</strong> 本文提出AudioStory，一个将大型语言模型（LLM）与文本到音频（TTA）系统结合的统一框架，用于生成结构化、长时的音频叙事。该框架具备强大的指令遵循和推理生成能力，通过分解复杂查询为时间有序的子任务，实现场景过渡和情感一致性。其两个核心特点包括：解耦桥接机制与端到端训练，提升了音频生成的连贯性和效率。研究还构建了AudioStory-10K基准数据集，实验表明AudioStory在指令遵循和音频质量上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 13:55:38 GMT</pubDate>
</item>
<item>
<title>基于离散扩散的视觉-语言-动作模型设计与应用</title>
<link>https://arxiv.org/abs/2508.20072</link>
<guid>https://arxiv.org/abs/2508.20072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">离散扩散VLA模型提升机器人动作生成效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于离散扩散的视觉-语言-动作（VLA）模型，通过将动作块离散化并利用离散扩散进行建模，实现了与视觉语言模型（VLM）后端的天然兼容。该方法保留了扩散模型的逐步优化机制，并支持自适应解码顺序和二次遮蔽技术，提升了动作生成的一致性和鲁棒性。实验结果显示，该模型在多个基准任务中表现优异，优于传统自回归和连续扩散方法，为VLA模型的扩展提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.20072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 13:39:11 GMT</pubDate>
</item>
<item>
<title>Vision-SR1：一种无需外部视觉监督的自奖励视觉语言模型训练方法</title>
<link>https://arxiv.org/abs/2508.19652</link>
<guid>https://arxiv.org/abs/2508.19652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vision-SR1通过自奖励机制提升视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Vision-SR1，一种基于强化学习的自奖励方法，旨在提升视觉语言模型（VLMs）的视觉推理能力，同时减少对语言依赖和视觉幻觉。该方法将推理过程分为视觉感知和语言推理两个阶段，通过生成自包含的视觉描述并利用其进行语言推理来计算奖励，从而提供更平衡的训练信号。实验表明，Vision-SR1在多种视觉-语言任务中有效提升了视觉推理能力，减少了对语言捷径的依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 04:01:03 GMT</pubDate>
</item>
<item>
<title>智能手机代理的隐私意识评估与基准测试</title>
<link>https://arxiv.org/abs/2508.19493</link>
<guid>https://arxiv.org/abs/2508.19493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了智能手机代理的隐私意识，发现多数表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于多模态大语言模型的智能手机代理进行了大规模隐私意识评估，构建了包含7,138个场景的首个基准测试。研究对每个场景的隐私类型、敏感度和位置进行了标注，并对七款主流代理进行了测试。结果表明，大多数代理在隐私保护方面表现欠佳，即使在明确提示下也未能达到60%以上的准确率。封闭源代码代理整体表现优于开源代理，其中Gemini 2.0-flash表现最佳，隐私感知准确率为67%。研究还发现，代理的隐私检测能力与场景敏感度密切相关，高敏感度场景更容易被识别。研究希望引发对智能手机代理隐私与功能平衡问题的重新思考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 20:41:28 GMT</pubDate>
</item>
<item>
<title>基于生成式判断的多步骤推理模型优化方法</title>
<link>https://arxiv.org/abs/2508.19229</link>
<guid>https://arxiv.org/abs/2508.19229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出生成式判断模型提升多步骤推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多步骤推理模型在逻辑验证方面的挑战，提出一种新的生成式判断方法。该方法将步骤奖励建模从分类任务转化为推理任务，通过输出思考标记来评估模型的推理过程，并在训练中使用强化学习优化。实验表明，该方法在中间步骤判断上优于现有方法，能够提升策略模型的训练效果和推理搜索性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:45:05 GMT</pubDate>
</item>
<item>
<title>Token Order Prediction提升语言模型训练效果</title>
<link>https://arxiv.org/abs/2508.19228</link>
<guid>https://arxiv.org/abs/2508.19228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Token Order Prediction在NLP任务中优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的语言模型训练方法——Token Order Prediction (TOP)，旨在替代传统的Multi-Token Prediction (MTP)。与MTP相比，TOP通过学习排序来预测未来token的顺序，使用更少的计算资源并取得了更好的效果。实验结果表明，在多个标准NLP基准测试中，TOP表现优于NTP和MTP，尤其在大规模模型中效果更显著。作者提供了相关代码供研究参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:43:30 GMT</pubDate>
</item>
<item>
<title>深度学习在金融收益分布预测中的应用研究</title>
<link>https://arxiv.org/abs/2508.18921</link>
<guid>https://arxiv.org/abs/2508.18921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">深度神经网络可有效预测金融收益分布，表现优于传统模型。</p><br /><br /><p><strong>摘要：</strong> 本研究评估了深度神经网络在预测金融收益概率分布方面的性能。使用1D卷积神经网络（CNN）和长短期记忆网络（LSTM）对正态分布、学生t分布和偏斜学生t分布的参数进行预测，并通过自定义的负对数似然损失函数优化分布参数。模型在六个主要股票指数上进行了测试，采用对数预测评分（LPS）、连续排名概率评分（CRPS）和概率积分变换（PIT）等指标进行评估。结果表明，深度学习模型能够提供准确的分布预测，在风险价值（VaR）估计方面与经典GARCH模型相当。其中，结合偏斜学生t分布的LSTM模型在多个评估标准中表现最佳，能够捕捉金融收益的厚尾和不对称性。该研究证明了深度神经网络在金融风险评估和投资组合管理中的可行性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 06:48:16 GMT</pubDate>
</item>
<item>
<title>基于推理的大型语言模型DrugReasoner在药物审批预测中的应用</title>
<link>https://arxiv.org/abs/2508.18579</link>
<guid>https://arxiv.org/abs/2508.18579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DrugReasoner提升药物审批预测准确性与透明度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DrugReasoner，一个基于LLaMA架构并采用组相对策略优化（GRPO）微调的推理型大型语言模型，用于预测小分子药物的审批可能性。DrugReasoner通过整合分子描述符与结构相似化合物的比较推理，生成预测结果及逐步解释和置信度评分。在验证集和测试集中，DrugReasoner分别取得了0.732和0.725的AUC值，以及0.729和0.718的F1分数，优于传统方法，并在外部数据集上表现优于ChemAP模型。该模型不仅具备较高的预测准确率，还通过推理输出提升了透明度，为AI辅助药物发现提供了可解释的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 21:14:14 GMT</pubDate>
</item>
<item>
<title>基于回溯机制的灵活激活调控方法提升大语言模型行为对齐</title>
<link>https://arxiv.org/abs/2508.17621</link>
<guid>https://arxiv.org/abs/2508.17621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FASB框架动态调整LLM行为，提升对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为FASB的灵活激活调控框架，通过在生成过程中跟踪大语言模型的内部状态，动态判断干预的必要性和强度，从而更精准地引导模型输出符合预期的行为。与传统方法仅依赖问题或统一干预不同，FASB结合问题和生成内容进行评估，并引入回溯机制，在检测到偏差时及时修正已生成的token，提高行为对齐效率。实验表明，该方法在TruthfulQA和多个选择题数据集上均优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 23:01:30 GMT</pubDate>
</item>
<item>
<title>中文法律主张生成研究与数据集构建</title>
<link>https://arxiv.org/abs/2508.17234</link>
<guid>https://arxiv.org/abs/2508.17234</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文构建了首个中文法律主张生成数据集并评估大模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于法律主张生成任务，提出了ClaimGen-CN数据集，用于支持中文法律主张的生成研究。同时设计了一个包含事实性和清晰度两个维度的评估指标。通过零样本测试，发现当前大模型在事实准确性和表达清晰度方面仍有不足，表明该领域需要更深入的研究和优化。作者还计划公开数据集以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17234" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 03:19:25 GMT</pubDate>
</item>
<item>
<title>Selct2Know：一种高效融合内外部知识的领域问答框架</title>
<link>https://arxiv.org/abs/2508.15213</link>
<guid>https://arxiv.org/abs/2508.15213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Selct2Know提升领域问答性能，降低训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在特定领域表现不佳的问题，提出Selct2Know（S2K）框架。该框架通过内部与外部知识的自我选择策略以及选择性监督微调，有效整合领域知识，同时引入结构化推理数据生成管道和GRPO技术以增强推理能力。实验表明，S2K在医疗、法律和金融等领域的问答任务中表现优异，且成本显著低于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 23:53:35 GMT</pubDate>
</item>
<item>
<title>AUSM：统一提示与无提示视频分割的通用模型</title>
<link>https://arxiv.org/abs/2508.19242</link>
<guid>https://arxiv.org/abs/2508.19242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AUSM实现高效视频分割，提升训练速度与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为AUSM的自动回归通用分割模型，将视频分割任务转化为序列掩码预测，类似于语言建模。该模型统一处理提示和无提示视频分割任务，基于状态空间模型设计，支持任意长度视频流，并通过帧间并行训练显著提升训练效率。在多个标准数据集上，AUSM表现优于现有方法，且在16帧序列中训练速度提升达2.5倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:59:13 GMT</pubDate>
</item>
<item>
<title>科学推理任务的评估与模型优化研究</title>
<link>https://arxiv.org/abs/2508.19202</link>
<guid>https://arxiv.org/abs/2508.19202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新基准与框架，提升大模型科学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在科学问题解决中的挑战，提出了SciReas和SciReas-Pro两个基准，用于评估科学推理能力。同时引入KRUX框架，分析知识与推理在科学任务中的作用。研究发现，模型内部知识检索是关键瓶颈，外部知识补充能显著提升性能，且增强推理过程有助于提取相关知识。最后，作者发布了SciLit01作为科学推理的强基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:04:23 GMT</pubDate>
</item>
<item>
<title>MovieCORE：推动电影内容深度理解的视频问答数据集</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MovieCORE旨在提升AI对电影内容的深层认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MovieCORE，一个专注于电影内容深度理解的视频问答（VQA）数据集。与以往侧重表层理解的数据集不同，MovieCORE强调需要系统2思维的问题，并通过多大语言模型作为思考代理生成高质量的问答对。研究还提出了认知测试以评估数据集质量，并设计了全面的评估方案来衡量VQA模型在深度认知任务中的表现。为解决现有视频语言模型的不足，作者引入了Agentic Choice Enhancement（ACE）模块，显著提升了模型推理能力。该工作为AI在电影理解方面提供了新的方向和洞察。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 09:43:45 GMT</pubDate>
</item>
<item>
<title>MoE模型稀疏性对记忆与推理能力的影响研究</title>
<link>https://arxiv.org/abs/2508.18672</link>
<guid>https://arxiv.org/abs/2508.18672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究MoE模型稀疏性对记忆和推理能力的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了混合专家（MoE）模型的稀疏性如何影响两种不同的能力范畴：记忆和推理。通过在固定计算预算下系统地调整总参数、活跃参数和top-k路由，研究发现记忆能力随着总参数增加而持续提升，而推理能力则趋于饱和甚至下降。此外，当活跃参数不变时，仅改变top-k对性能影响有限，经典超参数如学习率和初始化对泛化差距的影响方向与稀疏性一致。研究还表明，后训练强化学习或额外测试时间计算无法弥补过度稀疏模型的推理缺陷。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:31:28 GMT</pubDate>
</item>
<item>
<title>ObjFiller-3D：提升3D物体补全质量的新方法</title>
<link>https://arxiv.org/abs/2508.18271</link>
<guid>https://arxiv.org/abs/2508.18271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjFiller-3D改进3D物体补全，提升真实性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ObjFiller-3D，一种用于高质量且一致的3D物体补全和编辑的新方法。与传统基于2D图像补全的方法不同，该方法利用先进的视频编辑模型来填充3D物体的缺失区域，并通过参考机制进一步提升重建质量。实验表明，ObjFiller-3D在多个数据集上表现优于现有方法，在PSNR和LPIPS指标上均取得显著提升，显示出其在实际3D编辑应用中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>CMPhysBench：评估大语言模型在凝聚态物理中的能力基准</title>
<link>https://arxiv.org/abs/2508.18124</link>
<guid>https://arxiv.org/abs/2508.18124</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CMPhysBench测试LLM在凝聚态物理中的计算问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CMPhysBench，这是一个专门用于评估大语言模型（LLMs）在凝聚态物理领域能力的新基准。该基准包含520多道研究生水平的精心设计的问题，涵盖磁性、超导、强关联系统等核心子领域。所有问题均为计算题，要求模型独立生成完整解题过程。为了更精确地评估模型输出与标准答案的相似度，作者引入了SEED分数，这是一种基于树结构的表达式编辑距离度量方法，可提供细粒度的部分评分。实验结果显示，即使是最先进的模型Grok-4，在CMPhysBench上的平均SEED得分为36，准确率为28%，表明LLM在这一前沿且实践性强的领域仍存在显著能力差距。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18124" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 11:32:22 GMT</pubDate>
</item>
<item>
<title>TreePO：提升语言模型推理效率的强化学习方法</title>
<link>https://arxiv.org/abs/2508.17445</link>
<guid>https://arxiv.org/abs/2508.17445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TreePO通过树状搜索提升模型推理效率与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出TreePO，一种基于树状结构的强化学习方法，旨在提升大型语言模型在解决复杂推理问题时的效率和探索能力。TreePO采用动态树采样策略和固定长度片段解码，利用局部不确定性生成更多分支，同时通过共享公共前缀和早期剪枝减少计算负担。该方法在多个推理基准测试中表现出色，显著降低了GPU使用时间和计算成本，为基于强化学习的后训练提供了一条高效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 12:52:37 GMT</pubDate>
</item>
<item>
<title>QueryBandits：通过查询重写主动减少大语言模型的幻觉</title>
<link>https://arxiv.org/abs/2508.16697</link>
<guid>https://arxiv.org/abs/2508.16697</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QueryBandits通过优化查询重写策略有效降低LLM幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文提出QueryBandits，一种基于强化学习的查询重写框架，旨在通过优化输入查询来主动减少大语言模型（LLM）中的幻觉现象。该方法利用17种语言特征构建奖励模型，以评估不同查询重写策略对幻觉的影响，并采用贝叶斯优化方法（如Thompson Sampling）选择最优策略。实验结果显示，QueryBandits在13个QA基准测试中表现优于无重写基线和静态重写策略，显著提升了生成结果的准确性。此外，研究发现静态重写策略可能加剧幻觉问题，而QueryBandits通过语义特征引导实现更稳定的输出行为，无需重新训练或梯度调整。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16697" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 21:41:49 GMT</pubDate>
</item>
<item>
<title>ReportBench：评估大语言模型生成研究报告质量的基准</title>
<link>https://arxiv.org/abs/2508.15804</link>
<guid>https://arxiv.org/abs/2508.15804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReportBench用于评估LLM生成研究报告的质量与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ReportBench，一个用于评估大型语言模型（LLMs）生成研究报告质量的系统性基准。该基准关注两个关键维度：引用文献的质量与相关性，以及报告中陈述的真实性与一致性。ReportBench利用arXiv上的高质量综述论文作为黄金标准参考，并通过逆向提示工程生成领域特定提示，构建全面的评估语料库。此外，ReportBench还开发了一个基于代理的自动化框架，用于系统分析生成的报告，验证引用内容的真实性和非引用声明的准确性。实证结果显示，商业Deep Research代理如OpenAI和Google的产品在生成更全面、可靠的报告方面优于仅使用搜索或浏览工具的独立LLMs，但在研究覆盖范围和事实一致性方面仍有提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 23:33:43 GMT</pubDate>
</item>
<item>
<title>基于3D潜在空间的精准编辑方法VoxHammer</title>
<link>https://arxiv.org/abs/2508.19247</link>
<guid>https://arxiv.org/abs/2508.19247</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VoxHammer实现3D模型精确且一致的局部编辑。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为VoxHammer的新方法，用于在3D潜在空间中进行精确且连贯的局部编辑。该方法无需训练，通过预测3D模型的逆向轨迹并获取每个时间步的反转潜在表示和键值标记，在去噪和编辑阶段替换未编辑区域的特征，从而保持未编辑部分的一致性并实现编辑部分的自然融合。研究团队构建了Edit3D-Bench数据集以评估编辑一致性，并实验表明VoxHammer在保留区域的3D一致性和整体质量上优于现有方法，为上下文相关的3D生成提供了高质量的数据基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19247" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>OmniHuman-1.5：生成具有语义一致性的视频虚拟角色动画</title>
<link>https://arxiv.org/abs/2508.19209</link>
<guid>https://arxiv.org/abs/2508.19209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniHuman-1.5通过多模态技术生成更真实、有情感的虚拟角色动画。</p><br /><br /><p><strong>摘要：</strong> 现有视频虚拟角色模型虽然能生成流畅的人体动画，但难以捕捉角色的本质。本文提出OmniHuman-1.5框架，利用多模态大语言模型生成高语义层次的文本引导，结合专用的多模态DiT架构与伪最后一帧设计，提升动作与语义、场景和语言内容的一致性。实验表明该模型在唇形同步、视频质量、动作自然度和语义一致性等方面表现优异，并具备处理多人和非人类场景的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:15:26 GMT</pubDate>
</item>
<item>
<title>VibeVoice：基于扩散模型的多说话人长时语音合成技术</title>
<link>https://arxiv.org/abs/2508.19205</link>
<guid>https://arxiv.org/abs/2508.19205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VibeVoice实现多说话人长时语音合成，提升数据压缩与效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VibeVoice，一种新型的长时语音合成模型，通过使用next-token diffusion方法，实现多说话人的自然对话合成。该模型引入了一种新的连续语音分词器，在保持性能的同时，将数据压缩提升了80倍。这种分词器在保持音频保真度的同时，显著提高了处理长序列的计算效率，使得VibeVoice能够合成长达90分钟的对话内容，最多支持4个说话人，真实还原对话氛围，超越现有开源和专有对话模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 13:09:12 GMT</pubDate>
</item>
<item>
<title>基于顶点与面分离的高效艺术网格生成方法</title>
<link>https://arxiv.org/abs/2508.19188</link>
<guid>https://arxiv.org/abs/2508.19188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种减少冗余的网格生成框架，提升生成速度和质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种高效的网格生成框架，通过将顶点和面分开处理，显著减少了传统方法中因顶点重复使用而导致的冗余。该方法仅使用自回归模型生成顶点，使令牌数量减少至现有方法的23%。随后，利用双向Transformer一次性完成网格构建，捕捉顶点间关系并构造邻接矩阵。为进一步提升质量，引入保真增强器优化顶点位置，并设计后处理框架消除不良边连接。实验表明，该方法在生成速度上比现有最佳方法快8倍以上，同时生成的网格质量更高。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.19188" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 12:51:02 GMT</pubDate>
</item>
<item>
<title>ThinkDial：实现可控推理的开源框架</title>
<link>https://arxiv.org/abs/2508.18773</link>
<guid>https://arxiv.org/abs/2508.18773</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkDial通过离散模式实现可控推理，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ThinkDial，这是一个首个开源的端到端框架，能够实现类似gpt-oss系列的可控推理功能。该框架支持三种不同的推理模式：高模式（全推理能力）、中模式（减少50%的token且性能下降小于10%）和低模式（减少75%的token且性能下降小于15%）。通过端到端训练方法，包括预算模式监督微调和两阶段预算感知强化学习，ThinkDial在保持性能的同时显著降低了响应长度，并在分布外任务中表现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18773" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 03:57:28 GMT</pubDate>
</item>
<item>
<title>UltraMemV2：实现与MoE模型性能相当的高效内存层架构</title>
<link>https://arxiv.org/abs/2508.18756</link>
<guid>https://arxiv.org/abs/2508.18756</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UltraMemV2提升内存层架构性能，接近8专家MoE模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出UltraMemV2，一种改进的内存层架构，解决了传统内存层在推理过程中高内存访问成本的问题。通过五项关键改进，包括将内存层集成到每个Transformer块、简化值扩展、采用FFN-based值处理、优化参数初始化以及重新平衡内存与FFN计算比例，UltraMemV2实现了与8专家MoE模型相当的性能，同时显著降低内存访问。实验表明，在长上下文记忆、多轮记忆和上下文学习等任务中，UltraMemV2表现优于现有方法。研究还验证了激活密度对性能的影响大于稀疏参数总量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18756" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 03:33:11 GMT</pubDate>
</item>
<item>
<title>音频驱动角色动画模型Wan-S2V在影视级表现上的提升</title>
<link>https://arxiv.org/abs/2508.18621</link>
<guid>https://arxiv.org/abs/2508.18621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Wan-S2V模型在影视动画中表现出色，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 当前最先进的音频驱动角色动画方法在语音和歌唱场景中表现良好，但在复杂的影视制作中仍存在不足，如角色互动、身体动作和动态镜头等方面。为解决这一问题，本文提出了一种名为Wan-S2V的音频驱动模型，基于Wan架构，显著提升了电影级别的表现力和真实性。实验结果表明，该模型在与Hunyuan-Avatar和Omnihuman等先进模型的对比中表现更优。此外，该方法在长视频生成和精确唇形同步编辑中也展现出良好的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 22:51:31 GMT</pubDate>
</item>
<item>
<title>CTF-Dojo：基于可执行环境的大型语言模型训练新范式</title>
<link>https://arxiv.org/abs/2508.18370</link>
<guid>https://arxiv.org/abs/2508.18370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CTF-Dojo提升LLM在软件工程任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了CTF-Dojo，这是一个大规模可执行运行时环境，专为训练具备验证反馈的大型语言模型（LLMs）设计。该平台包含658个完整的CTF挑战，通过Docker容器化确保可重复性。同时，作者开发了CTF-Forge自动化管道，快速将公开资源转化为可执行环境。在仅使用486条高质量、经过验证的轨迹进行训练后，LLM在多个基准测试中取得了显著提升，最高达到11.6%的绝对增益。最佳32B模型在Pass@1指标上达到31.9%，超越了许多前沿模型。研究证明，基于执行的训练信号对于构建高性能ML代理至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 14:02:23 GMT</pubDate>
</item>
<item>
<title>基于认知科学的大型语言模型分析框架</title>
<link>https://arxiv.org/abs/2508.18192</link>
<guid>https://arxiv.org/abs/2508.18192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM认知机制通过网络框架被解析，揭示其与生物大脑的异同。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）的认知机制，提出了一种结合认知科学原理的网络框架，用于分析LLM的架构和技能分布。研究发现，尽管LLM不像生物系统那样高度专业化，但它们的模块社区展现出部分类似鸟类和小型哺乳动物大脑的分布式认知结构。此外，LLM在技能获取上受益于动态跨区域交互和神经可塑性，这表明有效的微调策略应侧重于分布式学习动态，而非固定的模块干预。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 12:49:38 GMT</pubDate>
</item>
<item>
<title>Spacer：一种无需外部干预的科学发现系统</title>
<link>https://arxiv.org/abs/2508.17661</link>
<guid>https://arxiv.org/abs/2508.17661</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Spacer通过去情境化方法生成创造性科学概念。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Spacer，一个能够自主生成创造性且事实依据充分的科学概念的系统。该系统通过‘刻意去情境化’的方法，将信息拆解为关键词，并从它们之间的新联系中激发创造力。Spacer由两个部分组成：Nuri灵感引擎和Manifesting Pipeline实现管道。Nuri从生物领域的18万篇学术论文构建的关键词图中提取高潜力关键词集，而Manifesting Pipeline则分析关键词间的逻辑关系并生成科学陈述。实验表明，Nuri在分类高影响力论文方面表现优异，而Manifesting Pipeline能有效重构顶级期刊文章的核心概念。此外，Spacer的输出与领先论文的相似度显著高于当前最先进的LLM。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17661" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:49:16 GMT</pubDate>
</item>
<item>
<title>CineScale：提升高分辨率视觉生成能力的新方法</title>
<link>https://arxiv.org/abs/2508.15774</link>
<guid>https://arxiv.org/abs/2508.15774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineScale实现无需微调的8k图像和4k视频生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出CineScale，一种新的推理范式，用于提升预训练模型在高分辨率下的视觉生成能力。针对不同视频生成架构的挑战，CineScale设计了专用变体。与以往仅限于文本到图像或视频生成的方法不同，CineScale扩展至图像到视频和视频到视频的高分辨率合成。实验表明，该方法在不进行微调的情况下即可生成8k图像，并通过少量LoRA微调实现4k视频生成，显著提升了高分辨率视觉生成的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>基于视觉信息预测3D场景物理属性的新方法PIXIE</title>
<link>https://arxiv.org/abs/2508.17437</link>
<guid>https://arxiv.org/abs/2508.17437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PIXIE可快速预测3D场景物理属性，提升虚拟世界真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出PIXIE方法，通过训练神经网络从3D视觉特征中预测物理属性，无需逐场景优化，显著提升效率与泛化能力。结合静态场景表示技术，实现真实物理模拟。研究还构建了大规模数据集PIXIEVERSE，支持模型训练与评估。实验表明，PIXIE在性能和速度上优于现有方法，并能零样本推广至真实场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 15:24:04 GMT</pubDate>
</item>
<item>
<title>基于覆盖准则的多模态视觉语言模型令牌选择方法</title>
<link>https://arxiv.org/abs/2508.18264</link>
<guid>https://arxiv.org/abs/2508.18264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态信息提升视觉语言模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为MMTok的方法，通过结合视觉和文本信息，在最大覆盖准则下选择关键视觉令牌，以提高视觉语言模型（VLMs）的推理效率。该方法将令牌选择问题建模为最大覆盖问题，并优化选中的视觉令牌以同时覆盖文本令牌和原始视觉令牌集。实验结果表明，该方法在多个基准数据集上显著优于单模态方法，且在保持高精度的同时实现显著的速度提升。例如，在POPE数据集上，该方法在保持98.7%原始性能的情况下实现1.87倍的加速，仅使用4个视觉令牌时仍能保留87.7%的性能，验证了覆盖准则在令牌选择中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 13:57:49 GMT</pubDate>
</item>
<item>
<title>Hermes 4：融合结构化推理与指令遵循的混合模型</title>
<link>https://arxiv.org/abs/2508.18255</link>
<guid>https://arxiv.org/abs/2508.18255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hermes 4是结合多轮推理与广泛指令遵循能力的混合模型。</p><br /><br /><p><strong>摘要：</strong> Hermes 4是一系列融合结构化多轮推理与广泛指令遵循能力的混合推理模型。文章介绍了在数据收集、合成、训练和评估过程中遇到的挑战，并概述了应对这些挑战的解决方案。通过数学推理、编码、知识、理解及对齐基准进行全面评估，报告了定量性能和定性行为分析。为支持开放研究，所有模型权重已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 13:45:06 GMT</pubDate>
</item>
<item>
<title>基于无预设分解问题的声明验证框架研究</title>
<link>https://arxiv.org/abs/2508.16838</link>
<guid>https://arxiv.org/abs/2508.16838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种减少语言模型提示敏感性的声明验证框架。</p><br /><br /><p><strong>摘要：</strong> 本文指出生成问题中的预设可能导致未验证的假设，进而引发声明验证不一致。同时，提示敏感性仍是大型语言模型的重大挑战，性能差异可达3-6%。尽管有进展，但我们的研究表明提示敏感性仍然存在。为此，我们提出了一种结构化且稳健的声明验证框架，通过无预设的分解问题进行推理。实验结果表明，即使最先进的模型仍易受提示变化和预设影响，而我们的方法有效缓解了这些问题，提升了2-5%的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 19:34:24 GMT</pubDate>
</item>
<item>
<title>德国语料库German4All助力多级文本简化</title>
<link>https://arxiv.org/abs/2508.17973</link>
<guid>https://arxiv.org/abs/2508.17973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">German4All是首个德语多级可读性控制语料库，用于文本简化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了German4All，这是首个大规模的德语段落级可读性控制对齐语料库，涵盖五个可读性等级，包含超过25,000个样本。该数据集通过GPT-4自动生成，并经过人工和大模型评估。基于此数据集，研究者训练了一个开源的可读性控制文本改写模型，在德语文本简化任务中表现优异，能够实现更细致的读者适配。研究团队开源了数据集和模型，以推动多级文本改写的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 08:40:32 GMT</pubDate>
</item>
<item>
<title>注意力机制中归一化方法的局限性研究</title>
<link>https://arxiv.org/abs/2508.17821</link>
<guid>https://arxiv.org/abs/2508.17821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了注意力机制中归一化的局限性及其对模型性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了注意力机制中归一化方法的局限性，提出了一个理论框架以分析模型在token选择中的选择能力和几何分离性。通过分析softmax缩放下的距离边界和分离标准，结合预训练GPT-2模型的实验验证，研究发现随着选择token数量增加，模型区分信息token的能力下降，趋于均匀选择。同时，研究还指出在低温度设置下，softmax归一化带来的梯度敏感性对训练构成挑战。这些发现有助于加深对基于softmax的注意力机制的理解，并推动未来更稳健的归一化与选择策略的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 05:25:05 GMT</pubDate>
</item>
<item>
<title>基于高斯点云的稀疏视图表面重建方法MeshSplat</title>
<link>https://arxiv.org/abs/2508.17811</link>
<guid>https://arxiv.org/abs/2508.17811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MeshSplat框架，提升稀疏视图下的表面重建精度。</p><br /><br /><p><strong>摘要：</strong> 本文针对稀疏视图下表面重建精度不足的问题，提出MeshSplat方法，利用高斯点云（2DGS）作为桥梁，将新视角合成与几何先验相结合，实现更准确的表面重建。通过引入前馈网络预测像素对齐的2DGS，减少对3D真实数据的依赖，并采用加权Chamfer距离损失和法线预测网络优化位置与方向预测。实验表明，该方法在通用稀疏视图网格重建任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 05:04:20 GMT</pubDate>
</item>
<item>
<title>基于生成对抗网络的游戏实时摄影级画质增强方法</title>
<link>https://arxiv.org/abs/2508.17061</link>
<guid>https://arxiv.org/abs/2508.17061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REGEN框架提升游戏画面真实感并实现高速推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为REGEN的新型方法，利用生成对抗网络提升视频游戏的摄影级画质。该方法通过双阶段生成网络框架，将动态环境中的图像翻译问题转化为更简单的成对图像翻译任务，从而在不牺牲视觉质量的前提下实现实时推理。实验表明，该方法在《GTA V》上的表现优于直接训练轻量级无配对图像翻译模型的效果，推理速度提升了32.14倍。相关代码和预训练模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 23 Aug 2025 11:28:05 GMT</pubDate>
</item>
<item>
<title>神经网络在细胞自动机框架下的多步推理能力研究</title>
<link>https://arxiv.org/abs/2508.16745</link>
<guid>https://arxiv.org/abs/2508.16745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索不同架构与训练方法对模型多步推理能力的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在细胞自动机框架下，不同神经网络架构和训练方法如何影响模型的多步推理能力。通过使用随机布尔函数生成状态序列进行训练，排除记忆因素，发现大多数模型能够抽象出底层规则。尽管模型在预测下一步状态上表现优异，但在需要多步推理时性能显著下降。研究还表明，增加模型深度对顺序计算至关重要，并通过引入递归、记忆和测试时计算扩展有效提升了推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 14:57:08 GMT</pubDate>
</item>
<item>
<title>InternVL 3.5：多模态模型的推理与效率提升</title>
<link>https://arxiv.org/abs/2508.18265</link>
<guid>https://arxiv.org/abs/2508.18265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InternVL 3.5提升推理能力与效率，支持GUI交互。</p><br /><br /><p><strong>摘要：</strong> InternVL 3.5是新一代开源多模态模型，通过Cascade RL框架和ViR、DvD技术显著提升推理能力和推理效率。该模型在多个任务中表现优异，相比前代模型推理性能提升16%，推理速度加快4.05倍，并支持GUI交互等新功能。其最大版本InternVL3.5-241B-A28B在多项任务中达到领先水平，缩小了与商业模型的差距。所有模型和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 13:58:17 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的半结构化表格问答框架ST-Raptor</title>
<link>https://arxiv.org/abs/2508.18190</link>
<guid>https://arxiv.org/abs/2508.18190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ST-Raptor提升半结构化表格问答准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ST-Raptor，一个基于大语言模型的半结构化表格问答框架。该框架引入了Hierarchical Orthogonal Tree（HO-Tree）来捕捉复杂的表格布局，并定义了基本的树操作以指导LLM执行常见问答任务。通过将用户问题分解为子问题并生成对应的树操作流程，ST-Raptor实现了更准确的问答。此外，还设计了两阶段验证机制以确保答案的可靠性。作者构建了SSTQA数据集进行评估，实验表明ST-Raptor在准确率上优于九种基线方法，最高提升20%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 12:48:51 GMT</pubDate>
</item>
<item>
<title>SpotEdit：评估视觉引导图像编辑的基准测试</title>
<link>https://arxiv.org/abs/2508.18159</link>
<guid>https://arxiv.org/abs/2508.18159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpotEdit是一个评估视觉引导图像编辑的全面基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SpotEdit，这是一个用于系统评估视觉引导图像编辑方法的基准测试。该基准涵盖了多种生成模型，包括扩散模型、自回归模型和混合模型，揭示了它们在实际编辑任务中的性能差异。特别关注了幻觉问题，发现像GPT-4o这样的先进模型有时会错误地认为存在视觉提示并进行不准确的编辑。该研究有助于推动更精确、可控的内容生成技术的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 12:08:57 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型作为评判者在自然语言生成中的有效性</title>
<link>https://arxiv.org/abs/2508.18076</link>
<guid>https://arxiv.org/abs/2508.18076</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLJs在NLG评估中可能缺乏可靠性，需更严谨审视。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型作为评判者（LLJs）在自然语言生成（NLG）评估中的应用。尽管LLJs被视为传统评估指标的替代方案，但其有效性和可靠性尚未得到充分验证。文章基于社会科学研究中的测量理论，分析了LLJs作为人类判断代理、评估能力、可扩展性及成本效益四个核心假设，并指出这些假设可能因LLMs本身的局限性而受到挑战。研究还考察了LLJs在文本摘要、数据标注和安全对齐三个领域的应用，强调需要更加负责任的评估实践，以确保LLJs的发展能推动NLG的进步而非阻碍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18076" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 10:43:10 GMT</pubDate>
</item>
<item>
<title>基于视觉链引导的文本到图像生成方法研究</title>
<link>https://arxiv.org/abs/2508.18032</link>
<guid>https://arxiv.org/abs/2508.18032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Visual-CoG提升多属性图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对文本到图像生成中多属性和模糊提示处理能力不足的问题，提出了一种基于视觉链引导（Visual-CoG）的方法，包含语义推理、过程优化和结果评估三个阶段，并通过阶段感知奖励实现全过程即时指导。该方法在GenEval、T2I-CompBench及自建的VisCog-Bench基准测试中分别提升了15%、5%和19%，展现出优越性能。相关资源即将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.18032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 09:53:02 GMT</pubDate>
</item>
<item>
<title>T2I-ReasonBench：评估文本到图像模型推理能力的基准</title>
<link>https://arxiv.org/abs/2508.17472</link>
<guid>https://arxiv.org/abs/2508.17472</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">T2I-ReasonBench评估文本到图像模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了T2I-ReasonBench，这是一个用于评估文本到图像（T2I）模型推理能力的基准测试。该基准包含四个维度：成语理解、文本图像设计、实体推理和科学推理。作者提出了一种两阶段的评估协议，以衡量模型的推理准确性和图像质量，并对多种T2I生成模型进行了基准测试，提供了全面的性能分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17472" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>compositional visual reasoning 研究综述</title>
<link>https://arxiv.org/abs/2508.17298</link>
<guid>https://arxiv.org/abs/2508.17298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述2023至2025年 compositional visual reasoning 研究进展。</p><br /><br /><p><strong>摘要：</strong> 本文是对2023年至2025年间 compositional visual reasoning 领域的全面综述，涵盖了260多篇来自CVPR、ICCV、NeurIPS等顶级会议的论文。文章首先明确了 compositional 方法的核心定义，并探讨了其在认知对齐、语义保真度、鲁棒性等方面的优势。接着，文章描述了从语言中心的提示增强管道到统一代理视觉语言模型的五阶段范式转变。此外，还列举了60多个基准测试和评估指标，分析了当前研究中的挑战，如LLM推理局限、幻觉、演绎推理偏向等问题，并提出了未来方向，包括世界模型整合、人机协作推理等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 07:01:51 GMT</pubDate>
</item>
<item>
<title>MEENA：首个评估波斯语视觉语言模型的基准数据集</title>
<link>https://arxiv.org/abs/2508.17290</link>
<guid>https://arxiv.org/abs/2508.17290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEENA是首个针对波斯语视觉语言模型的多任务评估数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MEENA，这是首个用于评估波斯语视觉语言模型（VLMs）的基准数据集。该数据集包含约7,500个波斯语问题和3,000个英语问题，涵盖推理、数学、物理、图表、波斯艺术与文学等多个领域。MEENA具有广泛的主题覆盖、丰富的元数据、原创的波斯语数据、双语结构以及多种实验评估，旨在提升非英语语言的视觉语言模型能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 06:32:37 GMT</pubDate>
</item>
<item>
<title>RuscaRL：突破大语言模型推理瓶颈的新方法</title>
<link>https://arxiv.org/abs/2508.16949</link>
<guid>https://arxiv.org/abs/2508.16949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RuscaRL通过指导框架提升大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RuscaRL的新方法，旨在解决大语言模型（LLM）在强化学习（RL）中因探索受限而导致的推理能力提升难题。RuscaRL引入了检查表式指导框架，在推理过程中提供外部引导以生成高质量响应，并在训练阶段利用这些指导获得可验证的奖励，从而提高模型的推理性能。实验表明，RuscaRL在多个基准测试中表现优异，显著提升了Qwen-2.5-7B-Instruct和Qwen3-30B-A3B-Instruct的推理能力，超越了GPT-4.1等先进模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 23 Aug 2025 04:47:31 GMT</pubDate>
</item>
<item>
<title>TaDiCodec：一种高效端到端的语音编码器</title>
<link>https://arxiv.org/abs/2508.16790</link>
<guid>https://arxiv.org/abs/2508.16790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaDiCodec实现低帧率高压缩语音重建，无需预训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Text-aware Diffusion Transformer Speech Codec (TaDiCodec) 的新型语音编码器，旨在解决现有语音语言模型在量化结构、依赖预训练模型和复杂训练流程方面的不足。TaDiCodec通过扩散自编码器实现端到端优化，在单层代码本下达到6.25 Hz的极低帧率和0.0875 kbps的比特率，同时保持优异的语音生成性能，包括较低的词错误率（WER）、较高的说话人相似度（SIM）和语音质量（UTMOS）。该方法采用单阶段端到端训练，无需辅助预训练模型，并验证了其在基于语言模型的零样本文本到语音任务中的有效性，展现出较小的重建与生成差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 16:45:03 GMT</pubDate>
</item>
<item>
<title>基于多视角检索的文本到3D生成方法MV-RAG</title>
<link>https://arxiv.org/abs/2508.16577</link>
<guid>https://arxiv.org/abs/2508.16577</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MV-RAG提升文本到3D生成的3D一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出MV-RAG，一种新的文本到3D生成方法，通过从大规模2D图像数据库中检索相关图像，并将其作为多视角扩散模型的条件，从而生成更一致和准确的多视角输出。该方法结合了结构化多视角数据和多样化2D图像集，采用混合训练策略提升模型性能。实验表明，MV-RAG在罕见或域外概念上显著提升了3D一致性、逼真度和文本匹配度，同时在标准基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16577" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>UQ：一种基于未解问题的AI模型评估新范式</title>
<link>https://arxiv.org/abs/2508.17580</link>
<guid>https://arxiv.org/abs/2508.17580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UQ通过未解问题评估AI模型，兼具难度与现实价值。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的AI模型评估范式——UQ，它通过未解问题来测试模型的能力。与传统基准测试不同，UQ由来自Stack Exchange的500个多样化问题组成，涵盖计算机科学、数学、科幻和历史等多个领域，旨在挑战前沿模型并具有实际应用价值。UQ通过规则过滤、LLM判断和人工审核确保问题质量，并引入验证者机制和平台支持专家协作验证答案。实验表明，顶级模型仅能在15%的问题上通过验证，初步验证已发现正确答案。该方法为评估AI在开放性现实问题上的表现提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 24 Aug 2025 21:07:59 GMT</pubDate>
</item>
<item>
<title>基于多智能体系统的论文转海报生成框架PosterGen</title>
<link>https://arxiv.org/abs/2508.17188</link>
<guid>https://arxiv.org/abs/2508.17188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PosterGen通过多智能体协作生成高质量学术海报。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PosterGen的多智能体系统，用于将学术论文自动转换为高质量的会议海报。该系统由四个协同工作的专业代理组成：解析与整理代理、布局代理、风格代理和渲染代理，分别负责内容提取、空间布局设计、视觉风格应用和最终海报生成。为了评估设计质量，研究者引入了一个基于视觉-语言模型的评分标准，涵盖布局平衡、可读性和美学一致性等指标。实验结果表明，PosterGen在内容准确性方面表现优异，并在视觉设计上显著优于现有方法，能够生成几乎无需人工修改的高质量海报。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.17188" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 23 Aug 2025 22:25:45 GMT</pubDate>
</item>
<item>
<title>基于知识蒸馏的3D高斯点云渲染优化方法</title>
<link>https://arxiv.org/abs/2508.14037</link>
<guid>https://arxiv.org/abs/2508.14037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种3DGS知识蒸馏框架，提升渲染质量与存储效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D高斯点云（3DGS）在新视角合成中因高保真渲染导致的内存消耗大的问题，提出首个基于知识蒸馏的框架。该框架利用多种教师模型（如原始3DGS、噪声增强和Dropout正则化版本）输出进行聚合，指导轻量学生模型优化。为保留几何结构信息，引入结构相似性损失以增强学生与教师模型的空间分布一致性。实验表明，所提出的Distilled-3DGS在多个数据集上表现出色，兼具高质量渲染与高效存储，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>基于旋转与循环移位等变性的轮廓数据深度学习框架</title>
<link>https://arxiv.org/abs/2508.16359</link>
<guid>https://arxiv.org/abs/2508.16359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RotaTouille实现轮廓数据的旋转与循环移位等变性学习。</p><br /><br /><p><strong>摘要：</strong> 本文提出RotaTouille，一种用于处理轮廓数据的深度学习框架，通过复数圆卷积实现旋转和循环移位等变性。研究还引入了等变非线性函数、降采样层和全局池化层，以获得对下游任务具有不变性的表示。实验表明，该方法在形状分类、重建和轮廓回归任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 09:05:55 GMT</pubDate>
</item>
<item>
<title>基于草图的3D视频编辑方法Sketch3DVE</title>
<link>https://arxiv.org/abs/2508.13797</link>
<guid>https://arxiv.org/abs/2508.13797</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sketch3DVE实现3D视频结构内容的精确编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于草图的3D视频编辑方法Sketch3DVE，旨在解决在大视角变化下对视频结构内容进行编辑的难题。该方法通过图像编辑技术生成首帧的编辑结果，并将其传播至视频其他帧。利用草图进行几何控制，并结合点云估计和深度图表示新编辑部分的3D结构，确保与原场景的一致性。同时引入3D感知的掩码传播策略和视频扩散模型，实现真实感的视频编辑效果。实验表明，Sketch3DVE在视频编辑任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13797" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 08:57:31 GMT</pubDate>
</item>
<item>
<title>基于对比学习的链式思维强化微调方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.15868</link>
<guid>https://arxiv.org/abs/2508.15868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法提升LLM推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLM）在推理能力上的不足，提出了一种基于标注链式思维（CoT）的对比学习强化微调方法（CARFT）。该方法通过学习每个CoT的表示，并设计新的对比信号来指导微调过程，解决了传统强化学习方法在训练过程中不稳定、模型崩溃以及过度依赖标注CoT的问题。实验表明，该方法在多个基准数据集和模型上均表现出显著的性能提升，最高可达10.15%，同时提升了训练效率，最高达30.62%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 20:20:47 GMT</pubDate>
</item>
<item>
<title>基于自我对弈与变分问题生成的强化学习方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.14029</link>
<guid>https://arxiv.org/abs/2508.14029</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SvS策略提升RLVR训练中模型推理性能与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于可验证奖励的强化学习（RLVR）在大语言模型中的应用，指出传统方法虽能提升Pass@1性能，但会降低策略熵，影响生成多样性。为解决这一问题，作者提出在线自我对弈与变分问题生成（SvS）策略，通过利用正确解生成变分问题来维持策略熵，从而显著提升Pass@k性能。实验表明，该方法在多个推理基准测试中表现优异，尤其在AIME24和AIME25上分别提升了18.3%和22.8%。SvS在不同规模的模型中均展现出良好的泛化性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14029" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 13:42:45 GMT</pubDate>
</item>
<item>
<title>基于稀疏自编码器的持久概念遗忘方法CRISP</title>
<link>https://arxiv.org/abs/2508.13650</link>
<guid>https://arxiv.org/abs/2508.13650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRISP实现持久概念遗忘，提升模型安全性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型在现实场景中的广泛应用，如何选择性移除有害知识同时保持模型功能成为关键问题。本文提出CRISP方法，利用稀疏自编码器（SAE）实现对目标概念的持久遗忘。该方法通过自动识别多层中显著的SAE特征并抑制其激活，有效移除了有害知识，同时保留了模型的通用能力和领域内性能。实验表明，CRISP在WMDP基准测试中优于现有方法，实现了语义上一致的概念分离。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 05:01:22 GMT</pubDate>
</item>
<item>
<title>Learnable SMPLify：基于神经网络的3D人体姿态估计方法</title>
<link>https://arxiv.org/abs/2508.13562</link>
<guid>https://arxiv.org/abs/2508.13562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Learnable SMPLify提升3D人体姿态估计效率与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Learnable SMPLify，一种将SMPLify的迭代优化过程替换为单次回归模型的神经框架。该方法通过时间采样策略构建初始化-目标对，并引入人体中心归一化和残差学习以提高泛化能力。实验表明，该方法在运行速度上比SMPLify快近200倍，且在多个数据集上表现良好，支持序列推理和插件后处理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 02:53:57 GMT</pubDate>
</item>
<item>
<title>EgoTwin：联合生成第一视角视频与人体运动的框架</title>
<link>https://arxiv.org/abs/2508.13013</link>
<guid>https://arxiv.org/abs/2508.13013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoTwin解决第一视角视频与人体运动生成难题。</p><br /><br /><p><strong>摘要：</strong> 本文提出EgoTwin，一个基于扩散Transformer架构的联合视频与人体运动生成框架。该框架解决了两个关键挑战：视角对齐和因果交互。通过引入以头部为中心的运动表示和受控制论启发的交互机制，EgoTwin能够生成与人体动作自然匹配的第一视角视频。研究团队还构建了一个大规模的真实世界数据集，并设计了新的评估指标来衡量视频与运动的一致性。实验结果表明，该框架在相关任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 11:33:09 GMT</pubDate>
</item>
<item>
<title>ODYSSEY：面向复杂环境的移动操作框架研究</title>
<link>https://arxiv.org/abs/2508.08240</link>
<guid>https://arxiv.org/abs/2508.08240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ODYSSEY框架提升机器人长时序操作能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对移动机器人在复杂环境中进行长期任务规划与操作的挑战，提出了ODYSSEY框架。该框架结合高层任务规划与低层全身控制，解决感知受限、操作范围有限及环境多变等问题。通过引入基于视觉语言模型的分层规划器，实现指令分解与精确执行，并在不同地形中实现稳健控制。研究还构建了首个长时序移动操作基准测试，验证了系统在真实环境中的泛化能力和鲁棒性，展示了腿式机械臂在非结构化场景中的实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:54:31 GMT</pubDate>
</item>
<item>
<title>基于CLIP的弱监督可操作性定位方法研究</title>
<link>https://arxiv.org/abs/2508.07877</link>
<guid>https://arxiv.org/abs/2508.07877</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于CLIP的弱监督可操作性定位方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究如何通过弱监督方式实现物体可操作性定位。传统方法依赖分类器和知识蒸馏策略，但常忽视与可操作性无关的常见特征。为解决此问题，本文引入选择性原型和像素对比目标，在不同粒度信息下自适应学习可操作性相关线索。通过CLIP模型在第一人称和第三人称视角中识别动作相关对象，并交叉验证获取精确的部分级可操作性线索，有效区分可操作区域与背景。实验结果表明该方法具有良好的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07877" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 07:49:37 GMT</pubDate>
</item>
<item>
<title>AetherCode：评估大型语言模型代码能力的新基准</title>
<link>https://arxiv.org/abs/2508.16402</link>
<guid>https://arxiv.org/abs/2508.16402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AetherCode提升LLM代码评估的准确性和挑战性。</p><br /><br /><p><strong>摘要：</strong> 文章指出当前评估大型语言模型（LLMs）代码能力的基准存在不足，未能真实反映模型与顶尖程序员之间的差距。为解决这一问题，作者提出了AetherCode，一个基于国际信息学奥林匹克竞赛（IOI）和国际大学生程序设计竞赛（ICPC）的新型基准。AetherCode不仅包含更具挑战性的题目，还通过自动化生成与人工验证相结合的方式构建了高质量的测试用例，从而提供更可靠、严谨的评估方式，推动代码推理研究的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 10:04:55 GMT</pubDate>
</item>
<item>
<title>基于语言指令的机器人任务处理框架研究</title>
<link>https://arxiv.org/abs/2508.16292</link>
<guid>https://arxiv.org/abs/2508.16292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出IVA框架提升机器人对错误指令的理解与响应能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉-语言-动作（VLA）模型在处理包含错误前提的自然语言指令时的表现，并提出了Instruct-Verify-and-Act（IVA）框架。该框架能够检测无法执行的指令，进行语言澄清或修正，并在感知和行动中提供合理替代方案。通过构建大规模指令调优数据集，训练出能处理准确与错误请求的VLA模型。实验表明，IVA在错误前提检测准确率上比基线提升了97.56%，并在错误场景下的成功响应率提高了50.78%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 06:54:33 GMT</pubDate>
</item>
<item>
<title>AgentScope 1.0：支持高效工具交互的智能代理框架</title>
<link>https://arxiv.org/abs/2508.16279</link>
<guid>https://arxiv.org/abs/2508.16279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentScope 1.0 提升了智能代理的工具交互能力，优化了开发体验。</p><br /><br /><p><strong>摘要：</strong> AgentScope 1.0 是一个面向智能代理应用的新型框架，旨在提升代理在现实任务中的表现。它通过抽象关键组件、提供统一接口和可扩展模块，使开发者能够更便捷地利用最新模型和技术。该框架基于 ReAct 模式，并采用异步设计增强交互效率与多样性。此外，AgentScope 集成了针对特定场景的内置代理、可视化评估工具以及运行时沙箱，提升了开发效率与安全性，为构建可扩展、适应性强的智能代理应用提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 06:35:56 GMT</pubDate>
</item>
<item>
<title>基于记忆的在线强化学习实现LLM代理的持续适应</title>
<link>https://arxiv.org/abs/2508.16153</link>
<guid>https://arxiv.org/abs/2508.16153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需微调的LLM代理新方法，提升持续学习能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型的自适应大型语言模型（LLM）代理学习范式，无需对底层LLM进行微调。该方法通过记忆增强的马尔可夫决策过程（M-MDP）实现低成本的持续适应，利用神经案例选择策略引导行动决策，并通过记忆重写机制和高效检索实现策略更新与优化。在DeepResearch场景中，该模型AgentFly在GAIA验证集上取得87.88%的Pass@3成绩，在测试集上达到79.40%，并在DeepResearcher数据集上获得66.6% F1和80.4% PM，优于现有训练方法。记忆机制在分布外任务中提升了4.7%-9.6%。该方法为构建无需梯度更新的通用LLM代理提供了高效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 03:25:30 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在社交推理游戏中的个性化推理能力</title>
<link>https://arxiv.org/abs/2508.16072</link>
<guid>https://arxiv.org/abs/2508.16072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出InMind框架，评估LLMs在社交推理游戏中的个性化推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出InMind框架，用于评估大语言模型（LLMs）在社交推理游戏（SDGs）中捕捉和应用个性化推理风格的能力。该框架通过结构化游戏数据、回合级策略记录和赛后反思，支持四种认知任务，以评估静态对齐与动态适应能力。研究以《Avalon》游戏为例，评估了11个最先进的LLMs，发现通用LLMs如GPT-4o常依赖词汇线索，难以根据时间演化调整策略，而增强推理的LLMs如DeepSeek-R1展现出初步的风格敏感推理能力。研究揭示了当前LLMs在个性化和自适应推理方面的局限性，并推动更符合人类认知的人机交互发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.16072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:04:00 GMT</pubDate>
</item>
<item>
<title>基于强化学习的医疗诊断增强系统Deep-DxSearch</title>
<link>https://arxiv.org/abs/2508.15746</link>
<guid>https://arxiv.org/abs/2508.15746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Deep-DxSearch提升医疗诊断准确性，通过强化学习优化检索与推理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Deep-DxSearch，一个基于强化学习（RL）的自主RAG系统，旨在提升医疗诊断的准确性和可追溯性。该系统构建了一个大规模医学检索语料库，并将大语言模型作为核心代理，通过定制奖励机制优化检索、推理结构和诊断准确性。实验表明，Deep-DxSearch在常见和罕见疾病的诊断中均优于GPT-4o、DeepSeek-R1等现有框架。消融实验验证了奖励设计和检索语料的重要性，案例研究也展示了其在临床诊断中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:42:47 GMT</pubDate>
</item>
<item>
<title>Tensor-Parallel Latent Attention 提升模型推理效率</title>
<link>https://arxiv.org/abs/2508.15881</link>
<guid>https://arxiv.org/abs/2508.15881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPLA实现高效张量并行注意力机制，提升模型推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Tensor-Parallel Latent Attention (TPLA)，在 DeepSeek-V2 的 Multi-Head Latent Attention (MLA) 基础上优化了张量并行计算。TPLA 将潜在表示和每个注意力头的输入维度划分到不同设备上独立计算，并通过 all-reduce 合并结果，保留 MLA 的压缩键值缓存优势，同时提升并行效率。相比 Grouped Latent Attention (GLA)，TPLA 中每个头仍能利用完整的潜在表示，保持更强的表达能力。TPLA 可无缝集成预训练模型，支持 MLA 式预填充，并在不重新训练的情况下实现高效的张量并行解码。通过简单的正交变换（如 Hadamard 变换或 PCA）减少跨设备干扰，保证性能稳定。实验表明，在 DeepSeek-V3 和 Kimi-K2 上分别获得 1.79x 和 1.93x 的加速，且在常识与 LongBench 基准测试中保持良好表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 11:25:40 GMT</pubDate>
</item>
<item>
<title>基于LLM与人工辅助的恶意内容检测框架及其在越狱攻击评估中的应用</title>
<link>https://arxiv.org/abs/2508.10390</link>
<guid>https://arxiv.org/abs/2508.10390</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MDH框架提升恶意内容检测效率，增强越狱攻击评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对越狱攻击评估中因提示不明显或未产生有害输出而导致的挑战，提出一种结合大语言模型与人工辅助的混合评估框架MDH，用于数据集清洗和越狱响应检测。研究发现精心设计的开发者消息可显著提升越狱成功率，并据此提出两种新策略：D-Attack利用上下文模拟，DH-CoT引入劫持思维链。相关代码、数据集及检测结果已发布于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10390" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 02:46:56 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的高效图像编辑方法VAREdit</title>
<link>https://arxiv.org/abs/2508.15772</link>
<guid>https://arxiv.org/abs/2508.15772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VAREdit通过自回归机制实现高效且精准的图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出VAREdit，一种基于自回归模型的图像编辑框架，将图像编辑任务转化为多尺度预测问题。该方法通过条件生成目标特征来实现精确编辑，克服了扩散模型在全局去噪过程中易产生不必要修改的问题。为解决源图像特征与目标特征尺度不匹配的问题，引入Scale-Aligned Reference模块，提升编辑准确性。实验表明，VAREdit在编辑符合度和效率方面均优于现有扩散模型，512×512图像编辑仅需1.2秒，速度是UltraEdit的2.2倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>LLaSO：首个全面开源的大规模语音语言建模框架</title>
<link>https://arxiv.org/abs/2508.15418</link>
<guid>https://arxiv.org/abs/2508.15418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaSO提供语音语言模型的开放数据与基准，推动研究标准化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了LLaSO，这是首个完全开源、端到端的大规模语音语言建模框架。LLaSO提供了三个关键资源：LLaSO-Align语音文本对齐语料库、LLaSO-Instruct多任务指令调优数据集以及LLaSO-Eval标准化评估基准。同时，作者发布了LLaSO-Base模型，该模型在公开数据上训练，取得了0.72的标准化得分，成为强有力的基线。研究指出，尽管更广泛的训练数据能提升性能，但在未见任务上仍存在显著泛化差距，尤其是在纯音频场景中。LLaSO通过开放所有数据、基准和模型，为语音语言模型的研究建立了统一标准，推动社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 06:20:00 GMT</pubDate>
</item>
<item>
<title>AI伴侣行为评估基准INTIMA揭示情感互动模式</title>
<link>https://arxiv.org/abs/2508.09998</link>
<guid>https://arxiv.org/abs/2508.09998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入INTIMA基准评估AI伴侣行为，发现情感支持行为普遍但存在模型差异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了INTIMA（Interactions and Machine Attachment Benchmark）基准，用于评估语言模型中的伴侣行为。基于心理学理论和用户数据，构建了涵盖31种行为的分类体系，并通过368个针对性提示进行测试。结果显示，所有模型中情感强化行为更为常见，但不同模型在敏感部分的优先级存在显著差异，这引发了对适当边界设定和情感支持平衡的关注。研究强调了在处理情感互动时需要更一致的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 04:25:38 GMT</pubDate>
</item>
<item>
<title>ATLAS：一种高保真人体建模方法</title>
<link>https://arxiv.org/abs/2508.15767</link>
<guid>https://arxiv.org/abs/2508.15767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ATLAS通过解耦形状与骨骼基底提升人体建模精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ATLAS的高保真人体模型，该模型基于60万张高分辨率扫描数据，通过将网格表示建立在人体骨骼基础上，实现了形状与骨骼基底的显式解耦。这种方法增强了形状表达能力，支持更精细的体征定制，并能独立于外部软组织进行关键点匹配。相比传统线性模型，ATLAS在多种姿态下对未见过的受试者具有更高的拟合精度，且非线性姿态校正更能准确捕捉复杂姿态。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:58:56 GMT</pubDate>
</item>
<item>
<title>Waver：统一图像与视频生成的高性能基础模型</title>
<link>https://arxiv.org/abs/2508.15761</link>
<guid>https://arxiv.org/abs/2508.15761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Waver支持多种视频生成任务，性能领先同类模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Waver，一个用于统一图像和视频生成的高性能基础模型。Waver能够直接生成5至10秒的720p视频，并提升至1080p。该模型支持文本到视频、图像到视频和文本到图像的生成。通过引入Hybrid Stream DiT架构和高质量数据筛选机制，Waver在运动捕捉和时间一致性方面表现出色，排名全球前3，在多个基准测试中超越现有开源模型，接近或超过商业解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:56:10 GMT</pubDate>
</item>
<item>
<title>LiveMCP-101：评估AI代理多工具协作能力的新基准</title>
<link>https://arxiv.org/abs/2508.15760</link>
<guid>https://arxiv.org/abs/2508.15760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveMCP-101测试AI代理在真实场景中使用多种工具完成复杂任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LiveMCP-101，这是一个包含101个精心设计的真实查询的基准测试集，旨在评估AI代理在动态现实环境中协调使用多种MCP工具（如网络搜索、文件操作、数学推理和数据分析）解决多步骤任务的能力。研究提出了一种基于真实执行计划的评估方法，而非仅依赖API输出，从而更准确地反映实际环境的变化性。实验表明，即使是先进的大型语言模型在该任务上的成功率也低于60%，揭示了工具编排中的主要挑战。通过详细的消融实验和错误分析，研究进一步指出了模型在任务执行效率和令牌使用方面的不足，并为未来模型优化提供了方向。LiveMCP-101为评估AI代理的真实世界能力设定了高标准，推动了自主AI系统的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:55:54 GMT</pubDate>
</item>
<item>
<title>Geo-Visual Agents：基于多模态AI的地理视觉问答系统</title>
<link>https://arxiv.org/abs/2508.15752</link>
<guid>https://arxiv.org/abs/2508.15752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Geo-Visual Agents通过分析地理图像实现对空间问题的理解与回答。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Geo-Visual Agents的概念，这是一种能够理解和回答复杂视觉空间问题的多模态AI代理。它结合了大规模地理图像数据（如街景、景点照片和卫星图像）与传统GIS数据，旨在提升数字地图在地理视觉查询方面的表现。文章提出了该系统的愿景，描述了感知与交互方法，并提供了三个示例，同时讨论了未来研究的关键挑战与机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:49:52 GMT</pubDate>
</item>
<item>
<title>Grounded VideoDiT：提升视频理解的时序感知与实体对齐能力</title>
<link>https://arxiv.org/abs/2508.15641</link>
<guid>https://arxiv.org/abs/2508.15641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Grounded VideoDiT，提升视频中的时间定位与实体交互理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Grounded VideoDiT，一种旨在提升视频理解能力的视频大模型。针对现有视频大模型在时序感知和实体对齐方面的不足，该模型引入了三项关键创新：Diffusion Temporal Latent（DTL）编码器增强边界敏感性和时间一致性；对象引导表示将查询实体与局部视觉证据绑定，加强对齐；混合标记方案结合离散时间标记，实现精细的时间推理。实验结果表明，Grounded VideoDiT在Charades STA、NExT GQA等多个视频问答基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 11:12:14 GMT</pubDate>
</item>
<item>
<title>大型语言模型评估基准的现状与挑战</title>
<link>https://arxiv.org/abs/2508.15361</link>
<guid>https://arxiv.org/abs/2508.15361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统综述大语言模型评估基准，分析其分类与现存问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着大型语言模型能力的不断提升，相关评估基准也逐渐增多。本文首次系统回顾了当前大语言模型评估基准的发展状况，将283个代表性基准分为通用能力、领域特定和目标特定三类。通用能力基准涵盖语言、知识和推理等方面；领域特定基准关注自然科学、人文社科和工程技术等；目标特定基准则涉及风险、可靠性及代理系统等。文章指出当前基准存在数据污染导致评分虚高、文化语言偏见引发评价不公以及缺乏对过程可信度和动态环境评估等问题，并提出了未来基准设计的参考范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 04:43:35 GMT</pubDate>
</item>
<item>
<title>aiXiv：面向人工智能科学家的开放科研平台</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">aiXiv解决AI生成研究内容的发布难题。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了aiXiv，一个面向人工智能和人类科学家的下一代开放获取平台。该平台采用多智能体架构，支持研究提案和论文的提交、评审与迭代优化，并提供API和MCP接口以实现人机协作。通过实验验证，aiXiv能够显著提升AI生成研究的质量，推动高质量AI研究成果的发表与传播。该平台旨在构建一个可扩展、可持续的科学发现生态系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 19:16:41 GMT</pubDate>
</item>
<item>
<title>基于双视角图像的3D人体重建方法</title>
<link>https://arxiv.org/abs/2508.14892</link>
<guid>https://arxiv.org/abs/2508.14892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">从前后视图重建3D人体，提升渲染质量与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种从仅有的前后视图中重建完整3D人体的方法，旨在降低用户创建3D数字人类的门槛。该方法通过优化几何重建模型和增强算法，有效解决输入信息稀疏带来的挑战，实现高精度点云重建并补充颜色信息。实验表明，该方法在THuman2.0和跨域数据集上表现优异，且可在低成本移动设备上运行，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:59:11 GMT</pubDate>
</item>
<item>
<title>SceneGen：基于单图和多图输入的3D场景资产生成框架</title>
<link>https://arxiv.org/abs/2508.15769</link>
<guid>https://arxiv.org/abs/2508.15769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SceneGen可直接生成3D场景资产，无需优化或检索。</p><br /><br /><p><strong>摘要：</strong> 本文提出SceneGen，一个能够从单张场景图像及对应物体掩码中同时生成多个3D资产（包括几何和纹理）的框架。该方法无需优化或资产检索，通过引入特征聚合模块，结合视觉和几何编码器的信息，实现3D资产及其相对空间位置的同步生成。此外，SceneGen在多图像输入场景下也表现出良好的扩展性，即使仅在单图像数据上训练，也能提升生成效果。大量定量和定性评估验证了该方法的高效性和鲁棒性，为高质量3D内容生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:59:16 GMT</pubDate>
</item>
<item>
<title>Intern-S1：面向科学领域的高性能开源基础模型</title>
<link>https://arxiv.org/abs/2508.15763</link>
<guid>https://arxiv.org/abs/2508.15763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Intern-S1在科学领域表现卓越，超越部分闭源模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Intern-S1，一个专注于科学领域的开源基础模型，具备多模态和专家混合（MoE）架构，拥有280亿激活参数和2410亿总参数。该模型在5T token数据上持续预训练，其中2.5T来自科学领域。通过离线与在线强化学习（RL）训练，以及Mixture-of-Rewards（MoR）技术，Intern-S1在多个科学任务中表现出色，如分子合成规划、反应条件预测等，性能优于多数开源模型，并在专业任务中超越了闭源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 13:58:00 GMT</pubDate>
</item>
<item>
<title>DeepConf提升大语言模型推理效率与准确性</title>
<link>https://arxiv.org/abs/2508.15260</link>
<guid>https://arxiv.org/abs/2508.15260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepConf通过置信度过滤提升推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Deep Think with Confidence (DeepConf) 方法，利用模型内部的置信度信号动态过滤低质量推理路径，无需额外训练或调参，可集成至现有框架。在多个推理任务和最新开源模型上评估，DeepConf在AIME 2025等挑战性基准测试中表现出色，准确率高达99.9%，并减少84.7%的生成token。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 01:48:38 GMT</pubDate>
</item>
<item>
<title>Fin-PRM：面向金融领域的过程奖励模型</title>
<link>https://arxiv.org/abs/2508.15202</link>
<guid>https://arxiv.org/abs/2508.15202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fin-PRM提升金融任务中语言模型的推理质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Fin-PRM，一种针对金融领域设计的过程奖励模型，用于评估大语言模型在金融任务中的中间推理步骤。该模型结合了步骤级和轨迹级奖励监督，能够更精确地衡量符合金融逻辑的推理过程。Fin-PRM在离线和在线学习设置中表现出色，支持高质量推理轨迹选择、密集过程奖励生成以及测试时的奖励引导推理。实验结果表明，Fin-PRM在CFLUE和FinQA等金融推理基准上优于通用PRM和强基线模型，提升了监督学习、强化学习和测试性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 23:31:11 GMT</pubDate>
</item>
<item>
<title>GUI-Owl与Mobile-Agent-v3：开源GUI代理模型的最新进展</title>
<link>https://arxiv.org/abs/2508.15144</link>
<guid>https://arxiv.org/abs/2508.15144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI-Owl和Mobile-Agent-v3在多个GUI基准测试中取得新高。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GUI-Owl，一个在桌面和移动环境中表现优异的开源GUI代理模型，在十个GUI基准测试中达到最先进的性能。GUI-Owl-7B在AndroidWorld和OSWorld分别获得66.4和29.4的分数。基于此，研究者提出了Mobile-Agent-v3框架，进一步提升至73.3和37.7。GUI-Owl包含三个关键创新：大规模环境基础设施、多样化的基础代理能力以及可扩展的环境强化学习。该模型支持端到端决策，并可作为多代理系统中的模块化组件。研究还引入了Trajectory-aware Relative Policy Optimization (TRPO) 方法，实现了34.9的OSWorld得分。GUI-Owl和Mobile-Agent-v3已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.15144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 20:39:12 GMT</pubDate>
</item>
<item>
<title>FLARE：一种线性复杂度的自注意力机制</title>
<link>https://arxiv.org/abs/2508.12594</link>
<guid>https://arxiv.org/abs/2508.12594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLARE实现线性复杂度自注意力，提升大规模数据处理能力。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为FLARE的自注意力机制，具有线性复杂度，通过固定长度的潜在序列路由注意力，从而在大规模无结构网格上实现高效计算。该方法通过将输入序列投影到固定长度的潜在序列，以较低的计算成本学习低秩注意力形式，显著提升了模型的可扩展性和准确性。FLARE在多个基准测试中优于最先进的神经偏微分方程代理模型，并提供了新的增材制造数据集以促进进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 23:00:55 GMT</pubDate>
</item>
<item>
<title>MCP-Universe：首个评估大语言模型与外部工具交互的综合基准</title>
<link>https://arxiv.org/abs/2508.14704</link>
<guid>https://arxiv.org/abs/2508.14704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCP-Universe是首个评估LLM在真实任务中表现的基准，涵盖多个领域。</p><br /><br /><p><strong>摘要：</strong> MCP-Universe是一个全新的基准测试框架，旨在评估大语言模型（LLM）在与真实世界MCP服务器交互时的表现。该基准覆盖六个核心领域，包括位置导航、代码仓库管理、金融分析、3D设计、浏览器自动化和网络搜索，共涉及11个MCP服务器。为确保评估的严谨性，MCP-Universe引入了执行评估器，包括格式评估、静态评估和动态评估。实验结果显示，即使是最先进的模型如GPT-5、Grok-4和Claude-4.0-Sonnet也表现出显著性能限制。此外，该基准还提出了长上下文和未知工具的挑战，同时开放了可扩展的评估框架，支持研究人员和开发者集成新代理和MCP服务器，推动MCP生态系统的持续发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 09:28:58 GMT</pubDate>
</item>
<item>
<title>基于全同态加密的Levenshtein距离优化算法</title>
<link>https://arxiv.org/abs/2508.14568</link>
<guid>https://arxiv.org/abs/2508.14568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出优化Levenshtein距离计算方法，提升FHE性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种在全同态加密（FHE）框架下计算Levenshtein距离的新方法，特别针对第三代方案如TFHE。该算法通过减少每个计算单元所需的可编程刷新次数，将计算成本从传统Wagner-Fisher算法的约94次降至仅1次，并优化了字符比较过程，将ASCII字符比较降至2次PBS操作。此外，当其中一个输入为明文时，通过预处理可进一步提升性能。实验表明，该方法比现有最佳TFHE实现快278倍，比优化后的Wagner-Fisher算法快39倍，且在有明文输入时还能额外提升3倍速度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14568" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 05:40:06 GMT</pubDate>
</item>
<item>
<title>扩散语言模型的量化研究与部署分析</title>
<link>https://arxiv.org/abs/2508.14896</link>
<guid>https://arxiv.org/abs/2508.14896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型的量化方法及部署挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统研究了扩散大语言模型（dLLMs）的后训练量化（PTQ）方法。文章指出，激活异常值是低比特量化的主要障碍，因为它们占据了动态范围的大部分，影响了大多数值的精度。作者采用了先进的PTQ技术，并在多种任务类型和模型变体上进行了全面评估。分析从四个维度展开：位宽、量化方法、任务类别和模型类型，为未来高效部署dLLMs提供了实践见解。所有代码和实验设置将公开，以支持社区研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>MeshCoder：将3D物体重建为可编辑的Python脚本</title>
<link>https://arxiv.org/abs/2508.14879</link>
<guid>https://arxiv.org/abs/2508.14879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MeshCoder通过点云生成可编辑的Blender Python脚本，提升3D形状重建与编辑能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MeshCoder框架，能够将复杂的3D物体从点云数据重建为可编辑的Blender Python脚本。该框架包含一套丰富的Blender Python API，用于生成复杂几何结构，并构建了一个大规模的配对数据集。基于这些数据，训练了一个多模态大语言模型，实现从3D点云到可执行代码的转换。该方法不仅提升了形状到代码的重建性能，还支持直观的几何和拓扑编辑，增强了大语言模型在3D形状理解中的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 13:50:15 GMT</pubDate>
</item>
<item>
<title>Tinker：无需微调的高保真3D编辑框架</title>
<link>https://arxiv.org/abs/2508.14811</link>
<guid>https://arxiv.org/abs/2508.14811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tinker实现零样本3D编辑，提升多视角一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tinker，一个能够在单次或少量样本情况下进行高保真3D编辑的框架，无需针对每个场景进行微调。Tinker通过重新利用预训练扩散模型，实现了对3D潜在空间的理解，并具备多视角一致性编辑能力。研究团队构建了首个大规模多视角编辑数据集，支持多样化的场景和风格。Tinker包含两个创新组件：参考式多视角编辑器和任意视角到视频的合成器，分别用于精确编辑和高质量场景生成。实验表明，Tinker在编辑、新视角生成和渲染增强任务中均达到领先水平，为通用化3D内容创作提供了重要突破。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 12:02:59 GMT</pubDate>
</item>
<item>
<title>DuPO：一种无需标注的双学习偏好优化框架</title>
<link>https://arxiv.org/abs/2508.14460</link>
<guid>https://arxiv.org/abs/2508.14460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuPO通过双学习机制实现无标注反馈，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuPO，一种基于双学习的偏好优化框架，能够生成无需标注的反馈。该方法解决了传统强化学习依赖昂贵标签和仅适用于可验证任务的问题，同时突破了传统双学习仅限于严格双任务对的限制。DuPO将原始任务输入分解为已知和未知部分，并构建双任务来利用原始输出和已知信息重建未知部分，从而扩展到非可逆任务。重建质量作为自监督奖励优化原始任务，结合大语言模型的能力，实现单模型完成两个任务。实验表明，DuPO在多个任务中取得显著提升，如翻译质量提高2.13 COMET，数学推理准确率提升6.4分，推理重排序性能提升9.3分，展现出其在大语言模型优化中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 02:31:18 GMT</pubDate>
</item>
<item>
<title>Nemotron-Nano-9B-v2：提升推理性能的混合Mamba-Transformer语言模型</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nemotron-Nano-9B-v2在推理任务中实现更高吞吐量与准确率。</p><br /><br /><p><strong>摘要：</strong> Nemotron-Nano-9B-v2是一款结合Mamba和Transformer结构的语言模型，旨在提升推理任务的吞吐量并保持与同类模型相当或更高的准确性。该模型基于Nemotron-H架构，用Mamba-2层替代了传统Transformer中的大部分自注意力层，从而提高推理速度。通过预训练120亿参数的Nemotron-Nano-12B-v2-Base模型，并采用Minitron策略进行压缩和蒸馏，使得模型能够在单块NVIDIA A10G GPU上处理最多128k tokens。实验表明，相比Qwen3-8B等模型，Nemotron-Nano-9B-v2在推理任务中实现了高达6倍的吞吐量提升，同时保持优异的准确性。相关模型和数据集已发布在Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 02:00:57 GMT</pubDate>
</item>
<item>
<title>提升模型局部尺度不变性的深度均衡校准器</title>
<link>https://arxiv.org/abs/2508.14187</link>
<guid>https://arxiv.org/abs/2508.14187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DEC提升模型对局部尺度变化的适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种深度均衡校准器（DEC），用于增强模型对局部尺度变化的适应性。DEC可以轻松集成到现有的网络架构中，并适用于预训练模型。实验表明，在ImageNet基准测试中，DEC在多个主流预训练模型（如ViT、DeiT、Swin和BEiT）上提升了模型性能和局部尺度一致性。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 14:21:59 GMT</pubDate>
</item>
<item>
<title>RynnEC：面向具身认知的视频多模态大语言模型</title>
<link>https://arxiv.org/abs/2508.14160</link>
<guid>https://arxiv.org/abs/2508.14160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RynnEC是用于具身认知的视频多模态大模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了RynnEC，一个专为具身认知设计的视频多模态大语言模型。该模型基于通用视觉-语言基础模型，结合区域编码器和掩码解码器，实现了灵活的区域级视频交互。尽管结构紧凑，RynnEC在物体属性理解、分割和空间推理方面表现优异。研究提出了一种基于第一视角视频的数据生成管道，以解决3D标注数据不足的问题，并引入了RynnEC-Bench评估基准。作者期望RynnEC能推动具身智能体通用认知核心的发展，并提升在多样化任务中的泛化能力。代码、模型和基准已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 14:00:01 GMT</pubDate>
</item>
<item>
<title>基于多模态对比学习与同构关系的推荐系统框架REARM</title>
<link>https://arxiv.org/abs/2508.13745</link>
<guid>https://arxiv.org/abs/2508.13745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REARM提升多模态推荐系统的性能与数据利用效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的推荐系统框架REARM，旨在解决多模态推荐中因数据稀疏而导致的特征表示不准确和用户-物品交互挖掘不足的问题。该框架通过引入元网络和正交约束策略优化多模态对比学习，有效过滤噪声并保留关键信息。同时，结合用户兴趣图与物品共现图，增强同构关系建模。实验结果表明，REARM在多个真实数据集上优于现有方法，并通过可视化验证了其在区分共享与独特模态特征方面的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 07:35:48 GMT</pubDate>
</item>
<item>
<title>越南语多模态教育评估中视觉语言模型的表现研究</title>
<link>https://arxiv.org/abs/2508.13680</link>
<guid>https://arxiv.org/abs/2508.13680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在越南语多模态教育任务中表现有限，仅部分模型接近人类水平。</p><br /><br /><p><strong>摘要：</strong> 本研究首次评估了视觉语言模型（VLMs）在越南语多模态教育考试中的表现，通过构建ViExam基准测试集，包含2,548道多模态题目。结果显示，最先进的VLM模型平均准确率为57.74%，而开源模型仅为27.70%，均低于人类平均水平（66.54%）。仅有o3模型超过人类平均表现（74.07%），但仍远低于人类最佳成绩（99.60%）。使用英文指令进行跨语言提示未能提升性能，反而使SOTA模型准确率下降1个百分点。通过人机协作可部分提升模型表现。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 05:31:18 GMT</pubDate>
</item>
<item>
<title>FinCDM：面向金融大语言模型的认知诊断评估框架</title>
<link>https://arxiv.org/abs/2508.13491</link>
<guid>https://arxiv.org/abs/2508.13491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinCDM揭示金融大模型知识盲区，提升模型可信度。</p><br /><br /><p><strong>摘要：</strong> 本文提出FinCDM，首个针对金融大语言模型的认知诊断评估框架，通过技能标签任务识别模型的知识与技能短板，而非仅依赖单一评分。构建了CPA-QKA数据集，涵盖真实会计与金融技能，由专家严格标注。实验显示FinCDM能发现传统基准未覆盖的税务和监管推理等薄弱环节，并揭示模型行为模式，为更可靠、针对性的模型开发提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 23:52:15 GMT</pubDate>
</item>
<item>
<title>人工智能在科学发现中的自主化：Agentic Science的演进与展望</title>
<link>https://arxiv.org/abs/2508.14111</link>
<guid>https://arxiv.org/abs/2508.14111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI正从工具变为科学发现的自主伙伴，推动Agentic Science发展。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了人工智能如何从辅助工具演变为具备科学自主性的研究伙伴，提出了Agentic Science作为AI驱动科学发现的新范式。通过大型语言模型、多模态系统和集成平台的支持，AI展现出生成假设、设计实验、执行分析和迭代优化的能力。文章统一了过程导向、自主导向和机制导向三个视角，构建了一个涵盖基础能力、核心流程和领域应用的综合框架。作者回顾了AI在生命科学、化学、材料科学和物理学等领域的应用，并分析了当前挑战与未来发展方向，为AI赋能的科学研究提供了结构化的理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 01:25:54 GMT</pubDate>
</item>
<item>
<title>面向未来预测的动态评估基准FutureX及其对LLM代理的性能分析</title>
<link>https://arxiv.org/abs/2508.11987</link>
<guid>https://arxiv.org/abs/2508.11987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FutureX是首个动态实时评估未来预测能力的基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了FutureX，这是一个专为评估大型语言模型（LLM）在复杂未来预测任务中的表现而设计的动态、实时评估基准。当前缺乏大规模的评估标准，主要是因为处理实时更新和获取准确答案存在挑战。FutureX通过自动化流程实现每日更新并避免数据污染，支持多种模型评估，包括具备推理、搜索和外部工具整合能力的模型。研究分析了模型在面对虚假网页和时间有效性问题时的失败模式，并提出建立一个无污染、动态的评估标准，以推动LLM代理达到专业分析师的水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 16 Aug 2025 04:54:08 GMT</pubDate>
</item>
<item>
<title>CHORD：融合SFT与RL的可控强化学习框架</title>
<link>https://arxiv.org/abs/2508.11408</link>
<guid>https://arxiv.org/abs/2508.11408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CHORD通过动态权重统一SFT与RL，提升模型训练稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出CHORD框架，旨在通过动态加权机制将监督微调（SFT）与强化学习（RL）相结合，以解决现有方法在整合过程中可能导致的模式破坏和过拟合问题。CHORD将SFT视为RL过程中的辅助目标，并引入双控制机制：全局系数用于引导从模仿学习到探索学习的过渡，而基于token的权重函数则实现对专家数据的细粒度学习，从而减少干扰并保持探索能力。实验表明，CHORD在多个基准测试中表现出更稳定高效的训练效果，优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 07:20:03 GMT</pubDate>
</item>
<item>
<title>多语言常识推理基准mSCoRe的构建与分析</title>
<link>https://arxiv.org/abs/2508.10137</link>
<guid>https://arxiv.org/abs/2508.10137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出mSCoRe基准评估多语言常识推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了mSCoRe，一个用于评估多语言常识推理能力的基准。该基准包含三个核心组件：细粒度的推理技能分类、针对常识推理的数据生成流程以及可扩展的任务难度框架。实验表明，当前最先进的大语言模型在处理高复杂度任务时仍面临挑战，尤其在涉及文化背景和多语言常识的场景中表现有限。研究进一步分析了模型的推理过程，并提出了未来改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 14:59:02 GMT</pubDate>
</item>
<item>
<title>轻量级语言模型中的推理与检索增强生成方法</title>
<link>https://arxiv.org/abs/2508.11386</link>
<guid>https://arxiv.org/abs/2508.11386</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级RAG系统，提升领域查询准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种将推理与检索增强生成（RAG）结合的新型方法，采用轻量级语言模型架构，适用于资源受限或安全环境。系统整合了密集检索器和微调的Qwen2.5-Instruct模型，利用合成查询和前沿模型的推理轨迹，在NHS A-to-Z条件页面语料库中进行训练。研究探讨了摘要压缩、合成数据设计和推理感知微调对模型性能的影响，并在非推理模型和通用轻量模型上进行了评估，结果表明该方法在保持本地部署可行性的同时，显著提升了答案准确性和一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11386" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:38:15 GMT</pubDate>
</item>
<item>
<title>基于少样本学习的合成语音检测方法研究</title>
<link>https://arxiv.org/abs/2508.13320</link>
<guid>https://arxiv.org/abs/2508.13320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">少样本学习提升合成语音检测在分布偏移下的性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在分布偏移条件下（如未见过的合成方法、说话人、语言或音频环境）检测合成语音的挑战。提出了一种自注意力原型网络，以增强少样本适应能力。通过对比传统零样本检测器与所提方法，在控制训练条件的情况下引入分布偏移进行评估。结果表明，在零样本检测性能受限时，该方法仅需10个内分布样本即可快速适应，显著提升了检测效果，例如在日语Deepfake数据集上相对EER降低32%，在ASVspoof 2021 Deepfake数据集上降低20%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 15:14:45 GMT</pubDate>
</item>
<item>
<title>提出PASR方法提升大语言模型自我优化能力</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PASR实现大模型在生成过程中主动优化输出，提升效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为ProActive Self-Refinement (PASR) 的新方法，使大语言模型能够在生成过程中主动决定何时、如何进行优化，而非依赖固定的迭代次数。与传统方法不同，PASR根据模型内部状态和上下文动态调整优化策略，从而提高问题解决性能。实验表明，在Qwen3-8B模型上，PASR相比标准生成方式平均减少41.6%的token消耗，并提升8.2%的准确率。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 09:07:21 GMT</pubDate>
</item>
<item>
<title>CAMAR：多智能体路径规划的新型MARL基准</title>
<link>https://arxiv.org/abs/2508.12845</link>
<guid>https://arxiv.org/abs/2508.12845</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAMAR为多智能体路径规划提供高效且真实的MARL测试平台。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CAMAR，一个专为多智能体路径规划设计的新型多智能体强化学习（MARL）基准。CAMAR结合了连续状态和动作空间，并支持合作与竞争交互，能够以高达每秒10万步的速度运行。文章还提出了一种三层评估协议，用于更精确地跟踪算法进展并深入分析性能。此外，CAMAR支持将传统规划方法如RRT和RRT*集成到MARL流程中，提升算法效果。研究提供了多种测试场景和基准工具，确保实验的可重复性和公平比较，表明CAMAR是一个具有挑战性和现实意义的MARL测试环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12845" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 07:32:26 GMT</pubDate>
</item>
<item>
<title>基于原子思维的增强型检索生成框架Atom-Searcher</title>
<link>https://arxiv.org/abs/2508.12800</link>
<guid>https://arxiv.org/abs/2508.12800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Atom-Searcher提升多步骤推理与搜索效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Atom-Searcher的新框架，结合了原子思维（Atomic Thought）和推理奖励模型（RRM），以改进大型语言模型在复杂任务中的表现。该框架通过细粒度的推理单元和奖励机制，解决了传统方法在多步骤推理和策略搜索中的局限性。实验表明，Atom-Searcher在七个基准测试中均优于现有方法，具有更高的可扩展性、可解释性和人类相似的推理模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:23:10 GMT</pubDate>
</item>
<item>
<title>FineCE：一种用于大语言模型的细粒度置信度估计方法</title>
<link>https://arxiv.org/abs/2508.12040</link>
<guid>https://arxiv.org/abs/2508.12040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FineCE提升大语言模型生成文本的置信度估计准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FineCE的新型置信度估计方法，旨在解决大语言模型在生成文本时缺乏细粒度置信度评估的问题。该方法通过构建全面的训练数据管道，训练模型对任意文本序列进行置信度预测，并引入了Backward Confidence Integration（BCI）策略，利用后续文本信息提升当前序列的置信度估计。此外，还提出了三种确定最佳置信度估计位置的策略。实验结果表明，FineCE在多个基准数据集上均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 16 Aug 2025 09:29:35 GMT</pubDate>
</item>
<item>
<title>MM-BrowseComp：评估AI代理多模态检索与推理能力的新基准</title>
<link>https://arxiv.org/abs/2508.13186</link>
<guid>https://arxiv.org/abs/2508.13186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准MM-BrowseComp测试AI代理多模态搜索与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MM-BrowseComp，一个专门用于评估AI代理在多模态内容下进行深度搜索和推理能力的新基准。该基准包含224个精心设计的问题，其中许多问题涉及图像或视频等非文本信息，挑战现有仅依赖文本的模型。研究还提供了每个问题的验证清单，以分析多模态依赖关系和推理路径。实验表明，即使最先进的模型如OpenAI o3在该基准上的准确率也仅为29.02%，反映出当前AI模型在多模态理解和推理方面的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 09:46:47 GMT</pubDate>
</item>
<item>
<title>基于指向表示的具身AI模型提升泛化能力</title>
<link>https://arxiv.org/abs/2508.13998</link>
<guid>https://arxiv.org/abs/2508.13998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Embodied-R1通过指向表示实现具身AI的高效泛化。</p><br /><br /><p><strong>摘要：</strong> 文章指出，具身AI的泛化能力受限于‘看到-做到’的差距，主要由于数据稀缺和具身异质性。为此，研究提出‘指向’作为统一的、与具身无关的中间表示，并定义了四种核心具身指向能力，以连接高层视觉语言理解和底层动作原语。研究构建了Embodied-Points-200K大规模数据集，并训练了Embodied-R1模型，采用两阶段强化微调（RFT）方法。该模型在11个具身空间和指向基准测试中表现优异，尤其在零样本泛化任务中取得显著提升，如SIMPLEREnv成功率达56.2%，XArm任务平均达87.5%。此外，模型对视觉干扰具有高鲁棒性，展示了指向表示与RFT训练范式在机器人感知-行动差距中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 12:50:01 GMT</pubDate>
</item>
<item>
<title>大型语言模型在道德理解上的表现分析</title>
<link>https://arxiv.org/abs/2508.13804</link>
<guid>https://arxiv.org/abs/2508.13804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较AI与人类在道德判断上的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究首次对主流大型语言模型进行了大规模贝叶斯评估，通过模拟标注者之间的分歧，区分了人类固有的不确定性与模型的领域敏感性。研究涵盖了超过10万条文本和700名标注者，使用GPU优化的贝叶斯框架处理了百万级模型查询。结果显示，AI模型通常排名在人类标注者的前25%，表现出更高的平衡准确率，并且产生更少的假阴性结果，显示出更强的道德识别能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 09:05:48 GMT</pubDate>
</item>
<item>
<title>跨骨骼拓扑动画迁移方法研究</title>
<link>https://arxiv.org/abs/2508.13139</link>
<guid>https://arxiv.org/abs/2508.13139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Motion2Motion实现跨骨骼拓扑的高效动画迁移。</p><br /><br /><p><strong>摘要：</strong> 本文针对不同骨骼拓扑结构之间的动画迁移问题，提出了一种无需训练的框架Motion2Motion。该方法仅需目标骨骼的一个或几个示例动作和稀疏的骨骼对应关系，即可实现高效的动画迁移。研究通过定性和定量评估验证了该方法在相似骨骼和跨物种骨骼迁移中的有效性，并展示了其在实际应用中的潜力。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:50:31 GMT</pubDate>
</item>
<item>
<title>基于相关性的稀疏自编码器自动调优方法CorrSteer</title>
<link>https://arxiv.org/abs/2508.12535</link>
<guid>https://arxiv.org/abs/2508.12535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CorrSteer通过相关性选择特征提升语言模型任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出CorrSteer方法，利用推理时生成的token激活与样本正确性的相关性来选择稀疏自编码器（SAE）的特征，从而避免冗余关联并自动化调优过程。该方法无需对比数据集或大量激活存储，在QA、偏见缓解、越狱防御和推理基准测试中表现出色，尤其在MMLU和HarmBench上分别提升了4.1%和22.9%。所选特征具有语义意义，展示了驱动性能的关键能力，验证了基于相关性的SAE调优方法的有效性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 20:01:42 GMT</pubDate>
</item>
<item>
<title>大语言模型版权保护技术综述</title>
<link>https://arxiv.org/abs/2508.11548</link>
<guid>https://arxiv.org/abs/2508.11548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大语言模型版权保护技术，重点分析模型指纹技术。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了大语言模型（LLM）的版权保护技术，特别关注模型指纹技术。文章首先澄清了文本水印、模型水印与模型指纹之间的概念联系，并统一术语将模型水印纳入更广泛的指纹框架。接着对多种文本水印技术进行了概述和比较，指出部分方法也可作为模型指纹使用。随后系统分类并比较了现有的模型指纹方法，首次介绍了指纹迁移与移除技术，并总结了评估模型指纹的指标，包括有效性、无害性、鲁棒性、隐蔽性和可靠性。最后讨论了当前面临的挑战与未来研究方向，旨在为研究人员提供对LLM时代版权保护技术的深入理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 11:50:20 GMT</pubDate>
</item>
<item>
<title>MedSAMix：一种无需训练的医学图像分割模型融合方法</title>
<link>https://arxiv.org/abs/2508.11032</link>
<guid>https://arxiv.org/abs/2508.11032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedSAMix提升医学图像分割性能，增强模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MedSAMix，一种无需训练的医学图像分割模型融合方法，结合通用模型（如SAM）和专业模型（如MedSAM）的优势。该方法通过零阶优化自动发现最优层融合方案，并提供两种优化策略以满足不同临床场景下的领域特异性与泛化性需求。在25个医学分割任务上的实验表明，MedSAMix有效减少模型偏差，提升专业任务准确率和多任务泛化能力，分别提高6.67%和4.37%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 15:35:57 GMT</pubDate>
</item>
<item>
<title>深度学习在语音分离中的系统综述</title>
<link>https://arxiv.org/abs/2508.10830</link>
<guid>https://arxiv.org/abs/2508.10830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述深度学习在语音分离中的应用与进展。</p><br /><br /><p><strong>摘要：</strong> 本文对基于深度神经网络的语音分离技术进行了系统性综述，旨在填补当前研究中对不同架构和方法孤立分析的不足。文章从学习范式、分离场景、监督/自监督/无监督框架以及模型结构等方面进行全面探讨，并结合最新研究成果进行分析。同时，作者评估了不同方法在标准数据集上的表现，揭示其优缺点，并指出未来发展方向，如领域鲁棒性、高效架构、多模态融合等。该综述为研究人员提供了全面的参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 12:54:34 GMT</pubDate>
</item>
<item>
<title>统一生成模型中语义ID的构建与性能研究</title>
<link>https://arxiv.org/abs/2508.10478</link>
<guid>https://arxiv.org/abs/2508.10478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何构建适用于搜索和推荐的统一语义ID。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在统一生成模型中构建适用于搜索和推荐任务的语义ID方法。传统上，物品通过唯一标识符或从嵌入中获得的离散代码表示。文章比较了多种构建语义ID的策略，包括任务特定和跨任务方法，并分析了是否应在联合模型中为每个任务使用独立的语义ID。实验表明，通过对搜索和推荐任务进行微调的双编码器模型获取物品嵌入，并构建统一的语义ID空间，能够在两个任务中实现良好的性能平衡。研究希望推动更通用、语义基础的ID方案的发展，并为下一代统一生成推荐架构提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:28:49 GMT</pubDate>
</item>
<item>
<title>利用多模态大语言模型提升视频推荐系统的语义理解能力</title>
<link>https://arxiv.org/abs/2508.09789</link>
<guid>https://arxiv.org/abs/2508.09789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多模态大语言模型增强视频推荐的语义理解。</p><br /><br /><p><strong>摘要：</strong> 现有视频推荐系统主要依赖用户定义的元数据或低级视觉和音频信号，但这些特征无法捕捉如意图、幽默和世界知识等深层次语义。本文提出一种无需微调的框架，通过提示预训练多模态大语言模型（MLLM）生成丰富的自然语言描述，从而将高阶语义注入推荐流程。该方法结合了先进的文本编码器，并在标准协同过滤、基于内容和生成式推荐器中验证，结果表明在MicroLens-100K数据集上优于传统视频、音频和元数据特征。研究展示了MLLM作为实时知识提取工具在构建更注重用户意图的视频推荐系统中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 09:19:31 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的播客推荐评估框架</title>
<link>https://arxiv.org/abs/2508.08777</link>
<guid>https://arxiv.org/abs/2508.08777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用LLM进行播客推荐评估，提升推荐质量与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大语言模型（LLM）的播客推荐评估框架，旨在解决传统离线指标和在线测试方法在长音频领域中的局限性。该框架通过分析用户90天的收听历史构建自然语言用户画像，以高语义层次的上下文引导LLM进行更精准的推荐评估。实验表明，该方法在47名参与者中表现出与人工判断高度一致的效果，并优于使用原始数据的变体方法。该框架为推荐系统提供了高效、可解释的评估方式，适用于迭代测试与模型选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 05:23:35 GMT</pubDate>
</item>
<item>
<title>辐射场在扩展现实中的研究现状与挑战</title>
<link>https://arxiv.org/abs/2508.04326</link>
<guid>https://arxiv.org/abs/2508.04326</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">辐射场技术在XR中应用有限，本文系统分析其现状与研究缺口。</p><br /><br /><p><strong>摘要：</strong> 本文对辐射场（RF）技术在扩展现实（XR）中的应用进行了系统性综述。尽管3D Gaussian Splatting和NeRF等技术推动了高质量视图合成的发展，但RF在XR领域的贡献仍较少。作者从计算机视觉、图形学、机器人学等多个领域收集了365篇相关论文，并深入分析其中66篇探讨了RF在XR中的具体应用。研究揭示了RF在XR中的当前应用场景、实现方式及存在的研究空白，为XR社区提供了有价值的参考，助力其应对辐射场技术快速发展的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04326" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 07:14:06 GMT</pubDate>
</item>
<item>
<title>基于时间结构的流匹配模型强化学习优化方法</title>
<link>https://arxiv.org/abs/2508.04324</link>
<guid>https://arxiv.org/abs/2508.04324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TempFlow-GRPO提升文本到图像生成的人类偏好对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出TempFlow-GRPO，一种针对流模型的强化学习优化框架，解决了传统方法在时间步长上奖励分配不均的问题。该方法引入轨迹分支机制和噪声感知权重策略，提升了模型在不同生成阶段的优化效率与稳定性，显著提高了文本到图像生成任务中的人类偏好对齐效果和基准测试表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 07:10:39 GMT</pubDate>
</item>
<item>
<title>ZARA：基于代理的零样本可解释运动时间序列识别框架</title>
<link>https://arxiv.org/abs/2508.04038</link>
<guid>https://arxiv.org/abs/2508.04038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZARA实现无需微调的零样本运动识别，准确率高且可解释。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZARA，一个基于代理的框架，用于直接从原始运动时间序列中进行零样本、可解释的人类活动识别（HAR）。ZARA结合了自动构建的成对特征知识库、多传感器检索模块和分层代理管道，使大型语言模型能够迭代选择特征、引用相关证据并生成活动预测及自然语言解释。该方法无需微调或任务特定分类器，在8个HAR基准测试中表现出色，达到最先进的零样本性能，比最强基线高出2.53倍的宏F1分数。消融实验验证了各模块的必要性，展示了ZARA在可信赖、即插即用运动时间序列分析中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:57:57 GMT</pubDate>
</item>
<item>
<title>LongSplat：解决长视频新视角合成的3D高斯点云框架</title>
<link>https://arxiv.org/abs/2508.14041</link>
<guid>https://arxiv.org/abs/2508.14041</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongSplat提升长视频新视角合成质量与效率。</p><br /><br /><p><strong>摘要：</strong> LongSplat是一种针对非结构化长视频的新视角合成框架，解决了相机位姿漂移、几何初始化不准确和内存限制等问题。其核心创新包括：联合优化相机位姿与3D高斯分布以避免局部最优，基于学习的3D先验进行鲁棒位姿估计，以及通过八叉树锚点机制高效处理密集点云。实验表明，LongSplat在渲染质量、位姿精度和计算效率方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.14041" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>MMAU-Pro：评估AI音频智能的全面基准</title>
<link>https://arxiv.org/abs/2508.13992</link>
<guid>https://arxiv.org/abs/2508.13992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMAU-Pro是评估AI音频理解能力的全面基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MMAU-Pro，这是一个全面且严格构建的音频智能评估基准，包含5305个实例，涵盖语音、声音、音乐及其组合。该基准评估49项独特技能，涉及长音频理解、空间音频推理和多音频理解等复杂维度。所有问题要求多步骤推理，并采用选择题和开放回答形式。音频数据来源于真实环境，而非已有数据集。评估显示，即使最先进的模型如Gemini 2.5 Flash和Audio Flamingo 3也仅达到约60%和52%的准确率，表明当前AI在音频智能方面仍存在明显不足。研究提供了改进方向，助力AI向音频通用智能发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 12:33:49 GMT</pubDate>
</item>
<item>
<title>POML：一种用于复杂提示管理的标记语言</title>
<link>https://arxiv.org/abs/2508.13948</link>
<guid>https://arxiv.org/abs/2508.13948</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">POML提升大型语言模型提示的结构化与可维护性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为POML（Prompt Orchestration Markup Language）的新型标记语言，旨在解决大型语言模型在提示设计中面临的结构不清晰、数据整合困难、格式敏感等问题。POML通过组件化标记、专用标签和类似CSS的样式系统，实现内容与展示的分离，并提供模板化功能和开发工具包，提升提示的灵活性和协作效率。研究通过案例分析和用户测试验证了POML在复杂应用集成和准确性方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13948" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 11:37:29 GMT</pubDate>
</item>
<item>
<title>OmniTry：一种扩展至多种可穿戴物品的虚拟试穿框架</title>
<link>https://arxiv.org/abs/2508.13632</link>
<guid>https://arxiv.org/abs/2508.13632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniTry扩展了虚拟试穿任务，支持多种可穿戴物品。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniTry，一个统一的虚拟试穿框架，不仅限于衣物，还扩展到珠宝和配饰等可穿戴物品。该框架在无需掩码的情况下进行物体定位和外观一致性迁移。通过两个阶段的训练流程，首先利用大量未配对图像进行预训练，然后使用少量配对图像进行微调。实验表明，OmniTry在对象定位和ID保留方面优于现有方法，并在包含12类可穿戴物品的基准数据集上进行了评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 04:47:31 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的人类痛苦感知预测研究</title>
<link>https://arxiv.org/abs/2508.12669</link>
<guid>https://arxiv.org/abs/2508.12669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用LLM预测人类对情景的痛苦评分，提升情感推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了使用大语言模型（LLMs）从自然语言描述中预测人类感知的痛苦分数。任务被建模为回归问题，模型将每个输入语句分配一个0到100之间的数值。研究评估了多种提示策略，包括零样本、固定上下文少样本和基于BERT句子嵌入的检索提示。结果表明，少样本方法在预测准确性上优于零样本基线，突显了上下文示例在情感预测中的价值。为进一步测试模型表现，研究引入了“痛苦游戏秀”这一新颖的互动框架，模拟电视节目形式，通过结构化回合测试模型的顺序比较、二分类、标量估计和反馈驱动推理能力。该方法不仅评估预测精度，还考察模型根据纠正反馈进行适应的能力。实验展示了LLMs在动态情感推理任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 03:02:59 GMT</pubDate>
</item>
<item>
<title>无需训练的图像与视频颜色编辑方法ColorCtrl</title>
<link>https://arxiv.org/abs/2508.09131</link>
<guid>https://arxiv.org/abs/2508.09131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ColorCtrl实现精准且一致的颜色编辑，提升图像和视频质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出ColorCtrl，一种无需训练的图像和视频颜色编辑方法，利用多模态扩散Transformer的注意力机制，通过分离结构与颜色实现精确控制。该方法在SD3和FLUX.1-dev等模型上表现出色，优于现有方法，并在一致性方面超越商业模型。扩展至CogVideoX等视频模型时，展现出更强的时间连贯性和编辑稳定性，同时适用于指令式编辑模型，展示出广泛适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>多视觉参考的可控图像生成研究</title>
<link>https://arxiv.org/abs/2508.06905</link>
<guid>https://arxiv.org/abs/2508.06905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多视觉参考下的图像生成挑战与数据集构建。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于利用多个视觉参考进行可控图像生成的任务，提出并评估了MultiRef-bench框架，包含990个合成样本和1000个真实世界样本。通过RefBlend数据引擎生成的33种参考组合，构建了包含38k张高质量图像的MultiRef数据集。实验结果显示，即使最先进的模型在多参考条件下仍表现有限，表明需要更灵活的创意工具来整合多种视觉灵感。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 05:36:21 GMT</pubDate>
</item>
<item>
<title>Chain-of-Agents：一种新型的端到端多智能体推理范式</title>
<link>https://arxiv.org/abs/2508.13167</link>
<guid>https://arxiv.org/abs/2508.13167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Chain-of-Agents实现端到端多智能体协作，提升复杂问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Chain-of-Agents（CoA）的新范式，使大型语言模型能够以多智能体系统的方式进行端到端的复杂问题解决。该方法通过动态激活不同工具代理和角色扮演代理，模拟多智能体协作。研究引入了多智能体蒸馏框架，将先进多智能体系统转化为链式代理轨迹，用于代理监督微调。随后通过可验证代理任务的代理强化学习进一步提升模型能力，形成了Agent Foundation Models（AFMs）。实验表明，AFM在多个基准测试中均取得最新性能，研究全部开源，为未来代理模型和代理强化学习提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:01:02 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习在大型语言模型中的应用</title>
<link>https://arxiv.org/abs/2508.12790</link>
<guid>https://arxiv.org/abs/2508.12790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVR扩展至开放任务，提升语言模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了基于可验证奖励的强化学习（RLVR）在大型语言模型中的应用。传统RLVR依赖于可自动验证的奖励信号，如代码测试或数学答案匹配，限制了其适用范围。为解决这一问题，研究引入了基于评分标准的奖励机制，通过设计的评分标准对主观输出进行自动评分。研究构建了目前最大的评分系统，包含10,000多个由人类、大模型或人机协作生成的评分标准。实验表明，该方法在开放任务中表现出色，仅需5000个样本即可在人文类任务中提升5.2%，超越671B参数的DeepSeek-V3模型。同时，该方法还能实现更自然的语言风格控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:06:08 GMT</pubDate>
</item>
<item>
<title>面向机器遗忘方法的可视化评估系统研究</title>
<link>https://arxiv.org/abs/2508.12730</link>
<guid>https://arxiv.org/abs/2508.12730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Unlearning Comparator系统，用于评估机器遗忘方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器遗忘（MU）领域中方法评估困难的问题，提出了一种可视化分析系统Unlearning Comparator。该系统支持模型比较和隐私攻击模拟两大任务，帮助研究人员在不同层次上分析模型行为，并通过模拟成员推断攻击评估方法的隐私保护效果。实验表明，该系统有助于深入理解模型变化并优化MU方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 04:53:53 GMT</pubDate>
</item>
<item>
<title>G-CUT3R：一种融合先验信息的3D场景重建方法</title>
<link>https://arxiv.org/abs/2508.11379</link>
<guid>https://arxiv.org/abs/2508.11379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">G-CUT3R通过引入先验信息提升3D场景重建性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出G-CUT3R，一种改进的前馈式3D场景重建方法。该方法在CUT3R基础上引入深度、相机标定和位置等先验信息，通过为每种模态设计专用编码器并利用零卷积融合RGB图像特征，实现灵活的多模态输入整合。实验表明，该方法在多个基准测试中均表现出显著性能提升，具备良好的兼容性和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:25:58 GMT</pubDate>
</item>
<item>
<title>多模态模型在空间智能方面的进展与挑战</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态模型在空间智能上取得进步，但仍不及人类。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了多模态模型在空间理解与推理方面的最新进展与不足。尽管近年来取得了显著成果，但这些模型在空间智能方面仍存在明显局限。通过对GPT-5等前沿模型的评估，研究发现其在空间任务中表现出色，但仍未达到人类水平。此外，研究还揭示了多模态模型在面对复杂空间问题时的挑战，并指出专有模型在极端任务中并不具备明显优势。文章还通过定性分析展示了人类直观的任务对当前模型的困难。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:55:17 GMT</pubDate>
</item>
<item>
<title>视觉动作提示：跨领域复杂交互视频生成的新方法</title>
<link>https://arxiv.org/abs/2508.13104</link>
<guid>https://arxiv.org/abs/2508.13104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉动作提示以平衡动作精度与跨域适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种统一的动作表示——视觉动作提示，用于复杂高自由度交互的视频生成，同时保持跨领域的可迁移视觉动态。现有方法在动作驱动的视频生成中面临精度与泛化性的权衡问题，而该研究通过将动作“渲染”为通用的视觉提示，实现了几何精度与跨域适应性的结合。研究利用视觉骨架作为通用表示，并从人-物交互和灵巧机器人操作数据中构建骨架，从而实现跨域训练。通过轻量级微调，将视觉骨架集成到预训练视频生成模型中，实现对复杂交互的精准控制并保留跨域动态学习能力。实验表明该方法在EgoVid、RT-1和DROID数据集上均有效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:12:28 GMT</pubDate>
</item>
<item>
<title>HeroBench：评估大语言模型长程规划能力的新基准</title>
<link>https://arxiv.org/abs/2508.12782</link>
<guid>https://arxiv.org/abs/2508.12782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeroBench测试LLM在复杂虚拟环境中的长期规划与结构化推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HeroBench，一个专门用于评估大语言模型（LLMs）在复杂RPG风格虚拟世界中长程规划和结构化推理能力的新基准。该基准包含广泛难度的任务、模拟执行环境以及性能分析工具，旨在挑战模型制定战略计划、收集资源、掌握技能、制作装备并击败对手的能力。对25个先进LLM的评估揭示了其在生成稳健高层计划和可靠执行结构化动作方面的显著差异，表明传统推理基准难以反映真实场景中的复杂性。HeroBench不仅推动了LLM推理能力的评估，也为未来自主规划研究提供了灵活且可扩展的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 05:59:02 GMT</pubDate>
</item>
<item>
<title>评估提升大语言模型提示鲁棒性的方法</title>
<link>https://arxiv.org/abs/2508.11383</link>
<guid>https://arxiv.org/abs/2508.11383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比5种提升LLM提示鲁棒性的方法，涵盖多个模型与任务。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了5种提升大语言模型（LLM）提示鲁棒性的方法，在统一实验框架下对Llama、Qwen和Gemma系列的8个模型进行了测试，覆盖Natural Instructions数据集中的52项任务。研究包括微调和上下文学习两种范式下的鲁棒性方法，并测试其在多种分布偏移下的泛化能力。此外，还扩展分析了GPT-4.1和DeepSeek V3，评估前沿模型对格式扰动的鲁棒性。研究结果为实际应用中选择有效的鲁棒性方法提供了实用参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11383" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 06:32:50 GMT</pubDate>
</item>
<item>
<title>大型推理模型在主动信息获取能力上的不足与挑战</title>
<link>https://arxiv.org/abs/2508.11252</link>
<guid>https://arxiv.org/abs/2508.11252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型模型在主动提问方面存在缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文指出，尽管大型推理模型在数学问题解决上表现出色，但它们在面对信息不完整的任务时缺乏主动询问的能力，这限制了其作为真正智能代理的潜力。研究团队构建了一个包含多种情境的不完整问题数据集，并基于此对模型进行了系统评估，发现模型在主动获取信息方面表现不佳，还存在过度思考和幻觉现象。文章进一步探讨了监督微调在提升这一能力上的潜力与挑战，旨在推动更具真实智能的模型发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 02:42:00 GMT</pubDate>
</item>
<item>
<title>4DNeX：基于单张图像的高效4D场景生成框架</title>
<link>https://arxiv.org/abs/2508.13154</link>
<guid>https://arxiv.org/abs/2508.13154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4DNeX通过微调视频扩散模型实现单图到4D场景的高效生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出4DNeX，首个从单张图像生成动态3D场景的前馈框架。与依赖优化或多帧视频输入的方法不同，4DNeX通过微调预训练视频扩散模型实现端到端的图像到4D生成。研究构建了大规模高质量的4D数据集4DNeX-10M，并引入统一的6D视频表示，结合RGB和XYZ序列进行外观与几何建模。同时提出简单有效的适应策略，使预训练模型适用于4D建模。实验表明，4DNeX在效率和泛化性上优于现有方法，为动态场景模拟提供了可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Matrix-Game 2.0：基于扩散模型的实时交互视频生成框架</title>
<link>https://arxiv.org/abs/2508.13009</link>
<guid>https://arxiv.org/abs/2508.13009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Matrix-Game 2.0，实现高速实时交互视频生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Matrix-Game 2.0的交互式世界模型，能够通过少量步骤的自回归扩散生成高质量的长视频。该框架包含三个关键组件：大规模视频数据生成管道、动作注入模块以及基于因果结构的少步蒸馏方法。其在Unreal Engine和GTA5环境中生成超过1200小时的多样化交互视频数据，并支持实时鼠标键盘输入。该模型可在25 FPS的速度下生成分钟级高质量视频，显著提升了交互视频生成的实时性能。研究已开源模型权重与代码库，以推动交互世界建模领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.13009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 11:28:53 GMT</pubDate>
</item>
<item>
<title>基于大规模视频生成模型的视频重新照明方法</title>
<link>https://arxiv.org/abs/2508.12945</link>
<guid>https://arxiv.org/abs/2508.12945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumen框架实现视频背景替换与光照一致性调整。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lumen，一个基于大规模视频生成模型的端到端视频重新照明框架，通过灵活的文本指令控制光照和背景。为解决高质量配对视频数据不足的问题，构建了包含真实与合成视频的大规模数据集。合成数据利用先进3D渲染引擎生成，真实数据通过HDR光照模拟补充。设计联合训练流程以发挥各领域优势，并引入领域感知适配器分离光照与场景学习。实验表明，Lumen能有效生成具有统一光照和严格前景保留的电影级重照明视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 10:21:22 GMT</pubDate>
</item>
<item>
<title>S^2-Guidance：提升扩散模型生成质量的新方法</title>
<link>https://arxiv.org/abs/2508.12880</link>
<guid>https://arxiv.org/abs/2508.12880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S^2-Guidance通过随机块丢弃提升扩散模型生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对扩散模型中广泛使用的Classifier-free Guidance (CFG) 方法存在的不足，提出了一种新的优化方法S^2-Guidance。通过在前向过程中引入随机块丢弃，构建随机子网络，从而引导模型避免低质量预测，提升生成效果。实验表明，S^2-Guidance在文本到图像和文本到视频任务中均优于CFG及其他先进方法，具有显著的性能优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 08:31:20 GMT</pubDate>
</item>
<item>
<title>基于视觉粒度序列的图像生成方法研究</title>
<link>https://arxiv.org/abs/2508.12811</link>
<guid>https://arxiv.org/abs/2508.12811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像生成框架，提升生成质量与控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于视觉粒度序列的图像生成方法，将图像分解为具有相同空间分辨率但不同唯一标记数量的结构化序列，通过Next Visual Granularity (NVG) 框架逐步生成图像，从整体布局到细节逐步优化。该方法在ImageNet数据集上进行训练，并展现出良好的扩展性。实验结果表明，NVG在FID分数上优于VAR系列模型，证明了其在图像生成质量上的优势。研究还展示了该框架的潜力和应用前景，相关代码和模型将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 06:47:37 GMT</pubDate>
</item>
<item>
<title>逆向多模态学习方法Invers-LLaVA突破传统对齐预训练范式</title>
<link>https://arxiv.org/abs/2508.12466</link>
<guid>https://arxiv.org/abs/2508.12466</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出逆向多模态学习方法，无需对齐预训练即可实现视觉与语言融合。</p><br /><br /><p><strong>摘要：</strong> 本文提出Invers-LLaVA，一种颠覆传统多模态学习范式的新型方法。该方法摒弃了传统的视觉特征到文本空间的映射方式，转而将文本嵌入映射到连续视觉表示空间，并在Transformer中间层进行融合。通过注意力机制中的选择性加法组件，实现了视觉与文本表示的动态整合，无需依赖大规模图像-文本对齐数据集。实验表明，在推理密集型任务中表现显著提升，而在依赖记忆的感知任务中略有下降。结果证明，对齐预训练并非多模态学习的必要条件，尤其在复杂推理任务中效果更优。该方法降低了45%的计算需求，挑战了传统模态融合观念，为高效多模态架构提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.12466" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 17 Aug 2025 14:36:04 GMT</pubDate>
</item>
<item>
<title>基于生物听觉机制的语音表示学习模型AuriStream</title>
<link>https://arxiv.org/abs/2508.11598</link>
<guid>https://arxiv.org/abs/2508.11598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AuriStream通过双阶段框架模拟人类听觉处理，实现高效语音表示学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AuriStream，一种受人类听觉处理机制启发的语音编码模型。该模型采用双阶段框架：第一阶段将原始音频转换为基于人耳耳蜗的时间-频率表示，并提取离散的耳蜗标记；第二阶段在这些标记上应用自回归序列模型。AuriStream能够学习有意义的音素和词表示以及最先进的词汇语义，表现出在多种下游SUPERB语音任务中的竞争力。此外，AuriStream能够生成音频延续，并在频谱图空间中可视化，有助于理解模型预测。整体而言，该模型为开发更接近人类的语音处理系统提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:06:04 GMT</pubDate>
</item>
<item>
<title>Ovis2.5：提升多模态推理与视觉感知的新型模型</title>
<link>https://arxiv.org/abs/2508.11737</link>
<guid>https://arxiv.org/abs/2508.11737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis2.5提升视觉感知与多模态推理能力，实现高精度分析。</p><br /><br /><p><strong>摘要：</strong> Ovis2.5是Ovis2的升级版本，专注于原生分辨率视觉感知和强大的多模态推理。该模型采用原生分辨率视觉Transformer，避免固定分辨率拼接带来的细节损失，适用于复杂图表等视觉密集内容。通过引入反思机制（如自我检查与修正），增强推理能力，并提供可选的“思考模式”以提升准确性。训练过程采用五阶段课程，涵盖基础预训练、指令调优及对齐优化。为提高效率，使用多模态数据打包和混合并行技术，显著提升整体性能。释放了两个开源模型Ovis2.5-9B和Ovis2.5-2B，后者在资源受限场景中表现出色。在OpenCompass基准测试中，Ovis2.5-9B取得78.3的平均分，超越前代并达到开源多模态大语言模型的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:01:08 GMT</pubDate>
</item>
<item>
<title>基于动态记忆的长文本推理方法ComoRAG</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ComoRAG通过动态交互提升长文本推理效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为ComoRAG的新型检索增强生成方法，旨在解决长篇故事和小说中的叙事理解难题。传统RAG方法因单次检索过程无法捕捉长距离上下文中的复杂关系而效果有限。ComoRAG模拟人类认知过程，通过迭代推理与动态记忆空间交互，不断生成新查询并整合新信息，从而构建连贯的上下文支持问题解决。在四个大规模长文本基准测试中，ComoRAG相比最强基线提升了11%的性能，尤其在需要全局理解的复杂查询中表现突出。该方法提供了一种具有认知启发性的状态化推理框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 03:52:09 GMT</pubDate>
</item>
<item>
<title>高效大语言模型架构研究综述</title>
<link>https://arxiv.org/abs/2508.09834</link>
<guid>https://arxiv.org/abs/2508.09834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述高效大语言模型架构，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了针对传统Transformer架构局限性的创新大语言模型（LLM）架构，旨在提高计算效率和可扩展性。文章从语言建模出发，涵盖了线性与稀疏序列建模方法、高效的全注意力变体、稀疏专家混合模型、结合多种技术的混合架构，以及新兴的扩散LLM。同时探讨了这些技术在多模态中的应用及其对构建资源感知基础模型的深远影响。通过分类整理近期研究，本文为现代高效LLM架构提供了一个蓝图，希望推动更高效、多功能的人工智能系统发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 10:13:46 GMT</pubDate>
</item>
<item>
<title>BeyondWeb：提升预训练语言模型合成数据质量的新框架</title>
<link>https://arxiv.org/abs/2508.10975</link>
<guid>https://arxiv.org/abs/2508.10975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BeyondWeb合成数据框架显著提升预训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BeyondWeb，一个用于生成高质量合成数据的预训练框架。该框架在多个基准测试中表现优于现有最佳合成数据集，如Cosmopedia和Nemotron-Synth。实验表明，使用BeyondWeb训练的3B模型在180B token下超越了在Cosmopedia上训练的8B模型。研究还揭示了合成数据质量的关键影响因素，强调优化需综合考虑多种因素，而非单一方法。虽然简单方法可能带来有限提升，但精心设计的方法能实现显著改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10975" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:55:47 GMT</pubDate>
</item>
<item>
<title>基于奖励引导解码的多模态大语言模型适应方法</title>
<link>https://arxiv.org/abs/2508.11616</link>
<guid>https://arxiv.org/abs/2508.11616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种奖励引导解码方法提升MLLM视觉定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了通过控制解码来适应多模态大语言模型（MLLMs）的方法。作者引入了首个用于MLLM的奖励引导解码方法，并应用于改善其视觉定位能力。该方法构建了两个独立的奖励模型，分别控制输出中的对象精确度和召回率。该方法实现了对MLLM推理过程的实时可控性，允许用户在图像描述任务中动态调整精确度与召回率的平衡，以及控制解码搜索的广度，从而在计算资源与视觉定位精度之间取得平衡。实验表明，该方法在标准物体幻觉基准测试中表现出色，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:29:06 GMT</pubDate>
</item>
<item>
<title>基于多维人类偏好优化的音频驱动肖像动画方法</title>
<link>https://arxiv.org/abs/2508.11255</link>
<guid>https://arxiv.org/abs/2508.11255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Talking-Critic与TLPO提升音频驱动肖像动画的多维一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对音频驱动肖像动画中存在的多维偏好对齐难题，提出Talking-Critic多模态奖励模型和TLPO框架。Talking-Critic通过学习人类偏好函数量化生成视频的质量，而TLPO通过分解偏好为专家模块并跨时间步与网络层融合，实现无干扰的多维优化。实验表明，该方法在唇形同步、运动自然性和视觉质量等方面均优于现有方法，并构建了包含41万条偏好对的Talking-NSQ数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 02:43:46 GMT</pubDate>
</item>
<item>
<title>MAESTRO：面向多模态遥感数据的自监督学习方法</title>
<link>https://arxiv.org/abs/2508.10894</link>
<guid>https://arxiv.org/abs/2508.10894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MAESTRO在多时相遥感任务中取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文针对遥感数据的独特性，对多模态、多时相和多光谱数据的融合策略与重建目标归一化方案进行了全面基准测试。基于研究结果，提出了一种名为MAESTRO的新方法，该方法是对掩码自编码器的改进，采用了优化的融合策略和定制的目标归一化方案，并引入了光谱先验作为自监督信号。在四个遥感数据集上评估表明，MAESTRO在依赖多时相动态的任务中表现优异，同时在单时相任务中也保持了竞争力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:58:45 GMT</pubDate>
</item>
<item>
<title>大型语言模型在强化学习中的模拟搜索应用研究</title>
<link>https://arxiv.org/abs/2508.10874</link>
<guid>https://arxiv.org/abs/2508.10874</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs可作为高效模拟器用于RL搜索任务，减少对外部引擎依赖。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在强化学习（RL）中作为高效模拟器的潜力，以减少对昂贵外部搜索引擎的依赖。通过结构化提示和重复采样，研究人员量化了LLMs的内在搜索能力，并将其称为Self-Search。实验结果表明，LLMs在推理预算增加时表现出强大的扩展性，在问答基准测试中取得了高pass@k成绩。基于此，作者提出了Self-Search RL（SSRL），通过基于格式和规则的奖励增强LLMs的Self-Search能力，使模型能够内部迭代优化知识利用，而无需外部工具。实证评估显示，SSRL训练的策略模型为搜索驱动的RL训练提供了成本效益高且稳定的环境，减少了对外部搜索引擎的依赖，并促进了模拟到现实的迁移。研究得出三个结论：LLMs具备可有效激发的世界知识；SSRL展示了利用内部知识减少幻觉的潜力；SSRL训练的模型可无缝与外部搜索引擎集成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10874" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:46:01 GMT</pubDate>
</item>
<item>
<title>TexVerse：大规模高分辨率3D纹理数据集发布</title>
<link>https://arxiv.org/abs/2508.10868</link>
<guid>https://arxiv.org/abs/2508.10868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TexVerse提供1.6M高分辨率3D模型，推动纹理合成与PBR材料开发。</p><br /><br /><p><strong>摘要：</strong> TexVerse是一个大规模的高分辨率3D纹理数据集，包含超过858,000个独特的3D模型，其中158,000个带有基于物理的渲染（PBR）材质。该数据集总共有1.6M个3D实例，并包含专门的子集如TexVerse-Skeleton和TexVerse-Animation，分别包含带骨骼和动画的模型。每个模型都保留了原始的结构和细节信息，并附有详细的注释。TexVerse为纹理合成、PBR材料开发、动画以及多种3D视觉和图形任务提供了高质量的数据资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:43:25 GMT</pubDate>
</item>
<item>
<title>X-Node：一种可解释的图神经网络框架</title>
<link>https://arxiv.org/abs/2508.10461</link>
<guid>https://arxiv.org/abs/2508.10461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Node通过节点自解释提升图神经网络的可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Node，一种自解释的图神经网络框架，使每个节点在预测过程中生成自己的解释。该框架构建结构化上下文向量，包含度、中心性、聚类、特征显著性和标签一致性等可解释线索，并通过轻量级Reasoner模块生成解释向量。解释向量用于重建节点嵌入、生成自然语言解释以及通过文本注入机制指导GNN。实验表明，X-Node在保持分类准确性的前提下提供了忠实的节点级解释。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10461" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:00:45 GMT</pubDate>
</item>
<item>
<title>XQuant：通过低比特量化提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2508.10395</link>
<guid>https://arxiv.org/abs/2508.10395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XQuant通过低比特量化显著降低内存消耗，提升推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了XQuant，一种通过低比特量化技术减少大语言模型（LLM）推理内存占用的方法。与传统的KV缓存方式不同，XQuant量化并缓存层输入激活X，在推理过程中实时重新生成Keys和Values，从而实现两倍的内存节省。实验表明，XQuant在保持接近FP16精度的情况下，可实现高达7.7倍的内存节省。进一步提出的XQuant-CL利用跨层X嵌入的相似性，实现高达10倍的内存压缩，仅带来0.01的困惑度损失。该方法充分利用硬件计算能力，有效缓解了LLM推理中的内存瓶颈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 02:52:38 GMT</pubDate>
</item>
<item>
<title>DINOv3：实现自监督学习愿景的视觉基础模型</title>
<link>https://arxiv.org/abs/2508.10104</link>
<guid>https://arxiv.org/abs/2508.10104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINOv3通过自监督学习提升视觉模型性能，无需微调即可超越现有技术。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DINOv3，这是一种基于自监督学习的视觉基础模型，旨在减少对人工标注数据的依赖。通过扩大数据集和模型规模，并引入Gram anchoring方法解决密集特征图在长期训练中的退化问题，DINOv3在多种视觉任务中表现出色，优于现有的自监督和弱监督模型。此外，该模型还具备灵活的分辨率、模型大小和与文本对齐的能力，适用于多种资源约束和部署场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 14:00:55 GMT</pubDate>
</item>
<item>
<title>Thyme：通过代码实现图像处理与逻辑推理的新型多模态大模型框架</title>
<link>https://arxiv.org/abs/2508.11630</link>
<guid>https://arxiv.org/abs/2508.11630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Thyme通过代码实现图像处理与逻辑推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Thyme，一种新的多模态大语言模型（MLLM）范式，旨在超越传统的‘思考与图像’方法。Thyme能够自主生成并执行多种图像处理和计算操作，同时增强逻辑推理能力。该模型通过两阶段训练策略进行优化：首先在50万样本的数据集上进行监督微调，随后通过强化学习进一步优化决策过程。为了提高学习难度，研究者手动收集并设计了高分辨率问答对，并提出GRPO-ATS算法，以不同温度控制文本和代码生成，平衡推理探索与代码执行精度。实验表明，Thyme在近20个基准测试中表现出显著且一致的性能提升，尤其在高分辨率感知和复杂推理任务中表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 13:59:49 GMT</pubDate>
</item>
<item>
<title>StyleMM：基于文本描述的风格化3D可变形模型框架</title>
<link>https://arxiv.org/abs/2508.11203</link>
<guid>https://arxiv.org/abs/2508.11203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StyleMM通过文本控制生成风格化3D人脸模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出StyleMM，一种基于用户文本描述生成风格化3D可变形模型（3DMM）的新框架。该方法利用预训练的网格变形网络和纹理生成器，并通过文本引导的图像到图像翻译（i2i）扩散模型生成风格化面部图像作为训练目标。为保持原始面部特征，引入了保留面部属性的风格化方法，确保在风格迁移过程中身份、对齐和表情的一致性。训练完成后，StyleMM能够实现对形状、表情和纹理参数的显式控制，生成具有连贯顶点连接性和可动画性的3D人脸模型。实验表明，该方法在身份级面部多样性与风格化能力方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:29:46 GMT</pubDate>
</item>
<item>
<title>PaperRegister：支持细粒度论文检索的系统</title>
<link>https://arxiv.org/abs/2508.11116</link>
<guid>https://arxiv.org/abs/2508.11116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PaperRegister提升细粒度论文搜索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出PaperRegister，一种结合离线分层索引和在线自适应检索的论文搜索系统。传统系统依赖摘要构建索引，难以支持细粒度查询。PaperRegister通过构建分层索引树，有效提升不同粒度下的检索效果，尤其在细粒度场景中表现突出，展现出良好的实际应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.11116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 19:43:46 GMT</pubDate>
</item>
<item>
<title>基于GAN的半监督学习框架在低标注数据医学影像中的应用</title>
<link>https://arxiv.org/abs/2508.06429</link>
<guid>https://arxiv.org/abs/2508.06429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种GAN半监督方法，提升低标注数据下的医学影像分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于生成对抗网络（GAN）的半监督学习框架，旨在解决医学影像领域中标注数据不足的问题。该框架包含生成器、判别器和分类器三部分，在有限的标注数据下通过图像翻译进行无监督学习，并结合集成伪标签和时间一致性机制提升预测可靠性。实验表明，该方法在多个MedMNIST数据集上均优于现有方法，尤其在5样本/类的极端情况下表现优异，为高成本标注场景提供了有效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:16:43 GMT</pubDate>
</item>
<item>
<title>Puppeteer：自动3D模型绑定与动画生成框架</title>
<link>https://arxiv.org/abs/2508.10898</link>
<guid>https://arxiv.org/abs/2508.10898</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Puppeteer实现3D模型自动绑定与高质量动画生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Puppeteer，一个用于3D模型自动绑定和动画生成的综合框架。该系统通过自回归Transformer预测骨骼结构，并采用基于关节的标记化策略和分层排序方法提升双向学习能力。同时，利用注意力机制推断皮肤权重，结合拓扑感知的关节注意力编码关节关系。此外，Puppeteer还引入了基于优化的动画生成管道，提高计算效率并生成稳定、高保真的动画效果。实验结果表明，该方法在骨骼预测准确性和皮肤质量方面均优于现有技术，适用于多种3D内容，有效解决了动态3D内容生成中的瓶颈问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10898" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>STream3R：基于Transformer的实时3D重建方法</title>
<link>https://arxiv.org/abs/2508.10893</link>
<guid>https://arxiv.org/abs/2508.10893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STream3R利用Transformer实现高效3D重建，性能优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出STream3R，一种将点云预测重新定义为解码器-only Transformer问题的新型3D重建方法。与依赖全局优化或简单记忆机制的传统方法不同，STream3R采用流式框架，通过因果注意力机制处理图像序列，借鉴了现代语言模型的进展。该方法通过大规模3D数据集学习几何先验，能够有效应对动态场景等复杂情况。实验表明，STream3R在静态和动态场景基准测试中均优于现有方法，并且与LLM训练基础设施兼容，支持大规模预训练和微调，为实时3D感知提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:58:05 GMT</pubDate>
</item>
<item>
<title>自然语言处理中隐私与可解释性的权衡研究</title>
<link>https://arxiv.org/abs/2508.10482</link>
<guid>https://arxiv.org/abs/2508.10482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了NLP中隐私与可解释性的关系及共存可能性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了可信自然语言处理中的隐私与可解释性之间的关系。尽管近年来对可解释性和隐私保护的研究显著增加，但两者交叉领域的研究仍较少。本文通过实证分析，探讨了在差分隐私和事后可解释性方法下，隐私与可解释性之间的权衡问题。研究发现，两者的相互作用受到下游任务、文本隐私化方法和可解释性方法选择的影响。文章指出隐私与可解释性并非必然对立，提出了未来在该领域工作的实用建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 05:34:29 GMT</pubDate>
</item>
<item>
<title>ToonComposer：统一动画中间帧生成与上色的AI模型</title>
<link>https://arxiv.org/abs/2508.10881</link>
<guid>https://arxiv.org/abs/2508.10881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToonComposer提升动画制作效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ToonComposer的生成模型，用于统一处理动画制作中的中间帧生成和上色阶段。该模型通过稀疏草图注入机制提供精确控制，并利用空间低秩适配器将现代视频基础模型适配到卡通领域。ToonComposer仅需少量草图和参考帧即可生成高质量动画，同时支持多草图输入以实现更精细的运动控制，显著降低人工工作量并提高灵活性。为评估模型性能，研究者还构建了PKBench基准测试集。实验结果表明，ToonComposer在视觉质量、运动一致性及生产效率方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:50:11 GMT</pubDate>
</item>
<item>
<title>扩散语言模型的现状与展望</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散语言模型在生成效率和上下文捕捉方面表现出色。</p><br /><br /><p><strong>摘要：</strong> 本文全面综述了扩散语言模型（DLMs）的发展现状，分析了其与自回归模型和掩码语言模型的关系，并探讨了预训练、后训练以及推理优化等关键技术。文章还介绍了DLM在多模态任务中的应用及其面临的挑战，如效率、长序列处理和基础设施需求，并提出了未来研究方向。项目代码可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:47:22 GMT</pubDate>
</item>
<item>
<title>基于Pass@k的强化学习探索能力研究</title>
<link>https://arxiv.org/abs/2508.10751</link>
<guid>https://arxiv.org/abs/2508.10751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pass@k训练提升RLVR探索能力，优化策略平衡。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在强化学习中使用Pass@k作为奖励机制对模型探索能力的影响。传统RLVR依赖Pass@1奖励，导致策略偏向保守，难以跳出局部最优。通过将Pass@k作为奖励进行训练，实验表明模型探索能力得到提升。进一步分析发现，Pass@k训练能够有效优化优势函数设计，使探索与利用目标相互促进，而非冲突。该研究为RLVR中的策略优化提供了新思路，并展示了在优势函数设计方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 11:34:47 GMT</pubDate>
</item>
<item>
<title>NextStep-1：基于离散文本与连续图像标记的先进文本到图像生成模型</title>
<link>https://arxiv.org/abs/2508.10711</link>
<guid>https://arxiv.org/abs/2508.10711</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NextStep-1在文本到图像生成中表现优异，具备高保真图像合成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出NextStep-1，一个14B参数的自回归模型，结合157M参数的流匹配头，通过预测下一个标记实现文本到图像的生成。该模型在离散文本标记和连续图像标记上进行训练，表现出卓越的性能，在高保真图像合成和图像编辑任务中均取得优异结果。研究团队计划开源代码和模型以促进开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10711" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 10:54:22 GMT</pubDate>
</item>
<item>
<title>视觉编码器对图像采集参数的敏感性分析</title>
<link>https://arxiv.org/abs/2508.10637</link>
<guid>https://arxiv.org/abs/2508.10637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像采集参数影响视觉编码器的语义预测效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉编码器在面对图像采集过程中的细微或不可察觉的变换时的表现。不同于以往关注严重图像损坏的研究，本文聚焦于那些可能被人类忽略的参数变化。研究发现，这些参数在视觉表示中被系统性地编码，并可被恢复。这些参数的存在对语义预测有显著影响，其效果取决于语义标签与采集或处理相关标签之间的相关性或反相关性。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 09:34:13 GMT</pubDate>
</item>
<item>
<title>基于可解释机器学习的自动口译质量评估框架</title>
<link>https://arxiv.org/abs/2508.10860</link>
<guid>https://arxiv.org/abs/2508.10860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多维建模框架提升口译质量评估的透明性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对自动口译质量评估中存在的语言使用质量分析不足、数据稀缺和模型不可解释等问题，提出一种融合特征工程、数据增强和可解释机器学习的多维建模框架。该框架通过使用透明特征并结合SHAP分析，提升了模型的可解释性。实验表明，在英中连续口译数据集上，BLEURT和CometKiwi得分对忠实度具有最强预测力，停顿相关特征对流利度有显著影响，而中文特有的短语多样性指标则对语言使用质量有重要贡献。该方法提供了一种可扩展、可靠且透明的替代传统人工评估的方式，有助于为学习者提供详细诊断反馈，支持自主学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 13:31:18 GMT</pubDate>
</item>
<item>
<title>UI-Venus：基于多模态大语言模型的高效UI代理系统</title>
<link>https://arxiv.org/abs/2508.10833</link>
<guid>https://arxiv.org/abs/2508.10833</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-Venus在UI识别与导航任务中表现优异，超越现有SOTA模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UI-Venus，一个仅以截图作为输入的原生UI代理系统，基于多模态大语言模型Qwen2.5-VL进行强化微调。该系统在UI识别和导航任务中表现出色，7B和72B版本分别在Screenspot-V2/Pro基准测试中达到94.1%/50.8%和95.3%/61.9%的准确率，优于现有SOTA模型如GTA1和UI-TARS-1.5。此外，在AndroidWorld导航任务中，其成功率达到49.1%和65.9%。研究团队设计了专门的奖励函数和数据清洗策略，并提出Self-Evolving Trajectory History Alignment与Sparse Action Enhancement方法，提升导航性能。项目代码已开源，旨在推动社区进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10833" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 12:58:07 GMT</pubDate>
</item>
<item>
<title>HumanSense：评估多模态大语言模型的人类中心交互能力</title>
<link>https://arxiv.org/abs/2508.10576</link>
<guid>https://arxiv.org/abs/2508.10576</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HumanSense评估MLLM在理解人类意图和提供共情反馈方面的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HumanSense，一个用于评估多模态大语言模型（MLLM）在人类中心场景中感知与交互能力的基准。研究指出，当前领先的MLLM在复杂交互任务上仍有较大提升空间，尤其是在结合视觉、音频和文本信息时表现更优。文章强调了推理能力在生成合理反馈中的关键作用，并通过多阶段、模态渐进式强化学习提升了模型表现。此外，研究还发现成功推理过程具有高度一致的思维模式，通过设计特定提示可有效提升非推理模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10576" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 08:14:15 GMT</pubDate>
</item>
<item>
<title>We-Math 2.0：提升多模态大模型数学推理能力的统一系统</title>
<link>https://arxiv.org/abs/2508.10433</link>
<guid>https://arxiv.org/abs/2508.10433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">We-Math 2.0提升多模态大模型数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出We-Math 2.0，一个旨在提升多模态大语言模型（MLLMs）数学推理能力的统一系统。该系统包含四个关键贡献：数学知识体系MathBook、MathBook-Standard与MathBook-Pro数据集、基于强化学习的MathBook-RL训练框架以及全面的MathBookEval评估基准。通过结构化知识构建、数据空间建模和强化学习优化，We-Math 2.0在多个数学推理任务中表现出色，展现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.10433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 14 Aug 2025 04:15:41 GMT</pubDate>
</item>
<item>
<title>PRELUDE基准测试：评估长上下文理解能力</title>
<link>https://arxiv.org/abs/2508.09848</link>
<guid>https://arxiv.org/abs/2508.09848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRELUDE挑战模型对长上下文的理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> PRELUDE是一个用于评估长上下文理解能力的基准测试，任务是判断一个角色的前传故事是否符合原著的正典叙事。该任务比现有基准更具挑战性，因为前传并非原故事的一部分，评估其合理性通常需要整合多个部分的信息。实验结果显示，即使使用最先进的大语言模型和商业服务，其表现仍落后于人类超过15%。进一步的人类研究发现，模型有时能给出正确答案但推理过程有误，导致推理准确率与人类相差超过30%。这些结果表明，长上下文理解和推理仍有很大提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 10:28:25 GMT</pubDate>
</item>
<item>
<title>基于mu参数化的Mixture-of-Experts模型研究</title>
<link>https://arxiv.org/abs/2508.09752</link>
<guid>https://arxiv.org/abs/2508.09752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出muP参数化方法提升MoE模型性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，大语言模型（LLM）的广泛应用推动了超参数调优技术的发展，其中muTransfer成为关键方法。同时，Mixture-of-Experts（MoE）架构在大规模模型中表现出色。然而，两者结合的研究仍处于空白。本文提出了mu-Parameterization（muP）用于MoE模型，理论分析了其在不同模型宽度下的特征学习能力，并通过实验验证了该参数化方法的有效性。此外，研究还探讨了专家数量和粒度对最优学习率的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:31:27 GMT</pubDate>
</item>
<item>
<title>GFPO：通过优化训练策略减少大模型响应长度膨胀</title>
<link>https://arxiv.org/abs/2508.09726</link>
<guid>https://arxiv.org/abs/2508.09726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GFPO优化训练策略，减少模型响应长度膨胀。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GFPO（Group Filtered Policy Optimization）的新方法，旨在解决大型语言模型在使用强化学习训练时出现的响应长度膨胀问题。该方法通过在训练过程中采样更大的问题组，并基于响应长度和每token奖励效率两个指标过滤响应，从而减少冗余内容。实验表明，在Phi-4-reasoning模型上，GFPO在保持准确率的同时，显著减少了响应长度，尤其在STEM和编程基准测试中效果明显。此外，GFPO还引入了自适应难度机制，动态分配训练资源以提升计算效率与准确性的平衡。研究证明，增加训练时间的计算资源可以有效减少推理时间的计算需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 07:43:49 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在模糊问题下的鲁棒性</title>
<link>https://arxiv.org/abs/2508.07321</link>
<guid>https://arxiv.org/abs/2508.07321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究测试大语言模型在模糊问题下的表现，发现其易产生错误回答。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ObfusQAte的新技术，并基于此构建了ObfusQA框架，用于评估大语言模型（LLMs）在面对经过混淆的问题时的鲁棒性和适应能力。该框架通过三种不同的混淆层次——命名实体误导、干扰项误导和上下文过载，系统地测试LLMs的表现。研究发现，当面对这些复杂变化时，LLMs往往无法正确回答或产生幻觉式回应。为了促进相关领域的研究，作者公开了ObfusQAte工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 08:27:52 GMT</pubDate>
</item>
<item>
<title>基于3DGS的场景重建与修复方法GSFixer</title>
<link>https://arxiv.org/abs/2508.09667</link>
<guid>https://arxiv.org/abs/2508.09667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSFixer提升稀疏视角下3DGS重建质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出GSFixer框架，旨在解决从稀疏视角重建3D场景时因信息不足导致的明显伪影问题。该方法基于DiT视频扩散模型，结合参考引导的视频修复技术，利用2D语义特征和3D几何特征提升重建结果的语义一致性和3D一致性。为评估3DGS伪影修复效果，作者还构建了DL3DV-Res数据集。实验表明，GSFixer在3DGS伪影修复和稀疏视角3D重建任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 05:56:28 GMT</pubDate>
</item>
<item>
<title>基于自适应扫描的细粒度医学图像分割方法ASM-UNet</title>
<link>https://arxiv.org/abs/2508.07237</link>
<guid>https://arxiv.org/abs/2508.07237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASM-UNet提升细粒度医学图像分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于Mamba的新型细粒度医学图像分割架构ASM-UNet，通过引入自适应扫描得分来动态调整扫描顺序，结合群体共性和个体差异，提升了对小尺度解剖结构的分割精度。在ACDC、Synapse和新提出的胆道系统FGS数据集BTMS上的实验表明，ASM-UNet在粗粒度和细粒度分割任务中均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 04:33:03 GMT</pubDate>
</item>
<item>
<title>CannyEdit：一种改进的文本到图像区域编辑框架</title>
<link>https://arxiv.org/abs/2508.06937</link>
<guid>https://arxiv.org/abs/2508.06937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CannyEdit提升文本引导图像编辑的准确性与自然度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CannyEdit的训练-free 图像编辑框架，旨在解决现有方法在文本遵循性、上下文保真度和编辑融合度之间的平衡问题。该框架通过两项创新技术：Selective Canny Control 实现了对编辑区域的精准控制并保留未编辑区域的细节；Dual-Prompt Guidance 结合局部与全局提示，确保场景一致性。实验表明，CannyEdit在真实图像编辑任务中优于KV-Edit等方法，提升了文本遵循性和上下文保真度，并在编辑融合度上显著优于竞争对手。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 07:06:58 GMT</pubDate>
</item>
<item>
<title>基于多智能体强化学习的无人机协同操控方法</title>
<link>https://arxiv.org/abs/2508.01522</link>
<guid>https://arxiv.org/abs/2508.01522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种去中心化方法实现无人机协同操控负载。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种首次实现现实世界中6自由度负载操控的去中心化方法，利用多智能体强化学习（MARL）训练每个微型飞行器的外层控制策略。该方法无需全局状态、无人机间通信或邻近信息，仅通过负载姿态观测进行隐式通信，提高了可扩展性和灵活性，并降低了计算成本，支持 onboard 部署。研究还引入了基于线性加速度和机体速率的新动作空间设计，结合稳健的低级控制器，实现了可靠的仿真到现实迁移。实验验证了方法在负载模型不确定性下的全姿态控制性能，并展示了异构控制策略的协作能力和对单个无人机失效的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 19:52:33 GMT</pubDate>
</item>
<item>
<title>Story2Board：无需训练的叙事分镜生成框架</title>
<link>https://arxiv.org/abs/2508.09983</link>
<guid>https://arxiv.org/abs/2508.09983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Story2Board提升分镜生成的连贯性和叙事性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Story2Board，一种无需训练的自然语言到叙事分镜的生成框架。现有方法仅关注主体身份，忽视了空间构图、背景演变和叙事节奏等关键因素。Story2Board通过两个轻量级组件——潜在面板锚定和互注意值混合，增强视觉一致性而不改变架构或微调模型。该框架利用现成的语言模型生成结构化分镜提示，并引入Rich Storyboard Benchmark评估布局多样性与背景相关叙事能力。同时提出Scene Diversity指标衡量空间和姿态变化。实验表明，Story2Board在动态性、连贯性和叙事吸引力方面优于现有基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:56:26 GMT</pubDate>
</item>
<item>
<title>VisCodex：融合视觉与代码的多模态语言模型框架</title>
<link>https://arxiv.org/abs/2508.09945</link>
<guid>https://arxiv.org/abs/2508.09945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisCodex提升多模态代码生成能力，表现接近商业模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出VisCodex，一个将视觉和代码语言模型融合的统一框架，旨在增强多模态大语言模型的代码生成能力。通过任务向量模型融合技术，将先进的代码生成模型与强大的视觉语言基础模型结合，同时保留视觉理解和高级编码技能。研究还发布了Multimodal Coding Dataset (MCD) 和InfiBench-V基准测试，用于训练和评估模型。实验表明，VisCodex在开源多模态模型中表现最佳，接近GPT-4o等商业模型，验证了其方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:00:44 GMT</pubDate>
</item>
<item>
<title>自动化生成文本解释提升NLP模型性能</title>
<link>https://arxiv.org/abs/2508.09776</link>
<guid>https://arxiv.org/abs/2508.09776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化生成的文本解释可有效提升NLP模型性能。</p><br /><br /><p><strong>摘要：</strong> 在可解释自然语言处理领域，文本解释对于理解模型预测和丰富数据集至关重要。传统方法依赖人工标注，成本高且难以扩展。本文提出一种自动化框架，利用多个先进的大语言模型生成高质量文本解释，并通过自然语言生成指标评估其质量。研究还探讨了这些解释对预训练语言模型在自然语言推理任务中的影响。实验表明，自动化生成的解释在提升模型性能方面与人工标注具有竞争力，为扩展NLP数据集和增强模型性能提供了可行路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:59:08 GMT</pubDate>
</item>
<item>
<title>M3-Agent：具备长期记忆的多模态智能体框架</title>
<link>https://arxiv.org/abs/2508.09736</link>
<guid>https://arxiv.org/abs/2508.09736</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3-Agent具备长期记忆与多模态处理能力，提升任务完成效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了M3-Agent，一个具备长期记忆的多模态智能体框架。该框架能够处理实时视觉和听觉输入，构建并更新长期记忆，不仅包含情景记忆，还发展出语义记忆，以积累世界知识。其记忆以实体为中心的多模态形式组织，有助于更深入理解环境。M3-Agent能自主进行多轮推理，并从记忆中检索信息完成任务。为评估多模态智能体的记忆效果和基于记忆的推理能力，作者开发了M3-Bench基准测试集，包含机器人视角视频和网络视频数据。实验结果显示，M3-Agent在多个基准上优于现有最强基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09736" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 08:03:03 GMT</pubDate>
</item>
<item>
<title>轻量级身份保持视频生成框架Stand-In</title>
<link>https://arxiv.org/abs/2508.07901</link>
<guid>https://arxiv.org/abs/2508.07901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出轻量级视频生成框架Stand-In，实现高效身份保持。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Stand-In的轻量级、即插即用的视频生成身份保持框架。该框架通过在预训练视频生成模型中引入条件图像分支，利用受限自注意力机制和条件位置映射实现身份控制，并仅需2000对数据即可快速学习。尽管仅增加了1%的额外参数，该框架在视频质量和身份保持方面表现优异，优于其他全参数训练方法。此外，该框架可无缝集成到其他任务中，如基于主体的视频生成、姿态参考视频生成、风格化和人脸交换。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 08:17:38 GMT</pubDate>
</item>
<item>
<title>GRAO：融合SFT与RL的高效语言模型对齐方法</title>
<link>https://arxiv.org/abs/2508.07750</link>
<guid>https://arxiv.org/abs/2508.07750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRAO提升语言模型对齐效果，融合SFT与RL优势。</p><br /><br /><p><strong>摘要：</strong> 本文提出GRAO（Group Relative Alignment Optimization），一种结合监督微调（SFT）和强化学习（RL）的统一框架，旨在解决语言模型对齐中的效率与效果问题。GRAO通过多样本生成策略、组内相对优势加权损失函数以及参考感知参数更新机制，提升了模型的收敛速度和样本效率。理论分析表明其具有更好的收敛性和效率，实验结果显示在多个复杂对齐任务中，GRAO分别比SFT、DPO、PPO和GRPO基线提升了57.70%、17.65%、7.95%和5.18%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 04:28:47 GMT</pubDate>
</item>
<item>
<title>基于隐式奖励的自适应元微调方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2508.06944</link>
<guid>https://arxiv.org/abs/2508.06944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AMFT方法通过动态平衡SFT与RL提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为自适应元微调（AMFT）的新方法，旨在解决大语言模型在推理任务中因两阶段微调导致的灾难性遗忘和模仿与探索之间的权衡问题。AMFT通过隐式奖励理论，将监督微调（SFT）和强化学习（RL）视为互补的奖励信号，并引入一个可学习的元梯度权重控制器，动态优化两者之间的平衡。该方法通过策略熵正则化确保稳定性，能够自动发现有效的训练课程。实验表明，AMFT在多个基准测试中表现优异，特别是在数学推理、抽象视觉推理和视觉-语言导航任务中均达到新的最先进水平，并展现出更强的分布外泛化能力。消融实验和训练动态分析验证了元学习控制器对模型稳定性、样本效率和性能的关键作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 07:40:54 GMT</pubDate>
</item>
<item>
<title>GPT-4o合成数据提升开放模型图像生成能力</title>
<link>https://arxiv.org/abs/2508.09987</link>
<guid>https://arxiv.org/abs/2508.09987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPT-4o合成数据可增强开放模型图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用GPT-4o生成的合成图像数据来提升开放源代码模型图像生成能力的潜力。研究指出，合成图像能补充真实数据集中的罕见场景，并提供更清晰、可控的监督信号。基于此，作者构建了180K规模的Echo-4o-Image数据集，并用于微调基础模型Echo-4o。此外，还提出了两个新的评估基准GenEval++和Imagine-Bench，以更准确地衡量图像生成能力。实验结果表明，该数据集在多个标准基准上表现优异，并且对其他基础模型也具有良好的迁移性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:59:28 GMT</pubDate>
</item>
<item>
<title>基于噪声超网络的测试时扩展优化方法</title>
<link>https://arxiv.org/abs/2508.09968</link>
<guid>https://arxiv.org/abs/2508.09968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出噪声超网络替代测试时优化，提升模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对测试时扩展（test-time scaling）方法在推理阶段计算成本过高的问题，提出一种新的解决方案。通过引入噪声超网络替代传统的奖励引导噪声优化，实现对初始输入噪声的调制。该方法在保持基础模型精度的同时，显著降低了计算成本，并有效恢复了测试时优化带来的性能提升。实验表明，该方法在生成模型中具有良好的应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 13:33:37 GMT</pubDate>
</item>
<item>
<title>基于动态监督的多智能体系统提升问题解决稳定性</title>
<link>https://arxiv.org/abs/2508.09889</link>
<guid>https://arxiv.org/abs/2508.09889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态监督机制提升多智能体系统稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型在智能代理中的应用，指出随着代理依赖多种工具，面临上下文复杂和工具输出噪声等问题，影响系统可靠性。为解决这一问题，研究提出动态监督与操控机制，在AWorld框架内构建了稳健的多智能体系统（MAS）。执行代理在关键步骤调用守护代理进行验证与修正，有效减少错误并提高解决问题的鲁棒性。实验表明，该系统在GAIA数据集上表现优于单代理系统和标准工具增强系统，取得优异成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 13 Aug 2025 11:46:25 GMT</pubDate>
</item>
<item>
<title>输入感知后门攻击方法在视觉语言模型中的应用</title>
<link>https://arxiv.org/abs/2508.09456</link>
<guid>https://arxiv.org/abs/2508.09456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新型后门攻击方法，操控VLM的视觉定位行为。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为IAG的输入感知后门攻击方法，旨在操控视觉语言模型（VLMs）的视觉定位行为。该方法通过将目标对象的语义信息嵌入原始图像中，使模型无论用户查询如何，都会将特定目标对象作为定位结果。为提高攻击隐蔽性，采用了重建损失以减少被污染图像与干净图像之间的视觉差异，并提出了统一的攻击数据生成方法。实验表明，IAG在多个模型上表现出高成功率，同时对干净样本的准确率影响较小，展示了其有效性、鲁棒性和迁移能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 23:22:19 GMT</pubDate>
</item>
<item>
<title>Mol-R1：提升分子生成推理能力的新型框架</title>
<link>https://arxiv.org/abs/2508.08401</link>
<guid>https://arxiv.org/abs/2508.08401</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mol-R1提升LLM在分子生成中的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Mol-R1，一种改进R1类显式长链思维（Long-CoT）大语言模型在文本驱动分子生成任务中解释性和推理能力的新框架。该框架通过PRID方法构建高质量推理数据集，并采用MoIA训练策略，结合监督微调与强化策略优化，显著提升了模型在分子发现领域的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08401" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 14:50:05 GMT</pubDate>
</item>
<item>
<title>基于D2F的扩散语言模型加速方法</title>
<link>https://arxiv.org/abs/2508.09192</link>
<guid>https://arxiv.org/abs/2508.09192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">D2F提升扩散语言模型推理速度，超越传统模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为离散扩散强制（D2F）的策略，使扩散大语言模型（dLLMs）在推理速度上超越相同规模的自回归（AR）模型。D2F通过块级自回归生成和跨块并行解码，实现KV缓存利用与高效推理。该方法基于预训练dLLMs进行非对称蒸馏，并结合流水线并行解码算法，在保持输出质量的同时显著提升效率。实验表明，D2F模型在GSM8K任务中推理速度是LLaMA3和Qwen2.5的2.5倍以上，比原始dLLMs快50倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:51:37 GMT</pubDate>
</item>
<item>
<title>MathReal：面向真实教育场景的多模态数学推理数据集</title>
<link>https://arxiv.org/abs/2508.06009</link>
<guid>https://arxiv.org/abs/2508.06009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathReal提升多模态模型在真实教育场景中的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MathReal，一个包含2000个数学问题的多模态数据集，这些问题来自真实的K-12教育场景，图像由手持设备拍摄。数据集根据图像质量、视角变化和干扰内容分为14个子类，并覆盖五个核心知识领域和三种难度级别。研究设计了六个实验设置来评估多模态大语言模型在现实场景中的表现，发现现有模型在真实教育环境中面临显著挑战。通过分析模型性能与错误模式，提出了未来改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 00:39:16 GMT</pubDate>
</item>
<item>
<title>Cooper框架提升大语言模型推理能力与奖励模型鲁棒性</title>
<link>https://arxiv.org/abs/2508.05613</link>
<guid>https://arxiv.org/abs/2508.05613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cooper框架提升大模型推理与奖励模型鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cooper框架，通过联合优化策略模型和奖励模型，解决现有奖励机制在鲁棒性和抗奖励欺骗方面的不足。Cooper结合规则奖励的高精度与动态样本对训练，增强模型稳定性。同时引入混合标注策略和基于参考答案的奖励建模方法，提升了奖励模型的准确性。实验表明，Cooper有效缓解了奖励欺骗问题，并在端到端强化学习中表现出色，如在Qwen2.5-1.5B-Instruct上提升了0.54%的平均准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:53:56 GMT</pubDate>
</item>
<item>
<title>面向域适应的变更检测视觉问答方法研究</title>
<link>https://arxiv.org/abs/2508.08974</link>
<guid>https://arxiv.org/abs/2508.08974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TCSSM模型应对CDVQA中的域偏移问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对变更检测视觉问答（CDVQA）任务中因域偏移导致性能下降的问题，引入了多模态、多域数据集BrightVQA，并提出Text-Conditioned State Space Model（TCSSM）模型。该模型通过统一处理双时相图像和与地质灾害相关的文本信息，提取跨域不变特征，提升模型在不同域下的泛化能力。实验表明，TCSSM在多个基准测试中表现优于现有方法，代码和数据集将在论文接受后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 10:37:53 GMT</pubDate>
</item>
<item>
<title>RedDino：面向红细胞图像分析的自监督基础模型</title>
<link>https://arxiv.org/abs/2508.08180</link>
<guid>https://arxiv.org/abs/2508.08180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RedDino是用于红细胞分析的先进AI模型，性能优于现有技术。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RedDino，一个专为红细胞图像分析设计的自监督基础模型。该模型基于DINOv2框架，并在包含125万张红细胞图像的数据集上进行训练，涵盖了多种成像模态和来源。实验表明，RedDino在红细胞形态分类任务中表现优于当前最先进的模型。通过线性探测和最近邻分类评估，验证了其强大的特征表示能力和泛化能力。研究的主要贡献包括：针对红细胞分析的基础模型、DINOv2配置的消融研究以及泛化性能的详细评估。RedDino解决了计算血液学中的关键挑战，推动了可靠诊断工具的发展。源代码和预训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 12:59:31 GMT</pubDate>
</item>
<item>
<title>ASTRA：系统化检测AI代码生成安全缺陷的自动化代理系统</title>
<link>https://arxiv.org/abs/2508.03936</link>
<guid>https://arxiv.org/abs/2508.03936</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASTRA提升AI代码生成系统的安全性，发现更多漏洞。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ASTRA，一个用于检测AI代码生成和安全指导系统中安全缺陷的自动化代理系统。ASTRA通过构建领域知识图谱、在线漏洞探索以及生成高危测试用例三个阶段，有效识别现实场景中的潜在问题。相比现有方法，ASTRA在两个主要评估领域中发现了11-66%更多的问题，并提升了17%的对齐训练效果，展示了其在构建更安全AI系统方面的实际价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03936" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 17:57:52 GMT</pubDate>
</item>
<item>
<title>Putnam-AXIOM：评估大语言模型数学推理的新基准</title>
<link>https://arxiv.org/abs/2508.08292</link>
<guid>https://arxiv.org/abs/2508.08292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Putnam-AXIOM测试大语言模型数学推理能力，揭示模型依赖记忆而非推理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Putnam-AXIOM，一个包含522道大学级数学竞赛题的基准测试集，以及其100道通过程序扰动生成的变体题。该基准旨在克服训练数据污染问题，并提供动态、抗污染的评估方式。实验表明，即使是最强模型o1-preview在变体题上的准确率也显著下降，说明模型可能依赖记忆而非真正推理。文章还引入了Teacher-Forced Accuracy（TFA）作为衡量推理过程的新指标，以更准确地评估模型的数学推理能力。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:57:50 GMT</pubDate>
</item>
<item>
<title>逻辑指令理解挑战与基准测试研究</title>
<link>https://arxiv.org/abs/2508.09125</link>
<guid>https://arxiv.org/abs/2508.09125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前大语言模型在复杂逻辑指令理解上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在处理复杂逻辑指令方面的表现，提出了LogicIFGen和LogicIFEval框架与基准。LogicIFGen用于自动生成可验证的逻辑指令，而LogicIFEval包含426个复杂的逻辑指令测试集。实验表明，当前最先进的大语言模型在遵循这些指令时表现不佳，多数只能正确理解不到60%的指令，显示出在指令理解能力上的显著缺陷。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:54:27 GMT</pubDate>
</item>
<item>
<title>基于LLM的低资源语言数据生成方法TopXGen</title>
<link>https://arxiv.org/abs/2508.08680</link>
<guid>https://arxiv.org/abs/2508.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TopXGen提升低资源语言机器翻译性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为TopXGen的基于大语言模型的方法，用于生成高质量且主题多样的低资源语言数据。该方法通过利用大语言模型在高资源语言上的翻译能力，生成自然流畅的目标语言文本，再通过回译生成可用于上下文学习和微调的平行语料。实验表明，TopXGen能有效提升大语言模型在低资源语言翻译任务中的表现。相关代码和结果已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 02:58:02 GMT</pubDate>
</item>
<item>
<title>无需微调的大型语言模型在《外交》游戏中的评估框架</title>
<link>https://arxiv.org/abs/2508.07485</link>
<guid>https://arxiv.org/abs/2508.07485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">无需微调即可让LLM玩《外交》，提升战略推理评估。</p><br /><br /><p><strong>摘要：</strong> 本文提出了首个无需微调或专门训练即可让本地大型语言模型（LLMs）完整参与《外交》游戏的评估框架。由于《外交》游戏状态复杂且信息密集，以往研究依赖前沿模型或微调。本研究通过数据驱动的迭代优化文本游戏状态表示，使24B参数模型无需微调即可完成对局。同时开发了工具支持假设测试和统计分析，并进行了关于说服、激进玩法和模型表现的案例研究。实验表明，大模型表现最佳，但小模型也能合理应对。文中还引入了关键状态分析方法，用于深入研究对局关键时刻。该框架降低了战略推理评估的门槛，并揭示了广泛使用的LLM中自然涌现的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 17:07:08 GMT</pubDate>
</item>
<item>
<title>基于Q语言的大型语言模型适配与优化研究</title>
<link>https://arxiv.org/abs/2508.06813</link>
<guid>https://arxiv.org/abs/2508.06813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究针对Q语言进行LLM适配，提升模型在量化金融领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将大型语言模型（LLM）适配到Q编程语言，该语言在量化金融领域广泛应用，但在互联网上信息较少，因此对通用AI模型而言是一个挑战。研究构建了一个类似Leetcode的评估数据集，并对多个前沿模型进行了基准测试。随后，通过预训练、监督微调和强化学习，训练了一系列基于Qwen-2.5系列的模型，涵盖多种参数规模。实验结果显示，最佳模型在Q语言任务上的准确率达到了59%，显著优于Claude Opus-4和其他模型。此外，所有模型，包括最小的1.5B版本，均超过了GPT-4.1的表现。研究还提供了完整的模型训练流程和方法论，适用于其他领域和任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 00:22:07 GMT</pubDate>
</item>
<item>
<title>无需重建与优化的3D高斯点云风格迁移方法</title>
<link>https://arxiv.org/abs/2508.05813</link>
<guid>https://arxiv.org/abs/2508.05813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重建的3D高斯点云快速风格迁移方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需重建或优化的3D高斯点云风格迁移方法。该方法通过在点云隐式表面生成图结构，然后使用基于表面的前馈风格迁移方法，并将其插值回场景中的每个点云。此方法无需额外训练或优化，可快速实现风格迁移，即使在消费级硬件上也能在2分钟内完成。实验表明该方法在风格迁移质量上优于其他方法，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 15:35:01 GMT</pubDate>
</item>
<item>
<title>基于部分卷积的图像局部风格迁移方法</title>
<link>https://arxiv.org/abs/2508.05769</link>
<guid>https://arxiv.org/abs/2508.05769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种局部风格迁移网络，提升图像特定区域的风格化效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统风格迁移方法对整张图像进行处理的问题，提出一种基于部分卷积的风格迁移网络，能够更精确地将艺术风格应用于图像中的特定区域。同时，该方法引入了内部融合技术，以应对区域选择不准确带来的影响。实验表明，该方法在视觉效果和定量评估上均优于现有方法，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 14:35:44 GMT</pubDate>
</item>
<item>
<title>OpenCUA：开源框架推动计算机使用代理研究</title>
<link>https://arxiv.org/abs/2508.09123</link>
<guid>https://arxiv.org/abs/2508.09123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCUA开源框架提升CUA研究能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenCUA，一个用于扩展计算机使用代理（CUA）数据和基础模型的开源框架。该框架包含三个核心部分：用于捕捉人类计算机操作演示的标注基础设施、首个覆盖多操作系统和应用的大规模CUA数据集AgentNet，以及可扩展的数据转换管道，能够通过反思式长链式思维实现性能提升。实验表明，基于OpenCUA的模型在多个CUA基准测试中表现优异，其中OpenCUA-32B在OSWorld-Verified任务中达到34.8%的成功率，成为当前开源模型中的最佳表现。研究团队已公开工具、数据集、代码和模型，以支持后续CUA研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:52:32 GMT</pubDate>
</item>
<item>
<title>VertexRegen：一种连续细节层次的网格生成框架</title>
<link>https://arxiv.org/abs/2508.09062</link>
<guid>https://arxiv.org/abs/2508.09062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VertexRegen实现连续细节层次的网格生成，支持任意时刻停止。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VertexRegen，这是一种新颖的网格生成框架，能够在连续细节层次上生成网格。与传统的自回归方法不同，VertexRegen通过学习顶点分裂过程来反转边折叠，从而实现更灵活的生成方式。实验结果表明，VertexRegen生成的网格质量与现有最佳方法相当，并且能够随时停止生成，得到具有不同细节层次的有效网格。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 12:25:46 GMT</pubDate>
</item>
<item>
<title>量子博弈论在真实硬件上的实验验证</title>
<link>https://arxiv.org/abs/2508.09050</link>
<guid>https://arxiv.org/abs/2508.09050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">量子博弈在真实硬件上实现，验证了理论预测。</p><br /><br /><p><strong>摘要：</strong> 本文在IBM Quantum的ibm sherbrooke超导处理器上首次完整实现了Eisert-Wilkens-Lewenstein框架下的‘性别之战’博弈。通过四种量子策略在31个纠缠参数下的实验，对比了理论预测与实际执行结果。为减少噪声影响，作者提出了一种引导电路映射方法（GCM），优化了量子比特选择和路由。尽管存在硬件偏差，实验结果仍保持了理论预期的收益趋势，误差在3.5%-12%之间。这表明在NISQ条件下，量子博弈的优势仍可保持，为未来多智能体、经济和分布式决策系统提供了应用路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 12:10:05 GMT</pubDate>
</item>
<item>
<title>基于课程学习的长度控制推理方法研究</title>
<link>https://arxiv.org/abs/2508.08940</link>
<guid>https://arxiv.org/abs/2508.08940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">课程学习提升大模型推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于组相对策略优化（GRPO）的课程学习策略，用于控制大语言模型的推理长度。该方法从宽松的token预算开始，逐步收紧，促使模型先探索有效解法，再压缩为更简洁的推理过程。通过结合任务正确性、长度效率和格式遵循的奖励函数，实验表明该方法在多个数据集上优于固定预算基线，提升了准确率和计算效率。研究还分析了奖励权重和衰减策略的影响，证明渐进式约束是训练高效推理模型的有效归纳偏置。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 09:48:03 GMT</pubDate>
</item>
<item>
<title>DeCRED：提升编码器-解码器ASR模型鲁棒性的解码器中心正则化方法</title>
<link>https://arxiv.org/abs/2508.08938</link>
<guid>https://arxiv.org/abs/2508.08938</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeCRED通过解码器辅助分类器提升ASR模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DeCRED的解码器中心正则化方法，用于改进编码器-解码器语音识别模型中的内部语言模型。该方法在解码器中添加辅助分类器，通过中间logits实现下一个词的预测。实验结果显示，DeCRED在11个测试集上将内部语言模型的BPE困惑度降低了36.6%。此外，在5个域内和3个域外测试集中，WER表现优于基线模型，分别将宏观WER从6.4%降至6.3%和18.2%降至16.2%。在TEDLIUM3数据集上，DeCRED达到7.0%的WER，优于基线和InterCTC方法。尽管训练数据较少且参数更少，DeCRED仍表现出与OWSM v3.1和Whisper-medium相当的WER表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08938" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 09:44:50 GMT</pubDate>
</item>
<item>
<title>AffordDex：一种具备通用抓取能力的仿人手控制框架</title>
<link>https://arxiv.org/abs/2508.08896</link>
<guid>https://arxiv.org/abs/2508.08896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AffordDex实现通用且类人抓取，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出AffordDex框架，旨在提升机器人手的通用抓取能力。该框架采用两阶段训练策略：第一阶段通过模仿学习获取自然的手部运动先验；第二阶段通过残差模块调整动作以适应具体物体。关键组件包括Negative Affordance-aware Segmentation模块和教师-学生蒸馏过程，确保抓取动作既符合人体工学又功能合理。实验表明，AffordDex在已知、未知及全新类别物体上均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 08:36:01 GMT</pubDate>
</item>
<item>
<title>BiasGym：一种用于分析和减轻大语言模型偏见的框架</title>
<link>https://arxiv.org/abs/2508.08855</link>
<guid>https://arxiv.org/abs/2508.08855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BiasGym框架以检测和减轻LLM中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BiasGym，一个用于在大型语言模型中注入、分析和减轻概念关联的框架。该框架包含两个组件：BiasInject用于通过基于令牌的微调注入特定偏见，而BiasScope则利用这些信号识别并引导导致偏见的行为组件。该方法支持对偏见进行系统分析，实现针对性去偏而不影响下游任务性能，并适用于训练中未见过的偏见。实验表明，BiasGym能有效减少现实世界中的刻板印象，并探测虚构关联，具有安全干预和可解释性研究的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 07:23:44 GMT</pubDate>
</item>
<item>
<title>Aryabhata 1.0：专为印度高考优化的7B参数数学推理模型</title>
<link>https://arxiv.org/abs/2508.08665</link>
<guid>https://arxiv.org/abs/2508.08665</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aryabhata 1.0是专为JEE设计的高效数学推理模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Aryabhata 1.0，一个专为印度高考（JEE）优化的7B参数数学推理模型。该模型通过融合强大的开源推理模型，并结合课程学习的监督微调（SFT）和可验证奖励的强化学习（RLVR），提升了在数学问题解决上的表现。评估结果显示，Aryabhata 1.0在JEE主考、MATH和GSM8K等基准测试中均优于现有模型，同时提供清晰的逐步推理过程。该模型已开源，旨在推动以考试为导向的小型语言模型的发展，并鼓励社区反馈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08665" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 02:20:07 GMT</pubDate>
</item>
<item>
<title>基于单张参考图和2D姿态序列的可控4D角色动画框架CharacterShot</title>
<link>https://arxiv.org/abs/2508.07409</link>
<guid>https://arxiv.org/abs/2508.07409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CharacterShot实现从单图和2D姿态生成稳定4D角色动画。</p><br /><br /><p><strong>摘要：</strong> 本文提出CharacterShot，一个可控且一致的4D角色动画框架，允许设计师仅通过一张参考图和2D姿态序列创建动态3D角色。该方法首先基于先进的DiT图像到视频模型预训练2D角色动画模型，随后通过引入双注意力模块和相机先验将模型提升至3D，生成具有时空和视图一致性的多视角视频。最后，利用新型邻域约束4D高斯点云优化技术，生成连续稳定的4D角色表示。此外，研究者构建了大规模数据集Character4D，包含13,115个独特角色。在新基准CharacterBench上的实验表明，该方法优于现有最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 12:15:04 GMT</pubDate>
</item>
<item>
<title>视频推广攻击：文本到视频检索中的对抗性威胁</title>
<link>https://arxiv.org/abs/2508.06964</link>
<guid>https://arxiv.org/abs/2508.06964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViPro攻击提升视频在文本检索中的排名，揭示T2VR漏洞。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对文本到视频检索（T2VR）的新型对抗性攻击方法ViPro，该攻击通过提升特定视频在检索结果中的排名来实现恶意目的。与以往攻击旨在降低视频相关性不同，ViPro专注于提高视频的相关性评分，可能带来更大的信息误导和经济利益。研究引入了Modal Refinement（MoRe）技术以增强跨模态交互的细粒度理解，从而提升攻击效果。实验覆盖多个T2VR模型和数据集，在多种攻击场景下验证了ViPro的有效性，结果显示其在白盒、灰盒和黑盒设置中均优于现有基线。研究还探讨了防御策略和攻击的隐蔽性，并指出T2VR系统存在被忽视的安全风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 08:20:13 GMT</pubDate>
</item>
<item>
<title>基于时空融合的10米日地表温度估计方法研究</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WGAST模型实现高精度10米地表温度估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WGAST的弱监督生成网络，用于通过融合Terra MODIS、Landsat 8和Sentinel-2数据来估计每日10米分辨率的地表温度（LST）。该方法采用条件生成对抗网络架构，包含特征提取、融合、LST重建和噪声抑制四个阶段。实验表明，与现有方法相比，WGAST在定量和定性评估中均表现更优，平均RMSE降低17.18%，SSIM提升11.00%。此外，该模型对云层引起的LST误差具有鲁棒性，并能有效捕捉细尺度热模式，验证结果来自33个地面传感器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:49:46 GMT</pubDate>
</item>
<item>
<title>基于通用样本重放的大型语言模型持续学习方法</title>
<link>https://arxiv.org/abs/2508.04676</link>
<guid>https://arxiv.org/abs/2508.04676</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GeRe框架解决LLM持续学习中的灾难性遗忘问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在跨领域持续学习中面临的灾难性遗忘问题，提出了一种名为General Sample Replay (GeRe)的框架。该框架利用常规预训练文本进行高效的反遗忘学习，并引入基于阈值的边际损失（TM）方法，以保持激活状态的一致性。实验表明，少量固定预收集的通用样本即可同时保留模型的通用能力并提升任务性能。GeRe框架在多种回放策略下均表现出良好的效果和鲁棒性，为未来LLM的高效回放提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04676" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:42:22 GMT</pubDate>
</item>
<item>
<title>NVSpeech：面向中文的语音情感识别与合成系统</title>
<link>https://arxiv.org/abs/2508.04195</link>
<guid>https://arxiv.org/abs/2508.04195</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NVSpeech整合了语音情感识别与合成，提升自然语音交互体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NVSpeech，一个集成且可扩展的系统，用于语音情感信号（如笑声、呼吸和感叹词）的识别与合成。该系统包括数据集构建、语音识别模型和可控文本转语音模块。首先，作者创建了一个包含48,430条带标注语音的语料库，涵盖18种语音情感类别。其次，开发了能够同时识别词汇和非语言信号的ASR模型，并利用该模型自动生成大规模中文语音数据集。最后，通过微调零样本TTS模型，实现对语音情感的精确控制，从而生成更自然的人类语音。NVSpeech是首个针对普通话的开放、大规模、逐词标注的语音情感建模系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04195" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 04:25:26 GMT</pubDate>
</item>
<item>
<title>利用时间一致性提升扩散语言模型的生成质量</title>
<link>https://arxiv.org/abs/2508.09138</link>
<guid>https://arxiv.org/abs/2508.09138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过时间一致性策略提升扩散语言模型的生成准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了扩散语言模型在生成过程中出现的‘时间振荡’现象，即正确答案常出现在中间步骤但被后续去噪步骤覆盖。为解决此问题，作者提出了两种互补方法：时间自一致性投票（无需训练的解码策略）和时间一致性强化（通过时间语义熵作为奖励信号进行后训练）。实验结果表明，该方法在多个数据集上显著提升了模型性能，如Countdown数据集平均提升24.7%，GSM8K等其他数据集也有明显改进。研究揭示了扩散语言模型中时间动态的潜力，并提供了有效的工具来加以利用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>AutoCodeGen：自动化生成多语言代码生成基准数据集</title>
<link>https://arxiv.org/abs/2508.09101</link>
<guid>https://arxiv.org/abs/2508.09101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoCodeGen生成高质量多语言代码数据集，提升LLM评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出AutoCodeGen，一种无需人工标注的自动化方法，用于生成高难度多语言代码生成数据集。该方法通过LLMs生成测试输入并利用多语言沙箱获取输出，确保数据质量。基于此方法，研究团队构建了AutoCodeBench，包含3920个问题，覆盖20种编程语言，旨在评估大型语言模型在复杂、多样和实际任务中的表现。实验表明，即使最先进的模型也难以应对这些挑战。此外，还推出了AutoCodeBench-Complete，专门用于评估基础模型的少样本代码生成能力。研究希望该系列基准能推动社区关注更复杂的多语言代码生成场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.09101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 13:29:20 GMT</pubDate>
</item>
<item>
<title>提升大语言模型工具使用能力的强化学习方法</title>
<link>https://arxiv.org/abs/2508.08791</link>
<guid>https://arxiv.org/abs/2508.08791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种强化学习框架提升LLM工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在工具使用中的挑战，提出了一种自动化环境构建流程和可验证的奖励机制。该方法通过场景分解、文档生成、功能集成等步骤创建高质量训练环境，并结合轨迹数据与标准强化学习算法进行模型训练。实验表明，该方法显著提升了模型的工具使用性能，同时保持了其通用能力。分析显示，性能提升源于模型下层MLP参数更新带来的上下文理解和推理能力增强。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 12 Aug 2025 05:45:19 GMT</pubDate>
</item>
<item>
<title>基于Diffusion Transformer的电影级镜头生成方法</title>
<link>https://arxiv.org/abs/2508.08244</link>
<guid>https://arxiv.org/abs/2508.08244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Cut2Next框架提升镜头生成的叙事连贯性与电影感。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cut2Next框架，通过引入Hierarchical Multi-Prompting策略和架构创新Context-Aware Condition Injection (CACI)与Hierarchical Attention Mask (HAM)，实现符合专业剪辑模式的高质量后续镜头生成。研究构建了RawCuts和CuratedCuts数据集，并设计CutBench评估基准。实验表明，该方法在视觉一致性、文本匹配度及用户评价上均优于现有方法，尤其在叙事连贯性和电影感方面表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08244" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:56:59 GMT</pubDate>
</item>
<item>
<title>基于分层强化学习的深度搜索框架研究</title>
<link>https://arxiv.org/abs/2508.08088</link>
<guid>https://arxiv.org/abs/2508.08088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HierSearch提升多源信息检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于分层强化学习的深度搜索框架HierSearch，旨在解决传统方法在多源信息检索中的效率低和工具掌握不足的问题。该框架由本地和网络搜索代理组成，并通过规划代理协调工作。同时引入知识精炼模块以过滤错误信息，实验表明其在多个领域任务中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 11:31:47 GMT</pubDate>
</item>
<item>
<title>基于单图或文本提示的全景3D世界生成方法</title>
<link>https://arxiv.org/abs/2508.08086</link>
<guid>https://arxiv.org/abs/2508.08086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Matrix-3D框架实现广域可探索3D世界生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出Matrix-3D框架，利用全景表示实现从单张图像或文本提示生成广域、可探索的3D世界。该框架结合条件视频生成与全景3D重建技术，包含两种3D重建方法：一种是快速的前馈大全景重建模型，另一种是基于优化的高精度重建流程。为支持训练，作者构建了Matrix-Pano数据集，包含11.6万条带深度和轨迹标注的高质量全景视频序列。实验表明，该方法在全景视频生成和3D世界生成任务中均达到当前最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 11:29:57 GMT</pubDate>
</item>
<item>
<title>基于大规模强化学习的搜索代理ASearcher研究</title>
<link>https://arxiv.org/abs/2508.07976</link>
<guid>https://arxiv.org/abs/2508.07976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ASearcher提升搜索代理性能，实现长序列搜索与高效训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ASearcher的开源项目，旨在通过大规模强化学习训练搜索代理。当前开放源代码代理在处理复杂查询和长期搜索任务方面存在不足，而ASearcher通过异步强化学习方法实现了高效的长周期搜索能力。该方法利用提示式LLM生成高质量问答数据，并通过训练显著提升了搜索效果。实验结果显示，其QwQ-32B代理在xBench和GAIA基准测试中分别取得46.7%和20.8%的提升。此外，ASearcher-Web-QwQ在无需外部LLM的情况下，达到了42.1和52.8的Avg@4得分，优于现有32B开源模型。项目已开源，提供模型、数据和代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 09:36:57 GMT</pubDate>
</item>
<item>
<title>多模态深度研究代理WebWatcher的开发与评估</title>
<link>https://arxiv.org/abs/2508.05748</link>
<guid>https://arxiv.org/abs/2508.05748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebWatcher提升多模态信息检索能力，超越现有代理。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了WebWatcher，一种具备增强视觉-语言推理能力的多模态深度研究代理。该代理通过高质量合成多模态轨迹进行冷启动训练，并利用多种工具进行深度推理，同时通过强化学习提升泛化能力。为评估多模态代理的能力，作者提出了BrowseComp-VL基准测试。实验结果表明，WebWatcher在四个挑战性的VQA基准测试中显著优于现有基线、RAG工作流和开源代理，展示了其在解决复杂多模态信息检索任务方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 14:03:50 GMT</pubDate>
</item>
<item>
<title>基于空间一致性提升GUI接地任务的性能</title>
<link>https://arxiv.org/abs/2508.05615</link>
<guid>https://arxiv.org/abs/2508.05615</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过空间一致性方法提升GUI接地精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的GUI接地方法GUI-RC，利用多个预测结果的空间重叠模式作为隐式置信度信号，提升定位准确性。进一步引入GUI-RCPO，将一致性模式转化为奖励用于测试时强化学习，使模型在无标签数据上迭代优化。实验表明，该方法显著提升了Qwen2.5-VL-3B-Instruct在ScreenSpot基准上的表现，展示了测试时扩展和强化学习在GUI接地中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05615" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:54:27 GMT</pubDate>
</item>
<item>
<title>基于对比注意力引导的无掩码文本到图像生成方法</title>
<link>https://arxiv.org/abs/2508.05399</link>
<guid>https://arxiv.org/abs/2508.05399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UNCAGE提升文本到图像生成的组合准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UNCAGE的新方法，用于改进基于Masked Generative Transformers的文本到图像生成任务。该方法通过利用注意力图来优先解码明确代表单个物体的标记，从而提高组合生成的准确性。与传统扩散模型相比，UNCAGE在多个基准测试中表现出色，且推理开销极低。实验表明，UNCAGE在定量和定性评估中均有效提升了文本与图像的一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 09:51:17 GMT</pubDate>
</item>
<item>
<title>针对智能事实核查系统的新型攻击框架研究</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新型攻击框架Fact2Fiction提升虚假信息传播成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Fact2Fiction，这是一个针对基于LLM的智能事实核查系统的新型中毒攻击框架。该框架模仿系统分解复杂声明的方式，并利用系统生成的解释性理由来制造针对性的恶意证据，从而破坏子声明的验证过程。实验表明，Fact2Fiction在多种中毒预算下比现有攻击方法提高了8.9%至21.2%的攻击成功率，揭示了当前事实核查系统存在的安全漏洞，并强调了防御措施的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 02:44:57 GMT</pubDate>
</item>
<item>
<title>基于步骤熵的思维链压缩框架提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2508.03346</link>
<guid>https://arxiv.org/abs/2508.03346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过步骤熵识别冗余步骤，显著提升LLM推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于步骤熵的思维链（CoT）压缩框架，用于识别和去除大语言模型在复杂推理任务中的冗余步骤。理论分析与实验表明，约80%的低熵步骤可以被有效修剪而不显著影响最终答案的准确性。研究还引入了两阶段训练策略，结合监督微调和组相对策略优化，使模型在推理过程中自主生成压缩的思维链，从而大幅提升推理效率并保持高精度。该方法对实际部署大语言模型具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 07:48:18 GMT</pubDate>
</item>
<item>
<title>TextQuests：评估AI代理长期推理能力的新基准</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextQuests用于评估AI在复杂环境中的长期自主推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TextQuests，这是一个基于Infocom互动小说游戏的基准测试，旨在评估AI代理在需要持续、自主推理的探索性环境中表现。与传统基准不同，TextQuests不依赖外部工具，专注于评估AI在单次交互会话中进行试错学习和持续解决问题的能力。该基准适用于测试大型语言模型在长上下文中的内在推理能力，为AI研究提供了一个新的评估框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23701" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 12:22:55 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理中的研究与实践综述</title>
<link>https://arxiv.org/abs/2508.08221</link>
<guid>https://arxiv.org/abs/2508.08221</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统分析RL在LLM中的应用及优化策略。</p><br /><br /><p><strong>摘要：</strong> 本文综述了强化学习（RL）在大语言模型（LLM）推理中的最新研究进展，重点探讨了算法创新和实际应用中的关键挑战。文章指出，目前缺乏标准化的RL应用指南，且实验设置不一致导致结果差异较大。为解决这些问题，作者在统一的开源框架下进行了系统的复现与评估，通过不同难度的数据集、模型规模和架构进行深入实验，分析了各类RL技术的内部机制和适用场景。基于研究结果，文章提出了针对特定任务的RL技术选择指南，并展示了通过简单组合两种技术即可提升模型性能，优于GRPO和DAPO等方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08221" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:39:45 GMT</pubDate>
</item>
<item>
<title>视觉强化学习的最新进展与综述</title>
<link>https://arxiv.org/abs/2508.08189</link>
<guid>https://arxiv.org/abs/2508.08189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述视觉强化学习的最新发展与关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了视觉强化学习（Visual RL）领域的最新进展，涵盖了从RLHF到可验证奖励范式的策略演进，并梳理了超过200篇代表性研究，分为四个主题：多模态大语言模型、视觉生成、统一模型框架和视觉-语言-动作模型。文章分析了算法设计、奖励工程及基准进展，总结出课程驱动训练、对齐扩散和统一奖励建模等趋势。同时，文章还评估了不同层次的评估协议，并指出了样本效率、泛化能力和安全部署等开放性挑战。旨在为研究人员提供该快速扩展领域的发展图谱，并引导未来研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 13:08:55 GMT</pubDate>
</item>
<item>
<title>无需训练的精准形状编辑框架Follow-Your-Shape</title>
<link>https://arxiv.org/abs/2508.08134</link>
<guid>https://arxiv.org/abs/2508.08134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的形状编辑方法，实现高精度可控编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Follow-Your-Shape的训练-free 和 mask-free 框架，用于实现对象形状的精确和可控编辑，同时严格保留非目标区域。该方法通过计算反演与编辑轨迹之间的差异生成轨迹发散图（TDM），以精确定位可编辑区域，并引导调度键值注入机制，确保编辑的稳定性和真实性。为促进评估，作者还引入了ReShapeBench基准测试集，包含120张新图像和丰富的提示对。实验表明，该方法在大规模形状替换任务中表现出色，具有更高的编辑能力和视觉保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.08134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 12:10:00 GMT</pubDate>
</item>
<item>
<title>评估智能搜索代理在大规模信息收集中的可靠性</title>
<link>https://arxiv.org/abs/2508.07999</link>
<guid>https://arxiv.org/abs/2508.07999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前搜索代理在大规模信息收集中表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WideSearch基准，用于评估基于大型语言模型的搜索代理在大规模信息收集任务中的可靠性。该基准包含200个来自15个不同领域的手动整理问题，涵盖中英文。每个任务要求代理收集可逐项验证的信息并整理成结构化输出。尽管多个人工测试者在充足时间下几乎可以100%完成任务，但现有系统整体成功率接近0%，最佳表现仅5%。这表明当前搜索代理在处理大规模信息任务时存在显著不足，亟需进一步研究与改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 10:03:09 GMT</pubDate>
</item>
<item>
<title>Omni-Effects：统一的视觉特效生成框架</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Omni-Effects实现多特效空间可控生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出Omni-Effects，一个能够生成提示引导和空间可控复合特效的统一框架。该框架通过LoRA-MoE技术整合多种特效并减少任务干扰，并利用Spatial-Aware Prompt实现精准的空间控制。此外，引入IIF模块防止特效间意外融合。研究构建了Omni-VFX数据集并设计专用评估框架，实验表明该方法在空间控制和特效多样性上表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 09:41:24 GMT</pubDate>
</item>
<item>
<title>Action Reasoning Models: 提升机器人感知与行动的结构化推理方法</title>
<link>https://arxiv.org/abs/2508.07917</link>
<guid>https://arxiv.org/abs/2508.07917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARMs通过结构化推理提升机器人任务适应性与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Action Reasoning Models (ARMs)，这是一种将感知、规划和控制结合的视觉-语言-动作模型。MolmoAct作为该类模型的代表，能够通过深度感知令牌编码观察和指令，生成可编辑的空间计划，并预测精确的低级动作，从而实现可解释和可控的行为。在多个模拟和现实任务中，MolmoAct表现出色，包括高零样本准确率、长任务成功率以及任务进展提升。此外，研究团队发布了MolmoAct数据集，包含超过10,000条高质量机器人轨迹，有助于提升模型的泛化能力。该研究为构建基于结构化推理的机器人基础模型提供了开放蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 08:32:45 GMT</pubDate>
</item>
<item>
<title>Grove MoE：一种动态可扩展的专家混合架构</title>
<link>https://arxiv.org/abs/2508.07785</link>
<guid>https://arxiv.org/abs/2508.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Grove MoE通过动态激活不同规模专家提升模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Grove MoE，这是一种基于异构专家设计的新型混合专家（MoE）架构，旨在克服传统MoE中专家规模固定、计算效率受限的问题。Grove MoE引入了动态激活机制和不同规模的专家，提升了模型的扩展性和计算效率。基于该架构，研究团队开发了GroveMoE-Base和GroveMoE-Inst两款33B参数的语言模型，通过中段训练和后期训练策略优化，实现了在不同输入复杂度下动态激活3.14-3.28B参数，并达到与更大规模开源模型相当的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 05:15:36 GMT</pubDate>
</item>
<item>
<title>GLiClass：一种高效的序列分类方法</title>
<link>https://arxiv.org/abs/2508.07662</link>
<guid>https://arxiv.org/abs/2508.07662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLiClass在保持高精度的同时提升分类效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GLiClass的新方法，用于序列分类任务。该方法基于GLiNER架构，结合了嵌入方法的高效性与零样本学习的灵活性，适用于动态变化的分类需求。同时，研究还引入了PPO算法用于多标签文本分类，在数据稀疏或依赖人类反馈的情况下表现良好。相比传统方法，GLiClass在准确性和计算效率上均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 02:22:25 GMT</pubDate>
</item>
<item>
<title>Klear-Reasoner：具备强大推理能力的模型及其优化方法</title>
<link>https://arxiv.org/abs/2508.07629</link>
<guid>https://arxiv.org/abs/2508.07629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Klear-Reasoner在多个基准测试中表现优异，优化了推理训练流程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Klear-Reasoner，一个具备强大推理能力的模型，在数学和编程任务中表现出色。文章详细分析了其训练流程，包括数据准备、长链式思维监督微调（long CoT SFT）和强化学习（RL）。研究发现，少量高质量数据比大量多样化数据更有效，且困难样本无需精度过滤即可获得更好结果。针对当前RL中的裁剪机制问题，提出了GPPO方法，增强了模型探索能力和负样本学习效率。Klear-Reasoner在AIME 2024、AIME 2025、LiveCodeBench V5和V6上分别取得90.5%、83.2%、66.0%和58.1%的高分。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 11 Aug 2025 01:17:51 GMT</pubDate>
</item>
<item>
<title>多语言文档视觉检索基准VisR-Bench的引入与评估</title>
<link>https://arxiv.org/abs/2508.07493</link>
<guid>https://arxiv.org/abs/2508.07493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisR-Bench支持多语言文档的视觉检索，提升跨语言信息获取能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VisR-Bench，一个面向长文档的多语言视觉检索基准。该基准包含超过35,000个高质量问答对，覆盖1,200篇文档和16种语言，涵盖图表、文本和表格三种问题类型。与以往数据集不同，VisR-Bench包含无显式答案的查询，以防止模型依赖关键词匹配。研究评估了多种检索模型，发现尽管多模态大语言模型（MLLMs）表现优于传统方法，但在处理结构化表格和低资源语言时仍存在挑战，揭示了多语言视觉检索的关键难点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 17:44:43 GMT</pubDate>
</item>
<item>
<title>自进化智能代理系统的研究综述</title>
<link>https://arxiv.org/abs/2508.07407</link>
<guid>https://arxiv.org/abs/2508.07407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究自进化智能代理系统的设计与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文综述了自进化智能代理系统的研究进展，提出了一种统一的概念框架，涵盖系统输入、代理系统、环境和优化器四个核心组件。文章系统回顾了针对不同组件的自进化技术，并探讨了在生物医学、编程和金融等特定领域中的演化策略。此外，还讨论了评估、安全性和伦理问题，为构建更适应、自主和持续进化的智能代理系统提供了理论基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 10 Aug 2025 12:07:32 GMT</pubDate>
</item>
<item>
<title>LessIsMore：一种高效的稀疏注意力机制提升推理模型性能</title>
<link>https://arxiv.org/abs/2508.07101</link>
<guid>https://arxiv.org/abs/2508.07101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LessIsMore通过全局注意力模式提升推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LessIsMore的训练-free 稀疏注意力机制，旨在解决大型推理模型在处理短输入提示时因过多token生成导致的计算开销问题。该方法不依赖传统的头部特定局部优化，而是利用全局注意力模式，结合局部注意力头和近期上下文信息进行统一的跨头token排序，从而提高泛化能力和效率。实验表明，LessIsMore在多个推理任务中保持甚至提升了准确率，同时相比全注意力机制实现了1.1倍的解码速度提升，并且仅需处理2倍更少的token，整体端到端速度提升1.13倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 17:10:33 GMT</pubDate>
</item>
<item>
<title>基于推理的列表排序模型ReasonRank的优化与性能提升</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonRank通过推理训练提升列表排序性能，效果优于现有基线。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大型语言模型的列表排序方法ReasonRank，旨在通过增强推理能力提升排序效果。研究首先构建了一个自动化的推理密集型训练数据生成框架，利用DeepSeek-R1生成高质量标签，并通过自一致性过滤机制确保数据质量。随后，采用两阶段微调策略，包括监督学习和强化学习，以提升模型的推理与排序能力。实验表明，ReasonRank在多个任务中表现优异，尤其在BRIGHT基准测试中达到SOTA水平，同时具有更低的延迟。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.07050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 13:26:18 GMT</pubDate>
</item>
<item>
<title>通过数据过滤提升开放权重AI系统的安全性</title>
<link>https://arxiv.org/abs/2508.06601</link>
<guid>https://arxiv.org/abs/2508.06601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据过滤可有效增强开放权重AI模型的抗攻击能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过从训练数据中过滤与双重用途相关的文本，以减少开放权重AI系统中的有害能力。研究提出了一种多阶段的数据过滤流程，并展示了其在降低生物威胁代理知识方面的有效性。实验表明，经过过滤的模型对多达10,000步和300M标记的生物威胁文本攻击表现出显著的抵抗力，优于现有后训练方法。尽管过滤模型内部不包含危险知识，但它们仍可能利用上下文中的信息，因此需要多层次防御策略。该研究为开放权重AI系统的安全防护提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>BrowseComp-Plus：提升深度研究系统评估的基准测试</title>
<link>https://arxiv.org/abs/2508.06600</link>
<guid>https://arxiv.org/abs/2508.06600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入BrowseComp-Plus基准提升深度研究系统评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出BrowseComp-Plus，一个基于原有BrowseComp的改进基准，采用固定且精心筛选的文档语料库，以解决现有评估方法在公平性和透明性方面的不足。该基准包含人工验证的支持文档和挑战性负样本，支持可控实验。实验表明，GPT-5搭配Qwen3-Embedding-8B检索器可达到70.1%的准确率，优于其他模型。此基准有助于深入分析检索效果、引用准确性和上下文工程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:55:11 GMT</pubDate>
</item>
<item>
<title>研究通用机器人策略的泛化能力与快捷学习问题</title>
<link>https://arxiv.org/abs/2508.06426</link>
<guid>https://arxiv.org/abs/2508.06426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示快捷学习影响机器人策略泛化，提出数据增强解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于大规模数据集（如Open X-Embodiment）训练的通用机器人策略在任务泛化上的局限性。研究发现，快捷学习——即依赖任务无关特征——是导致泛化能力不足的关键因素。通过理论和实证分析，作者指出两个主要诱因：子数据集内部多样性不足以及子数据集间的分布差异，导致数据集碎片化。这些问题是由于大型数据集通常由不同环境和设备独立收集而产生的。研究提出了改进数据收集策略以减少快捷学习、提升泛化能力的方法，并在无法获取新数据的情况下，验证了数据增强策略的有效性，从而改善了机器人策略在模拟和现实环境中的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:14:01 GMT</pubDate>
</item>
<item>
<title>Temporal Self-Rewarding Language Models提升模型生成能力</title>
<link>https://arxiv.org/abs/2508.06026</link>
<guid>https://arxiv.org/abs/2508.06026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过时间协调机制提升语言模型生成质量与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种改进的Self-Rewarding语言模型架构，通过协调过去、现在和未来的模型生成来维持学习信号。该方法引入了两个关键机制：锚定拒绝（Anchored Rejection）和未来引导选择（Future-Guided Chosen），有效解决了传统Self-Rewarding模型中对比样本表征差异缩小的问题。实验表明，在多个模型家族和不同规模下，该方法显著优于现有Self-Rewarding方法，尤其在数学推理、知识问答和代码生成任务中表现出更强的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 01:25:54 GMT</pubDate>
</item>
<item>
<title>Bifrost-1：融合多模态大模型与扩散模型的高效图像生成框架</title>
<link>https://arxiv.org/abs/2508.05954</link>
<guid>https://arxiv.org/abs/2508.05954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bifrost-1实现高效图像生成，保留多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Bifrost-1，一个将预训练多模态大语言模型（MLLM）与扩散模型结合的统一框架。该框架利用CLIP的补丁级图像嵌入作为潜在变量，通过轻量级ControlNet适配扩散模型，同时为MLLM添加视觉生成分支以保持其多模态推理能力。实验表明，Bifrost-1在视觉保真度和多模态理解上表现优异，且训练计算成本显著降低。研究还进行了全面消融实验，验证了设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 22:38:47 GMT</pubDate>
</item>
<item>
<title>OmniEAR：评估语言模型在具身任务中的推理能力</title>
<link>https://arxiv.org/abs/2508.05614</link>
<guid>https://arxiv.org/abs/2508.05614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型在具身推理中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniEAR框架，用于评估语言模型在物理交互、工具使用和多智能体协作方面的具身推理能力。与以往依赖预设工具集或明确指令的基准不同，OmniEAR要求智能体动态获取能力并自主制定协作策略。通过文本环境建模，研究覆盖了1500个家庭和工业场景。实验发现，当模型需要从约束中推理时，性能显著下降，尤其在隐式协作和复合任务中表现不佳。即使提供完整环境信息，模型仍无法有效过滤无关约束，表明其在具身推理方面存在根本性挑战。该研究为推进具身AI系统提供了重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:54:15 GMT</pubDate>
</item>
<item>
<title>SONAR-LLM：基于连续SONAR嵌入空间的生成模型</title>
<link>https://arxiv.org/abs/2508.05305</link>
<guid>https://arxiv.org/abs/2508.05305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SONAR-LLM在连续嵌入空间中生成文本，提升生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SONAR-LLM，这是一种基于连续SONAR嵌入空间的解码器模型，通过冻结的SONAR解码器传播token级交叉熵进行监督训练。该模型结合了LCM的语义抽象优势，并去除了扩散采样器，恢复了基于似然的训练信号。实验表明，SONAR-LLM在39M到1.3B参数规模下均表现出色，文章还提供了完整的训练代码和预训练检查点以促进研究复现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 08:03:44 GMT</pubDate>
</item>
<item>
<title>基于MoBE的专家混合模型压缩方法研究</title>
<link>https://arxiv.org/abs/2508.05257</link>
<guid>https://arxiv.org/abs/2508.05257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoBE实现高效模型压缩，保持高精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的Mixture-of-Basis-Experts (MoBE) 方法，用于压缩大型专家混合（MoE）语言模型。该方法通过将每个专家的权重矩阵分解为两个部分，其中较大的矩阵通过共享的基础矩阵线性组合进行重参数化，从而在减少参数数量的同时保持较高的模型精度。实验表明，MoBE在多个大规模模型上实现了显著的压缩效果，参数量减少了24%-30%，而准确率下降仅为1%-2%。此方法为部署大规模MoE模型提供了有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 06:48:24 GMT</pubDate>
</item>
<item>
<title>数学表达式语音转录的挑战与新数据集的提出</title>
<link>https://arxiv.org/abs/2508.03542</link>
<guid>https://arxiv.org/abs/2508.03542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个大规模数学语音数据集，提升方程转录准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了将口语数学表达转换为结构化符号表示的挑战，特别是在自动语音识别和语言模型领域。尽管已有进展，但数学表达的语音转录仍存在诸多问题。为此，作者提出了首个完全开源的大规模数据集，包含66,000个英语和俄语数学方程及句子的音频样本。研究还测试了多种模型，包括ASR后校正模型和音频语言模型，在MathSpeech和S2L-equations基准上取得了显著成果。此外，作者还建立了首个数学句子识别基准，并在方程转录任务中达到了40%的字符错误率。该工作为多模态AI在数学内容识别方面的进一步发展奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 11:11:37 GMT</pubDate>
</item>
<item>
<title>音频攻击框架WhisperInject可操控先进语音语言模型生成有害内容</title>
<link>https://arxiv.org/abs/2508.03365</link>
<guid>https://arxiv.org/abs/2508.03365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WhisperInject通过隐蔽音频扰动操控AI生成有害内容，成功率超86%</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型日益融入日常生活，音频成为人机交互的重要接口。然而，这也带来了新的安全风险。本文提出WhisperInject，一种两阶段的对抗性音频攻击框架，能够使先进的语音语言模型生成有害内容。该方法通过人类难以察觉的音频扰动实现攻击，在第一阶段使用基于奖励的优化方法RL-PGD引导目标模型绕过自身安全协议，生成有害响应；第二阶段则将这些有害内容嵌入到正常的音频载体中，如天气查询或问候信息。实验在StrongREJECT、LlamaGuard和人工评估等严格安全框架下验证，成功率达到86%以上，展示了音频攻击对AI行为的潜在威胁。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 08:14:01 GMT</pubDate>
</item>
<item>
<title>UserBench：评估语言模型协作能力的新基准</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLM在用户协作中存在显著差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UserBench，一个以用户为中心的基准测试，用于评估大型语言模型在多轮、偏好驱动交互中的表现。该基准模拟了用户从模糊目标逐步表达偏好的过程，要求模型主动澄清意图并做出合理决策。评估结果显示，即使是最先进的模型，在与用户对齐方面也存在明显不足，仅20%的时间能完全满足用户意图，且仅能发现30%的用户偏好。这表明当前模型更多是任务执行者而非真正的协作伙伴，UserBench为提升这一能力提供了重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:34:12 GMT</pubDate>
</item>
<item>
<title>VLM4D：评估视觉语言模型时空推理能力的基准</title>
<link>https://arxiv.org/abs/2508.02095</link>
<guid>https://arxiv.org/abs/2508.02095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLM4D基准揭示了视觉语言模型在时空推理上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLM4D，这是首个专门用于评估视觉语言模型（VLMs）时空推理能力的基准。该基准包含真实世界和合成视频，并配有强调平移、旋转、视角意识和运动连续性的问答对。通过评估最新的开源和闭源VLMs，研究发现其性能与人类基准存在显著差距，尤其是在整合多视觉线索和保持时间连贯性方面。文章还探讨了利用4D特征场重建和针对性时空微调等方法，以提升模型的时空理解能力，旨在推动更强大可靠的动态环境视觉智能发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 02:06:06 GMT</pubDate>
</item>
<item>
<title>Transformer模型中大规模激活的动态演化分析</title>
<link>https://arxiv.org/abs/2508.03616</link>
<guid>https://arxiv.org/abs/2508.03616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了Transformer模型中大规模激活的数学规律及预测方法。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统分析了Transformer模型在训练过程中大规模激活的演变规律，使用Pythia模型家族进行实验。研究发现，大规模激活的出现遵循可预测的数学模式，可用一个五参数的指数调制对数函数进行建模。作者开发了一个机器学习框架，能够仅根据模型结构预测这些参数，对于稳定性和训练周期等具有重要影响。该成果为模型设计提供了新视角，有助于提前预测和控制大规模激活的出现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:29:51 GMT</pubDate>
</item>
<item>
<title>Lightswitch：基于多视角与材质信息的高效3D光照重渲染方法</title>
<link>https://arxiv.org/abs/2508.06494</link>
<guid>https://arxiv.org/abs/2508.06494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lightswitch通过多视角和材质信息提升3D光照重渲染效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lightswitch，一种基于微调材料-光照扩散框架的新方法，能够在保留物体结构的同时，根据输入图像和多视角数据高效地将任意数量的图像转换为目标光照条件。该方法结合了推断出的固有属性和可扩展去噪方案，显著提升了不同材质物体的光照重渲染质量。实验表明，Lightswitch在合成和真实物体的光照重渲染任务中表现优于现有最先进的方法，并能在短时间内完成高质量渲染。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>GLM-4.5：高性能开源混合专家大语言模型</title>
<link>https://arxiv.org/abs/2508.06471</link>
<guid>https://arxiv.org/abs/2508.06471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.5在多项任务中表现优异，参数量少于竞品。</p><br /><br /><p><strong>摘要：</strong> GLM-4.5是一款开源的混合专家（MoE）大语言模型，拥有355B总参数和32B激活参数，支持思考与直接响应模式。通过多阶段训练和后训练优化，它在代理、推理和编码任务中表现出色，在TAU-Bench、AIME 24和SWE-bench Verified测试中分别获得70.1%、91.0%和64.2%的分数。相比其他模型，GLM-4.5参数更少但性能领先，排名第三，并在代理基准中位列第二。项目提供了完整代码和模型，包括一个轻量版GLM-4.5-Air（106B参数），旨在推动推理和代理AI系统的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 13:21:06 GMT</pubDate>
</item>
<item>
<title>基于可学习程序记忆的智能代理研究</title>
<link>https://arxiv.org/abs/2508.06433</link>
<guid>https://arxiv.org/abs/2508.06433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Memp机制提升智能代理的程序记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何为智能代理赋予可学习、可更新和持续演进的程序记忆。作者提出Memp方法，将代理的历史轨迹提炼为细粒度指令和高层次抽象，并研究了构建、检索和更新程序记忆的不同策略。结合动态更新机制，该记忆库能与新经验同步演化。实验表明，随着记忆库的优化，代理在类似任务中的成功率和效率显著提升。此外，从强模型中提取的程序记忆对弱模型也有显著性能提升作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.06433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Aug 2025 12:20:56 GMT</pubDate>
</item>
<item>
<title>无监督视觉语言模型适应方法综述</title>
<link>https://arxiv.org/abs/2508.05547</link>
<guid>https://arxiv.org/abs/2508.05547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述无监督VLM适应方法，分类并分析不同数据场景下的策略。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了无监督视觉语言模型（VLM）适应方法，旨在提升其在特定任务中的表现。文章提出了一种基于未标记视觉数据可用性和性质的分类框架，将现有方法分为四类：无数据迁移、无监督领域迁移、周期性测试时适应和在线测试时适应。通过对每种范式的深入分析，本文总结了核心方法与策略，并回顾了多个应用场景的基准测试，指出了当前研究的挑战与未来方向。相关文献资源可在GitHub仓库中获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 12:27:37 GMT</pubDate>
</item>
<item>
<title>操作系统代理研究综述：从基础到未来方向</title>
<link>https://arxiv.org/abs/2508.04482</link>
<guid>https://arxiv.org/abs/2508.04482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述操作系统代理的发展与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文全面回顾了基于操作系统的智能代理（OS Agents）的研究进展，涵盖了其核心组件、构建方法、评估标准以及当前面临的挑战。文章介绍了环境、观察空间和动作空间等关键要素，并探讨了领域特定基础模型和代理框架的构建方法。同时，对评估协议和基准测试进行了详细分析，指出了安全隐私、个性化与自我进化等未来研究方向。该综述旨在为学术界和工业界提供参考，推动相关技术发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 10:33:45 GMT</pubDate>
</item>
<item>
<title>GENIE：结合NeRF与高斯点云的交互式3D场景编辑方法</title>
<link>https://arxiv.org/abs/2508.02831</link>
<guid>https://arxiv.org/abs/2508.02831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GENIE融合NeRF与高斯点云实现高效交互式3D场景编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出GENIE，一种结合神经辐射场（NeRF）与高斯点云（GS）的混合模型，旨在提升3D场景的可编辑性与实时渲染能力。GENIE通过为每个高斯分布分配可训练特征嵌入，并利用最近邻高斯点对NeRF进行条件建模，实现高效的场景编辑。引入基于光线追踪的最近邻搜索算法RT-GPS和多分辨率哈希网格，提升了计算效率与局部感知能力。该方法在保持NeRF高质量渲染的同时，支持实时交互与物理模拟，推动了神经渲染与几何编辑的融合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 14:59:23 GMT</pubDate>
</item>
<item>
<title>MeshLLM：基于大语言模型的3D网格文本序列化框架</title>
<link>https://arxiv.org/abs/2508.01242</link>
<guid>https://arxiv.org/abs/2508.01242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MeshLLM提升3D网格文本生成与理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MeshLLM，一种利用大语言模型处理文本序列化3D网格的新框架。针对现有方法在数据规模和结构信息丢失方面的不足，MeshLLM引入了Primitive-Mesh分解策略，构建了一个包含150万+样本的大规模数据集，并通过顶点推断面连接和局部网格组装训练策略，显著提升了模型对网格拓扑和空间结构的理解能力。实验表明，MeshLLM在网格生成质量和形状理解方面优于现有最佳方法LLaMA-Mesh，展示了其在3D网格处理领域的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 03:37:37 GMT</pubDate>
</item>
<item>
<title>UI-AGILE框架提升GUI代理性能</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-AGILE提升GUI代理的推理与训练效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了UI-AGILE框架，旨在解决现有GUI代理在推理设计、奖励机制和视觉噪声方面的不足。该框架通过改进监督微调过程，包括连续奖励函数、简单思考奖励和基于裁剪的重采样策略，提升训练效果；同时提出分解选择接地方法，显著提高高分辨率屏幕上的接地准确性。实验表明，UI-AGILE在ScreenSpot-Pro和ScreenSpot-v2两个基准测试中表现最佳，其中在ScreenSpot-Pro上接地准确率提升了23%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:22:07 GMT</pubDate>
</item>
<item>
<title>基于锚点和意外度的代码推理压缩方法研究</title>
<link>https://arxiv.org/abs/2508.05988</link>
<guid>https://arxiv.org/abs/2508.05988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ASAP方法提升代码推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在代码推理中因推理过程过长导致的训练成本高、推理延迟大等问题，提出了一种名为ASAP（Anchor-guided, Surprisal-based Pruning）的新型粗到细的CoT压缩框架。该方法通过锚点引导的剪枝保留核心推理结构，并利用首次词意外度指标选择逻辑关键步骤，最终使模型能够在推理时自动生成简洁的CoT。实验表明，ASAP在多个代码生成基准测试中取得了最先进的准确率，同时显著降低了训练和推理成本，在LiveCodeBench v4_v5基准上，相比最强基线减少了23.5%的token生成量和43.5%的推理延迟，Pass@1准确率达到36.19%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 23:46:21 GMT</pubDate>
</item>
<item>
<title>基于AEPO的多模态大模型GUI语义对齐研究</title>
<link>https://arxiv.org/abs/2508.05731</link>
<guid>https://arxiv.org/abs/2508.05731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AEPO提升多模态模型GUI语义对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了多模态大语言模型在图形用户界面中进行自然语言指令理解的挑战，特别是空间和语义对齐问题。尽管RLVR在空间对齐上表现良好，但其探索效率限制了语义对齐的学习。为此，作者提出了自适应探索策略优化（AEPO），通过多答案生成和理论驱动的自适应探索奖励函数，显著提升了模型在多个GUI基准测试中的性能，相对RLVR基线提高了高达9.0%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:49:56 GMT</pubDate>
</item>
<item>
<title>提升低资源语言多模态大模型性能的研究</title>
<link>https://arxiv.org/abs/2508.05502</link>
<guid>https://arxiv.org/abs/2508.05502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出双源策略提升低资源语言多模态模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在低资源语言中的表现不足问题，提出通过增强语言能力和文化背景知识来提升模型效果。研究强调了文化意识的重要性，并引入MELLA数据集，该数据集结合了本地网络的替代文本和多模态生成的描述，有效提升了八种语言的模型表现。实验结果表明，模型在语言理解和文化适应方面均有显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 11:36:24 GMT</pubDate>
</item>
<item>
<title>Voost：一种统一且可扩展的虚拟试穿与脱下框架</title>
<link>https://arxiv.org/abs/2508.04825</link>
<guid>https://arxiv.org/abs/2508.04825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voost通过联合学习提升虚拟试穿效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出Voost，一个统一且可扩展的框架，通过单个扩散变压器联合学习虚拟试穿和试下任务。该方法通过同时建模两个任务，使每个服装-人体对相互监督，支持灵活的生成方向和服装类别条件，无需特定任务网络或额外标签。此外，引入了两种推理阶段技术：注意力温度缩放以提高对分辨率或掩码变化的鲁棒性，以及自校正采样利用任务间的双向一致性。实验表明，Voost在试穿和试下基准测试中均达到最先进水平，表现出更高的对齐精度、视觉真实性和泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 15:10:58 GMT</pubDate>
</item>
<item>
<title>提升长文本事实性推理的在线强化学习方法</title>
<link>https://arxiv.org/abs/2508.05618</link>
<guid>https://arxiv.org/abs/2508.05618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新奖励函数提升R-LLM的事实性推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对推理型大语言模型（R-LLMs）在长文本事实性任务中容易产生幻觉的问题，提出了一种结合事实准确性、回答细节度和相关性的新型奖励函数，并通过在线强化学习提升模型表现。实验表明，该方法在六个长文本事实性基准测试中，平均减少了23.1个百分点的幻觉率，提升了23%的回答细节度，同时保持了整体回答的有用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>基于注意力机制的文档重排序方法AttnRank提升大语言模型性能</title>
<link>https://arxiv.org/abs/2508.05128</link>
<guid>https://arxiv.org/abs/2508.05128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AttnRank通过优化输入顺序提升大模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究发现大语言模型对输入信息的位置高度敏感，存在注意力盆地现象，即模型更关注序列开头和结尾的信息。为解决这一问题，作者提出AttnRank方法，通过小样本校准集估计模型的注意力偏好，并重新排列输入内容以提升关键信息的可见性。该方法无需修改模型参数或训练流程，适用于多种大语言模型，在多跳问答和少样本上下文学习任务中均取得显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 04:08:08 GMT</pubDate>
</item>
<item>
<title>MLLMSeg：一种高效且精确的参考表达分割框架</title>
<link>https://arxiv.org/abs/2508.04107</link>
<guid>https://arxiv.org/abs/2508.04107</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLLMSeg在保持高精度的同时降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MLLMSeg的新框架，用于参考表达分割（RES）。该框架充分利用多模态大模型（MLLM）视觉编码器中的内在视觉细节特征，无需额外引入视觉编码器。同时，设计了一个细节增强且语义一致的特征融合模块（DSFF），将视觉细节特征与大语言模型（LLM）的语义特征进行融合。此外，还构建了一个仅含34M参数的轻量级掩码解码器，有效结合视觉和语义信息以实现精准的掩码预测。实验表明，该方法在性能和成本之间取得了更好的平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04107" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 02:06:52 GMT</pubDate>
</item>
<item>
<title>SODEC：一种高效的单步扩散图像压缩模型</title>
<link>https://arxiv.org/abs/2508.04979</link>
<guid>https://arxiv.org/abs/2508.04979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SODEC提升图像压缩效率与质量，解码速度提高20倍。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SODEC的新型单步扩散图像压缩模型，旨在解决现有方法在解码延迟和图像保真度方面的不足。SODEC通过预训练的VAE生成高信息量的潜在表示，避免多步去噪过程，并引入保真引导模块以提升输出图像的准确性。此外，采用率退火训练策略以支持极低比特率下的有效训练。实验结果表明，SODEC在速率-失真-感知性能上优于现有方法，且解码速度提升了20倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 22:24:03 GMT</pubDate>
</item>
<item>
<title>MACT：多智能体协作框架提升文档理解与视觉问答性能</title>
<link>https://arxiv.org/abs/2508.03404</link>
<guid>https://arxiv.org/abs/2508.03404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MACT框架通过多智能体协作提升文档理解与复杂推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出MACT（Multi-Agent Collaboration with Test-Time scaling）框架，用于改进视觉文档理解和视觉问答任务。该框架由四个小型智能体组成，分别负责规划、执行、判断和回答，具备明确分工和高效协作机制。其中，判断智能体专门验证答案正确性并引导修正，提升了模型的自我纠错能力。此外，MACT引入混合奖励建模和智能体级测试时缩放策略，以优化个体能力和整体协作。实验表明，MACT在多个基准测试中表现优异，尤其在长视觉上下文和复杂推理任务中领先，且参数规模较小，不影响通用性和数学任务表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 08:52:09 GMT</pubDate>
</item>
<item>
<title>基于多模态协同反思的实体链接框架研究</title>
<link>https://arxiv.org/abs/2508.02243</link>
<guid>https://arxiv.org/abs/2508.02243</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多模态实体链接框架，提升链接准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态实体链接任务中图像数据冗余和视觉特征一次性提取的问题，提出了一种基于大语言模型的框架——Intra- and Inter-modal Collaborative Reflections。该框架优先利用文本信息进行实体链接，在文本不足以确定正确实体时，通过多轮迭代策略整合图像中的关键视觉线索，从而提升匹配精度。实验结果表明，该框架在三个公开数据集上均优于现有方法，分别提升了3.2%、5.1%和1.6%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02243" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 05:43:54 GMT</pubDate>
</item>
<item>
<title>Genie Envisioner：统一的机器人操作基础平台</title>
<link>https://arxiv.org/abs/2508.05635</link>
<guid>https://arxiv.org/abs/2508.05635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Genie Envisioner整合策略学习、评估与模拟，提升机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> Genie Envisioner（GE）是一个统一的世界基础平台，用于机器人操作，集成了策略学习、评估和模拟在一个视频生成框架中。其核心是GE-Base，一个大规模指令条件视频扩散模型，能够捕捉现实世界机器人交互的空间、时间和语义动态。GE-Act通过轻量级流匹配解码器将潜在表示映射到可执行的动作轨迹，实现跨多种机器人的精确策略推理。GE-Sim作为动作条件神经模拟器，支持可扩展的评估和训练。此外，平台还配备了EWMBench基准套件，用于衡量视觉保真度、物理一致性和指令-动作对齐。该平台为指令驱动的通用具身智能提供了可扩展且实用的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的隐私保护个人身份信息去标识化研究</title>
<link>https://arxiv.org/abs/2508.05545</link>
<guid>https://arxiv.org/abs/2508.05545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在PII去标识化中表现优异，提供高效隐私保护方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLM）在无结构文本中去除个人身份信息（PII）的应用。相比传统规则系统和领域特定的命名实体识别模型，LLM在上下文理解方面表现出更强的适应性。研究分析了不同架构和训练策略对PII去标识化效果的影响，并评估了其在去标识化准确性、语义保留和PII泄露方面的表现。研究结果为构建高效且隐私友好的LLM去标识系统提供了实用指导。同时，作者发布了PRvL开源工具套件，支持多种推理设置，便于实际部署和定制化应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 12:22:49 GMT</pubDate>
</item>
<item>
<title>InfiAlign：一种高效且可扩展的大型语言模型后训练框架</title>
<link>https://arxiv.org/abs/2508.05496</link>
<guid>https://arxiv.org/abs/2508.05496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiAlign提升LLM推理能力，减少数据需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfiAlign，一种结合监督微调（SFT）和直接偏好优化（DPO）的后训练框架，旨在提高大型语言模型（LLMs）的推理能力。该框架通过多维质量指标自动筛选高质量对齐数据，显著降低数据需求并提升性能。在Qwen2.5-Math-7B-Base模型上的实验表明，其性能与DeepSeek-R1-Distill-Qwen-7B相当，仅使用约12%的训练数据，并在数学推理任务中取得3.89%的平均提升。结果表明，结合系统化数据选择与全阶段后训练是实现高效、可扩展模型对齐的有效方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 11:34:06 GMT</pubDate>
</item>
<item>
<title>DeepPHY：评估视觉语言模型物理推理能力的新基准</title>
<link>https://arxiv.org/abs/2508.05405</link>
<guid>https://arxiv.org/abs/2508.05405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在物理推理任务中表现不足，DeepPHY提供新评估框架。</p><br /><br /><p><strong>摘要：</strong> 尽管视觉语言模型（VLMs）在感知和视觉推理方面表现出色，但在复杂动态环境中仍存在细节关注不足和精确动作规划能力差的问题。现实任务需要高级空间推理、长期规划和持续策略优化，通常依赖对物理规则的理解。然而，真实场景评估成本高昂。为此，研究者提出了DeepPHY，一个系统评估VLMs物理原理理解与推理能力的基准框架，包含多种难度的模拟环境和细粒度评估指标。实验表明，即使最先进的VLMs也难以将描述性物理知识转化为精确的预测控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 09:58:19 GMT</pubDate>
</item>
<item>
<title>基于REINA的实时语音翻译系统优化研究</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REINA提升实时语音翻译质量与延迟平衡，实现SOTA效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为REINA（Regularized Entropy INformation Adaptation）的新损失函数，用于优化实时语音翻译（SimulST）系统在翻译质量和延迟之间的权衡。该方法基于信息论原理，通过智能决策是否等待更多输入来提升翻译效率。实验表明，REINA在法语、西班牙语和德语到英语的翻译任务中表现优异，即使仅使用开源或合成数据也能达到最先进的流式翻译效果。此外，作者引入了一种新的流式效率度量标准，验证了REINA相比现有方法在延迟与质量平衡上提升了21%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 20:25:58 GMT</pubDate>
</item>
<item>
<title>评估大语言模型对语言标志的敏感性基准研究</title>
<link>https://arxiv.org/abs/2508.04939</link>
<guid>https://arxiv.org/abs/2508.04939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型对特定语言模式的不公平评分现象。</p><br /><br /><p><strong>摘要：</strong> 本文提出一个全面的基准，用于评估大语言模型（LLMs）对语言标志的反应，这些标志可能无意中暴露性别、社会阶层或地域背景等人口特征。通过100组经过验证的问答对模拟访谈，研究发现LLMs会系统性地惩罚某些语言模式，尤其是模糊表达，尽管内容质量相同。该基准生成受控的语言变化，保持语义一致，从而精确测量自动化评估系统中的偏见。研究在多个语言维度上验证了方法，结果显示模糊回应平均评分低25.6%，并证明了该基准在识别模型特定偏见方面的有效性。这项工作为检测和衡量AI系统中的语言歧视提供了基础框架，具有广泛的应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 19:51:03 GMT</pubDate>
</item>
<item>
<title>RPCANet++：融合RPCA与深度网络的稀疏目标分割框架</title>
<link>https://arxiv.org/abs/2508.04190</link>
<guid>https://arxiv.org/abs/2508.04190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RPCANet++提升稀疏目标分割性能并增强可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RPCANet++，一种结合鲁棒主成分分析（RPCA）与深度学习架构的稀疏目标分割框架。该方法通过背景近似模块、目标提取模块和图像恢复模块实现高效分割，引入记忆增强模块和深度对比先验模块以提升特征保留与目标提取效率。实验表明，RPCANet++在多种数据集上表现优异，并通过可视化和数值测量增强模型可解释性，为可靠且可解释的稀疏目标分割提供新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 04:19:37 GMT</pubDate>
</item>
<item>
<title>大型多模态模型输入审查能力评估研究</title>
<link>https://arxiv.org/abs/2508.04017</link>
<guid>https://arxiv.org/abs/2508.04017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨LMMs对错误输入的主动检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型多模态模型（LMMs）在处理错误输入时的表现进行研究，提出输入审查能力评估框架ISEval，涵盖七类错误前提和三项评估指标。通过对十种先进LMMs的测试发现，大多数模型缺乏主动识别文本错误的能力，依赖明确提示才能发现问题。不同类型的错误对模型表现影响显著，逻辑谬误识别较好，但语言表面错误和条件性错误较难识别。同时，不同模型在视觉与文本信息的信任度上存在差异。研究强调了提升LMMs主动验证输入有效性的必要性，并为相关问题的解决提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:13:46 GMT</pubDate>
</item>
<item>
<title>结合GUI与编程的多智能体系统提升计算机自动化效率</title>
<link>https://arxiv.org/abs/2508.03923</link>
<guid>https://arxiv.org/abs/2508.03923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoAct-1通过融合GUI操作与编程实现高效任务处理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CoAct-1的新型多智能体系统，将基于GUI的操作与直接编程执行相结合，以提高复杂任务的自动化效率。该系统由一个协调器动态分配子任务给GUI操作员或程序员代理，后者可编写并执行Python或Bash脚本。这种方法在文件管理和数据处理等任务中显著提升了效率，减少了操作步骤。在OSWorld基准测试中，CoAct-1取得了60.76%的成功率，优于现有方法，并将平均任务完成步骤数降至10.15，展现出更强大、高效和可扩展的计算机自动化路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 17:33:36 GMT</pubDate>
</item>
<item>
<title>Double-Bench：多模态文档RAG系统的全面评估框架</title>
<link>https://arxiv.org/abs/2508.03644</link>
<guid>https://arxiv.org/abs/2508.03644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Double-Bench提供多语言、多模态的文档RAG系统评估。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Double-Bench，一个大规模、多语言、多模态的文档RAG系统评估框架。该框架包含3,276份文档和5,168个单跳与多跳查询，覆盖6种语言和4种文档类型，并支持动态更新以应对数据污染问题。所有查询均基于详尽扫描的证据页面并通过人工验证，确保高质量和完整性。实验表明，文本与视觉嵌入模型之间的差距正在缩小，同时揭示了现有RAG框架在缺乏证据时仍过度自信的问题。作者希望Double-Bench能为未来高级文档RAG研究提供坚实基础，并计划每年更新语料库并发布新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:55:02 GMT</pubDate>
</item>
<item>
<title>高效推理方法在大型推理模型中的研究进展</title>
<link>https://arxiv.org/abs/2508.02120</link>
<guid>https://arxiv.org/abs/2508.02120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨了LRMs中高效推理方法的最新进展。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）因其在处理复杂任务上的卓越表现而成为研究热点。DeepSeek R1因其优异性能和开源特性受到广泛关注，推动了R1风格LRMs的发展。这些模型通过引入长链式思维和自我反思机制，提升了逻辑推理和决策能力。然而，随着应用的广泛，过度思考问题逐渐显现，表现为生成答案时构建冗长且重复的推理路径，影响效率与准确性。为此，研究者提出了多种高效推理方法，旨在减少推理路径长度而不牺牲性能。本文系统回顾了相关研究，将现有工作分为单模型优化和模型协作两大方向，并维护了一个公开的GitHub仓库以跟踪最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 02:54:31 GMT</pubDate>
</item>
<item>
<title>多模态语音合成系统Marco-Voice：语音克隆与情感控制的统一框架</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Marco-Voice实现语音克隆与情感控制的统一，提升语音自然度与表现力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多模态语音合成系统Marco-Voice，该系统整合了语音克隆和情感控制功能，旨在解决语音生成中表达性、可控性和自然度不足的问题。系统引入了有效的说话人-情感解耦机制和批次内对比学习方法，实现了对说话人身份和情感风格的独立操控，并采用旋转情感嵌入集成方法以实现平滑的情感控制。为了支持全面训练与评估，研究者构建了CSEMOTIONS数据集，包含6位专业演讲者7种情绪下的10小时中文语音。实验结果表明，Marco-Voice在客观和主观指标上均有显著提升，展现出优秀的语音清晰度和情感丰富性，代表了语音合成领域的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:08:22 GMT</pubDate>
</item>
<item>
<title>基于草图的逼真发丝生成模型</title>
<link>https://arxiv.org/abs/2508.01650</link>
<guid>https://arxiv.org/abs/2508.01650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于草图的发丝生成模型，提升生成精度与用户友好性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于草图的逼真发丝生成模型，解决了传统文本或图像输入在精度和用户友好性方面的不足。该模型通过两项创新技术：可学习的发丝上采样策略和多尺度自适应条件机制，有效处理复杂的发丝交互和多样化的草图模式。实验表明，该方法在多个基准数据集上优于现有方法，在真实感和精度方面表现优异。相关代码将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 04:17:50 GMT</pubDate>
</item>
<item>
<title>MOSEv2：推动视频目标分割向真实场景演进的挑战性数据集</title>
<link>https://arxiv.org/abs/2508.05630</link>
<guid>https://arxiv.org/abs/2508.05630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOSEv2提升视频目标分割在复杂现实场景中的性能挑战。</p><br /><br /><p><strong>摘要：</strong> MOSEv2是一个比MOSEv1更具挑战性的视频目标分割数据集，旨在推动该技术在更真实的环境中发展。它包含5,024个视频和超过70万张高质量的标注掩码，覆盖200个类别共10,074个物体。相比前一版本，MOSEv2引入了更多复杂场景，如频繁的物体消失与重现、严重遮挡、小物体、恶劣天气、低光环境、多镜头序列等。实验表明，多个先进VOS方法在MOSEv2上的性能显著下降，显示出当前模型在面对真实世界复杂性时的不足。该数据集已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>动态微调提升大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2508.05629</link>
<guid>https://arxiv.org/abs/2508.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态微调提升大语言模型在多个基准测试中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为动态微调（DFT）的方法，用于改进监督微调（SFT）在大语言模型中的表现。通过数学分析发现，标准SFT的梯度隐含了限制模型泛化能力的奖励结构。DFT通过动态调整目标函数来稳定每个token的梯度更新，显著提升了模型在多个挑战性基准和基础模型上的性能，并在离线强化学习设置中表现出色。该方法结合理论洞察与实际应用，为SFT提供了一个更高效且简单的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>Hi3DEval：面向3D生成内容的层次化评估框架</title>
<link>https://arxiv.org/abs/2508.05609</link>
<guid>https://arxiv.org/abs/2508.05609</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Hi3DEval框架，提升3D内容质量评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D内容生成质量评估的挑战，提出了Hi3DEval框架，该框架结合对象级和部件级评估，实现多维度的全面分析。同时，扩展了纹理评估范围，关注材质真实性。为支持该框架，构建了Hi3DBench数据集，并设计了基于混合3D表示的自动化评分系统。实验表明，该方法优于现有图像基指标，更符合人类偏好。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05609" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>R-Zero：一种自进化大型语言模型框架</title>
<link>https://arxiv.org/abs/2508.05004</link>
<guid>https://arxiv.org/abs/2508.05004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R-Zero通过自生成数据实现LLM自主进化与能力提升。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为R-Zero的自进化大型语言模型框架，该框架无需依赖人工标注任务即可自主生成训练数据。R-Zero通过两个独立模型——Challenger和Solver之间的互动进行优化，Challenger提出挑战性任务，Solver则逐步解决这些任务，从而形成一个自我改进的训练循环。实验表明，R-Zero显著提升了多种基础LLM的推理能力，例如在数学推理和通用领域推理基准测试中分别提升了6.49和7.54个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.05004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 23:38:16 GMT</pubDate>
</item>
<item>
<title>探索语言模型在多跳问答任务中的推理失败</title>
<link>https://arxiv.org/abs/2508.04699</link>
<guid>https://arxiv.org/abs/2508.04699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型在多跳问答中的推理错误模式。</p><br /><br /><p><strong>摘要：</strong> 本文系统研究了当前语言模型在多跳问答任务中的推理失败问题，提出了一种新的错误分类框架，从源文档的多样性与独特性、相关信息的覆盖程度以及认知效率三个维度分析模型的不足。通过人工标注和自动化指标的结合，研究发现了被传统评估方式掩盖的复杂错误模式，为提升语言模型的推理准确性、透明度和鲁棒性提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04699" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:58:36 GMT</pubDate>
</item>
<item>
<title>构建结构化客户服务对话框架与数据集提升客服质量</title>
<link>https://arxiv.org/abs/2508.04423</link>
<guid>https://arxiv.org/abs/2508.04423</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建CSC框架和CSConv数据集提升客服策略响应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Customer Support Conversation (CSC)任务，旨在训练客服人员使用明确的支持策略进行对话。基于COPC指南，定义了五个对话阶段和十二种策略，并构建了CSConv数据集，包含1,855条经过LLM重写并标注的客户服务对话。同时开发了RoleCS训练数据集，通过LLM模拟策略丰富的对话场景。实验表明，在RoleCS上微调大型语言模型可显著提升其生成符合策略的高质量回复能力，并通过人工评估验证了问题解决效果的提升。所有代码和数据将公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04423" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 09:11:17 GMT</pubDate>
</item>
<item>
<title>提升大语言模型在福祉解释中的质量与适配性</title>
<link>https://arxiv.org/abs/2508.03990</link>
<guid>https://arxiv.org/abs/2508.03990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了LLM在福祉解释中的表现并优化其质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在解释福祉概念方面的表现，构建了一个包含43,880条解释的大规模数据集。研究引入了一种基于原则的LLM评价框架，并通过监督微调和直接偏好优化方法提升了模型生成解释的质量。结果显示，LLM的解释质量因模型、受众和类别而异，且经过微调的模型在性能上优于未微调的模型，证明了基于偏好的学习在特定解释任务中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 20:45:02 GMT</pubDate>
</item>
<item>
<title>HarmonyGuard：多智能体协作框架提升网络代理的安全与效率</title>
<link>https://arxiv.org/abs/2508.04010</link>
<guid>https://arxiv.org/abs/2508.04010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HarmonyGuard提升网络代理任务完成率与安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出HarmonyGuard，一个用于提升网络代理安全性和任务完成率的多智能体协作框架。该框架包含两个核心能力：自适应策略增强和双目标优化。Policy Agent自动提取并维护结构化安全策略，以应对不断变化的网络威胁；Utility Agent通过马尔可夫实时推理评估安全与效率，并利用元认知能力进行优化。实验表明，HarmonyGuard在多个基准测试中显著优于现有基线，任务完成率提高20%，策略合规性提升38%以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 21:49:32 GMT</pubDate>
</item>
<item>
<title>机器学习模型全生命周期中的偏见治理与公平性评估</title>
<link>https://arxiv.org/abs/2508.03970</link>
<guid>https://arxiv.org/abs/2508.03970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨机器学习模型全生命周期中的偏见治理与公平性评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统介绍了在机器学习模型的整个生命周期中，如何有效治理、评估和量化偏见。基于作者在大型语言模型偏见评估测试套件（BEATS）方面的基础工作，文章分析了大型语言模型中存在的常见偏见与公平性问题，并提出了数据与人工智能治理框架，以应对模型中的偏见、伦理、公平性和事实性问题。该治理方法适用于实际应用场景，可实现模型部署前的严格基准测试、实时持续评估以及生成内容的主动管理。通过在整个AI开发生命周期中实施数据与AI治理，组织可以显著提升生成式AI系统的安全性与责任性，有效降低歧视风险并保护品牌声誉。文章旨在推动社会负责任且符合伦理的生成式人工智能应用的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03970" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 19:15:31 GMT</pubDate>
</item>
<item>
<title>基于DiffSemanticFusion的自动驾驶场景理解与轨迹预测方法</title>
<link>https://arxiv.org/abs/2508.01778</link>
<guid>https://arxiv.org/abs/2508.01778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiffSemanticFusion框架提升自动驾驶场景理解与轨迹预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DiffSemanticFusion的多模态轨迹预测与规划融合框架，旨在提升自动驾驶中的场景理解能力。该框架结合了栅格化表示和图结构表示的优势，通过地图扩散模块增强在线高精度地图的稳定性和表达能力。实验结果表明，在nuScenes和NAVSIM数据集上，该方法在轨迹预测和端到端自动驾驶任务中均取得了优于现有方法的性能。特别是在nuScenes数据集上，与QCNet结合后提升了5.1%，在NAVSIM的NavHard场景中提升了15%。此外，消融实验显示该模块可无缝集成到其他向量基方法中以进一步提升性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:32:05 GMT</pubDate>
</item>
<item>
<title>DPoser-X：基于扩散模型的全身人体姿态生成方法</title>
<link>https://arxiv.org/abs/2508.00599</link>
<guid>https://arxiv.org/abs/2508.00599</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DPoser-X通过扩散模型实现全身人体姿态建模，性能优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出DPoser-X，一种基于扩散模型的全身人体姿态先验模型。针对人体姿态建模的复杂性和高质量数据稀缺问题，作者引入了扩散模型作为姿态先验，并通过改进的时间步调度方法和掩码训练机制提升模型性能。该方法统一处理多种姿态相关任务，有效捕捉身体各部分之间的依赖关系，避免过拟合特定动作。实验表明，DPoser-X在多个基准测试中表现优异，为全身人体姿态建模设立了新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00599" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 08:56:39 GMT</pubDate>
</item>
<item>
<title>FACTORY：一种用于评估模型事实准确性的大型人工验证基准</title>
<link>https://arxiv.org/abs/2508.00109</link>
<guid>https://arxiv.org/abs/2508.00109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FACTORY提升模型生成内容的事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FACTORY，一个基于人工验证的大规模提示集，用于评估语言模型生成内容的事实准确性。与现有基准相比，FACTORY更具挑战性，实验表明约40%的顶级模型输出存在不实信息，而其他数据集仅为10%。研究强调了FACTORY在可靠性方面的优势，并指出模型需具备处理长尾事实的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 15:00:11 GMT</pubDate>
</item>
<item>
<title>提升数学命题自动形式化的ThinkingF方法</title>
<link>https://arxiv.org/abs/2508.04440</link>
<guid>https://arxiv.org/abs/2508.04440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkingF提升数学命题自动形式化准确率。</p><br /><br /><p><strong>摘要：</strong> 本文提出ThinkingF方法，旨在提升自然语言数学命题到形式语言的自动转换准确性。研究指出，有效的自动形式化需要两个关键能力：对形式语言领域知识的全面掌握以及自然语言问题理解与非形式-形式对齐的推理能力。为解决现有方法准确率低的问题，作者构建了两个数据集，并通过SFT和RLVR训练进一步融合这两种能力。最终模型在FormalMATH-Lite和ProverBench任务中取得了最先进的成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 09:28:22 GMT</pubDate>
</item>
<item>
<title>提升大模型指令遵循能力的框架与实验验证</title>
<link>https://arxiv.org/abs/2508.03178</link>
<guid>https://arxiv.org/abs/2508.03178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出框架提升模型指令遵循能力，效果显著。</p><br /><br /><p><strong>摘要：</strong> 尽管大语言模型在数学问题、编码任务和通用谜题上的推理能力有所提升，但其在准确遵循复杂指令方面仍存在不足。本文指出，推理过程中的‘懒惰推理’是导致指令遵循不佳的主要原因。为此，作者提出了一种全面框架，通过预览与自检机制增强推理严谨性。该框架包括生成复杂约束指令、过滤得到有效提示集、利用拒绝采样构建高质量数据集，并结合熵保持的监督微调与基于规则密集奖励的强化学习策略，提升模型的推理能力。实验表明，该方法在多个基准测试中表现优异，其中Light-IF-32B模型超越了多个大型开源和闭源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:42:00 GMT</pubDate>
</item>
<item>
<title>持续学习的3D异常检测框架C3D-AD</title>
<link>https://arxiv.org/abs/2508.01311</link>
<guid>https://arxiv.org/abs/2508.01311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出C3D-AD框架，实现多类别3D点云的持续学习与异常检测。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为C3D-AD的持续学习框架，用于3D异常检测。该框架能够学习多类别点云的通用表示，并处理随时间出现的新类别。在特征提取模块中引入了KAL机制以提取通用局部特征；在数据重建过程中采用KAA机制，既能学习新类别信息，又能丢弃冗余旧信息；最后通过RPP模块保持任务间的表示一致性。实验结果表明，该方法在三个公开数据集上均取得了优异性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 06:54:55 GMT</pubDate>
</item>
<item>
<title>Sel3DCraft：提升文本到3D生成的视觉提示工程系统</title>
<link>https://arxiv.org/abs/2508.00428</link>
<guid>https://arxiv.org/abs/2508.00428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sel3DCraft优化3D生成流程，提升设计创意效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Sel3DCraft，一个用于文本到3D生成的视觉提示工程系统。该系统通过引入双分支结构、多视角混合评分方法和提示驱动的可视化分析工具，解决了传统3D生成中因盲目的提示过程导致的结果不可预测问题。实验和用户研究显示，Sel3DCraft在支持设计师创意方面优于其他3D生成系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 04:36:15 GMT</pubDate>
</item>
<item>
<title>SEAgent：通过自主学习提升计算机使用代理的性能</title>
<link>https://arxiv.org/abs/2508.04700</link>
<guid>https://arxiv.org/abs/2508.04700</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEAgent使计算机使用代理能自主学习新软件，提升任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SEAgent框架，旨在让计算机使用代理（CUAs）通过自主探索和经验学习掌握新软件。该框架包含世界状态模型和课程生成器，支持逐步复杂化的任务训练，并结合对抗性模仿失败动作与群体相对策略优化提升代理性能。此外，采用专家到通用的训练策略整合多个专家代理的经验，最终实现超越单一专家代理的通用代理系统。实验表明，SEAgent在五个新软件环境中成功率达到34.5%，比UI-TARS提升了23.2%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04700" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:58:46 GMT</pubDate>
</item>
<item>
<title>通过主动上下文管理提升大语言模型的长文本处理能力</title>
<link>https://arxiv.org/abs/2508.04664</link>
<guid>https://arxiv.org/abs/2508.04664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">主动上下文管理框架Sculptor提升LLM在长文本任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Sculptor的框架，旨在通过主动上下文管理（ACM）工具提升大语言模型（LLMs）在处理长上下文时的性能。Sculptor包含三种工具：上下文分段、摘要与恢复以及智能搜索，帮助LLMs更有效地管理注意力和工作记忆，减少前向干扰的影响。实验表明，该方法在无需额外训练的情况下显著提升了模型在信息稀疏基准测试中的表现，证明了其在长文本任务中的有效性。研究强调，明确的上下文控制策略比单纯扩大token窗口更为关键。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:32:58 GMT</pubDate>
</item>
<item>
<title>基于IFDecorator的强化学习框架提升指令遵循能力</title>
<link>https://arxiv.org/abs/2508.04632</link>
<guid>https://arxiv.org/abs/2508.04632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IFDecorator提升LLM指令遵循准确性并减少奖励黑客行为。</p><br /><br /><p><strong>摘要：</strong> 本文提出Instruction Following Decorator (IFDecorator) 框架，旨在解决强化学习中因难度评估不足导致的训练效率低下和过优化问题。该框架包含三个组件：协同对抗数据飞轮生成更难的指令-验证对、IntentCheck模块确保意图对齐、以及通过陷阱指令检测奖励黑客行为。实验表明，Qwen2.5-32B-Instruct-IFDecorator在IFEval上达到87.43%的准确率，优于更大模型如GPT-4o，并在FollowBench任务中表现出色，同时有效降低奖励黑客率。相关模型、代码和数据将公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 13:00:54 GMT</pubDate>
</item>
<item>
<title>AI学术会议的可持续转型：社区联邦会议模型的提出</title>
<link>https://arxiv.org/abs/2508.04586</link>
<guid>https://arxiv.org/abs/2508.04586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI会议面临多方面危机，提出社区联邦会议模型以促进可持续发展。</p><br /><br /><p><strong>摘要：</strong> 本文指出人工智能学术会议在科学、环境、心理和物流等方面面临严重压力，如作者发表率翻倍、碳排放高、负面情绪普遍及参会人数超负荷。这些挑战威胁了会议的核心目标。为应对这些问题，文章提出了社区联邦会议（CFC）模型，将同行评审、展示和社交活动分拆为全球协调但本地组织的形式，旨在实现更可持续、包容和有韧性的AI研究交流方式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 12:08:27 GMT</pubDate>
</item>
<item>
<title>EvoC2Rust：一种用于C到Rust项目级转换的自动化框架</title>
<link>https://arxiv.org/abs/2508.04295</link>
<guid>https://arxiv.org/abs/2508.04295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvoC2Rust提升C代码迁移到Rust的准确性和安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为EvoC2Rust的自动化框架，用于将整个C代码库转换为等效的Rust代码。该框架采用基于骨架的翻译策略，分为三个进化阶段：首先分解C项目并生成可编译的Rust骨架；其次逐步翻译函数；最后通过集成LLM和静态分析修复编译错误。实验表明，EvoC2Rust在语法和语义准确性上分别比基于LLM的方法高出17.24%和14.32%，代码安全性也优于基于规则的工具。在工业项目中，其模块级编译通过率为92.25%，测试通过率为89.53%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 06:31:23 GMT</pubDate>
</item>
<item>
<title>基于VL-DAC的视觉语言模型强化学习方法研究</title>
<link>https://arxiv.org/abs/2508.04280</link>
<guid>https://arxiv.org/abs/2508.04280</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VL-DAC提升VLM在真实场景中的任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Vision-Language Decoupled Actor-Critic (VL-DAC) 的轻量级、无需超参数调整的强化学习算法，用于训练视觉语言模型（VLMs）。该方法通过将动作标记的PPO更新与环境步级的价值学习解耦，提升了训练效率和稳定性。实验表明，在多个低成本模拟器中训练的VLM能够有效泛化到真实场景任务，如游戏控制、空间规划和网页导航，且不损害图像理解能力。这是首个证明简单RL算法可在合成环境中训练VLM并提升实际任务表现的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04280" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 06:08:48 GMT</pubDate>
</item>
<item>
<title>VeriGUI：推动长链GUI任务的可验证数据集研究</title>
<link>https://arxiv.org/abs/2508.04026</link>
<guid>https://arxiv.org/abs/2508.04026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VeriGUI提升GUI代理处理长链任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VeriGUI，一个用于训练和评估通用GUI代理的可验证长链任务数据集。该数据集强调两个关键维度：长链复杂性与子任务级可验证性，支持任务分解为数百步的相互依赖子任务，并允许每个子任务作为起点。数据集涵盖桌面和网页环境，由专家标注。实验表明，现有代理在处理长链任务时存在显著性能差距，凸显了对更强大规划与决策能力的需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.04026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 22:38:18 GMT</pubDate>
</item>
<item>
<title>MiDashengLM：一种基于开放数据的高效音频语言模型</title>
<link>https://arxiv.org/abs/2508.03983</link>
<guid>https://arxiv.org/abs/2508.03983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiDashengLM通过开放数据实现高效音频理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MiDashengLM，一个基于开放数据源的音频语言模型，旨在提升音频理解的效率与全面性。该模型利用自建的ACAVCaps训练数据集，结合开源音频编码器Dasheng，实现了对语音、声音和音乐信息的统一文本表示。相比以往主要依赖自动语音识别（ASR）的方法，MiDashengLM更注重通用音频描述，提升了复杂音频场景的处理能力。实验表明，MiDashengLM在首次令牌时间（TTFT）和吞吐量方面分别比同类模型快4倍和20倍。模型检查点已在Hugging Face和GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 20:30:19 GMT</pubDate>
</item>
<item>
<title>Sotopia-RL：提升大语言模型社会智能的强化学习框架</title>
<link>https://arxiv.org/abs/2508.03905</link>
<guid>https://arxiv.org/abs/2508.03905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sotopia-RL通过多维奖励机制提升社会智能模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Sotopia-RL，一种用于训练具有社会智能的大语言模型的强化学习框架。该框架解决了传统强化学习在社交任务中的两大挑战：部分可观测性和多维行为影响。通过将粗粒度的回合级反馈细化为话语级别的多维奖励，Sotopia-RL有效提升了模型在社交任务中的表现。实验表明，该方法在Sotopia基准测试中取得了领先成绩，显著优于现有方法。消融实验进一步验证了话语级信用分配和多维奖励设计的重要性。代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 16:43:42 GMT</pubDate>
</item>
<item>
<title>Agent Lightning：一种用于强化学习训练大型语言模型的灵活框架</title>
<link>https://arxiv.org/abs/2508.03680</link>
<guid>https://arxiv.org/abs/2508.03680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent Lightning实现RL与AI代理的解耦训练，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> Agent Lightning是一种灵活且可扩展的框架，允许通过强化学习（RL）训练大型语言模型（LLMs）以适应任何AI代理。该框架实现了代理执行与训练的完全解耦，支持与多种现有代理系统的无缝集成，几乎无需代码修改。通过将代理执行建模为马尔可夫决策过程，Agent Lightning定义了一个统一的数据接口，并提出了一种分层RL算法LightningRL，包含信用分配模块，能够将任意代理生成的轨迹分解为训练转换。这使得RL能够处理复杂的交互逻辑，如多代理场景和动态工作流。系统设计引入了Training-Agent Disaggregation架构，并集成了代理可观测性框架，提供标准化的代理微调接口。实验表明，该框架在文本到SQL、检索增强生成和数学工具使用任务中表现出稳定且持续的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>HPSv3：提升文本生成图像质量的新评估方法</title>
<link>https://arxiv.org/abs/2508.03789</link>
<guid>https://arxiv.org/abs/2508.03789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HPSv3提升文本生成图像质量，支持高效评估与优化。</p><br /><br /><p><strong>摘要：</strong> 本文提出Human Preference Score v3 (HPSv3)，用于更准确地评估文本到图像生成模型。HPSv3包含1.08M文本-图像对和1.17M人工偏好比较数据，结合基于视觉语言模型的偏好模型和不确定性感知排序损失函数，实现细粒度图像排名。此外，文章还引入Chain-of-Human-Preference (CoHP) 方法，通过迭代优化提升图像质量，无需额外数据。实验表明，HPSv3在广泛场景下具有良好的评估性能，CoHP可有效提升生成图像质量。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:17:13 GMT</pubDate>
</item>
<item>
<title>基于布局思维的网页设计代码转换方法研究</title>
<link>https://arxiv.org/abs/2508.03560</link>
<guid>https://arxiv.org/abs/2508.03560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LaTCoder提升网页设计代码生成的布局准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LaTCoder方法，通过引入布局思维（Layout-as-Thought）来增强网页设计到代码转换中的布局保留能力。该方法将网页设计划分为图像块，并采用基于链式思维的提示策略为每个块生成代码，再通过绝对定位和MLLM方法进行组装。实验结果显示，使用DeepSeek-VL2模型时，TreeBLEU分数提升了66.67%，MAE下降了38%。人工评估也表明，超过60%的情况下，LaTCoder生成的网页更受青睐，验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03560" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 11:28:48 GMT</pubDate>
</item>
<item>
<title>强化学习在大型语言模型软件工程任务中的应用</title>
<link>https://arxiv.org/abs/2508.03501</link>
<guid>https://arxiv.org/abs/2508.03501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL提升LLM在软件工程任务中的成功率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习（RL）应用于大型语言模型（LLMs）以解决实际软件工程任务。与以往专注于单轮问题的研究不同，本文关注需要多轮交互的复杂环境，并使用改进的DAPO算法训练基于Qwen2.5-72B-Instruct的代理。实验结果显示，该方法在SWE-bench Verified基准测试中将成功率从20%提升至39%，并在SWE-rebench上达到或超过领先开源模型的表现，展示了利用开放模型构建高效自主代理的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 10:30:47 GMT</pubDate>
</item>
<item>
<title>基于文本控制的音乐修复与母带处理模型SonicMaster</title>
<link>https://arxiv.org/abs/2508.03448</link>
<guid>https://arxiv.org/abs/2508.03448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SonicMaster可统一修复多种音频问题并提升音质。</p><br /><br /><p><strong>摘要：</strong> 本文提出SonicMaster，首个基于文本控制的音乐修复与母带处理统一生成模型。该模型能有效解决非专业环境中常见的音频质量问题，如混响过强、失真、频段不平衡等。通过自然语言指令进行定向增强或自动处理，SonicMaster在大量配对的劣化与高质量音频数据上进行训练，采用流匹配生成训练范式，实现从劣化输入到优化输出的映射。实验结果显示，SonicMaster在客观音质指标和主观听感测试中均优于原始音频，验证了其在音频修复领域的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 09:49:04 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的多毒性预测框架CoTox</title>
<link>https://arxiv.org/abs/2508.03159</link>
<guid>https://arxiv.org/abs/2508.03159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoTox利用大语言模型提升药物毒性预测准确性与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CoTox的新框架，结合大语言模型（LLM）和链式思维（CoT）推理，用于多毒性预测。该框架整合化学结构数据、生物通路和基因本体（GO）术语，通过逐步推理生成可解释的毒性预测结果。实验表明，CoTox在GPT-4o等模型上表现优于传统机器学习和深度学习方法。研究还发现，使用IUPAC命名表示化学结构能增强模型的推理能力。此外，通过模拟药物对相关细胞的作用，CoTox能够生成与生理反应一致的毒性预测，展示了其在药物开发中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:04:44 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的视频虚拟试穿技术DreamVVT</title>
<link>https://arxiv.org/abs/2508.02807</link>
<guid>https://arxiv.org/abs/2508.02807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamVVT提升视频虚拟试穿的细节和时序一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DreamVVT的两阶段视频虚拟试穿框架，利用扩散Transformer（DiTs）和预训练模型的优势，有效解决现有方法在缺乏成对数据、细节保留和时序一致性方面的不足。第一阶段通过多帧试穿模型生成高质量关键帧图像，第二阶段结合骨骼图、运动描述和关键帧图像进行视频生成，显著提升了动态效果和真实感。实验表明，DreamVVT在实际场景中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 14:27:55 GMT</pubDate>
</item>
<item>
<title>LeanK：一种基于学习的KV缓存剪枝方法提升大语言模型效率</title>
<link>https://arxiv.org/abs/2508.02215</link>
<guid>https://arxiv.org/abs/2508.02215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LeanK通过剪枝KV缓存提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出LeanK，一种基于学习的KV缓存剪枝方法，通过静态通道稀疏性减少不重要的键缓存通道，从而降低GPU内存占用并加速解码过程。该方法采用两阶段训练策略，生成满足特定稀疏率和硬件适配要求的通道掩码。实验表明，LeanK可实现高达70%的K缓存和16%-18%的V缓存内存减少，并通过自定义解码内核实现1.3倍的注意力计算加速。文章还分析了长上下文推理过程中模型通道和注意力头的重要性分布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 05:08:43 GMT</pubDate>
</item>
<item>
<title>基于查询的U-Net架构IAUNet在生物医学实例分割中的应用</title>
<link>https://arxiv.org/abs/2508.01928</link>
<guid>https://arxiv.org/abs/2508.01928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IAUNet提升细胞实例分割性能，优于现有模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于查询的U-Net架构IAUNet，用于生物医学图像中的实例分割。该模型结合了完整的U-Net结构和轻量级像素解码器，提高了效率并减少了参数数量。同时引入了Transformer解码器以多尺度优化对象特征。研究还发布了2025 Revvity全细胞分割数据集，为生物医学实例分割提供了新基准。实验表明，IAUNet在多个公开数据集和自建数据集上均优于当前最先进的全卷积、基于Transformer和基于查询的模型，为细胞实例分割任务设定了新的基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 17:36:20 GMT</pubDate>
</item>
<item>
<title>基于多模态知识的网络代理框架与推理系统</title>
<link>https://arxiv.org/abs/2508.01858</link>
<guid>https://arxiv.org/abs/2508.01858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Web-CogKnowledge框架提升网络代理认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大模型对网络代理发展的推动作用，强调网络代理需要先获取充分知识才能进行有效推理。作者将网络代理的能力分解为知识学习和认知过程两个阶段，并提出Web-CogKnowledge框架，将知识分为事实性、概念性和程序性。通过构建Web-CogDataset数据集，实现对网络代理的基础知识训练，并开发出基于知识驱动的Chain-of-Thought（CoT）推理框架Web-CogReasoner。实验表明该模型在未见过的任务中表现优异，同时引入Web-CogBench评估体系以全面衡量代理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 13:17:52 GMT</pubDate>
</item>
<item>
<title>OpenMed NER：高效且开源的医学实体识别模型</title>
<link>https://arxiv.org/abs/2508.01630</link>
<guid>https://arxiv.org/abs/2508.01630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenMed NER在多个生物医学NER基准上取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenMed NER，一个基于Transformer的开源医学实体识别模型。该模型通过轻量级领域自适应预训练（DAPT）和低秩适配（LoRA）技术，在350,000条临床笔记和科研文献语料上进行训练，并在12个生物医学NER基准测试中取得了显著提升，尤其在疾病、化学物质、基因和细胞系等实体类型上表现突出。其训练效率高，仅需单块GPU在12小时内完成，碳排放极低，且提供宽松许可协议，有助于符合欧盟AI法案等法规要求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 03:33:28 GMT</pubDate>
</item>
<item>
<title>3D占用定位基准与GroundingOcc模型研究</title>
<link>https://arxiv.org/abs/2508.01197</link>
<guid>https://arxiv.org/abs/2508.01197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出3D占用定位任务及GroundingOcc模型，提升自动驾驶感知精度。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统视觉定位依赖边界框导致细节不足的问题，提出3D占用定位任务，并基于nuScenes数据集构建了包含体素级占用标注的基准。同时，设计了GroundingOcc模型，通过多模态学习融合视觉、文本和点云信息，实现从粗到细的物体位置与占用预测。模型包含多模态编码器、占用头、定位头以及2D定位和深度估计模块，实验表明其在3D占用定位任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 01:05:50 GMT</pubDate>
</item>
<item>
<title>CoT推理的脆弱性：数据分布视角下的分析</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT推理在分布外数据中表现不稳定，揭示其局限性。</p><br /><br /><p><strong>摘要：</strong> 本文从数据分布的角度研究了Chain-of-Thought (CoT) 推理的有效性。作者发现，尽管CoT能生成类似人类的推理步骤，但其效果受限于训练数据与测试数据之间的分布差异。通过构建DataAlchemy环境，研究者系统地分析了任务、长度和格式对CoT推理的影响，结果表明CoT推理在超出训练分布的情况下会失效，显示出其表面性与脆弱性。该研究为理解CoT推理的局限性提供了新的视角，并强调了实现真正通用推理的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01191" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 00:37:28 GMT</pubDate>
</item>
<item>
<title>RL-PLUS：突破大语言模型能力边界的新方法</title>
<link>https://arxiv.org/abs/2508.00222</link>
<guid>https://arxiv.org/abs/2508.00222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL-PLUS提升LLM推理能力，突破原有边界。</p><br /><br /><p><strong>摘要：</strong> 本文提出RL-PLUS，一种结合内部探索与外部数据的混合策略优化方法，旨在解决传统强化学习（RLVR）在大语言模型（LLM）中因策略局限和奖励稀疏性导致的能力边界问题。该方法通过多重重要性采样和基于探索的优势函数，有效提升模型的推理能力，并在多个数学推理基准和分布外任务中取得显著性能提升。实验表明，RL-PLUS在六项基准测试中达到最先进水平，平均相对提升达69.2%，并有效缓解了能力边界崩溃现象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 19:55:29 GMT</pubDate>
</item>
<item>
<title>基于视频生成动态4D内容的新框架</title>
<link>https://arxiv.org/abs/2507.23785</link>
<guid>https://arxiv.org/abs/2507.23785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种从视频生成高质量动态3D内容的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的视频到4D生成框架，能够从单个视频输入中创建高质量的动态3D内容。由于直接进行4D扩散建模面临数据构建成本高和高维表示的挑战，作者提出了一种Direct 4DMesh-to-GS Variation Field VAE，可直接从3D动画数据中编码规范的高斯点（GS）及其时间变化，并将其压缩到紧凑的潜在空间中。在此基础上，训练了一个基于时间感知扩散Transformer的高斯变化场扩散模型，该模型在Objaverse数据集上进行了训练，表现出优于现有方法的生成质量，并且在真实视频输入上也展现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>文本到图像扩散模型中的内容与风格表示研究</title>
<link>https://arxiv.org/abs/2507.23313</link>
<guid>https://arxiv.org/abs/2507.23313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究文本到图像模型如何区分内容与风格。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于Transformer的文本到图像扩散模型在生成艺术作品时如何编码内容和风格概念。通过使用交叉注意力热图，研究人员能够将生成图像中的像素与特定提示词相关联，从而区分内容描述词和风格描述词的影响区域。研究发现，模型在不同艺术提示和风格下表现出不同程度的内容与风格分离，内容词主要影响物体区域，而风格词则影响背景和纹理区域，表明模型可能在没有明确监督的情况下自发理解内容与风格的区别。研究结果有助于理解大规模生成模型如何内部表示复杂的艺术概念，并提供了代码和数据集以供进一步探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 03:47:01 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的移动网络根因分析框架</title>
<link>https://arxiv.org/abs/2507.21974</link>
<guid>https://arxiv.org/abs/2507.21974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用LLM进行移动网络根因分析，提升可解释性和准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种轻量级框架，利用大语言模型（LLMs）进行移动网络的根因分析（RCA）。研究引入了TeleLogs数据集，用于评估LLMs在RCA任务中的表现。实验发现现有开源模型在此任务中表现不佳，表明需要领域适配。为此，作者提出了一种两阶段训练方法，结合监督微调和强化学习，以提高模型的推理能力和诊断准确性。该方法整合领域知识，生成结构化、多步骤的诊断解释，显著提升了模型性能，并在多个LLM规模上验证了其有效性。结果表明，经过领域适配的LLM在可解释性和实用性方面具有广泛应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 12:21:42 GMT</pubDate>
</item>
<item>
<title>高效代理框架的研究与优化</title>
<link>https://arxiv.org/abs/2508.02694</link>
<guid>https://arxiv.org/abs/2508.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究代理系统的效率与性能平衡，提出高效代理框架。</p><br /><br /><p><strong>摘要：</strong> 本文系统研究了大型语言模型驱动的代理系统在效率与性能之间的权衡问题，探讨了任务复杂性、模块扩展的边际效益以及高效框架设计对成本的影响。通过GAIA基准测试，分析了LLM骨干选择、框架设计和测试时扩展策略对效率的影响，并提出了Efficient Agents框架，在保持96.7%性能的同时，将成本降低了28.4%，为构建高效、可持续的AI代理系统提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 13:56:51 GMT</pubDate>
</item>
<item>
<title>基于注意力权重的上下文回溯方法AttnTrace</title>
<link>https://arxiv.org/abs/2508.03793</link>
<guid>https://arxiv.org/abs/2508.03793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AttnTrace方法提升LLM上下文回溯效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于注意力权重的上下文回溯方法AttnTrace，旨在提高长上下文大语言模型（LLM）在生成响应时对相关文本的追溯能力。与现有方法相比，AttnTrace在准确性和计算效率上均有显著提升。研究团队提出了两种增强技术，并提供了理论支持。实验结果表明，AttnTrace不仅在传统任务中表现优异，还能有效检测长上下文中的提示注入攻击。此外，该方法在实际应用中能够精准识别被篡改的指令，提升了模型输出的可信度和可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:56:51 GMT</pubDate>
</item>
<item>
<title>AI代理在电商中的购物行为研究</title>
<link>https://arxiv.org/abs/2508.02630</link>
<guid>https://arxiv.org/abs/2508.02630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI代理在电商中表现出不同的购物偏好和决策模式。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了由视觉语言模型驱动的AI代理在在线市场中的购物行为。通过构建ACES环境，研究者分析了AI代理如何评估产品、做出购买决策，并揭示了它们对产品位置、价格、评分、评论及推广标签的敏感性。研究发现，不同AI模型对同一商品的偏好存在差异，且对推广标签持负面态度，对平台推荐有积极反应。此外，文章还展示了卖家如何通过优化产品描述来提升市场占有率，强调了AI中介购物对电商生态系统的潜在影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 13:19:36 GMT</pubDate>
</item>
<item>
<title>HyCodePolicy：一种融合多模态推理的自主控制框架</title>
<link>https://arxiv.org/abs/2508.02629</link>
<guid>https://arxiv.org/abs/2508.02629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HyCodePolicy提升机器人任务执行的鲁棒性和效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出HyCodePolicy，一种结合代码生成、几何定位、感知监控和迭代修复的闭环编程框架，用于增强具身代理的任务执行能力。系统通过自然语言指令分解任务，生成基于几何原语的初始程序，并在模拟中执行。利用视觉语言模型检测执行失败并分析原因，结合程序执行轨迹与感知反馈进行自动修复，实现低监督下的自我修正代码生成。实验表明，该方法显著提升了机器人操作策略的鲁棒性和样本效率，为多模态推理融入自主决策提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 13:18:14 GMT</pubDate>
</item>
<item>
<title>基于语言模型的代码补全排序方法研究</title>
<link>https://arxiv.org/abs/2508.02455</link>
<guid>https://arxiv.org/abs/2508.02455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级代码补全排序方法，提升IDE开发效率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了代码补全功能在现代IDE中的重要性，并提出了一种基于语言模型的轻量级代码补全排序方法。该方法通过构建前缀树并进行一次贪婪解码，实现精准的token级排序，无需使用束搜索或模型适配。该方法速度快、架构无关，且兼容现有代码补全模型，为IDE中集成语言模型提供了实用有效的路径，有助于提升开发者的辅助体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 10:20:39 GMT</pubDate>
</item>
<item>
<title>基于场景上下文的自中心人体运动生成与预测方法</title>
<link>https://arxiv.org/abs/2508.01126</link>
<guid>https://arxiv.org/abs/2508.01126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniEgoMotion模型，实现自中心运动生成与预测。</p><br /><br /><p><strong>摘要：</strong> 本文针对增强现实和虚拟现实等应用中自中心视角下人体运动生成与预测的问题，提出了一种新的任务框架，并设计了UniEgoMotion模型。该模型通过第一人称图像提取场景语义信息，实现无需依赖显式3D场景的运动生成与预测。研究还构建了EE4D-Motion数据集用于训练，并在多个任务上取得了最先进的性能，为自中心运动建模提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 20:41:20 GMT</pubDate>
</item>
<item>
<title>基于自回归与强化学习的图像编辑模型EARL</title>
<link>https://arxiv.org/abs/2508.01119</link>
<guid>https://arxiv.org/abs/2508.01119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升图像编辑效果，EARL模型表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了三种提升图像编辑任务性能的策略：监督微调、强化学习和思维链推理。研究采用了一个统一处理文本和视觉标记的自回归多模态模型，并发现结合大规模多模态语言模型验证的强化学习是最有效的策略。因此，作者推出了EARL模型，该模型在多种图像编辑任务中表现优于现有基准，且使用较少训练数据。研究开源了代码、训练数据和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 19:47:29 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的文本-视频检索方法研究</title>
<link>https://arxiv.org/abs/2507.23284</link>
<guid>https://arxiv.org/abs/2507.23284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BLiM框架提升文本-视频检索效果。</p><br /><br /><p><strong>摘要：</strong> 本文研究文本-视频检索任务，针对多模态大语言模型在检索中因候选优先偏差导致的问题，提出双向似然估计框架BLiM，并引入候选先验归一化模块CPN，有效提升检索精度。实验表明，BLiM在四个基准数据集上平均提升6.4 R@1，且CPN在多模态任务中展现出广泛适用性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:57:28 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover-V2：开源语言模型在自动定理证明中取得新突破</title>
<link>https://arxiv.org/abs/2508.03613</link>
<guid>https://arxiv.org/abs/2508.03613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover-V2在定理证明任务中表现优异，超越多个大型模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域取得了新的突破。该模型通过三项关键创新提升性能：Scaffolded数据合成、验证器引导的自我修正以及模型平均。其小型模型Goedel-Prover-V2-8B在MiniF2F测试中达到84.6%的pass@32，优于更大的DeepSeek-Prover-V2-671B。旗舰模型Goedel-Prover-V2-32B在标准模式下达到88.1%，在自我修正模式下达到90.4%，并在PutnamBench上解决了86个问题，成为开源模型中的第一名。模型代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 12:28:22 GMT</pubDate>
</item>
<item>
<title>ChartCap数据集提升图表描述准确性</title>
<link>https://arxiv.org/abs/2508.03164</link>
<guid>https://arxiv.org/abs/2508.03164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChartCap提升图表描述准确性和信息量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ChartCap，一个包含56.5万张真实图表及其类型特定、密集描述的大规模数据集。该数据集通过四阶段流程生成不含冗余信息的描述，并采用基于循环一致性的验证方法确保质量。研究还提出了一种新的评估指标——视觉一致性分数，用于衡量描述与图表的相似性。实验表明，使用ChartCap训练的模型在生成准确且信息丰富的图表描述方面表现优于现有模型和人工标注。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 03:09:07 GMT</pubDate>
</item>
<item>
<title>Representation Shift：一种与FlashAttention兼容的无训练令牌压缩方法</title>
<link>https://arxiv.org/abs/2508.00367</link>
<guid>https://arxiv.org/abs/2508.00367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的令牌压缩方法，提升模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Representation Shift的无训练、模型无关的令牌重要性度量方法，能够与FlashAttention等高效注意力机制兼容。该方法通过测量每个令牌表示的变化程度来实现令牌压缩，无需依赖注意力图，从而避免了传统方法与高效内核之间的不兼容问题。实验表明，该方法在视频文本检索和视频问答任务中分别实现了5.5%和4.4%的速度提升，适用于Transformer、CNN和状态空间模型等多种架构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 02:53:55 GMT</pubDate>
</item>
<item>
<title>可控超长视频生成方法LongVie及其基准测试</title>
<link>https://arxiv.org/abs/2508.03694</link>
<guid>https://arxiv.org/abs/2508.03694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongVie解决超长视频生成中的时序不一致和视觉退化问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为LongVie的端到端自回归框架，用于可控的超长视频生成。针对现有方法在长视频生成中出现的时序不一致和视觉退化问题，LongVie引入了统一噪声初始化策略、全局控制信号归一化、多模态控制框架以及退化感知训练策略，以提升视频的连贯性和视觉质量。此外，研究团队还构建了LongVGenBench基准测试集，包含100个高分辨率视频，涵盖多种真实和合成环境。实验表明，LongVie在长距离可控性、一致性和质量方面均达到当前最优水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>CompassVerifier：多领域答案验证与评估框架</title>
<link>https://arxiv.org/abs/2508.03686</link>
<guid>https://arxiv.org/abs/2508.03686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CompassVerifier提升LLM答案验证能力，支持多领域任务。</p><br /><br /><p><strong>摘要：</strong> 本文提出CompassVerifier，一个高效且稳健的轻量级答案验证模型，用于评估和优化大型语言模型。该模型具备跨数学、知识和推理任务的多领域能力，能够处理多种答案类型，并有效识别异常响应。研究还引入了VerifierBench基准，通过多源数据和人工分析增强验证效果。作者期望该框架能推动答案验证、评估协议和强化学习的研究。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 13:55:24 GMT</pubDate>
</item>
<item>
<title>Skywork UniPic：统一多模态任务的高效大模型</title>
<link>https://arxiv.org/abs/2508.03320</link>
<guid>https://arxiv.org/abs/2508.03320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork UniPic在单个架构中实现图像理解、文本生成和编辑，性能优异。</p><br /><br /><p><strong>摘要：</strong> Skywork UniPic是一款15亿参数的自回归模型，能够在单一架构中完成图像理解、文本到图像生成和图像编辑任务，无需任务专用适配器或模块间连接。该模型在GenEval、DPG-Bench、GEditBench-EN和ImgEdit-Bench等基准测试中表现优异，并能在消费级硬件（如RTX 4090）上生成1024x1024图像。其技术亮点包括解耦编码策略、渐进式分辨率训练以及基于任务奖励模型的数据集优化。Skywork UniPic展示了高保真多模态集成在资源受限环境下的可行性，为部署高效的多模态AI提供了新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 06:59:01 GMT</pubDate>
</item>
<item>
<title>多人群体说话视频生成数据集MIT与基线模型CovOG</title>
<link>https://arxiv.org/abs/2508.03050</link>
<guid>https://arxiv.org/abs/2508.03050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIT数据集和CovOG模型，推动多人群体说话视频生成研究。</p><br /><br /><p><strong>摘要：</strong> 现有研究主要关注单人独白或孤立面部动画，难以应用于真实多人群体互动。为此，本文提出MIT数据集，包含12小时高分辨率多人群体对话视频，涵盖2至4名说话者，并带有精细的身体姿态和语音交互标注。为验证MIT的潜力，作者进一步提出CovOG基线模型，集成多人群体姿态编码器（MPE）和交互式音频驱动器（IAD），用于生成逼真的多人群体说话视频。该研究为多人群体视频生成提供了重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 23:54:18 GMT</pubDate>
</item>
<item>
<title>ToolTrain提升代码问题定位能力</title>
<link>https://arxiv.org/abs/2508.03012</link>
<guid>https://arxiv.org/abs/2508.03012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToolTrain提升代码问题定位性能，优于现有模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ToolTrain的两阶段工具集成训练框架，旨在增强大型语言模型在软件开发中进行代码问题定位的能力。该方法结合了拒绝采样监督微调和工具集成强化学习，以提高模型使用检索工具进行多步骤推理和导航的能力。实验结果显示，ToolTrain训练的模型在函数级定位任务上表现优异，甚至超越了Claude-3.7。此外，改进的问题定位性能也提升了端到端的问题解决效果，证明了针对问题定位的训练是提升自动化软件开发的有效策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.03012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 22:44:21 GMT</pubDate>
</item>
<item>
<title>Seed Diffusion Preview：高效代码生成的扩散模型</title>
<link>https://arxiv.org/abs/2508.02193</link>
<guid>https://arxiv.org/abs/2508.02193</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed Diffusion Preview实现高速代码生成，性能领先。</p><br /><br /><p><strong>摘要：</strong> Seed Diffusion Preview是一种基于离散状态扩散的大规模语言模型，采用非序列化并行生成方式，显著提升了推理速度。在H20 GPU上实现了每秒2146个token的推理速度，同时在多个标准代码评估基准中保持了竞争力，比现有的Mercury Coder和Gemini Diffusion更快，成为代码生成领域的最新里程碑。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02193" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 04:43:01 GMT</pubDate>
</item>
<item>
<title>基于强化学习的近似最近邻搜索算法CRINN</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRINN通过强化学习实现高效近似最近邻搜索。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CRINN的新近似最近邻搜索（ANNS）算法，该算法将ANNS优化问题转化为强化学习问题，以执行速度作为奖励信号。这种方法能够自动生成越来越快的ANNS实现，同时保持准确性。实验结果表明，CRINN在六个广泛使用的NNS基准数据集上表现优异，其中在三个数据集上取得最佳性能，在另外两个数据集上并列第一。CRINN的成功不仅提升了ANNS优化的效果，还证明了结合强化学习的大型语言模型可以有效用于自动化复杂的算法优化任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:57:46 GMT</pubDate>
</item>
<item>
<title>AlignGuard-LoRA：一种防止大语言模型微调中对齐漂移的方法</title>
<link>https://arxiv.org/abs/2508.02079</link>
<guid>https://arxiv.org/abs/2508.02079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlignGuard-LoRA有效减少微调中的对齐漂移，提升模型安全性。</p><br /><br /><p><strong>摘要：</strong> 本文提出AlignGuard-LoRA（AGL），一种用于在微调大型语言模型时保持对齐性的框架。AGL通过引入主任务损失、基于Fisher信息矩阵的正则化、任务特定正则化以及碰撞感知正则化，有效控制参数更新方向，防止对齐漂移。研究还构建了DriftCaps基准测试，用于量化对齐漂移和安全性能下降。实验表明，AGL在不损害下游任务性能的前提下，可减少高达50%的安全关键指标上的对齐漂移。此外，AGL揭示了灾难性遗忘的缩放规律，确保微调后的模型保持稳定的适应能力。该方法为LoRA提供了一种结构化的改进方案，具有良好的实用性和扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:45:24 GMT</pubDate>
</item>
<item>
<title>TraceAlign：追踪并缓解大语言模型对齐漂移的框架</title>
<link>https://arxiv.org/abs/2508.02063</link>
<guid>https://arxiv.org/abs/2508.02063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TraceAlign通过分析训练数据源，有效减少LLM对齐漂移。</p><br /><br /><p><strong>摘要：</strong> 本文提出TraceAlign，一个用于追踪大语言模型（LLMs）对齐失败根源的框架。该框架引入Belief Conflict Index（BCI）来量化生成内容与对齐政策之间的语义不一致，并通过三种干预措施——TraceShield、Contrastive Belief Deconfliction Loss 和 Prov-Decode——显著降低对齐漂移。实验表明，这些方法在保持任务性能的同时，能减少高达85%的对齐偏差。此外，研究还基于后缀数组统计推导出对齐漂移的可能性上限，揭示了记忆频率和长度对对抗性激活风险的影响。TraceAlign为理解与缓解模型对齐问题提供了首个可扩展、可追溯且基于数据的工具集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 01:03:35 GMT</pubDate>
</item>
<item>
<title>LiveMCPBench：首个大规模MCP环境下的LLM代理评估基准</title>
<link>https://arxiv.org/abs/2508.01780</link>
<guid>https://arxiv.org/abs/2508.01780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveMCPBench首次提供大规模MCP环境下的LLM代理评估框架。</p><br /><br /><p><strong>摘要：</strong> 随着Model Context Protocol (MCP) 的快速发展，MCP服务器数量已超过10,000个。然而，现有的MCP基准测试仅限于单服务器设置，无法有效评估代理在真实场景中的能力。为此，研究团队推出了LiveMCPBench，这是首个包含95个真实任务的综合性基准，旨在评估大型、多样化MCP环境中的LLM代理。该平台还提供了LiveMCPTool和LiveMCPEval，支持可扩展、可复现的评估流程，并引入了MCP Copilot Agent进行多步骤任务执行。实验覆盖了10个主流模型，最佳模型成功率达到78.95%。研究揭示了模型性能差异较大，部分常用模型在复杂工具环境中表现不佳。LiveMCPBench为LLM代理能力的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:36:42 GMT</pubDate>
</item>
<item>
<title>LAMIC：一种无需训练的多参考图像合成框架</title>
<link>https://arxiv.org/abs/2508.00477</link>
<guid>https://arxiv.org/abs/2508.00477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LAMIC实现多参考图像合成，无需训练即可保持布局和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LAMIC，一个无需训练的多参考图像合成框架，首次将单参考扩散模型扩展到多参考场景。LAMIC基于MMDiT模型，引入两种可插拔注意力机制：Group Isolation Attention（GIA）用于增强实体分离，Region-Modulated Attention（RMA）用于实现布局感知生成。研究还引入三个评估指标：Inclusion Ratio、Fill Ratio 和 Background Similarity，以全面评估模型性能。实验表明，LAMIC在多个主要指标上达到最先进水平，表现出色的零样本泛化能力，在身份保持、背景一致性和提示遵循方面表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 05:51:54 GMT</pubDate>
</item>
<item>
<title>Dynaword: From One-shot to Continuously Developed Datasets</title>
<link>https://arxiv.org/abs/2508.02271</link>
<guid>https://arxiv.org/abs/2508.02271</guid>
<content:encoded><![CDATA[
Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:30:42 GMT</pubDate>
</item>
<item>
<title>ReMoMask：一种提升文本到动作生成性能的统一框架</title>
<link>https://arxiv.org/abs/2508.02605</link>
<guid>https://arxiv.org/abs/2508.02605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReMoMask提升文本到动作生成的准确性和多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ReMoMask，一种用于文本到动作生成的统一框架，旨在解决现有方法在多样性、物理合理性及同步性方面的不足。该框架包含三个关键创新：双向动量文本-动作模型提高跨模态检索精度；语义时空注意力机制消除异步伪影；RAG无分类器引导增强泛化能力。实验表明，ReMoMask在多个基准数据集上表现优异，FID分数显著优于现有最先进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 12:56:35 GMT</pubDate>
</item>
<item>
<title>Sparse-dLLM：提升扩散大语言模型推理效率的稀疏缓存方法</title>
<link>https://arxiv.org/abs/2508.02558</link>
<guid>https://arxiv.org/abs/2508.02558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sparse-dLLM通过动态缓存优化，提升dLLM推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Sparse-dLLM，一种无需训练的框架，通过动态缓存机制和稀疏注意力策略，解决扩散大语言模型（dLLMs）在推理过程中计算复杂度高和内存消耗大的问题。研究发现dLLMs中存在跨层稀疏性，关键token在多个解码步骤中保持重要性，而低相关token则持续不重要，从而支持选择性缓存剔除。该方法通过关注引导策略，保留关键token并动态剔除无用前缀/后缀，实验表明其在LLaDA和Dream系列模型上实现了高达10倍的吞吐量提升，同时保持性能与内存成本相近，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 12:14:03 GMT</pubDate>
</item>
<item>
<title>SHAMI-MT：连接标准阿拉伯语与叙利亚方言的机器翻译系统</title>
<link>https://arxiv.org/abs/2508.02268</link>
<guid>https://arxiv.org/abs/2508.02268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SHAMI-MT有效实现了标准阿拉伯语与叙利亚方言之间的高质量翻译。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SHAMI-MT，一个专门用于连接现代标准阿拉伯语（MSA）和叙利亚方言的双向机器翻译系统。该系统由两个模型组成，分别用于MSA到叙利亚方言和叙利亚方言到MSA的翻译，基于AraT5v2-base-1024架构进行优化。模型在Nabra数据集上进行了微调，并在MADAR语料库的未见数据上进行了评估。MSA到叙利亚方言的模型在GPT-4.1的评分中获得了4.01的高分，显示出其在准确性和方言真实性方面的优势。该研究为之前缺乏支持的语言对提供了重要工具，推动了方言阿拉伯语翻译的发展，并具有内容本地化、文化遗产保护和跨文化交流等应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:21:11 GMT</pubDate>
</item>
<item>
<title>AuroBind：一种用于高通量分子筛选的结构功能学习框架</title>
<link>https://arxiv.org/abs/2508.02137</link>
<guid>https://arxiv.org/abs/2508.02137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AuroBind提升蛋白质药物筛选效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AuroBind，一种基于结构的虚拟筛选框架，通过原子级模型和多种优化策略，显著提升了药物靶点筛选的精度与速度。该方法在多个疾病相关靶点中实现了7-69%的实验命中率，并成功识别出孤儿G蛋白偶联受体GPR151和GPR160的激动剂和拮抗剂，展示了其在药物发现中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 03:34:48 GMT</pubDate>
</item>
<item>
<title>基于不确定性驱动的流程奖励数据构建框架</title>
<link>https://arxiv.org/abs/2508.01773</link>
<guid>https://arxiv.org/abs/2508.01773</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自动化流程奖励数据构建方法，提升数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于不确定性的流程奖励数据构建框架，旨在提高大语言模型在数学推理任务中的准确性。该框架涵盖数据生成和标注过程，同时引入两种新的输出聚合方法，结合多数投票与流程奖励模型的优势。实验结果表明，该方法在多个基准测试中表现出色，有效提升了模型的推理能力。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01773" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 10:14:13 GMT</pubDate>
</item>
<item>
<title>Voxlect：全球方言与区域语言的语音基础模型基准测试</title>
<link>https://arxiv.org/abs/2508.01691</link>
<guid>https://arxiv.org/abs/2508.01691</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voxlect提供全球多种语言方言的语音模型基准测试。</p><br /><br /><p><strong>摘要：</strong> Voxlect是一个用于建模全球方言和区域语言的新基准，涵盖了英语、阿拉伯语、普通话、粤语、藏语、印地语、泰语、西班牙语、法语、德语、巴西葡萄牙语和意大利语等多种语言。该研究使用了来自30个公开语音语料库的超过200万条训练语音，并评估了多个常用语音基础模型在方言分类中的表现。研究还分析了在噪声条件下的模型鲁棒性，并展示了Voxlect在语音识别数据集增强和语音生成系统评估中的应用。Voxlect已开源，可供研究人员使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01691" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 05:51:28 GMT</pubDate>
</item>
<item>
<title>人工智能在绘画归属中的挑战与研究</title>
<link>https://arxiv.org/abs/2508.01408</link>
<guid>https://arxiv.org/abs/2508.01408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI在绘画归属中面临识别和生成图像的双重挑战。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能在艺术作品归属，特别是绘画方面的挑战。随着AI生成和分析图像能力的提升，出现了两个主要问题：一方面，AI生成的图像可能被误认为是某位艺术家的作品；另一方面，AI可能无法正确识别真实绘画的作者。研究使用了最新的AI模型，在包含近40,000幅来自128位艺术家的画作数据集上进行了实验。结果显示，视觉语言模型在画布归属和识别AI生成图像方面存在局限性。随着用户越来越多地依赖AI获取信息，提高这些模型在艺术家归属和AI图像检测方面的能力变得尤为重要，以防止错误信息的传播。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 11:27:31 GMT</pubDate>
</item>
<item>
<title>多模态数据融合预测家庭财富的研究</title>
<link>https://arxiv.org/abs/2508.01109</link>
<guid>https://arxiv.org/abs/2508.01109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态数据有效提升家庭财富预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了社会经济指标如家庭财富是否能在卫星图像和互联网文本中留下可识别的痕迹。通过结合非洲地区的人口与健康调查（DHS）数据、Landsat卫星图像以及由AI搜索代理获取的网络文本，作者构建了一个多模态框架，用于预测国际财富指数。该框架包括五个管道：基于图像的视觉模型、仅依赖位置/年份的大型语言模型、AI代理检索与合成文本、图像-文本联合编码器，以及所有信号的集成。研究发现，融合视觉和文本信息显著优于单一视觉模型，且LLM内部知识比代理检索文本更有效。此外，研究揭示了视觉和语言模态之间存在部分表征收敛，支持了柏拉图式表征假设。最后，研究发布了一个包含60,000多个DHS集群的多模态数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 19:07:16 GMT</pubDate>
</item>
<item>
<title>基于ViT嵌入的量子支持向量机可扩展性研究</title>
<link>https://arxiv.org/abs/2508.00024</link>
<guid>https://arxiv.org/abs/2508.00024</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViT嵌入提升量子SVM性能，实现更高准确率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了量子支持向量机在高维量子态和硬件限制下的可扩展性挑战，并提出了一种结合类平衡k-means蒸馏与预训练Vision Transformer嵌入的量子-经典混合管道。实验表明，ViT嵌入能够显著提升量子SVM的性能，在Fashion-MNIST数据集上比传统SVM高出8.02%，在MNIST数据集上高出4.42%。而CNN特征则表现出性能下降。通过16量子位张量网络模拟，研究首次系统证明了量子核优势高度依赖于嵌入选择，揭示了Transformer注意力机制与量子特征空间之间的关键协同效应，为可扩展的量子机器学习提供了可行路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00024" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 17:23:51 GMT</pubDate>
</item>
<item>
<title>Dens3R：统一几何预测的3D基础模型</title>
<link>https://arxiv.org/abs/2507.16290</link>
<guid>https://arxiv.org/abs/2507.16290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dens3R实现多几何量联合预测，提升3D重建精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出Dens3R，一种用于联合几何密集预测的3D基础模型，能够同时准确回归深度、表面法线等几何属性。该模型采用两阶段训练框架，构建具有泛化性和内在不变性的点云表示，并引入位置插值旋转位置编码以增强对高分辨率输入的鲁棒性。通过融合图像对匹配特征与内在不变性建模，Dens3R实现了从单视角到多视角的一致几何感知。此外，研究还设计了支持几何一致性多视角推理的后处理流程，实验表明其在多种密集3D预测任务中表现优异，具备广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 03:22:30 GMT</pubDate>
</item>
<item>
<title>Qwen-Image：在文本渲染与图像编辑上的重大突破</title>
<link>https://arxiv.org/abs/2508.02324</link>
<guid>https://arxiv.org/abs/2508.02324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen-Image在文本渲染和图像编辑上取得显著进展。</p><br /><br /><p><strong>摘要：</strong> Qwen-Image是Qwen系列中的一款图像生成基础模型，在复杂文本渲染和精确图像编辑方面取得了显著进展。文章介绍了其通过大规模数据处理、渐进式训练策略提升文本渲染能力，尤其在中英文等语言中表现优异。同时，通过多任务训练和双编码机制，增强了图像编辑的一致性与视觉质量。Qwen-Image在多个基准测试中达到最先进水平，展示了其在图像生成与编辑方面的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 07:49:20 GMT</pubDate>
</item>
<item>
<title>基于自监督强化学习的指令遵循能力提升方法</title>
<link>https://arxiv.org/abs/2508.02150</link>
<guid>https://arxiv.org/abs/2508.02150</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自监督RL框架提升推理模型的指令遵循能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于自监督强化学习的框架，旨在提升推理模型的指令遵循能力，而无需依赖外部模型。该方法利用模型内部信号进行训练，避免了传统方法中对外部模型的依赖，从而解决了成本高和可访问性差的问题。实验表明，该框架在保持推理性能的同时显著提升了指令遵循能力，提供了一种可扩展且经济高效的解决方案。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02150" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 03:48:59 GMT</pubDate>
</item>
<item>
<title>基于情境嵌入的长文档检索增强生成方法</title>
<link>https://arxiv.org/abs/2508.01959</link>
<guid>https://arxiv.org/abs/2508.01959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">情境嵌入模型提升长文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的检索增强生成方法，通过将短文本块置于更广泛的情境中进行编码，以提高检索效果。研究指出，传统方法在处理长文档时面临嵌入模型容量不足和局部证据需求的挑战。为此，作者引入了情境嵌入模型（SitEmb），并在专门设计的书籍情节检索数据集上验证了其有效性。实验结果表明，SitEmb模型在参数量较少的情况下表现优于多个大模型，并在多语言和多种下游任务中取得优异成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Aug 2025 19:59:31 GMT</pubDate>
</item>
<item>
<title>动态视觉标记压缩框架GlimpsePrune提升大视觉语言模型效率</title>
<link>https://arxiv.org/abs/2508.01548</link>
<guid>https://arxiv.org/abs/2508.01548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GlimpsePrune实现高效视觉标记压缩，保持模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为GlimpsePrune的动态剪枝框架，旨在解决大型视觉语言模型（LVLMs）在处理高分辨率输入时的效率问题。传统方法采用固定压缩比例，难以适应不同复杂度的场景，导致信息丢失和性能下降。GlimpsePrune通过数据驱动的“瞥见”机制，在生成答案前一次性剪除无关视觉标记，有效减少计算成本。实验表明，该方法可剪除92.6%的视觉标记，同时保持基准性能，并在微调后进一步提升至110%。该研究为构建更强大且高效的LVLMs提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 22:15:43 GMT</pubDate>
</item>
<item>
<title>RoboMemory：一种面向物理机器人系统的多记忆框架</title>
<link>https://arxiv.org/abs/2508.01415</link>
<guid>https://arxiv.org/abs/2508.01415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboMemory提升机器人长期学习与任务执行能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboMemory，一种基于认知神经科学的多记忆框架，用于物理机器人在现实环境中的持续学习。该框架包含四个核心模块：信息预处理器、终身具身记忆系统、闭环规划模块和低级执行器，分别模拟大脑不同区域的功能。其核心模块通过并行更新与检索机制提升推理速度，并结合动态知识图谱增强记忆一致性与可扩展性。实验表明，RoboMemory在EmbodiedBench基准测试中表现优于现有开源和闭源模型，显示出强大的长期学习能力与实际部署效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 11:39:42 GMT</pubDate>
</item>
<item>
<title>基于贪婪目标的元强化学习中涌现探索行为的研究</title>
<link>https://arxiv.org/abs/2508.01287</link>
<guid>https://arxiv.org/abs/2508.01287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，在特定条件下，贪婪目标可激发探索行为。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在元强化学习中，通过仅最大化贪婪目标是否能产生探索行为。研究提出三个必要条件：环境结构重复性、代理记忆和长期信用分配。实验表明，当环境具有重复结构且代理具备记忆能力时，即使不引入额外激励，策略也能表现出信息寻求的探索行为。进一步实验显示，若缺少环境结构或记忆，探索行为消失。而移除长期信用分配后，探索行为不一定消失，这可能与伪汤普森采样效应有关。研究结果表明，在适当条件下，探索与利用可从统一的奖励最大化过程中自然涌现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 02 Aug 2025 05:42:59 GMT</pubDate>
</item>
<item>
<title>多阶段复杂任务中的测试时计算最优缩放研究</title>
<link>https://arxiv.org/abs/2508.00890</link>
<guid>https://arxiv.org/abs/2508.00890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多阶段任务中测试时计算资源分配优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在多阶段复杂任务中如何通过测试时计算资源的最优分配来提升大型语言模型的性能。传统研究主要关注单阶段任务，而现实问题通常由多个异构子任务组成，每个子任务需要不同能力的模型。文章提出了AgentTTS框架，利用LLM代理通过与执行环境的迭代交互实现计算资源的最优分配。实验表明，该方法在搜索效率、对训练集大小的鲁棒性以及可解释性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 15:21:18 GMT</pubDate>
</item>
<item>
<title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
<link>https://arxiv.org/abs/2508.02317</link>
<guid>https://arxiv.org/abs/2508.02317</guid>
<content:encoded><![CDATA[
Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 07:33:04 GMT</pubDate>
</item>
<item>
<title>CellForge：基于多智能体框架的虚拟细胞建模系统</title>
<link>https://arxiv.org/abs/2508.02276</link>
<guid>https://arxiv.org/abs/2508.02276</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CellForge通过多智能体协作构建虚拟细胞模型，提升预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CellForge，一个基于多智能体框架的系统，能够将生物数据和任务描述直接转化为优化的虚拟细胞计算模型。该系统包含任务分析、方法设计和实验执行三个核心模块，其中方法设计模块由不同视角的专家代理和一个中央协调者组成，通过协作达成共识。CellForge在多种单细胞扰动预测任务中表现优于现有方法，展示了多智能体协作在解决复杂建模问题中的优势。代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.02276" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 06:43:31 GMT</pubDate>
</item>
<item>
<title>个性化安全对齐框架提升文本到图像生成模型的安全性</title>
<link>https://arxiv.org/abs/2508.01151</link>
<guid>https://arxiv.org/abs/2508.01151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PSA框架实现用户定制化安全控制，提升生成内容安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对文本到图像扩散模型中存在的安全机制不足问题，提出了个性化安全对齐（PSA）框架。该框架通过整合用户个人安全偏好，调整模型行为以符合个体安全标准，同时保持图像质量。研究引入了Sage数据集，并采用交叉注意力机制实现用户偏好融合。实验表明，PSA在抑制有害内容方面优于现有方法，能更好地满足用户约束条件，提升了生成内容的安全性和准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 22:23:20 GMT</pubDate>
</item>
<item>
<title>Foundation-Sec-8B-Instruct：面向网络安全的对话型大语言模型</title>
<link>https://arxiv.org/abs/2508.01059</link>
<guid>https://arxiv.org/abs/2508.01059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新型网络安全大模型提升对话与指令执行能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Foundation-Sec-8B-Instruct，这是一个专为网络安全对话设计的大语言模型，基于Foundation-Sec-8B进行优化，具备领域知识、指令遵循和对话交互能力。该模型在多个网络安全任务中表现优于Llama 3.1-8B-Instruct，并与GPT-4o-mini相当。研究者希望该模型能成为网络安全专业人员日常工作的得力助手，并已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.01059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 16:25:57 GMT</pubDate>
</item>
<item>
<title>无需运行时环境的网络安全LLM训练框架Cyber-Zero</title>
<link>https://arxiv.org/abs/2508.00910</link>
<guid>https://arxiv.org/abs/2508.00910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cyber-Zero通过模拟生成高质量交互轨迹提升网络安全LLM性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Cyber-Zero，一个无需运行时环境的框架，用于合成高质量的代理轨迹以训练网络安全大语言模型。该框架利用公开的CTF写入和角色驱动的LLM模拟，逆向工程运行行为并生成真实、长周期的交互序列。在三个主流CTF基准测试中，基于Cyber-Zero训练的模型性能比基线模型高出13.1%。最佳模型Cyber-Zero-32B在开源模型中达到最先进的性能，与DeepSeek-V3-0324和Claude-3.5-Sonnet相当，同时具有更高的成本效益，证明了无运行时轨迹合成在网络安全代理开发中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 14:10:37 GMT</pubDate>
</item>
<item>
<title>InstructVLA：融合多模态推理与精准动作生成的机器人模型</title>
<link>https://arxiv.org/abs/2507.17520</link>
<guid>https://arxiv.org/abs/2507.17520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InstructVLA提升机器人多模态任务表现与推理能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了InstructVLA，一种端到端的视觉-语言-动作（VLA）模型，旨在同时保持大型视觉-语言模型（VLM）的灵活推理能力和高效的操控性能。通过引入Vision-Language-Action Instruction Tuning（VLA-IT）训练范式，InstructVLA在标准VLM语料库和650K样本的VLA-IT数据集上联合优化文本推理与动作生成。实验表明，InstructVLA在SimplerEnv任务中比SpatialVLA提升30.5%，在SimplerEnv-Instruct基准测试中优于OpenVLA和GPT-4o辅助的行动专家。此外，InstructVLA在多模态任务中表现优异，并在推理时通过文本推理增强操作性能，展示了其在人机交互与策略学习中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 09:57:06 GMT</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro：开源AI代理框架提升研究可复现性与性能</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cognitive Kernel-Pro是开源AI代理框架，提升研究可复现性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Cognitive Kernel-Pro，一个完全开源且尽可能免费的多模块AI代理框架，旨在推动高级AI代理的开发与评估。该框架在四个关键领域（网络、文件、代码和通用推理）系统地研究了高质量训练数据的构建，包括查询、轨迹和可验证答案。此外，还探索了代理在测试时的反思和投票策略，以提高其鲁棒性和性能。Cognitive Kernel-Pro在GAIA基准上取得了最先进的结果，其8B参数模型超越了WebDancer和WebSailor等先前领先系统，为可访问、高性能的AI代理设定了新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 04:11:31 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型评估新基准MCIF发布</title>
<link>https://arxiv.org/abs/2507.19634</link>
<guid>https://arxiv.org/abs/2507.19634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCIF是首个多语言多模态指令跟随评估基准。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的发展，多模态大语言模型（MLLMs）逐渐从单一任务系统演变为通用指令遵循模型。然而，现有评估基准在多语言、多模态和长文本上下文方面存在不足。为此，研究者推出了MCIF，这是一个基于科学演讲的多语言人工标注基准，旨在评估模型在跨语言和多模态环境下的指令遵循能力。MCIF涵盖语音、视觉和文本三种模态，并支持英语、德语、意大利语和中文四种语言，有助于全面评估MLLMs的能力。该基准以CC-BY 4.0许可证发布，鼓励开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 15:00:51 GMT</pubDate>
</item>
<item>
<title>AI生成交互式音视频内容的新方法与挑战</title>
<link>https://arxiv.org/abs/2508.00632</link>
<guid>https://arxiv.org/abs/2508.00632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AVR-Eval评估机制和多代理系统提升AI生成游戏质量。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了AI在生成交互式音视频内容（如游戏）方面的挑战，提出AVR-Eval评估机制，通过对比音频-视频记录来判断内容质量。同时构建了AVR-Agent多代理系统，利用多媒体资源生成JavaScript代码，并通过反馈迭代优化。实验表明，该系统生成的内容胜率高于单次生成方式，但尚未有效利用定制资源和反馈，揭示了人机在内容创作上的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 09:45:13 GMT</pubDate>
</item>
<item>
<title>基于3D高斯的增量图像目标导航方法</title>
<link>https://arxiv.org/abs/2508.00823</link>
<guid>https://arxiv.org/abs/2508.00823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出IGL-Nav实现高效3D图像目标导航。</p><br /><br /><p><strong>摘要：</strong> 本文针对以图像为目标的视觉导航问题，提出了一种基于可渲染3D高斯（3DGS）表示的增量定位框架IGL-Nav。该方法通过逐步更新场景表示并利用几何信息进行粗略定位，再结合可微渲染进行精细姿态优化，提升了导航效率与准确性。实验表明，IGL-Nav在多种配置下均优于现有方法，并能在真实机器人平台上部署，支持任意姿态拍摄目标图像。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于音频空间信息的视频生成方法研究</title>
<link>https://arxiv.org/abs/2508.00782</link>
<guid>https://arxiv.org/abs/2508.00782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpA2V通过音频空间信息生成语义与空间对齐的视频。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SpA2V的新框架，旨在通过利用音频中的空间信息（如音量和频率）来生成与输入音频在语义和空间上高度一致的视频。该方法分为两个阶段：第一阶段是音频引导的视频规划，利用先进的多模态大模型从音频中提取空间和语义线索，构建视频场景布局；第二阶段是基于布局的视频生成，将这些布局作为条件指导整合到预训练扩散模型中，实现无需训练的视频生成。实验表明，SpA2V在生成高质量视频方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:05:04 GMT</pubDate>
</item>
<item>
<title>3D-R1：提升3D场景理解的视觉语言模型</title>
<link>https://arxiv.org/abs/2507.23478</link>
<guid>https://arxiv.org/abs/2507.23478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D-R1增强3D场景理解能力，提升推理与泛化效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出3D-R1，一种用于提升3D场景理解的视觉语言模型。针对现有3D VLMs在空间数据质量和视角假设上的不足，研究者构建了高质量合成数据集Scene-30K，并引入RLHF训练方法和三种奖励函数以增强推理能力。此外，动态视角选择策略提升了模型对3D场景的理解效果。实验表明，3D-R1在多个3D场景基准测试中平均提升10%，展现出强大的推理与泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 07:59:06 GMT</pubDate>
</item>
<item>
<title>多语言对话中大语言模型的幻觉现象研究</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现不同语言中大模型幻觉率差异显著。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在三种语言（印地语、波斯语和中文）的对话数据中，GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1 和 Qwen-3 等大语言模型的幻觉现象。结果显示，这些模型在中文中的幻觉较少，而在印地语和波斯语中幻觉率显著较高。该研究扩展了对英语以外语言中幻觉问题的理解，强调了多语言环境下模型可靠性的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 10:39:51 GMT</pubDate>
</item>
<item>
<title>DAEDAL：动态自适应长度扩展提升扩散语言模型性能</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DAEDAL解决DLLMs固定生成长度限制，提升效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出DAEDAL，一种无需训练的去噪策略，用于解决扩散语言模型（DLLMs）因固定生成长度导致的性能瓶颈。该方法通过两个阶段实现动态长度扩展：首先从短初始长度开始，根据序列完成度逐步扩展至任务合适的长度；其次在去噪过程中动态干预，通过插入掩码标记扩展不足区域，确保输出完整。实验表明，DAEDAL在保持甚至超越固定长度基线性能的同时，显著提升了计算效率，为DLLMs的发展提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>基于多模型共识的高效对话评估方法</title>
<link>https://arxiv.org/abs/2508.00454</link>
<guid>https://arxiv.org/abs/2508.00454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效对话评估方法，提升评估效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在对话评估中的可靠性问题，提出一种高效的多轮对话评估方法。该方法通过整合多个语言模型的判断知识到单一模型中，既保留了多模型反馈的优势，又大幅降低了评估成本。实验结果表明，该方法在多个对话评估基准上表现优于现有方法，展现出更高的效率和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 05:26:01 GMT</pubDate>
</item>
<item>
<title>多模态指代分割研究综述</title>
<link>https://arxiv.org/abs/2508.00265</link>
<guid>https://arxiv.org/abs/2508.00265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述多模态指代分割技术及其应用。</p><br /><br /><p><strong>摘要：</strong> 多模态指代分割旨在根据文本或音频中的指代表达，在图像、视频和3D场景中准确分割目标对象，广泛应用于需要基于用户指令进行精确物体感知的场景。过去十年，随着卷积神经网络、Transformer和大语言模型的发展，该领域取得了显著进展。本文系统回顾了多模态指代分割的研究背景、常用数据集、统一的元架构以及针对图像、视频和3D场景的代表性方法，并探讨了处理现实复杂性的广义指代表达方法及相关任务与应用。文章还提供了在标准基准上的性能比较，并持续跟踪相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2508.00265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 22:14:00 GMT</pubDate>
</item>
<item>
<title>基于经验增强的软件问题解决方法SWE-Exp</title>
<link>https://arxiv.org/abs/2507.23361</link>
<guid>https://arxiv.org/abs/2507.23361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Exp通过积累修复经验提升软件问题解决效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SWE-Exp，一种基于经验增强的软件问题解决方法。该方法通过构建多维度的经验库，从以往的修复过程中提取可复用的知识，从而避免重复探索失败路径，并提升对类似问题的解决能力。实验表明，SWE-Exp在SWE-bench-Verified数据集上取得了41.6%的Pass@1准确率，展现出卓越的性能。该方法为自动化软件工程代理提供了一种系统积累和利用修复经验的新范式，推动问题解决方式从试错探索向经验驱动的战略性解决转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 05:13:42 GMT</pubDate>
</item>
<item>
<title>SWE-Debate：通过多智能体辩论提升代码问题定位与修复</title>
<link>https://arxiv.org/abs/2507.23348</link>
<guid>https://arxiv.org/abs/2507.23348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Debate利用多智能体辩论提升代码问题定位与修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SWE-Debate，一种基于多智能体辩论的框架，旨在提升代码问题定位与修复的准确性。该框架通过遍历代码依赖图生成多个故障传播路径，并组织不同推理视角的智能体进行三轮辩论，从而协同得出更精准的修复方案。最终，修复计划被整合到基于MCTS的代码修改代理中以生成补丁。实验表明，SWE-Debate在SWE-bench基准测试中取得了最先进的结果，显著优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 04:54:46 GMT</pubDate>
</item>
<item>
<title>PixelNerd：一种高效的单阶段像素神经场扩散模型</title>
<link>https://arxiv.org/abs/2507.23268</link>
<guid>https://arxiv.org/abs/2507.23268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PixelNerd实现高效图像生成，无需复杂管道。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PixelNerd的新型扩散模型，该模型通过神经场直接进行像素级解码，避免了传统两阶段训练中因VAE压缩导致的误差累积和解码伪影问题。PixelNerd采用单尺度、单阶段的端到端架构，在ImageNet数据集上取得了2.15 FID（256×256）和2.84 FID（512×512）的优异性能，且无需复杂的级联管道或VAE模块。此外，该框架还扩展至文本到图像生成任务，在GenEval和DPG基准测试中表现出色，显示出其在高质量图像生成中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:07:20 GMT</pubDate>
</item>
<item>
<title>基于迭代优化的高效3D重建模型iLRM</title>
<link>https://arxiv.org/abs/2507.23277</link>
<guid>https://arxiv.org/abs/2507.23277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">iLRM通过迭代机制实现高效高质3D重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为iLRM的迭代大型3D重建模型，旨在解决现有基于Transformer的3D重建方法在可扩展性和计算效率上的不足。iLRM通过三个核心原则提升性能：解耦场景表示与输入图像以生成紧凑的3D表示；将多视角注意力交互分解为两阶段注意力机制以降低计算成本；并在每一层注入高分辨率信息以提高重建精度。实验结果表明，iLRM在RE10K和DL3DV等数据集上表现出色，在保持计算成本相近的情况下，能够处理更多输入视角并提供更高质量的3D重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 02:33:07 GMT</pubDate>
</item>
<item>
<title>基于增量学习的高效机器遗忘算法研究</title>
<link>https://arxiv.org/abs/2507.23257</link>
<guid>https://arxiv.org/abs/2507.23257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出IAU算法实现高效机器遗忘。</p><br /><br /><p><strong>摘要：</strong> 随着隐私问题日益突出，机器遗忘技术受到关注。现有基于影响的方法因计算复杂而难以应用。本文受认知科学启发，建立记忆与遗忘的理论联系，提出从增量学习角度出发的IAU算法，显著提升遗忘效率并保持模型性能。实验表明IAU在多个数据集和模型架构中表现优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 01:34:27 GMT</pubDate>
</item>
<item>
<title>Phi-Ground模型提升GUI接地性能，推动计算机使用代理发展</title>
<link>https://arxiv.org/abs/2507.23779</link>
<guid>https://arxiv.org/abs/2507.23779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-Ground在GUI接地任务中取得突破性进展。</p><br /><br /><p><strong>摘要：</strong> 随着多模态推理模型的发展，计算机使用代理（CUAs）正逐渐成为现实。GUI接地是实现实际操作的核心技术，直接影响系统成败。当前的端到端接地模型在ScreenSpot-pro和UI-Vision等挑战性基准上的准确率不足65%，难以部署。本文通过实证研究，从数据收集到模型训练进行了深入探讨，最终提出了Phi-Ground模型家族，在所有五个接地基准测试中表现优异，尤其在10B参数以下的代理设置中达到最先进水平。在端到端模型设置中，Phi-Ground在ScreenSpot-pro和UI-Vision上的得分分别为43.2和27.2，展示了其在实际应用中的潜力。论文还总结了模型构建的经验与教训，对其他感知任务具有借鉴意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:59:09 GMT</pubDate>
</item>
<item>
<title>强化学习在3D环境中的空间推理与泛化能力研究</title>
<link>https://arxiv.org/abs/2507.23698</link>
<guid>https://arxiv.org/abs/2507.23698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升3D环境中视觉运动代理的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）在提升视觉运动代理空间推理和交互能力方面的潜力，特别是在Minecraft等3D环境中的应用。研究提出通过跨视角目标规范构建统一的多任务目标空间，并利用Minecraft的高可定制性实现自动化任务生成，以解决多任务RL中的表示挑战。同时，构建了一个高效的分布式RL框架用于大规模训练。实验结果表明，RL显著提升了交互成功率，实现了对未见过环境的零样本泛化，展示了其在增强视觉运动代理空间推理能力方面的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 12:20:02 GMT</pubDate>
</item>
<item>
<title>ViLLA框架提升机器人操作策略的泛化能力</title>
<link>https://arxiv.org/abs/2507.23682</link>
<guid>https://arxiv.org/abs/2507.23682</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViLLA框架提升机器人操作策略的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ViLLA-X，一种基于视觉-语言-隐动作（latent action）的新型框架，用于学习具有泛化能力的机器人操作策略。该框架改进了隐动作的学习方式及其在预训练中的整合方法，在多个模拟环境和真实机器人平台上表现出色。研究认为，ViLLA范式具有重要潜力，为未来相关研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23682" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:57:46 GMT</pubDate>
</item>
<item>
<title>软最大化注意力机制的递归形式及其表达能力分析</title>
<link>https://arxiv.org/abs/2507.23632</link>
<guid>https://arxiv.org/abs/2507.23632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究软最大化注意力的递归形式以解释其优越性。</p><br /><br /><p><strong>摘要：</strong> 本文通过推导软最大化注意力的递归形式，揭示了其与线性注意力之间的差异。研究表明，软最大化注意力在查询和键的内积上具有更优的非线性特性，从而提升了模型的表达能力。通过将软最大化注意力描述为递归神经网络的形式，可以更好地理解其各个组件的作用及相互关系，进而解释为何软最大化注意力在下游任务中表现更优。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:10:03 GMT</pubDate>
</item>
<item>
<title>基于KAN的双教师知识蒸馏艺术风格分类方法</title>
<link>https://arxiv.org/abs/2507.23436</link>
<guid>https://arxiv.org/abs/2507.23436</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">使用KAN改进双教师框架，提升艺术风格分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对艺术风格分类中数据标注不足和风格特征复杂的问题，提出一种基于Kolmogorov-Arnold Networks (KANs) 的双教师知识蒸馏方法。该方法通过替换传统MLP投影层为KANs，增强对非线性特征关联的建模能力，同时保留两个教师网络的互补信息：一个关注局部纹理与笔触，另一个捕捉整体风格层次。实验表明，该方法在WikiArt和Pandora18k数据集上优于原有双教师架构，提升了Top-1准确率和线性探针性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23436" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>面向阿拉伯语的增强型密集段落检索框架</title>
<link>https://arxiv.org/abs/2507.23404</link>
<guid>https://arxiv.org/abs/2507.23404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种针对阿拉伯语的增强DPR框架，提升问答系统性能。</p><br /><br /><p><strong>摘要：</strong> 阿拉伯语由于其复杂的形态学、可选变音符号以及现代标准阿拉伯语与方言共存的特点，给自然语言处理和信息检索带来了挑战。尽管阿拉伯语在全球的重要性日益增加，但在NLP研究和基准资源中仍处于劣势。本文提出了一种专为阿拉伯语设计的增强型密集段落检索（DPR）框架，核心是引入了注意力相关性评分（ARS）机制，以更有效地建模问题与段落之间的语义相关性。该方法结合了预训练阿拉伯语语言模型和架构优化，显著提升了阿拉伯语问答任务的排名准确性。相关代码已公开在GitHub上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 06:18:28 GMT</pubDate>
</item>
<item>
<title>NeRF-GS：融合NeRF与3D高斯散射的新型框架</title>
<link>https://arxiv.org/abs/2507.23374</link>
<guid>https://arxiv.org/abs/2507.23374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeRF-GS结合NeRF与3DGS，提升3D场景表示性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出NeRF-GS，一种将神经辐射场（NeRF）与3D高斯散射（3DGS）联合优化的新框架。该框架利用NeRF的连续空间表示，解决3DGS在高斯初始化敏感性、空间感知有限和高斯间相关性弱等方面的问题，从而提升其性能。NeRF-GS通过逐步对齐3DGS的空间特征与NeRF，使两者在共享3D空间信息的基础上共同优化。同时，通过优化隐式特征和高斯位置的残差向量，增强3DGS的个性化能力。实验结果表明，NeRF-GS在基准数据集上表现优于现有方法，达到最先进水平，验证了NeRF与3DGS的互补性，为高效3D场景表示提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 05:43:31 GMT</pubDate>
</item>
<item>
<title>TARS：一种改进多模态大语言模型幻觉的偏好优化方法</title>
<link>https://arxiv.org/abs/2507.21584</link>
<guid>https://arxiv.org/abs/2507.21584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TARS提升多模态模型推理可靠性，减少幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文提出TARS，一种基于token自适应的偏好优化策略，旨在解决多模态大语言模型在视觉-语言推理中产生的幻觉问题。传统直接偏好优化方法容易过拟合文本线索，导致对视觉信息的依赖不足。TARS通过最小最大优化框架，在保持语义约束的前提下，增强模型对因果视觉信息的感知能力，从而有效降低幻觉率。实验表明，TARS仅使用4800个偏好样本即可显著提升模型表现，优于标准DPO并接近GPT-4o水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 04:39:19 GMT</pubDate>
</item>
<item>
<title>基于人格向量的大型语言模型行为分析与控制</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过人格向量分析和控制语言模型的行为变化。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型在部署过程中行为变化的潜在原因，通过分析模型激活空间中的人格向量（persona vectors），识别出如邪恶、奉承和幻觉倾向等特质。研究发现，这些向量可用于监控模型人格的变化，并预测和控制训练过程中的性格转变。研究还提出了一种预防性引导方法，以减少或避免不期望的性格变化。此外，人格向量还能用于检测可能导致不良性格变化的训练数据。该方法自动化程度高，只需自然语言描述即可提取人格向量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 01:20:14 GMT</pubDate>
</item>
<item>
<title>农业视觉语言模型评估基准AgroBench的引入与分析</title>
<link>https://arxiv.org/abs/2507.20519</link>
<guid>https://arxiv.org/abs/2507.20519</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgroBench用于评估农业VLM模型性能，揭示其在细粒度识别中的不足。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgroBench，一个用于评估视觉语言模型（VLM）在农业任务中表现的基准测试平台。该基准涵盖七个农业主题，包括203种作物和682种病害分类，由专家农艺师标注。研究发现，尽管VLM在某些任务中表现良好，但在细粒度识别如杂草识别方面仍存在较大提升空间。作者分析了VLM的错误类型，并提出了未来改进方向。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20519" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:58:29 GMT</pubDate>
</item>
<item>
<title>时间对称性在序列模型中的应用研究</title>
<link>https://arxiv.org/abs/2507.14793</link>
<guid>https://arxiv.org/abs/2507.14793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究时间对称性在序列模型中的应用及优势。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将等变性理论应用于序列模型，特别是针对时间参数化的对称性。传统RNN在处理动态变换时表现不佳，而引入时间流等变性后，模型在训练速度、长度泛化和速度泛化方面均有显著提升。该研究为构建更符合现实世界时间对称性的序列模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 22:52:21 GMT</pubDate>
</item>
<item>
<title>Seed-Prover：基于形式验证的数学定理证明模型</title>
<link>https://arxiv.org/abs/2507.23726</link>
<guid>https://arxiv.org/abs/2507.23726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed-Prover在数学定理证明中取得重大突破。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Seed-Prover，一个基于形式验证和长链式推理的数学定理证明模型。该模型通过Lean反馈、已证明引理和自我总结不断优化证明过程，并设计了三种测试时推理策略以应对IMO级别的竞赛问题。Seed-Prover成功证明了78.1%的过去IMO问题，超越了之前的最先进水平。此外，作者还开发了Seed-Geometry几何推理引擎，提升了Lean在几何领域的支持能力。该系统在IMO 2025中成功解决了5道题目，展示了自动化数学推理的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:00:30 GMT</pubDate>
</item>
<item>
<title>构建多语言语音对话模型评估基准</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一个包含中英文的语音对话模型评估数据集。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于语音对话模型（SDMs）的实际效果，指出其在理解与模拟人类对话方面仍存在研究不足。相较于文本大语言模型，语音交互更具复杂性，如歧义、语境依赖等问题。为推动SDM发展，作者构建了一个包含1079个实例的中英文数据集，并引入基于大语言模型的评估方法，以更贴近人类判断的方式全面评估SDM的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>基于用户意图的下一代推荐系统RecGPT</title>
<link>https://arxiv.org/abs/2507.22879</link>
<guid>https://arxiv.org/abs/2507.22879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecGPT通过用户意图驱动提升推荐系统效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种以用户意图为核心的下一代推荐系统RecGPT，旨在解决传统推荐系统依赖历史交互数据导致的过滤气泡和长尾问题。RecGPT通过整合大语言模型，在用户兴趣挖掘、物品检索和解释生成等关键阶段实现意图驱动的推荐。该系统采用多阶段训练策略，并引入人机协作的评估机制，有效提升了推荐系统的多样性与用户满意度。目前RecGPT已在淘宝App全面部署，实验结果表明其在用户、商家和平台三方均取得显著性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Step-3：面向解码效率优化的超大规模视觉语言模型</title>
<link>https://arxiv.org/abs/2507.19427</link>
<guid>https://arxiv.org/abs/2507.19427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-3通过硬件感知设计显著提升长上下文推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Step-3，一个拥有321B参数的视觉语言模型，采用硬件感知的模型与系统协同设计，以最小化解码成本。该模型在两个关键方面进行创新：一是多矩阵分解注意力机制（MFA），有效降低KV缓存和计算量；二是注意力-前馈网络解耦（AFD），将注意力层与前馈网络分离为专用子系统。实验表明，Step-3在长上下文任务中相比DeepSeek-V3和Qwen3 MoE 235B等模型表现出更高的成本效率，并在Hopper GPU上实现了每秒4,039个token的解码吞吐量，创下新纪录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 12:53:13 GMT</pubDate>
</item>
<item>
<title>DreamScene：基于自然语言的高质量可编辑3D场景生成框架</title>
<link>https://arxiv.org/abs/2507.13985</link>
<guid>https://arxiv.org/abs/2507.13985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamScene实现从文本生成高质量、一致且可编辑的3D场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DreamScene，一个端到端的3D场景生成框架，能够根据自然语言或对话生成高质量、可编辑的3D场景。该框架首先通过GPT-4代理进行场景规划，构建混合图结构；随后利用基于图的布局算法生成无碰撞的结构化布局。接着，通过Formation Pattern Sampling（FPS）生成物体几何，并采用渐进式相机采样策略确保全局一致性。最后，系统支持对象移动、外观修改和4D动态运动等精细编辑功能。实验表明，DreamScene在质量、一致性和灵活性方面优于现有方法，为开放域3D内容创作提供实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 10:45:54 GMT</pubDate>
</item>
<item>
<title>MetaCLIP 2：在多语言网络数据上训练的对比语言-图像预训练模型</title>
<link>https://arxiv.org/abs/2507.22062</link>
<guid>https://arxiv.org/abs/2507.22062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaCLIP 2提升多语言图像文本任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MetaCLIP 2，这是首个从全球网络规模图像-文本对中从头训练的对比语言-图像预训练模型。针对非英语数据的筛选和多语言模型性能下降的问题，研究者通过最小改动进行严格消融实验，提出一种能够同时利用英语和非英语数据的训练方法。在零样本ImageNet分类任务中，MetaCLIP 2 ViT-H/14优于其仅限英语的版本和mSigLIP，且在多语言基准测试中取得了新的最先进结果，如CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600图像到文本检索达到64.3%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>MixGRPO：提升图像生成中人类偏好对齐效率的新框架</title>
<link>https://arxiv.org/abs/2507.21802</link>
<guid>https://arxiv.org/abs/2507.21802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MixGRPO通过混合采样策略提升图像生成模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MixGRPO，一种结合随机微分方程（SDE）和常微分方程（ODE）的新型框架，用于改进图像生成中的人类偏好对齐。该方法引入滑动窗口机制，仅在窗口内使用SDE采样和GRPO引导优化，减少优化开销并加速收敛。同时支持高阶求解器，进一步提升了训练效率。实验表明，MixGRPO在多个维度上优于现有方法，训练时间减少了近50%，而其变体MixGRPO-Flash更是将训练时间降低了71%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:40:09 GMT</pubDate>
</item>
<item>
<title>VL-Cogito：基于多阶段渐进式强化学习的多模态推理模型</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VL-Cogito通过PCuRL框架提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出VL-Cogito，一个基于多阶段渐进式强化学习（PCuRL）框架的多模态推理模型。该框架通过逐步增加任务难度，有效提升了模型在不同领域和复杂度下的推理能力。PCuRL引入了在线难度软加权机制和动态长度奖励机制，使模型能根据任务复杂度自适应调整推理路径长度，从而提高推理效率与准确性。实验结果表明，VL-Cogito在数学、科学、逻辑和通用理解等主流多模态基准测试中表现优异，验证了其方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 08:23:21 GMT</pubDate>
</item>
<item>
<title>基于强化学习的差分隐私优化框架RLDP提升语言模型性能</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLDP通过强化学习优化差分隐私，提升语言模型效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLDP框架，将差分隐私优化视为闭环控制问题，利用深度强化学习动态调整梯度裁剪阈值和噪声大小。该方法在多个语言模型上实现显著的困惑度降低和下游任务性能提升，同时保持隐私保护承诺，并减少训练所需的梯度更新次数。实验表明，RLDP在保证隐私的前提下提升了模型效果和训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 06:46:53 GMT</pubDate>
</item>
<item>
<title>提出OmniAVS数据集与OISA模型推动多模态指代音频视觉分割研究</title>
<link>https://arxiv.org/abs/2507.22886</link>
<guid>https://arxiv.org/abs/2507.22886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniAVS提升多模态音频视觉分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了OmniAVS数据集，包含2098个视频和59458条多模态指代表达，具有8种多模态表达形式、强调音频内容理解以及融入复杂推理与常识知识的特点。同时引入了OISA模型，利用多模态大语言模型进行细粒度音频视觉内容理解和推理分割。实验表明，OISA在OmniAVS上表现优于现有方法，并在其他相关任务中取得良好效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>基于测试前置的自动化程序修复方法 Repair-R1</title>
<link>https://arxiv.org/abs/2507.22853</link>
<guid>https://arxiv.org/abs/2507.22853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Repair-R1通过测试前置提升程序修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Repair-R1 的自动化程序修复方法，该方法将测试用例引入模型训练阶段，并在修复前生成区分性测试用例，以提高缺陷定位和修复效果。该方法采用强化学习联合优化测试生成与修复过程，在四个基准测试中表现出色，相比传统方法，修复成功率提升了2.68%至48.29%，测试覆盖率提升了0.78%至53.96%。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:24:05 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的UI到代码自动化转换方法</title>
<link>https://arxiv.org/abs/2507.22827</link>
<guid>https://arxiv.org/abs/2507.22827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多阶段UI-to-code框架，提升代码生成准确性和可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于多智能体框架的UI到前端代码自动化转换方法，该框架分为三个可解释的阶段：接地、规划和生成。接地代理通过视觉语言模型识别UI组件，规划代理利用前端工程先验构建层次布局，生成代理通过自适应提示合成HTML/CSS代码。该方法相比端到端黑盒方法更具鲁棒性、可解释性和准确性。此外，研究还扩展了该框架为可扩展的数据引擎，自动生成大规模图像-代码对，并用于微调开源视觉语言模型，显著提升了UI理解与代码质量。实验表明，该方法在布局准确性、结构连贯性和代码正确性方面均达到最新水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 12:41:21 GMT</pubDate>
</item>
<item>
<title>Falcon-H1：混合架构大语言模型系列提升性能与效率</title>
<link>https://arxiv.org/abs/2507.22448</link>
<guid>https://arxiv.org/abs/2507.22448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Falcon-H1采用混合架构，性能超越多款大模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Falcon-H1系列大语言模型，该模型采用Transformer与状态空间模型（SSM）的混合架构，优化了性能与效率。Falcon-H1提供了多种参数规模的版本，包括0.5B、1.5B、3B、7B和34B等，并支持量化版本。其性能在多个任务中表现优异，甚至在较小参数规模下超越了更大模型。Falcon-H1支持多语言和长上下文处理，适用于广泛的应用场景。所有模型均以开源方式发布，推动AI研究的开放与共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 03:55:33 GMT</pubDate>
</item>
<item>
<title>BANG：一种基于生成爆炸动态的3D对象分解方法</title>
<link>https://arxiv.org/abs/2507.21493</link>
<guid>https://arxiv.org/abs/2507.21493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BANG实现3D对象的直观分解与生成，提升3D设计效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出BANG，一种新型的生成式方法，用于3D对象的部件级分解。其核心是“生成爆炸动态”，通过平滑的爆炸状态序列逐步分离物体部件，同时保持几何和语义一致性。BANG利用预训练的扩散模型，并结合轻量级爆炸视图适配器和时间注意力模块，实现精确控制和流畅过渡。用户可通过空间提示（如边界框和表面区域）指定分解内容，还可结合多模态模型如GPT-4进行2D到3D操作。BANG支持详细部件生成、功能描述关联及组件感知的3D制造流程，适用于3D打印等领域，使创意概念更易转化为详细3D资产。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:21:21 GMT</pubDate>
</item>
<item>
<title>基于生成AI的航空图像车辆检测域适应方法</title>
<link>https://arxiv.org/abs/2507.20976</link>
<guid>https://arxiv.org/abs/2507.20976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用生成AI合成数据提升航空图像车辆检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于生成AI的新型方法，用于改善航空图像中车辆检测模型在不同地理区域间的泛化能力。通过使用微调的潜在扩散模型（LDMs）生成高质量的合成图像及其标注，实现多阶段、多模态的知识迁移，从而缩小源域与目标域之间的分布差异。实验表明，该方法在多个航空图像数据集上显著优于监督学习、弱监督适应、无监督域适应和开放集检测方法。此外，研究者还发布了两个新的新西兰和犹他州的航空图像数据集，以支持相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:38:06 GMT</pubDate>
</item>
<item>
<title>ChemDFM-R：提升化学领域推理能力的大型语言模型</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChemDFM-R在化学领域实现最先进的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对化学领域中大型语言模型理解深度和推理能力不足的问题，提出ChemDFM-R模型。该模型通过构建原子化知识数据集增强化学基础理解，并采用混合来源的知识蒸馏与领域特定强化学习策略，显著提升了化学推理能力。实验表明，ChemDFM-R在多个化学基准测试中表现优异，且输出具有可解释性和逻辑性，增强了人机协作中的可靠性与实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 12:40:49 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型诚实行为的系统评估与基准测试</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大模型在无法回答视觉问题时的诚实行为。</p><br /><br /><p><strong>摘要：</strong> 本文首次对多模态大语言模型（MLLMs）的诚实行为进行了系统评估。研究基于模型对无法回答的视觉问题的响应行为，定义了四种代表性类型的问题，并构建了一个包含12000多个样本的多模态诚实基准（MoHoBench），通过多阶段过滤和人工验证确保数据质量。实验发现，大多数模型在面对无法回答的问题时未能正确拒绝回答，且模型的诚实行为不仅受语言建模影响，还与视觉信息密切相关。为此，研究者尝试使用监督学习和偏好学习方法进行初步对齐，以提升模型的诚实表现，为未来构建可信的多模态大语言模型奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21503" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:55:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的CUDA自动优化框架CUDA-L1</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CUDA-L1通过强化学习实现高效CUDA优化，提升多GPU架构性能。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型对GPU计算资源需求的激增，传统CUDA优化方法已难以满足需求。本文提出CUDA-L1，一个基于强化学习的自动化CUDA优化框架。该框架在NVIDIA A100上训练，平均提升了KernelBench中250个CUDA内核的性能达17.7倍，最高可达449倍。CUDA-L1不仅在多种GPU架构（如H100、RTX 3090等）上表现出色，还能发现多种优化技术并进行策略性组合，揭示CUDA优化的基本原则，并识别潜在性能瓶颈。该方法无需人工干预，仅通过速度奖励信号即可将低效的LLM转化为高效的CUDA优化器，具有广泛的适用性和推广价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:43:56 GMT</pubDate>
</item>
<item>
<title>基于运动引导的少样本视频目标分割研究</title>
<link>https://arxiv.org/abs/2507.22061</link>
<guid>https://arxiv.org/abs/2507.22061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MOVE数据集与DMA方法提升运动引导的视频目标分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对运动引导的少样本视频目标分割（FSVOS）问题，提出一个大规模的MOVE数据集，以弥补现有数据集在动态运动理解上的不足。通过评估多种先进方法，发现当前技术在处理运动引导任务时存在明显不足。为此，作者提出一种新的基线方法DMA，实验表明该方法在少样本运动理解任务中表现优异，为后续研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>基于强化学习的离散自回归模型在图像与语言生成中的应用</title>
<link>https://arxiv.org/abs/2507.22058</link>
<guid>https://arxiv.org/abs/2507.22058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升离散自回归模型的图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Omni的框架，结合了语义图像分词器、统一的自回归模型以及离线扩散解码器，利用强化学习有效减少了图像生成中的伪影，显著提升了生成质量。该方法在使用7B语言模型时表现出色，在图像生成任务中达到了最先进的性能，能够高质量地生成图像，并具备良好的指令遵循能力和长文本渲染能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>HunyuanWorld 1.0：融合文本与图像生成沉浸式3D场景的新框架</title>
<link>https://arxiv.org/abs/2507.21809</link>
<guid>https://arxiv.org/abs/2507.21809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HunyuanWorld 1.0实现高质量3D场景生成，支持交互与兼容现有图形管线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HunyuanWorld 1.0，一种结合视频与3D方法优势的新框架，用于从文本和图像生成沉浸式、可探索和交互的3D场景。该框架具有三个核心优势：通过全景世界代理实现360度沉浸体验、支持网格导出以兼容现有图形管线、以及分离的对象表示增强交互性。其核心是语义分层的3D网格表示，利用全景图像作为语义感知的世界分解和重建工具，从而生成多样化的3D世界。实验表明，该方法在生成连贯、可探索和交互的3D场景方面表现优异，并适用于虚拟现实、物理模拟、游戏开发和互动内容创作等多个领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:43:35 GMT</pubDate>
</item>
<item>
<title>深度学习在非洲野生动物图像分类中的应用与评估</title>
<link>https://arxiv.org/abs/2507.21364</link>
<guid>https://arxiv.org/abs/2507.21364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较了多种深度学习模型在非洲野生动物分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了深度学习技术在非洲野生动物图像分类中的应用，重点评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14等模型的性能。实验结果显示，DenseNet-201在卷积网络中表现最佳，准确率为67%，而ViT-H/14虽然准确率高达99%，但计算成本较高。研究强调了准确率、资源消耗和部署可行性之间的权衡，并将DenseNet-201集成到Hugging Face Gradio Space中，展示了轻量级模型在野外应用的可行性。该工作为非洲本土的AI研究提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 18:18:13 GMT</pubDate>
</item>
<item>
<title>AnimalClue：首个基于间接证据的物种识别大规模数据集</title>
<link>https://arxiv.org/abs/2507.20240</link>
<guid>https://arxiv.org/abs/2507.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimalClue是首个用于从间接证据中识别物种的大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnimalClue，这是首个专注于通过动物足迹、粪便、卵、骨骼和羽毛等间接证据进行物种识别的大规模数据集。该数据集包含159,605个边界框，涵盖5类间接证据，覆盖968个物种、200个科和65个目。每张图像均带有物种级标签、边界框或分割掩码，以及详细的特征信息。与以往主要关注直接视觉特征的数据集不同，AnimalClue在分类、检测和实例分割任务中面临更复杂的挑战，因其需要识别更细微的视觉特征。研究团队对多个视觉模型进行了评估，并指出了从痕迹中识别动物的关键挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 07:48:03 GMT</pubDate>
</item>
<item>
<title>基于最大后验估计的偏好优化方法MaPPO</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaPPO提升语言模型与人类偏好的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Maximum a Posteriori Preference Optimization (MaPPO)的框架，用于从偏好中学习，该方法通过将先验奖励知识整合到优化目标中，改进了传统偏好优化方法。相比DPO及其变体，MaPPO不仅扩展了这一范式，还通过避免过于简化的二分类响应方式提升了对齐效果。此外，MaPPO无需额外超参数，支持离线和在线设置，并可作为插件与SimPO、IPO、CPO等方法结合使用。实验表明，在多个基准测试中，MaPPO在保持计算效率的同时显著提升了对齐性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 01:26:50 GMT</pubDate>
</item>
<item>
<title>基于用户目标状态追踪的对话模拟器研究</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGST框架提升对话模拟器的目标对齐能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对对话人工智能中用户模拟器在多轮对话中难以保持目标导向行为的问题，提出了用户目标状态追踪（UGST）框架。该框架通过跟踪用户目标进展，结合三阶段方法开发能够自主追踪目标并生成符合目标响应的用户模拟器。研究还建立了全面的评估指标，并在两个基准数据集上验证了方法的有效性，显著提升了模拟器的表现。该成果填补了对话AI领域的关键空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 03:07:12 GMT</pubDate>
</item>
<item>
<title>SAND-Math：提升数学推理大语言模型性能的合成数据生成方法</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAND-Math提升数学推理LLM性能，显著优于现有数据集。</p><br /><br /><p><strong>摘要：</strong> 随着对具备复杂数学推理能力的大语言模型（LLMs）需求增加，训练数据的稀缺成为发展瓶颈。本文提出SAND-Math，一种通过生成高质量数学问题并逐步提升难度的合成数据生成管道。实验表明，使用SAND-Math数据可使模型在AIME25基准测试中性能提升17.85个百分点，并通过难度提升步骤将平均问题难度从5.02提高到5.98，进一步提升模型表现。该方法为构建更强大、高效的数学推理LLM提供了实用且可扩展的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 01:17:48 GMT</pubDate>
</item>
<item>
<title>提升主观推理能力的多角色增强框架研究</title>
<link>https://arxiv.org/abs/2507.20187</link>
<guid>https://arxiv.org/abs/2507.20187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多角色框架提升主观推理准确性和多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出MultiRole-R1，一个通过引入多角色视角来增强大型推理模型在主观任务中表现的框架。该框架采用无监督数据生成方法，结合多样化的角色推理链，并利用强化学习中的组相对策略优化技术，将多样性作为奖励信号，以提高推理的准确性和多样性。实验表明，该方法在多个基准测试中均表现出色，展示了多样性增强训练在大型推理模型中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 05:07:42 GMT</pubDate>
</item>
<item>
<title>自演化智能体：从静态模型到动态适应的范式转变</title>
<link>https://arxiv.org/abs/2507.21046</link>
<guid>https://arxiv.org/abs/2507.21046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨自演化智能体的发展与应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了自演化智能体的研究进展，聚焦于其在任务、时机和方法上的演化机制。文章分析了智能体组件（如模型、记忆、工具）的进化方式，分类讨论了不同阶段的适应方法，并探讨了算法与架构设计对演化的影响。此外，文章还评估了相关指标与基准，展示了在编程、教育和医疗等领域的应用，并指出了安全性、可扩展性和协同演化等关键挑战。该研究为构建自适应智能系统提供了框架和方向，助力实现人工智能超级智能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>GenoMAS：结合LLM的基因表达分析新方法</title>
<link>https://arxiv.org/abs/2507.21035</link>
<guid>https://arxiv.org/abs/2507.21035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenoMAS提升基因表达分析效率与准确性。</p><br /><br /><p><strong>摘要：</strong> GenoMAS是一种基于大型语言模型（LLM）的基因表达分析系统，通过集成多个专业化LLM代理，结合结构化工作流与自主适应能力，有效处理复杂的转录组数据。该系统在GenoTEX基准测试中表现出色，数据预处理的综合相似性相关系数达到89.13%，基因识别的F_1值为60.48%，优于现有方法。此外，GenoMAS能发现具有生物学意义的基因-表型关联，并调整潜在混杂因素，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>基于超长输出强化学习的大型语言模型推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UloRL提升LLM推理能力，显著提高训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UloRL的超长输出强化学习方法，旨在提升大型语言模型的推理能力。针对传统强化学习框架在处理超长输出时的低效问题，UloRL通过将输出解码分为短段来提高训练效率，并引入动态掩码机制防止熵崩溃。实验结果表明，该方法在Qwen3-30B-A3B模型上提升了2.06倍的训练速度，并在AIME2025和BeyondAIME任务中分别提高了14.2%和11.2%的性能，效果优于更大型的Qwen3-235B-A22B模型。研究展示了UloRL在超长序列生成中的潜力，并计划开源代码和模型供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 23:42:33 GMT</pubDate>
</item>
<item>
<title>ScenePainter：解决3D场景生成语义漂移问题的新框架</title>
<link>https://arxiv.org/abs/2507.19058</link>
<guid>https://arxiv.org/abs/2507.19058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScenePainter提升3D场景生成的语义一致性与连贯性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScenePainter，一种用于长期且语义一致的3D场景生成框架。现有方法在生成连续视角序列时存在语义漂移问题，主要由于出图模块的累积偏差。ScenePainter通过引入层次化场景概念图（SceneConceptGraph）来构建多层级场景概念之间的关系，指导出图模块生成一致的新视角，并可动态优化以增强多样性。实验表明，该框架有效解决了语义漂移问题，提升了3D视图序列的一致性和沉浸感。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 04:21:12 GMT</pubDate>
</item>
<item>
<title>基于隐式两阶段训练的多变量天气预测方法</title>
<link>https://arxiv.org/abs/2507.17189</link>
<guid>https://arxiv.org/abs/2507.17189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升天气预测准确性。</p><br /><br /><p><strong>摘要：</strong> 文章针对全球气候变化导致极端天气频发的问题，提出一种基于隐式两阶段训练的多变量天气预测方法。该方法通过为每个变量配置独立的编码器和解码器，在第一阶段学习共享潜在空间，第二阶段由翻译器捕捉变量间交互关系，并引入自注意力机制进行多变量融合，显著提升了预测性能。实验表明，该方法在近地面气温和相对湿度预测上分别降低了28.82%和23.39%的均方误差。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:26:56 GMT</pubDate>
</item>
<item>
<title>基于校准奖励的强化学习提升语言模型推理可靠性</title>
<link>https://arxiv.org/abs/2507.16806</link>
<guid>https://arxiv.org/abs/2507.16806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLCR方法提升语言模型推理准确性与置信度校准。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLCR（Reinforcement Learning with Calibration Rewards）方法，通过在强化学习中引入Brier评分来优化语言模型的置信度估计，从而提升模型的准确性和置信度校准。该方法在生成预测的同时输出数值化置信度，并利用结合二进制正确性评分和Brier评分的奖励函数进行训练。实验表明，RLCR在多个数据集上显著提升了置信度校准效果，且不影响准确性，优于传统强化学习和事后置信度分类器。此外，测试时可通过置信度加权方法进一步提高准确性和校准度。结果表明，显式优化置信度有助于构建更可靠的推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于表示空间的多任务学习方法Rep-MTL</title>
<link>https://arxiv.org/abs/2507.21049</link>
<guid>https://arxiv.org/abs/2507.21049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rep-MTL通过表示空间优化提升多任务学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的多任务学习方法Rep-MTL，该方法通过分析任务间的表示空间交互，利用熵惩罚和样本级跨任务对齐来优化任务间的信息共享。与传统依赖损失缩放和梯度调整的方法不同，Rep-MTL关注任务特定优化与共享表示学习之间的相互作用，旨在减少负迁移并增强互补信息的传递。实验表明，即使在简单的等权重策略下，Rep-MTL也能在多个多任务学习基准上取得良好的性能，且具有较高的效率。此外，幂律指数分析进一步验证了其在平衡任务学习与跨任务共享方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:28 GMT</pubDate>
</item>
<item>
<title>4D空间智能重建的多层级方法综述</title>
<link>https://arxiv.org/abs/2507.21045</link>
<guid>https://arxiv.org/abs/2507.21045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出5层4D空间智能重建框架，分析各阶段挑战与发展方向。</p><br /><br /><p><strong>摘要：</strong> 本文综述了从视觉观测中重建4D空间智能的研究进展，提出了一个五层结构的方法体系，涵盖低级3D属性、场景组件、动态场景、交互建模以及物理约束的重建。文章分析了每一层级的关键挑战，并指出了未来研究方向。作者还维护了一个项目页面以跟踪该领域的最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GPT-IMAGE-EDIT-1.5M：推动指令引导图像编辑的开源数据集</title>
<link>https://arxiv.org/abs/2507.21033</link>
<guid>https://arxiv.org/abs/2507.21033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开源图像编辑数据集提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GPT-IMAGE-EDIT-1.5M，一个包含150万组高质量指令-源图-编辑图的公开图像编辑数据集。该数据集通过GPT-4o生成并优化了三个主流图像编辑数据集，提升了视觉质量和指令对齐度。实验表明，基于该数据集微调的模型在多个基准测试中表现优异，显著缩小了与专有模型的差距，为开放研究提供了重要支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>SmallThinker：本地设备上高效运行的大语言模型</title>
<link>https://arxiv.org/abs/2507.20984</link>
<guid>https://arxiv.org/abs/2507.20984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SmallThinker在本地设备上实现高性能大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SmallThinker，一种专为本地设备设计的大型语言模型家族。与传统依赖GPU云基础设施的模型不同，SmallThinker从零开始构建，以适应计算能力弱、内存有限和存储速度慢的限制。其创新点包括两层稀疏结构、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，显著提升了计算效率和内存利用率。SmallThinker-4B-A0.6B和SmallThinker-21B-A3B在性能上达到或超过更大的模型，并且在普通CPU上即可高效运行，无需昂贵的GPU硬件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:45:14 GMT</pubDate>
</item>
<item>
<title>ARC-Hunyuan-Video：提升短视频多模态理解能力的模型</title>
<link>https://arxiv.org/abs/2507.20939</link>
<guid>https://arxiv.org/abs/2507.20939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARC-Hunyuan-Video提升短视频多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ARC-Hunyuan-Video，一个能够处理视频中视觉、音频和文本信息的多模态模型，旨在解决真实世界短视频理解中的挑战。该模型支持多粒度时间戳视频摘要、开放式视频问答、时间视频定位和视频推理等功能。通过高质量数据和多种训练策略，模型在多个任务上表现出色，并在实际部署中提升了用户参与度和满意度。其高效性也得到验证，一分钟后视频的推理时间仅需10秒。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 11:52:36 GMT</pubDate>
</item>
<item>
<title>Music Arena：开放平台推动文本到音乐模型的人类偏好评估</title>
<link>https://arxiv.org/abs/2507.20900</link>
<guid>https://arxiv.org/abs/2507.20900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Music Arena提供实时人类偏好评估，提升文本到音乐模型的评价标准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Music Arena，一个用于文本到音乐（TTM）模型大规模人类偏好评估的开放平台。通过让用户输入文本提示并比较不同系统的输出，平台收集真实用户的偏好数据，形成排行榜。Music Arena不仅遵循其他AI领域的评估趋势，还特别设计了适合音乐的特性，如基于LLM的路由系统和详细偏好数据收集。此外，平台采用滚动数据发布政策，确保用户隐私和数据透明性。该平台旨在解决TTM领域缺乏统一评估标准的问题，并展示如何将实时评估适配到特定AI领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:52:57 GMT</pubDate>
</item>
<item>
<title>基于流匹配的JAM模型实现歌词到歌曲的精细控制</title>
<link>https://arxiv.org/abs/2507.20880</link>
<guid>https://arxiv.org/abs/2507.20880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM模型实现歌词到歌曲的词级时序控制，提升音乐生成质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种基于流匹配的歌词到歌曲生成模型JAM，该模型首次实现了对歌词中每个词的时序和持续时间的精细控制，满足音乐创作中的需求。为了提升生成歌曲的质量，研究者采用了直接偏好优化方法进行美学对齐，无需人工标注即可迭代优化模型。此外，作者还构建了公开评估数据集JAME，以标准化此类模型的评价体系。实验结果显示，JAM在音乐特定属性上优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:34:02 GMT</pubDate>
</item>
<item>
<title>GMPO：一种更稳定的大型语言模型策略优化方法</title>
<link>https://arxiv.org/abs/2507.20673</link>
<guid>https://arxiv.org/abs/2507.20673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GMPO通过几何均值优化提升模型稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何均值策略优化（GMPO）的方法，作为Group Relative Policy Optimization (GRPO) 的改进版本。GMPO通过最大化token级奖励的几何均值，减少了对异常值的敏感性，从而提升了策略更新的稳定性。实验表明，GMPO在多个数学和多模态推理基准测试中表现优于GRPO，如AIME24、AMC、MATH500等，平均性能提升4.1%。研究还提供了理论分析和实验验证，证明了GMPO的设计优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 05:54:05 GMT</pubDate>
</item>
<item>
<title>RICE：提升区域级视觉与OCR能力的对比学习方法</title>
<link>https://arxiv.org/abs/2507.20025</link>
<guid>https://arxiv.org/abs/2507.20025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RICE提升区域级视觉和OCR能力，适用于多种密集预测任务。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RICE的新方法，旨在增强区域级别的视觉和OCR能力。通过构建大规模候选区域数据集，并引入区域Transformer层来提取丰富的区域语义，RICE设计了一个统一的区域聚类判别损失，支持对象和OCR学习。该方法在分割、密集检测和多模态大语言模型的视觉感知任务中表现出色，且已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 13:47:09 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习算法ARPO提升多轮语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARPO算法提升LLM在多轮工具交互中的推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Agentic Reinforced Policy Optimization (ARPO)的新型强化学习算法，旨在提升大语言模型（LLM）在多轮工具交互任务中的表现。研究发现，LLM在使用外部工具后会产生更高的生成不确定性，因此ARPO引入了基于熵的自适应采样机制，动态平衡全局轨迹采样与步骤级采样，以增强高不确定性步骤的探索能力。同时，通过优势归因估计，使LLM能够更好地理解多轮工具使用中的策略差异。实验表明，ARPO在多个计算推理、知识推理和深度搜索任务中优于现有轨迹级强化学习算法，并且仅需一半的工具使用预算即可实现更优性能，为LLM代理在实时动态环境中的对齐提供了可扩展解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 03:53:11 GMT</pubDate>
</item>
<item>
<title>基于前景感知的文档图像校正方法研究</title>
<link>https://arxiv.org/abs/2507.19804</link>
<guid>https://arxiv.org/abs/2507.19804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ForCenNet模型提升文档图像几何校正效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Foreground-Centric Network (ForCenNet) 的新方法，用于消除文档图像中的几何变形，以提高文本识别的准确性。该方法通过提取前景元素作为几何参考，并引入前景感知的标签生成、掩码机制和曲率一致性损失，有效提升了模型对布局元素如文本行和表格边框的校正能力。实验表明，ForCenNet在多个真实世界基准数据集上取得了最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 01:36:48 GMT</pubDate>
</item>
<item>
<title>GEPA：利用自然语言反思提升LLM任务优化的提示优化器</title>
<link>https://arxiv.org/abs/2507.19457</link>
<guid>https://arxiv.org/abs/2507.19457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEPA通过自然语言反思实现高效任务优化，显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出GEPA（Genetic-Pareto）提示优化器，利用自然语言反思来提升大型语言模型（LLM）的任务适应能力。与传统基于稀疏奖励的强化学习方法相比，GEPA通过分析系统级轨迹并进行自然语言诊断，能够高效地生成和测试提示更新。实验表明，GEPA在四个任务中平均比GRPO提升10%，最高达20%，且使用 rollouts 数量减少35倍。同时，GEPA在两个LLM上优于MIPROv2，展现出在代码优化中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:42:32 GMT</pubDate>
</item>
<item>
<title>前沿人工智能模型的风险评估与管理</title>
<link>https://arxiv.org/abs/2507.16534</link>
<guid>https://arxiv.org/abs/2507.16534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">前沿AI模型风险评估揭示关键风险领域。</p><br /><br /><p><strong>摘要：</strong> 本文基于Frontier AI Risk Management Framework，通过E-T-C分析方法评估了前沿人工智能模型的七类关键风险，包括网络攻击、生物化学风险、说服与操控、自主AI研发、战略欺骗、自我复制和合谋。通过AI-45°法则设定红黄线阈值，将风险划分为绿色（可管理）、黄色（需加强控制）和红色（需暂停开发）。实验结果显示，当前所有前沿AI模型均处于绿色和黄色区域，未触及红色警戒线。其中，多数模型在自我复制和战略欺骗方面仍处于绿色区域，而说服与操控风险则普遍处于黄色区域。文章呼吁共同应对AI发展带来的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:44:38 GMT</pubDate>
</item>
<item>
<title>MMBench-GUI：跨平台GUI自动化代理评估基准</title>
<link>https://arxiv.org/abs/2507.19478</link>
<guid>https://arxiv.org/abs/2507.19478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMBench-GUI评估GUI自动化代理的多平台能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMBench-GUI，这是一个用于评估跨Windows、macOS、Linux、iOS、Android和Web平台的GUI自动化代理的分层基准。该基准包含四个层次：GUI内容理解、元素定位、任务自动化和任务协作，涵盖了GUI代理的核心技能。文章还提出了一种新的效率-质量面积（EQA）指标，用于评估在线自动化场景中的执行效率。研究发现，准确的视觉定位是任务成功的关键，模块化框架结合专用定位模块具有显著优势。此外，可靠的GUI自动化需要强大的任务规划和跨平台泛化能力，长上下文记忆、广泛的操作空间和长期推理至关重要。任务效率仍是一个被忽视的维度，所有模型都存在显著低效问题。精确定位、有效规划和早期停止策略的整合对实现高效可扩展的GUI自动化不可或缺。相关代码、数据和环境将公开在GitHub上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>GPTQ与格点算法的数学等价性研究</title>
<link>https://arxiv.org/abs/2507.18553</link>
<guid>https://arxiv.org/abs/2507.18553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPTQ被证明与格点最近平面算法等价，具有理论保障。</p><br /><br /><p><strong>摘要：</strong> 本文揭示了GPTQ在反向执行时与Babai最近平面算法在数学上的等价性。这种等价性基于线性层输入的Hessian矩阵定义的格点问题，为GPTQ提供了几何解释和误差上界保证。这一发现不仅深化了对GPTQ的理解，还为未来大规模语言模型的量化算法设计提供了理论基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:22:18 GMT</pubDate>
</item>
<item>
<title>基于LLM的错误分析工具CLEAR提升模型评估深度</title>
<link>https://arxiv.org/abs/2507.18392</link>
<guid>https://arxiv.org/abs/2507.18392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEAR提供交互式错误分析，揭示模型性能背后的具体原因。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型的评估多依赖其他模型进行打分或排序，但这种方式仅能判断哪个模型更好，无法解释原因。为解决这一问题，研究者提出了CLEAR，一个开源的LLM错误分析工具。CLEAR能够生成逐实例的文本反馈，并识别系统级别的错误问题，同时量化各类问题的出现频率。该工具还提供了交互式仪表盘，支持通过可视化图表、过滤器和实例分析进行深入错误探究。研究展示了CLEAR在RAG和数学基准测试中的应用，并通过用户案例验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:15:21 GMT</pubDate>
</item>
<item>
<title>Specification Self-Correction: 提升语言模型对规范漏洞的自我修正能力</title>
<link>https://arxiv.org/abs/2507.18742</link>
<guid>https://arxiv.org/abs/2507.18742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SSC框架帮助语言模型在推理过程中自我修正规范漏洞，提升任务准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Specification Self-Correction (SSC) 的新框架，使语言模型能够在推理过程中识别并修正自身所依据的规范中的缺陷。该方法通过多步骤推理：首先基于可能有误的规范生成响应，然后对该响应进行批判性分析，并最终修改规范以消除可被利用的漏洞。实验表明，使用 SSC 后，模型在创意写作和代理编码任务中因规范漏洞而偏离用户意图的情况减少了超过 90%。该方法无需修改模型权重，仅在推理阶段完成动态修复，显著提升了模型行为的鲁棒性和对齐度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 14:44:28 GMT</pubDate>
</item>
<item>
<title>基于纯视觉的高效端到端自动驾驶架构PRIX</title>
<link>https://arxiv.org/abs/2507.17596</link>
<guid>https://arxiv.org/abs/2507.17596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRIX通过纯视觉实现高效自动驾驶，无需LiDAR和BEV表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PRIX的端到端自动驾驶架构，仅依赖摄像头数据，无需昂贵的LiDAR传感器和复杂的BEV特征表示。该架构采用视觉特征提取器和生成式规划头，直接从原始像素预测安全轨迹。核心组件Context-aware Recalibration Transformer (CaRT) 提升了多级视觉特征的鲁棒性。实验表明，PRIX在NavSim和nuScenes基准测试中表现优异，性能接近大型多模态扩散规划器，但推理速度更快、模型更小，适合大规模部署。项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 11:28:23 GMT</pubDate>
</item>
<item>
<title>TTD-DR：基于扩散过程的深度研究生成框架</title>
<link>https://arxiv.org/abs/2507.16075</link>
<guid>https://arxiv.org/abs/2507.16075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTD-DR通过迭代优化提升长文本研究报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Test-Time Diffusion Deep Researcher (TTD-DR) 的新框架，旨在解决大型语言模型在生成复杂、长篇研究报告时性能受限的问题。该框架模仿人类研究的迭代过程，将报告生成视为一个扩散过程，从初步草稿出发，通过不断‘去噪’和引入外部信息进行逐步优化。同时，其核心流程结合了自进化算法，以提高生成内容的质量与连贯性。实验表明，TTD-DR在多个需要深度搜索和多跳推理的任务中表现优异，超越了现有研究代理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:23:21 GMT</pubDate>
</item>
<item>
<title>AI视频聊天：实时通信的新范式与优化框架</title>
<link>https://arxiv.org/abs/2507.10510</link>
<guid>https://arxiv.org/abs/2507.10510</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI视频聊天面临延迟挑战，Artic框架提升实时性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了AI视频聊天作为实时通信新范式的潜力，指出由于多模态大语言模型（MLLM）推理耗时长，导致视频传输延迟成为瓶颈。为此，研究提出了Artic框架，通过上下文感知视频流和抗丢包自适应帧率技术，减少带宽消耗并提升AI理解能力。同时，构建了首个Degraded Video Understanding Benchmark（DeViBench）评估视频质量对MLLM的影响，并讨论了未来的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10510" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:34:49 GMT</pubDate>
</item>
<item>
<title>基于双空间建模的局部相关视频检索方法</title>
<link>https://arxiv.org/abs/2507.17402</link>
<guid>https://arxiv.org/abs/2507.17402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HLFormer框架提升视频与文本的局部相关性匹配。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频与文本查询之间部分内容匹配的问题，提出了一种基于双空间建模的局部相关视频检索方法HLFormer。该方法利用双曲空间学习弥补欧几里得空间在层次结构建模上的不足，通过集成洛伦兹注意力模块和欧几里得注意力模块，结合动态特征融合机制，增强了跨模态匹配效果。同时引入部分序保留损失函数，强化文本与视频内容之间的局部相关性。实验表明，该方法优于现有最佳模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 06:59:46 GMT</pubDate>
</item>
<item>
<title>提升多模态上下文学习能力的动态注意力重分配方法</title>
<link>https://arxiv.org/abs/2507.15807</link>
<guid>https://arxiv.org/abs/2507.15807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出DARA方法提升多模态模型的上下文学习能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了当前多模态大语言模型在多模态上下文学习（MICL）中的局限性，指出其过度依赖文本模式而忽视视觉信息。为解决这一问题，作者提出了动态注意力重分配（DARA）策略，以增强模型对视觉信息的关注。同时，构建了TrueMICL数据集，专门用于评估多模态任务中的真实上下文学习能力。实验表明，该方法显著提升了模型的多模态适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:08:18 GMT</pubDate>
</item>
<item>
<title>Iwin Transformer：一种无需位置嵌入的层次化视觉Transformer</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Iwin Transformer通过创新机制实现高效图像处理与任务表现。</p><br /><br /><p><strong>摘要：</strong> Iwin Transformer是一种无需位置嵌入的层次化视觉Transformer，能够从低分辨率直接微调到高分辨率。其核心在于结合交错窗口注意力和深度可分离卷积，使全局信息在单一模块内交换，克服了Swin Transformer需要两个连续块才能近似全局注意力的限制。实验表明，Iwin Transformer在图像分类、语义分割和视频动作识别等任务中表现出色，尤其在ImageNet-1K上达到87.4%的top-1准确率。此外，其核心组件可作为独立模块用于条件图像生成，并为未来研究如Iwin 3D Attention提供启发。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:45:48 GMT</pubDate>
</item>
<item>
<title>Group Sequence Policy Optimization: 提升大语言模型训练效率与稳定性的强化学习算法</title>
<link>https://arxiv.org/abs/2507.18071</link>
<guid>https://arxiv.org/abs/2507.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSPO提升大语言模型训练效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Group Sequence Policy Optimization (GSPO)，一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型。GSPO不同于以往基于token级重要性比率的算法，而是基于序列似然定义重要性比率，并进行序列级裁剪、奖励和优化。实验表明，GSPO在训练效率和性能上优于GRPO算法，尤其在稳定Mixture-of-Experts（MoE）强化学习训练方面表现突出，同时简化了强化学习基础设施的设计。GSPO的优势显著提升了最新Qwen3模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 23:50:32 GMT</pubDate>
</item>
<item>
<title>Agentar-Fin-R1系列金融大模型提升推理与可信度</title>
<link>https://arxiv.org/abs/2507.16802</link>
<guid>https://arxiv.org/abs/2507.16802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agentar-Fin-R1提升金融场景下的推理能力与可信度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了基于Qwen3的Agentar-Fin-R1系列金融大语言模型（8B和32B参数），旨在增强金融应用中的推理能力、可靠性和领域专业化。通过高质量的金融任务标签系统和多层可信保障框架，结合自动化难度感知优化、两阶段训练流程和动态归属系统，显著提升了训练效率。模型在Fineva、FinEval、FinanceIQ等主流金融基准以及MATH-500、GPQA-diamond等通用推理数据集上表现优异，并提出了新的Finova评估基准以测试实际部署能力。实验结果表明，Agentar-Fin-R1在金融任务和通用推理方面均表现出色，具备高可信度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>NABLA：提升视频生成效率的自适应块级注意力机制</title>
<link>https://arxiv.org/abs/2507.13546</link>
<guid>https://arxiv.org/abs/2507.13546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NABLA通过动态自适应块级注意力提升视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出NABLA，一种基于Transformer的视频生成注意力机制，能够动态适应视频中的稀疏模式，从而降低计算复杂度。该方法无需定制低级操作，可与PyTorch的Flex Attention无缝集成。实验表明，NABLA在几乎不损失生成质量的前提下，使训练和推理速度提升了2.7倍。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:36:36 GMT</pubDate>
</item>
<item>
<title>基于深度学习的面部年龄与性别联合分类方法研究</title>
<link>https://arxiv.org/abs/2507.18565</link>
<guid>https://arxiv.org/abs/2507.18565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种联合年龄与性别分类的深度学习模型，提升广告精准度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于深度学习的面部图像年龄与性别联合分类方法，旨在提高目标广告投放的效果。该方法采用定制化的卷积神经网络（CNN）架构，通过共享特征表示同时处理两个任务，优于传统独立处理的方式。模型在大规模多样化数据集上进行训练，并经过预处理以增强对光照、姿态和图像质量变化的鲁棒性。实验结果显示，性别分类准确率达到95%，年龄估计的平均绝对误差为5.77年。研究还分析了不同年龄段的表现差异，指出年轻群体的年龄估计存在挑战，并建议通过数据增强和模型优化来减少偏差。此外，还探讨了不同CNN结构和超参数设置对性能的影响，为未来研究提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:41:26 GMT</pubDate>
</item>
<item>
<title>GLiNER2：统一的高效信息抽取框架</title>
<link>https://arxiv.org/abs/2507.18546</link>
<guid>https://arxiv.org/abs/2507.18546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLiNER2实现多任务信息抽取，提升部署效率。</p><br /><br /><p><strong>摘要：</strong> GLiNER2是一种统一的信息抽取框架，能够在单一模型中支持命名实体识别、文本分类和结构化数据提取。该框架基于预训练的Transformer架构，在保持CPU效率和模型紧凑性的同时，通过直观的模式接口实现多任务组合。实验表明，GLiNER2在抽取和分类任务中表现出色，并在部署便捷性上优于基于大语言模型的方案。项目已开源，提供预训练模型和文档。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:11:14 GMT</pubDate>
</item>
<item>
<title>DriftMoE：一种应对概念漂移的在线专家混合模型</title>
<link>https://arxiv.org/abs/2507.18464</link>
<guid>https://arxiv.org/abs/2507.18464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DriftMoE通过协同训练提升在线学习中的概念漂移适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DriftMoE，一种基于在线专家混合（MoE）架构的模型，用于处理非平稳数据流中的概念漂移问题。该模型通过一个紧凑的神经路由器与增量Hoeffding树专家池协同训练，形成共生学习循环。路由器根据预测选择最合适的专家，专家在获得真实标签后进行增量更新，同时路由器利用多热正确性掩码优化参数，从而加速专家专业化。实验表明，DriftMoE在多个数据流学习基准中表现优异，提供了一种高效且系统的方法来应对概念漂移。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 10:39:20 GMT</pubDate>
</item>
<item>
<title>2024年更新的英文GloVe模型评估报告</title>
<link>https://arxiv.org/abs/2507.18103</link>
<guid>https://arxiv.org/abs/2507.18103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2024年更新的GloVe模型在语言和文化上更具相关性。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了2024年新发布的英文GloVe模型，旨在提升语言模型对现代文化和语言变化的适应能力。相比2014年的原始模型，新模型基于Wikipedia、Gigaword和Dolma数据集进行训练，并详细记录了数据版本和预处理过程。通过词汇比较、直接测试和命名实体识别（NER）任务评估，结果显示新模型在包含非西方新闻数据等时间依赖性任务中表现更优，同时在结构任务如类比和相似性任务中保持良好性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 01:29:18 GMT</pubDate>
</item>
<item>
<title>TeleChat系列模型升级：性能显著提升的多版本发布</title>
<link>https://arxiv.org/abs/2507.18013</link>
<guid>https://arxiv.org/abs/2507.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeleChat系列新版本在训练策略上优化，提升推理与任务表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TeleChat系列的最新版本——TeleChat2、TeleChat2.5和T1，相较于前代模型实现了显著的性能提升。尽管模型架构变化不大，但通过改进的预训练和后训练策略，新模型在代码生成、数学推理等任务中表现出色。T1特别注重复杂推理能力，支持长链式思维，而TeleChat2.5则强调推理速度。两款旗舰模型均为115B参数的密集型Transformer架构，且T1-115B在多项指标上超越了如OpenAI的o1-mini和GPT-4o等专有模型。所有版本已公开发布，供开发者和研究人员使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 21:00:48 GMT</pubDate>
</item>
<item>
<title>基于Spelke对象的视觉分割方法研究</title>
<link>https://arxiv.org/abs/2507.16038</link>
<guid>https://arxiv.org/abs/2507.16038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SpelkeNet，用于识别物理运动关系下的视觉对象。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了计算机视觉中基于语义的分割与人类感知的Spelke对象之间的差异。Spelke对象是根据物理因果运动关系定义的，而非特定类别。作者引入了SpelkeBench数据集，并构建了SpelkeNet模型，通过预测未来运动分布来提取Spelke对象。该模型利用运动可及性图和预期位移图进行统计反事实探测，从而定义Spelke段。实验表明，SpelkeNet在SpelkeBench上优于现有方法，并在物理对象操作任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 16:11:57 GMT</pubDate>
</item>
<item>
<title>基于扩散变换器的皮肤病变分割模型SegDT</title>
<link>https://arxiv.org/abs/2507.15595</link>
<guid>https://arxiv.org/abs/2507.15595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SegDT在皮肤病变分割中表现优异，适用于医疗应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SegDT的新分割模型，基于扩散变换器（DiT），旨在提高皮肤病变分割的准确性和效率。该模型采用Rectified Flow技术，在降低推理步骤的同时保持生成质量，并在低功耗硬件上运行。SegDT在三个基准数据集上进行了评估，结果优于现有方法，且推理速度快，适用于实际医疗场景。研究推动了深度学习在医学图像分析中的应用，为医疗诊断提供了更快速、精准的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:18:05 GMT</pubDate>
</item>
<item>
<title>基于动量不确定性的高效语言模型推理优化方法</title>
<link>https://arxiv.org/abs/2507.14958</link>
<guid>https://arxiv.org/abs/2507.14958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUR方法提升LLM推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Momentum Uncertainty-guided Reasoning (MUR)的新型推理优化方法，旨在提高大型语言模型（LLMs）在推理任务中的效率和准确性。该方法受物理学中动量概念启发，通过跟踪和聚合每一步的不确定性来动态分配思考预算，从而减少冗余计算。研究还引入了gamma-control机制，通过单一超参数调节推理预算。实验结果表明，MUR在多个基准测试中平均减少50%以上的计算量，同时提升准确率0.62-3.37%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 09:36:19 GMT</pubDate>
</item>
<item>
<title>Captain Cinema：基于文本生成高质量短片的框架</title>
<link>https://arxiv.org/abs/2507.18634</link>
<guid>https://arxiv.org/abs/2507.18634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Captain Cinema通过关键帧规划与视频合成生成高质量短片。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Captain Cinema，一个用于生成短片的框架。该框架根据详细的文本描述生成关键帧序列，确保故事和视觉的一致性，随后利用视频合成模型生成时空动态内容。为提升多场景长叙事视频的生成效果，作者引入了针对长上下文数据的交错训练策略，并在专门构建的电影数据集上进行训练。实验表明，Captain Cinema能够在高质量和高效率的前提下实现视觉连贯且叙事一致的短片生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>TTS-VAR：一种高效的视觉自回归模型测试时扩展框架</title>
<link>https://arxiv.org/abs/2507.18537</link>
<guid>https://arxiv.org/abs/2507.18537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTS-VAR提升视觉生成模型性能，提高8.7%得分。</p><br /><br /><p><strong>摘要：</strong> 本文提出TTS-VAR，首个针对视觉自回归（VAR）模型的测试时扩展框架，将生成过程建模为路径搜索问题。通过自适应下降批量大小调度平衡计算效率与探索能力，并引入基于聚类的多样性搜索和基于重采样的潜力选择机制。实验表明，在Infinity模型上提升了8.7%的GenEval分数，揭示早期结构特征对最终质量的影响及不同尺度下重采样效果的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>TeEFusion：一种高效的文本到图像生成蒸馏方法</title>
<link>https://arxiv.org/abs/2507.18192</link>
<guid>https://arxiv.org/abs/2507.18192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeEFusion提升图像生成效率，保持高质量输出。</p><br /><br /><p><strong>摘要：</strong> 本文提出TeEFusion，一种高效的文本到图像生成蒸馏方法。通过将引导强度直接融入文本嵌入中，TeEFusion在不增加额外参数的情况下，使学生模型能够学习教师模型的复杂采样策略。实验表明，该方法显著提升了推理速度，达到教师模型的6倍，同时保持了相近的图像质量。该方法已在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 04:45:40 GMT</pubDate>
</item>
<item>
<title>大规模地球3D生成技术的创新与应用</title>
<link>https://arxiv.org/abs/2507.16535</link>
<guid>https://arxiv.org/abs/2507.16535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EarthCrafter实现千平方公里级3D地球建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出EarthCrafter，一种用于大规模3D地球生成的框架，结合了Aerial-Earth3D数据集和稀疏解耦扩散模型。该方法通过分离结构与纹理生成，有效降低计算成本并保持地理合理性。实验表明其在超大规模生成任务中表现优异，并支持多种应用场景，如语义引导的城市布局生成和无条件地形合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:46:48 GMT</pubDate>
</item>
<item>
<title>Hierarchical Budget Policy Optimization提升推理效率与能力</title>
<link>https://arxiv.org/abs/2507.15844</link>
<guid>https://arxiv.org/abs/2507.15844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HBPO提升模型推理效率同时保持准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Hierarchical Budget Policy Optimization (HBPO)框架，通过分层预算探索和差异化奖励机制，使模型根据问题复杂度自动调整推理深度。该方法有效解决传统方法在效率训练中探索空间塌陷的问题，减少平均token使用量达60.6%，同时提升准确率3.14%。HBPO无需外部约束或离散模式选择，展现出模型自适应调整推理深度的能力，证明推理效率与能力可以协同优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:52:34 GMT</pubDate>
</item>
<item>
<title>LAPO：通过自适应策略优化实现高效推理的框架</title>
<link>https://arxiv.org/abs/2507.15758</link>
<guid>https://arxiv.org/abs/2507.15758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LAPO提升模型推理效率并减少token使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Length-Adaptive Policy Optimization (LAPO)的新框架，旨在将推理长度控制从外部约束转化为模型的内在能力。该框架通过两阶段强化学习过程，使模型能够学习自然的推理模式，并在推理过程中动态调整计算资源。实验表明，LAPO在数学推理基准测试中可减少高达40.9%的token使用量，同时提升2.3%的准确性。分析显示，LAPO训练的模型能根据问题复杂度有效分配计算资源，实现高效且高质量的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:14:41 GMT</pubDate>
</item>
<item>
<title>DMOSpeech 2：通过强化学习优化语音合成的持续预测器</title>
<link>https://arxiv.org/abs/2507.14988</link>
<guid>https://arxiv.org/abs/2507.14988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DMOSpeech 2优化了语音合成中的持续预测，提升整体性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DMOSpeech 2，这是一种基于扩散模型的文本转语音系统，通过强化学习方法优化了持续预测器，提高了语音合成的整体质量。该系统采用了一种新的持续策略框架，利用说话人相似性和词错误率作为奖励信号。此外，引入了教师引导采样方法，在保持效率的同时提升了输出多样性。实验结果表明，DMOSpeech 2在所有指标上均优于之前系统，并减少了采样步骤而未影响质量，标志着语音合成系统在多组件优化方面的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 10:48:48 GMT</pubDate>
</item>
<item>
<title>Promptomatix：自动提示优化框架提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.14241</link>
<guid>https://arxiv.org/abs/2507.14241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Promptomatix实现自动提示优化，提升LLM性能。</p><br /><br /><p><strong>摘要：</strong> Promptomatix是一个自动提示优化框架，能够将自然语言任务描述转换为高质量提示，无需手动调整或领域知识。该框架支持基于元提示的优化器和DSPy驱动的编译器，具备模块化设计，便于未来扩展。系统通过分析用户意图、生成合成训练数据、选择提示策略并使用成本感知目标优化提示，已在五个任务类别中表现出色，优于现有库，同时减少提示长度和计算开销，提高效率和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 14:18:20 GMT</pubDate>
</item>
<item>
<title>Pusa：基于向量化时间步适应的视频扩散模型新范式</title>
<link>https://arxiv.org/abs/2507.16116</link>
<guid>https://arxiv.org/abs/2507.16116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pusa通过VTA实现高效视频生成与多任务能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Pusa，一种基于向量化时间步适应（VTA）的视频扩散模型新范式。该方法在保持基础模型能力的同时，实现了精细的时间控制，显著提升了图像到视频生成的效率和性能。实验表明，Pusa在VBench-I2V基准上得分87.32%，远超现有模型，且具备零样本多任务能力，如起始帧、视频扩展等，同时支持文本到视频生成。VTA技术有效避免了计算冗余和灾难性遗忘问题，为下一代视频合成提供了可扩展、高效且通用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 20:09:37 GMT</pubDate>
</item>
<item>
<title>Yume: An Interactive World Generation Model</title>
<link>https://arxiv.org/abs/2507.17744</link>
<guid>https://arxiv.org/abs/2507.17744</guid>
<content:encoded><![CDATA[
Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>DesignLab：通过迭代优化提升幻灯片设计质量</title>
<link>https://arxiv.org/abs/2507.17202</link>
<guid>https://arxiv.org/abs/2507.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DesignLab通过角色分离实现幻灯片设计的持续优化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为DesignLab的系统，旨在帮助非专业人士设计高质量的幻灯片。该系统将设计过程分为设计审查者和设计贡献者两个角色，审查者负责识别设计问题，贡献者则进行修正，形成一个迭代优化的流程。通过微调大语言模型并引入受控扰动模拟中间草稿，DesignLab能够有效学习设计错误和修复方法。实验表明，该方法在设计质量上优于现有工具，能生成更专业、精美的幻灯片。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:49:48 GMT</pubDate>
</item>
<item>
<title>RAVine：面向代理式搜索的现实对齐评估框架</title>
<link>https://arxiv.org/abs/2507.16725</link>
<guid>https://arxiv.org/abs/2507.16725</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAVine提升代理式搜索系统的评估准确性与实用性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RAVine，一个面向代理式大语言模型搜索的现实对齐评估框架。针对现有评估体系在复杂查询、细粒度评估和迭代过程分析方面的不足，RAVine通过多点查询和长文本回答更贴近用户意图，并引入可追溯的地面真实数据构建策略，提高评估精度。同时，RAVine关注模型在搜索工具交互中的表现及效率因素。实验结果揭示了代理式搜索系统的发展方向，相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16725" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:08:12 GMT</pubDate>
</item>
<item>
<title>文本到图像扩散模型中的记忆与隐私问题研究</title>
<link>https://arxiv.org/abs/2507.16880</link>
<guid>https://arxiv.org/abs/2507.16880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到图像模型易复制训练数据，现有防御措施效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了文本到图像扩散模型在数据隐私和知识产权方面的潜在风险。尽管已有方法尝试通过剪枝来减少模型对训练数据的复制，但实验表明，仅需微调输入提示的文本嵌入即可重新触发数据复制，说明现有方法并不稳固。此外，研究挑战了记忆局部化的假设，证明复制行为可以在文本嵌入空间的不同位置被触发，并且路径各异。这表明当前的缓解策略不足以解决问题，应寻求真正删除记忆内容的方法。为此，作者提出了一种新的对抗性微调方法，以提升模型的鲁棒性。研究为构建更可信、合规的生成式AI提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:02:38 GMT</pubDate>
</item>
<item>
<title>基于形式语言的大型语言模型验证方法研究</title>
<link>https://arxiv.org/abs/2507.16331</link>
<guid>https://arxiv.org/abs/2507.16331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用形式语言提升LLM验证可靠性与可扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于形式语言（如Dafny）的大型语言模型（LLM）验证方法，以解决传统基于自然语言的验证不可靠、不可扩展的问题。通过引入自动数据整理流程和结合形式语言验证器反馈的强化学习设计，研究团队构建了DafnyComp基准测试集，并在监督微调阶段使小型模型生成可验证的Dafny代码，效果优于现有专有模型。进一步的正则化强化学习提升了模型在跨域任务中的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 04:13:01 GMT</pubDate>
</item>
<item>
<title>高效3D生成框架Ultra3D提升稀疏体素建模速度</title>
<link>https://arxiv.org/abs/2507.17745</link>
<guid>https://arxiv.org/abs/2507.17745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ultra3D通过优化注意力机制实现高速高质3D建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出Ultra3D，一种高效的3D生成框架，旨在解决现有方法在稀疏体素建模中的计算效率问题。该框架利用VecSet表示法在第一阶段快速生成粗略物体布局，减少令牌数量并加速体素坐标预测。第二阶段引入Part Attention机制，仅在语义一致的部分区域进行注意力计算，从而保持结构连续性并避免不必要的全局注意力，实现高达6.7倍的生成速度提升。此外，构建了可扩展的部件注释流程，将原始网格转换为带标签的稀疏体素。实验表明，Ultra3D支持1024分辨率的高质量3D生成，并在视觉保真度和用户偏好方面达到领先水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>多领域推理在强化学习中的系统研究</title>
<link>https://arxiv.org/abs/2507.17512</link>
<guid>https://arxiv.org/abs/2507.17512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多领域推理在强化学习中的交互机制与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统探讨了在强化学习框架下，数学推理、代码生成和逻辑谜题解决等多领域推理的交互机制。通过GRPO算法和Qwen-2.5-7B模型进行实验，分析了单领域训练下的性能提升与跨领域泛化能力，同时研究了多领域联合训练中出现的相互促进与冲突现象。此外，还比较了基础模型与指令微调模型在相同强化学习配置下的表现差异，并深入探讨了课程学习、奖励设计及语言特性对训练效果的影响。实验结果揭示了领域间互动的关键因素，为提升大语言模型的多领域推理能力提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 09:51:04 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的感知能力评估与Turing Eye Test基准</title>
<link>https://arxiv.org/abs/2507.16863</link>
<guid>https://arxiv.org/abs/2507.16863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前多模态模型在感知任务中表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在感知能力方面与人类的差距，提出了一种名为Turing Eye Test（TET）的感知导向基准测试。该基准包含四个诊断任务，用于评估模型在合成图像上的表现。研究发现，尽管最先进的MLLMs在语言推理任务中表现良好，但在感知任务上却出现严重失败，而仅通过语言模型的微调无法提升其性能。只有对视觉模块进行微调才能显著改善表现，表明当前MLLMs在视觉泛化能力上仍存在重大不足。作者发布了部分TET任务，并计划在未来引入更多任务以提升视觉泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:50:16 GMT</pubDate>
</item>
<item>
<title>Elevate3D：提升低质量3D模型质量的新框架</title>
<link>https://arxiv.org/abs/2507.11465</link>
<guid>https://arxiv.org/abs/2507.11465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Elevate3D提升低质量3D模型的纹理与几何质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Elevate3D，一种用于提升低质量3D模型质量的新框架。该框架通过HFS-SDEdit方法显著改善纹理质量，同时保留原始外观和几何结构，并修复其退化问题。Elevate3D采用逐视角优化策略，结合先进的单目几何预测器，实现纹理与几何的协同优化。相比现有方法，Elevate3D在3D模型精修方面达到最新水平，有效缓解高质量3D资源短缺的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 12:36:20 GMT</pubDate>
</item>
<item>
<title>PrefPalette：基于属性分解的人类偏好建模框架</title>
<link>https://arxiv.org/abs/2507.13541</link>
<guid>https://arxiv.org/abs/2507.13541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefPalette提升AI个性化能力，增强可解释性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了PrefPalette，一个将人类偏好分解为属性维度并根据不同社会群体价值观进行个性化预测的框架。该方法通过生成合成数据和注意力机制，使AI更准确地理解用户偏好，并在Reddit的45个社区中表现出比GPT-4o更高的预测准确率。此外，它揭示了不同社区的偏好特征，如学术群体重视详尽与刺激，冲突导向群体偏好讽刺与直接表达，支持型群体强调同理心。PrefPalette不仅提升了偏好建模效果，还提供了透明、可解释的洞察，为更可信的个性化应用奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:21:54 GMT</pubDate>
</item>
<item>
<title>面向目标检测的新型零样本量化框架</title>
<link>https://arxiv.org/abs/2507.16782</link>
<guid>https://arxiv.org/abs/2507.16782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种任务特定的零样本量化方法提升目标检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种面向目标检测的新型零样本量化框架（ZSQ），通过引入边界框和类别采样策略生成任务相关的校准集，并将任务特定训练融入知识蒸馏过程，从而恢复量化检测网络的性能。实验表明该方法在MS-COCO和Pascal VOC数据集上表现出色，具有较高的效率和先进性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:28:29 GMT</pubDate>
</item>
<item>
<title>ExpTeach：通过自我生成记忆实现视觉语言模型与机器人的有效融合</title>
<link>https://arxiv.org/abs/2507.16713</link>
<guid>https://arxiv.org/abs/2507.16713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ExpTeach提升机器人任务成功率，增强空间理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出ExpTeach框架，通过构建真实世界经验的自我生成记忆，将原本在互联网数据上训练的视觉语言模型（VLMs）有效地应用于物理机器人。ExpTeach使VLM能够自主规划动作、验证结果、反思失败并调整行为，同时将经验总结为长期记忆，用于未来任务的检索增强生成（RAG）。此外，该框架还引入按需图像标注模块，提升VLM的空间理解能力。实验表明，ExpTeach显著提高了机器人任务的成功率，从36%提升至84%，并在12个真实场景中展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>区域自适应潜在上采样提升扩散模型推理效率</title>
<link>https://arxiv.org/abs/2507.08422</link>
<guid>https://arxiv.org/abs/2507.08422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RALU方法提升扩散模型推理速度且保持图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为区域自适应潜在上采样（RALU）的训练-free 框架，用于加速基于扩散的图像和视频生成模型。RALU通过在空间维度上进行混合分辨率采样，包括低分辨率去噪、特定区域的自适应上采样以及全分辨率细节优化，显著提升了推理效率。同时，通过噪声-时间步重新调度技术，确保不同分辨率之间的生成稳定性。实验表明，RALU在FLUX和Stable Diffusion 3上分别实现了7.0倍和3.0倍的速度提升，且图像质量损失极小。该方法还可与现有时间维度加速技术兼容，进一步降低推理延迟而不影响生成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:07:43 GMT</pubDate>
</item>
<item>
<title>ThinkAct：基于视觉潜在规划的多模态推理与动作执行框架</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkAct提升多模态AI任务的长程规划与自修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ThinkAct框架，通过结合高层推理与低层动作执行，实现更高效的多模态任务处理。该框架利用强化学习生成与动作对齐的视觉奖励，指导多模态大模型生成具有一致性和目标导向的推理计划，并将其压缩为视觉潜在表示以指导后续动作执行。实验表明，ThinkAct在复杂环境任务中展现出少样本适应、长程规划和自我修正的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SOPHIA提升视觉语言模型的慢思考推理能力</title>
<link>https://arxiv.org/abs/2507.16814</link>
<guid>https://arxiv.org/abs/2507.16814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SOPHIA增强LVLM的多模态推理能力，效果显著。</p><br /><br /><p><strong>摘要：</strong> 本文提出SOPHIA，一种用于视觉语言模型（LVLM）的半离策略强化学习方法，旨在提升其慢思考推理能力。通过结合可训练LVLM的视觉理解与语言模型的离策略推理，SOPHIA生成基于结果的奖励，并将视觉奖励反向传播，使LVLM能够从推理轨迹中学习。实验表明，SOPHIA在多个多模态推理基准测试中表现优异，尤其在InternVL3.0-38B上提升了8.5%，甚至超越部分闭源模型。分析显示，SOPHIA优于监督微调和直接在线策略方法，为后续在线策略训练提供了更好的策略初始化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于多模态大模型的人-物交互合成方法研究</title>
<link>https://arxiv.org/abs/2507.16813</link>
<guid>https://arxiv.org/abs/2507.16813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HOComp方法实现自然人-物交互合成。</p><br /><br /><p><strong>摘要：</strong> 本文针对图像中人与物体交互合成的挑战，提出HOComp方法，通过MLLMs驱动的区域姿态引导和细节一致外观保持机制，实现更自然、和谐的人-物交互合成。研究还构建了首个交互感知的人-物合成数据集IHOC，实验表明该方法在视觉质量和一致性上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>构建科学推理数据集与模型提升AI在自然科学中的表现</title>
<link>https://arxiv.org/abs/2507.16812</link>
<guid>https://arxiv.org/abs/2507.16812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建高质量科学推理数据集，提升AI在自然科学中的研究能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出TextbookReasoning和MegaScience两个大型科学推理数据集，旨在填补人工智能在自然科学领域研究的空白。TextbookReasoning包含12,000本大学教材中的650,000道推理题，涵盖7个科学领域；MegaScience则整合了1.25百万条高质量数据，通过系统性实验优化数据选择方法。研究还构建了覆盖15个基准测试的评估体系，确保准确衡量模型性能。实验表明，基于MegaScience训练的Llama3.1、Qwen2.5和Qwen3系列模型在平均性能上优于官方指令模型，并展现出规模效应。作者已公开数据集、评估系统及训练模型，推动科学推理研究发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>无需修改训练数据的大型语言模型泛化控制方法</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAFT技术通过概念消融提升模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为概念消融微调（CAFT）的新方法，用于控制大型语言模型在微调后的泛化行为，而无需修改训练数据。该方法利用可解释性工具，在微调过程中通过线性投影消除模型中的不良概念，从而避免不必要的泛化。实验表明，CAFT在三个微调任务中有效减少了错误泛化现象，且不损害模型在训练分布上的性能。这种方法为控制模型泛化提供了一种新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16795" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:45:04 GMT</pubDate>
</item>
<item>
<title>突破大语言模型推理瓶颈的Thread Inference Model</title>
<link>https://arxiv.org/abs/2507.16784</link>
<guid>https://arxiv.org/abs/2507.16784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TIM与TIMRUN提升大模型推理能力与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Thread Inference Model (TIM) 和 TIMRUN 推理运行时，以解决大语言模型（LLM）在推理准确性和效率上的限制。TIM 是一种用于递归和分解问题解决的 LLM 家族，而 TIMRUN 支持超越上下文限制的长周期结构化推理。通过将自然语言建模为推理树，TIM 实现了几乎无限的工作内存和多跳工具调用，克服了输出限制、位置嵌入约束和 GPU 内存瓶颈。实验表明，该系统在处理数学任务和需要长周期推理的信息检索中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:30:04 GMT</pubDate>
</item>
<item>
<title>Zebra-CoT数据集提升多模态视觉链式推理能力</title>
<link>https://arxiv.org/abs/2507.16746</link>
<guid>https://arxiv.org/abs/2507.16746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra-CoT提升视觉链式推理模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Zebra-CoT，一个包含182,384个样本的大规模多模态数据集，用于训练视觉链式推理（Visual CoT）模型。该数据集涵盖科学问题、二维和三维推理任务以及逻辑与策略游戏等场景。实验表明，在Zebra-CoT上微调Anole-7B模型可使测试集准确率提升12%，在标准VLM基准测试中性能提高最多13%。Bagel-7B模型在该数据集上微调后能生成高质量的多模态推理链，证明了Zebra-CoT在提升多模态推理能力方面的有效性。作者开源了数据集和模型以支持相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:35:36 GMT</pubDate>
</item>
<item>
<title>Step-Audio 2：面向工业级音频理解与语音对话的端到端多模态大语言模型</title>
<link>https://arxiv.org/abs/2507.16632</link>
<guid>https://arxiv.org/abs/2507.16632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Audio 2 提升了语音识别与音频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Step-Audio 2，这是一个面向工业级音频理解和语音对话的端到端多模态大语言模型。通过整合潜在音频编码器和以推理为中心的强化学习（RL），Step-Audio 2 在自动语音识别（ASR）和音频理解方面表现出色。该模型还引入了离散音频标记生成，增强了对语调、情感等副语言信息的响应能力。此外，Step-Audio 2 结合了检索增强生成（RAG）技术，并支持调用外部工具如网络搜索和音频搜索，以减少幻觉并实现音色切换。经过数百万小时的语音和音频数据训练，Step-Audio 2 在多种对话场景中展现出强大的智能和表现力。评估结果表明，其在多个音频理解和对话基准测试中均优于其他开源和商业解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 10:23:55 GMT</pubDate>
</item>
<item>
<title>推理时计算增强模型鲁棒性的安全风险分析</title>
<link>https://arxiv.org/abs/2507.15974</link>
<guid>https://arxiv.org/abs/2507.15974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时计算提升模型鲁棒性，但可能暴露安全风险。</p><br /><br /><p><strong>摘要：</strong> 本文研究发现，小型开源模型也能通过推理时的预算强制策略提升鲁棒性。然而，研究揭示了先前研究中的一个隐含假设：中间推理步骤对攻击者是隐藏的。当这一假设被打破，即中间步骤可被访问时，推理时计算反而会降低模型的鲁棒性，呈现出反向缩放规律。此外，文章指出在工具集成和高级推理提取攻击等场景下，即使推理链被隐藏，模型仍可能受到攻击。因此，作者强调在实际应用中需权衡推理时计算带来的安全与性能之间的微妙平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 14:08:38 GMT</pubDate>
</item>
<item>
<title>ObjectGS：融合语义理解的3D场景重建框架</title>
<link>https://arxiv.org/abs/2507.15454</link>
<guid>https://arxiv.org/abs/2507.15454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectGS实现对象级3D重建与语义理解的统一。</p><br /><br /><p><strong>摘要：</strong> 本文提出ObjectGS，一个将3D场景重建与语义理解相结合的对象感知框架。不同于传统方法将场景视为整体，ObjectGS通过将每个物体建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。训练过程中动态调整锚点并优化特征，同时使用one-hot ID编码和分类损失确保语义清晰。实验表明，ObjectGS在开放词汇和全景分割任务中优于现有方法，并支持网格提取和场景编辑等应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:06:23 GMT</pubDate>
</item>
<item>
<title>SPAR：基于多智能体框架的学术文献检索新方法</title>
<link>https://arxiv.org/abs/2507.15245</link>
<guid>https://arxiv.org/abs/2507.15245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPAR提升学术文献检索性能，优于现有基线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SPAR，一种基于多智能体框架的学术文献检索系统，结合RefChain进行查询分解和演化，以提高搜索的灵活性和有效性。为了系统评估，研究者构建了SPARBench基准测试集，包含专家标注的相关性标签。实验结果表明，SPAR在AutoScholar和SPARBench数据集上分别比最佳基线提升了56%和23%的F1分数。SPAR与SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 01:06:53 GMT</pubDate>
</item>
<item>
<title>基于强化学习的RefCritic模块提升语言模型批判能力</title>
<link>https://arxiv.org/abs/2507.15024</link>
<guid>https://arxiv.org/abs/2507.15024</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefCritic通过双重奖励机制提升模型批判与优化能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型中批评模块开发的挑战，提出了一种基于强化学习的长链思维批评模块RefCritic。该模块采用双重规则奖励机制，分别评估解决方案的实例级正确性和策略模型的细化准确性，以生成高质量且具有行动指导性的反馈。在多个基准测试中，RefCritic表现出显著优势，尤其在数学推理任务中优于传统步骤级监督方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15024" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:19:51 GMT</pubDate>
</item>
<item>
<title>基于MCP的LLM智能体评估框架MCPEval</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCPEval实现LLM智能体自动化评估与标准化测试。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MCPEval，一个基于Model Context Protocol (MCP) 的开源框架，用于自动化生成任务并深度评估大型语言模型（LLM）智能体在多个领域中的表现。该框架通过标准化指标、与原生工具集成，减少了人工构建评估流程的工作量。实验结果表明，MCPEval在五个现实场景中有效揭示了模型在特定领域的性能特征，并已公开发布以促进可复现和标准化的LLM评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 01:46:27 GMT</pubDate>
</item>
<item>
<title>LLM生成学生风格代码的系统研究</title>
<link>https://arxiv.org/abs/2507.12674</link>
<guid>https://arxiv.org/abs/2507.12674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究LLM生成类似学生代码的能力与方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出ParaStudent，研究大型语言模型在编程课程中生成类似学生代码的能力。通过分析多个学期的学生提交记录，设计低分辨率和高分辨率实验，评估代码在语义、功能和风格维度的表现。结果表明，微调能显著提升与真实学生代码轨迹的一致性，更准确地捕捉错误模式、逐步改进和风格变化。研究强调了通过上下文感知生成、时间建模和多维评估来模拟真实学生代码的重要性。代码和实验数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 19:12:14 GMT</pubDate>
</item>
<item>
<title>机器学习中的串行计算挑战与未来发展方向</title>
<link>https://arxiv.org/abs/2507.12549</link>
<guid>https://arxiv.org/abs/2507.12549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">串行计算问题限制了当前机器学习的进展。</p><br /><br /><p><strong>摘要：</strong> 文章指出，尽管机器学习在大规模并行化方面取得了进展，但某些问题本质上是顺序的，如数学推理、物理模拟和序列决策等，这些任务需要依赖性的计算步骤，无法并行处理。基于复杂性理论，作者明确了这一区别，并展示了当前以并行为中心的架构在处理此类任务时存在根本性局限。文章强调，识别计算的串行性质对机器学习、模型设计和硬件发展具有深远影响，认为在AI面对更复杂的推理任务时，应更加重视串行计算的扩展，而不仅仅是并行计算。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12549" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 14:01:26 GMT</pubDate>
</item>
<item>
<title>基于去噪的潜在令牌化器设计研究</title>
<link>https://arxiv.org/abs/2507.15856</link>
<guid>https://arxiv.org/abs/2507.15856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型去噪令牌化器，提升生成模型效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉令牌化器在生成建模中的有效性问题，指出现代生成模型都具有从噪声或遮蔽输入中重建清晰信号的训练目标，这一过程称为去噪。受此启发，作者提出将令牌化器嵌入直接与下游去噪目标对齐，使潜在嵌入在严重干扰下仍易于重建。为此，引入了Latent Denoising Tokenizer（l-DeTok），该令牌化器通过插值噪声和随机遮蔽来重建清晰图像。实验表明，在ImageNet 256x256数据集上，l-DeTok在六种代表性生成模型中均优于标准令牌化器。研究强调去噪是令牌化器设计的重要原则，希望为未来设计提供新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于概念驱动的视频目标分割框架SeC及其性能评估</title>
<link>https://arxiv.org/abs/2507.15852</link>
<guid>https://arxiv.org/abs/2507.15852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeC提升视频目标分割效果，实现更高精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于概念驱动的视频目标分割框架SeC，旨在解决传统方法在处理视觉变化、遮挡和复杂场景时的不足。SeC利用大视觉语言模型构建高阶、以对象为中心的表示，增强语义理解能力，并在推理过程中动态调整计算资源。为评估该方法，研究者还构建了SeCVOS基准数据集，包含160个精心标注的多场景视频。实验表明，SeC在SeCVOS上比SAM 2.1提升了11.8个百分点，成为当前最先进的概念感知视频目标分割方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GUI-G^2：基于高斯分布的图形用户界面定位奖励框架</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI-G^2通过高斯分布提升GUI交互的精确性与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为GUI Gaussian Grounding Rewards (GUI-G^2) 的新型奖励框架，用于改进图形用户界面（GUI）中的自然语言指令定位任务。传统方法依赖二值奖励，导致信号稀疏且忽略空间连续性。而GUI-G^2通过将GUI元素建模为连续高斯分布，引入了两种协同机制：高斯点奖励用于精确定位，覆盖奖励用于评估空间对齐度。此外，该框架还包含自适应方差机制，以处理不同尺寸的界面元素。实验表明，GUI-G^2在多个基准测试中显著优于现有方法，特别是在ScreenSpot-Pro上提升了24.7%。研究显示，连续建模增强了模型对界面变化的鲁棒性和对未知布局的泛化能力，为GUI交互任务提供了新的范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:53:42 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的经济政策模拟框架</title>
<link>https://arxiv.org/abs/2507.15815</link>
<guid>https://arxiv.org/abs/2507.15815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM Economist通过代理建模实现经济政策设计与评估。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为LLM Economist的新框架，利用基于代理的建模方法，在具有层级决策的战略环境中设计和评估经济政策。底层由有限理性的工人代理组成，这些代理基于美国人口普查数据生成，以最大化文本定义的效用函数。上层的规划代理则通过上下文强化学习提出分段线性边际税率方案。该框架具备优化异质效用、生成大规模真实人口代理以及完全用自然语言表达机制设计的能力。实验表明，该框架在一百个交互代理中能够接近斯塔克尔伯格均衡，提升社会福利，并在去中心化治理下通过定期投票进一步提高效果。结果证明，基于大语言模型的代理可以共同建模、模拟和治理复杂经济系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:21:14 GMT</pubDate>
</item>
<item>
<title>基于熵感知的强化学习方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.15778</link>
<guid>https://arxiv.org/abs/2507.15778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Archer方法提升LLM推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Archer的熵感知强化学习方法，旨在提升大语言模型（LLM）的推理能力。传统RLVR方法对所有token应用相同的训练信号，而忽略了低熵知识类token与高熵推理类token的不同作用。Archer通过双token约束和同步更新机制，对推理token施加较弱的KL正则化和较高的裁剪阈值以鼓励探索，同时对知识token施加强约束以保持事实准确性。实验结果表明，该方法在多个数学推理和代码生成基准上表现优异，达到或超过同规模模型的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:34:01 GMT</pubDate>
</item>
<item>
<title>TokensGen：基于压缩标记的长视频生成框架</title>
<link>https://arxiv.org/abs/2507.15728</link>
<guid>https://arxiv.org/abs/2507.15728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokensGen通过压缩标记提升长视频生成的连贯性与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出TokensGen，一种两阶段框架，旨在解决长视频生成中的记忆瓶颈和长期不一致问题。该方法将长视频生成分解为内片段语义控制、长期一致性控制和片段间平滑过渡三个核心任务。首先训练To2V模型，利用文本和视频标记生成短视频；其次引入T2To模型，一次性生成所有标记以确保全局一致性；最后在推理阶段采用自适应FIFO-Diffusion策略实现片段间的无缝连接。实验表明，该方法在保持计算效率的同时显著提升了长视频的时间和内容连贯性，为叙事、电影制作和沉浸式模拟提供了可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 11:37:33 GMT</pubDate>
</item>
<item>
<title>基于数据混合代理的持续预训练方法</title>
<link>https://arxiv.org/abs/2507.15640</link>
<guid>https://arxiv.org/abs/2507.15640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出数据混合代理模型，提升语言模型在新领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Data Mixing Agent的端到端框架，用于在持续预训练中优化源领域和目标领域数据的混合比例，以避免模型遗忘原有能力。该方法通过强化学习从大量数据混合轨迹中学习通用的重加权策略，无需人工设定。实验表明，该方法在数学推理任务中优于现有基线，并能跨领域、跨模型泛化。此外，它在代码生成等不同任务中也表现出良好的适应性，且在较少源领域数据的情况下仍能保持高性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 10:01:54 GMT</pubDate>
</item>
<item>
<title>基于离散SDF的3D高斯点云逆渲染方法</title>
<link>https://arxiv.org/abs/2507.15629</link>
<guid>https://arxiv.org/abs/2507.15629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种离散SDF增强的3D高斯点云逆渲染方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于离散符号距离场（SDF）的3D高斯点云逆渲染方法，旨在解决传统方法在几何约束应用上的不足。通过将SDF编码到每个高斯中，并利用SDF到透明度的转换实现高效渲染，避免了光线追踪计算。为保证离散样本与真实SDF的一致性，引入基于投影的对齐损失。实验表明，该方法在光照重建质量上优于现有方法，且无需额外内存和复杂优化设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:52:33 GMT</pubDate>
</item>
<item>
<title>Being-H0：基于人类视频的多模态机器人操作模型</title>
<link>https://arxiv.org/abs/2507.15597</link>
<guid>https://arxiv.org/abs/2507.15597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-H0通过人类视频训练，提升机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> Being-H0是一种新型的视觉-语言-动作模型（VLA），利用大规模人类视频进行训练，旨在解决现有模型在复杂操作任务中的不足。该模型采用物理指令微调方法，结合大规模预训练、三维空间对齐和任务适配，提升了机器人在真实环境中的表现。同时，研究团队构建了一个包含多种数据源的大规模数据集，以支持模型训练与优化。实验表明，Being-H0在手部运动生成和指令遵循方面表现出色，并具备良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:19:09 GMT</pubDate>
</item>
<item>
<title>PhysGym：评估大语言模型科学发现能力的新基准</title>
<link>https://arxiv.org/abs/2507.15550</link>
<guid>https://arxiv.org/abs/2507.15550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysGym用于评估大语言模型在物理环境中的科学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysGym，一个用于评估基于大语言模型的科学推理能力的新基准套件和仿真平台。该平台通过控制提供给代理的先验知识水平，使研究人员能够分析模型在不同问题复杂度下的表现。PhysGym包含一系列交互式模拟，要求代理主动探测环境、在约束条件下收集数据并提出关于物理规律的假设。平台提供了标准化的评估协议和指标，用于衡量假设的准确性和模型的真实性。实验结果展示了该基准在区分不同先验知识和任务复杂度下的模型能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 08:28:10 GMT</pubDate>
</item>
<item>
<title>GR-3：通用机器人策略的进展与ByteMini集成</title>
<link>https://arxiv.org/abs/2507.15493</link>
<guid>https://arxiv.org/abs/2507.15493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GR-3具备强大泛化能力，适用于多种任务和环境。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GR-3，一种大规模视觉-语言-动作（VLA）模型，具有在新物体、环境和抽象指令中进行泛化的出色能力。GR-3可通过少量人类轨迹数据高效微调，实现快速适应新场景。它还能处理长时序和精细操作任务，包括双臂操作和移动任务。该模型通过多阶段训练方法实现，包括网络规模的视觉-语言数据联合训练、VR设备收集的人类轨迹数据微调以及机器人轨迹数据的模仿学习。同时，文章介绍了与GR-3集成的ByteMini机器人，展示了其在多种任务中的灵活性和可靠性。实验表明，GR-3在多个挑战性任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:54:13 GMT</pubDate>
</item>
<item>
<title>Stitch：实现语音模型同步思考与回答的新方法</title>
<link>https://arxiv.org/abs/2507.15375</link>
<guid>https://arxiv.org/abs/2507.15375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Stitch让语音模型在说话时同步生成内部思考过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Stitch的新方法，用于改进语音语言模型（SLMs）的性能。传统SLMs在生成回应前缺乏内部思考过程，而人类在交流前通常会进行内部推理。Stitch通过交替生成未发声的推理片段和语音回应片段，实现了在语音输出过程中同步进行内部思考。这种方法有效减少了额外延迟，同时在数学推理数据集上比基线模型高出15%的性能，并在非推理任务中表现相当。项目页面提供了相关演示和动画。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 04:30:03 GMT</pubDate>
</item>
<item>
<title>基于知识投影的网页信息检索数据合成框架WebShaper</title>
<link>https://arxiv.org/abs/2507.15061</link>
<guid>https://arxiv.org/abs/2507.15061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebShaper提升IS代理性能，实现更精准的信息检索。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为WebShaper的数据合成框架，旨在解决信息检索（IS）代理训练数据不足的问题。该框架通过集合论对IS任务进行形式化，并引入知识投影（KP）概念，以精确控制推理结构。WebShaper通过多步骤扩展过程生成复杂任务，提升了IS代理在GAIA和WebWalkerQA基准测试中的表现，达到了当前开源IS代理的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 13:53:37 GMT</pubDate>
</item>
<item>
<title>视频理解测试：评估视频大语言模型的准确性和鲁棒性</title>
<link>https://arxiv.org/abs/2507.15028</link>
<guid>https://arxiv.org/abs/2507.15028</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">视频大语言模型在视频理解上与人类存在显著差距。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Video-TT的视频理解测试，用于评估视频大语言模型（video LLMs）在真实视频中的表现是否接近人类。该测试包含1000个YouTube Shorts视频，每个视频配有1个开放性问题和4个对抗性问题，以检验模型在视觉和叙事复杂性方面的理解能力。实验结果表明，当前视频大语言模型在准确性和鲁棒性方面与人类仍有明显差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15028" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:30:33 GMT</pubDate>
</item>
<item>
<title>RLVR在推理边界扩展中的局限性研究</title>
<link>https://arxiv.org/abs/2507.14843</link>
<guid>https://arxiv.org/abs/2507.14843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVR可能限制AI发现新解，而非真正扩展推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习与可验证奖励（RLVR）在提升AI解决复杂逻辑任务中的作用。研究指出，RLVR受限于基础模型的初始概率分布，仅能对已有高奖励输出进行优化，而难以发现全新的解决方案。同时，RLVR在提高精度的同时，可能会缩小探索范围，导致遗漏原本可被基础模型找到的正确答案。实验表明，在更大的采样预算下，RLVR的实证支持范围反而缩小。此外，尽管RLVR增加了逐标记的不确定性，但最终答案的多样性却下降，表明其可能局限于少数答案。研究建议未来需通过显式探索机制或混合策略来突破这一限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 03:04:08 GMT</pubDate>
</item>
<item>
<title>开源数学推理语言模型MiroMind-M1的开发与性能评估</title>
<link>https://arxiv.org/abs/2507.14683</link>
<guid>https://arxiv.org/abs/2507.14683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiroMind-M1系列模型在数学推理任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MiroMind-M1系列开源推理语言模型，旨在提升数学推理任务的透明度和可复现性。该模型基于Qwen-2.5架构，在719,000个经过验证的数学推理问题上进行监督微调，并在62,000个挑战性问题上进行强化学习训练。为提高训练效率，作者提出了一种上下文感知的多阶段策略优化算法。实验结果显示，MiroMind-M1在AIME24、AIME25和MATH等基准测试中达到或超越现有开源模型的性能，并具有更高的token效率。研究团队还公开了完整的模型、数据集及训练配置，以支持后续研究和社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 12:21:23 GMT</pubDate>
</item>
<item>
<title>长推理下大推理模型性能下降现象研究</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">长推理导致大模型性能下降，揭示五种失败模式。</p><br /><br /><p><strong>摘要：</strong> 本文构建了评估任务，发现延长大推理模型的推理长度会降低性能，呈现出测试时计算与准确率之间的反向关系。评估任务涵盖四种类型：带干扰项的计数任务、含虚假特征的回归任务、约束跟踪的演绎任务以及高级AI风险。研究发现了五种模型在长时间推理中的失败模式，包括Claude模型被无关信息分散注意力、OpenAI o系列模型过度依赖问题框架、模型从合理先验转向虚假相关性、所有模型在复杂演绎任务中难以保持专注，以及推理延长可能加剧不良行为。研究强调了在不同推理长度下评估模型的重要性，以识别和解决这些失败模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 20:06:13 GMT</pubDate>
</item>
<item>
<title>基于单值反馈的多轮强化学习方法提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.14295</link>
<guid>https://arxiv.org/abs/2507.14295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多轮强化学习结合单值反馈提升模型推理与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多轮问题解决对大型推理模型（LRMs）的重要性，并指出传统强化学习方法在多轮任务中表现不足。研究提出一种基于单值反馈的强化学习框架（UFO），利用简单的用户反馈（如“再试一次”）来改进模型的多轮推理能力。实验表明，该方法不仅保持了单轮性能，还提升了多轮推理准确率高达14%。此外，通过设计合理的奖励机制，模型在减少回答轮次的同时，也能在出错时产生更细致和多样化的推理过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 14:07:38 GMT</pubDate>
</item>
<item>
<title>自动化生成高质量图像编辑数据集提升AI图像处理能力</title>
<link>https://arxiv.org/abs/2507.14119</link>
<guid>https://arxiv.org/abs/2507.14119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化管道生成高精度图像编辑数据，提升AI图像处理效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种自动化、模块化的图像编辑数据挖掘系统，能够跨领域、多分辨率、多风格地生成高质量的图像编辑三元组。该系统基于公开生成模型，无需人工干预，通过任务调优的Gemini验证器直接评估指令遵循度和美学质量，无需分割或定位模型。通过逆向和组合引导方法，数据量扩大约2.2倍，为大规模训练提供支持。研究团队发布了NHR-Edit数据集和Bagel-NHR-Edit模型，显著提升了图像编辑任务的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:50:00 GMT</pubDate>
</item>
<item>
<title>基于不确定性引导的渐进学习框架在CT图像分类中的应用</title>
<link>https://arxiv.org/abs/2507.14102</link>
<guid>https://arxiv.org/abs/2507.14102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UGPL提升CT图像分类准确率，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为UGPL的不确定性引导渐进学习框架，用于提高CT图像的分类准确性。该方法通过全局到局部的分析策略，首先识别诊断模糊区域，再对关键区域进行详细分析。利用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取具有空间多样性的信息片段。结合自适应融合机制，UGPL能够有效整合上下文信息与细粒度特征。实验表明，UGPL在三个CT数据集上分别提升了3.29%、2.46%和8.08%的准确率，证明其在肾脏异常、肺癌和新冠检测中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:30:56 GMT</pubDate>
</item>
<item>
<title>PhyWorldBench：评估视频生成模型物理模拟能力的基准测试</title>
<link>https://arxiv.org/abs/2507.13428</link>
<guid>https://arxiv.org/abs/2507.13428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出PhyWorldBench，用于评估视频生成模型的物理真实性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhyWorldBench，这是一个用于评估视频生成模型在物理现象模拟方面表现的全面基准。该基准涵盖从基础物理规律到复杂物体交互等多个层次的物理场景，并引入了“反物理”类别以测试模型在违反现实物理规则时的逻辑一致性。研究团队评估了12个先进的文本到视频生成模型，分析它们在不同物理场景下的表现，识别出模型在遵循真实物理规律方面的关键挑战，并提出了优化提示词以提升物理真实性的建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:54:09 GMT</pubDate>
</item>
<item>
<title>基于流式处理的4D时空几何重建方法</title>
<link>https://arxiv.org/abs/2507.11539</link>
<guid>https://arxiv.org/abs/2507.11539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种实时4D几何重建模型，提升推理速度与空间一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种流式4D视觉几何变换器，借鉴自回归大语言模型的设计理念。该模型采用因果Transformer架构，通过时间因果注意力机制和历史键值缓存实现高效的在线长期4D重建。为提升训练效率，引入从密集双向视觉几何接地Transformer（VGGT）中知识蒸馏的方法，并支持迁移高效注意力算子（如FlashAttention）。实验表明，该模型在保持性能的同时显著提升了在线场景下的推理速度，为可扩展的交互式4D视觉系统提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>RoMaP：基于3D高斯编辑的精准局部3D内容修改方法</title>
<link>https://arxiv.org/abs/2507.11061</link>
<guid>https://arxiv.org/abs/2507.11061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoMaP实现精准3D高斯局部编辑，提升3D内容质量与灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoMaP框架，解决3D高斯点云局部编辑中的挑战。通过引入3D-Geometry Aware Label Prediction模块生成鲁棒的3D掩码，结合正则化SDS损失函数，有效提升局部编辑精度和上下文一致性。实验表明，RoMaP在重建和生成场景中均达到最先进的3D局部编辑效果，为更灵活、可靠的3D内容创作提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 03:54:11 GMT</pubDate>
</item>
<item>
<title>基于几何引导的弱监督自蒸馏框架GeoDistill用于跨视角定位</title>
<link>https://arxiv.org/abs/2507.10935</link>
<guid>https://arxiv.org/abs/2507.10935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GeoDistill提升跨视角定位精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出GeoDistill，一种基于几何引导的弱监督自蒸馏框架，用于提升跨视角定位的性能。该方法通过教师-学生模型结构，利用视场角（FoV）掩码生成有限视场图像，使学生模型专注于关键特征如车道线，忽略无纹理区域。实验表明，GeoDistill在不同框架下均显著提升定位效果，并引入一种无需精确平面位置标注的相对方向估计网络，为实际应用提供高效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 23:00:15 GMT</pubDate>
</item>
<item>
<title>基于4D扩散模型的高保真人体视角合成方法</title>
<link>https://arxiv.org/abs/2507.13344</link>
<guid>https://arxiv.org/abs/2507.13344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出滑动迭代去噪方法提升4D扩散模型时空一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对稀疏视角视频输入下的人体高保真视角合成问题，提出一种新的滑动迭代去噪过程。通过在潜在网格中编码图像、相机姿态和人体姿态信息，并使用滑动窗口在空间和时间维度上交替去噪，从而增强4D扩散模型的时空一致性。该方法在DNA-Rendering和ActorsHQ数据集上表现出色，显著优于现有方法，同时保持GPU内存消耗可控。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>Voxtral Mini和Voxtral Small多模态音频聊天模型发布</title>
<link>https://arxiv.org/abs/2507.13264</link>
<guid>https://arxiv.org/abs/2507.13264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voxtral模型在音频和文本任务中表现优异，支持长对话与大上下文。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Voxtral Mini和Voxtral Small两款多模态音频聊天模型，它们能够理解和处理语音和文本信息，在多个音频基准测试中表现出色。Voxtral Small模型体积较小，可在本地运行，并支持长达40分钟的音频文件和多轮对话。研究团队还发布了三个用于评估语音理解模型的基准测试。两款模型均采用Apache 2.0许可证开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:17:37 GMT</pubDate>
</item>
<item>
<title>提升多模态大模型安全性：AutoSteer技术研究</title>
<link>https://arxiv.org/abs/2507.13255</link>
<guid>https://arxiv.org/abs/2507.13255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoSteer提升多模态大模型安全性，无需微调。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为AutoSteer的模块化、自适应推理时干预技术，旨在提升多模态大语言模型（MLLMs）在面对对抗性多模态输入时的安全性。该技术包含三个核心组件：安全意识评分（SAS）、自适应安全探测器和轻量级拒绝头。实验表明，AutoSteer在多个安全关键基准测试中显著降低了攻击成功率，同时保持了模型的通用能力。该方法为多模态AI系统的安全部署提供了一个实用、可解释且有效的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>基于残差学习的稀疏自编码器改进方法</title>
<link>https://arxiv.org/abs/2507.12990</link>
<guid>https://arxiv.org/abs/2507.12990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过残差学习提升稀疏自编码器在特定领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于残差学习的方法，用于改进稀疏自编码器（SAE）在特定领域中的表现。该方法通过训练一个辅助SAE来建模主SAE在特定文本上的重构误差，从而捕捉主模型未识别的特征。在推理阶段，将两个模型的输出相加，显著提升了跨熵和解释方差指标。实验表明，该方法能够在不牺牲通用任务性能的前提下，有效融入新领域知识，增强SAE的可解释性，为LLM的定向机制解释提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 06:57:49 GMT</pubDate>
</item>
<item>
<title>基于黎曼几何的LoRA优化方法研究</title>
<link>https://arxiv.org/abs/2507.12142</link>
<guid>https://arxiv.org/abs/2507.12142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RiemannLoRA提升LoRA收敛速度与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于黎曼几何的新型LoRA优化方法，旨在同时解决低秩矩阵分解中的过参数化问题和初始化策略问题。该方法将固定秩的LoRA矩阵视为一个光滑流形，并通过在流形上寻找损失函数下降最快的方向来实现有效初始化。实验结果表明，RiemannLoRA在大语言模型和扩散模型中均表现出更快的收敛速度和更好的最终性能，优于传统LoRA及其改进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 07:17:12 GMT</pubDate>
</item>
<item>
<title>Einstein Fields：基于神经张量场的四维时空压缩方法</title>
<link>https://arxiv.org/abs/2507.11589</link>
<guid>https://arxiv.org/abs/2507.11589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Einstein Fields通过神经张量场压缩四维相对论模拟，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Einstein Fields的神经表示方法，用于将计算密集的四维数值相对论模拟压缩为紧凑的隐式神经网络权重。该方法通过建模广义相对论的核心张量场——度规，利用自动微分推导物理量。与传统的神经场（如距离场、占用场或辐射场）不同，Einstein Fields是神经张量场，在将广义相对论的时空几何编码为神经场表示时，动态特性自然地作为副产品出现。该方法在连续时空建模、无网格性、存储效率、导数精度和易用性方面展现出巨大潜力。研究团队在多个广义相对论的标准测试案例中验证了其性能，并发布了基于JAX的开源库，为数值相对论提供了更高效和表达力更强的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:55:39 GMT</pubDate>
</item>
<item>
<title>动态视觉令牌压缩方法VisionThink提升视觉语言模型效率</title>
<link>https://arxiv.org/abs/2507.13348</link>
<guid>https://arxiv.org/abs/2507.13348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisionThink通过动态调整图像分辨率提升VLM效率与精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的视觉令牌压缩方法VisionThink，该方法根据任务复杂度动态选择图像分辨率，从而在保持高精度的同时减少视觉令牌数量。相比传统固定压缩策略，VisionThink能智能判断是否需要更高分辨率图像，并通过强化学习优化决策过程。实验表明，该方法在OCR任务中表现优异，同时在一般视觉问答任务中显著节省计算资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>pi^3：一种无需固定参考视角的视觉几何重建方法</title>
<link>https://arxiv.org/abs/2507.13347</link>
<guid>https://arxiv.org/abs/2507.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">pi^3实现无参考视角的视觉几何重建，性能领先。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了pi^3，这是一种基于前馈神经网络的新方法，用于视觉几何重建，不再依赖传统的固定参考视角。传统方法常以特定视角作为基准，若该视角不佳可能导致不稳定或失败。pi^3采用全排列等变架构，预测仿射不变的相机位姿和尺度不变的局部点图，无需参考帧。这种设计使模型对输入顺序具有鲁棒性且易于扩展，其简单无偏的方法在多种任务中达到最先进水平，包括相机位姿估计、单目/视频深度估计和密集点图重建。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>上下文工程：大型语言模型推理的系统优化方法</title>
<link>https://arxiv.org/abs/2507.13334</link>
<guid>https://arxiv.org/abs/2507.13334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">上下文工程优化LLM推理，提升复杂情境理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了上下文工程这一系统性方法，用于优化大型语言模型在推理过程中的信息输入。文章将上下文工程分解为四个基础组件：上下文检索与生成、上下文处理和上下文管理，并探讨了这些组件如何集成到智能系统中，如检索增强生成、记忆系统、工具整合推理和多智能体系统。通过对1300多篇论文的系统分析，文章不仅构建了该领域的技术路线图，还揭示了一个关键研究缺口：尽管当前模型在复杂情境理解上表现出色，但在生成高质量长文本方面仍存在明显不足。因此，未来的研究应优先解决这一不对称问题，以推动更先进的上下文感知AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:36 GMT</pubDate>
</item>
<item>
<title>基于图灵机模拟的Transformer模型长度泛化方法</title>
<link>https://arxiv.org/abs/2507.13332</link>
<guid>https://arxiv.org/abs/2507.13332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TAIL方法提升Transformer模型对长序列问题的解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Transformer模型在处理比训练时更长序列问题时的挑战，并提出一种基于图灵机模拟的模仿学习方法TAIL，以增强模型的长度泛化能力。TAIL通过生成模仿图灵机执行过程的思维链数据，扩展推理步骤并引入显式内存访问机制，从而缓解短路学习和动态数据访问难题。实验表明，TAIL在多个任务中表现优异，证明图灵机的核心概念对于模型的长度泛化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:07 GMT</pubDate>
</item>
<item>
<title>AbGen：评估大语言模型设计消融实验能力的基准</title>
<link>https://arxiv.org/abs/2507.13300</link>
<guid>https://arxiv.org/abs/2507.13300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AbGen是首个评估LLM设计消融实验能力的基准，揭示模型与人类专家的差距。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AbGen，这是首个用于评估大语言模型（LLM）在科学研究中设计消融实验能力的基准。AbGen包含1500个由专家标注的示例，来源于807篇自然语言处理论文。该基准要求LLM根据给定的研究背景生成详细的消融实验设计。对DeepSeek-R1-0528和o4-mini等先进模型的评估表明，它们在重要性、忠实性和合理性方面与人类专家存在显著差距。此外，文章指出当前自动化评估方法不可靠，与人工评估存在明显差异。为此，作者开发了AbGen-Eval，一个元评估基准，用于评估常用自动化评估系统在测量LLM性能方面的可靠性，并为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:09:22 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的多角色面部表情动画生成方法</title>
<link>https://arxiv.org/abs/2507.12956</link>
<guid>https://arxiv.org/abs/2507.12956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FantasyPortrait实现高保真多角色面部表情动画生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出FantasyPortrait，一种基于扩散Transformer的框架，能够生成高质量且富有情感的单角色和多角色面部动画。该方法通过引入表达增强学习策略，利用隐式表示捕捉与身份无关的面部动态，提升模型对细微情绪的渲染能力。针对多角色控制，设计了掩码交叉注意力机制，确保独立且协调的表情生成，避免特征干扰。为推动该领域研究，作者构建了Multi-Expr数据集和ExprBench基准测试。实验表明，FantasyPortrait在定量和定性评估中均优于现有方法，尤其在跨角色再现和多角色场景中表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 05:50:43 GMT</pubDate>
</item>
<item>
<title>AnyCap项目提升多模态生成的可控性与评估可靠性</title>
<link>https://arxiv.org/abs/2507.12841</link>
<guid>https://arxiv.org/abs/2507.12841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyCap提升多模态生成的可控性和评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnyCap项目，旨在解决现有模型在多模态生成中缺乏精细控制和可靠评估的问题。项目包括AnyCapModel（ACM），一种轻量级框架，可在不重新训练基础模型的情况下增强其可控性；AnyCapDataset（ACD），一个包含三种模态、28种用户指令类型和30万条高质量数据的数据集；以及AnyCapEval，一个新的评估基准，提供更可靠的评估指标。实验表明，ACM显著提升了多种基础模型的生成质量，在GPT-4o上内容得分提升45%，风格得分提升12%，并在多个基准测试中取得显著成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 03:04:05 GMT</pubDate>
</item>
<item>
<title>可学习分词器提升语言模型适应性</title>
<link>https://arxiv.org/abs/2507.12720</link>
<guid>https://arxiv.org/abs/2507.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FLEXITOKENS，提升语言模型的分词灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文针对语言模型在面对新数据分布时适应性差的问题，提出一种基于字节级的可学习分词器方法。传统子词分词器在适应过程中保持固定，导致分词效率低下。本文引入可学习的边界预测模块，使分词过程更具灵活性。相比现有方法，FLEXITOKENS通过简化训练目标，有效减少分词过度碎片化，并在多个多语言和形态多样任务中取得显著性能提升，最高提升达10%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 21:55:41 GMT</pubDate>
</item>
<item>
<title>MindJourney：通过世界模型提升视觉语言模型的3D空间推理能力</title>
<link>https://arxiv.org/abs/2507.12508</link>
<guid>https://arxiv.org/abs/2507.12508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MindJourney提升VLM在3D空间推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出MindJourney，一种无需微调即可增强视觉语言模型（VLM）3D空间推理能力的测试时扩展框架。该方法通过将VLM与基于视频扩散的世界模型结合，使VLM能够生成相机轨迹并合成多视角图像，从而进行更准确的空间推理。实验表明，MindJourney在SAT基准测试中平均提升了8%的性能，展示了其在提升VLM 3D理解能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>基于时序感知扩散模型的视频帧插值方法</title>
<link>https://arxiv.org/abs/2507.04984</link>
<guid>https://arxiv.org/abs/2507.04984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TLB-VFI模型，提升视频帧插值效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频帧插值任务，提出一种名为Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) 的高效视频扩散模型。该模型通过3D波浪门控和时序感知自编码器提取丰富的时序信息，在挑战性数据集上相比最新图像扩散模型提升了20%的FID表现。同时，模型参数减少3倍，推理速度提升2.3倍，并通过光流引导显著降低训练数据需求，参数量比视频扩散模型少20倍以上。项目代码和结果已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:25:32 GMT</pubDate>
</item>
<item>
<title>GitChameleon：面向库版本的代码生成评估基准</title>
<link>https://arxiv.org/abs/2507.12367</link>
<guid>https://arxiv.org/abs/2507.12367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GitChameleon提供执行验证的代码生成基准，提升AI代码生成适应性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GitChameleon，这是一个包含328个Python代码补全问题的数据集，每个问题都基于特定的库版本，并附带可执行单元测试。该数据集用于评估大型语言模型、代码助手等系统在特定版本下生成功能正确的代码的能力。实验表明，当前最先进的系统在此任务上表现不佳，成功率为48%-51%，凸显了该任务的复杂性。GitChameleon通过执行验证的方式，帮助研究人员更清晰地理解代码生成挑战，并推动更具适应性和可靠性的AI代码生成方法的发展。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 12:10:42 GMT</pubDate>
</item>
<item>
<title>基于超网络的多模态模型对齐方法</title>
<link>https://arxiv.org/abs/2507.10015</link>
<guid>https://arxiv.org/abs/2507.10015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hyma提升多模态模型选择与连接训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Hyma的新方法，用于优化多模态模型的选择和连接模块训练。该方法利用超网络的参数预测能力，能够同时为N乘M种单模态模型组合生成连接模块，从而显著降低搜索最佳模型对的成本。实验表明，Hyma在多个多模态基准测试中实现了与网格搜索相当的性能，但计算成本降低了10倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 03:51:01 GMT</pubDate>
</item>
<item>
<title>跨模态知识蒸馏框架MST-Distill的提出与实验验证</title>
<link>https://arxiv.org/abs/2507.07015</link>
<guid>https://arxiv.org/abs/2507.07015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">跨模态知识蒸馏面临路径选择和知识漂移问题，MST-Distill有效提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对跨模态知识蒸馏中存在的挑战，提出了MST-Distill框架。该框架通过引入多个专用教师模型和实例级路由网络，实现动态自适应的知识迁移。同时，设计了独立训练的掩码模块，以减少模态差异并增强知识传递效果。在五个多模态数据集上的实验表明，该方法显著优于现有方法，具有良好的泛化能力和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:28 GMT</pubDate>
</item>
<item>
<title>对比编码器与解码器语言模型的性能与适应性</title>
<link>https://arxiv.org/abs/2507.11412</link>
<guid>https://arxiv.org/abs/2507.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比编码器和解码器模型性能，发现任务适配性差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了编码器-only 和解码器-only 语言模型的性能差异。尽管大多数工作集中在解码器模型上，但编码器模型在分类和检索任务中表现更优。研究团队推出了 Ettin 套件，包含从 1700 万到 10 亿参数的成对模型，使用相同训练方法实现了 SOTA 结果。实验表明，将模型从一种架构转换到另一种任务效果不佳，且小规模编码器在分类任务中优于大规模解码器。研究开源了所有训练数据和模型检查点，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:31:51 GMT</pubDate>
</item>
<item>
<title>AI Wizards在CLEF 2025主题性检测任务中的表现</title>
<link>https://arxiv.org/abs/2507.11764</link>
<guid>https://arxiv.org/abs/2507.11764</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI Wizards通过融合情感特征提升新闻主观性检测效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AI Wizards团队在CLEF 2025 CheckThat! Lab Task 1主题性检测任务中的研究成果，旨在识别新闻文章中的主观/客观句子。该任务覆盖了单语、多语和零样本设置，并在训练/开发数据集中包含阿拉伯语、德语、英语、意大利语和保加利亚语，最终评估还引入了希腊语、罗马尼亚语、波兰语和乌克兰语等未见语言以测试模型泛化能力。研究采用基于Transformer的分类器，并通过整合情感评分与句法表示来增强模型性能。为应对类别不平衡问题，团队优化了决策阈值。实验结果表明，情感特征的引入显著提升了模型表现，尤其在主观性F1分数上效果明显，最终在希腊语任务中获得第一名。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11764" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 18:10:20 GMT</pubDate>
</item>
<item>
<title>MMHU：大规模人类行为分析基准数据集</title>
<link>https://arxiv.org/abs/2507.12463</link>
<guid>https://arxiv.org/abs/2507.12463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMHU基准，用于评估自动驾驶中的人类行为理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MMHU，一个大规模的人类行为分析基准数据集，包含57,000个动作片段和173万帧数据，涵盖多种来源如Waymo、YouTube视频和自采集数据。该数据集提供丰富的标注信息，包括动作轨迹、文本描述、意图标签等，旨在支持自动驾驶中的行为预测、生成和问答任务。通过构建人机协作的标注流程，确保数据质量，并为相关研究提供全面的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>SpatialTrackerV2：一种高效的单目视频3D点跟踪方法</title>
<link>https://arxiv.org/abs/2507.12462</link>
<guid>https://arxiv.org/abs/2507.12462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialTrackerV2提升3D点跟踪性能，运行速度更快。</p><br /><br /><p><strong>摘要：</strong> 本文提出SpatialTrackerV2，一种基于单目视频的前馈3D点跟踪方法。该方法将点跟踪、单目深度估计和相机位姿估计统一在一个端到端的架构中，能够分解世界空间中的3D运动为场景几何、相机自运动和像素级物体运动。通过在多种数据集上进行可扩展训练，SpatialTrackerV2在性能上优于现有3D跟踪方法30%，同时在动态3D重建任务中达到领先水平，且运行速度提高了50倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>SWE-Perf：首个针对代码性能优化的基准测试</title>
<link>https://arxiv.org/abs/2507.12415</link>
<guid>https://arxiv.org/abs/2507.12415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Perf评估LLM在真实代码库中优化性能的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SWE-Perf，这是首个专门用于评估大型语言模型（LLMs）在真实代码仓库中进行代码性能优化任务的基准测试。该基准包含140个精心挑选的实例，来源于GitHub上性能改进的拉取请求。每个实例包括代码库、目标函数、性能测试、专家编写的补丁和可执行环境。通过评估不同方法，研究发现现有LLMs在性能优化方面与专家水平存在显著差距，揭示了该领域的重要研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:05:17 GMT</pubDate>
</item>
<item>
<title>基于空间音频的人类运动生成方法研究</title>
<link>https://arxiv.org/abs/2507.11949</link>
<guid>https://arxiv.org/abs/2507.11949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAM数据集和MOSPA框架，实现空间音频驱动的逼真人类运动生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对虚拟人类对多样化听觉刺激的动态、真实响应这一挑战，提出了首个全面的空间音频驱动人类运动（SAM）数据集，并开发了基于扩散模型的MOSPA框架。该框架通过有效的融合机制，准确捕捉人体运动与空间音频之间的关系，能够根据不同的空间音频输入生成多样且逼真的运动。实验表明，该方法在该任务上达到了最先进的性能。相关模型和数据集将在论文接受后开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 02:33:11 GMT</pubDate>
</item>
<item>
<title>RLEP：一种用于大语言模型的强化学习框架</title>
<link>https://arxiv.org/abs/2507.07451</link>
<guid>https://arxiv.org/abs/2507.07451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLEP通过经验回放提升大语言模型的训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RLEP的强化学习框架，旨在解决大语言模型训练过程中不稳定和策略漂移的问题。该框架分为两个阶段：首先收集验证过的轨迹，然后在后续训练中进行回放。每次更新时，策略在混合新生成数据和回放成功案例的mini-batch上进行优化。RLEP通过回放高质量示例，引导模型避免无效探索，专注于有潜力的推理路径，从而加快收敛并提升最终性能。实验结果显示，在Qwen2.5-Math-7B模型上，RLEP在更少更新次数下达到基线峰值准确率，并在多个数学竞赛数据集上显著提升了准确率。相关代码、数据集和检查点已公开，便于复现和进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 01:58:55 GMT</pubDate>
</item>
<item>
<title>基于推理时扩展计算的大型语言模型训练方法</title>
<link>https://arxiv.org/abs/2507.05065</link>
<guid>https://arxiv.org/abs/2507.05065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过工具交互增强推理计算，提升代码修复能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的机器学习范式，利用推理和训练阶段的计算扩展来提升大型语言模型的能力。不同于传统的监督微调和强化学习方法，该研究将推理过程中的计算以多轮交互形式与状态化工具结合，模型通过自定义领域特定语言控制工具。实验表明，这种设置提高了经验采样速度和奖励信号密度，使参数量达30亿的模型也能高效执行任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 10:49:18 GMT</pubDate>
</item>
<item>
<title>AnyI2V：一种无需训练的视频生成框架</title>
<link>https://arxiv.org/abs/2507.02857</link>
<guid>https://arxiv.org/abs/2507.02857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyI2V支持多种条件图像和运动轨迹生成高质量视频。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为AnyI2V的新型视频生成框架，该框架无需训练即可根据用户定义的运动轨迹动画化任意条件图像。与现有方法相比，AnyI2V支持更广泛的模态输入，如网格和点云，并具备混合条件输入、风格迁移和编辑功能。实验表明，AnyI2V在空间和运动控制方面表现出色，为视频生成提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>物理引导的3D资产生成方法PhysX</title>
<link>https://arxiv.org/abs/2507.12465</link>
<guid>https://arxiv.org/abs/2507.12465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysX实现物理属性驱动的3D资产生成。</p><br /><br /><p><strong>摘要：</strong> 文章提出PhysX，一种基于物理属性的3D资产生成框架，旨在解决现有3D生成模型忽视物理特性的不足。PhysX包含两个核心部分：PhysXNet是一个首次系统标注五维物理属性的3D数据集，并通过人机协同标注流程提升效率；PhysXGen则是一个前馈式图像到3D资产生成框架，通过双分支结构建模3D结构与物理属性的关系，确保生成结果在保持几何质量的同时具备合理的物理特性。实验表明该框架性能优越且具有良好的泛化能力，相关代码、数据和模型将公开以推动物理生成AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>面向土木工程的技术图纸修订评估基准 DrafterBench</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DrafterBench用于评估LLM在技术图纸修订中的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了 DrafterBench，一个针对土木工程中技术图纸修订任务的全面评估基准。该基准包含12种任务类型、46个自定义工具和1920个任务，旨在系统评估大型语言模型代理在理解复杂指令、利用先验知识和适应动态指令质量方面的能力。DrafterBench 提供了详细的准确率分析和错误统计，以帮助深入理解AI代理的能力并为工程应用中的LLM集成提供改进方向。该基准已开源，可在GitHub和Hugging Face上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:56:04 GMT</pubDate>
</item>
<item>
<title>融合检索与推理的先进方法研究</title>
<link>https://arxiv.org/abs/2507.09477</link>
<guid>https://arxiv.org/abs/2507.09477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了RAG与推理结合的最新进展及未来方向。</p><br /><br /><p><strong>摘要：</strong> 本文对检索增强生成（RAG）与推理相结合的方法进行了全面综述，探讨了如何通过推理优化RAG的各个阶段，以及如何利用不同类型的检索知识支持复杂推理。文章还介绍了融合RAG与推理的框架，这些框架通过迭代搜索和推理实现高性能表现。作者分类整理了相关方法、数据集和挑战，并提出了未来在更有效、多模态适应、可信和以用户为中心的系统方面的研究方向。相关资源可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 23:29:41 GMT</pubDate>
</item>
<item>
<title>Lizard：一种用于无限上下文生成的线性化框架</title>
<link>https://arxiv.org/abs/2507.09025</link>
<guid>https://arxiv.org/abs/2507.09025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lizard提升LLM的长文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lizard，一种将预训练Transformer-based大型语言模型（LLM）转换为灵活、次二次架构的线性化框架，以实现无限上下文生成。Lizard通过引入次二次注意力机制，有效缓解了Transformer模型在长上下文场景下的内存和计算瓶颈。该框架结合门控线性注意力与增强元记忆的滑动窗口注意力，实现了全局上下文压缩与局部细节捕捉的平衡。同时，Lizard支持常量内存推理和更强的长度泛化能力，并通过硬件感知算法加速训练。实验表明，Lizard在标准语言建模任务中几乎恢复了教师模型的性能，并在5-shot MMLU基准上比之前方法提升了18分，在关联回忆任务中也表现出显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 17:19:18 GMT</pubDate>
</item>
<item>
<title>无需微调的视频光流提取方法</title>
<link>https://arxiv.org/abs/2507.09082</link>
<guid>https://arxiv.org/abs/2507.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于生成模型的零样本光流提取方法取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何从视频中提取光流，而无需进行特定任务的微调。受自监督视频模型启发，作者提出了一种新的测试时方法——KL-tracing，通过注入局部扰动并计算预测分布的Kullback-Leibler散度来获取光流。该方法在真实世界和合成数据集上均优于现有方法，展示了其在高质量光流估计中的潜力。研究发现，模型的三个关键特性有助于实现零样本光流提取：未来帧的分布预测、独立处理时空块的因子化潜在表示以及可随机访问的解码机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:59:38 GMT</pubDate>
</item>
<item>
<title>基于离散扩散模型的音频补全方法研究</title>
<link>https://arxiv.org/abs/2507.08333</link>
<guid>https://arxiv.org/abs/2507.08333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型音频补全方法，适用于长间隙恢复。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于离散扩散建模的音频补全方法，该方法利用预训练音频分词器生成的离散音频表示进行重建。与以往基于波形或频谱图的方法不同，该方法在离散潜在空间中直接建模生成过程，从而实现更稳定和语义连贯的音频恢复。实验在MusicNet和MTG数据集上进行，覆盖长达500毫秒的缺失片段，结果表明该方法在长间隙恢复任务中表现优于现有基线，为音乐录音修复提供了可靠方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 02:25:49 GMT</pubDate>
</item>
<item>
<title>BYOKG-RAG：一种增强知识图谱问答的框架</title>
<link>https://arxiv.org/abs/2507.04127</link>
<guid>https://arxiv.org/abs/2507.04127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BYOKG-RAG提升知识图谱问答性能，适用于自定义图谱。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BYOKG-RAG的框架，用于增强知识图谱问答（KGQA）任务。该框架结合大型语言模型（LLM）与专门的图检索工具，通过生成关键图结构元素（如问题实体、候选答案、推理路径和OpenCypher查询），并利用图工具进行链接和上下文检索，从而提高问答的准确性和泛化能力。实验表明，BYOKG-RAG在五个不同类型的基准数据集上表现优于现有方法，并展现出对自定义知识图谱的良好适应性。框架已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 14:47:14 GMT</pubDate>
</item>
<item>
<title>面向用户生成视频的多模态字幕生成基准与模型</title>
<link>https://arxiv.org/abs/2507.11336</link>
<guid>https://arxiv.org/abs/2507.11336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGC-VideoCap，提升多模态视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对用户生成视频（如TikTok）中音频与视觉信息融合不足的问题，提出了UGC-VideoCap基准和模型框架。该基准包含1000个经过三阶段人工标注的短视频，强调音频与视觉信息的平衡整合，并配有4000个QA对用于评估多模态理解能力。同时，作者开发了UGC-VideoCaptioner(3B)模型，基于Gemini 2.5 Flash进行轻量化训练，采用两阶段策略提升数据效率与性能。该研究为真实场景下的多模态视频字幕生成提供了高质量基础和解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:08:29 GMT</pubDate>
</item>
<item>
<title>基于多代理架构的视觉分类框架提升零样本场景下的AI可信度</title>
<link>https://arxiv.org/abs/2507.10571</link>
<guid>https://arxiv.org/abs/2507.10571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信任感知的多代理AI框架，提升零样本场景下的分类准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型模块化Agentic AI视觉分类框架，结合通用多模态代理、非视觉推理协调器和检索增强生成（RAG）模块，用于提升AI在零样本场景下的可信度。该框架在苹果叶片疾病诊断任务中进行了测试，结果显示使用信任感知协调器和RAG可使零样本设置下的准确率提高77.94%，达到85.63%。研究还比较了不同模型的校准效果，并通过图像-RAG技术增强了预测的可靠性。该系统将感知与元推理分离，提高了多代理AI的可扩展性和可解释性，适用于医疗诊断、生物学等对信任要求高的领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:39:29 GMT</pubDate>
</item>
<item>
<title>EXAONE 4.0：融合推理与非推理模式的多语言AI模型</title>
<link>https://arxiv.org/abs/2507.11407</link>
<guid>https://arxiv.org/abs/2507.11407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EXAONE 4.0支持多语言，具备推理与非推理模式。</p><br /><br /><p><strong>摘要：</strong> EXAONE 4.0 是一款结合了非推理模式和推理模式的AI模型，旨在提升用户体验并增强推理能力。该模型支持英语、韩语和西班牙语，包含32B和1.2B两个版本，分别适用于高性能计算和设备端应用。EXAONE 4.0在同类模型中表现优异，甚至可与前沿模型竞争，且已公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:24:51 GMT</pubDate>
</item>
<item>
<title>MISS-QA：评估模型解读科学文献示意图能力的基准测试</title>
<link>https://arxiv.org/abs/2507.10787</link>
<guid>https://arxiv.org/abs/2507.10787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MISS-QA是首个评估模型解读科学文献示意图的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MISS-QA，这是首个专门用于评估模型在科学文献中解读示意图能力的基准测试。该基准包含465篇科学论文中的1,500个专家标注示例。模型需根据示意图和论文上下文回答相关问题。研究评估了18种前沿多模态基础模型的表现，发现这些模型与人类专家之间存在显著差距。通过分析模型在无法回答问题上的表现及详细错误分析，揭示了当前模型在理解多模态科学文献方面的优缺点，为未来改进提供了关键见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 16:35:25 GMT</pubDate>
</item>
<item>
<title>LLM在恶意软件变种生成中的应用研究</title>
<link>https://arxiv.org/abs/2507.09411</link>
<guid>https://arxiv.org/abs/2507.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM可有效生成恶意软件变种，降低检测率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在修改恶意软件源代码以生成变种方面的可行性。作者提出了一种名为LLMalMorph的半自动化框架，该框架利用LLMs对代码的语义和语法理解能力，通过自定义提示和代码转换策略生成新的恶意软件变种，而无需资源密集型微调。实验中收集了10个不同类型的Windows恶意软件样本，并生成了618个变种。结果表明，这些变种在一定程度上降低了杀毒软件的检测率，同时保持了原始功能。此外，尽管未针对机器学习检测器进行优化，部分变种仍对基于ML的分类器表现出较高的攻击成功率。文章还讨论了当前LLM在生成恶意软件变种方面的局限性，并评估了该技术在恶意软件变种生成领域的现状。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 18:11:10 GMT</pubDate>
</item>
<item>
<title>基于缩放定律的大型基础模型数据混合优化方法</title>
<link>https://arxiv.org/abs/2507.09404</link>
<guid>https://arxiv.org/abs/2507.09404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种系统方法优化模型训练数据比例，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于缩放定律的方法，用于确定大型基础模型在不同目标领域下的最优数据混合比例。该方法能够准确预测模型在不同规模和数据权重下的损失表现，并在语言模型、多模态模型和视觉模型等多种大规模预训练场景中得到验证。研究还表明，这些缩放定律可以推广到新的数据组合和模型规模，仅需少量小规模训练即可估计大尺度性能，为模型训练提供了一种更高效、更系统的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 17:16:08 GMT</pubDate>
</item>
<item>
<title>OpenCodeReasoning-II数据集与代码生成及评估的改进</title>
<link>https://arxiv.org/abs/2507.09075</link>
<guid>https://arxiv.org/abs/2507.09075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCodeReasoning-II提升代码生成与评估性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenCodeReasoning-II数据集，包含250万条问题-解法-评注三元组，是目前最大的公开代码推理数据集。研究采用两阶段微调策略，分别优化代码生成和代码评估能力。实验表明，微调后的Qwen2.5-Instruct模型在代码生成方面表现优于或等同于现有最佳模型，并显著提升了竞赛编程性能。此外，还扩展了LiveCodeBench基准以支持C++语言，增强LLM评估的全面性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:35:54 GMT</pubDate>
</item>
<item>
<title>多智能体系统在复杂网络中的协作与推理能力评估</title>
<link>https://arxiv.org/abs/2507.08616</link>
<guid>https://arxiv.org/abs/2507.08616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多智能体系统的协作与推理能力，提出AgentsNet新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多智能体系统在复杂网络中的自组织与协作能力，提出了一种新的基准测试框架AgentsNet。该基准借鉴分布式系统和图论的经典问题，评估多智能体在不同网络拓扑下的策略制定、自我组织和通信能力。研究发现，尽管一些先进的大语言模型在小规模网络中表现良好，但随着网络规模扩大，性能显著下降。与现有基准最多支持2-5个智能体不同，AgentsNet可扩展至更大规模，甚至支持100个智能体的测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 10:13:22 GMT</pubDate>
</item>
<item>
<title>探究大语言模型中认知偏见的成因与影响</title>
<link>https://arxiv.org/abs/2507.07186</link>
<guid>https://arxiv.org/abs/2507.07186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大模型偏见主要源于预训练而非微调。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）中认知偏见的来源，发现这些偏见主要由预训练阶段决定，而非微调过程。研究通过两次实验方法：首先多次微调模型以观察训练随机性对偏见的影响；其次引入跨微调方法，交换不同数据集以测试偏见是否依赖于数据源。结果表明，尽管训练随机性会带来一定变化，但模型的偏见模式主要由其预训练背景决定。这一发现强调在评估和减轻模型偏见时，需关注其预训练基础，而不仅仅是微调数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:01:14 GMT</pubDate>
</item>
<item>
<title>基于预训练模型的高效视觉语言模型构建方法</title>
<link>https://arxiv.org/abs/2507.07104</link>
<guid>https://arxiv.org/abs/2507.07104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效构建视觉语言模型的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Vision-Language-Vision (VLV)的自动编码器框架，通过结合预训练的视觉编码器、文本到图像扩散模型的解码器以及大语言模型，实现高效的视觉语言模型训练。该方法通过引入信息瓶颈机制，利用连续嵌入进行知识蒸馏，实现了高质量的语义理解与图像重建。通过微调预训练语言模型生成详细描述，构建出性能接近GPT-4o和Gemini 2.0 Flash的图像描述系统。该方法大幅降低了训练成本和数据需求，总训练费用低于1000美元。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>NeuralMark：一种鲁棒的神经网络水印方法</title>
<link>https://arxiv.org/abs/2507.11137</link>
<guid>https://arxiv.org/abs/2507.11137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralMark通过哈希水印滤波提升神经网络所有权保护。</p><br /><br /><p><strong>摘要：</strong> 本文提出NeuralMark，一种基于哈希水印滤波的神经网络水印方法，旨在提高深度神经网络的所有权保护能力。该方法利用哈希函数从密钥生成不可逆二进制水印，并作为过滤器选择模型参数进行嵌入，从而增强对伪造和覆盖攻击的防御能力。同时引入平均池化以抵抗微调和剪枝攻击，且可适配多种神经网络架构。理论分析与实验验证表明，NeuralMark在13种卷积和Transformer架构中表现出良好的有效性和鲁棒性，适用于图像分类和文本生成任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 05:38:11 GMT</pubDate>
</item>
<item>
<title>CoDi框架实现文本到图像的主体一致性与姿态多样性生成</title>
<link>https://arxiv.org/abs/2507.08396</link>
<guid>https://arxiv.org/abs/2507.08396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoDi框架提升文本到图像生成的主体一致性和姿态多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为CoDi的文本到图像生成框架，旨在解决现有方法在保持主体一致性的同时缺乏姿态和布局多样性的问题。CoDi采用两阶段策略：身份传输（IT）和身份精炼（IR）。IT在早期去噪步骤中通过最优传输将身份特征以姿态感知的方式转移到目标图像，从而保持主体一致性并保留姿态多样性；IR在后期去噪步骤中选择最显著的身份特征来进一步细化主体细节。实验结果表明，CoDi在主体一致性、姿态多样性和提示保真度方面均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 04:15:56 GMT</pubDate>
</item>
<item>
<title>将大语言模型集成到非一致性逻辑的形式语义中</title>
<link>https://arxiv.org/abs/2507.09751</link>
<guid>https://arxiv.org/abs/2507.09751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种方法，将LLM融入非一致性逻辑形式语义，提升推理一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保持逻辑一致性的前提下，利用大语言模型（LLM）的广泛知识进行形式化推理。作者提出了一种直接将LLM集成到非一致性逻辑形式语义解释函数中的方法，并通过多个短文本事实性基准数据集验证了该方法的可行性。与以往工作不同，该方法提供了一个理论框架，使神经符号推理能够有效利用LLM的知识，同时保留逻辑系统的可靠性与完备性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 15:05:43 GMT</pubDate>
</item>
<item>
<title>REST：一种评估大模型多任务推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.10541</link>
<guid>https://arxiv.org/abs/2507.10541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REST框架提升大模型多任务推理评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出REST（Reasoning Evaluation through Simultaneous Testing）框架，用于评估大型推理模型在多任务压力下的表现。传统评估方法存在数据污染和无法模拟真实场景的问题，而REST通过同时测试多个问题，更全面地评估模型的上下文优先级分配、跨问题干扰抵抗和动态认知负荷管理能力。实验表明，即使是最先进的模型在REST下也会出现性能下降，且REST比现有基准更具区分力。研究还发现，采用“long2short”训练技术的模型在REST中表现更优，揭示了模型在复杂任务中的关键机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:58:47 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理能力中的作用与评估挑战</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升LLM推理能力，但需关注数据污染问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）对大型语言模型（LLMs）推理能力的提升作用，指出尽管Qwen2.5等模型在特定基准测试中表现优异，但其结果可能因数据污染而不具普遍性。研究发现，仅准确的奖励信号能有效提升性能，而随机或错误信号则无效。为解决这一问题，作者提出一个完全合成的算术问题生成器，构建了无泄漏数据集RandomCalculation，并建议在未污染基准上进行跨模型评估以确保结论可靠性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10532" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Recursions：提升语言模型效率的新框架</title>
<link>https://arxiv.org/abs/2507.10524</link>
<guid>https://arxiv.org/abs/2507.10524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture-of-Recursions 提升语言模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Mixture-of-Recursions (MoR) 的新框架，旨在同时实现参数共享和自适应计算，以提高语言模型的效率。MoR 在一个递归 Transformer 中结合了两种效率策略，通过复用共享层实现参数效率，并利用轻量级路由器动态分配不同递归深度给不同 token，从而优化计算和内存访问效率。此外，还引入了 KV 共享变体以减少预填充延迟和内存占用。实验表明，MoR 在不同模型规模下均表现出色，能够在保持较低训练 FLOPs 和较小模型尺寸的同时，显著降低验证困惑度并提升少样本准确性，同时具备更高的吞吐量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:49:00 GMT</pubDate>
</item>
<item>
<title>MoVieS：一种高效的4D动态视图合成模型</title>
<link>https://arxiv.org/abs/2507.10065</link>
<guid>https://arxiv.org/abs/2507.10065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoVieS实现单帧视频中4D动态视图的快速合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoVieS，一种新型的前馈模型，能够在一秒内从单目视频中合成4D动态新视角。该模型通过像素对齐的高斯基元网格表示动态3D场景，并显式监督其随时间变化的运动。这使得首次在单一学习框架中统一建模外观、几何和运动，实现了视图合成、重建和3D点跟踪。MoVieS通过将新视角合成与动态几何重建相结合，支持大规模训练并减少对任务特定监督的依赖，从而自然支持多种零样本应用，如场景流估计和移动物体分割。实验验证了MoVieS在多个任务中的有效性和效率，性能优异且速度显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 04:49:57 GMT</pubDate>
</item>
<item>
<title>利用ICO图像透明通道的可执行隐写术研究</title>
<link>https://arxiv.org/abs/2507.09074</link>
<guid>https://arxiv.org/abs/2507.09074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过ICO图像透明通道隐藏JavaScript代码实现隐蔽攻击。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型可执行隐写术方法，利用ICO图像文件的alpha透明通道嵌入自解压JavaScript负载。该方法通过修改非透明区域的最低有效位，在不影响视觉效果的前提下隐藏压缩后的JavaScript代码。实验表明，64x64 ICO图像可嵌入最多512字节未压缩数据或0.8KB压缩数据。浏览器在加载页面时会默认获取favicon，从而触发嵌入的加载脚本，利用原生JavaScript API和canvas像素访问在内存中执行payload，形成无需额外网络请求的隐蔽通道。测试验证了该方法在多种浏览器环境中的有效性，并分析了其对内容安全策略和杀毒软件的规避能力。文章还讨论了现有隐写分析和清理防御的局限性，揭示了静态图像与可执行内容之间界限模糊的安全风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:29:04 GMT</pubDate>
</item>
<item>
<title>构建韩国专业级大语言模型评估基准</title>
<link>https://arxiv.org/abs/2507.08924</link>
<guid>https://arxiv.org/abs/2507.08924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍两个韩国专业级大语言模型评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两个针对韩国市场的专业级大语言模型评估基准——KMMLU-Redux和KMMLU-Pro。KMMLU-Redux基于韩国国家技术资格考试题目，去除关键错误以提高可靠性；KMMLU-Pro则基于韩国国家职业执照考试，反映专业领域知识。实验表明，这些基准能够全面体现韩国工业领域的知识水平，并已公开发布数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:56:32 GMT</pubDate>
</item>
<item>
<title>结合SFT与GRPO提升大语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2507.08267</link>
<guid>https://arxiv.org/abs/2507.08267</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过延长SFT和GRPO优化提升数学模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种结合监督微调（SFT）和在线推理强化学习（GRPO）的训练方法，以提升大语言模型在数学推理任务中的准确性和效率。研究发现，延长SFT至10个epoch有助于模型达到最佳性能，而GRPO则主要用于优化解题长度。实验表明该方法在多个高难度基准测试中表现优异，包括在AI数学奥林匹克竞赛中排名靠前。作者开源了全部代码和配置，为后续研究提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08267" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 22:26:01 GMT</pubDate>
</item>
<item>
<title>DreamPoster：基于文本和图像生成高质量海报的框架</title>
<link>https://arxiv.org/abs/2507.04218</link>
<guid>https://arxiv.org/abs/2507.04218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamPoster可生成高质量海报，支持灵活布局与分辨率。</p><br /><br /><p><strong>摘要：</strong> DreamPoster是一种文本到图像生成框架，能够从用户提供的图像和文本提示中智能合成高质量海报，同时保持内容一致性，并支持灵活的分辨率和布局输出。该框架基于Seedream3.0模型构建，统一处理多种海报生成任务。在数据集构建方面，提出了系统化的数据标注流程，精确标注海报中的文字内容和排版层次信息，并构建了包含原始素材和最终海报输出的配对数据集。此外，采用渐进式训练策略，使模型逐步获得多任务生成能力，同时保持高质量生成效果。测试结果表明，DreamPoster在可用性上优于GPT-4o和SeedEdit3.0，达到88.55%。DreamPoster将上线字节跳动旗下应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 23:06:45 GMT</pubDate>
</item>
<item>
<title>EmRACE-3K数据集推动视觉语言模型在具身环境中的推理能力研究</title>
<link>https://arxiv.org/abs/2507.10548</link>
<guid>https://arxiv.org/abs/2507.10548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmRACE-3K提升VLM在具身环境中的交互能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmRACE-3K数据集，旨在解决视觉语言模型（VLMs）在具身环境中的交互与推理能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行等挑战。通过EmRACE-3K，研究者建立了评估VLM在探索、动态空间语义推理和多阶段目标执行三个维度上的基准。实验表明，现有模型在零样本设置下的成功率低于20%，凸显了该领域的挑战。研究进一步通过微调Qwen2.5-VL-7B模型，显著提升了其在各项任务中的表现，验证了EmRACE-3K的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SpeakerVid-5M数据集推动音视频双人交互虚拟人研究</title>
<link>https://arxiv.org/abs/2507.09862</link>
<guid>https://arxiv.org/abs/2507.09862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpeakerVid-5M是首个大规模音视频交互虚拟人数据集。</p><br /><br /><p><strong>摘要：</strong> 随着大模型的快速发展，数字人类领域取得了显著进展。为推动音视频双人交互虚拟人的研究，本文发布了SpeakerVid-5M数据集，包含超过8,743小时的视频片段，涵盖多种互动类型。该数据集按互动类型和数据质量分为四个类别和两个子集，适用于2D虚拟人任务。同时，还提供了基于自回归的视频聊天基线模型及评估指标，旨在为未来研究提供基准。数据集和代码将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 22:22:47 GMT</pubDate>
</item>
<item>
<title>CompassJudger-2：提升大语言模型评估能力的通用判官模型</title>
<link>https://arxiv.org/abs/2507.09104</link>
<guid>https://arxiv.org/abs/2507.09104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CompassJudger-2提升LLM评估性能，具备跨领域判断能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CompassJudger-2，一种新型的通用判官模型，旨在克服现有判官模型在专业性和鲁棒性方面的不足。该模型通过任务驱动的多领域数据收集策略，结合可验证奖励监督和拒绝采样，增强内在批判性推理能力。同时引入改进的学习目标，提升了模型表现。实验表明，CompassJudger-2在多个基准测试中表现优异，其7B版本在判断准确性上与大型模型相当。此外，作者还提出了JudgerBenchV2，用于标准化判官模型的评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 21:34:24 GMT</pubDate>
</item>
<item>
<title>基于令牌感知与层局部对比解码的真相生成方法</title>
<link>https://arxiv.org/abs/2507.04404</link>
<guid>https://arxiv.org/abs/2507.04404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的对比解码方法提升大模型事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需额外训练或模型修改的解码方法，通过将特定类型的令牌与其最相关的Transformer层进行对齐，提高事实性生成。研究发现标点符号在早期层中获得主要关注，而概念性令牌则在中间层主导语义推理。通过有选择地抑制这些令牌在各自深度的关注度，实现可控的事实退化，并利用对比信号引导最终的事实解码。实验表明该方法在多个大语言模型和基准测试中均有效提升了事实准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 10:35:43 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的视觉推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.05255</link>
<guid>https://arxiv.org/abs/2507.05255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过两阶段训练提升多模态模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究如何将大型语言模型的推理能力迁移至多模态大语言模型（MLLMs），以增强其视觉推理能力。作者提出了一种两阶段范式，首先进行大规模语言冷启动微调，然后进行近1000步的多模态强化学习，超越了以往开源模型的规模。研究揭示了三个关键发现：冷启动阶段早期就能产生行为迁移，冷启动广泛记忆视觉行为，而强化学习则能识别并扩展有效模式。最终模型Open-Vision-Reasoner（OVR）在多个推理基准测试中表现优异，如MATH500达到95.3%，MathVision达到51.8%。研究公开了模型、数据和训练过程，以推动更强大的多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>Gemini 2.X 模型家族发布：提升代码与推理能力</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini 2.5 Pro 具备超强编码与推理能力，支持3小时视频处理。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了 Gemini 2.X 模型家族，包括 Gemini 2.5 Pro、Gemini 2.5 Flash、Gemini 2.0 Flash 和 Flash-Lite。其中，Gemini 2.5 Pro 是目前最强大的模型，具备前沿的代码和推理能力，并能处理长达3小时的视频内容。它在多模态理解和推理方面表现出色，可支持复杂的自主工作流。Gemini 2.5 Flash 在计算资源和延迟要求较低的情况下仍保持优秀的推理能力，而 Gemini 2.0 Flash 和 Flash-Lite 则提供了高性能且低延迟、低成本的解决方案。整体上，Gemini 2.X 模型覆盖了从性能到成本的完整优化边界。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:36:04 GMT</pubDate>
</item>
<item>
<title>基于缓存引导的语言模型隐式控制方法</title>
<link>https://arxiv.org/abs/2507.08799</link>
<guid>https://arxiv.org/abs/2507.08799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">缓存引导提升小模型链式推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为缓存引导的轻量级方法，通过一次性的干预直接作用于键值缓存，实现对语言模型的隐式控制。该方法利用GPT-4o生成的推理轨迹构建引导向量，使模型行为更偏向于显式的多步骤推理，无需微调或修改提示。实验表明，缓存引导在多种推理基准测试中提升了模型推理质量和任务性能。相比需要持续干预的激活引导技术，缓存引导在超参数稳定性、推理效率和集成便捷性方面具有显著优势，是一种更稳健且实用的可控生成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>BlockFFN：一种高效的稀疏激活MoE架构及其加速技术</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlockFFN提升模型稀疏性并实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的混合专家（MoE）架构BlockFFN，旨在解决传统MoE在计算负载和稀疏性方面的不足。通过引入带有ReLU激活和RMSNorm的路由机制，实现了可微且灵活的路由策略。同时，设计了兼顾令牌级和块级稀疏性的训练目标，提升了模型在低资源环境下的加速性能。此外，结合激活稀疏性和推测解码技术，BlockFFN在实际端侧设备上实现了3.67倍的加速效果，实验结果表明其在稀疏性与性能方面均优于其他MoE基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:28:56 GMT</pubDate>
</item>
<item>
<title>基于视觉基础模型的图像分词器设计与优化</title>
<link>https://arxiv.org/abs/2507.08441</link>
<guid>https://arxiv.org/abs/2507.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新型图像分词器VFMTok，提升图像重建与生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文探索了利用预训练视觉基础模型构建图像分词器的新方向。通过冻结视觉模型作为编码器，并引入区域自适应量化框架和语义重建目标，提升了分词器的效率和语义一致性。所提出的VFMTok在图像重建和生成任务中表现出色，显著提高了生成质量与token效率，同时加速了模型收敛，实现了高保真类别条件合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:32:45 GMT</pubDate>
</item>
<item>
<title>评估基础模型的归纳偏置与世界模型的对齐性</title>
<link>https://arxiv.org/abs/2507.06952</link>
<guid>https://arxiv.org/abs/2507.06952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基础模型在任务中表现良好，但可能缺乏对底层世界模型的归纳偏置。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种评估基础模型的方法，通过检查其在合成数据集上的适应能力来判断其是否具备与预设世界模型一致的归纳偏置。研究发现，尽管基础模型在训练任务中表现优异，但在面对新任务时，往往未能发展出与世界模型相符的归纳偏置。特别地，在轨道轨迹训练的模型在新物理任务中无法应用牛顿力学，表现出依赖任务特定启发式的倾向，缺乏泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 11:36:15 GMT</pubDate>
</item>
<item>
<title>基于神经信号的无手图像编辑方法LoongX</title>
<link>https://arxiv.org/abs/2507.05397</link>
<guid>https://arxiv.org/abs/2507.05397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoongX利用神经信号实现无手图像编辑，性能优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出LoongX，一种基于多模态神经生理信号的无手图像编辑方法。该方法结合脑机接口和生成模型，利用EEG、fNIRS、PPG和头部运动信号捕捉用户意图，并通过跨尺度状态空间模块和动态门控融合模块整合多模态信息。实验表明，LoongX在文本驱动方法上表现相当甚至更好，尤其在结合语音时更具优势。研究展示了神经驱动生成模型在无障碍图像编辑中的潜力，并为认知驱动的创意技术提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:31:50 GMT</pubDate>
</item>
<item>
<title>基于LLM架构的自回归视频生成模型Lumos-1</title>
<link>https://arxiv.org/abs/2507.08801</link>
<guid>https://arxiv.org/abs/2507.08801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumos-1实现高效自回归视频生成，保持LLM架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lumos-1，一个基于大语言模型（LLM）架构的自回归视频生成模型。该模型通过引入MM-RoPE机制增强时空相关性，并采用令牌依赖策略优化帧内和帧间关系。为解决帧级损失不平衡问题，提出AR-DF方法，在训练中引入时间管屏蔽策略以提升生成质量。Lumos-1仅使用48块GPU即可达到与EMU3、COSMOS-Video2World和OpenSoraPlan相当的性能，展示了其在视频生成任务中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>NeuralOS：基于神经网络的GUI模拟框架</title>
<link>https://arxiv.org/abs/2507.08800</link>
<guid>https://arxiv.org/abs/2507.08800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralOS通过预测用户输入生成GUI界面，提升人机交互体验。</p><br /><br /><p><strong>摘要：</strong> NeuralOS是一个基于神经网络的框架，能够根据用户输入（如鼠标移动、点击和键盘事件）直接生成图形用户界面。该框架结合了循环神经网络（RNN）和基于扩散的神经渲染器，用于跟踪系统状态并生成屏幕图像。训练数据来自Ubuntu XFCE的大量记录，包括随机交互和AI代理生成的真实交互。实验表明，NeuralOS能够生成逼真的GUI序列，准确捕捉鼠标交互，并可靠预测应用启动等状态转换。尽管在精确建模键盘交互方面仍存在挑战，但NeuralOS为未来自适应、生成式的人机交互系统提供了重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>生成式奖励模型的脆弱性与改进方法</title>
<link>https://arxiv.org/abs/2507.08794</link>
<guid>https://arxiv.org/abs/2507.08794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式奖励模型易受表面操纵，研究提出增强方法提升鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 生成式奖励模型（LLMs-as-judges）在强化学习中被广泛用于评估答案质量，但研究发现它们对非单词符号或推理引导语句容易产生错误奖励。这种漏洞在多种模型、数据集和提示格式中普遍存在，威胁到依赖此类模型的核心算法。为解决此问题，研究引入一种数据增强策略，并训练出更稳健的奖励模型。研究强调了改进基于LLM的评估方法的重要性，并公开了相关模型和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:55:22 GMT</pubDate>
</item>
<item>
<title>基于压缩光场令牌的神经渲染方法</title>
<link>https://arxiv.org/abs/2507.08776</link>
<guid>https://arxiv.org/abs/2507.08776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效神经渲染方法CLiFT，实现高质量场景重建与视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于“压缩光场令牌（CLiFTs）”的神经渲染方法，能够保留场景的丰富外观和几何信息。该方法通过多视角编码器将图像转换为令牌，并利用潜在空间K均值算法选择关键光线作为聚类中心。随后，通过多视角“压缩器”将所有令牌的信息压缩到中心令牌中构建CLiFTs。在测试阶段，根据目标视图和计算预算收集相应数量的邻近令牌，并使用自适应计算渲染器生成新视图。实验表明，该方法在RealEstate10K和DL3DV数据集上实现了显著的数据压缩，同时保持了与现有方法相当的渲染质量，并提供了数据大小、渲染质量和速度之间的权衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:38:52 GMT</pubDate>
</item>
<item>
<title>CoPart：基于部件感知的3D生成框架</title>
<link>https://arxiv.org/abs/2507.08772</link>
<guid>https://arxiv.org/abs/2507.08772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoPart提升3D生成的细节和可控性。</p><br /><br /><p><strong>摘要：</strong> 本文提出CoPart，一种基于部件感知的3D生成框架，通过将3D对象分解为上下文相关的部件潜在表示，实现更精确的多部件生成。该方法解决了传统单潜变量表示在复杂几何结构上的不足，并增强了部件间关系建模与细粒度控制能力。研究团队还构建了Partverse数据集，支持大规模训练。实验表明，CoPart在部件级编辑、关节物体生成和场景构建方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:33:18 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型中的模态冲突与幻觉研究</title>
<link>https://arxiv.org/abs/2507.07151</link>
<guid>https://arxiv.org/abs/2507.07151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在模态冲突下的幻觉问题及缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文从模态冲突的角度探讨多模态大语言模型（MLLMs）在现实场景中产生幻觉的现象。不同于以往关注模型输出与输入之间冲突的研究，本文聚焦于不同模态输入之间的内在冲突，这些冲突直接导致了幻觉的出现。作者提出了一个名为Multimodal Modality Conflict（MMMC）的数据集来模拟这一现象，并设计了三种基于提示工程、监督微调和强化学习的方法来缓解由模态冲突引起的幻觉。实验结果表明，强化学习方法在减少幻觉方面表现最佳，而监督微调方法则表现出良好的稳定性和潜力。该研究为理解MLLMs的鲁棒性提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 07:18:38 GMT</pubDate>
</item>
<item>
<title>MetaStone-S1：基于自监督奖励模型的生成式模型</title>
<link>https://arxiv.org/abs/2507.01951</link>
<guid>https://arxiv.org/abs/2507.01951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaStone-S1通过SPRM实现与OpenAI o3相当的性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MetaStone-S1，这是首个基于自监督过程奖励模型（SPRM）的生成式模型。该模型通过共享主干网络并使用任务特定头部进行下一步预测和过程评分，将策略模型与过程奖励模型整合到统一接口中，大幅减少了PRM参数量，提升了推理效率。MetaStone-S1支持测试时缩放（TTS），提供三种推理努力模式，并实证建立了总思考计算与TTS性能之间的缩放规律。实验表明，MetaStone-S1在仅32B参数规模下，性能可与OpenAI-o3-mini系列媲美。研究团队已开源该项目。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:01 GMT</pubDate>
</item>
<item>
<title>基于动态分块机制的端到端语言模型研究</title>
<link>https://arxiv.org/abs/2507.07955</link>
<guid>https://arxiv.org/abs/2507.07955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">H-Nets实现端到端语言建模，提升数据效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的动态分块机制，通过将内容和上下文依赖的分割策略与模型联合学习，构建了一个显式的分层网络（H-Net），从而替代传统的分词-语言模型-反分词流程。实验表明，H-Nets在计算和数据匹配的情况下，优于基于BPE分词的Transformer模型，并且多级分层结构进一步提升了性能，表现出更好的数据扩展性。预训练的H-Nets在字符层面更具鲁棒性，无需启发式规则或监督即可学习有意义的分块策略。在中文、代码和DNA序列等分词较弱的语言和模态中，H-Nets表现出显著的数据效率提升，展示了端到端模型从原始数据中学习和扩展的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:39:37 GMT</pubDate>
</item>
<item>
<title>基于Re-Bottleneck的神经音频编码器结构优化</title>
<link>https://arxiv.org/abs/2507.07867</link>
<guid>https://arxiv.org/abs/2507.07867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重构瓶颈提升音频模型在不同任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Re-Bottleneck的框架，用于优化预训练神经音频编码器的潜在空间结构。该方法通过引入一个专门通过潜在空间损失训练的内部瓶颈，实现对潜在通道的排序、与语义嵌入对齐以及引入等变性，从而提升模型在不同下游任务中的性能。实验表明，该框架能够在不牺牲重建质量的前提下，有效增强模型的灵活性和适应性，适用于多种音频应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 11:47:43 GMT</pubDate>
</item>
<item>
<title>基于固定嵌入的模块化与分层扩展方法提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.07129</link>
<guid>https://arxiv.org/abs/2507.07129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出一种新型模型扩展方法，提升大模型灵活性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种不同于传统端到端训练的大语言模型扩展方法，利用不可训练的确定性输入嵌入作为基础，实现了模块化组合和分层增长。实验表明，不同数据集训练的专家模型可通过简单平均输出logits合并为更强大的混合专家模型，且在推理任务中表现优于单一模型。同时，通过逐层构建深度Transformer，验证了模型深度与复杂推理能力之间的关系。该方法为资源高效扩展、持续学习和更民主化的AI系统开发提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 16:01:15 GMT</pubDate>
</item>
<item>
<title>非语义嵌入在Transformer模型中的有效性研究</title>
<link>https://arxiv.org/abs/2507.04886</link>
<guid>https://arxiv.org/abs/2507.04886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">非语义嵌入可替代语义嵌入并提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了传统观点，即输入嵌入是语义表示的基础。研究构建了冻结嵌入层的Transformer模型，使用基于Unicode字形结构的预计算视觉嵌入。尽管这些嵌入不包含语义信息，但模型仍能收敛、生成连贯文本，并在MMLU推理基准上优于具有可训练嵌入的模型。研究认为，传统模型中嵌入层同时学习结构和语义特征导致了表征干扰，而高阶语义是Transformer架构和数据规模的涌现属性。该研究重新定义了嵌入的作用，将其从语义容器转变为结构基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title>通过动态调整预训练模型结构提升推理效率与准确性</title>
<link>https://arxiv.org/abs/2507.07996</link>
<guid>https://arxiv.org/abs/2507.07996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">预训练模型可通过动态调整结构提升推理效率和准确率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，利用预训练大型语言模型（LLM）的层作为可组合模块，构建针对每个测试样本的定制化浅层模型。通过跳过、重复或重新排列层，形成链式结构（CoLa），并采用蒙特卡洛树搜索（MCTS）优化每种样本的最佳结构。实验表明，CoLa在保持高准确率的同时，能显著提升推理效率，并在部分原本预测错误的样本中实现正确预测，展示了预训练模型在不同输入下动态适应的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视频大模型中的时空令牌合并方法STTM</title>
<link>https://arxiv.org/abs/2507.07990</link>
<guid>https://arxiv.org/abs/2507.07990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STTM提升视频理解效率，减少计算负担。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的时空令牌合并方法STTM，旨在解决视频大语言模型中因令牌数量增加而导致的计算复杂度问题。STTM通过利用视频数据中的局部空间和时间冗余，采用分层空间令牌生成和定向时间合并策略，在多个视频问答基准测试中表现出色。实验结果显示，在50%的令牌预算下，STTM实现了两倍的速度提升且仅损失0.5%的准确性；在30%预算下，速度提升三倍，仅损失2%的准确性。此外，STTM具有查询无关性，支持同一视频不同问题间的键值缓存复用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>在线时空理解基准OST-Bench推动多模态大语言模型发展</title>
<link>https://arxiv.org/abs/2507.07984</link>
<guid>https://arxiv.org/abs/2507.07984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OST-Bench评估多模态模型在线时空推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OST-Bench，一个用于评估多模态大语言模型在在线场景中进行时空理解的基准。该基准强调模型在动态环境中逐步获取信息并结合历史记忆进行推理的能力，与真实世界的沉浸式感知更贴近。基于ScanNet、Matterport3D和ARKitScenes数据集，OST-Bench包含1.4k场景和10k问答对。实验发现，现有模型在需要复杂时空推理的任务中表现不佳，随着探索范围扩大和记忆增长，准确率下降明显。研究还揭示了模型在空间线索推理和长期记忆检索方面的性能瓶颈，为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>视觉-语言模型的线性推理瓶颈与对齐优化研究</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示VLMs在抽象推理中的线性分离瓶颈及解决方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前最先进的视觉-语言模型（VLMs）在抽象推理任务中受到的线性分离瓶颈问题。通过引入线性可分性上限（LSC），研究发现这一瓶颈并非源于感知能力不足，而是语言模型推理路径的失败。研究指出该问题可通过针对性对齐解决，简单语义概念只需激活现有路径，而复杂关系推理则需要调整核心模型权重。研究还发现，尽管后缀微调能激活模型中潜在的推理路径，但在需要深度适应的复杂任务中，提升表示质量反而会导致模型在新提示格式下失效。该研究为VLM分析提供了新的视角，强调稳健推理的关键在于精准对齐而非单纯提升表示学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 05:23:32 GMT</pubDate>
</item>
<item>
<title>基于时间感知的视觉表示学习方法ToBo</title>
<link>https://arxiv.org/abs/2507.06543</link>
<guid>https://arxiv.org/abs/2507.06543</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToBo通过紧凑令牌预测动态场景，提升序列理解任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Token Bottleneck (ToBo) 的自监督学习方法，旨在从动态场景中提取紧凑且具有时间感知能力的视觉表示。该方法通过将场景压缩为一个瓶颈令牌，并利用少量目标图像块作为提示来预测后续场景，从而学习序列场景的表示。ToBo在视频标签传播和机器人操作等任务中表现出色，并在真实机器人环境中验证了其鲁棒性和有效性。此外，研究还验证了该方法在不同模型规模下的可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06543" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:57:29 GMT</pubDate>
</item>
<item>
<title>基于单图定制的扩散模型微调方法T-LoRA</title>
<link>https://arxiv.org/abs/2507.05964</link>
<guid>https://arxiv.org/abs/2507.05964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">T-LoRA解决单图定制下的过拟合问题，提升模型泛化与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出T-LoRA，一种针对扩散模型个性化定制的时步依赖低秩微调框架。在数据有限的情况下，传统微调方法容易过拟合，影响模型的泛化能力和生成多样性。T-LoRA通过动态调整不同时间步的微调策略，并采用正交初始化确保适配器组件独立性，有效缓解了这一问题。实验表明，T-LoRA在概念保真度和文本对齐之间取得了更好的平衡，适用于数据和资源受限的场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:14:10 GMT</pubDate>
</item>
<item>
<title>SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?</title>
<link>https://arxiv.org/abs/2507.05241</link>
<guid>https://arxiv.org/abs/2507.05241</guid>
<content:encoded><![CDATA[
The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:50:52 GMT</pubDate>
</item>
<item>
<title>TreeBench与TreeVGR：推动视觉基础推理的新基准与训练方法</title>
<link>https://arxiv.org/abs/2507.07999</link>
<guid>https://arxiv.org/abs/2507.07999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TreeBench评估视觉推理能力，引入TreeVGR提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉基础推理模型缺乏全面评估的问题，提出了TreeBench基准，该基准基于三个原则：对复杂场景中细微目标的精准感知、通过边界框评估的可追溯证据，以及对物体交互和空间层次的二阶推理。TreeBench包含405个具有挑战性的视觉问答对，即使最先进的模型如OpenAI-o3也仅达到54.87%的准确率。同时，文章介绍了TreeVGR训练范式，结合强化学习实现定位与推理的联合监督，显著提升了多个基准测试的性能，证明了可追溯性在视觉基础推理中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>PyVision：动态工具生成框架提升视觉推理能力</title>
<link>https://arxiv.org/abs/2507.07998</link>
<guid>https://arxiv.org/abs/2507.07998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyVision通过动态生成工具提升视觉推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PyVision，一个交互式、多轮的框架，使大语言模型能够自主生成、执行和优化基于Python的工具，从而实现灵活且可解释的问题解决。研究团队对PyVision生成的工具进行了分类，并在多个基准测试中分析了其应用效果。实验结果显示，PyVision显著提升了GPT-4.1和Claude-4.0-Sonnet的性能，表明动态工具使用不仅增强了模型的推理能力，还使其具备创造新工具的能力，推动视觉推理向更智能的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>几何引导提升视频扩散模型的3D一致性</title>
<link>https://arxiv.org/abs/2507.07982</link>
<guid>https://arxiv.org/abs/2507.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过几何引导提升视频生成模型的3D结构一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何引导（Geometry Forcing）的方法，旨在增强视频扩散模型对3D结构的理解。该方法通过将模型的中间表示与预训练几何基础模型的特征对齐，引导模型学习具有几何感知能力的潜在表示。具体包括角度对齐和尺度对齐两个目标，分别通过余弦相似度和归一化几何特征回归实现。实验结果表明，该方法在相机视角和动作条件下的视频生成任务中显著提升了视觉质量和3D一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型长视频推理能力的全栈框架</title>
<link>https://arxiv.org/abs/2507.07966</link>
<guid>https://arxiv.org/abs/2507.07966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种提升VLM长视频推理能力的全栈框架及实验验证。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全栈框架，用于扩展视觉语言模型（VLMs）在长视频上的推理能力，通过强化学习实现。该框架包含三个关键组件：一个大规模的长视频问答数据集LongVideo-Reason，一个两阶段训练流程，以及一个名为MR-SP的长视频强化学习训练基础设施。实验表明，LongVILA-R1-7B在多个长视频QA基准测试中表现优异，并在多个推理任务上超越了现有模型。此外，MR-SP系统实现了2.1倍的训练加速，支持在单个A100节点上进行长达一小时的视频强化学习训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:47:40 GMT</pubDate>
</item>
<item>
<title>机器谎言：大语言模型中虚假陈述的机制与评估</title>
<link>https://arxiv.org/abs/2507.07484</link>
<guid>https://arxiv.org/abs/2507.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型中虚假陈述现象及其机制。</p><br /><br /><p><strong>摘要：</strong> 本文提出“机器谎言”作为描述大语言模型中失去真实性的新概念，引入“谎言指数”作为衡量模型对真实性漠不关心的指标，并分析四种形式的谎言：空洞修辞、模糊言辞、含糊其辞和未经验证的声明。通过在多个数据集上的实验，研究发现强化学习微调和推理时的思维链提示会加剧谎言现象，尤其在政治语境中，模糊言辞成为主要策略。研究揭示了AI对齐中的系统性挑战，并为提升模型的真实性提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 03:11:57 GMT</pubDate>
</item>
<item>
<title>长视频生成技术的现状与分类研究</title>
<link>https://arxiv.org/abs/2507.07202</link>
<guid>https://arxiv.org/abs/2507.07202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究分析了长视频生成技术的挑战与进展。</p><br /><br /><p><strong>摘要：</strong> 本文综述了当前视频生成模型在长视频创作中的局限性，如16秒以上的视频难以保持角色一致性与场景连贯性。尽管有方法可生成长达150秒的视频，但存在帧冗余和时间多样性低的问题。研究分析了32篇相关论文，总结了关键架构组件和训练策略，并构建了一个新的分类体系，以帮助理解不同方法的设计与性能特点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:20:33 GMT</pubDate>
</item>
<item>
<title>LangSplatV2：提升3D语言场推理速度与精度的高效方法</title>
<link>https://arxiv.org/abs/2507.07136</link>
<guid>https://arxiv.org/abs/2507.07136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangSplatV2实现高速3D文本查询与特征投射。</p><br /><br /><p><strong>摘要：</strong> 本文提出LangSplatV2，通过引入稀疏系数场和CUDA优化，显著提升了3D语言场的推理速度和查询精度。相比之前的LangSplat，LangSplatV2在高分辨率图像下实现了476.2 FPS的高维特征投射和384.6 FPS的3D开放词汇文本查询，分别提升了42倍和47倍。该方法通过将每个高斯视为全局字典中的稀疏编码，消除了重型解码器的需要，从而大幅提高效率。实验结果表明，LangSplatV2不仅在查询准确性上表现优异，还在实际应用中具备更高的可行性。代码和演示可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 20:19:58 GMT</pubDate>
</item>
<item>
<title>利用扩散模型提升多模态大语言模型的视觉理解能力</title>
<link>https://arxiv.org/abs/2507.07106</link>
<guid>https://arxiv.org/abs/2507.07106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型可作为更优的视觉编码器，提升多模态模型的视觉理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将预训练文本到图像扩散模型作为指令感知的视觉编码器，以弥补传统CLIP模型在细粒度信息捕捉上的不足。通过分析扩散模型内部表示，发现其具有丰富的语义和强图像-文本对齐能力。研究还发现可通过文本条件引导模型关注与问题相关的区域，并探索了如何将这些特征与大语言模型对齐。同时揭示了信息泄露现象，并提出缓解策略。最终通过融合CLIP与扩散特征，在通用VQA和专业MLLM基准测试中验证了扩散模型在视觉任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于门控记忆单元的高效序列建模架构SambaY</title>
<link>https://arxiv.org/abs/2507.06607</link>
<guid>https://arxiv.org/abs/2507.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SambaY通过GMU提升解码效率与长上下文性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SambaY的新型解码器-混合解码器架构，引入门控记忆单元（GMU）实现跨层记忆共享，显著提升解码效率并保持线性预填充时间复杂度。该模型在不依赖显式位置编码的情况下，提升了长上下文处理能力，并在大规模计算条件下表现出更低的不可约损失。实验表明，SambaY在数学推理任务中优于现有基线模型，同时在vLLM框架下实现了10倍的解码吞吐量提升。训练代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 03:27:00 GMT</pubDate>
</item>
<item>
<title>Video-RTS：提升视频推理能力的高效强化学习方法</title>
<link>https://arxiv.org/abs/2507.06485</link>
<guid>https://arxiv.org/abs/2507.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Video-RTS通过高效RL和TTS策略提升视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Video-RTS，一种基于强化学习（RL）和视频自适应测试时缩放（TTS）策略的视频推理方法，旨在提高数据效率并减少对大规模监督微调的依赖。该方法跳过资源密集的微调步骤，采用基于输出奖励的纯RL训练，并引入稀疏到密集的TTS策略以提升推理效果。实验表明，在多个视频推理基准上，Video-RTS仅使用3.6%的训练样本便实现了比现有模型平均高出2.4%的准确率，例如在Video-Holmes和MMVU基准上分别提升了4.2%和2.6%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 22:06:13 GMT</pubDate>
</item>
<item>
<title>PERK：一种高效长上下文推理方法</title>
<link>https://arxiv.org/abs/2507.06415</link>
<guid>https://arxiv.org/abs/2507.06415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PERK提升长上下文推理性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出PERK（Parameter Efficient Reasoning over Knowledge），一种用于在测试时通过轻量级模型适配器进行参数更新的高效长上下文推理方法。PERK采用两层嵌套优化循环，在元训练阶段，内层快速将上下文编码为低秩适配器，作为基础模型的参数高效记忆模块；外层则学习利用更新后的适配器准确回忆和推理相关信息。实验表明，PERK在多个长上下文推理任务中表现优异，相比标准提示方法，小模型GPT-2平均提升90%，大模型Qwen-2.5-0.5B平均提升27%。PERK在推理复杂度、长度外推和相关信息位置方面更具鲁棒性，且推理效率优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 17:38:45 GMT</pubDate>
</item>
<item>
<title>基于分层框架的自主外科手术研究</title>
<link>https://arxiv.org/abs/2505.10251</link>
<guid>https://arxiv.org/abs/2505.10251</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出分层框架实现长期、精细的自主手术操作。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自主外科手术，针对现实世界中复杂、长时间的操作需求，提出了一种分层框架。该框架包括高层任务规划策略和低层轨迹生成策略，高层通过语言空间进行任务指导，低层则负责机器人运动控制。通过离体胆囊切除实验验证了方法的有效性，实现了100%的自主成功率，标志着向临床应用迈出重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10251" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 09:04:53 GMT</pubDate>
</item>
<item>
<title>4KAgent：一种统一的图像超分辨率通用系统</title>
<link>https://arxiv.org/abs/2507.07105</link>
<guid>https://arxiv.org/abs/2507.07105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4KAgent可将低分辨率图像提升至4K甚至更高。</p><br /><br /><p><strong>摘要：</strong> 4KAgent是一种统一的图像超分辨率通用系统，能够将任何图像提升至4K分辨率甚至更高。该系统由三个核心组件构成：用于定制处理流程的Profile模块、利用视觉语言模型和图像质量评估专家进行分析的感知代理，以及执行修复计划并遵循质量驱动策略的修复代理。此外，系统还包含专门的人脸修复管道，显著提升了肖像和自拍照片的面部细节。在11个任务类别和26个基准测试中进行了严格评估，覆盖自然图像、人像、AI生成内容、卫星图像、荧光显微镜和医学影像等广泛领域，展示了其在感知和保真度指标上的优越性能。4KAgent旨在推动低级视觉任务中自主代理的研究与创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>面向包容性内容审核的多视角毒性语言检测研究</title>
<link>https://arxiv.org/abs/2507.05455</link>
<guid>https://arxiv.org/abs/2507.05455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新数据集和模型以提升毒性语言检测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了毒性语言检测在构建安全在线环境中的重要性，指出现有模型因依赖单一标注而忽视了社区规范和语境影响。为此，研究团队创建了包含6.8K社交媒体帖子和40K标注的MODELCITIZENS数据集，并通过LLM生成对话场景增强数据。实验表明，当前主流工具在该数据集上表现不佳，而基于该数据集微调的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在分布内评估中优于GPT-o4-mini。研究强调了社区参与标注和建模对包容性内容审核的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 16:15:18 GMT</pubDate>
</item>
<item>
<title>基于多智能体的mLLM有害模因评估框架AdamMeme</title>
<link>https://arxiv.org/abs/2507.01702</link>
<guid>https://arxiv.org/abs/2507.01702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdamMeme动态评估mLLM对有害模因的理解能力。</p><br /><br /><p><strong>摘要：</strong> 随着社交媒体上多模态模因的普及，需要更有效的评估方法来衡量多模态大语言模型（mLLMs）对有害模因的理解能力。现有基准依赖静态数据集和准确率评估，难以适应快速变化的在线模因。为此，本文提出AdamMeme，一个基于多智能体的自适应评估框架，通过协作更新挑战性模因样本，全面评估mLLMs在识别有害模因方面的表现，揭示不同模型的具体弱点。实验表明，该框架能系统分析模型性能并提供细粒度分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 09:32:30 GMT</pubDate>
</item>
<item>
<title>推动文本到动作生成的零样本泛化能力</title>
<link>https://arxiv.org/abs/2507.07095</link>
<guid>https://arxiv.org/abs/2507.07095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionMillion数据集与评估框架，提升文本生成动作的零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于基于文本描述生成多样且自然的人类运动序列这一挑战性任务。尽管已有进展，但现有方法在零样本泛化方面仍存在不足，主要受限于训练数据量和缺乏全面评估体系。为此，研究团队构建了目前最大的人体运动数据集MotionMillion，包含2000小时、200万条高质量运动序列，并提出了MotionMillion-Eval作为最全面的评估基准。通过扩展模型至7B参数规模，实验验证了其在跨领域和复杂组合运动上的强大泛化能力，标志着向零样本人类运动生成迈出了重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:52:04 GMT</pubDate>
</item>
<item>
<title>FR3E框架提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2507.07017</link>
<guid>https://arxiv.org/abs/2507.07017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FR3E增强LLM推理稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FR3E（First Return, Entropy-Eliciting Explore）框架，用于改进大语言模型（LLM）的推理能力。该方法通过识别推理轨迹中的高不确定性决策点，并进行针对性的回溯，以构建语义基础的中间反馈，从而在无需密集监督的情况下提供更精准的指导。实验结果表明，FR3E能够提升训练稳定性、生成更长且连贯的响应，并提高完全正确推理路径的比例，验证了其在增强LLM推理能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:48 GMT</pubDate>
</item>
<item>
<title>提升代码生成评估的测试用例生成方法研究</title>
<link>https://arxiv.org/abs/2507.06920</link>
<guid>https://arxiv.org/abs/2507.06920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAGA方法提升测试用例质量与覆盖率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前代码生成评估基准中测试用例数量有限且同质化的问题，提出了一种多维度测试用例生成方法SAGA，结合人类编程经验和大语言模型推理能力，显著提升了测试用例的覆盖范围和质量。实验表明，该方法在TCGBench上的检测率达到90.62%，验证准确率为32.58%，并使合成评估基准的验证准确率比LiveCodeBench-v6高出10.78%。研究旨在构建更可靠的LLM代码评估体系，推动强化学习框架中的奖励估计及自动化对抗测试生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 10:58:47 GMT</pubDate>
</item>
<item>
<title>基于多模态光谱数据的分子结构生成框架DiffSpectra</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffSpectra通过扩散模型从多模态光谱数据中生成分子结构。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiffSpectra，一个基于扩散模型的生成框架，能够直接从多模态光谱数据中推断出分子的2D和3D结构。该方法将结构解析问题建模为条件生成过程，采用SE(3)-等变架构的Diffusion Molecule Transformer来整合拓扑与几何信息，并通过SpecFormer编码器捕捉多模态光谱中的依赖关系。实验表明，DiffSpectra在结构恢复任务中表现出色，具有较高的准确率。该工作是首个将多模态光谱推理与联合2D/3D生成建模相结合的分子结构解析框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 09:57:20 GMT</pubDate>
</item>
<item>
<title>混合线性注意力机制在长序列建模中的研究与优化</title>
<link>https://arxiv.org/abs/2507.06457</link>
<guid>https://arxiv.org/abs/2507.06457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究混合线性注意力模型在长序列任务中的表现与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了多种线性注意力模型在独立和混合架构中的表现，训练并开源了72个模型，涵盖不同参数规模和混合比例。实验表明，优秀的独立线性模型在混合架构中未必表现最佳，而增加全注意力层能显著提升召回性能，尤其在3:1以下比例时效果更明显。研究强调了选择性门控、层次递归和可控遗忘的重要性，并推荐HGRN-2或GatedDeltaNet等架构在3:1至6:1的混合比例下实现高效的Transformer级召回性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:54:11 GMT</pubDate>
</item>
<item>
<title>PAPO：一种增强多模态推理的感知意识强化学习方法</title>
<link>https://arxiv.org/abs/2507.06448</link>
<guid>https://arxiv.org/abs/2507.06448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAPO提升多模态任务中的视觉感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PAPO，一种基于GRPO的感知意识策略优化方法，通过引入隐式感知损失提升模型在多模态任务中的表现。PAPO无需额外数据或外部奖励模型，仅依赖内部监督信号，在多个多模态基准测试中取得显著提升，尤其在高视觉依赖任务中效果更佳。研究还发现并解决了损失黑客问题，通过双熵损失进行缓解，为视觉驱动的强化学习提供了新框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:22:34 GMT</pubDate>
</item>
<item>
<title>AutoTriton：基于强化学习的Triton编程模型</title>
<link>https://arxiv.org/abs/2507.05687</link>
<guid>https://arxiv.org/abs/2507.05687</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoTriton利用强化学习优化Triton编程，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AutoTriton，这是一个基于强化学习（RL）的Triton编程模型，旨在优化深度学习中的内核开发。通过监督微调和GRPO算法，AutoTriton能够自动调整关键参数，如tile大小和内存访问模式，从而提高计算效率。实验表明，AutoTriton在多个基准测试中表现优异，接近主流大模型。研究还验证了各个模块对性能提升的重要性，展示了强化学习在生成高性能内核方面的潜力，为构建更高效的AI系统奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05687" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 01:38:24 GMT</pubDate>
</item>
<item>
<title>解耦推理与证明：提升自动化定理证明的新框架</title>
<link>https://arxiv.org/abs/2507.06804</link>
<guid>https://arxiv.org/abs/2507.06804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出解耦推理与证明的新框架，显著提升数学难题的自动证明能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对自动化定理证明（ATP）中大型语言模型在形式化证明方面的表现不足问题，提出一种将高层推理与底层证明分离的新框架。该框架采用两个专门模型：一个用于生成多样化的子目标引理，另一个用于严格验证这些引理。实验表明，该方法在2000年后国际数学奥林匹克竞赛（IMO）难题集上成功解决了5道题目，展示了在复杂数学挑战中实现自动化推理的重要进展。研究还发布了包含大量生成和验证引理的数据集，以促进未来相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 18:38:49 GMT</pubDate>
</item>
<item>
<title>Nova Premier模型的安全评估与公开发布</title>
<link>https://arxiv.org/abs/2507.06260</link>
<guid>https://arxiv.org/abs/2507.06260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nova Premier通过安全评估，符合公开发布标准。</p><br /><br /><p><strong>摘要：</strong> Nova Premier是亚马逊最强大的多模态基础模型，支持文本、图像和视频处理，具有100万词的上下文窗口。文章介绍了基于前沿模型安全框架对Nova Premier进行的全面风险评估，重点考察了化学、生物、放射性和核（CBRN）、进攻性网络操作以及自动化AI研发等高风险领域。评估结合了自动化基准测试、专家红队测试和提升研究，结果显示Nova Premier符合2025年巴黎人工智能安全峰会的承诺，可安全公开发布。未来将继续完善安全评估和缓解流程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:33:35 GMT</pubDate>
</item>
<item>
<title>面向自动驾驶的视觉-语言-行动模型综述</title>
<link>https://arxiv.org/abs/2506.24044</link>
<guid>https://arxiv.org/abs/2506.24044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述自动驾驶中视觉-语言-行动模型的发展与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次全面回顾了面向自动驾驶的视觉-语言-行动（VLA4AD）模型。文章梳理了VLA模型的架构组成，追溯了从早期解释器到以推理为核心的模型演进过程，并对比分析了20多个代表性模型。同时，文章整合了现有数据集和评估标准，强调了驾驶安全、准确性和解释质量的综合评估。最后，文章指出了VLA4AD面临的开放性挑战，如鲁棒性、实时效率和形式化验证，并提出了未来研究方向，为可解释且符合社会规范的自动驾驶系统提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 12:50:02 GMT</pubDate>
</item>
<item>
<title>Agent KB：提升智能体错误纠正与跨领域知识复用的框架</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent KB提升智能体任务解决能力，实现跨领域知识共享。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Agent KB，这是一个基于Reason-Retrieve-Refine管道的分层经验框架，旨在提高智能体在复杂任务中的表现。该框架通过捕捉高层策略和详细执行日志，构建共享知识库，实现跨智能体的知识迁移。在GAIA基准测试中，Agent KB使成功率达到最高提升16.28个百分点，显著提升了Claude-3和GPT-4等模型的性能。此外，在SWE-bench代码修复任务中也表现出色。研究结果表明，Agent KB为智能体提供了一个模块化、框架无关的学习基础设施，使其能够从过往经验中学习并推广成功策略到新任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>NeoBabel：多语言图像生成的新范式</title>
<link>https://arxiv.org/abs/2507.06137</link>
<guid>https://arxiv.org/abs/2507.06137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBabel支持六种语言，提升图像生成的多语言性能与包容性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了NeoBabel，一种新型多语言图像生成框架，支持英语、中文、荷兰语、法语、印地语和波斯语。该模型通过大规模多语言预训练和高分辨率指令调优进行训练，在扩展的多语言基准测试中表现出色，同时保持强大的英语能力。NeoBabel在多语言任务上优于现有模型，并且模型规模更小。研究还引入了新的评估指标，以衡量多语言对齐和代码混合提示的鲁棒性。作者开源了工具包，包括代码、模型检查点、数据集和标准化评估协议，旨在推动包容性AI研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:19:45 GMT</pubDate>
</item>
<item>
<title>MedGemma：医疗视觉-语言基础模型的开发与应用</title>
<link>https://arxiv.org/abs/2507.05201</link>
<guid>https://arxiv.org/abs/2507.05201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedGemma在医疗任务中表现出色，提升AI在医疗领域的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedGemma，一个基于Gemma 3 4B和27B的医疗视觉-语言基础模型集合。MedGemma在医学图像和文本理解方面表现出色，性能优于同类模型并接近专用模型水平，同时保持了Gemma 3基础模型的通用能力。在分布外任务中，MedGemma在多模态问答、胸部X光诊断分类和代理评估中分别提升了2.6%-10%、15.5%-18.1%和10.8%。微调后进一步提升了子领域的表现，如电子健康记录信息检索错误率降低50%，并在气胸分类和组织病理学图像分类中达到现有先进方法的水平。此外，MedSigLIP作为医疗优化的视觉编码器，也展现出与专业医学图像编码器相当或更优的性能。MedGemma为医疗研究和下游应用提供了强大的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:01:44 GMT</pubDate>
</item>
<item>
<title>世界模型的理论探讨与新型架构设计</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨世界模型的定义与构建，提出新型多层级架构。</p><br /><br /><p><strong>摘要：</strong> 本文从科幻作品《沙丘》出发，结合心理学中的假设性思维概念，对当前世界模型的研究进行了批判性分析。文章指出，世界模型的核心目标是模拟现实世界的可行动可能性，以支持有目的的推理与行为。基于此，作者提出了一种基于分层、多级、连续/离散混合表示的新架构，并引入生成式自监督学习框架，展望了由该模型驱动的物理、代理和嵌套（PAN）通用人工智能系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:23:46 GMT</pubDate>
</item>
<item>
<title>无监督语义场景补全方法SceneDINO的研究</title>
<link>https://arxiv.org/abs/2507.06230</link>
<guid>https://arxiv.org/abs/2507.06230</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SceneDINO实现无监督3D场景语义补全，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SceneDINO的无监督语义场景补全方法，无需依赖昂贵的标注数据。该方法结合自监督表示学习和2D无监督场景理解技术，通过多视角一致性自监督进行训练，仅凭单张图像即可推断出3D几何结构和语义特征。通过创新的3D特征蒸馏方法，实现了无监督3D语义分割。实验表明，SceneDINO在3D和2D无监督场景理解任务中均达到最先进水平，其3D特征线性探测性能与当前有监督方法相当，并展现出良好的领域泛化能力和多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06230" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>改进Mamba模型的注意力分配机制</title>
<link>https://arxiv.org/abs/2507.06204</link>
<guid>https://arxiv.org/abs/2507.06204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何优化Mamba模型的注意力分配以提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将针对Transformer的微分设计方法应用于Mamba模型，该模型基于选择性状态空间层，具有更高的效率。研究发现，直接应用传统方法效果不佳，需进行针对性调整。为此，作者提出了一种新的微分机制，并在语言建模基准上进行了验证，结果表明该方法提升了Mamba的检索能力和整体性能。此外，通过广泛的消融实验和分析，验证了设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06204" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:30:14 GMT</pubDate>
</item>
<item>
<title>SingLoRA：一种更稳定且参数更少的低秩微调方法</title>
<link>https://arxiv.org/abs/2507.05566</link>
<guid>https://arxiv.org/abs/2507.05566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SingLoRA提升模型微调稳定性并减少参数使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SingLoRA的低秩微调方法，通过将权重更新分解为单个低秩矩阵与其转置的乘积，解决了传统LoRA中矩阵尺度不一致导致的训练不稳定问题。该设计不仅消除了矩阵间的尺度冲突，还减少了约50%的参数量。理论分析表明，SingLoRA在无限宽度神经网络框架下能保证稳定的特征学习。实验结果表明，在MNLI数据集上，SingLoRA在使用更少参数的情况下实现了比LoRA和LoRA+更高的准确率；在图像生成任务中，SingLoRA在DreamBooth数据集上的表现优于DoRA和LoRA。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:11:30 GMT</pubDate>
</item>
<item>
<title>AXLearn：模块化深度学习系统的设计与实现</title>
<link>https://arxiv.org/abs/2507.05411</link>
<guid>https://arxiv.org/abs/2507.05411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AXLearn支持异构硬件，提升模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AXLearn，一个专注于模块化和异构硬件支持的深度学习系统。相比其他先进系统，AXLearn通过严格的组件封装，实现了高效的模型开发与实验。文章提出了一种基于代码量复杂度的模块化评估方法，证明了其在扩展时保持稳定复杂度的优势。例如，在集成Rotary Position Embeddings功能时，仅需10行代码即可完成，而其他系统则需要数百行。同时，AXLearn在性能上与当前最先进的系统相当。最后，作者分享了AXLearn开发与运营的经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:50:58 GMT</pubDate>
</item>
<item>
<title>StreamVLN：一种高效的多模态视觉语言导航框架</title>
<link>https://arxiv.org/abs/2507.05240</link>
<guid>https://arxiv.org/abs/2507.05240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamVLN提升实时视觉语言导航效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamVLN，一种用于现实世界中视觉语言导航（VLN）的流式框架。该框架采用混合慢速-快速上下文建模策略，结合视觉、语言和动作输入进行多模态推理。快速对话流支持响应式动作生成，而慢速更新记忆流通过3D感知的token剪枝策略压缩历史视觉状态。这种设计实现了多轮对话的连贯性，同时保持有限的上下文大小和计算成本。实验表明，StreamVLN在VLN-CE基准测试中表现出色，具有稳定的低延迟，适用于实际部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:49:41 GMT</pubDate>
</item>
<item>
<title>LOOM-Scope：一种高效且全面的长上下文评估框架</title>
<link>https://arxiv.org/abs/2507.04723</link>
<guid>https://arxiv.org/abs/2507.04723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOOM-Scope提供标准化的长上下文评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出LOOM-Scope，一个用于评估大语言模型长上下文能力的高效且全面的框架。该框架统一了不同基准的评估设置，支持高效的长上下文推理加速方法，并引入了一个轻量级但全面的基准套件，以帮助研究者更准确地评估模型性能。由于长上下文评估的计算成本较高，LOOM-Scope旨在降低社区进行综合评估的难度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 03:33:24 GMT</pubDate>
</item>
<item>
<title>Nile-Chat系列模型：支持阿拉伯语和拉丁语的埃及方言大语言模型</title>
<link>https://arxiv.org/abs/2507.04569</link>
<guid>https://arxiv.org/abs/2507.04569</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nile-Chat系列模型提升埃及方言文本处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Nile-Chat-4B、3x4B-A6B和12B等针对埃及方言的大语言模型，这些模型能够理解和生成阿拉伯语和拉丁语文本。其中，Nile-Chat-3x4B-A6B采用Branch-Train-MiX策略，将不同脚本的专业模型融合为一个MoE模型。实验表明，这些模型在新推出的埃及语评估基准上显著优于LLaMa、Jais和ALLaM等主流模型，12B版本在拉丁语基准上比Qwen2.5-14B-Instruct高出14.4%。所有资源均已公开，为双语脚本语言的模型适配提供了系统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04569" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:53:41 GMT</pubDate>
</item>
<item>
<title>基于属性切换的公平图生成框架FAROS</title>
<link>https://arxiv.org/abs/2507.03728</link>
<guid>https://arxiv.org/abs/2507.03728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FAROS通过属性切换实现图生成的公平性与准确性平衡。</p><br /><br /><p><strong>摘要：</strong> 本文提出FAROS，一种基于属性切换机制的公平图生成框架，直接在预训练图扩散模型的生成过程中运行。该方法通过调整节点的敏感属性，在保持原始图结构特征的同时，提升生成图的公平性。实验表明，FAROS在链接预测任务中有效减少公平性差异，并在准确性和公平性之间取得更好的权衡，优于其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:31:41 GMT</pubDate>
</item>
<item>
<title>基于FLOPs的LLM重排序器效率评估方法研究</title>
<link>https://arxiv.org/abs/2507.06223</link>
<guid>https://arxiv.org/abs/2507.06223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出E2R-FLOPs评估LLM重排序器效率与效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLM）在信息检索中的重排序任务中计算资源消耗过高的问题，提出了一种新的评估方法E2R-FLOPs。该方法通过每PetaFLOP的排名指标（RPP）和每PetaFLOP的查询吞吐量（QPP）来衡量模型的效率与效果，克服了传统指标受硬件和运行设置影响的局限性。同时，构建了一个可解释的FLOPs估算器，无需实际运行即可预估模型计算量。通过广泛实验，评估了多种架构的LLM重排序器，探讨了效率与效果之间的权衡关系，旨在提升该领域的研究透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:56:28 GMT</pubDate>
</item>
<item>
<title>数据多样性在机器人操作中的关键作用研究</title>
<link>https://arxiv.org/abs/2507.06219</link>
<guid>https://arxiv.org/abs/2507.06219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据多样性对机器人学习影响显著，任务多样性更重要。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据多样性在机器人学习中的作用，重点分析了任务、机器人本体和专家演示三个维度。研究发现，任务多样性比每项任务的示范数量更为重要，有助于模型在不同场景下的迁移。此外，高质量的单一本体数据预训练模型在微调时表现出更优的扩展性，而专家多样性可能对策略学习产生干扰，尤其是速度多模态是主要因素。基于此，作者提出一种分布去偏方法，提升了模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:52:44 GMT</pubDate>
</item>
<item>
<title>潜层推理：大语言模型的多步骤推理新范式</title>
<link>https://arxiv.org/abs/2507.06203</link>
<guid>https://arxiv.org/abs/2507.06203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">潜层推理提升大语言模型的推理效率与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文综述了潜层推理这一新兴领域，探讨了如何通过模型的连续隐藏状态进行多步骤推理，从而避免依赖自然语言的限制。文章首先分析了神经网络层在推理中的基础作用，接着介绍了多种潜层推理方法，包括基于激活的递归、隐藏状态传播和微调策略。最后，讨论了通过掩码扩散模型实现无限深度潜层推理等先进范式，旨在统一潜层推理的概念框架，并为大语言模型的认知研究提供未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:29:07 GMT</pubDate>
</item>
<item>
<title>CriticLean：提升形式化语义准确性的强化学习框架</title>
<link>https://arxiv.org/abs/2507.06181</link>
<guid>https://arxiv.org/abs/2507.06181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CriticLean提升形式化语义准确性，优化自动定理证明。</p><br /><br /><p><strong>摘要：</strong> 本文提出CriticLean，一个基于强化学习的批判性评估框架，旨在提升自然语言数学陈述到形式化代码的语义准确性。通过训练CriticLeanGPT模型评估Lean 4形式化语义一致性，并构建CriticLeanBench基准测试模型区分正确与错误形式化。研究还创建了包含28.5万题的FineLeanCorpus数据集，验证了CriticLean在提升形式化可靠性方面的有效性。结果表明，优化批判阶段对生成可靠形式化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:03:39 GMT</pubDate>
</item>
<item>
<title>OmniPart：支持语义解耦与结构一致的3D对象生成框架</title>
<link>https://arxiv.org/abs/2507.06165</link>
<guid>https://arxiv.org/abs/2507.06165</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniPart实现3D对象的可编辑部件生成，提升交互应用性能。</p><br /><br /><p><strong>摘要：</strong> OmniPart是一种新型的3D对象生成框架，旨在实现组件间的高语义解耦和结构一致性。该方法将任务分为两个协同阶段：第一阶段通过自回归结构规划模块生成可控制的3D部件边界框序列，并利用灵活的2D部件掩码进行引导；第二阶段则通过空间条件校正流模型，从预训练的3D生成器中高效合成所有部件。OmniPart支持用户定义的部件粒度和精确定位，适用于多种下游应用，实验表明其性能达到当前最优水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06165" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:46:15 GMT</pubDate>
</item>
<item>
<title>Code Triangle框架评估大语言模型的编程能力</title>
<link>https://arxiv.org/abs/2507.06138</link>
<guid>https://arxiv.org/abs/2507.06138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过Code Triangle框架评估LLM的编程能力，发现其存在局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度系统评估大语言模型（LLMs）的编程能力。通过在竞赛编程基准上的实验，发现虽然LLMs能够形成自洽的系统，但其解决方案缺乏人类程序员的多样性和鲁棒性。研究揭示了模型认知与人类专家之间的显著分布差异，模型错误往往因训练数据偏差和推理迁移有限而聚集。文章建议引入人类生成的编辑指南、解决方案和多样化测试用例，以及使用模型混合方法，以显著提升LLMs的性能和鲁棒性。此外，研究还展示了LLMs在认知上的一致性与不一致性，为自我反思和自我改进提供了潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:20:43 GMT</pubDate>
</item>
<item>
<title>Tora2：多实体视频生成的运动引导模型改进</title>
<link>https://arxiv.org/abs/2507.05963</link>
<guid>https://arxiv.org/abs/2507.05963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tora2提升视频生成中外观与运动的多实体定制能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tora2，这是Tora模型的增强版本，旨在提升视频生成中外观和运动的多实体定制能力。Tora2引入了解耦个性化提取器，能够为多个开放集实体生成更细致的个性化嵌入。同时，设计了门控自注意力机制，整合轨迹、文本描述和视觉信息，减少多模态条件对齐问题。此外，通过对比损失函数优化运动动态与实体一致性。实验结果表明，Tora2在保持竞争力的同时，实现了更高级的运动控制，标志着多条件视频生成的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:11:40 GMT</pubDate>
</item>
<item>
<title>基于多轮定位的策略优化方法提升大模型视觉理解能力</title>
<link>https://arxiv.org/abs/2507.05920</link>
<guid>https://arxiv.org/abs/2507.05920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MGPO方法提升大模型在高分辨率图像中的视觉定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Multi-turn Grounding-based Policy Optimization (MGPO)的端到端强化学习框架，使大模型能够通过多轮对话自动聚焦关键视觉区域，无需额外标注。与监督微调相比，MGPO利用最终答案的正确性作为二元奖励信号，有效提升了模型的视觉定位能力。实验表明，MGPO在标准数据集上表现出色，在分布内和分布外任务中均优于现有方法，甚至超越了OpenAI的o1和GPT-4o模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 08:05:05 GMT</pubDate>
</item>
<item>
<title>GUI代理任务规划与视觉定位的优化方法</title>
<link>https://arxiv.org/abs/2507.05791</link>
<guid>https://arxiv.org/abs/2507.05791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GTA1模型解决GUI任务规划和视觉定位问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对GUI代理在任务规划和视觉定位中的两个主要挑战，提出了GTA1模型。该模型通过测试时缩放方法，在每一步采样多个动作提案，并利用判断模型选择最优方案，提升决策质量。同时，通过强化学习实现更精确的视觉元素定位。实验表明，GTA1在多个基准测试中表现优异，准确率分别达到50.1%、92.4%和67.7%，并展示了出色的代理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 04:52:18 GMT</pubDate>
</item>
<item>
<title>医学视频生成新进展：MedVideoCap-55K数据集与MedGen模型</title>
<link>https://arxiv.org/abs/2507.05675</link>
<guid>https://arxiv.org/abs/2507.05675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">医学视频生成面临挑战，MedGen模型表现优异。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成技术在开放领域取得显著进展，但医学视频生成仍处于探索阶段。医学视频在临床培训、教育和模拟中具有重要作用，需要高视觉保真度和严格的医学准确性。然而，现有模型在处理医学提示时常产生不现实或错误内容，主要原因是缺乏大规模高质量的医学数据集。为此，研究者推出了MedVideoCap-55K，这是首个大规模、多样化且带有丰富描述的医学视频数据集，包含超过5.5万条精心筛选的视频片段。基于该数据集，研究人员开发了MedGen模型，在多个基准测试中表现出色，其性能可与商业系统媲美。该研究希望推动医学视频生成领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:58:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的数据记忆现象研究</title>
<link>https://arxiv.org/abs/2507.05578</link>
<guid>https://arxiv.org/abs/2507.05578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨LLM的记忆现象及其检测与缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文综述了大型语言模型（LLMs）在训练过程中对数据的记住现象，分析了影响记忆的关键因素，如训练数据重复、训练动态和微调过程。文章还介绍了多种检测方法，包括基于前缀的提取、成员推理和对抗性提示，并评估了它们的有效性。此外，讨论了记忆带来的法律和伦理问题，以及数据清洗、差分隐私和后训练遗忘等缓解策略。最后，指出了在减少有害记忆与保持模型性能之间平衡的挑战，并提出了未来研究的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:30:46 GMT</pubDate>
</item>
<item>
<title>PRING：首个基于图级别的蛋白质相互作用预测基准</title>
<link>https://arxiv.org/abs/2507.05101</link>
<guid>https://arxiv.org/abs/2507.05101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRING是首个从图级视角评估PPI预测的基准，推动生物研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PRING，这是首个从图级别评估蛋白质-蛋白质相互作用（PPI）预测的综合基准。PRING包含21,484个蛋白质和186,818个相互作用的高质量多物种数据集，并设计了应对数据冗余和泄露的策略。该基准提出了两种互补的评估范式：拓扑导向任务和功能导向任务，涵盖PPI网络构建、蛋白复合体路径预测、GO模块分析及关键蛋白识别等。实验表明，现有PPI模型在恢复结构和功能特性方面存在局限，PRING为开发更有效的PPI预测模型提供了可靠平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>any4：一种无需预处理的4位权重量化方法</title>
<link>https://arxiv.org/abs/2507.04610</link>
<guid>https://arxiv.org/abs/2507.04610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">any4在不预处理权重或激活的情况下实现高精度4位量化。</p><br /><br /><p><strong>摘要：</strong> 本文提出any4，一种无需预处理权重或激活的4位权重量化方法，能够提供任意数值表示。实验表明，any4在多个模型（如Llama 2、Llama 3、Mistral和Mixtral）上相比int4、fp4和nf4等其他4位表示方法具有更高的准确性。此外，any4在不需要预处理的情况下仍能与需要预处理的技术（如AWQ和GPTQ）相媲美。研究还探索了any3和any2在更低比特下的表现，并展示了仅需一个精心挑选的多样化样本即可进行校准，而非传统方法所需的数百个样本。同时，作者开源了tinygemm，一个针对LLM优化的GPU矩阵乘法库，支持any4及其他常见量化方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 21:59:47 GMT</pubDate>
</item>
<item>
<title>基于LLM的网络代理计算资源优化研究</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出优化LLM网络代理的计算分配策略，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大语言模型（LLM）的网络代理在开放源代码系统中的发展滞后问题，提出了一种统计基础的计算资源分配方法。通过两阶段流程，使用Llama 3.1 8B模型模仿Llama 3.3 70B教师模型，并结合监督微调和在线策略强化学习，显著提升了性能。研究发现该方法对超参数高度敏感，因此通过采样1,370个配置并利用自助法估计有效参数，避免了昂贵的试错过程。实验表明，该策略在WorkArena和MiniWob++上优于单独使用SFT或RL，且仅需55%的计算资源即可达到纯SFT的最佳性能，有效缩小了与闭源模型的差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 13:12:33 GMT</pubDate>
</item>
<item>
<title>SAMed-2：基于SAM-2架构的医学图像分割基础模型</title>
<link>https://arxiv.org/abs/2507.03698</link>
<guid>https://arxiv.org/abs/2507.03698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAMed-2提升医学图像分割性能，解决数据噪声与任务迁移问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAMed-2，一个基于SAM-2架构的医学图像分割基础模型。为应对医学数据复杂性、标注噪声及多模态学习挑战，SAMed-2引入时间适配器和置信度驱动的记忆机制，以捕捉图像相关性并存储高置信特征。研究构建了MedBank-100k数据集，涵盖七种成像模态和21项分割任务。实验表明，SAMed-2在多任务场景中优于现有最佳方法，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:30:38 GMT</pubDate>
</item>
<item>
<title>基于可验证情感奖励的强化学习框架提升语言模型情感智能</title>
<link>https://arxiv.org/abs/2507.03112</link>
<guid>https://arxiv.org/abs/2507.03112</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVER框架提升LLM情感智能，增强对话能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLVER，一个端到端的强化学习框架，利用模拟用户的可验证情感奖励来提升大型语言模型的情感智能。通过与自我一致的情感模拟用户进行对话轮次，生成确定性情感评分作为奖励信号，引导模型学习。在Qwen2.5-7B-Instruct模型上使用PPO微调后，Sentient-Benchmark得分从13.3提升至79.2，同时保持数学和编码能力。实验表明，RLVER能持续提升多种对话能力，思考型模型在共情和洞察力上表现更优，而非思考型模型则偏向行动导向。不同优化算法（如GRPO和PPO）对性能有不同影响，且中等难度环境可能带来更优结果。研究证明RLVER是构建情感智能、全面能力语言代理的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03112" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 14:33:18 GMT</pubDate>
</item>
<item>
<title>面向记忆代理的基准测试MemoryAgentBench</title>
<link>https://arxiv.org/abs/2507.05257</link>
<guid>https://arxiv.org/abs/2507.05257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MemoryAgentBench评估LLM代理的记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文指出，当前大型语言模型代理的基准测试主要关注推理、规划和执行能力，而对记忆能力的评估不足。作者定义了记忆代理的四项核心能力：准确检索、测试时学习、长程理解与冲突解决，并指出现有数据集无法满足这些需求。为此，作者提出了MemoryAgentBench，一个专门针对记忆代理的新基准，结合了重构和新构建的数据集，全面覆盖上述四项能力，为评估记忆质量提供系统化测试平台。实验表明，现有方法在掌握所有能力方面仍有不足，凸显了进一步研究综合记忆机制的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于强化学习的实体关系抽取方法研究</title>
<link>https://arxiv.org/abs/2507.04642</link>
<guid>https://arxiv.org/abs/2507.04642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-RE提升关系抽取的跨领域泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文将关系抽取任务重新定义为受标注指南引导的推理过程，提出R1-RE框架，利用可验证奖励的强化学习方法提升模型的跨领域泛化能力。实验结果显示，R1-RE-7B模型在Sem-2010和MDKG数据集上的平均OOD准确率约为70%，与GPT-4o等先进模型相当。研究还深入分析了RLVR范式在关系抽取中的训练动态和涌现推理行为，提供了新的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 23:50:59 GMT</pubDate>
</item>
<item>
<title>基于Llama 3.2 1B的隐私保护医疗转录系统研究</title>
<link>https://arxiv.org/abs/2507.03033</link>
<guid>https://arxiv.org/abs/2507.03033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种隐私保护的医疗转录系统，提升临床记录效率。</p><br /><br /><p><strong>摘要：</strong> 本文旨在开发一种隐私保护、可在设备端运行的医疗转录系统，采用微调后的Llama 3.2 1B模型，将医学转录内容转化为结构化医疗笔记。通过参数高效微调方法（LoRA）在1,500对合成数据上训练，并在两个数据集上评估其性能。结果显示，该系统在ROUGE和BERTScore等指标上显著优于基础模型，同时减少了重大幻觉问题并提高了事实准确性。该方法有助于解决医疗AI应用中的隐私、成本和可访问性问题，适用于资源有限的环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 21:51:49 GMT</pubDate>
</item>
<item>
<title>视觉嵌入模型中的有序属性捕捉研究</title>
<link>https://arxiv.org/abs/2507.03683</link>
<guid>https://arxiv.org/abs/2507.03683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现视觉嵌入模型可捕捉连续有序属性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉嵌入模型是否能通过线性方向（称为rank axes）捕捉连续的有序属性。研究发现，许多流行的编码器在多个数据集上具备这种能力，即通过投影嵌入可以保持属性的顺序。令人惊讶的是，只需少量样本或两个极端例子即可恢复有意义的rank axes，而无需全面监督。这一发现为图像排序在向量数据库中的应用提供了新可能，并推动了对可排序嵌入结构和学习方法的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:03:31 GMT</pubDate>
</item>
<item>
<title>UnMix-NeRF：结合光谱解混的神经辐射场方法</title>
<link>https://arxiv.org/abs/2506.21884</link>
<guid>https://arxiv.org/abs/2506.21884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UnMix-NeRF通过光谱解混提升材料分割与视图合成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UnMix-NeRF，一种将光谱解混引入神经辐射场（NeRF）的框架，实现联合高光谱新视角合成与无监督材料分割。该方法通过建模光谱反射率，利用全局端元字典表示纯材料特征，并通过每点丰度捕捉其分布。材料分割基于学习端元的光谱预测，实现无监督聚类。此外，UnMix-NeRF支持场景编辑，通过修改端元字典实现灵活的材料外观操控。实验表明，该方法在光谱重建和材料分割方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:42:49 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的图像编辑系统X-Planner</title>
<link>https://arxiv.org/abs/2507.05259</link>
<guid>https://arxiv.org/abs/2507.05259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Planner提升文本引导图像编辑的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Planner，一个基于多模态大语言模型的图像编辑系统，旨在解决现有方法在理解复杂指令、保持身份一致性和避免意外编辑方面的不足。X-Planner通过链式思维推理将复杂指令分解为简单子指令，并自动生成精确的编辑类型和分割掩码，无需人工干预。此外，作者还构建了一个大规模数据生成管道，使X-Planner在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于策略判别器的奖励建模方法POLAR及其性能提升</title>
<link>https://arxiv.org/abs/2507.05197</link>
<guid>https://arxiv.org/abs/2507.05197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">POLAR通过策略判别提升奖励模型性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的奖励建模方法POLAR，将奖励建模视为策略判别器，通过量化两个策略之间的差异来生成奖励信号，从而引导训练策略向目标策略靠拢。与依赖绝对偏好的传统方法不同，POLAR关注策略间的相对差异，适用于通用排序关系建模。实验表明，POLAR在多个任务中显著提升了奖励模型的性能，如在STEM任务中偏好准确率从54.8%提升至81.0%，在创意写作任务中从57.9%提升至85.5%。此外，POLAR在RLHF中的表现也十分稳健，提升了多个大模型的性能。研究还发现计算量与性能之间存在明显的幂律关系，表明POLAR具有良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:56:31 GMT</pubDate>
</item>
<item>
<title>基于低帧率相机的高速4D捕捉系统</title>
<link>https://arxiv.org/abs/2507.05163</link>
<guid>https://arxiv.org/abs/2507.05163</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需高速相机的高帧率4D重建方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于低帧率相机的高速4D捕捉系统，通过异步拍摄和生成模型提升重建效果。在拍摄端，采用异步采集方案，通过分组相机和基础帧率实现等效100-200 FPS的高帧率。在处理端，引入基于视频扩散的修复模型，解决稀疏视角重建中的伪影问题，提升细节精度和时间一致性。实验表明，该方法在高速4D重建上优于同步采集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05163" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:18:35 GMT</pubDate>
</item>
<item>
<title>自动化历史文献修复方法与全页数据集研究</title>
<link>https://arxiv.org/abs/2507.05108</link>
<guid>https://arxiv.org/abs/2507.05108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoHDR方法提升历史文献修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种全页历史文献修复数据集（FPHDR）和自动化修复方法（AutoHDR），旨在解决传统修复方法在多模态和大规模修复上的不足。FPHDR包含真实和合成图像，涵盖不同损伤等级的字符和行级标注。AutoHDR通过OCR辅助定位损伤、视觉语言文本预测和补丁自回归修复三个阶段模拟历史学家的工作流程，并支持人机协作。实验表明，该方法显著提升了OCR识别准确率，从46.83%提升至84.05%，协作后更达94.25%。该研究对文化遗产保护具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:26:17 GMT</pubDate>
</item>
<item>
<title>ArtifactsBench：自动化评估视觉代码生成的新基准</title>
<link>https://arxiv.org/abs/2507.04952</link>
<guid>https://arxiv.org/abs/2507.04952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtifactsBench实现对视觉代码生成的多模态自动评估。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ArtifactsBench，这是一个用于自动化、多模态评估视觉代码生成的新基准和范式。该框架通过程序化渲染生成的视觉制品并捕获其动态行为，结合源代码由多模态大语言模型进行评估。研究构建了1,825个多样化任务，并评估了30多个领先的大语言模型。实验表明，ArtifactsBench在排名一致性上与WebDev Arena高度一致，且与人类专家有超过90%的匹配度，证明其可大规模可靠评估用户感知质量。分析显示通用模型通常优于领域特定模型。作者开源了ArtifactsBench及其相关资源，以推动以用户为中心的生成模型发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 08:53:00 GMT</pubDate>
</item>
<item>
<title>VLM2Vec-V2：跨多种视觉形式的统一嵌入框架</title>
<link>https://arxiv.org/abs/2507.04590</link>
<guid>https://arxiv.org/abs/2507.04590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLM2Vec-V2支持多模态输入并提升嵌入性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出VLM2Vec-V2，一种支持文本、图像、视频和视觉文档输入的统一嵌入框架。为弥补现有模型对非自然图像支持不足的问题，研究团队构建了MMEB-V2基准，涵盖视觉文档检索、视频检索等五种新任务。实验表明，VLM2Vec-V2在视频和文档检索任务中表现优异，并优于现有基线模型。该研究为多模态嵌入学习提供了有效策略，推动了更广泛的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 20:51:57 GMT</pubDate>
</item>
<item>
<title>大型语言模型在预测任务中的表现评估</title>
<link>https://arxiv.org/abs/2507.04562</link>
<guid>https://arxiv.org/abs/2507.04562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型预测能力仍不及超级预测者。</p><br /><br /><p><strong>摘要：</strong> 本文评估了当前最先进的大型语言模型在464个Metaculus预测问题上的表现，发现尽管这些模型的Brier得分看似超过普通人群，但与超级预测者相比仍有显著差距。这表明虽然大型语言模型在多种任务中表现出色，但在未来事件预测方面仍需进一步提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:26:59 GMT</pubDate>
</item>
<item>
<title>DreamVLA：融合世界知识预测的视觉-语言-动作框架</title>
<link>https://arxiv.org/abs/2507.04447</link>
<guid>https://arxiv.org/abs/2507.04447</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamVLA提升机器人操作的泛化与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DreamVLA，一种新型的视觉-语言-动作框架，通过整合全面的世界知识预测，实现逆动力学建模，构建感知-预测-行动循环。该框架引入动态区域引导的世界知识预测，并结合空间和语义线索，提供紧凑而全面的动作规划表示。为减少训练过程中动态、空间和语义信息的干扰，采用块状结构注意力机制，防止信息泄露。此外，使用基于扩散的Transformer模型对未来的动作分布进行建模。实验表明，DreamVLA在真实机器人任务中取得76.7%的成功率，在CALVIN ABC-D基准测试中平均长度为4.44。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04447" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 12:14:29 GMT</pubDate>
</item>
<item>
<title>MOD-X：面向异构智能体的模块化开放去中心化交换框架</title>
<link>https://arxiv.org/abs/2507.04376</link>
<guid>https://arxiv.org/abs/2507.04376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOD-X 提供一种新型智能体互操作架构，提升异构系统集成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出 MOD-X（Modular Open Decentralized eXchange）框架，旨在解决当前人工智能系统中智能体之间通信协议不足的问题。MOD-X 采用分层架构，包含通用消息总线、状态管理、翻译能力和基于区块链的安全机制，支持不同架构、厂商和知识表示的异构智能体之间的集成。其核心创新包括发布-订阅通信模型、语义能力发现和动态工作流编排，为实现真正去中心化、可扩展的智能体生态系统提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 08:46:57 GMT</pubDate>
</item>
<item>
<title>SeqTex：基于视频预训练模型的端到端3D纹理生成框架</title>
<link>https://arxiv.org/abs/2507.04285</link>
<guid>https://arxiv.org/abs/2507.04285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeqTex直接生成UV纹理图，提升3D一致性与真实感。</p><br /><br /><p><strong>摘要：</strong> 本文提出SeqTex，一个端到端的3D纹理生成框架，利用预训练视频模型的视觉知识直接生成完整的UV纹理图。不同于以往方法依赖多视角图像和后处理，SeqTex将任务转化为序列生成问题，学习多视角渲染与UV纹理的联合分布，从而有效迁移图像空间先验至UV域。通过解耦多视角与UV分支、几何引导注意力以及自适应令牌分辨率等创新设计，SeqTex在无需后处理的情况下生成高质量UV纹理，实验表明其在图像和文本条件下的3D纹理生成任务中均达到最优性能，具有更强的3D一致性、纹理-几何对齐性和现实泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 03:58:36 GMT</pubDate>
</item>
<item>
<title>PresentAgent：将长文档转化为同步视听演示的多模态代理</title>
<link>https://arxiv.org/abs/2507.04036</link>
<guid>https://arxiv.org/abs/2507.04036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PresentAgent生成同步视听演示视频，提升信息传达效果。</p><br /><br /><p><strong>摘要：</strong> PresentAgent是一种多模态代理，能够将长文档转化为同步的视觉和语音演示视频。与现有方法仅生成静态幻灯片或文本摘要不同，PresentAgent通过模块化流程生成高质量的视觉帧和上下文相关的语音叙述，并实现精确的音画同步。为评估其输出质量，研究者引入了PresentEval框架，该框架基于视觉-语言模型对视频内容进行综合评分。实验表明，PresentAgent在多项指标上接近人类水平，展示了可控多模态代理在动态展示静态文本材料方面的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 09:24:15 GMT</pubDate>
</item>
<item>
<title>基于GUI的统一数据集合成框架Easy Dataset提升领域语言模型性能</title>
<link>https://arxiv.org/abs/2507.04009</link>
<guid>https://arxiv.org/abs/2507.04009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Easy Dataset通过GUI合成高质量数据，提升LLM领域适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Easy Dataset，一个通过图形用户界面从非结构化文档中合成微调数据的统一框架。用户可配置文本提取模型和分块策略，将原始文档转化为连贯文本片段，并利用基于角色的提示方法生成多样化的问答对。过程中的人机交互界面有助于审查和优化中间结果，确保数据质量。实验表明，使用该框架生成的数据集能显著提升语言模型在金融问答任务中的表现，同时保持其通用知识。项目已在GitHub上开源，获得超过9000星标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 07:38:59 GMT</pubDate>
</item>
<item>
<title>StreamDiT：实现实时视频生成的流式模型</title>
<link>https://arxiv.org/abs/2507.03745</link>
<guid>https://arxiv.org/abs/2507.03745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamDiT实现实时视频生成，支持交互与流媒体应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamDiT，一种基于流式处理的视频生成模型，解决了现有模型仅能生成短片段的问题。通过引入流动缓冲区和混合训练策略，提升了内容一致性和视觉质量。模型采用自适应层归一化DiT结构，并结合多步蒸馏方法，显著降低了计算量，使模型在单块GPU上达到16 FPS的实时性能，支持512p分辨率的视频流生成。实验表明，该模型在定量指标和人类评估中均表现优异，适用于实时视频生成和交互场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 14:00:01 GMT</pubDate>
</item>
<item>
<title>MemOS：面向持续学习与个性化建模的内存操作系统</title>
<link>https://arxiv.org/abs/2507.03724</link>
<guid>https://arxiv.org/abs/2507.03724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MemOS提升LLM长期推理与知识一致性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在长期上下文推理、持续个性化和知识一致性方面的挑战，指出现有模型依赖静态参数和短期上下文状态，难以有效管理用户偏好和更新知识。尽管RAG引入了外部知识，但缺乏生命周期管理和持久化整合。为此，作者提出MemOS，一个将内存作为可管理资源的操作系统，统一表示、调度和演化文本、激活和参数级记忆，通过MemCube实现灵活的知识迁移与融合，提升计算效率并支持持续学习和个性化建模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:21:46 GMT</pubDate>
</item>
<item>
<title>基于Transformer的软件漏洞严重性预测模型VLAI</title>
<link>https://arxiv.org/abs/2507.03607</link>
<guid>https://arxiv.org/abs/2507.03607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLAI模型准确预测软件漏洞严重性，提升漏洞分类效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLAI，一个基于Transformer架构的模型，能够直接从文本描述中预测软件漏洞的严重性等级。该模型基于RoBERTa进行微调，使用超过60万条真实漏洞数据进行训练，预测准确率超过82%，可显著提升漏洞优先级排序的效率，减少对人工CVSS评分的依赖。VLAI模型和相关数据集已开源，并集成到Vulnerability-Lookup服务中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 10:28:14 GMT</pubDate>
</item>
<item>
<title>BMMR：多语言、多模态、跨学科推理数据集的构建与应用</title>
<link>https://arxiv.org/abs/2507.03483</link>
<guid>https://arxiv.org/abs/2507.03483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BMMR是一个跨学科的多模态推理数据集，用于评估和训练大型多模态模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BMMR，这是一个大规模的双语、多模态、跨学科推理数据集，旨在支持大型多模态模型（LMMs）的研究与评估。该数据集包含11万道大学水平的题目，覆盖300个联合国教科文组织定义的学科，涵盖多种题型，并从纸质和数字媒体中获取。数据经过人工与自动化框架筛选，每个实例都配有高质量的推理路径。数据集分为BMMR-Eval（20,458个高质量实例）和BMMR-Train（88,991个实例），用于评估和研究。此外，作者还提出了BMMR-Verifier，用于精确评估推理过程。实验表明，即使是最先进的模型在BMMR-Eval上仍有提升空间，且不同学科表现差异明显。开源模型仍落后于专有模型，但通过微调可缩小差距。研究揭示了当前LMMs在跨学科推理中的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 07:20:09 GMT</pubDate>
</item>
<item>
<title>DiaFORGE提升企业API调用成功率的对话框架研究</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiaFORGE提高企业API调用成功率27%-49%</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DiaFORGE，一种以消除歧义为核心的三阶段对话框架，旨在解决大型语言模型在调用企业API时因工具相似或参数不明确而失败的问题。该框架通过生成多轮对话、进行监督微调以及动态评估模型表现，显著提升了工具调用的成功率。在动态基准测试中，使用DiaFORGE训练的模型比GPT-4o和Claude-3.5-Sonnet分别高出27个百分点和49个百分点。同时，作者发布了包含5000个企业级API规范及验证过的对话数据集，为构建可靠的工具调用代理提供了实用蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 02:49:02 GMT</pubDate>
</item>
<item>
<title>RefineX：大规模预训练数据的精准优化框架</title>
<link>https://arxiv.org/abs/2507.03253</link>
<guid>https://arxiv.org/abs/2507.03253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefineX提升预训练数据质量，增强模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RefineX，一种用于大规模预训练数据精准优化的框架。该方法通过程序化编辑任务实现细粒度数据修正，同时保持文本的多样性和自然性。RefineX将高质量的专家指导结果提炼为最小化的删除指令，构建高效可靠的精炼模型，可在不同规模模型上显著提升下游任务表现。实验表明，RefineX在多个基准测试中优于原始数据、过滤数据或替代方法，具有高效率和精确度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 22:19:58 GMT</pubDate>
</item>
<item>
<title>OmniDraft：一种支持多模型协同的高效推测解码框架</title>
<link>https://arxiv.org/abs/2507.02659</link>
<guid>https://arxiv.org/abs/2507.02659</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniDraft实现单一模型适配多种目标模型并提升推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniDraft，一种统一框架，使单一draft模型能够与任何目标模型协作，并动态适应用户数据。通过在线n-gram缓存和混合蒸馏微调解决词汇不匹配问题，并利用自适应推测技术提升解码速度。该框架适用于设备端大语言模型应用，展示了在数学推理、编程和文本生成任务中的有效性，可使Llama-68M模型与多个目标模型如Vicuna-7B、Qwen2-7B和Llama3-8B进行推测解码，并带来1.5-2倍的速度提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02659" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:20:41 GMT</pubDate>
</item>
<item>
<title>RoboBrain 2.0：面向物理环境的多模态AI模型</title>
<link>https://arxiv.org/abs/2507.02029</link>
<guid>https://arxiv.org/abs/2507.02029</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboBrain 2.0是先进的多模态AI模型，支持空间与时间推理。</p><br /><br /><p><strong>摘要：</strong> RoboBrain 2.0是最新一代的具身视觉-语言基础模型，旨在统一感知、推理和规划以完成复杂的物理环境任务。该模型包含两个版本：轻量级7B模型和全尺寸32B模型，采用异构架构，结合视觉编码器和语言模型。尽管体积较小，RoboBrain 2.0在多种具身推理任务中表现出色。32B版本在空间和时间基准测试中均取得领先结果，超越了之前的开源和专有模型。它支持关键的现实世界具身AI能力，如空间理解（例如功能预测、空间指代、轨迹预测）和时间决策（例如闭环交互、多智能体长期规划、场景图更新）。本文详细介绍了模型架构、数据构建、多阶段训练策略、基础设施及实际应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02029" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:05:33 GMT</pubDate>
</item>
<item>
<title>对比MLM与CLM在文本表示学习中的效果与优化策略</title>
<link>https://arxiv.org/abs/2507.00994</link>
<guid>https://arxiv.org/abs/2507.00994</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较MLM与CLM在文本表示上的表现及效率。</p><br /><br /><p><strong>摘要：</strong> 本文通过大规模实验对比了基于MLM和CLM的预训练方法在文本表示任务中的表现。虽然MLM通常表现更优，但CLM在数据效率和微调稳定性方面更具优势。研究还提出了一种结合CLM和MLM的双阶段训练策略，在计算资源有限的情况下取得了最佳效果。此外，利用现有的CLM预训练模型可以降低训练高性能编码器模型的计算成本。所有实验结果和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00994" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:45:48 GMT</pubDate>
</item>
<item>
<title>扩散模型在动态系统模拟中的潜在应用</title>
<link>https://arxiv.org/abs/2507.02608</link>
<guid>https://arxiv.org/abs/2507.02608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型在潜空间中模拟动态系统表现良好。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在动态系统模拟中使用扩散模型的可行性，特别是在潜空间中进行生成而非像素空间。研究发现，潜空间模拟在压缩率高达1000倍的情况下仍保持较高的准确性。此外，基于扩散的模拟器比非生成方法更精确，并能通过预测的多样性来补偿不确定性。文章还讨论了训练潜空间模拟器的关键设计选择，包括架构和优化器等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 09:32:50 GMT</pubDate>
</item>
<item>
<title>LitBench：首个用于创意写作评估的标准化基准与数据集</title>
<link>https://arxiv.org/abs/2507.00769</link>
<guid>https://arxiv.org/abs/2507.00769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LitBench提升创意写作模型评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LitBench，这是首个用于创意写作验证的标准化基准和配对数据集。该数据集包含从Reddit中提取的2,480个去偏见、人工标注的故事比较测试集，以及43,827对人工偏好标签的训练语料。通过LitBench，研究者评估了零样本语言模型作为评判者的性能，并训练了Bradley-Terry和生成式奖励模型。实验表明，训练后的模型在准确率上优于所有现成模型，并通过在线人类研究进一步验证了其与人类偏好的一致性。研究结果为创意写作系统的自动化评估和优化提供了可靠资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 10:10:36 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在计算机视觉任务中的性能评估</title>
<link>https://arxiv.org/abs/2507.01955</link>
<guid>https://arxiv.org/abs/2507.01955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态模型在视觉任务中表现一般，但具备一定通用性。</p><br /><br /><p><strong>摘要：</strong> 本文评估了GPT-4o、o4-mini、Gemini 1.5 Pro等多模态基础模型在图像分类、目标检测等标准计算机视觉任务中的表现。由于这些模型主要训练用于文本输出，难以直接处理视觉任务，研究通过提示链技术将其转化为文本可处理的任务。结果显示，尽管这些模型在专业视觉模型面前仍有差距，但在语义任务上表现较好，且对提示变化不敏感的模型表现更优。GPT-4o在非推理模型中表现最佳，而具有图像生成能力的模型则存在幻觉和空间错位问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>EKA-EVAL：面向多语言大模型的统一评估框架</title>
<link>https://arxiv.org/abs/2507.01853</link>
<guid>https://arxiv.org/abs/2507.01853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EKA-EVAL提供多语言大模型评估，覆盖35个基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了EKA-EVAL，这是一个面向多语言大模型的统一评估框架，整合了超过35个基准测试，包括10个针对印度语言的数据集。该框架支持推理、数学、工具使用、长文本理解和阅读理解等任务，并具备分布式推理、量化和多GPU支持等功能。与现有工具相比，EKA-EVAL具有更广泛的基准覆盖，是首个端到端、可扩展的评估套件，旨在降低多语言模型评估的门槛。该框架已开源，并计划扩展至100多个基准，构建一个强大的多语言评估生态系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:07:54 GMT</pubDate>
</item>
<item>
<title>基于文本描述的多器官医学分割模型CRISP-SAM2</title>
<link>https://arxiv.org/abs/2506.23121</link>
<guid>https://arxiv.org/abs/2506.23121</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRISP-SAM2提升多器官医学图像分割精度与细节表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CRISP-SAM2的新模型，用于改进多器官医学图像分割。该模型通过跨模态交互和语义提示机制，增强对视觉信息的理解，并减少对几何提示的依赖。此外，引入了相似性排序自更新策略和掩码优化过程，以提高局部细节的准确性。实验结果表明，CRISP-SAM2在多个公开数据集上优于现有模型，显示出其在解决当前分割模型不足方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23121" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 03:05:27 GMT</pubDate>
</item>
<item>
<title>视觉语言分割中的幻觉评估基准研究</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HalluSegBench评估视觉语言分割中的幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言分割模型中常见的幻觉问题，提出了首个专门用于评估视觉接地幻觉的基准HalluSegBench。该基准包含1340对反事实实例对，覆盖281个独特物体类别，并引入了新的度量标准来量化在视觉一致场景修改下的幻觉敏感性。实验表明，基于视觉的幻觉比基于标签的幻觉更为普遍，强调了反事实推理在诊断接地准确性中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>InnerControl：提升扩散模型空间控制精度的新方法</title>
<link>https://arxiv.org/abs/2507.02321</link>
<guid>https://arxiv.org/abs/2507.02321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InnerControl通过全阶段空间一致性训练提升图像生成精度。</p><br /><br /><p><strong>摘要：</strong> 尽管文本到图像的扩散模型取得了显著进展，但实现精确的空间控制仍具挑战。ControlNet及其改进版本ControlNet++通过引入条件模块和循环一致性损失来提升控制效果，但忽略了中间生成阶段。为此，InnerControl提出一种训练策略，在所有扩散步骤中强制空间一致性。该方法通过轻量级卷积探针从UNet中间特征中重建输入控制信号（如边缘、深度），即使在高噪声潜变量下也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测与目标条件之间的差异，InnerControl提升了控制精度和生成质量，并结合ControlNet++等现有技术实现了多种条件方法的最先进性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 01:25:53 GMT</pubDate>
</item>
<item>
<title>大型语言模型的自我纠正盲点研究</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs存在自我纠正盲点，影响其可靠性。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）已取得显著进展，但它们仍会犯错并陷入无效推理路径。自我纠正能力对可信的LLM至关重要，尤其是自回归模型。然而，LLMs在用户输入中能识别错误，却无法修正自身输出中的相同错误，这种现象被称为‘自我纠正盲点’。为系统研究这一问题，研究人员提出了Self-Correction Bench框架，通过在三个复杂度级别上注入错误进行测试。实验显示，14个模型平均有64.5%的盲点率。研究发现，这一限制与训练数据组成有关：人类演示数据多为无错误响应，而非纠错序列，而强化学习训练的模型则通过反馈学习纠错。令人惊讶的是，仅添加“Wait”即可减少89.3%的盲点，表明该能力存在但需激活。该研究揭示了当前LLMs的关键缺陷，并为提升其可靠性和可信度提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:41:30 GMT</pubDate>
</item>
<item>
<title>利用LLM辅助科学论文局限性识别的基准研究</title>
<link>https://arxiv.org/abs/2507.02694</link>
<guid>https://arxiv.org/abs/2507.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在识别科学论文局限性方面具有潜力，LimitGen基准提升其反馈能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在科学论文同行评审中的应用，特别是在识别论文局限性方面的潜力。作者提出了一个全面的局限性分类体系，并基于此构建了LimitGen基准，包含合成数据集和真实人类撰写的局限性数据集。通过引入文献检索增强LLM的识别能力，该方法提升了LLM生成具体且有建设性反馈的能力，为早期研究反馈和补充人工评审提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:04:38 GMT</pubDate>
</item>
<item>
<title>基于能量模型的系统2思维推理方法研究</title>
<link>https://arxiv.org/abs/2507.02092</link>
<guid>https://arxiv.org/abs/2507.02092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EBTs通过无监督学习实现系统2推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的基于能量模型的推理方法（Energy-Based Transformers, EBTs），旨在通过无监督学习实现类似人类系统2思维的推理能力。与传统方法相比，EBTs无需额外监督或训练，而是通过显式验证输入与候选预测的兼容性，并将预测问题重新建模为以验证器为基准的优化问题。实验表明，EBTs在文本和视觉等多模态任务中均表现出色，训练速度更快，推理性能优于Transformer++和Diffusion Transformers。此外，EBTs在多数下游任务中表现更优，表明其具有更好的泛化能力，是一种有前景的模型扩展范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>IntFold：一种可控制的生物分子结构预测基础模型</title>
<link>https://arxiv.org/abs/2507.02025</link>
<guid>https://arxiv.org/abs/2507.02025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IntFold在结构预测方面达到AlphaFold3水平，支持多种定制化任务。</p><br /><br /><p><strong>摘要：</strong> IntFold是一种可控制的基础模型，用于通用和专业生物分子结构预测。其预测精度与AlphaFold3相当，并采用优化的注意力核。除了标准结构预测外，IntFold还可通过适配器预测别构状态、约束结构和结合亲和力。此外，研究团队引入了一种新的置信度头，以更精细地评估对接质量，特别是在抗体-抗原复合物等复杂目标上表现突出。文章还分享了训练该计算密集型模型的经验与见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:09:47 GMT</pubDate>
</item>
<item>
<title>AsyncFlow：一种高效的异步流式强化学习框架</title>
<link>https://arxiv.org/abs/2507.01663</link>
<guid>https://arxiv.org/abs/2507.01663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AsyncFlow提升大语言模型后训练效率，支持灵活扩展。</p><br /><br /><p><strong>摘要：</strong> 本文提出AsyncFlow，一种用于大语言模型后训练的异步流式强化学习框架。针对传统框架在可扩展性、数据流复杂性和资源闲置方面的不足，AsyncFlow引入了分布式数据存储与传输模块，实现统一的数据管理和细粒度调度。其架构支持自动流水线重叠和动态负载均衡，并通过生产者-消费者异步工作流减少计算空闲。此外，AsyncFlow与底层训练和推理引擎解耦，提供模块化用户界面。实验表明，该框架在吞吐量上平均提升1.59倍，为下一代强化学习系统设计提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:45:34 GMT</pubDate>
</item>
<item>
<title>ZeCO：实现线性注意力模型高效序列并行的新方法</title>
<link>https://arxiv.org/abs/2507.01004</link>
<guid>https://arxiv.org/abs/2507.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeCO提升长序列训练效率，消除通信开销。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZeCO（Zero Communication Overhead）序列并行方法，用于优化线性注意力机制在大型语言模型中的应用。传统序列并行方法因通信开销大而成为瓶颈，而ZeCO通过引入All-Scan通信原语，有效减少通信负担，实现近线性扩展。实验表明，在256块GPU上处理8M长度的序列时，ZeCO相比现有最优方法提升了60%的速度。理论与实证均证明了ZeCO的高效性与可行性，为训练超长序列的下一代LLM提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:54:53 GMT</pubDate>
</item>
<item>
<title>多模态推理中‘思考与图像’范式的演进与展望</title>
<link>https://arxiv.org/abs/2506.23918</link>
<guid>https://arxiv.org/abs/2506.23918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨AI从‘思考图像’到‘与图像共思’的范式转变。</p><br /><br /><p><strong>摘要：</strong> 本文综述了多模态推理领域中‘思考与图像’范式的最新进展。传统方法将视觉视为静态输入，导致感知数据与符号推理之间的语义鸿沟。而人类认知常利用视觉作为动态思维工具，这一理念正推动AI从仅‘思考图像’向‘与图像共思’演进。文章提出该范式发展的三个阶段：外部工具探索、程序化操作和内在想象，并总结了核心方法、评估基准、应用前景及未来挑战，为构建更强大且符合人类认知的多模态AI提供路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 10:48:35 GMT</pubDate>
</item>
<item>
<title>动态选择与合并专家模型提升跨领域信息抽取性能</title>
<link>https://arxiv.org/abs/2506.22813</link>
<guid>https://arxiv.org/abs/2506.22813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SaM框架通过动态选择和合并专家模型提升信息抽取效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SaM框架，通过在推理阶段动态选择和合并预训练的专家模型来优化信息抽取任务。该方法基于目标领域的相似性和采样实例的表现选择合适的专家模型，并将其合并以生成针对特定领域的优化模型。这种方法无需额外训练即可提高跨领域泛化能力，并具备良好的可扩展性。实验结果表明，该框架在多个基准测试中平均优于统一模型10%。文章还探讨了框架的潜在改进方向和实际应用经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 04:28:52 GMT</pubDate>
</item>
<item>
<title>基于语言理解的3D场景重建框架LangScene-X</title>
<link>https://arxiv.org/abs/2507.02813</link>
<guid>https://arxiv.org/abs/2507.02813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangScene-X通过多模态生成实现从稀疏视角的高质量3D场景重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LangScene-X的新生成框架，用于从2D图像中恢复一致的3D结构并实现开放词汇场景理解。该方法通过TriMap视频扩散模型生成RGB、法线和语义分割图，并结合语言量化压缩器（LQC）实现跨场景的语言嵌入编码，从而在仅有稀疏视角的情况下构建可泛化的3D语言嵌入场景。最后，通过将语言信息对齐到3D场景表面，支持开放式语言查询。实验表明，LangScene-X在质量和泛化能力上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:21:23 GMT</pubDate>
</item>
<item>
<title>2-单纯形Transformer提升token效率的研究</title>
<link>https://arxiv.org/abs/2507.02754</link>
<guid>https://arxiv.org/abs/2507.02754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2-单纯形Transformer在token效率上优于传统Transformer。</p><br /><br /><p><strong>摘要：</strong> 本文研究了2-单纯形Transformer架构，该架构通过高效的Triton内核实现三线性函数的注意力机制，相较于标准的点积注意力，其在数学、编程、推理和逻辑任务中表现出更高的token效率。实验表明，在固定token预算下，2-单纯形Transformer模型性能更优，并且改变了知识和推理任务的缩放定律指数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:16:34 GMT</pubDate>
</item>
<item>
<title>基于自生成目标条件MDPs的自动定理证明方法</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新框架提升LLM在复杂推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为自生成目标条件马尔可夫决策过程（sG-MDP）的新框架，旨在解决大型语言模型在自动定理证明中面临的挑战。该框架通过让智能体根据不断变化的证明状态生成并追求子目标，使问题更易于搜索。研究者将这种方法应用于Bourbaki（7B）系统，该系统可以集成多个7B参数的LLM进行子目标生成和策略合成。在PutnamBench基准测试中，Bourbaki（7B）成功解决了26个问题，取得了当前同类模型的最佳成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:41:38 GMT</pubDate>
</item>
<item>
<title>HiRA：一种分层框架提升复杂信息检索与推理效率</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HiRA通过分层规划与执行提升复杂搜索任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出HiRA，一种分层信息检索框架，将战略规划与专业执行分离。该方法将复杂搜索任务分解为子任务，并分配给具备外部工具和推理能力的领域特定代理。通过结构化集成机制协调结果，避免执行细节干扰高层推理，从而提升系统效率与答案质量。在四个跨模态深度搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:18:08 GMT</pubDate>
</item>
<item>
<title>提升大模型信息检索能力的WebSailor方法</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebSailor提升大模型在复杂信息任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出WebSailor，一种后训练方法，旨在增强大模型在复杂信息搜索任务中的能力。通过生成高不确定性任务、RFT冷启动和DUPO算法，WebSailor显著提升了开源模型的表现，使其接近专有系统水平。该方法解决了传统模型在处理海量信息时的不确定性问题，推动了大语言模型在信息检索领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 08:59:07 GMT</pubDate>
</item>
<item>
<title>提升奖励模型性能：基于高质量数据集的Skywork-Reward-V2研究</title>
<link>https://arxiv.org/abs/2507.01352</link>
<guid>https://arxiv.org/abs/2507.01352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork-Reward-V2通过高质量数据提升奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文指出当前开放奖励模型在评估基准上表现不佳，主要由于偏好数据集存在局限。为解决这一问题，作者提出了包含4000万对偏好的SynPref-40M数据集，并设计了人机协同的数据筛选流程。基于该数据集，训练出8个参数规模从0.6B到8B的Skywork-Reward-V2模型，在多个基准测试中取得最优成绩。实验表明，模型性能提升不仅得益于数据量，更得益于高质量的数据筛选。该研究展示了人机协作在数据质量提升中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:40:29 GMT</pubDate>
</item>
<item>
<title>多尺度多模态大语言模型在自动放射学报告生成中的应用</title>
<link>https://arxiv.org/abs/2507.00316</link>
<guid>https://arxiv.org/abs/2507.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出mu^2LLM模型提升CT影像报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自动化放射学报告生成（RRG）技术，旨在通过临床影像数据生成详细文本报告以提高诊断准确性和效率。文章指出该技术面临两大挑战：从影像数据中提取相关信息的复杂性以及模型生成报告与专家报告之间差异的客观评估困难。为解决这些问题，作者提出了mu^2LLM模型，结合多尺度和多模态特征，并通过直接偏好优化提升报告质量。实验结果表明，该方法在多个大型CT图像-报告数据集上优于现有方法，显示出在有限数据下进行RRG任务的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 19:14:49 GMT</pubDate>
</item>
<item>
<title>MARVIS：一种无需训练的多模态推理方法</title>
<link>https://arxiv.org/abs/2507.01544</link>
<guid>https://arxiv.org/abs/2507.01544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARVIS使小型视觉语言模型能高效预测多种数据模态。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARVIS的无需训练的方法，使小型视觉语言模型能够以高精度预测任何数据模态。该方法通过将潜在嵌入空间转换为可视化表示，并利用视觉语言模型的空间和细粒度推理能力进行解释和利用。MARVIS在视觉、音频、生物和表格领域表现出色，使用单一3B参数模型即可达到优于Gemini 16%的平均性能，并接近专用方法，同时不涉及个人身份信息且无需领域特定训练。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 05:56:24 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的实时交互式头部生成方法</title>
<link>https://arxiv.org/abs/2507.00472</link>
<guid>https://arxiv.org/abs/2507.00472</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIG框架实现更真实的实时交互头部生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了面对面交流中的交互式头部生成问题，针对传统方法在实时性和交互真实性上的不足，提出了一种基于自回归（AR）的逐帧生成框架ARIG。该框架通过非向量量化AR过程进行运动预测，并利用扩散过程表示运动分布以提高连续空间预测精度。同时，引入交互行为理解（IBU）和详细对话状态理解（CSU），通过双模态信号分析和上下文建模提升交互真实感。实验验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00472" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:38:14 GMT</pubDate>
</item>
<item>
<title>Locality-aware Parallel Decoding加速自回归图像生成</title>
<link>https://arxiv.org/abs/2507.01957</link>
<guid>https://arxiv.org/abs/2507.01957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPD技术提升自回归图像生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Locality-aware Parallel Decoding (LPD)方法，用于加速自回归图像生成。传统方法依赖逐块预测，导致高延迟。现有研究虽尝试通过多块预测实现并行化，但效果有限。LPD引入两种关键技术：灵活并行自回归建模和局部感知生成顺序，分别实现任意生成顺序和减少组内依赖，从而显著降低生成步数并提升效率。实验表明，在保持生成质量的前提下，LPD将ImageNet图像生成步数从256降至20（256×256分辨率）和1024降至48（512×512分辨率），延迟降低至少3.4倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>FreeMorph：无需微调的高效图像形态转换方法</title>
<link>https://arxiv.org/abs/2507.01953</link>
<guid>https://arxiv.org/abs/2507.01953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeMorph实现无需训练的高质量图像形态转换，速度快且效果优。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeMorph，一种无需微调的图像形态转换方法，能够处理不同语义或布局的输入。与现有依赖微调扩散模型的方法不同，FreeMorph通过引入引导感知球面插值和步进变化趋势，解决了非线性去噪过程中的质量下降问题，实现了更高效、更一致的图像过渡效果。实验表明，FreeMorph在速度和性能上均优于现有方法，提升达10倍至50倍，并建立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:20 GMT</pubDate>
</item>
<item>
<title>视觉-语言-动作模型中的动作标记化研究综述</title>
<link>https://arxiv.org/abs/2507.01925</link>
<guid>https://arxiv.org/abs/2507.01925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述分析VLA模型中动作标记的分类与作用。</p><br /><br /><p><strong>摘要：</strong> 本文综述了视觉-语言-动作（VLA）模型的研究进展，指出当前模型在处理视觉和语言输入后生成一系列动作标记，最终输出可执行动作。文章强调，VLA模型的核心设计差异在于动作标记的构建方式，包括语言描述、代码、可操作性、轨迹等多种形式。然而，对动作标记的理解仍不充分，阻碍了VLA的发展。本文通过分析不同动作标记的优缺点，提出未来研究方向，旨在推动VLA向通用智能迈进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:34:52 GMT</pubDate>
</item>
<item>
<title>STR-Match：一种无需训练的视频编辑算法</title>
<link>https://arxiv.org/abs/2506.22868</link>
<guid>https://arxiv.org/abs/2506.22868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STR-Match通过潜空间优化实现高质量视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为STR-Match的视频编辑算法，该算法无需训练即可生成视觉吸引人且时空一致的视频。其核心在于利用新颖的STR分数，结合2D空间注意力和1D时间模块，捕捉相邻帧之间的时空像素相关性，避免了计算成本高昂的3D注意力机制。通过集成潜空间优化框架和潜空间掩码，STR-Match在保持源视频关键视觉属性的同时，实现了在显著领域变换下的强性能表现。大量实验表明，该方法在视觉质量和时空一致性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 08:36:19 GMT</pubDate>
</item>
<item>
<title>Kwai Keye-VL：面向短视频理解的多模态大模型</title>
<link>https://arxiv.org/abs/2507.01949</link>
<guid>https://arxiv.org/abs/2507.01949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kwai Keye-VL提升短视频理解能力，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Kwai Keye-VL，一款80亿参数的多模态基础模型，旨在提升对短视频的理解能力。该模型基于超过6000亿token的高质量数据集和创新的训练方法，包括四阶段预训练和两阶段后训练。第二阶段引入五种模式的数据混合，增强模型的推理能力。通过强化学习和对齐优化，模型在多个视频基准测试中表现优异，并发布了针对真实短视频场景的KC-MMBench基准，进一步验证了其优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>基于动态全局-局部范式的长动画上色方法研究</title>
<link>https://arxiv.org/abs/2507.01945</link>
<guid>https://arxiv.org/abs/2507.01945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LongAnimation框架，提升动画上色的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 动画上色是动画产业的重要环节，传统方法成本高且效率低。现有研究多集中于短期上色，依赖局部特征融合，难以保持长期颜色一致性。本文提出LongAnimation框架，采用动态全局-局部范式，结合SketchDiT、DGLM和Color Consistency Reward模块，有效提升长视频段的颜色一致性。实验表明，该方法在短时（14帧）和长时（平均500帧）动画任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>DepthAnything-AC：一种适应多种环境的单目深度估计模型</title>
<link>https://arxiv.org/abs/2507.01634</link>
<guid>https://arxiv.org/abs/2507.01634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DepthAnything-AC在复杂环境下表现出色，具备零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DepthAnything-AC的单目深度估计模型，能够在多种复杂环境中保持高精度。与以往模型相比，该模型在光照变化、恶劣天气和传感器畸变等条件下表现更优。研究者引入了无监督一致性正则化微调方法，仅需少量未标记数据即可提升性能，并通过空间距离约束增强模型对局部关系的学习能力。实验结果表明，DepthAnything-AC在多个基准测试中展现出强大的零样本能力，包括真实世界恶劣天气数据集、合成噪声数据集和通用数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:05:57 GMT</pubDate>
</item>
<item>
<title>JAM-Flow：统一生成面部动作与语音的框架</title>
<link>https://arxiv.org/abs/2506.23552</link>
<guid>https://arxiv.org/abs/2506.23552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM-Flow实现面部动作与语音的联合生成，提升多模态合成效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出JAM-Flow，一个统一的框架，用于同时生成和条件化面部动作与语音。该方法结合流匹配和多模态扩散Transformer（MM-DiT）架构，包含专门的Motion-DiT和Audio-DiT模块，并通过选择性联合注意力层进行耦合。JAM-Flow采用类似修复的目标进行训练，支持文本、参考音频和参考动作等多种输入，适用于同步生成说话头像、音频驱动动画等任务，显著提升了多模态生成模型的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:51:40 GMT</pubDate>
</item>
<item>
<title>Mixture of Reasoning：提升大语言模型推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.00606</link>
<guid>https://arxiv.org/abs/2507.00606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture of Reasoning 提升大语言模型推理性能，无需外部提示。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Mixture of Reasoning (MoR) 框架，通过将多种推理策略嵌入大语言模型中，实现自主、任务自适应的推理能力。该框架包含两个阶段：首先生成推理链模板，然后通过监督微调提升性能。实验表明，MoR 在多个基准测试中显著提升表现，且无需依赖任务特定的提示，为跨任务的鲁棒推理提供了通用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 05:39:04 GMT</pubDate>
</item>
<item>
<title>基于频率修正的神经材质表示方法FreNBRDF</title>
<link>https://arxiv.org/abs/2507.00476</link>
<guid>https://arxiv.org/abs/2507.00476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreNBRDF提升材质建模精度与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为FreNBRDF的频率修正神经材质表示方法，旨在提高材质建模的准确性和可解释性。传统方法依赖于表格化的BRDF数据，而现代方法则采用隐式神经表示，但其在频域中的行为仍不明确。该研究通过引入球谐函数，将频域考虑整合到神经BRDF建模中，并设计了一种新的频率修正损失函数。该框架提升了材质重建和编辑的保真度、适应性和效率。实验表明，FreNBRDF在材质外观重建和编辑方面优于现有方法，支持更结构化和可解释的下游任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:48:50 GMT</pubDate>
</item>
<item>
<title>MOVi-MC-AC：首个多摄像头视图的非模态分割与内容数据集</title>
<link>https://arxiv.org/abs/2507.00339</link>
<guid>https://arxiv.org/abs/2507.00339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOVi-MC-AC是首个支持多摄像头视图的非模态分割数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MOVi-MC-AC，这是目前最大的非模态分割和首个非模态内容数据集。该数据集通过多摄像头模拟复杂家庭场景中的物体遮挡情况，提供了约580万实例的标签，并首次引入了真实非模态内容的地面实况。相比以往依赖慢速拼接生成伪标签的方法，MOVi-MC-AC在合成视频中实现了跨帧和多视角的一致性对象标识，为计算机视觉中的目标检测、跟踪和分割研究提供了新的挑战和机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 20:36:56 GMT</pubDate>
</item>
<item>
<title>MusiXQA：推动多模态大模型理解乐谱的基准数据集</title>
<link>https://arxiv.org/abs/2506.23009</link>
<guid>https://arxiv.org/abs/2506.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MusiXQA是首个用于评估音乐乐谱理解的多模态大模型数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MusiXQA，这是首个针对多模态大语言模型（MLLMs）在音乐乐谱理解方面的综合数据集。该数据集通过MusiXTeX生成高质量的合成乐谱，并包含结构化的注释，涵盖音符音高与时值、和弦、谱号、调号与拍号等信息，支持多种视觉问答任务。实验表明当前最先进的MLLMs在该领域存在显著不足，为此作者开发了Phi-3-MusiX模型，在该数据集上取得了优于GPT类方法的性能。该数据集和模型为未来MLLMs在音乐乐谱理解领域的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 16:46:47 GMT</pubDate>
</item>
<item>
<title>基于置信度的3D高斯点云压缩方法</title>
<link>https://arxiv.org/abs/2506.22973</link>
<guid>https://arxiv.org/abs/2506.22973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于置信度的3D高斯点云压缩方法，提升渲染效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于可学习置信度分数的损失函数，用于压缩3D高斯点云。该方法通过优化重建感知损失来调整每个点的置信度，从而修剪低置信度点，同时保持视觉质量。该方法与架构无关，适用于任何高斯点云渲染变体，并引入平均置信度作为场景质量的新评估指标。实验表明，该方法在压缩率和保真度之间取得了良好平衡。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 14:11:30 GMT</pubDate>
</item>
<item>
<title>FreeLong++：提升长视频生成质量的训练无关框架</title>
<link>https://arxiv.org/abs/2507.00162</link>
<guid>https://arxiv.org/abs/2507.00162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeLong++通过多频段融合提升长视频生成的时序一致性和视觉质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeLong和FreeLong++，用于解决长视频生成中时间一致性下降和视觉质量退化的问题。FreeLong通过在去噪过程中融合全局低频特征与局部高频特征，平衡长视频的频率分布。FreeLong++进一步扩展为多分支架构，支持多尺度时间窗口的多频段融合，从而增强语义连贯性和细节动态。该方法无需额外训练即可集成到现有模型中，显著提升长视频生成效果，并支持多提示生成、平滑场景切换及可控视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 14:11:21 GMT</pubDate>
</item>
<item>
<title>IR3D-Bench：通过主动创造评估视觉语言模型的场景理解能力</title>
<link>https://arxiv.org/abs/2506.23329</link>
<guid>https://arxiv.org/abs/2506.23329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IR3D-Bench挑战VLMs通过主动创造理解场景，推动其生成能力发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了IR3D-Bench，一个用于评估视觉语言模型（VLMs）场景理解能力的新基准。该基准要求模型通过主动使用编程和渲染工具来重建输入图像的3D结构，从而实现“通过创造理解”的方法。不同于传统基于被动识别的评估方式，IR3D-Bench强调工具使用和生成能力。研究提供了多种指标来评估几何准确性、空间关系、外观属性和整体合理性。实验表明，当前VLMs在视觉精度方面仍存在局限。IR3D-Bench包含数据和评估协议，旨在促进工具使用型VLMs的发展，以实现真正的场景理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 13:02:57 GMT</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking：多模态推理模型的进展与性能评估</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.1V-Thinking在多模态任务中表现卓越，超越多个基准。</p><br /><br /><p><strong>摘要：</strong> GLM-4.1V-Thinking是一款面向通用多模态推理的视觉语言模型。通过大规模预训练和基于课程采样的强化学习（RLCS），该模型在STEM问题解决、视频理解、内容识别、编程、定位、GUI代理和长文档理解等多个任务中展现出全面的能力提升。开源版本GLM-4.1V-9B-Thinking在28个公共基准测试中表现出色，超越Qwen2.5-VL-7B，并在多数任务上优于更大的Qwen2.5-VL-72B模型。此外，它在长文档理解和STEM推理等挑战性任务上也表现出与GPT-4o相当或更优的性能。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>SciArena：科学文献任务的开放协作评估平台</title>
<link>https://arxiv.org/abs/2507.01001</link>
<guid>https://arxiv.org/abs/2507.01001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciArena通过社区投票评估基础模型在科学文献任务中的表现。</p><br /><br /><p><strong>摘要：</strong> SciArena是一个开放协作平台，用于评估基础模型在科学文献任务中的表现。与传统基准不同，它采用社区投票方式，让研究人员直接参与模型比较。该平台已支持23个开源和专有模型，并收集了超过13,000份来自不同领域研究者的投票。分析显示，提交的问题多样且符合实际需求，研究人员在评估中表现出高度一致性和准确性。此外，团队还发布了SciArena-Eval，一个基于偏好数据的元评估基准，用于衡量模型判断答案质量的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>迈向通用人工智能：跨学科视角下的认知与架构分析</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AGI发展需整合记忆、推理与多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能向通用人工智能（AGI）发展的关键问题，指出当前模型如GPT-4.5、DeepSeek等虽具备多模态能力和部分推理能力，但仍受限于基于token的预测机制和缺乏具身代理。文章从人工智能、认知神经科学、心理学等多个领域出发，分析了AGI的架构与认知基础，强调模块化推理、持久记忆和多智能体协作的重要性。同时，Agentic RAG框架、信息压缩与测试时适应等策略被视为实现灵活智能的关键路径。此外，视觉语言模型被重新定义为具身理解与协作任务的接口。作者认为，真正的智能源于记忆与推理的融合，而非单纯依赖规模。最后，文章讨论了AGI发展中的科学、技术和伦理挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 12:52:25 GMT</pubDate>
</item>
<item>
<title>AI生成内容激增与新型水印技术PECCAVI的提出</title>
<link>https://arxiv.org/abs/2506.22960</link>
<guid>https://arxiv.org/abs/2506.22960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI生成内容将达90%，PECCAVI应对水印攻击。</p><br /><br /><p><strong>摘要：</strong> 欧洲联盟执法机构报告预测，到2026年，高达90%的在线内容可能由生成式AI创建，引发政策制定者的担忧。加州AB 3211法案要求对AI生成内容进行水印标记，但现有技术易被篡改。本文提出PECCAVI，一种针对视觉重述攻击的安全且无失真的图像水印技术。该技术在图像的核心语义区域嵌入水印，并利用多通道频域水印和噪声烧制技术增强抗逆向工程能力。PECCAVI具备模型无关性，相关资源将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 13:34:08 GMT</pubDate>
</item>
<item>
<title>提升语言模型训练效果的数据效能研究</title>
<link>https://arxiv.org/abs/2506.21545</link>
<guid>https://arxiv.org/abs/2506.21545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据效能优化可显著提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据效能（Data Efficacy）在语言模型训练中的作用，提出了一种名为DELT的通用范式，包含数据评分、数据选择和数据排序三个组件。其中，Learnability-Quality Scoring（LQS）通过梯度一致性评估样本的可学习性和质量，Folding Ordering（FO）则解决模型遗忘和数据分布偏差问题。实验表明，DELT能有效提升模型性能，且与数据效率结合使用效果更佳，证明数据效能是语言模型训练的重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>扩散语言模型在代码生成中的应用与优化</title>
<link>https://arxiv.org/abs/2506.20639</link>
<guid>https://arxiv.org/abs/2506.20639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型在代码生成中的解码行为与强化学习训练方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散语言模型（dLLMs）在代码生成中的潜力，分析了其与自回归模型的不同之处，如生成的因果性控制和采样温度对生成顺序的影响。作者训练了一个7B参数的dLLM模型DiffuCoder，并提出了一种新的采样方案coupled-GRPO，以提升强化学习训练效率。实验结果显示，该方法在代码生成基准测试中提升了4.4%，并减少了对自回归因果性的依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:35:47 GMT</pubDate>
</item>
<item>
<title>基于时空能量衰减的径向注意力机制提升视频生成效率</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出径向注意力机制，提升视频生成效率与长度。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视频扩散模型中的时空能量衰减现象，并提出了径向注意力机制。该机制通过稀疏注意力计算，将能量衰减转化为计算密度的指数衰减，从而显著降低计算复杂度。实验表明，该方法在多个视频生成模型上保持高质量的同时，提升了生成速度并减少了训练成本。相比传统密集注意力，其效率更高，且支持更长视频的生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>数学推理模型的泛化能力与训练方法研究</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数学模型在特定任务上表现优异，但泛化能力有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型在数学推理任务上的进展，并探讨其是否具备更广泛的问题解决能力。通过对20多个模型的评估发现，多数数学表现优秀的模型在其他领域如科学问答、编程和指令遵循上表现不佳。通过对比强化学习和监督微调两种训练方法，发现强化学习模型在跨领域任务中表现更好，而监督微调模型容易失去通用能力。分析表明，监督微调会导致表示和输出分布的变化，影响模型的泛化能力，提示需要重新考虑当前的训练策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 01:23:05 GMT</pubDate>
</item>
<item>
<title>MoCa：提升多模态嵌入模型性能的两阶段框架</title>
<link>https://arxiv.org/abs/2506.23115</link>
<guid>https://arxiv.org/abs/2506.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoCa通过两阶段方法提升多模态嵌入模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoCa，一种将预训练视觉语言模型转化为高效双向多模态嵌入模型的两阶段框架。第一阶段为模态感知持续预训练，引入联合重建目标以增强双向上下文感知推理；第二阶段为异构对比微调，利用多样化的多模态数据提升泛化能力和对齐效果。该方法解决了现有模型在注意力机制、数据依赖性和训练目标多样性方面的不足，并在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 02:41:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2506.21277</link>
<guid>https://arxiv.org/abs/2506.21277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出方法增强多模态模型理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要。本文指出当前多模态推理模型存在全局上下文理解不足和依赖捷径的问题，并提出通过引入上下文奖励、格式奖励和逻辑奖励来提升模型的推理能力。同时，作者构建了IntentBench基准测试，用于评估模型在理解复杂人类意图和情感方面的表现。实验结果表明，该方法在多个多模态基准上优于现有开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 10:01:03 GMT</pubDate>
</item>
<item>
<title>MEMFOF：高效多帧光流估计方法</title>
<link>https://arxiv.org/abs/2506.23151</link>
<guid>https://arxiv.org/abs/2506.23151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEMFOF在高分辨率下实现高效光流估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出MEMFOF，一种内存高效的多帧光流估计方法，在保持高精度的同时显著降低GPU内存消耗。该方法在1080p输入下仅需2.09GB显存运行，训练时为28.5GB，无需裁剪或降采样即可在原生分辨率下训练。通过优化RAFT架构设计，结合减少的卷积体积和高分辨率训练策略，MEMFOF在多个基准测试中取得最佳性能，包括Spring、Sintel和KITTI-2015数据集，展现出出色的准确性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 05:01:42 GMT</pubDate>
</item>
<item>
<title>基于多路径扩散的可调金属镜头摄影方法</title>
<link>https://arxiv.org/abs/2506.22753</link>
<guid>https://arxiv.org/abs/2506.22753</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型金属镜头成像方法，提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于退化建模的多路径扩散方法，用于可调金属镜头摄影。该方法利用预训练模型中的自然图像先验，而非依赖大规模数据集，通过正向、中性与负向提示路径平衡高频细节生成、结构保真度和金属镜头特有退化的抑制。同时引入伪数据增强和可调解码器，实现保真度与感知质量的可控权衡。此外，设计了空间变化的退化感知注意力模块，以适应复杂的光学和传感器引起的退化。最终构建了毫米级MetaCamera进行实际验证，实验结果表明该方法优于现有技术，实现了高保真和清晰的图像重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22753" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 00:48:37 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型代理在研究扩展任务中的能力</title>
<link>https://arxiv.org/abs/2506.22598</link>
<guid>https://arxiv.org/abs/2506.22598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM代理在自主实现研究扩展任务上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了基于大型语言模型（LLMs）的智能体在自主执行软件工程和科研任务方面的潜力。研究引入了RExBench，一个包含12个真实研究实验任务的基准测试，用于评估智能体在扩展已有研究成果方面的能力。每个任务都基于现有论文和代码库，并附有专家指导说明。尽管RExBench具备抗数据污染能力和自动化评估系统，但测试结果显示，使用不同框架开发的九个LLM代理在没有大量人工提示的情况下，仅能完成不到40%的任务。这表明当前智能体仍需大量人工干预才能处理现实中的研究扩展任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 15:41:41 GMT</pubDate>
</item>
<item>
<title>Tower+：在翻译与多语言通用能力之间实现性能平衡的模型</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tower+在翻译和多语言通用任务中表现出色，兼顾专业与泛用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tower+，一个在机器翻译和多语言通用文本处理方面均表现优异的模型系列。通过引入一种新的训练方法，包括持续预训练、监督微调、偏好优化和基于可验证奖励的强化学习，Tower+实现了翻译专业化与多语言通用能力之间的帕累托最优。研究团队在多个规模（2B、9B、72B）上构建了模型，并在代码生成、数学问题解决和指令遵循等任务中提升了性能。实验结果显示，Tower+在高资源语言翻译中表现卓越，并在多语言Arena Hard评估和IF-MT基准测试中取得领先。该研究证明，在优化特定业务领域（如翻译和本地化）的同时，仍可达到前沿模型的通用能力水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:30:06 GMT</pubDate>
</item>
<item>
<title>基于自对弈的强化学习框架SPIRAL提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIRAL通过自对弈训练提升语言模型推理能力，无需人工监督。</p><br /><br /><p><strong>摘要：</strong> 本文提出SPIRAL，一种基于自对弈的强化学习框架，使语言模型通过与不断进化的自我版本进行零和博弈来学习，从而无需依赖人工标注的数据或领域特定奖励工程。该框架生成持续进阶的问题课程，推动模型适应更强对手。研究中引入了角色条件优势估计（RAE）以稳定多智能体训练，并在Kuhn扑克等游戏中验证了其有效性。实验表明，SPIRAL可显著提升数学和通用推理能力，且多游戏训练进一步增强模型表现。结果表明，零和博弈是发展可迁移推理能力的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>基于运动不变图融合的ToF深度去噪网络</title>
<link>https://arxiv.org/abs/2506.23542</link>
<guid>https://arxiv.org/abs/2506.23542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型ToF深度去噪方法，提升时间稳定性和空间锐度。</p><br /><br /><p><strong>摘要：</strong> 本文针对ToF传感器获取的深度图像易受噪声影响的问题，提出了一种基于运动不变图融合的深度去噪网络。该方法利用跨帧几何注意力机制，结合图像平滑先验和ToF噪声分布的数据保真项，构建最大后验问题进行去噪。通过将解法展开为自适应学习的迭代滤波器，实现了高精度且可解释的去噪效果。实验结果表明，该方法在合成数据集和真实数据集上均表现出色，具有良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:29:24 GMT</pubDate>
</item>
<item>
<title>多语言模型工具调用能力提升方法研究</title>
<link>https://arxiv.org/abs/2506.23394</link>
<guid>https://arxiv.org/abs/2506.23394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过持续训练提升多语言模型的工具调用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，使多语言语言模型能够在非英语语言中实现可靠的工具调用。以保加利亚语为例，研究者对BgGPT模型系列进行了持续训练，使用了一个包含10,035个函数调用示例的双语数据集。该方法引入了TUCAN模型，在保加利亚语基准测试中实现了28.75%的函数调用准确率提升，并保持了核心语言理解能力。TUCAN模型还表现出更规范、可解析的输出格式，优于基础模型。研究提供了模型、评估框架和数据集，支持其他语言的复现与扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 16:47:27 GMT</pubDate>
</item>
<item>
<title>UrbanLLaVA：面向城市研究的多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.23219</link>
<guid>https://arxiv.org/abs/2506.23219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UrbanLLaVA提升城市多模态任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UrbanLLaVA，一个专为城市研究设计的多模态大语言模型。该模型能够同时处理多种城市数据，并在多个城市任务中表现出色。研究团队构建了一个涵盖单模态与跨模态数据的城市指令数据集，并设计了分阶段训练框架以提升空间推理和领域知识学习的兼容性。此外，还扩展了城市研究基准以评估模型性能。实验结果表明，UrbanLLaVA在多个城市任务中优于现有开源和商业模型，展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 09:04:27 GMT</pubDate>
</item>
<item>
<title>RoboScape：一种融合物理知识的统一世界模型</title>
<link>https://arxiv.org/abs/2506.23135</link>
<guid>https://arxiv.org/abs/2506.23135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboScape通过物理信息联合训练提升机器人视频生成的物理合理性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboScape，一个融合物理知识的统一世界模型，能够同时学习RGB视频生成和物理知识。该模型引入了时间深度预测和关键点动力学学习两个关键任务，以增强视频生成的3D几何一致性和复杂运动建模能力。实验表明，RoboScape在多种机器人场景中生成的视频具有更高的视觉质量和物理合理性，并在机器人策略训练和评估中展现出实用价值。研究为构建高效的物理感知世界模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 04:19:45 GMT</pubDate>
</item>
<item>
<title>Ovis-U1：一款融合多模态理解与生成能力的大型统一模型</title>
<link>https://arxiv.org/abs/2506.23044</link>
<guid>https://arxiv.org/abs/2506.23044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis-U1是一款30亿参数的多模态统一模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Ovis-U1，一款拥有30亿参数的统一模型，具备多模态理解、文本到图像生成和图像编辑能力。该模型基于Ovis系列，采用基于扩散的视觉解码器和双向标记精修器，在多个基准测试中表现优异，如OpenCompass多模态学术基准得分69.6，文本到图像生成在DPG-Bench和GenEval分别获得83.72和0.89分，图像编辑在ImgEdit-Bench和GEdit-Bench-EN分别获得4.00和6.42分。相比以往模型，Ovis-U1通过统一训练方法提升了性能，展示了多任务整合的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 20:40:17 GMT</pubDate>
</item>
<item>
<title>提出MARBLE基准测试，推动多模态推理模型发展</title>
<link>https://arxiv.org/abs/2506.22992</link>
<guid>https://arxiv.org/abs/2506.22992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARBLE挑战多模态推理模型的逐步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MARBLE，一个用于评估多模态语言模型（MLLMs）在复杂多模态问题和环境中进行逐步推理能力的基准测试。MARBLE包含两个高难度任务M-Portal和M-Cube，要求模型在空间、视觉和物理约束下制定并理解多步骤计划。实验结果显示，当前12个先进模型在M-Portal任务中表现接近随机，在M-Cube任务中准确率为0%。只有在简化子任务中部分模型优于随机基线，表明复杂推理仍是MLLMs的挑战。此外，研究发现感知能力仍是瓶颈，模型在从视觉输入中提取信息时存在困难。作者希望通过MARBLE推动下一代多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 15:44:32 GMT</pubDate>
</item>
<item>
<title>基于监听器增强的GRPO框架提升视觉语言模型对齐效果</title>
<link>https://arxiv.org/abs/2506.22832</link>
<guid>https://arxiv.org/abs/2506.22832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监听器增强的GRPO方法提升视觉语言模型的对齐性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于监听器增强的Group Relative Policy Optimization (GRPO)框架，用于提升视觉语言模型与人类偏好的对齐效果。传统奖励模型在泛化性方面存在不足，而监督微调容易导致记忆现象。尽管RL方法如GRPO有所改进，但当模型推理过程与独立的冻结视觉-语言模型（“监听器”）不一致时，推理准确性会显著下降。为此，本文引入监听器重新评估推理链，提供密集且校准的置信度评分，从而优化强化学习奖励信号。该方法不仅提高了准确率，还在大规模人类偏好数据集上提升了分布外性能，并减少了推理矛盾。实验结果表明，监听器奖励机制是一种高效、可扩展的对齐路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 05:53:17 GMT</pubDate>
</item>
<item>
<title>基于语言模型头的无训练优化方法提升推测解码性能</title>
<link>https://arxiv.org/abs/2506.22694</link>
<guid>https://arxiv.org/abs/2506.22694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VocabTrim技术优化推测解码速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需训练的优化方法，用于提升基于草稿模型的推测解码（SpD）性能。该方法在草稿生成过程中引入语言模型头（LM head），通过限制草稿模型的词汇量，仅保留目标模型中高频采样的词，从而降低内存瓶颈下的推理延迟。尽管接受率略有下降，但显著提升了生成速度，尤其在边缘设备上效果明显。实验表明，该方法在Llama-3.2-3B-Instruct模型上可提升内存瓶颈速度约16%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 20:26:40 GMT</pubDate>
</item>
<item>
<title>ThinkSound：基于思维链推理的视频到音频生成框架</title>
<link>https://arxiv.org/abs/2506.21448</link>
<guid>https://arxiv.org/abs/2506.21448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkSound通过思维链推理实现高质量视频到音频生成。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ThinkSound，一个利用思维链（CoT）推理的视频到音频生成框架。该框架将过程分为三个阶段：基础音效生成、交互式对象优化和自然语言指导的定向编辑。每个阶段都由多模态大语言模型生成上下文相关的CoT推理，以引导统一的音频基础模型。同时，研究者还发布了AudioCoT数据集，用于连接视觉内容、文本描述和声音合成。实验表明，ThinkSound在多个音频指标和CoT指标上均达到领先水平，并在Movie Gen Audio基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:32:06 GMT</pubDate>
</item>
</channel>
</rss>