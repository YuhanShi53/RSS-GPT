<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>长文本语言模型的上下文处理能力评估</title>
<link>https://arxiv.org/abs/2411.05000</link>
<guid>https://arxiv.org/abs/2411.05000</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究评估了17种长文本语言模型在信息检索中的表现与局限性。</p><br><br><p><strong>摘要：</strong> 随着长文本语言模型（LLMs）上下文限制的增加，其应用范围也在不断扩大。然而，尽管近年来长上下文模型快速发展，我们对这些模型如何有效利用上下文的理解却滞后。为此，我们进行了检索实验，评估17种领先的LLM的能力，特别是它们在上下文窗口中跟踪信息线程的能力。研究发现，许多模型在同时跟踪多个线程时表现出色，几乎没有显著性能损失。但对一些模型而言，实际有效上下文限制显著短于支持的上下文长度，且随着上下文窗口的增长，准确性逐渐下降。此外，我们强调不同分词器的标记计数不应直接比较，因为它们对应的书写字符数量可能有显著不同。我们已发布相关代码和长上下文实验数据。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2411.05000 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 09:15:42 GMT</pubDate>
<pubDate>Fri, 08 Nov 2024 09:15:42 GMT</pubDate>
</item>
<item>
<title>VideoGLaMM：实现视频与文本的细粒度对齐</title>
<link>https://arxiv.org/abs/2411.04923</link>
<guid>https://arxiv.org/abs/2411.04923</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出VideoGLaMM模型，实现视频和文本的精确对齐。</p><br><br><p><strong>摘要：</strong> 本研究介绍了VideoGLaMM，一个专为实现视频和文本之间细粒度像素级对齐而设计的大型多模态模型（LMM）。该模型通过用户提供的文本输入，利用大的语言模型、双重视觉编码器以及时空解码器的无缝连接，强调了视频内容的空间和时间细节。为提升细粒度对齐，我们构建了一个多模态数据集，包括38000个视频问答三元组及相应的物体和掩码，进一步推动了语音与视觉的紧密对齐。我们在基础对话生成、视觉对齐和视频分割等三项具有挑战性的任务上评估了VideoGLaMM，结果表明该模型在所有任务中均优于现有的方法，展现出更强的性能和实用性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2411.04923 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 09:06:00 GMT</pubDate>
<pubDate>Fri, 08 Nov 2024 09:06:00 GMT</pubDate>
</item>

<item>
<title>BitNet a4.8: 高效的4-bit激活的大型语言模型</title>
<link>https://arxiv.org/abs/2411.04965</link>
<guid>https://arxiv.org/abs/2411.04965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BitNet a4.8在降低推理成本的同时保持大型语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BitNet a4.8，这是一款为1-bit大型语言模型（LLMs）设计的提升版，能够实现4-bit激活，以降低推理成本。BitNet a4.8采用混合量化和稀疏化策略，有效减小由异常信道引入的量化误差。具体而言，本文在注意力和前馈网络层的输入中使用4-bit激活，而稀疏化中间状态并采用8-bit量化。大量实验表明，BitNet a4.8在训练成本相当的情况下，其性能可与BitNet b1.58相媲美，且在推理时表现更快，同时启用4-bit（INT4/FP4）内核。此外，BitNet a4.8仅激活55%的参数，并支持3-bit KV缓存，进一步提升了大规模LLM的部署和推理效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 04:39:54 GMT</pubDate>
</item>
<item>
<title>SVDQuant：提升扩散模型的4位量化效率</title>
<link>https://arxiv.org/abs/2411.05007</link>
<guid>https://arxiv.org/abs/2411.05007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SVDQuant以提高扩散模型的4位量化效率与图像质量。</p><br /><br /><p><strong>摘要：</strong> 在扩散模型生成高质量图像的背景下，随着模型规模的扩大，它们在内存和延迟方面面临挑战。为了解决这些问题，本文提出了SVDQuant，一种新的4位量化范式，能够有效处理权重和激活的极端灵敏性。该方法通过将激活的异常值转移到权重中，并利用奇异值分解（SVD）在低秩分支中吸纳这些异常值，从而简化量化过程。此外，我们设计了一个推理引擎Nunchaku，将低秩分支的内核与低位分支融合，以减少冗余内存访问，提升速度。实验结果显示，在多个模型上，SVDQuant能够有效提升图像质量，同时在FLUX.1模型上减少内存使用3.5倍，实现3倍的速度提升，为PC上的交互应用铺平了道路。我们的量化库和推理引擎已开源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.05007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 04:09:26 GMT</pubDate>
</item>
<item>
<title>GazeGen: 基于视线控制的视觉内容生成系统</title>
<link>https://arxiv.org/abs/2411.04335</link>
<guid>https://arxiv.org/abs/2411.04335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GazeGen通过用户视线生成和操控视觉内容，提供创新的人机交互体验。</p><br /><br /><p><strong>摘要：</strong> GazeGen是一种用户交互系统，通过用户的视线生成视觉内容（图像和视频）。该系统允许用户直观地操控视觉内容，能够针对感兴趣区域进行图像的添加、删除、重新定位及表面材质的更改，甚至将静态图像转换为视频。GazeGen的核心是DFT Gaze（Distilled and Fine-Tuned Gaze）代理，它是一种超轻量级模型，仅有281K参数，提供针对个体用户眼睛的实时精确视线预测。通过知识蒸馏和个性化适配技术，DFT Gaze以小体积模型实现高效的视线预测，支持多种以视线为驱动的任务，并经过在AEA和OpenEDS2020基准上的验证，显示出低的角度误差和延迟。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 03:37:45 GMT</pubDate>
</item>
<item>
<title>多面化心智技能对话数据集及其在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2411.04496</link>
<guid>https://arxiv.org/abs/2411.04496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多面化心智技能对话数据集，推动大语言模型的对话能力提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出一个名为“多面化心智技能”的对话数据集，旨在提升基于大语言模型（LLM）的对话代理在复杂社交对话中的应对能力。数据集涵盖约10万条对话，提供了多轮、多面向的对话技能，并考虑到多样的社交背景。为此，我们引入了一系列新的LLM，命名为Thanos，参数规模分别为1B、3B和8B。通过大量实验，Thanos模型有效展示了心智技能的规划过程，且在推断多面向技能方面表现出较强的泛化能力。此外，结果表明Thanos显著提高了生成对话的质量，并在人工评估中促进了积极的社交行为。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 03:31:51 GMT</pubDate>
</item>
<item>
<title>多语言环境中代码混合对信息提取的挑战与解决方案</title>
<link>https://arxiv.org/abs/2411.04752</link>
<guid>https://arxiv.org/abs/2411.04752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了代码混合对信息提取的影响及其解决方案。</p><br /><br /><p><strong>摘要：</strong> 代码混合是指在单一句子中融入多种语言的词汇和语法元素，在多语言社会中尤为普遍。本文重点关注在孟加拉语和英语混合的代码混合对提取相关信息的挑战，尤其是在印度的社交媒体上。研究提出了一种新方法，旨在自动识别代码混合对话中最相关的答案。通过在Facebook上实验一个包含查询和文档的数据集，研究验证了该方法在提取复杂代码混合数字对话中的相关信息的有效性。这项研究为多语言和非正式文本环境中的自然语言处理领域做出了贡献，使用GPT-3.5 Turbo以及相关文档的顺序性构建了一个数学模型，以帮助检测与查询相对应的相关文档。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 02:53:47 GMT</pubDate>
</item>
<item>
<title>DimensionX框架：从单张图像生成真实感3D和4D场景</title>
<link>https://arxiv.org/abs/2411.04928</link>
<guid>https://arxiv.org/abs/2411.04928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DimensionX框架通过视频扩散技术实现3D和4D场景生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DimensionX，一个从单张图像生成真实感3D和4D场景的框架，利用视频扩散技术来实现这一目标。我们提出ST-Director，通过从维度变化数据中学习维度感知的LoRAs，解耦了视频扩散中的空间和时间因素，从而提升了空间结构和时间动态的细致控制能力。这一可控的视频扩散方法使得我们能够从序列帧中重建3D和4D表示。此外，我们还引入了一种轨迹感知机制以增强3D生成效果，以及一种保持身份的去噪策略来改进4D生成。通过在多种真实和合成数据集上的广泛实验，DimensionX在可控视频生成及3D和4D场景生成方面表现出色，超越了现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 02:46:11 GMT</pubDate>
</item>
<item>
<title>OpenCoder：独特的高质量代码语言模型及其开放研究价值</title>
<link>https://arxiv.org/abs/2411.04905</link>
<guid>https://arxiv.org/abs/2411.04905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCoder是一款开源高性能代码语言模型，有助于科学研究的可重复性。</p><br /><br /><p><strong>摘要：</strong> OpenCoder是一款高质量的代码语言模型（LLM），在代码生成、推理任务及代理系统等领域表现出色。尽管开源代码LLM的性能逐渐接近专有模型，但具有严谨科学研究特性的高质量开源代码LLM仍较为稀缺。为填补这一空白，OpenCoder不仅性能与领先模型相当，还提供了详尽的训练数据、处理流程、实验结果及训练协议，旨在为研究社区提供“开放食谱”。文章中强调了创建顶尖代码LLM的关键因素，包括数据清理优化规则、代码相关文本语料的回忆以及高质量的合成数据，通过这样全面的开放性，OpenCoder旨在扩大高质量代码LLM的获取渠道，促进科研进步与可重复性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 02:34:51 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Transformers：高效的多模态大语言模型架构</title>
<link>https://arxiv.org/abs/2411.04996</link>
<guid>https://arxiv.org/abs/2411.04996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新型多模态Transformer架构，显著降低训练计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mixture-of-Transformers (MoT)，一种针对多模态系统的稀疏Transformer架构，旨在提高预训练效率并降低计算资源消耗。MoT通过将模型的非嵌入参数按模态解耦，支持模态特定处理，实现全局自注意力机制。实验显示，在Chameleon 7B设置中，MoT以仅55.8%的计算量实现与稠密基线相当的性能；扩展到语音处理时，MoT运用37.2%的计算量达到与稠密基线相同的语音性能。在不同目标训练的Transfusion设置中，MoT也表现优异，760M MoT模型在图像生成关键指标上超越了1.4B稠密基线。此外，系统分析表明，MoT能够在相对较短的时间内实现与稠密基线相似的图像和文本质量，展示了其在实际应用中的潜力与优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 02:09:26 GMT</pubDate>
</item>
<item>
<title>SG-I2V：一种自我引导的可控图像到视频生成框架</title>
<link>https://arxiv.org/abs/2411.04989</link>
<guid>https://arxiv.org/abs/2411.04989</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SG-I2V提供了一种无须微调的图像到视频生成控制方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SG-I2V，一个可控的图像到视频生成框架，具有自我引导特性，允许通过仅依赖预训练的图像到视频扩散模型提供零-shot控制，省去了微调和外部知识的需求。针对调整生成视频中特定元素（如物体运动或相机运动）的复杂性，SG-I2V展示了比无监督基线优越的性能，同时在视觉质量和运动保真度上与监督模型具有竞争力。这种方法为图像到视频生成领域提供了新的思路，简化了生成过程，提升了效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04989" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 02:01:51 GMT</pubDate>
</item>
<item>
<title>DynaMem：动态空间语义记忆驱动的开放词汇移动操控</title>
<link>https://arxiv.org/abs/2411.04999</link>
<guid>https://arxiv.org/abs/2411.04999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DynaMem 提升了开放词汇移动操控在动态环境中的适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 DynaMem，一种新的开放世界移动操控方法，利用动态空间语义记忆来表示机器人的环境。DynaMem 构建了一个 3D 数据结构，维护点云的动态记忆，并通过多模态大规模语言模型或先进的视觉-语言模型生成的开放词汇特征来处理开放词汇物体定位查询。在 DynaMem 的支持下，机器人能够探索新环境、搜索记忆中未找到的物体，并在物体移动、出现或消失时持续更新记忆。我们在三个位于真实环境的 Stretch SE3 机器人以及九个离线场景中进行了大量实验，针对非静态物体实现了 70%的平均拾取和放置成功率，比现有静态系统的表现提高了 2 倍以上。我们的代码及实验和部署视频已开源，访问项目网站获取更多信息。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 01:59:21 GMT</pubDate>
</item>
<item>
<title>推出TIP-I2V：首个大规模图像到视频生成用户提示数据集</title>
<link>https://arxiv.org/abs/2411.04709</link>
<guid>https://arxiv.org/abs/2411.04709</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了首个大规模图像到视频生成提示数据集TIP-I2V。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TIP-I2V，这是第一个针对图像到视频生成的大规模数据集，包含超过170万条独特的用户提供的文本和图像提示。尽管目前图像到视频模型受到广泛关注，但现有的模型在用户提供的文本和图像提示研究方面缺乏专用数据集。我们详细描述了创建这个数据集的耗时和昂贵过程，并与两个流行的数据集（VidProM和DiffusionDB）进行了比较，以突出基本和语义信息的差异。TIP-I2V的开发为图像到视频的研究提供了新的可能，研究人员可以利用这些用户提示来分析用户偏好和评估模型性能，从而更好地推动相关技术的发展。该项目的详细信息已公开，促进了后续相关研究的开展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04709" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 00:50:12 GMT</pubDate>
</item>
<item>
<title>ReCapture：基于用户视频生成新视角视频的方法</title>
<link>https://arxiv.org/abs/2411.05003</link>
<guid>https://arxiv.org/abs/2411.05003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReCapture方法可从单一用户视频生成新角度视频。</p><br /><br /><p><strong>摘要：</strong> ReCapture是一种创新的方法，可以从用户提供的单一视频生成具有新摄像机轨迹的视频。该方法首先使用多视图扩散模型或者基于深度的点云渲染生成带有新摄像机轨迹的噪声锚点视频，然后通过我们提出的掩码视频微调技术，将锚点视频重生成清晰且时间上连续的新视角视频。值得注意的是，该方法不仅能够从不同的角度重生原始视频中的场景动作，还可以合理地幻想在参考视频中不可观察的场景部分，极大地拓展了视频生成的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.05003" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 08 Nov 2024 00:45:10 GMT</pubDate>
</item>
<item>
<title>探索o1-preview模型在医学挑战基准上的表现</title>
<link>https://arxiv.org/abs/2411.03590</link>
<guid>https://arxiv.org/abs/2411.03590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究o1-preview模型在医疗基准中的表现及其与Medprompt的对比。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了运行时引导策略，如Medprompt，在提升大型语言模型（LLMs）在复杂任务中的表现方面的重要性。特别说明了o1-preview模型如何设置新的范式，通过在生成最终响应前进行运行时推理，显示出在多种医学挑战基准上的优越性能。相比于需要提示技术的GPT-4系列，o1-preview在多个医疗基准的测试中表现更为出色。研究还发现，少样本提示可能会削弱o1的表现，提示工程策略在推理本土模型中的有效性得到质疑。通过成本和准确性分析，本文总结了不同运行时策略下的表现，指出GPT-4o在特定上下文中依然具有重要价值，而o1-preview则在高成本下提供了最佳性能。最后，我们强调了在现有医学基准上，o1-preview模型已达到近饱和状态，并指出需要新的具有挑战性的基准来推动研究进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.03590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 15:19:53 GMT</pubDate>
</item>
<item>
<title>Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models</title>
<link>https://arxiv.org/abs/2411.03884</link>
<guid>https://arxiv.org/abs/2411.03884</guid>
<content:encoded><![CDATA[
Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 12:13:57 GMT</pubDate>
</item>
<item>
<title>自我一致性偏好优化：提升模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2411.04109</link>
<guid>https://arxiv.org/abs/2411.04109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出自我一致性偏好优化，显著提升模型在推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 自我对齐技术正在快速发展，尤其在没有人工标注的情况下提升模型性能。然而，现有的方法常因难以准确奖励而在复杂推理任务中失效。为此，本文扩展了自我一致性的概念，提出自我一致性偏好优化（ScPO），用于在不监督的新问题上训练模型，使一致答案优于不一致答案。研究表明，ScPO在推理任务（如GSM8K和MATH）上的表现显著优于传统奖励模型训练，缩小了与人工有监督训练的差距。此外，将ScPO与标准有监督学习结合，能更进一步提升结果。在ZebraLogic上，ScPO使Llama-3 8B的表现优于Llama-3 70B、Gemma-2 27B和Claude-3 Haiku。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.04109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 11:51:40 GMT</pubDate>
</item>
<item>
<title>Agent K v1.0：全自动数据科学代理的创新与应用</title>
<link>https://arxiv.org/abs/2411.03562</link>
<guid>https://arxiv.org/abs/2411.03562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent K v1.0是一个全自动的数据科学代理，表现出色，达到了接近人类大师的水平。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Agent K v1.0，这是一款设计用于自动化、优化和通用化各类数据科学任务的全自动代理。Agent K v1.0通过经验学习，完全管理数据科学生命周期，利用灵活的结构化推理框架动态处理嵌套结构中的记忆，从而应对复杂的推理任务。它通过选择性存储和检索关键信息来优化长短期记忆，基于环境奖励引导未来的决策。评估结果表明，Agent K v1.0在Kaggle竞赛中的成功率达92.5%，并在与人类竞争者的比较中，排名前38%，展示了其达到Kaggle大师水平的能力，获得了6个金奖、3个银奖和7个铜奖。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.03562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 09:01:15 GMT</pubDate>
</item>
<item>
<title>MM-Detect: 一种针对多模态大语言模型的数据污染检测框架</title>
<link>https://arxiv.org/abs/2411.03823</link>
<guid>https://arxiv.org/abs/2411.03823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MM-Detect框架，解决多模态大语言模型的数据污染问题。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型（MLLMs）的快速发展，它们在各种多模态基准上的表现优越。但在训练过程中数据污染问题仍然存在，给性能评估和比较带来了挑战。虽然现有多种方法可以检测大语言模型（LLMs）中的数据污染，但由于MLLMs具有多种模态和多次训练阶段，这些方法效果有限。本文介绍了一种针对MLLMs的多模态数据污染检测框架MM-Detect，实验结果表明MM-Detect对不同程度的污染敏感，并且能够显著提升在多模态基准上因训练集泄露而导致的性能。此外，本文还探讨了数据污染可能源于大语言模型的预训练阶段及MLLMs的微调阶段，为污染引入的不同阶段提供了新的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.03823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 07 Nov 2024 08:55:50 GMT</pubDate>
</item>
<item>
<title>基于可变长度表示的2D图像编码解码方法</title>
<link>https://arxiv.org/abs/2411.02393</link>
<guid>https://arxiv.org/abs/2411.02393</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于图像信息量学习可变长度标记表示的新方法。</p><br /><br /><p><strong>摘要：</strong> 当前的视觉系统通常为图像分配固定长度的表示，这与人类智能和大型语言模型的动态表示能力形成对比。本文提出了一种新方法，通过编码-解码架构对2D图像进行可变长度标记表示的学习。该方法使用递归处理方式，将2D图像标记精炼为1D潜在标记，并在多个迭代中增加表示能力。每一次迭代都更新已有的潜在标记，并以适应图像的复杂性和上下文信息变化的方式增加新的标记。通过重建损失和FID指标验证了该标记策略的有效性，表明标记数量与图像的信息量和下游任务要求相匹配。此外，递归处理的方式显示出标记专门化的潜力，进一步揭示了物体或部分发现的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02393" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 15:16:58 GMT</pubDate>
</item>
<item>
<title>视觉语言模型中的输入令牌压缩与推理优化</title>
<link>https://arxiv.org/abs/2411.03312</link>
<guid>https://arxiv.org/abs/2411.03312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视觉语言模型中输入令牌压缩与推理性能的最佳权衡。</p><br /><br /><p><strong>摘要：</strong> 本文探讨视觉语言模型（VLM）在推理时面临的高延迟问题，并提出通过减少输入图像令牌数来降低推理成本的方案。我们建立了一个模型，表征视觉令牌数量与语言模型参数之间的最佳权衡，并发现对于视觉推理任务，在给定推理计算预算下，使用最大化语言模型（LLM）并将视觉令牌数压缩至最低（通常为单个令牌）可以实现推理性能的最佳化。这一发现表明，计算优化的推理环境需要显著更高的令牌压缩比。我们还为适应高令牌压缩设置的解决方案奠定了初步基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.03312" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 12:57:50 GMT</pubDate>
</item>
<item>
<title>GarVerseLOD：高保真3D服装重建的新数据集与框架</title>
<link>https://arxiv.org/abs/2411.03047</link>
<guid>https://arxiv.org/abs/2411.03047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GarVerseLOD数据集，提升单幅图像服装重建质量与泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GarVerseLOD，一个新型数据集和框架，旨在提高从单幅非约束图像中进行高保真3D服装重建的鲁棒性。尽管神经隐式函数在多图像服装数字化方面取得了进展，但在面对复杂布料变形和体态时，现有方法仍面临泛化困难。GarVerseLOD收集了6000个高质量服装模型，细致的几何细节由专业艺术家手动创建，并通过层次化的数据集结构，提高模型的泛化能力和推理准确性。此外，采用基于条件扩散模型的新标记范式，为每个服装模型生成大量高光真实感的配对图像，确保GarVerseLOD在实图生成鸥等实际图像中表现优异。实验结果显示，GarVerseLOD相较于之前的方法，在生成独立服装作品的质量上显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.03047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 10:19:13 GMT</pubDate>
</item>
<item>
<title>视觉显著性与物体检测准确性的关系研究</title>
<link>https://arxiv.org/abs/2411.02844</link>
<guid>https://arxiv.org/abs/2411.02844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现视觉显著性与物体检测准确性关系更紧密。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了物体检测准确性与深度预测和视觉显著性预测之间的关系，通过使用最新模型（DeepGaze IIE、Depth Anything、DPT-Large 和 Itti 模型）在 COCO 和 Pascal VOC 数据集上进行的全面实验，发现视觉显著性与物体检测准确性之间的相关性明显高于深度预测。具体而言，在 Pascal VOC 数据集中，视觉显著性的 mArho 高达 0.459，而深度预测的 mArho 仅为 0.283。此外，分析显示不同物体类别之间的相关性存在显著差异，大型物体的相关性值可高达小型物体的三倍。这些发现表明，将视觉显著性特征融入物体检测架构可能比深度信息更具优势，尤其对特定物体类别而言，同时这些类别特定的变化为特征工程和数据集设计改进提供了重要的见解，可能促使更高效和更准确的物体检测系统的开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 09:16:37 GMT</pubDate>
</item>
<item>
<title>基于激活传输的模型生成控制框架</title>
<link>https://arxiv.org/abs/2410.23054</link>
<guid>https://arxiv.org/abs/2410.23054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新框架AcT，有效控制生成模型的行为。</p><br /><br /><p><strong>摘要：</strong> 随着大型生成模型能力的提升及其广泛应用，关于其可靠性、安全性及潜在误用的担忧逐渐增加。针对这些问题，本文介绍了一种新颖的激活传输（AcT）框架，通过最优运输理论引导模型激活，从而实现精细化的行为控制，且对计算开销影响极小。实验结果表明，在大型语言模型和文本到图像的扩散模型中，AcT有效缓解了有害内容，诱导了任意概念，并提高了模型的真实性。同时，AcT在图像生成中实现了精细的风格控制和概念否定，展示了其广泛的适用性和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 09:15:03 GMT</pubDate>
</item>
<item>
<title>Sample-Efficient Alignment for LLMs</title>
<link>https://arxiv.org/abs/2411.01493</link>
<guid>https://arxiv.org/abs/2411.01493</guid>
<content:encoded><![CDATA[
We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle's preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 04:15:36 GMT</pubDate>
</item>
<item>
<title>DreamPolish: Domain Score Distillation With Progressive Geometry Generation</title>
<link>https://arxiv.org/abs/2411.01602</link>
<guid>https://arxiv.org/abs/2411.01602</guid>
<content:encoded><![CDATA[
We introduce DreamPolish, a text-to-3D generation model that excels in producing refined geometry and high-quality textures. In the geometry construction phase, our approach leverages multiple neural representations to enhance the stability of the synthesis process. Instead of relying solely on a view-conditioned diffusion prior in the novel sampled views, which often leads to undesired artifacts in the geometric surface, we incorporate an additional normal estimator to polish the geometry details, conditioned on viewpoints with varying field-of-views. We propose to add a surface polishing stage with only a few training steps, which can effectively refine the artifacts attributed to limited guidance from previous stages and produce 3D objects with more desirable geometry. The key topic of texture generation using pretrained text-to-image models is to find a suitable domain in the vast latent distribution of these models that contains photorealistic and consistent renderings. In the texture generation phase, we introduce a novel score distillation objective, namely domain score distillation (DSD), to guide neural representations toward such a domain. We draw inspiration from the classifier-free guidance (CFG) in textconditioned image generation tasks and show that CFG and variational distribution guidance represent distinct aspects in gradient guidance and are both imperative domains for the enhancement of texture quality. Extensive experiments show our proposed model can produce 3D assets with polished surfaces and photorealistic textures, outperforming existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 03:43:19 GMT</pubDate>
</item>
<item>
<title>Zebra-Llama：针对罕见疾病的专业语言模型</title>
<link>https://arxiv.org/abs/2411.02657</link>
<guid>https://arxiv.org/abs/2411.02657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra-Llama是一款专注于罕见疾病Ehlers-Danlos综合症的高精度语言模型。</p><br /><br /><p><strong>摘要：</strong> 罕见疾病在医疗保健中面临独特挑战，常常遭遇诊断延迟和信息碎片化的问题。针对这一情况，Zebra-Llama是一款专门设计的上下文感知语言模型，利用了精确的检索增强生成（RAG）能力，旨在处理Ehlers-Danlos综合症（EDS）的相关查询。EDS的复杂性在于其多样的症状和不断演变的诊断标准，这使得所需的可靠知识稀缺。通过基于医学文献、患者体验和临床资源的问答进行新型的上下文感知微调，Zebra-Llama在应对EDS相关问题时展现了前所未有的能力。在专家评估的测试集上，与基础模型（Llama 3.1-8B-Instruct）相比，Zebra-Llama在全面性、准确性、清晰度和引用可靠性方面都有显著提升。这项研究不仅为EDS患者提供了更可靠的信息，也为其他罕见疾病开发专门的AI解决方案奠定了基础，推动了在罕见疾病管理中推广专家级知识的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 02:52:12 GMT</pubDate>
</item>
<item>
<title>LLaMo：基于大语言模型的分子图助手</title>
<link>https://arxiv.org/abs/2411.00871</link>
<guid>https://arxiv.org/abs/2411.00871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaMo是一个结合语言与图表述的分子图模型，表现出色。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在指令跟随和泛化能力方面取得了显著进展，催生了大型视觉语言模型（LVLMs）。然而，在分子领域中，LLMs及其指令调优的能力尚未得到充分探索。为此，我们提出了LLaMo：一种基于大型语言模型的分子图助手。LLaMo是一个端到端训练的分子图语言模型，通过多层图投影器将图表示转化为图标记，利用跨注意力机制抽象每个图神经网络（GNN）层的输出表示和基元表示。此外，我们引入机器生成的分子图指令数据，以对大型分子图语言模型进行指令调优，提升分子和语言的通用理解能力。实验结果表明，LLaMo在分子描述生成、性质预测和IUPAC名称预测等多项任务中表现最佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 02:46:13 GMT</pubDate>
</item>
<item>
<title>动态早期退出框架：提高机器人视觉语言动作模型的效率</title>
<link>https://arxiv.org/abs/2411.02359</link>
<guid>https://arxiv.org/abs/2411.02359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一个动态早期退出框架以优化机器人视觉语言动作模型的计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种动态早期退出框架（DeeR-VLA），旨在提升机器人视觉语言动作模型（MLLM）的效率。该框架利用多出口架构，根据特定情况自动调整激活的模型规模，以避免冗余计算，满足机器人平台的计算和内存限制。此外，本文还开发了新算法，设定了早期终止标准，依据预定义的需求进行调整，包括平均计算成本、峰值计算消耗和GPU内存使用情况。通过在CALVIN机器人操作基准上的测试，DeeR在不影响性能的前提下，实现了LLM计算成本降低5.2-6.5倍，GPU内存降低2-6倍。这一创新为复杂任务下的MLLM应用提供了更高的效率和适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 01:07:04 GMT</pubDate>
</item>
<item>
<title>HtmlRAG：提升RAG系统知识能力的HTML利用方法</title>
<link>https://arxiv.org/abs/2411.02959</link>
<guid>https://arxiv.org/abs/2411.02959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HtmlRAG通过使用HTML优化RAG系统，提升知识获取效果。</p><br /><br /><p><strong>摘要：</strong> Retrieval-Augmented Generation (RAG)能够改善大语言模型的知识能力并缓解幻觉问题。现有RAG系统通常通过检索搜索结果，并提取纯文本来增强生成，但这一过程丢失了许多HTML中固有的结构和语义信息。为此，提出HtmlRAG，它利用HTML而非纯文本作为检索知识的格式。研究表明，HTML在建模外部文档知识方面更具优势。尽管使用HTML面临着额外的输入令牌和噪声问题，我们通过HTML清理、压缩和剪枝策略来应对，确保最大程度保留信息。在六个问答数据集上的实验结果证实，使用HTML的RAG系统在性能上优于传统方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 06 Nov 2024 01:03:49 GMT</pubDate>
</item>
<item>
<title>SALSA：改进大语言模型的强化学习人类反馈方法</title>
<link>https://arxiv.org/abs/2411.01798</link>
<guid>https://arxiv.org/abs/2411.01798</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SALSA通过灵活模型参考提升大语言模型的探索和性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SALSA（基于模型汤的对齐学习），旨在克服传统人类反馈强化学习中的限制。RLHF通常使用KL散度作为参考，以限制模型偏离初始策略，然而这限制了对奖励空间的探索。SALSA通过对两个独立的监督微调模型进行权重空间平均，构建了一个更灵活的参考模型，允许在KL散度上有更大偏差，助力模型探索解决方案的有前景区域。通过在多个流行开源模型（如Llama2-7B、Mistral-7B和Gemma-2B）及各类基准测试中进行广泛实验，SALSA在促进更深层次探索及实现更优的对齐性方面显著优于传统的Proximal Policy Optimization（PPO）方法，提高了模型的稳健性和超分布外泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.01798" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 20:23:08 GMT</pubDate>
</item>
<item>
<title>AutoVFX：基于自然语言的自动化视觉特效生成框架</title>
<link>https://arxiv.org/abs/2411.02394</link>
<guid>https://arxiv.org/abs/2411.02394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoVFX框架实现了基于自然语言的自动视觉特效生成。</p><br /><br /><p><strong>摘要：</strong> AutoVFX是一个创新的框架，旨在通过单个视频和自然语言指令自动创建逼真且动态的视觉特效（VFX）视频。通过整合神经场景建模、基于大语言模型的代码生成以及物理模拟，AutoVFX能够提供物理基础、照片级真实感的编辑效果，并可直接使用自然语言进行控制。我们进行了广泛的实验以验证AutoVFX在各类视频和指令下的有效性，定量和定性结果表明，在生成质量、指令对齐、编辑多样性和物理合理性方面，AutoVFX显著优于所有竞争方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 19:07:10 GMT</pubDate>
</item>
<item>
<title>基于预训练扩散模型的高效噪声线性逆问题求解算法</title>
<link>https://arxiv.org/abs/2411.00359</link>
<guid>https://arxiv.org/abs/2411.00359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种高效算法，通过预训练扩散模型解决噪声线性逆问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种使用预训练的扩散模型来高效解决噪声线性逆问题的新算法。我们扩展了去噪扩散隐式模型（DDIM）的范式，提出了约束扩散隐式模型（CDIM），其修改了扩散更新，以强制最终输出满足特定约束。在无噪声逆问题中，CDIM能够精确满足约束；而在有噪声的情况下，我们对CDIM进行了推广，以确保噪声残差分布的确切约束。通过多种任务和指标的实验结果表明，CDIM在性能上表现优异，其推理速度与无约束的DDIM相当，达到了比之前条件扩散方法快10到50倍的效果。我们展示了该方法在超分辨率、去噪、填补、去模糊和3D点云重建等问题上的广泛适用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 18:10:16 GMT</pubDate>
</item>
<item>
<title>图像目标表征 (IGOR)：跨人机学习的统一动作空间</title>
<link>https://arxiv.org/abs/2411.00785</link>
<guid>https://arxiv.org/abs/2411.00785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IGOR实现了人类与机器人间的统一语义动作空间学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了图像目标表征（IGOR），旨在学习一个统一、语义一致的人类与各种机器人间的动作空间。通过这个统一的潜在动作空间，IGOR实现了大规模机器人和人类活动数据之间的知识转移。我们通过将初始图像和目标状态之间的视觉变化压缩成潜在动作来实现这一目标。IGOR能够生成互联网规模的视频数据的潜在动作标签，支持多种任务的基础策略和世界模型训练。我们展示了：（1）IGOR为人类和机器人学习了一个语义一致的动作空间，表征了物体的各种可能运动；（2）IGOR能够将视频中物体的运动迁移到其他视频中，跨越人类和机器人，利用潜在动作模型和世界模型的联合使用；（3）IGOR能够通过基础策略模型将潜在动作与自然语言对齐，并与低级策略模型整合，以实现高效的机器人控制。我们相信IGOR为人机知识转移和控制开辟了新的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 17:36:36 GMT</pubDate>
</item>
<item>
<title>LoCAL：增强大型多模态模型的长文档理解能力</title>
<link>https://arxiv.org/abs/2411.01106</link>
<guid>https://arxiv.org/abs/2411.01106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoCAL框架提升了多模态模型对长文档的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LoCAL的框架，旨在增强大型多模态模型（LMMs）对复杂、长文档的理解能力。传统的方法在处理多页视觉丰富文档时效率低下，而将所有页面直接呈现给LMM则导致性能问题。LoCAL通过实现两个特定的LMM适配器，一个用于证据页面检索，另一个用于问题回答，使LMM能够有效作为多模态检索器，根据用户的问题获取相关页面。实验证明，LoCAL在公共基准测试上展示了尖端性能，展示了该框架的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.01106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 16:19:49 GMT</pubDate>
</item>
<item>
<title>多专家提示：提升大语言模型生成效果的新方法</title>
<link>https://arxiv.org/abs/2411.00492</link>
<guid>https://arxiv.org/abs/2411.00492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出多专家提示法，显著提升大语言模型的生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多专家提示（Multi-expert Prompting），这是对ExpertPrompting（Xu et al., 2023）的新增强设计，旨在提升大语言模型（LLM）的生成能力。该方法通过模拟多个专家的响应并聚合这些响应，选择其中最佳的结果，以满足输入指令。该过程通过我们设计的七个子任务进行，这些子任务源自名义小组技术（Ven和Delbecq, 1974），一种成熟的决策框架。评估结果表明，多专家提示法在提升生成的真实度、准确性、信息量和实用性方面显著优于ExpertPrompting和其他基线方法，同时降低了生成内容的毒性和伤害性。在与ChatGPT的对比中，其真实度比最佳基线高出8.69%。此外，多专家提示高效、可解释，并能灵活适应不同场景，无需人工构建提示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 14:47:35 GMT</pubDate>
</item>
<item>
<title>MVPaint：一种新型的3D纹理生成与优化框架</title>
<link>https://arxiv.org/abs/2411.02336</link>
<guid>https://arxiv.org/abs/2411.02336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MVPaint是一种提高3D纹理生成质量的新框架，强调多视图一致性。</p><br /><br /><p><strong>摘要：</strong> MVPaint是一个新提出的3D纹理生成与优化框架，旨在解决当前纹理生成方法中存在的局部不连续性和多视图一致性差等问题。该框架主要由三个关键模块组成：同步多视图生成（SMG）用于生成粗略的多视图图像；空间意识3D修补（S3I）用于有效填补未观测到的纹理区域；以及UV优化（UVR），通过UV空间超分辨率和空间意识接缝平滑算法，改善纹理质量并消除纹理不连续性。此外，MVPaint还建立了两个T2T评估基准，Objaverse T2T和GSO T2T，以实验验证其优越性。实验结果表明，与现有方法相比，MVPaint在生成高保真纹理和增强跨视图一致性方面具有显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 14:06:50 GMT</pubDate>
</item>
<item>
<title>PPLLaVA：一种适用于长短视频理解的新型模型</title>
<link>https://arxiv.org/abs/2411.02327</link>
<guid>https://arxiv.org/abs/2411.02327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出PPLLaVA模型，实现短长视频的有效理解。</p><br /><br /><p><strong>摘要：</strong> 在过去一年中，视频基础的大型语言模型取得了显著进展，但如何开发一个适用于短和长视频理解的统一模型仍是一大挑战。现有大多数视频LLM无法处理长达一小时的视频，而针对长视频的方法在短视频或图像上效果不佳。本文针对视频中的冗余内容问题，提出了一种新的池化策略——PPLLaVA（Prompt-guided Pooling LLaVA），该模型包括三个核心组件：基于CLIP的视觉提示对齐、提示引导池化以及用于扩展长提示的上下文设计。此外，代码库整合了最先进的视频直接偏好优化（DPO）和视觉交叉训练。通过大量实验验证，PPLLaVA在图像基准测试和视频任务中表现出色，能有效处理从秒到小时的视频，展现出卓越的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 13:39:45 GMT</pubDate>
</item>
<item>
<title>量化大语言模型的准确性与性能权衡研究</title>
<link>https://arxiv.org/abs/2411.02355</link>
<guid>https://arxiv.org/abs/2411.02355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究分析了不同量化格式对大语言模型的准确性与性能影响。</p><br /><br /><p><strong>摘要：</strong> 本文对量化大语言模型（LLM）的准确性与性能权衡进行了全面的实证研究，评估了FP8、INT8和INT4等流行的量化格式在Llama-3.1模型家族上的表现。研究表明，FP8权重和激活量化在所有模型规模上均为无损，INT8格式经过适当调优后其准确性仅有1-3%的轻微下降，而INT4的权重量化在与8位整数权重和激活量化相竞争的同时也表现不俗。此外，通过对各种GPU架构进行推断性能分析，发现W4A16格式在同步部署中提供最佳的性价比，而W8A8格式则在高端GPU上进行异步连续批处理时表现优越。研究结果为不同规模和性能需求的量化大语言模型的部署提供了实用指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 12:31:24 GMT</pubDate>
</item>
<item>
<title>Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title>
<link>https://arxiv.org/abs/2411.01192</link>
<guid>https://arxiv.org/abs/2411.01192</guid>
<content:encoded><![CDATA[
We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5 base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 08:19:26 GMT</pubDate>
</item>
<item>
<title>Survey of Cultural Awareness in Language Models: Text and Beyond</title>
<link>https://arxiv.org/abs/2411.00860</link>
<guid>https://arxiv.org/abs/2411.00860</guid>
<content:encoded><![CDATA[
Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 07:06:11 GMT</pubDate>
</item>
<item>
<title>DynaSaur: Large Language Agents Beyond Predefined Actions</title>
<link>https://arxiv.org/abs/2411.01747</link>
<guid>https://arxiv.org/abs/2411.01747</guid>
<content:encoded><![CDATA[
Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur{https://github.com/adobe-research/dynasaur}.
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 05:09:05 GMT</pubDate>
</item>
<item>
<title>Training-free Regional Prompting for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2411.02395</link>
<guid>https://arxiv.org/abs/2411.02395</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 04:57:16 GMT</pubDate>
</item>
<item>
<title>基于CamVid-30K数据集的3D和4D生成框架GenXD</title>
<link>https://arxiv.org/abs/2411.02319</link>
<guid>https://arxiv.org/abs/2411.02319</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GenXD框架，实现3D和4D场景生成。</p><br /><br /><p><strong>摘要：</strong> 近期2D视觉生成的发展极为顺利，然而在实际应用中，3D和4D生成依然面临挑战，主要原因是缺乏大规模4D数据和有效的模型设计。本文提出了一个联合研究3D和4D生成的方法，利用日常生活中常见的相机和物体运动。我们首先通过数据精炼流程从视频中获取相机位姿和物体运动强度，随后推出了一个大规模实际4D场景数据集——CamVid-30K。利用这些数据，我们开发了框架GenXD，能够生成各种3D或4D场景。我们提出了多视角时间模块，能够有效分离相机和物体运动，从而实现对3D和4D数据的无缝学习。此外，GenXD还采用了掩码潜在条件，以支持多种条件视图。其可以生成沿着相机轨迹的视频以及一致的3D视图，为3D表现提供支持。通过对不同实际和合成数据集的广泛评估，我们展示了GenXD在3D和4D生成方面的有效性和多样性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02319" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 04:32:54 GMT</pubDate>
</item>
<item>
<title>Hunyuan-Large：最强大的开源Transformer专家模型</title>
<link>https://arxiv.org/abs/2411.02265</link>
<guid>https://arxiv.org/abs/2411.02265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan-Large是当前最大的开源混合专家Transformer模型，参数达到3890亿。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Hunyuan-Large，这是当前最大的开源Transformer-based混合专家模型，拥有3890亿参数和520亿激活参数，能够处理多达256K个token。通过全面评估，Hunyuan-Large在语言理解与生成、逻辑推理、数学问题解决、编码、长上下文和综合任务等多个基准测试中表现优异，超越了LLama3.1-70B，并在与更大规模的LLama3.1-405B模型比较时表现相当。Hunyuan-Large的关键实践包括比以往文献中更大规模的合成数据、混合专家路由策略、键值缓存压缩技术以及专家特定的学习率策略。此外，文章还探讨了混合专家模型的规模规律和学习率安排，为未来模型开发与优化提供了重要见解与指导。Hunyuan-Large的代码和检查点已发布，以促进未来的创新与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 04:31:53 GMT</pubDate>
</item>
<item>
<title>解码器Transformer基模型中激活稀疏性的量化研究</title>
<link>https://arxiv.org/abs/2411.02335</link>
<guid>https://arxiv.org/abs/2411.02335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究深入探讨了LLMs中激活稀疏性的影响因素及量化特性。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于解码器的Transformer模型中的激活稀疏性进行了全面的定量研究。我们提出了PPL-p%稀疏性这一精确的激活稀疏性度量标准，并通过大量实验证明了几种重要现象。研究发现，不同的激活函数在性能上表现相似，但其训练过程中的稀疏性趋势却截然相反。此外，激活比例与训练数据量呈现出收敛的幂律和对数空间的反向幂律演变，表明ReLU激活函数在提升稀疏性方面优于SiLU。最后，激活比例在固定参数规模下与宽度-深度比的线性关系以及激活稀疏性的极限值对参数规模的敏感性低，揭示了LLMs在激活稀疏性上潜在的高效性与可解释性。这些发现为提高LLMs的效率及可解释性提供了重要的指导意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 03:27:24 GMT</pubDate>
</item>
<item>
<title>AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents</title>
<link>https://arxiv.org/abs/2410.24024</link>
<guid>https://arxiv.org/abs/2410.24024</guid>
<content:encoded><![CDATA[
Autonomous agents have become increasingly important for interacting with the real world. Android agents, in particular, have been recently a frequently-mentioned interaction method. However, existing studies for training and evaluating Android agents lack systematic research on both open-source and closed-source models. In this work, we propose AndroidLab as a systematic Android agent framework. It includes an operation environment with different modalities, action space, and a reproducible benchmark. It supports both large language models (LLMs) and multimodal models (LMMs) in the same action space. AndroidLab benchmark includes predefined Android virtual devices and 138 tasks across nine apps built on these devices. By using the AndroidLab environment, we develop an Android Instruction dataset and train six open-source LLMs and LMMs, lifting the average success rates from 4.59% to 21.50% for LLMs and from 1.93% to 13.28% for LMMs. AndroidLab is open-sourced and publicly available at https://github.com/THUDM/Android-Lab.
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 03:00:37 GMT</pubDate>
</item>
<item>
<title>WebRL：开放LLM的在线强化学习框架</title>
<link>https://arxiv.org/abs/2411.02337</link>
<guid>https://arxiv.org/abs/2411.02337</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebRL框架通过在线学习提升开放LLM的网页交互能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WebRL，一个针对开放大型语言模型（LLM）的自我进化在线课程强化学习框架，旨在训练高效的网页代理。WebRL解决了构建LLM网页代理中的三大关键挑战：训练任务稀缺、反馈信号稀疏及在线学习中的策略分布漂移。具体而言，WebRL整合了自我进化的课程，依据失败尝试生成新任务，采用稳健的结果监督奖励模型（ORM），以及自适应强化学习策略，以确保持续改进。通过将WebRL应用于开放的Llama-3.1和GLM-4模型，Llama-3.1-8B的成功率从4.8%提升至42.4%，GLM-4-9B的成功率从6.1%提升至43%。这些开放模型的表现显著超越了GPT-4-Turbo（17.6%）和GPT-4o（13.9%），并在开放LLM训练的前沿网页代理（AutoWebGLM，18.2%）中占据领先地位。研究结果展示了WebRL在缩小开放与专有LLM网页代理之间差距的有效性，为构建更具可访问性和强大的自主网页交互系统奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02337" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 02:55:57 GMT</pubDate>
</item>
<item>
<title>AdaCache：加速视频生成的自适应缓存方法</title>
<link>https://arxiv.org/abs/2411.02397</link>
<guid>https://arxiv.org/abs/2411.02397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AdaCache方法，显著提升视频生成速度且保持质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为自适应缓存（AdaCache）的训练无关方法，旨在加速视频生成过程，尤其是在处理时间跨度较长的视频时。传统的扩散变换器（DiTs）因模型规模和注意力机制的加重导致推理速度缓慢，AdaCache通过缓存计算和制定适合每个视频生成的缓存计划，显著提高了质量与延迟之间的权衡。此外，文中还引入了运动正则化（MoReg）方案，根据视频内容中的运动信息调控计算分配，从而进一步提升生成效率。实验表明，该方法在多个视频DiT基准上实现了显著的推理速度提升，高达4.7倍，同时不牺牲生成质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 02:09:03 GMT</pubDate>
</item>
<item>
<title>专用稀疏自编码器：揭示基础模型中的关键概念</title>
<link>https://arxiv.org/abs/2411.00743</link>
<guid>https://arxiv.org/abs/2411.00743</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍专用稀疏自编码器以捕捉基础模型中的稀有概念。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新型的专用稀疏自编码器（SSAEs），旨在通过聚焦特定子领域，揭示基础模型（FMs）中的稀有但重要的概念。与传统的稀疏自编码器相比，SSAEs能更有效地捕捉数据稀疏性及尾部概念。我们阐述了SSAEs的训练方法，强调密集检索在数据选择中的有效性以及倾斜经验风险最小化这一训练目标对提高概念回忆的益处。在对SSAEs进行的评估中，结果显示其在下游困惑度和L_0稀疏性等标准指标上，均优于通用稀疏自编码器。在“Bias in Bios”数据集的案例研究中，SSAEs在去除虚假的性别信息后，达到了最低组分类准确率提高12.5%的显著效果，展示了SSAEs在揭示基础模型内部机制方面的强大实用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00743" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 01:40:29 GMT</pubDate>
</item>
<item>
<title>视频生成模型对物理规律的学习与评估</title>
<link>https://arxiv.org/abs/2411.02385</link>
<guid>https://arxiv.org/abs/2411.02385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视频生成模型如何从视觉数据学习物理规律的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视频生成模型在无须人类先验知识的情况下，能否从视觉数据中发现基本物理规律。通过建立一个二维模拟测试平台，研究对象运动和碰撞的生成视频，评估模型在不同场景下的表现，包括相同分布、不同分布和组合一般化。实验结果显示，尽管模型在相同分布内具有完美的一般化能力，但在不同分布场景中表现失败。进一步研究揭示模型在一般化过程中偏向于特定的训练样本和特征，未能抽象出一般物理规律。本研究表明，单靠模型规模的扩大不足以使视频生成模型发现基本的物理规律。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.02385" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 01:33:34 GMT</pubDate>
</item>
<item>
<title>LibMoE: 提升混合专家算法在大语言模型中的可访问性</title>
<link>https://arxiv.org/abs/2411.00918</link>
<guid>https://arxiv.org/abs/2411.00918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LibMoE为混合专家算法研究提供了模块化和高效的框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LibMoE，一个全面的模块化框架，旨在简化混合专家（MoE）算法的研究、训练和评估。LibMoE遵循模块化设计、高效训练和全面评估三大核心原则，使得MoE在大语言模型（LLM）中的应用更加普及。通过LibMoE，我们对五种最先进的MoE算法在三个不同的LLM和11个数据集上进行了广泛的基准测试，研究结果显示，尽管各MoE算法特性不同，但在广泛任务上的平均表现相似。我们相信，LibMoE将为研究人员在下一代MoE和LLM的研究进展提供重要帮助。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 00:57:15 GMT</pubDate>
</item>
<item>
<title>探讨视觉语言模型的数学推理能力及DynaMath基准的开发</title>
<link>https://arxiv.org/abs/2411.00836</link>
<guid>https://arxiv.org/abs/2411.00836</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨视觉语言模型在数学推理中的局限性，并介绍了新的DynaMath基准。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉语言模型（VLMs）在处理涉及视觉背景的数学推理任务中的表现，发现当前的领先模型如GPT-4o在面临相似问题的变化时常常失败，显示出其推理能力的局限性。为了评估VLM在不同问题变体下的鲁棒性，我们引入了DynaMath，一个动态的视觉数学基准，旨在深入评估VLM的能力。DynaMath包含501个高质量的多主题种子问题，通过Python程序形式呈现，基于这些程序自动生成了超过5000个具体问题。评估结果表明，最坏情况模型的准确性明显低于平均准确性。这强调了对VLM推理能力的鲁棒性研究的必要性，DynaMath为开发更可靠的数学推理模型提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00836" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 05 Nov 2024 00:47:47 GMT</pubDate>
</item>
<item>
<title>Fashion-VDM：视频虚拟试穿的新方法</title>
<link>https://arxiv.org/abs/2411.00225</link>
<guid>https://arxiv.org/abs/2411.00225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fashion-VDM模型实现了高质量视频虚拟试穿效果。</p><br /><br /><p><strong>摘要：</strong> Fashion-VDM是一种新的视频扩散模型，旨在生成高质量的虚拟试穿视频。该方法通过输入服装图像和人物视频，生成展示用户穿着指定服装的动态视频，同时保持用户的身份和动作一致性。尽管以图像为基础的虚拟试穿技术已取得显著成绩，但现有的视频虚拟试穿方法在服装细节和时间一致性方面仍存在不足。为此，Fashion-VDM引入了基于扩散的架构，采用分裂的无分类器引导，以增强对条件输入的控制，并实施渐进的时间训练策略，实现单次处理64帧、512像素的视频生成。此外，实验证明联合图像-视频训练在视频数据有限时尤为有效。本研究的定性与定量实验表明，Fashion-VDM在视频虚拟试穿领域树立了新的标杆。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 20:18:24 GMT</pubDate>
</item>
<item>
<title>Physics in Next-token Prediction</title>
<link>https://arxiv.org/abs/2411.00660</link>
<guid>https://arxiv.org/abs/2411.00660</guid>
<content:encoded><![CDATA[
We discovered the underlying physics in Next-token Prediction (NTP). We identified the law of information conservation within NTP and proposed the First Law of Information Capacity (IC-1), demonstrating that the essence of intelligence emergence in auto-regressive models is fundamentally a process of information transfer. We also introduced Landauer's Principle into NTP, formulating the Second Law of Information Capacity (IC-2), which establishes the relationship between auto-regressive model training and energy consumption. Additionally, we presented several corollaries, which hold practical significance for production practices. Finally, we validated the compatibility and complementarity of our findings with existing theories.
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 16:14:19 GMT</pubDate>
</item>
<item>
<title>TOMATO：评估多模态基础模型在视频理解中的时间推理能力</title>
<link>https://arxiv.org/abs/2410.23266</link>
<guid>https://arxiv.org/abs/2410.23266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明当前多模态基础模型在视频时间推理上的能力被高估，提出了新评测基准TOMATO。</p><br /><br /><p><strong>摘要：</strong> 尽管现有基准显示多模态基础模型（MFMs）在视频理解中表现卓越，但我们的研究表明，它们在视觉时间推理能力上可能被高估。许多问题可通过单个、少量或无序帧解决，因此，我们提出三项原则和相应指标，以系统地评估当前视觉时间推理任务：多帧增益、帧顺序敏感性和帧信息差异。基于这些原则，我们引入了TOMATO（Temporal Reasoning Multimodal Evaluation），一个新的基准，用于严格评估MFMs在视频理解中的时间推理能力。TOMATO包含1484个精心策划的人类标注问题，涵盖六个任务，应用于1417个视频。我们的综合评估显示，最佳模型的表现与人类之间存在57.3%的差距，深入分析发现MFMs在理解连续帧的动态关系方面存在更根本的局限性。TOMATO将为评估下一代MFMs提供关键的平台，并呼吁社区开发能够更好理解人类世界动态的AI系统。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 15:02:37 GMT</pubDate>
</item>
<item>
<title>CityGaussianV2：大型场景重建的新方法</title>
<link>https://arxiv.org/abs/2411.00771</link>
<guid>https://arxiv.org/abs/2411.00771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出CityGaussianV2，解决大型场景重建中的几何精度和效率问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法CityGaussianV2，用于解决3D Gaussian Splatting在大型场景重建中的几何精度和效率问题。尽管3DGS在辐射场重建和新视角合成中表现出色，但在复杂场景中的表面表现仍是一个重大挑战。该方法通过实现分解梯度加密和深度回归技术，消除模糊伪影并加快收敛，同时引入延长滤波器以减少由于2D Gaussian Splatting退化引起的高斯计数爆炸。此外，CityGaussianV2优化了并行训练管道，在存储和训练时间方面实现了显著的提升，达到10倍压缩，节省至少25%的训练时间，并减少50%的内存使用。实验结果表明，该方法在视觉质量、几何精度以及存储与训练成本之间取得了良好的平衡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 13:49:30 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的人脸匿名化技术研究</title>
<link>https://arxiv.org/abs/2411.00762</link>
<guid>https://arxiv.org/abs/2411.00762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的人脸匿名化方法，不依赖人脸识别模型。</p><br /><br /><p><strong>摘要：</strong> 当前的人脸匿名化技术通常依赖于通过人脸识别模型计算的身份丧失，但这种方法往往不够准确且不可靠。此外，许多方法需要额外的数据如人脸特征点和面具来指导合成过程。与此不同，我们的方法采用扩散模型，仅依赖重建损失，消除了对面部特征点或面具的需求，同时仍能生成细致入微的图像。通过对两个公共基准的量化和定性评估，我们的模型在身份匿名化、面部属性保留和图像质量三大关键领域达到了领先水平。除了其主要的匿名化功能外，我们的模型还可以通过添加额外的面部图像进行换脸任务，展示了其多功能性和广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 12:50:06 GMT</pubDate>
</item>
<item>
<title>结合掩蔽语言建模与因果语言建模的混合训练方法</title>
<link>https://arxiv.org/abs/2410.24159</link>
<guid>https://arxiv.org/abs/2410.24159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合掩蔽与因果语言建模的混合训练方法，效果优于单一模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种将掩蔽语言建模与因果语言建模融合的简单方法，即GPT-BERT模型。该混合训练目标使得模型能够在单一的Transformer架构中，灵活地像标准的因果或掩蔽语言模型一样使用。我们在BabyLM Challenge 2024上对这种灵活行为的预训练过程进行了测试，结果表明，该混合预训练方式在性能上超越了仅使用掩蔽模型或因果模型的单一模型。我们将模型、训练语料和代码公开发布，供相关研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 12:48:14 GMT</pubDate>
</item>
<item>
<title>基于词频的embedding空间校正提升任务性能</title>
<link>https://arxiv.org/abs/2411.00680</link>
<guid>https://arxiv.org/abs/2411.00680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用Zipf法则加权的PCA白化方法可显著提升任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了神经模型中词嵌入空间的偏斜问题，并指出现有方法在建模和纠正这一空间对称性时，普遍假设词频是均匀分布的，然而实际词频遵循非均匀的Zipf法则。出人意料的是，使用基于实际词频进行加权的PCA白化显著提升了任务性能，超过了现有的基线。理论上，我们的方法和现有方法可以被明确分类：词表示根据指数族分布，基础度量可以是均匀或Zipfian。采用后者后，我们可以自然强调低频词的重要性，从信息几何和不平衡分类的损失函数中均可显现。此外，我们的理论支持了流行的自然语言处理方法，如skip-gram负采样、WhiteningBERT和无头语言模型，它们的词嵌入能够将实际词频编码到基础的概率模型中，从而取得良好效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 10:46:08 GMT</pubDate>
</item>
<item>
<title>引入常加速度流的图像生成新框架</title>
<link>https://arxiv.org/abs/2411.00322</link>
<guid>https://arxiv.org/abs/2411.00322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出常加速度流（CAF）框架，提升图像生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了常加速度流（CAF）框架，以解决目前直流和再流程序在快速生成中的局限性。CAF基于简单的常加速度方程，引入了可学习的加速度变量，从而更加精确和丰富地估计常微分方程（ODE）流。为进一步提升估计精度，本文还提出了初始速度条件化和初始速度再流处理两种技术。通过在玩具数据集、CIFAR-10和ImageNet 64x64上的全面研究，CAF在单步生成方面显著优于最先进的基础线，并在少步耦合保留和反演方面显示出显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 08:53:22 GMT</pubDate>
</item>
<item>
<title>提升WikiNER语料库质量的研究</title>
<link>https://arxiv.org/abs/2411.00030</link>
<guid>https://arxiv.org/abs/2411.00030</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了WikiNER语料库的质量及其法语修订版本WikiNER-fr-gold。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了多语言命名实体识别语料库WikiNER的质量，并提供了其修订版WikiNER-fr-gold。WikiNER的注释采用半监督方式生成，未进行后期手动验证，因此被称为银标标准。我们随机抽取了原法语子语料库的20%（26,818句，700k标记），并总结了每类实体类型，以制定注释指南。接着，进行了语料库的修订。最后，分析了WikiNER-fr语料库中的错误和不一致，并探讨了未来研究的发展方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00030" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 08:34:23 GMT</pubDate>
</item>
<item>
<title>推出OS-Atlas：增强开源GUI代理的基础模型</title>
<link>https://arxiv.org/abs/2410.23218</link>
<guid>https://arxiv.org/abs/2410.23218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OS-Atlas是一个开源GUI行动模型，通过数据与建模创新提升性能。</p><br /><br /><p><strong>摘要：</strong> OS-Atlas是一种先进的开源GUI行动模型，旨在提升GUI对接和应对新环境场景的能力。传统的商业视觉语言模型（VLMs）如GPT-4o与GeminiProVision被广泛运用，但研究人员因其显著的性能差异而不愿采用开源VLM。为推动此领域的研究，OS-Atlas在多平台上合成GUI对接数据，提供了超过1300万个GUI元素的跨平台开源数据集。此外，通过针对模型训练的创新，OS-Atlas在理解GUI屏幕截图及应用于未见界面方面表现出色。经过在移动、桌面和网络等六个基准测试中的广泛评估，OS-Atlas展现出相较于以往最先进模型的显著性能提升，同时为不断提高和拓展开源VLM的能力提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 05:04:10 GMT</pubDate>
</item>
<item>
<title>SambaMixer：一种用于锂离子电池健康状态预测的新结构化状态空间模型</title>
<link>https://arxiv.org/abs/2411.00233</link>
<guid>https://arxiv.org/abs/2411.00233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SambaMixer模型用于高精度预测锂离子电池的健康状态。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SambaMixer的新型结构化状态空间模型(SSM)，用于预测锂离子电池的健康状态(SOH)。该模型基于MambaMixer架构，专为处理多变量时间信号而设计。我们在NASA电池放电数据集上评估了SambaMixer的性能，结果显示该模型在该数据集上的表现优于现有技术。此外，本文还引入了一种新颖的基于锚点的重采样方法，以确保时间信号的期望长度，同时作为增强技术。通过使用位置编码对样本时间和循环时间差进行条件预测，我们进一步提升了模型性能，学习了电池的恢复效应。实验结果证明，SambaMixer能够以高准确性和鲁棒性预测锂离子电池的健康状态。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 04:55:37 GMT</pubDate>
</item>
<item>
<title>人机交互中的生成式AI应用及其用户界面设计</title>
<link>https://arxiv.org/abs/2410.22370</link>
<guid>https://arxiv.org/abs/2410.22370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文调查用户与生成式AI的交互模式，关注用户引导的交互。</p><br /><br /><p><strong>摘要：</strong> 随着生成式AI应用的迅猛发展，人机交互的复杂性也随之提高。现有文献虽然探讨了人类如何与生成式AI互动，但缺乏对用户界面设计及模式的具体分析。本文提供了一项全面的调查，展示了人类与AI交互的分类，以及为满足各种相关用例而设计的用户交互模式。特别关注用户引导的交互，调查用户主动发起的互动，未考虑用户暗示的信号。通过本调查，我们希望为设计师和开发者提供不同交互模式的汇编，降低学习生成式AI应用设计的门槛。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 02:47:59 GMT</pubDate>
</item>
<item>
<title>引入图推理结构的问答数据集GRS-QA</title>
<link>https://arxiv.org/abs/2411.00369</link>
<guid>https://arxiv.org/abs/2411.00369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRS-QA数据集通过图结构显著提升了多跳问答性能的评估。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了图推理结构问答数据集（GRS-QA），旨在解决现有多跳问答（M-QA）数据集中推理结构不明确的问题。GRS-QA数据集为问答对提供了语义背景和推理结构，明确捕捉推理路径，通过构建推理图节点和逻辑流的边，提供了细致的推理结构评估。这些不同结构的推理图使得对大语言模型（LLMs）在不同推理结构下的表现进行精细评估成为可能。实证分析表明，LLMs在处理不同推理结构的问题时表现差异，推动了对文本结构与语义关系的进一步探讨。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 02:47:03 GMT</pubDate>
</item>
<item>
<title>个性化大语言模型的使用与挑战</title>
<link>https://arxiv.org/abs/2411.00027</link>
<guid>https://arxiv.org/abs/2411.00027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨个性化大语言模型的分类及其面临的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次将个性化大语言模型（LLMs）在文本生成与个性化应用之间进行整合，提出了个性化LLMs使用的分类法。文章系统化总结了个性化LLMs的基础定义，讨论了个性化的各个新颖面向及其应用，旨在统一相关文献，明确不同使用场景下的个性化技术、数据集和评估方法。同时，论文还强调了现有研究中尚未解决的挑战和重要问题，旨在为研究人员和实践者提供清晰的指南，以促进个性化LLMs的发展与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 02:45:46 GMT</pubDate>
</item>
<item>
<title>高效适配器插入方案提升文本到图像模型性能</title>
<link>https://arxiv.org/abs/2410.22901</link>
<guid>https://arxiv.org/abs/2410.22901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种有效的适配器插入方法，以提升文本到图像模型的任务执行能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种高效的适配器插入方法，用于文本到图像基础模型，旨在执行复杂的下游任务并保持基本模型的泛化能力。该方法的核心思想是优化与2D特征图相关的注意力机制，从而提升适配器的性能。通过在表情视频生成任务中的验证，该方法取得了显著的结果。本研究希望能为大型文本到图像模型的后训练任务提供新的见解。此外，由于该方法与SD1.5衍生模型具有良好的兼容性，因而对开源社区也具有一定的价值。相关代码将会公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 02:37:38 GMT</pubDate>
</item>
<item>
<title>M2RC-EVAL: 多语言代码补全基准与数据集</title>
<link>https://arxiv.org/abs/2410.21157</link>
<guid>https://arxiv.org/abs/2410.21157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了M2RC-EVAL基准，以提升多语言代码补全能力。</p><br /><br /><p><strong>摘要：</strong> 现有的代码补全基准通常只关注有限的编程语言，难以全面评估大型语言模型在多语言场景下的表现。为此，本文提出了M2RC-EVAL基准，覆盖18种编程语言，并提供两种细粒度注释（桶级别和语义级别），以便在不同的补全场景中进行分析。同时，本文还整理了M2RC-INSTRUCT数据集，以提高现有代码大型语言模型的代码补全能力。通过 umfassenden 实验结果证明了M2RC-EVAL 和 M2RC-INSTRUCT 的有效性，为多语言代码补全的研究提供了新的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 02:05:25 GMT</pubDate>
</item>
<item>
<title>Randomized Autoregressive Visual Generation</title>
<link>https://arxiv.org/abs/2411.00776</link>
<guid>https://arxiv.org/abs/2411.00776</guid>
<content:encoded><![CDATA[
This paper presents Randomized AutoRegressive modeling (RAR) for visual generation, which sets a new state-of-the-art performance on the image generation task while maintaining full compatibility with language modeling frameworks. The proposed RAR is simple: during a standard autoregressive training process with a next-token prediction objective, the input sequence-typically ordered in raster form-is randomly permuted into different factorization orders with a probability r, where r starts at 1 and linearly decays to 0 over the course of training. This annealing training strategy enables the model to learn to maximize the expected likelihood over all factorization orders and thus effectively improve the model's capability of modeling bidirectional contexts. Importantly, RAR preserves the integrity of the autoregressive modeling framework, ensuring full compatibility with language modeling while significantly improving performance in image generation. On the ImageNet-256 benchmark, RAR achieves an FID score of 1.48, not only surpassing prior state-of-the-art autoregressive image generators but also outperforming leading diffusion-based and masked transformer-based methods. Code and models will be made available at https://github.com/bytedance/1d-tokenizer
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 01:26:28 GMT</pubDate>
</item>
<item>
<title>利用IC-LoRA优化文本到图像的生成能力</title>
<link>https://arxiv.org/abs/2410.23775</link>
<guid>https://arxiv.org/abs/2410.23775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过IC-LoRA提升文本到图像的生成质量。</p><br /><br /><p><strong>摘要：</strong> 近期研究探讨了扩散变换器（DiTs）在任务无关的图像生成中的应用，但生成图像的保真度仍不理想。本文重新评估并简化了该框架，假设文本到图像的DiTs具有内在的上下文生成能力，仅需进行最小调优。通过多种任务实验，我们展示了现有的文本到图像DiTs可在无需调优的情况下有效进行上下文生成。基于此，我们提出了一种简单的管道IC-LoRA，利用DiTs的上下文能力：1）连接图像而非令牌，2）对多个图像进行联合标注，3）使用小数据集（如20sim，100样本）进行任务特定的LoRA调优，而不是大数据集的全参数调优。此方法无需修改原有的DiT模型，只需改动训练数据，显著生成更高保真度的图像集，更好地遵循提示。尽管调优数据具有任务特定性，但我们的框架在架构和流程上保持任务无关，为社区提供了一种强大的工具，并为未来的产品级任务无关生成系统的研究提供了重要启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23775" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 01:19:58 GMT</pubDate>
</item>
<item>
<title>基于人类问题解决过程的双组件微调方法提升大型语言模型的科学问题解答能力</title>
<link>https://arxiv.org/abs/2411.00412</link>
<guid>https://arxiv.org/abs/2411.00412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出双组件微调方法，提升LLM在科学问题解答中的准确性和工具使用精度。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在解决简单科学问题时显示出良好的能力，但在处理复杂问题时常出现幻觉。为了解决这一问题，本文提出了一种双组件微调的方法。第一部分是世界知识蒸馏（WKD），LLMs通过学习工具信息生成的解决方案来内化领域知识；第二部分是工具使用适应（TUA），我们根据模型直接回答的准确性将问题划分为简单和复杂两类。在处理简单问题时，模型保持WKD的对齐目标，而在面对复杂问题时，则学习智能切换使用工具。我们在六个科学基准数据集上验证了该方法，结果表明模型的回答准确率平均提高了28.18%，工具使用精度平均提升了13.89%，超越了包括GPT-4o和Claude-3.5在内的最先进模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2411.00412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 04 Nov 2024 00:32:14 GMT</pubDate>
</item>
<item>
<title>NeuZip: 一种新型神经网络权重压缩方案</title>
<link>https://arxiv.org/abs/2410.20650</link>
<guid>https://arxiv.org/abs/2410.20650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuZip有效减少了神经网络内存占用，同时保持性能。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新型的权重压缩方案NeuZip，针对神经网络中的浮点数进行熵基础压缩，提升了内存利用效率且不牺牲性能。通过NeuZip，我们成功地将训练Llama-3 8B模型所需的内存从31GB减少到不到16GB，而训练动态保持完全不变。在推理阶段，此方法能够将内存使用量减少一半以上，并保持几乎无损的性能表现。我们的代码已公开，便于研究者和开发者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 20:16:51 GMT</pubDate>
</item>
<item>
<title>AAAR-1.0: 评估大语言模型在科研任务中的表现</title>
<link>https://arxiv.org/abs/2410.22394</link>
<guid>https://arxiv.org/abs/2410.22394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AAAR-1.0基准数据集，以评估LLMs在科研任务中的能力。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了AAAR-1.0，一个旨在评估大语言模型（LLMs）在日常科研任务中表现的基准数据集。该数据集专注于三项基本的、需要专业知识的研究任务：方程推理（EquationInference）、实验设计（ExperimentDesign）和论文弱点识别（PaperWeakness），以及审稿批评（REVIEWCRITIQUE）。AAAR-1.0与以往基准的不同之处在于，它明确面向研究，要求深厚的领域专业知识；同时，任务设计反映了研究人员的日常活动。通过对多种开源和专有LLMs的评估，研究揭示了这些模型在复杂研究任务中的潜力与局限性，未来将持续对AAAR-1.0进行迭代和改进。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 18:26:59 GMT</pubDate>
</item>
<item>
<title>一种新型带有瓶颈的最小熵耦合压缩框架研究</title>
<link>https://arxiv.org/abs/2410.21666</link>
<guid>https://arxiv.org/abs/2410.21666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了一种新的压缩框架，适用于重构分布与源分布不一致的情形。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种新型的有损压缩框架，旨在处理重构分布与源分布发生偏差的情况，特别适用于需要联合压缩和检索的应用场景。我们提出的框架扩展了经典的最小熵耦合方法，通过添加瓶颈增强了耦合中的随机性控制。文章将最小熵耦合与瓶颈（MEC-B）分解为两个优化问题：编码器的熵约束信息最大化（EBIM）和解码器的最小熵耦合（MEC）。通过详细分析，我们给出了具有保证性能的EBIM贪婪算法，并对功能映射附近的最优解进行了描述，提供了对问题结构复杂性的深入理论见解。此外，我们通过在限制速率下的马尔可夫编码游戏（MCGs）实验，展示了MEC-B的实际应用。这些实验模拟了一个马尔可夫决策过程中的通信场景，显示出在不同压缩率下MDP奖励与接收者准确度之间的权衡，验证了我们的方法相较于传统压缩基线的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 17:50:02 GMT</pubDate>
</item>
<item>
<title>GlotCC：覆盖1000多种语言的清洁文本语料库</title>
<link>https://arxiv.org/abs/2410.23825</link>
<guid>https://arxiv.org/abs/2410.23825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GlotCC是一个清洁的2TB少数语言语料库，覆盖1000多种语言。</p><br /><br /><p><strong>摘要：</strong> 随着预训练语言模型的发展，对大型文本语料库的需求日益增加。然而，目前的语料库主要集中于拥有较大社区的语言，缺乏覆盖多种少数语言的语料。为此，研究团队推出了GlotCC，一个清洁、文档级的2TB通用语料库，来自CommonCrawl，涵盖超过1000种语言。该数据库采用开放源代码的可重复生成流程，并经过严格的噪声清理，确保其可靠性。GlotCC及其生成系统（包括流水线、语言识别模型和过滤器）已向研究社区开放，促进对少数语言研究的深入开展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 14:34:51 GMT</pubDate>
</item>
<item>
<title>利用丰富语言输入提升强化学习智能体的学习能力</title>
<link>https://arxiv.org/abs/2410.24218</link>
<guid>https://arxiv.org/abs/2410.24218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究不同类型的语言输入对强化学习智能体学习的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过不同类型的语言输入来增强强化学习(RL)智能体的学习能力。尽管近年来在该领域取得了一些进展，但大多数研究仍采用简单的低级指令作为语言输入，未能反映自然人类交流的复杂性。我们重点研究了语言信息量的不同层次（如对过去行为的反馈和未来指导）及语言表达的多样性如何影响智能体的学习与推理。基于四个RL基准的实证结果显示，使用多样且信息丰富的语言反馈训练的智能体在面对新任务时能更好地泛化和快速适应。这些结果强调了在开放世界中，语言使用在教授智能体新任务方面的重要作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 14:10:51 GMT</pubDate>
</item>
<item>
<title>SelfCodeAlign：透明的自我对齐代码语言模型训练管道</title>
<link>https://arxiv.org/abs/2410.24198</link>
<guid>https://arxiv.org/abs/2410.24198</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SelfCodeAlign 提供了一种无需大量人类标注的透明自对齐代码 LLM 训练方法。</p><br /><br /><p><strong>摘要：</strong> SelfCodeAlign 是一种监督性微调方法，可显著提升大型语言模型（LLMs）遵循人类指令的能力。该管道实现了代码 LLM 的自我对齐，且无需广泛的人类标注或蒸馏。SelfCodeAlign 利用相同的基础模型进行推理，首先从高质量的种子代码片段中提取多样化的编码概念，生成新任务。随后为每个任务采样多个响应，并配对测试用例进行验证，最终选取通过示例进行指令调优。在实验中，使用 SelfCodeAlign 和 CodeQwen1.5-7B 生成了 74,000 个指令-响应对的数据集，基于此数据集微调后的模型在 HumanEval+ 上达到了 67.1 的 pass@1，超越了 CodeLlama-70B-Instruct，尽管其体量小十倍。该方法在不同大小的 LLM 中表现出色，表明基础模型在自身数据分布下的对齐效果更佳。同时，SelfCodeAlign 的效果优于直接从其他领先模型进行蒸馏的方法，进一步推动了 StarCoder2-Instruct 的诞生，这是一款完全透明且具有许可证的自我对齐代码 LLM。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24198" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 10:52:22 GMT</pubDate>
</item>
<item>
<title>BitStack：一种新型训练无关权重压缩方法</title>
<link>https://arxiv.org/abs/2410.23918</link>
<guid>https://arxiv.org/abs/2410.23918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BitStack 提供了一种高效的权重压缩技术，优化内存使用与模型性能之间的平衡。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 BitStack，一种新颖的训练无关权重压缩方法，旨在优化大型语言模型（LLMs）的内存使用与性能。随着 LLMs 的广泛应用，内存限制成为部署的主要挑战。传统的压缩方法多依赖预设的压缩比，且需要为每种环境单独进行压缩，导致应用复杂。BitStack 通过权重分解，能够动态调整模型大小，并最小化内存与存储设备之间的传输。该方法在每次分解迭代中考虑参数的重要性，实现每个参数约 1 位的残差块，能够根据当前内存的可用性加载不同数量的块。大量实验表明，在极端压缩比下，BitStack 在保持灵活的大小控制的同时，性能与量化基线相当甚至更优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 08:24:16 GMT</pubDate>
</item>
<item>
<title>多意图任务导向对话系统的研究与实现</title>
<link>https://arxiv.org/abs/2410.22476</link>
<guid>https://arxiv.org/abs/2410.22476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种多意图检测的新方法及数据集。</p><br /><br /><p><strong>摘要：</strong> 在任务导向的对话系统中，意图检测是理解用户查询和提供适当响应的关键。现有研究主要集中于简单查询的单一意图，未能有效处理复杂的多意图查询并提取不同的意图范围。此外，缺乏多语言、多意图的数据集。本研究针对多意图提取、意图检测和开发多语言多标签意图数据集这三项关键任务进行了探讨。我们引入了一种新的多标签多类意图检测数据集（MLMCID-dataset），该数据集从现有基准数据集中整理而来。同时，我们提出了一种基于指针网络的架构（MLMCID），用于提取意图范围并检测具有粗细粒度标签的多意图，形成六元组。综合分析表明，我们的指针网络系统在各类数据集的准确率和F1分数方面优于基准方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 06:28:48 GMT</pubDate>
</item>
<item>
<title>CARE系统：提升大语言模型个性化探索能力</title>
<link>https://arxiv.org/abs/2410.24032</link>
<guid>https://arxiv.org/abs/2410.24032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了CARE系统，通过多智能体框架提升大语言模型的个性化探索能力。</p><br /><br /><p><strong>摘要：</strong> 大语言模型的崛起改变了用户与知识系统的互动方式，但这些系统在处理模糊查询和缺乏上下文信息时，往往难以提供个性化支持。本文提出了旨在增强个性化探索的系统——Collaborative Assistant for Personalized Exploration (CARE)。该系统通过多智能体大语言模型框架和结构化用户界面相结合，设计了包括聊天面板、解决方案面板和需求面板在内的界面，支持迭代查询优化和动态解决方案生成。通过对22名参与者的用户研究，结果显示CARE系统与baseline LLM聊天机器人相比，用户对其更为偏好，尤其赞赏其降低认知负荷、激发创造力和提供定制化解决方案的能力。这一研究展示了CARE将LLM系统从被动信息检索者转变为主动的个性化问题解决与探索伙伴的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 05:22:01 GMT</pubDate>
</item>
<item>
<title>Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2410.22366</link>
<guid>https://arxiv.org/abs/2410.22366</guid>
<content:encoded><![CDATA[
Sparse autoencoders (SAEs) have become a core ingredient in the reverse engineering of large-language models (LLMs). For LLMs, they have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigated the possibility of using SAEs to learn interpretable features for a few-step text-to-image diffusion models, such as SDXL Turbo. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net. We find that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find one block that deals mainly with image composition, one that is mainly responsible for adding local details, and one for color, illumination, and style. Therefore, our work is an important first step towards better understanding the internals of generative text-to-image models like SDXL Turbo and showcases the potential of features learned by SAEs for the visual domain.   Code is available at https://github.com/surkovv/sdxl-unbox
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 05:16:02 GMT</pubDate>
</item>
<item>
<title>层级梯度分析：快思维与慢思维对大型语言模型训练的影响</title>
<link>https://arxiv.org/abs/2410.23743</link>
<guid>https://arxiv.org/abs/2410.23743</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究快思维与慢思维对大型语言模型层级梯度的影响，揭示训练稳定性差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在训练大型语言模型（LLMs）时，快思维与慢思维对不同层梯度的影响。通过分析不同响应和初始模型下的梯度模式，我们发现快速思维（无链式思维）导致更大梯度和层间梯度差异，相较之下，慢思维（详细链式思维）则展现了学习稳定性。此外，预训练的LLMs对快速思维的不稳定性影响较小，而指令调优的LLMs则受其影响较大。我们的研究表明，慢思维的梯度模式能够有效区分正确与无关的推理路径。在非推理知识学习任务中，仅延长响应长度并未展现出相同的慢思维行为。这些发现不仅加强了对LLM训练的基础理解，也为构建可泛化的系统-2智能体开辟了新的思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23743" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 04:18:28 GMT</pubDate>
</item>
<item>
<title>DELTA: Dense Efficient Long-range 3D Tracking for any video</title>
<link>https://arxiv.org/abs/2410.24211</link>
<guid>https://arxiv.org/abs/2410.24211</guid>
<content:encoded><![CDATA[
Tracking dense 3D motion from monocular videos remains challenging, particularly when aiming for pixel-level precision over long sequences. We introduce \Approach, a novel method that efficiently tracks every pixel in 3D space, enabling accurate motion estimation across entire videos. Our approach leverages a joint global-local attention mechanism for reduced-resolution tracking, followed by a transformer-based upsampler to achieve high-resolution predictions. Unlike existing methods, which are limited by computational inefficiency or sparse tracking, \Approach delivers dense 3D tracking at scale, running over 8x faster than previous methods while achieving state-of-the-art accuracy. Furthermore, we explore the impact of depth representation on tracking performance and identify log-depth as the optimal choice. Extensive experiments demonstrate the superiority of \Approach on multiple benchmarks, achieving new state-of-the-art results in both 2D and 3D dense tracking tasks. Our method provides a robust solution for applications requiring fine-grained, long-term motion tracking in 3D space.
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 03:24:24 GMT</pubDate>
</item>
<item>
<title>Self-Lengthen：提升大语言模型长文本生成能力的新框架</title>
<link>https://arxiv.org/abs/2410.23933</link>
<guid>https://arxiv.org/abs/2410.23933</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Self-Lengthen框架，提升大语言模型的长文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 近期在大语言模型（LLMs）方面的进展显著提升了其处理长文本的能力，但在生成长且一致的输出方面仍存在明显不足。这一限制源于训练期间缺乏针对长文本生成的有效指导，而后期训练数据主要由短期问答对构成。为了解决这一问题，本文提出了一种创新的迭代训练框架Self-Lengthen，该框架仅利用LLMs的内在知识和技能，无需辅助数据或专有模型。该框架由生成器和扩展器两个角色组成，生成器负责生成初步回复，随后由扩展器对回复进行拆分和扩展，进而形成新的、更长的回复，通过这种方式，模型能够逐步学习处理日益增长的响应长度。实验结果表明，Self-Lengthen在长文本生成性能上优于现有方法，尤其在应用于Qwen2和LLaMA3等顶级开源LLMs时表现突出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23933" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 01:33:22 GMT</pubDate>
</item>
<item>
<title>基于约束回译的数据生成技术提升复杂指令遵循能力</title>
<link>https://arxiv.org/abs/2410.24175</link>
<guid>https://arxiv.org/abs/2410.24175</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新技术，通过约束回译生成高质量复杂指令响应数据集CRAB。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在遵循复杂指令时的局限性，并提出了一种名为约束回译的新数据生成技术。该方法利用现有高质量指令响应对，仅通过先进的LLMs添加已经由响应满足的复杂约束，从而有效降低成本与数据噪声。实验中，使用Llama3-70B-Instruct进行约束回译，生成了一个名为CRAB的高质量复杂指令响应数据集。结果表明，基于CRAB进行后训练能显著提升多个基础LLMs的复杂指令遵循能力，经过广泛的指令遵循基准评估。此外，约束回译还被发现是一个有效的辅助训练目标。研究代码、数据和模型将公开，旨在促进未来的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24175" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 01:27:02 GMT</pubDate>
</item>
<item>
<title>BenchX：统一的医学视觉语言预训练基准框架</title>
<link>https://arxiv.org/abs/2410.21969</link>
<guid>https://arxiv.org/abs/2410.21969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchX是一个统一的基准框架，促进医学视觉语言预训练方法的比较与分析。</p><br /><br /><p><strong>摘要：</strong> 本文提出了BenchX，一个针对医学视觉语言预训练（MedVLP）方法的统一基准框架，旨在填补当前不同MedVLP方法在数据集、预处理和微调实现上的差异所带来的评估挑战。BenchX包括三个主要组成部分：涵盖九个数据集和四个医学任务的综合数据集、标准化的数据预处理、训练-测试拆分和参数选择的基准套件，以及适应不同MedVLP方法的一致微调协议。此外，通过使用BenchX，我们为九种最先进的MedVLP方法建立了基准，并发现一些早期的MedVLP方法在性能上得到了增强，超过了更新的版本，从而促使对先前MedVLP工作的开发与结论进行重新审视。这一框架为将来的研究提供了参考与基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 01:14:45 GMT</pubDate>
</item>
<item>
<title>从合成视频和自然图像学习有效视频表示</title>
<link>https://arxiv.org/abs/2410.24213</link>
<guid>https://arxiv.org/abs/2410.24213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过合成视频学习有效视频表示的新方法，无需自然视频训练。</p><br /><br /><p><strong>摘要：</strong> 本文展示了如何利用合成视频和自然图像学习有效的视频表示，而不需要自然视频的训练。我们提出了一系列通过简单生成过程合成的视频数据集，这些数据集模拟了一组不断增长的自然视频特性（如运动、加速度和形状变换）。在这些生成的数据集上进行预训练的视频模型，其下游性能随着数据集的发展逐渐提高。特别是，预训练于合成视频的VideoMAE模型在UCF101动作分类上缩小了97.2%的性能差距，相较于从零开始训练，且在HMDB51数据集上表现优于预训练模型。此外，向预训练阶段引入静态图像裁剪的方式，使得其性能与UCF101预训练相当，并在UCF101-P的14个分布外数据集中优于UCF101预训练模型。通过分析数据集的低级属性，我们还识别出帧多样性、帧与自然数据的相似性与下游性能之间的相关性。我们的研究提供了一种更可控、透明的视频数据预训练替代方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.24213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 01 Nov 2024 00:27:58 GMT</pubDate>
</item>
<item>
<title>SlowFast-VGen：一种新型双速学习系统用于长视频生成</title>
<link>https://arxiv.org/abs/2410.23277</link>
<guid>https://arxiv.org/abs/2410.23277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SlowFast-VGen，一个双速学习系统用于促进长视频生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SlowFast-VGen，一种创新的双速学习系统，旨在改善长视频生成中的情节一致性。传统模型通常聚焦于缓慢学习，忽视了快速学习阶段，这导致在生成长视频时，时间上较为遥远的帧之间存在不一致。SlowFast-VGen结合了一个用于世界动态缓慢学习的掩码条件视频扩散模型，及一个基于时序LoRA模块的快速学习策略，使得其在推理时能够高效地存储情节记忆。我们还提出了一个慢-快学习循环算法，能无缝集成内部快速学习循环与外部缓慢学习循环，支持上下文感知的技能学习。此外，研究团队收集了包括20万段视频及语言行动注释的庞大数据集，经过广泛实验，SlowFast-VGen在各项指标上超越了基线模型，有效提升了长时间规划任务的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 17:10:24 GMT</pubDate>
</item>
<item>
<title>大语言模型的记忆化与推理能力的复杂性</title>
<link>https://arxiv.org/abs/2410.23123</link>
<guid>https://arxiv.org/abs/2410.23123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大语言模型在推理中依赖于记忆，同时也具备推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在推理能力上的表现及其背后的机制，特别是其记忆化倾向的影响。我们通过基于骑士与骗子（K&amp;K）难题的动态生成逻辑推理基准，系统性地测量了推理任务的记忆化程度。研究发现，尽管LLMs经过微调后能在训练难题上取得接近完美的准确率，但在轻微扰动的情况下却常常失败，这表明它们在解决这些训练难题时严重依赖记忆。同时，虽然微调过程导致了显著的记忆化，但也提升了模型的泛化能力。通过对扰动测试、不同难度级别的迁移性、模型内部探测和错误回答微调进行深入分析，我们验证了LLMs在K&amp;K难题中能够学习推理，而非单纯依赖记忆。我们的研究结果揭示了LLMs在逻辑难题解决过程中，推理能力与记忆的复杂相互作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 16:59:58 GMT</pubDate>
</item>
<item>
<title>通过眼动模式解码阅读目标的可能性研究</title>
<link>https://arxiv.org/abs/2410.20779</link>
<guid>https://arxiv.org/abs/2410.20779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探索眼动模式是否能解码信息检索与普通阅读的目标。</p><br /><br /><p><strong>摘要：</strong> 本研究首次探讨了是否能够从眼动模式中解码两种常见的阅读目标：信息检索和普通阅读。我们使用大规模的眼动追踪数据，采用多种先进的模型来分析眼动与文本之间的关系，并引入了一种新的模型集成方法。研究系统地评估了模型在三种泛化级别上的表现：新文本、新参与者以及两者结合。结果表明，眼动信息在解码阅读目标中具有重要价值。此外，我们还进行了一项错误分析，借助于已有的实证研究，重点关注普通阅读与信息检索之间的差异，并利用丰富的文本注释，揭示了影响任务难度的关键文本特性和参与者眼动模式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 14:43:49 GMT</pubDate>
</item>
<item>
<title>REM框架：基于自然语言的视频概念分割</title>
<link>https://arxiv.org/abs/2410.23287</link>
<guid>https://arxiv.org/abs/2410.23287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REM框架通过自然语言描述实现视频概念的准确分割与追踪。</p><br /><br /><p><strong>摘要：</strong> REM是一种新型框架，旨在通过自然语言描述对视频中的多种概念进行分割。该方法利用视频扩散模型在互联网规模数据集上学习的视觉-语言表示，保持生成模型原始表示的完整性，并在狭域的Referral Object Segmentation数据集上进行微调。REM不仅能够精确分割和追踪稀有和未见物体，还能概括非物体动态概念，如海浪的运动。我们引入的新基准Referral Video Process Segmentation (Ref-VPS) 验证了这一能力。在实验中，REM在领域内数据集如Ref-DAVIS上与当前最先进的方法表现持平，在域外数据上通过互联网规模的预训练提升了区域相似性，最高超出其他方法十二个百分点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 14:24:01 GMT</pubDate>
</item>
<item>
<title>开放数据的有毒内容过滤新方法</title>
<link>https://arxiv.org/abs/2410.22587</link>
<guid>https://arxiv.org/abs/2410.22587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出针对开放数据的新型有毒内容过滤管道。</p><br /><br /><p><strong>摘要：</strong> 随着开放源代码大语言模型的普及，安全性问题日益受到关注。本文提出了一种数据策划管道，以减少在公共领域数据上训练的模型的有害输出。针对公共领域数据的独特挑战，研究人员创建了一个名为ToxicCommons的自定义训练数据集，涵盖五种不同维度的有毒内容分类。利用该数据集，作者训练了一个名为Celadon的定制分类器，旨在更高效地检测开放数据中的有毒内容。最后，文章描述了一种平衡的内容过滤方法，优化了训练可用过滤数据的安全性过滤。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 12:27:05 GMT</pubDate>
</item>
<item>
<title>TokenFormer：一种可扩展的变换器架构解决方案</title>
<link>https://arxiv.org/abs/2410.23168</link>
<guid>https://arxiv.org/abs/2410.23168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokenFormer通过新的参数-令牌注意机制减少训练成本，提升变换器架构的灵活性。</p><br /><br /><p><strong>摘要：</strong> TokenFormer是一种新型的可扩展架构，旨在解决当前变换器模型在扩展过程中面临的高计算成本问题。传统变换器模型的扩展依赖于固定参数数量，进行架构修改时需要从头重新训练，导致成本高昂且不可持续。TokenFormer通过引入参数-令牌注意机制，使模型参数被视为令牌，替代了变换器中的线性投影，从而在不重新训练的情况下实现高效扩展。该模型支持从124M扩展到1.4B参数，并可以逐步添加新的键-值参数对，性能与完全从头训练的变换器相当，同时显著降低了训练成本。相关代码和模型可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 09:59:54 GMT</pubDate>
</item>
<item>
<title>通过专家选择路由攻击Mixture-of-Experts模型以披露用户提示</title>
<link>https://arxiv.org/abs/2410.22884</link>
<guid>https://arxiv.org/abs/2410.22884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新攻击，利用MoE模型的结构缺陷提取用户提示。</p><br /><br /><p><strong>摘要：</strong> 本文展示了一种对Mixture-of-Experts (MoE)模型的攻击方法，该方法通过将恶意查询与受害者的查询放在同一批次中，成功利用了专家选择路由（Expert-Choice-Routing），从而完全披露受害者的提示。我们在一个两层的Mixtral模型上成功演示了这一攻击，利用torch.topk的tie-handling行为。研究表明，通过O({VM}^2)个查询（其中V为词汇大小，M为提示长度），或者在我们考虑的设置中平均每个令牌100个查询，可以提取整个提示。这是首次利用架构缺陷提取用户提示的攻击，从而引入了一类新的大语言模型（LLM）漏洞。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 07:03:18 GMT</pubDate>
</item>
<item>
<title>基于xLSTM的大型重复动作模型研究</title>
<link>https://arxiv.org/abs/2410.22391</link>
<guid>https://arxiv.org/abs/2410.22391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于xLSTM的大型重复动作模型，具备快速推理和优良表现。</p><br /><br /><p><strong>摘要：</strong> 近年来，强化学习领域出现了一种趋势，旨在通过序列建模在大规模数据集上离线训练大动作模型。现有模型主要基于Transformer架构，尽管能产生强大的智能体，但由于推理速度较慢，Transformer方法在实际应用中受到限制，尤其是在机器人领域。为此，研究者提出现代递归架构，如xLSTM和Mamba，这些架构在训练中具备类似于Transformer的并行化优势，同时提供快速推理能力。本文研究了这些现代递归架构在大型动作模型中的适应性，并提出了一种以xLSTM为核心的大型递归动作模型（LRAM），其具有线性时间的推理复杂度和自然的序列长度外推能力。通过在6个领域的432个任务上进行实验，LRAM在性能和速度上都表现出色，优于传统的Transformer方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 06:40:34 GMT</pubDate>
</item>
<item>
<title>CORAL: 针对多轮对话的检索增强生成基准</title>
<link>https://arxiv.org/abs/2410.23090</link>
<guid>https://arxiv.org/abs/2410.23090</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CORAL是一个用于评估多轮对话检索增强生成系统的大规模基准。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）已成为提升大型语言模型（LLMs）的强大范式，通过外部知识检索来增强性能。然而，现有研究主要集中在单轮RAG上，缺乏对现实世界多轮对话复杂性的探讨。为弥补这一空缺，本文介绍了CORAL，一个旨在评估RAG系统在多轮对话环境中的表现的大规模基准。CORAL包含自动从维基百科衍生的多样化信息检索对话，解决了开放领域覆盖、知识密集度、自由格式响应和主题转换等关键挑战。该基准支撑三项核心任务：文段检索、响应生成和引用标注。我们提出了一个统一框架，以标准化各种对话RAG方法，并在CORAL上对这些方法进行了全面评估，显示出显著的改进机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.23090" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 01:48:44 GMT</pubDate>
</item>
<item>
<title>自学习假设文档嵌入法在医疗信息检索中的应用</title>
<link>https://arxiv.org/abs/2410.20050</link>
<guid>https://arxiv.org/abs/2410.20050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍了一种新方法SL-HyDE，用于改进医疗信息检索的零-shot效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法，称为自学习假设文档嵌入法（SL-HyDE），旨在解决医疗领域缺乏相关性标记数据的问题，以提升零-shot密集检索的有效性。SL-HyDE利用大型语言模型生成假设文档，这些文档基于给定查询生成，封装了关键的医疗背景信息，辅助密集检索器识别最相关的文档。该自学习框架逐步完善伪文档的生成和检索，使用未标记的医疗语料库，避免了依赖任何相关性标记数据。此外，文章还介绍了中文医疗信息检索基准（CMIRB），这是一个基于真实医疗情境的综合评估框架，包括五个任务和十个数据集。通过对CMIRB上十种模型的基准测试，建立了医疗信息检索系统的严格评估标准。实验结果表明，SL-HyDE在检索准确性方面显著超越现有方法，并在不同的LLM和检索器配置中展现出强大的泛化能力和可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Oct 2024 00:46:54 GMT</pubDate>
</item>
<item>
<title>视觉语言模型中任务表示的内部机制研究</title>
<link>https://arxiv.org/abs/2410.22330</link>
<guid>https://arxiv.org/abs/2410.22330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明视觉语言模型通过共享任务向量有效编码不同模态和任务指定。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉语言模型（VLMs）内部如何编码任务表示，重点分析了任务通过示例或指令进行指定的方式。我们的研究发现，概念上相似的任务即使以不同的方式指定，其任务向量表示也会相似。VLMs的输出过程可分为输入、任务和答案三个distinct阶段，这一过程在不同模态和任务规范中表现一致。此外，我们发现通过组合示例和指令基础的任务向量可以生成更优的任务表示。这些发现揭示了VLMs的基本机制，特别是在不同模态和任务规范中以共享方式表示任务的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 22:09:38 GMT</pubDate>
</item>
<item>
<title>REPOCOD：评估大型语言模型在真实代码生成中的应用</title>
<link>https://arxiv.org/abs/2410.21647</link>
<guid>https://arxiv.org/abs/2410.21647</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REPOCOD基准测试展示LLMs在真实软件开发中的局限性。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在Python代码生成中表现优异，但现有简单的生成基准无法解答它们是否能替代人类程序员。为此，提出了REPOCOD基准，包含来自11个真实项目的980个问题，其中超过58%需要文件或仓库级别的上下文信息。REPOCOD在平均解决方案长度（331.6个标记）和圈复杂度（9.00）上均超越了现有基准。我们的评估显示，十种LLMs在REPOCOD上的性能均未超过30 pass@1，揭示出构建更强大LLMs以支持真实软件开发的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21647" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 20:02:23 GMT</pubDate>
</item>
<item>
<title>引入前缀共享的偏好调优新方法</title>
<link>https://arxiv.org/abs/2410.20305</link>
<guid>https://arxiv.org/abs/2410.20305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种前缀共享的偏好调优方法，提升训练效率。</p><br /><br /><p><strong>摘要：</strong> 随着离线配对偏好优化算法在偏好数据细调中的广泛应用，传统的监督细调方法在多个任务上表现不佳，且在使用长共享提示时常出现冗余计算问题。为此，本文提出了一种新方法——前缀共享偏好调优，通过将选择和拒绝的响应作为一个序列处理，并使用自定义块稀疏注意力掩码以防止响应之间的交叉污染，从而在不影响收敛性的前提下，实现了在主流DPO数据集上训练吞吐量提升1.1-1.5倍。结合序列打包后，我们观察到在小序列长度的数据集中，速度提升更为显著，达到1.3-1.6倍。虽然本文主要集中于直接偏好优化（DPO），但该方法同样适用于其他配对偏好调优技术。通过提高计算效率，我们的工作旨在使基于偏好的细调方法更广泛、更多样化应用于不同规模的模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 16:10:05 GMT</pubDate>
</item>
<item>
<title>利用相关反馈的真实文档嵌入优化稠密检索系统</title>
<link>https://arxiv.org/abs/2410.21242</link>
<guid>https://arxiv.org/abs/2410.21242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReDE-RF方法以提高低资源环境下的稠密文档检索效果。</p><br /><br /><p><strong>摘要：</strong> 在缺乏相关性监督的情况下，构建有效的稠密检索系统面临挑战。近期研究利用大型语言模型（LLM）生成假设文档以寻找最相关的真实文档，但此方法依赖于LLM的领域知识且生成假设文档效率低下。为解决这些问题，本文提出了“真实文档嵌入的相关反馈”（ReDE-RF）方法，从相关反馈中汲取灵感，将假设文档生成重新构建为相关性估计任务，LLM只需选择相关文档以进行最近邻检索，从而不再需要领域特定知识，且提高了检索延迟。实验证明ReDE-RF在多种低资源检索数据集上表现优于现有最先进的零样本稠密检索方法，同时显著降低了每次查询的延迟。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 16:01:12 GMT</pubDate>
</item>
<item>
<title>新方法评估大语言模型的记忆化风险</title>
<link>https://arxiv.org/abs/2410.19482</link>
<guid>https://arxiv.org/abs/2410.19482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新的概率方法评估大语言模型的记忆化率。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型(LLMs)对训练数据的记忆能力愈发引发担忧，现有的记忆化率测量方法主要依赖单一序列贪婪采样，这可能低估了真实的记忆化程度。本文提出了一种可量化目标序列提取概率的新方法，通过考虑多种采样方案和多次尝试，克服了现有方法的局限性。这种基于概率的衡量方式揭示了与通过可发现提取得到的记忆化率相比，可能存在更高的记忆化率。研究还探讨了不同采样方案对提取能力的影响，为LLM的记忆化及其潜在风险提供了更全面的评估。我们的贡献包括提出新的记忆化定义、实证证据以及对不同模型、规模、采样方案和训练数据重复的全面评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 15:48:20 GMT</pubDate>
</item>
<item>
<title>利用RARe方法提升嵌入模型在检索任务中的性能</title>
<link>https://arxiv.org/abs/2410.20088</link>
<guid>https://arxiv.org/abs/2410.20088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明RARe方法能提高嵌入模型在检索中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在解码器语言模型中广泛使用的上下文示例是否能提升嵌入模型在检索任务中的性能。我们提出了一种简单的方法RARe，旨在利用语义相似的上下文示例对预训练模型进行微调。该方法可以适用于多种基础架构，实验结果表明，在多个开放域检索数据集上的nDCG性能提升可达2.72%。特别是，RARe展示了相较于不使用上下文示例的模型更强的领域外泛化能力，这与解码器语言模型中的上下文学习结果相似。文章还分析了上下文示例增强的设计选择，为未来的研究奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 14:31:13 GMT</pubDate>
</item>
<item>
<title>链式推理对大型语言模型性能影响的研究</title>
<link>https://arxiv.org/abs/2410.21333</link>
<guid>https://arxiv.org/abs/2410.21333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究链式推理在特定任务中对语言模型性能的负面影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了链式推理（CoT）在应用于大型语言和多模态模型时的有效性，尤其是识别其对模型性能的负面影响的任务特征。通过借鉴认知心理学的案例，作者分析了在某些情况下，语言表达和深思熟虑对人类表现造成负面效应，并推测这些限制同样适用于语言模型。研究涵盖了三种任务：隐性统计学习、视觉识别以及包含例外模式的分类。在广泛的实验中，结果显示多种最先进模型在推理时间推理时性能显著下降，例如，OpenAI o1-preview相较于GPT-4o的准确率下降高达36.3%。此外，作者也识别了三种只满足人类表现条件（i）而不满足条件（ii）的任务，发现尽管在这些任务中语言思维降低了人类表现，但CoT保持或提高了模型表现。总体而言，研究表明，虽然模型的认知过程与人类不完全相同，但考虑到思维对人类表现的负面影响可以帮助识别其对模型的负面影响，从而为理解提示选择和推理时间推理的影响提供了新思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 12:21:23 GMT</pubDate>
</item>
<item>
<title>CLEAR: 跨模态遗忘的新基准与挑战</title>
<link>https://arxiv.org/abs/2410.18057</link>
<guid>https://arxiv.org/abs/2410.18057</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了CLEAR基准，以评估多模态机器遗忘方法。</p><br /><br /><p><strong>摘要：</strong> 机器遗忘在深度学习模型中对于增强隐私和安全性至关重要，尤其在大型多模态语言模型中，通过去除特定的私有或有害信息来实现。尽管在文本和视觉模态方面已有显著进展，但多模态遗忘（MMU）却仍然缺乏探索，部分原因在于缺少合适的开放源基准。为此，本文引入CLEAR，一个新基准，旨在评估MMU方法。CLEAR包含200个虚构个体和3700张与相应问答对相关联的图像，能够全面评估不同模态的遗忘能力。我们评估了10种机器遗忘方法，并对此进行了适应以适合MMU，并突出显示了多模态遗忘特有的新挑战。研究表明，在LoRA权重上使用简单的l1正则化显著减轻了灾难性遗忘，保持了模型在保留数据上的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18057" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 05:21:40 GMT</pubDate>
</item>
<item>
<title>自主多模态网络代理的开发框架</title>
<link>https://arxiv.org/abs/2410.19609</link>
<guid>https://arxiv.org/abs/2410.19609</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种用于自主多模态网络代理的开发框架。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型和多模态模型的快速发展，尤其是像GPT-4o这样的专有模型，引发了对开发能够处理现实世界场景的自主代理的极大兴趣。尽管近期的开源努力试图为代理赋予在环境中探索和自我改进的能力，但大多数仍局限于在合成环境中构建仅文本的代理，这些环境中的奖励信号清晰可定义。这些代理在缺乏明确基准信号的实际设置中难以泛化。本文提出了一种开源框架，旨在促进多模态网络代理的开发，使其能够自主进行现实世界的探索并自我提升。我们首先使用模仿学习训练基础模型以获取基本能力，然后让代理探索开放网络并收集其轨迹反馈。之后，代理通过从另一通用模型评判的高表现轨迹中学习，进一步优化其策略。这个探索-反馈-优化的循环可以持续进行多次迭代。实验结果显示，我们的网络代理在每次迭代后成功自我提升，在多个测试集上表现强劲。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19609" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 03:04:13 GMT</pubDate>
</item>
<item>
<title>AutoKaggle：数据科学任务的智能协作框架</title>
<link>https://arxiv.org/abs/2410.20424</link>
<guid>https://arxiv.org/abs/2410.20424</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoKaggle通过协作多智能体系统提升数据科学任务的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AutoKaggle的框架，旨在帮助数据科学家解决涉及表格数据的复杂任务。该框架采用协作多智能体系统，通过迭代开发流程来整合代码执行、调试和全面的单元测试，确保代码的正确性和逻辑一致性。AutoKaggle提供高度可定制的工作流程，允许用户在每个阶段进行干预，从而将自动智能与人类专业知识相结合。我们采用8个Kaggle竞赛模拟真实应用场景的数据处理工作流，结果显示AutoKaggle在数据科学典型流程中实现了0.85的验证提交率和0.82的综合得分，从而证明了其在处理复杂数据科学任务中的有效性和实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20424" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 02:16:36 GMT</pubDate>
</item>
<item>
<title>社交关系推理框架：结合视觉基础模型与大型语言模型</title>
<link>https://arxiv.org/abs/2410.21411</link>
<guid>https://arxiv.org/abs/2410.21411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将视觉模型与语言模型结合用于社交关系推理。</p><br /><br /><p><strong>摘要：</strong> 社交关系推理的目标是从图像中识别朋友、配偶和同事等关系类别。现有方法在通用性和可解释性方面存在局限。为此，本文提出了一个名为{
ame}的框架，结合视觉基础模型（VFM）和大型语言模型（LLM），在模块化框架中提供社交关系识别的强基线。具体而言，该框架通过VFM将图像内容转换为文本的社交故事，然后利用LLM进行基于文本的推理。{
ame}引入系统设计原则以分别适应VFM和LLM，并弥合其差距。结果表明，即使没有额外模型训练，该方法在两个数据库上实现了竞争性的零-shot结果，并且通过LLM生成语言解释的决策提供了可解释的答案。此外，手动设计LLM推理阶段的提示过程繁琐，因此需要一种自动提示优化方法。为了解决这个独特的长提示优化问题，本文进一步提出了贪婪段提示优化（GSPO），通过在段级别利用梯度信息进行贪婪搜索。实验结果表明，GSPO显著改善了性能，并且该方法在不同图像风格下也具有良好的通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 01:47:45 GMT</pubDate>
</item>
<item>
<title>基于操作中心性表示的机器人视觉预训练研究</title>
<link>https://arxiv.org/abs/2410.22325</link>
<guid>https://arxiv.org/abs/2410.22325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出操作中心性表示框架，提升机器人视觉预训练效果。</p><br /><br /><p><strong>摘要：</strong> 预训练视觉表示显著提高了机器人学习的效率，但由于缺乏大规模的领域内数据，现有研究往往依赖于野外人类视频进行预训练。然而，这些人类视频的表示受分布偏移影响，且缺乏任务完成所需的动态信息。本文评估了不同预训练表示与机器人的下游操作任务的相关性，发现“操作中心性”是任务成功率的重要指标。基于此发现，我们提出了一种操作中心性表示（MCR）框架，捕捉操作任务的视觉特征和动态信息。MCR通过使用DROID机器人数据集进行视觉编码预训练，利用机器人自我感知状态和行动的相关数据，并引入新颖的对比损失函数，结合行为克隆损失和时间对比损失。实验结果表明，MCR在四个模拟领域的实验任务中 outperforming 了最强基准方法14.8%，同时在三项现实世界任务中提升了数据效率学习的效果达76.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22325" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 01:23:17 GMT</pubDate>
</item>
<item>
<title>基于在线学习流的高质量推理轨迹生成方法</title>
<link>https://arxiv.org/abs/2410.22304</link>
<guid>https://arxiv.org/abs/2410.22304</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法，利用在线学习生成高质量的推理轨迹以提高LLM的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 数学推理能力对于大语言模型（LLMs）至关重要，然而生成详细且准确的推理轨迹仍然是一个重大挑战。本文介绍了一种新颖的方法，通过使用在线学习流，生成高质量的推理轨迹以进行LLM的微调。该方法采用增量输出生成流，组件LLMs通过迭代通信协作构建解决方案。我们使用在线直接偏好优化（DPO）学习和回滚进行流的训练，为每个训练样本生成DPO对并实时更新模型。通过与直接模型推理生成的推理轨迹进行比较，研究表明该方法在提升LLM在数学推理任务中的表现方面具有显著效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.22304" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 00:46:03 GMT</pubDate>
</item>
<item>
<title>ShadowKV：高吞吐量长上下文LLM推理系统</title>
<link>https://arxiv.org/abs/2410.21465</link>
<guid>https://arxiv.org/abs/2410.21465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ShadowKV通过低秩关键缓存和精确KV选择策略提升LLM推理效率。</p><br /><br /><p><strong>摘要：</strong> 随着长上下文大语言模型（LLM）的广泛应用，对高吞吐量推理的需求日益增长。然而，随着序列长度的增加，关键-值（KV）缓存的内存占用和每次生成token时的访问需求导致了低吞吐量的问题。为解决这一问题，我们提出了ShadowKV，一个高吞吐量的长上下文LLM推理系统。该系统通过存储低秩关键缓存，并将值缓存转移，以降低内存占用，实现更大的批处理和更长的序列长度。为了最小化解码延迟，ShadowKV采用了准确的KV选择策略，能够动态重构稀疏KV对。经过在多个基准测试（如RULER、LongBench和Needle In A Haystack）及多种模型上（包括Llama-3.1-8B、GLM-4-9B-1M等）的评估，ShadowKV能够支持高达6倍的批处理规模并在A100 GPU上提高3.04倍的吞吐量，同时不牺牲准确性，甚至超过在假设无限GPU内存下能够实现的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 00:26:43 GMT</pubDate>
</item>
<item>
<title>基于人机协同的视觉强化学习在复杂机器人操作中的应用</title>
<link>https://arxiv.org/abs/2410.21845</link>
<guid>https://arxiv.org/abs/2410.21845</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍一种强化学习系统，展示了高效的机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> 强化学习（RL）为自主获取复杂机器人操作技能提供了巨大潜力，但在现实环境中实现这一潜力面临挑战。我们提出了一种基于人机协同的视觉强化学习系统，在动态操作、精密组装和双臂协调等多样化的灵巧操作任务中展现出卓越表现。该方法结合了演示和人类修正、高效的RL算法以及其他系统设计选择，能够在1到2.5小时的训练中达到接近完美的成功率和快速的周期时间。我们的研究表明，该方法显著超越了模仿学习基线和以往的RF方法，成功率平均提高了2倍，执行速度提高了1.8倍。通过广泛的实验和分析，我们提供了对该方法有效性的深入见解，并展示了其在反应控制和预测控制策略中学习到的强健和自适应策略。这项工作启示了新一代的学习型机器人操作技术，旨在促进工业应用和研究进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21845" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Oct 2024 00:22:10 GMT</pubDate>
</item>
<item>
<title>EoRA：一种无训练的低秩近似模型压缩补偿方法</title>
<link>https://arxiv.org/abs/2410.21271</link>
<guid>https://arxiv.org/abs/2410.21271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出EoRA方法以补偿压缩模型造成的错误，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本研究将模型压缩问题重新定义为定制补偿问题，提出一种名为Training-free Eigenspace Low-Rank Approximation（EoRA）的方法。EoRA通过直接最小化压缩引起的错误来补偿压缩模型的缺陷，而无需梯度训练，能够在短时间内使用少量校准数据实现快速优化。该方法利用输入激活的特征空间，将压缩误差投影并根据特征值优先重建重要的误差成分。EoRA的优势在于与微调和量化的无缝结合，能够显著提升补偿效果。其在LLaMA2/3模型上针对语言生成、常识推理和数学推理等多项任务表现优异，取得了如ARC-Easy/ARC-Challenge和MathQA任务中显著的性能提升（例如，LLaMA3-8B在量化至4-bit和剪枝为2:4稀疏时的31.31%和12.88%的提升）。EoRA为在不同容量和效率需求下部署大型语言模型提供了一种可扩展且高效的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 23:57:10 GMT</pubDate>
</item>
<item>
<title>基于双层优化的模仿学习框架改善类人机器人运动模仿</title>
<link>https://arxiv.org/abs/2410.01968</link>
<guid>https://arxiv.org/abs/2410.01968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过双层优化框架改善类人机器人运动模仿的有效性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于双层优化的模仿学习框架，用于改善类人机器人从人类运动捕捉数据（MoCap）中学习的有效性。由于机器人与人类在形态上的差异，如关节自由度和力限制，直接复现人类行为存在挑战。因此，使用不切实际的运动捕捉数据可能会对机器人政策的表现产生负面影响。本研究开发了一种生成性的潜在动力学模型，采用新颖的自一致性自编码器，使其能够学习稀疏且结构化的运动表示，同时捕捉数据集中期望的运动模式。经过模拟实验，结果显示该方法通过调整参考运动以确保物理一致性，从而提升了机器人政策的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.01968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 16:56:34 GMT</pubDate>
</item>
<item>
<title>VideoWebArena: 评估长文本视频理解的基准测试</title>
<link>https://arxiv.org/abs/2410.19100</link>
<guid>https://arxiv.org/abs/2410.19100</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VideoWebArena基准，评估长文本视频理解的多模态代理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoWebArena（VideoWA），该基准旨在评估长文本多模态代理在视频理解中的能力。VideoWA划分了长文本视频代理任务，包括技能保留和事实保留两大类，共包含2,021个基于手工制作视频教程的代理任务，视频总时长近4小时。研究发现，最佳模型在事实保留任务上的成功率为13.3%，在事实保留问答对上的成功率为45.8%，远低于人类绩效的73.9%和79.3%。在技能保留任务上，长文本模型在使用教程时的表现较差，相较于不使用教程，WebArena任务表现下降了5%，VisualWebArena任务下降了10.3%。这项工作强调了提升长文本多模态模型代理能力的必要性，并为未来长文本视频代理的发展提供了测试平台。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19100" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 15:27:01 GMT</pubDate>
</item>
<item>
<title>递归变换器：参数共享和性能优化的新方法</title>
<link>https://arxiv.org/abs/2410.20672</link>
<guid>https://arxiv.org/abs/2410.20672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出递归变换器，通过参数共享有效减少大语言模型的规模与成本。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨大语言模型（LLMs）部署的高成本问题，提出一种通过参数共享减少模型规模和成本的方案，即“递归变换器”。我们对变换器中的“层绑定”技术进行了重新审视，并提出了将现有LLM高效转换为共享层参数的递归变换器的新方法，最大限度地减少性能损失。这些递归变换器从标准的预训练变换器高效初始化，使用单个独特层块，通过循环重复使用，从而实现紧凑的结构。为进一步增强性能，研究引入了放松递归变换器，通过深度低秩适配（LoRA）模块增加层绑定约束的灵活性，同时保留模型的紧凑性。实验结果表明，递归模型在保持规模相似的情况下超越了传统预训练模型和知识蒸馏基准，并能够恢复大部分原始全尺寸模型的性能。最后，我们提出了连续深度批处理这一新推理范式，结合早期退出原理，理论分析显示其推理吞吐量有潜在的2-3倍提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 15:24:15 GMT</pubDate>
</item>
<item>
<title>从无标注对话中高效推导结构化工作流的研究</title>
<link>https://arxiv.org/abs/2410.18481</link>
<guid>https://arxiv.org/abs/2410.18481</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了如何从无标注对话中高效推导结构化工作流的D2F嵌入方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了从无标注对话中推导结构化工作流的挑战，并提出了一种新的D2F嵌入方法。D2F嵌入通过将对话发言映射到潜在空间，不同于传统句子嵌入，更有效地根据其交际和信息功能进行分组。D2F的特点在于能够将对话建模为潜在空间中的连续轨迹，并通过聚类操作将潜在空间量化，便于提取潜在工作流。为预训练D2F，研究者汇总了二十个任务导向的对话数据集，并引入了一种新颖的软对比损失，以利用行动的语义信息指导表示学习过程。经过多种句子嵌入方法的评估，D2F在多样化领域中展现出更优的定性和定量结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18481" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 12:06:28 GMT</pubDate>
</item>
<item>
<title>SGRv2：一种提高样本效率的模仿学习框架</title>
<link>https://arxiv.org/abs/2406.10615</link>
<guid>https://arxiv.org/abs/2406.10615</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SGRv2通过改善视觉和动作表示来提升机器人的样本效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SGRv2的模仿学习框架，旨在通过改进视觉和动作表示来增强样本效率。SGRv2的关键设计在于引入了一种重要的归纳偏置——动作局部性，这一理论认为机器人的动作主要受到目标物体及其与周围环境交互的影响。通过在模拟和真实环境中进行的广泛实验，验证了动作局部性在提高样本效率中的重要性。在RLBench任务中，SGRv2仅使用5个演示就能在关键帧控制中表现优异，并在26个任务中超过RVT基线的23个。此外，在ManiSkill2和MimicGen的稠密控制评估中，SGRv2的成功率是SGR的2.54倍。在真实环境中，SGRv2能在仅使用八个演示的情况下，完成多种任务，并以显著更高的成功率超越基线模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2406.10615" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 08:43:55 GMT</pubDate>
</item>
<item>
<title>文档解析的现状与挑战</title>
<link>https://arxiv.org/abs/2410.21169</link>
<guid>https://arxiv.org/abs/2410.21169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了文档解析的现状及相关挑战。</p><br /><br /><p><strong>摘要：</strong> 文档解析是将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化、机器可读数据的重要过程。最近在大语言模型领域的成就使得文档解析在知识库构建和训练数据生成中成为不可或缺的一环。本文全面回顾了文档解析的现状，涵盖了从模块化管道系统到由大型视觉-语言模型驱动的端到端模型的关键方法。重点探讨了布局检测、内容提取（包括文本、表格和数学表达）及多模态数据集成等核心组件，并讨论了模块化文档解析系统和视觉-语言模型在处理复杂布局、集成多个模块及识别高密度文本时所面临的挑战。最后，文章强调了开发更大和更多样化数据集的重要性，并指出了未来研究的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 07:59:34 GMT</pubDate>
</item>
<item>
<title>Bielik 7B v0.1：波兰语处理的七十亿参数生成文本模型</title>
<link>https://arxiv.org/abs/2410.18565</link>
<guid>https://arxiv.org/abs/2410.18565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik 7B v0.1是针对波兰语处理的创新生成文本模型。</p><br /><br /><p><strong>摘要：</strong> Bielik 7B v0.1是一个针对波兰语处理的七十亿参数生成文本模型，采用了创新的训练技术，如加权指令交叉熵损失和自适应学习率。为了评估模型性能，我们创建了开放PL LLM排行榜和波兰MT-Bench，评估多种自然语言处理任务及对话能力。在RAG Reader任务中，该模型较Mistral-7B-v0.1提高了9个百分点的平均得分，并在波兰MT-Bench中表现出色，尤其是在推理（6.15/10）和角色扮演（7.83/10）类别。这一模型标志着波兰语言人工智能的重大进展，为多样化的语言应用提供了强大的工具，并在此领域设定了新的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 06:47:10 GMT</pubDate>
</item>
<item>
<title>AgentStore：一种创新的动态异构代理集成平台</title>
<link>https://arxiv.org/abs/2410.18603</link>
<guid>https://arxiv.org/abs/2410.18603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentStore平台显著提升了计算机任务自动化的能力。</p><br /><br /><p><strong>摘要：</strong> AgentStore 是一个可扩展的平台，旨在动态集成异构代理以自动化计算机任务。现有代理方法在通用性和专业能力上存在不足，尤其是在处理实际开源计算任务时。该平台允许用户整合第三方代理，持续丰富系统能力，并能够适应快速变化的操作系统。此外，AgentStore 还提出了一种新的核心 MetaAgent 和 AgentToken 策略，有效管理多样代理并利用其专业与通用能力。实验结果表明，AgentStore 在多个基准测试中超越了以往系统的限制，尤其在 OSWorld 基准测试中，将表现从 11.21% 提高到了 23.85%。这些结果表明，AgentStore 能够在通用性和专业化方面增强代理系统，为开发专业通用的计算机助手铺平道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 06:14:11 GMT</pubDate>
</item>
<item>
<title>GPT-4o System Card</title>
<link>https://arxiv.org/abs/2410.21276</link>
<guid>https://arxiv.org/abs/2410.21276</guid>
<content:encoded><![CDATA[
GPT-4o is an autoregressive omni model that accepts as input any combination of text, audio, image, and video, and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning all inputs and outputs are processed by the same neural network. GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time in conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50\% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models. In line with our commitment to building AI safely and consistent with our voluntary commitments to the White House, we are sharing the GPT-4o System Card, which includes our Preparedness Framework evaluations. In this System Card, we provide a detailed look at GPT-4o's capabilities, limitations, and safety evaluations across multiple categories, focusing on speech-to-speech while also evaluating text and image capabilities, and measures we've implemented to ensure the model is safe and aligned. We also include third-party assessments on dangerous capabilities, as well as discussion of potential societal impacts of GPT-4o's text and vision capabilities.
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 05:38:26 GMT</pubDate>
</item>
<item>
<title>Fast Best-of-N Decoding via Speculative Rejection</title>
<link>https://arxiv.org/abs/2410.20290</link>
<guid>https://arxiv.org/abs/2410.20290</guid>
<content:encoded><![CDATA[
The safe and effective deployment of Large Language Models (LLMs) involves a critical step called alignment, which ensures that the model's responses are in accordance with human preferences. Prevalent alignment techniques, such as DPO, PPO and their variants, align LLMs by changing the pre-trained model weights during a phase called post-training. While predominant, these post-training methods add substantial complexity before LLMs can be deployed. Inference-time alignment methods avoid the complex post-training step and instead bias the generation towards responses that are aligned with human preferences. The best-known inference-time alignment method, called Best-of-N, is as effective as the state-of-the-art post-training procedures. Unfortunately, Best-of-N requires vastly more resources at inference time than standard decoding strategies, which makes it computationally not viable. In this work, we introduce Speculative Rejection, a computationally-viable inference-time alignment algorithm. It generates high-scoring responses according to a given reward model, like Best-of-N does, while being between 16 to 32 times more computationally efficient.
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 05:27:08 GMT</pubDate>
</item>
<item>
<title>Neural Fields in Robotics: A Survey</title>
<link>https://arxiv.org/abs/2410.20220</link>
<guid>https://arxiv.org/abs/2410.20220</guid>
<content:encoded><![CDATA[
Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 04:47:11 GMT</pubDate>
</item>
<item>
<title>GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation</title>
<link>https://arxiv.org/abs/2410.20474</link>
<guid>https://arxiv.org/abs/2410.20474</guid>
<content:encoded><![CDATA[
We introduce a novel training-free spatial grounding technique for text-to-image generation using Diffusion Transformers (DiT). Spatial grounding with bounding boxes has gained attention for its simplicity and versatility, allowing for enhanced user control in image generation. However, prior training-free approaches often rely on updating the noisy image during the reverse diffusion process via backpropagation from custom loss functions, which frequently struggle to provide precise control over individual bounding boxes. In this work, we leverage the flexibility of the Transformer architecture, demonstrating that DiT can generate noisy patches corresponding to each bounding box, fully encoding the target object and allowing for fine-grained control over each region. Our approach builds on an intriguing property of DiT, which we refer to as semantic sharing. Due to semantic sharing, when a smaller patch is jointly denoised alongside a generatable-size image, the two become "semantic clones". Each patch is denoised in its own branch of the generation process and then transplanted into the corresponding region of the original noisy image at each timestep, resulting in robust spatial grounding for each bounding box. In our experiments on the HRS and DrawBench benchmarks, we achieve state-of-the-art performance compared to previous training-free spatial grounding approaches.
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 04:09:51 GMT</pubDate>
</item>
<item>
<title>COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training</title>
<link>https://arxiv.org/abs/2410.19313</link>
<guid>https://arxiv.org/abs/2410.19313</guid>
<content:encoded><![CDATA[
FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT.
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 03:44:09 GMT</pubDate>
</item>
<item>
<title>Language Models And A Second Opinion Use Case: The Pocket Professional</title>
<link>https://arxiv.org/abs/2410.20636</link>
<guid>https://arxiv.org/abs/2410.20636</guid>
<content:encoded><![CDATA[
This research tests the role of Large Language Models (LLMs) as formal second opinion tools in professional decision-making, particularly focusing on complex medical cases where even experienced physicians seek peer consultation. The work analyzed 183 challenging medical cases from Medscape over a 20-month period, testing multiple LLMs' performance against crowd-sourced physician responses. A key finding was the high overall score possible in the latest foundational models (&gt;80% accuracy compared to consensus opinion), which exceeds most human metrics reported on the same clinical cases (450 pages of patient profiles, test results). The study rates the LLMs' performance disparity between straightforward cases (&gt;81% accuracy) and complex scenarios (43% accuracy), particularly in these cases generating substantial debate among human physicians. The research demonstrates that LLMs may be valuable as generators of comprehensive differential diagnoses rather than as primary diagnostic tools, potentially helping to counter cognitive biases in clinical decision-making, reduce cognitive loads, and thus remove some sources of medical error. The inclusion of a second comparative legal dataset (Supreme Court cases, N=21) provides added empirical context to the AI use to foster second opinions, though these legal challenges proved considerably easier for LLMs to analyze. In addition to the original contributions of empirical evidence for LLM accuracy, the research aggregated a novel benchmark for others to score highly contested question and answer reliability between both LLMs and disagreeing human practitioners. These results suggest that the optimal deployment of LLMs in professional settings may differ substantially from current approaches that emphasize automation of routine tasks.
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 03:23:01 GMT</pubDate>
</item>
<item>
<title>MarDini：融合影片扩散模型与受限自回归的创新方法</title>
<link>https://arxiv.org/abs/2410.20280</link>
<guid>https://arxiv.org/abs/2410.20280</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MarDini通过结合受限自回归与扩散模型，提升视频生成效率和质量。</p><br /><br /><p><strong>摘要：</strong> MarDini是一种新型的视频扩散模型，结合了受限自回归（MAR）与统一扩散模型（DM）框架的优点。该模型采用不对称网络设计，MAR负责时序规划，而DM专注于空间生成。具体而言，MAR基于低分辨率输入生成各个遮挡帧的规划信号，而轻量级生成模型则利用这些信号通过扩散去噪生成高分辨率帧。MarDini支持在任意帧位置和数量的遮挡帧上进行视频生成，能够处理视频插值、图像转视频和视频扩展等任务。该设计高效地分配了计算资源，使得在低分辨率规划模型中的计算密集型时空注意力得以在大规模上实现。MarDini在视频插值方面创造了全新状态，同时在几次推理步骤内，其生成的视频质量与更为复杂的图像转视频模型相匹配。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20280" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 02:06:53 GMT</pubDate>
</item>
<item>
<title>基于双重策略的图像恢复研究</title>
<link>https://arxiv.org/abs/2410.18666</link>
<guid>https://arxiv.org/abs/2410.18666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GenIR和DreamClear，解决图像恢复中的数据集和模型挑战。</p><br /><br /><p><strong>摘要：</strong> 本研究针对图像恢复领域面临的数据集不足和高容量模型缺乏的挑战，提出了一种双重策略：GenIR和DreamClear。GenIR是一个创新的数据策划管道，通过三阶段流程（图像-文本对构建、双提示微调与数据生成及过滤）解决现有数据集的局限，最终构建出一个包含一百万张高质量图像的大规模数据集。DreamClear则是一种基于Diffusion Transformer的图像恢复模型，利用文本到图像（T2I）扩散模型的生成先验和多模态大语言模型的感知能力，达到逼真的图像恢复效果。为了增加模型的适应性，我们引入了混合自适应调制器（MoAM），该模块通过动态整合多种恢复专家，扩展了模型针对多种实际退化的处理能力。实验结果验证了DreamClear在实际图像恢复中的卓越表现，彰显了我们双重策略的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 01:55:51 GMT</pubDate>
</item>
<item>
<title>视觉搜索助手：提升视觉语言模型的信息检索能力</title>
<link>https://arxiv.org/abs/2410.21220</link>
<guid>https://arxiv.org/abs/2410.21220</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉搜索助手，以增强视觉语言模型对新视觉内容的理解和响应能力。</p><br /><br /><p><strong>摘要：</strong> 传统的信息检索方法在处理未知视觉内容时存在局限，尤其是在大规模视觉语言模型（VLMs）未见过的物体识别方面。为解决这一问题，本文提出了视觉搜索助手框架，促进VLMs与网络代理之间的协作，利用VLMs的视觉理解能力及网络代理的实时信息访问，进行开放世界的检索增强生成。通过整合视觉和文本表示，即使在处理新颖图像时，模型也能提供有效响应。实验证明，视觉搜索助手在开放集和闭合集的问答基准上显著优于其他模型，具有广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21220" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 01:12:04 GMT</pubDate>
</item>
<item>
<title>LongReward：提升长上下文模型性能的重强化学习方法</title>
<link>https://arxiv.org/abs/2410.21252</link>
<guid>https://arxiv.org/abs/2410.21252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongReward方法通过多维度评估显著提升了长上下文模型的性能。</p><br /><br /><p><strong>摘要：</strong> 尽管在开发长上下文的大型语言模型（LLMs）方面取得了显著进展，但LLM合成数据质量的下降对监督微调（SFT）模型的长上下文性能产生了负面影响。为了应对这一挑战，本文提出了LongReward，一种利用现成的LLM为长上下文模型响应提供奖励的新方法，关注四个重要维度：有用性、逻辑性、可信性和完整性。通过结合LongReward和离线增强学习算法DPO，我们能够有效提高长上下文SFT模型的表现。实验结果表明，LongReward显著改善了模型的长上下文性能，同时增强了其执行短指令的能力。此外，LongReward与传统短上下文DPO的长上下文DPO可以一起使用，而不影响各自的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 00:45:41 GMT</pubDate>
</item>
<item>
<title>小型语言模型的全面综述</title>
<link>https://arxiv.org/abs/2410.20011</link>
<guid>https://arxiv.org/abs/2410.20011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文对小型语言模型的架构和优化方法进行综述。</p><br /><br /><p><strong>摘要：</strong> 本文对小型语言模型（SLMs）进行了全面的综述，探讨其在各种语言任务中的重要性和效率，适用于移动设备和边缘计算等场景。文章重点介绍了SLMs的架构、训练技术及模型压缩方法，提出了一种新的分类法来描述优化SLMs的技术，包括模型压缩、剪枝和量化等。此外，文中总结了用于评估SLMs的基准数据集及常用评估指标，并指出了当前面临的主要挑战，旨在为研究人员和实践者提供开发和部署高效小型语言模型的重要资源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.20011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 00:38:22 GMT</pubDate>
</item>
<item>
<title>LARP: 一种新型视频标记化工具提升自回归生成模型性能</title>
<link>https://arxiv.org/abs/2410.21264</link>
<guid>https://arxiv.org/abs/2410.21264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LARP通过全局标记化改进视频生成的自回归模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LARP，一种新型视频标记化工具，旨在克服现有视频标记化方法在自回归生成模型中的局限。与传统的逐块标记方法不同，LARP采用一种整体标记方案，通过学习的整体查询来获取视觉内容信息，从而捕获更全球性和语义性的表示。LARP支持可变数量的离散标记，使其能够根据任务需求灵活有效地进行标记。通过集成轻量级自回归变换器，LARP在训练过程中优化离散标记空间，使其更适合视频重构和自回归生成。此外，LARP在训练中定义离散标记的顺序，推动其逐渐朝向最佳配置，从而在推理时实现更平滑、准确的自回归生成。实验结果显示，LARP在UCF101类条件视频生成基准中实现了最先进的FVD，证明了其在视频自回归生成领域的兼容性和潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.21264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Oct 2024 00:36:00 GMT</pubDate>
</item>
<item>
<title>Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design</title>
<link>https://arxiv.org/abs/2410.19123</link>
<guid>https://arxiv.org/abs/2410.19123</guid>
<content:encoded><![CDATA[
The proliferation of large language models (LLMs) has led to the adoption of Mixture-of-Experts (MoE) architectures that dynamically leverage specialized subnetworks for improved efficiency and performance. Despite their benefits, MoE models face significant challenges during inference, including inefficient memory management and suboptimal batching, due to misaligned design choices between the model architecture and the system policies. Furthermore, the conventional approach of training MoEs from scratch is increasingly prohibitive in terms of cost. In this paper, we propose a novel framework Read-ME that transforms pre-trained dense LLMs into smaller MoE models (in contrast to "upcycling" generalist MoEs), avoiding the high costs of ground-up training. Our approach employs activation sparsity to extract experts. To compose experts, we examine the widely-adopted layer-wise router design and show its redundancy, and thus we introduce the pre-gating router decoupled from the MoE backbone that facilitates system-friendly pre-computing and lookahead scheduling, enhancing expert-aware batching and caching. Our codesign therefore addresses critical gaps on both the algorithmic and system fronts, establishing a scalable and efficient alternative for LLM inference in resource-constrained settings. Read-ME outperforms other popular open-source dense models of similar scales, achieving improvements of up to 10.1% on MMLU, and improving mean end-to-end latency up to 6.1%. Codes are available at: https://github.com/VITA-Group/READ-ME.
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 18:48:54 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的知识冲突检测与管理</title>
<link>https://arxiv.org/abs/2410.16090</link>
<guid>https://arxiv.org/abs/2410.16090</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型如何识别知识冲突及其管理机制。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）能够存储大量事实知识，但其参数知识可能与上下文信息冲突，导致错误的模型行为。本研究探讨了LLMs是否能够识别知识冲突，并通过分析残留流确定模型依赖的知识来源。我们发现LLMs可以在残留流中内部注册知识冲突的信号，通过探测模型的中间激活层准确检测这些冲突。这种方法允许我们在生成答案之前检测残留流中的冲突，而无需修改输入或模型参数。此外，残留流在模型依赖上下文知识与参数知识解决冲突时展现出显著不同的模式，这些模式可以用于预测LLMs在冲突情况下的行为，从而在生成答案之前防止意外输出。我们的分析为理解LLMs如何内部管理知识冲突提供了新的视角，并为控制知识选择过程的方法的发展奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16090" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 14:03:26 GMT</pubDate>
</item>
<item>
<title>新闻源偏见评估方法的改进与应用</title>
<link>https://arxiv.org/abs/2410.17655</link>
<guid>https://arxiv.org/abs/2410.17655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于超链接交互的新闻源偏见评估新方法，提升分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨新闻源偏见评估的重要性，并提出一种扩展的新闻媒体可靠性估计方法，重点关注媒体及其长期的网页交互。通过对大型新闻媒体超链接图进行实验，验证了四种强化学习策略在事实报告和政治偏见这两种挑战性偏见描述上的分类性能显著提升。在2023年的CLEF CheckThat! Lab挑战中，我们的方法在F1-score和MAE指标上均优于已有结果。此外，本文还贡献了最大的带有事实报告和政治偏见标签的新闻源媒体注释数据集。研究结果表明，基于超链接交互对新闻媒体进行剖析是可行的，为不断变化的媒体环境提供了鸟瞰视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 14:02:25 GMT</pubDate>
</item>
<item>
<title>利用大语言模型优化数据集标注质量研究</title>
<link>https://arxiv.org/abs/2410.18889</link>
<guid>https://arxiv.org/abs/2410.18889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，使用大型语言模型可显著提高标注数据集的准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了标准化数据集在自然语言处理基准测试中的重要性，同时分析了专家标注与众包标注在数据标注过程中的优势和劣势。随着对更大数据集需求的增加，专家标注的成本难以承受，而众包虽然可扩展，却影响标注的准确性。我们采用大语言模型（LLM）作为标注检测工具，通过对来自TRUE基准的四个不同任务和领域的数据集进行案例研究，比较了专家、众包和LLM基于的标注在一致性、标注质量和效率方面的表现。结果显示，许多标注错误的存在是导致模型性能误判的重要因素。我们提出的修正措施可改善模型性能，并强调了标记错误对模型训练和评估的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 11:55:06 GMT</pubDate>
</item>
<item>
<title>利用多视图视频学习物体动态的机器人交互框架</title>
<link>https://arxiv.org/abs/2410.18912</link>
<guid>https://arxiv.org/abs/2410.18912</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了一种基于视频的框架，以学习物体的动态变化。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了一种新框架，通过多视图RGB视频直接学习物体的动态行为，重点关注机器人动作轨迹及其对场景动态的影响。利用3D高斯表示法与图神经网络相结合，构建了一个基于粒子的动态模型，该模型在稀疏控制粒子上运行。这一方法能够在离线机器人交互数据的基础上，预测物体的运动，适应不同的初始配置和未见过的机器人动作。通过对控制粒子的运动进行插值，可以渲染预测的未来物体状态，实现基于动作的视屏预测。该动态模型还可应用于基于模型的规划框架，以进行物体操作任务的研究。实验结果表明，该框架能够有效地建模各种可变形材料的复杂形状和动态特点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18912" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 09:11:06 GMT</pubDate>
</item>
<item>
<title>Reflection-Bench: probing AI intelligence with reflection</title>
<link>https://arxiv.org/abs/2410.16270</link>
<guid>https://arxiv.org/abs/2410.16270</guid>
<content:encoded><![CDATA[
The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the debate on the intelligence of large language models (LLMs), we propose Reflection-Bench, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory reflection ability. We discuss the underlying causes of these results and suggest potential avenues for future research. In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. Our data and code are available at https://github.com/YabYum/ReflectionBench.
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 07:28:30 GMT</pubDate>
</item>
<item>
<title>SALAD：一种基于令牌的连续表示的零-shot文本到语音模型</title>
<link>https://arxiv.org/abs/2410.16048</link>
<guid>https://arxiv.org/abs/2410.16048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SALAD，一个基于连续表示的文本到语音模型，表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SALAD的模型，旨在解决离散令牌在连续模式下重构质量的局限性。SALAD是一种按照每个令牌运作的潜在扩散模型，实现了零-shot文本到语音合成。它在最近提出的图像生成扩散头的基础上进行了扩展，支持生成变长输出。通过语义令牌来提供上下文信息并确定停止条件，我们提出了三种连续变体，提升了流行的离散语音合成技术。同时，为每个变体实现了离散基线，并对离散与连续语音建模技术进行了比较分析。结果表明，SALAD在可懂度评分上优于其他方法，并且在语音质量和说话者相似度方面与真实音频达到了相当的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 05:10:35 GMT</pubDate>
</item>
<item>
<title>探究tokenization对大型语言模型计数能力的影响</title>
<link>https://arxiv.org/abs/2410.19730</link>
<guid>https://arxiv.org/abs/2410.19730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨tokenization对大型语言模型计数能力的影响及其理论分析。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）中tokenization对计数能力的影响，指出Transformer架构的固有限制导致推理能力受限，尤其在计数任务中表现不佳。通过理论与实验分析，揭示了不同输入tokenization形式对模型性能的显著影响。尽管先前研究已经探讨了Transformer模型在计数任务中的上限，但通用LLMs的原因机制与专家模型存在差异，因此这些发现并不直接适用。我们提出改进tokenization方法的建议，以提升LLMs在复杂计数任务中的推理能力，推动未来研究的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 05:02:39 GMT</pubDate>
</item>
<item>
<title>视觉-语言模型在身体决策中的应用和挑战</title>
<link>https://arxiv.org/abs/2410.17856</link>
<guid>https://arxiv.org/abs/2410.17856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉-时空上下文提示以提升VLM在复杂决策中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉-语言模型（VLM）在开放世界环境中进行身体决策的挑战，尤其是如何将低级观测中的个体实体与抽象概念有效连接。针对语言无法准确传达空间信息的局限性，提出了一种新的通信协议——视觉-时空上下文提示，通过利用过去和现在的物体分割来指导政策与环境的互动。我们训练了ROCKET-1，这是一个低级政策模型，能够基于视觉观测和分割掩膜预测动作。实验结果表明，该方法显著提升了VLM在创造性任务中的表现，尤其是在空间理解方面，从而使得代理能够完成以往难以实现的任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 02:54:44 GMT</pubDate>
</item>
<item>
<title>基于Prereq-Tune的LLM幻觉减少策略研究</title>
<link>https://arxiv.org/abs/2410.19290</link>
<guid>https://arxiv.org/abs/2410.19290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Prereq-Tune策略，以减少LLM中的幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，LLM幻觉的一个加剧因素是预训练与微调之间的知识不一致，这导致模型在处理不熟悉的微调数据时生成错误的、看似合理的输出。为解决这一问题，本文提出了一种新颖的微调策略——Prereq-Tune，旨在减少知识不一致对模型的影响。该策略通过引入额外的前置学习阶段，专注于学习必要的知识，使后续的微调过程只关注任务技能。Prereq-Tune还能够与虚构的合成数据结合，增强LLM输出与其内部知识之间的关联性。实验结果表明，Prereq-Tune在提高LLM的真实性方面优于现有的基线方法，尤其是在短问答和长文本生成任务中。此外，该策略为LLM中知识控制生成开辟了新的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 02:46:08 GMT</pubDate>
</item>
<item>
<title>结合人类反馈与语言模型提升偏好注释质量的路由框架</title>
<link>https://arxiv.org/abs/2410.19133</link>
<guid>https://arxiv.org/abs/2410.19133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种路由框架，结合人类与LM输入，提升偏好注释质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的路由框架，可有效结合人类与语言模型（LM）的输入，以提高偏好注释的质量，并降低人类注释的整体成本。通过优化问题，我们训练了一个性能预测模型来预测奖励模型在任意人类与LM注释组合上的表现，并采用路由策略最大化预测性能。使用新的偏好数据集MultiPref进行训练，本方法展示了在使用路由框架的混合策略时，奖励模型的性能较单独使用人类或LM注释均有提高。此外，我们将选择性人类偏好收集应用于三个其他数据集，结果表明方法具备良好的泛化能力。我们还分析了路由模型的特征，以识别哪些实例更适合人类反馈，以期未来推动偏好收集的高效与准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 02:40:29 GMT</pubDate>
</item>
<item>
<title>FasterCache：加速视频扩散模型推断的新策略</title>
<link>https://arxiv.org/abs/2410.19355</link>
<guid>https://arxiv.org/abs/2410.19355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FasterCache，通过特征重用加速视频扩散模型的推断且不损失质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FasterCache的新策略，旨在无训练地加速视频扩散模型的推断，并实现高质量生成。通过分析现有的基于缓存的方法，发现直接重用相邻步骤特征会因为细微变动的损失而降低视频质量。研究表明，在相同时间步内，条件与无条件特征之间存在显著冗余，这为有效加速提供了可能。FasterCache引入了动态特征重用策略，既保留了特征的区分性，又维持了时间连续性。同时，CFG-Cache优化了条件输出与无条件输出的重用，进一步提升推断速度而不影响视频质量。实验证明，FasterCache在视频扩散模型中的应用能显著加速视频生成（例如在Vchitect-2.0上实现1.67倍的加速），并在视频质量上与基线模型相当，且在推断速度和视频质量上持续超越现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 02:14:39 GMT</pubDate>
</item>
<item>
<title>通过未标记轨迹数据学习强化学习中的高效探索策略</title>
<link>https://arxiv.org/abs/2410.18076</link>
<guid>https://arxiv.org/abs/2410.18076</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SUPE方法，有效利用未标记数据提高强化学习探索效率。</p><br /><br /><p><strong>摘要：</strong> 本文研究在强化学习中如何利用未标记的先前轨迹数据来学习高效的探索策略。与传统监督学习不同，强化学习的微调不涉及模仿任务特定数据，而是通过逐步自我改进来寻找解决方案。我们提出的SUPE（Skills from Unlabeled Prior data for Exploration）方法，首先使用变分自编码器（VAE）提取低级技能，接着利用乐观奖励模型对未标记轨迹进行伪标签化，将先前数据转化为具有任务相关性的高阶样本。最终，SUPE将这些转化后的样本作为额外的离线数据用于在线强化学习，从而学习出一个能高效组合预训练低级技能的高阶策略。实验证明，SUPE在解决一系列长时程、稀疏奖励的任务中，可靠地超越了之前的策略。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18076" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 00:57:59 GMT</pubDate>
</item>
<item>
<title>Infinity-MM: 规模化多模态指令数据集与高性能模型的突破</title>
<link>https://arxiv.org/abs/2410.18558</link>
<guid>https://arxiv.org/abs/2410.18558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过Infinity-MM数据集提升开放源代码VLM的性能。</p><br /><br /><p><strong>摘要：</strong> Vision-Language Models (VLMs) 近年来取得了显著进展，但开放源代码指令数据的规模和质量限制了其性能。为了解决这一问题，本文提出了Infinity-MM，一个具有4000万样本的大规模多模态指令数据集，经过严格的质量筛选和去重处理。此外，我们还基于开放源代码VLM提出了一种合成指令生成方法，利用详细的图像注释和多样化的问题生成技术。通过这些数据，我们训练了一个2亿参数的VLM，Aquila-VL-2B，在类似规模的模型中达到目前的最佳性能。这一成果表明，扩大指令数据和生成合成数据能显著提升开放源代码模型的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 00:50:14 GMT</pubDate>
</item>
<item>
<title>MMAU：评估多模态音频理解模型的新基准</title>
<link>https://arxiv.org/abs/2410.19168</link>
<guid>https://arxiv.org/abs/2410.19168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMAU基准旨在评估AI对音频的理解能力，强调复杂推理与专业知识。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMAU，一个新颖的基准，旨在评估多模态音频理解模型在需要专业知识和复杂推理的任务中的表现。MMAU包括10000个精心策划的音频片段，配以人工注释的自然语言问题和答案，涵盖了语音、环境声音和音乐。该基准包含信息提取和推理问题，要求模型展示27项独特技能，面对挑战性任务。与现有基准相比，MMAU更强调先进的感知和推理能力以及领域知识，挑战模型处理类似专家所面临的问题。通过评估18个开源和专有的音频语言模型，显示出MMAU带来的显著挑战，尤其是最新的Gemini Pro v1.5和Qwen2-Audio尽管是领先模型，但准确率仅为52.97%和52.50%，显示出显著的改进空间。我们希望MMAU能推动音频和多模态研究社区发展更先进的音频理解模型，解决复杂的音频任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 00:39:59 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的ECG图像解读新工具PULSE的开发与评估</title>
<link>https://arxiv.org/abs/2410.19008</link>
<guid>https://arxiv.org/abs/2410.19008</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了新开发的ECG图像指令调优数据集和PULSE模型。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型的发展，ECG图像解读面临新的挑战。现有自动解读方法局限于特定心脏病，且依赖生理信号，而在资源有限的环境中，通常只能获取打印或数字化的ECG图像。为了解决这些问题，本文提出了ECGInstruct数据集，涵盖超过一百万个样本，涉及多种ECG相关任务，并基于此开发了为ECG图像理解量身定制的PULSE模型。此外，本文还建立了ECGBench评估基准，覆盖九个不同数据集的四个主要ECG图像解读任务。实验结果表明，PULSE的表现超过了一般的大语言模型，准确率提升幅度在15%到30%之间，展示了PULSE在临床中增强ECG解读的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.19008" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Oct 2024 00:09:51 GMT</pubDate>
</item>
<item>
<title>Pantograph：基于 Lean 4 的机器辅助定理证明工具</title>
<link>https://arxiv.org/abs/2410.16429</link>
<guid>https://arxiv.org/abs/2410.16429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了 Pantograph，一款结合机器学习的新型定理证明工具，提升了 Lean 4 的推理能力。</p><br /><br /><p><strong>摘要：</strong> 机器辅助定理证明是通过结构化推理自动生成数学定理证明的过程。最近，结合机器学习模型与证明助手进行定理证明的研究引起了广泛关注。本文介绍了 Pantograph，这是一款为 Lean 4 证明助手提供多功能接口的工具，能够通过强大的搜索算法（如蒙特卡洛树搜索）实现高效的证明搜索。此外，Pantograph 还通过更加稳健的推理步骤处理方式增强了高层次推理能力。文中提供了 Pantograph 的架构和特性概述，并报告了一个示范用例：结合机器学习模型和证明草图来证明 Lean 4 的定理。Pantograph 的创新特性为未来研究者设计更复杂、功能更强大的定理证明器铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 21:19:46 GMT</pubDate>
</item>
<item>
<title>ZIP-FIT：基于压缩测量任务对齐的数据选择框架</title>
<link>https://arxiv.org/abs/2410.18194</link>
<guid>https://arxiv.org/abs/2410.18194</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZIP-FIT框架通过gzip压缩测量数据与目标任务分布的对齐性，显著提高模型在具体任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 数据选择对优化语言模型（LM）在特定任务上的表现至关重要。然而，大多数现有方法未能有效考虑目标任务分布。当前方法要么完全忽视任务特定需求，要么依赖于无法捕捉细微模式的近似方法。即便有考虑任务分布的方法，往往也依赖于简单且有噪声的表示如哈希n-gram特征，导致冲突并引入噪声。我们提出了ZIP-FIT，一个使用gzip压缩直接测量潜在训练数据与目标任务分布对齐度的数据选择框架。在Autoformalization和Python代码生成任务的广泛评估中，ZIP-FIT显著超越了领先基线DSIR和D4。采用ZIP-FIT选择数据训练的模型在交叉熵损失上以高达85.1%的速度实现最低值，表明更好的任务对齐能够带来更高效的学习。此外，ZIP-FIT在选择时比DSIR快65.8%，比D4快两个数量级。值得注意的是，ZIP-FIT表明，更小的高对齐数据集通常优于更大但针对性不足的数据集，强调了高质量数据的重要性。我们的研究揭示了任务感知数据选择对高效领域适应的重要性，以及压缩为测量任务对齐提供了一种原则性的方式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18194" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 21:18:55 GMT</pubDate>
</item>
<item>
<title>变压器模型及生成AI关键组件的数学问题与概率优化分析</title>
<link>https://arxiv.org/abs/2410.18441</link>
<guid>https://arxiv.org/abs/2410.18441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文分析变压器模型的关键组件及生成AI技术相关的数学和概率优化问题。</p><br /><br /><p><strong>摘要：</strong> 本文深入分析了生成AI领域中变压器模型的数学问题表述和概率优化探索。通过算法和概率优化的视角，讨论了对当前先进方法的潜在增强。特别地，我们提出了一种基于与字节对编码（BPE）算法相似的初始设置的子词编码（SWE）最优解，并与WordPiece方法的目标相似，以最大化训练数据的似然性。此外，我们展示了交叉熵优化方法用于word2vec模型的超参数优化，并提出了 rotary positional encoding（RoPE）和 attention with linear biases（ALiBi）的分解组合方法，结合了谐波级数。本文还介绍了一种通过概率分布在矩阵中确定哪个块可能参与注意力计算的概率FlashAttention（PrFlashAttention）方法，并保持自回归语言模型的下三角形状。最后，我们基于名为阶梯自适应量化（SAQ）的框架，提出了多查询注意力（MQA）中关键值（KV）缓存的逐步量化降级，以实现合理的模型质量和成本节约。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 16:54:20 GMT</pubDate>
</item>
<item>
<title>异步强化学习人类反馈的研究与实践</title>
<link>https://arxiv.org/abs/2410.18252</link>
<guid>https://arxiv.org/abs/2410.18252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出异步训练的RLHF方法，以提高计算效率与性能。研究表明，在线DPO算法对离线数据的鲁棒性最强。</p><br /><br /><p><strong>摘要：</strong> 在强化学习人类反馈（RLHF）的主流范式中，当前的做法是通过在线和在政策上同步生成生成样本，利用奖励模型进行标注，并通过反馈在大型语言模型（LLM）自身输出上进行学习。尽管该范式性能优越，但在计算效率上存在不足。基于经典深度强化学习文献，我们提出将生成与学习过程分离，从而实现异步生成新样本并同时对旧样本进行训练，促进更快的训练和更高的计算优化。然而，异步训练依赖于一个相对未被深入探索的领域：在线但离策略的RLHF，即在模型先前迭代生成的样本上进行学习。为了解该领域的挑战，我们探讨了一个基础问题：在异步训练中，我们可以容忍多少离策略性，以加速学习而又不损害性能。经过多种RLHF算法的测试，我们发现在线DPO在处理离策略数据时表现出最强的鲁棒性，且随着模型规模的增加，鲁棒性进一步增强。此外，我们还研究了异步RLHF的计算优化，但发现这些优化会影响最终性能，形成权衡。最终，我们通过在指令跟随任务上以比同步训练快40%的速度训练LLaMA 3.1 8B，验证了异步RLHF的可扩展性。 </p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 16:45:45 GMT</pubDate>
</item>
<item>
<title>基于文本到图像扩散模型的视频对象分割方法研究</title>
<link>https://arxiv.org/abs/2410.18538</link>
<guid>https://arxiv.org/abs/2410.18538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种结合扩散模型和跟踪机制的视频对象分割方法，提升了分割准确性及一致性。</p><br /><br /><p><strong>摘要：</strong> 视频中的对象分割面临着诸多挑战，主要涉及每个像素的准确标记及标签在各个帧之间的一致性。尤其在任意粒度的分割任务中，分段数目可变且掩膜仅基于一两张样本图像定义，进一步增加了复杂性。为了解决这一问题，本文提出了一种利用预训练文本到图像扩散（text to image diffusion）模型的方法，并结合了一种额外的跟踪机制。通过实验，我们证明了这一方法在多种分割场景下的有效性，并且在性能上优于现有的最先进的替代方案。这项研究为改进视频对象分割的技术提供了新的思路和有力的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 15:00:52 GMT</pubDate>
</item>
<item>
<title>利用稀疏自编码器解决大语言模型中的知识冲突</title>
<link>https://arxiv.org/abs/2410.15999</link>
<guid>https://arxiv.org/abs/2410.15999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出 SpARE 方法，解决大语言模型中的知识冲突，提升问答任务性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）能够在其参数中存储大量事实知识，但其参数知识可能会与上下文提供的信息发生冲突，这种现象称为知识冲突。这种冲突可能导致模型表现不佳，例如依赖过时或不正确的信息。通过分析 LLM 的内部激活，研究发现中间层可以内部注册知识冲突的信号，这些信号允许我们检测知识冲突的发生，并在推理时应用干预策略进行解决。为此，本文提出了 SpARE，一种无需训练的表示工程方法，利用预训练的稀疏自编码器（SAEs）控制 LLM 的知识选择行为。SpARE 识别出控制知识选择行为的功能特征，并在推理时对 LLM 的内部激活进行编辑。实验结果表明，SpARE 在开放领域问答任务中，能够有效控制两种知识源的使用，以解决知识冲突，超越现有的表示工程方法（提升 10%），以及对比解码方法（提升 15%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 14:56:51 GMT</pubDate>
</item>
<item>
<title>通过对比检索头减少大型语言模型的幻觉</title>
<link>https://arxiv.org/abs/2410.18860</link>
<guid>https://arxiv.org/abs/2410.18860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法DeCoRe，通过对比检索头来降低大型语言模型的幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）常常出现幻觉现象，即通过错误的上下文表示或不准确的内部知识回忆而产生不真实或事实错误的输出。近期研究发现，Transformer架构中存在特定的注意力头，称为检索头，负责提取相关的上下文信息。我们假设，屏蔽这些检索头可能会导致幻觉现象，并且通过对比基础LLM与屏蔽LLM的输出，可以减少幻觉。为此，我们提出了一种新颖的无训练解码策略——DeCoRe，通过动态对比基础LLM和屏蔽LLM的输出，使用条件熵作为指导，增强了上下文和模型参数中的信息。这一策略显著改善了在需要高度上下文真实性的任务上的表现，包括摘要（XSum 提高了18.6%）、指令跟随（MemoTrap 提高了10.9%），以及开放书籍问答（NQ-Open 提高了2.4%和 NQ-Swap 提高了5.5%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 14:22:45 GMT</pubDate>
</item>
<item>
<title>数据扩展在机器人操控中的应用研究</title>
<link>https://arxiv.org/abs/2410.18647</link>
<guid>https://arxiv.org/abs/2410.18647</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了数据扩展在机器人操控中的影响，展示了一种高效的数据收集策略。</p><br /><br /><p><strong>摘要：</strong> 本论文调查了数据扩展法在机器人领域，特别是在机器人操控中的有效性，旨在了解是否能产生针对任何对象和环境的单任务机器人策略。通过系统性地收集跨多个环境和对象的数据，我们研究了政策的泛化性能与训练环境、对象及演示数量之间的关系。我们的研究共收集了40,000多条演示，并在严格的评估标准下进行了15,000多次真实机器人运行。结果显示，策略的泛化性能与环境和对象的数量呈现出大致的幂律关系。环境和对象的多样性对性能影响远大于绝对训练演示数量；一旦每个环境或对象的演示数量达到一定阈值，额外演示的贡献就微乎其微。基于这些发现，我们提出了一种高效的数据收集策略。经过四个数据收集者一个下午的努力，我们为两个任务收集了足够的数据，使得策略在新环境下对见过的对象达到约90%的成功率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18647" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 12:49:33 GMT</pubDate>
</item>
<item>
<title>Taipan：高效处理超长上下文的混合架构</title>
<link>https://arxiv.org/abs/2410.18572</link>
<guid>https://arxiv.org/abs/2410.18572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Taipan结合了Mamba-2和选择性注意力层，解决了长上下文语言建模的效率与性能问题。</p><br /><br /><p><strong>摘要：</strong> 高效的长上下文语言建模仍然是自然语言处理中的一大挑战。尽管Transformer在语言任务中占据主导地位，但在训练时面临二次计算复杂度的问题，同时推理时的内存消耗呈线性增长。近期的状态空间模型（State Space Models, SSMs）如Mamba提供了常量内存使用的替代方案，然而在需要广泛上下文检索的任务中表现欠佳。为此，我们提出了Taipan，一个新颖的混合架构，结合了Mamba-2和选择性注意力层（Selective Attention Layers, SALs）。SALs能够识别需要长距离交互的标记，去除次要特征，并通过注意力模块增强其表征。这种方法在保留Mamba效率的同时，在内存密集型任务中取得了类似Transformer的性能，通过控制注意力预算，Taipan将准确预测的上下文长度扩展到100万标记，同时保持计算效率。实验结果表明，Taipan在不同规模和任务上表现优越，为高效的长上下文语言建模提供了有前景的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18572" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 08:53:46 GMT</pubDate>
</item>
<item>
<title>基于残差值的变换器：ResFormer与SVFormer</title>
<link>https://arxiv.org/abs/2410.17897</link>
<guid>https://arxiv.org/abs/2410.17897</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ResFormer通过残差连接缓解注意力集中问题，SVFormer显著提升训练速度与性能。</p><br /><br /><p><strong>摘要：</strong> Transformer能够通过自注意力机制捕捉长距离依赖关系，但在堆叠多个注意力层时，会导致注意力集中现象。为了解决这一问题，提出了跨层注意力的方法，使得早期层的信息能够直接传递给后续层，但该方法计算开销较大。为此，本文提出了基于残差值的变换器（ResFormer），通过将第一层的值与后续所有层相加，来近似跨层注意力。一个变体是单层值变换器（SVFormer），该模型使所有层共享第一层的值嵌入，从而使KV缓存减少近50%。通过大量实验结果表明，ResFormer能够缓解深层中的注意力集中问题，且在大多数层中增强了表示能力，训练误差和下游任务性能均优于传统Transformer、DenseFormer和NeuTRENO。SVFormer训练速度明显快于传统Transformer，并且在序列长度和累计学习率的影响下，表现出优于其他方法（如GQA和CLA）的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17897" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 08:04:10 GMT</pubDate>
</item>
<item>
<title>ADEM-VL：一种高效的视觉语言模型融合方法</title>
<link>https://arxiv.org/abs/2410.17779</link>
<guid>https://arxiv.org/abs/2410.17779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ADEM-VL，提高视觉语言模型的效率，减少参数、加速训练和推理。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态融合的快速发展使视觉语言（VL）模型在图像描述和视觉问答等多模态应用中表现出色。然而，构建VL模型需要大量硬件资源，其效率受到两个关键因素的制约：语言模型输入序列的扩展增加了计算操作，同时大量额外的可学习参数提高了内存复杂性。这些挑战显著限制了此类模型的更广泛应用。为此，我们提出了ADEM-VL，这是一种基于预训练大语言模型（LLM）的高效视觉语言方法，通过采用无参数交叉注意力机制进行多模态融合的相似性测量。该方法仅需将视觉特征嵌入语言空间，显著减少训练参数数量，加速训练和推理速度。为了增强融合模块中的表示学习，我们引入了一种高效的多尺度特征生成方案，只需通过视觉编码器进行一次正向传递。此外，我们提出了一种自适应融合方案，动态丢弃与每个文本令牌相关性较低的视觉信息，确保融合过程优先考虑最相关的视觉特征。通过在视觉问答、图像描述和指令跟随等各种任务上的实验，我们演示了我们的框架优于现有方法。在ScienceQA数据集上，我们的方法平均准确率提高了0.77%，并减少了训练和推理延迟，证明了框架的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 05:49:27 GMT</pubDate>
</item>
<item>
<title>为阿拉伯语大型多模态模型开发的综合评估基准——CAMEL-Bench</title>
<link>https://arxiv.org/abs/2410.18976</link>
<guid>https://arxiv.org/abs/2410.18976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文开发了针对阿拉伯语的大型多模态模型评估基准CAMEL-Bench，涵盖多领域任务。结果显示现有模型表现需大幅改善。</p><br /><br /><p><strong>摘要：</strong> 近年来，开发大型多模态模型（LMM）以执行各种视觉推理和理解任务受到广泛关注。这促使了多项LMM评估基准的出台，评估模型在不同任务上的表现。然而，现有的LMM评估基准多数集中于英语。本研究开发了一个全面的阿拉伯语LMM评估基准，名为CAMEL-Bench，以代表超过4亿人的大群体。该基准涵盖八个不同领域和38个子领域，包括多图像理解、复杂视觉感知、手写文档理解、视频理解、医学影像、植物病害和基于遥感的土地利用理解，旨在评估广泛场景的通用性。CAMEL-Bench包含约29,036个问题，经过从更大样本池中筛选，并由母语人士手动验证质量，以确保模型评估的可靠性。我们还对闭源模型（如GPT-4系列）和开源LMM进行了评估。分析结果表明，尤其是开源模型的性能需要显著提升，即使是闭源的GPT-4o，其整体得分也仅为62%。我们的基准和评估脚本已开源，供其他研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 05:21:52 GMT</pubDate>
</item>
<item>
<title>成本高效的复杂图表问答生成方法——Code-as-Intermediary Translation</title>
<link>https://arxiv.org/abs/2410.18798</link>
<guid>https://arxiv.org/abs/2410.18798</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法CIT，通过代码作为中介，合成复杂图表问答数据以提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Code-as-Intermediary Translation (CIT) 的新方法，旨在通过高效且可扩展的数据合成来提升多模态大语言模型 (MLLMs) 的视觉推理能力。CIT采用代码作为中介，将视觉图表表示转换为文本表示，从而让语言模型能够理解跨模态信息。我们通过基于文本的合成技术构建图表绘制代码，生成了ReachQA数据集，包含了3000个推理密集型图表和20000个问答对，以增强模型的识别和推理能力。实验结果表明，在使用我们的数据集进行微调后，模型在与图表相关的基准测试中表现良好，并且在像MathVista这样的通用数学基准上也显著提升了多模态推理能力。本文的代码和数据集已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18798" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 03:55:54 GMT</pubDate>
</item>
<item>
<title>Waffle：一种提升大型语言模型HTML结构理解能力的新策略</title>
<link>https://arxiv.org/abs/2410.18362</link>
<guid>https://arxiv.org/abs/2410.18362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Waffle是一种新的微调策略，通过结构感知注意机制提升LLMs的HTML理解能力。</p><br /><br /><p><strong>摘要：</strong> Web开发需要将UI设计转化为功能网页，然而HTML的层次结构和样式的复杂性常常对开发者造成挑战。尽管大型语言模型（LLMs）在生成源代码方面表现出了潜力，但在UI到HTML代码生成的过程中，依然面临两大挑战：首先是如何有效地表示HTML的层次结构，其次是如何弥合UI设计的视觉性质与HTML代码的文本格式之间的鸿沟。为了解决这些问题，我们提出了Waffle，这是一种新的微调策略，采用结构感知注意机制，以提高LLMs对于HTML结构的理解能力；同时通过对比微调的方法，使LLMs能更好地理解UI图像与HTML代码之间的关系。经过Waffle微调的模型在我们新推出的基准WebSight-Test和现有基准Design2Code上，显示出比当前微调方法高出最多9.00个百分点的HTML匹配度，CW-SSIM提高0.0982，CLIP提高32.99，LLEM提高27.12个百分点，表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 03:50:58 GMT</pubDate>
</item>
<item>
<title>HalluEditBench：基于真实幻觉的知识编辑方法基准评估</title>
<link>https://arxiv.org/abs/2410.16251</link>
<guid>https://arxiv.org/abs/2410.16251</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HalluEditBench提供了一种方法，对知识编辑技术在纠正大型语言模型中的幻觉进行全面评估。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在生成内容时存在幻觉问题，即产生非事实信息。知识编辑作为一种新兴的修正错误事实知识的范式，避免了从头开始重新训练的成本。然而，现有的知识编辑评估数据集存在一个共同问题，即无法确保在评估之前，LLMs确实生成了幻觉回答。因此，考量不同知识编辑技术的有效性变得困难。本研究提出了HalluEditBench，以全面基准化知识编辑方法在纠正真实世界幻觉方面的表现。我们细致构建了一个包含9个领域、26个主题和6000多个幻觉实例的大型幻觉数据集。然后，从有效性、泛化性、可移植性、本地性和稳健性五个维度综合评估知识编辑方法的性能。通过HalluEditBench，我们提供了对不同知识编辑方法在纠正幻觉方面的潜力和限制的新见解，这将激励未来的改进，并促进知识编辑领域的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16251" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 03:32:14 GMT</pubDate>
</item>
<item>
<title>基于注意力机制的人类运动生成交互编辑研究</title>
<link>https://arxiv.org/abs/2410.18977</link>
<guid>https://arxiv.org/abs/2410.18977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MotionCLR模型，通过注意力机制提升运动生成的编辑能力与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了人类运动生成的交互编辑问题，针对现有运动扩散模型在词级文本与运动对应建模及可解释性方面的不足，提出了一种基于注意力机制的运动扩散模型MotionCLR。该模型通过自注意力和跨注意力机制分别建模模态内和模态间的交互。自注意力机制用于衡量帧之间的顺序相似性，影响运动特征的顺序；而跨注意力机制则用于寻找细粒度的词序对应，并激活运动序列中的相应时间步。基于这些关键特性，本文开发了一系列简单有效的运动编辑方法，如运动的（去）强调、就地运动替换及基于示例的运动生成等。为了验证注意力机制的可解释性，我们还探讨了基于注意力图的动作计数和依赖于基础的运动生成能力。实验结果表明，我们的方法在生成和编辑能力上表现出色且具有良好的可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 03:25:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型在算术学习中的符号学习能力研究</title>
<link>https://arxiv.org/abs/2410.15580</link>
<guid>https://arxiv.org/abs/2410.15580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明LLMs在算术学习中是一种纯符号学习者，发现学习使用的子组复杂性对结果有显著影响。</p><br /><br /><p><strong>摘要：</strong> 本文通过双侧实验探讨大型语言模型（LLMs）在算术学习中的表现。首先，我们调查LLMs是否能够在算术学习中利用部分积，发现尽管经过学习后LLMs可以识别一些部分积，但未能有效地应用于具体的算术任务。随后，我们考察LLMs如何符号性地处理算术，假设其困难源自子组的复杂性和选择。研究结果显示，当固定子组复杂性时，LLMs对不同算术运算的处理呈现出相似性。进一步分析位置级别的准确率，发现其遵循U型模式：LLMs在第一和最后位置快速学习最简单的模式，而在中间位置逐渐掌握较难的模式。这表明LLMs在学习过程中遵循从容易到困难的子组选择策略。我们的研究确认LLMs在算术任务中是纯符号学习者，并强调通过子组水平量化深入理解其学习过程的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 03:22:03 GMT</pubDate>
</item>
<item>
<title>W-Bench：评估图像水印在高级编辑技巧下的鲁棒性与VINE方法</title>
<link>https://arxiv.org/abs/2410.18775</link>
<guid>https://arxiv.org/abs/2410.18775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出W-Bench基准，评估水印在编辑技术下的鲁棒性，提出VINE方法强化水印能力。</p><br /><br /><p><strong>摘要：</strong> 当前的图像水印方法对大型文本到图像模型所启用的高级编辑技术具有脆弱性，这些技术会在编辑过程中扭曲嵌入的水印，给版权保护带来显著挑战。为了解决这一问题，我们引入了W-Bench，这是首个全面的基准，用于评估水印方法在多种图像编辑技术（如图像再生、全局编辑、局部编辑和图像到视频生成）下的鲁棒性。通过对十一种代表性水印方法与常见编辑技术的广泛评估，我们证明大多数方法在经过编辑后无法检测到水印。为了解决这一局限性，我们提出了VINE，一种显著增强水印在各种图像编辑技术下鲁棒性的水印方法，同时保持高图像质量。我们的方法包含两项关键创新：（1）分析图像编辑的频率特征，识别模糊失真具有相似的频率特性，从而可以在训练中将其用作替代攻击，以增强水印鲁棒性；（2）利用大型预训练的扩散模型SDXL-Turbo，调整它以适应水印任务，实现更不易察觉且稳健的水印嵌入。实验结果表明，方法在各种图像编辑技术下表现优秀，超越了现有方法，在图像质量和鲁棒性方面均有显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18775" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 02:40:26 GMT</pubDate>
</item>
<item>
<title>多草稿投机采样的优化选择方案研究</title>
<link>https://arxiv.org/abs/2410.18234</link>
<guid>https://arxiv.org/abs/2410.18234</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种优化的投机采样方案，通过两步选择过程提高接受概率和效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多草稿投机采样中最优选择方案的分解方法。我们提出一种两步解决方案，其中第一步使用重要性采样（IS）选择一个中间token，第二步应用单草稿投机采样生成最终output token。在两个相同草稿模型的情况下，我们确定了目标模型与草稿模型分布之间的必要和充分条件，以确保接受概率等于1，并提供了最优接受概率的明确表达式。此外，我们的理论分析激发了一种基于加权重要性采样的新型token选择方案。实验结果显示，在多种场景下，该方案在块效率和token速率方面均优于基线方案，验证了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18234" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 02:32:27 GMT</pubDate>
</item>
<item>
<title>ScaleQuest: 可扩展的数学推理数据合成方法</title>
<link>https://arxiv.org/abs/2410.18693</link>
<guid>https://arxiv.org/abs/2410.18693</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScaleQuest是一种新颖的数据合成方法，自动生成数学推理问题，提升LLMs推理能力。</p><br /><br /><p><strong>摘要：</strong> 高质量数据是提升大语言模型（LLMs）推理能力的重要因素。当前研究表明，利用强大模型（如GPT-4）持续扩展数据合成能够更有效地提升推理性能。然而，开源社区在规模化和可负担的数据合成方法方面仍存在不足。为了解决这一问题，我们提出了ScaleQuest，这是一种可扩展的新数据合成方法，使用“小型”（如7B）开源模型无需种子数据，系统地生成问题。通过高效的ScaleQuest，我们自动构建了一个包含100万个问题-解决方案对的数学推理数据集，证明其有效性超过了现有的开源数据集。该数据集可普遍提升主流开源模型（如Mistral、Llama3、DeepSeekMath和Qwen2-Math）的性能，MATH上的提升幅度达到29.2%至46.4%。特别是，仅对Qwen2-Math-7B-Base模型进行微调，即可超过经过良好对齐的Qwen2-Math-7B-Instruct和封闭源数据的强大模型，如GPT-4-Turbo和Claude-3.5 Sonnet。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18693" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 01:44:14 GMT</pubDate>
</item>
<item>
<title>LOGO：通过高效偏好优化实现长上下文对齐的训练策略</title>
<link>https://arxiv.org/abs/2410.18533</link>
<guid>https://arxiv.org/abs/2410.18533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出LOGO，一种高效的长上下文对齐训练策略，通过偏好优化提升生成能力。</p><br /><br /><p><strong>摘要：</strong> 在处理长输入序列方面，长上下文模型（LCMs）表现出巨大的潜力，能够有效地处理超过100M的token输入。尽管在定位上下文中的关键token信息方面取得了显著进展，但当前LCMs的生成性能仍不尽如人意，经常出现不对齐的响应，比如幻觉现象。为了提升LCMs的生成能力，现有研究探讨了数据规模和质量在预训练和指令微调中的影响。尽管取得了一定程度的改进，前述方法在有效性和效率上依然存在不足。本文提出了一种新的训练策略LOGO（长上下文对齐通过高效偏好优化），首先引入偏好优化来进行长上下文对齐。为了解决由于长序列带来的GPU内存限制问题，LOGO采用了一种无参考的偏好优化策略，并运用位置合成方法构建训练数据。通过在单个8timesA800 GPU机器上训练0.3B的数据，LOGO使Llama-3-8B-Instruct-80K模型在处理实际长上下文任务时达到了与GPT-4相媲美的性能，同时保持了模型在其他任务（如语言建模和MMLU）上的原有能力。此外，LOGO可以扩展模型的上下文窗口大小，同时增强其生成性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 01:32:39 GMT</pubDate>
</item>
<item>
<title>基于分块计算的对比损失优化策略</title>
<link>https://arxiv.org/abs/2410.17243</link>
<guid>https://arxiv.org/abs/2410.17243</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新型的分块计算策略，显著提升对比损失训练性能，支持更大批量尺寸，降低显存消耗。</p><br /><br /><p><strong>摘要：</strong> 在对比损失的表示学习中，较大的批量尺寸通过提供更多负样本来提升模型性能。然而，批量尺寸的扩展受到GPU显存消耗的限制，主要是因为相似度矩阵的完全实例化。为了解决这一问题，本文提出了一种基于分块计算的策略，将对比损失的计算划分为任意小的块，从而避免完全物化相似度矩阵。此外，引入多层次分块策略，利用分布式系统的层次结构，在GPU层面采用基于环的通信优化同步，使用CUDA核心级别的融合核函数以减少I/O开销。实验结果表明，该方法可将批量尺寸扩展到前所未有的水平，例如，在8或32块A800 80GB上对CLIP-ViT-L/14模型进行对比训练时，支持4M或12M的批量尺寸而不损失准确性。与现有记忆高效解决方案相比，该方法在内存消耗上实现了两个数量级的减少，同时保持了相当的速度。代码将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17243" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:57:09 GMT</pubDate>
</item>
<item>
<title>STRING：提升大语言模型长上下文有效性的新方法</title>
<link>https://arxiv.org/abs/2410.18745</link>
<guid>https://arxiv.org/abs/2410.18745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出STRING以改善大语言模型有效上下文长度，通过重写无效位置显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 随着分布式训练和高效注意力机制的进步，大型语言模型（LLMs）的上下文窗口大小显著扩大。然而，最近的研究表明，开源LLMs的有效上下文长度通常低于训练长度的一半。本文将这一限制归因于LLMs在预训练和后期训练阶段形成的左偏相对位置频率分布，这妨碍了模型有效获取远距离信息。为了解决这一挑战，本文提出了一种新的位置嵌入方法：ShifTed Rotray position embeddING（STRING）。STRING在推理过程中，将经过良好训练的位置移至无效位置，从而在不增加额外训练的情况下，提升现有训练长度内的性能。实验结果显示，STRING显著改善了最新大型模型，如Llama3.1 70B和Qwen2 72B在流行长上下文基准RULER和InfiniteBench上的表现，分别提升了10分以上，并在开源LLMs中创造了新的最先进的结果。与商业模型相比，采用STRING的方法的Llama 3.1 70B甚至在性能上超越了GPT-4-128K，并明显胜过Claude 2和Kimi-chat。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:47:25 GMT</pubDate>
</item>
<item>
<title>Framer：互动帧插值与创意过渡</title>
<link>https://arxiv.org/abs/2410.18978</link>
<guid>https://arxiv.org/abs/2410.18978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Framer为图像间插值提供交互方式，支持用户选择关键点，提升插值效果与控制。</p><br /><br /><p><strong>摘要：</strong> 我们提出Framer用于交互式帧插值，旨在根据用户创意在两幅图像之间产生平滑过渡。我们的设计允许用户定制过渡过程，通过关键点的选择来调整运动轨迹。这一机制带来了两个显著的好处：首先，引入人机交互有助于缓解从一幅图像转换到另一幅图像时产生的多种可能性问题，从而增强局部动作的细致控制；其次，关键点作为最基本的交互形式，有助于建立帧间的对应关系，使模型能够处理更具挑战性的情况，例如起始和结束帧中的对象具有不同的形状和风格。此外，我们的系统还提供了“自动驾驶”模式，引入模块自动估计关键点并优化轨迹，以简化实际使用。大量实验结果表明，Framer在多种应用上表现出色，如图像变换、延时视频生成、卡通插值等。我们将发布代码、模型和接口以促进进一步的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:41:55 GMT</pubDate>
</item>
<item>
<title>通过数据中心化技术提升大语言模型的奖励建模</title>
<link>https://arxiv.org/abs/2410.18451</link>
<guid>https://arxiv.org/abs/2410.18451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一系列数据中心化技术来增强LLMs的奖励建模，结果显著提高了模型表现。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了一系列用于提升大语言模型（LLMs）奖励建模的方法，重点关注数据中心化技术。我们提出了高效的数据选择与过滤策略，以策划高质量的开源偏好数据集，最终形成了Skywork-Reward数据集合，该数据集仅包含8万个偏好对，显著小于现有的数据集。利用这一策划后的数据集，我们开发了Skywork-Reward模型系列，包含Skywork-Reward-Gemma-27B和Skywork-Reward-Llama-3.1-8B，其中前者在RewardBench排行榜上当前占据第一的位置。值得注意的是，我们的技术和数据集直接提升了许多顶级模型在RewardBench上的表现，凸显了我们在实际偏好学习应用中所作贡献的现实影响。通过本研究，我们展示了数据质量对模型性能的重要性，进一步指导将来在奖励建模领域的数据策划工作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:22:37 GMT</pubDate>
</item>
<item>
<title>CCI3.0-HQ：优化数据质量的高质量汉语语料库</title>
<link>https://arxiv.org/abs/2410.18505</link>
<guid>https://arxiv.org/abs/2410.18505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CCI3.0-HQ 是一个高质量的中文数据集，提升了语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 我们介绍了 CCI3.0-HQ（https://huggingface.co/datasets/BAAI/CCI3-HQ），这是一个 500GB 的高质量子集，来源于中文语料库 Internet 3.0（CCI3.0）（https://huggingface.co/datasets/BAAI/CCI3-Data）。该数据集采用了新颖的两阶段混合过滤流程，以显著提升数据质量。为了评估其有效性，我们在 100B 标记的多个数据集上，从头开始训练了一个 0.5B 参数的模型，结果在 10 个基准测试中，在零-shot 设置下，表现超越了 CCI3.0、SkyPile 和 WanjuanV1。高质量的过滤过程有效地提炼了 Qwen2-72B-instruct 模型的能力，最终使得紧凑的 0.5B 模型在中文网页数据分类任务中达到了最佳 F1 分数。我们相信，这个开放获取的数据集将促进高质量语言模型的广泛可用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18505" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:18:10 GMT</pubDate>
</item>
<item>
<title>模型编辑对语言模型表现的影响评估</title>
<link>https://arxiv.org/abs/2410.18785</link>
<guid>https://arxiv.org/abs/2410.18785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文评估了模型编辑方法对语言模型的影响，发现编辑会导致性能下降，且安全性受损。</p><br /><br /><p><strong>摘要：</strong> 本文对不同模型的编辑方法进行了全面评估，发现现有编辑方式在小规模知识更新时表现良好，但在编辑数量增加时容易导致模型性能下降和知识结构的破坏。研究结果显示，现有编辑方法在一般基准测试中导致了不可避免的性能下降，且当编辑数量稍多时，模型的内在知识结构会受到显著影响。此外，指令调优的模型在编辑后对通用知识的性能下降更少，大规模模型相较于小规模模型在编辑时更具抵抗力。然而，编辑后的模型安全性普遍减弱，即便是经过安全对齐的模型。研究表明当前的编辑方法更适合于小规模知识更新，激励着对更实用和可靠的编辑方法的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:14:39 GMT</pubDate>
</item>
<item>
<title>一致性模型的稳定一致性调优</title>
<link>https://arxiv.org/abs/2410.18958</link>
<guid>https://arxiv.org/abs/2410.18958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了稳定一致性调优（SCT）框架，显著提升一致性模型的生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的框架，通过将扩散模型的去噪过程建模为马尔可夫决策过程（MDP），并将一致性模型训练视为通过时序差分（TD）学习进行价值估计，从而更好地理解一致性模型。我们首先分析了现有一致性训练/调整策略的局限性，基于简单一致性调优（ECT）提出了稳定一致性调优（SCT），该方法通过利用得分恒等式进行方差减少学习。实验证明，SCT在CIFAR-10和ImageNet-64等基准数据集上表现出明显的性能提升。其中，SCT在ImageNet-64上的1步FID为2.42，2步FID为1.55，创下了一致性模型的最新一阶段技术（SoTA）成绩。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:12:23 GMT</pubDate>
</item>
<item>
<title>无界：基于生成模型的无尽角色生活模拟游戏</title>
<link>https://arxiv.org/abs/2410.18975</link>
<guid>https://arxiv.org/abs/2410.18975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">无界是一款使用生成模型的角色生活模拟游戏，提供开放式互动体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一款名为《无界》的生成无限游戏，该游戏通过生成模型超越了传统有限游戏的界限，灵感来源于James P. Carse对有限与无限游戏的分类。《无界》是一个角色生活模拟的沙盒游戏，玩家可以通过喂养、玩耍和引导自主虚拟角色，与其在虚拟世界中互动。为实现《无界》的开发，我们提出了在大型语言模型(LLM)和视觉生成领域的技术创新。具体而言，我们展示了(1)一个专业化、精炼的大型语言模型，该模型实时动态生成游戏机制、叙事和角色互动；(2)一种新的动态区域图像提示适配器(IP-Adapter)，确保角色在多个环境中的一致且灵活的视觉生成。我们通过定性和定量分析评价了我们的系统，显示出角色生活模拟、用户指令跟随、叙事连贯性以及角色和环境视觉一致性等方面相比传统相关方法有显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18975" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Oct 2024 00:08:39 GMT</pubDate>
</item>
<item>
<title>基于价值引导的策略引导：提升通用机器人政策的部署性能</title>
<link>https://arxiv.org/abs/2410.13816</link>
<guid>https://arxiv.org/abs/2410.13816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种通过重排序策略来提升通用机器人性能的方法，称为V-GPS。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于多样化演示数据集训练的大型通用机器人政策在控制各种机器人和获取广泛的操作技能方面显示出显著的效果。然而，这些政策的训练数据往往质量参差不齐，尤其是人类收集的演示不太可能完美完成任务。更大规模的数据集也使得挑选高质量示例变得困难。此外，优化数据在不同实体间的有效性仍不明确。为了应对这些挑战，本文提出了一种通用且广泛适用的方法，通过使用离线强化学习学习的价值函数在部署时对通用机器人政策的动作进行重排序，从而增强其性能。我们称之为价值引导的策略引导（V-GPS）。这一方法适用于多种不同的通用政策，无需微调或访问政策的权重。我们展示了相同的价值函数可以提升五种不同架构的最先进政策的性能，即使它们是在不同数据集上训练的，在12个任务的多个机器人平台上均取得了一致的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 22:55:04 GMT</pubDate>
</item>
<item>
<title>大视野合成模型 (LVSM)：一种可扩展的新视角合成方法</title>
<link>https://arxiv.org/abs/2410.17242</link>
<guid>https://arxiv.org/abs/2410.17242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVSM提供了一种全新方式，通过稀疏图像输入实现新视角合成，具有优越的质量与可扩展性。</p><br /><br /><p><strong>摘要：</strong> 我们提出了大视野合成模型（LVSM），这是一种基于变换器的新方法，旨在从稀疏视图输入中实现可扩展和通用的新视角合成。我们引入了两种架构：(1) 编码-解码的LVSM，首先将输入图像标记编码为固定数量的1D潜在标记，形成完全学习的场景表示，然后从这些标记解码出新视角图像；(2) 仅解码的LVSM，直接将输入图像映射到新视角输出，完全消除了中间场景表示。两种模型都摆脱了以往方法中使用的3D归纳偏见——从3D表示（如NeRF、3DGS）到网络设计（如视差投影、平面扫描），采取了完全数据驱动的方法。虽然编码-解码模型因其独立的潜在表示而实现了更快的推理，只有解码的LVSM却在质量、可扩展性和零样本泛化方面表现更佳，超越了之前的最佳方法1.5至3.5 dB PSNR。全面评估显示，这两种LVSM变体在多个数据集上均达到最先进的新视角合成质量。值得注意的是，我们的模型即便在计算资源减少（1-2个GPU）的情况下也超越了所有先前方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 21:53:00 GMT</pubDate>
</item>
<item>
<title>LongVU: 一种用于长视频理解的时空自适应压缩机制</title>
<link>https://arxiv.org/abs/2410.17434</link>
<guid>https://arxiv.org/abs/2410.17434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongVU提出了一种新颖的时空自适应压缩方法，有效减少长视频的视觉信息冗余。</p><br /><br /><p><strong>摘要：</strong> Multimodal Large Language Models (MLLMs) 在视频内容理解方面取得了显著进展，但处理长视频仍面临上下文长度的限制。为了解决这一问题，我们提出了 LongVU，一种时空自适应压缩机制，可以在保持长视频视觉细节的同时减少视频token数量。我们的思路是利用跨模态查询和帧间依赖关系，自适应地减少视频中的时间和空间冗余。具体而言，首先利用 DINOv2 特征去除相似度较高的冗余帧；接着，通过文本引导的跨模态查询进行选择性帧特征压缩；最后，根据时间依赖关系，在帧之间进行空间token的减少。我们的适应性压缩策略能够在给定的上下文长度内有效处理大量帧，而不会造成显著的视觉信息损失。LongVU 在多个视频理解基准测试中表现优异，特别是在 VideoMME 和 MLVU 等长期视频理解任务上表现突出。此外，在轻量级 LLM 的情况下，LongVU 也能以较小的规模实现视频理解性能的领先。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 16:18:05 GMT</pubDate>
</item>
<item>
<title>DynamicCity：一种动态城市四维激光雷达生成框架</title>
<link>https://arxiv.org/abs/2410.18084</link>
<guid>https://arxiv.org/abs/2410.18084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DynamicCity是一个新颖的4D LiDAR生成框架，能够捕捉动态环境的演变，超越现有静态场景生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着LiDAR场景生成技术的快速发展，现有方法主要集中于生成静态和单帧场景，忽视了现实世界驾驶环境的动态特性。为此，本文提出DynamicCity，一个新颖的4D LiDAR生成框架，能够生成大规模、高质量的LiDAR场景，捕捉动态环境的时间演变。DynamicCity主要由两个关键模型组成：第一是一个变分自编码器（VAE）模型，用于学习HexPlane作为紧凑的4D表示。通过采用新颖的投影模块，DynamicCity有效压缩4D LiDAR特征为六个2D特征图，从而显著提高了HexPlane拟合质量。同时，采用扩展与挤压策略并行重建3D特征体，提高了网络训练效率和重建精度。第二是基于Diffusion Transformer（DiT）的模型，用于HexPlane生成。我们提出了填充回放操作，将HexPlane的六个特征平面重组为一个平方的2D特征图，从而支持多种4D生成应用。通过在CarlaSC和Waymo数据集上进行的大量实验显示，DynamicCity在多个指标上显著超越现有的4D LiDAR生成方法。代码将公开以促进未来研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18084" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 09:34:33 GMT</pubDate>
</item>
<item>
<title>MedINST: 一种多领域多任务的生物医学指令元数据集</title>
<link>https://arxiv.org/abs/2410.13458</link>
<guid>https://arxiv.org/abs/2410.13458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedINST是一个创新的生物医学指令元数据集，包含133个 NLP 任务和700多万个训练样本。</p><br /><br /><p><strong>摘要：</strong> 在医学分析领域，大型语言模型（LLM）技术的整合已带来显著进展，但缺乏大型、多样化和良好注释的数据集仍然是一个主要挑战。医学数据和任务在格式、大小等方面各不相同，需要广泛的预处理和标准化，以有效用于培训LLM。为了解决这些挑战，我们引入了MedINST，生物医学指令的元数据集，这是一个新颖的多领域、多任务指令元数据集。MedINST包含133个生物医学自然语言处理（NLP）任务和超过700万条训练样本，使其成为迄今为止最全面的生物医学指令数据集。利用MedINST作为元数据集，我们策划了MedINST32，这是一个具有不同任务难度的挑战性基准，旨在评估LLM的泛化能力。我们对多个LLM进行了MedINST的微调，并在MedINST32上进行了评估，展示了跨任务的增强泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 09:33:36 GMT</pubDate>
</item>
<item>
<title>ARKit LabelMaker：首个大规模真实世界3D数据集及其语义标注</title>
<link>https://arxiv.org/abs/2410.13924</link>
<guid>https://arxiv.org/abs/2410.13924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了ARKit LabelMaker，首个大规模真实世界的3D数据集，并展示其在语义分割任务中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ARKit LabelMaker，这是首个大规模、真实世界的3D数据集，并配有密集的语义标注。我们在ARKitScenes数据集的基础上，采用自动生成的密集语义标注，补充了现有的数据集。为适应大规模预训练的需求，我们扩展了LabelMaker自动标注流程，引入了最先进的分割模型，并确保在大规模处理中的鲁棒性。此外，我们还在ScanNet和ScanNet200数据集上推动了3D语义分割模型的最新性能，证明了我们生成的数据集的有效性。这项工作不仅是3D视觉的关键进展，也是推动该领域发展的重要步骤。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 08:14:13 GMT</pubDate>
</item>
<item>
<title>多语言奖励模型评估基准 M-RewardBench 的构建与分析</title>
<link>https://arxiv.org/abs/2410.15522</link>
<guid>https://arxiv.org/abs/2410.15522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究建立了 M-RewardBench，评估多语言奖励模型在多样语言设置中的表现，并发布相关数据集与代码。</p><br /><br /><p><strong>摘要：</strong> 在本研究中，我们系统性地评估了奖励模型 (RMs) 在多语言环境中的表现。首先，我们构建了首个多语言奖励模型评估基准 M-RewardBench，包含 2.87k 个偏好实例，涵盖 23 种类型多样的语言，旨在测试 RMs 的对话、安全性、推理和翻译能力。随后，我们对多种奖励模型在 M-RewardBench 上进行了严格的评估，提供了有关其在不同语言中表现的新见解。研究发现，RMs 在英语和非英语语言之间存在显著的性能差距，并且 RMs 的偏好会随着语言的不同而显著变化。此外，我们还发现了不同多语言因素对 RMs 性能的影响，特别是翻译质量提升与高资源语言的模型表现之间的正相关性。为促进对多语言设置下 RMs 评估的理解，本研究发布了 M-RewardBench 数据集及其代码库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 07:41:28 GMT</pubDate>
</item>
<item>
<title>基于合成数据集的直接偏好优化推动文本到图像模型的进步</title>
<link>https://arxiv.org/abs/2410.18013</link>
<guid>https://arxiv.org/abs/2410.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨一种基于合成数据集的DPO方法，以提高文本到图像模型的性能。</p><br /><br /><p><strong>摘要：</strong> 直接偏好优化（DPO）已成为与人类反馈对齐文本到图像（T2I）模型的有效方法。然而，成功应用DPO需要大量资源来收集和标注大规模数据集，其中百万级生成图像及其人类偏好标签的需求尤为突出。此外，随着T2I模型的快速发展，这些人类偏好数据集很快可能变得过时。本研究提出了一种可扩展的方法，通过预训练的奖励函数生成配对图像的偏好，从而实现大规模合成数据集的收集，显著提高数据集收集效率。我们展示了此类数据集不仅能实现多个模型的预测平均化，还能收集排名偏好而非简单的配对偏好。进一步地，本文引入RankDPO，利用排名反馈增强基于DPO的方法。在使用我们合成生成的偏好数据集“Syn-Pic”对SDXL和SD3-Medium模型应用RankDPO后，在T2I-Compbench、GenEval和DPG-Bench基准上，模型的提示遵循性以及视觉质量（通过用户研究验证）都有所提升。这一流程提供了一个实用且可扩展的方案，来构建更好的偏好数据集，以提升文本到图像模型的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 06:43:59 GMT</pubDate>
</item>
<item>
<title>针对多模态大语言模型的TP-Eval评估框架</title>
<link>https://arxiv.org/abs/2410.18071</link>
<guid>https://arxiv.org/abs/2410.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TP-Eval评估框架，以个性化提示减少多模态大语言模型评估偏差，提升模型能力识别。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型（MLLMs）逐渐受到关注，其评估变得至关重要。然而，目前的基准测试往往忽视了提示敏感性的问题，微小的提示变化可能导致显著的性能波动，从而低估模型的真实能力。同时，不同模型对提示的偏好存在差异，使用相同的提示会导致评估偏差。本文分析了现有基准的局限性，并提出了一种新的评估框架TP-Eval，该框架引入了一种提示个性化方法，以减少评估偏差并充分挖掘模型潜力。TP-Eval将原始提示重写为针对不同模型的定制提示，特别提出了一系列针对MLLM评估场景的模块设计。实验结果表明，该方法有效揭示了模型能力，TP-Eval将为学术界开发更全面、可靠的MLLM评估基准发挥重要作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 06:05:07 GMT</pubDate>
</item>
<item>
<title>基于轻量级多模态应用控制的手机应用代理架构</title>
<link>https://arxiv.org/abs/2410.17883</link>
<guid>https://arxiv.org/abs/2410.17883</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新型手机应用控制架构LiMAC，用于高效交互和控制Android应用。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个创新的手机应用控制架构，称为“应用代理（app agents）”，旨在提高不同Android应用之间的交互和控制效率。所提出的轻量级多模态应用控制（LiMAC）接收文本目标和过去的移动观察序列作为输入，包括屏幕截图和相应的UI树，以生成准确的操作。为了解决智能手机固有的计算约束，LiMAC集成了一种小型动作变换器（AcT）及微调的视觉语言模型（VLM），用于实时决策和任务执行。我们在两个开源移动控制数据集上评估了LiMAC，结果表明我们的小型化方法在性能上优于微调版本的开源VLM，如Florence2和Qwen2-VL。同时，相较于利用闭源基础模型（如GPT-4o）的提示工程基准，LiMAC的表现也显著优越。具体而言，LiMAC在整体操作准确率上提高了高达19%，相较于微调的VLM，较提示工程基准则提高了42%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17883" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 04:42:04 GMT</pubDate>
</item>
<item>
<title>多图像增强直接偏好优化(MIA-DPO)在视觉偏好对齐中的应用</title>
<link>https://arxiv.org/abs/2410.17637</link>
<guid>https://arxiv.org/abs/2410.17637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MIA-DPO方法，通过网格拼贴等方式增强多图像数据，优化视觉偏好对齐。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的视觉偏好对齐方法——多图像增强直接偏好优化(MIA-DPO)，旨在处理多图像输入的复杂性。现有的视觉对齐方法主要针对单幅图像，难以有效应对多图像任务的挑战，原因包括多样化训练数据的稀缺与多图像数据标注的高成本。MIA-DPO方法通过将单幅图像数据扩展为网格拼贴或画中画形式的非相关图像，显著降低了多图像数据标注的成本。我们观察到LVLMs的注意力值在不同图像之间变化显著，利用这些注意力值来识别和过滤出可能被模型错误关注的拒绝响应。MIA-DPO在构建选择/拒绝对时，不依赖（i）人工标注，（ii）额外数据或（iii）外部模型或API，确保了方法的高效性和适应性。该方法适用于多种架构，并在五个多图像基准测试中表现优于现有方法，在LLaVA-v1.5上实现了平均3.0%的性能提升，在最新的InternLM-XC2.5上提升4.3%。此外，MIA-DPO对模型理解单幅图像的能力影响甚微。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 04:23:39 GMT</pubDate>
</item>
<item>
<title>基于自回归语言模型的扩散语言模型构建与评估</title>
<link>https://arxiv.org/abs/2410.17891</link>
<guid>https://arxiv.org/abs/2410.17891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用自回归模型构建扩散模型的方法，生成流畅文本并进行有效评估。</p><br /><br /><p><strong>摘要：</strong> 扩散语言模型（DLMs）作为文本生成建模的新范式，可能解决自回归（AR）模型的局限性。然而，目前的DLMs在规模上与AR模型相比仍存在不足，且在语言建模基准上的比较不够公平。同时，从头训练时，大规模扩散模型面临挑战。鉴于开源自回归语言模型的普遍性，本文提出了一种适应这些模型以构建文本扩散模型的方法。我们阐明了自回归模型与扩散建模目标之间的关系，并引入了一种简单的持续预训练方法来训练扩散模型。通过系统评估语言建模、推理和常识基准，我们展示了可以将从127M到7B参数的自回归模型（如GPT2和LLaMA）转换为扩散模型DiffuGPT和DiffuLLaMA，训练使用不到200B的tokens。实验结果表明，这些模型超越了早期的DLMs，并在性能上与自回归模型竞争。我们发布了一套DLMs（包括127M、355M和7B参数），能够生成流畅文本，进行上下文学习，填补中间内容且不重新排序提示，并能遵循指令。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 04:21:03 GMT</pubDate>
</item>
<item>
<title>面向世界模拟器的双重评估框架：WorldSimBench</title>
<link>https://arxiv.org/abs/2410.18072</link>
<guid>https://arxiv.org/abs/2410.18072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WorldSimBench评估框架，分类预测模型功能并评估世界模拟器的效果。 </p><br /><br /><p><strong>摘要：</strong> 近年来，预测模型在预测物体和场景的未来状态方面表现出色。然而，基于内在特征的分类方式尚缺乏，导致预测模型发展受限。此外，现有基准不能有效评估具高度能力的具身预测模型。从这一角度出发，本文将预测模型的功能分为等级，并首次提出一个名为WorldSimBench的双重评估框架。该框架包括显性感知评估和隐性操作评估，涵盖了来自视觉角度的人类偏好评估和在具身任务中的动作级评估，涉及开放式具身环境、自动驾驶和机器人操作三种具有代表性的具身场景。在显性感知评估中，我们引入HF-Embodied 数据集，这是一个基于精细人类反馈的视频评估数据集，用于训练符合人类认知标准的人类偏好评估器，以显式评估世界模拟器的视觉真实感。在隐性操作评估中，我们通过评估生成的视频是否能够准确转化为动态环境中的正确控制信号，来评估世界模拟器的视频动作一致性。我们的综合评估为推动视频生成模型的进一步创新提供了重要见解，标志着世界模拟器朝向具身人工智能的重要进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.18072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Oct 2024 04:15:52 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的复合人工智能系统优化研究</title>
<link>https://arxiv.org/abs/2410.16392</link>
<guid>https://arxiv.org/abs/2410.16392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文调查了使用LLM优化复合AI系统的原则和新兴趋势。</p><br /><br /><p><strong>摘要：</strong> 本文通过调查基于大型语言模型（LLM）的复合人工智能系统优化的最新进展，探讨了几种复合AI系统的原型，以及LLM驱动的端到端优化方法。尤其强调了LLM作为优化器的优势，它能够有效避免梯度计算，并生成复杂的代码和指令，从而简化优化过程。研究还结合程序分析的概念，为如何提示LLM优化复合AI系统提供了统一的视角。论文总结了当前的挑战和未来方向，展望LLM在复合AI系统中的广泛影响。完整参考文献列表可在指定链接中找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 23:12:07 GMT</pubDate>
</item>
<item>
<title>智能内窥镜技术在结肠镜检查中的前沿探索</title>
<link>https://arxiv.org/abs/2410.17241</link>
<guid>https://arxiv.org/abs/2410.17241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨智能内窥镜技术在结肠镜检查中的应用与挑战，提出多模态解决方案。</p><br /><br /><p><strong>摘要：</strong> 结肠镜检查是目前检测结直肠癌最敏感的筛查方法之一。本研究旨在探索智能结肠镜技术的前沿及其在多模态医学应用中的潜力。首先，通过分类、检测、分割和视觉语言理解四个任务评估结肠镜场景感知的当前数据驱动和模型驱动格局。这一评估帮助我们识别特定领域的挑战，并揭示结肠镜检查中的多模态研究仍有待深入探索。为迎接多模态时代的到来，我们建立了三个基础性举措：大规模多模态指令调优数据集ColonINST、专为结肠镜设计的多模态语言模型ColonGPT，以及一个多模态基准测试。为了持续关注该快速发展领域的最新动态，我们提供了一个公共网站，供用户获取最新信息： https://github.com/ai4colonoscopy/IntelliScope。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 20:17:31 GMT</pubDate>
</item>
<item>
<title>3DGS-增强器：提升3D高斯点云渲染质量的创新管道</title>
<link>https://arxiv.org/abs/2410.16266</link>
<guid>https://arxiv.org/abs/2410.16266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3DGS-Enhancer通过视频扩散先验改进3DGS表现，实现高质量新视图生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为3DGS-Enhancer的新管道，旨在提升3D高斯点云(3DGS)表示的质量。尽管3DGS在生成高质量的新视图方面取得了显著进展，但在稀疏输入视图的挑战性设置下，生成高质量的新视图仍然困难，通常会导致在欠采样区域出现明显的伪影。我们提出的3DGS-Enhancer方法通过利用2D视频扩散先验，解决了三维视图一致性的问题，将其重构为视频生成过程中的时间一致性。该方法恢复了渲染的新视图的视图一致性潜在特征，并通过一个时空解码器与输入视图集成。然后，这些增强的视图用于微调初始的3DGS模型，从而显著改善其渲染性能。在大规模的无界场景数据集上的广泛实验表明，3DGS-Enhancer在重建性能和高保真渲染结果上优于最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 17:25:03 GMT</pubDate>
</item>
<item>
<title>xGen-MM-Vid：高效捕捉视频时序信息的多模态语言模型</title>
<link>https://arxiv.org/abs/2410.16267</link>
<guid>https://arxiv.org/abs/2410.16267</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">xGen-MM-Vid 采用时序编码器高效处理视频信息，具有小模型高准确率的优势。</p><br /><br /><p><strong>摘要：</strong> xGen-MM-Vid（BLIP-3-Video）是一种为视频设计的多模态语言模型，特别着重于高效捕捉多个帧的时序信息。该模型引入了'时序编码器'，除去传统的视觉标记器，通过将多个帧的标记序列映射为一组紧凑的视觉标记，从而显著减少视觉标记的数量（例如，使用32个标记而不是4608个）。研究探索了包括可学习的时空池化和诸如Token Turing Machines等序列模型的不同类型的时序编码器。实验结果表明，BLIP-3-Video在视频问答任务中的准确率与更大规模的最先进模型（如34B参数模型）相当，同时模型仅有4B参数，且通过使用更少的视觉标记实现了更高的效率。该项目的官方网站为 https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16267" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 14:05:28 GMT</pubDate>
</item>
<item>
<title>增强视觉语言模型的推理能力：基于详细理由的训练与强化学习</title>
<link>https://arxiv.org/abs/2410.16198</link>
<guid>https://arxiv.org/abs/2410.16198</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过细化训练数据和强化学习提升视觉语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 在视觉语言模型（VLMs）中，链式推理（CoT）对于提升可解释性和可信度至关重要。然而，目前的训练方法缺乏丰富的CoT推理数据，主要依赖于短小的注释和最低限度的推理。我们展示了仅用短答案训练VLM在需要更详细回应的推理任务中的泛化能力不足。为了解决此问题，我们提出了一种双重方法。首先，利用GPT-4o模型提取推理过程，从而丰富训练数据，并对VLM进行微调，提升其CoT性能。其次，我们应用强化学习进一步校准推理质量。具体而言，通过将模型生成的推理链与注释的短答案进行比较，构建正负样本对，进而使用Direct Preference Optimization算法来提升模型的推理能力。实验结果显示，在基准数据集上CoT推理有显著提升，并且在直接答案预测的泛化能力上也表现良好。本工作强调在训练中融入详细推理的重要性，并利用强化学习来增强VLM的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16198" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 13:48:55 GMT</pubDate>
</item>
<item>
<title>数学推理参数的隔离与干预：Math Neurosurgery 方法</title>
<link>https://arxiv.org/abs/2410.16930</link>
<guid>https://arxiv.org/abs/2410.16930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新的方法 MathNeuro，用于隔离大语言模型中的数学推理参数，以改进模型的数学表现。</p><br /><br /><p><strong>摘要：</strong> 数学推理是大型语言模型（LLM）研究的关键领域，但关于其在模型参数中的编码及是否可独立于其他技能进行干预的研究较少。为了改善数学能力而不影响模型的其他功能，我们提出了数学神经外科（Math Neurosurgery，MathNeuro）方法。该方法通过前向传播，利用权重和激活值计算参数重要性，从而识别数学专用参数。MathNeuro通过删除非数学行为中重要的参数，成功地剔除了LLM的数学推理能力，但保留了其一般语言能力。我们的实验表明，在GSM8K数据集上，按照MathNeuro识别的参数进行修剪，能够提升预训练或指令调优的LLM性能4%-17%。此外，MathNeuro在数据利用效率方面表现良好，只需一个样本即可有效识别数学专用参数。此研究展示了未来在数学专用参数干预领域的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 09:15:06 GMT</pubDate>
</item>
<item>
<title>减轻长距离视觉-指令交互影响的新方法：同心因果注意力</title>
<link>https://arxiv.org/abs/2410.15926</link>
<guid>https://arxiv.org/abs/2410.15926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示RoPE在LVLM中导致视觉信息误导的问题，并提出同心因果注意力（CCA）作为解决方案。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型视觉语言模型（LVLM）表现出卓越的零样本对话和推理能力，尤其是在处理多模态查询时。然而，它们仍然面临着物体幻觉的问题，即生成的文本回应与输入图像不一致。我们的初步研究表明，物体幻觉与RoPE（旋转位置编码）密切相关，RoPE作为一种广泛采用的位置信息建模设计，因其长期衰减特性，使得LVLM在视觉线索与指令标记之间的距离较大时更易出现幻觉。此外，我们观察到在多模态对齐中颠倒视觉标记的顺序会产生相似的效果。测试结果显示，RoPE的长期衰减对捕捉视觉与指令交互带来挑战。为此，我们提出了一种简单而有效的定位对齐策略——同心因果注意力（CCA），旨在减轻RoPE的长期衰减影响，进一步缩短视觉标记与指令标记之间的相对距离。通过CCA，视觉标记能够更好地与指令标记交互，从而增强模型的感知能力，降低物体幻觉的发生率。我们的定位对齐方法在多个物体幻觉基准测试中显著优于现有的幻觉缓解策略。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 08:54:06 GMT</pubDate>
</item>
<item>
<title>自我引导优化：一种无人工标注的偏好信号生成方法</title>
<link>https://arxiv.org/abs/2410.17131</link>
<guid>https://arxiv.org/abs/2410.17131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新算法SSO，能够无人工干预生成高质量的偏好信号，提升自动化对齐效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新算法，自我引导优化（Self-Steering Optimization, SSO），旨在通过最小化人工干预生成高质量的偏好信号，以提升自动化对齐的效果。SSO在迭代训练过程中依据预定义原则自动生成偏好信号，进而消除了对人工标注的需求。该算法在保持选择与拒绝响应之间准确间隔的同时，确保它们均符合当前策略模型的学习能力。SSO不仅支持策略模型的在线和离线训练，还能增强奖励模型的训练效果。我们通过两个基础模型Qwen2和Llama3.1验证了SSO的有效性，结果表明SSO在整个迭代训练过程中产生了准确的、符合政策的偏好信号。SSO在没有任何人工标注或外部模型的情况下，在六个主观或客观基准上显著提升了性能。此外，通过SSO生成的偏好数据还显著改善了奖励模型在Rewardbench上的表现。我们的研究为偏好优化提供了一种可扩展的方法，为更高效、更有效的自动化对齐奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 05:49:09 GMT</pubDate>
</item>
<item>
<title>SpectroMotion：结合3D高斯点云与物理基础渲染的动态高光场景重建新方法</title>
<link>https://arxiv.org/abs/2410.17249</link>
<guid>https://arxiv.org/abs/2410.17249</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的方法，结合3D Gaussian Splatting与物理基础渲染，重建动态高光场景。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了SpectroMotion，一种新颖的方法，将3D Gaussian Splatting (3DGS)、物理基础渲染 (PBR)和变形场结合，以重建动态高光场景。以往扩展3DGS的方法在准确表示高光表面方面存在不足。我们的创新在于引入了一种残差校正技术，用于动态变形过程中的表面法线计算，并配备了适应时变光照条件的可变形环境贴图。此外，我们实施了一种粗到细的训练策略，显著提升了场景几何和高光颜色预测的质量。实验表明，我们的模型在动态高光对象的视图合成上优于以往方法，并且是现有3DGS方法中唯一能够合成真实世界动态高光场景的技术，尤其在渲染复杂、动态和高光场景方面表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17249" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 04:41:54 GMT</pubDate>
</item>
<item>
<title>PyramidDrop：提高大型视觉语言模型效率的视觉冗余降低策略</title>
<link>https://arxiv.org/abs/2410.17247</link>
<guid>https://arxiv.org/abs/2410.17247</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出PyramidDrop策略，有效减少LVLM中的视觉冗余，显著提升训练和推理效率。</p><br /><br /><p><strong>摘要：</strong> 随着大型视觉语言模型（LVLM）在图像输入信息表达中的广泛应用，传统方法往往需要大量的图像token，这增加了计算成本，尤其在输入图像分辨率提高时，计算需求呈现平方级增长，影响了训练和推理的效率。虽然有研究通过在模型初期减少图像token数量的方法进行了尝试，但这些方法往往导致重要信息的丢失，从而降低了模型性能。本研究通过实证分析发现，LVLM在浅层模型中需要所有视觉token，而在深层模型中，token的冗余性逐渐增加。基于此，我们提出了PyramidDrop策略，该策略在每个阶段结束时按预定义比例丢弃部分图像token，形成金字塔式的视觉token结构，这一过程基于轻量级的相似性计算，时间开销可以忽略不计。大量实验表明，PyramidDrop可以将LLaVA-NeXT的训练时间提升40%，推理FLOPs减少55%，且性能与对比方法相当。此外，PyramidDrop还可以作为推理加速的即插即用策略，具备更优的性能和更低的推理成本。我们希望PyramidDrop所提供的洞察与方法能够启发未来的研究，深入探讨图像token在LVLM中的作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17247" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 04:36:27 GMT</pubDate>
</item>
<item>
<title>JMMMU：首个针对日语的大规模多模态模型基准测试</title>
<link>https://arxiv.org/abs/2410.17250</link>
<guid>https://arxiv.org/abs/2410.17250</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了JMMMU，一个针对日语多模态模型的基准测试，探讨了文化对模型性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了JMMMU（Japanese MMMU），这是第一个旨在评估大型多模态模型（LMMs）在日本文化背景下的专家级任务的大规模日语基准。为了便于全面的文化意识评估，JMMMU包含两个互补的子集：（i）文化无关（CA）子集，选择文化独立的主题（例如，数学）并翻译成日语，使其能够与其英语对应物MMMU进行逐一比较；（ii）文化特定（CS）子集，包含新创作的主题，反映日本文化背景。通过CA子集的测试，我们观察到在日语环境下许多LMMs的性能出现下降，这主要归因于语言差异。使用CS子集则揭示了模型对日本文化的理解不足。此外，结合这两个子集，我们发现一些LMMs在CA子集中表现良好，而在CS子集中表现不佳，这表明它们对日语的理解存在很多表面化的问题，缺乏深度的文化理解。我们希望这一工作不仅能够推动LMM在日语领域的表现，同时也为打造高标准和文化多样性的多语言LMM开发基准提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17250" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 03:52:20 GMT</pubDate>
</item>
<item>
<title>EvoPress：一种适应性动态压缩的广义框架</title>
<link>https://arxiv.org/abs/2410.14649</link>
<guid>https://arxiv.org/abs/2410.14649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了新框架EvoPress，实现LLM动态压缩，优化精度与效率。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型（LLMs）计算成本的增加，LLM压缩方法如量化、稀疏化和结构化剪枝盛行。然而，当前的方法往往依据启发式规则评估各层对模型性能的影响，这些假设如误差单调性，未必适用于实际情况。本文提出了EvoPress，一种新的动态压缩框架，具有可证明的收敛性及低样本和评估复杂性。研究表明，误差单调性在LLMs中并不成立，导致某些压缩模型的总体层级误差和低于其他模型，却表现更差。EvoPress通过动态调整不同层的压缩水平，达到了优化后的压缩效果。我们在Llama、Mistral和Phi模型上进行了实验，结果显示EvoPress在结构剪枝、非结构稀疏性和动态位宽量化等多个压缩方法上设定了新的最优结果。代码已公开于GitHub上，促进更多研究者在该领域的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 02:56:38 GMT</pubDate>
</item>
<item>
<title>MiniPLM：高效灵活的知识蒸馏框架用于预训练语言模型</title>
<link>https://arxiv.org/abs/2410.17215</link>
<guid>https://arxiv.org/abs/2410.17215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniPLM通过精炼训练数据分布，提升小型语言模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 知识蒸馏（KD）被广泛用于通过大型教师语言模型（LM）训练小型高性能学生LM。尽管KD在微调过程中有效，但其在预训练阶段面临效率、灵活性和有效性等挑战。现有的方法通常因在线教师推理导致高计算成本，或要求教师与学生LM之间的标记匹配，此外，还可能导致教师生成的训练数据难度和多样性丧失。为了应对这些问题，我们提出了MiniPLM框架，通过精炼训练数据分布以融入教师的知识，从而实现学生LM的预训练。MiniPLM通过离线教师LM推理提高效率，让多种学生LM能够在不增加训练时间成本的情况下进行KD；同时，MiniPLM仅在训练语料库上操作，增强了跨模型家族的灵活性。此外，MiniPLM通过利用大模型与小模型之间的差异，提升了训练数据的难度和多样性，帮助学生LM获取更加多样化和复杂的知识。大量实验表明，MiniPLM在9个广泛使用的下游任务上提升了学生LM的性能，改善了语言建模能力，并减少了预训练计算量。MiniPLM的优势还体现在大规模预训练的可扩展性上，我们的模型、代码和数据可在 https://github.com/thu-coai/MiniPLM获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.17215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Oct 2024 02:38:05 GMT</pubDate>
</item>
<item>
<title>寻找模仿阈值：对文本到图像模型版权侵权的研究</title>
<link>https://arxiv.org/abs/2410.15002</link>
<guid>https://arxiv.org/abs/2410.15002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨文本到图像模型在模仿概念时的训练阈值，以及其对版权和隐私的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了文本到图像模型的训练数据集中概念频率与模型模仿能力之间的关系，提出了寻找模仿阈值（Finding the Imitation Threshold, FIT）这一新问题。该研究聚焦于人脸和艺术风格两个领域，通过创建四个数据集并评估三种文本到图像模型，探讨了训练数据量对模仿能力的影响。结果表明，这些模型的模仿阈值范围在200到600张图像之间，具体取决于领域和模型。模仿阈值的发现为版权侵权的实证基础提供了支持，同时也为希望遵循版权和隐私法律的文本到图像模型开发者提供了指导原则。本研究发布的代码和数据可在 https://github.com/vsahil/MIMETIC-2.git 获得，项目网站则可访问 https://how-many-van-goghs-does-it-take.github.io。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15002" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 15:55:59 GMT</pubDate>
</item>
<item>
<title>Agent-to-Sim (ATS)：从视频学习3D代理的交互行为模型</title>
<link>https://arxiv.org/abs/2410.16259</link>
<guid>https://arxiv.org/abs/2410.16259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ATS框架通过视频观察学习3D代理的自然行为，支持真实到仿真转移。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Agent-to-Sim (ATS)框架，该框架通过随机录像收集，学习3D代理（如动物和人类）的交互行为模型。与以往依赖于标记跟踪和多视角摄像头的方法不同，ATS通过在单一环境中 طول时间（例如一个月）记录的非侵入式视频观察，获取自然行为数据。为了实现持续的3D跟踪，ATS开发了一种粗到细的登记方法，能够在一个规范的3D空间中持续跟踪代理和摄像头，从而构建一个完整的、持久的时空4D表示。利用从4D重建中获取的代理感知和运动的配对数据，ATS训练生成代理行为的模型。这一框架支持从视频录制到互动行为模拟器的真实到仿真转移，展示了在宠物（如猫、狗和兔子）及人类的实验结果，这些结果均基于智能手机拍摄的单目RGBD视频。此项研究为无创3D代理行为分析提供了新方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 15:28:31 GMT</pubDate>
</item>
<item>
<title>简约模型与上下文学习：关联探索与改进建议</title>
<link>https://arxiv.org/abs/2410.14086</link>
<guid>https://arxiv.org/abs/2410.14086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了简约模型在上下文学习中的重要性，提出了通过预先编码法优化学习过程的方法。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了机器学习中通用性的目标，以及在实践中观察到的简单模型常常能够更好地泛化的现象，即奥卡姆剃刀原则。尽管简单模型的重要性显而易见，当前的大多数机器学习方法仅仅关注于最小化训练误差，偶尔通过正则化或架构设计间接促进简约性。在这篇文章中，我们将奥卡姆剃刀原则与上下文学习联系起来，深入分析了某些序列模型（如Transformer）在推理时从过去的观察中学习的能力。特别是，我们展示了用于训练上下文学习者的下一个标记预测损失与一种称为预先编码（prequential coding）的数据压缩技术是直接等价的，最小化此损失实际上等同于同时最小化训练误差与基于上下文隐含学习的模型复杂性。我们的理论和实证实验不仅为上下文学习提供了规范性的分析，也揭示了现有上下文学习方法的不足之处，并提出了改进建议。代码已在 https://github.com/3rdCore/PrequentialCode 提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 12:21:20 GMT</pubDate>
</item>
<item>
<title>基于动态深度的混合层跳过模型</title>
<link>https://arxiv.org/abs/2410.13184</link>
<guid>https://arxiv.org/abs/2410.13184</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种通过Router-Tuning和MindSkip优化Mixture of Depths的训练成本和性能。</p><br /><br /><p><strong>摘要：</strong> 传统的Transformer模型在处理每个输入标记时分配固定的计算资源，造成低效和不必要的计算。为了解决这一问题，本文提出了Mixture of Depths (MoD)的动态深度调整方法，通过跳过不重要的层来提高计算效率。然而，现有的MoD方法仍需进一步探索，主要面临两个挑战：(1) 由于需要训练整个模型及其路由器，导致高昂的训练成本；(2) 跳过重要层可能导致性能下降。为应对第一个问题，本文提出了Router-Tuning方法，仅在小数据集上微调路由器，从而显著降低全模型训练的计算开销。针对第二个挑战，本文提出了MindSkip，采用动态深度的注意力机制，确保在保留模型性能的同时，显著提升计算和内存效率。通过大量实验，验证了我们的方法在实现竞争性结果的同时，显著提高了计算效率，如21%的速度提升和仅0.2%的性能下降。相关代码已在https://github.com/CASE-Lab-UMD/Router-Tuning发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13184" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 12:16:10 GMT</pubDate>
</item>
<item>
<title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title>
<link>https://arxiv.org/abs/2410.15460</link>
<guid>https://arxiv.org/abs/2410.15460</guid>
<content:encoded><![CDATA[
As large language models (LLMs) become increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations-outputs that are factually inaccurate or irrelevant to user input-have grown. Our research investigates the relationship between the training process and the emergence of hallucinations to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M-12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce SEnsitive Neuron Dropout (SeND), a novel training protocol designed to mitigate hallucinations by reducing variance during training. SeND achieves this by deterministically dropping neurons with significant variability on a dataset, referred to as Sensitive Neurons. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SeND to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to domains such as Wikipedia and Medical datasets.
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 11:59:59 GMT</pubDate>
</item>
<item>
<title>Ichigo：一种基于混合模态的语音与文本处理模型</title>
<link>https://arxiv.org/abs/2410.15316</link>
<guid>https://arxiv.org/abs/2410.15316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Ichigo，一个混合模态模型，通过统一的transformer架构高效处理语音和文本。</p><br /><br /><p><strong>摘要：</strong> Large Language Models（LLMs）在自然语言处理领域取得了显著突破。然而，将其应用于语音任务依然面临挑战，特别是在音频和文本模态的整合方面。本文提出了Ichigo，一种混合模态模型，能够无缝处理交替序列的语音和文本。Ichigo采用了令牌化的早期融合方法，将语音量化为离散令牌，并对语音和文本模态使用统一的transformer架构。这种方法使得跨模态的联合推理与生成成为可能，而无需单独的适配器。我们详细介绍了包括在多语言语音识别数据集上进行预训练和在精心策划的指令数据集上进行微调的综合训练方法。实验结果显示，Ichigo在语音问答基准测试中表现出色，超越了现有的开源语音语言模型，同时与级联系统的结果相当。值得注意的是，Ichigo的首次令牌生成延迟仅为111毫秒，显著低于其他现有模型。我们的方法不仅推动了多模态人工智能的发展，也为小型研究团队有效贡献于开源语音语言模型提供了框架。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 11:40:31 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的连续马尔可夫决策过程动态预测</title>
<link>https://arxiv.org/abs/2410.11711</link>
<guid>https://arxiv.org/abs/2410.11711</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种利用预训练大型语言模型进行连续马尔可夫决策过程动态预测的方法。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）零-shot 能力的不断提升，其应用领域已扩展至自然语言处理之外的多个领域。在强化学习中，尽管 LLMs 在文本环境中得到了广泛应用，但其在连续状态空间中的整合仍然尚未得到充分研究。本文探讨如何利用预训练的 LLMs 进行连续马尔可夫决策过程的动态预测。我们确定了处理多元数据和引入控制信号这两个关键挑战，这些挑战限制了 LLMs 在此设置下的潜力，并提出了解决方案——解耦式上下文学习（Disentangled In-Context Learning, DICL）。我们在模型驱动策略评估和数据增强的离线强化学习两个设置中展示了该方法的概念性应用，并提供了对相关方法的理论分析。实验结果进一步验证了我们的方法能够生成良好校准的置信度估计。代码已发布在 https://github.com/abenechehab/dicl。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11711" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 08:05:14 GMT</pubDate>
</item>
<item>
<title>Alchemy：通过符号变换合成形式化定理的框架</title>
<link>https://arxiv.org/abs/2410.15748</link>
<guid>https://arxiv.org/abs/2410.15748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Alchemy，一个通过符号变换合成形式化定理的框架，显著扩充Mathlib中的定理数量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Alchemy的通用框架，该框架旨在通过符号变换构建形式化定理，以解决神经定理证明（NTP）面临的数据稀缺问题。聚焦于Mathlib中的候选定理，Alchemy识别出所有可以应用于这些定理的可调用定理，并通过将相关术语替换为其等价形式或前提进行变换，从而显著增加定理数量，将Mathlib中的定理数量从11万增长至600万。此外，本文对扩充后的语料库进行了持续预训练和监督微调，实验结果表明该方法的有效性，在Leandojo基准上实现了5%的绝对性能提升，并在具有一定出分布的数据集miniF2F上提升了2.5%的绝对性能。最后，本文还对合成数据的组成和训练范式进行了全面分析，为强大的定理证明器的开发提供了有价值的指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 06:55:29 GMT</pubDate>
</item>
<item>
<title>面向长指令的长距离依赖样本选择框架 GATEAU</title>
<link>https://arxiv.org/abs/2410.15633</link>
<guid>https://arxiv.org/abs/2410.15633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出 GATEAU 框架，通过优质长样本提升长指令的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模语言模型处理长指令时面临的长上下文对齐挑战，提出了GATEAU这一新框架。该框架旨在识别具有重要长距离依赖关系的优质样本。现有研究通过合成长指令样本扩展数据集，但缺乏确保数据质量的明确策略可能会引入低质量样本。GATEAU通过两种方法实现样本选择：Homologous Models' Guidance (HMG) 和 Contextual Awareness Measurement (CAM)。HMG利用同源模型的困惑度评分测量生成响应的难度，评估由于长距离依赖关系导致的挑战；CAM则评估模型注意力如何集中于重要片段，以理解长输入上下文的难度。实验结果表明，GATEAU能够有效识别出富含长距离依赖关系的样本，基于这些样本训练的模型在指令跟随和长上下文理解能力方面表现更优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 04:46:20 GMT</pubDate>
</item>
<item>
<title>AutoTrain Advanced：简化训练自定义数据集的开源工具</title>
<link>https://arxiv.org/abs/2410.15735</link>
<guid>https://arxiv.org/abs/2410.15735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoTrain Advanced 是一款开源无代码工具，可用于不同任务的模型训练与微调。</p><br /><br /><p><strong>摘要：</strong> 随着开源模型的进步，在自定义数据集上训练（或微调）模型已成为开发特定工业或开源应用解决方案的重要部分。然而，目前尚无单一工具可以简化不同类型模态或任务的训练过程。因此，AutoTrain（又称 AutoTrain Advanced）的出现至关重要。AutoTrain Advanced 是一个开源、无代码工具/库，旨在为各种任务训练（或微调）模型，包括大型语言模型（LLM）微调、文本分类/回归、标记分类、序列到序列任务、句子变换器微调、视觉语言模型（VLM）微调、图像分类/回归以及表格数据的分类和回归任务。该库提供了针对自定义数据集模型训练的最佳实践。AutoTrain Advanced 可在完全本地模式或云机器上使用，并与 Hugging Face Hub 上共享的数万个模型及其变体兼容，从而为用户提供了灵活高效的模型训练解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 04:10:40 GMT</pubDate>
</item>
<item>
<title>预训练蒸馏：扩大知识蒸馏在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2410.16215</link>
<guid>https://arxiv.org/abs/2410.16215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了在预训练阶段应用知识蒸馏的方法，验证了模型间的学习效能。</p><br /><br /><p><strong>摘要：</strong> 知识蒸馏（KD）旨在将大教师模型的知识传递给更小的学生模型。以往在大语言模型（LLMs）领域的KD研究主要集中在后训练阶段，学生LLM直接从教师模型生成的指令及相应响应中学习。本文通过在LLMs的预训练阶段扩展KD，提出了预训练蒸馏（PD）的方法。首先，我们使用GLM-4-9B作为教师LLM进行了一项初步实验，验证了PD在1.9B参数的学生LLM上的有效性。考虑到蒸馏的关键影响因素，我们系统地探讨了预训练蒸馏中的设计空间，包括四个方面：logits处理、损失选择、缩放法则及离线或在线logits。我们的广泛实验探索了预训练蒸馏的设计空间，找到了更优的配置及一些有趣的结论，如更大的学生LLM通常能更从预训练蒸馏中受益，而更大的教师LLM并不一定能保证更好的效果。我们希望本研究对未来的预训练蒸馏实践提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 04:09:39 GMT</pubDate>
</item>
<item>
<title>SAM2Long：面向复杂长视频的改进训练自由视频目标分割策略</title>
<link>https://arxiv.org/abs/2410.16268</link>
<guid>https://arxiv.org/abs/2410.16268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAM2Long策略通过约束树搜索选择最佳分割路径，有效减少了错误积累，提高了长视频目标分割性能。</p><br /><br /><p><strong>摘要：</strong> Segment Anything Model 2 (SAM 2)在图像和视频的目标分割领域表现出色，但其贪婪选择记忆设计导致了错误积累问题，限制了在复杂长视频中的性能。为此，我们提出了SAM2Long，一个改进的训练自由视频目标分割策略。该策略考虑帧内分割不确定性，并通过约束树搜索选择多条分割路径中的最佳视频级别结果。在实际操作中，我们对整个视频保持固定数量的分割路径。每帧提出多个掩码，根据现有路径创建不同候选分支，然后选择累计得分较高的固定数量分支作为下一帧的新路径。在处理完最后一帧后，选择累计得分最高的路径作为最终分割结果。得益于其启发式搜索设计，SAM2Long在处理遮挡和物体重现方面表现出色，能够有效地对复杂长视频中的对象进行分割和跟踪。实验结果表明，SAM2Long在24组对比中平均提高了3.0个点，在SA-V和LVOS等长视频目标分割基准上提升了多达5.3个点。代码已在https://github.com/Mark12Ding/SAM2Long发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 03:38:14 GMT</pubDate>
</item>
<item>
<title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title>
<link>https://arxiv.org/abs/2410.16184</link>
<guid>https://arxiv.org/abs/2410.16184</guid>
<content:encoded><![CDATA[
Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 02:51:09 GMT</pubDate>
</item>
<item>
<title>FrugalNeRF：高效的少样本神经辐射场框架</title>
<link>https://arxiv.org/abs/2410.16271</link>
<guid>https://arxiv.org/abs/2410.16271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FrugalNeRF通过跨尺度几何适配方案优化少样本NeRF，提升3D重建精度和训练效率。</p><br /><br /><p><strong>摘要：</strong> 神经辐射场（NeRF）在少样本场景下面临显著挑战，主要表现在过拟合及高保真渲染所需的长训练时间。现有方法如FreeNeRF和SparseNeRF虽然采用了频率正则化或预训练先验，但在复杂调度和偏差问题上仍显得无力。为此，我们提出了FrugalNeRF，这是一种新的少样本NeRF框架，通过在多个尺度间共享权重体素，高效地表示场景细节。我们的关键贡献是一种跨尺度几何适配方案，该方案基于重投影误差选择伪地面真实深度，从而在不依赖外部学习先验的情况下引导训练，充分利用训练数据。同时，FrugalNeRF也可集成预训练的先验，提升质量而不影响收敛速度。在LLFF、DTU和RealEstate-10K数据集上的实验表明，FrugalNeRF在超越其他少样本NeRF方法的同时，显著减少了训练时间，是一种实用的高效准确的3D场景重建解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 02:26:36 GMT</pubDate>
</item>
<item>
<title>Meta-Chunking: 基于深层语义关系的文本分块方法</title>
<link>https://arxiv.org/abs/2410.12788</link>
<guid>https://arxiv.org/abs/2410.12788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Meta-Chunking概念，通过两种新策略提升RAG在知识密集任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Meta-Chunking的概念，以填补Retrieval-Augmented Generation (RAG) 中文本分块的重要缺失，影响知识密集型任务的质量。Meta-Chunking是介于句子与段落之间的一种分块方法，涉及段落内具有深层次语言逻辑关系的一系列句子。为此，本文设计了两种基于大语言模型（LLMs）的分块策略：Margin Sampling Chunking和Perplexity Chunking。前者利用LLMs对连续句子进行二元分类，以判断是否需要分割，基于边际抽样获得的概率差异做出决策；后者通过分析困惑度分布的特征精确识别文本分块边界。考虑到不同文本的复杂性，本文还提出了一种将Meta-Chunking与动态合并相结合的策略，以实现细粒度和粗粒度文本分块之间的平衡。在11个数据集上进行的实验表明，Meta-Chunking能够更有效地提升基于RAG的单跳和多跳问答性能。例如，在2WikiMultihopQA数据集上，Meta-Chunking的表现比相似分块高出1.32，同时时间消耗仅为45.8%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 02:24:32 GMT</pubDate>
</item>
<item>
<title>Pangea：面向多语言和多文化背景的多模态大语言模型</title>
<link>https://arxiv.org/abs/2410.16153</link>
<guid>https://arxiv.org/abs/2410.16153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Pangea及其多语言多模态训练数据集PangeaIns，展示了在跨文化情境下的评估优势。</p><br /><br /><p><strong>摘要：</strong> 尽管近年来多模态大型语言模型（MLLMs）取得了显著进展，但其开发主要集中在英语和西方中心的数据集与任务上，导致世界大部分语言和文化背景被忽视。本文提出了Pangea，这是一个多语言多模态的LLM，基于PangeaIns训练，后者是一个多达600万条指令的多样化数据集，涵盖39种语言。PangeaIns的特点包括：1）高质量的英语指令，2）精心机器翻译的指令，以及3）具有文化相关性的多模态任务，以确保跨文化的覆盖。为全面评估模型的能力，我们引入了PangeaBench，这是一个涵盖47种语言的综合评估套件，包含14个数据集。结果显示，Pangea在多语言环境和多样文化背景下显著超越现有的开放源代码模型。消融研究进一步揭示了英语数据比例、语言普及度和多模态训练样本数量对整体性能的重要性。我们全面开源我们的数据、代码和训练检查点，以促进包容性和强大的多语言MLLM的发展，推动更广泛的语言和文化范围的公平性和可及性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 02:07:31 GMT</pubDate>
</item>
<item>
<title>跨语言自动评估套件：Hercule的设计与实现</title>
<link>https://arxiv.org/abs/2410.13394</link>
<guid>https://arxiv.org/abs/2410.13394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了CIA套件，包含Hercule模型和Recon测试集，以实现多语言评估。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了跨语言自动评估(CIA)套件及其组成部分，包括评估模型Hercule和多语言测试集Recon。此框架旨在解决当前自然语言处理(NLP)领域在非英语语言中的评估挑战。测试集包含500条人类注释的指令，涵盖多种任务能力，并提供六种语言的人类评分。这一套件可以用于基准评估通用多语言大型语言模型(LLMs)，并促成评估模型的元评估。Hercule模型通过学习使用英语参考答案为目标语言的响应评分，克服了在低资源场景中目标语言参考答案不足的问题。实验结果表明，Hercule模型在与人工评判对比中，表现出更高的一致性，显示了其在见未见语言上的零样本评估能力。这项研究是对跨语言评估的首次全面探讨，提出了一种可扩展且有效的多语言评估方法。所有代码、数据集和模型将在公开平台上发布，以推动该领域的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 01:37:42 GMT</pubDate>
</item>
<item>
<title>PUMA：赋能统一的多模态大语言模型的多粒度视觉生成</title>
<link>https://arxiv.org/abs/2410.13861</link>
<guid>https://arxiv.org/abs/2410.13861</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PUMA提出了一种统一的多模态大语言模型框架，适应不同的图像生成任务的粒度需求。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态基础模型在视觉-语言理解方面取得了显著进展。而多模态大语言模型（MLLM）在视觉内容生成方面也展现出潜力。然而，现有研究在统一的MLLM框架内对不同图像生成任务的粒度需求关注不足，从文本到图像生成所需的多样性，到图像操作所需的精准可控性。在此背景下，我们提出了PUMA，赋能统一的MLLM与多粒度视觉生成。PUMA将多粒度视觉特征统一作为MLLM的输入和输出，有效解决了不同图像生成任务的粒度需求。经过多模态预训练和任务特定的指令调优，PUMA在各种多模态任务中表现出色。这项工作为实现真正统一的MLLM迈出了重要一步，使其能够适应各种视觉任务的粒度需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13861" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 01:33:52 GMT</pubDate>
</item>
<item>
<title>CompassJudger-1：开源全能评估模型及其基准测试</title>
<link>https://arxiv.org/abs/2410.16256</link>
<guid>https://arxiv.org/abs/2410.16256</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了CompassJudger-1，首个开源通用评估模型，及其新创建的JudgerBench基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CompassJudger-1，这是首个开源的全能评估模型，旨在提升大型语言模型（LLMs）的评估效率和准确性。针对主观评估方法的人力资源消耗和重复性不足的问题，CompassJudger-1提供了多项功能，包括单一评分、双模型比较、格式化评估和生成反馈等。此外，为了评估不同评估模型的能力，文章还建立了JudgerBench，这是一个新的基准，涵盖多种主观评估任务和广泛主题。发布CompassJudger-1和JudgerBench旨在促进研究社区的合作，加速LLM评估方法的发展。这些工具的开源能够为研究者提供全面的解决方案，灵活适应各种评估需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.16256" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 01:25:17 GMT</pubDate>
</item>
<item>
<title>融合上下文信息的综合语音标记器DM-Codec的研究</title>
<link>https://arxiv.org/abs/2410.15017</link>
<guid>https://arxiv.org/abs/2410.15017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DM-Codec模型，通过上下文信息的引入，显著提高语音标记的准确性和质量。</p><br /><br /><p><strong>摘要：</strong> 近年来，语音语言模型的快速发展在语音标记化和合成方面取得了显著进步。然而，将复杂的、多维度的语音属性精准映射到离散标记中依然具有挑战性。现有语音表示通常分为来自音频编解码器的声学标记和来自自监督学习模型的语义标记。尽管近期有尝试将声学和语义标记统一，但却忽视了上下文表示在全面语音建模中的重要作用。我们的实证研究表明，缺乏上下文表示使得语音转录中的字错误率（WER）和信息丢失（WIL）升高。为了解决这些局限，我们提出了两种新的蒸馏方法：1）一种语言模型（LM）引导的蒸馏方法，融合上下文信息；2）一种结合LM与自监督语音模型（SM）引导的蒸馏技术，将声学、语义和上下文等多模态表示有效蒸馏为综合语音标记器，命名为DM-Codec。DM-Codec架构采用简化的编码器-解码器框架，配备残差向量量化器（RVQ），并在训练过程中融入LM与SM。实验结果表明，DM-Codec在LibriSpeech基准数据集上显著优于现有最先进的语音标记化模型，将WER减少了最多13.46%，WIL下降了9.82%，并将语音质量提升了5.84%，可懂度改善了1.85%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.15017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 01:20:41 GMT</pubDate>
</item>
<item>
<title>Baichuan Alignment：提升AI模型对齐技术的深入分析</title>
<link>https://arxiv.org/abs/2410.14940</link>
<guid>https://arxiv.org/abs/2410.14940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文分析Baichuan系列模型的对齐技术，为AI研究提供重要见解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Baichuan Alignment，这是一项对Baichuan系列模型所采用的对齐技术的详细分析，旨在为AI研究提供宝贵的见解。研究探讨了对齐过程中的关键组成部分，包括优化方法、数据策略、能力增强和评估过程。该过程分为三个主要阶段：Prompt Augmentation System (PAS)、Supervised Fine-Tuning (SFT)和Preference Alignment。文中详细记录了遇到的问题、应用的解决方案及所做的改进。通过与已有基准的比较，展示了Baichuan Alignment带来的技术进步。Baichuan-Instruct为内部模型，而Qwen2-Nova-72B和Llama3-PBM-Nova-70B则是基于Qwen2-72B和Llama-3-70B的优化指令版本。Baichuan-Instruct在核心能力方面表现出显著提升，用户体验提升幅度在17%至28%之间，并且在专业基准上表现优异。在开源基准评估中，Qwen2-Nova-72B和Llama3-PBM-Nova-70B在几乎所有数据集上均超越了其官方的指令版本。本文旨在澄清对齐过程中的关键技术，促进社区对这一领域的更深入理解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 01:17:50 GMT</pubDate>
</item>
<item>
<title>SemiEvol：一种半监督微调框架用于大规模语言模型的适应性</title>
<link>https://arxiv.org/abs/2410.14745</link>
<guid>https://arxiv.org/abs/2410.14745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究介绍SemiEvol框架，通过半监督方式有效利用标记和未标记数据进行大语言模型的微调。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型（LLMs）的广泛应用，监督微调（SFT）成为了适应特定领域或任务的关键。然而，现实应用中可用的标记数据极为有限，这对SFT的效果造成了很大挑战。因此，需要一种高效利用标记和未标记数据的微调框架。为此，本文提出了一个名为SemiEvol的半监督微调框架，通过传播和选择的方式进行LLM的适应性调整。在知识传播方面，SemiEvol采用了双层次的方法，通过权重内传播和上下文内传播将标记数据的知识传递给未标记数据。在知识选择方面，框架结合了协同学习机制，选择更高质量的伪响应样本。我们在七个通用或特定领域的数据集上使用GPT-4o-mini和Llama-3.1进行了实验，结果显示模型在目标数据上的性能得到了显著提升。此外，我们将SemiEvol与SFT和自进化方法进行了比较，突显了其在混合数据场景下的实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 00:58:50 GMT</pubDate>
</item>
<item>
<title>利用大型语言模型评估认知行为疗法的潜力：CBT-BENCH基准的提出</title>
<link>https://arxiv.org/abs/2410.13218</link>
<guid>https://arxiv.org/abs/2410.13218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨利用大型语言模型辅助心理治疗的潜力，并提出CBT-BENCH基准，以评估其在认知行为疗法中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文探讨当前患者需求与可用心理健康支持之间的显著差距，重点分析大型语言模型（LLMs）在专业心理治疗中的潜在应用。为此，本文提出了一种新的基准——CBT-BENCH，旨在系统性评估认知行为疗法（CBT）辅助。这一基准包含三个任务层次：第一层是基本CBT知识获取，通过选择题进行评估；第二层为认知模型理解，包括认知扭曲分类、主要核心信念分类和细粒度核心信念分类；第三层为治疗响应生成，主要任务是生成对患者发言的响应。这些任务涵盖了CBT的关键要素，可以通过人工智能的帮助得到增强，同时构建了一种能力要求的层级结构，从基本知识的复述到参与真实的治疗对话。我们对代表性的LLMs在这个基准上的表现进行了评估。实验结果表明，虽然LLMs在复述CBT知识方面表现良好，但在需要深度分析患者认知结构和生成有效响应的复杂真实场景中，它们的能力仍有不足，这为未来的研究指明了方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Oct 2024 00:40:03 GMT</pubDate>
</item>
<item>
<title>Shakti：为边缘设备优化的高效语言模型</title>
<link>https://arxiv.org/abs/2410.11331</link>
<guid>https://arxiv.org/abs/2410.11331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Shakti是一款针对资源受限环境优化的语言模型，适用于边缘设备和多领域应用。</p><br /><br /><p><strong>摘要：</strong> Shakti是一款拥有25亿参数的语言模型，专为资源受限的环境优化而设计，适合在边缘设备如智能手机、可穿戴设备和物联网系统中运行。它结合了高性能的自然语言处理（NLP）和优化的效率与精确度，理想适用于计算资源和内存有限的实时人工智能应用。Shakti支持多种地方语言及领域特定任务，在医疗、金融和客户服务等行业中表现出色。基准评估表明，Shakti在保持低延迟和设备上高效性的同时，其表现与更大型模型相竞争，使其成为边缘人工智能领域的领先解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 14:19:02 GMT</pubDate>
</item>
<item>
<title>灵活视觉变换器 FiTv2：一种针对任意分辨率图像生成的变换器架构</title>
<link>https://arxiv.org/abs/2410.13925</link>
<guid>https://arxiv.org/abs/2410.13925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了灵活视觉变换器FiTv2，旨在解决图像生成中的分辨率限制问题。</p><br /><br /><p><strong>摘要：</strong> 自然界是分辨率无限的，在此背景下，现有的扩散模型（如扩散变换器）往往面临处理非培训域图像分辨率的挑战。为了解决这个问题，我们将图像概念化为具有动态大小的标记序列，而非传统的固定分辨率网格。这种视角促进了一种灵活的培训策略，可以在培训和推理过程中无缝适应各种长宽比，从而提高分辨率泛化能力并消除因图像裁剪引入的偏差。在此基础上，我们提出了灵活视觉变换器FiT，专为生成无约束分辨率和长宽比的图像而设计。我们进一步改进FiT至FiTv2，引入了几个创新设计，包括Query-Key向量标准化、AdaLN-LoRA模块、修正的流调度程序和Logit-Normal采样器。FiTv2在精心调整的网络结构支持下，展现出2倍于FiT的收敛速度，并通过先进的无培训外推技术，实现了在分辨率外推和多样分辨率生成方面的显著适应性。此外，我们对FiTv2模型的可扩展性进行探索，发现较大的模型展现出更好的计算效率。最终，我们提出了一种高效的后培训策略，旨在为高分辨率生成调整预训练模型。综合实验表明FiTv2在广泛分辨率下的出色性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 13:46:40 GMT</pubDate>
</item>
<item>
<title>Mini-Omni2：一款多模态视觉音频助手</title>
<link>https://arxiv.org/abs/2410.11190</link>
<guid>https://arxiv.org/abs/2410.11190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mini-Omni2 是一款能够实时响应视觉和音频查询的多模态助手，具备强大的交互能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Mini-Omni2，一款能够处理视觉和音频查询的多模态助手。Mini-Omni2 结合了预训练的视觉和听觉编码器，确保在各个单一模态中的性能。通过提出三阶段的训练流程，本研究旨在对齐各个模态，最终使语言模型可以处理多模态输入和输出。Mini-Omni2 在经过有限数据集的训练后，展示出在响应用户查询方面的灵活性。我们还引入了一种基于命令的中断机制，以增强用户与助手之间的互动。Mini-Omni2 是对 GPT-4o 功能的一种接近再现，旨在为后续研究提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 13:21:46 GMT</pubDate>
</item>
<item>
<title>混合自回归变换器（HART）：一种高效的图像生成模型</title>
<link>https://arxiv.org/abs/2410.10812</link>
<guid>https://arxiv.org/abs/2410.10812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">混合自回归变换器（HART）通过混合标记器有效提高图像生成质量和效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种混合自回归视觉生成模型——混合自回归变换器（HART），其能够直接生成1024x1024像素的图像，生成质量可与扩散模型相媲美。现有的自回归（AR）模型面临因离散标记器的图像重建质量差以及训练成本高昂而导致的限制。为了解决这些问题，我们提出了混合标记器，该标记器将自编码器的连续潜空间分解为两个组件：代表整体视觉内容的离散标记和代表残余细节的连续标记。离散组件由可扩展分辨率的离散自回归模型建模，而连续组件则通过仅37M参数的轻量级残留扩散模块进行学习。与仅使用离散标记器的VAR模型相比，我们的混合方法在MJHQ-30K数据集上将重建FID从2.11提升至0.30，生成FID也从7.85提高至5.38，提升幅度达31%。HART在FID和CLIP分数上也超过了最先进的扩散模型，其吞吐量提高4.5至7.7倍，计算量减少6.9至13.4倍。代码已开源，链接为https://github.com/mit-han-lab/hart。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 13:15:53 GMT</pubDate>
</item>
<item>
<title>BiGR：一种基于紧凑二进制潜在代码的条件图像生成模型</title>
<link>https://arxiv.org/abs/2410.14672</link>
<guid>https://arxiv.org/abs/2410.14672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BiGR是一种新型条件图像生成模型，利用二进制潜在代码实现高效的图像生成与表征。</p><br /><br /><p><strong>摘要：</strong> 我们提出了一种新颖的条件图像生成模型BiGR，使用紧凑的二进制潜在代码进行生成训练，旨在提升生成与表征能力。BiGR是首个在同一框架内统一生成与判别的条件生成模型。该模型具有二进制分词器、掩蔽建模机制和用于二进制编码预测的二进制转码器。此外，我们还引入了一种新颖的熵排序采样方法，以实现高效的图像生成。广泛的实验验证了BiGR在生成质量（以FID-50k衡量）和表征能力（通过线性探测精度证明）方面的优越性能。此外，BiGR在各种视觉任务中的零-shot泛化能力也得到了展示，实现了图像修复、扩展、编辑、插值和丰富等应用，无需结构调整。我们的研究结果表明，BiGR有效地统一了生成和判别任务，为该领域的进一步发展奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 13:05:25 GMT</pubDate>
</item>
<item>
<title>Montessori-Instruct：针对学生学习过程的合成数据框架</title>
<link>https://arxiv.org/abs/2410.14208</link>
<guid>https://arxiv.org/abs/2410.14208</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新型合成数据框架，以优化学生语言模型的学习过程。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了Montessori-Instruct，这是一种新型合成数据框架，旨在通过教师语言模型的合成数据能力，针对学生语言模型的学习过程进行定制。我们利用合成训练数据点对学生的局部数据影响，以表征学生的学习偏好。然后，采用直接偏好优化（Direct Preference Optimization, DPO）训练教师模型，以生成更符合学生学习偏好的合成数据。实验证明，在Alpaca Eval和MT-Bench上使用Llama3-8B-Instruct（教师）和Llama3-8B（学生）的组合，Montessori-Instruct相较于标准合成方法表现出显著的提升，分别提高了18.35%和46.24%。我们的算法还超越了由更强大的教师模型GPT-4o合成的数据。进一步分析表明，教师学习能力的提升使得能生成对学生更具影响力的训练数据，从而提升学生的学习效果，局部数据影响的优势也有助于准确测量学生的偏好。此外，Montessori-Instruct在不同学生模型上的稳健性表现良好。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14208" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 12:58:48 GMT</pubDate>
</item>
<item>
<title>平衡式说服训练：提升模型对正负说服的适应性</title>
<link>https://arxiv.org/abs/2410.14596</link>
<guid>https://arxiv.org/abs/2410.14596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了平衡式说服训练（PBT），提升模型对正负说服的适应性，增强其在多代理辩论中的表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在面对对立的对话者时易受到说服，这可能带来风险。本文首次探讨了如何增强模型抵抗负面说服的能力，同时强调模型也应能接受有益的正面说服。单纯优化某一类说服会导致模型在另一类上的表现不佳。为此，我们引入了平衡式说服训练（PBT），通过多代理递归对话树创建数据，并通过偏好优化训练模型，以恰当地接受说服。PBT在抵御误信息和应对挑战方面表现良好，同时在包含正负说服的整体数据上提升了模型表现。尤其是在多代理辩论中，PBT模型表现出更好的合作性。在没有PBT的情况下，强模型和弱模型的组合在表现上不稳定，其表现受呈现顺序影响。然而，PBT使得团队合作更平稳，强模型能 consistently 提升弱模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 12:28:08 GMT</pubDate>
</item>
<item>
<title>大语言模型的自我预测：内省能力的探索</title>
<link>https://arxiv.org/abs/2410.13787</link>
<guid>https://arxiv.org/abs/2410.13787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了大语言模型是否具备内省能力，实验结果表明经过微调的模型可以更好地预测自身行为。</p><br /><br /><p><strong>摘要：</strong> 本研究旨在探索大语言模型（LLMs）的内省能力，即模型是否能够获取并报告其内部状态所反映的知识，从而增强模型的可解释性。我们通过微调模型，使其能够预测在假设场景下自己行为的特征。例如，当输入为P时，模型会预测其输出是倾向短期还是长期选项。如果模型M1具备内省能力，它在预测自身行为方面应优于模型M2，即使M2在训练上更为强大。我们的实验使用了GPT-4、GPT-4o和Llama-3模型，结果表明，经过微调的模型M1在自我预测中表现优于其他模型，并且即使在我们故意修改其真实行为后，M1仍能准确预测自身行为。然而，尽管在简单任务上成功获取内省能力，在更复杂的任务或需要超出训练分布的推广能力上，我们的探索则表现不佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 11:27:51 GMT</pubDate>
</item>
<item>
<title>训练方法对神经网络层重要性的影响</title>
<link>https://arxiv.org/abs/2410.14470</link>
<guid>https://arxiv.org/abs/2410.14470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，训练方法影响神经网络不同层的关键性，改善的训练方式增加了早期层的重要性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了在恒定架构和训练数据的情况下，不同训练管道对神经网络决策函数中各层参数重要性的影响。通过对多种ImageNet-1k分类模型的实验评估，我们发现训练方法显著影响各层对任务的重要性。例如，改进的训练策略和自监督训练提升了早期层的关键性，而较少利用深层层次。相反，像对抗训练这样的策略则显示出与之相反的趋势。这些初步结果进一步扩展了以往研究，提供了对神经网络内部机制更加细致的理解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 08:23:59 GMT</pubDate>
</item>
<item>
<title>视觉语言模型的挑战：自然图像中的对抗样本研究</title>
<link>https://arxiv.org/abs/2410.14669</link>
<guid>https://arxiv.org/abs/2410.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了视觉语言模型在自然图像和人类易答问题上的不足，提出自然基准评估方法。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉语言模型（VLMs）在复杂的视觉问答（VQA）基准测试中取得了显著进展。然而，本研究显示，VLMs 在处理普通图像和人类易于回答的问题时仍面临挑战，这类样本被称为自然对抗样本。我们发现，通过使用 CLIP 和 ChatGPT 等现成模型，相对简单地生成这些 VQA 样本。为此，我们提出了一种半自动的方法，收集了一个新的基准数据集，NaturalBench，包含 10,000 个经过人类验证的 VQA 样本。该基准设计优先考虑视觉元素，通过将每个问题与两幅不同答案的图像配对，有效防止了模型的盲目解答，提升了挑战性。对 53 个先进的 VLMs 的评估结果显示，包括 LLaVA-OneVision、Cambrian-1、Llama3.2-Vision、Molmo、Qwen2-VL 和 GPT-4o 在内的模型相较于人类表现，落后 50%-70%。分析表明，NaturalBench 的难点主要体现在组合性和偏见两个方面，这些发现为后续研究提供了启示。此外，借助我们的方法，NaturalBench 的框架也可应用于多样的数据源，包括长标题和非英语语言，展示了其动态评估 VLMs 的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 06:33:45 GMT</pubDate>
</item>
<item>
<title>DAWN：非自回归扩散模型的动态头像生成框架</title>
<link>https://arxiv.org/abs/2410.13726</link>
<guid>https://arxiv.org/abs/2410.13726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DAWN框架通过非自回归扩散模型生成高质量动态头像视频，解决了传统方法的生成速度慢和错误积累等问题。</p><br /><br /><p><strong>摘要：</strong> DAWN（Dynamic frame Avatar With Non-autoregressive diffusion）是一种创新的框架，旨在通过单一的肖像和语音音频生成生动而逼真的动态视频。传统的扩散基础的动态头像生成方法大多数依赖于自回归策略，导致在生成过程中效用有限、错误累积和生成速度较慢等问题。为了克服这些挑战，DAWN提出了一套新的生成方式，其主要由两个组件组成：音频驱动的整体面部动态生成和音频驱动的头部姿态和眨眼生成。大量实验表明，DAWN能生成真实且生动的视频，同时具有准确的口型运动和自然的姿态/眨眼动作。此外，DAWN实现了高速度生成，并显示出强大的外推能力，能够确保高质量长期视频的稳定生成。这一成果展示了DAWN在动态头像视频生成领域的巨大潜力和影响力，也期待其能够引发在扩散模型中的更多非自回归方法的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 06:31:06 GMT</pubDate>
</item>
<item>
<title>关于强化学习中人为反馈的边际损失问题及其影响</title>
<link>https://arxiv.org/abs/2410.13828</link>
<guid>https://arxiv.org/abs/2410.13828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了边际损失方法在语言模型对齐中的不足及其带来的潜在问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习从人为反馈（RLHF）在语言模型对齐中的主导地位，特别是边际损失方法的不足之处。我们发现，这种方法在偏好和反偏好响应的单独理想行为上存在不足，可能导致两种不良后果：一是不偏好的响应（如不安全响应）的概率增加，二是理想的偏好响应的概率降低。这种现象的根源在于边际损失将偏好概率的变化与反偏好概率的梯度耦合，阻碍了偏好概率的提高。因此，我们提出了“梯度纠缠”的概念，阐明了在对齐语言模型时，偏好和反偏好对数概率的梯度内积相对单独梯度范数过大时，梯度纠缠的问题会变得显著。我们理论上探讨了这种内积为何会导致训练动态的差异，并通过实证研究验证了这些发现。最后，本文为改进边际损失法设计了潜在算法方案，以缓解此类不足，从而提升语言模型对齐的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:50:33 GMT</pubDate>
</item>
<item>
<title>用户中心的金融专业能力评估基准：UCFE</title>
<link>https://arxiv.org/abs/2410.14059</link>
<guid>https://arxiv.org/abs/2410.14059</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍UCFE基准，评估大型语言模型在复杂金融任务中的表现，结合人类专家反馈。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UCFE（User-Centric Financial Expertise）基准，这是一个创新框架，旨在评估大型语言模型（LLMs）处理复杂现实金融任务的能力。UCFE基准采用了一种混合方法，结合了人类专家评估和动态、任务特定的交互，以模拟不断发展的金融场景的复杂性。首先，我们进行了包含804名参与者的用户研究，收集他们对金融任务的反馈。其次，根据这些反馈，我们创建了包含广泛用户意图和交互的数据集。该数据集作为基准测试12个LLM服务的基础，采用了LLM-as-Judge方法论。我们的结果显示基准分数与人类偏好之间存在显著的一致性，Pearson相关系数为0.78，验证了UCFE数据集和我们评估方法的有效性。UCFE基准不仅揭示了LLM在金融领域的潜力，还提供了一个评估其性能和用户满意度的稳健框架。基准数据集和评估代码已公开。 </p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14059" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:46:41 GMT</pubDate>
</item>
<item>
<title>DPLM-2：一种多模态蛋白质基础模型</title>
<link>https://arxiv.org/abs/2410.13782</link>
<guid>https://arxiv.org/abs/2410.13782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DPLM-2是一个多模态蛋白质模型，实现了序列和结构的联合生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了DPLM-2，一种多模态蛋白质基础模型，扩展了离散扩散蛋白质语言模型（DPLM），同时考虑了序列和结构。为了实现结构学习，3D坐标通过无查找量化的标记器转换为离散标记。DPLM-2在实验和高质量合成结构数据上进行训练，学习序列与结构的联合分布及其边际和条件分布。我们还实施了高效的预热策略，以利用大规模进化数据与经过预训练的基于序列的蛋白质语言模型的结构归纳偏差之间的关联。实证评估表明，DPLM-2能够同时生成高度兼容的氨基酸序列及其对应的3D结构，消除了两阶段生成方法的需要。同时，DPLM-2在多种条件生成任务中表现出竞争力，包括折叠、反折叠及 scaffolding 与多模态基序输入，并为预测任务提供结构感知表示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:42:52 GMT</pubDate>
</item>
<item>
<title>机器生成文本检测器的评估方法研究</title>
<link>https://arxiv.org/abs/2410.14677</link>
<guid>https://arxiv.org/abs/2410.14677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">随着LLMs的发展，生成文本质量显著提高，机器生成文本检测器的可靠性亟待加强。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在自回归大型语言模型（LLMs）快速发展的背景下，机器生成文本检测器的评估方法。尽管许多检测器在基准数据集上表现出高达99.9%的识别质量，但在实际应用中，检测器的性能显著下降。这引发了关于现有检测器的可信度及其高评分是否受到评估数据集质量低下的影响的讨论。为此，本文强调了发展稳健且高质量的生成数据评估方法的必要性，以抵御未来模型的偏见和低泛化能力。我们系统性地回顾了致力于AI生成内容检测的比赛数据集，并提出了评估含有AI生成片段的数据集质量的方法。此外，我们还讨论了利用高质量生成数据来提升检测模型训练和训练数据集本身质量的可能性。我们希望通过本研究，增进对人类与机器文本之间动态关系的理解，同时支持在日益自动化的世界中信息的完整性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.14677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:20:23 GMT</pubDate>
</item>
<item>
<title>基于学习门控的稀疏注意力机制SeerAttention</title>
<link>https://arxiv.org/abs/2410.13276</link>
<guid>https://arxiv.org/abs/2410.13276</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeerAttention通过学习门控机制自适应选择重要块，实现了稀疏注意力的动态捕捉，提升了长上下文处理效率。</p><br /><br /><p><strong>摘要：</strong> Attention是现代大语言模型（LLMs）的基础，但其平方复杂性限制了效率和扩展性，尤其是在长上下文窗口中。本文提出了SeerAttention，一种新型注意力机制，通过引入可学习的门控，动态选择注意力图中的重要块，达成块级稀疏化。这种方法有效平衡了准确性与加速性能。为了高效地学习该门控网络，作者开发了一种定制的FlashAttention实现，能够以最低开销提取块级注意力图的真实值。在后训练阶段，SeerAttention显著超过了现有的基于静态或启发式的稀疏注意力方法，且更能灵活适应不同的上下文长度和稀疏比例。在长上下文微调中，SeerAttention在32k的上下文长度下，可以实现90%的稀疏率，且仅有最小的困惑度损失，相较FlashAttention-2实现了5.67倍的加速。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13276" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:11:11 GMT</pubDate>
</item>
<item>
<title>利用KeyNMF研究中国媒体中的信息动态：以2024年欧洲议会选举为例</title>
<link>https://arxiv.org/abs/2410.12791</link>
<guid>https://arxiv.org/abs/2410.12791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的主题建模方法KeyNMF，并应用于研究中国媒体的信息动态。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了中华人民共和国（PRC）通过华人媒体干预欧洲选举的问题，特别是针对2024年欧洲议会选举。论文首先提出了一种新颖的主题建模方法KeyNMF，该方法结合了变换器（transformer）基础的上下文嵌入模型，实现了对静态和动态主题的有效建模。通过基准评估，验证了KeyNMF在多种中国数据集和指标上具有竞争力。接着，我们将KeyNMF与现有复杂系统信息动态描述方法结合，形成了一条研究信息动态的完整流程。本文应用该流程于五个新闻网站的数据，专注于2024年欧洲议会选举前的时间段。研究方法和结果表明，KeyNMF在研究中国媒体中的信息动态方面表现出色，为后续更广泛的研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 05:00:34 GMT</pubDate>
</item>
<item>
<title>世界模型增强的自主网络代理研究</title>
<link>https://arxiv.org/abs/2410.13232</link>
<guid>https://arxiv.org/abs/2410.13232</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出一种世界模型增强的自主网络代理，以改善长时间任务中的决策能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在构建自主代理方面引起了广泛关注，但当前基于LLM的网络代理在长时间任务中的表现仍然不尽如人意，常常导致例如重复购买不可退票的机票等错误。相比之下，人类能够通过对潜在结果（例如失去金钱）的意识，避免这类不可逆转的错误，这种能力被称为“世界模型”。本研究首先通过初步分析确认了当前LLM（如GPT-4o、Claude-3.5-Sonnet等）缺乏世界模型。随后，我们提出了一种世界模型增强（WMA）的网络代理，它通过模拟动作结果来改善决策能力。为了解决将LLM训练为世界模型时面临的挑战，如观察之间的重复元素和冗长HTML输入，我们提出了一种聚焦过渡的观测抽象，其中预测目标是自由形式的自然语言描述，专门突出时间步之间的重要状态差异。在WebArena和Mind2Web上的实验表明，我们的世界模型提高了代理的策略选择能力且无需训练，同时展示出我们的代理在成本和时间上的效率相较于近期基于树搜索的代理。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13232" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 04:41:07 GMT</pubDate>
</item>
<item>
<title>自我演化的AI训练：借助扩散模型改善低质量数据学习</title>
<link>https://arxiv.org/abs/2410.13674</link>
<guid>https://arxiv.org/abs/2410.13674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过扩散课程(DisCL)方法，提高深度神经网络在长期分类和低质量数据学习中的表现。</p><br /><br /><p><strong>摘要：</strong> 低质量或稀缺数据为深度神经网络的训练带来了显著挑战。虽然经典的数据增强方法无法生成足够多样化的新数据，但扩散模型为通过文本引导生成高质量和多样化的合成数据开辟了新途径。然而，仅依赖文本引导无法控制合成图像与原始图像的相似性，可能导致不合分布的数据，从而损害模型性能。为了克服这一限制，本文研究了图像引导，以实现合成图像和真实图像之间的插值范围。强图像引导生成的图像与训练数据相似但难以学习，而弱图像引导的合成图像则便于模型学习但导致与原始数据的分布差距增大。生成的全谱数据使我们能够构建一种新颖的“扩散课程(DisCL)”，该方法根据训练阶段调整图像合成的引导水平，识别并集中关注模型的难样本，并评估合成图像的最有效指导水平以提高困难数据学习。在长尾分类和低质量数据学习等挑战性任务中应用DisCL，能够通过低引导图像进行高质量特征的学习，从而为学习可能在多样性或质量上较弱的高引导图像进行热身。大量实验表明，在iWildCam数据集上，应用DisCL后OOD和ID宏观准确率分别提升2.7%和2.1%；在ImageNet-LT上，DisCL将基础模型的尾类准确率从4.4%提高到23.64%，并在全类准确率上提升4.02%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 02:09:52 GMT</pubDate>
</item>
<item>
<title>MagicTailor：组件可控的个性化文本到图像生成</title>
<link>https://arxiv.org/abs/2410.13370</link>
<guid>https://arxiv.org/abs/2410.13370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MagicTailor框架，解决文本到图像生成中组件可控个性化面临的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的任务——组件可控的个性化，旨在提升文本到图像（T2I）模型生成图像的精细化控制能力。尽管现有方法通过参考图像实现了概念的复制，但在组件的细粒度定制方面仍显不足。该任务面临两大挑战：语义污染导致个性化概念被不必要的视觉元素干扰，语义不均衡则造成概念与组件之间的学习失调。为了解决这些难题，我们设计了MagicTailor框架，利用动态掩模退化（Dynamic Masked Degradation, DM-Deg）技术动态干扰不必要的视觉语义，同时采用双流平衡（Dual-Stream Balancing, DS-Bal）建立一种平衡的学习模式，以优化所需视觉语义的学习效果。通过广泛的比较、消融和分析，MagicTailor在这一挑战性任务中表现出色，并展示了其在实际应用中的潜力，为更细致和富有创意的图像生成奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Oct 2024 01:06:41 GMT</pubDate>
</item>
<item>
<title>o1模型在推理能力提升中的研究：对比测试时计算方法的深入分析</title>
<link>https://arxiv.org/abs/2410.13639</link>
<guid>https://arxiv.org/abs/2410.13639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨o1模型在推理任务中的表现，并与现有方法进行对比，揭示其推理模式和性能优势。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的不断演进，面临的复杂任务越来越多，加大了研究的关注力度。OpenAI的o1模型展示了通过测试时计算（Test-time Compute）策略来显著提升推理能力的潜力。本文旨在深入探讨o1的推理模式，并与其他现存的测试时计算方法（如BoN、Step-wise BoN、Agent Workflow和Self-Refine）进行比较，基于OpenAI的GPT-4o在多个推理基准测试（数学、编码和常识推理）上的表现进行分析。研究结果首先表明，o1模型在大多数数据集上取得了最佳性能。其次，对于多样化响应搜索方法（如BoN），我们发现奖励模型的能力以及搜索空间限制了方法的上限。第三，在将问题拆分成多个子问题的策略中，Agent Workflow由于其领域特定的系统提示在规划推理过程时表现优于Step-wise BoN。最后，我们总结了o1的六种推理模式，并对多个推理基准进行了详细分析。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 22:29:50 GMT</pubDate>
</item>
<item>
<title>探索视觉自回归模型的规模化问题：连续与离散代币、随机与固定生成顺序的影响</title>
<link>https://arxiv.org/abs/2410.13863</link>
<guid>https://arxiv.org/abs/2410.13863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了文本到图像生成中自回归模型规模化的问题，发现连续代币模型表现更佳，随机生成顺序优于固定顺序。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了文本到图像生成中自回归模型的规模化问题，重点关注两个关键因素：模型使用离散代币还是连续代币，以及代币是否采用随机或固定的栅格顺序生成。通过实证结果发现，尽管所有模型在验证损失方面都有良好的规模效应，但它们在评估性能（如FID、GenEval分数和视觉质量）上呈现出不同趋势。使用连续代币的模型比使用离散代币的模型在视觉质量上显著优越。此外，生成顺序和注意力机制也对GenEval评分有显著影响，随机顺序模型的GenEval评分显著高于栅格顺序模型。在这些发现的启发下，我们训练了Fluid，一个基于连续代币的随机顺序自回归模型。Fluid 10.5B模型在MS-COCO 30K上达到了新的零-shot FID 6.16的状态，整体GenEval评分为0.69。我们希望这些发现和结果能够鼓励未来进一步缩小视觉和语言模型之间的规模差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 18:31:49 GMT</pubDate>
</item>
<item>
<title>JudgeBench：评估LLM基础评判模型的新基准</title>
<link>https://arxiv.org/abs/2410.12784</link>
<guid>https://arxiv.org/abs/2410.12784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出JudgeBench评估框架，以客观方式评估LLM基础评判者的能力。</p><br /><br /><p><strong>摘要：</strong> 随着LLM（大语言模型）基础评判者在模型评估中的应用日益广泛，其自身的可靠性问题却鲜有关注。现有评估基准多集中在评判与人类偏好的对齐，常常忽视了更具挑战性的任务中众包人类偏好带来的局限。为此，本文提出了一种新颖的评估框架，旨在客观地评估LLM基础评判者。基于这一框架，我们推出了JudgeBench，作为一项新基准，用于在知识、推理、数学和编码等领域评估LLM基础评判者的能力。JudgeBench通过将现有的困难数据集转换为带有客观正确性偏好的挑战响应对，利用一套新颖的管道进行构建。我们的综合评估结果显示，JudgeBench相较于以往基准提供了更具挑战性的任务，许多强大的模型（如GPT-4o）表现仅略高于随机猜测。总的来说，JudgeBench为日益先进的LLM基础评判者的评估提供了一套可靠平台。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 17:25:57 GMT</pubDate>
</item>
<item>
<title>WorldCuisines：多元文化的视觉问答基准</title>
<link>https://arxiv.org/abs/2410.12705</link>
<guid>https://arxiv.org/abs/2410.12705</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldCuisines是一个针对多语言和多文化的视觉问答基准，包含超过100万个饮食相关的数据点。</p><br /><br /><p><strong>摘要：</strong> Vision Language Models (VLMs) 在处理文化特定知识时面临挑战，尤其是对于非英语语言和缺失代表性的文化背景。为此，我们推出了WorldCuisines，这是一个针对多语言和多文化的视觉理解基准。该基准包含一个视觉问答（VQA）数据集，涵盖30种语言和方言，跨越9个语言家族，并拥有超过100万的数据点，是迄今为止最大规模的多文化VQA基准。数据集任务包括识别菜品名称及其来源。我们提供了两种规模的评估数据集（12k和60k实例）以及一个训练数据集（100万实例）。我们的研究表明，虽然VLMs在正确的地理上下文中表现较好，但在敌对上下文中和预测特定地方菜系及语言时却存在困难。为了支持未来的研究，我们还发布了一个包含注释食品条目和图像的知识库，配合VQA数据一起提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12705" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 14:10:54 GMT</pubDate>
</item>
<item>
<title>Open Materials 2024: 大规模开放数据集及预训练模型的发布</title>
<link>https://arxiv.org/abs/2410.12771</link>
<guid>https://arxiv.org/abs/2410.12771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们发布了OMat24数据集和EquiformerV2模型，以推动AI辅助材料科学的发展。</p><br /><br /><p><strong>摘要：</strong> 随着对新材料发现与设计能力的日益重视，人工智能（AI）在材料科学中的应用得到广泛关注。为了解决材料发现中的数据短缺问题，我们推出了Open Materials 2024 (OMat24)大规模开放数据集及一系列预训练模型。OMat24包含超过1.1亿个集中于结构和组成多样性的密度泛函理论（DFT）计算。我们的EquiformerV2模型在Matbench Discovery排行榜上实现了最先进的性能，能够以高达0.9的F1分数和每个原子20 meV的精度预测基态稳定性和形成能。我们还探讨了模型规模、辅助去噪目标以及微调对不同数据集（包括OMat24、MPtraj和Alexandria）上性能的影响。OMat24数据集和模型的开放发布将使研究界能够在我们的研究基础上继续开展工作，推动AI辅助材料科学的进一步进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 13:03:29 GMT</pubDate>
</item>
<item>
<title>推进语音大语言模型的五级发展路线图与评估基准</title>
<link>https://arxiv.org/abs/2410.13268</link>
<guid>https://arxiv.org/abs/2410.13268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一套五级路线图，旨在指导语音大语言模型的发展，并设计评估基准以揭示其当前局限性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的成功，结合语音和音频数据的努力日益增多，旨在创建能够处理文本和非文本输入的一般基础模型。最近的进展，如GPT-4o，突显了端到端语音LLMs的潜力，这种模型可以保留非语义信息和世界知识，以实现更深层次的语音理解。为了引导语音LLMs的发展，本文提出了一种五级路线图，涵盖从基础的自动语音识别（ASR）到能够整合非语义信息和抽象声学知识以完成复杂任务的先进超人模型。此外，我们设计了SAGI基准，标准化了各个任务在这五个级别上的关键方面，以揭示利用抽象声学知识和能力完整性的挑战。研究结果显示在处理副语言线索和抽象声学知识方面存在不足，并提出了未来的研究方向。本论文概述了推进语音LLMs的路线图，引入了评估基准，并提供了对其当前局限性和潜能的关键见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 12:46:43 GMT</pubDate>
</item>
<item>
<title>MobA：基于多模态大语言模型的移动助手</title>
<link>https://arxiv.org/abs/2410.13757</link>
<guid>https://arxiv.org/abs/2410.13757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MobA是一种新型移动助手，通过多模态大语言模型提升理解与规划能力，解决复杂指令处理问题。</p><br /><br /><p><strong>摘要：</strong> 当前移动助手因依赖系统API以及对复杂用户指令和多样化界面的理解能力有限而面临挑战。为此，我们提出MobA，这是一种新型的移动助手，采用多模态大语言模型（MLLM），通过复杂的两级代理架构增强理解和规划能力。高层的全局代理（Global Agent, GA）负责理解用户命令、追踪历史记忆并规划任务，而低层的局部代理（Local Agent, LA）根据GA提供的子任务和记忆，预测详细行动（以函数调用形式呈现）。集成的反思模块则提高了任务完成的效率，使系统能处理未见过的复杂任务。在真实评估中，MobA在任务执行效率和完成率方面表现出了显著改善，彰显了基于MLLM的移动助手的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 09:41:59 GMT</pubDate>
</item>
<item>
<title>gamma-MoD: 提升多模态大语言模型计算效率的新策略</title>
<link>https://arxiv.org/abs/2410.13859</link>
<guid>https://arxiv.org/abs/2410.13859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出gamma-MoD策略，通过激活度指标优化大语言模型计算，显著提高效率。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLMs）取得了显著进展，但其高计算成本仍是实际应用的障碍。受到自然语言处理中的深度混合（MoD）启发，本文旨在从“激活token”的角度解决这一问题。我们的关键见解是，如果大多数token在层计算中是冗余的，则可以通过MoD层直接跳过。然而，直接将MLLMs的密集层转换为MoD层会导致性能显著下降。为了解决这一问题，我们提出了一种名为gamma-MoD的创新MoD适应策略。在gamma-MoD中，提出了一种新指标：注意力图的排序（ARank），以引导MLLM中MoD的部署。通过ARank，我们能够有效识别哪些层是冗余的并应替换为MoD层。基于ARank，我们进一步提出两种新设计，以最大化MLLM的计算稀疏性，同时保持性能，即共享视角-语言路由器和掩码路由学习。通过这些设计，超过90%的MLLM稠密层可以有效转换为MoD层。为了验证我们的方法，我们将其应用于三种流行的MLLM，并在9个基准数据集上进行了广泛的实验。实验结果不仅验证了gamma-MoD对现有MLLM的显著效率提升，还确认了其在不同MLLM上的泛化能力。例如，gamma-MoD可以在略微性能下降（-1.5%）的情况下，将LLaVA-HR的训练和推理时间分别减少31.0%和53.2%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 08:06:48 GMT</pubDate>
</item>
<item>
<title>基于高质量数据的长输出能力模型调优研究</title>
<link>https://arxiv.org/abs/2410.10210</link>
<guid>https://arxiv.org/abs/2410.10210</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了调优模型以实现长输出能力的方法，强调数据质量的重要性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的快速发展，它们在生成长输出方面的能力存在显著差异。近期研究表明，模型在对齐训练过程中缺乏长输出数据是导致这一不平衡的主要原因。为了解决这一问题，我们尝试通过用填补数据缺口的数据重新对齐基础模型，从而使模型在接到指令时能够生成长篇输出。本文探讨了数据质量在模型长输出调优过程中的影响，以及如何从人类对齐的模型（如指令或聊天模型）出发进行调优。通过精心的数据策划，我们展示了在仅用少量训练数据实例和计算资源的情况下，能够实现与我们调优模型相似的性能提升。此外，我们通过尝试将我们的调优方案应用于几种不同模型，评估了此方法的通用性。研究结果表明，尽管不同模型在未经过调整时生成长输出的能力差异较大，但通过高质量数据和轻量计算资源进行调优的方法，能够在我们实验的所有模型中一致性地实现显著改善。我们公开了用于调优长写作能力的数据集、模型调优和评估的实现方案以及微调后的模型，这些都可供公众访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10210" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 08:01:07 GMT</pubDate>
</item>
<item>
<title>无指导自回归视觉生成的条件对比对齐方法</title>
<link>https://arxiv.org/abs/2410.09347</link>
<guid>https://arxiv.org/abs/2410.09347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Condition Contrastive Alignment，提升自回归视觉生成性能，减少对指导抽样的依赖。</p><br /><br /><p><strong>摘要：</strong> Classifier-Free Guidance (CFG) 是提高视觉生成模型样本质量的重要技术。然而，在自回归（AR）多模态生成中，CFG导致语言和视觉内容之间设计不一致，违背了对不同模态进行统一设计的理念。为了解决这个问题，我们提出Condition Contrastive Alignment (CCA) 方法，旨在促进高性能的无指导自回归视觉生成，并分析其与指导抽样方法的理论联系。不同于通过改变抽样过程以达到理想抽样分布的指导方法，CCA直接对预训练模型进行微调，以适应相同的分布目标。实验结果表明，CCA能够在仅进行一个epoch的微调（相当于约1%的预训练周期）后，显著增强所有测试模型的无指导性能，其表现与指导抽样方法相当。这大大减少了AR视觉生成中的指导抽样需求，并将采样成本降低了一半。此外，通过调整训练参数，CCA能够在样本多样性与真实性之间实现类似于CFG的权衡。这一研究实验性地确认了语言目标对齐与视觉目标指导方法之间的紧密理论联系，从而统一了两个之前独立的研究领域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 07:42:08 GMT</pubDate>
</item>
<item>
<title>TransAgent：通过多源知识蒸馏提升视觉-语言基础模型</title>
<link>https://arxiv.org/abs/2410.12183</link>
<guid>https://arxiv.org/abs/2410.12183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransAgent 框架通过多源知识蒸馏提升 CLIP 等视觉-语言基础模型在多样化数据上的表现。</p><br /><br /><p><strong>摘要：</strong> 视觉-语言基础模型（如 CLIP）在迁移学习方面展现了巨大的潜力，得益于大规模的图像-文本预训练。然而，目标域数据在下游任务中可能与预训练阶段存在较大差异，这使得模型的泛化能力受到限制。为了解决这一问题，我们提出了一个通用且简洁的 TransAgent 框架，它以统一的方式传输孤立代理的知识，有效引导 CLIP 模型通过多源知识蒸馏进行泛化。通过该框架，我们灵活地与 11 个异构代理进行合作，增强视觉-语言基础模型，而在推理阶段没有额外的成本。最终，TransAgent 在 11 个视觉识别数据集上达到了最先进的性能。在相同的低样本设置下，它在平均上比流行的 CoOp 超出约 10%，在包含较大领域偏移的 EuroSAT 数据集上则超过了 20%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 07:10:06 GMT</pubDate>
</item>
<item>
<title>Long-LRM：基于3D高斯重建的长序列图像大场景重建模型</title>
<link>https://arxiv.org/abs/2410.12781</link>
<guid>https://arxiv.org/abs/2410.12781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Long-LRM是一种高效的3D高斯重建模型，能快速重建大场景。</p><br /><br /><p><strong>摘要：</strong> Long-LRM是一种通用的3D高斯重建模型，能够从长序列的输入图像中重建大场景。该模型可处理32张960x540分辨率的源图像，且在单个A100 80G GPU上仅需1.3秒。我们的架构结合了最新的Mamba2模块和经典的Transformer模块，使得能够处理比以往更多的tokens，同时通过高效的token合并和高斯剪枝步骤来平衡质量和效率。与之前只能处理1到4张输入图像并仅能重建小部分大场景的前馈模型不同，Long-LRM在一次前馈步骤中便可重建整个场景。在DL3DV-140和Tanks and Temples等大规模场景数据集上，我们的方法在性能上可与基于优化的方法相媲美，同时效率提升两个数量级。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12781" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 06:59:14 GMT</pubDate>
</item>
<item>
<title>中文图像含义理解基准CII-Bench的提出与评估</title>
<link>https://arxiv.org/abs/2410.13854</link>
<guid>https://arxiv.org/abs/2410.13854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CII-Bench评估多模态大语言模型对中文图像的高阶理解能力，揭示其在传统文化理解方面的不足。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型（MLLM）能力的不断提升，对其高阶感知和理解能力的评估需求日益增加。然而，目前仍缺乏针对中文视觉内容的MLLM高阶评估工作。为填补这一空白，我们提出了中文图像含义理解基准（CII-Bench），旨在评估MLLM在中文图像上的高阶感知和理解能力。CII-Bench在若干方面独树一帜：首先，基准中的图像来自中文互联网，并经过人工审查，相关答案也由人工精心撰写。此外，CII-Bench还纳入了代表中国传统文化的图像，如著名的中国传统绘画，能够深刻反映模型对中国传统文化的理解。通过对多种MLLM在CII-Bench上的广泛实验，我们发现MLLM的表现与人类存在显著差距，最高准确率为64.4%，而人类平均准确率为78.2%。同时，MLLM在理解中国传统文化图像时表现较差，反映出其高阶语义理解的局限性及对中国传统文化知识的欠缺。最后，研究发现大多数模型在提示中加入图像情感线索时准确率有所提升。我们相信CII-Bench将促进MLLM对中文语义及中文特定图像的更好理解，推动通用人工智能（AGI）的进程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 06:54:13 GMT</pubDate>
</item>
<item>
<title>基于检索增强个性化的多模态大语言模型框架</title>
<link>https://arxiv.org/abs/2410.13360</link>
<guid>https://arxiv.org/abs/2410.13360</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了检索增强个性化框架（RAP），实现多模态语言模型的个性化助手。 </p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种检索增强个性化（RAP）框架，用于提升多模态大语言模型（MLLMs）的个性化能力。RAP通过三个步骤实现个性化助手的创建：首先，设计键值数据库存储用户相关信息，例如姓名、头像等；其次，通过多模态检索器在用户发起对话时从数据库中检索相关信息；最后，将输入查询和检索到的相关信息输入到多模态语言模型中，以生成个性化的知识增强响应。与以往方法不同，RAP支持实时的概念编辑，通过更新外部数据库以适应用户需求。此外，为了提升生成质量和与用户特定信息的一致性，本文设计了一个数据收集管道，并创建了用于个性化训练的专用数据集。通过该数据集，训练了一系列个性化的多模态助手模型。RAP-MLLMs能够在无须额外微调的情况下，凭借大规模的预训练数据集，推广到无限的视觉概念。该模型在个性化图像描述、问答和视觉识别等多种任务中展示了出色的灵活性和生成质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.13360" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 06:47:51 GMT</pubDate>
</item>
<item>
<title>MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization</title>
<link>https://arxiv.org/abs/2410.12957</link>
<guid>https://arxiv.org/abs/2410.12957</guid>
<content:encoded><![CDATA[
Generating music that aligns with the visual content of a video has been a challenging task, as it requires a deep understanding of visual semantics and involves generating music whose melody, rhythm, and dynamics harmonize with the visual narratives. This paper presents MuVi, a novel framework that effectively addresses these challenges to enhance the cohesion and immersive experience of audio-visual content. MuVi analyzes video content through a specially designed visual adaptor to extract contextually and temporally relevant features. These features are used to generate music that not only matches the video's mood and theme but also its rhythm and pacing. We also introduce a contrastive music-visual pre-training scheme to ensure synchronization, based on the periodicity nature of music phrases. In addition, we demonstrate that our flow-matching-based music generator has in-context learning ability, allowing us to control the style and genre of the generated music. Experimental results show that MuVi demonstrates superior performance in both audio quality and temporal synchronization. The generated music video samples are available at https://muvi-v2m.github.io.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 06:33:00 GMT</pubDate>
</item>
<item>
<title>Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</title>
<link>https://arxiv.org/abs/2410.13334</link>
<guid>https://arxiv.org/abs/2410.13334</guid>
<content:encoded><![CDATA[
Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as `jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept of PCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense method PCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation. PCDefense stands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 06:13:36 GMT</pubDate>
</item>
<item>
<title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2410.13848</link>
<guid>https://arxiv.org/abs/2410.13848</guid>
<content:encoded><![CDATA[
In this paper, we introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 04:23:02 GMT</pubDate>
</item>
<item>
<title>MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures</title>
<link>https://arxiv.org/abs/2410.13754</link>
<guid>https://arxiv.org/abs/2410.13754</guid>
<content:encoded><![CDATA[
Perceiving and generating diverse modalities are crucial for AI models to effectively learn from and engage with real-world signals, necessitating reliable evaluations for their development. We identify two major issues in current evaluations: (1) inconsistent standards, shaped by different communities with varying protocols and maturity levels; and (2) significant query, grading, and generalization biases. To address these, we introduce MixEval-X, the first any-to-any real-world benchmark designed to optimize and standardize evaluations across input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring evaluations generalize effectively to real-world use cases. Extensive meta-evaluations show our approach effectively aligns benchmark samples with real-world task distributions and the model rankings correlate strongly with that of crowd-sourced real-world evaluations (up to 0.98). We provide comprehensive leaderboards to rerank existing models and organizations and offer insights to enhance understanding of multi-modal evaluations and inform future research.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 03:48:45 GMT</pubDate>
</item>
<item>
<title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title>
<link>https://arxiv.org/abs/2410.11842</link>
<guid>https://arxiv.org/abs/2410.11842</guid>
<content:encoded><![CDATA[
In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 03:16:53 GMT</pubDate>
</item>
<item>
<title>SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2410.13293</link>
<guid>https://arxiv.org/abs/2410.13293</guid>
<content:encoded><![CDATA[
Many students struggle with math word problems (MWPs), often finding it difficult to identify key information and select the appropriate mathematical operations.Schema-based instruction (SBI) is an evidence-based strategy that helps students categorize problems based on their structure, improving problem-solving accuracy. Building on this, we propose a Schema-Based Instruction Retrieval-Augmented Generation (SBI-RAG) framework that incorporates a large language model (LLM).Our approach emphasizes step-by-step reasoning by leveraging schemas to guide solution generation. We evaluate its performance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo, and introduce a "reasoning score" metric to assess solution quality. Our findings suggest that SBI-RAG enhances reasoning clarity and problem-solving accuracy, potentially providing educational benefits for students
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 02:59:02 GMT</pubDate>
</item>
<item>
<title>LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2410.13618</link>
<guid>https://arxiv.org/abs/2410.13618</guid>
<content:encoded><![CDATA[
The rapid growth of model scale has necessitated substantial computational resources for fine-tuning. Existing approach such as Low-Rank Adaptation (LoRA) has sought to address the problem of handling the large updated parameters in full fine-tuning. However, LoRA utilize random initialization and optimization of low-rank matrices to approximate updated weights, which can result in suboptimal convergence and an accuracy gap compared to full fine-tuning. To address these issues, we propose LoLDU, a Parameter-Efficient Fine-Tuning (PEFT) approach that significantly reduces trainable parameters by 2600 times compared to regular PEFT methods while maintaining comparable performance. LoLDU leverages Lower-Diag-Upper Decomposition (LDU) to initialize low-rank matrices for faster convergence and orthogonality. We focus on optimizing the diagonal matrix for scaling transformations. To the best of our knowledge, LoLDU has the fewest parameters among all PEFT approaches. We conducted extensive experiments across 4 instruction-following datasets, 6 natural language understanding (NLU) datasets, 8 image classification datasets, and image generation datasets with multiple model types (LLaMA2, RoBERTa, ViT, and Stable Diffusion), providing a comprehensive and detailed analysis. Our open-source code can be accessed at https://github.com/SKDDJ/LoLDU{https://github.com/SKDDJ/LoLDU}.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 02:30:02 GMT</pubDate>
</item>
<item>
<title>VidPanos: Generative Panoramic Videos from Casual Panning Videos</title>
<link>https://arxiv.org/abs/2410.13832</link>
<guid>https://arxiv.org/abs/2410.13832</guid>
<content:encoded><![CDATA[
Panoramic image stitching provides a unified, wide-angle view of a scene that extends beyond the camera's field of view. Stitching frames of a panning video into a panoramic photograph is a well-understood problem for stationary scenes, but when objects are moving, a still panorama cannot capture the scene. We present a method for synthesizing a panoramic video from a casually-captured panning video, as if the original video were captured with a wide-angle camera. We pose panorama synthesis as a space-time outpainting problem, where we aim to create a full panoramic video of the same length as the input video. Consistent completion of the space-time volume requires a powerful, realistic prior over video content and motion, for which we adapt generative video models. Existing generative models do not, however, immediately extend to panorama completion, as we show. We instead apply video generation as a component of our panorama synthesis system, and demonstrate how to exploit the strengths of the models while minimizing their limitations. Our system can create video panoramas for a range of in-the-wild scenes including people, vehicles, and flowing water, as well as stationary background features.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 02:17:13 GMT</pubDate>
</item>
<item>
<title>Movie Gen: A Cast of Media Foundation Models</title>
<link>https://arxiv.org/abs/2410.13720</link>
<guid>https://arxiv.org/abs/2410.13720</guid>
<content:encoded><![CDATA[
We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user's image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 02:16:45 GMT</pubDate>
</item>
<item>
<title>BenTo: Benchmark Task Reduction with In-Context Transferability</title>
<link>https://arxiv.org/abs/2410.13804</link>
<guid>https://arxiv.org/abs/2410.13804</guid>
<content:encoded><![CDATA[
Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a &lt;4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 01:58:57 GMT</pubDate>
</item>
<item>
<title>PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</title>
<link>https://arxiv.org/abs/2410.13785</link>
<guid>https://arxiv.org/abs/2410.13785</guid>
<content:encoded><![CDATA[
Alignment of large language models (LLMs) involves training models on preference-contrastive output pairs to adjust their responses according to human preferences. To obtain such contrastive pairs, traditional methods like RLHF and RLAIF rely on limited contrasting patterns, such as varying model variants or decoding temperatures. This singularity leads to two issues: (1) alignment is not comprehensive; and thereby (2) models are susceptible to jailbreaking attacks. To address these issues, we investigate how to construct more comprehensive and diversified contrasting patterns to enhance preference data (RQ1) and verify the impact of the diversification of contrasting patterns on model alignment (RQ2). For RQ1, we propose PopAlign, a framework that integrates diversified contrasting patterns across the prompt, model, and pipeline levels, introducing six contrasting strategies that do not require additional feedback labeling procedures. Regarding RQ2, we conduct thorough experiments demonstrating that PopAlign significantly outperforms existing methods, leading to more comprehensive alignment.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 01:39:45 GMT</pubDate>
</item>
<item>
<title>MedMobile: A mobile-sized language model with expert-level clinical capabilities</title>
<link>https://arxiv.org/abs/2410.09019</link>
<guid>https://arxiv.org/abs/2410.09019</guid>
<content:encoded><![CDATA[
Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (~60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 01:31:56 GMT</pubDate>
</item>
<item>
<title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title>
<link>https://arxiv.org/abs/2410.13824</link>
<guid>https://arxiv.org/abs/2410.13824</guid>
<content:encoded><![CDATA[
Text-rich visual understanding-the ability to process environments where dense textual content is integrated with visuals-is crucial for multimodal large language models (MLLMs) to interact effectively with structured environments. To enhance this capability, we propose synthesizing general multimodal instructions from webpage UIs using text-based large language models (LLMs). Despite lacking direct visual input, text-based LLMs are able to process structured text representations from webpage accessibility trees. These instructions are then paired with UI screenshots to train multimodal models. We introduce MultiUI, a dataset containing 7.3 million samples from 1 million websites, covering diverse multimodal tasks and UI layouts. Models trained on MultiUI not only excel in web UI tasks-achieving up to a 48\% improvement on VisualWebBench and a 19.1\% boost in action accuracy on a web agent dataset Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to non-UI domains, such as document understanding, OCR, and chart interpretation. These results highlight the broad applicability of web UI data for advancing text-rich visual understanding across various scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 01:04:36 GMT</pubDate>
</item>
<item>
<title>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</title>
<link>https://arxiv.org/abs/2410.13830</link>
<guid>https://arxiv.org/abs/2410.13830</guid>
<content:encoded><![CDATA[
Recent advances in customized video generation have enabled users to create videos tailored to both specific subjects and motion trajectories. However, existing methods often require complicated test-time fine-tuning and struggle with balancing subject learning and motion control, limiting their real-world applications. In this paper, we present DreamVideo-2, a zero-shot video customization framework capable of generating videos with a specific subject and motion trajectory, guided by a single image and a bounding box sequence, respectively, and without the need for test-time fine-tuning. Specifically, we introduce reference attention, which leverages the model's inherent capabilities for subject learning, and devise a mask-guided motion module to achieve precise motion control by fully utilizing the robust motion signal of box masks derived from bounding boxes. While these two components achieve their intended functions, we empirically observe that motion control tends to dominate over subject learning. To address this, we propose two key designs: 1) the masked reference attention, which integrates a blended latent mask modeling scheme into reference attention to enhance subject representations at the desired positions, and 2) a reweighted diffusion loss, which differentiates the contributions of regions inside and outside the bounding boxes to ensure a balance between subject and motion control. Extensive experimental results on a newly curated dataset demonstrate that DreamVideo-2 outperforms state-of-the-art methods in both subject customization and motion control. The dataset, code, and models will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 01:04:28 GMT</pubDate>
</item>
<item>
<title>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</title>
<link>https://arxiv.org/abs/2410.13085</link>
<guid>https://arxiv.org/abs/2410.13085</guid>
<content:encoded><![CDATA[
Artificial Intelligence (AI) has demonstrated significant potential in healthcare, particularly in disease diagnosis and treatment planning. Recent progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new possibilities for interactive diagnostic tools. However, these models often suffer from factual hallucination, which can lead to incorrect diagnoses. Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to address these issues. However, the amount of high-quality data and distribution shifts between training data and deployment data limit the application of fine-tuning methods. Although RAG is lightweight and effective, existing RAG-based approaches are not sufficiently general to different medical domains and can potentially cause misalignment issues, both between modalities and between the model and the ground truth. In this paper, we propose a versatile multimodal RAG system, MMed-RAG, designed to enhance the factuality of Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an adaptive retrieved contexts selection method, and a provable RAG-based preference fine-tuning strategy. These innovations make the RAG process sufficiently general and reliable, significantly improving alignment when introducing retrieved contexts. Experimental results across five medical datasets (involving radiology, ophthalmology, pathology) on medical VQA and report generation demonstrate that MMed-RAG can achieve an average improvement of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available in https://github.com/richard-peng-xia/MMed-RAG.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:55:13 GMT</pubDate>
</item>
<item>
<title>A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models</title>
<link>https://arxiv.org/abs/2410.13841</link>
<guid>https://arxiv.org/abs/2410.13841</guid>
<content:encoded><![CDATA[
Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties via operations like pruning, quantization, low-rank approximation, and extrapolation, a unified framework for systematically examining these characteristics has been lacking. In this paper, we propose a novel perspective based on Riemann sum approximation of the loss function to elucidate delta parameter editing operations. Our analysis categorizes existing methods into three classes based on their post-editing performance: competitive, decreased, and improved, explaining how they are expressed by the Riemann sum approximation term and how they alter the model performance. Extensive experiments on both visual and language models, including ViT, LLaMA 3, Qwen 2, and Mistral, corroborate our theoretical findings. Furthermore, we introduce extensions to existing techniques like DARE and BitDelta, highlighting their limitations in leveraging the properties of delta parameters and reorganizing them into general expressions to enhance the applicability and effectiveness of delta parameter editing in post-trained models.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:51:24 GMT</pubDate>
</item>
<item>
<title>AERO: Softmax-Only LLMs for Efficient Private Inference</title>
<link>https://arxiv.org/abs/2410.13060</link>
<guid>https://arxiv.org/abs/2410.13060</guid>
<content:encoded><![CDATA[
The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads, primarily due to nonlinear operations. In this paper, we present a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models. We introduce AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the first time, we propose a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI. Furthermore, we devise a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23times communication and 1.94times latency reduction. We validate the effectiveness of AERO by benchmarking it against the state-of-the-art.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:45:35 GMT</pubDate>
</item>
<item>
<title>Retrospective Learning from Interactions</title>
<link>https://arxiv.org/abs/2410.13852</link>
<guid>https://arxiv.org/abs/2410.13852</guid>
<content:encoded><![CDATA[
Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:43:05 GMT</pubDate>
</item>
<item>
<title>Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2410.13198</link>
<guid>https://arxiv.org/abs/2410.13198</guid>
<content:encoded><![CDATA[
Generative Error Correction (GEC) has emerged as a powerful post-processing method to enhance the performance of Automatic Speech Recognition (ASR) systems. However, we show that GEC models struggle to generalize beyond the specific types of errors encountered during training, limiting their ability to correct new, unseen errors at test time, particularly in out-of-domain (OOD) scenarios. This phenomenon amplifies with named entities (NEs), where, in addition to insufficient contextual information or knowledge about the NEs, novel NEs keep emerging. To address these issues, we propose DARAG (Data- and Retrieval-Augmented Generative Error Correction), a novel approach designed to improve GEC for ASR in in-domain (ID) and OOD scenarios. We augment the GEC training dataset with synthetic data generated by prompting LLMs and text-to-speech models, thereby simulating additional errors from which the model can learn. For OOD scenarios, we simulate test-time errors from new domains similarly and in an unsupervised fashion. Additionally, to better handle named entities, we introduce retrieval-augmented correction by augmenting the input with entities retrieved from a database. Our approach is simple, scalable, and both domain- and language-agnostic. We experiment on multiple datasets and settings, showing that DARAG outperforms all our baselines, achieving 8\% -- 30\% relative WER improvements in ID and 10\% -- 33\% improvements in OOD settings.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:25:05 GMT</pubDate>
</item>
<item>
<title>FlatQuant: Flatness Matters for LLM Quantization</title>
<link>https://arxiv.org/abs/2410.09426</link>
<guid>https://arxiv.org/abs/2410.09426</guid>
<content:encoded><![CDATA[
Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). Due to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5%. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely 0.07x, bringing up to 2.3x speedup for prefill and 1.7x speedup for decoding, respectively. Code is available at: https://github.com/ruikangliu/FlatQuant.
]]></content:encoded>
<pubDate>Fri, 18 Oct 2024 00:05:54 GMT</pubDate>
</item>
<item>
<title>From Commands to Prompts: LLM-based Semantic File System for AIOS</title>
<link>https://arxiv.org/abs/2410.11843</link>
<guid>https://arxiv.org/abs/2410.11843</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have demonstrated significant potential in the development of intelligent applications and systems such as LLM-based agents and agent operating systems (AIOS). However, when these applications and systems interact with the underlying file system, the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based semantic file system ( LSFS ) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update monitoring and summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations (e.g., CRUD, group by, join) powered by vector database. Our experiments show that LSFS offers significant improvements over traditional file systems in terms of user convenience, the diversity of supported functions, and the accuracy and efficiency of file operations. Additionally, with the integration of LLM, our system enables more intelligent file management tasks, such as content summarization and version comparison, further enhancing its capabilities.
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 23:07:33 GMT</pubDate>
</item>
<item>
<title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title>
<link>https://arxiv.org/abs/2410.11900</link>
<guid>https://arxiv.org/abs/2410.11900</guid>
<content:encoded><![CDATA[
Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce Faithful Logic-Aided Reasoning and Exploration (\ours), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on 7 out of 9 diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that {\ours} allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 21:59:33 GMT</pubDate>
</item>
<item>
<title>语言模型校准：应对RLHF中的过度自信现象</title>
<link>https://arxiv.org/abs/2410.09724</link>
<guid>https://arxiv.org/abs/2410.09724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了模型过度自信的原因，并提出两种强化学习变种以改善模型校准。</p><br /><br /><p><strong>摘要：</strong> 本文研究语言模型的校准问题，特别是强化学习中的人类反馈（RLHF）如何导致模型在自身响应上的过度自信现象。我们指出，RLHF训练的奖励模型在实际响应质量与高置信度分数之间存在固有偏差。为此，本文提出了两种改进的近端策略优化（PPO）方法：PPO-M和PPO-C。PPO-M通过在奖励模型训练中整合显式置信度分数来校准奖励模型，而PPO-C则是在PPO过程中根据当前奖励与过去奖励的移动平均差异调节奖励分数。这两种方法可以无缝集成到现有的PPO流程中，且不需要额外的黄金标签。我们在Llama3-8B和Mistral-7B模型上，对六个多样化的数据集进行了评估，包括多选择题和开放式生成。实验结果表明，这两种方法能有效减少校准误差，同时维持与标准PPO相当的性能，并且在开放式对话场景中不影响模型能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 14:29:03 GMT</pubDate>
</item>
<item>
<title>优化潜在空间的图像生成模型：DiGIT的探索</title>
<link>https://arxiv.org/abs/2410.12490</link>
<guid>https://arxiv.org/abs/2410.12490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出DiGIT，提升图像自回归生成模型的性能，首次 outperform LDMs。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了潜在空间在图像生成模型中的稳定性，特别是自回归模型与潜在扩散模型（LDMs）和掩模图像模型（MIMs）之间的差异。尽管自回归模型在自然语言处理（NLP）中表现卓越，但在图像生成领域存在明显劣势。为解决这一问题，提出了一种简单有效的离散图像标记器（DiGIT），旨在稳定图像生成中的潜在空间。通过实验验证，DiGIT 在图像理解和生成上均显著提升了自回归模型的性能，显示出基于下一个标记预测原则的优势。值得注意的是，DiGIT 的实现首次使自回归图像模型超越了 LDMs，并展示了随模型规模扩大而带来的显著改进。这一发现强调了优化潜在空间和离散标记化的结合对于推动图像生成模型能力的潜力。代码可在 https://github.com/DAMO-NLP-SG/DiGIT 中获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 13:58:54 GMT</pubDate>
</item>
<item>
<title>ChroKnowBench：评估大规模语言模型的时间知识积累</title>
<link>https://arxiv.org/abs/2410.09870</link>
<guid>https://arxiv.org/abs/2410.09870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ChroKnowBench基准和ChroKnowledge框架，以评估大规模语言模型的时间知识及其更新效果。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了ChroKnowBench，一个用于评估大规模语言模型（LLMs）时间知识积累的新基准数据集，旨在解决知识的累积性质与时间依赖性。该基准涵盖多个领域，区分了随着时间演变的知识（如科学发现、修订法律）与不变的知识（如数学真理、公理事实）。基于此基准，研究者们引入了ChroKnowledge（Chronological Categorization of Knowledge），一个用于评价和更新LLMs非参数时间知识的采样框架。评估结果显示，模型在提取时间知识的能力上与训练数据格式有关，并且往往在时间边界处出现知识回忆的截断。为此，研究者提出了ChroKnowPrompt，通过逐步引导模型穿越时间跨度，来有效地引发时间知识的提取。结果表明，该框架在生物医学领域和通用领域成功更新了整体知识，分别提升了11.9%和2.8%。该非参数方法适用于开源模型和专有LLMs，确保了其全面适用性，展示了通过此方法提取内在时间知识的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 11:56:11 GMT</pubDate>
</item>
<item>
<title>可控安全对齐框架：满足多样化安全需求的语言模型适应性</title>
<link>https://arxiv.org/abs/2410.08968</link>
<guid>https://arxiv.org/abs/2410.08968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出可控安全对齐（CoSA）框架，使语言模型根据安全配置灵活适应用户需求。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLM）的安全对齐方法采用一刀切的方式，模型对任何被提供者认定为不安全的内容拒绝交互。这种方法在面对不同文化和地区的社会规范时缺乏灵活性。此外，由于用户的安全需求各不相同，静态的安全标准可能过于严格，影响模型的实用性，并且重新对齐的成本也较高。为此，我们提出可控安全对齐（CoSA）框架，旨在在不重新训练的情况下，使模型适应多样化的安全要求。CoSA通过将安全配置作为系统提示的一部分，使模型遵循自由形式的自然语言描述来调整安全行为。通过在推理过程中由授权用户修改安全配置，可以简单地调整模型的安全行为。我们还提出了CoSAlign，一种数据中心的方法，使得LLM能够轻松适应多样化的安全配置。此外，我们设计了一个新颖的可控性评估协议，考虑了有用性和配置安全，并将其总结为CoSA评分。我们构建了CoSApien，这是一个包含真实世界LLM用例及其评估提示的人类创作基准。实验证明，CoSAlign在可控性方面相较强基线，包括上下文对齐，获得了显著提升。我们的框架鼓励更好地表现和适应多元化的人类价值观，从而增加LLM的实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 11:46:21 GMT</pubDate>
</item>
<item>
<title>动态词汇头（DyVo）提升稀疏检索模型的实体识别效果</title>
<link>https://arxiv.org/abs/2410.07722</link>
<guid>https://arxiv.org/abs/2410.07722</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过动态词汇头（DyVo）与维基百科概念结合，提升了稀疏检索模型对实体的识别与检索能力。</p><br /><br /><p><strong>摘要：</strong> 学习型稀疏检索（LSR）模型使用预训练变换器的词汇，但往往将实体拆分成不合理的片段，从而降低检索准确性，并限制模型吸纳最新的世界知识。为此，本研究通过结合维基百科的概念和实体，增强了LSR的词汇，从而更有效地解决歧义并保持与不断发展的知识保持同步。我们的方法核心在于动态词汇（DyVo）头，该头利用现有实体嵌入和实体检索组件，识别与查询或文档相关的实体。DyVo头用于生成实体权重，随后将这些权重与词片权重合并，以创建高效编码和检索的联合表征，并使用倒排索引进行检索。实验结果显示，在三个富含实体的文档排序数据集上，DyVo模型的表现显著优于最先进的基线模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07722" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 10:24:44 GMT</pubDate>
</item>
<item>
<title>特征在不同文本领域之间的稳定性与转变研究</title>
<link>https://arxiv.org/abs/2410.12391</link>
<guid>https://arxiv.org/abs/2410.12391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究特征如何在不同文本领域的模型中出现、消失和持续。</p><br /><br /><p><strong>摘要：</strong> 本文研究了特征在不同领域文本的模型中的表现及其稳定性，聚焦于对一个基础的一层Transformer语言模型进行适应和测试。该基础模型是基于BabyLM语料库和来自The Stack的Python代码集训练的。我们将该基础模型适应于两个新的文本领域：TinyStories和Lua编程语言。随后，通过球面线性插值方法将这两个模型合并。我们的探索旨在揭示在小规模模型和稀疏自编码器的典型迁移学习场景中，特征的稳定性和转变过程。这为理解不同领域间特征的演变提供了更深入的见解，进而有助于提高迁移学习和模型适应能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 10:10:35 GMT</pubDate>
</item>
<item>
<title>通过逆向强化学习解读大型语言模型的隐性奖励函数</title>
<link>https://arxiv.org/abs/2410.12491</link>
<guid>https://arxiv.org/abs/2410.12491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究运用逆向强化学习解析毒性对齐的LLM隐性奖励，探讨模型大小与可解释性等关键问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法，通过逆向强化学习(Inverse Reinforcement Learning, IRL)恢复大型语言模型(LLMs)的隐性奖励函数，以理解它们的决策过程。研究聚焦于毒性对齐的LLM，采用多种规模的模型进行实验，提取奖励模型的预测准确率达到80.40%。分析中揭示了奖励函数的非可识别性、模型规模与可解释性之间的关系以及RLHF过程中的潜在问题。结果表明，通过IRL提取的奖励模型能够用于新LLM的微调，在毒性基准测试上实现了相当或更优的性能。这项工作为理解和改进大型语言模型的对齐提供了新的视角，并且对这些强大系统的负责任开发与部署具有重要意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 08:37:56 GMT</pubDate>
</item>
<item>
<title>神经形态变换（NeuMeta）：自适应神经网络的连续权重学习</title>
<link>https://arxiv.org/abs/2410.11878</link>
<guid>https://arxiv.org/abs/2410.11878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuMeta提出了一种新学习范式，实现自适应神经网络，并在不同配置下无缝生成权重。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的学习范式，即神经形态变换（NeuMeta），旨在构建可自我变换的神经网络。与为不同架构或规模制造单独模型不同，NeuMeta直接学习神经网络的连续权重流形。训练后，可以直接从流形中为任何大小的网络采样权重，甚至在先前未见过的配置中，无需重新训练。为了实现这一雄心勃勃的目标，NeuMeta将神经隐式函数作为超网络进行训练，接受模型空间内的坐标作为输入，并生成相应的不变权重值。训练过程中，发现最终性能与学习流形的平滑性密切相关。为提高平滑性，我们采用了两种策略：首先，通过解决最短哈密顿路径问题对权重矩阵进行置换以实现模型内部平滑性。其次，在训练隐式函数时对输入坐标添加噪声，以确保不同大小的模型均表现一致。因此，NeuMeta在合成多种网络配置的参数方面显示出良好效果。我们的广泛测试表明，NeuMeta在图像分类、语义分割和图像生成任务中，即使在75%的压缩率下，也能保持全尺寸性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 08:23:01 GMT</pubDate>
</item>
<item>
<title>基于连续时间的生成模型的稳定训练与快速采样</title>
<link>https://arxiv.org/abs/2410.11081</link>
<guid>https://arxiv.org/abs/2410.11081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，优化基于扩散的生成模型，在连续时间上实现稳定快速采样。</p><br /><br /><p><strong>摘要：</strong> 一致性模型（Consistency Models, CMs）是一类强大的基于扩散的生成模型，旨在实现快速采样。虽然现有的CMs大多采用离散时间步训练，这带来了额外的超参数并易受到离散化误差影响。但连续时间形式可以缓解这些问题，然而其成功受到训练不稳定性的限制。为了解决这一问题，我们提出了一个简化的理论框架，用于统一以前的扩散模型和CMs的参数化，找出了不稳定性的根本原因。在此分析的基础上，我们引入了扩散过程参数化、网络架构和训练目标的关键改进。这些改变使我们能够在前所未有的规模上训练连续时间CMs，达到1.5亿参数在ImageNet 512x512上。我们的训练算法仅使用两个采样步骤，即在CIFAR-10上实现了2.06的FID分数，在ImageNet 64x64上为1.48，在ImageNet 512x512上为1.88，FID分数与最佳扩散模型之间的差距缩小到10%以内。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 07:42:04 GMT</pubDate>
</item>
<item>
<title>WorldMedQA-V：多语言多模态医疗基准测试数据集</title>
<link>https://arxiv.org/abs/2410.12722</link>
<guid>https://arxiv.org/abs/2410.12722</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldMedQA-V是一个更新的多语言多模态基准数据集，旨在评估医疗领域的视觉语言模型。</p><br /><br /><p><strong>摘要：</strong> 随着多模态/视觉语言模型（VLMs）在全球医疗环境中的广泛应用，确保其安全性、有效性和公平性变得日益重要。本研究提出了WorldMedQA-V，一个更新的多语言多模态基准测试数据集，旨在评估VLMs在医疗领域的表现。该数据集包括568个标记的多项选择题和568个医学图像，覆盖来自巴西、以色列、日本和西班牙的四个国家，提供了原语言及经过本地临床医生验证的英文翻译。我们还提供了常见开源和闭源模型的基准性能测试结果，涵盖本地语言和英文翻译，并针对模型在有无图像的情况下的表现进行了对比。WorldMedQA-V基准旨在更好地匹配AI系统与其所处的多样化医疗环境，促进更公平、高效和具代表性的应用。该数据集为未来在多语言和多模态的医疗场景中的模型发展和应用提供了重要的基准依据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12722" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 07:19:55 GMT</pubDate>
</item>
<item>
<title>Large Language Model Evaluation via Matrix Nuclear-Norm</title>
<link>https://arxiv.org/abs/2410.10672</link>
<guid>https://arxiv.org/abs/2410.10672</guid>
<content:encoded><![CDATA[
As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}-norm \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 05:44:04 GMT</pubDate>
</item>
<item>
<title>VidEgoThink：评估自我视角视频理解能力的综合基准</title>
<link>https://arxiv.org/abs/2410.11623</link>
<guid>https://arxiv.org/abs/2410.11623</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidEgoThink基准评估自我视角视频理解能力，揭示多模态大语言模型在此领域的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）的进步为具身人工智能应用开辟了新方向。在之前工作EgoThink的基础上，我们提出了VidEgoThink，这是一个用于评估自我视角视频理解能力的综合基准。为了弥合MLLMs与具身AI低级控制之间的差距，我们设计了四个相互关联的关键任务：视频问答、层次规划、视觉定位和奖励建模。为降低手动标注成本，我们基于Ego4D数据集开发了一种自动数据生成管道，利用GPT-4o的先验知识和多模态能力，随后三位人工标注者过滤生成的数据，以确保多样性和质量，最终形成了VidEgoThink基准。我们对三种类型的模型进行了广泛实验：基于API的MLLMs、开源图像基MLLMs和开源视频基MLLMs。实验结果表明，包括GPT-4o在内的所有MLLMs在自我视角视频理解相关任务上表现较差。这些发现表明，基础模型仍需重大进展才能有效应用于具身人工智能中的第一人称场景。总体而言，VidEgoThink反映了将MLLMs应用于自我视角视觉的研究趋势，旨在实现与人类能力相似的主动观察与互动，探究复杂现实环境中的行为。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11623" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 05:23:48 GMT</pubDate>
</item>
<item>
<title>ZipVL：针对大规模视觉语言模型的高效推理框架</title>
<link>https://arxiv.org/abs/2410.08584</link>
<guid>https://arxiv.org/abs/2410.08584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZipVL通过动态重要令牌比例分配策略，提高大规模视觉语言模型的推理效率，减小计算和内存瓶颈。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ZipVL，一个高效的推理框架，旨在解决大规模视觉语言模型（LVLMs）在预填充阶段的计算瓶颈和解码阶段的内存瓶颈。ZipVL采用动态重要令牌比例分配策略，该比例依据层特定的注意力分数分布自适应确定，以优化较低复杂性任务的效率，同时保持高复杂性任务的表现。我们通过标准化注意力分数选择重要令牌，并仅对这些重要令牌执行注意力机制，从而加速预填充阶段。此外，在解码阶段，我们对键值（KV）缓存采用混合精度量化，对重要令牌的缓存使用高位量化，对不太重要的缓存应用低位量化。实验结果显示，ZipVL能够将预填充阶段的速度提升2.6倍，同时将GPU内存使用减少50.0%，在LongVA-7B模型上，在Video-MME基准上仅有0.2%的准确率下降，从而有效增强了LVLMs的生成效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 03:40:12 GMT</pubDate>
</item>
<item>
<title>多模态模型中的幻觉问题研究：挑战与前景</title>
<link>https://arxiv.org/abs/2410.12787</link>
<guid>https://arxiv.org/abs/2410.12787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究系统分析了大规模多模态模型中的幻觉现象，提出评估基准并探讨解决方案。</p><br /><br /><p><strong>摘要：</strong> 近期大规模多模态模型（LMMs）的进展显著提升了在多种任务中的表现，且仍在持续研究如何整合视频和音频等附加模态。然而，现有LMMs仍然易受幻觉影响，即实际的多模态输入与生成的文本输出之间的差异，这限制了它们在各种实际场景中的应用。本研究首次系统性地调查了涉及语言、视觉和音频这三种最常见模态的LMMs中的幻觉现象。我们的研究揭示了两个关键原因：对单一模态先验的过度依赖及模态间虚假的关联。为了解决这些挑战，我们推出了基准The Curse of Multi-Modalities (CMM)，全面评估LMMs中的幻觉，并深入分析其根本问题。研究结果突显出关键脆弱点，包括模态整合的不平衡和来自训练数据的偏见，强调了平衡的跨模态学习和增强幻觉减轻策略的必要性。基于我们的观察和发现，我们提出了可能增强LMMs可靠性的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 02:21:58 GMT</pubDate>
</item>
<item>
<title>HumanEval-V：评估大型多模态模型的视觉理解与编程能力的基准</title>
<link>https://arxiv.org/abs/2410.12381</link>
<guid>https://arxiv.org/abs/2410.12381</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HumanEval-V是一个新基准，评估大型多模态模型的编码与视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在编码任务上的评估成为人工智能进步的重要工具，视觉感知能力的引入使得大型多模态模型（LMMs）变得愈发重要。然而，目前对于LMMs在视觉推理和代码生成方面的评估基准相对缺乏，为此我们提出了HumanEval-V。该基准包含108个精心设计的初级Python编码任务，来源于CodeForces和Stack Overflow，并经过适当调整以保证其独创性。每个任务都包括视觉元素和预定义的函数签名，模型需要基于这些内容生成代码解决方案，并通过手工编写的测试用例进行评估。在对19个先进的LMM进行评测后，我们发现目前LMM在视觉推理和编码能力上存在显著挑战，与商用模型如GPT-4o相比，其通过率仅为13%（pass@1）和36.4%（pass@10），而一些开源模型的通过率更低于4%。这些结果为未来的研究指明了增强LMM能力的关键方向。我们的代码和基准已开源，链接为：https://github.com/HumanEval-V/HumanEval-V-Benchmark。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12381" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:49:58 GMT</pubDate>
</item>
<item>
<title>跨模态时间理解的新模型与数据集研究</title>
<link>https://arxiv.org/abs/2410.12109</link>
<guid>https://arxiv.org/abs/2410.12109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出OCTAV数据集及OMCAT模型，提升音视频跨模态时间理解的能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在文本生成与理解方面取得了显著进展，最近的研究扩展到了整合视觉和音频输入的多模态LLMs。然而，这些模型在细粒度的跨模态时间理解，特别是在音频与视频流之间相关事件的关联上仍然面临挑战。为了解决这些问题，我们提出了两个关键贡献：一个新的数据集OCTAV（Omni Context and Temporal Audio Video），旨在捕捉音视频之间的事件过渡；以及OMCAT（Omni Context Aware Transformer）模型，该模型利用RoTE（Rotary Time Embeddings），对RoPE进行创新扩展，提升了时间锚定任务的时效性和计算效率。通过强大的三阶段训练管道——特征对齐、指令调优和OCTAV特定训练——OMCAT在跨模态时间理解上表现卓越。我们的模型在音频视觉问答（AVQA）任务和OCTAV基准上展现了先进的性能，并通过全面的实验和消融研究验证了其在时间推理和跨模态对齐方面的显著提升。我们的数据集和代码将公开发布，演示页面链接为：https://om-cat.github.io。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:18:30 GMT</pubDate>
</item>
<item>
<title>ProSA：评估大型语言模型提示敏感性的框架</title>
<link>https://arxiv.org/abs/2410.12405</link>
<guid>https://arxiv.org/abs/2410.12405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProSA框架评估大型语言模型在不同提示下的性能敏感性，提出新的度量指标并揭示其内在机制。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在多项任务中展现出令人印象深刻的能力，但其表现对所使用的提示高度敏感。这种可变性给准确评估和用户满意度带来了挑战。目前的研究常常忽视实例级提示变异及其对主观评估的影响。为了解决这些问题，我们提出了ProSA框架，该框架旨在评估和理解LLMs的提示敏感性。ProSA引入了一种新的敏感性度量指标PromptSensiScore，并利用解码置信度来阐明其内在机制。我们的广泛研究涵盖多项任务，揭示提示敏感性在不同数据集和模型间的波动状况，发现较大的模型表现出更强的鲁棒性。此外，我们观察到少量示例能够缓解这一敏感性问题，主观评估也受到提示敏感性的影响，特别是在复杂的推理任务中。值得注意的是，模型的更高置信度与提示鲁棒性之间呈正相关。我们相信这项工作将成为研究LLMs提示敏感性的重要工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:00:15 GMT</pubDate>
</item>
<item>
<title>基于模型亲缘关系的高效语言模型合并策略</title>
<link>https://arxiv.org/abs/2410.12613</link>
<guid>https://arxiv.org/abs/2410.12613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出模型亲缘关系，优化大语言模型合并策略，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了模型亲缘关系（Model Kinship）的概念，旨在衡量大型语言模型（LLMs）之间的相似性和相关性。通过综合实证分析，我们发现模型亲缘关系与模型合并后性能增益之间存在一定的关系，这可以帮助我们在选择候选模型时做出更明智的判断。基于这一发现，我们提出了一种新的合并策略——基于模型亲缘关系的Top-k贪婪合并。这种方法在基准数据集上的表现更佳，并且我们发现利用模型亲缘关系作为标准可以促进持续的模型合并，缓解模型进化过程中的性能退化（局部最优）问题。此外，模型亲缘关系还可以帮助我们摆脱陷阱，实现更有效的模型演化。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:55:42 GMT</pubDate>
</item>
<item>
<title>DocLayout-YOLO：一种高速高准确率的文档布局分析方法</title>
<link>https://arxiv.org/abs/2410.12628</link>
<guid>https://arxiv.org/abs/2410.12628</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种名为DocLayout-YOLO的新方法，旨在提升文档布局分析的速度和准确性。</p><br /><br /><p><strong>摘要：</strong> 在文档理解系统中，文档布局分析至关重要，但面临速度与准确性之间的权衡。针对这一问题，本文提出DocLayout-YOLO，一种通过文档特定优化在预训练和模型设计上提高准确性并保持速度优势的新方法。我们提出Mesh-candidate BestFit算法，将文档合成视为二维装箱问题，从而生成大规模、多样化的DocSynth-300K数据集，实现稳健的文档预训练。使用DocSynth-300K进行预训练显著提升了各种文档类型的微调性能。在模型优化方面，我们提出了Global-to-Local可控感受野模块，能够更好地处理文档元素的多尺度变化。此外，为验证不同文档类型的性能，我们引入了复杂且具挑战性的基准测试DocStructBench。大量实验证明，DocLayout-YOLO在速度和准确性方面均表现优异。代码、数据和模型可在https://github.com/opendatalab/DocLayout-YOLO获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12628" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:44:41 GMT</pubDate>
</item>
<item>
<title>长文本对齐的文本到图像生成模型的优化方法 LongAlign</title>
<link>https://arxiv.org/abs/2410.11817</link>
<guid>https://arxiv.org/abs/2410.11817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LongAlign方法，通过分段编码和偏好优化提高长文本的图像生成对齐效果。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像（T2I）扩散模型的快速发展，它们能够根据给定文本生成前所未有的结果。然而，对于长文本的输入，现有的编码方法（如CLIP）面临限度，导致生成的图像与长文本的对齐变得具有挑战性。为了解决这些问题，本文提出了LongAlign方法，包括一种分段级编码方法，用于处理长文本，以及一种分解的偏好优化方法，以实现有效的对齐训练。在分段级编码中，长文本被拆分为多个段落并单独处理，从而克服了预训练编码模型的最大输入长度限制。在偏好优化方面，我们提供了基于CLIP的分解偏好模型来微调扩散模型。我们深入研究了CLIP-based偏好模型的评分机制，并发现偏好分数可以分解为两个组件：一个与文本相关的部分用于测量T2I对齐程度，另一个与文本无关的部分评估人类偏好的其他视觉方面。通过提出重加权策略，分配这两个组件不同的权重，我们减少了过拟合现象，提高了对齐效果。经过大约20小时的微调，512x512的Stable Diffusion (SD) v1.5显著超越了PixArt-alpha和Kandinsky v2.2等更强大的基础模型。相关代码已发布于https://github.com/luping-liu/LongAlign。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:20:12 GMT</pubDate>
</item>
<item>
<title>限制因素与问题影响：语言智能体规划能力的挑战</title>
<link>https://arxiv.org/abs/2410.12409</link>
<guid>https://arxiv.org/abs/2410.12409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究语言智能体在自主规划中面临的限制因素及其影响，包括约束条件作用有限与问题效应减弱。</p><br /><br /><p><strong>摘要：</strong> 自主规划自人工智能诞生以来一直是一个重要研究领域。早期的规划智能体主要针对特定任务提供精准解决方案，但缺乏通用能力。随着大型语言模型（LLMs）及其强大推理能力的出现，自动生成合适方案的兴趣重新涌现。然而，现有研究表明，目前的语言智能体仍未达到人类级别的规划能力，OpenAI的o1模型在复杂真实世界规划基准上仅获得15.6%的成绩。本文探讨了阻碍语言智能体实现人类级规划的深层次原因，并通过特征归因研究识别了两个关键因素：约束条件的有限作用和问题的减弱影响。虽然目前的解决策略在一定程度上减轻了这些挑战，但并未完全解决，显示出智能体在达到人类级智能方面仍然任重道远。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:09:59 GMT</pubDate>
</item>
<item>
<title>MultiVENT 2.0：多语言事件驱动的视频检索基准</title>
<link>https://arxiv.org/abs/2410.11619</link>
<guid>https://arxiv.org/abs/2410.11619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MultiVENT 2.0是一个大型多语言视频检索基准，涵盖218,000个新闻视频和3,906个事件查询，挑战现有的多模态视频检索系统。</p><br /><br /><p><strong>摘要：</strong> 随着从大规模多模态集合中有效提取和综合信息的需求日益增长，现有的视频检索数据集受到范围限制，主要关注将模糊描述性的查询与小规模、专业编辑的英语视频进行匹配。为了解决这一问题，我们推出了MultiVENT 2.0，这是一个大型多语言事件驱动的视频检索基准，包含超过218,000个新闻视频和3,906个针对特定世界事件的查询。这些查询特别针对视频中的视觉内容、音频、嵌入文本和文本元数据的信息，要求系统充分利用所有这些来源以成功完成任务。初步结果表明，当前最先进的视觉-语言模型在这一任务上面临很大挑战，而其他方法虽然显示出一定的潜力，但仍不足以有效应对这一问题。这些发现突显了建立更强大的多模态检索系统的必要性，有效的视频检索是实现多模态内容理解和生成任务的关键一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 16:08:59 GMT</pubDate>
</item>
<item>
<title>EchoPrime：一种多视角视频基础模型用于全面心脏超声学解读</title>
<link>https://arxiv.org/abs/2410.09704</link>
<guid>https://arxiv.org/abs/2410.09704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EchoPrime通过多视角视频分析，实现精确的心脏超声自动化评估。</p><br /><br /><p><strong>摘要：</strong> Echocardiography是评估心脏结构和功能的最广泛使用的影像学方法，而人工智能（AI）在其应用中具有潜在优势。然而，目前大多数echocardiography AI模型仅限于单一视角和单一任务，未能利用全面检查中多个视角的互补信息，从而限制了其性能和应用范围。为了解决这一问题，我们提出了EchoPrime，这是一种经过训练的多视角、视角信息驱动的视频基础模型，基于1200万个视频-报告对进行训练。EchoPrime使用对比学习建立针对所有标准视角的统一嵌入模型，并具备对常见和罕见疾病的表示。它通过视角分类与视角驱动的解剖注意模型整合视频特定解释，准确映射超声波视角与解剖结构之间的关系。通过增强信息检索，EchoPrime整合全面研究中的所有超声波视频，实现全面的临床解读。在来自两个独立医疗系统的数据集中，EchoPrime在23个心脏形态和功能的多样化基准上达到了最先进的性能，超越了任务特定方法和以前的基础模型。经过严格的临床评估，EchoPrime可以帮助医生自动化地初步评估全面超声心动图。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 13:31:06 GMT</pubDate>
</item>
<item>
<title>NesTools：评估大型语言模型的嵌套工具学习能力的新基准</title>
<link>https://arxiv.org/abs/2410.11805</link>
<guid>https://arxiv.org/abs/2410.11805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出NesTools，评估大型语言模型的嵌套工具学习能力，用于填补相关研究空白。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）与工具学习的结合在现实应用中取得了显著成果，嵌套工具学习的能力成为研究的重点。然而，目前的研究仍然比较薄弱，现有基准缺乏相关的数据实例。为了应对这一问题，本文提出了NesTools，以填补在综合嵌套工具学习评估中的空白。NesTools采用了一种新颖的自动数据生成方法，构建了大规模的嵌套工具调用，涵盖多种嵌套结构。通过人工审查和细化，生成的数据集质量高，且与真实场景密切相关。因此，NesTools可以作为评估LLMs嵌套工具学习能力的新基准。我们在22个LLMs上进行了广泛的实验，并提供了详细的分析，结果显示当前LLMs在复杂的嵌套工具学习任务中的表现仍然存在不足之处。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 12:59:44 GMT</pubDate>
</item>
<item>
<title>Agent-as-a-Judge框架：针对智能体系统的新评估方法</title>
<link>https://arxiv.org/abs/2410.10934</link>
<guid>https://arxiv.org/abs/2410.10934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Agent-as-a-Judge框架，针对智能体系统的评估提供新的思路和方法。</p><br /><br /><p><strong>摘要：</strong> 当今的评估技术对智能体系统的评估显得不足，常常只关注最终结果，而忽略了智能体系统解决问题的逐步过程，或者需要过多的人工劳动。为此，我们提出了Agent-as-a-Judge框架，利用智能体系统相互评估的方式，作为LLM-as-a-Judge框架的自然延伸，结合了能够为整个任务解决过程提供中间反馈的智能体特征。我们将该框架应用于代码生成任务，开发了名为DevAI的新基准，包含55个真实的自动化AI开发任务以及365个分层用户需求的详细手动注释。通过Agent-as-a-Judge基准测试了三种流行的智能体系统，结果显示其表现远超LLM-as-a-Judge，与人类评估基线同样可靠。总体而言，Agent-as-a-Judge为现代智能体系统的发展提供了富有意义和可靠的奖励信号，实现动态和可扩展的自我改进，标志着该领域的一个重要进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10934" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 12:41:48 GMT</pubDate>
</item>
<item>
<title>基于COCO的互动图像抠图数据集与方法研究</title>
<link>https://arxiv.org/abs/2410.06593</link>
<guid>https://arxiv.org/abs/2410.06593</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出COCO-Matting数据集及SEMat方法，以改善交互式自然图像抠图性能。</p><br /><br /><p><strong>摘要：</strong> 近期的研究尝试将强大的交互式分割模型如SAM适应于交互式抠图，并通过合成抠图数据集进行微调。然而，基于合成数据训练的模型在复杂及遮挡场景中表现不佳。为了解决这一挑战，本文提出了一个基于COCO数据集的新抠图数据集COCO-Matting。具体来说，COCO-Matting的构建包括配件融合和从语义分割掩码到抠图标签的转换，选取复杂的真实世界图像并将其语义分割掩码转换为抠图标签。构建的COCO-Matting包含38,251个在复杂自然场景下的人物实例级α遮罩。此外，现有的基于SAM的抠图方法从冻结的SAM中提取中间特征和掩码，仅通过端到端的抠图损失训练一个轻量级的抠图解码器，未能充分挖掘预训练SAM的潜力。因此，本文提出了SEMat，通过改革网络架构和训练目标以提升性能。新提出的特征对齐变压器能够提取细粒度的边缘和透明特征，而抠图对齐解码器则旨在细分抠图特定对象，并将粗糙掩码转换为高精度的抠图。通过在七个多样化数据集上的广泛实验，证明了本方法的卓越性能，验证了其在交互自然图像抠图中的有效性。此外，我们将代码、模型和数据集开源于 https://github.com/XiaRho/SEMat。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06593" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 11:19:09 GMT</pubDate>
</item>
<item>
<title>基于LLMtimesMapReduce框架的长文本处理研究</title>
<link>https://arxiv.org/abs/2410.09342</link>
<guid>https://arxiv.org/abs/2410.09342</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LLMtimesMapReduce框架，通过分块和聚合策略提升长文本处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着长文本处理需求的增加，扩大大型语言模型（LLMs）的上下文窗口成为重要的研究领域。本文提出了一种新颖的训练无关框架——LLMtimesMapReduce，利用分而治之的策略实现全面的文档理解。该框架将整个文档分割为多个部分供大语言模型处理，然后将中间答案聚合以生成最终输出。面对长文本处理中的难点，特别是分割文本可能导致重要的长距离信息丢失的问题，提出了两类干扰信息的分类，包括块间依赖和块间冲突。为此，本文设计了一个结构化信息协议来处理块间依赖，并引入了上下文置信度校准机制以解决块间冲突。实验结果表明，LLMtimesMapReduce在性能上优于多个代表性的开源和商业长上下文LLMs，并适用于多种模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09342" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 10:23:09 GMT</pubDate>
</item>
<item>
<title>互惠增强效应：文本分类中词级与文本级分类的协同关系</title>
<link>https://arxiv.org/abs/2410.09745</link>
<guid>https://arxiv.org/abs/2410.09745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过实验证实互惠增强效应在文本分类中的有效性，展示词级信息对文本级分类的提升作用。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了互惠增强效应（Mutual Reinforcement Effect, MRE），阐述词级与文本级分类在文本分类任务中的协同关系及其相互促进的可能性。以往研究中未能充分证明这一机制，因此我们通过实验证实了MRE理论的有效性。我们在21个MRE混合数据集上进行实验，发现模型的表现受到MRE影响。在实验中，我们对比了不同的模型微调实验，结果确认了MRE的存在。此外，我们将MRE扩展到提示学习中，利用词级信息作为语言化工具来提升模型对文本级分类标签的预测能力。在最后的实验中，F1-score在21个MRE混合数据集中有18个超过了基线值，这进一步验证了词级信息提高了语言模型对整体文本理解的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 09:54:44 GMT</pubDate>
</item>
<item>
<title>SimBa：通过注入简约偏差来提升深度强化学习的网络规模</title>
<link>https://arxiv.org/abs/2410.09754</link>
<guid>https://arxiv.org/abs/2410.09754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SimBa架构通过简约偏差提高深度强化学习的样本效率，表现超过多种深度RL算法。</p><br /><br /><p><strong>摘要：</strong> 近年来，计算机视觉（CV）和自然语言处理（NLP）的进展主要得益于网络参数的扩展，虽然传统理论认为大规模网络容易过拟合。然而，这些大型网络通过集成简约偏差组件来指导模型趋向于简单且可泛化的解决方案。在深度强化学习（RL）领域，针对网络设计与扩展的研究相对较少。因此，我们提出了SimBa，一种通过注入简约偏差来扩展深度RL参数的架构。SimBa包含三个关键组件：（i）观察标准化层，通过运行统计量标准化输入，（ii）残差前馈块，为输入到输出提供线性路径，以及（iii）层归一化，用于控制特征幅度。通过将SimBa集成到各类深度RL算法中，包括离策略、在策略和无监督方法，样本效率得到了显著提高。此外，仅仅将SimBa架构应用于软演员评论家（SAC），即能在DMC、MyoSuite和HumanoidBench等环境中匹敌或超越当前最先进的深度RL方法，展现出SimBa在多样化强化学习算法和环境中的广泛适应性与有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 09:31:13 GMT</pubDate>
</item>
<item>
<title>MoE LLMs在嵌入模型中的应用研究</title>
<link>https://arxiv.org/abs/2410.10814</link>
<guid>https://arxiv.org/abs/2410.10814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明Mixture-of-Experts LLMs在嵌入任务中表现出色，无需进一步微调。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Mixture-of-Experts（MoE）大语言模型（LLMs）作为嵌入模型的潜力。尽管传统的解码器架构限制了LLMs在嵌入任务上的表现，我们的研究显示MoE中的专家路由器可以作为一种即插即用的嵌入模型，能够在多个嵌入任务上取得令人满意的效果，而无需进行进一步的微调。通过对MoE路由权重（RW）和隐藏状态（HS）的深入分析，我们发现RW在处理输入提示时更具鲁棒性，并且更侧重于高层语义。借此分析动机，我们提出了MoEE模型，该模型结合了RW和HS，这一组合在性能上优于单独使用其中任何一个。我们还探索了它们的结合方式及提示策略，发现RW和HS的相似性加权和优于它们连接后的相似性。最后，我们在Massive Text Embedding Benchmark（MTEB）的20个数据集上进行了6项嵌入任务的实验，结果表明MoEE在LLM基础的嵌入任务上实现了显著的性能提升，无需进一步微调。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 07:49:01 GMT</pubDate>
</item>
<item>
<title>扩散模型的高效性综述：理论与实践</title>
<link>https://arxiv.org/abs/2410.11795</link>
<guid>https://arxiv.org/abs/2410.11795</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述扩散模型的设计原则与高效应用，助力模型应用与研究。</p><br /><br /><p><strong>摘要：</strong> 扩散模型作为近年来备受关注的生成模型，展示了在图像合成、视频生成、分子设计、3D场景渲染及多模态生成等多种生成任务中的卓越优势。其成功归功于渐进式设计原则以及高效的架构、训练、推理和部署方法论。然而，目前尚缺乏一项综合深入的综述来总结这些原则和实践，为快速理解与应用扩散模型提供帮助。因此，本文从高效性角度出发，梳理了现有的研究和实践，重点关注架构设计、模型训练、快速推理和可靠部署的深刻原则和高效做法，旨在以读者友好的方式指导进一步理论研究、算法迁移及新场景下的模型应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11795" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 07:46:08 GMT</pubDate>
</item>
<item>
<title>LVD-2M：用于长视频生成的新型长拍视频数据集</title>
<link>https://arxiv.org/abs/2410.10816</link>
<guid>https://arxiv.org/abs/2410.10816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一个新数据集LVD-2M，旨在促进长视频生成模型的研究。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成模型的不断发展，长视频生成的研究受到越来越多的关注。现有的视频生成模型通常依赖于短视频数据集，而获取高质量的长视频数据集则成为这一领域发展的障碍。本文提出了LVD-2M数据集，旨在推动长视频生成研究。该数据集的四个关键特性包括：每个视频至少持续10秒、无场景切割的长拍视频、丰富的动态内容和临时密集的字幕。为了实现这一目标，本文介绍了一种新颖的筛选高质量长拍视频的流程与临时密集字幕生成的分层视频标注管道。通过定义一系列定量评估视频质量的指标，如场景切割、动态程度和语义质量，成功过滤出高质量的长拍视频。在此基础上，创建了包括200万个视频的LVD-2M数据集。每个视频的时长超过10秒，且配有临时密集的字幕。进一步通过微调视频生成模型，验证了LVD-2M在长视频生成中的有效性。希望该工作能为长视频生成的未来研究作出重大贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 06:15:43 GMT</pubDate>
</item>
<item>
<title>RoboDual：协同的通用与专业政策机器人系统</title>
<link>https://arxiv.org/abs/2410.08001</link>
<guid>https://arxiv.org/abs/2410.08001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboDual通过通用与专业政策的结合，提升了多任务处理的效率与精度。</p><br /><br /><p><strong>摘要：</strong> 随着对多功能机器人系统需求的增加，RoboDual应运而生，结合了通用策略和专业策略的优势。通用策略利用大量跨体数据实现广泛适应和高层次推理，而专业策略则针对特定领域数据进行优化，拥有更高的任务精准度和效率。然而，通用策略在推理效率和训练成本上存在不足。RoboDual通过采用基于扩散变压器的专业政策，进行多步动作预测，依赖于视觉-语言-行动（VLA）通用政策所提供的高层任务理解和离散化动作输出。在实际应用中，RoboDual较OpenVLA提高了26.7%的性能，在CALVIN任务上提高了12%。尽管只使用了5%的示范数据，RoboDual依然保持强劲表现，并在实际部署中实现了3.8倍的控制频率提升。代码将公开分享，项目页面可访问：https://opendrivelab.com/RoboDual/。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 04:20:04 GMT</pubDate>
</item>
<item>
<title>What Matters in Transformers? Not All Attention is Needed</title>
<link>https://arxiv.org/abs/2406.15786</link>
<guid>https://arxiv.org/abs/2406.15786</guid>
<content:encoded><![CDATA[
While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\% speedup with only a 2.4\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 03:14:52 GMT</pubDate>
</item>
<item>
<title>动态修正解码方法（DeCo）在多模态大型语言模型中的应用</title>
<link>https://arxiv.org/abs/2410.11779</link>
<guid>https://arxiv.org/abs/2410.11779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种动态修正解码方法（DeCo），有效减少多模态大型语言模型的幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）常常出现幻觉现象，但其背后的原因尚不明确。本文通过实证分析发现，尽管MLLMs在最终输出中错误生成对象，但它们能在前几层识别视觉对象。我们推测这是由于语言模型强烈的知识优先性抑制了视觉信息，从而导致幻觉现象。基于此，我们提出了一种新颖的动态修正解码方法（DeCo），该方法可以自适应地选择合适的前几层，并将知识按比例整合到最终层，以调整输出的logits。DeCo是模型无关的，可以与多种经典解码策略无缝结合，并应用于不同的MLLMs。在广泛使用的基准上，我们评估了DeCo，结果表明其能够显著降低幻觉率，相比基线表现出改善，展示了其减轻幻觉的潜力。代码可在 https://github.com/zjunlp/DeCo 获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:56:29 GMT</pubDate>
</item>
<item>
<title>MTU-Bench：一种多粒度的大语言模型工具使用基准</title>
<link>https://arxiv.org/abs/2410.11710</link>
<guid>https://arxiv.org/abs/2410.11710</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MTU-Bench通过多样化场景和无需GPT或人类评估的基础指标，提升大语言模型的工具使用能力评估。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多粒度工具使用基准，称为MTU-Bench，旨在针对大语言模型（LLMs）的工具使用能力进行评估。现有的工具使用基准存在评估场景不足和高昂的评估成本等局限性。为了克服这些问题，MTU-Bench覆盖了五种工具使用场景，包括单轮单工具、单轮多工具、多轮单工具、多轮多工具以及超出分布的任务。此外，MTU-Bench的所有评估指标均基于预测结果与真实值的比较，无需依赖GPT或人类评估。该基准数据集通过转换现有高质量数据集，模拟了真实世界的工具使用场景。我们还提出了一个指令数据集MTU-Instruct，以增强现有LLMs的工具使用能力。通过实验结果表明，MTU-Bench在评估工具使用能力方面有效，相关的代码和数据将会在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11710" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:14:22 GMT</pubDate>
</item>
<item>
<title>SecCodePLT：全面评估代码生成AI安全风险的平台</title>
<link>https://arxiv.org/abs/2410.11096</link>
<guid>https://arxiv.org/abs/2410.11096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SecCodePLT平台，全面评估代码生成AI的安全风险，包括不安全编码和网络攻击的可行性。</p><br /><br /><p><strong>摘要：</strong> 已有研究确立了多个基准，揭示了代码生成AI（Code GenAI）的安全风险，主要集中在两个方面：模型生成不安全代码的潜力和其在网络攻击中的应用效用。虽然现有基准取得了显著进展，但仍有进一步改进的机会。目前的一些基准主要关注模型提供攻击建议的能力，而忽视了生成可执行攻击的能力。此外，大多数基准过于依赖静态评估指标，而动态指标如通过测试案例的能力可能更为准确。而专家验证的基准虽然提供了高质量数据，但多以小规模运作。为了解决这些问题，我们开发了SecCodePLT，这是一个统一且全面的评估平台，用于检测代码生成AI的风险。在不安全编码方面，我们推出了一种新数据创建方法，将专家与自动生成结合，确保数据质量的同时实现大规模生成。我们还将样本与测试案例关联，以进行代码相关的动态评估。在网络攻击有用性方面，我们设置了真实的评估环境，并构建样本以引导模型生成实际攻击，同时在该环境中应用动态指标。经过广泛实验，我们展示了SecCodePLT在安全相关性上优于现有的SOTA基准CyberSecEval，并能更好地识别SOTA模型面临的不安全编码和网络攻击有用性方面的风险。最后，我们将SecCodePLT应用于SOTA代码代理Cursor，首次识别出该高级编码代理的非平凡安全风险。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:10:40 GMT</pubDate>
</item>
<item>
<title>利用多语言大模型解决低资源语言医疗数据稀缺问题</title>
<link>https://arxiv.org/abs/2410.10626</link>
<guid>https://arxiv.org/abs/2410.10626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出了一种新型的MoE路由方法，以提高多语言医疗模型对低资源语言的适应性。</p><br /><br /><p><strong>摘要：</strong> 在医疗领域，多语言大模型（LLMs）可以打破语言障碍，提高医疗服务的获取率，但数据稀缺问题依然突出，特别是在低资源语言中。为了解决这一挑战，本文首先构建了一个高质量的医疗数据集，并进行了质量分析。接着，我们从多语言视角探索LLMs的内部信息流，采用专家混合（MoE）模块化。技术上，我们提出了一种新型的MoE路由方法，利用语言特定的专家与跨语言路由。受电路理论的启发，我们的路由分析揭示了一种信息流机制：早期层集中跨语言信息流，而后期层则表现出语言特定的分歧。基于此，我们发展了后MoE架构，仅在后期层应用稀疏路由，同时保持其他层的密集性。实验结果表明，该方法提升了多语言模型对其他语言的泛化能力，同时保持了解释性。最后，为了高效扩展模型至50种语言，我们引入了语言家族专家的概念，借助语言学先验，能够在不增加额外参数的情况下扩展语言数量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:01:31 GMT</pubDate>
</item>
<item>
<title>基于空间和角度高斯表示的实时高质量照明与视图合成</title>
<link>https://arxiv.org/abs/2410.11419</link>
<guid>https://arxiv.org/abs/2410.11419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种结合高斯表示和三重溅射过程的技术，实现多视角图像的实时照明与视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于空间和角度高斯表示的技术，以及一种三重溅射过程，旨在实现实时、高质量的新照明和视图合成。为了描述复杂的外观，我们为每个空间高斯引入了一种效果良好的反射函数，即兰伯特反射与角度高斯的混合模型。生成自阴影的过程是通过将所有空间高斯溅射向光源，并计算阴影值，随后利用小型多层感知网络进行精细化处理。此外，为了补偿全局照明等其他效果，我们训练了另一个网络，以计算和添加每个空间高斯对应的RGB元组。本文展示的结果在30个样本上进行了验证，样本涵盖了几何形状（从坚固到松散）和外观（从半透明到各向异性）的广泛变化，并使用了多种输入数据，包括合成/重建对象的渲染图像、手持相机拍摄的照片、以及来自专业灯台的图像。我们在单个普通GPU上的训练时间为40-70分钟，并达到了每秒90帧的渲染速度。与现有技术相比，我们的结果在质量和性能上均表现优异。代码和数据已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 00:26:35 GMT</pubDate>
</item>
<item>
<title>探索去规范化解码器中激活函数的优化</title>
<link>https://arxiv.org/abs/2410.09637</link>
<guid>https://arxiv.org/abs/2410.09637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究发现ReLU在去LayerNorm模型中超越GELU，改善学习动态和信息保留。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了在去LayerNorm的解码器中选择激活函数的问题，发现ReLU在性能上显著优于GELU。传统上，变换器模型偏爱GELU，但我们的实证研究显示ReLU在这类架构中提供了8.2%的困惑度改善。我们分析了GELU在早期层中所产生的熵过载现象，导致注意力头的表现能力未能充分利用。这表明，像GELU这样平滑的激活函数不适合去LayerNorm架构，而ReLU的几何特性则在缺乏LayerNorm的情况下改善学习动态和信息保留。这项研究为优化具有显著挑战的变换器架构提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 20:33:34 GMT</pubDate>
</item>
<item>
<title>大语言模型中的语言结构与内部电路的对应关系研究</title>
<link>https://arxiv.org/abs/2410.09223</link>
<guid>https://arxiv.org/abs/2410.09223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨大语言模型如何处理不同语言的形态句法过程，发现共享电路与语言特异组件的共存。</p><br /><br /><p><strong>摘要：</strong> 本研究通过对大语言模型(LLMs)的机制解释工具，探讨其内部结构与语言的形态句法过程之间的关系。我们主要提出两个问题：第一，当两种语言使用相同的形态句法过程时，LLMs是否使用共享的内部电路处理这些过程？第二，当两种语言采用不同的形态句法过程时，LLMs是否使用不同的内部电路？针对英语和中文的多语言及单语言模型，我们分析了涉及两项任务的内部电路。研究结果表明，模型在处理相同句法过程时，无论是在何种语言中，均使用相同的电路，甚至在完全独立训练的单语言模型中也是如此。此外，我们还发现多语言模型在处理某些特有的语言过程（例如形态标记）时，会运用语言特定的组件（如注意力头和前馈网络）。这些结果为大语言模型在同时建模多种语言时，如何在利用共通结构与保留语言差异之间进行权衡提供了新的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 14:32:40 GMT</pubDate>
</item>
<item>
<title>VisRAG：面向多模态文档的视觉-语言检索增强生成</title>
<link>https://arxiv.org/abs/2410.10594</link>
<guid>https://arxiv.org/abs/2410.10594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了VisRAG，一个基于视觉-语言模型的检索增强生成系统，用于多模态文档的处理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VisRAG，一个解决现有基于文本的检索增强生成（RAG）系统局限的新方法。传统RAG系统无法有效利用多模态文档中的视觉信息，因此我们提出了VisRAG，通过直接使用视觉-语言模型（VLM）处理文档。这一新颖的流程避免了在文本解析过程中可能引起的信息丢失，从而最大限度地保留了原始文档中的信息。我们收集了开放源代码和合成数据来训练VisRAG中的检索器，探索多种生成方法。实验结果显示，VisRAG在检索和生成两个阶段均优于传统的文本基础RAG，整体性能提升幅度达到25-39%。进一步的分析表明，VisRAG在利用训练数据方面表现出色，并展现出强大的泛化能力，成为多模态文档RAG的有前景的解决方案。我们的代码和数据已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 13:58:17 GMT</pubDate>
</item>
<item>
<title>提升大型语言模型推理能力的训练方法</title>
<link>https://arxiv.org/abs/2410.10630</link>
<guid>https://arxiv.org/abs/2410.10630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新的训练方法，使大型语言模型具备推理能力，从而提高指令响应效果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在指令跟随能力的广泛应用，如何使其具备更好的推理能力成为研究的重点。本文提出了一种新的训练方法，通过迭代搜索和优化程序，探索可能的思维生成空间，使现有模型在没有额外人类数据的情况下学习思考能力。对此，每个指令的思维候选方案使用一个判别模型进行评分，并通过偏好优化进行改进。研究显示，这种新方法在AlpacaEval和Arena-Hard等基准测试中表现优越，同时还在非推理类任务（如营销、健康和一般知识）上取得改善。该方法为大型语言模型的训练提供了新的视角，证明了思考能力在多种任务中的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 13:51:05 GMT</pubDate>
</item>
<item>
<title>MMCOMPOSITION：评估大规模视觉-语言模型的组合能力的新基准</title>
<link>https://arxiv.org/abs/2410.09733</link>
<guid>https://arxiv.org/abs/2410.09733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MMCOMPOSITION基准，以评估视觉-语言模型的组合能力，发现GPT-4o在此方面表现不佳。</p><br /><br /><p><strong>摘要：</strong> 随着大规模视觉-语言模型（VLM）的出现，跨模态理解取得了显著进步，促进了图像和视频标注、视觉问答和跨模态检索等任务的精确集成。尽管VLM具有优越的能力，研究者对其组合能力的理解仍然不够全面。现有基准仅从对象、关系和属性的粗略角度评估组合性，忽视了对对象交互、计数和复杂构图的深层次推理。然而，组合能力是促进VLM跨模态推理和理解的关键能力。为此，我们提出了MMCOMPOSITION，一个新的人类标注基准，旨在全面和准确地评估VLM的组合能力。通过MMCOMPOSITION，我们能够量化和探索主流VLM的组合能力。令人惊讶的是，实验发现GPT-4o的组合能力低于最佳开源模型，同时分析了其背后的原因。我们的实验分析揭示了VLM在细粒度组合感知和推理方面的局限性，并指出了VLM设计和训练的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 12:32:30 GMT</pubDate>
</item>
<item>
<title>LiveXiv：基于科学ArXiv论文的可扩展实时基准测试</title>
<link>https://arxiv.org/abs/2410.10783</link>
<guid>https://arxiv.org/abs/2410.10783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveXiv是一个自动生成科学论文视觉问答对的基准测试，评估多模态模型的真实能力。</p><br /><br /><p><strong>摘要：</strong> LiveXiv是一个新的基准测试，旨在通过科学ArXiv论文评估多模态模型的能力。它通过实时访问领域特定的手稿，在没有人工干预的情况下，自动生成视觉问答对（VQA）。提议的方案利用手稿中的图表和数据向量，采用高效的评估方法，使用有限数量模型的结果来估计所有模型的表现，从而降低整体评估费用。该基准的首个版本对多种开放和私有的大型多模态模型（LMMs）进行了基准测试，展现了独特的挑战性并揭示了模型的真实能力，避免了测试数据污染。为了保证高质量，研究团队还收集并评估了经过人工验证的子集，与自动注释的结果进行对比，发现性能差异小于2.5%。该数据集已在HuggingFace上公开，并且代码也将提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10783" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 04:40:56 GMT</pubDate>
</item>
<item>
<title>问题树（ToP）：解决复杂推理任务的新方法</title>
<link>https://arxiv.org/abs/2410.06634</link>
<guid>https://arxiv.org/abs/2410.06634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出问题树（ToP），在复杂任务中表现优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本研究提出一种新方法——问题树（Tree of Problems，ToP），旨在处理可分解为相同子任务的复杂问题。尽管大语言模型（LLMs）在许多任务中表现卓越，尤其是在上下文学习方面，链式思考（Chain-of-Thought，CoT）提示在复杂推理中也取得了良好结果，但仍存在一些挑战。为了解决这些困难，树状思维（Tree of Thoughts，ToT）和图形思维（Graph of Thoughts，GoT）提出了将复杂问题划分为子问题的思路。我们假设，问题树作为ToT的简化版本，能够更有效地处理可以划分为相同子任务的复杂任务。我们的实证结果显示，ToP在多个复杂推理任务上超越了ToT和GoT的表现，同时也优于CoT。研究所用的所有代码已公开，方便其他研究者使用和验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 04:34:39 GMT</pubDate>
</item>
<item>
<title>基于动态最优控制的矩形流模型图像反演与编辑</title>
<link>https://arxiv.org/abs/2410.10792</link>
<guid>https://arxiv.org/abs/2410.10792</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新方法，通过动态最优控制进行矩形流模型的图像反演和编辑。</p><br /><br /><p><strong>摘要：</strong> 本文针对生成模型中的两个关键任务：图像反演与编辑，提出了一种基于矩形流（Rectified Flows, RFs）的框架。尽管扩散模型在图像生成领域取得了显著成功，但其反演过程中由于漂移和扩散的非线性特性导致了忠实性和可编辑性问题。现有的扩散模型反演方法往往需要额外参数的训练或测试时对潜变量的优化，这在实际操作中成本高昂。矩形流提供了一种有前景的替代方案，但其反演方法尚未得到充分研究。我们提出的一种RF反演方法，基于动态最优控制，通过线性二次调节器导出。我们证明了所得到的向量场等价于一个矩形的随机微分方程。此外，我们将该框架扩展为Flux的随机采样器。我们的反演方法在零-shot反演和编辑任务中表现出色，在笔触到图像合成和语义图像编辑方面优于现有方法，且大规模人类评估结果验证了其用户偏好的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10792" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 03:10:41 GMT</pubDate>
</item>
<item>
<title>长时记忆评估框架：提升聊天助手的记忆能力</title>
<link>https://arxiv.org/abs/2410.10813</link>
<guid>https://arxiv.org/abs/2410.10813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LongMemEval基准，评估聊天助手的长时记忆能力，探索优化设计以提升记忆召回与问答表现。</p><br /><br /><p><strong>摘要：</strong> 近期的大型语言模型（LLM）驱动的聊天助手系统已集成内存组件，以跟踪用户与助手之间的对话历史，从而实现更准确和个性化的响应。然而，它们在持续交互中的长期记忆能力仍未得到充分探索。本文介绍了LongMemEval，这是一个综合性基准，旨在评估聊天助手五种核心的长期记忆能力：信息提取、多会话推理、时间推理、知识更新和弃权。LongMemEval包含500个精心策划的问题，嵌入可自由扩展的用户-助手对话历史中，给现有的长期记忆系统带来了重大挑战。实验结果显示，商业聊天助手和长上下文LLM在持续交互中的信息记忆准确率下降了30%。我们提出一个统一框架，将长期记忆设计分解为四个设计选择，涵盖索引、检索和读写阶段。基于关键实验洞察，提出几种内存设计方案，包括会话分解以优化值粒度、事实增强键扩展以增强索引结构，以及时间感知查询扩展以改进搜索范围。实验结果表明，这些优化显著提高了在LongMemEval上的记忆召回率和后续问答表现。总体而言，本研究为推动基于LLM的聊天助手长期记忆能力提供了宝贵的资源和指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 03:01:29 GMT</pubDate>
</item>
<item>
<title>TemporalBench：评估视频中的细粒度时间理解的新基准</title>
<link>https://arxiv.org/abs/2410.10818</link>
<guid>https://arxiv.org/abs/2410.10818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了TemporalBench，一个新的基准，用于评估视频的细粒度时间理解能力。</p><br /><br /><p><strong>摘要：</strong> 理解细粒度时间动态对于多模态视频的理解和生成至关重要。由于缺乏细粒度时间标注，现有视频基准多类似静态图像基准，无法有效评估模型的时间理解能力。为此，本文介绍了TemporalBench，一个专为评估视频的细粒度时间理解而设计的新基准。TemporalBench包含约10,000对视频问答对，源于约2,000个高质量的人类标注，这些标注详细描述了视频片段中的时间动态。该基准提供了一个独特的测试平台，以评估各种时间理解和推理能力，如动作频率、运动幅度、事件顺序等，支持视频问答、视频字幕生成、短视频和长视频理解等多项任务，以及多种模型，如多模态视频嵌入模型和文本生成模型。结果显示，诸如GPT-4o等最先进模型在TemporalBench上的问答准确率仅为38.5%，表明人类与AI在时间理解上存在显著差距（约30%）。此外，我们注意到多选QA的一个关键缺陷，即大型语言模型（LLMs）能够检测负面描述中的微妙变化，并将中心化描述作为预测线索，因此我们提出了多个二元准确度（MBA）来纠正这种偏差。我们希望TemporalBench能够促进研究，提高模型的时间推理能力。数据集和评估代码将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 02:16:22 GMT</pubDate>
</item>
<item>
<title>自主操作的改进型3D扩散政策（iDP3）在多样化环境下的应用</title>
<link>https://arxiv.org/abs/2410.10803</link>
<guid>https://arxiv.org/abs/2410.10803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种改进型3D扩散政策（iDP3），使人形机器人在多样环境中自主执行技能。</p><br /><br /><p><strong>摘要：</strong> 自主操作的人形机器人一直是机器人研究者追求的目标。然而，由于获取可泛化技能的难度，自主操作的人形机器人往往只能在特定的场景中进行有效操作。近年来，3D视动政策（如3D Diffusion Policy, DP3）的进展为拓展这些能力到多样化环境提供了希望。然而，3D视动政策通常依赖于相机标定和点云分割，这对移动机器人（如人形机器人）的应用造成了挑战。为了解决这些问题，本文提出了一种新的3D视动政策——改进型3D扩散政策（iDP3），它利用自我中心的3D视觉表示，消除了上述限制。我们展示了iDP3能够使全尺寸人形机器人在多样的真实场景中自主执行技能，并使用仅在实验室收集的数据进行训练。相关视频可以在: https://humanoid-manipulation.github.io 获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 02:14:57 GMT</pubDate>
</item>
<item>
<title>Cavia：一种可控相机的多视角视频生成框架</title>
<link>https://arxiv.org/abs/2410.10774</link>
<guid>https://arxiv.org/abs/2410.10774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cavia框架实现了从输入图像生成多个时空一致的视频，支持精确控制相机运动与物体运动。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像到视频生成领域取得了显著突破，但生成帧在3D一致性和相机可控性方面仍存在未解决的问题。尽管有研究尝试将相机控制融入生成过程中，但结果往往仅限于简单轨迹，或缺乏对同一场景的多条独特相机路径生成一致视频的能力。为了解决这些限制，我们提出了Cavia，这是一种新颖的相机可控、多视角视频生成框架，能够将输入图像转换为多个时空一致的视频。我们的框架将空间和时间注意力模块扩展为视角集成注意力模块，从而提高视角和时间一致性。这种灵活的设计允许与多种策划数据源进行联合训练，包括场景级静态视频、对象级合成多视角动态视频以及现实世界单目动态视频。到目前为止，Cavia是首个允许用户精确指定相机运动的同时获得物体运动的系统。大量实验表明，Cavia在几何一致性和感知质量上优于最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:58:29 GMT</pubDate>
</item>
<item>
<title>Animate-X：针对各种角色类型的通用动画框架</title>
<link>https://arxiv.org/abs/2410.10306</link>
<guid>https://arxiv.org/abs/2410.10306</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Animate-X通用动画框架，以提高对人形和拟人角色的动画效果。</p><br /><br /><p><strong>摘要：</strong> 随着过去几年动画技术的进步，通过参考图像和目标姿势序列生成高质量视频的角色图像动画取得了显著进展。然而，现有大部分方法仅适用于人类形象，难以在游戏和娱乐行业使用的拟人角色上进行有效泛化。深入分析认为，这一局限性源于对运动建模的不足，导致无法理解驱动视频的动作模式，从而将姿势序列僵硬地施加于目标角色。为此，本文提出了Animate-X，一个基于LDM的通用动画框架，能够处理各种角色类型（统称为X），包括拟人角色。为增强运动表现，我们引入了Pose Indicator，旨在通过隐式与显式方式捕捉驱动视频的综合运动模式。隐式方式利用CLIP视觉特征提取运动的整体模式和动作间的时间关系，而显式方式则通过预先模拟推理过程中可能出现的输入来增强LDM的泛化能力。此外，我们还提出了一个新的动画拟人基准（A^2Bench）来评估Animate-X在通用动画图像上的性能。广泛实验结果验证了Animate-X相对于现有最先进方法的优越性和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10306" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:34:40 GMT</pubDate>
</item>
<item>
<title>MEGA-Bench：一种大规模多模态评估套件</title>
<link>https://arxiv.org/abs/2410.10563</link>
<guid>https://arxiv.org/abs/2410.10563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEGA-Bench 是一项涵盖 505 个现实任务的多模态评估套件， 提供多样化的输出格式和细致的模型评估。</p><br /><br /><p><strong>摘要：</strong> MEGA-Bench 是一个新建立的评估套件，旨在将多模态评估扩展到超过 500 个现实世界任务，以应对用户日常使用中的高度异质性需求。我们的目标是优化高质量数据样本的收集，涵盖多样且丰富的多模态任务，同时实现成本有效且准确的模型评估。我们从 16 位专家注释员处收集了 505 个现实任务，共计超过 8000 个样本，以广泛覆盖多模态任务空间。不同于 MMMU、MMBench 和 MMT-Bench 通过标准多项选择题统一这些问题，MEGA-Bench 接纳了多种输出格式，包括数字、短语、代码、LaTeX、坐标、JSON 和自由格式等。为适应这些格式，我们开发了超过 40 种指标来评估这些任务。MEGA-Bench 还提供跨多个维度（如应用、输入类型、输出格式和技能）进行细致的能力报告，使用户能够深入交互和可视化模型能力。我们在 MEGA-Bench 上评估了多种前沿的视觉语言模型，以了解它们在这些维度上的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:21:36 GMT</pubDate>
</item>
<item>
<title>TVBench: Redesigning Video-Language Evaluation</title>
<link>https://arxiv.org/abs/2410.07752</link>
<guid>https://arxiv.org/abs/2410.07752</guid>
<content:encoded><![CDATA[
Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:06:10 GMT</pubDate>
</item>
<item>
<title>大规模数据选择在监督微调中的关键性研究</title>
<link>https://arxiv.org/abs/2410.09335</link>
<guid>https://arxiv.org/abs/2410.09335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大量数据选择在监督微调中的重要性，强调多样性优于单纯的高质量数据。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了在大规模数据集中进行监督微调（SFT）时，如何选择代表性的训练数据以提高模型性能。我们在200万样本的数据集中复现了多种自评分方法，发现这些方法普遍无法显著优于随机选择。特别是在 SFT 过程中，数据选择的多样性显得尤为重要，而不仅仅是关注数据的高质量。此外，我们还探讨了当前方法的局限性，解释了它们在大规模数据集中的低效表现。经过分析，我们发现通过令牌长度对数据进行过滤是一种稳定且高效的改进方法，尤其对相对较弱的基础模型（如 Llama3）来说，训练长文本数据时表现出显著的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:50:02 GMT</pubDate>
</item>
<item>
<title>LOKI: 评估大型多模态模型Synthetic Data检测能力的新基准</title>
<link>https://arxiv.org/abs/2410.09732</link>
<guid>https://arxiv.org/abs/2410.09732</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOKI是一个新颖的基准，评估大型多模态模型在合成数据检测中的能力和局限性。</p><br /><br /><p><strong>摘要：</strong> 随着AI生成内容的快速发展，互联网将面临大量合成数据的挑战，真实与合成内容的区分变得愈发困难。因此，合成数据检测受到了广泛关注，尤其是大型多模态模型（LMMs）在此任务中的表现引起了极大的兴趣。LMMs能够为其真实性判断提供自然语言解释，从而增强合成内容检测的可解释性。为此，我们提出了LOKI这一新基准，旨在评估LMMs在多种模态中检测合成数据的能力。LOKI涵盖视频、图像、3D、文本和音频等多种模态，包含18K个细致策划的问题，涵盖26个子类别并设有明确的难度等级。该基准不仅包括粗粒度判断和多项选择题，还包含细粒度的异常选择与解释任务，从而对LMMs进行全面分析。我们在LOKI上对22个开源LMM和6个闭源模型进行了评估，突显了它们作为合成数据检测器的潜力，同时也揭示了LMM能力开发中的一些局限性。更多信息请访问 https://opendatalab.github.io/LOKI/ 。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09732" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:49:53 GMT</pubDate>
</item>
<item>
<title>VIF-RAG：提高检索增强生成系统指令跟随对齐的自动化框架</title>
<link>https://arxiv.org/abs/2410.09584</link>
<guid>https://arxiv.org/abs/2410.09584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VIF-RAG 提出了一个用于提升 RAG 系统指令跟随对齐的新方法，包含自动化、可扩展的合成管道和 FollowRAG 基准。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型（LLMs）的发展，检索增强生成（RAG）系统在多种应用中表现出色，但指令跟随（IF）对齐的研究尚不充分。为了解决这个问题，我们提出了 VIF-RAG，这是首个自动化、可扩展且可验证的合成管道，旨在提升 RAG 系统中的指令跟随对齐。该方法首先手动构建一组较小的原子指令，并发展组合规则以合成和验证复杂指令。随后，我们利用监督模型进行指令重写，同时生成代码以自动化验证指令质量。最后，我们整合指令和大量 RAG 数据样本，自动化生成超过 10 万条高质量的 VIF-RAG-QA 数据集。此外，为了填补 RAG 系统指令跟随自动评估的空白，我们推出了 FollowRAG 基准，包括约 3000 个测试样本，涵盖 22 类一般指令约束和四个知识密集型 QA 数据集。我们展示了 VIF-RAG 在多个通用指令约束下显著提升 LLM 性能，并就实现 RAG 系统中的 IF 对齐提供了实用见解。我们的代码和数据集已公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:40:06 GMT</pubDate>
</item>
<item>
<title>MMIE：评估大型视觉语言模型的交错多模态理解与生成的基准</title>
<link>https://arxiv.org/abs/2410.10139</link>
<guid>https://arxiv.org/abs/2410.10139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MMIE，一个大型知识密集型基准，用于评估大型视觉语言模型的交错多模态理解与生成能力。</p><br /><br /><p><strong>摘要：</strong> 在交错多模态理解与生成（Interleaved Multimodal Comprehension and Generation）的研究领域，尽管已有显著进展，但评估这一能力仍显不足。现有基准在数据规模、范围和评估深度上存在局限，评估指标往往成本高、偏见明显，缺乏实用性的可靠性。为了解决这些挑战，我们提出MMIE，这是一套大型知识密集型基准，旨在评估大型视觉语言模型（LVLMs）在交错多模态理解与生成方面的能力。MMIE包含20K精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域，包括数学、编程、物理、文学、健康和艺术等。它支持交错的输入与输出，提供多项选择和开放式问题格式，以评估不同的能力。此外，我们提出了一种可靠的自动评估指标，利用经过人类标注数据和系统评估标准微调的评分模型，旨在减少偏见，提高评估准确性。大量实验展示了我们的基准和指标在全面评估交错LVLMs方面的有效性。具体而言，我们评估了八个LVLMs，结果表明，甚至是最好的模型也还有显著改进的空间，多数仅取得中等结果。我们相信MMIE将推动交错LVLMs的发展。我们将公开发布我们的基准和代码，网址为https://mmie-bench.github.io/。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:30:53 GMT</pubDate>
</item>
<item>
<title>针对大型语言模型的数学推理能力的奥林匹亚级基准测试</title>
<link>https://arxiv.org/abs/2410.07985</link>
<guid>https://arxiv.org/abs/2410.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个专门评估大型语言模型奥林匹亚级数学推理能力的基准，包含4428个高难度数学问题。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在数学推理能力方面取得显著突破，现有基准如GSM8K和MATH在模型解决这些问题时精确度已达到94.8%，这表明这些基准已无法有效挑战模型。为此，我们提出了一个全面且复杂的基准，专门设计用于评估LLMs在奥林匹亚级数学推理方面的能力。与现有的奥林匹亚相关基准不同，我们的数据集专注于数学，包含4428个经过严格人类标注的竞赛级问题。这些问题被细致地分类为33个子领域，并跨越10个不同的难度等级，使得我们能够全面评估模型在奥林匹亚数学推理表现方面的能力。此外，我们还基于这一基准进行了深入分析。实验结果显示，即使是最先进的模型，如OpenAI o1-mini和OpenAI o1-preview，在处理高度挑战性的奥林匹亚级问题时仍面临显著困难，其准确率分别为60.54%和52.55%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:06:51 GMT</pubDate>
</item>
<item>
<title>基于计划去噪的离散扩散框架DDPD</title>
<link>https://arxiv.org/abs/2410.06264</link>
<guid>https://arxiv.org/abs/2410.06264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了DDPD框架，通过规划与去噪结合，实现了更高效的生成过程。</p><br /><br /><p><strong>摘要：</strong> 离散扩散方法在标准基准上取得了最佳性能，接近或超越了自回归模型。本文介绍了一种新颖的离散扩散框架——计划去噪的离散扩散（DDPD），它将生成过程分为两个模型：规划者和去噪器。在推理时，规划者通过识别最需要去噪的位置（包括初始损坏位置和需要进一步细化的位置）来选择下一个去噪位置。这种计划与去噪的方法使得在生成过程中能够更高效地重建，通过迭代识别和以最佳顺序去噪来处理损坏。DDPD在语言建模基准上表现优于传统的仅有去噪器的掩蔽扩散方法，在text8、OpenWebText和ImageNet 256x256的基于令牌的生成任务中取得了优越结果。值得注意的是，在语言建模中，DDPD显著缩小了扩散和自回归方法在生成困惑度上的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 20:35:31 GMT</pubDate>
</item>
<item>
<title>Synth-SONAR：基于扩散模型与GPT提示的声纳图像合成框架</title>
<link>https://arxiv.org/abs/2410.08612</link>
<guid>https://arxiv.org/abs/2410.08612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Synth-SONAR框架，利用扩散模型和GPT提示生成高质量声纳图像，提升数据多样性和现实感。</p><br /><br /><p><strong>摘要：</strong> 声纳图像合成对水下探索、海洋生物学和国防等应用至关重要。然而，传统方法往往依赖昂贵且大量的数据采集，影响数据质量和多样性。为了解决这些问题，本研究提出了一种新的声纳图像合成框架，Synth-SONAR，结合了扩散模型和GPT提示。Synth-SONAR的创新点主要体现在三个方面：首先，利用基于生成的AI风格注入技术和公开可用的真实/模拟数据，生成最大规模的声纳数据集。其次，构建了一个双重文本条件的声纳扩散模型层次，可以合成高质量和多样性的粗略与精细声纳图像。第三，采用高级（粗略）和低级（详细）文本基础的声纳生成方法，有效利用视觉语言模型（VLMs）和GPT提示中的高级语义信息。在推理过程中，该方法从文本提示中生成多样化且真实的声纳图像，首次将GPT提示应用于声纳图像合成。Synth-SONAR在生产高质量合成声纳数据集方面实现了最新的领先成绩，显著提升了数据的多样性和真实感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 18:14:02 GMT</pubDate>
</item>
<item>
<title>GenARM：一种有效的自回归奖励模型用于无重训练的大型语言模型对齐</title>
<link>https://arxiv.org/abs/2410.08193</link>
<guid>https://arxiv.org/abs/2410.08193</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenARM通过自回归奖励模型实现有效的无训练对齐，支持多目标和弱到强的指导。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）展现了令人印象深刻的能力，但需要与人类偏好进行谨慎对齐。传统的训练时方法使用人类偏好数据集对LLMs进行微调，成本高昂且难以处理多样化用户偏好。而测试时对齐方法则通过奖励模型（RMs）指导冻结的LLMs，避免了重新培训的需求。目前的测试时方法依赖于轨迹级奖励模型，旨在评估完整响应，这在需要从部分响应计算下一个标记奖励的自回归文本生成中显得不够合适。为此，我们提出了GenARM，这一测试时对齐方法利用自回归奖励模型——一种新型的奖励参数化设计，能够预测下一个标记的奖励，从而促进高效且有效的自回归生成。我们从理论上证明，这种参数化可以在KL正则化的强化学习框架内可证明地引导冻结LLMs朝向由传统RMs能够实现的任何分布。实验结果表明，GenARM显著优于现有的测试时对齐基准，并且与训练时方法的性能相匹配。此外，GenARM支持高效的弱到强指导，使得较大的LLMs能够与较小的RMs进行对齐，而无需承担训练大模型的高成本。同时，GenARM还支持多目标对齐，实时调整偏好维度之间的权衡，满足多样化用户偏好的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08193" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 17:04:55 GMT</pubDate>
</item>
<item>
<title>利用简单分层方法提高大型语言模型生成的多样性</title>
<link>https://arxiv.org/abs/2410.09038</link>
<guid>https://arxiv.org/abs/2410.09038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SimpleStrat方法，利用语言模型对输出进行分层，从而提高生成内容的多样性和质量。</p><br /><br /><p><strong>摘要：</strong> 在生成多样化响应方面，大型语言模型（LLMs）扮演着重要角色，尤其在规划搜索和合成数据生成等应用中。传统方法依赖于提高温度来增强多样性，然而，研究表明这不仅导致个体生成质量下降，而且其效果依赖于模型预测概率与真实答案分布的相似性。为了克服这一问题，本文提出了一种新的方法——SimpleStrat，通过语言模型自身对生成空间进行分层，在推理时随机选择一个层级并从中抽取样本。此外，为了衡量生成结果的多样性，本文引入了CoverageQA数据集，该数据集由多个同样合理的答案组成的模糊问题构成，利用Kullback-Leibler散度（KL散度）来评估输出分布与有效答案的均匀分布之间的差异。在对比评估中，使用SimpleStrat方法在召回率方面比GPT-4o提高了0.05，而相较于Llama 3则在KL散度上减少了0.36，展现了该方法在提高生成多样性和质量上的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 16:35:17 GMT</pubDate>
</item>
<item>
<title>MiRAGeNews数据集：对抗AI生成假新闻的多模态检测</title>
<link>https://arxiv.org/abs/2410.09045</link>
<guid>https://arxiv.org/abs/2410.09045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究推出MiRAGeNews数据集，旨在检测AI生成的假新闻，提高内容真实性。使用此数据集训练的多模态检测器效果显著。</p><br /><br /><p><strong>摘要：</strong> 随着虚假新闻内容的泛滥和AI生成图像技术的迅速发展，AI生成的假新闻内容变得更加危险。为此，我们提出了MiRAGeNews数据集，该数据集包含12,500对高质量的真实和AI生成的图像-标题配对，使用了最先进的生成模型。我们的研究表明，这一数据集对人类的检测能力构成了显著挑战（F-1为60%），同时对当前最先进的多模态大语言模型的检测性能也很低（F-1小于24%）。基于该数据集，我们训练了一种多模态检测器（MiRAGe），在来自域外图像生成器和新闻发布者的图像-标题配对任务上，F-1提高了5.1%，超越了现有的基线方法。我们将代码和数据公开发布，以助力后续对AI生成内容的检测研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 13:41:24 GMT</pubDate>
</item>
<item>
<title>I-Max框架：提升文本到图像RFTs的分辨率潜力</title>
<link>https://arxiv.org/abs/2410.07536</link>
<guid>https://arxiv.org/abs/2410.07536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍I-Max框架，利用新的Projected Flow策略和推理工具，提升RFTs在分辨率扩展中的稳定性和图像细节质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Rectified Flow Transformers（RFTs）在扩展生成分辨率方面的挑战，尤其是数据质量和训练成本的问题。针对现有的无调优分辨率外推方法，往往在生成稳定性方面存在不足，我们提出了I-Max框架。I-Max框架包括两大特性：首先是最新的Projected Flow策略，旨在实现更稳定的分辨率外推；其次是一个先进的推理工具包，可帮助模型知识向更高分辨率的泛化。通过在Lumina-Next-2K和Flux.1-dev数据集上的实验，结果表明I-Max能够有效提升分辨率外推的稳定性，并带来图像细节的增强和伪影的修正，证实了无调优分辨率外推的实际应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 12:22:47 GMT</pubDate>
</item>
<item>
<title>ZeroComp：一种有效的零样本3D物体合成方法</title>
<link>https://arxiv.org/abs/2410.08168</link>
<guid>https://arxiv.org/abs/2410.08168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroComp通过使用ControlNet和Stable Diffusion实现无配对图像的3D物体合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ZeroComp的有效零样本3D物体合成方法，该方法在训练过程中不需要配对的复合场景图像。ZeroComp利用ControlNet对内在图像进行条件处理，并结合Stable Diffusion模型以利用其场景先验，这两者共同运作为一个高效的渲染引擎。在训练阶段，ZeroComp使用基于几何、反射率和遮蔽的内在图像，完全不依赖于含有复合物体的场景的图像配对。训练完成后，ZeroComp能够无缝地将虚拟3D物体集成到场景中，并调整光照，使复合效果更加真实。为了验证ZeroComp的效果，我们开发了一个高质量评估数据集，结果显示其在定量和人类感知基准测试中均优于依赖显式照明估计和生成技术的方法。此外，ZeroComp在真实和户外图像合成上也展现出潜力，即使其训练数据仅限于合成的室内数据，依然能够有效实现图像合成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 11:29:03 GMT</pubDate>
</item>
<item>
<title>Mentor-KD：多步推理能力的知识蒸馏方法</title>
<link>https://arxiv.org/abs/2410.09037</link>
<guid>https://arxiv.org/abs/2410.09037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Mentor-KD方法，通过中介模型增强CoT标注与软标签提供，以有效蒸馏LLMs的推理能力。 </p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）在复杂任务中表现出的卓越性能，Chain-of-Thought（CoT）提示法已成为主要研究方向。近年来，有研究提出了一种推理蒸馏（Reasoning Distillation）的方法，通过对LLM教师生成的多步推理进行微调，以转移其推理能力。然而，现有方法在蒸馏集中存在两大挑战：1）数据质量不足，2）软标签提供不充分。为了解决这些问题，本文提出了Mentor-KD方法，旨在有效蒸馏LLMs的多步推理能力至小型语言模型。具体而言，我们利用一个中介模型，进行特定任务的微调，以增强CoT注释的数量和质量，并在推理蒸馏过程中为学生模型提供软标签。通过广泛的实验，我们验证了Mentor-KD在各种模型和复杂推理任务中的有效性，显示出该方法在提升推理能力方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 09:26:42 GMT</pubDate>
</item>
<item>
<title>SAE Match：基于稀疏自编码器的神经网络层间特征对齐</title>
<link>https://arxiv.org/abs/2410.07656</link>
<guid>https://arxiv.org/abs/2410.07656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SAE Match，通过折叠参数最小化均方误差，实现神经网络层间特征的有效对齐。</p><br /><br /><p><strong>摘要：</strong> 理解深度神经网络中特征在不同层之间的演变是机械可解释性中的一个基本挑战，尤其是由于多义性和特征叠加使得这一过程更加复杂。尽管稀疏自编码器（SAEs）已被用来从单个层中提取可解释特征，但不同层之间的特征对齐仍然是一个开放的问题。在本文中，我们提出了一种新颖的数据无关的方法SAE Match，用于对齐神经网络不同层的SAE特征。我们的方法通过最小化SAEs的折叠参数之间的均方误差来匹配特征，这一技术在编码器和解码器权重中引入激活阈值，以解决特征尺度差异的问题。通过对Gemma 2语言模型的广泛实验，我们证明了我们的方法能够有效捕捉特征在不同层间的演变，从而提高特征匹配质量。我们还展示了特征在多个层中持续存在，并且我们的方法能够近似不同层之间的隐藏状态。我们的工作推动了对神经网络中特征动态的理解，并为机械可解释性研究提供了一种新的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 08:07:48 GMT</pubDate>
</item>
<item>
<title>DA-Code：针对代理的数据科学任务的代码生成基准</title>
<link>https://arxiv.org/abs/2410.07331</link>
<guid>https://arxiv.org/abs/2410.07331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DA-Code是一个评估LLMs在数据科学任务上的代码生成能力的基准，涵盖复杂数据处理的实际任务。</p><br /><br /><p><strong>摘要：</strong> 我们介绍了DA-Code，一个专门设计用来评估LLMs在代理数据科学任务中的代码生成能力的基准。DA-Code的设计包含三个核心要素。首先，任务具有内在挑战性，要求高级编码技能，特别是在上下文处理和计划方面。其次，DA-Code中的所有示例均基于真实且多样的数据，涵盖各种复杂的数据整理和分析任务。最后，为了完成这些任务，模型必须使用复杂的数据科学编程语言，以进行精细化的数据处理并得出结论。我们在一个可控和可执行的环境中搭建了这个基准，确保它与现实世界的数据分析场景相一致，并具备可扩展性。评估套件经过注释者的精心设计，以确保评估的准确性和稳健性。我们开发了DA-Agent基准线并进行了实验，结果表明，尽管基线在现有框架中表现更好，但当前最佳的LLMs准确率仅为30.5%，仍大有提升空间。我们将在https://da-code-bench.github.io发布我们的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 07:27:37 GMT</pubDate>
</item>
<item>
<title>多智能体协作数据选择机制在大型语言模型预训练中的应用</title>
<link>https://arxiv.org/abs/2410.08102</link>
<guid>https://arxiv.org/abs/2410.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多智能体协作框架，以提高大型语言模型预训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）预训练的有效数据选择至关重要。虽然已有多种方法致力于提升数据效率，但很少有研究关注这些方法之间的固有冲突，以实现最优数据选择。为了解决这一问题，我们提出了一种新颖的多智能体协作数据选择机制。在该框架中，每种数据选择方法作为独立代理工作，设计了一个代理控制台，以动态整合所有代理在整个LLM训练过程中的信息。我们通过广泛的实证研究评估了我们的多智能体框架。实验结果表明，我们的方法显著提高了数据效率，加速了LLM训练的收敛，并在多个语言模型基准上实现了平均10.5%的性能提升，相比于最先进的方法。有了这样的机制，数据选择的冲突问题得以更好地解决，进一步推动了大型语言模型的研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:56:36 GMT</pubDate>
</item>
<item>
<title>StructRAG：基于结构化知识增强大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2410.08815</link>
<guid>https://arxiv.org/abs/2410.08815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出StructRAG框架，通过结构化知识增强LLMs在知识密集任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval-augmented generation (RAG) 是一种增强大型语言模型 (LLMs) 在知识密集任务中表现的关键方法，但现有RAG方法在处理知识密集推理任务时面临挑战。这是因为相关信息往往散布不均，使得现有方法难以准确识别关键信息并进行全局推理。为了解决这个问题，本文提出了一种新框架StructRAG，它基于人类在处理知识密集推理任务时将原始信息转换为多种结构化知识的认知理论，能够识别适合特定任务的最佳结构类型，将原始文档重构为该结构格式，并基于结果结构推理出答案。在各种知识密集任务上的广泛实验表明，StructRAG在复杂情境中表现出色，达到了最先进的性能，展示了其在复杂现实世界应用中增强LLMs作为有效解决方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:14:19 GMT</pubDate>
</item>
<item>
<title>通过KV预测减少变换器模型的首次输出时间</title>
<link>https://arxiv.org/abs/2410.08391</link>
<guid>https://arxiv.org/abs/2410.08391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出KV预测方法，通过辅助模型生成KV缓存，提高变换器模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 推理过程中的prompt处理步骤通常会消耗大量时间，尤其是在边缘设备上使用亿级参数模型时。为了解决首次输出时间（TTFT）过长的问题，我们提出了一种名为KV预测的新方法。通过使用一个小型辅助模型处理prompt，生成近似的KV缓存，从而减少基模型在自回归生成时对计算资源的需求。该方法在维持相对准确性的同时显著提高了效率，相比基线方法，KV预测在TriviaQA上准确度提升了15%-50%，在HumanEval的Python代码补全任务中提升了最多30%。此外，我们在Apple M2 Pro CPU上进行了基准测试，验证了FLOPs的改善直接转化为TTFT的速度提升。本研究展示了在多种TTFT计算预算下的有效性，为变换器模型的应用提供了新的可能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:41:21 GMT</pubDate>
</item>
<item>
<title>VITask：提升大型视觉语言模型任务适应性的框架</title>
<link>https://arxiv.org/abs/2410.06456</link>
<guid>https://arxiv.org/abs/2410.06456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VITask通过集成任务特定模型，提升视觉语言模型的任务适应能力，展示在医学诊断中的有效性。</p><br /><br /><p><strong>摘要：</strong> 该研究提出了一种名为VITask的创新框架，旨在提升大型视觉语言模型（VLMs）在特定任务中的适应能力。由于预训练与微调之间的领域差异，传统VLMs在任务特定应用中往往表现不佳。VITask通过引入任务特定模型（TSMs）并实施三种关键策略，即示例提示（EP）、响应分布对齐（RDA）和对比响应调整（CRT），有效改善任务特定表现。EP使TSM特征能够引导VLMs，而RDA则在推理过程中通过借鉴示例提示模型的经验，使VLMs能够适应而不需TSMs。CRT进一步优化了正确图像-响应对的排名，从而降低了生成不当响应的风险。在12个医学诊断数据集和9种成像模式下的实验结果表明，VITask的表现超越了传统的指令调优VLMs和TSMs，证明了其有效整合两种模型互补特性的能力。此外，VITask在TSM集成的灵活性和对不完整指令的鲁棒性方面也展现了实际优势，成为任务特定VLM微调的多功能和高效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:10:18 GMT</pubDate>
</item>
<item>
<title>增强大型语言模型的长度控制与复制粘贴能力</title>
<link>https://arxiv.org/abs/2410.07035</link>
<guid>https://arxiv.org/abs/2410.07035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的方法提升大型语言模型的长度控制与复制粘贴能力，显著改进性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在角色扮演、创意写作、数学推理和编码等领域显示出强大的能力，然而在长度控制方面仍然面临挑战。这一问题主要源于模型在生成文本时缺乏位置意识，导致难以遵循特定的长度限制。为了解决这一问题，我们提出了两种新方法：PositionID Prompting和PositionID Fine-Tuning，使模型能够持续监控和管理生成文本的长度。此外，我们还引入了PositionID CP Prompting，使LLMs能够准确地执行复制和粘贴操作。为了评估长度控制和复制粘贴功能，我们开发了两个基准测试。实验结果表明，我们的方法显著提高了模型对长度限制的遵守程度和复制粘贴的准确性，同时不牺牲响应质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:03:18 GMT</pubDate>
</item>
<item>
<title>Meissonic：高效的非自回归文本到图像建模</title>
<link>https://arxiv.org/abs/2410.08261</link>
<guid>https://arxiv.org/abs/2410.08261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Meissonic 提升非自回归图像建模，生成高质量、高分辨率图像。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成领域，扩散模型如 Stable Diffusion 取得了重要进展，但其和自回归语言模型的根本不同，使得统一语言-视觉模型的开发面临挑战。针对这一问题，我们提出了 Meissonic，它将非自回归的遮蔽图像建模（MIM）文本到图像生成技术提升至与先进扩散模型（如 SDXL）相当的水平。通过综合采用一系列架构创新、先进的位置信息编码策略以及优化的采样条件，Meissonic 显著提高了 MIM 的性能和效率。同时，我们利用高质量的训练数据，集成了基于人类偏好评分的信息微调条件，并采用特征压缩层来进一步提升图像的真实感和分辨率。我们的模型不仅在生成高质量、高分辨率图像方面与现有模型如 SDXL 相匹配，且往往超过其性能。大量实验验证了 Meissonic 的能力，展示了它作为文本到图像合成新标准的潜力。我们发布了一个能够生成 1024x1024 分辨率图像的模型检查点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 01:19:46 GMT</pubDate>
</item>
<item>
<title>Baichuan-Omni：首个开源7B多模态大语言模型</title>
<link>https://arxiv.org/abs/2410.08565</link>
<guid>https://arxiv.org/abs/2410.08565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Baichuan-Omni，这是一个开源的7B多模态大语言模型，具备图像、视频、音频与文本的处理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们介绍了Baichuan-Omni，首个开源的7B多模态大语言模型（MLLM），它能够同时处理和分析图像、视频、音频和文本等多种信息模式，以提供先进的多模态交互体验及出色的性能。我们提出了一种有效的多模态训练方案，从7B模型开始，经过两个阶段的多模态对齐和多任务微调，以处理音频、图像、视频和文本等多种模式。这一方法使语言模型具备有效处理视觉和音频数据的能力。Baichuan-Omni在多个全模态和多模态基准测试中表现出色。我们的目标是希望这一贡献为开放源代码社区提供一个具有竞争力的基准，促进多模态理解和实时交互的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:29:48 GMT</pubDate>
</item>
<item>
<title>基于语义得分蒸馏采样的复杂3D内容生成研究</title>
<link>https://arxiv.org/abs/2410.09009</link>
<guid>https://arxiv.org/abs/2410.09009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的SDS方法，SemanticSDS，通过语义嵌入提高复杂3D场景生成的表现力和准确性。</p><br /><br /><p><strong>摘要：</strong> 生成高质量的3D资产仍然是计算机图形学和视觉研究中的关键挑战。由于3D数据的稀缺，当前的前沿方法利用预训练的2D扩散先验，通过得分蒸馏采样（SDS）进行优化。尽管技术有所进步，然而，制作複杂3D场景及多个物体或复杂交互仍然困难。为了解决这一问题，近期方法逐渐引入了框或布局引导，但这些布局引导的组合方法通常在提供细粒度控制方面表现不佳，较为粗糙且缺乏表现力。为此，我们提出了一种新的得分蒸馏采样方法——语义得分蒸馏采样（SemanticSDS），旨在有效提高组合文本到3D生成的表现力与准确性。我们的做法整合了新的语义嵌入，这些嵌入在不同渲染视图间保持一致，并且能够清晰地区分不同的物体和部分。通过将这些嵌入转化为一个语义图，我们引导了区域特定的SDS过程，从而实现精确的优化与组合生成。通过利用显式的语义引导，我们的方法解锁了现有预训练扩散模型的组合能力，在复杂物体和场景的3D内容生成方面达到了优秀的质量。实验结果表明，我们的SemanticSDS框架在生成尖端复杂3D内容方面非常有效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:24:03 GMT</pubDate>
</item>
<item>
<title>SuperCorrect：一种改进小型模型推理能力的两阶段框架</title>
<link>https://arxiv.org/abs/2410.09008</link>
<guid>https://arxiv.org/abs/2410.09008</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出SuperCorrect框架，通过大模型的监督，提高小模型的推理与自我纠错能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）如GPT-4和PaLM在推理任务中表现出显著的改进。然而，较小的模型例如Llama-3-8B和DeepSeekMath-Base在复杂数学推理中仍面临挑战，尤其在独立检测和纠正推理错误方面。为了解决这一问题，我们提出了一个名为SuperCorrect的创新两阶段框架，利用大型教师模型来监督和纠正小型学生模型的推理和反思过程。在第一阶段，我们从教师模型中提取层次化的高层次和详细思维模板，以指导学生模型更好地引导细致的推理思考。在第二阶段，我们引入跨模型的协作直接偏好优化（DPO）方法，以增强学生模型的自我纠错能力，学生模型在训练过程中跟随教师的纠正轨迹。这个跨模型DPO方法教会学生模型有效定位和解决错误思维，突破思维瓶颈，获取新的技能和知识，从而应对挑战性的问题。大量实验证明，SuperCorrect优于以前的方法，尤其是我们的SuperCorrect-7B模型在MATH和GSM8K基准上分别比DeepSeekMath-7B提升了7.8%/5.3%和比Qwen2.5-Math-7B提升了15.1%/6.3%，在所有7B模型中达到了新的SOTA性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09008" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:22:21 GMT</pubDate>
</item>
<item>
<title>EvolveDirector：基于公共资源训练文本到图像生成模型的框架</title>
<link>https://arxiv.org/abs/2410.07133</link>
<guid>https://arxiv.org/abs/2410.07133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvolveDirector框架利用公共API生成的数据训练文本到图像生成模型，显著降低数据需求，实现优越的生成能力。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的快速进步，文本到图像生成表现出惊人的内容创作能力。然而，大多数先进模型依赖于专有数据，并提供有限的开放API，限制了其在下游任务中的应用。为探讨利用公共资源训练一个可与先进模型媲美的文本到图像生成模型的可行性，我们提出了EvolveDirector框架。该框架通过公共API与先进模型交互，获得文本-图像数据对以训练基础模型。实验显示，尽管通过生成数据训练的模型能接近先进模型的生成能力，但需要大量的样本（1000万个或更多），这会带来高昂的时间和计算资源成本，尤其是API调用费用。为了解决这一问题，EvolveDirector利用预训练的大型视觉-语言模型（VLM）来指导基础模型的演化，持续评估基础模型，并通过判别、扩展、删除和变异等操作动态更新和优化训练数据集。实验结果表明，这种方法显著减少了所需的数据量。EvolveDirector还能够在面对多个高级模型时选择其生成的最佳样本，以学习强大且均衡的能力。最终训练出的模型Edgen显示出优于这些先进模型的效果。代码和模型权重可在https://github.com/showlab/EvolveDirector获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:17:02 GMT</pubDate>
</item>
<item>
<title>利用加速偏好优化加快人类反馈下的强化学习</title>
<link>https://arxiv.org/abs/2410.06293</link>
<guid>https://arxiv.org/abs/2410.06293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种加速偏好优化框架，结合动量技术加速大语言模型的对齐。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的人类反馈（RLHF）正在成为对齐大型语言模型（LLM）的重要工具。直接偏好优化（DPO）是一种流行的方法，它将RLHF视为一个政策优化问题，而不显式估计奖励函数。此方法克服了两步法所面临的稳定性和效率问题，并展示了可以通过动量技术加速RLHF的潜力。本文首次表明，迭代偏好优化方法可以视为一种近端点方法。基于此观察，提出了一种通用的加速偏好优化（APO）框架，统一了许多现有的偏好优化算法，并采用Nesterov动量技术加速LLM的对齐过程。理论上，APO能够实现比标准迭代偏好优化方法（包括DPO和自我博弈偏好优化（SPPO））更快的收敛速度。实证结果显示，APO在AlpacaEval 2.0基准上相较于DPO、迭代DPO及其他强基线表现出更强的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 23:06:15 GMT</pubDate>
</item>
<item>
<title>Data Advisor：提升数据生成质量的增强LLM方法</title>
<link>https://arxiv.org/abs/2410.05269</link>
<guid>https://arxiv.org/abs/2410.05269</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Data Advisor，优化LLM生成数据，提高数据质量与覆盖率，特别针对安全对齐问题。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型 (LLM) 对齐中，数据是至关重要的元素。尽管近期研究探讨了使用LLM进行高效数据收集，但LLM生成的数据常常存在质量问题，如缺乏代表性、某些方面的缺失以及低质量数据点。为了解决这些问题，我们提出了Data Advisor，这是一种增强的LLM方法，用于生成符合特定特征的数据集。Data Advisor基于一组预定义原则，监控生成数据的状态，识别当前数据集的弱点，并据此建议下一轮数据生成的策略。Data Advisor可轻松集成到现有的数据生成方法中，以提升数据的质量和覆盖率。在对三种代表性LLM（即Mistral、Llama2和Falcon）进行的安全对齐实验中，Data Advisor显示了提升模型安全性与应对各种细粒度安全问题的有效性，同时不牺牲模型的实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05269" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 19:05:54 GMT</pubDate>
</item>
<item>
<title>基于神经符号学习的LLM世界模型对齐与探索</title>
<link>https://arxiv.org/abs/2410.07484</link>
<guid>https://arxiv.org/abs/2410.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WALL-E代理，通过规则学习对LLM进行环境对齐，显著提升在Minecraft和ALFWorld等开放世界中的探索效率与成功率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）作为模型基础代理的世界模型的潜力，尽管存在LLMs的先验知识与特定环境动态之间的差距，我们的研究表明，这些差距可以通过对LLM与其部署环境进行对齐来弥补。这种“世界对齐”可以通过在LLMs上进行规则学习高效实现。由于LLMs具有丰富的先验知识，通常只需少量额外规则即可使LLM预测与实定环境动态一致。为此，本文提出了一种神经符号方法，通过比较代理探索的轨迹与世界模型预测，诱导、更新和修剪规则，从而以无梯度的方式学习这些规则。最终的世界模型由LLM及学习的规则组成。我们的具身LLM代理“WALL-E”基于模型预测控制（MPC）构建，通过基于精确世界模型优化前瞻性动作，MPC显著提高了探索和学习效率。与现有LLM代理相比，WALL-E的推理只需少量主规则，而不是将冗长的缓冲轨迹纳入LLM输入。在Minecraft和ALFWorld的开放世界挑战中，WALL-E相比现有方法有更高的成功率，且重规划时间和使用的tokens更少。在Minecraft中，WALL-E的成功率超过基线15-30%，重规划轮次减少8-20轮，使用的tokens仅为60-80%。在ALFWorld中，其成功率在仅6次迭代后飙升至95%的新纪录。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 17:14:49 GMT</pubDate>
</item>
<item>
<title>向量-内联学习：扩展大型语言模型的能力</title>
<link>https://arxiv.org/abs/2410.05629</link>
<guid>https://arxiv.org/abs/2410.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型如何通过轻量级投影器处理连续向量，实现向量-内联学习能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在文本数据的上下文学习（ICL）能力方面表现出色。本文探索了这些能力是否可以扩展到来自不同领域的连续向量，这些向量是通过黑箱预训练编码器获得的。我们通过轻量级投影器将输入数据与LLM的嵌入空间对齐，发现LLMs能够有效地处理和学习这些投影向量，称之为向量-内联学习（Vector-ICL）。尤其值得注意的是，使用通用语言建模目标对投影器进行预训练可以实现Vector-ICL，而任务特定的微调则进一步提升了性能。我们在各种任务和模态上进行了实验，包括文本重建、数值函数回归、文本分类、摘要生成、分子描述、时间序列分类、图分类和fMRI解码。结果表明，Vector-ICL在许多情况下超越了少量样本的ICL和领域特定模型或微调。我们还进行了分析和案例研究，表明LLMs在处理向量表示方面的潜力超越了传统的基于标记的范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 16:04:02 GMT</pubDate>
</item>
<item>
<title>Zebra：一种新型生成自回归变换器用于解决时间依赖性参数偏微分方程</title>
<link>https://arxiv.org/abs/2410.03437</link>
<guid>https://arxiv.org/abs/2410.03437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra 是一款不需梯度适应的新型变换器，能够灵活应对各种参数 PDE，展示卓越的性能。</p><br /><br /><p><strong>摘要：</strong> 解决时间依赖性参数偏微分方程（PDEs）是一项具有挑战性的任务，因为模型必须适应系数、强迫项和边界条件等参数的变化。数据驱动的神经网络求解器通常依赖于从PDE参数分布中采样的数据进行训练，以期在新的实例上实现泛化，或依靠基于梯度的适应和元学习，隐式编码来自观察的数据动态。然而，这通常会增加推理复杂性。受到大型语言模型（LLMs）在上下文学习能力的启发，我们提出了Zebra，一种新型生成自回归变换器，旨在解决参数PDEs，而无需在推理时进行梯度适应。Zebra通过在预训练和推理过程中利用上下文信息，能够动态适应新任务，条件依赖于包含上下文轨迹或先前状态的输入序列。这种方法使得Zebra能够灵活处理任意大小的上下文输入，并通过采样多个解轨迹支持不确定性量化。我们在多种挑战性的PDE场景中评估了Zebra，展示了其适应性、鲁棒性以及相较于现有方法的优越性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 15:11:34 GMT</pubDate>
</item>
<item>
<title>DART：一种新型的非马尔可夫扩散模型</title>
<link>https://arxiv.org/abs/2410.08159</link>
<guid>https://arxiv.org/abs/2410.08159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DART提出了一种统一自回归与扩散框架的图像生成模型，提升了训练与推理效率。</p><br /><br /><p><strong>摘要：</strong> 扩散模型已成为视觉生成的主流方法，但其基于马尔可夫过程的特性限制了模型充分利用生成轨迹的能力，导致在训练和推理时效率低下。为此，本文提出了一种新的模型DART，基于变换器架构，将自回归（AR）和扩散过程结合在一个非马尔可夫的框架内。DART通过空间和光谱的方式迭代去噪图像块，采用与标准语言模型相同的架构。DART不依赖于图像量化，增强了图像建模的有效性，保持了灵活性。同时，DART能够在一个统一模型中无缝地训练文本和图像数据。我们的实验结果表明，DART在类别条件和文本到图像生成任务上表现出色，提供了一种可扩展、高效的替代方案。通过这一统一框架，DART为高质量图像合成设立了新的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:56:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型的任务超叠现象及其内在机制研究</title>
<link>https://arxiv.org/abs/2410.05603</link>
<guid>https://arxiv.org/abs/2410.05603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在单次推理过程中同时执行多个不同任务的能力，称为任务超叠现象。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在上下文学习（ICL）方面的一个惊人现象：模型能够在单次推理调用中同时执行多个计算上独立的ICL任务，这一能力被称为‘任务超叠’。我们提供了针对不同LLM家族和规模的实证证据，表明即使在模型训练时仅学习一个任务，也能出现这一现象。此外，我们提供了理论解释，认为这一能力在变换器的表达能力范围内。我们还探讨了LLMs如何在超叠过程中内部组合任务向量的机制。研究表明，较大的模型能够并行解决更多ICL任务，并更好地校准其输出分布。这些发现为LLMs潜在能力提供了新的见解，进一步支持了‘LLMs作为模拟器的超叠’的观点，并引发了关于实现同时任务执行的机制的思考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:27:35 GMT</pubDate>
</item>
<item>
<title>自动化基准测试中的作弊现象及其影响</title>
<link>https://arxiv.org/abs/2410.07137</link>
<guid>https://arxiv.org/abs/2410.07137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，常量输出模型可在自动化基准测试中作弊，表现异常优异，呼吁开发反作弊机制。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在自动化大语言模型（LLM）基准测试中存在的作弊问题。研究表明，即使是一个始终输出固定响应（与输入无关）的“空模型”，也能在多个基准测试上取得高分，如在 AlpacaEval 2.0 上获得 86.5% 的 LC 胜率，在 Arena-Hard-Auto 上获得 83.0 分，而在 MT-Bench 上获得 9.55 分。这些作弊输出展示出了可转移性，因为假设测试指令是私有且不可访问的。尽管本研究主要作为概念验证，但某些对手可以利用 LLM 生成更不易察觉的作弊回复，从而从高胜率和推广影响中不道德地获利。因此，本文呼吁开发可靠的反作弊机制，以确保自动化基准测试的可信度及其在评估语言模型时的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:28:45 GMT</pubDate>
</item>
<item>
<title>LPZero：自动设计零成本代理的框架</title>
<link>https://arxiv.org/abs/2410.04808</link>
<guid>https://arxiv.org/abs/2410.04808</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPZero是一个框架，能够自动设计零成本（ZC）代理，提升NLP任务中的性能和排名一致性。</p><br /><br /><p><strong>摘要：</strong> 在神经架构搜索（NAS）面临的大量计算开销的背景下，零成本（ZC）代理作为一种有前途的方法逐渐受到关注。然而，现有的ZC代理往往依赖于专家知识，且存在显著的试错成本，尤其在自然语言处理（NLP）任务中，许多ZC代理无法超越简单基线表现。为了解决这些问题，我们提出了一个新颖的框架LPZero，这是首个能够为各种任务自动设计ZC代理的系统，且其排名一致性优于人工设计的代理。具体而言，我们将ZC代理建模为符号方程，并整合了一个统一的代理搜索空间，该空间涵盖了由预定义的数学符号构成的现有ZC代理。LPZero利用遗传编程进行启发式搜索，以找到最佳的符号组合。同时，我们提出了一种基于规则的剪枝策略（RPS），该策略可以预先消除不太有前景的代理，从而降低代理降低性能的风险。通过对FlexiBERT、GPT-2和LLaMA-7B的广泛实验，LPZero在下游任务中的排名能力和性能均优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04808" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:09:02 GMT</pubDate>
</item>
<item>
<title>GLOV：利用大语言模型优化视觉语言模型的隐式优化方法</title>
<link>https://arxiv.org/abs/2410.06154</link>
<guid>https://arxiv.org/abs/2410.06154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GLOV方法，利用大语言模型作为隐式优化器提升视觉任务性能。</p><br /><br /><p><strong>摘要：</strong> 在这项研究中，我们提出了一种新颖的方法（GLOV），使大语言模型（LLMs）能够作为隐式优化器，提升视觉语言模型（VLMs）在下游视觉任务中的表现。通过对下游任务描述的元提示，GLOV查询出适合的VLM提示（如使用CLIP进行零-shot分类），并根据适应性函数获得的纯度度量对这些提示进行排名。在每个优化步骤中，排名后的提示及其准确性作为上下文示例被送入LLM，帮助LLM了解下游VLM所偏爱的文本提示类型。此外，我们在每个优化步骤中明确引导LLM生成过程，通过将来自上一步中正负解的嵌入差异向量添加至网络的中间层，从而在下一步的生成过程中引导LLM朝向下游VLM所偏好的语言类型。这一策略显著提升了下游视觉任务的表现。我们在16个多样化的数据集上全面评估了GLOV，使用两类VLM（即双编码器模型如CLIP和编码器-解码器模型如LLaVa），结果显示，所发现的解决方案能在识别性能上提升高达15.0%和57.5%（平均分别提升3.8%和21.6%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 07:34:00 GMT</pubDate>
</item>
<item>
<title>WorFBench：一个用于评估工作流生成能力的统一基准</title>
<link>https://arxiv.org/abs/2410.07869</link>
<guid>https://arxiv.org/abs/2410.07869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了WorFBench工作流生成基准及WorFEval评估协议，揭示LLM在序列和图规划间的能力差异。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在解决推理和规划任务方面取得了显著进展，其中将复杂问题分解为可执行工作流是关键步骤。现有的工作流评估框架存在局限，不能全面反映LLM的能力。为此，我们引入了WorFBench，这是一种统一的工作流生成基准，涵盖多样化场景和复杂图形工作流结构。同时，我们提出了WorFEval，一种系统的评估协议，利用子序列和子图匹配算法来准确量化LLM代理的工作流生成能力。通过对不同类型的LLMs进行综合评估，我们发现LLM代理在序列规划能力和图规划能力之间存在显著差距，即使是GPT-4，二者之间的差距约为15%。此外，我们训练了两个开源模型，并评估了它们在持出任务上的泛化能力。值得注意的是，生成的工作流能够提高下游任务的表现，使得推理过程所需时间减少，从而达到更优秀的性能。代码和数据集将发布于https://github.com/zjunlp/WorFBench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:48:20 GMT</pubDate>
</item>
<item>
<title>基于运动先验的变形3D高斯点云重建框架MotionGS</title>
<link>https://arxiv.org/abs/2410.07707</link>
<guid>https://arxiv.org/abs/2410.07707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了MotionGS，一个通过运动先验引导变形的3D高斯点云重建框架，显著提升动态场景重建效果。</p><br /><br /><p><strong>摘要：</strong> 动态场景重建在3D视觉领域一直是一个长期挑战。最近，3D高斯点云技术的兴起为这一问题提供了新的视角。尽管后续工作迅速将静态3D高斯扩展到动态场景，但往往缺乏对物体运动的明确约束，导致优化困难和性能下降。为了解决这些问题，本文提出了一个新颖的变形3D高斯点云框架——MotionGS，旨在探索明确的运动先验以引导3D高斯的变形。具体而言，我们首先引入了一种光流解耦模块，该模块将光流解耦为相机流和运动流，分别对应于相机移动和物体运动。然后，运动流可以有效约束3D高斯的变形，从而模拟动态物体的运动。此外，还提出了一种相机姿态优化模块，以交替优化3D高斯和相机姿态，减轻不准确相机姿态的影响。在单目动态场景中的广泛实验验证了MotionGS在定性和定量结果上都超越了最新的技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:17:59 GMT</pubDate>
</item>
<item>
<title>基于数学推理和代码生成的数学继续预训练方法</title>
<link>https://arxiv.org/abs/2410.08196</link>
<guid>https://arxiv.org/abs/2410.08196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的数学继续预训练方法，通过生成代码和推理步骤提升语言模型的数学能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的数学继续预训练方法，通过生成数学代码及其相应的推理步骤，提升大型语言模型的数学推理能力。最初，我们构建了一个高质量的数学继续预训练数据集，融合了数学相关的网络数据、使用数学包的代码、数学教科书以及合成数据。接着，我们通过提取LaTeX表达式、这些表达式所需的条件及其结果，生成推理步骤。基于提取的信息，我们生成了相应的代码，以准确捕捉数学推理过程。在每个推理步骤后附加生成的代码，形成了自然语言推理步骤与其对应代码的配对数据。将这些数据与原始数据集结合，形成了一个包含19.2B标记的高性能数学预训练语料库，命名为MathCode-Pile。使用该语料库训练几种流行的基础模型显著提升了它们的数学能力，最终形成了MathCoder2模型系列。为了确保透明性和易于重现，我们将所有数据处理和训练代码开源，代码可在https://github.com/mathllm/MathCoder2获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 04:16:58 GMT</pubDate>
</item>
<item>
<title>SFTMix: 基于Mixup的指令调优方法研究</title>
<link>https://arxiv.org/abs/2410.05248</link>
<guid>https://arxiv.org/abs/2410.05248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SFTMix，通过Mixup正则化提高指令调优性能，降低对高质量数据集的依赖。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）交互驱动任务的指令调优阶段，通常通过下一个标记预测（NTP）损失对指令-响应对进行训练。以往旨在提升指令调优表现的研究往往强调需要更高质量的监督微调（SFT）数据集，这通常伴随高昂的数据过滤成本或人工标注的劳动力。然而，这些方法未能充分利用数据集的内在特性，导致高计算和劳动成本，限制了可扩展性和性能提升。本文提出了SFTMix，一种新颖的策略，通过Mixup正则化提升指令调优性能，而无需精心策划的数据集。我们观察到LLMs在语义表示空间中的信心不均匀，认为不同信心水平的示例在指令调优过程中应发挥不同角色。在此基础上，SFTMix利用训练动态识别不同信心水平的示例，减轻对高信心示例的过拟合，同时增强对低信心示例的学习信号。这一方法显著提升了在多种指令跟随和医疗领域特定SFT任务中的表现，证明了SFTMix对不同LLM家族的适应性以及对任何规模数据集的可扩展性。全面的消融研究进一步验证了SFTMix设计选择的稳健性，强调其在语言处理应用中提升性能的通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:19:15 GMT</pubDate>
</item>
<item>
<title>Agent S: 基于多模态大语言模型的自主交互框架</title>
<link>https://arxiv.org/abs/2410.08164</link>
<guid>https://arxiv.org/abs/2410.08164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent S 是一种开放的自主代理框架，可通过GUI自动化复杂任务，实现人机交互的变革。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Agent S，一个开放的自主代理框架，旨在通过图形用户界面与计算机进行自主交互，从而实现人机交互的变革，自动化复杂和多步骤的任务。Agent S解决了自动化计算机任务的三个主要挑战：获取领域特定知识、规划长期任务及处理动态非均匀界面。为此，Agent S 引入了经验增强层次规划，能够在多个层次上从外部知识搜索和内部经验检索中学习，从而促进高效的任务规划和子任务执行。此外，Agent S 采用了代理-计算机接口（ACI），更好地充分发挥基于多模态大语言模型（MLLMs）的GUI代理的推理和控制能力。根据OSWorld基准的评估结果，Agent S 在成功率上超过了基线水平9.37%，实现了83.6%的相对改进，并且取得了新的最先进成果。全面分析展示了各个组件的有效性，并为未来的改进提供了洞见。此外，Agent S 在新发布的WindowsAgentArena基准上展现出良好的广泛泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:06:26 GMT</pubDate>
</item>
<item>
<title>大语言与视觉模型（LLVMs）的感知能力研究</title>
<link>https://arxiv.org/abs/2410.04751</link>
<guid>https://arxiv.org/abs/2410.04751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统探讨了LLVMs在感知任务中的表现及其内在机制。</p><br /><br /><p><strong>摘要：</strong> 本文系统地研究了大语言与视觉模型（LLVMs），特别是它们在基础感知任务（如MMVP）上的低表现。通过对几种LLVMs家族（如LLaVA）的10个评估基准进行评估，我们发现多个有趣的特性：1）即便视觉块序列随机排列，它们仍以全局方式处理图像；2）在解决数学问题时，模型并不总是需要详细的数字信息；3）交叉模态的对齐在复杂推理任务中存在过拟合现象，从而导致模型失去部分视觉编码器的原始感知能力；4）模型较低层次的表示空间（低于25%）在性能和增强视觉理解中起着关键作用。基于这些观察，本文提出了未来改进LLVMs及构建更具挑战性评估基准的潜在方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:01:45 GMT</pubDate>
</item>
<item>
<title>AlphaLLM-CPL：一种基于MCTS行为蒸馏的自我改进框架</title>
<link>https://arxiv.org/abs/2410.06508</link>
<guid>https://arxiv.org/abs/2410.06508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AlphaLLM-CPL框架，利用MCTS生成的轨迹对LLM进行自我改进，显著提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们提出了一种新的框架，AlphaLLM-CPL，用于通过蒙特卡洛树搜索（MCTS）行为蒸馏来提升大语言模型（LLM）的推理能力。尽管现有的蒸馏方法利用MCTS生成的轨迹，但仍然未能充分利用这些丰富的信息，限制了LLM推理性能的提升。AlphaLLM-CPL引入了两个关键创新：首先，它从共享同一父节点的子节点构造逐步轨迹对，以提供更有效的逐步信息用于MCTS行为蒸馏。其次，AlphaLLM-CPL采用了课程偏好学习，在每个离线训练周期中动态调整轨迹对的训练顺序，优先考虑关键学习步骤，从而减轻过拟合。通过在数学推理任务上的实验结果表明，AlphaLLM-CPL显著优于以往的MCTS行为蒸馏方法，极大地提升了LLM的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:06:42 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的进展及应用</title>
<link>https://arxiv.org/abs/2410.08151</link>
<guid>https://arxiv.org/abs/2410.08151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出扩展现有视频扩散模型的方法，实现长达1分钟的视频生成。</p><br /><br /><p><strong>摘要：</strong> 当前最前沿的视频扩散模型在生成高质量视频方面表现出色。然而，由于计算限制，这些模型通常只能生成时长约为10秒（240帧）的视频。本文展示了如何在不改变现有架构的情况下，将这些模型自然扩展为自回归视频扩散模型。我们的关键思想是为潜在帧分配逐渐增加的噪声水平，而不是使用单一的噪声水平，从而允许潜在帧之间的细粒度条件及注意窗口之间的大重叠。这种渐进的视频去噪方法使我们的模型在自回归生成视频帧时，能够避免质量下降或突发场景变化。我们在长视频生成任务上取得了最先进的结果，生成了长达1分钟（1440帧、24帧每秒）的视频。本文视频可在https://desaixie.github.io/pa-vdm/上查看。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:02:23 GMT</pubDate>
</item>
<item>
<title>大卷积核在现代卷积神经网络设计中的应用</title>
<link>https://arxiv.org/abs/2410.08049</link>
<guid>https://arxiv.org/abs/2410.08049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出大卷积核作为设计现代卷积神经网络（ConvNets）的新范式，并引入UniRepLKNet架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出了大卷积核作为现代卷积神经网络（ConvNets）设计的新范式。研究表明，使用少量大卷积核，而非堆叠多个小卷积核，可能是一种更优的设计策略。我们提出了一套专门针对大卷积核ConvNets的架构设计指南，以优化其效率和性能。UniRepLKNet架构的设计原则特别强调大卷积核ConvNets在无需深层堆叠的情况下捕捉广泛空间信息的能力。实验结果表明，该模型在ImageNet上达到了88.0%的准确率，ADE20K数据集上获得了55.6%的mIoU，以及在COCO检测上取得了56.4%的AP，表现出显著的可扩展性及在多种场景下的优异性能，包括时间序列预测、音频、点云及视频识别。与视觉Transformer相比，大卷积核ConvNets具有更大的有效感受野和更高的形状偏置，克服了小卷积核CNN的纹理偏置。这些结果展示了大卷积核ConvNets的通用建模能力。所有代码和模型已在https://github.com/AILab-CVC/UniRepLKNet上公开，促进社区的进一步研究与发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:49:57 GMT</pubDate>
</item>
<item>
<title>简化和扩展扩散模型 rectification 的新策略</title>
<link>https://arxiv.org/abs/2410.07303</link>
<guid>https://arxiv.org/abs/2410.07303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出了 Rectified Diffusion，简化了 rectification 方法并验证其在 Stable Diffusion 上的效果。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视觉生成方面取得了显著进展，但由于解决生成常微分方程（ODE）的计算密集性，生成速度仍然较慢。本文提出的 Rectified Diffusion 方法，通过使用预训练的扩散模型获取噪声和样本的匹配对，简化了训练程序。我们认为，以往方法中包含的流匹配和 v 预测等组件并非必要，主要目标应是实现一阶近似 ODE 路径，而非强求路径的直线性。我们的方法不再局限于流匹配模型，而是广泛适用于各种扩散模型。经过在 Stable Diffusion v1-5 和 Stable Diffusion XL 上的验证，我们的方法不仅降低了训练成本，还提升了性能。我们的代码已开放在 GitHub 上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07303" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:27:35 GMT</pubDate>
</item>
<item>
<title>基于偏好学习的多模态轨迹检索增强方法</title>
<link>https://arxiv.org/abs/2410.03450</link>
<guid>https://arxiv.org/abs/2410.03450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MART方法，通过偏好学习优化轨迹检索，提升机器人在未见场景中的任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法MLLM as ReTriever (MART)，旨在提升机器人执行复杂任务的能力。当前的检索方法多关注表面相似性，未充分考虑轨迹对特定任务的有效性。MART利用交互数据，通过偏好学习对大型语言模型（MLLM）进行微调，使检索过程能够更好地评估和优先选择适用于未见任务的轨迹。为进一步加强理解，文章引入Trajectory Abstraction机制，利用MLLM的摘要能力将轨迹用更少的符号表示，并保留关键身分信息，这帮助代理更好地抓住轨迹中的重要里程碑。实验结果显示，在不同环境下，MART方法显著提升了代理在未见场景中的任务成功率，表明其在多模态轨迹检索与代理行为的研究中具有重要意义。所有基准任务集和模拟器代码修改将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:21:00 GMT</pubDate>
</item>
<item>
<title>PrefixQuant：一种高效的稀疏化量化技术用于大型语言模型的推理加速</title>
<link>https://arxiv.org/abs/2410.05265</link>
<guid>https://arxiv.org/abs/2410.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefixQuant通过离线隔离高频稀疏令牌，实现了高效的静态量化，显著提升推理速度与准确性。</p><br /><br /><p><strong>摘要：</strong> 量化在大型语言模型（LLM）部署中尤为重要，因为它能提高内存效率和推理速度。目前的激活量化方法主要处理通道级别的离群点，往往忽视令牌级别的离群点，因此依赖于昂贵的每令牌动态量化技术。为了解决这个问题，我们提出了PrefixQuant，一种独特的技术，能够离线识别高频离群令牌，并将其前缀存储在KV缓存中，从而防止推理时生成离群令牌，并简化量化。根据我们的知识，PrefixQuant首次实现了高效的每张量静态量化，超过了昂贵的每令牌动态量化。以W4A4KV4（4位权重、4位激活和4位KV缓存）上的Llama-3-8B为例，PrefixQuant结合每张量静态量化实现了7.43的WikiText2困惑度以及71.08%的常识推理任务平均准确率，分别比之前的动态量化方法QuaRot提升了0.98困惑度与5.98准确率。此外，使用PrefixQuant的W4A4量化模型的推理速度比FP16模型快1.60至2.81倍，相较于QuaRot模型快1.2至1.3倍。我们的代码已发布在https://github.com/ChenMnZ/PrefixQuant。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:13:37 GMT</pubDate>
</item>
<item>
<title>Optima：提升大语言模型多智能体系统通信效率与任务有效性的框架</title>
<link>https://arxiv.org/abs/2410.08115</link>
<guid>https://arxiv.org/abs/2410.08115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Optima通过提高通信效率和任务有效性，解决大语言模型多智能体系统中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Optima，一个新颖的框架，旨在解决大语言模型（LLM）基础的多智能体系统（MAS）面临的低通信效率、较差的可扩展性和缺乏有效参数更新优化方法等关键挑战。Optima通过对LLM进行训练，采用迭代的生成、排名、选择和训练模式，结合一个平衡任务性能、 tokens 效率和交流可读性的奖励函数，大幅增强了通信效率和任务有效性。我们探索了多种强化学习算法，包括监督微调（Supervised Fine-Tuning）、直接偏好优化（Direct Preference Optimization）及其混合方法，提供了它们的有效性与效率权衡的深入见解。此外，我们结合受蒙特卡洛树搜索启发的技术生成DPO数据，将对话回合视为树节点，以探索多样的互动路径。在信息不对称问答和复杂推理等常见多智能体任务上评估后，Optima相较于单智能体基线和基于Llama 3 8B的传统MAS表现出一致而显著的提升，在需要大量信息交换的任务中实现了多达2.8倍的性能提升，同时消耗的tokens少于10%。此外，Optima的效率提升为更有效利用推理计算开辟了新可能，推动了推理时间的规模法则的改进。通过解决大语言模型基础多智能体系统中的基本挑战，Optima展示了朝着可扩展、高效和有效的多智能体系统发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:11:50 GMT</pubDate>
</item>
<item>
<title>重复训练示例在变压器模型中的效益研究</title>
<link>https://arxiv.org/abs/2410.07041</link>
<guid>https://arxiv.org/abs/2410.07041</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，在固定训练步数下，重复训练的示例效果优于单次使用的示例。</p><br /><br /><p><strong>摘要：</strong> 本文研究了变压器模型在算法生成数据集上的性能表现，重点关注训练示例重复使用的问题。在处理最大公约数、模乘法和矩阵特征值这三种数学问题时，我们发现，在固定的训练步数下，使用重复的示例的小规模训练集的模型性能优于使用单次示例的大规模训练集的模型。这表明，重复训练的益处超越了数据多样性带来的优势。此外，我们还展示了两集训练的方法，即对小随机子集的重复使用与对训练集其余部分的正常采样相结合，可以加速学习并提升模型性能。这项研究在受控环境下提供了关于深度学习中泛化与记忆之间尚不清晰的相互关系的洞察。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07041" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:59:17 GMT</pubDate>
</item>
<item>
<title>基于局部对抗负例损失的视觉语言模型增强方法</title>
<link>https://arxiv.org/abs/2410.05210</link>
<guid>https://arxiv.org/abs/2410.05210</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新方法FSC-CLIP，通过局部硬负例损失提升视觉语言模型的组合理解能力，保持多模态任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法——Fine-grained Selective Calibrated CLIP (FSC-CLIP)，旨在提升预训练视觉语言模型（VLMs）的组合理解能力，同时不影响零-shot多模态任务的性能。传统的微调方法通常在提高组合推理的同时降低多模态能力，其主要原因是使用全局硬负例（HN）损失，导致图像和文本的全局表示受到影响。这种全局HN损失会推送与原始文本高度相似的HN文本，从而损害模型的多模态表示。为克服这一限制，FSC-CLIP整合了局部硬负例损失和选择性校准正则化。这些创新提供了精细的负向监督，同时保持了模型的表示完整性。我们的广泛评估显示，FSC-CLIP在多项组合性和多模态任务基准上不仅达到了与最先进模型相当的组合能力，而且保持了强大的多模态能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05210" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:47:13 GMT</pubDate>
</item>
<item>
<title>DICE：用于可控编辑的离散反演方法</title>
<link>https://arxiv.org/abs/2410.08207</link>
<guid>https://arxiv.org/abs/2410.08207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICE是首个用于离散扩散模型的精确反演方法，支持灵活的内容编辑。</p><br /><br /><p><strong>摘要：</strong> 离散扩散模型在图像生成和掩蔽语言建模等任务中取得了成功，但在可控内容编辑方面存在局限性。我们提出了DICE（Discrete Inversion for Controllable Editing），这是第一个支持离散扩散模型（包括多项式扩散和掩蔽生成模型）的精确反演方法。通过在反向扩散过程中记录噪声序列和掩蔽模式，DICE实现了对离散数据的准确重建和灵活编辑，无需预定义掩蔽或注意力操作。我们在图像和文本领域展示了DICE的有效性，并在VQ-Diffusion、Paella和RoBERTa等模型上进行了评估。结果显示，DICE在提高编辑能力的同时，也保持了高数据保真度，为离散空间中的精细内容操控提供了新的机遇。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:32:24 GMT</pubDate>
</item>
</channel>
</rss>