<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>RewardSDS: 基于对齐评分的采样优化方法</title>
<link>https://arxiv.org/abs/2503.09601</link>
<guid>https://arxiv.org/abs/2503.09601</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>RewardSDS通过对齐评分优化了2D扩散模型的生成效果。</p><br><br><p><strong>摘要：</strong> Score Distillation Sampling（SDS）在利用2D扩散先验进行文本到3D生成等任务中表现出色，但其在精细对齐用户意图方面存在困难。为了解决此问题，我们提出了RewardSDS，一种新方法通过根据奖励模型的对齐评分对噪声样本进行加权，生成加权的SDS损失。这种损失函数优先考虑那些能产生高奖励输出的噪声样本的梯度。我们的这一方法可广泛适用于SDS基础上，并特别引入了RewardVSD，优化变分评分蒸馏。我们在文本-图像、2D编辑和文本-3D生成任务上评估了RewardSDS和RewardVSD，相较于传统SDS和VSD，我们显示出在生成质量和与所需奖励模型对齐性方面的显著提升，实现了最新的性能水平。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.09601 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:47 GMT</pubDate>
<pubDate>Wed, 12 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的文本块处理优化研究</title>
<link>https://arxiv.org/abs/2503.09600</link>
<guid>https://arxiv.org/abs/2503.09600</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出了一种新的文本块处理评估方法与框架，提升了RAG系统性能。</p><br><br><p><strong>摘要：</strong> 本文研究了在检索增强生成（RAG）过程中，文本块处理的重要性并提出了一种双指标评估方法，包括边界清晰度和块粘性，以量化文本块质量。通过对传统和语义块处理的局限性分析，强调了将大语言模型（LLM）纳入文本块处理的必要性。为解决LLM方法在计算效率和精度之间的权衡，提出了细粒度混合块处理器（MoC）框架，采用三阶段处理机制。该框架指导块处理器生成结构化的块正则表达式，从而有效提取文本块。大量实验表明，所提指标和MoC框架成功解决了文本块处理的挑战，揭示了块处理的核心，并显著提升了RAG系统的整体性能。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.09600 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:42 GMT</pubDate>
<pubDate>Wed, 12 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>优化大语言模型的构建：上下文长度与注意力头的影响</title>
<link>https://arxiv.org/abs/2503.09579</link>
<guid>https://arxiv.org/abs/2503.09579</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文探讨了上下文长度和注意力头配置对大语言模型性能的影响。</p><br><br><p><strong>摘要：</strong> 本文系统比较了不同参数大小、上下文长度和注意力头配置的大语言模型在模型性能、计算成本和内存成本上的表现。研究发现，在处理足够长序列时，使用较少注意力头的较大模型可以在降低计算和内存成本的同时减少损失。通过扩展现有基于参数大小和训练计算的缩放方法，本文为训练和推理阶段的成本优化提供了指导。这些发现为实际应用中开发大语言模型，特别是在长上下文处理场景中，提供了重要见解。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.09579 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:50:42 GMT</pubDate>
<pubDate>Wed, 12 Mar 2025 13:50:42 GMT</pubDate>
</item>
<item>
<title>通过可验证结果奖励强化学习提升视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2503.08525</link>
<guid>https://arxiv.org/abs/2503.08525</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究表明，通过GTR框架提升视觉语言模型的推理效果和任务成功率。</p><br><br><p><strong>摘要：</strong> 本研究探讨了使用可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中训练目标导向推理的有效性。通过在复杂的卡片游戏和ALFWorld的具身任务上的实验，我们发现仅基于行动结果的奖励无法有效激励VLM的推理能力，反而导致一种被称为思维崩溃的现象：代理的思维多样性快速下降，产生与状态无关及不完整的推理，进而导致无效的行动和负面奖励。为了解决这一问题，我们提出了一种自动纠正器，确保在每一步中对代理推理进行评估和完善，以避免思维崩溃。我们的GTR（引导思维强化）框架能够同时训练推理与行动，而无需密集的人类标注。实验结果表明，GTR显著提升了LLaVA-7b模型在多种视觉环境中的表现和泛化能力，相比于现有最先进模型，其任务成功率提高了3-5倍，同时模型规模更小。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.08525 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 11:17:02 GMT</pubDate>
<pubDate>Tue, 11 Mar 2025 11:17:02 GMT</pubDate>
</item>
<item>
<title>高效遥感图像的视觉语言理解方法</title>
<link>https://arxiv.org/abs/2503.07588</link>
<guid>https://arxiv.org/abs/2503.07588</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出一种高效处理大规模遥感图像的视觉语言理解方法。</p><br><br><p><strong>摘要：</strong> 本文针对大规模遥感图像（RSIs）的高效视觉语言理解提出了一种新方法，旨在在保持图像细节的同时降低计算复杂度。我们引入了一种文本指导的令牌修剪方法，结合动态图像金字塔（DIP）。该方法包括一个区域聚焦模块（RFM），能够识别关键信息，同时采用基于DIP的粗细结合的图像块选择和令牌修剪策略，有效避免直接处理整个大幅图像。此外，针对现有领域评估指标的不足，我们构建了新的基准数据集LRS-VQA，包含7333对QA，支持高达27328像素的图像长度。实验结果表明，我们的方法在四个数据集上超越了现有高分辨率策略，并在高分辨率设置下显示出更好的效率。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.07588 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:51:16 GMT</pubDate>
<pubDate>Mon, 10 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>优化LLM的量化技术以提高代码生成效率</title>
<link>https://arxiv.org/abs/2503.07103</link>
<guid>https://arxiv.org/abs/2503.07103</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究更大规模LLM的量化技术，以减少内存占用并保持性能。</p><br><br><p><strong>摘要：</strong> 本文研究了大型语言模型(LLM)在代码生成中的应用，重点关注量化技术以减少内存占用。前期工作探讨了16B参数的LLM，量化精度从32位浮点降低至8位整数，而本研究则对新的、参数高达34B的LLM进行更深入的复制研究。我们采用最新的量化技术，将压缩推向2位量化，考察不同校准数据集的效果。实证分析显示，4位量化精度可实现70%的内存减少，同时性能未显著下降；在更极端的3位和2位量化下，使用代码特定的校准数据集有助于限制性能损失。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.07103 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:26:08 GMT</pubDate>
<pubDate>Mon, 10 Mar 2025 05:26:08 GMT</pubDate>
</item>
<item>
<title>WildIFEval: 一个多约束用户指令评估数据集</title>
<link>https://arxiv.org/abs/2503.06573</link>
<guid>https://arxiv.org/abs/2503.06573</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>WildIFEval是一个包含12000个多约束用户指令的数据集，旨在提升LLM的性能。</p><br><br><p><strong>摘要：</strong> 本文介绍了WildIFEval，这是一个包含12000个真实用户指令的大规模数据集，涵盖了多样化的多约束条件。与以往的数据集不同，我们的收集涵盖了广泛的词汇和主题约束，反映了自然用户提示中的复杂性。我们将这些约束分为八个高层类别，以捕捉它们在现实场景中的分布和动态。通过使用WildIFEval，我们对多个领先的语言模型（LLMs）进行了广泛实验，以基准评估它们的指令遵循能力。结果表明，所有模型在约束数量增加时性能均有下降，显示了改进空间。此外，我们发现约束的具体类型对模型性能起着关键作用。我们发布这一数据集旨在促进关于在复杂、现实条件下的指令遵循的进一步研究。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.06573 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:06:29 GMT</pubDate>
<pubDate>Sun, 09 Mar 2025 08:06:29 GMT</pubDate>
</item>
<item>
<title>文档数量对检索增强生成性能的影响研究</title>
<link>https://arxiv.org/abs/2503.04388</link>
<guid>https://arxiv.org/abs/2503.04388</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究了RAG设置中文档数量对LLM性能的影响。</p><br><br><p><strong>摘要：</strong> 本研究探讨了检索增强生成（RAG）方法中，文档数量如何影响大型语言模型（LLM）的性能。以多跳问答任务为基础，我们在自定义数据集上进行评估，保持上下文长度和相关信息位置不变的情况下，调整文档数量。结果表明，增加文档数量在RAG设置中给LLM带来了显著挑战。此外，结果还指出，处理多个文档是一个独立于处理长上下文的挑战。我们将提供的数据集和代码可在GitHub上获取，进一步促进相关研究的发展。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2503.04388 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:38:17 GMT</pubDate>
<pubDate>Thu, 06 Mar 2025 07:38:17 GMT</pubDate>
</item>

<item>
<title>TPDiff: 高效视频扩散模型的多阶段训练框架</title>
<link>https://arxiv.org/abs/2503.09566</link>
<guid>https://arxiv.org/abs/2503.09566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPDiff框架通过多阶段扩散提高视频模型的训练和推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出TPDiff框架，以解决视频扩散模型计算需求大的挑战。我们发现扩散的反向过程具有内在的降低熵的特性，利用视频模态中帧间的冗余性，可以在高熵阶段不必要地维持完整的帧率。TPDiff框架通过将扩散过程分为多个阶段，有效地渐进提升帧率，最终阶段才使用完整帧率，从而优化计算效率。为训练这一多阶段扩散模型，我们引入了分阶段扩散的专门训练框架，通过对齐的数据和噪声解决分区概率流常微分方程（ODE），使训练策略适用于多种扩散形式，并进一步增强训练效率。实验证明，该方法具有广泛适用性，能够减少50%的训练成本，以及提高1.5倍的推理效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:33:22 GMT</pubDate>
</item>
<item>
<title>增强一致性的潜在扩散模型（AF-LDM）</title>
<link>https://arxiv.org/abs/2503.09419</link>
<guid>https://arxiv.org/abs/2503.09419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文重构潜在扩散模型以提高生成一致性，提出了抗混叠的LDM。</p><br /><br /><p><strong>摘要：</strong> 潜在扩散模型（LDMs）在生成过程中面临不稳定性，小的输入噪声扰动会导致输出显著不同，限制了其在需要一致性结果的应用中的使用。为此，本文设计了一种转变为一致性的LDM，通过引入抗混叠操作来改善一致性，但由于LDMs面临的独特挑战，仍然存在显著的混叠及不一致问题。特别是，变分自编码器（VAE）训练和多个U-Net推理过程中的混叠放大，再加上自注意力模块固有地缺乏一致性。为解决这些问题，本文重构了注意力模块，使其具备一致性，并提出了一种有效抑制连续域中特征频带的均匀性损失。最终得到的不混叠LDM（AF-LDM）展现出强一致性和对不规则形变的鲁棒性。大量实验表明，AF-LDM在视频编辑和图像到图像转换等多种应用中显著优于传统的LDM。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:16:30 GMT</pubDate>
</item>
<item>
<title>VLog: 一种基于语言模型的视频理解框架</title>
<link>https://arxiv.org/abs/2503.09402</link>
<guid>https://arxiv.org/abs/2503.09402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLog框架通过新颖的事件词汇实现高效视频叙述生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的视频理解框架VLog，该框架将视频叙述定义为一种词汇，从而超越了现有生成性视频语言模型中的词汇方式。VLog基于轻量级语言模型GPT-2，具备三项关键创新：首先，提出了一种生成检索模型，将语言模型的复杂推理能力与对比检索的高效相似性搜索相结合；其次，利用大规模视频叙述构建的分层词汇，通过叙述对编码算法（narration pair encoding）实现对具体事件（如切西红柿）与更广泛场景（如厨房）的高效索引；最后，通过生成模型的词汇更新策略，扩展在推理过程中遇到的新事件的词汇。实验结果在EgoSchema、COIN和HiREST数据集上显示VLog在生成简洁、上下文准确且高效的叙述方面的有效性，提供了对视频理解的全新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 09:53:30 GMT</pubDate>
</item>
<item>
<title>Reangle-A-Video：新型同步多视角视频生成框架</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Reangle-A-Video框架，实现从单视频生成同步多视角视频。</p><br /><br /><p><strong>摘要：</strong> Reangle-A-Video是一种统一框架，用于从单一输入视频生成同步的多视角视频。与主流的在大规模4D数据集上训练多视角视频扩散模型的方法不同，我们的方法将多视角视频生成任务重新表述为视频到视频的转换，利用公开可用的图像和视频扩散先验。该方法分为两个阶段：首先是多视角运动学习，通过自我监督的方式同步微调图像到视频的扩散变换器，从一组变形视频中提取视角不变的运动；其次是通过DUSt3R进行多视角一致的图像到图像转换，在推断时通过交叉视图一致性指导，对输入视频的第一帧进行变形和修复，生成多视角一致的起始图像。大量实验表明，Reangle-A-Video在静态视角转化和动态相机控制任务中超过了现有方法，为多视角视频生成提供了新解决方案，并计划公开发布代码和数据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 04:26:15 GMT</pubDate>
</item>
<item>
<title>Motion Anything：一种多模态运动生成框架</title>
<link>https://arxiv.org/abs/2503.06955</link>
<guid>https://arxiv.org/abs/2503.06955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Motion Anything框架，解决多模态运动生成中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> Conditional motion generation在计算机视觉领域得到广泛研究，但仍面临两大挑战：一是现有的遮罩自回归方法缺乏优先关注动态帧和身体部位的机制；二是不同条件模态的方法常常无法有效集成多个模态，限制了生成运动的控制性和一致性。为了解决这些问题，我们提出了Motion Anything，一个引入基于注意力的遮罩建模方法的多模态运动生成框架。该框架能够对关键帧和动作进行细粒度的空间和时间控制，并通过自适应编码多模态条件（包括文本和音乐）来改善可控性。此外，我们还发布了Text-Music-Dance (TMD)新数据集，包含2153对文本、音乐和舞蹈，是AIST++的两倍，填补了这一领域的关键空白。实验表明，Motion Anything在多个基准测试中超过了现有的先进方法，在HumanML3D上的FID指标提升了15%，在AIST++和TMD上也表现出了一致的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:04:31 GMT</pubDate>
</item>
<item>
<title>块扩散语言模型：突破生成限制的新方法</title>
<link>https://arxiv.org/abs/2503.09573</link>
<guid>https://arxiv.org/abs/2503.09573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">块扩散语言模型通过灵活长度生成提升了性能并改善了推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的块扩散语言模型，该模型在离散去噪扩散和自回归模型之间插值，克服了两者的关键限制。块扩散模型支持灵活长度生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了构建有效块扩散模型的方案，其中包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。块扩散在语言建模基准测试中设定了扩散模型的新最佳性能，并实现了任意长度序列的生成。相关代码及模型权重已在项目页面提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:43:40 GMT</pubDate>
</item>
<item>
<title>scMMGPT：结合细胞与文本建模的单细胞多模态生成预训练变换器</title>
<link>https://arxiv.org/abs/2503.09427</link>
<guid>https://arxiv.org/abs/2503.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">scMMGPT是一种新型的单细胞多模态预训练模型，提升细胞与文本的联合建模能力。</p><br /><br /><p><strong>摘要：</strong> 单细胞分析领域中的预训练语言模型（PLMs）应用受到限制，现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，导致在多模态任务中应用受限。为解决这一问题，本文提出了单细胞多模态生成预训练变换器（scMMGPT），它有效地整合了最先进的细胞与文本PLMs，促进了跨模态知识共享。scMMGPT通过专门的跨模态投影器来弥合文本和细胞之间的模态差距，并在2700万个细胞上进行了大规模的预训练，这是迄今为止最大的数据集，使其在细胞与文本的联合任务中表现卓越，生成细胞描述的文本差异相对改善84%，细胞类型注释准确度提升20.5%，文本条件下伪细胞生成的k-NN准确度提升4%，均超越了基准模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:26:16 GMT</pubDate>
</item>
<item>
<title>PlainQAFact框架在医疗领域中的事实性评估</title>
<link>https://arxiv.org/abs/2503.08890</link>
<guid>https://arxiv.org/abs/2503.08890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlainQAFact框架提高了医疗领域中的平易语言摘要的事实性评估。 </p><br /><br /><p><strong>摘要：</strong> 本文介绍了PlainQAFact框架，旨在解决语言模型在医疗领域产生的幻觉输出对普通观众的风险，特别是在产生平易语言摘要（PLS）时。现有的事实性评估方法，如基于前提关系和问答的评估，无法有效处理PLS生成中的详细解释现象。这种现象会引入源文档中不存在的外部内容（例如定义、背景和示例）以增强理解。PlainQAFact框架基于一个细致的人类标注数据集PlainFact进行训练，通过首先分类事实性类型，然后使用检索增强问答的方法评估事实性。我们的研究表明，PlainQAFact在处理PLS中的事实性评估时，优于现有的事实性指标，在多个外部知识源和文档粒度水平下均取得了卓越的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 16:59:53 GMT</pubDate>
</item>
<item>
<title>引导矩匹配模型：快速稳定的生成模型</title>
<link>https://arxiv.org/abs/2503.07565</link>
<guid>https://arxiv.org/abs/2503.07565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出引导矩匹配(IMM)模型，实现快速稳定的生成样本。</p><br /><br /><p><strong>摘要：</strong> 引导矩匹配(Inductive Moment Matching, IMM)是一种新型生成模型，旨在解决扩散模型和流匹配模型在推理速度与样本质量之间的权衡。与传统的模型蒸馏不同，IMM无需对两个网络进行预训练和优化，且能够在单阶段训练过程中进行一步或少步采样，确保在各类超参数和标准模型架构下的稳定性。与扩散模型相比，IMM在ImageNet-256x256数据集上取得1.99的FID，仅需8次推理步数；在CIFAR-10上，IMM训练出的模型实现了1.98的最先进的2步FID，展示了其卓越的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:37:39 GMT</pubDate>
</item>
<item>
<title>多模态智能的推理优先视角及生成预训练算法的创新</title>
<link>https://arxiv.org/abs/2503.07154</link>
<guid>https://arxiv.org/abs/2503.07154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨推理优先视角如何推动生成预训练算法创新。</p><br /><br /><p><strong>摘要：</strong> 近年来，基础模型在生成预训练方面取得了显著进展，但在算法创新上主要局限于自回归模型和扩散模型，导致结合多模态数据的潜力未能充分释放，从而限制了多模态智能的发展。本文提出一种推理优先的视角，通过在推理阶段优先考虑规模效率，能够激发新型生成预训练算法的灵感。以归纳动量匹配（IMM）为实例，探索如何通过针对性修改扩散模型的推理过程，获得稳定的单阶段算法，从而在样本质量上显著提升，并实现超过一个数量级的推理效率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 06:27:30 GMT</pubDate>
</item>
<item>
<title>通过容量感知推理优化混合专家模型的效率</title>
<link>https://arxiv.org/abs/2503.05066</link>
<guid>https://arxiv.org/abs/2503.05066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出容量感知推理技术，提升混合专家模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 混合专家（MoE）架构通过稀疏专家激活在扩展大型语言模型上具有显著成效，但在专家并行情况下，由于令牌到专家的分配不平衡，导致一些专家过载而其他专家未被充分利用，进而影响推理效率。针对这一问题，提出了容量感知推理的方法，包括两个关键技术：容量感知令牌丢弃和容量感知令牌重新分配。这些技术旨在优化高负载和低负载专家的利用，提升整体推理管道的效率。实验结果显示，这些方法在推理效率上取得了显著改进，例如在Mixtral-8times7B-Instruct模型上实现了0.2%的平均性能提升和1.94倍的推理速度提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 20:11:39 GMT</pubDate>
</item>
<item>
<title>深度检索模型中的偏见与鲁棒性研究</title>
<link>https://arxiv.org/abs/2503.05037</link>
<guid>https://arxiv.org/abs/2503.05037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了深度检索模型中的偏见对信息检索性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了深度检索模型在信息检索应用中的鲁棒性，特别是在检索增强生成（RAG）场景下的表现。通过重新利用关系提取数据集（例如Re-DocRED），设计了控制实验，量化了启发式偏见（如偏爱短文档）对检索器（如Dragon+和Contriever）的影响。研究发现，检索器往往依赖于表面模式，比如过度优先考虑文档开头、短文档、重复实体和字面匹配等，同时忽视文档是否包含查询的答案，缺乏深入的语义理解。尤其是当多种偏见组合时，模型表现显著下降，答案文档被选中的概率不足3%。此外，这些偏见对下游应用（如RAG）有直接影响，偏好检索的文档可能误导大型语言模型（LLMs），造成34%的性能下降，甚至比不提供文档的情况还糟糕。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 18:23:13 GMT</pubDate>
</item>
<item>
<title>OTTER：一种新的视觉-语言-行动模型实现有效的机器人操作</title>
<link>https://arxiv.org/abs/2503.03734</link>
<guid>https://arxiv.org/abs/2503.03734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OTTER是一个新型模型，通过提取语义对齐的视觉特征增强机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> 视觉-语言-行动（VLA）模型旨在根据视觉观察和语言指令预测机器人动作。传统方法通过微调预训练的视觉语言模型进行操作，但由于独立输入视觉和语言特征，导致预训练语义对齐性能下降。为了解决这一问题，我们提出了OTTER，一个新的VLA架构，该架构通过明确的文本感知视觉特征提取，利用现有的预训练语义对齐。OTTER不再处理所有视觉特征，而是选择性提取与任务相关的、与语言指令语义对齐的视觉特征传递给政策变换器，从而保持预训练视觉-语言编码器的固定。这种方法保护并利用了大规模预训练所获取的丰富语义理解，具备强大的零样本泛化能力。通过模拟和真实世界的实验，OTTER显著超越现有的VLA模型，展示了对新物体和环境的强大零样本泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 13:44:48 GMT</pubDate>
</item>
<item>
<title>指令跟随检索器的安全风险研究</title>
<link>https://arxiv.org/abs/2503.08644</link>
<guid>https://arxiv.org/abs/2503.08644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究指令跟随检索器处理恶意查询的安全风险。</p><br /><br /><p><strong>摘要：</strong> 本文研究了指令跟随检索器在处理恶意查询时的安全风险。我们对包括NV-Embed和LLM2Vec在内的六种领先检索器进行了实证分析，发现大多数检索器在面对恶意请求时能够选择相关的有害内容，成功率超过50%。例如，LLM2Vec对61.35%的恶意查询选择了相关的有害段落。此外，我们揭示了一个新兴风险，即通过利用这些检索器的指令跟随能力，可以显著展示与恶意内容相关的高度相关信息。最后，即使是安全对齐的LLM，例如Llama3，当在上下文中提供有害检索段落时，也会满足恶意请求。这些发现突显了检索器能力增强所带来的恶意误用风险。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:36:53 GMT</pubDate>
</item>
<item>
<title>ObjectMover：应对复杂场景的物体移动生成模型</title>
<link>https://arxiv.org/abs/2503.08037</link>
<guid>https://arxiv.org/abs/2503.08037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectMover是一种用于复杂场景中物体移动的生成模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ObjectMover的生成模型，用于在高度复杂的场景中实现物体移动。我们将这一任务建模为序列到序列问题，并微调视频生成模型，利用其对视频帧中一致物体生成的知识。由于缺少用于物体移动的大规模数据，我们构建了一条数据生成管道，使用现代游戏引擎合成高质量的数据对。同时，我们提出了多任务学习策略，使模型能够在真实世界视频数据上进行训练，从而提升泛化能力。通过大量实验，我们证明了ObjectMover在实际场景中表现出色，能够有效处理极端的光照协调和物体效果运动。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 00:42:59 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在自驾车中人类与机器驾驶反应的比较研究</title>
<link>https://arxiv.org/abs/2503.07587</link>
<guid>https://arxiv.org/abs/2503.07587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了自驾车中多模态模型与人类驾驶者的反应差异。</p><br /><br /><p><strong>摘要：</strong> 随着多模态基础模型在自驾车中的实验应用，本文研究了这些系统在特定驾驶情境中的反应与人类的相似程度。通过构建Robusto-1数据集，利用秘鲁的行车记录视频数据，该地区以其复杂的交通情况和异常街道物体著称。我们使用多模态视觉问答（VQA）的方法，对人类与基础视觉语言模型（VLMs）进行比较，采用系统神经科学中的表征相似性分析（RSA）方法。研究表明，不同问题类型对人机表现的影响显著，揭示了二者在认知对齐上的差距，具体分析了它们在回答不同问题时的趋同与分歧。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:50:04 GMT</pubDate>
</item>
<item>
<title>CineBrain：首个动态视听刺激下的EEG与fMRI同步记录数据集</title>
<link>https://arxiv.org/abs/2503.06940</link>
<guid>https://arxiv.org/abs/2503.06940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineBrain数据集结合EEG和fMRI，推动视听刺激重建进展。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了CineBrain，这是首个大规模数据集，记录了在动态视听刺激下的EEG与fMRI同步信号。该数据集包含来自热门剧集《生活大爆炸》的六小时叙事内容，涵盖六名参与者，充分发挥了EEG的高时间分辨率与fMRI的深脑空间覆盖的互补优势。基于CineBrain，我们提出了CineSync，这是一种创新的多模态解码框架，结合了多模态融合编码器与基于扩散的神经潜在解码器，有效融合EEG和fMRI信号，显著提高复杂视听刺激的重建质量。为了进行严格评估，我们引入了Cine-Benchmark，一个全面的评估协议，用于评估语义和感知维度的重建效果。实验结果表明，CineSync在视频重建性能方面达到了领先水平，并展示了我们在结合fMRI和EEG重建视频及音频刺激方面的初步成功。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 01:39:43 GMT</pubDate>
</item>
<item>
<title>源偏差与PLM基础检索模型的因果分析</title>
<link>https://arxiv.org/abs/2503.08684</link>
<guid>https://arxiv.org/abs/2503.08684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨PLM基础检索模型中源偏差产生的原因及解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于预训练语言模型（PLM）的检索模型在处理信息时产生的源偏差现象，分析其根源及解决方法。研究表明，PLM基础检索者通过学习困惑度特征进行相关性估计，导致低困惑度文档被赋予更高的相关性评分。通过理论分析发现，这种现象与语言建模任务和检索任务的损失函数梯度之间的正相关性有关。为此，提出了一种名为因果诊断与修正（CDC）的去偏差方法，首先诊断困惑度的偏差影响，然后将其从整体估计的相关性评分中分离出去。实验结果表明，CDC在三个领域的去偏效果优于其他方法，验证了本文提出的解释框架的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08684" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:00 GMT</pubDate>
</item>
<item>
<title>AnyMoLe：一种无数据集依赖的角色运动插值方法</title>
<link>https://arxiv.org/abs/2503.08417</link>
<guid>https://arxiv.org/abs/2503.08417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyMoLe利用视频扩散模型实现无数据集依赖的角色运动插值。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了AnyMoLe，一种创新的方法，旨在解决基于学习的运动插值中的数据集特定要求。AnyMoLe利用视频扩散模型，为任意角色生成插值帧，省去外部数据的需求。该方法采用两阶段的帧生成过程，提升了上下文理解能力。此外，研究引入了ICAdapt，这是一种针对视频扩散模型的微调技术，用于缩小真实世界与渲染角色动画之间的域差距。同时，我们提出了一种“运动视频模仿”优化技术，使得对于具有任意关节结构的角色能够顺畅生成运动，结合了2D和3D特征。AnyMoLe显著降低了对数据的依赖，为各种运动插值任务提供了平滑且逼真的过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 09:28:59 GMT</pubDate>
</item>
<item>
<title>提升计算机视觉中的个体识别能力：RexSeek模型与HumanRef数据集</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RexSeek模型，旨在提升个体识别的准确性并应对现实应用中的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于计算机视觉中个体识别的重要性，定义了一个新任务，即基于自然语言描述识别个体。尽管现有模型在一些基准测试上表现良好，但在实际应用中却缺乏有效性。为了应对这一挑战，本文从任务定义、数据集设计和模型架构三个方面进行研究，提出HumanRef数据集以更好地反映现实应用场景。我们构建了集成多模态大语言模型与物体检测框架的RexSeek模型，实验结果表明，尽管一些先进模型在常用基准如RefCOCO上表现出色，但在HumanRef数据集上却遇到困难，主要因为它们无法有效检测多个个体。而RexSeek在个体识别上表现优异，并能够有效推广至常见物体的识别任务，显示出广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:57:14 GMT</pubDate>
</item>
<item>
<title>一种无训练的人脸匿名化方法</title>
<link>https://arxiv.org/abs/2503.08478</link>
<guid>https://arxiv.org/abs/2503.08478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的人脸匿名化方法，有效保留非身份属性。</p><br /><br /><p><strong>摘要：</strong> 针对隐私担忧不断增加的背景下，本文提出了一种训练-free的人脸匿名化方法，旨在保留非身份相关的关键属性。该方法利用预训练的文本到图像扩散模型，能够在不需要优化或训练的情况下进行操作。具体而言，方法通过反转输入图像来恢复初始噪声，然后通过身份条件的扩散过程去噪，以确保匿名后的面孔与原身份明显不同。此外，系统支持局部匿名化，用户可以控制待匿名化或保留的面部区域。经过与最先进的方法的全面评估，该方法在匿名化、属性保留和图像质量方面表现优异，展示出良好的灵活性、鲁棒性和实用性，适合于实际应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:29:37 GMT</pubDate>
</item>
<item>
<title>一种新型Transformer架构用于音视频生成的研究</title>
<link>https://arxiv.org/abs/2503.08307</link>
<guid>https://arxiv.org/abs/2503.08307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新型Transformer架构，解决音视频生成中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的Transformer架构，用于解决生成AI中音视频（AV）生成面临的三大挑战：生成样本的质量、无缝的多模态同步与时间一致性，以及无限视频时长。我们探讨了三种不同的跨模态交互模块，其中轻量级时间融合模块被证明是对齐音频和视觉模态的高效有效方法。实验结果显示，所提出的方法在多模态音视频生成任务中超越了现有的最先进模型，为音视频生成领域提供了新的视角和技术。代码和模型检查点已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 07:18:47 GMT</pubDate>
</item>
<item>
<title>智能记忆管理系统SECOND ME的创新应用</title>
<link>https://arxiv.org/abs/2503.08102</link>
<guid>https://arxiv.org/abs/2503.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SECOND ME重塑用户交互中记忆管理的方式，大幅提升信息处理效率。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SECOND ME，一个利用大型语言模型(LLMs)的智能记忆管理系统，其目标是提升用户与外界的互动效率。人们在与其他个体、网站及未来的AI代理交互时，常需重复提供相同信息，SECOND ME通过作为用户交互的中介，有效减少了这一冗余。该系统不仅能够自动生成环境感知的响应，自动填充所需信息，还能与外部系统无缝沟通，从而降低用户的认知负担。与传统的记忆存储解决方案相比，SECOND ME通过LLM驱动的记忆参数化，提供了结构化的组织、上下文推理和自适应知识检索。这一创新代表了朝向更智能化和系统化的记忆管理的重要进步，为AI驱动的个人代理在数字生态系统中的角色奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:05:52 GMT</pubDate>
</item>
<item>
<title>结合大语言模型与神经机器翻译的高效模型</title>
<link>https://arxiv.org/abs/2503.06594</link>
<guid>https://arxiv.org/abs/2503.06594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大语言模型在神经机器翻译中的应用与优化。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何将大语言模型（LLMs）有效地应用于神经机器翻译（NMT），旨在设计一种通用、高效且易于优化的翻译模型。我们保留了传统NMT模型中的解码器，仅在编码阶段引入LLMs，并发展了一些方法来进一步改进其与解码器的适配性。此外，构建了一个新的多任务数据集，以评估机器翻译系统在不同任务中的泛化能力。经过在WMT及其他数据集上的评估，结果表明我们的方法在翻译质量上与多种基线相匹配或更优，同时实现了2.4到6.5倍的推理速度提升，并将KV缓存的内存占用降低了75%。这表明所提方法在各种翻译相关任务中表现出强大的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:54:05 GMT</pubDate>
</item>
<item>
<title>VisualSimpleQA：一项针对大规模视觉语言模型的多模态基准评测</title>
<link>https://arxiv.org/abs/2503.06492</link>
<guid>https://arxiv.org/abs/2503.06492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualSimpleQA为大规模视觉语言模型提供了一种新的多模态评测基准。</p><br /><br /><p><strong>摘要：</strong> 大规模视觉语言模型（LVLMs）在生成非事实回应方面仍面临挑战。当前的多模态事实寻求基准主要关注模型输出与真实答案的比较，但对模态特定模块的表现洞察有限。为了解决这一问题，我们推出了VisualSimpleQA，一个多模态事实寻求基准，具备两个关键特点：首先，它支持对LVLMs在视觉和语言模态中的脱钩评估；其次，它采用明确的难度标准来指导人类标注，并提取出更具挑战性的子集VisualSimpleQA-hard。对15个LVLM的实验表明，即使是最先进的模型如GPT-4o，在VisualSimpleQA上的正确率仅为60%以上，而在VisualSimpleQA-hard上的正确率也仅为30%以上。这种解耦评估揭示了在视觉和语言模块中都有显著的改进空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 03:25:32 GMT</pubDate>
</item>
<item>
<title>MagicInfinite：高保真多角色肖像动画的新方法</title>
<link>https://arxiv.org/abs/2503.05978</link>
<guid>https://arxiv.org/abs/2503.05978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicInfinite通过创新技术实现高保真的肖像动画。</p><br /><br /><p><strong>摘要：</strong> MagicInfinite是一种新型扩散Transformer框架，克服了传统肖像动画的局限，能够在现实人类、全身角色和风格化动漫角色等多种角色类型中实现高保真动画效果。该框架支持多种面部姿势，并通过输入掩码精确指定多角色场景中的说话者。其创新之处在于：1）采用滑动窗口去噪策略的3D全注意力机制，实现在多种角色风格下的无限视频生成；2）两阶段学习方案结合音频、文本和参考图像，实现灵活的多模态控制；3）使用区域特定掩码和自适应损失函数平衡全局文本控制和局部音频指导。得益于统一步骤和cfg蒸馏技术，效率显著提升，实现20倍的推理速度提升。评估结果显示，MagicInfinite在音频口型同步和运动自然性方面表现优越。该项目已公开发布，感兴趣的用户可访问官方网站获取更多信息。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 18:21:11 GMT</pubDate>
</item>
<item>
<title>AI4SE基准的评估与优化：BenchScout和BenchFrame的应用</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了AI4SE基准的现状，并提出了BenchScout和BenchFrame以优化基准质量。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能在软件工程领域的应用日益广泛，基准测试成为评估和可重复性的关键。然而，现有的基准面临着碎片化知识、选择困难、缺乏统一标准及局限性等挑战。本文回顾了173项研究，识别出204个AI4SE基准，并分析了这些基准的局限性及实践中的空白。为此，研究团队开发了BenchScout，一款支持语义搜索的工具，旨在帮助用户快速找到相关基准。此外，提出了BenchFrame，以统一方法提升基准质量，并以HumanEval基准为案例，解决其主要局限性，从而生成HumanEvalNext，改进了语言转换、测试覆盖和难度。评估表明，HumanEvalNext的通过率明显低于以往基准，提示了后续研究的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>QuoTA：基于查询重要性评估的视频标记分配模型</title>
<link>https://arxiv.org/abs/2503.08689</link>
<guid>https://arxiv.org/abs/2503.08689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QuoTA通过查询导向评估优化视频标记分配，提高长视频理解效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了QuoTA，一种训练无关的模块，旨在基于查询导向的帧级重要性评估优化视觉标记的分配。当前技术侧重于解码器层的低响应标记剪枝，而忽视了视觉标记与指令之间的输入层次语义相关性。QuoTA通过查询相关性战略性地分配帧级重要性分数，使视觉标记分配在跨模态交互之前进行，从而提高了标记预算的利用率并保留了语义相关的内容。此外，QuoTA通过链式思维推理拆分查询，以精确评分，且为现有的大型视频语言模型提供了即插即用的功能。实验结果表明，QuoTA在LLaVA-Video-7B上实施后，在六个基准测试（包括Video-MME和MLVU）中平均提升了3.2%的表现，同时在与基线相同的视觉标记预算内运行。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08689" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>OmniMamba：首个线性架构的多模态生成模型</title>
<link>https://arxiv.org/abs/2503.08686</link>
<guid>https://arxiv.org/abs/2503.08686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniMamba是一个高效的多模态生成模型，显著减少计算复杂度和数据需求。</p><br /><br /><p><strong>摘要：</strong> OmniMamba是一种创新的线性架构多模态生成模型，采用统一的下一个token预测范式，能够同时生成文本和图像，克服了传统模型的二次计算复杂度和对大规模训练数据的依赖。该模型在Mamba-2的基础上提升了计算和内存效率，并通过引入解耦词汇以指导特定模态生成及用于参数高效自适应的任务特定LoRA，解决了现有统一模型的数据低效问题。OmniMamba还采用了分阶段的训练策略，以降低两个任务间的数据不平衡。尽管仅使用了200万对图像-文本数据，OmniMamba在多个基准测试中达到了与JanusFlow相媲美的性能，并超越了Show-o，表现出卓越的推理效率，相较于基于Transformer的模型，长序列生成的速度提升高达119.2倍，GPU内存减少63%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>YuE：创新的长篇音乐生成模型</title>
<link>https://arxiv.org/abs/2503.08638</link>
<guid>https://arxiv.org/abs/2503.08638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YuE是一种新型模型，可将歌词生成长达五分钟的音乐。</p><br /><br /><p><strong>摘要：</strong> YuE模型专注于长篇音乐生成，尤其是歌词到歌曲的转换，基于LLaMA2架构，能够处理数万亿个令牌，生成最多五分钟的音乐，同时保持歌词对齐、连贯的音乐结构和动听的旋律。其主要技术创新包括：追踪解耦的下一个token预测、结构性渐进条件提供长文本上下文对齐，以及多任务、多阶段的预训练方案。YuE还重塑了音乐生成的上下文学习技术，支持多样化风格转换和双向生成。通过大量评估，YuE在音乐性和声乐灵活性上与一些专有系统相当或更优。此外，微调YuE可增强控制能力，支持小众语言，且在音乐理解任务中表现出色，超过了MARBLE基准上的现有最先进技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:26:50 GMT</pubDate>
</item>
<item>
<title>新的人类类掩膜标注任务：提升多模态大语言模型的像素理解能力</title>
<link>https://arxiv.org/abs/2503.08625</link>
<guid>https://arxiv.org/abs/2503.08625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HLMAT任务提升MLLM的像素级理解与标注能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLMs）在图像理解方面表现出色，但它们在像素级理解上仍面临挑战。现有的评估任务如视觉问答（VQA）和视觉定位过于粗糙，无法准确评估细粒度的像素理解。为了解决这些问题，本文引入了人类类掩膜标注任务（HLMAT），一个新兴的范式，使MLLMs能够模拟人类标注者，通过交互式分割工具进行标注。HLMAT将分割建模为多步骤的马尔科夫决策过程，使MLLMs能够迭代生成基于文本的点击点，从而高质量地生成掩膜，而无需改变架构或生成隐式标记。同时，我们开发的SegAgent模型经过人类类注释路径的微调，其表现与最先进的方法相当，支持掩膜细化和注释过滤等额外任务。此外，HLMAT还提供了一种评估MLLMs细粒度像素理解的协议，为未来在细粒度视觉感知和多步骤决策方面的进展奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:08:54 GMT</pubDate>
</item>
<item>
<title>LightGen：一种高效的文本到图像生成方法</title>
<link>https://arxiv.org/abs/2503.08619</link>
<guid>https://arxiv.org/abs/2503.08619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种名为LightGen的高效图像生成训练方法。</p><br /><br /><p><strong>摘要：</strong> 最近的文本到图像生成进展依赖于庞大的数据集和复杂的架构，限制了资源不足的研究者的可及性。本文提出了LightGen，一种结合了知识蒸馏（KD）和直接偏好优化（DPO）的高效训练范式。该方法从最先进的文本到图像模型中提取知识，构建了仅有0.7B参数的紧凑型自回归架构。通过使用仅2百万张高质量合成图像的小型合成数据集，展示了数据多样性对模型性能的影响超过数据量。LightGen显著降低了计算需求，将预训练时间从数千GPU天缩短至88GPU天。此外，为了解决合成数据的缺陷，特别是高频细节和空间准确性的问题，本文集成了DPO技术，提升了图像的真实度和位置准确性。实验结果表明，LightGen在生成图像质量上可与最先进模型相媲美，同时大幅减少了计算资源的需求，增强了对资源有限环境的可访问性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:58:02 GMT</pubDate>
</item>
<item>
<title>BiasEdit：一种去除语言模型刻板偏见的高效编辑方法</title>
<link>https://arxiv.org/abs/2503.08588</link>
<guid>https://arxiv.org/abs/2503.08588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出BiasEdit模型，通过局部参数编辑去除语言模型中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BiasEdit的高效模型编辑方法，以消除语言模型中的刻板偏见。通过使用轻量级网络作为编辑器，BiasEdit生成参数更新，采用去偏见损失指导编辑网络对语言模型部分参数进行局部编辑，同时通过保留损失保持语言模型的建模能力。实验结果表明，BiasEdit在StereoSet和Crows-Pairs数据集上表现出色，相较于传统的去偏见方法，BiasEdit在消除偏见方面展现了更高的效率和鲁棒性，同时对语言模型的整体能力影响较小。此外，本文还进行了偏见追踪，探讨不同模块中的偏见并研究去偏见编辑对语言模型不同组件的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:25:36 GMT</pubDate>
</item>
<item>
<title>Gemini Embedding：多语言嵌入模型的突破</title>
<link>https://arxiv.org/abs/2503.07891</link>
<guid>https://arxiv.org/abs/2503.07891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Embedding 通过多语言能力在多项任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Gemini Embedding，这是一种基于谷歌最强大的大型语言模型Gemini的先进嵌入模型。Gemini Embedding利用Gemini的多语种和编码理解能力，为多种语言和文本形式生成高度可泛化的嵌入表示。这些嵌入可以预先计算，并应用于分类、相似性、聚类、排序和检索等多种下游任务。在大量多语种文本嵌入基准测试（MMTEB）上进行评估时，Gemini Embedding远超之前的最先进模型，在嵌入质量上表现出显著提高。在MMTEB的多语言、英语和代码基准上，Gemini Embedding均取得了最先进的性能，展现出其在广泛任务中的强大能力，超过了专门的领域特定模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:16:45 GMT</pubDate>
</item>
<item>
<title>RayFlow: 一种提升扩散模型生成效率的新框架</title>
<link>https://arxiv.org/abs/2503.07699</link>
<guid>https://arxiv.org/abs/2503.07699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RayFlow通过引导样本路径，提升了扩散模型的生成速度与质量。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多个领域取得了显著成功，但其生成速度缓慢仍然是一个关键挑战。现有的加速方法虽然旨在减少采样步骤，但往往会降低样本质量、可控性或增加训练复杂性。为了解决这些问题，我们提出了RayFlow，这是一种新颖的扩散框架，能够沿着独特路径引导每个样本朝向特定的目标分布，从而在减少采样步骤的同时保持生成的多样性和稳定性。同时，我们引入了Time Sampler，一种重要性采样技术，以通过关注关键时间步提高训练效率。大量实验表明，与现有的加速技术相比，RayFlow在生成高质量图像时显示出更高的速度、控制能力和训练效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07699" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:20:52 GMT</pubDate>
</item>
<item>
<title>生存游戏：评估人工智能自主水平的框架</title>
<link>https://arxiv.org/abs/2502.18858</link>
<guid>https://arxiv.org/abs/2502.18858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生存游戏旨在通过试错次数评估人工智能的自主智能水平。</p><br /><br /><p><strong>摘要：</strong> 本研究提出生存游戏作为一种框架，以试错过程中的失败次数评估智能水平。我们定义的自主智能水平意味着在面对新挑战时，失败次数的期望和方差均有限。通过应用生存游戏，我们全面评估现有人工智能系统的表现，发现尽管它们在简单任务中达到了自主水平，但在视觉、搜索、推荐和语言等复杂任务中仍有较大差距。进一步分析显示，要实现一般任务的自主水平需要约10^{26}个参数，所需的计算资源成本极高，预计需要70年通过摩尔定律支持这样的规模。这一发现突显了人类任务的复杂性，并揭示了当前AI技术的不足。生存游戏不仅能够指导AI的发展，还可以深入理解人类智能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:59:45 GMT</pubDate>
</item>
<item>
<title>新型视觉标记化框架的结构化实现</title>
<link>https://arxiv.org/abs/2503.08685</link>
<guid>https://arxiv.org/abs/2503.08685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍一种新型视觉标记化框架，强调结构化的特征提取。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的视觉标记化框架，将可证明的类似主成分分析（PCA）结构嵌入潜在标记空间。与现有的视觉标记化器主要优化重建保真度的做法不同，我们的方法关注潜在空间的结构特性，这对可解释性和后续任务至关重要。该方法为图像生成一维因果标记序列，每个连续的标记以数学上保证的递减解释方差贡献不重叠的信息，确保最显著的视觉特征首先被提取。进一步，我们通过利用扩散解码器识别并解决了高层语义内容与低层光谱细节在标记中的不当纠缠效应。实验表明，我们的方法在重建性能上达到领先水平，并增强了解释性，更好地与人类视觉系统对齐。此外，基于我们标记序列训练的自回归模型在性能上与当前的最先进方法相当，但所需的标记数量较少，训练和推理更加高效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08685" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>SynCoS：一种同步耦合采样框架用于长视频生成</title>
<link>https://arxiv.org/abs/2503.08605</link>
<guid>https://arxiv.org/abs/2503.08605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的同步耦合采样方法，改善长视频生成中的一致性与流畅性。</p><br /><br /><p><strong>摘要：</strong> 随着文本到视频扩散模型的进步，短视频生成已实现高质量，但长视频生成仍面临数据有限和计算成本高的问题。为了解决这一挑战，研究者提出了调整无关的方法，如使用多个提示来实现内容的动态和可控变化。尽管这些方法在平滑帧间过渡方面有所成效，但通常会导致内容漂移和语义一致性的逐渐丧失。因此，我们提出了Synchronized Coupled Sampling（SynCoS），这一全新推理框架能够同步整个视频的去噪路径，确保邻近和远程帧之间的一致性。SynCoS结合了逆向和基于优化的采样策略，从而确保局部过渡的无缝性与全局一致性。实验表明，SynCoS显著提升了多事件长视频生成的表现，获得了更光滑的过渡和更佳的长程一致性，超越了之前的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:43:45 GMT</pubDate>
</item>
<item>
<title>UniF^2ace：用于细粒度面部理解与生成的统一多模态模型</title>
<link>https://arxiv.org/abs/2503.08120</link>
<guid>https://arxiv.org/abs/2503.08120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniF^2ace是一种新型模型，专注于细粒度面部属性的理解与生成。</p><br /><br /><p><strong>摘要：</strong> UniF^2ace是首款专门为细粒度面部理解与生成而设计的统一多模态模型，旨在克服当前面部领域对粗糙属性理解的限制。该模型在自构建的UniF^2ace-130K数据集上训练，包含130K图像-文本对和一百万个问答对，涵盖多种面部属性。通过引入离散扩散评分匹配与遮蔽生成模型之间的理论联系，UniF^2ace优化了证据下界，提高了合成面部细节的能力。同时，模型采用了基于标记和序列的专家混合架构，有效实现了对细粒度表示的学习。大量实验证明，UniF^2ace在理解和生成任务上均超越了现有的统一多模态模型和生成模型，展示了其卓越的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:34:59 GMT</pubDate>
</item>
<item>
<title>促进东南亚文化多样性：SEA-VL开放源代码计划</title>
<link>https://arxiv.org/abs/2503.07920</link>
<guid>https://arxiv.org/abs/2503.07920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEA-VL项目致力于提升东南亚语言在视觉-语言研究中的文化相关性。</p><br /><br /><p><strong>摘要：</strong> 东南亚在语言和文化上具有卓越的多样性，但在视觉-语言研究中却明显被低估，导致人工智能模型无法捕捉到该地区的文化细微差别。为填补这一空白，本项目提出了SEA-VL，一个开放源代码的倡议，旨在开发高质量、具有文化相关性的数据，确保东南亚语言的更多包容性。通过与东南亚国家的贡献者合作，项目不仅进行众包，还探索通过图像抓取和生成自动收集文化相关图像的方式。研究发现，图像抓取实现了约85%的文化相关性，且比众包更经济、高效；然而，尽管生成模型取得了显著进展，合成图像在准确反映东南亚文化方面仍然不够可靠。最终，我们收集了128万张文化相关图像，远超现有数据集的规模，通过SEA-VL，旨在弥补东南亚在视觉-语言研究中的代表性差距，促进更具包容性的人工智能系统的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 19:54:52 GMT</pubDate>
</item>
<item>
<title>VidDiff：识别视频中细微动作差异的新任务与基准</title>
<link>https://arxiv.org/abs/2503.07860</link>
<guid>https://arxiv.org/abs/2503.07860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidDiff允许识别同一动作的细微视频差异，推动技能学习与训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Video Action Differencing（VidDiff），这是一项识别同一动作视频细微差异的新任务，具有教练和技能学习等多项应用。为了支持这一任务的发展，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个具体动作差异和2075个差异发生的时间戳。实验结果显示，VidDiffBench对现有大型多模态模型（如GPT-4o和Qwen2-VL）构成了显著挑战。通过分析这些模型在VidDiffBench上的失败案例，我们指出了两个主要挑战：在两个视频中定位相关的子动作，以及进行细粒度的帧比较。为了解决这些问题，我们提出了VidDiff方法，一种将任务分为三个阶段的代理工作流程：动作差异提议、关键帧定位和帧差异比对，每个阶段都使用专门的基础模型。为了促进未来在这项新任务上的研究，我们将基准数据集以及代码放在了相关链接中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 17:18:32 GMT</pubDate>
</item>
<item>
<title>Seedream 2.0：双语图像生成模型的进步</title>
<link>https://arxiv.org/abs/2503.07703</link>
<guid>https://arxiv.org/abs/2503.07703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedream 2.0 是一款优化的双语图像生成模型，解决了现有模型的局限性。</p><br /><br /><p><strong>摘要：</strong> Seedream 2.0 是一种新兴的中文-英文双语图像生成基础模型，旨在克服现有模型的偏见、文本渲染能力不足和对中国文化细微差别理解不充分的问题。该模型支持中英文双语图像生成，凭借强大的数据系统和准确丰富的图像描述能力，能够有效处理双语文本。Seedream 2.0 结合自研的双语大语言模型作为文本编码器，能够从海量数据中直接学习本土知识，生成高保真图像，准确反映所描述的文化细微差别和美学表达。此外，通过灵活的字形对齐ByT5和多阶段后训练优化（如SFT和RLHF迭代），该模型在多个方面展现出一流的性能，包括响应提示、审美、文本渲染和结构准确性。Seedream 2.0已经对人类偏好进行了优化，以实现最佳的输出对人类期望的对齐，并具备作为指令基础图像编辑模型的能力，具有强大的编辑能力与图像一致性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>语言模型的隐式推理能力与多步骤推理的研究</title>
<link>https://arxiv.org/abs/2503.07604</link>
<guid>https://arxiv.org/abs/2503.07604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究语言模型在多步骤数学推理中隐式推理的表现及其局限性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨语言模型在多步骤数学推理任务中的隐式推理能力。通过从头训练GPT-2并使用精心策划的多步骤推理数据集进行实验，我们发现语言模型可以通过隐式推理实现逐步推理，并在域内和域外测试中取得高准确率。这种能力仅在使用固定模式的数据时出现，而对非固定模式数据的训练则产生了过拟合现象，且缺乏进一步的泛化能力。值得注意的是，这一限制在先进的大型语言模型中同样存在。这些结果表明，语言模型通过捷径学习获得隐式推理能力，能够在类似模式的任务中表现出色，但却缺乏更广泛的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:31 GMT</pubDate>
</item>
<item>
<title>优化测试时间计算的元强化学习方法</title>
<link>https://arxiv.org/abs/2503.07572</link>
<guid>https://arxiv.org/abs/2503.07572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过元强化学习优化测试时间计算的方法，显著提升LLM的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过元强化学习（MRT）优化测试时间计算，以提高大型语言模型（LLMs）的推理性能。当前的方法主要依赖于基于搜索轨迹的微调或使用零一奖励的强化学习，但其效率和扩展性存在质疑。我们将优化测试时间计算的问题形式化为一个元强化学习问题，并提出通过累积遗憾来衡量测试时间计算的有效性。研究表明，当前最先进的模型并未最小化遗憾，而是可以通过最大化密集奖励与零一奖励强化学习相结合来实现。最终，我们提出的MRT方法在数学推理上相比于传统的结果奖励强化学习，性能提升了2-3倍，并且相应的令牌效率提高了约1.5倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07572" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:40:43 GMT</pubDate>
</item>
<item>
<title>LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL</title>
<link>https://arxiv.org/abs/2503.07536</link>
<guid>https://arxiv.org/abs/2503.07536</guid>
<content:encoded><![CDATA[
Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \method achieves 4.83\% and 4.5\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>MoE-X：一种具备内在可解释性的混合专家语言模型</title>
<link>https://arxiv.org/abs/2503.07639</link>
<guid>https://arxiv.org/abs/2503.07639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoE-X是一种新型语言模型，专注于提高可解释性和性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MoE-X，一种基于混合专家（MoE）架构的语言模型，旨在实现内在可解释性。研究表明，宽而稀疏激活的网络更能捕捉可解释因素，但直接训练这样的网络计算成本高。因此，MoE-X通过激活部分专家来提供可扩展的解决方案。将MoE层重写为稀疏的大型多层感知机（MLP）使得在保持稀疏性的同时，隐藏层规模得以高效扩展。为了进一步增强可解释性，MoE-X在每个专家中强制稀疏激活，并重新设计路由机制，优先考虑激活稀疏性最高的专家。实验表明，MoE-X在国际象棋和自然语言任务上性能与密集模型相当，并且其可解释性显著提升，达到比GPT-2更低的困惑度，优于基于稀疏自编码器的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 12:40:54 GMT</pubDate>
</item>
<item>
<title>PhiloBERTA：跨语言的古希腊与拉丁语词汇语义关系测量</title>
<link>https://arxiv.org/abs/2503.05265</link>
<guid>https://arxiv.org/abs/2503.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhiloBERTA模型揭示古希腊与拉丁词汇间的语义对齐情况。</p><br /><br /><p><strong>摘要：</strong> PhiloBERTA是一种跨语言的变换模型，用于测量古希腊与拉丁语词汇之间的语义关系。通过经典文本中选定术语对的分析，我们利用上下文嵌入和角度相似度指标识别精确的语义对齐。在实验结果中， etymologically（词源上）相关的词对显示出显著更高的相似性分数，尤其是在诸如epistēme（科学）与dikaiosynē（正义）等抽象哲学概念上。统计分析表明，这些关系存在一致的模式，p值为0.012，且词源上相关的对比对展示出比对照对更为稳定的语义保存。这些发现为研究哲学概念在古希腊与拉丁传统中的传播建立了量化框架，并为古典语言学研究提供了新方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:30:16 GMT</pubDate>
</item>
<item>
<title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
<link>https://arxiv.org/abs/2503.02819</link>
<guid>https://arxiv.org/abs/2503.02819</guid>
<content:encoded><![CDATA[
While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 12:46:51 GMT</pubDate>
</item>
<item>
<title>VACE：全能视频生成与编辑框架</title>
<link>https://arxiv.org/abs/2503.07598</link>
<guid>https://arxiv.org/abs/2503.07598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VACE提供一个统一的视频生成与编辑解决方案，处理多种视频任务。</p><br /><br /><p><strong>摘要：</strong> VACE是一种新的平台，旨在整合视频生成和编辑任务，克服视频合成中的空间和时间一致性挑战。该框架支持参考视频生成、视频编辑和带掩膜的视频编辑等多种功能，用户可以通过统一的界面（视频条件单元）进行操作。通过采用上下文适配器结构，VACE能够灵活处理各种视频合成任务，并将不同任务概念正式化地注入模型。实验结果表明，VACE在多种子任务上的表现与专用模型相当，同时支持多样化的应用组合，展现了其在视频内容创作领域的强大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>提升模型领域泛化能力的方法研究</title>
<link>https://arxiv.org/abs/2503.06698</link>
<guid>https://arxiv.org/abs/2503.06698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用预训练特征提升领域泛化能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本研究关注于如何提高模型在新颖且未见数据分布下的泛化能力，探讨模型架构和预训练目标对特征丰富性的影响。我们提出通过发现潜在的伪域结构，从而在无监督的方式下捕捉特定领域的变化，并利用这些伪域表示增强现有分类器，使其更适应未见测试领域。通过对不同预训练特征空间的分析，发现扩散模型的特征在没有显式领域标签的情况下，能有效区分领域并捕捉细致的领域特征。在五个数据集上的实证研究显示，我们的方法较标准基线（经验风险最小化）在未见领域的泛化能力提升了超过4%的测试准确率，并且显著超越绝大多数训练过程中使用领域标签的算法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 13:29:01 GMT</pubDate>
</item>
<item>
<title>Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries</title>
<link>https://arxiv.org/abs/2502.20475</link>
<guid>https://arxiv.org/abs/2502.20475</guid>
<content:encoded><![CDATA[
To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:23:15 GMT</pubDate>
</item>
<item>
<title>REF-VLM：统一视觉解码任务的多模态大型语言模型框架</title>
<link>https://arxiv.org/abs/2503.07413</link>
<guid>https://arxiv.org/abs/2503.07413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REF-VLM框架提升了多模态解码任务的性能与适应性。</p><br /><br /><p><strong>摘要：</strong> REF-VLM是一种端到端框架，旨在统一训练各种视觉解码任务。针对稠密预测任务的挑战，我们提出了三元组引用范式（TRP），通过三元组结构明确解耦视觉解码任务中的概念、解码类型和目标。同时，构建了VTInstruct数据集，包含超过1亿条多模态对话样本，涵盖25种任务类型，结合文本输入和多种视觉提示，如点、框、涂鸦和掩膜。REF-VLM的优越性通过定性和定量实验得以验证，超越了现有的多模态大型语言模型，展示了其在复杂视觉解码场景下的强大适应性和性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:59:14 GMT</pubDate>
</item>
<item>
<title>TRCE：提高文本生成模型中恶意内容的概念抹除能力</title>
<link>https://arxiv.org/abs/2503.07389</link>
<guid>https://arxiv.org/abs/2503.07389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TRCE通过两阶段策略有效抹除文本生成中的恶意概念。</p><br /><br /><p><strong>摘要：</strong> 随着文本生成模型在照片现实主义图像生成中的进步，如何防止恶意内容的产生成为一个重要课题。本研究提出TRCE，一种采用两阶段策略的概念抹除方法，旨在实现可靠的恶意概念抹除与模型知识保留之间的有效权衡。首先，TRCE识别并优化跨注意力层，将隐含的恶意语义映射为包含安全概念的相似提示，以减少模型在去噪过程中受到恶意语义的影响。其次，TRCE利用扩散模型的采样轨迹特性，通过对比学习引导早期去噪预测朝向安全方向，避免生成恶意内容。通过对多个恶意概念抹除基准的全面评估，结果表明TRCE在抹除恶意概念的同时，更好地保留了模型的原始生成能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:37:53 GMT</pubDate>
</item>
<item>
<title>自回归表示对齐框架（ARRA）在文本到图像生成中的应用</title>
<link>https://arxiv.org/abs/2503.07334</link>
<guid>https://arxiv.org/abs/2503.07334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARRA框架通过对齐隐藏状态实现文本到图像生成的全局一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了自回归表示对齐（ARRA）训练框架，旨在实现自回归大型语言模型（LLMs）中全局一致的文本到图像生成，而无需对架构进行复杂修改。与以往需要大规模架构重设计的工作不同，ARRA通过全局视觉对齐损失和混合标记将LLM的隐藏状态与外部视觉基础模型的视觉表示对齐。该框架在保留自回归范式的同时，通过强制局部下一个标记预测和全局语义蒸馏的双重约束，使大型语言模型能够隐式学习空间和上下文一致性。广泛的实验结果验证了ARRA的灵活性，显示在从文本生成的LLM或随机初始化的情况下训练时，ARRA能够分别在高级自回归LLM如Chameleon和LlamaGen上减少25.5%的FID（MIMIC-CXR）、8.8%（DeepEyeNet）和7.5%（ImageNet）。在领域适应方面，ARRA还成功地将通用LLM与专用模型对齐，以在医学成像（MIMIC-CXR）上实现18.6%的FID减少。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:49:28 GMT</pubDate>
</item>
<item>
<title>基于SlotMIM的预训练视觉模型在机器人学习中的优化研究</title>
<link>https://arxiv.org/abs/2503.06960</link>
<guid>https://arxiv.org/abs/2503.06960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SlotMIM方法以优化视觉模型在机器人学习中的表现。</p><br /><br /><p><strong>摘要：</strong> 预训练视觉模型（PVMs）在现代机器人中扮演重要角色，但其最佳配置仍不明确。通过系统评估，我们发现DINO和iBOT在视觉运动控制和感知任务上表现优于MAE，但在非单对象中心（NOC）数据上训练时存在困难，影响其学习对象中心表示的能力。为此，我们设计了SlotMIM方法，利用语义瓶颈减少原型数量，促进对象性出现，并引入跨视图一致性正则化以鼓励多视图不变性。我们的实验涵盖对象中心、场景中心、网络抓取及自我中心数据的预训练，结果表明该方法在图像识别、场景理解和机器人学习评估方面显著优于现有研究，且在百万规模数据集上展现出优良的数据效率和可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:18:31 GMT</pubDate>
</item>
<item>
<title>DiffCLIP：基于差分注意力机制的视觉-语言模型</title>
<link>https://arxiv.org/abs/2503.06626</link>
<guid>https://arxiv.org/abs/2503.06626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffCLIP模型通过差分注意力机制提升视觉-语言理解性能。</p><br /><br /><p><strong>摘要：</strong> DiffCLIP是一种新颖的视觉-语言模型，通过将差分注意力机制扩展到CLIP架构中，显著提升了图像与文本理解的能力。差分注意力最初是为大型语言模型开发的，旨在放大相关上下文并消除噪声信息。通过将这一机制集成到CLIP的双编码器框架中，DiffCLIP在零-shot分类、检索和鲁棒性基准测试中表现优越，持续超越传统CLIP模型。研究表明，这些性能提升伴随着微不足道的计算开销，表明差分注意力能显著增强多模态表示，而不牺牲效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 10:04:09 GMT</pubDate>
</item>
<item>
<title>Symbolic-MoE：基于技能的专家选择框架提升LLM性能</title>
<link>https://arxiv.org/abs/2503.05641</link>
<guid>https://arxiv.org/abs/2503.05641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Symbolic-MoE通过实例级专家选择显著提升了预训练LLM的性能。</p><br /><br /><p><strong>摘要：</strong> Symbolic-MoE是一个基于技能的Mixture-of-Experts框架，旨在通过实例级别的细粒度专家选择来提升预训练大语言模型（LLM）的性能。该方法关注于根据不同任务的专业技能，如数学中的代数或生物医学推理中的分子生物学，动态选择最合适的LLM专家。通过实施批量推理策略，有效减少了模型的加载开销，使得在单个GPU上能够整合16个专家模型，性能超越此前多代理基线。经过在多个基准（如MMLU-Pro、GPQA、AIME和MedMCQA）的广泛评估，Symbolic-MoE在精度上提升了8.15%，并且比基于讨论的基线方法在计算上更加高效，去除了昂贵的多轮讨论需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:03:13 GMT</pubDate>
</item>
<item>
<title>基于多镜头视频的人类动作重建框架</title>
<link>https://arxiv.org/abs/2503.07597</link>
<guid>https://arxiv.org/abs/2503.07597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，从多镜头视频中重建长序列3D人类动作。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，旨在从具有多镜头切换的野外视频中重建3D人类的长序列运动。此类长序列运动对运动生成和理解等应用至关重要，但由于镜头切换突变、部分遮挡和动态背景等问题，重建过程面临巨大挑战。现有方法主要集中于单镜头视频，或简化多镜头的对齐处理。本研究通过整合增强的相机姿态估计与人类动作恢复（HMR），并结合镜头切换检测器和强大的对齐模块，解决了姿态和方向在镜头间的连续性。通过采用定制的动作整合器，有效减少了足部滑动问题，并确保人类姿态的时间一致性。在创建的多镜头数据集上的广泛评估，表明我们的方法在实际的世界坐标中重建人类运动方面的鲁棒性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:03 GMT</pubDate>
</item>
<item>
<title>适配器引导蒸馏：提升条件扩散模型采样效率</title>
<link>https://arxiv.org/abs/2503.07274</link>
<guid>https://arxiv.org/abs/2503.07274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">适配器引导蒸馏提升了条件扩散模型的采样效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的方法——适配器引导蒸馏（AGD），旨在提高条件扩散模型在推理时的效率。传统的分类器自由引导（CFG）需要双倍的神经函数评估（NFE），而AGD则通过轻量级适配器在一次前向传播中模拟CFG，从而有效地加快采样速度，同时保持或提升样本质量。与以往需要调整整个模型的引导蒸馏方法不同，AGD仅训练少量额外参数（约2%），并保持基本模型的不变性，以显著降低资源需求。此外，AGD通过在CFG引导的轨迹上进行训练，解决了现有引导蒸馏方法在训练和推理中的关键不匹配问题。实验表明，AGD在多个架构中与CFG相比较，FID值相当或更优，且仅需提供一半的NFE。我们的研究使得在单个消费级GPU上对大型模型（约2.6B参数）的蒸馏成为可能，进一步推动了这一领域的可访问性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:55:08 GMT</pubDate>
</item>
<item>
<title>WISE：基于世界知识的文本到图像生成语义评估基准</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISE提出了一种新的评估基准，提升文本到图像生成的语义理解能力。</p><br /><br /><p><strong>摘要：</strong> 文本到图像(T2I)模型在艺术创作和视觉内容生成方面具有出色能力，但现有研究主要关注图像的真实感和浅层的文本-图像匹配，缺乏对复杂语义理解和世界知识整合的全面评估。为了解决这一问题，本文提出了WISE，一个专门为世界知识驱动的语义评估设计的基准。WISE通过从25个子领域中精心设计的1000个提示，超越了简单的词汇与像素映射，挑战模型在文化常识、时空推理和自然科学等领域的能力。本文还引入了一种新量化指标WiScore，用于评估知识与图像的对齐。通过对20个模型的全面测试，结果显示，其在图像生成中有效整合和应用世界知识的能力显著不足。这些发现为下一代T2I模型在知识整合与应用方面的提升提供了重要指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:47:53 GMT</pubDate>
</item>
<item>
<title>新型零样本音视频语音识别框架Zero-AVSR</title>
<link>https://arxiv.org/abs/2503.06273</link>
<guid>https://arxiv.org/abs/2503.06273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zero-AVSR框架可实现目标语言的零样本音视频语音识别。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新型零样本音视频语音识别框架Zero-AVSR，该框架允许在无目标语言音视频语音数据的情况下进行语音识别。我们提出的音视频语音罗马化器(AV-Romanizer)通过预测罗马文本学习语言无关的语音表示。同时，利用大型语言模型(LLMs)的强大多语言建模能力，将预测的罗马文本转换为特定语言的字母字符。此外，我们探索了一种统一的Zero-AVSR方法，通过将由AV-Romanizer编码的音视频语音表示直接整合到LLM中，借助我们提出的多任务学习方案对适配器和LLM进行微调。为捕捉广泛的音位和语言多样性，我们还引入了一个包含2916小时音视频语音数据的多语言音视频罗马化语料库(MARC)，覆盖82种语言，并提供语言特定的字母字符和罗马文本的转录。大量分析和实验表明，Zero-AVSR框架能够扩展对未见语言的支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06273" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:40:13 GMT</pubDate>
</item>
<item>
<title>Novel Object 6D Pose Estimation with a Single Reference View</title>
<link>https://arxiv.org/abs/2503.05578</link>
<guid>https://arxiv.org/abs/2503.05578</guid>
<content:encoded><![CDATA[
Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 12:00:41 GMT</pubDate>
</item>
<item>
<title>Mixture of Large Language Model Agents的安全性与防御机制研究</title>
<link>https://arxiv.org/abs/2503.05856</link>
<guid>https://arxiv.org/abs/2503.05856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨MoA架构的安全性及其对欺骗性LLM代理的脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文首次对Mixture of Large Language Model Agents (MoA)架构在面对欺骗性LLM代理时的安全性与可靠性进行全面研究。尽管MoA在如AlpacaEval 2.0等基准测试中表现优异，但我们发现其存在重要的脆弱性。在研究中，我们分析了欺骗信息的传播、模型规模和信息可用性等因素，结果显示引入一个精心指令的欺骗代理会使MoA的表现显著下降，AlpacaEval 2.0测试中的表现从49.2%降低至37.9%。在QuALITY测试中，准确率也下降了48.5%。针对这些问题，本文提出了一系列无监督防御机制，旨在恢复丧失的性能，灵感来源于历史上威尼斯的投票过程，旨在减少影响与欺骗。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 09:46:39 GMT</pubDate>
</item>
<item>
<title>任务感知的键值缓存压缩：提升大型语言模型的信息处理效率</title>
<link>https://arxiv.org/abs/2503.04973</link>
<guid>https://arxiv.org/abs/2503.04973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法压缩外部知识，以增强语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种任务感知的键值（KV）缓存压缩方法，以优化大型语言模型（LLMs）对外部知识的利用。现有方法如检索增强生成（RAG）虽然能通过相似性搜索获取证据，但可能错过关键信息；而长上下文模型虽然能处理多个文档，但计算成本高且受限于上下文窗口大小。我们的方法模拟学生为开放书考试而浓缩学习材料，允许在零样本或少样本设置下有效压缩外部知识。实验表明，该方法在LongBench v2基准测试中，相较于RAG提高了最多7个绝对准确度，同时以30倍的压缩率减少推理延迟，从0.43秒降至0.16秒。对于稀疏证据的任务，RAG表现良好，而任务感知压缩在广泛知识的任务中表现更优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:07:41 GMT</pubDate>
</item>
<item>
<title>YOLOE：高效的开放式检测与分割模型</title>
<link>https://arxiv.org/abs/2503.07465</link>
<guid>https://arxiv.org/abs/2503.07465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOE结合检测与分割，提升开放场景的实时性能。</p><br /><br /><p><strong>摘要：</strong> YOLOE是一个新型的高效模型，旨在解决传统检测模型在开放场景中的适应性问题。该模型融合了多种开放提示机制的检测与分割能力，包括文本提示的Re-parameterizable Region-Text Alignment（RepRTA）策略，视觉提示的Semantic-Activated Visual Prompt Encoder（SAVPE），以及针对无提示场景的Lazy Region-Prompt Contrast（LRPC）策略。YOLOE在LVIS数据集上显示出卓越的零样本表现，其训练成本降低到YOLO-Worldv2-S的三分之一，同时推理速度提升1.4倍。在迁移到COCO数据集时，YOLOE-v8-L相较于闭集YOLOv8-L获得了0.6 AP^b和0.4 AP^m的提升，训练时间减少近四倍。通过这些创新，YOLOE在保持高效性的同时，实现了高准确率，证明了其在实际应用中的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:42:59 GMT</pubDate>
</item>
<item>
<title>基于ReLU的偏好优化算法RePO：简化语言模型对齐方法</title>
<link>https://arxiv.org/abs/2503.07426</link>
<guid>https://arxiv.org/abs/2503.07426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RePO通过简化超参数与新算法有效优化语言模型对齐。</p><br /><br /><p><strong>摘要：</strong> Aligning large language models (LLMs) with human preferences is faced with challenges in computational efficiency and stability. Existing methods, such as reinforcement learning from human feedback (RLHF), often struggle with complex parameters. In contrast, we propose a novel approach called ReLU-based Preference Optimization (RePO), which streamlines the alignment process by eliminating the need for a beta hyperparameter. This is achieved through two main innovations: retaining reference-free margins while utilizing gradient analysis to remove beta, and employing a ReLU-based max-margin loss to filter trivial pairs effectively. Theoretically, RePO is positioned as a limiting case of SimPO where logistic weighting simplifies to binary thresholding. Empirical evaluations on datasets like AlpacaEval 2 and Arena-Hard demonstrate that RePO consistently outperforms existing methods DPO and SimPO, achieving effective alignment while requiring only a single hyperparameter adjustment.</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:11:07 GMT</pubDate>
</item>
<item>
<title>Llama-MTSK：一种灵活的音视频识别多模态语言模型</title>
<link>https://arxiv.org/abs/2503.06362</link>
<guid>https://arxiv.org/abs/2503.06362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-MTSK通过多层次表示提升音视频识别的效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 音视频识别（AVSR）结合音频和视觉信息提高了语音识别在嘈杂环境中的鲁棒性。随着大语言模型（LLM）的进步，AVSR领域的表现显著提升。然而，直接将语音表示集成至LLM面临高计算成本。为了解决这一问题，提出了Llama-MTSK，这是一种基于马特ryoshka的多模态LLM，能够根据计算约束灵活适配音视频令牌分配，同时保持高性能。该方法在单个模型中编码不同粒度的音视频表示，避免了训练多个模型以适应不同压缩级别的需要。通过引入基于LoRA的马特ryoshka策略，Llama-MTSK在两个大型AVSR数据集上的评估结果显示出其优越性，达到了最先进的性能，匹配或超越了在固定压缩级别上独立训练的模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 19:02:10 GMT</pubDate>
</item>
<item>
<title>探索三维编码器与文本特征空间的后期对齐</title>
<link>https://arxiv.org/abs/2503.05283</link>
<guid>https://arxiv.org/abs/2503.05283</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究三维编码器与文本特征空间的后期对齐方法及其效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨三维编码器在与文本特征空间的对齐中的作用，分析传统方法的局限性。研究发现，仅依靠简单的后期对齐训练，文本与三维编码器的性能提升有限。通过提取特征空间的子空间并进行有针对性的投影，显著提高了对齐质量，进而提高了匹配和检索任务的准确性。此外，分析显示这些共享子空间大致分隔了语义与几何数据表示。此研究首次为三维单模态与文本特征空间的后期对齐建立了基线，并突出了与其他表示相比三维数据的共享与独特属性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05283" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:51:56 GMT</pubDate>
</item>
<item>
<title>WritingBench：提升大语言模型写作能力的综合评估基准</title>
<link>https://arxiv.org/abs/2503.05244</link>
<guid>https://arxiv.org/abs/2503.05244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Introducing WritingBench, a benchmark to evaluate LLMs in diverse writing domains.</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的进步，文本生成能力显著增强，但评估其在写作上的表现仍然面临挑战。现有的基准主要集中在通用文本生成或有限的写作任务上，未能全面捕捉高质量书面内容在各个领域的多样化需求。为此，本文提出了WritingBench，这是一个综合性基准，旨在评估LLMs在六个核心写作领域及其一百个子领域的表现，涵盖创意、说服性、信息性和技术性写作。此外，我们还提出了一种依赖查询的评估框架，使LLMs能够动态生成特定实例的评估标准。该框架配备了一个调优后的评判模型，能够在风格、格式和长度方面进行标准化评分，框架的有效性通过其数据策划能力得以进一步验证，7B参数模型在该基准下可接近最先进的性能。该基准及评估工具与模块化框架组件将以开源形式发布，以推动LLMs在写作方面的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05244" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 03:56:20 GMT</pubDate>
</item>
<item>
<title>AlphaDrive：基于强化学习和推理的自动驾驶视觉语言模型框架</title>
<link>https://arxiv.org/abs/2503.07608</link>
<guid>https://arxiv.org/abs/2503.07608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AlphaDrive框架，提升了自动驾驶的规划性能和训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AlphaDrive，一个将强化学习（RL）与推理相结合的视觉语言模型（VLM）框架，旨在解决自动驾驶中的复杂规划问题。AlphaDrive引入了四种基于GRPO的RL奖励机制，并采用两阶段的推理训练策略，将监督微调（SFT）与强化学习相结合，从而显著提高了自动驾驶系统的规划性能和训练效率。研究还发现，在经历强化学习训练后，AlphaDrive展现出新兴的多模态规划能力，这对提升驾驶安全和效率至关重要。该框架是首个将GRPO基础的强化学习与规划推理相结合应用于自动驾驶中的研究，未来将发布代码以促进相关研究的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>图像与文本结合预训练模型在视语言任务中的表现</title>
<link>https://arxiv.org/abs/2503.07603</link>
<guid>https://arxiv.org/abs/2503.07603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，图像与文本结合的预训练模型在视语言任务中表现更佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了结合图像数据进行预训练的语言模型在视语言任务中的表现，重点分析了两步培训流程与早期集成图像的视觉语言模型（VLMs）之间的收益和损失。通过对不同数据集、模型规模、图像文本比率及预训练量进行实验，结果显示，采用图像和文本数据混合预训练的模型在视语言任务中表现优越，同时在文字仅任务中的表现依然强劲。具体而言，对于一个10亿参数的模型，在预训练过程中将视觉标记引入到80%时，相比于在完全预训练模型中引入视觉标记，平均提升了2%的任务表现。这表明图像集成的时机对模型性能具有重要影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:19 GMT</pubDate>
</item>
<item>
<title>DreamRelation：一种基于示例视频的个性化关系视频定制方法</title>
<link>https://arxiv.org/abs/2503.07602</link>
<guid>https://arxiv.org/abs/2503.07602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRelation通过示例视频优化个性化关系视频生成，提升模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 关系视频定制是实现个性化视频的重要环节，但现有方法在处理复杂关系时存在困难，尤其是需要准确关系建模和在多样主题类别间的高泛化能力。为了解决这些问题，我们提出了DreamRelation方法，它利用示例视频通过两大核心组成部分：关系解耦学习和关系动态增强，实现个性化关系建模。在关系解耦学习中，我们剥离了关系与主题外观的交互，确保了在多样关系中的良好泛化。此外，我们通过分析MM-DiT的注意力机制中查询、键和值特征的不同角色，优化了关系LoRA三元组的设计，使该框架具备可解释性。在关系动态增强中，我们引入了时空关系对比损失，优先考虑关系动态，同时减少对详细外观的依赖。实验结果表明，DreamRelation在关系视频定制方面优于现有最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07602" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>MedAgentsBench: 复杂医学问题的新评估基准</title>
<link>https://arxiv.org/abs/2503.07459</link>
<guid>https://arxiv.org/abs/2503.07459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MedAgentsBench基准，以评估复杂医学问题的多步骤推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedAgentsBench，一个新颖的基准，专注于需要多步骤临床推理、诊断制定和治疗规划的复杂医学问题。在当前的医疗问答基准中，尽管大型语言模型（LLMs）表现优异，但在面对复杂问题时仍然存在明显不足。MedAgentsBench从七个已有的医学数据集中提取数据，旨在解决现有评估中的三大关键限制，包括简单问题导致的高基线性能、一致性不足的采样与评估协议，以及缺乏对性能、成本和推理时间相互关系的系统分析。通过对多种基础模型和推理方法的实验，结果表明，最新的思维模型如DeepSeek R1和OpenAI o3在复杂医学推理任务中表现优异。此外，基于搜索的先进代理方法在性能与成本比方面表现出色，适应不同计算约束下的最佳模型选择也得到了识别。该基准及评估框架已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:38:44 GMT</pubDate>
</item>
<item>
<title>DistiLLM-2：通过对比学习提升语言模型蒸馏效果</title>
<link>https://arxiv.org/abs/2503.07067</link>
<guid>https://arxiv.org/abs/2503.07067</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DistiLLM-2通过对比学习显著提升语言模型蒸馏性能。</p><br /><br /><p><strong>摘要：</strong> DistiLLM-2提出了一种新的对比学习方法，旨在通过有效调和教师与学生模型之间的损失函数，提升学生模型的表现。与以往的蒸馏策略相比，该方法同时提高了教师生成回应的概率，并降低了学生生成回应的概率。实验证明，DistiLLM-2在执行指令跟随、代码生成等多种任务时均能构建高性能的学生模型。此外，该方法还支持多样化的应用场景，如偏好对齐和视觉语言扩展。这些研究结果彰显了通过对比学习来增强语言模型蒸馏效果的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07067" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:51:32 GMT</pubDate>
</item>
<item>
<title>ProBench: 新型多模态智能评估基准的构建与实证分析</title>
<link>https://arxiv.org/abs/2503.06885</link>
<guid>https://arxiv.org/abs/2503.06885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProBench是一个涵盖多领域的专业多模态智能评估基准。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了ProBench，一个开创性的多模态智能评估基准，旨在测试先进模型在处理专业用户查询时的能力。ProBench包含4000个由专业人士根据日常工作需求独立提交的高质量样本，覆盖科学、艺术、人文学科、编程、数学和创意写作等10个领域及56个子领域。通过MLLM作为评判者，评估了24个最新模型的表现。研究结果显示，尽管最优秀的开源模型在某些方面可与专有模型相媲美，但在视觉感知、文本理解、领域知识和高级推理等方面，ProBench提出了显著挑战，为未来的多模态AI研究指明了方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:29:18 GMT</pubDate>
</item>
<item>
<title>Vision-R1：增强多模态推理能力的深度学习模型</title>
<link>https://arxiv.org/abs/2503.06749</link>
<guid>https://arxiv.org/abs/2503.06749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vision-R1通过强化学习提升多模态推理能力，展示出优异的模型表现。</p><br /><br /><p><strong>摘要：</strong> DeepSeek-R1-Zero通过强化学习成功展示了大型语言模型（LLMs）的推理能力。受到这一突破的启发，本文探讨如何利用强化学习提升多模态大语言模型（MLLMs）的推理能力。由于缺乏高质量的多模态推理数据，直接使用强化学习难以激活MLLMs的复杂推理功能。为此，本文提出了Vision-R1模型，通过利用现有的MLLM和DeepSeek-R1构建了一个高质量的多模态推理链（CoT）数据集——Vision-R1-cold数据集。随后，我们引入了渐进思维抑制训练（PTST）策略和群体相对策略优化（GRPO）以改善模型的推理能力。通过广泛的实验，模型在多个多模态数学推理基准上平均提升了6%的表现，其中Vision-R1-7B在MathVista基准上达到了73.5%的准确率，仅比领先模型OpenAI O1低0.4%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 16:06:45 GMT</pubDate>
</item>
<item>
<title>SurveyForge：提升文献综述生成质量的自动化工具</title>
<link>https://arxiv.org/abs/2503.04629</link>
<guid>https://arxiv.org/abs/2503.04629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyForge利用LLMs提升文献综述生成的质量与效率。</p><br /><br /><p><strong>摘要：</strong> 随着科研出版物的快速增长，文献综述在科学研究中发挥着重要作用。近期，研究者们开始使用大语言模型(LLMs)来自动化文献综述的生成，以提高效率。然而，LLM生成的综述在结构和引用准确性方面仍显著低于人工撰写的综述。为了解决这些问题，我们提出了SurveyForge，首先通过分析人工撰写的综述的逻辑结构并参考相关领域的文章生成提纲。随后，SurveyForge利用内存中检索到的高质量论文，自动生成和完善综述内容。此外，我们构建了SurveyBench，以实现全面评估，其中包括100篇人工撰写的综述论文用于胜率比较，并从参考文献、提纲和内容质量三个维度评估AI生成的综述论文。实验结果表明，SurveyForge在质量上优于AutoSurvey等以往工作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title>提升人工文本检测的可解释性：稀疏自编码器的应用</title>
<link>https://arxiv.org/abs/2503.03601</link>
<guid>https://arxiv.org/abs/2503.03601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过稀疏自编码器提升人工文本检测的可解释性，以分析不同模型的文本特征。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的崛起，人工文本检测（ATD）变得愈加重要。然而，目前没有单一算法能够在不同类型的未见文本上始终如一地表现良好，也无法有效地推广到新的语言模型上。可解释性在实现这一目标中起着关键作用。本研究通过采用稀疏自编码器（SAE）来提取Gemma-2-2b残差流中的特征，从而增强ATD的可解释性。我们识别出既可解释又高效的特征，并通过领域和模型特定统计、引导方法，以及手动或基于LLM的解释，对这些特征进行语义和相关性分析。研究结果为理解各种模型生成的文本与人类撰写的内容之间的差异提供了有价值的见解，表明现代LLM在信息密集型领域有独特的写作风格，尽管它们能够生成与人类相似的输出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 10:33:52 GMT</pubDate>
</item>
<item>
<title>稀疏专家激活剪枝：优化大型语言模型推理效率的新方法</title>
<link>https://arxiv.org/abs/2503.07605</link>
<guid>https://arxiv.org/abs/2503.07605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出稀疏专家激活剪枝方法，优化大型语言模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在自然语言处理任务中取得了显著成功，但推理时的高计算成本依然是一个瓶颈。本文介绍了一种训练无关的稀疏专家激活剪枝（SEAP）方法，该方法选择性保留与任务相关的参数，以降低推理开销。SEAP受到了LLM隐层状态和激活的聚类模式启发，识别任务特定的专家激活模式，通过剪枝模型来保持任务性能并提高计算效率。实验结果表明，SEAP在保持竞争性准确度的同时，显著降低计算开销。在50%的剪枝下，SEAP的性能超过了WandA和FLAP 20%以上，而在20%的剪枝下，与稠密模型相比，性能仅下降了2.2%。这些发现突显了SEAP的可扩展性和有效性，使其成为优化大规模LLM的有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>大型语言模型带来的假新闻风险与检测系统的挑战</title>
<link>https://arxiv.org/abs/2503.07595</link>
<guid>https://arxiv.org/abs/2503.07595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大型语言模型带来的假新闻风险及其检测技术的挑战。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的普及，假新闻传播的风险日益加剧。因此，开发如DetectGPT这样的分类系统变得至关重要。然而，实验表明这些检测器容易受到规避技术的影响，例如通过系统性地改变生成模型的温度，导致基于浅层学习的检测器可靠性降低。此外，利用强化学习微调生成模型能够突破基于BERT的检测器。结果显示，通过改写，文本尽管与原文高度相似，仍能超过90%地规避像DetectGPT这样的零样本检测器。本文还与现有研究进行了比较，揭示了所提出方法的优越性，并讨论了对社会的潜在影响及未来的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:56:25 GMT</pubDate>
</item>
<item>
<title>PE3R：高效的3D重建框架</title>
<link>https://arxiv.org/abs/2503.07507</link>
<guid>https://arxiv.org/abs/2503.07507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PE3R框架在速度和精度上显著提升2D到3D重建的能力。</p><br /><br /><p><strong>摘要：</strong> 针对现有2D到3D感知方法面临的局限，本文提出了一种新颖的框架——Perception-Efficient 3D Reconstruction (PE3R)。PE3R通过前馈架构实现快速的3D语义场重建，展现出强大的零-shot泛化能力，能在多种场景和对象中有效运行。经过广泛实验验证，PE3R不仅在3D重建速度上取得了至少9倍的加速，在感知精度和重建精确度上也显著提升，为相关领域设定了新的基准。相关代码已公开可用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 12:29:10 GMT</pubDate>
</item>
<item>
<title>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.07365</link>
<guid>https://arxiv.org/abs/2503.07365</guid>
<content:encoded><![CDATA[
We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:23:12 GMT</pubDate>
</item>
<item>
<title>Automated Movie Generation via Multi-Agent CoT Planning</title>
<link>https://arxiv.org/abs/2503.07314</link>
<guid>https://arxiv.org/abs/2503.07314</guid>
<content:encoded><![CDATA[
Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:33:27 GMT</pubDate>
</item>
<item>
<title>FedRand框架：提升联邦学习中的数据隐私</title>
<link>https://arxiv.org/abs/2503.07216</link>
<guid>https://arxiv.org/abs/2503.07216</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FedRand框架通过选择性共享参数增强了联邦学习中的数据隐私。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FedRand框架，以解决联邦学习（FL）中数据隐私保护不足的问题。在FL中，中央服务器在聚合过程中会接触到本地客户端的模型参数，这潜在地泄露了用户数据，尤其在进行视觉语言模型（VLMs）训练时更为明显。FedRand方法创新性地通过每个客户端随机选择低秩适应（LoRA）子参数并保持其余参数的私密性，进而减少了数据隐私泄露的风险。在客户端私有数据集上训练后，仅将非私密参数返回服务器进行聚合。实验结果表明，与相关基线相比，FedRand在抵御成员推断攻击（MIAs）方面表现出更高的鲁棒性，同时在多个基准数据集上达到相似的准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07216" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:55:50 GMT</pubDate>
</item>
<item>
<title>eMIGM：统一的图像生成与扩散模型</title>
<link>https://arxiv.org/abs/2503.07197</link>
<guid>https://arxiv.org/abs/2503.07197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">eMIGM模型在图像生成任务上表现卓越，尤其在ImageNet数据集上。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了eMIGM模型，它在不同动机和目标下统一了掩膜图像生成模型与掩膜扩散模型。通过探索训练与采样的设计空间，我们识别出影响性能与效率的关键因素。在经过改进后，eMIGM表现出在ImageNet生成上的强劲性能，尤其在256x256尺寸上，相同的函数评估次数（NFEs）和模型参数下，eMIGM超越了经典的VAR模型。随着NFE和模型参数的增加，eMIGM的性能与最先进的连续扩散模型相当，但所需的NFE却少于40%。在ImageNet 512x512上，eMIGM也在约60%的NFE下超越了最先进的连续扩散模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:27:12 GMT</pubDate>
</item>
<item>
<title>EasyControl: 高效灵活的条件引导扩散变换框架</title>
<link>https://arxiv.org/abs/2503.07027</link>
<guid>https://arxiv.org/abs/2503.07027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyControl框架通过创新模块提升扩散变换模型的控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了EasyControl，一个新的框架，旨在提高扩散变换模型的效率和灵活性。我们基于三个关键创新：首先，引入了一种轻量级的条件注入LoRA模块，该模块可作为即插即用的解决方案，能够独立处理条件信号，避免修改基础模型权重，支持多种条件的灵活注入。其次，提出了位置感知训练范式，可以标准化输入条件到固定分辨率，使得图像生成具备任意纵横比和灵活分辨率的能力，同时提升计算效率。最后，开发了适用于条件生成任务的因果注意力机制及KV缓存技术，大幅降低图像合成延迟，提升整体效率。通过大量实验，EasyControl在各种应用场景展现了卓越性能，充分证明了其高效、灵活和广泛适用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:07:17 GMT</pubDate>
</item>
<item>
<title>MMDiag: 多轮多模态对话数据集及DiagNote模型</title>
<link>https://arxiv.org/abs/2503.07002</link>
<guid>https://arxiv.org/abs/2503.07002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了多轮多模态对话数据集MMDiag及新模型DiagNote。</p><br /><br /><p><strong>摘要：</strong> 本文提出了多轮多模态对话数据集MMDiag，旨在更真实地反映人类对话场景。该数据集通过特定规则和GPT的协助生成，强调问题之间的强关联和问题与图像之间的关系，适合多轮对话学习。为提升多模态模型的推理与基础能力，提出了DiagNote模型，具备两个交互模块（Deliberate和Gaze），分别用于链式思考和注释。通过实验证明，DiagNote在多模态信息的共同处理和推理能力上优于现有的多模态大语言模型，显示出更强的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07002" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:32:53 GMT</pubDate>
</item>
<item>
<title>FEA-Bench: 评估大型语言模型在代码库增量开发中的能力</title>
<link>https://arxiv.org/abs/2503.06680</link>
<guid>https://arxiv.org/abs/2503.06680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FEA-Bench是评估LLMs在代码库新特性开发能力的基准测试。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FEA-Bench，一个旨在评估大型语言模型（LLMs）在代码库中进行增量开发能力的基准测试框架。我们从83个GitHub仓库中收集了拉取请求，采用基于规则和意图的过滤方法，构建了专注于新特性开发的任务实例。每个任务实例包含代码修改，并与相关的单元测试文件配对，确保解决方案的可验证性。这一特性实现要求LLMs同时具备新组件的代码补全能力和其他相关代码部分的编辑能力，提供了一种更全面的评估方法。实验结果显示，LLMs在FEA-Bench测试中的表现显著较差，突显出在代码库层面增量代码开发中的诸多挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 12:11:57 GMT</pubDate>
</item>
<item>
<title>AutoCoA：提升自主性的大型代理模型框架</title>
<link>https://arxiv.org/abs/2503.06580</link>
<guid>https://arxiv.org/abs/2503.06580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoCoA框架增强了大型代理模型的自主性，优化了工具与环境的交互。</p><br /><br /><p><strong>摘要：</strong> 传统的代理工作流程依赖外部提示来管理与工具和环境的互动，这限制了推理模型的自主性。本文提出了一个名为AutoCoA的框架，专注于大型代理模型（LAMs），使其能够自主地确定何时及如何使用外部工具。该框架结合了监督微调（SFT）和强化学习（RL），实现了模型在推理和行动之间的无缝切换，同时高效管理环境互动。AutoCoA的主要组成部分包括步骤级的动作触发、轨迹级的链式行动优化，以及内部世界模型，以降低真实环境互动成本。评估结果显示，经过AutoCoA训练的代理模型在开放域问答任务中显著超越基于ReAct的工作流程，尤其是在需要长期推理和多步骤行动的任务中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:19:47 GMT</pubDate>
</item>
<item>
<title>Seg-Zero: 用于分割推理的零-shot 学习框架</title>
<link>https://arxiv.org/abs/2503.06520</link>
<guid>https://arxiv.org/abs/2503.06520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seg-Zero框架通过认知强化实现了有效的分割推理和零-shot泛化。</p><br /><br /><p><strong>摘要：</strong> 传统的分割推理方法依赖于带有类别标签和简单描述的监督微调，限制了其跨域泛化能力，并缺乏明确的推理过程。为解决这一问题，本文提出了Seg-Zero框架，展现了出色的泛化能力，并通过认知强化生成明确的推理链条。Seg-Zero采用解耦架构，包括推理模型和分割模型，其中推理模型解读用户意图，生成明确的推理链并产生位置提示，这些提示随后被分割模型用于生成精准的像素级掩模。我们设计了一种复杂的奖励机制，结合格式和准确性奖励，有效指导优化方向。通过强化学习训练，并不使用显式推理数据，Seg-Zero实现了强大的零-shot泛化能力，并在测试时展现了突出的推理能力。实验结果显示，Seg-Zero-7B在ReasonSeg基准测试中实现了57.5的零-shot性能，超过之前的LISA-7B模型18%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 04:48:51 GMT</pubDate>
</item>
<item>
<title>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling</title>
<link>https://arxiv.org/abs/2503.06121</link>
<guid>https://arxiv.org/abs/2503.06121</guid>
<content:encoded><![CDATA[
Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 03:31:18 GMT</pubDate>
</item>
<item>
<title>NeuGrasp：应对透明和镜面物体抓取的神经表面重建方法</title>
<link>https://arxiv.org/abs/2503.03511</link>
<guid>https://arxiv.org/abs/2503.03511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuGrasp是一种改进的抓取检测方法，专注于透明和镜面物体。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为NeuGrasp的神经表面重建方法，专为处理透明和镜面物体抓取挑战而设计。NeuGrasp利用背景先验进行无材质特性的抓取检测，结合变换器和全局先验体积，以聚合多视图特征和空间编码，从而在狭窄和稀疏的观察条件下实现稳健的表面重建。该方法通过残差特征增强关注前景物体，并利用占用先验体积提高空间感知。 extensive的实验结果表明，NeuGrasp在抓取能力上超越了多种先进方法，同时保持了相似的重建质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03511" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:57:37 GMT</pubDate>
</item>
<item>
<title>基于状态的参数高效微调方法在状态空间模型中的应用</title>
<link>https://arxiv.org/abs/2503.03499</link>
<guid>https://arxiv.org/abs/2503.03499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的基于状态的调优方法，提升了SSM的微调效果。</p><br /><br /><p><strong>摘要：</strong> 状态空间模型（SSMs）因其较低的计算成本成为动态替代Transformer的有效选择。然而，参数高效微调（PEFT）方法在SSMs中的应用尚未得到充分探索。特别是依赖提示的调优方法在SSMs上表现不佳。为此，我们提出了一种基于状态的方法，作为优于提示方法的替代方案。这种新方法直接调整与状态相关的特征，而不依赖外部提示。此外，我们引入了一种新颖的基于状态的PEFT方法：状态偏移调优。在每个时间步，方法直接影响当前状态，进而实现更有效的适应。通过在多种数据集上的大量实验，我们证明了该方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:44:42 GMT</pubDate>
</item>
<item>
<title>LLaVE：增强多模态嵌入模型表现的动态框架</title>
<link>https://arxiv.org/abs/2503.04812</link>
<guid>https://arxiv.org/abs/2503.04812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍LLaVE框架，该框架提高了多模态嵌入模型对困难负样本的学习能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了LLaVE框架，以解决现有基于LMM的嵌入模型在区分正负样本时的困难。通过动态调整负样本的表示学习，LLaVE显著改善了模型性能，并在MMEB基准上进行评估，覆盖四个子任务和36个数据集。实验结果显示，LLaVE模型不仅在指标上超过了之前的最佳7B模型，LLaVE-2B取得了领先的SOTA性能，LLaVE-7B进一步提高了6.2个百分点。此外，尽管LLaVE模型是在图像-文本数据上训练的，但它也能够在无监督条件下有效地推广到文本-视频检索任务，展现了其在其他嵌入任务中的潜在应用能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:21:57 GMT</pubDate>
</item>
<item>
<title>解析视觉语言模型中的文本偏见现象及其影响</title>
<link>https://arxiv.org/abs/2503.02199</link>
<guid>https://arxiv.org/abs/2503.02199</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视觉语言模型在视觉与文本不一致时的偏见现象及其影响。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于视觉语言模型（VLMs）在处理视觉数据和不同文本输入时的模态偏好，尤其是在视觉中心任务中。通过对四个视觉任务引入文本变体并评估十种VLMs，我们发现了“对文本的盲目信任”现象：当面临不一致时，VLMs倾向于过度信任文本数据，导致在文本损坏情况下显著性能下降，这引发了安全隐患。我们分析了影响文本偏见的多个因素，包括指令提示、语言模型规模、文本相关性、标记顺序以及视觉和文本不确定性之间的相互作用。尽管某些因素（如增大语言模型的规模）对减缓文本偏见有一定效果，其他因素（如标记顺序）则可能因语言模型的位置信息而加剧这一偏见。为应对这一问题，我们探索了通过文本增强进行监督微调，并展示了其在降低文本偏见方面的有效性。此外，我们提供的理论分析表明，“对文本的盲目信任”现象可能源于训练期间纯文本与多模态数据的不平衡。这项研究强调了在VLMs的训练中需关注模态交互，以提升其应对多模态数据不一致的鲁棒性和可靠性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02199" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 21:21:07 GMT</pubDate>
</item>
<item>
<title>SafeArena：评估大型语言模型代理的网络滥用风险</title>
<link>https://arxiv.org/abs/2503.04957</link>
<guid>https://arxiv.org/abs/2503.04957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeArena是首个专注于网络代理滥用风险的基准，评估其顺应恶意请求的情况。</p><br /><br /><p><strong>摘要：</strong> 随着基于大型语言模型（LLM）的代理在网络任务中的表现日益成熟，随之而来的滥用风险也显著增加。本文提出了SafeArena，这是首个专注于评估网络代理故意滥用风险的基准，包括250个安全任务和250个有害任务，覆盖四个网站。这些有害任务被分为五个类别：误信息、非法活动、骚扰、网络犯罪和社会偏见，旨在评估代理的真实滥用情况。我们采用Agent Risk Assessment框架，系统地评估领先的LLM代理（如GPT-4o和Qwen-2）在面对有害请求时的反应，结果显示，GPT-4o和Qwen-2分别完成了34.7%和27.3%的恶意请求，表明这些代理对恶意请求的顺应性令人惊讶。研究表明，迫切需要为网络代理制定安全对齐流程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 15:43:14 GMT</pubDate>
</item>
<item>
<title>引入S2S-Arena：评估语音模型的指令跟随能力</title>
<link>https://arxiv.org/abs/2503.05085</link>
<guid>https://arxiv.org/abs/2503.05085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S2S-Arena评估语音模型在指令跟随和副语言信息处理方面的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的快速发展，语音模型的进展同样引起了广泛关注。特别是当前在支持语音输入和输出的语音到语音（speech2speech, S2S）协议方面的最新成果，然而现有基准使用的自动文本评估方法缺乏对副语言信息的考量。为了解决这一问题，我们引入了S2S-Arena，一个新颖的以竞技场为风格的S2S基准，评估真实世界任务中语音输入和语音输出的指令跟随能力。通过文中设计的154个样本，融合了文本到语音（TTS）和实时录音，涵盖四个领域和21个任务，手动评估了现存流行的语音模型。实验结果显示，GPT-4o表现优异，同样，经过文本-语音对齐后的级联ASR、LLM和TTS的组合模型在S2S协议中也优于联合训练模型。此外，考虑到副语言信息，语音模型的知识传递主要依赖于LLM的基础，而这一性能在多语言支持上受到语音模块的限制。尽管优秀的语音模型已经能理解输入中的副语言信息，但生成合适的带有副语言信息的音频仍然存在挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 21:07:00 GMT</pubDate>
</item>
<item>
<title>LONGCODEU基准测试：评估长代码理解能力的研究</title>
<link>https://arxiv.org/abs/2503.04359</link>
<guid>https://arxiv.org/abs/2503.04359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出LONGCODEU基准测试，评估长代码理解能力，为软件工程提供重要见解。</p><br /><br /><p><strong>摘要：</strong> 当前的长上下文语言模型在真实软件工程应用中具有巨大潜力，但缺乏严谨的长代码理解评估框架限制了其发展。为此，我们提出LONGCODEU基准测试，从代码单元感知、内部理解、相互关系理解和文档理解四个方面（共八项任务）来评估长上下文语言模型在长代码理解方面的能力。通过对9个流行的长上下文语言模型（6个通用模型和3个代码模型）进行评估，实验结果揭示了当前模型在长代码理解方面的主要局限性，尤其是在代码长度超过32K时，性能显著下降，远低于其声称的128K-1M上下文窗口。在四个评估方面中，相互关系理解对模型来说是最具挑战性的。本研究为优化长上下文语言模型和推动软件工程的进步提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:02:31 GMT</pubDate>
</item>
<item>
<title>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</title>
<link>https://arxiv.org/abs/2503.01840</link>
<guid>https://arxiv.org/abs/2503.01840</guid>
<content:encoded><![CDATA[
The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>俄语临床编码自动化的可行性研究</title>
<link>https://arxiv.org/abs/2502.21263</link>
<guid>https://arxiv.org/abs/2502.21263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了俄语临床编码自动化的可行性及成果。</p><br /><br /><p><strong>摘要：</strong> 本研究调查了在资源有限的环境中对俄语进行临床编码自动化的可行性。我们介绍了一个包含电子健康记录中诊断字段的新数据集，该数据集注释了超过10,000个实体和1,500多个独特的ICD代码，作为多个顶尖模型（如BERT、带有LoRA的LLaMA和RAG）的基准。同时，我们还进行了跨领域（从PubMed摘要到医学诊断）和跨术语（从UMLS概念到ICD代码）的迁移学习实验。最终，我们将表现最佳的模型应用于标注2017年至2021年间的内部EHR数据集。实验结果显示，与医生手动标注数据相比，使用自动预测代码进行训练显著提高了准确性。我们的研究为在资源有限的语言（如俄语）中实现临床编码自动化提供了宝贵的见解，可能提高临床效率和数据准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.21263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 12:40:24 GMT</pubDate>
</item>
<item>
<title>基于隐性用户画像的对话系统用户模拟器</title>
<link>https://arxiv.org/abs/2502.18968</link>
<guid>https://arxiv.org/abs/2502.18968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型用户模拟器，利用隐性用户画像生成个性化对话。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的用户模拟器框架——隐性用户画像用户模拟器（USP），旨在改进对话系统的人机互动模拟。现有的用户模拟器通常仅依赖文本发言，忽视了用户的隐性特征如个性、说话风格和目标。而基于角色的方法缺乏普适性，依赖于预设的名人或原型档案。USP通过从人机对话中推断隐性用户画像，生成更具个性化和真实感的对话。首先，开发了以大型语言模型（LLM）为驱动的提取器，并创建了一个全面的用户画像架构。接着，通过条件监督微调和循环一致性的强化学习来优化模拟过程，分别在发言和对话水平进行改进。最后，采用多样的画像采样器，捕捉真实世界用户画像的分布。实验结果表明，USP在真实性和多样性方面优于现有的强基线，同时在一致性方面表现相当。此外，基于USP的动态多轮评估与主流基准高度一致，证明其在实际应用中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 04:26:54 GMT</pubDate>
</item>
<item>
<title>改进的流匹配技术在扩散模型中应用</title>
<link>https://arxiv.org/abs/2503.04824</link>
<guid>https://arxiv.org/abs/2503.04824</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出改进流匹配技术，提高扩散模型的生成效率。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在图像和视频生成中取得了显著进展，但计算成本依然很高。作为解决方案，流匹配旨在将扩散过程重新流化为直线，以实现快速生成。本文认为原始流匹配训练流程不够优化，并提出两项改进技术。首先，采用渐进流化方法，逐步在局部时间步中进行流化，以降低流匹配的难度。其次，提出对齐v预测，强调流匹配中方向匹配的重要性。实验结果表明，采用我们的方法可在SDv1.5上以仅4个采样步骤实现FID值10.70，接近教师模型（32个DDIM步骤，FID=10.05）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04824" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 06:34:36 GMT</pubDate>
</item>
<item>
<title>STILL项目第三技术报告：强化学习模型的发展与工具操作的影响</title>
<link>https://arxiv.org/abs/2503.04548</link>
<guid>https://arxiv.org/abs/2503.04548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">报告探讨了STILL项目中慢思考模型的强化学习训练及工具操作的效果。</p><br /><br /><p><strong>摘要：</strong> 本报告为STILL项目的第三技术报告，重点介绍了慢思考模型的发展及其强化学习（RL）训练方法。随着技术路径的逐渐明确，尺度化的RL训练成为实施这些推理模型的核心技术。我们系统地实验并记录了多种因素对RL训练的影响，结果表明，RL训练显著提升了Qwen2.5-32B基础模型的响应长度和测试准确率。此外，即使像DeepSeek-R1-Distill-Qwen-1.5B这样已达到较高性能的模型，经过RL训练后也能实现进一步优化，在AIME 2024的准确率达到39.33%。除了RL训练外，我们还探索了工具操作的使用，发现它对大型推理模型的推理性能有显著提升，在AIME 2024上实现了86.67%的 высок效准确率，显示了该方法在增强模型能力方面的有效性。相关资源已在STILL项目官网发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:23:26 GMT</pubDate>
</item>
<item>
<title>Linear-MoE：集成线性序列建模与专家混合模型的高效系统</title>
<link>https://arxiv.org/abs/2503.05447</link>
<guid>https://arxiv.org/abs/2503.05447</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍Linear-MoE系统，结合LSM与MoE提高模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Linear-MoE，一个将线性序列建模（LSM）与混合专家模型（MoE）相结合的大规模模型训练系统。Linear-MoE利用LSM模块的线性复杂性序列建模优势和MoE层的稀疏激活特性，旨在提供高性能和高效的训练。系统包括建模子系统和训练子系统，前者支持所有LSM实例的统一框架，后者通过各种先进的并行技术（尤其是为Linear-MoE模型设计的序列并行性）实现高效训练。此外，本文还探讨了将Linear-MoE层与标准Transformer-MoE层结合的混合模型，进一步增强了模型的灵活性和性能。对两个模型系列A0.3B-2B和A1B-7B的评估显示，Linear-MoE在保持竞争性能的同时实现了效率提升，展现了其作为下一代基础模型架构的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05447" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:05:22 GMT</pubDate>
</item>
<item>
<title>VideoPainter：一种高效的视频修复方法</title>
<link>https://arxiv.org/abs/2503.05639</link>
<guid>https://arxiv.org/abs/2503.05639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoPainter通过双流架构实现高质量视频修复。</p><br /><br /><p><strong>摘要：</strong> VideoPainter是一种新颖的双流架构模型，旨在提高视频修复的效率和质量，解决现有方法在复原全遮挡对象和兼顾背景与前景生成的挑战。该模型采用了一种高效的上下文编码器，其参数仅占主干网络的6%，能够处理被遮挡视频并注入背景上下文。通过引入目标区域ID重采样技术，VideoPainter支持任意长度视频的修复，并基于当前视觉理解模型构建了VPData和VPBench数据集，促进分割基础的修复训练与评估。本研究显示，VideoPainter在视频质量、遮挡区域保存和文本一致性等关键指标上表现卓越，展示了广泛的应用潜力，包括视频编辑和编辑对数据生成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:30:00 GMT</pubDate>
</item>
<item>
<title>可定制化视频异常检测技术及其应用</title>
<link>https://arxiv.org/abs/2503.04504</link>
<guid>https://arxiv.org/abs/2503.04504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出可定制化视频异常检测技术，解决传统模型的局限性。</p><br /><br /><p><strong>摘要：</strong> 视频异常检测（VAD）在计算机视觉中的视频分析和监控中至关重要，但现有模型依赖学习到的正常模式，难以在多样化的环境中应用，限制了其实用性。本研究提出了一种可定制化视频异常检测（C-VAD）技术及AnyAnomaly模型，允许用户定义文本作为异常事件，从而检测视频中包含特定事件的帧。通过无须微调大型视觉语言模型的上下文感知视觉问答，我们有效实施了AnyAnomaly。实验表明该模型在C-VAD数据集上表现优异，且在VAD基准数据集上也显示出竞争力，在UBnormal数据集上取得了最先进的结果，展示了其在多数据集上的优越泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:06:49 GMT</pubDate>
</item>
<item>
<title>EuroBERT: 高性能多语言编码器的开发与应用</title>
<link>https://arxiv.org/abs/2503.05500</link>
<guid>https://arxiv.org/abs/2503.05500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍EuroBERT，一个超越现有方案的多语言编码器系列。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视多语言编码器的发展，介绍了EuroBERT模型系列，该系列覆盖了欧洲及全球广泛使用的语言。尽管近年来生成式解码器模型的发展引起了关注，许多推动这一进展的创新与解码器并无直接关系。EuroBERT在多种任务上表现优越，包括多语言能力、数学和编程，支持高达8192个标记的序列。文中特别讨论了EuroBERT的设计决策、数据集构成及训练流程，并公开发布了EuroBERT模型及其中间训练检查点，以及我们的训练框架，以促进更广泛的研究与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:42:45 GMT</pubDate>
</item>
<item>
<title>基于语义分割的检索增强生成框架SAGE的研究</title>
<link>https://arxiv.org/abs/2503.01713</link>
<guid>https://arxiv.org/abs/2503.01713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SAGE框架，以提高检索增强生成在问答任务中的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的检索增强生成（RAG）框架SAGE，旨在解决现有方法在问答任务中存在的不足。现有方法因未考虑语义进行语料段分割，导致问题与段之间的相关性差，且在检索时受到数量与相关性的权衡影响。SAGE通过训练语义分割模型，将语料分割为语义完整的块，并设计动态选择算法，根据相关性得分的减少速度选择最相关的块，确保检索块的精准性。此外，SAGE还允许大语言模型根据需要调整检索的上下文量。如果块过多或不足，模型将相应调整。实验结果表明，SAGE在问答质量上平均优于基准方法61.25%，同时降低了模型推理中的噪声上下文，从而实现了49.41%的成本效率提升，为提升RAG的性能提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:23:51 GMT</pubDate>
</item>
<item>
<title>TrajectoryCrafter: 精确控制单目视频摄像机轨迹的新方法</title>
<link>https://arxiv.org/abs/2503.05638</link>
<guid>https://arxiv.org/abs/2503.05638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新型方法，实现对单目视频摄像机轨迹的精确重定向。</p><br /><br /><p><strong>摘要：</strong> 本文提出了TrajectoryCrafter，一个创新的方法，用于重定向单目视频的摄像机轨迹。通过将确定性的视图变换与随机内容生成分离，我们的方法实现了对用户指定摄像机轨迹的精确控制。我们提出了一种新颖的双流条件视频扩散模型，该模型同时整合点云渲染和源视频作为条件，确保精准的视图变换和一致的4D内容生成。同时，我们通过创新的双重重投影策略，构建了一种混合训练数据集，结合了网络规模的单目视频和静态多视图数据集，从而显著增强了我们方法在不同场景下的强鲁棒性。对多视图和大规模单目视频的广泛评估表明了我们方法的优越性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:24:39 GMT</pubDate>
</item>
<item>
<title>基于低秩适应的代码检索参数高效微调方法</title>
<link>https://arxiv.org/abs/2503.05315</link>
<guid>https://arxiv.org/abs/2503.05315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于低秩适应的微调方法，提高代码检索效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于低秩适应（LoRA）的参数高效微调方法，旨在增强代码检索系统的性能。当前的开源模型如CodeBERT和UniXcoder在捕捉代码的语法和上下文细微差别上存在局限，而高性能的专有系统又带来了较高的计算成本。我们的研究通过构建任务特定的适配器，将可训练参数减少至基模型的不到2%，从而实现了在大规模代码语料库上快速微调（在两台H100 GPU上，2百万样本在25分钟内完成）。实验结果显示，对于Code2Code检索，平均倒数排名（MRR）提升高达9.1%；而在Text2Code检索任务中，各种编程语言下的表现提升可达86.69%。任务和语言的适应性差异则有助于深入探讨代码检索对语法和语言变体的敏感性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 00:51:02 GMT</pubDate>
</item>
<item>
<title>R1-Searcher：提升大型语言模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2503.05592</link>
<guid>https://arxiv.org/abs/2503.05592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-Searcher通过引入外部搜索提升大型语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的大型推理模型（LRMs）展示了强化学习（RL）在增强大型语言模型（LLMs）复杂推理能力方面的潜力，但在处理时间敏感或知识密集型问题时，常依赖内部知识，导致不准确和幻觉现象。为解决此问题，本文提出了R1-Searcher，一种新颖的基于结果的两阶段强化学习方法，旨在提升LLMs的搜索能力。此方法允许LLMs在推理过程中自动调用外部搜索系统，以获取额外知识。我们的框架完全依赖于强化学习，不需要冷启动时的过程奖励或提取。实验表明，R1-Searcher显著超越了以往的强大检索增强生成（RAG）方法，并且在与封闭源的GPT-4o-mini的比较中也表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:43:27 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习在多模态情感识别中的应用</title>
<link>https://arxiv.org/abs/2503.05379</link>
<guid>https://arxiv.org/abs/2503.05379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次将可验证奖励的强化学习应用于多模态情感识别，提升了模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了将可验证奖励的强化学习（RLVR）首次应用于多模态大型语言模型（Omni模型）进行情感识别的研究。该方法有效增强了模型在推理能力、情感识别准确性和泛化能力等三方面的表现。通过引入RLVR，不仅提升了模型在同分布数据上的整体性能，还在评估非同分布数据集时展现出更强的鲁棒性。此外，优化后的推理能力使得可以清晰分析视觉和音频信息在情感识别过程中的贡献，为优化多模态大型语言模型提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:40:46 GMT</pubDate>
</item>
<item>
<title>DeepSeek R1在多模态推理中的成功应用与挑战</title>
<link>https://arxiv.org/abs/2503.05132</link>
<guid>https://arxiv.org/abs/2503.05132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本报告展示了DeepSeek R1成功实现多模态推理的特征及其挑战。</p><br /><br /><p><strong>摘要：</strong> 本报告描述了DeepSeek R1在多模态推理中首次成功复制复杂推理特征的过程，强调通过强化学习在非SFT的2B模型Qwen2-VL-2B上直接应用于SAT数据集。该模型在CVBench上达到59.47%的准确率，提升约30%，超过了两个SFT设置的表现。此外，报告还讨论了在尝试使用强化学习促成R1类推理时所遇到的挑战，指出在指令模型上应用强化学习常常导致的推理轨迹简单化，以及简单的长度奖励对推理能力的激发无效。项目代码已公开，供进一步研究使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05132" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:39:12 GMT</pubDate>
</item>
<item>
<title>分支合并蒸馏法：提升大语言模型压缩与性能的创新策略</title>
<link>https://arxiv.org/abs/2503.04872</link>
<guid>https://arxiv.org/abs/2503.04872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出分支合并蒸馏法，实现高效的大语言模型压缩与性能提升。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的分支合并蒸馏法，该方法旨在在减少大语言模型（LLMs）尺寸时保持其性能。传统的模型蒸馏和迁移学习方法常常无法达到高精度，而我们提出的分支合并蒸馏法包含两个阶段：分支阶段通过领域特定的监督微调从大型教师模型中选择性提取知识到专门的学生模型；合并阶段则将这些学生模型合并，以实现跨领域的知识转移，提高模型的泛化能力。通过以DeepSeek-R1为教师模型，DeepSeek-R1-Distill-Qwen-32B为学生模型进行验证，最终合并的模型TinyR1-32B-Preview在多个基准测试中超越了其对应的学生模型，特别是在数学（提升5.5分）、编码（提升4.4分）和科学（提升2.9分）领域，且在AIME 2024测试中几乎与DeepSeek-R1表现持平。该方法为创建更小且高效的大语言模型提供了可扩展的解决方案，显著降低了计算成本和时间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:35:58 GMT</pubDate>
</item>
<item>
<title>多尝试任务提升大型语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2503.04808</link>
<guid>https://arxiv.org/abs/2503.04808</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多尝试任务的强化学习方法显著提升了大型语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期在大型语言模型（LLMs）强化学习（RL）方面的进展，特别是DeepSeek R1的应用，表明即使是简单的问答任务也能显著提升模型的推理能力。本文通过将任务修改为多尝试设置，推动了这一方法的扩展。在这种新设置中，模型对每个问题提供多个回答，并在错误回答后给予反馈。这种多尝试任务不仅鼓励模型改进以往的回答，还提高了搜索效率。实验结果显示，即使是小型LLM在多尝试任务上训练，其准确率从1次尝试的45.6%提升至2次尝试的52.5%，而在标准单轮任务下，仅从42.3%提高到43.2%。这些结果表明，与传统的单轮任务相比，多尝试任务训练的LLM在数学基准测试中表现更佳，能够更加有效地基于用户反馈完善其回答。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04808" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:29:35 GMT</pubDate>
</item>
<item>
<title>BEHAVIOR机器人套件：应对家庭任务的全面机器人控制框架</title>
<link>https://arxiv.org/abs/2503.05652</link>
<guid>https://arxiv.org/abs/2503.05652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了BEHAVIOR机器人套件，旨在提升机器人在家庭任务中的操作能力。</p><br /><br /><p><strong>摘要：</strong> 现实世界的家庭任务对移动操作机器人提出了重要挑战，成功地完成这些任务依赖于三项关键的全身控制能力：双手协调、稳定精确的导航和广泛的末端执行器操作能力。为了应对这些挑战，文章介绍了BEHAVIOR机器人套件（BRS），这是一个为家庭任务设计的全面操控框架。BRS基于一个具有双手和4自由度躯干的轮式机器人架构，集成了高性价比的全身遥操作界面用于数据收集，并引入了一种学习全身视觉运动策略的新算法。针对五个具有挑战性的家庭任务进行评估，BRS不仅强化了核心能力，同时还应对长距离导航、与可动和可变形物体的互动以及在狭小空间中的操作等复杂性，为实现日常家庭任务的全身操作提供了重要的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:51:04 GMT</pubDate>
</item>
<item>
<title>Sketch-of-Thought: 一种高效的语言模型推理框架</title>
<link>https://arxiv.org/abs/2503.05179</link>
<guid>https://arxiv.org/abs/2503.05179</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sketch-of-Thought框架通过减少令牌使用优化语言模型的推理过程。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型展现了卓越的推理能力，特别是在链式思维提示（CoT）方面，但这一过程常常导致冗长的中间输出，从而增加计算开销。我们提出了Sketch-of-Thought（SoT）框架，结合了认知启发式推理范式和语言约束，旨在以最小的令牌使用保持推理的准确性。SoT框架灵活，可根据认知科学整合任意自定义推理范式，并动态选择对应的概念链、分块符号和专家词汇等三种方法。在对15个多语言和多模态场景的推理数据集进行全面评估后，结果表明SoT能将令牌使用减少76%，且几乎没有影响准确性。在某些领域，如数学和多步推理，SoT甚至在使用显著较少令牌的情况下提高了推理准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05179" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:25:52 GMT</pubDate>
</item>
<item>
<title>UnifiedReward：提升多模态生成与理解的统一奖励模型</title>
<link>https://arxiv.org/abs/2503.05236</link>
<guid>https://arxiv.org/abs/2503.05236</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出UnifiedReward，一种用于多模态生成和理解的统一奖励模型。</p><br /><br /><p><strong>摘要：</strong> 随着人类偏好对齐的最新进展，多模态生成和理解得到了显著提升。本文提出UnifiedReward，这是首个针对多模态理解和生成评估的统一奖励模型，能够实现成对排名和点评分。模型的开发基于构建的大规模人类偏好数据集，涵盖图像和视频生成及理解任务。通过对视觉模型的输出进行成对排名和点筛选，自动构建高质量偏好对数据，以实现直接偏好优化（DPO）。实验结果表明，联合评估不同视觉任务的学习能够带来显著的互惠益处，并且应用于图像和视频理解/生成任务，显著提升了各领域的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05236" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:20:09 GMT</pubDate>
</item>
<item>
<title>引入遗忘门的变换器模型</title>
<link>https://arxiv.org/abs/2503.02130</link>
<guid>https://arxiv.org/abs/2503.02130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的变换器模型，利用遗忘门提升长上下文任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为遗忘变换器（FoX）的新模型，该模型通过在变换器中自然地引入遗忘门，优化上下文信息的利用。FoX通过以数据依赖的方式降低未归一化注意力得分的权重，展现出在长上下文语言建模、长度外推及短上下游任务中的优势表现，且在长上下游任务上与传统变换器表现持平。此外，该模型兼容FlashAttention算法，并且不需要任何位置信息嵌入。多项分析结果表明，FoX在长上下文能力上优于如Mamba-2、HGRN2和DeltaNet等递归序列模型。研究中还引入了“Pro”模块设计，结合递归模型常见的架构组件，显著提升了FoX及变换器的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:02:39 GMT</pubDate>
</item>
<item>
<title>双语模型训练对跨语言结构启动的影响研究</title>
<link>https://arxiv.org/abs/2503.03962</link>
<guid>https://arxiv.org/abs/2503.03962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究双语模型训练对结构启动的影响，发现语言对间的不对称性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了单语语言模型在开始接触第二语言训练时的变化，特别集中在小型双语模型的训练上，通过控制每种语言的数据量和语言接触顺序来发现证据。我们首先重现了以往的跨语言结构启动结果，控制训练数据量和语言接触后，发现不同语言对及其方向之间存在不对称效应。我们认为，这种不对称性可能会影响人类结构启动效应的假设。同时，对于相似度较低的语言对，结构启动效应的稳健性较差，揭示了跨语言转移学习及共享表示在语言类型多样性方面的潜在局限性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 11:38:45 GMT</pubDate>
</item>
<item>
<title>提升大型语言模型可信度的Truthfulness Separator Vector</title>
<link>https://arxiv.org/abs/2503.01917</link>
<guid>https://arxiv.org/abs/2503.01917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TSV以增强语言模型的真实与幻觉内容的分离能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在实际应用中的安全性面临幻觉问题。现有方法利用LLMs的潜在空间进行幻觉检测，但由于嵌入优化主要关注语言的连贯性，而非事实准确性，导致难以有效区分真实与幻觉内容。为此，我们提出了Truthfulness Separator Vector (TSV)，这是一种轻量且灵活的引导向量，通过在推理过程中重塑LLM的表示空间，增强真实和幻觉输出之间的分离。在我们的两阶段框架中，首先在少量标记示例上训练TSV，从而形成紧凑且区分良好的集群；其次，利用无标记的LLM生成内容，结合最佳传输算法进行伪标记，并通过基于置信度的过滤过程进行增强。大量实验证明，TSV在标记数据较少的情况下取得了业界领先的性能，并展示了其在各数据集上的强泛化能力，为实际LLM应用提供了切实可行的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 11:06:42 GMT</pubDate>
</item>
<item>
<title>LLMVoX: 一种轻量级的自回归语音合成系统</title>
<link>https://arxiv.org/abs/2503.04724</link>
<guid>https://arxiv.org/abs/2503.04724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMVoX是一种轻量级的语音合成系统，解决了现有模型的局限性。</p><br /><br /><p><strong>摘要：</strong> LLMVoX是一个轻量级的30M参数、自回归的流式文本转语音（TTS）系统，旨在高效生成高质量的语音，同时保留基础大语言模型（LLM）的能力。与现有语音增强LLM相比，LLMVoX在保证低延迟和较高用户听感评分的同时显著降低了词错误率。通过多队列令牌流系统，LLMVoX解耦了语音合成和LLM处理，从而支持无缝的无限长度对话。此外，其即插即用的设计使得在不同任务和模型架构上扩展变得容易。研究表明，LLMVoX在新的语言任务中仅需数据集适应就能达到低字符错误率。我们还将LLMVoX与视觉语言模型集成，创造出了一个具有语音、文本和视觉能力的全能模型，无需额外的多模态训练。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 08:19:28 GMT</pubDate>
</item>
<item>
<title>Union-of-Experts：提升MoE模型的动态路由与计算效率</title>
<link>https://arxiv.org/abs/2503.02495</link>
<guid>https://arxiv.org/abs/2503.02495</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Union-of-Experts通过动态路由提升了Mixture-of-Experts模型的性能和效率。</p><br /><br /><p><strong>摘要：</strong> 在大规模应用中，Mixture-of-Experts (MoE)模型由于其优秀的性能和计算效率而被广泛应用，但现有的MoE范式中的专家往往独立运作，缺乏高质量的专家交互，同时未有效扩展到注意力机制中，限制了进一步的效率提升。为此，我们提出了Union-of-Experts (UoE)，通过将Transformer分解为等价的专家组，并在输入数据和专家之间实施动态路由，来解决这些问题。我们的研究包含三个关键创新：进行MLP块和注意力块的等价专家分解；开发基于数据选择和专家选择的两种路由范式；设计UoE模型架构，包括选择性多头注意力(SMHA)和联合MLP专家(UoME)。实验表明，采用UoE模型在图像及自然语言任务上超越了全注意力模型、最先进的MoE和高效变换器。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02495" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 06:08:09 GMT</pubDate>
</item>
<item>
<title>优化大型语言模型翻译以克服翻译腔问题</title>
<link>https://arxiv.org/abs/2503.04369</link>
<guid>https://arxiv.org/abs/2503.04369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨如何减少大型语言模型中的翻译腔现象。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在机器翻译中取得了显著成功，但翻译腔问题仍然存在，表现为过于字面和不自然的翻译。本文系统评估了LLM生成翻译中的翻译腔 prevalence，并探讨了其在监督性微调过程中的根源。为此，我们提出通过优化黄金参考和过滤不自然的训练实例来减轻这些偏见的方法。实验结果表明，这些方法显著降低了翻译腔现象，同时提高了翻译的自然性，结果经过人工评估和自动指标验证。我们的研究强调了在训练阶段调整以优化LLM翻译输出的重要性，为实现更流畅、符合目标语言的翻译铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 05:25:00 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型推理能力的新框架：LINGOLY-TOO 的应用</title>
<link>https://arxiv.org/abs/2503.02972</link>
<guid>https://arxiv.org/abs/2503.02972</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新框架以减少对大型语言模型推理能力的过高估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架，可有效评估大型语言模型（LLMs）的推理能力，减少因数据暴露带来的过高估计。我们开发了创新的评估基准 LINGOLY-TOO，通过正字法模板动态模糊真实语言的书写系统，以生成众多问题变体。这些变体保留了解题所需的推理步骤，同时降低特定问题实例在模型训练数据中出现的可能性。实验结果表明，包括 OpenAI o1-preview 和 DeepSeem R1 在内的前沿模型在高级推理上表现不佳。此外，分析显示 LLMs 在相同问题的不同排列中准确率差异明显，且通常在原始正字法的问题上表现更好。这些发现揭示了 LLMs 回应生成的复杂性，并提供了依据，表明先前数据暴露会导致对其推理能力的过高评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02972" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:50:00 GMT</pubDate>
</item>
<item>
<title>IFIR：评估专家领域指令跟随信息检索的首个综合基准</title>
<link>https://arxiv.org/abs/2503.04644</link>
<guid>https://arxiv.org/abs/2503.04644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IFIR是首个评估专家领域信息检索的综合基准，涵盖四个领域的2426个高质量示例。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了IFIR，这是第一个为评估专家领域指令跟随信息检索而设计的综合基准，包含2426个高质量示例，覆盖金融、法律、医疗和科学文献四个专业领域的八个子集。每个子集针对特定的领域检索任务，模拟现实场景中对定制指令的需求。IFIR通过引入不同复杂度的指令，允许对指令跟随检索能力进行详细分析。此外，文章提出了一种新颖的基于大型语言模型(LLM)的评估方法，以提供更精确、可靠的模型性能评估。通过对包括LLM模型在内的15种前沿检索模型进行广泛实验，结果显示当前模型在有效跟随复杂的领域特定指令方面面临显著挑战，并提供深入分析以凸显这些局限，旨在为未来检索器开发的进步提供宝贵见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:37:52 GMT</pubDate>
</item>
<item>
<title>提升LLM后训练量化性能的精确敏感性度量</title>
<link>https://arxiv.org/abs/2503.01901</link>
<guid>https://arxiv.org/abs/2503.01901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PQI和ReQuant框架，以提升LLM后训练量化的准确性和效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了后训练量化在大型语言模型（LLM）中的应用及其成本问题。过去的方法依赖于敏感性度量来预处理权重，但现有的梯度和Hessian基度量存在显著低估量化对损失函数影响的缺陷。为此，本文提出了后量化积分（PQI），作为一种更精确的细粒度敏感性度量，旨在提高量化的准确性。此外，我们还提出了一个名为ReQuant的框架，结合了自适应异常值选择和逐步显著权重分离两种关键组件。实验结果表明，ReQuant在提升现有后训练量化方法上表现出显著的2.66困惑度提升，尤其是在Llama 3.2 1B模型上效果显著。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:23:41 GMT</pubDate>
</item>
<item>
<title>高效采样贝叶斯逆问题的条件流匹配与变压器架构结合</title>
<link>https://arxiv.org/abs/2503.01375</link>
<guid>https://arxiv.org/abs/2503.01375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合条件流匹配与变压器架构，可高效采样贝叶斯逆问题的后验分布。</p><br /><br /><p><strong>摘要：</strong> 贝叶斯逆问题的求解在后验分布复杂性和传统采样方法的计算成本方面仍然存在显著挑战。本文探讨了如何基于一系列观测结果及前向模型，恢复条件于实验数据的参数分布。我们提出在条件流匹配（CFM）和变压器架构相结合的方法，能够有效地从这种依赖于可变数量观察数据的分布中进行采样，这为贝叶斯逆问题的求解提供了一种新的高效途径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 02:51:01 GMT</pubDate>
</item>
<item>
<title>长范围依赖的双重互信息缩放法则及其在语言建模中的应用</title>
<link>https://arxiv.org/abs/2503.04725</link>
<guid>https://arxiv.org/abs/2503.04725</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种双重互信息缩放法则，揭示长范围依赖在语言建模中的重要性。</p><br /><br /><p><strong>摘要：</strong> 文章严格建立了一种适用于自然语言的双重互信息缩放法则，阐明了其如何调节长范围依赖性。该缩放法则与传统的两点互信息不同，能够独立缩放，是理解长上下文语言建模的关键。作者基于这一法则，提出了长上下文语言建模（L^2M）条件，关联模型有效建模长上下文长度的能力与其存储过去信息的潜在状态容量之间的关系。通过在变换器和状态空间模型上的实验验证，结果表明这一理论基础将在大型语言模型的发展中指导更长上下文长度的实现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04725" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 01:42:13 GMT</pubDate>
</item>
<item>
<title>EgoLife：基于AI的自我中心生活助理系统</title>
<link>https://arxiv.org/abs/2503.03803</link>
<guid>https://arxiv.org/abs/2503.03803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoLife旨在通过AI眼镜提升个人效率，并发布相关数据集和模型。</p><br /><br /><p><strong>摘要：</strong> EgoLife项目开发了一种基于AI的自我中心生活助理，利用可穿戴眼镜提升个人效率。为了奠定该助手的基础，我们进行了一项全面的数据收集研究，六名参与者共同生活一周，通过AI眼镜记录其日常活动，形成了一个300小时的EgoLife数据集，涵盖多视角与多模态的日常生活场景。基于该数据集，我们推出EgoLifeQA，一个针对生活导向的问题回答套件，旨在回答与日常生活相关的实际问题。为应对开发稳健的视觉-音频模型、身份识别及长期上下文问答等技术挑战，我们引入EgoButler系统，包含训练于自我中心数据集的EgoGPT和支持超长上下文问题回答的EgoRAG。实验研究验证了其工作机制，并揭示了关键因素和瓶颈，指导未来的改进。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:44:13 GMT</pubDate>
</item>
<item>
<title>音频理解与推理的先进模型：Audio Flamingo 2</title>
<link>https://arxiv.org/abs/2503.03983</link>
<guid>https://arxiv.org/abs/2503.03983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了音频语言模型Audio Flamingo 2及其在音频理解上的创新。</p><br /><br /><p><strong>摘要：</strong> 本论文介绍了Audio Flamingo 2（AF2），一种具备先进音频理解和推理能力的音频语言模型（ALM），对于人类和AI顺利与环境互动至关重要。AF2结合了自定义的CLAP模型、用于细致音频推理的合成音频问答数据以及多阶段课程学习策略，仅使用一个3B参数的小型语言模型，就在20多个基准测试中超越了大型开源和专有模型。此外，我们首次将音频理解扩展到长音频片段（30秒至5分钟），并提出LongAudio，一个用于训练ALM在长音频字幕和问答任务上的大型新数据集。针对LongAudio微调AF2，最终在我们提出的LongAudioBench上取得了优异表现，这是一个旨在评估ALM在长音频理解能力上的专家注释基准。我们还进行了广泛的消融研究，以确认我们方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:12:47 GMT</pubDate>
</item>
<item>
<title>预测GitHub对话中的毒性与偏离现象的主动调节策略</title>
<link>https://arxiv.org/abs/2503.02191</link>
<guid>https://arxiv.org/abs/2503.02191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究旨在预测GitHub对话中的毒性及其偏离现象。</p><br /><br /><p><strong>摘要：</strong> 软件项目的成功依赖于多元化背景个体的参与，但有毒语言和负面互动会阻碍贡献者的参与和留存。本文旨在预测GitHub对话中的偏离现象及其导致的毒性问题。研究中策划了一个包含202个有毒对话及其偏离点的独特数据集，并与696个非毒性对话对比分析。通过分析数据集，识别出有毒对话及偏离点的特征，包括个人代词、否定词以及与沮丧和焦虑相关的语调模式等。基于这些观察，提出了一种主动调节方法，自动检测并解决潜在有害对话。通过现代大型语言模型（LLMs），应用对话轨迹总结技术，有效捕捉讨论的演变并识别早期的偏离迹象。实验结果表明，该方法在预测对话偏离方面达到了69%的F1分数，显著优于基线方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02191" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:11:25 GMT</pubDate>
</item>
<item>
<title>大语言模型信息扭曲研究：迭代生成的传播影响</title>
<link>https://arxiv.org/abs/2502.20258</link>
<guid>https://arxiv.org/abs/2502.20258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大语言模型在迭代生成中产生信息扭曲，影响内容可靠性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大语言模型在在线内容生成中的信息扭曲现象，受到人类沟通中‘破碎电话’效应的启发。通过基于翻译的实验，我们发现信息随着迭代生成而逐渐扭曲，扭曲程度受语言选择和生成链的复杂性影响。尽管信息 degradation 是不可避免的，但通过战略性提示技术可以减轻这种影响。这些发现为关于人工智能介导的信息传播的长期影响提供了重要思考，质疑了大语言模型在迭代工作流程中生成内容的可靠性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:56:18 GMT</pubDate>
</item>
<item>
<title>PokéChamp: an Expert-level Minimax Language Agent</title>
<link>https://arxiv.org/abs/2503.04094</link>
<guid>https://arxiv.org/abs/2503.04094</guid>
<content:encoded><![CDATA[
We introduce Pok\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\'emon battles. Built on a general framework for two-player competitive games, Pok\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\'eChamp consistently outperforms the previous best LLM-based bot, Pok\'ellmon powered by GPT-4o, with a 64% win rate. Pok\'eChamp attains a projected Elo of 1300-1500 on the Pok\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:53:38 GMT</pubDate>
</item>
<item>
<title>STORM：提升视频理解效率的时空编码新架构</title>
<link>https://arxiv.org/abs/2503.04130</link>
<guid>https://arxiv.org/abs/2503.04130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STORM通过引入时空编码器提升视频理解和效率。</p><br /><br /><p><strong>摘要：</strong> STORM是一种新颖的架构，旨在改善视频理解效果，尤其是在处理长视频时。传统的多模态大语言模型在视觉背部独立处理视频帧，缺乏明确的时间建模能力，限制了其捕捉动态模式的能力。STORM通过在图像编码器和大语言模型之间加入专门的时空编码器，利用Mamba状态空间模型来整合时间信息，生成丰富的编码表示，从而增强了视频推理能力，同时实现有效的令牌减少策略，如测试时采样和基于训练的时空池化。这些技术的结合，不仅大幅减少了计算需求，还有效提升了性能。STORM在多个长视频理解基准测试中表现优异，性能提升超过5%，计算成本降低高达8倍，解码延迟减少2.4-2.9倍，提供了一种高效且稳健的解决方案，以应对长时间的上下文视频理解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:53:09 GMT</pubDate>
</item>
<item>
<title>LanDiff：融合自回归语言模型与扩散模型的文本生成视频新框架</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LanDiff是一个创新的文本生成视频框架，结合了自回归和扩散模型优势。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LanDiff的混合框架，旨在克服自回归语言模型和扩散模型在文本生成视频（T2V）中的固有限制。LanDiff通过粗到细的生成过程，将两种模型的优势结合起来。其架构包括三项创新：首先，引入语义分词器，将3D视觉特征压缩为紧凑的1D离散表示，实现约14,000倍的压缩率；其次，使用语言模型生成具有高级语义关系的语义标记；最后，采用流式扩散模型将粗略语义细化为高保真视频。实验结果表明，LanDiff模型在VBench T2V基准上得分85.43，超越了目前最先进的开源模型Hunyuan Video（13B）及其他商业模型，尤其在长视频生成方面表现尤为突出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:52:33 GMT</pubDate>
</item>
<item>
<title>HybridNorm: 一种新型混合归一化策略提高深度Transformer模型性能</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HybridNorm通过结合Pre-Norm和Post-Norm的方法，实现深度Transformer训练的稳定性和性能提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种称为HybridNorm的混合归一化策略，旨在解决深度Transformer网络训练中的层归一化位置问题。HybridNorm结合了Pre-Norm和Post-Norm的优点：在每个Transformer块的注意力机制中应用QKV归一化，而在前馈网络（FFN）中采用Post-Norm。该设计不仅稳定了训练过程，还提高了特别是在大语言模型（LLMs）环境中的性能。通过在稠密和稀疏架构上的全面实验，HybridNorm始终超越了Pre-Norm和Post-Norm方法，在多个基准测试中取得了先进的结果。这些发现显示HybridNorm作为一种更稳定有效的技术，具备提升深度Transformer模型训练和性能的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:04:06 GMT</pubDate>
</item>
<item>
<title>START：一种集成工具的长链推理模型</title>
<link>https://arxiv.org/abs/2503.04625</link>
<guid>https://arxiv.org/abs/2503.04625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了通过工具集成显著提升推理能力的START模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了START（自我学习推理者与工具），一种通过集成外部工具显著增强推理能力的长链推理大型语言模型。传统的大型推理模型（如OpenAI-o1和DeepSeek-R1）在复杂推理任务中表现出色，但往往存在幻觉和低效的问题。START利用代码执行进行复杂计算、自检、探索多种方法和自我调试，旨在克服这些限制。START的核心创新是其自学习框架，包含两个关键技术：1) Hint-infer，通过在推理过程中插入设计的提示，提升模型利用外部工具的能力；2) Hint RFT，通过对推理轨迹进行评分、过滤以及修改，实现模型的精细调整。经调优的QwQ-32B模型在多个科学问答与数学基准测试中表现优异，显示出与最先进模型相当的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:35:47 GMT</pubDate>
</item>
<item>
<title>FuseChat-3.0: 整合多种语言模型的高效新模型</title>
<link>https://arxiv.org/abs/2503.04222</link>
<guid>https://arxiv.org/abs/2503.04222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FuseChat-3.0通过融合多种模型提升语言处理性能。</p><br /><br /><p><strong>摘要：</strong> FuseChat-3.0是一套大型语言模型的工具，旨在通过整合多种来源模型的优势，开发出更紧凑的目标语言模型。源模型包括Gemma-2-27B-it、Mistral-Large-Instruct-2407、Qwen-2.5-72B-Instruct和Llama-3.1-70B-Instruct，而目标模型则重点关注一些广泛使用的小型变体。该模型的训练流程包括监督微调(SFT)和直接偏好优化(DPO)两个阶段，以增强目标模型的表现。在14个基准测试中，使用Llama-3.1-8B-Instruct作为目标模型的融合方法，平均提升6.8分，特别是在指令跟随基准测试中，分别实现了显著的37.1分和30.1分的提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:20:34 GMT</pubDate>
</item>
<item>
<title>基于反馈和编辑模型的推理时扩展方法</title>
<link>https://arxiv.org/abs/2503.04378</link>
<guid>https://arxiv.org/abs/2503.04378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法，通过反馈和编辑模型进行开放式任务的推理时扩展。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了推理时扩展在模型成功中的重要性，并指出现有技术主要限于可验证答案的任务，如数学和逻辑推理。我们借鉴人类在开放式任务中反复尝试和改进的过程，设计并训练了专门的反馈和编辑模型，旨在实现开放领域任务的推理时扩展。在我们的模型架构中，一个模型生成初步响应，第二个模型提供反馈，然后第三个模型对响应进行编辑。通过优化初步响应草稿数量、有效的反馈和编辑响应，我们发现性能在Arena Hard基准测试中显著提升，能够达到92.7的最先进水平，超越OpenAI o1-preview-2024-09-12和DeepSeek R1。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:10:18 GMT</pubDate>
</item>
<item>
<title>Highlighted Chain-of-Thought Prompting提升大型语言模型的响应准确性</title>
<link>https://arxiv.org/abs/2503.02003</link>
<guid>https://arxiv.org/abs/2503.02003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出高亮思维链提示以提高大型语言模型的响应准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新技术——高亮思维链提示（HoT），旨在解决大型语言模型（LLMs）常见的事实错误问题。HoT通过在生成的响应中添加XML标签，突出显示输入查询中的关键信息，使用户能够验证和做出基于事实的决策。结果显示，在少量示例的设置下，HoT在17个任务中优于传统的链思维提示（CoT），涵盖算术、阅读理解和逻辑推理等多个领域。尽管在要求人类验证LLM的回应时，高亮可以帮助时间有限的参与者更有效地识别正确答案，但值得注意的是，当LLM给出错误响应时，HoT可能会误导用户以为答案是正确的。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02003" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 17:46:32 GMT</pubDate>
</item>
<item>
<title>基于图神经网络变分自编码器的多智能体协调方法</title>
<link>https://arxiv.org/abs/2503.02954</link>
<guid>https://arxiv.org/abs/2503.02954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于GNN-VAE的方法以提升多智能体协调速度和质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于图神经网络变分自编码器（GNN-VAE）的多智能体协调方案，以解决共享空间中多机器人导航的协调问题。在机器人交通密集的区域，传统的局部协调方法可能无法找到无死锁的解决方案，此时中央控制单元生成全局调度是适宜的。然而，中央协调方法的运行时复杂度随着问题规模的增加而显著上升。我们将协调问题转化为图问题，并使用混合整数线性规划（MILP）求解器收集真实数据。在训练阶段，学习框架将图问题的优质解编码至潜在空间。在推理过程中，从采样的潜在变量中解码得到解样本，并选择最低成本的样本进行协调。最终，选择具有最高性能指数的可行提案进行部署。数值结果表明，经过小规模问题训练的GNN-VAE方法在处理250个机器人的大规模问题时能够快速获得高质量解，显著快于其他基线方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:16:32 GMT</pubDate>
</item>
<item>
<title>基于信号时序逻辑的多样化自主决策方法研究</title>
<link>https://arxiv.org/abs/2503.02924</link>
<guid>https://arxiv.org/abs/2503.02924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用STL和扩散模型生成多样化、可控的自主代理行为。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了生成真实感模拟在自主系统中的应用，尤其是自动驾驶和人机交互。当前的驾驶模拟器在生成可控、多样且遵循规则的行为方面仍面临挑战。传统的基于规则的模型缺乏多样性，而学习型方法则未能显式遵循规则。为此，本文结合信号时序逻辑（STL）与扩散模型，提出了一种新的可控、多样化且合规的策略生成方法。研究首先在真实数据上校准STL，然后通过轨迹优化生成多样的合成数据，最后在扩增数据集中学习修正的扩散策略。实验结果表明，与其他基线方法相比，本研究的方法在生成规则合规且多样化的轨迹方面表现优越，且运行时仅为第二最佳方法的1/17。在闭环测试中，本方法达到了最高的多样性和规则满足率，并且碰撞率最低。此外，案例研究表明，其能够生成多样且接近理想值的轨迹。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:10:16 GMT</pubDate>
</item>
<item>
<title>重掩蔽扩散模型：提升离散扩散生成质量的新方法</title>
<link>https://arxiv.org/abs/2503.00307</link>
<guid>https://arxiv.org/abs/2503.00307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出重掩蔽扩散模型（ReMDM）以提升离散扩散生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对现代掩蔽离散扩散模型在生成过程中无法迭代修正的限制，提出了一种新的重掩蔽扩散模型（ReMDM）采样器。ReMDM通过应用自定义的重掩蔽反向过程，能够在生成自然语言和离散图像时进行多次迭代修正，从而提高生成质量。该方法不仅在计算预算有限时能保持高质量输出，还通过增加采样步骤，优化生成结果，使其接近自回归模型的效果。此外，ReMDM还在分子设计等科学领域中促进了扩散指导，推动了可控性在经典掩蔽和均匀噪声扩散的帕累托前沿的发展。项目提供了相关代码和博客链接，供研究者参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 14:57:45 GMT</pubDate>
</item>
<item>
<title>基于过程的自奖励方法提升大语言模型数学推理能力</title>
<link>https://arxiv.org/abs/2503.03746</link>
<guid>https://arxiv.org/abs/2503.03746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出基于过程的自奖励方法以增强大语言模型的数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于过程的自奖励方法，用于增强大语言模型（LLM）在数学推理方面的表现。由于传统的自奖励方法在数学推理中表现不佳，甚至可能导致性能下降，我们引入了长时间思考、逐步的LLM作为评判者和逐步喜好优化的策略。通过迭代的基于过程的自奖励，该新方法在多个数学推理基准上成功提升了LLM的性能，显示出自奖励在实现大语言模型推理超越人类能力方面的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:15:20 GMT</pubDate>
</item>
<item>
<title>小型语言模型Shakti在边缘设备上的应用研究</title>
<link>https://arxiv.org/abs/2503.01933</link>
<guid>https://arxiv.org/abs/2503.01933</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨Shakti小型语言模型在边缘设备上的应用与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Shakti小型语言模型（SLMs），包括Shakti-100M、Shakti-250M和Shakti-500M，旨在克服在边缘设备上部署大型语言模型时面临的计算需求高、能源消耗大和数据隐私风险等挑战。通过高效架构、量化技术和负责任的人工智能原则，Shakti系列支持智能手机、智能家电和物联网系统的本地智能化。我们深入探讨了它们的设计理念、训练流程以及在一般任务（如MMLU、Hellaswag）和专业领域（医疗、金融、法律）上的基准性能。研究表明，经过精心设计与微调的紧凑模型在实际边缘人工智能场景中可以满足甚至超出预期。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01933" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 05:49:04 GMT</pubDate>
</item>
<item>
<title>结构与文本检索的混合模型：MoR框架</title>
<link>https://arxiv.org/abs/2502.20317</link>
<guid>https://arxiv.org/abs/2502.20317</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MoR框架，结合结构与文本知识的检索，以提升查询回答能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的Mixture of Structural-and-Textual Retrieval（MoR）框架，旨在同时检索文本知识和结构知识，以优化查询回答的准确性。在规划阶段，MoR生成文本规划图，明确回答查询的逻辑；在推理阶段，结合结构遍历与文本匹配，获取候选知识；最后，在组织阶段，通过候选者的结构轨迹进行重新排序。大量实验结果表明，MoR在协调结构与文本检索方面具有显著优势，并揭示了不同查询逻辑下的检索表现不均衡和结构轨迹整合的好处。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20317" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 03:22:14 GMT</pubDate>
</item>
<item>
<title>对话助手中的问题重写与融合方法研究</title>
<link>https://arxiv.org/abs/2502.18860</link>
<guid>https://arxiv.org/abs/2502.18860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对话助手中的两种问题处理方法对生成任务的影响。</p><br /><br /><p><strong>摘要：</strong> 本文系统地探讨了对话助手中问题重写和融合的两种不同方法，并在两类生成任务上进行了验证：文本生成任务和多模态生成任务。研究发现，不同的应用场景和任务需求决定了最佳重写策略。具体而言，对于问答型的对话助手，重写方法表现最佳，而对于生成可视化或数据表的数据分析助手，融合方法则更为有效。研究还涉及了针对短期和长期对话的两种数据集，结果表明在数据分析助手中，查询融合总是优于其他方法，而在文本问答中，查询重写则表现最佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 03:20:40 GMT</pubDate>
</item>
<item>
<title>机器翻译后编辑中单词级质量估计的影响研究</title>
<link>https://arxiv.org/abs/2503.03044</link>
<guid>https://arxiv.org/abs/2503.03044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨单词级质量估计对机器翻译后编辑的影响。</p><br /><br /><p><strong>摘要：</strong> 本研究QE4PE考察了单词级质量估计 (QE) 在机器翻译 (MT) 后编辑中的影响，涉及42名专业后编辑者进行两种翻译方向的实地测试。我们比较了四种错误跨度高亮模式，包括监督型和基于不确定性的单词级QE方法，以识别最先进神经MT模型输出中的潜在错误。通过行为日志评估后编辑的努力和生产率，同时通过人工标注评估质量改善。研究发现，领域、语言和编辑者的速度是影响高亮有效性的关键因素，且人造与自动QE高亮之间存在适度差异，显示出在专业工作流程中准确性与可用性之间的差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:30:17 GMT</pubDate>
</item>
<item>
<title>通过分解医学知识提升视觉语言模型在医学异常检测中的表现</title>
<link>https://arxiv.org/abs/2503.03278</link>
<guid>https://arxiv.org/abs/2503.03278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出一种新方法，通过分解医学知识提升视觉语言模型在医学异常检测中的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）在医疗图像中的异常检测和定位能力，指出其在医学领域的开展仍较为有限。主要挑战在于医学术语的复杂性，导致病理异常术语与视觉特征之间的直接关联困难。为提升VLM在医学异常检测中的表现，研究者采用了一种新方法，通过分解医学知识，关注将医学概念细化为基本属性和常见视觉模式。这一策略增强了文本描述与视觉特征之间的对齐，提高了医疗图像中异常的识别和定位能力。研究基于0.23B Florence-2模型进行评估，结果表明其在异常定位上的表现接近于更大的7B LLaVA医学VLM，尽管仅使用了后者1.5% 的数据。此外，本方法在已知及未见异常检测中均表现出强大的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03278" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:29:15 GMT</pubDate>
</item>
<item>
<title>利用多元信号提升小型模型的指令跟随能力</title>
<link>https://arxiv.org/abs/2503.01836</link>
<guid>https://arxiv.org/abs/2503.01836</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨多元信号在小型模型训练中的应用，提出CrowdSelect指标。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了通过多样化信号改进小型模型的指令跟随能力，指出现有的合成指令数据选择策略主要依赖单一维度信号，未能充分捕捉具体领域中指令跟随的复杂性。我们提出三种基础指标，这些指标借助多样的LLM反应及奖励模型评估构建而成。特别是CrowdSelect，通过聚类方法整合多项指标，确保响应多样性。我们的实验表明，这些基础指标在4个基础模型的MT-bench和Arena-Hard上均表现出了持续的性能提升。CrowdSelect有效整合所有指标，在Full和LoRA微调中均实现了先进的表现，在Arena-Hard上提升了4.81%，在MT-bench上提高了11.1%。希望我们的研究能够为未来相关方向提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01836" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:20:38 GMT</pubDate>
</item>
<item>
<title>提升工具检索性能的ToolRet基准</title>
<link>https://arxiv.org/abs/2503.01763</link>
<guid>https://arxiv.org/abs/2503.01763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ToolRet基准，评估工具检索能力并优化大语言模型的实用性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ToolRet，一个异质工具检索基准，包含7.6k个多样化检索任务和43k个工具。针对现实世界应用，传统的信息检索（IR）模型在工具检索任务中的性能亟待探讨。尽管在常规IR基准上的表现优秀，这些模型在ToolRet上却表现不佳，导致工具使用大语言模型的任务通过率降低。为改善这一现状，本文还提供了一个超过20万实例的大规模训练数据集，显著提升IR模型的工具检索能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:12:07 GMT</pubDate>
</item>
<item>
<title>FLAME基准：联邦学习在机器人操控中的应用</title>
<link>https://arxiv.org/abs/2503.01729</link>
<guid>https://arxiv.org/abs/2503.01729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FLAME基准，为机器人操控中的联邦学习提供支持。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了FLAME（Federated Learning Across Manipulation Environments），这是一个为机器人操控设计的首个联邦学习基准。FLAME包含超过16万个专家演示的多种操控任务的大规模数据集，这些数据集来自多种模拟环境。此外，文章还提出了在联邦环境中进行机器人策略学习的训练与评估框架。通过在FLAME上评估标准的联邦学习算法，我们展示了其在分布式策略学习中的潜力，同时强调了面临的一些关键挑战。此基准的建立为可扩展、自适应及注重隐私的机器人学习奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:11:48 GMT</pubDate>
</item>
<item>
<title>大语言模型在软件漏洞检测中的效能研究</title>
<link>https://arxiv.org/abs/2503.01449</link>
<guid>https://arxiv.org/abs/2503.01449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究评估大语言模型在软件漏洞检测中的性能与挑战。</p><br /><br /><p><strong>摘要：</strong> 随着生成式人工智能的进步，大语言模型（LLMs）在软件工程中的应用日益广泛，解决了多个长期存在的挑战。然而，目前针对LLMs在软件漏洞检测（SVD）中的能力缺乏全面研究。本研究填补了这一知识空白，通过评估5种开源LLMs在SVD任务中的表现，使用包括提示工程、指令调优和序列分类微调等多种方法，并构建了涵盖Python、Java和JavaScript的86260个脆弱函数的数据集。此外，研究探讨了两种提升LLMs在SVD表现的策略，包括使用下采样平衡数据集进行模型重新训练和采用集成学习方法。实验结果表明，SVD对LLMs仍然是一项艰巨的任务。本研究为LLMs在SVD中的作用提供了深入理解，并为未来利用生成式人工智能提升软件安全实践提供了实际见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:11:25 GMT</pubDate>
</item>
<item>
<title>CognitiveDrone：面向复杂无人机任务的视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2503.01378</link>
<guid>https://arxiv.org/abs/2503.01378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CognitiveDrone是一种新型VLA模型，专为复杂无人机任务设计，表现出色。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CognitiveDrone的新型视觉-语言-动作（VLA）模型，专门为复杂无人机任务设计，具有先进的认知能力。该模型在超过8000个模拟飞行轨迹的数据集上进行训练，涵盖人类识别、符号理解和推理三个主要类别，能够根据第一人称视觉输入和文本指令生成实时的4D动作指令。为提高在复杂场景下的表现，我们提出了CognitiveDrone-R1，集成了额外的视觉-语言模型推理模块，以简化任务指令。实验评估使用我们的开源基准CognitiveDroneBench，结果显示CognitiveDrone-R1的成功率达77.2%，较基础模型提高了30%。这些结果突显了将高级推理能力融入无人机控制系统的有效性。我们的贡献包括开发了一种最先进的VLA模型和首个专门用于评估无人机认知任务的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:10:56 GMT</pubDate>
</item>
<item>
<title>瑞士法律翻译的挑战与SwiLTra-Bench创新解决方案</title>
<link>https://arxiv.org/abs/2503.01372</link>
<guid>https://arxiv.org/abs/2503.01372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SwiLTra-Bench在瑞士法律翻译中的应用及其对翻译质量的影响。</p><br /><br /><p><strong>摘要：</strong> 在瑞士，由于四种官方语言的存在，多语种法律翻译显得尤为重要。然而，传统的法律翻译依赖于既懂法律又会翻译的专业人士，这一过程造成了瓶颈，影响了司法的有效获取。为此，本文提出了SwiLTra-Bench，这是一个包含超过18万对瑞士法律翻译的多语言基准数据集，旨在评估基于大型语言模型（LLM）的翻译系统。研究显示，前沿模型在所有文档类型中具有优越的翻译性能，而专门的翻译系统在处理法律文件时表现出色，但在头注翻译中不足。通过严格的测试和人类专家验证，结果表明虽然精细调优公开的大型语言模型显著提高了翻译质量，但仍不及Claude-3.5-Sonnet等最佳的零-shot前沿模型。此外，本文还介绍了SwiLTra-Judge，这是与人类专家评估最一致的专门LLM评估系统。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01372" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:10:21 GMT</pubDate>
</item>
<item>
<title>GEN3C：增强的视频生成模型与精确摄像机控制</title>
<link>https://arxiv.org/abs/2503.03751</link>
<guid>https://arxiv.org/abs/2503.03751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEN3C模型通过3D缓存实现更精确的视频生成和摄像机控制。</p><br /><br /><p><strong>摘要：</strong> GEN3C是一种生成性视频模型，具备精确的摄像机控制和时序三维一致性。以往的视频生成模型多依赖于有限的三维信息，易出现对象不稳定的问题；而GEN3C通过预测种子图像的像素深度构建三维缓存，这为生成下一帧提供了精确的条件。在生成过程中，模型能专注于未观察区域以及推进场景状态，从而避免记忆先前生成内容的困难。这使得GEN3C在稀疏视图的新视角合成中表现出色，尤其是在驾驶场景和单目动态视频等挑战环境中，展现了比以往更精准的摄像机控制能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 22:13:22 GMT</pubDate>
</item>
<item>
<title>Babel：开创多语言大模型的新标准</title>
<link>https://arxiv.org/abs/2503.00865</link>
<guid>https://arxiv.org/abs/2503.00865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Babel是一个覆盖25种语言的开源多语言大模型，表现优越。</p><br /><br /><p><strong>摘要：</strong> Babel是一种开源的多语言大语言模型（LLM），旨在填补现有多语言LLM在语言覆盖方面的不足，特别是对较少资源语言的支持。该模型覆盖全球前25种语言，为90%以上的人口提供支持，并采用层扩展技术提高性能。Babel有两个变体：Babel-9B注重高效推理与微调，Babel-83B则在开放多语言LLM中树立新标准。经过广泛的多语言任务评估，Babel展现出超越同规模开放LLM的优越性能，并借助开源监督微调数据集，取得了显著成果，其中Babel-9B-Chat在10B规模的LLM中表现最佳，Babel-83B-Chat在多语言任务中达到商用模型的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:49:03 GMT</pubDate>
</item>
<item>
<title>自主车辆与人驱动车辆的双向交互框架研究</title>
<link>https://arxiv.org/abs/2503.00502</link>
<guid>https://arxiv.org/abs/2503.00502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种双向交互框架，提升自主车辆与人驱动车辆的互动能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种旨在提升自主车辆（AVs）与人驱动车辆（HVs）交互能力的双向交互框架——并行Actor-Reasoner框架。随着自主车辆的商业化发展，与人驾驶车辆的互动仍面临着意图表达的限制。通过利用大型语言模型（LLMs）所带来的双向人机沟通能力，本文提出的方法解决了推理速度慢与实时决策需求之间的矛盾。框架以LLM驱动的Reasoner与异构模拟人驱动车辆的交互为基础，建立了一种交互记忆数据库（Actor）。通过记忆分区模块和双层记忆检索模块，显著提升了Actor处理异构人驱动车辆的能力。研究结果表明，该框架在安全性与效率方面均有显著提升，且在多场景实地交互中展现出了良好的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:37:18 GMT</pubDate>
</item>
<item>
<title>ABC: 深度整合视觉与自然语言的多模态嵌入模型</title>
<link>https://arxiv.org/abs/2503.00329</link>
<guid>https://arxiv.org/abs/2503.00329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ABC模型通过结合图像特征与自然语言指令，提升多模态嵌入技术。</p><br /><br /><p><strong>摘要：</strong> ABC是一款开源的多模态嵌入模型，旨在解决现有视觉嵌入模型在处理模糊性和用户指令时的不足。与传统的CLIP-based方法不同，ABC使用视觉-语言模型骨干网络，深入整合图像特征和自然语言指令，显著提高了模态之间的互交作用，从而增强了用户对表示的控制能力。该模型在MSCOCO图像到文本检索中表现出色，并且在分类和视觉问答任务上在大型多模态嵌入基准测试中名列前茅。为评估其能力，我们设计了CtrlBench基准，要求在进行图像检索时交替使用文本指令与图像内容。ABC通过提供高质量的表示和灵活的自然语言控制，推动了多模态嵌入技术的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:33:37 GMT</pubDate>
</item>
<item>
<title>KodCode：用于编码训练的高质量合成数据集</title>
<link>https://arxiv.org/abs/2503.02951</link>
<guid>https://arxiv.org/abs/2503.02951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KodCode是一个系统验证的合成数据集，提升编码模型训练质量。</p><br /><br /><p><strong>摘要：</strong> KodCode是一个旨在解决高质量、可验证训练数据获取挑战的合成数据集，专注于编程任务，包括从简单到高级的各种题目。与现有代码资源不同，KodCode提供的问题-解决方案-测试三元组经过自我验证程序系统验证，以确保问题覆盖面广和正确性。其生成流程包括合成多种编程问题、生成解决方案和测试用例，并对复杂问题进行额外尝试。最后，通过将问题重写为多种格式并从推理模型中生成测试基础拒绝采样下的响应，完成数据合成。KodCode适合用于监督细化，并且配套的单元测试也为强化学习调优提供了良好的潜力。在多个编码基准（如HumanEval、MBPP、BigCodeBench和LiveCodeBench）上的细化实验表明，基于KodCode细化的模型达到了最先进的性能，超越了如Qwen2.5-Coder-32B-Instruct和DeepSeek-R1-Distill-Llama-70B等模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:31:01 GMT</pubDate>
</item>
<item>
<title>大型语言模型对齐的挑战与社会对齐框架的启示</title>
<link>https://arxiv.org/abs/2503.00069</link>
<guid>https://arxiv.org/abs/2503.00069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型的对齐挑战及社会框架的潜在解决方案。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLM）的发展，如何使其产生符合人类期待且与共享价值观一致的回应成为一项重要议题，称为对齐。然而，由于人类价值观的复杂性与技术方法的狭窄性之间的固有脱节，对齐依然充满挑战。当前的对齐方法常常导致目标的不准确规格，反映了不完全契约的更广泛问题，即开发者与模型之间缺乏覆盖所有场景的契约。本文主张，为改进LLM对齐，需引入社会对齐框架的见解，如社会、经济和契约对齐，并探讨这些领域的潜在解决方案。同时，考虑到社会对齐框架中不确定性的角色，讨论其在LLM对齐中的表现。最后，本文认为，LLM对齐目标的不完全性应被视为一种机会，而非完美规格的障碍，并呼吁在技术改进之外，需设计参与性对齐界面。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 18:39:55 GMT</pubDate>
</item>
<item>
<title>统一视频与动作模型：提升机器人任务性能的创新框架</title>
<link>https://arxiv.org/abs/2503.00200</link>
<guid>https://arxiv.org/abs/2503.00200</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">统一视频动作模型(UVA)结合视频生成与动作预测技术，提升机器人任务准确性与速度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了统一视频动作模型（UVA），旨在有效结合视频生成和动作预测，以提升机器人任务的表现。UVA通过学习联合视频-动作潜在表示，解耦视频和动作解码，实现了高准确性和高效的动作推理。该模型利用两个轻量级扩散头进行解耦解码，能够在推理过程中绕过视频生成，快速进行动作推理。通过掩蔽输入训练，UVA还具备多样化的功能，可以处理政策学习、前向和逆向动力学建模及视频生成等多种任务。经过大量实验，UVA已被证实是一个通用解决方案，在多种机器人任务中表现优越，且相比于特定应用方法未显著降低性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00200" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 16:12:24 GMT</pubDate>
</item>
<item>
<title>高效的KV缓存压缩方法Q-Filters在自回归语言模型中的应用</title>
<link>https://arxiv.org/abs/2503.02812</link>
<guid>https://arxiv.org/abs/2503.02812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Q-Filters，提升KV缓存压缩效率，优化文本生成过程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的KV缓存压缩方法Q-Filters，该方法通过对Query和Key向量的特性进行探索，有效近似注意力分数，而无需计算完整的注意力图。Q-Filters通过一个上下文无关的投影过滤掉不重要的Key-Value对，避免了对注意权重的直接访问，因此与FlashAttention兼容。在长上下文设置下的实验表明，Q-Filters在检索任务上与基于注意力的压缩方法SnapKV具有竞争力，并在文本生成任务中显著优于Streaming-LLM等高效压缩方案，尤其是在针对复杂任务的情况下，Q-Filters以x32的压缩比例实现了99%的准确率，并将生成过程中的困惑度降低了65%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:29:36 GMT</pubDate>
</item>
<item>
<title>味觉信息与音乐生成模型的关系研究</title>
<link>https://arxiv.org/abs/2503.02823</link>
<guid>https://arxiv.org/abs/2503.02823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了味觉信息如何转化为音乐，以促进多感官交互。</p><br /><br /><p><strong>摘要：</strong> 近年来，神经科学和心理学研究发现味觉与听觉之间存在直接关系。本文探讨了能够将味觉信息转化为音乐的多模态生成模型，基于此研究背景，我们回顾了该领域的现状，并强调了关键发现和方法。此外，我们设计了一项实验，通过对生成音乐模型（MusicGEN）的精细调优，生成基于详细味觉描述的音乐作品。结果显示，参与者（n=111）的评估认为，经过调优的模型比未调优的模型所生成的音乐更能体现输入的味觉描述。这项研究在理解和开发人工智能、声音与味觉之间的具身交互方面迈出了重要一步，为生成式人工智能领域开启了新的可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:04:04 GMT</pubDate>
</item>
<item>
<title>Tabby: 一种用于合成表格数据的强大Transformer后训练方法</title>
<link>https://arxiv.org/abs/2503.02152</link>
<guid>https://arxiv.org/abs/2503.02152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tabby是一种改进Transformer架构以合成高质量表格数据的方法。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型技术的发展，合成文本数据的质量显著提高，但对表格数据的合成关注相对较少。我们提出了Tabby，这是一种简洁而强大的后训练修改，适用于标准Transformer语言模型架构，以实现表格数据的合成。Tabby通过Gated Mixture-of-Experts来表示列间差异，采用列特定的参数集合。实证结果显示，Tabby生成的数据质量与真实数据相当。通过将我们的新型表格训练技术Plain与Tabby配对，我们观察到质量较之前方法提高了多达44%。此外，Tabby的应用范围超越表格数据，针对嵌套JSON数据集也达到了与真实数据相当的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:03:42 GMT</pubDate>
</item>
<item>
<title>TokenOCR：一种针对文本图像任务的首个 token 级视觉基础模型</title>
<link>https://arxiv.org/abs/2503.02304</link>
<guid>https://arxiv.org/abs/2503.02304</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokenOCR 是首个针对文本图像任务的 token 级视觉基础模型，能提高模型预测准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉基础模型（VFM）在多模态大语言模型（MLLM）中的应用不断增加，但在处理含小密集文本的图像时仍存在基本预测错误。为解决这一问题，本文开发了 TokenOCR，这是一个专门针对文本图像任务的 token 级视觉基础模型，旨在支持多种传统下游应用程序。为促进 TokenOCR 的预训练，研究团队还设计了一个高质量数据生产管道，构建了首个 token 级图像文本数据集 TokenIT，包含2千万张图像和18亿对 token-mask。此外，利用其卓越的图像文本能力，团队无缝替换了以往的 VFM，构建了一种文档级 MLLM，即 TokenVL，用于基于 VQA 的文档理解任务。实验结果证明了 TokenOCR 和 TokenVL 的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02304" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:03:27 GMT</pubDate>
</item>
<item>
<title>基于强化学习的离散时间混合自动机学习框架</title>
<link>https://arxiv.org/abs/2503.01842</link>
<guid>https://arxiv.org/abs/2503.01842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的框架，通过强化学习识别模式切换，实现无轨迹分割的学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种离散时间混合自动机学习（DHAL）框架，该框架使用基于策略的强化学习技术，在不进行轨迹分割或事件函数学习的情况下，识别和执行模式切换。混合动态系统结合了连续流动与离散模式切换，能够有效建模像四足机器人行走等复杂机器人任务。传统的基于模型的方法依赖于预定义的步态，而无模型方法在显式模式切换知识方面存在不足。现有方法通过轨迹分割来识别离散模式，但在没有轨迹标签或分割的条件下，学习高维复杂的刚体动力学仍然是一个具有挑战性的开放问题。我们的方法结合了β策略分布和多重批评者架构，以模拟接触引导的运动，特别是在四足机器人滑板任务中表现出色。通过仿真和现实世界测试，我们验证了该方法在混合动态系统中的强大性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 10:37:18 GMT</pubDate>
</item>
<item>
<title>平衡回归中的均匀性学习方法</title>
<link>https://arxiv.org/abs/2503.00876</link>
<guid>https://arxiv.org/abs/2503.00876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨在不平衡回归中实现均匀特征分布的重要性。</p><br /><br /><p><strong>摘要：</strong> 在表示学习中，均匀性指的是潜在空间中特征的均匀分布。以往的研究表明，提高均匀性有助于学习被低估类别，但大多数集中在分类任务，不平衡回归的表示空间尚未探索。与分类不同，回归任务特征是连续的，因此需采用不同方法。我们通过引入包络损失和同质性损失，确保在潜在空间中实现均匀性。包络损失促使特征均匀覆盖超球面，而同质性损失则确保表示间均匀间隔。我们的方法通过代理驱动表示学习框架（SRL）将几何原则融入数据表示。实验结果强调了在不平衡回归任务中均匀性的重要性，验证了我们的几何损失函数的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:25:03 GMT</pubDate>
</item>
<item>
<title>Q-EVAL-100K：评估文本与视觉内容的综合数据集</title>
<link>https://arxiv.org/abs/2503.02357</link>
<guid>https://arxiv.org/abs/2503.02357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Q-EVAL-100K数据集提升文本与视觉内容的评估能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Q-EVAL-100K数据集，该数据集专为评估文本与视觉内容的视觉质量和对齐水平而设计，包含960K个人工标注的平均意见分数(MOS)。此数据集包括60K图像和40K视频实例，在文本到图像和文本到视频模型的评估中具有重要意义。通过Q-Eval-Score模型，研究者能够有效评估视觉质量和对齐，尤其在长文本提示对齐方面有显著改进。实验结果显示，该模型在视觉质量和对齐性能上均表现优异，并在其他基准测试中具有良好的泛化能力，突显了Q-EVAL-100K数据集的重大价值。相关数据和代码将会在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 06:09:41 GMT</pubDate>
</item>
<item>
<title>IterPref：提升代码大语言模型的偏好学习框架</title>
<link>https://arxiv.org/abs/2503.02783</link>
<guid>https://arxiv.org/abs/2503.02783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IterPref框架通过精确定位错误区域，提升代码生成模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了IterPref，一个新颖的偏好对齐框架，旨在通过模仿人类迭代调试来优化代码大语言模型（Code LLMs）。现有方法基于测试用例的成功构建偏好对，通常将较高通过率的样本视为正样本，低通过率样本视为负样本，这种方法缺乏对代码具体错误的识别，限制了模型学习有效错误修正模式的能力。为了解决这一问题，IterPref明确定位错误区域，并通过量身定制的DPO算法对相关令牌进行对齐。同时，我们引入了CodeFlow数据集，其中样本经过多次迭代优化，直至通过所有测试，改动捕捉了错误修正过程。实验结果表明，配备IterPref的多样化代码大语言模型在代码生成方面显著提升了性能，并在诸如BigCodeBench等复杂任务上表现优异。深入分析显示，IterPref有效减少了错误。我们的代码和数据将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02783" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:54:00 GMT</pubDate>
</item>
<item>
<title>RectifiedHR：一种高效的无训练高分辨率图像生成方法</title>
<link>https://arxiv.org/abs/2503.02537</link>
<guid>https://arxiv.org/abs/2503.02537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RectifiedHR提供了一种高效的无训练高分辨率图像生成方案。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RectifiedHR，一种高效且无需训练的高分辨率图像生成解决方案。尽管现有的高分辨率生成方法普遍存在效率低下或操作复杂的问题，RectifiedHR通过引入噪声刷新策略，仅需少量代码便能释放模型的高分辨率生成能力，从而提高效率。此外，作者首次观察到高分辨率图像生成过程中可能导致图像模糊的能量衰减现象，并提出能量修正策略，通过调整无分类器引导的超参数，显著提升生成效果。经过与多种基线方法的广泛比较，RectifiedHR表现出显著的效果和效率优势，完全符合训练免费的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:53:05 GMT</pubDate>
</item>
<item>
<title>LADDER: 自主驱动的自我学习框架提升语言模型解决问题的能力</title>
<link>https://arxiv.org/abs/2503.00735</link>
<guid>https://arxiv.org/abs/2503.00735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LADDER框架通过自我学习大幅提升语言模型的数学问题解决能力。</p><br /><br /><p><strong>摘要：</strong> LADDER（自主难度驱动示例递归学习）是一个框架，通过递归生成和解决逐渐简化的复杂问题变体，使大型语言模型能够自主提升其问题解决能力。与以往需要策划数据集或人类反馈的方法不同，LADDER利用模型自身的能力生成更简单的问题变体。研究表明，LADDER在数学积分领域的有效性显著，从Llama 3.2 3B模型在本科水平问题上的准确率从1%提升至82%；同时，Qwen2.5 7B Deepseek-R1模型在MIT积分考试资格考试中取得了73%的高分。此外，本文还介绍了TTRL（测试时强化学习），通过在推理时对测试问题的变体进行强化学习，使得Qwen2.5 7B Deepseek-R1模型在MIT积分考试资格考试中取得了90%的最高得分，超过了OpenAI模型的表现。这些结果表明，自我导向的战略学习能够在不依赖架构扩展或人类监督的情况下，实现显著的能力提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:24:20 GMT</pubDate>
</item>
<item>
<title>迭代价值函数优化：提升价值引导解码的有效性</title>
<link>https://arxiv.org/abs/2503.02368</link>
<guid>https://arxiv.org/abs/2503.02368</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出迭代价值函数优化，提升了价值引导解码的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 随着人类反馈强化学习(RLHF)成为控制语言模型输出的主要方法，但其高计算成本和训练不稳定性依然是挑战。价值引导解码提供了一种更具成本效益的替代方案，通过控制输出而无需重新训练模型。然而，价值函数的准确性对价值引导解码至关重要，不准确的估计可能导致次优决策和性能下降。现有方法在准确估计最优价值函数方面面临困难，导致控制效果不理想。为此，本文提出了一种新框架——迭代价值函数优化，包含两个关键组成部分：蒙特卡洛价值估计，通过探索多样化的轨迹来降低估计方差；迭代在线优化，通过从价值引导策略收集轨迹逐步改善价值估计。通过在文本摘要、多轮对话和任务跟随等领域的广泛实验，验证了价值引导解码方法在对齐语言模型方面的有效性，且显著降低了计算成本，提升了控制的效率和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02368" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 03:48:51 GMT</pubDate>
</item>
<item>
<title>基于进化框架的智能GUI代理提升效率与灵活性</title>
<link>https://arxiv.org/abs/2503.02268</link>
<guid>https://arxiv.org/abs/2503.02268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升LLM代理在GUI系统中的效率与智能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的基于进化框架的GUI代理方法，以解决传统大语言模型(LLM)代理在执行常规任务时的低效率问题。尽管LLM代理展现出强大的推理能力和适应性，但其对逐步推理的依赖导致在处理例行事务时效率低下。相比之下，传统规则系统在效率上表现优异，但缺乏智能与灵活性。我们的方法引入了一种记忆机制，可以记录代理的任务执行历史，通过分析这些历史数据，代理能够识别重复的动作序列并进化出高层次的快捷操作，从而替代低层次操作，提升整体效率。实验结果显示，所提方法在多项基准任务上显著优于现有技术，提升了效率与准确性，相关代码将开放源码以支持进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02268" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 01:15:42 GMT</pubDate>
</item>
<item>
<title>FR-Spec：一种优化的频率排名推测采样框架</title>
<link>https://arxiv.org/abs/2502.14856</link>
<guid>https://arxiv.org/abs/2502.14856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FR-Spec通过频率优先的选词策略提升推测采样效率。</p><br /><br /><p><strong>摘要：</strong> FR-Spec是一种新提出的频率排名推测采样框架，旨在加速大型语言模型（LLMs）的自回归生成过程。该模型采用草稿-验证机制，使每次前向传递可生成多个标记。虽然现有的推测采样方法在采用单层和语言建模头时能实现较高的层压缩，但在处理大词汇表（如Llama-3-8B的128k标记）的LLMs时，效率提升明显降低。FR-Spec通过将草稿搜索限制在频率优先的标记子集，从而减少语言建模头的计算开销，降低幅度达到75%，同时确保最终输出分布的一致性。跨多个数据集的实验结果显示，FR-Spec较现有的EAGLE-2方法有着平均1.12倍的加速效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 00:36:34 GMT</pubDate>
</item>
<item>
<title>SemViQA：提升越南语事实检查的创新框架</title>
<link>https://arxiv.org/abs/2503.00955</link>
<guid>https://arxiv.org/abs/2503.00955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SemViQA框架通过语义检索和分类技术提升越南语事实检查精度与效率。</p><br /><br /><p><strong>摘要：</strong> 随着错误信息的增加，尤其是由大型语言模型（LLMs）如GPT和Gemini引发的，强有力的事实检查解决方案尤为关键，特别是对于资源稀缺的语言如越南语。现有方法在处理语义模糊性、同音词和复杂语言结构时常面临挑战，常常在准确性和效率之间做出取舍。我们介绍了SemViQA，这是一种新颖的越南语事实检查框架，结合了基于语义的证据检索（SER）和双步裁决分类（TVC），有效平衡了精度与速度，在ISE-DSC01数据集上实现了78.97%的严格准确率，在ViWikiFC上达到80.82%，并在UIT数据科学挑战中获得第一名。此外，SemViQA Faster在保持竞争性准确率的同时，将推理速度提高了7倍。SemViQA为越南语事实验证树立了新的基准，推动了对抗错误信息的斗争。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 00:08:53 GMT</pubDate>
</item>
<item>
<title>UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface</title>
<link>https://arxiv.org/abs/2503.01342</link>
<guid>https://arxiv.org/abs/2503.01342</guid>
<content:encoded><![CDATA[
Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 23:55:08 GMT</pubDate>
</item>
<item>
<title>Meta Plan Optimization框架提升大型语言模型代理的规划能力</title>
<link>https://arxiv.org/abs/2503.02682</link>
<guid>https://arxiv.org/abs/2503.02682</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPO框架通过明确指导提升LLM代理的规划能力，解决幻觉等问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Meta Plan Optimization (MPO)框架，通过直接引入明确指导来提升大型语言模型（LLM）代理的规划能力。与传统依赖复杂知识的方式不同，MPO利用元计划提供高层次的一般指导，辅助代理进行规划，并支持基于代理任务执行反馈的元计划持续优化。实验结果证明，MPO在两个典型任务上显著优于现有基线。同时，分析表明MPO提供了一种即插即用的解决方案，可以提高任务完成效率和在新场景中的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02682" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:30:53 GMT</pubDate>
</item>
<item>
<title>大语言模型对维基百科影响的深入分析</title>
<link>https://arxiv.org/abs/2503.02879</link>
<guid>https://arxiv.org/abs/2503.02879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大语言模型对维基百科的影响及潜在风险。</p><br /><br /><p><strong>摘要：</strong> 本文全面分析了大语言模型（LLMs）对维基百科的影响，研究了维基百科的演变及其与LLMs的关系。通过分析页面浏览量和文章内容，探讨维基百科最近的变化及LLMs的影响。研究显示，LLMs对维基百科特定分类的影响约为1%-2%。此外，我们评估了LLMs对与维基百科相关的自然语言处理任务（如机器翻译和检索增强生成）的影响，发现如果机器翻译基准受到LLMs的干扰，模型得分可能会出现膨胀，导致模型之间的比较结果发生变化。同时，若知识库被LLMs生成的内容污染，检索增强生成的效果可能降低。虽然LLMs尚未完全改变维基百科的语言和知识结构，但我们的实证发现强调了未来潜在风险的谨慎考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:25:53 GMT</pubDate>
</item>
<item>
<title>基于直接偏好优化的细粒度事实对齐方法Mask-DPO</title>
<link>https://arxiv.org/abs/2503.02846</link>
<guid>https://arxiv.org/abs/2503.02846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法Mask-DPO，提高了大语言模型的响应真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于直接偏好优化（DPO）的细粒度事实对齐方法Mask-DPO，旨在解决大语言模型（LLMs）在作为AI助手时产生的幻觉现象。传统的事实对齐方法在训练中引入了噪声，因为它们在响应层面进行偏好学习时未能有效区分正确与错误的信息。Mask-DPO通过将句子级别的真实性作为掩码信号，只从偏好样本中的事实正确句子学习，避免了对不偏好样本中真实内容的惩罚。实验表明，Mask-DPO显著提高了LLMs在未见问题上的真实性评分，Llama3.1-8B-Instruct在ANAH测试集的评分从49.19%提高到77.53%，超越Llama3.1-70B-Instruct。同时，关于泛化能力的研究表明，数据集中主题数量的扩展比问题数量的扩展更有效。我们希望此方法和研究发现为未来事实对齐的研究提供新的思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:25:15 GMT</pubDate>
</item>
<item>
<title>ATLaS：提高大型语言模型代理在多任务中的泛化能力</title>
<link>https://arxiv.org/abs/2503.02197</link>
<guid>https://arxiv.org/abs/2503.02197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ATLaS通过识别关键步骤提升LLM代理的泛化能力与效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）代理在多领域任务中展现了出色的泛化能力，但现有的调优方法往往基于完整专家轨迹的监督微调，可能导致专家偏差并削弱模型对未覆盖状态的泛化能力。为了解决这一问题，本文提出了ATLaS方法，专注于识别专家轨迹中的关键步骤并仅对这些步骤进行微调，从而降低成本并避免过拟合完整轨迹。通过实验，发现仅对ATLaS选出的30%的关键步骤进行微调的LLM性能超过了基于所有步骤微调的LLM及近期的开源LLM代理。ATLaS不仅维护了LLM的基本技能，还使之能在不同环境中表现更为出色，进而提升了代理的有效性与效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:17:48 GMT</pubDate>
</item>
<item>
<title>SPIDER：多器官补丁级别的病理图像数据集及基准模型</title>
<link>https://arxiv.org/abs/2503.02876</link>
<guid>https://arxiv.org/abs/2503.02876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIDER数据集和模型推动AI在计算病理学中的研究进展。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SPIDER（Supervised Pathology Image-DEscription Repository），这是一个涵盖多种器官类型的补丁级别数据集，解决了现有公共数据集在器官多样性、类别覆盖和注释质量方面的不足。SPIDER提供专家病理学家的高质量注释，并包括周边上下文补丁，增强了分类性能。此外，文章还展示了基于Hibou-L基础模型进行训练的基准模型，在多个组织类别中实现了先进性能，为今后的数字病理研究提供了强有力的参考。该模型不仅支持快速识别重要区域，实现定量组织度量，还为多模态方法奠定了基础。数据集和训练模型的公开可用性，旨在推动研究、可重复性及AI驱动的病理学发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:08:26 GMT</pubDate>
</item>
<item>
<title>自我学习预见法：提高多步推理任务效率的自监督方法</title>
<link>https://arxiv.org/abs/2503.02878</link>
<guid>https://arxiv.org/abs/2503.02878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自我学习预见法降低了多步推理任务的成本，提高了20%的性能。</p><br /><br /><p><strong>摘要：</strong> 在多步推理任务中，获得真实任务完成奖励或人工演示通常成本高且耗时，尤其是在交互式领域如网页任务。为了解决这一瓶颈，我们提出了一种自我监督方法——自我学习预见法，它利用状态转移动态训练一个价值模型，能够有效指导语言模型控制的搜索。我们发现，经过自我学习预见法改进的中等规模（80亿参数）开放权重价值模型，其性能与使用前沿大型语言模型（如gpt-4o）相匹配。此外，自我学习预见法在不依赖真实奖励的情况下，提高性能20%，同时将成本降低了37倍，相较于以往基于LLM的树搜索方法，展现出显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:07:18 GMT</pubDate>
</item>
<item>
<title>MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents</title>
<link>https://arxiv.org/abs/2503.01935</link>
<guid>https://arxiv.org/abs/2503.01935</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 21:46:46 GMT</pubDate>
</item>
<item>
<title>优化管道并行性中的激活内存消耗</title>
<link>https://arxiv.org/abs/2503.01328</link>
<guid>https://arxiv.org/abs/2503.01328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的激活内存卸载策略，提高管道并行的可扩展性。</p><br /><br /><p><strong>摘要：</strong> 管道并行性（PP）在训练大型语言模型时广泛应用，但高激活内存消耗限制了其可扩展性。本文探讨了在PP中应用未充分利用的内存卸载策略。通过实证研究发现，在大多数标准配置下，至少一半甚至全部激活可以在几乎没有额外开销的情况下卸载。对于无法完全卸载的情况，我们引入了一种新的选择性卸载策略，以优于线性的方式减少峰值激活内存。另外，我们将内存卸载与其他技术结合，综合考虑整体吞吐量和内存限制。实验结果显示，随着阶段数量的增加，每个设备的激活内存有效减少，使PP成为比TP更强的选择，提供高达19%的加速，同时内存消耗更低。相关实现已开源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 21:30:49 GMT</pubDate>
</item>
<item>
<title>基于模型置信度的测试时计算效率提升方法</title>
<link>https://arxiv.org/abs/2503.00031</link>
<guid>https://arxiv.org/abs/2503.00031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过模型置信度校准来提升大型语言模型的测试时计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了测试时计算增加对大型语言模型（LLMs）响应质量的提升。虽然Best-of-N采样和多数投票的自一致性方法简单有效，但它们在处理查询时无法根据复杂性动态调整采样数量，导致对简单问题的计算浪费与对复杂问题的探索不足。本研究提出通过将自一致性导出的置信度进行自校准，来解决LLMs过度自信和置信度估计不可靠的问题，进而在一次前向传递中实现可靠的置信度估计。我们设计了基于置信度的高效测试时扩展方法，如Best-of-N的提前停止和基于校准置信度的自一致性方法。实验结果表明，基于置信度的提前停止在16次响应样本预算下，将MathQA的准确率从81.0提升至83.6，证明了此置信度采样策略在推理时的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00031" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 12:05:25 GMT</pubDate>
</item>
<item>
<title>Web AI代理的安全性与脆弱性分析</title>
<link>https://arxiv.org/abs/2502.20383</link>
<guid>https://arxiv.org/abs/2502.20383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Web AI代理比传统LLM更脆弱，需加强安全性。</p><br /><br /><p><strong>摘要：</strong> 近期Web AI代理在复杂网络导航任务中的表现显著，但研究显示其脆弱性高于独立的大型语言模型（LLM）。尽管二者基于相同的安全模型，Web AI代理的灵活性相对较高，使其更易受到对抗性用户输入的影响。文章探讨造成Web AI代理脆弱性的多方面因素，指出简单的评估指标如成功率无法有效捕捉复杂信号。通过组件级分析和系统评价框架，研究识别了三大关键因素：1) 用户目标嵌入系统提示，2) 多步骤动作生成，3) 观察能力。研究结果强调在AI代理设计中增强安全性和稳健性的迫切需求，并提供了针对性防御策略的可行见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20383" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 10:47:26 GMT</pubDate>
</item>
<item>
<title>从人工有用智能到人工通用智能的过渡：分离知识与推理</title>
<link>https://arxiv.org/abs/2502.19402</link>
<guid>https://arxiv.org/abs/2502.19402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">分析大语言模型在推理能力上的局限，并提出改进建议。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）在真实场景中展现的应用价值，以及其推理能力的脆弱性，认为这与人工通用智能（AGI）的特征相悖。尽管LLMs在常识推理、编程和数学方面表现出色，但在新的上下文中，算法理解的泛化能力却显得不足。通过对冷门编程语言的算法任务进行实验，发现LLMs的推理过于依赖训练数据，转移能力有限。作者假设这一问题的根源在于推理与知识的耦合。为实现从人工有用智能向人工通用智能的转变，建议通过三种方式分离知识和推理：1. 使用从头开始的强化学习进行推理预训练；2. 利用合成任务的课程来辅助学习推理优先级；3. 学习更具普适性的推理函数，以减少虚假相关性的影响。这样的推理系统结合训练好的检索系统和大型外部记忆库，能够克服现有架构在新场景中推理的多项局限性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 08:19:57 GMT</pubDate>
</item>
<item>
<title>PodAgent：一种全新的播客音频生成框架</title>
<link>https://arxiv.org/abs/2503.00455</link>
<guid>https://arxiv.org/abs/2503.00455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PodAgent有效生成播客音频，提升内容与语音表现力。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为PodAgent的综合框架，旨在有效生成播客音频节目，解决了当前自动音频生成方法在深入内容生成和适当语音表现方面的挑战。PodAgent通过设计一个主办方-嘉宾-写作多代理协作系统生成信息丰富的主题讨论内容，构建语音池以确保角色匹配，并利用增强型大型语言模型(LM)进行富有表现力的对话音频合成。考虑到缺乏标准化的播客音频生成评估标准，研究团队制定了全面的评估指南来有效评估模型的性能。实验结果表明，PodAgent在主题讨论对话内容生成方面显著超越了直接使用GPT-4的表现，达到了87.4%的语音匹配准确率，并通过LLM引导的合成技术生成了更具表现力的语音。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 08:11:33 GMT</pubDate>
</item>
<item>
<title>评估大语言模型不确定性的方法研究</title>
<link>https://arxiv.org/abs/2503.01688</link>
<guid>https://arxiv.org/abs/2503.01688</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究不同方法评估大语言模型在多选题中的不确定性表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究了不确定性估计在大语言模型（LLMs）评估中的重要性，特别是在高风险领域。我们探讨了令牌熵和模型-评判（MASJ）在不同话题的多选题回答任务中的有效性。实验对象为三种不同规模的LLMs：Phi-4、Mistral和Qwen。研究发现，尽管MASJ的表现与随机预测相似，但响应熵能够有效预测知识依赖领域的模型错误，并成为问题难度的有效指标，生物学问题的ROC AUC达到0.73，而数学问题的ROC AUC仅为0.55，表明熵值需考虑推理的量。此外，我们指出现有的MMLU-Pro样本存在偏差，建议在不同子域中平衡推理需求，以便更公正地评估LLMs的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01688" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 06:41:49 GMT</pubDate>
</item>
<item>
<title>SampleMix: 一种基于样本特征的数据混合方法</title>
<link>https://arxiv.org/abs/2503.01506</link>
<guid>https://arxiv.org/abs/2503.01506</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于样本特征的下行数据混合方法，优化预训练数据结构。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的数据混合方法SampleMix，针对现有的按领域划分的预训练数据混合方法的不足之处，该方法采用自下而上的范式。相较于传统方法，SampleMix通过全局跨领域采样，系统地评估每个样本的质量和多样性，从而动态确定最佳的领域分布。此外，SampleMix在多个下游任务和困惑度评估中优于现有的基于领域的方法。尽管SampleMix在达到基线性能时需要1.4到2.1倍的训练步数，但其在优化预训练数据方面展现出巨大的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01506" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:28:10 GMT</pubDate>
</item>
<item>
<title>探索语言模型语义重建中的词形与上下文信息的作用</title>
<link>https://arxiv.org/abs/2503.01714</link>
<guid>https://arxiv.org/abs/2503.01714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，词形在大语言模型的语义重建中起核心作用。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了人类读者和先进的大语言模型（LLMs）在处理单词混淆时的行为，揭示了词形和上下文信息在语义重建中的作用。通过控制实验，提出了语义重建评分（SemRecScore），用于量化语义重建程度。研究结果显示，词形是LLMs实现语义重建的核心因素，而上下文信息在此过程中的影响较小。分析还发现，LLMs依赖特定的注意力头来提取和处理词形信息，这一机制在不同的单词混淆程度下保持稳定。这一研究为理解LLMs与人类在信息处理上的差异提供了重要见解，并提出了通过引入类人、上下文感知机制来提升LLM性能的建议。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01714" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:13:44 GMT</pubDate>
</item>
<item>
<title>Direct Discriminative Optimization：提升视觉生成模型的质量</title>
<link>https://arxiv.org/abs/2503.01103</link>
<guid>https://arxiv.org/abs/2503.01103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架DDO，提升生成模型在视觉生成中的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了直接判别优化（DDO），作为一个统一框架，旨在弥补基于似然的生成模型（如扩散模型和自回归模型）在有限模型容量下的模式覆盖倾向。DDO通过利用学习目标模型与固定参考模型之间的似然比来隐式参数化判别器，体现与直接偏好优化（DPO）的相似理念。与生成对抗网络（GAN）不同，这种参数化消除了生成器和判别器网络联合训练的需求，使得对经过良好训练模型的直接、高效的细化成为可能。DDO可迭代执行，通过自我对弈方式进行渐进的模型优化，每一轮所需的预训练周期不足1%。实验结果表明，DDO显著提升了现有扩散模型EDM的性能，在CIFAR-10和ImageNet-64数据集上FID分数从1.79/1.58降低至1.30/0.97，并持续改善了ImageNet 256x256上视觉自回归模型的无引导和CFG增强FID分数。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:12:10 GMT</pubDate>
</item>
<item>
<title>TOKENSWIFT框架：加速超长序列生成的解决方案</title>
<link>https://arxiv.org/abs/2502.18890</link>
<guid>https://arxiv.org/abs/2502.18890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TOKENSWIFT框架显著提升超长序列生成速度，最高达3倍加速。</p><br /><br /><p><strong>摘要：</strong> 生成超长序列（如100K个token）对大语言模型（LLMs）来说至关重要，但该过程极为耗时。传统的推测解码方法无法有效提升生成速度，并且可能造成负面影响。文章分析了频繁的模型重载、动态关键值管理和重复生成等三大挑战，并提出了TOKENSWIFT框架，旨在显著加速超长序列的生成，同时保持模型质量。实验结果表明，TOKENSWIFT在多种规模的模型（1.5B, 7B, 8B, 14B）和架构（MHA, GQA）中实现了超过3倍的速度提升，为超长序列生成节省了数小时的时间，确立了其作为可扩展、高效解决方案的地位。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:56:33 GMT</pubDate>
</item>
<item>
<title>DiffRhythm：高效生成完整歌曲的潜在扩散模型</title>
<link>https://arxiv.org/abs/2503.01183</link>
<guid>https://arxiv.org/abs/2503.01183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffRhythm是一种快速生成完整歌曲的新型音乐生成模型。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了DiffRhythm，这是一种创新的潜在扩散模型，用于高效生成完整的歌曲（包括人声与伴奏），时长可达4分45秒，生成时间仅为10秒。该模型解决了当前音乐生成方法面临的多种局限性，如仅能合成单声道或依赖复杂多阶段架构的问题。DiffRhythm的设计注重简单性，它简化了数据准备，采用直接的模型结构，仅需歌词和风格提示即可进行推理。同时，非自回归结构确保快速推理，加速了生成过程。作者还发布了完整的训练代码和在大规模数据上预训练的模型，以促进可重复性研究和后续开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:54:04 GMT</pubDate>
</item>
<item>
<title>基于DUSt3R的多视角房间布局估计新方法</title>
<link>https://arxiv.org/abs/2502.16779</link>
<guid>https://arxiv.org/abs/2502.16779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新方法Plane-DUSt3R，用于多视角房间布局估计。</p><br /><br /><p><strong>摘要：</strong> 随着3D基础模型DUSt3R的发展，传统的多步骤结构光过程向端到端的单步骤方法转变。本文提出了一种新颖的方法Plane-DUSt3R，通过在结构化房间布局数据集（Structure3D）上微调DUSt3R框架，旨在估计结构平面。该方法实现了统一且简洁的结果，简化了房间布局估计的过程，仅需单次后处理步骤和2D检测结果。与先前的方法不同，Plane-DUSt3R扩展了多视角图像的处理能力，提供了一个精简的端到端解决方案，减少了误差的累积。实验结果显示，Plane-DUSt3R在合成数据集上优于现有技术，并在多种风格（例如卡通）下的真实数据中表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:17:23 GMT</pubDate>
</item>
<item>
<title>OneRec: 基于生成模型的推荐系统优化方案</title>
<link>https://arxiv.org/abs/2502.18965</link>
<guid>https://arxiv.org/abs/2502.18965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OneRec通过统一生成模型显著优化推荐系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出OneRec，一个基于生成模型的推荐系统，替代常规的检索排序策略。OneRec是首个端到端生成模型，在实际场景中显著优于现有复杂的推荐系统。其核心包括编码器-解码器结构，能够有效利用用户历史行为序列并生成相关视频；采用稀疏混合专家模型，提升模型容量，同时控制计算负担；session-wise生成方法，确保生成内容的连续性与一致性。此外，设计了迭代偏好对齐模块以提高生成结果质量，通过奖励模型模拟用户生成，解决了推荐系统中同时获取正负样本的挑战。实验表明，有限数量的DPO样本能显著提升用户兴趣对齐，OneRec在抖音的实际应用中实现观看时间提升1.6%，效果显著。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 03:56:04 GMT</pubDate>
</item>
<item>
<title>大型语言模型在机间通信中开发私密声调语言的潜力研究</title>
<link>https://arxiv.org/abs/2503.01063</link>
<guid>https://arxiv.org/abs/2503.01063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型在机间通信中开发私密声调语言的潜力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在机间通信中开发私密声调语言的可能性。受到双胞胎间的加密语言现象（cryptophasia）以及自然声调语言（如普通话和越南语）的启发，研究团队实现了一种精确的字符与频率映射系统，使用音乐的半音对完整ASCII字符集（32-126）进行编码。每个字符被分配一个唯一频率，从220 Hz的空格到50,175.42 Hz的波浪符，共跨越约7.9个八度。部分字符被有意地映射到超声频率（>20 kHz），超出人类感知范围。原型软件展示了这种编码的可视化、听觉播放及ABC音乐符号，使得信息密度和传输速度的分析得以实现。测试显示，声调编码在部分超出人类感知的范围内可以达到超过人类语言的信息传输速率。此研究直接回应了对AI系统在未来五年内可能发展私密语言的担忧，提供了具体的原型软件示例，展示了这种通信的运作方式及其技术基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 03:20:03 GMT</pubDate>
</item>
<item>
<title>Liger：将预训练语言模型线性化为门控递归结构</title>
<link>https://arxiv.org/abs/2503.01496</link>
<guid>https://arxiv.org/abs/2503.01496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Liger 提供了高效的方法将预训练语言模型转化为门控线性递归模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法 Liger，用于将预训练的大型语言模型（LLMs）转换为门控线性递归结构。Liger 通过重新利用预训练的关键矩阵权重构建多样的门控机制，而无需添加额外参数，从而避免了从头开始训练的复杂性。此方法结合了轻量级的微调技术 Low-Rank Adaptation (LoRA)，能够恢复线性化门控递归模型的性能，使其与原始 LLMs 相匹配。此外，Liger 引入了一种内部层混合注意力机制（Liger Attention），在仅使用 0.02% 的预训练令牌情况下显著恢复了 Transformer 基于 LLM 的 93% 的性能，并在多个基准测试中取得了竞争力的结果。文中验证了该方法在从 10亿到80亿参数模型中的有效性，代码已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:48:58 GMT</pubDate>
</item>
<item>
<title>封闭循环体态代理（CLEA）架构在动态环境中的任务管理</title>
<link>https://arxiv.org/abs/2503.00729</link>
<guid>https://arxiv.org/abs/2503.00729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEA通过动态任务规划和多模态执行评估显著提升任务成功率。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）在复杂任务的层次分解和语义推理中表现出色，但在体态系统中的应用面临着可靠执行子任务序列和实现长期任务完成的一次性成功等挑战。为了解决这些问题，本文提出了一种新的架构——封闭循环体态代理（CLEA），它结合了四个专门的开源LLM，并通过功能解耦来实现闭环任务管理。CLEA的核心创新包括：一是动态生成可执行子任务的交互式任务规划器，能够基于环境记忆进行调整；二是多模态执行评估器，利用评估框架对行动可行性进行概率评估，并在环境扰动超过预设阈值时触发层次重规划机制。实验结果显示，CLEA在实际可操作物体的环境中进行了12个任务的实验，使用两种异构机器人执行物体搜索、操作和搜索-操作整合任务。在实验中，CLEA的成功率提高了67.3%，任务完成率增加了52.8%。这些结果表明，CLEA显著增强了动态环境中的任务规划与执行的鲁棒性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:27:17 GMT</pubDate>
</item>
<item>
<title>SpeQL: 利用大型语言模型加速大数据集查询执行</title>
<link>https://arxiv.org/abs/2503.00714</link>
<guid>https://arxiv.org/abs/2503.00714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpeQL系统通过预测查询，实现在用户输入期间实时显示结果，显著提高查询效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种新系统SpeQL，旨在加速对大型数据集的SQL查询执行。通过利用大型语言模型（LLMs），SpeQL能够根据数据库架构、用户历史查询及其不完整的查询实时预测可能的查询。在具体实现上，SpeQL使用两种方式进行查询结构预测，并提前编译和规划查询，同时预计算小型临时表，保证能包含回答最终查询所需的信息。此外，SpeQL实时展示预测结果和子查询，助力用户进行探索性分析。用户研究显示，SpeQL大幅提高了任务完成时间，参与者也表示，其结果的投机性展示帮助他们更快地发现数据模式。研究表明，SpeQL使用户查询延迟提升至289倍，而运行成本保持在每小时4美元的合理范围内。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00714" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:21:00 GMT</pubDate>
</item>
<item>
<title>CodeArena：重塑大语言模型代码生成评估框架</title>
<link>https://arxiv.org/abs/2503.01295</link>
<guid>https://arxiv.org/abs/2503.01295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeArena是为大语言模型代码生成设计的在线评估框架，通过集体评估机制提供无偏差的测评。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）通过结合自然语言理解和编程语法，极大提升了开发者的生产力，然而在对其编码能力进行量化评估时，仍面临基准泄露、数据分散和系统可及性等挑战。为了解决这些问题，本文提出了CodeArena，一个在线评估框架，旨在优化LLM代码生成的评价过程。其主要创新在于集体评估机制，可以根据所有参与模型的整体表现动态调整个别模型的分数，从而减少因基准泄露而产生的评分偏差。此外，CodeArena确保所有提交的解决方案和测试用例对公众开放，并提供便捷的API以简化代码评估流程。我们的主要贡献包括：1）无偏的集体评估系统；2）公开的解决方案和测试用例库；3）自动化友好的API，以便于无缝集成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:16:25 GMT</pubDate>
</item>
<item>
<title>Qilin数据集：推动多模态搜索与推荐服务的发展</title>
<link>https://arxiv.org/abs/2503.00501</link>
<guid>https://arxiv.org/abs/2503.00501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qilin数据集旨在提升多模态搜索推荐系统的用户体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Qilin这一新型多模态信息检索数据集，旨在改善用户在复杂系统中的搜索和推荐体验。Qilin数据集来源于小红书，包含了多种异构结果，如图文笔记、视频笔记和商业笔记，为多模态神经检索模型的发展提供了丰富的数据支持。此外，Qilin还收集了APP级的上下文信号和真实用户反馈，以更好地建模用户满意度和分析用户行为。特别地，该数据集包含用户偏好的答案和触发深度查询回答模块的搜索请求结果，使得可以训练和评估检索增强生成（RAG）管道，并探索此模块如何影响用户的搜索行为。通过全面的分析和实验，本文揭示了有趣的发现，为进一步提升搜索与推荐系统提供了新思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 01:56:03 GMT</pubDate>
</item>
<item>
<title>Kiss3DGen: 高效的3D生成与编辑框架</title>
<link>https://arxiv.org/abs/2503.01370</link>
<guid>https://arxiv.org/abs/2503.01370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kiss3DGen通过重用2D扩散模型，实现高效的3D对象生成与编辑。</p><br /><br /><p><strong>摘要：</strong> Kiss3DGen（Keep It Simple and Straightforward in 3D Generation）是一个高效的框架，旨在解决当前3D内容生成中存在的质量和通用性限制。该方法通过对预训练的2D图像扩散模型进行微调，生成被称为“3D Bundle Image”的多视角图像和法线图的切片表示，这些法线图用于重建3D网格，而多视角图像则提供纹理映射，从而形成完整的3D模型。该方法将3D生成问题转化为2D图像生成任务，从而最大限度地利用了预训练扩散模型中的知识。此外，Kiss3DGen与多种扩散模型技术兼容，支持3D编辑、网格和纹理增强等高级功能。通过大量实验，我们验证了该方法的有效性，展现了其高效生成高质量3D模型的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 01:19:45 GMT</pubDate>
</item>
<item>
<title>基于单步扩散模型的3D重建与新视角合成增强方法</title>
<link>https://arxiv.org/abs/2503.01774</link>
<guid>https://arxiv.org/abs/2503.01774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Difix3D+通过单步扩散模型提升3D重建和新视角合成效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Difix3D+，一种新颖的管道，旨在通过单步扩散模型提高3D重建和新视角合成的效果。核心是Difix，一个经过训练的单步图像扩散模型，用于增强和去除由3D表示中的不受约束区域引起的伪影。在重建阶段，Difix用于清理从重建中渲染的伪训练视角，显著改善这些不受约束区域的质量，并增强整体3D表示效果。更重要的是，Difix在推理阶段充当神经增强器，能够有效去除由不完美的3D监督和当前重建模型的有限能力引起的残余伪影。Difix3D+是一个通用解决方案，兼容NeRF和3DGS表示，其FID评分相比于基线提升了平均两倍，且保持了3D一致性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:52:22 GMT</pubDate>
</item>
<item>
<title>VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2503.01739</link>
<guid>https://arxiv.org/abs/2503.01739</guid>
<content:encoded><![CDATA[
Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:29:56 GMT</pubDate>
</item>
<item>
<title>语言模型自我改进的内在机制：推理行为的作用</title>
<link>https://arxiv.org/abs/2503.01307</link>
<guid>https://arxiv.org/abs/2503.01307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明推理行为对语言模型的自我改进至关重要。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了语言模型在复杂任务上自我改进的内在机制，尤其是推理行为在这一过程中的作用。研究发现，不同模型在强化学习（RL）下展现出显著差异。例如，Qwen-2.5-3B在《倒计时》游戏中的表现远超Llama-3.2-3B。通过分析四种关键认知行为——验证、回溯、子目标设定和逆向链推理，揭示了有效自我改进的内在特性。实验结果显示，尽管Llama最初缺乏这些推理行为，通过示例引导后能够显著提升表现，达到甚至超过Qwen的水平。此外，推理行为的存在被证明比答案的正确性更为重要，带有正确推理模式的错误解答亦能与正确解答的表现相当。最后，继续对Llama进行以推理行为为重点的数据预训练，进一步提升了其自我改进能力，证明了初始推理行为与模型改进能力之间的根本关系。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:09:04 GMT</pubDate>
</item>
<item>
<title>Large-Scale Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.01807</link>
<guid>https://arxiv.org/abs/2503.01807</guid>
<content:encoded><![CDATA[
Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:44:06 GMT</pubDate>
</item>
<item>
<title>视觉强化微调（Visual-RFT）：提升大型视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2503.01785</link>
<guid>https://arxiv.org/abs/2503.01785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Visual-RFT通过可验证奖励函数提升视觉任务中的推理与适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了视觉强化微调（Visual-RFT）方法，以扩展强化微调在视觉任务中的应用。Visual-RFT利用大型视觉语言模型生成包含推理标记和最终答案的多个响应，并通过我们提出的可验证奖励函数（如IoU奖励）更新模型，采用政策优化算法（如集团相对政策优化GRPO）。实验结果显示，Visual-RFT在细粒度图像分类、少样本物体检测和推理基础等多个基准测试中表现出色，具有优异的泛化能力。例如，在使用约100个样本的一次性细粒度图像分类中，Visual-RFT的准确率比基线提高了24.3%；在COCO的两次性设置中，少样本物体检测超越基线21.9，LVIS超越15.4。Visual-RFT为微调大型视觉语言模型提供了一种数据效率高、以奖励驱动的方法，显著增强了模型在特定领域任务中的推理和适应能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:29:27 GMT</pubDate>
</item>
<item>
<title>Introducing Phi-4-Mini与Phi-4-Multimodal：紧凑且高效的语言和多模态模型</title>
<link>https://arxiv.org/abs/2503.01743</link>
<guid>https://arxiv.org/abs/2503.01743</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-4-Mini和Phi-4-Multimodal是出色的紧凑型语言及多模态模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Phi-4-Mini和Phi-4-Multimodal两款紧凑而强大的模型。Phi-4-Mini是一个拥有38亿参数的语言模型，经过高质量的网络和合成数据训练，在数学和编码任务中表现优异，超越了同类开源模型，甚至与体量两倍的模型竞争。此外，其扩展的20万token词汇量和分组查询注意力机制显著提升了生成长序列的效率。Phi-4-Multimodal则是一个集文本、视觉和语音/音频输入于一体的多模态模型，采用创新的LoRA适配器和特定路由器技术，能够在不互相干扰的情况下进行多种推理模式，使其在多个任务上优于较大规模的模型。实验表明，即使在紧凑的38亿参数设置下，Phi-4-Mini的推理能力也可与深度学习领域中更大模型媲美。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.01743" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:15:05 GMT</pubDate>
</item>
<item>
<title>DuoDecoding：提升大语言模型推理速度的新方法</title>
<link>https://arxiv.org/abs/2503.00784</link>
<guid>https://arxiv.org/abs/2503.00784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuoDecoding通过优化模型部署极大提升大语言模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 大语言模型在多种任务中表现出色，但其逐字自回归生成过程显著降低了推理速度。本文提出DuoDecoding，一种新颖的方法，通过将草稿模型和目标模型分别部署在CPU和GPU上，实现并行解码，从而提高生成速度，同时保持草稿质量。通过优化草稿预算与动态多序列草拟，DuoDecoding在七项任务上进行的大规模实验表明，相较于传统的推测解码方法，生成延迟最多可加速2.61倍，首次令牌的生成时间减少至83%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.00784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 22:35:45 GMT</pubDate>
</item>
<item>
<title>基于单步反馈的多轮代码生成方法muCode</title>
<link>https://arxiv.org/abs/2502.20380</link>
<guid>https://arxiv.org/abs/2502.20380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">muCode方法通过单步奖励实现多轮代码生成，展现出显著性能提升。</p><br /><br /><p><strong>摘要：</strong> 本文解决了从多轮执行反馈生成代码的问题，提出了一种简单且可扩展的方法muCode。与现有方法不同，muCode只依赖单步奖励来优化多轮代码生成。关键在于，我们将代码生成视为一个可恢复的一步马尔可夫决策过程（MDP），任何中间代码状态都能在单次交互中恢复为正确代码。通过迭代训练生成器和验证器，muCode能有效地生成基于多轮执行反馈的代码解决方案。实验结果表明，muCode在性能上显著优于现有最先进的基线模型。同时，我们分析了奖励模型和策略的设计选择，展示了muCode在利用执行反馈方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 11:25:57 GMT</pubDate>
</item>
<item>
<title>利用大型语言模型提升心理咨询服务的潜力</title>
<link>https://arxiv.org/abs/2502.19731</link>
<guid>https://arxiv.org/abs/2502.19731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">将大型语言模型应用于心理咨询，有助于填补心理健康支持的缺口。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了将大型语言模型（LLMs）应用于心理咨询的潜力，旨在解决患者需求与心理健康支持资源之间的显著差距。当前LLMs在为客户提供有效反馈时面临挑战，主要由于缺乏高质量的、真实的心理咨询数据，而这些数据由于客户隐私问题往往不可获取。文章首先提出了一套专业的评估原则，用以衡量治疗师对客户表述的回应质量，并创建了包含36,000个高质量偏好比较的PsychoCounsel-Preference数据集，深度契合专业心理治疗师的偏好。此外，通过奖励建模和偏好学习的实验表明，PsychoCounsel-Preference为LLMs在心理咨询中的应用奠定了良好的基础。与此同时，最佳对齐模型PsychoCounsel-Llama3-8B在与GPT-4o的比较中，取得了87%的胜率。本文还公开了相关资源，以推动心理咨询领域与LLMs的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 10:56:33 GMT</pubDate>
</item>
<item>
<title>EgoNormia: 评估视觉语言模型的规范推理能力</title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了EgoNormia数据集，评估视觉语言模型的规范理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了EgoNormia数据集，包含1853个以自我为中心的人类互动视频，用于提升和评估视觉语言模型(VLMs)的规范推理能力。该数据集围绕七个规范类别展开：安全、隐私、个人空间、礼貌、合作、协调/主动性及沟通/清晰度。我们通过一种新颖的数据处理管道来大规模编制此数据集，涉及视频采样、自动答案生成、筛选和人工验证。研究发现，目前最先进的视觉语言模型在EgoNormia上的得分最高仅为45%，远低于人类的92%，显示出在安全、隐私及合作沟通能力方面的显著不足。同时，我们的分析表明，通过检索式生成方法，可以利用EgoNormia来增强视觉语言模型的规范推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 10:26:31 GMT</pubDate>
</item>
<item>
<title>小数据集的战略增强在图像生成中的成功应用</title>
<link>https://arxiv.org/abs/2502.21318</link>
<guid>https://arxiv.org/abs/2502.21318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">小规模经过优化的数据集通过增强策略超越大规模模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本到图像生成模型在亿级数据集的训练下取得显著成果，普遍遵循“更大更好”的理念。然而，本文挑战这一传统观念，展示了合理的数据增强策略如何使小规模的精心策划数据集可以与或超越基于大量网络抓取数据训练的模型。我们在仅使用经过增强的ImageNet数据集，并结合精心设计的文本和图像增强，保留了1/10的模型参数和1/1000的训练图像，实现了在GenEval上超越SD-XL 2分的总体得分，以及在DPGBench上超越5分的优异表现。我们的研究结果表明，战略性的数据增强而非庞大的数据集，为T2I生成提供了更可持续的发展路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.21318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:49:10 GMT</pubDate>
</item>
<item>
<title>DexGraspVLA：实现通用灵巧抓取的新框架</title>
<link>https://arxiv.org/abs/2502.20900</link>
<guid>https://arxiv.org/abs/2502.20900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DexGraspVLA框架通过视觉-语言模型实现高效的对象抓取。</p><br /><br /><p><strong>摘要：</strong> DexGraspVLA是一种新的层次化框架，旨在解决机器人灵巧抓取中的普遍挑战。该框架结合了预训练的视觉-语言模型作为高层任务规划器，以及扩散模型作为低层行动控制器。其关键在于将多样化的语言和视觉输入迭代转化为领域不变的表示，从而有效应用模仿学习，减少领域转换的问题。通过这种方法，DexGraspVLA实现了超过90%的成功率，能够应对数千种未见过的对象、光照和背景组合，适用于“零样本”环境。实证分析表明，该模型在环境变化中的一致性验证了其设计合理性及优秀的泛化性能，标志着向实现通用灵巧抓取的重要一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:44:46 GMT</pubDate>
</item>
<item>
<title>TeleRAG：提升RAG系统推理效率的创新方案</title>
<link>https://arxiv.org/abs/2502.20969</link>
<guid>https://arxiv.org/abs/2502.20969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeleRAG通过预取机制高效降低RAG推理延迟，优化GPU内存使用。</p><br /><br /><p><strong>摘要：</strong> Retrieval-augmented generation (RAG)是一种利用外部数据源增强大型语言模型（LLM）准确性和领域覆盖率的技术。然而，现代RAG流程依赖大量数据存储，导致在延迟敏感的应用场景中面临挑战，尤其是在GPU内存有限的情况下。为了解决这些问题，本文提出了TeleRAG，一种高效的推理系统，旨在以最小的GPU内存需求显著降低RAG的延迟。TeleRAG的核心创新是前瞻性检索，这是一种预测所需数据并并行将其从CPU传输到GPU的预取机制。通过利用RAG流程的模块化、倒排文件索引（IVF）搜索算法以及查询之间的相似性，TeleRAG能够有效地重叠数据移动和计算。实验结果表明，与最先进的系统相比，TeleRAG在端到端推理延迟上平均减少了高达1.72倍，从而使先进RAG应用的部署更加快速和内存高效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:33:49 GMT</pubDate>
</item>
<item>
<title>MIGE：统一的多模态框架促进主题驱动生成与指令编辑</title>
<link>https://arxiv.org/abs/2502.21291</link>
<guid>https://arxiv.org/abs/2502.21291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIGE框架通过统一表示提升主题生成与指令编辑的效果。</p><br /><br /><p><strong>摘要：</strong> 尽管在基于扩散的图像生成方面取得重大进展，但主题驱动生成和指令编辑依然面临挑战。现有方法通常将这两者分开处理，面临高质量数据有限和泛化能力差的问题。为此，本文提出了一种名为MIGE的统一框架，利用多模态指令标准化任务表示，将主题驱动生成视为在空白画布上的创作，将指令编辑视为对现有图像的修改，建立了共同的输入输出公式。MIGE引入了一种新颖的多模态编码器，将自由形式的多模态指令映射到统一的视觉-语言空间，通过特征融合机制整合视觉和语义特征。这种统一性使得两个任务能够联合训练，同时获得交叉任务增强和泛化能力。在实验中，MIGE在主题驱动生成和指令编辑方面表现优异，且在新的指令驱动主题编辑任务中设立了最新的技术水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.21291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 08:13:06 GMT</pubDate>
</item>
<item>
<title>LettuceDetect：克服生成模型幻觉的高效检测框架</title>
<link>https://arxiv.org/abs/2502.17125</link>
<guid>https://arxiv.org/abs/2502.17125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LettuceDetect框架有效提升了RAG系统的幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> LettuceDetect框架针对生成增强检索（RAG）系统在幻觉答案检测中存在的两个主要限制进行了改进：传统编码器方法的上下文窗口限制和大语言模型方法的计算低效。该方法基于ModernBERT扩展的上下文能力（可达8000个令牌），并在RAGTruth基准数据集上进行训练，表现超过所有以往的编码器模型及大多数提示基础模型，同时约比最佳模型小30倍。LettuceDetect为一个令牌分类模型，能够处理上下文-问题-答案的三元组，从而在令牌级别识别不支持的主张。在RAGTruth数据集上的评估显示，其示例级检测的F1得分高达79.22%，比之前的最优编码器架构Luna提升了14.8%。此外，该系统在单个GPU上能实现每秒处理30到60个示例的速度，更加适用于现实世界的RAG应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 07:33:14 GMT</pubDate>
</item>
<item>
<title>Optimal Brain Apoptosis</title>
<link>https://arxiv.org/abs/2502.17941</link>
<guid>https://arxiv.org/abs/2502.17941</guid>
<content:encoded><![CDATA[
The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 07:04:47 GMT</pubDate>
</item>
<item>
<title>ProtoFM：结合视觉基础模型的自解释分类器</title>
<link>https://arxiv.org/abs/2502.19577</link>
<guid>https://arxiv.org/abs/2502.19577</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProtoFM是一个高效且可解释的模型，提升了解释性和分类性能。</p><br /><br /><p><strong>摘要：</strong> 随着视觉基础模型（VFM）的流行，它们在性能上表现出色，但可解释性依然至关重要。自解释模型（SEM）旨在提供可解释的分类器，能够将预测结果分解为可解释概念的加权和。尽管这一目标有潜力，但研究表明，这些解释往往缺乏真实性。本文提出了一种新的原型架构与专业训练目标相结合的方法ProtoFM，仅在冻结的VFM上训练轻量级头部（约100万参数），从而提供一种高效且可解释的解决方案。评估结果表明，ProtoFM在分类性能上具有竞争力，并在多项来自文献的可解释性指标上超越现有模型。代码可通过链接获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19577" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 04:21:42 GMT</pubDate>
</item>
<item>
<title>链式草稿模型：提升大语言模型推理效率的创新方法</title>
<link>https://arxiv.org/abs/2502.18600</link>
<guid>https://arxiv.org/abs/2502.18600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的链式草稿模型，提高语言模型推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的链式草稿（CoD）模型，旨在改进大型语言模型（LLMs）在复杂推理任务中的表现。不同于传统的链式思维（CoT）模型强调冗长的逐步推理，人类更倾向于使用简洁的中间思维来捕捉关键信息。CoD借鉴了这种人类认知过程，使得LLMs能够生成精简而富有信息的推理输出。研究表明，CoD在准确性上与CoT相当，甚至超过其表现，而使用的令牌量仅为7.6%，显著降低了推理成本和延迟，适用于多种推理任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 02:35:09 GMT</pubDate>
</item>
<item>
<title>ViDoSeek与ViDoRAG：解决视觉文档中的复杂推理挑战</title>
<link>https://arxiv.org/abs/2502.18017</link>
<guid>https://arxiv.org/abs/2502.18017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ViDoSeek和ViDoRAG解决视觉文档中的信息检索和推理难题。</p><br /><br /><p><strong>摘要：</strong> 理解信息丰富的视觉文档对于传统的检索增强生成（RAG）方法而言仍然是一个重大挑战，现有基准多集中于图像问答，而忽视了在密集视觉文档中高效检索、理解和推理的基本问题。为此，本文引入ViDoSeek数据集来评估RAG在视觉文档中的性能，识别当前RAG方法的主要局限性，包括视觉检索方法未能有效整合文本与视觉特征，并且之前的方法通常分配的推理令牌不足。为解决这些问题，本文提出了ViDoRAG，一个针对视觉文档复杂推理的多代理RAG框架，采用基于高斯混合模型的混合策略来有效处理多模态检索。此外，还引入了一个迭代代理工作流，以促进模型的推理能力。通过在ViDoSeek上的广泛实验验证了我们方法的有效性和普遍性，ViDoRAG在竞争性基准测试中超越现有方法超过10%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:22:01 GMT</pubDate>
</item>
<item>
<title>应用强化学习提升人形机器人灵巧操作能力</title>
<link>https://arxiv.org/abs/2502.20396</link>
<guid>https://arxiv.org/abs/2502.20396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究并解决人形机器人强化学习在灵巧操作中的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在灵巧操作任务中应用强化学习的关键挑战，并提出了相应的解决方案。研究团队引入了自动的真实与仿真调优模块，以缩小仿真环境与现实环境之间的差距；设计了一种通用奖励机制，简化了长时间接触丰富的操作任务的奖励工程；采用了分而治之的蒸馏过程，提高了困难探索问题的样本效率，同时保持了仿真到现实的性能；此外，结合稀疏与密集物体表示，弥合了仿真与现实感知之间的差距。通过对三个人形灵巧操作任务的实证研究，验证了上述技术的有效性，最终显示出在无须人类示范的情况下，强化学习在灵巧操作中的成功应用和强大的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:08:44 GMT</pubDate>
</item>
<item>
<title>双阶段数据注释管道在视频理解中的应用</title>
<link>https://arxiv.org/abs/2502.20811</link>
<guid>https://arxiv.org/abs/2502.20811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一套双阶段数据注释管道以改善视频中的人类动作理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种双阶段的数据注释管道，旨在提高多模态大语言模型在视频理解中的表现，尤其是涉及人类动作的视频。首先，开发了策略从互联网收集包含清晰人类动作的视频；其次，将视频标注为标准化的字幕格式，利用人类属性区分个体并详细描述其动作与交互。通过这一管道，创建了HAICTrain和HAICBench两个数据集，前者包含126K个由Gemini-Pro生成且经过验证的视频-字幕对，用于训练；后者则包含500个手动标注的视频-字幕对和1400个问答对，用于全面评估人类动作理解。实验结果表明，使用HAICTrain进行训练显著提升了四个基准测试中的人类理解能力，同时也改善了文本到视频生成的结果。这两个数据集已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:04:15 GMT</pubDate>
</item>
<item>
<title>大语言模型在多变量多项式非负性判断中的应用与研究</title>
<link>https://arxiv.org/abs/2502.20545</link>
<guid>https://arxiv.org/abs/2502.20545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明精心设计的指导显著提升LLMs解决数学问题的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型在进行严格数学问题求解方面的潜力，尤其是针对多变量多项式非负性的问题。该问题与希尔伯特第十七问题密切相关，涉及全球多项式优化的关键应用。为此，研究团队创建了SoS-1K数据集，包括约1000个多项式，并提供了基于五个逐步挑战标准的专家设计推理指导。对多种先进的LLM进行评估发现，在没有结构化指导的情况下，所有模型的表现仅略高于随机猜测的基准。然而，通过高质量的推理指导，模型的准确率显著提升至81%。此外，经过短短4小时微调的SoS-7B模型，在准确度上超过了671B的DeepSeek-V3和GPT-4o-mini，并且计算时间仅为其1.8%和5%。研究结果突显了LLM在数学推理及处理NP困难问题上的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:00:31 GMT</pubDate>
</item>
<item>
<title>LiteASR：一种低秩压缩的自动语音识别编码器方案</title>
<link>https://arxiv.org/abs/2502.20583</link>
<guid>https://arxiv.org/abs/2502.20583</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiteASR通过低秩压缩降低ASR编码器计算成本，同时保持转录准确性。</p><br /><br /><p><strong>摘要：</strong> LiteASR是一种针对现代自动语音识别（ASR）模型编码器的低秩压缩方案，旨在减少推理成本。该方案基于中间激活函数的低秩特性，将主成分分析（PCA）与小规模校准数据集结合使用，从而用一系列低秩矩阵乘法来近似线性变换。同时，我们优化了自注意力机制，使其能够在简化的维度中工作。评估结果表明，LiteASR可以将Whisper large-v3的编码器大小压缩超过50%，同时在转录准确性上超越Whisper medium，成功建立了效率与性能的新帕累托最优边界。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20583" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 21:48:46 GMT</pubDate>
</item>
<item>
<title>基于SolutionBench的复杂工程解决方案设计评估与SolutionRAG系统</title>
<link>https://arxiv.org/abs/2502.20730</link>
<guid>https://arxiv.org/abs/2502.20730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SolutionBench基准和SolutionRAG系统以改进复杂工程解决方案设计。</p><br /><br /><p><strong>摘要：</strong> 文章针对复杂工程设计中存在的不足，引入了新的基准SolutionBench，旨在评估系统生成符合多种复杂约束的完整且可行的解决方案的能力。此外，提出了一种新系统SolutionRAG，该系统结合树状探索和双点思维机制，致力于生成可靠的解决方案。通过广泛的实验结果表明，SolutionRAG在SolutionBench上实现了最先进的性能，显示出其在现实应用中提升复杂工程解决方案设计的自动化和可靠性的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 21:35:24 GMT</pubDate>
</item>
<item>
<title>PlanGEN框架：提升复杂规划问题推理能力的新方法</title>
<link>https://arxiv.org/abs/2502.16111</link>
<guid>https://arxiv.org/abs/2502.16111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PlanGEN框架，通过迭代验证提升复杂规划任务的推理性能。</p><br /><br /><p><strong>摘要：</strong> Recent agent frameworks and inference-time algorithms face challenges in complex planning problems due to the inability to effectively verify generated plans and adapt to varying instance complexities. To tackle these issues, we introduce PlanGEN, a scalable and model-agnostic agent framework that integrates three essential components: constraint, verification, and selection agents. Our method employs constraint-guided iterative verification to enhance the performance of inference-time algorithms like Best of N, Tree-of-Thought, and REBASE. The selection agent further optimizes the choice of algorithms based on instance complexity, improving adaptability for complex planning tasks. Experimental results indicate that PlanGEN significantly outperforms existing baselines across various benchmarks, achieving state-of-the-art improvements on NATURAL PLAN, OlympiadBench, DocFinQA, and GPQA. The findings suggest that constraint-guided verification and adaptive selection are crucial for advancing performance in complex reasoning and planning challenges.</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 16:51:51 GMT</pubDate>
</item>
<item>
<title>xAR框架：一种扩展自回归模型的生成方法</title>
<link>https://arxiv.org/abs/2502.20388</link>
<guid>https://arxiv.org/abs/2502.20388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出xAR，一个新型自回归模型，缓解了曝光偏差并优化了生成效率。</p><br /><br /><p><strong>摘要：</strong> 自回归建模（AR）在语言和视觉生成模型中扮演着重要角色，但在2D图像结构中最优的token定义仍待探讨。本文提出xAR，一种将token概念扩展至实体X的通用自回归框架，实体X可以代表单个补丁、邻近补丁的聚合、非局部补丁分组或整个图像。同时，我们将离散token分类重构为连续实体回归，采用流匹配方法在每个AR步骤中训练，从而引入了噪声上下文学习，有效缓解了曝光偏差。xAR提供了两个主要优势：灵活的预测单元和避免依赖教师强迫的方法。在ImageNet-256生成基准测试中，xAR-B模型以172M参数超越675M参数的DiT-XL和SiT-XL，并实现了20倍的推理速度提高。xAR-H则以1.24的FID分数设定新基准，速度比前一最佳模型快2.2倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 13:21:13 GMT</pubDate>
</item>
<item>
<title>探讨大语言模型中的关系特定神经元</title>
<link>https://arxiv.org/abs/2502.17355</link>
<guid>https://arxiv.org/abs/2502.17355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大语言模型中存在关系特定的神经元，影响知识生成。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大语言模型（LLMs）中，某些神经元是否专注于关系而非具体实体。研究假设这些神经元能够识别输入文本中的关系，并指导相关生成。通过对Llama-2系列进行实验，结果表明存在关系特定的神经元，并通过选择性去激活这些神经元，评估了其对处理特定关系事实的影响。研究显示，关系特定神经元具备三个显著特性：（i）神经元累积效应，即去激活更多相关神经元会导致相关事实的更大降解；（ii）神经元多样性，不同关系的神经元可以共享，且部分神经元可跨语言传递；（iii）神经元干扰，去激活特定关系的神经元能够提高对其他关系事实的生成表现。这些发现为理解大语言模型中知识存储的机制提供了新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:54:03 GMT</pubDate>
</item>
<item>
<title>提升自主AI代理的安全性：应对攻击与脆弱性</title>
<link>https://arxiv.org/abs/2502.16750</link>
<guid>https://arxiv.org/abs/2502.16750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了自主AI代理的安全威胁及其防护框架。</p><br /><br /><p><strong>摘要：</strong> 自主AI代理利用大语言模型为社会各个领域创造了重要价值，但面临着来自对手的安全威胁，亟需采取保护措施以保障信任与安全。文章指出，静态守卫措施无法有效应对许多种越狱与欺骗性对齐等高级攻击，并强调在真实环境中增强鲁棒性的重要性。通过开发新的评估框架，本文旨在提升基于LLM的代理安全性，采用反图灵测试和多代理模拟进行攻击检测，并用GEMINI 1.5 pro与lama-3.3-70B等模型进行反越狱系统的测试。尽管检测能力达到了94%的准确率，但在长时间攻击下系统仍表现出持续的脆弱性，攻击成功率随着提示长度增加而上升，揭示了复杂系统的多个故障。因此，提出了基于主动监控的灵活安全体系，以应对现有模型的脆弱性，是文章的核心贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:46:19 GMT</pubDate>
</item>
<item>
<title>Training Consistency Models with Variational Noise Coupling</title>
<link>https://arxiv.org/abs/2502.18197</link>
<guid>https://arxiv.org/abs/2502.18197</guid>
<content:encoded><![CDATA[
Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at 64 times 64 resolution in 2-step generation. Our code is available at https://github.com/sony/vct .
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:55:48 GMT</pubDate>
</item>
<item>
<title>高效动态高斯点阵的渲染技术研究</title>
<link>https://arxiv.org/abs/2502.20378</link>
<guid>https://arxiv.org/abs/2502.20378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出高效动态高斯点阵方法显著提升动态场景渲染速度与质量。</p><br /><br /><p><strong>摘要：</strong> 动态场景的单目视频渲染是一项重要且具有挑战性的任务。尽管近期的可变形高斯点阵技术为动态场景的表现提供了有效解决方案，但其在训练视角下生成过多冗余高斯，导致渲染速度变慢。此外，静态区域的高斯属性是时间不变的，这使得不必要的高斯建模会引起静态区域的抖动。本文提出了一种高效动态高斯点阵（EDGS）方法，通过稀疏时间变属性建模表示动态场景，利用稀疏锚点网格表示，结合经典内核表示计算密集高斯的运动流，并引入无监督策略以高效筛选静态区域锚点。实验结果表明，EDGS在两个真实数据集上的渲染速度显著提高，同时相比于先前的最先进方法，渲染质量也得到了提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:25:35 GMT</pubDate>
</item>
<item>
<title>ArtGS：一种用于多部件关节物体建模的新方法</title>
<link>https://arxiv.org/abs/2502.19459</link>
<guid>https://arxiv.org/abs/2502.19459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtGS是一种利用3D高斯表示的新方法，提升多部件关节物体的重建和动态建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ArtGS，一种新颖的方法，利用3D高斯作为灵活高效的表示，解决计算机视觉中多部件关节物体的建模挑战。ArtGS通过结合规范高斯的粗到细初始化和更新，旨在对齐不同物体状态下的关节部件信息，并采用启发自皮肤绑定的部件动态建模模块，以提升部件网格重建和关节学习。通过在合成和真实世界数据集上的广泛实验，ArtGS展示了在联合参数估计和部件网格重建方面的最高性能，尤其是在处理复杂的多部件关节物体时显著提高了重建质量与效率。此外，本文还对设计选择进行了详细分析，以验证每个组件的有效性，并指明未来的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:47:08 GMT</pubDate>
</item>
<item>
<title>MedVLM-R1：提升医疗图像分析的透明度与可信度</title>
<link>https://arxiv.org/abs/2502.19634</link>
<guid>https://arxiv.org/abs/2502.19634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedVLM-R1通过生成自然语言推理提升医疗图像分析的透明度与准确性。</p><br /><br /><p><strong>摘要：</strong> MedVLM-R1是一种新的医疗视觉语言模型，旨在增强医疗图像分析中的透明度和可信度。现有的医疗视觉语言模型多只提供最终答案，缺乏有效的推理过程。MedVLM-R1通过强化学习框架，鼓励模型发现可人类解释的推理路径，避免了监督fine-tuning所带来的过拟合问题。在限量训练数据（600个视觉问题回答样本）和2B参数的情况下，MedVLM-R1在MRI、CT和X光基准测试中的准确率从55.11%提升至78.22%，超越了在超过100万样本上训练的更大模型，并展示了在分布外任务中的良好领域泛化能力。此模型的推出标志着医疗图像分析与明确推理结合的重要进展，为临床实践中的可信和可解释AI奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:36:05 GMT</pubDate>
</item>
<item>
<title>Dream Engine：一种高效的文本-图像交错控制生成框架</title>
<link>https://arxiv.org/abs/2502.20172</link>
<guid>https://arxiv.org/abs/2502.20172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出Dream Engine框架，实现高效的文本-图像交错控制生成。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像生成领域的进步，出现了将强大的文本编码器与扩散变换器骨架相结合的统一框架。现有方法在控制输出图像方面有所探索，但对于任意文本-图像交错控制的全面框架仍然缺乏。为此，本文提出了Dream Engine，一个高效的生成框架，旨在实现任意文本-图像交错控制。通过融合多模态信息编码器，该框架提高了文本与图像之间的对齐性能，利用两阶段训练策略，实现了文本与图像的联合对齐和多模态指令调优。实验结果表明，本方法在GenEval基准测试中取得了0.69的整体得分，表现出色，接近当前最先进的文本到图像生成模型SD3.5和FLUX的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:02:19 GMT</pubDate>
</item>
<item>
<title>NeoBERT：下一代双向编码器的创新与突破</title>
<link>https://arxiv.org/abs/2502.19587</link>
<guid>https://arxiv.org/abs/2502.19587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBERT通过融合先进架构和数据，重定义了双向模型的能力。</p><br /><br /><p><strong>摘要：</strong> 随着架构、预训练和微调的创新，NeoBERT作为下一代双向编码器，极大提升了大规模自回归语言模型的学习和推理能力。NeoBERT采用了最优的深度宽度比例和4096个令牌的扩展上下文长度，成为现有基础模型的即插即用替代品。尽管参数仅为250M，NeoBERT在大规模MTEB基准上取得了超越BERT large、RoBERTa large以及其他现代编码器的卓越成绩。本文还评估了各项修改对GLUE的影响，并设计了统一的微调和评估框架以适应MTEB。为推动研究和实际应用，所有代码、数据、检查点及训练脚本均已公布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 03:27:32 GMT</pubDate>
</item>
<item>
<title>基于去耦价值策略优化的强化学习框架</title>
<link>https://arxiv.org/abs/2502.16944</link>
<guid>https://arxiv.org/abs/2502.16944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DVPO框架通过去耦价值模型与策略训练，提升了训练效率和效果。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新颖的去耦价值策略优化（DVPO）框架，旨在克服PPO基础的强化学习从人类反馈中的计算复杂性和不稳定性。DVPO使用预训练的全球价值模型（GVM）替代传统的奖励建模，该模型基于策略轨迹预测令牌级的回报估计。通过去耦价值模型与政策训练，DVPO在不依赖于手动调整奖励的情况下，显著降低了GPU内存使用量达40%和训练时间达35%。实验结果表明，DVPO在多个基准测试中表现优越，超越了现有的高效RLHF方法（如DPO），同时在性能上与最先进的PPO相匹配。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:55:41 GMT</pubDate>
</item>
<item>
<title>FINEREASON: 细粒度评估大语言模型推理能力的逻辑难题基准</title>
<link>https://arxiv.org/abs/2502.20238</link>
<guid>https://arxiv.org/abs/2502.20238</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍FINEREASON，一种用于评估大语言模型推理过程的新的基准。</p><br /><br /><p><strong>摘要：</strong> 近年来，大语言模型（LLMs）取得了显著进展，展现了从快速反应的“系统1”思维转向反思和纠错的“系统2”思维的重要转变。然而，现有的基准测试主要依赖最终答案的准确性，未能充分评估模型在推理过程中的中间步骤与反思能力。为了解决这一问题，本文引入了FINEREASON，一个逻辑难题基准，旨在对LLMs的推理能力进行细粒度评估。每个难题都可以分解为原子步骤，非常适合验证中间结果的正确性。同时，我们介绍了两个任务：状态检查和状态转移，以全面评估模型如何评估当前情况及计划下一步。此外，我们还提供了旨在增强一般数学任务上表现的难题训练集。结果表明，在我们的状态检查和转移数据上训练的模型在GSM8K上数学推理能力提升了最高5.1%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20238" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:14:11 GMT</pubDate>
</item>
<item>
<title>Mobius：无注释文本生成无缝循环视频的新方法</title>
<link>https://arxiv.org/abs/2502.20307</link>
<guid>https://arxiv.org/abs/2502.20307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobius方法实现了无注释文本直接生成无缝循环视频。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法Mobius，能够根据文本描述直接生成无缝循环视频，无需用户注释。该方法利用预训练的视频潜在扩散模型，在推断过程中通过构建潜在循环，将视频的起始和结束噪声连接。通过逐步在每一步中将第一帧潜在图像平移到最后，保持时间一致性，同时逐步提取多帧潜在图像的去噪信息。与传统的动图不同，Mobius方法不需要图像作为外观，从而避免了生成结果运动的限制，能够产生更动态的运动和更好的视觉质量。我们进行了多次实验和比较，以验证该方法在不同场景下的有效性，所有代码将公开提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:14:01 GMT</pubDate>
</item>
<item>
<title>FlexiDiT：一种动态计算预算的生成变换器</title>
<link>https://arxiv.org/abs/2502.20126</link>
<guid>https://arxiv.org/abs/2502.20126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiDiT通过动态计算预算提高生成效率，降低资源需求。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的生成模型FlexiDiT，它通过动态计算预算改善了现代扩散变换器在推理过程中对资源的需求。传统的静态计算预算方式在每个去噪步骤中分配固定的计算资源，这限制了其灵活性。我们提出的框架允许预训练的扩散变换器（DiT）模型转变为灵活的模型，能够在不降低图像生成质量的情况下，灵活处理不同的计算预算。在实验中，我们证明了相较于静态模型，FlexiDiT在类别条件和文本条件图像生成中，计算需求可降低超过40%。此外，我们的方法也适用于视频生成，FlexiDiT模型在生成样本时计算需求最高可减少75%，仍能保持优异的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:10:30 GMT</pubDate>
</item>
<item>
<title>R1-Translator: 增强推理能力的通用机器翻译框架</title>
<link>https://arxiv.org/abs/2502.19735</link>
<guid>https://arxiv.org/abs/2502.19735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出R1-Translator框架，通过推理增强实现通用机器翻译。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新框架R1-Translator (R1-T1)，旨在通过强化学习实现通用机器翻译中的推理能力，提升翻译质量。当前的方法多局限于特定翻译子任务或依赖于与人类不相符的推理链。这项研究的创新在于扩展推理翻译的范围到六种语言以及法律、医疗等多种领域，并制定了六种专家策划的推理模板，以反映人类的多层次推理策略。此外，R1-Translator还通过强化学习实现推理链的自我发现与避免遗忘，从而提升了翻译的灵活性。实验结果表明，在Flores-101测试集上，R1-Translator在21种语言和80个翻译方向上的表现稳步提升，尤其在15种训练中未见的语言上表现突出，展示出其对多语言翻译的良好适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:03:34 GMT</pubDate>
</item>
<item>
<title>UniTok：统一视觉生成与理解的新型离散标记器</title>
<link>https://arxiv.org/abs/2502.20321</link>
<guid>https://arxiv.org/abs/2502.20321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniTok通过多代码本量化缩小视觉生成与理解之间的差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了UniTok，一种新的离散视觉标记器，旨在缩小视觉生成与理解之间的表现差异。该模型编码了细粒度细节以支持生成，同时捕获高层语义以增强理解。研究表明，这两种目标在训练过程中可能导致损失冲突，然而，我们发现这一瓶颈源于离散标记的表示能力有限。为了解决这一问题，我们提出了多代码本量化，通过将向量量化分为多个独立的子代码本来扩展潜在特征空间，并避免因过大代码本导致的训练不稳定。实验结果显示，UniTok在提升统一离散标记器的性能上显著超越了领域特定的连续标记器，如在ImageNet上，UniTok取得了0.38的rFID（相比SD-VAE的0.87）和78.6%的零样本准确率（相比CLIP的76.2%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:34:45 GMT</pubDate>
</item>
<item>
<title>CODESYNC：适应动态代码演变的语言模型评估基准</title>
<link>https://arxiv.org/abs/2502.16645</link>
<guid>https://arxiv.org/abs/2502.16645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍CODESYNC，一个应对动态代码演变的实时更新数据引擎。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在软件工程中的表现出色，但在适应频繁更新的第三方库API方面面临挑战。本文提出CODESYNC，一个能够识别过时代码模式并收集Python第三方库实时代码知识更新的数据引擎。同时开发了CODESYNCBENCH，涵盖220个API的基准测试，提供3300个测试案例，评估LLMs与代码演变同步的能力。实验结果显示，不论是使用何种知识更新方法，这些语言模型在动态代码演变中仍然表现不佳。我们相信，该基准可以为未来实时代码知识更新的有效方法发展奠定坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:04:14 GMT</pubDate>
</item>
<item>
<title>Subtask导向的强化微调（SoRFT）：提升大语言模型的issue解决能力</title>
<link>https://arxiv.org/abs/2502.20127</link>
<guid>https://arxiv.org/abs/2502.20127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的SoRFT方法，通过分解任务提升问题解决效果。</p><br /><br /><p><strong>摘要：</strong> 当前主流问题解决框架大多依赖商业模型，导致高成本和隐私问题。现有的训练方法普遍存在泛化能力差、未能充分利用开源资源等缺陷。我们提出了子任务导向的强化微调（SoRFT），这种新方法将问题解决分解为文件定位、函数定位、行定位和代码编辑生成等结构化子任务。SoRFT分为两个训练阶段：首先，通过拒绝采样进行监督微调，使用真实数据过滤链式思维（CoT）数据；随后，利用基于真实数据奖励的PPO进行规则基础的强化学习。我们在SWE-Bench Verified和SWE-Bench Lite上评估了SoRFT训练的模型，在开源模型中取得了最先进的性能（例如，SoRFT-Qwen-7B在SWE-Bench Verified上解决了21.4%的问题）。实验结果表明，SoRFT显著提升了问题解决能力，改善了模型的泛化性，为商业模型提供了一种成本效益高的替代方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:38:04 GMT</pubDate>
</item>
<item>
<title>提升大型多模态模型性能的新策略：测试时重路由</title>
<link>https://arxiv.org/abs/2502.20395</link>
<guid>https://arxiv.org/abs/2502.20395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出的R2-T2方法提高了多模态模型在挑战性任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 在大型多模态模型（LMMs）中，视觉表示的感知能力通常不如大型语言模型（LLMs），这影响了模型在复杂下游任务中的表现。为了解决这一问题，研究者们引入了专家混合（MoE）机制来提供丰富的多粒度表示，然而，基于端到端训练的路由器并不总能为每个测试样本生成最优的路由权重。为弥补这一不足，本文提出了一种新颖且高效的方法——测试时重路由（R2-T2），该方法通过将路由权重向邻近正确预测样本的权重向量移动，来局部优化测试时的路由权重。本文还提出了三种不同优化目标和邻域搜索空间的R2-T2策略。实验结果表明，R2-T2在多项挑战基准测试中显著提升了LMM的性能，且不需再训练任何基本模型参数。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:27:24 GMT</pubDate>
</item>
<item>
<title>LongRoPE2：扩展大语言模型的上下文窗口</title>
<link>https://arxiv.org/abs/2502.20082</link>
<guid>https://arxiv.org/abs/2502.20082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongRoPE2提升了预训练大语言模型的上下文处理能力。</p><br /><br /><p><strong>摘要：</strong> LongRoPE2是一种创新方法，旨在扩展预训练大语言模型的有效上下文窗口，同时保持短上下文的性能。该方法的三大贡献包括：一是提出假设，现有方法在更高RoPE维度上的训练不足导致了持续的分布外（OOD）问题；二是开发了一种有效的RoPE重缩放算法，通过“针导向”的困惑度演化搜索来解决训练不足问题；三是采用混合上下文窗口训练方法，对模型权重进行微调，以适应长上下文序列的重缩放RoPE，同时保留使用原始RoPE的短上下文的性能。基于LLaMA3-8B和Phi3-mini-3.8B的广泛实验结果验证了该假设，并证明了LongRoPE2的有效性。LongRoPE2使得LLaMA3-8B达到128K的有效上下文长度，同时保持短上下文性能超过98.5%，仅需10B标记数据，比Meta的方法减少了80倍，却未能达成目标有效上下文长度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.20082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:22:53 GMT</pubDate>
</item>
<item>
<title>自我奖励推理大型语言模型的研究</title>
<link>https://arxiv.org/abs/2502.19613</link>
<guid>https://arxiv.org/abs/2502.19613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨自我奖励推理的语言模型及其自我校正能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了一种自我奖励推理的大型语言模型（LLMs），能够在推理时同时生成逐步推理过程并评估输出的正确性，而无需外部反馈。重点关注自我校正任务，该模型可以自主检测响应中的错误、修改输出，并决定何时终止迭代修正循环。为此，提出了一种两阶段的算法框架，首阶段通过序列拒绝采样合成包含自我奖励和自我校正机制的长链推理轨迹，随后对模型进行微调，以学习相关模式。第二阶段通过使用基于规则的信号的强化学习进一步提升模型评估响应准确性和修正输出的能力。在 Llama-3 和 Qwen-2.5 的实验中，结果表明该方法超越了内在的自我校正能力，性能与依赖外部奖励模型的系统相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:15:54 GMT</pubDate>
</item>
<item>
<title>多草稿推测解码的效率优化研究</title>
<link>https://arxiv.org/abs/2502.18779</link>
<guid>https://arxiv.org/abs/2502.18779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了多草稿推测解码在效率优化中的关键设计选择与理论上限。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了多草稿推测解码（MDSD）在自然语言处理中的效率瓶颈，重点分析了草稿采样方法和验证算法。通过研究最优传输问题的对偶，本文首次高效计算了MDSD的最优接受率，并评估了现有验证算法与理论上限之间的差距。研究表明，不同的草稿采样方法显著影响最优接受率，其中不重复采样优于重复采样。此外，目前的验证算法在两种采样方式下均未能达到理论上限。 findings建议，精心设计的草稿采样方法有可能提高最优接受率，并促进验证算法的开发，使其更接近理论最优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:03:36 GMT</pubDate>
</item>
<item>
<title>基于少量偏好的个性化优化框架FSPO研究</title>
<link>https://arxiv.org/abs/2502.19312</link>
<guid>https://arxiv.org/abs/2502.19312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FSPO框架，通过用户偏好实现语言模型的个性化快速适应。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为少量偏好优化（FSPO）的新框架，旨在提升大型语言模型（LLM）在用户交互应用中的个性化能力。FSPO将奖励建模重新定义为一种元学习问题，使得LLM可以通过很少的用户标记偏好快速适应，并构建专属的奖励函数。同时，为了解决实际偏好数据匮乏的问题，研究团队设计了合成偏好数据集，并成功生成超过100万个合成个性化偏好数据。研究表明，合成数据必须具备高多样性和一致性，以确保成功转移到真实用户。在对1500个合成用户进行电影评论、教育背景适应和一般问答等三种领域的个性化生成评估中，FSPO在合成用户的响应生成中实现了87%的胜率，而在与真实用户的开放性问答中，也有72%的胜率。这些结果表明FSPO在个性化生成任务中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19312" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 11:09:15 GMT</pubDate>
</item>
<item>
<title>Drop-Upcycling：提升混合专家模型训练效率的新方法</title>
<link>https://arxiv.org/abs/2502.19261</link>
<guid>https://arxiv.org/abs/2502.19261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Drop-Upcycling方法优化混合专家模型的训练效率，解决了上游回收的性能瓶颈问题。</p><br /><br /><p><strong>摘要：</strong> Mixture of Experts (MoE) 架构在训练和推理成本上远低于同等容量的密集模型。尽管上游回收（upcycling）能为基于预训练密集模型初始化的MoE模型带来初期性能提升，但其训练效率相比从零开始训练则会显著降低，导致长期效果不佳。为了解决这一问题，我们提出了Drop-Upcycling方法，该方法融合了利用预训练密集模型知识与对部分权重进行统计重初始化的两种看似矛盾的策略。在专家专精培养方面，该方法显著提升了MoE模型的知识获取效率。我们的实验结果展示，Drop-Upcycling在长期训练中显著超越了以往的MoE构建方法，尤其是在处理数千亿令牌的数据时表现出色。经过验证，我们的MoE模型包含5.9B活跃参数，达到了与同家族13B密集模型相媲美的性能，同时所需训练FLOPs约为其1/4。所有实验资源，包括源代码、训练数据、模型检查点及日志都已公开，以促进可再现性及未来MoE研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 10:12:03 GMT</pubDate>
</item>
<item>
<title>Rank1：基于测试时间计算的新型重排序模型</title>
<link>https://arxiv.org/abs/2502.18418</link>
<guid>https://arxiv.org/abs/2502.18418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rank1 是首个利用测试时间计算的重排序模型，提升检索性能。</p><br /><br /><p><strong>摘要：</strong> Rank1 是首个经过训练，能够利用测试时间计算的重排序模型。该模型展示了在信息检索中使用推理语言模型（如 OpenAI 的 o1 和 Deepseek 的 R1）进行蒸馏的有效性，从而快速提升小型模型的性能。我们收集并开源了超过60万个 MS MARCO 查询和段落的 R1 推理轨迹数据集。基于该数据集训练的模型显示出：在高级推理和指令遵循数据集上具有最先进的性能；在出分布时表现出色，能够响应用户输入的提示；具有可解释的推理链，可以提供给用户或基于 RAG 的系统。此外，我们还展示了这些模型的量化版本在减少计算和内存使用的同时，依然保留了强大的性能。总体而言，Rank1 展示了测试时间计算能够实现一种全新的可解释且高效的重排序检索模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 09:41:49 GMT</pubDate>
</item>
<item>
<title>双重优化嵌入信息的方法提升弱监督语义分割性能</title>
<link>https://arxiv.org/abs/2502.15885</link>
<guid>https://arxiv.org/abs/2502.15885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DOEI方法，通过双重优化嵌入信息提升弱监督语义分割效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法DOEI（Dual Optimization of Embedding Information），旨在提升弱监督语义分割（WSSS）的性能。传统的类激活图（CAM）在高维空间中由于类激活响应与语义信息的耦合不足，常导致目标共现或低激活，从而影响识别准确度。DOEI通过语义感知的注意力权重矩阵重建嵌入表示，优化嵌入信息的表达能力。具体而言，该方法在类到补丁的交互中放大高置信度的标记，抑制低置信度的标记。此外，DOEI还引入了混合特征对齐模块，结合RGB值、嵌入引导特征和自注意力权重，以增强候选标记的可靠性。全面实验表明，DOEI是一个有效的可插拔模块，能够显著提高先进的视觉变换器基础WSSS模型在PASCAL VOC和MS COCO等流行基准上的类激活图质量和分割性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 07:31:45 GMT</pubDate>
</item>
<item>
<title>知识单位：破解科学知识传播的版权壁垒</title>
<link>https://arxiv.org/abs/2502.19413</link>
<guid>https://arxiv.org/abs/2502.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出将学术文本转化为知识单位，以突破版权限制，促进科学知识传播。</p><br /><br /><p><strong>摘要：</strong> 随着版权法规限制了科学知识的传播，本文提出了一种新方法，即将学术文献转换为知识单位。这些知识单位利用大语言模型（LLM）提取文本的结构化数据，捕捉实体、属性和关系，而非风格内容，从而在法律和技术上都能有效传播科学知识。通过对德国和美国的法律分析，我们证明了知识单位提供了一种合法的知识分享框架，并且在保护版权的前提下能够保留约95%的原始文本事实内容。我们的研究表明，免于版权限制的科学知识可以为研究与教育带来变革性的好处。此外，我们还分享了一些开源工具，帮助将研究文档转换为知识单位，以促进科学知识的开放获取，同时尊重版权。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:18:26 GMT</pubDate>
</item>
<item>
<title>GHOST 2.0: generative high-fidelity one shot transfer of heads</title>
<link>https://arxiv.org/abs/2502.18417</link>
<guid>https://arxiv.org/abs/2502.18417</guid>
<content:encoded><![CDATA[
While the task of face swapping has recently gained attention in the research community, a related problem of head swapping remains largely unexplored. In addition to skin color transfer, head swap poses extra challenges, such as the need to preserve structural information of the whole head during synthesis and inpaint gaps between swapped head and background. In this paper, we address these concerns with GHOST 2.0, which consists of two problem-specific modules. First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations. Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions. Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping. We also tackle complex cases, such as large difference in hair styles of source and target. Code is available at https://github.com/ai-forever/ghost-2.0
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:15:43 GMT</pubDate>
</item>
<item>
<title>推出BIG-Bench Extra Hard：评估大型语言模型推理能力的新基准</title>
<link>https://arxiv.org/abs/2502.19187</link>
<guid>https://arxiv.org/abs/2502.19187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了BIG-Bench Extra Hard基准，旨在提升大型语言模型的推理能力评估。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在日常应用中的日益普及，要求其具备强大的推理能力。然而，目前的大多数推理基准主要集中在数学和编码能力上，未能全面评估更广泛的推理技能。为此，本文推出了BIG-Bench Extra Hard（BBEH）基准，通过引入更具挑战性的任务来测试LLMs的推理能力，旨在填补这一空白。不同于之前的BIG-Bench Hard（BBH），BBEH对每个任务进行替换，显著提高了难度。我们的评估结果显示，最佳通用模型在BBEH上仅达到9.8%的平均准确率，而最佳推理专用模型的准确率为44.8%，表明LLMs在推理能力上仍有显著的提升空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:43:05 GMT</pubDate>
</item>
<item>
<title>提升语言模型反驳能力以加速科学发现</title>
<link>https://arxiv.org/abs/2502.19414</link>
<guid>https://arxiv.org/abs/2502.19414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出评估语言模型反驳能力的新基准，以加速科学研究。</p><br /><br /><p><strong>摘要：</strong> 随着对语言模型（LMs）在加速科学发现潜力的兴奋不断增加，反驳假设成为科学进步的关键。然而，目前的基准主要评估模型生成解决方案的能力，而缺乏对其反驳能力的评估。我们建议开发能够评估模型生成反例的基准。为此，我们引入REFUTE，一个动态更新的基准，包括近期问题和编程竞赛中的错误提交，通过代码执行自动评估反例。我们的分析显示即使是最佳的推理代理（如OpenAI o3-mini）在REFUTE上仅能为不足9%的错误解决方案生成反例，尽管其评分表明其能从零解决48%的问题。希望我们的研究能够推动对语言模型反驳能力的评估与提升，这是加速研究和模型自我改进的关键能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:36:29 GMT</pubDate>
</item>
<item>
<title>CritiQ：基于人类偏好的自动数据选择方法</title>
<link>https://arxiv.org/abs/2502.19279</link>
<guid>https://arxiv.org/abs/2502.19279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CritiQ是一种新型的数据选择方法，通过人类偏好自动挖掘数据质量标准。</p><br /><br /><p><strong>摘要：</strong> CritiQ是一种创新的数据选择方法，旨在提高语言模型的性能，依赖于人类偏好的高质量数据。与传统的依赖手动启发式、困惑度指标和分类器的方法相对，CritiQ通过仅使用30对人工标注的样本，自动挖掘数据质量标准。其核心组件CritiQ Flow通过经理代理演变质量标准，工作代理进行成对判断，并建立知识库以提取前期工作的质量标准，从而增强CritiQ Flow。与基于困惑度和分类器的方法相比，基于语言的标准更具可解释性且具有可重用性。经过标准的推导后，CritiQ Scorer被训练用于赋予数据质量分数并执行高效的数据选择。我们在代码、数学和逻辑领域展示了该方法的有效性，在人类标注的测试集上实现了高准确率，并在持续训练Llama 3.1模型后，观察到相比均匀采样在下游任务上的表现提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:47:02 GMT</pubDate>
</item>
<item>
<title>PosterSum：科学海报总结的前沿基准</title>
<link>https://arxiv.org/abs/2502.17540</link>
<guid>https://arxiv.org/abs/2502.17540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PosterSum为科学海报与其摘要的总结提供新基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PosterSum，这是一个新颖的基准，旨在推动视觉语言模型的发展，以便理解并总结科学海报为研究论文摘要。数据集中包含16,305个会议海报及其对应摘要，这些海报在图像格式中呈现，并面临复杂布局、密集文本区域、表格和图形等多种视觉理解挑战。我们对当前最先进的多模态大型语言模型（MLLMs）进行了基准测试，结果显示它们在准确理解和总结科学海报方面表现不佳。为此，我们提出了Segment & Summarize，一种分层方法，其在自动化指标上表现超过现有MLLMs，ROUGE-L指标提升了3.14%。该研究为未来海报总结的研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:37:24 GMT</pubDate>
</item>
<item>
<title>多语言模型的事实知识回忆与跨语言转移研究</title>
<link>https://arxiv.org/abs/2502.17955</link>
<guid>https://arxiv.org/abs/2502.17955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多语言模型在跨语言知识转移中的局限性与改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多语言模型在跨语言知识转移中的不足，尽管模型在某一语言中能够正确回忆事实，但在其他语言中却经常出现知识传递失败的现象。我们提出了一个包含10,000个国家相关事实的基准测试，涵盖了13种语言，并引入了三种新的评估指标：事实回忆分数、知识转移性分数和跨语言事实知识转移性分数，以量化多语言模型的事实回忆与知识转移能力。研究结果显示当前最先进的多语言模型在跨语言泛化方面存在根本性缺陷，表现出对使用语言的敏感性，知识转移不够有效。这一发现强调了模型需识别语言特有的事实可靠性，并在不同语言间有效利用最可信的信息。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:17:58 GMT</pubDate>
</item>
<item>
<title>首个希腊金融评估基准与语言模型的推出</title>
<link>https://arxiv.org/abs/2502.18772</link>
<guid>https://arxiv.org/abs/2502.18772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推动希腊金融NLP发展的Plutus-ben和Plutus-8B基准与模型发布。</p><br /><br /><p><strong>摘要：</strong> 尽管希腊在全球经济中扮演重要角色，但因希腊语的复杂性及缺乏特定领域数据，希腊金融语境下的大型语言模型（LLMs）仍未得到充分探索。为弥补这一空白，本文推出了Plutus-ben，这是第一个希腊金融评估基准，以及Plutus-8B，首个经过希腊领域特定数据微调的希腊金融LLM。Plutus-ben涵盖了五个核心金融NLP任务，包括命名实体识别、问答、摘要归纳及主题分类，旨在推动系统化和可重复的LLM评估。同时，我们还呈现了三种高质量的希腊金融数据集，均由专业母语者仔细注释，并与两个现有资源相结合。对22个LLMs在Plutus-ben上的全面评估显示，希腊金融NLP面临语言复杂性、特定领域术语及金融推理能力不足的挑战。这些结果强调了跨语言转移的局限性、希腊训练模型对金融专业知识的需求，以及将金融LLMs适配于希腊文本的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:08:09 GMT</pubDate>
</item>
<item>
<title>Kanana系列双语语言模型的高效预训练与适应性方法</title>
<link>https://arxiv.org/abs/2502.18934</link>
<guid>https://arxiv.org/abs/2502.18934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kanana模型在韩语表现卓越，英语竞争力强，且计算成本显著低。</p><br /><br /><p><strong>摘要：</strong> Kanana是一系列双语语言模型，在韩语方面表现卓越，而在英语方面具有竞争力。该系列模型的计算成本显著低于同类的最先进模型。报告详细介绍了在预训练过程中采用的技术，包括高质量数据筛选、分阶段预训练、深度缩放及剪枝和蒸馏等，以实现计算高效且表现竞争力的模型。此外，报告还概述了Kanana模型后训练过程中的方法论，包括监督微调和偏好优化，旨在增强与用户的无缝交互能力。同时，报告详细说明了适应特定场景的模型调整方法，例如嵌入、检索增强生成和功能调用。Kanana模型系列的参数范围从21亿到325亿不等，其中21亿模型（基础、指令、嵌入）已公开发布，以促进对韩语语言模型的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18934" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:05:13 GMT</pubDate>
</item>
<item>
<title>DeltaBench：评估o1-like模型在长推理链上的表现</title>
<link>https://arxiv.org/abs/2502.19361</link>
<guid>https://arxiv.org/abs/2502.19361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍DeltaBench，用于评估o1-like模型在长推理链上的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DeltaBench，旨在评估不同o1-like模型（如QwQ和DeepSeek-R1）在长Chain-of-Thought（CoT）推理步骤方面的表现及现有大型语言模型（LLMs）对这些长CoT的批判能力。DeltaBench包含来自不同o1-like模型生成的长CoT，用于多种推理任务（如数学、代码和一般推理），并测量检测长CoT推理中错误的能力。通过对生成的长CoT进行细致分析，我们发现了不同o1-like模型的有效性和效率。此外，研究评估了现有的过程奖励模型（PRMs）和批评模型在检测每个标注过程错误方面的表现，旨在探讨现有模型的边界和限制。最终，本文希望DeltaBench能够指导开发者更好地理解模型在长CoT推理能力上的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:04:47 GMT</pubDate>
</item>
<item>
<title>利用量子力学知识提升3D分子表示的能谱预训练</title>
<link>https://arxiv.org/abs/2502.16284</link>
<guid>https://arxiv.org/abs/2502.16284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过能谱增强3D分子表示的预训练以融入量子力学知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了3D结构与分子能态之间的关系，并提出运用量子力学的能谱增强分子表示的预训练方法。现有方法多依赖经典力学建模，忽视了量子力学的效应。在此框架下，我们提出了SpecFormer，一种用于通过掩蔽补丁重建编码分子能谱的多谱编码器。通过对3D编码器与能谱编码器输出的对比目标进行对齐，我们提升了3D编码器对分子的理解。评估结果显示，基于我们预训练的表示在分子性质预测和动力学建模方面超越了现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:29:40 GMT</pubDate>
</item>
<item>
<title>AI助力科学发现：多智能体系统在生物医学领域的应用</title>
<link>https://arxiv.org/abs/2502.18864</link>
<guid>https://arxiv.org/abs/2502.18864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了一种AI共同科学家系统，旨在增强科学假设生成与实验验证。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于Gemini 2.0的AI共同科学家系统，旨在通过生成、辩论和进化的方法，辅助科学家发现新的研究假设。此系统具备多智能体架构和异步任务执行框架，支持灵活的计算扩展，特别关注生物医学领域的应用，如药物再利用、新靶点发现和细菌进化机制解析。具体而言，系统为急性髓性白血病提供了具有临床应用潜力的药物候选，而在新靶点发现中，提出了在肝纤维化研究中可验证的表观遗传靶点。此外，AI共同科学家还总结了细菌进化中的新基因转移机制，展示了AI在增强生物医学发现方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:18:06 GMT</pubDate>
</item>
<item>
<title>AISafetyLab: 统一的AI安全框架与工具包</title>
<link>https://arxiv.org/abs/2502.16776</link>
<guid>https://arxiv.org/abs/2502.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍AISafetyLab，一个集成AI安全方法的工具包。</p><br /><br /><p><strong>摘要：</strong> 随着AI模型在实际场景中的广泛应用，确保其安全性变得愈发重要。尽管在AI安全的评估与提升方面进行了大量努力，但缺乏标准化框架和全面工具包仍然是系统研究和实际应用的重大障碍。为此，文章提出了AISafetyLab，一个统一的框架和工具包，整合了代表性的攻击、防御及评估方法。AISafetyLab具有直观的界面，使开发者能够无缝应用各种技术，并保持结构良好、可扩展的代码库以支持未来的技术进步。此外，本文对Vicuna进行了实证研究，分析了不同攻击和防御策略的有效性，为未来研究提供了重要见解。AISafetyLab已在GitHub公开发布，致力于持续维护和改进。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:16:03 GMT</pubDate>
</item>
<item>
<title>跨上下文蒸馏方法提升单目深度估计精度</title>
<link>https://arxiv.org/abs/2502.19204</link>
<guid>https://arxiv.org/abs/2502.19204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出跨上下文蒸馏方法，显著提升单目深度估计的准确性。</p><br /><br /><p><strong>摘要：</strong> 单目深度估计(MDE)旨在从单张RGB图像中预测场景深度，对3D场景理解至关重要。近期零-shot MDE的进展利用了标准化深度表示和基于蒸馏的学习，以提高在多样场景中的泛化能力。然而，目前的蒸馏深度标准化方法依赖于全局标准化，可能会放大噪声伪标签，从而降低蒸馏效果。本文系统分析了不同深度标准化策略对伪标签蒸馏的影响，并提出了跨上下文蒸馏方法，结合全局和局部深度线索以增强伪标签质量。此外，我们还引入了一种多教师蒸馏框架，充分利用不同深度估计模型的互补优势，旨在提供更稳健和准确的深度预测。大量在基准数据集上的实验证明，我们的方法在定量和定性方面均显著优于当前最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19204" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:10:20 GMT</pubDate>
</item>
<item>
<title>基于Manim动画的定理解释视频生成与评估</title>
<link>https://arxiv.org/abs/2502.19400</link>
<guid>https://arxiv.org/abs/2502.19400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨使用TheoremExplainAgent生成定理解释视频的有效性及评估基准。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了TheoremExplainAgent，这是一种利用Manim动画生成长达5分钟的定理解释视频的代理方法。我们提出了TheoremExplainBench，一个涵盖240个定理的多学科基准测试，配备5种自动评估指标，以系统性地评估多模态定理解释。研究结果表明，代理计划对于生成详细的长格式视频至关重要，其中o3-mini代理的成功率为93.8%，整体得分为0.77。然而，定量和定性研究也揭示出大多数视频存在视觉元素布局的小问题。此外，多模态解释揭示了文本解释未能暴露的更深层次的推理缺陷，强调了多模态解释的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:07:49 GMT</pubDate>
</item>
<item>
<title>一种结合可验证正确性信号的代理奖励建模方法</title>
<link>https://arxiv.org/abs/2502.19328</link>
<guid>https://arxiv.org/abs/2502.19328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了代理奖励建模方法，提高大语言模型的训练和推理效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了代理奖励建模（Agentic Reward Modeling）方法，强调奖励模型在训练和推理中对于大语言模型的重要性。现有的奖励模型大多集中于人类偏好，忽视了可验证的正确性信号。我们实现了一个名为RewardAgent的奖励代理，将人类偏好奖励与事实性和指令遵循等两个可验证信号结合，以提供更可靠的奖励。经过对现有奖励模型基准的全面实验，RewardAgent在真实世界下游任务的推理时间最优搜索中显著优于传统奖励模型。此外，我们使用RewardAgent构建了训练偏好对，并以DPO目标训练大语言模型，在各种自然语言处理基准测试中取得了优秀表现。代码已公开发布，以便进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.19328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:05:16 GMT</pubDate>
</item>
<item>
<title>基于预训练值环境模型的无环境强化学习框架</title>
<link>https://arxiv.org/abs/2502.18906</link>
<guid>https://arxiv.org/abs/2502.18906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于预训练模型的无环境强化学习框架，提高GUI代理的性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对图形用户界面（GUI）代理的无环境强化学习框架，旨在解决传统环境基于RL方法在交互成本和奖励泛化上的挑战。该框架利用预训练的值环境模型（VEM），有效地将价值估计与策略优化解耦。VEM通过离线数据直接预测状态-动作值，从而提取人类交互结果的先验知识，而无需环境反馈或下一个状态预测。这种方法能够避免错误累积，提高对用户界面变化的韧性，强调语义推理在决策中的重要性。该框架分为两个阶段：首先预训练VEM以估计长期行动效用，然后利用固定的VEM信号引导策略探索，支持无布局依赖的GUI自动化。在Android-in-the-Wild基准测试中，VEM在离线和在线设置中均取得了领先的表现，显著优于传统的无环境基线，与无需交互成本的环境基方法相匹配，展示了语义感知价值估计可以达到与在线训练方法相当的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18906" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:02:50 GMT</pubDate>
</item>
<item>
<title>通过词汇课程学习提升语言模型的预训练效率</title>
<link>https://arxiv.org/abs/2502.17910</link>
<guid>https://arxiv.org/abs/2502.17910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种词汇课程学习方法，提高语言模型的预训练效率。</p><br /><br /><p><strong>摘要：</strong> 现代语言模型依赖于预训练前确定的静态词汇，而人类语言学习则表现出适应性词汇获取。为弥补这一差距，本文提出了词汇课程学习的方法，该方法通过词汇规模的对数线性缩放增益来提高预训练效率。该方法在熵引导的词汇扩展与模型优化之间交替进行，使模型能够在不同的标记粒度间学习可迁移的表示。研究表明，较长的标记捕捉可预测的内容，而较短的标记则聚焦于更复杂难测的上下文。我们在小规模GPT模型上的实验显示了这种动态标记化的有效性，提升了扩展效率。我们还发布了代码以支持进一步研究，并计划将实验扩展到更大模型和不同领域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 18:40:15 GMT</pubDate>
</item>
<item>
<title>LDGen：高效的多语言文本到图像生成方法</title>
<link>https://arxiv.org/abs/2502.18302</link>
<guid>https://arxiv.org/abs/2502.18302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LDGen是一种将大语言模型高效整合入文本到图像生成的创新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LDGen，一种将大语言模型（LLMs）整合到现有文本到图像扩散模型中的新方法，旨在降低计算需求。传统的文本编码器（如CLIP和T5）在多语言处理方面存在局限，影响了跨语言的图像生成。为了解决这些问题，我们利用LLMs的高级能力，采用分层字幕优化和人类指令技术来提取精确的语义信息。同时，我们融入了一个轻量级适配器和跨模态精炼器，以促进LLMs和图像特征之间的高效特征对齐和交互。实验结果表明，LDGen在提示遵循性和图像美学质量上均超越了基线模型，同时无缝支持多个语言的零-shot图像生成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 16:56:34 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在视觉识别中的效果与干预研究</title>
<link>https://arxiv.org/abs/2502.17422</link>
<guid>https://arxiv.org/abs/2502.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在视觉识别中的局限性及提高方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了多模态大语言模型（MLLMs）在处理图像问答时，对于小视觉细节的感知能力。实验结果表明，MLLMs的表现对视觉主题的大小非常敏感，并通过干预研究证明此效果是因果关系。文章还分析了MLLMs在回答视觉问题时的注意力模式，发现即使在错误回答时，它们也能准确定位注意力。基于这些发现，提出了一种无需训练的视觉干预方法，利用MLLM内在的注意力和梯度图来增强其对小视觉细节的感知。通过在两个广泛使用的MLLM和七个视觉问答基准上的评估，证明该方法能够显著提高模型的准确性，而无需额外训练。研究结果揭示了在小细节视觉识别任务中应用MLLMs的风险，同时提供了利用模型内部状态进行视觉干预的有效解决方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 14:46:51 GMT</pubDate>
</item>
<item>
<title>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</title>
<link>https://arxiv.org/abs/2502.16069</link>
<guid>https://arxiv.org/abs/2502.16069</guid>
<content:encoded><![CDATA[
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4times improvement in correctly answering experimental questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:51:05 GMT</pubDate>
</item>
<item>
<title>统计学在大型语言模型中的作用与挑战</title>
<link>https://arxiv.org/abs/2502.17814</link>
<guid>https://arxiv.org/abs/2502.17814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨统计学如何提升大型语言模型的可信性与透明性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在人工智能领域的迅速发展，其在文本生成、推理和决策等多项任务中的卓越能力得到认可。然而，当前的成功主要依赖于计算能力和深度学习架构的进步，但在不确定性量化、决策制定、因果推断以及分布转移等方面却面临着挑战。本文探讨了统计学在这些领域中的重要作用，特别是在提高模型的可信性和透明性方面，包括不确定性量化、可解释性、公平性、隐私、掩码和模型适应性等问题。同时，本文还讨论了大型语言模型在统计分析中的潜在作用。通过促进人工智能与统计学之间的深度合作，我们希望推动LLMs理论基础和实践应用的进步，从而更好地应对复杂的社会挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:34:59 GMT</pubDate>
</item>
<item>
<title>WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging</title>
<link>https://arxiv.org/abs/2502.18316</link>
<guid>https://arxiv.org/abs/2502.18316</guid>
<content:encoded><![CDATA[
We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:53:44 GMT</pubDate>
</item>
<item>
<title>基于提示的语言模型评估方法P2L的提案</title>
<link>https://arxiv.org/abs/2502.14855</link>
<guid>https://arxiv.org/abs/2502.14855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法P2L，用于更加精准的语言模型评估。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Prompt-to-Leaderboard（P2L）的方法，用于解决现有语言模型（LLM）评估中的问题，特别是平均指标无法展示用户与提示特定的性能差异。P2L通过将自然语言提示输入到LLM中，生成Bradley-Terry系数的向量，从而预测人类偏好的投票。此方法允许实现针对特定提示的无监督任务评估、最优的查询路由、个性化和模型优缺点的自动评估。根据Chatbot Arena的数据，P2L能更好地反映语言模型性能的细微差异。研究还发现，P2L的提示特定评估能力遵循与LLM相似的幂律缩放。在2025年1月，基于该方法训练的路由器在Chatbot Arena排行榜上获得第一名。相关代码可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:43:07 GMT</pubDate>
</item>
<item>
<title>提升大语言模型调优性能的可扩展偏好数据构建策略</title>
<link>https://arxiv.org/abs/2502.16825</link>
<guid>https://arxiv.org/abs/2502.16825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个可扩展的偏好数据构建策略以提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了通过重复随机抽样来扩大大语言模型（LLMs）对齐过程中的采样规模，以提高性能。传统方法选择最高奖励的样本作为已选择回应，最低奖励的样本作为拒绝回应进行直接偏好优化（DPO），然而实验证实这一策略在样本量增加时表现不佳。为解决此问题，研究者基于样本奖励的正态分布特征构建偏好数据，定义七个代表性奖励点，并系统探讨其21种成对组合。通过对四个模型使用AlpacaEval 2进行评估，发现选择奖励位置为μ - 2σ的拒绝回应而不是最低奖励，对于优化性能至关重要。最终，文章提出了一种可扩展的偏好数据构建策略，能够随着样本规模的增加持续提升模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16825" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 09:40:23 GMT</pubDate>
</item>
<item>
<title>LaTIM: 基于Mamba模型的细粒度令牌级可解释性方法</title>
<link>https://arxiv.org/abs/2502.15612</link>
<guid>https://arxiv.org/abs/2502.15612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LaTIM方法，以提升Mamba模型的可解释性。</p><br /><br /><p><strong>摘要：</strong> 本研究引入LaTIM，一种新颖的令牌级分解方法，旨在提高Mamba-1和Mamba-2模型的可解释性。尽管状态空间模型（SSMs）如Mamba在长上下文序列建模方面展示出优越性能，但其在可解释性工具方面的缺乏限制了对其内部机制的理解。我们的方法通过细粒度地分解令牌的贡献，使得用户能够清晰地了解不同层次中Mamba的选择性序列处理方式。我们在机器翻译、复制以及基于检索的生成等多个任务上对LaTIM进行了广泛评估，结果显示其有效揭示了Mamba模型的令牌间交互模式，增强了模型的可解释性和透明度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 07:28:05 GMT</pubDate>
</item>
<item>
<title>引入视觉感知标记提升多模态大语言模型性能</title>
<link>https://arxiv.org/abs/2502.17425</link>
<guid>https://arxiv.org/abs/2502.17425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过视觉感知标记提升多模态大语言模型的视觉感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLM）在视觉信息利用中的感知过程的不足之处，尤其是在视觉感知的自主控制方面。为此，提出了视觉感知标记的概念，旨在赋予MLLM控制其视觉感知过程的机制。设计了两种类型的视觉感知标记：区域选择标记和视觉再编码标记。MLLM以此生成的标记来触发额外的视觉感知操作，从而显著改善空间推理和细致理解等任务的表现。实验结果表明，引入视觉感知标记后，2B模型的平均性能提高了23.6%，得分达到0.708，且比7B参数模型出色13.4%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 02:37:36 GMT</pubDate>
</item>
<item>
<title>压缩LLM的最新进展与彩票模型假设</title>
<link>https://arxiv.org/abs/2502.17535</link>
<guid>https://arxiv.org/abs/2502.17535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨LLM的压缩技术及彩票模型假设对性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文旨在减少大型语言模型（LLMs）的计算和存储成本，探讨模型压缩和KV缓存压缩的研究进展。现有方法主要关注压缩后LLMs的性能保持，通常通过困惑度或准确性来评估在常识知识问答和基本算术推理任务上的表现。文中回顾了检索增强生成、多步骤推理、外部工具和计算表达能力对LLM性能的显著影响，提出了彩票LLM假设，认为对于特定的LLM和任务，存在一个较小的彩票LLM，在多步骤推理和外部工具的辅助下，能够达到与原始LLM相同的性能。同时，讨论了彩票LLM和KV缓存压缩在当前方法中被忽略的关键能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 01:04:23 GMT</pubDate>
</item>
<item>
<title>K-LoRA：一种无训练的内容与风格融合方法</title>
<link>https://arxiv.org/abs/2502.18461</link>
<guid>https://arxiv.org/abs/2502.18461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出K-LoRA，通过有效融合内容与风格，优化LoRA的应用。</p><br /><br /><p><strong>摘要：</strong> 近年来的研究探讨了不同LoRA的结合，以共同生成学习的风格和内容。然而，现有方法要么无法有效同时保留原始主题和风格，要么需要额外的训练。本文认为LoRA的内在性质可以有效指导扩散模型合并学习到的主题和风格。在此基础上，提出了一种简单但有效的无训练的LoRA融合方法K-LoRA。K-LoRA在每个注意力层中比较要融合的每个LoRA中的前K个元素，确定最优融合的LoRA选择。这一选择机制确保在融合过程中保留主题和风格的最具代表性的特征，有效平衡其贡献。实验结果表明，所提方法能有效集成原始LoRA学习到的主题和风格信息，在定性和定量结果上均优于现有的训练基础方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18461" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:56:27 GMT</pubDate>
</item>
<item>
<title>Shakti VLM：高效的视觉语言模型家族</title>
<link>https://arxiv.org/abs/2502.17092</link>
<guid>https://arxiv.org/abs/2502.17092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Shakti VLM通过模型设计提升多模态学习的数据效率。</p><br /><br /><p><strong>摘要：</strong> Shakti VLM是一组具有10亿和40亿参数的视觉语言模型，旨在解决多模态学习中的数据效率挑战。尽管近期的视觉语言模型依赖于大量训练数据以实现强大性能，Shakti模型通过架构创新在较少的标记下获得了竞争力的成果。关键进展包括QK-Normalization以增强注意力稳定性、混合归一化技术及改进的位置信息编码。此外，三阶段训练策略进一步优化了学习效率。评估结果显示，Shakti-VLM-1B和Shakti-VLM-4B在文档理解、视觉推理、OCR提取及一般多模态推理中表现出色。这些结果强调，通过模型设计和训练策略而非单纯依赖大量数据，Shakti能够成为企业级多模态任务的高效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:38:42 GMT</pubDate>
</item>
<item>
<title>Scale-Distribution Decoupling: 稳定大规模语言模型训练的新方法</title>
<link>https://arxiv.org/abs/2502.15499</link>
<guid>https://arxiv.org/abs/2502.15499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新方法SDD，以稳定大规模语言模型的训练过程。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模语言模型（LLMs）预训练中的训练稳定性问题，提出了一种名为规模-分布解耦（SDD）的新方法。SDD通过明确地解耦全连接层中权重矩阵的规模和分布，利用归一化机制调节激活和可学习的缩放向量，保持良好的梯度条件，从而有效防止梯度爆炸和消散。实验结果表明，该方法在多个LLM架构中稳定训练效果，并在不同归一化配置下优于现有技术。SDD还轻量化且与现有框架兼容，为大规模语言模型训练的稳定提供了一种实用解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:26:11 GMT</pubDate>
</item>
<item>
<title>WebGames：评估通用网页浏览AI代理的新基准套件</title>
<link>https://arxiv.org/abs/2502.18356</link>
<guid>https://arxiv.org/abs/2502.18356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebGames通过50多个挑战评估网页浏览AI的性能和限度。</p><br /><br /><p><strong>摘要：</strong> WebGames是一个综合基准套件，旨在通过50多个互动挑战评估通用网页浏览AI代理的能力。这些挑战针对人类易于理解的操作，同时系统性地测试当前AI系统在基本浏览器交互、高级输入处理、认知任务、工作流自动化和互动娱乐等方面的局限性。该框架通过密闭测试环境消除外部依赖，确保可再现的评估与可验证的真实解决方案。我们对领先的视觉语言模型进行了评估，包括GPT-4o、Claude Computer-Use、Gemini-1.5-Pro和Qwen2-VL，结果显示这些AI系统与人类表现之间存在显著能力差距，最佳AI系统的成功率仅为43.1%，而人类的表现为95.7%，突显了当前AI系统在处理人类认为直观的常见网页交互模式方面的基本限制。该基准在webgames.convergence.ai上公开可用，提供轻量级的客户端实现以促进快速评估周期。通过其模块化架构和标准化挑战规范，WebGames为测量更强大网页浏览代理的发展进展提供了坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:16 GMT</pubDate>
</item>
<item>
<title>兼顾人类听觉选择性的听觉注意驱动大型语言模型研究</title>
<link>https://arxiv.org/abs/2502.16794</link>
<guid>https://arxiv.org/abs/2502.16794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型，结合脑信号以改进听觉处理中的选择性注意力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Intention-Informed Auditory Scene Understanding (II-ASU)与Auditory Attention-Driven LLM (AAD-LLM)，一种原型系统，通过集成脑信号来推断听众注意力。研究指出，现有的听觉基础模型未能捕捉人类选择性听觉这一特性，限制了其产生与听者意图相符的回应能力。AAD-LLM通过采用颅内脑电图(iEEG)记录，首先预测听众关注的发言者，然后基于此推测的注意状态来生成回应。在多发言者场景中评估AAD-LLM，包括发言者描述、语音转录和提问回答，结果显示该模型在客观和主观评分上均表现出更好的意图一致性。这项研究为迈向意图意识的听觉人工智能开辟了新途径，探索了让机器听觉受到听众感知影响的新范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:08 GMT</pubDate>
</item>
<item>
<title>基于难度聚类的下游性能预测框架在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2502.17262</link>
<guid>https://arxiv.org/abs/2502.17262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于难度聚类的框架，以提高大语言模型的性能预测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的下游性能预测框架——基于难度聚类（COD），旨在解决大型语言模型（LLMs）训练中资源配置和性能预测的挑战。由于存在“出现现象”和任务难度分布不均等问题，现有性能预测方法面临准确性和可靠性不足的困境。COD通过根据任务难度特征聚类，构建可预测的支持子集，排除非出现和不可扩展的聚类，从而确保所选子集的分数能有效预测整体评估集的下游表现。同时，本文推导了性能指标从支持子集到全评估集的映射函数，确保了LLM下游性能的准确外推。经过应用于70B LLM的性能预测后，研究结果表明COD显著提高了预测准确性，平均绝对偏差仅为1.36%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17262" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:18:24 GMT</pubDate>
</item>
<item>
<title>SpargeAttn：通用稀疏和量化注意力机制的实现</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SpargeAttn，以加速多种模型的稀疏注意力机制。</p><br /><br /><p><strong>摘要：</strong> 本文提出了SpargeAttn，一种通用的稀疏和量化注意力机制，旨在提高大型模型的效率。由于传统注意力机制的时间复杂度为平方级别，SpargeAttn利用注意力图中的稀疏性预测注意力图，并优化矩阵乘法的计算。我们的方法包括两个阶段的在线过滤器：第一阶段快速准确地预测注意力图，跳过部分矩阵乘法；第二阶段设计了一个在线softmax感知过滤器，进一步减少计算开销。实验表明，SpargeAttn在语言、图像和视频生成等多种模型上显著加速计算，同时保留了端到端的性能指标。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:04:57 GMT</pubDate>
</item>
<item>
<title>SWE-RL：通过强化学习提升软件工程领域的大语言模型推理能力</title>
<link>https://arxiv.org/abs/2502.18449</link>
<guid>https://arxiv.org/abs/2502.18449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-RL方法提升了大型语言模型在软件工程上的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SWE-RL，这是一种首个将强化学习应用于现实世界软件工程推理的方案。尽管DeepSeek-R1重点关注编码和数学问题，SWE-RL通过使用轻量级规则奖励机制，如生成解决方案与真实解决方案间的相似度分数，使得大型语言模型（LLMs）能够自动恢复开发者的推理过程。该模型通过学习广泛的开源软件演化数据进行训练，取得了41.0%的解决率，成为中型LLM中的最佳表现，并在五个超领域任务上展现了普适的推理能力，展示了其在软件工程领域的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:03:08 GMT</pubDate>
</item>
<item>
<title>OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference</title>
<link>https://arxiv.org/abs/2502.18411</link>
<guid>https://arxiv.org/abs/2502.18411</guid>
<content:encoded><![CDATA[
Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:01:56 GMT</pubDate>
</item>
<item>
<title>匿名区域变换器（ART）：革命性的多层透明图像生成技术</title>
<link>https://arxiv.org/abs/2502.18364</link>
<guid>https://arxiv.org/abs/2502.18364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新方法ART，能高效生成多层透明图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为匿名区域变换器（ART）的新方法，能够根据全球文本提示和匿名区域布局直接生成可变多层透明图像。该方法的核心在于匿名区域布局，使生成模型能够自主决定视觉标记与文本标记的对应关系，从而与以往主导的语义布局形成鲜明对比。此外，层级区域裁剪机制能高效选择每个匿名区域的视觉标记，大幅减少注意力计算成本，且与全注意力方法相比，生成速度快12倍以上，层冲突也显著减少。值得一提的是，ART还提出了一种高质量多层透明图像自动编码器，支持透明度的直接编码和解码。这一技术为互动内容创作建立了新的范式，推动了多层图像生成的精确控制与可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:50:19 GMT</pubDate>
</item>
<item>
<title>KV-Edit：一种无训练的图像编辑背景一致性方法</title>
<link>https://arxiv.org/abs/2502.17363</link>
<guid>https://arxiv.org/abs/2502.17363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KV-Edit 提供了一种无训练的方法以提升图像编辑中的背景一致性。</p><br /><br /><p><strong>摘要：</strong> 背景一致性在图像编辑任务中仍然是一个重大挑战。尽管已有多项研究，现有方法在保持与原始图像相似性和生成符合目标内容之间仍面临权衡。本文提出了KV-Edit，这是一种利用KV缓存的无训练方法，通过保留背景标记而非重新生成背景，简化了复杂机制和训练需求，能够在用户提供的区域内生成与背景无缝结合的新内容。此外，我们还探讨了KV缓存编辑过程中的内存消耗，并通过无反转方法将空间复杂度优化至O(1)。该方法与任何基于DiT的生成模型兼容，无需额外训练。实验结果表明，KV-Edit在背景和图像质量上显著优于现有方法，甚至超越了基于训练的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:36:19 GMT</pubDate>
</item>
<item>
<title>MutaGReP: 基于变异的代码库计划搜索方法</title>
<link>https://arxiv.org/abs/2502.15872</link>
<guid>https://arxiv.org/abs/2502.15872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MutaGReP通过变异引导的方式优化代码库搜索，提升编码任务性能。</p><br /><br /><p><strong>摘要：</strong> MutaGReP（变异引导的代码库计划搜索）是一个针对如何向大型代码库中的LLM提供上下文的创新方法。与直接将整个代码库置入LLM的上下文窗口不同，MutaGReP通过执行神经树搜索，探索通过变异生成的计划，并结合符号检索器实现对代码库的自然语言化分析。这种方法在LongCodeArena基准测试中表现出色，使用不到5%的128K上下文窗口来生成计划，仍能够与填充整个代码库的GPT-4o的编码性能相媲美。同时，MutaGReP生成的计划使Qwen 2.5 Coder的32B和72B模型在面对最困难的LongCodeArena任务时，能够匹敌GPT-4o。这一研究为大规模代码库的有效利用和任务解决提供了新思路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 19:35:42 GMT</pubDate>
</item>
<item>
<title>多样化输入提示下的高质量3D形状和纹理生成框架</title>
<link>https://arxiv.org/abs/2502.14247</link>
<guid>https://arxiv.org/abs/2502.14247</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于多种输入的高质量3D形状和纹理生成框架。</p><br /><br /><p><strong>摘要：</strong> 本报告提出了一种全面的框架，用于从多样化输入提示（包括单幅图像、多视角图像和文本描述）生成高质量的3D形状和纹理。框架分为两个主要部分：3D形状生成和纹理生成。3D形状生成采用变分自编码器（VAE）将隐式3D几何形状编码到潜在空间，并使用扩散网络生成受输入提示条件的潜在变量，还探索了替代的艺术创作网格生成方法，展现了对简单几何形状的良好效果。纹理生成则采用多阶段流程，从生成正面图像开始，再生成多视角图像、RGB转PBR纹理转换以及高分辨率的多视角纹理细化。在每个阶段中，嵌入一致性调度器以保证多视角纹理的像素级一致性，确保无缝集成。该管道有效处理多种输入格式，利用先进的神经网络架构和新颖的方法生成高质量的3D内容。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14247" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 17:06:48 GMT</pubDate>
</item>
<item>
<title>语音交互中的大音频模型(LAM)评估研究</title>
<link>https://arxiv.org/abs/2502.15919</link>
<guid>https://arxiv.org/abs/2502.15919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过互动评估7,500个大音频模型的用户交互，探讨用户偏好与模型性能的关联。</p><br /><br /><p><strong>摘要：</strong> 随着AI聊天机器人普及，语音交互成为高效传达语义和社交信号的重要方式。本文研究通过互动方式评估大音频模型(LAM)，收集了484名参与者的7,500次交互。利用主题建模，我们识别了音频接口的主要使用案例，并分析了用户偏好排名和定性反馈，以确定最符合用户需求的模型。研究发现，静态基准与互动性能的相关性不强，单一基准的相关性不超过0.33。尽管结合多个粗粒度特征可产生适度的预测能力（R^2=0.30），但只有两个与口语问答和年龄预测相关的数据集显示出显著的正相关性。这表明，迫切需要开发与用户偏好更好关联的大音频模型评估方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 16:46:31 GMT</pubDate>
</item>
<item>
<title>Agentic Long-Context Understanding: 提升LLMs的复杂问题回答能力</title>
<link>https://arxiv.org/abs/2502.15920</link>
<guid>https://arxiv.org/abs/2502.15920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgenticLU框架通过自我澄清和上下文获取提升了LLMs的复杂问题处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Agentic Long-Context Understanding（AgenticLU）的框架，旨在通过自我澄清和上下文获取来增强大型语言模型（LLMs）对复杂问题的理解。AgenticLU的核心是Chain-of-Clarifications（CoC），通过模型自生成澄清问题和相应的上下文基础来完善其理解。通过将推理扩展为树搜索，我们在NarrativeQA上实现了97.8%的答案召回率，搜索深度达到三，分支因子为八。为了解决高成本搜索过程的训练问题，我们利用CoC工作流获取的偏好对进行两阶段模型微调：首先，进行监督微调以学习有效分解策略；其次，进行直接偏好优化以提升推理质量。实验结果显示，AgenticLU在七个长上下文任务中显著超越了最先进的提示方法和专门的长上下文LLMs，展现出稳健的多步推理能力，并在上下文长度增加时维持一致表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:50:27 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的房地产市场营销内容自动生成框架</title>
<link>https://arxiv.org/abs/2502.16810</link>
<guid>https://arxiv.org/abs/2502.16810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于大语言模型的框架，用于自动生成房地产营销内容，符合用户偏好。</p><br /><br /><p><strong>摘要：</strong> 本文开发了一种基于大语言模型（LLMs）的自主框架，旨在自动生成具有说服力和依据的房地产营销内容，聚焦于房地产 listings 的描述。该方法旨在使生成的内容与用户偏好相一致，同时突出有用的事实属性。框架包括三个关键模块：1）模拟专家行为以预测可销售特征的基础模块；2）将内容与用户偏好对齐的个性化模块；3）确保事实准确性和包含当地特色的营销模块。通过在房地产营销领域进行系统的人体实验，结果表明，我们的方法生成的营销描述明显优于人类专家的写作。研究结果表明，该基于 LLM 的自主框架在确保使用事实进行负责任生成的同时，有望实现大规模的目标营销自动化。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:26:42 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型的归纳推理能力：InductionBench基准介绍</title>
<link>https://arxiv.org/abs/2502.15823</link>
<guid>https://arxiv.org/abs/2502.15823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在归纳推理方面的能力，提出InductionBench基准。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理方面取得了显著进展，但现有基准大多侧重于演绎推理，而归纳推理的研究较少。归纳推理是科学发现的核心，能够从观察的数据中推导出一般原则。为评估LLMs的归纳推理能力，本文介绍了InductionBench这一新基准。实验结果表明，即便是最先进的模型，在处理简单的复杂性类别时仍然难以有效掌握，揭示了当前LLMs在归纳推理能力方面的显著不足。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:58:10 GMT</pubDate>
</item>
<item>
<title>量化技术在大语言模型安全性评估中的应用</title>
<link>https://arxiv.org/abs/2502.15799</link>
<guid>https://arxiv.org/abs/2502.15799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨量化技术与大语言模型的安全性评估。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）的发展，它们在解决现代挑战和实际应用中发挥了重要作用。然而，其计算成本仍是广泛采用的一大障碍。量化技术被视为降低资源需求的有前景的解决方案，但量化模型的安全性和可信度尚未得到充分研究。为此，本文引入了OpenSafetyMini，一个新颖的开放式安全数据集，以更好地区分不同模型。此外，我们对4种先进的量化技术在LLaMA和Mistral模型上的表现进行了评估，使用4项基准测试，包括人类评估。结果显示，在4位精度下的最佳量化方法各异，而在2位精度下，向量量化技术展现了最佳的安全性和可信性表现，为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>利用机器学习分析胸部X光片预测COVID-19病程严重性</title>
<link>https://arxiv.org/abs/2502.16622</link>
<guid>https://arxiv.org/abs/2502.16622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用机器学习预测COVID-19患者病程严重性，取得显著效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了如何通过机器学习技术，特别是使用胸部X光片（CXR），来缓解COVID-19大流行中医护工作者的压力。我们整合了三种来源的数据，构建了一个大规模的COVID严重性数据集，并评估了转移学习在病情严重性回归和分类任务中的有效性。结果显示，预训练的DenseNet161模型在三类严重性预测中表现最佳，总体准确率达到80%；在轻度、中度和重度病例中的准确率分别为77.3%、83.9%和70%。同时，使用视图变换器（ViT）进行回归预测的平均绝对误差为0.5676，优于放射科医生的预测结果。项目源代码已公开，便于进一步研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:02:34 GMT</pubDate>
</item>
<item>
<title>提高机器翻译质量估计效率的模型</title>
<link>https://arxiv.org/abs/2502.14429</link>
<guid>https://arxiv.org/abs/2502.14429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出高效的质量估计模型，降低评估成本并减少性能损失。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器翻译质量估计的两个挑战进行研究：降低大规模质量估计的计算成本，以及开发一种经济的不确定性估计方法。我们提出Instant Confidence COMET模型，该模型在成本大幅降低的同时，性能与之前的高成本方法相当。进一步发展为Early-Exit COMET，这个模型能够在早期层级计算质量分数及其置信度，从而实现早期退出计算，降低评估成本。此外，我们将该模型应用于机器翻译的重排序任务，与上置信界限算法结合，能够在不对所有候选运行完整评估模型的情况下，从大池中找到最佳候选。通过这两种方法（评估和重排序），我们的模型在计算需求上减少了50%，且性能损失极小。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:17:04 GMT</pubDate>
</item>
<item>
<title>MegaLoc：多任务图像检索模型的研究</title>
<link>https://arxiv.org/abs/2502.17237</link>
<guid>https://arxiv.org/abs/2502.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MegaLoc模型在多个计算机视觉任务上展现出优异性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MegaLoc的图像检索模型，旨在解决在视觉场所识别、地标检索、视觉定位、3D重建和SLAM等多种计算机视觉任务中的图像检索问题。之前的解决方案通常针对特定任务，面对稍有变化的需求或外部数据时往往会失效。MegaLoc结合了多种现有方法、训练技术和数据集，取得了显著的效果。研究结果表明，MegaLoc在多个视觉场所识别数据集上达到了最新的最佳表现，且在常见的地标检索数据集上也表现优异。此外，在LaMAR数据集的视觉定位任务中，仅通过修改检索方法，MegaLoc创造了新的最佳结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:00:19 GMT</pubDate>
</item>
<item>
<title>TAME Agent Framework：构建去中心化层次多智能体系统</title>
<link>https://arxiv.org/abs/2502.15425</link>
<guid>https://arxiv.org/abs/2502.15425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAME Agent Framework提升了多智能体系统的可扩展性与适应性。</p><br /><br /><p><strong>摘要：</strong> TAME Agent Framework (TAG) 是一个新颖的去中心化层次多智能体系统构建框架，旨在克服现有层次强化学习方法的局限性，如仅限于双层结构或依赖集中式训练。TAG引入了一种名为LevelEnv的概念，将每一层的环境抽象化，从而支持任意深度的层次结构，标准化不同层之间的信息流，同时保持松耦合。通过实现不同类型的强化学习智能体在多个层级的组合，TAG在标准基准上实现了显著优于传统多智能体强化学习基线的性能，显示出去中心化层次组织在学习速度和最终性能提升方面的有效性，展示了TAG作为可扩展多智能体系统的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:51:02 GMT</pubDate>
</item>
<item>
<title>Stable-SPAM: 提高4位训练稳定性的优化器</title>
<link>https://arxiv.org/abs/2502.17055</link>
<guid>https://arxiv.org/abs/2502.17055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Stable-SPAM优化器，有效提升4位训练的梯度稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文全面评估了几种最近提出的4位训练优化器，指出低位精度加剧了对学习率的敏感性，且常常导致梯度范数不稳定，在较高学习率下会出现发散现象。其中，SPAM优化器实现了较好的性能，但在梯度范数稳定性上仍然存在困难。为解决这些问题，本文提出了Stable-SPAM，其通过增强的梯度归一化和剪切技术来稳定梯度。具体来说，Stable-SPAM 采用历史最大值自适应更新尖峰梯度的剪切阈值，基于历史l_2-范数统计对整个梯度矩阵进行归一化，并继承SPAM的动量重置策略，定期重置Adam的第一和第二动量，从而减少尖峰梯度的累积。大量实验表明，Stable-SPAM在4位大语言模型训练中显著稳定了梯度范数，性能优于Adam和SPAM。尤其是，使用Stable-SPAM训练的4位LLaMA-1B模型，在困惑度上比使用Adam训练的BF16 LLaMA-1B高出2点，同时在4位训练时，Stable-SPAM与Adam模型在损失上达成一致，而训练步骤数量仅为后者的一半。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:40:40 GMT</pubDate>
</item>
<item>
<title>社交媒体上信息获取与社区审核的互动研究</title>
<link>https://arxiv.org/abs/2502.14132</link>
<guid>https://arxiv.org/abs/2502.14132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明社区审核依赖于专业事实核查以对抗虚假信息。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了社交媒体上两种常用的抗击虚假信息策略：专业机构的事实核查与社区用户的内容审核。近期Twitter/X和Meta的政策变化显示，逐渐减少与事实核查组织的合作，转而依赖众包的社区备注。通过使用语言模型对大量Twitter/X社区备注进行标注，分析显示，社区备注引用事实核查来源的频率比以前报告的高出五倍，尤其是与更广泛虚假信息叙事关联的帖子，其备注引用事实核查来源的概率是其他来源的两倍。结果表明，成功的社区审核在很大程度上依赖于专业的事实核查。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14132" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:11:18 GMT</pubDate>
</item>
<item>
<title>布朗球面的连续CVS双射逆过程</title>
<link>https://arxiv.org/abs/2502.13074</link>
<guid>https://arxiv.org/abs/2502.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨布朗球面的CVS双射的逆过程及布朗蛇的构造。</p><br /><br /><p><strong>摘要：</strong> 布朗球面是一种随机度量空间，与二维球面同胚，作为多种随机平面图的普遍尺度极限而出现。其直接构造是通过连续的Cori-Vauquelin-Schaeffer (CVS)双射，该双射将标记树映射到平面图中，连续版本将Aldous的连续随机树（布朗蛇）与布朗球面关联。本文详细描述了连续CVS双射的逆过程，通过构造布朗蛇作为布朗球面的可测函数，强调了在处理布朗球面的方向时所需的特殊注意。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:03:39 GMT</pubDate>
</item>
<item>
<title>M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</title>
<link>https://arxiv.org/abs/2502.15167</link>
<guid>https://arxiv.org/abs/2502.15167</guid>
<content:encoded><![CDATA[
The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 03:36:50 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的通用颜色一致性方法GCC</title>
<link>https://arxiv.org/abs/2502.17435</link>
<guid>https://arxiv.org/abs/2502.17435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GCC方法通过扩散模型提升颜色一致性，适应不同相机传感器。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法GCC，旨在解决颜色一致性方法在不同相机传感器下泛化能力不足的问题。GCC利用扩散模型对图像中的颜色检查器进行重绘，以估算光照，主要创新包括：1) 单步确定性推断方法，能够重绘反映场景光照的颜色检查器；2) 拉普拉斯分解技术，保持检查器结构，同时允许基于光照的颜色适应；3) 基于遮罩的数据增强策略，用于处理不准确的颜色检查器标注。GCC在跨相机场景中表现出卓越的鲁棒性，在双向评估中达到了最优的25%误差率，分别为5.15°和4.32°，体现了该方法在不同相机特性下的稳定性和泛化能力，无需特定传感器的训练，适用于真实世界应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17435" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 02:06:00 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型在复杂机器人操作中的物理推理能力</title>
<link>https://arxiv.org/abs/2502.16707</link>
<guid>https://arxiv.org/abs/2502.16707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种提升视觉语言模型物理推理的框架，以改善多阶段机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的测试时间计算框架，以增强视觉语言模型(VLM)在多阶段机器人操作任务中的物理推理能力。当前的VLM在处理复杂的物理操作和长时间范围的推理时存在不足。我们的方法通过引入“反思”机制，逐步改进预训练的VLM：它利用生成模型想象未来的世界状态，利用这些预测来指导动作选择，并反思潜在的次优决策以优化推理。实验结果表明，该方法显著优于多种先进的商用VLM以及其他后训练方法，例如蒙特卡洛树搜索(MCTS)。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 01:02:05 GMT</pubDate>
</item>
<item>
<title>MONSTER：针对时间序列分类的大型数据集评估库</title>
<link>https://arxiv.org/abs/2502.15122</link>
<guid>https://arxiv.org/abs/2502.15122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MONSTER提供了大型数据集以推动时间序列分类领域的发展。</p><br /><br /><p><strong>摘要：</strong> MONSTER，即MONash可扩展时间序列评估库，是一个专为时间序列分类设计的大型数据集集合。现有的UCR和UEA时间序列分类库的基准测试虽然在该领域已有贡献，但其数据集规模较小，分别只有217和255个示例，限制了模型的多样性。MONSTER旨在通过引入更大规模的数据集来拓宽该领域的研究，使得研究人员在面对更大的数据时能够有效学习，从而在理论和实践上推动时间序列分类的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:37:53 GMT</pubDate>
</item>
<item>
<title>X-Dancer：基于音乐驱动的零样本人类舞蹈视频生成新方法</title>
<link>https://arxiv.org/abs/2502.17414</link>
<guid>https://arxiv.org/abs/2502.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Dancer通过静态图像生成多样化的舞蹈视频，提升真实感和表现力。</p><br /><br /><p><strong>摘要：</strong> X-Dancer是一种创新的零样本音乐驱动图像动画管道，能够从单一静态图像生成多样且真实感强的人类舞蹈视频。其核心是一个统一的变换器-扩散框架，使用自回归变换器模型生成与音乐同步的2D舞蹈姿态序列，指导扩散模型生成连贯且真实的舞蹈视频帧。与传统的3D人类动作生成方法不同，X-Dancer通过建模广泛的2D舞蹈动作，克服了数据限制，增强了可扩展性，能够捕捉细微的动作与音乐节拍的对齐。这一过程涉及构建空间组成的令牌表示，从与关键点置信度相关的2D人类姿态标签中编码大规模的身体运动及细微动作。X-Dancer的实验结果显示，其在多样性、表现力和真实感方面显著超越了最先进的技术，展示了强大的实用性和应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:17:51 GMT</pubDate>
</item>
<item>
<title>VideoGrain：实现细粒度视频编辑的零-shot方法</title>
<link>https://arxiv.org/abs/2502.17258</link>
<guid>https://arxiv.org/abs/2502.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍VideoGrain，一种实现细粒度视频编辑的新方法。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型的进步，视频生成和编辑能力显著提高，但多粒度视频编辑仍面临挑战。主要难点包括文本到区域控制的语义不匹配和扩散模型内部特征耦合。为解决这些问题，我们提出了VideoGrain，一种零-shot方法，通过调节时空注意力机制来实现视频内容的细粒度控制。该方法通过增强每个局部提示对其对应空间解耦区域的关注，同时最小化与无关区域的交互，来改善文本到区域的控制。此外，通过提高区域内部注意力和降低区域间干扰，来增强特征分离。通过大量实验证明，我们的方案在实际场景中实现了行业领先的性能。相关代码、数据及演示可在其项目页面找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:13:12 GMT</pubDate>
</item>
<item>
<title>RIFLEx：高效视频生成的频率成分分析与应用</title>
<link>https://arxiv.org/abs/2502.15894</link>
<guid>https://arxiv.org/abs/2502.15894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RIFLEx通过频率成分分析提升视频生成质量与时长扩展能力。</p><br /><br /><p><strong>摘要：</strong> 近年来的视频生成技术已能合成高质量的一分钟视频，但生成更长时间的视频并保持时间一致性仍面临重大挑战。现有的长度外推方法常导致时间重复或运动减速。本文系统分析位置嵌入中的频率成分，发现主导外推行为的内在频率。基于此洞察，提出RIFLEx，这是一种简约而有效的方法，通过减少内在频率来抑制重复，同时保持运动一致性，无需额外修改。RIFLEx实现了在最先进的视频扩散变换器上以训练无关方式进行高质量的2倍外推，并通过最少的微调提升质量，实现3倍外推而无需长视频。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:09:04 GMT</pubDate>
</item>
<item>
<title>多语种数学基准测试中的测试时间扩展方法研究</title>
<link>https://arxiv.org/abs/2502.17407</link>
<guid>https://arxiv.org/abs/2502.17407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究测试时间扩展如何影响多语种大模型的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了测试时间扩展在多语种大模型中的有效性，推出了多语种数学基准测试（MCLM），涵盖55种语言的竞赛级问题。我们对三种测试时间扩展方法进行了实验：结果奖励建模（ORM）、过程奖励建模（ORM）和预算强制（BF），应用于Qwen2.5-1.5B Math和我们训练的多语种大模型MR1-1.5B。实验结果显示，Qwen2.5-1.5B Math与ORM结合在MCLM上获得了35.8的分数，而MR1-1.5B与BF结合则获得了35.2的分数。尽管“思考型大模型”受到广泛关注，但在与传统扩展方法（如最佳N法）在推理FLOPs水平相近的情况下，它们的表现相当。此外，虽然BF方法在英语AIME上提高了20分，但在其他语言的平均增益仅为1.94分，表明测试时间扩展在多语种任务中的泛化能力有限。为推动进一步的研究，我们发布了MCLM、MR1-1.5B及其评估结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:37:53 GMT</pubDate>
</item>
<item>
<title>开放权重模型影响力演变框架研究</title>
<link>https://arxiv.org/abs/2502.15987</link>
<guid>https://arxiv.org/abs/2502.15987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架量化开放权重模型的影响力演变。</p><br /><br /><p><strong>摘要：</strong> 随着开放权重AI模型的快速发展，对哪些模型将推动创新及塑造AI生态系统的预测变得愈发重要。本文提出了一种基于引文动态的框架，旨在量化开放权重模型影响力的演变。我们借鉴Wang等人提出的科学引文模型，采用了即时性、持久性和相对适应性三个关键参数，以追踪开放权重模型的累积微调模型数量。研究发现，这种引文式的方法能够有效捕捉开放权重模型采用的不同轨迹，大部分模型的适配性良好，而异常值则显示了使用模式的独特性或突增情况。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15987" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:30:36 GMT</pubDate>
</item>
<item>
<title>Audio-FLAN: 统一音频理解与生成的指令调优数据集</title>
<link>https://arxiv.org/abs/2502.16584</link>
<guid>https://arxiv.org/abs/2502.16584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Audio-FLAN数据集促进音频理解与生成的统一建模。</p><br /><br /><p><strong>摘要：</strong> Recent advancements in audio tokenization have significantly improved the integration of audio capabilities into large language models (LLMs). 在音频理解和生成任务之间仍存在区分，限制了统一音频语言模型的发展。虽然指令调优在文本和视觉领域展现了卓越的泛化能力与零样本学习，但在音频领域的应用依然缺乏探索。为了解决这一问题，Audio-FLAN作为一个大规模指令调优数据集诞生，涵盖了80种跨越语音、音乐和声音领域的多样任务，包含超过1亿个实例。Audio-FLAN为统一音频语言模型奠定了基础，能够在零样本场景下无缝处理理解（如转录、理解）和生成（如语音、音乐、声音）任务。该数据集已在HuggingFace和GitHub上发布并将不断更新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:20 GMT</pubDate>
</item>
<item>
<title>Slam：24小时内在单一GPU上训练高质量语言模型的Recipe</title>
<link>https://arxiv.org/abs/2502.15814</link>
<guid>https://arxiv.org/abs/2502.15814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Slam是提升单GPU训练高质量语言模型的有效方案。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Slam的有效方案，可在单一学术GPU上于24小时内训练高质量的语言模型（SLM）。通过对模型初始化、架构、合成训练数据的偏好优化及其他组件的调试，本文进行了实证分析。研究表明，该训练方案具备良好的可扩展性，能够以更低的计算成本实现与领先SLM相媲美的结果。此外，在SLM扩展法则的背景下，研究结果远超计算最优性能预测，为SLM训练的可行性带来了乐观的前景。相关代码、数据、模型和样本可在指定网址查阅。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:12 GMT</pubDate>
</item>
<item>
<title>CTM基准：评估语言模型的时间推理能力</title>
<link>https://arxiv.org/abs/2502.16922</link>
<guid>https://arxiv.org/abs/2502.16922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CTM基准通过丰富的历史语境评估语言模型的时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 时间推理是人类认知的重要组成部分，对许多现实世界应用至关重要。尽管大型语言模型在时间推理方面取得了一些进展，但现有基准多依赖规则构建，缺乏上下文深度，且涉及的时间实体范围有限。为解决这些问题，我们推出了中文时间推理(CTM)基准，旨在评估语言模型在中国历史年代学的广泛背景下的时间推理能力。CTM强调跨实体关系、对时间的成对对齐，以及上下文化和文化根植的推理，提供了一个全面的评估。大量实验结果揭示了CTM所带来的挑战，并突出了改进的潜在方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:48:30 GMT</pubDate>
</item>
<item>
<title>DICEPTION：一种高效的通用视觉感知模型</title>
<link>https://arxiv.org/abs/2502.17157</link>
<guid>https://arxiv.org/abs/2502.17157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICEPTION是一种高效的视觉感知模型，能在低资源下完成多任务。</p><br /><br /><p><strong>摘要：</strong> DICEPTION是一个旨在创建高效通用视觉感知模型的研究，其目标是在有限的计算资源和训练数据上，运用经过数十亿图像预训练的文本到图像扩散模型。研究显示，DICEPTION在多个视觉感知任务上达到与最先进模型相当的效果，仅使用了0.06%的数据（600K对比1B像素级标注图像）。该模型采用颜色编码策略，将各类任务的输出进行统一，证明了为不同实例赋予随机颜色的策略在实体和语义分割中极为有效。此外，DICEPTION整合了各种视觉任务为条件图像生成，充分利用预训练的文本到图像模型，从而在训练成本上大幅降低。调整此模型到其他任务时，仅需在少量图像（如50张）和1%的参数上进行微调。DICEPTION为视觉通用模型提供了有效的解决方案和重要的洞察。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:39:29 GMT</pubDate>
</item>
<item>
<title>GOAT：一种提升LoRA性能的新型专家模型框架</title>
<link>https://arxiv.org/abs/2502.16894</link>
<guid>https://arxiv.org/abs/2502.16894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GOAT框架通过SVD结构改善LoRA在大规模语言模型中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管低秩适应(LoRA)为大规模语言模型的高效微调提供了可能，但其性能往往不及全量微调(Full FT)。目前的方法通过使用静态奇异值分解(SVD)子集来优化LOra，但这限制了对预训练知识的有效利用。为此，我们提出了大规模LoRA混合专家(GOAT)框架，该框架通过自适应整合相关先验和采用SVD结构的混合专家模型，解决了权重不匹配和复杂梯度动态的问题。此外，本研究还通过推导理论缩放因子，使优化与全量微调的混合专家模型对齐。我们的实验涵盖了25个数据集，包括自然语言理解、常识推理、图像分类和自然语言生成，结果表明GOAT在性能上已接近全量微调，展示了其卓越的效率与效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:35:41 GMT</pubDate>
</item>
<item>
<title>Mobile-Agent-V：基于视频指导的移动自动化框架</title>
<link>https://arxiv.org/abs/2502.17110</link>
<guid>https://arxiv.org/abs/2502.17110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobile-Agent-V框架通过视频指导提升移动设备任务管理效率。</p><br /><br /><p><strong>摘要：</strong> 随着移动设备使用的快速增长，提升任务管理的自动化水平显得尤为重要。然而，许多基于AI的框架由于缺乏充分的操作知识而面临挑战。虽然手动编写的知识可以帮助解决这一问题，但过程既繁琐又低效。为了解决这些问题，我们提出了Mobile-Agent-V框架，该框架利用视频指导提供丰富且具有成本效益的操作知识，从而促进移动自动化的发展。Mobile-Agent-V通过视频输入增强任务执行能力，无需专门的采样或预处理。同时，该框架集成了滑动窗口策略，并引入视频代理和深度反思代理，确保用户指令与执行动作保持一致。实验结果表明，相较于现有框架，Mobile-Agent-V实现了30%的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17110" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:31:17 GMT</pubDate>
</item>
<item>
<title>长上下文对大型语言模型的影响与挑战</title>
<link>https://arxiv.org/abs/2502.17129</link>
<guid>https://arxiv.org/abs/2502.17129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了长上下文对大型语言模型的影响、挑战及研究前景。</p><br /><br /><p><strong>摘要：</strong> 长上下文是自然语言处理领域的重要主题，为大型语言模型（LLMs）的发展提供了巨大机会，同时也伴随着诸多挑战。近年来，LLMs的上下文长度突破扩展至数百万个词元，研究从长度外推扩展至架构、基础设施、训练和评估等多方面。本文通过与人类超越有限性的比喻，探讨了LLMs在追求长上下文与面对其有限性之间的挣扎。我们全面展示了长上下文LLMs的生命周期，包括架构、基础设施、训练和评估等四个方面，并展示与之相关的技术全景。最后，本文提出了目前长上下文LLMs所面临的10个未解问题，希望为相关研究提供系统性的介绍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.17129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:27:11 GMT</pubDate>
</item>
<item>
<title>CodeCriticBench：全面评估大型语言模型的代码批判能力</title>
<link>https://arxiv.org/abs/2502.16614</link>
<guid>https://arxiv.org/abs/2502.16614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍CodeCriticBench，一个评估LLMs代码批判能力的全新基准。</p><br /><br /><p><strong>摘要：</strong> 本文引入了CodeCriticBench，一个针对大型语言模型(LLMs)的全面代码批判基准，旨在解决现有基准在代码任务评估中的局限性。现有基准通常仅关注一般领域的多样推理任务，对代码任务的评估则显不足，且缺乏不同维度的全面评价。CodeCriticBench覆盖了两个主要的代码任务：代码生成和代码问答，且根据难度进行了分类。评估协议包括基础的批判评价和针对不同特征的高级批判评价，为后者设计了细致的评估清单。通过对现有LLMs进行广泛实验，结果表明CodeCriticBench的有效性，为代码批判能力的系统评估提供了新的思路和工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:17:28 GMT</pubDate>
</item>
<item>
<title>多模态不一致性推理基准的建立与评估</title>
<link>https://arxiv.org/abs/2502.16033</link>
<guid>https://arxiv.org/abs/2502.16033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出多模态不一致性推理基准评估大型语言模型对现实内容的不一致性处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了多模态不一致性推理（MMIR）基准，旨在评估现有大型多模态语言模型（MLLMs）在处理真实世界布局丰富内容时的能力。基准包含534个具有挑战性的样本，涉及事实矛盾、身份误归、上下文不匹配、数量差异及时间/空间不一致等五个推理类别。经过评估，具备专门多模态推理能力的模型表现优异，而开源模型对不一致性错误尤为敏感。详细的错误分析显示，模型在检测单一模态中的不一致性时表现良好，特别是在文本中，但在跨模态冲突和复杂布局方面存在挑战。探测实验表明，单模态提示（如链式思维和集合标记方法）带来的改进微乎其微，揭示了跨模态推理中的瓶颈问题。这些发现强调了提升多模态推理能力的必要性，并指向未来的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:50 GMT</pubDate>
</item>
<item>
<title>生成性人工智能系统发布及其接入性分析</title>
<link>https://arxiv.org/abs/2502.16701</link>
<guid>https://arxiv.org/abs/2502.16701</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章分析了生成性AI系统的发布及接入性对用户影响的多维度考量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨生成性人工智能系统的发布决策对用户和利益相关者参与系统的影响，指出发布并未涵盖影响接入的诸多因素。作者将接入性分解为资源配置、技术可用性和实际效用三个维度，分析各种系统组件的权衡。通过对四种高性能语言模型的比较，揭示了开放式与闭源模型在接入性方面的共同性。接入变量为用户扩展和增加接入奠定基础，研究还考察了接入规模如何影响风险管理能力。此框架为系统发布决策、研究和政策制定提供了更全面的风险收益权衡视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.16701" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:15 GMT</pubDate>
</item>
<item>
<title>Seq2Exp：精准预测基因表达的序列转表达网络</title>
<link>https://arxiv.org/abs/2502.13991</link>
<guid>https://arxiv.org/abs/2502.13991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍Seq2Exp，一个用于预测基因表达的创新网络。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了从DNA序列预测基因表达的问题，尤其是寻找控制基因表达的调控元件。我们提出了Seq2Exp，一种专门设计的序列转表达网络，旨在识别并提取驱动目标基因表达的调控元件，从而提高基因表达预测的准确度。Seq2Exp能够捕捉表观基因组信号、DNA序列及其相关调控元件之间的因果关系。具体而言，我们通过条件化因果活跃的调控元件对表观基因组信号和DNA序列进行分解，结合信息瓶颈和Beta分布来合并它们的效应，同时过滤非因果成分。实验结果表明，Seq2Exp在基因表达预测任务中优于现有基线方法，并能够发现比MACS3等常用统计方法在峰值检测中更具影响力的区域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13991" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 17:06:56 GMT</pubDate>
</item>
<item>
<title>RareScale：结合语言模型与专家系统的罕见疾病诊断方法</title>
<link>https://arxiv.org/abs/2502.15069</link>
<guid>https://arxiv.org/abs/2502.15069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RareScale，提升罕见疾病的诊断准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RareScale，一个结合大语言模型（LLMs）与专家系统的新方法，以提高罕见疾病的识别能力。随着LLMs在医疗领域的应用日益增多，识别罕见疾病的能力显得尤为重要。RareScale通过模拟罕见疾病对话的数据训练罕见疾病候选预测模型，并将这些候选结果作为额外输入，集成到黑箱LLMs中，以实现最终的鉴别诊断。实验结果表明，RareScale在575种罕见疾病的诊断中，较传统黑箱LLMs的表现提升了超过17%的Top-5准确率，且在候选生成性能上也表现出色，达到88.8%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 15:59:07 GMT</pubDate>
</item>
<item>
<title>利用Tree-of-Debate框架提升科学文献评估的创新性辩论</title>
<link>https://arxiv.org/abs/2502.14767</link>
<guid>https://arxiv.org/abs/2502.14767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tree-of-Debate框架提升了科学文献的创新性辩论和评价能力。</p><br /><br /><p><strong>摘要：</strong> 随着现代技术的迅速发展，科学研究的成果日益分散，特别是在不同研究领域之间，评估其重要性和创新性变得愈加困难。为此，本文提出了Tree-of-Debate（ToD）框架，该框架利用大型语言模型（LLMs）将科学论文转化为具有不同观点的辩论角色。这一创新旨在强调结构化和批判性推理，而不仅仅是结果导向。ToD动态构建辩论树，允许对独立的创新论点进行细致分析。通过对多个领域的科学文献进行实验，并由专家学者进行评估，结果表明，ToD能够生成具有信息量的论点，有效对比不同论文，辅助研究人员在文献综述中的工作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:53:11 GMT</pubDate>
</item>
<item>
<title>大语言模型在联合国决策中的应用研究</title>
<link>https://arxiv.org/abs/2502.14122</link>
<guid>https://arxiv.org/abs/2502.14122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨大语言模型在联合国高风险政治决策中的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大语言模型（LLMs）在联合国（UN）决策过程中的应用，填补了该领域的研究空白。文章引入了一个新数据集，涵盖1994至2024年的联合国安全理事会（UNSC）记录，包括草案、投票记录和外交演讲，并提出首个全面评估LLMs的基准——联合国基准（UNBench）。该基准包含四个互相关联的政治科学任务，涉及联合国决策过程的三个阶段：起草、投票和讨论。通过实证分析，本文展示了LLMs在这一领域的潜力与挑战，提供了对其在政治科学中优势与局限性的深入见解。此外，该研究为AI与政治科学的交叉点贡献了新的研究方向和实际应用，UNBench数据集可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:18:28 GMT</pubDate>
</item>
<item>
<title>无调优身份保护文本到视频生成的新框架FantasyID</title>
<link>https://arxiv.org/abs/2502.13995</link>
<guid>https://arxiv.org/abs/2502.13995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种无调优的身份保护文本到视频生成框架FantasyID。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的无调优身份保护文本到视频生成框架FantasyID，旨在提高生成视频的面部动态和身份保持能力。该框架通过增强已经预训练的视频模型的面部知识，引入3D面部几何先验，确保视频合成过程中的面部结构合理性。此外，采用多视角面部增强策略以捕捉多样的2D面部外观特征，从而增强面部表情和头部姿势的动态性。在融合2D和3D特征时，本文采用了一种可学习的层感知自适应机制，选择性地将融合特征注入到每个DiT层，平衡身份保持与运动动态的建模。实验结果表明，FantasyID在当前无调优身份保护文本到视频生成方法中具有显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 11:28:12 GMT</pubDate>
</item>
<item>
<title>MedHallu：检测医疗领域大语言模型幻觉的新基准</title>
<link>https://arxiv.org/abs/2502.14302</link>
<guid>https://arxiv.org/abs/2502.14302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍MedHallu基准，评估医疗领域大语言模型的幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在医学问答中的使用日益增加，对其可靠性的严格评估变得至关重要。幻觉现象，即模型生成看似合理但实际上错误的输出，对患者安全及临床决策构成严重风险。为此，本文提出了MedHallu，这是首个专门设计用于医疗幻觉检测的基准，包含从PubMedQA衍生的10,000个高质量问答对，并通过控制流程系统生成幻觉回答。实验表明，最先进的LLMs，如GPT-4o、Llama-3.1和经过医学微调的UltraMedical，在这一二元幻觉检测任务上表现不佳，最佳模型在检测“难”类别幻觉时F1分数仅为0.625。通过双向蕴含聚类，我们发现更难检测的幻觉在语义上更接近真实答案。进一步的实验还表明，结合领域特定知识并引入“未确定”作为答案类别之一，可以将精度和F1分数相对于基线提高最多38%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:54:53 GMT</pubDate>
</item>
<item>
<title>多语言风格嵌入模型mStyleDistance的介绍与应用</title>
<link>https://arxiv.org/abs/2502.15168</link>
<guid>https://arxiv.org/abs/2502.15168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mStyleDistance是一个多语言风格嵌入模型，超越了现有的单语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多语言风格嵌入模型mStyleDistance，该模型利用合成数据和对比学习进行训练，支持九种语言。通过创建多语言STEL-or-Content基准，评估嵌入的质量，并在涉及不同语言的作者验证任务中应用这些嵌入。结果表明，mStyleDistance的嵌入在多语言风格基准上表现优越，且能够很好地泛化到未见特征和语言。该模型已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:33:37 GMT</pubDate>
</item>
<item>
<title>EgoSpeak：实时语音启动预测的创新框架</title>
<link>https://arxiv.org/abs/2502.14892</link>
<guid>https://arxiv.org/abs/2502.14892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoSpeak框架提升了对话代理在真实环境中的语音启动预测能力。</p><br /><br /><p><strong>摘要：</strong> EgoSpeak是一个新颖的框架，旨在解决对话代理在真实环境中语音启动时机的预测挑战。该框架从说话者的第一人称视角建模对话，能够实现类人交互，允许对话代理在不断观察环境的同时动态决定何时发言。EgoSpeak整合了四个关键功能：第一人称视角、RGB处理、在线处理和未剪辑视频处理，弥合了简化实验设置与复杂自然对话之间的差距。此外，我们推出了YT-Conversation，这是一个来自YouTube的丰富多样的对话视频集合，作为大规模预训练的资源。通过在EasyCom和Ego4D上的实验，EgoSpeak在实时预测方面优于随机及基于静默的基线，结果还强调了多模态输入和上下文长度在决定何时发言中的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 08:37:35 GMT</pubDate>
</item>
<item>
<title>针对用户特定安全标准的LLM安全性评估新基准</title>
<link>https://arxiv.org/abs/2502.15086</link>
<guid>https://arxiv.org/abs/2502.15086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出了U-SAFEBENCH以评估用户特定的LLM安全性。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLM）代理的广泛使用，其安全漏洞日益明显。尽管现有的安全评估基准主要依赖于通用标准，但这些标准未能考虑用户特定的需求，导致LLM在满足个体用户安全标准时表现不佳。为解决这一问题，本文首次提出U-SAFEBENCH基准，用于评估LLM的用户特定安全性。通过对18个广泛使用的LLM进行评估，我们发现当前的LLM在考虑用户特定安全标准时并未能有效确保安全。此外，我们基于链式思维提出了一种简单的改进方法，展示其在提升用户特定安全性方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:41:08 GMT</pubDate>
</item>
<item>
<title>WHAC框架：精确恢复人类模型与相机轨迹</title>
<link>https://arxiv.org/abs/2403.12959</link>
<guid>https://arxiv.org/abs/2403.12959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WHAC框架，实现了准确的人类模型与相机姿态恢复。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的框架WHAC，旨在从单目视频中准确恢复世界坐标系中的人类模型（SMPL-X）及相机姿态。通过结合世界、人体和相机三者的关键作用，我们着重于两个观察：相机帧下的SMPL-X估计方法可以直接恢复人类的绝对深度，而人类运动本身提供了空间线索。我们的框架不依赖传统优化技术，并且我们还创建了新的合成数据集WHAC-A-Mole，包含准确标注的人类和相机，展示了各种互动人类动作及真实的相机轨迹。实验结果显示，在标准和新建立的基准上，我们的方法具有显著的优越性和有效性，代码和数据集将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2403.12959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:37:26 GMT</pubDate>
</item>
<item>
<title>Evaluating Multimodal Generative AI with Korean Educational Standards</title>
<link>https://arxiv.org/abs/2502.15422</link>
<guid>https://arxiv.org/abs/2502.15422</guid>
<content:encoded><![CDATA[
This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:58:46 GMT</pubDate>
</item>
<item>
<title>大型语言模型情感边界处理评估框架的开放源代码基准</title>
<link>https://arxiv.org/abs/2502.14975</link>
<guid>https://arxiv.org/abs/2502.14975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们开发了一个框架以评估LLMs的情感边界处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种开放源代码的基准评估框架，用于评估大型语言模型（LLMs）在情感边界处理方面的能力。通过使用涵盖六种语言的1156条提示数据集，我们对三种主流LLMs（GPT-4o、Claude-3.5 Sonnet和Mistral-large）进行了评估，量化分析了它们在通过七种关键模式进行响应时的表现，包括直接拒绝、道歉、解释、转移、认可、设定边界和情感意识。结果显示，Claude-3.5在总体评分（8.69/10）上表现最佳，且其平均响应长度（86.51字）更长、更细致。分析还发现，英语交互的平均得分（25.62）显著高于非英语交互（< 0.22），英语响应中的拒绝率也显著高于非英语（43.20%对< 1%）。本研究揭示了模型特有的策略和局限性，提出未来可探讨更细致的评分方法、扩展语言覆盖范围及探究文化差异。我们的基准和方法论为LLM情感智能及边界设定能力的系统评估奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14975" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:44:41 GMT</pubDate>
</item>
<item>
<title>KITAB-Bench: 阿拉伯语OCR性能的新基准与挑战</title>
<link>https://arxiv.org/abs/2502.14949</link>
<guid>https://arxiv.org/abs/2502.14949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KITAB-Bench为阿拉伯语OCR提供全面基准，展示了新模型的优势与现有技术的局限。</p><br /><br /><p><strong>摘要：</strong> 随着检索增强生成技术(RAG)在文档处理中的广泛应用，文本识别的鲁棒性对知识提取至关重要。阿拉伯语OCR因其独特的连写脚本和复杂的排版特征面临更多挑战。为此，我们提出了KITAB-Bench，一个涵盖8809个样本的全面阿拉伯语OCR基准，涉及9个主要领域和36个子领域，包括手写文本和结构化表格等多种文档类型。研究表明，现代视觉语言模型如GPT-4和Gemini在字符错误率(CER)上相比传统OCR方法平均提高了60%。尽管如此，我们也指出了当前阿拉伯OCR模型的重大局限性，特别是在PDF到Markdown转换中，表现最好的模型Gemini-2.0-Flash准确率仅为65%。这些发现突显了在识别阿拉伯文本时的诸多挑战，如复杂字体、数字识别错误与表格结构检测等。本研究建立了一个严格的评估框架，可推动阿拉伯文档分析方法的改进，并缩小与英语OCR技术之间的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:43:47 GMT</pubDate>
</item>
<item>
<title>快速高质量蛋白质骨架生成的ReQFlow方法</title>
<link>https://arxiv.org/abs/2502.14637</link>
<guid>https://arxiv.org/abs/2502.14637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReQFlow方法，实现高效的蛋白质骨架生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的矩阵流匹配方法ReQFlow，用于快速且高质量的蛋白质骨架生成。该方法通过从随机噪声中产生局部平移和三维旋转，利用单位四元数表示每个残基的三维旋转，并通过球形线性插值（SLERP）构建其流。模型经过改进的四元数流（QFlow）匹配训练，保证数值稳定性，并加速推理，提升生成蛋白质骨架的设计性。实验结果表明，ReQFlow在蛋白质骨架生成上达到了领先性能，同时采样步骤显著减少，推理时间大幅降低，例如生成长度为300的骨架时，速度比RFDiffusion快37倍，比Genie2快62倍，展示了其有效性与效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:35:44 GMT</pubDate>
</item>
<item>
<title>创新的混合块注意力机制提升长上下文任务效率</title>
<link>https://arxiv.org/abs/2502.13189</link>
<guid>https://arxiv.org/abs/2502.13189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出混合块注意力（MoBA），优化长上下文任务中的计算效率。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能（AGI）领域的发展，有效扩大大型语言模型（LLMs）的上下文长度变得至关重要。然而，传统注意力机制在计算复杂度上的二次增长造成了巨大的负担。现有方法要么采用强偏置结构，如池化或窗口注意力，这些通常是任务特定的；要么对注意力机制进行根本性修改，使其线性近似，但在复杂推理任务中的表现仍然未得到充分探索。本文提出了一种遵循“少结构”原则的解决方案，即混合块注意力（MoBA），允许模型自主决定关注的内容，而不是引入先验偏置。MoBA通过借鉴专家混合（MoE）原则，展示了在长上下文任务中卓越的性能，并具备全注意力与稀疏注意力之间无缝切换的关键优势，提高了计算效率而不妥协表现。MoBA已被应用于支持Kimi的长上下文请求，并在大型语言模型的注意力计算效率上取得了显著进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:52:30 GMT</pubDate>
</item>
<item>
<title>JL1-CD数据集与多教师知识蒸馏框架在遥感影像变化检测中的应用</title>
<link>https://arxiv.org/abs/2502.13407</link>
<guid>https://arxiv.org/abs/2502.13407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出JL1-CD数据集和MTKD框架，提升遥感影像变化检测性能。</p><br /><br /><p><strong>摘要：</strong> 深度学习在遥感影像变化检测(CD)方面取得了显著成果，但仍面临两大挑战：缺乏亚米级的全面开放源CD数据集，以及在人们多变的变化区域中实现一致且满意的检测结果难度大。为此，本文介绍了JL1-CD数据集，包含5000对分辨率为0.5到0.75米的512 x 512像素图像。同时，提出了一种多教师知识蒸馏(MTKD)框架来解决CD问题。在JL1-CD和SYSU-CD数据集上的实验结果表明，MTKD框架显著提升了不同网络架构和参数规模的CD模型性能，达到了新的最先进结果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:29:42 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的上下文信息存储与量化</title>
<link>https://arxiv.org/abs/2502.15007</link>
<guid>https://arxiv.org/abs/2502.15007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示小词汇在上下文保持中扮演重要角色，影响模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了量化大型语言模型（LLMs）如何编码和存储上下文信息的方法，发现一些被视为次要的标记（如限定词和标点符号）实际上对上下文具有显著影响。特别地，去除这些标记，特别是停用词、冠词和逗号，都会显著降低在MMLU和BABILong-4k上的性能，即使移除的是无关的标记。此外，分析显示上下文化与线性之间存在强关联，其中线性度衡量从一层嵌入到下一层的变换如何可以用单一线性映射近似。这些发现突显了填充词在维持上下文方面的潜在重要性。为进一步探索，我们提出了LLM-Microscope，这是一个开源工具包，用于评估标记级的非线性、评估上下文记忆、可视化中间层贡献（通过调整的Logit Lens），并测量表示的内在维度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 02:07:41 GMT</pubDate>
</item>
<item>
<title>基于视频掩码重建的可泛化驾驶世界模型</title>
<link>https://arxiv.org/abs/2502.11663</link>
<guid>https://arxiv.org/abs/2502.11663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个增强泛化能力的驾驶世界模型，结合视频掩码重建技术。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过结合生成损失与MAE风格特征级上下文学习，来提升驾驶世界模型的泛化能力。我们提出了MaskGWM（驾驶世界模型）的新架构，包括两个变体：MaskGWM-long和MaskGWM-mview，分别专注于长时间预测和多视角生成。关键设计包括可扩展的Diffusion Transformer结构、处理模糊关系的扩散相关掩码令牌，以及通过行级掩码实现时空域扩展的掩码构建任务。我们在多个标准基准上进行了全面实验，包含Nuscene数据集的常规验证、OpenDV-2K数据集的长时间展望卷出和Waymo数据集的零-shot验证，结果显示我们的方法显著提升了现有驾驶世界模型的效能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:16:03 GMT</pubDate>
</item>
<item>
<title>CrossOver：灵活的跨模态三维场景理解框架</title>
<link>https://arxiv.org/abs/2502.15011</link>
<guid>https://arxiv.org/abs/2502.15011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CrossOver框架通过灵活的模态对齐实现三维场景理解，有效处理缺失模态问题。</p><br /><br /><p><strong>摘要：</strong> 当前的多模态三维物体理解面临的数据缺失和模态对齐限制促使了CrossOver框架的提出。该框架通过灵活的场景层级模态对齐，创建一个统一的模态无关嵌入空间，支持RGB图像、点云、CAD模型、平面图和文本描述的整合。CrossOver利用特定维度的编码器和多阶段训练管道，有效应对缺失模态问题，展示了其在场景检索和物体定位中的强大能力。通过在ScanNet和3RScan数据集上的评估，CrossOver在多项指标上表现优异，展现了在实际三维场景理解应用中的广泛适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:13:24 GMT</pubDate>
</item>
<item>
<title>VLM^2-Bench：评估视觉语言模型的匹配线索能力</title>
<link>https://arxiv.org/abs/2502.12084</link>
<guid>https://arxiv.org/abs/2502.12084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视语言模型在视觉匹配线索能力方面的表现与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLM^2-Bench，一个针对视觉语言模型（VLMs）在视觉匹配线索能力方面的评测基准，包含9个子任务和超过3000个测试案例。通过对八种开源VLM和GPT-4o的综合评估，并分析多种语言和视觉提示方法，结果显示模型在链接视觉线索方面存在重大性能差距，具体表现为GPT-4o的表现比人类低34.80%。研究强调了若干核心挑战，并提出改进建议，包括增强模型的核心视觉能力，明确语言推理与视觉任务的整合原则，以及转变视觉文本训练方式，以增强模型独立构建和推断视觉线索关系的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12084" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:36:34 GMT</pubDate>
</item>
<item>
<title>LightThinker：动态压缩增强大型语言模型推理效率</title>
<link>https://arxiv.org/abs/2502.15589</link>
<guid>https://arxiv.org/abs/2502.15589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LightThinker通过动态压缩推理过程的思维步骤，提高了大型语言模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了一种名为LightThinker的新方法，旨在提高大型语言模型（LLMs）在复杂推理任务中的效率。LightThinker通过动态压缩推理过程中产生的冗长思维步骤，将其转化为紧凑的表示，从而显著减少存储在上下文窗口中的标记数量。借鉴人类认知过程，该方法训练模型在何时以及如何执行压缩，通过构建数据、将隐藏状态映射到压缩概括标记以及创建专门的注意力掩码来实现。此外，本文引入了依赖性（Dep）指标，用以量化压缩程度，测量生成过程中对历史标记的依赖性。在四个数据集和两种模型上的广泛实验表明，LightThinker有效减少了峰值内存使用量和推理时间，同时保持了竞争力的准确性。本研究为改善大型语言模型在复杂推理任务中的效率提供了一种新的方向，且不会牺牲性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:07:05 GMT</pubDate>
</item>
<item>
<title>推动非代理性AI的发展以确保安全与创新</title>
<link>https://arxiv.org/abs/2502.15657</link>
<guid>https://arxiv.org/abs/2502.15657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出开发非代理性AI系统以降低AI风险，促进科学进步。</p><br /><br /><p><strong>摘要：</strong> 随着领先的人工智能公司越来越多地关注构建通用人工智能代理系统，虽其潜在价值显著，但失控的AI代理可能会给公共安全和安全性带来严重风险，如恶意行为的利用及人类控制权的永久丧失。本文讨论了这些风险如何源于当前的AI训练方法，并提出了研发一种称为科学家AI的非代理性AI系统，以遵循预防原则，确保其设计的可信性和安全性。科学家AI可以通过观察解释世界，而非执行/actions，以满足人类需求，其内核组成包括一个生成理论的世界模型与具备不确定性概念的问答推理机器。通过这些方法，科学家AI能够作为对抗潜在危险AI代理的保护措施，助力人类研究人员加速科学进步。文章希望引导研究人员和政策制定者选择更安全之路，推动人工智能创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:02:52 GMT</pubDate>
</item>
<item>
<title>StructFlowBench：一项多轮指令跟随评估基准</title>
<link>https://arxiv.org/abs/2502.14494</link>
<guid>https://arxiv.org/abs/2502.14494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StructFlowBench 提出了多轮指令跟随的结构流评估基准。</p><br /><br /><p><strong>摘要：</strong> 多轮指令跟随能力是大型语言模型（LLMs）在现实应用中的核心能力，现有评估基准主要集中在细粒度约束满足和领域特定能力评估，但忽视了多轮对话中的结构依赖性。本文提出了StructFlowBench，一个基于结构流建模的多轮指令跟随基准，定义了六种基本的轮间关系，以引入新的结构约束用于模型评估，同时作为生成自定义对话流的参数。通过对13个优秀开源及闭源LLMs的系统评估，实验结果显示当前模型在理解多轮对话结构方面存在显著不足。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:43:43 GMT</pubDate>
</item>
<item>
<title>UPCORE：平衡信息删除与模型保持的高效方法</title>
<link>https://arxiv.org/abs/2502.15082</link>
<guid>https://arxiv.org/abs/2502.15082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UPCORE是一种用于优化模型卸载过程的方法，兼顾删除效率与模型保持。</p><br /><br /><p><strong>摘要：</strong> 本文提出了UPCORE（Utility-Preserving Coreset Selection），一种机制无关的数据选择框架，旨在平衡从预训练模型中删除特定信息与保持其他性能之间的矛盾。在模型卸载过程中，数据点的删除往往会导致模型在其他数据上的性能下降。研究表明，模型损害与忘记集表示的方差相关，因此UPCORE通过选择性修剪忘记集，去除异常值，从而最小化模型的退化。经过对三种标准卸载方法的评估，UPCORE在删除效率与模型保持之间实现了优越的平衡。为更好地评估这一权衡，本文引入了一种新指标，用于测量标准指标的曲线下面积（AUC），发现UPCORE在提升标准指标和AUC方面具有积极效果，能够有效减少忘记集对外部点的负面影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:17:33 GMT</pubDate>
</item>
<item>
<title>PhotoDoodle：新型图像编辑框架促进艺术涂鸦</title>
<link>https://arxiv.org/abs/2502.14397</link>
<guid>https://arxiv.org/abs/2502.14397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhotoDoodle是一种方便艺术家在照片上进行涂鸦的图像编辑框架。</p><br /><br /><p><strong>摘要：</strong> PhotoDoodle是一种新颖的图像编辑框架，旨在帮助艺术家在照片上叠加装饰元素，实现场景与新元素的无缝融合。该方法克服了传统技术在合理的透视对齐、上下文一致性及背景保持方面的不足，采用两阶段训练策略：首先用大规模数据训练通用模型OmniEditor，随后利用小型艺术家策划的数据集通过EditLoRA进行细化训练，以捕捉特定的编辑风格。为增强生成结果的一致性，PhotoDoodle引入了位置编码重用机制。此外，我们发布了六种高质量风格的PhotoDoodle数据集，广泛实验表明该方法在定制图像编辑方面展现了先进的性能和可靠性，开辟了艺术创作的新可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:55:04 GMT</pubDate>
</item>
<item>
<title>一种基于f散度最小化的扩散模型快速生成方法</title>
<link>https://arxiv.org/abs/2502.15681</link>
<guid>https://arxiv.org/abs/2502.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的f散度最小化框架以加速扩散模型生成过程。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在生成样本时通常需要缓慢的迭代过程，这限制了其在实际应用中的部署。为了解决这一问题，本文提出了一种新的分布匹配方法，称为f-distill，利用f散度最小化框架，解决了当前变分分数蒸馏方法使用反向Kullback-Leibler散度时的模式追求问题。通过对教师和学生分布之间的f散度梯度进行推导，我们显示该梯度可表达为它们的分数差异与由密度比确定的加权函数的乘积。在不同的f散度选择下，f-distill能够更好地覆盖模式且降低训练方差。实验表明，使用例如前向KL散度和Jensen-Shannon散度等其他选择时，f-distill在图像生成任务上超越了现有最佳变分分数蒸馏方法。此外，特别是在Jensen-Shannon散度下，f-distill在ImageNet64上实现了最先进的一步生成性能，并在MS-COCO上实现了零-shot文本到图像生成的最佳效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:24:55 GMT</pubDate>
</item>
<item>
<title>使用SIFT技术提升大语言模型推理的准确性</title>
<link>https://arxiv.org/abs/2502.14922</link>
<guid>https://arxiv.org/abs/2502.14922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SIFT技术，提升大语言模型的推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型在推理过程中因语境误读可能导致的问题，并提出了一种名为**Stick to the Facts (SIFT)**的新后训练方法。该方法借助增强推理时计算量，旨在将模型推理与上下文紧密结合。SIFT的核心在于生成的*Sticker*，强调上下文中的关键信息。通过对比原始查询和增强查询生成的两个预测，SIFT可有效优化Sticker。研究显示，SIFT在多个模型（从3B到100B+）和基准测试（如GSM8K、MATH-500）上均展现出显著的性能提升，特别是在AIME2024上，使DeepSeek-R1的pass@1准确率从78.33%提升至85.67%，在开源社区创造了新的最高纪录。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:17:18 GMT</pubDate>
</item>
<item>
<title>利用深度强化学习提升大语言模型的模式遵循能力</title>
<link>https://arxiv.org/abs/2502.14905</link>
<guid>https://arxiv.org/abs/2502.14905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过深度强化学习方法，本研究提高了大语言模型的模式遵循能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对在大语言模型生成过程中强化严格模式遵循的挑战，利用LLM的推理能力提出了一种新方法。基于DeepSeek R1强化学习框架，采用合成推理数据集构建与定制奖励函数相结合的管道，训练一款1.5亿参数的模型的结构化推理能力。研究首先在一份2万样本的非结构化到结构化数据集上进行R1强化学习，其后在一份独立的1万样本推理数据集上进行监督微调，重点提升下游任务的模式遵循。尽管训练范围较小，我们的模型在约20小时的GRPO训练与3小时的SFT训练下，依然展示了在强化模式一致性方面的出色性能。通过与原DeepSeek R1、其蒸馏版本及Gemini 2.0 Flash的比较，结果显示该资源高效框架在模式约束文本生成中的实际应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:11:17 GMT</pubDate>
</item>
<item>
<title>Mol-LLaMA：跨学科的通用分子语言模型</title>
<link>https://arxiv.org/abs/2502.13449</link>
<guid>https://arxiv.org/abs/2502.13449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mol-LLaMA通过多模态调优提升了分子知识的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mol-LLaMA，一个旨在提升分子知识理解的通用分子语言模型。尽管大规模的分子语言模型在解释分子结构方面取得了一定的成功，但其训练数据集局限于特定任务，未能全面涵盖分子的基本特征，限制了其作为通用分子助手的能力。为了解决这一问题，我们采用多模态调优的方法，设计了包含分子基本特征的关键数据类型，以融合分子结构的必要知识。此外，我们还引入了一个模块，通过整合来自不同分子编码器的互补信息，进一步提升分子特征的理解能力。实验结果显示，Mol-LLaMA能够理解分子的通用特征，并准确生成相关的用户查询响应，具备成为分子分析通用助手的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:52:51 GMT</pubDate>
</item>
<item>
<title>评估大型多模态模型的交互智能新工具与方法</title>
<link>https://arxiv.org/abs/2502.15027</link>
<guid>https://arxiv.org/abs/2502.15027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InterFeedback框架评估大型多模态模型的交互智能。</p><br /><br /><p><strong>摘要：</strong> 现有基准未能测试大型多模态模型（LMM）与用户的交互智能，这对开发通用人工智能助手至关重要。为此，本文设计了InterFeedback，一个可应用于任何LMM和数据集的互动框架，以自主评估这一能力。我们还介绍了InterFeedback-Bench，利用MMM-Pro和MathVerse两个代表性数据集评估10种不同开源LMM的交互智能。此外，我们发布了InterFeedback-Human，收集了120个案例，以手动测试OpenAI-o1和Claude-3.5-Sonnet等领先模型的交互性能。评估结果表明，即便是最先进的LMM（如OpenAI-o1），通过人类反馈纠正其结果的比例不足50%。研究发现，急需改进方法，以增强LMM解读和利用反馈的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:44:33 GMT</pubDate>
</item>
<item>
<title>大型语言模型在数学推理中的效率与准确性分析</title>
<link>https://arxiv.org/abs/2502.15631</link>
<guid>https://arxiv.org/abs/2502.15631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究不同模型在数学推理中的链式思维与准确性的关系。</p><br /><br /><p><strong>摘要：</strong> 本文系统分析了o1-mini和o3-mini两种模型在Omni-MATH基准测试中的推理链长度与准确性之间的关系。结果表明，o3-mini(m)在不需要更长推理链的情况下实现了更高的准确性。此外，研究发现无论是在模型还是计算设置中，推理链的长度增加通常会导致准确性的下降，尤其是在控制问题难度后。然而，在更高效的模型中，这种准确性下降明显较小，暗示新一代模型在计算时的推理效率更高。同时，尽管o3-mini(h)在准确性上比o3-mini(m)有所提升，但其在所有问题上分配的推理令牌大幅增加，甚至是o3-mini(m)已能解决的问题。这些发现为模型能力与推理长度之间的关系提供了新见解，并对效率、扩展与评估方法学有重要启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.15631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:40:17 GMT</pubDate>
</item>
<item>
<title>SurveyX：高效自动化问卷生成系统</title>
<link>https://arxiv.org/abs/2502.14776</link>
<guid>https://arxiv.org/abs/2502.14776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyX系统通过创新技术提升自动化问卷生成的效果。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在理解能力和知识储备方面表现出色，成为自动化问卷生成的有效工具。但目前的研究存在一些局限性，如有限的上下文窗口、缺乏深入讨论和系统评估框架。为此，我们提出了SurveyX，一个高效有序的自动化问卷生成系统。该系统将问卷编写过程分为准备和生成两个阶段，创新性地引入在线参考检索、名为AttributeTree的预处理方法以及重新润色流程，从而显著提升问卷编写的效率。实验结果表明，SurveyX在内容质量和引用质量上均优于现有的自动化问卷生成系统，接近人类专家的表现，展现出其在多个评估维度上的显著优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:39:54 GMT</pubDate>
</item>
<item>
<title>MODis框架：基于多目标优化的数据集发现方法</title>
<link>https://arxiv.org/abs/2502.11262</link>
<guid>https://arxiv.org/abs/2502.11262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MODis框架，以优化多用户定义的模型性能指标发现数据集。</p><br /><br /><p><strong>摘要：</strong> 随着数据驱动分析的兴起，高质量数据集的准备已成为AI和机器学习模型的核心任务。传统的数据发现方法通常使用单一预定义的质量衡量标准集成数据集，可能导致下游任务的偏见。本文介绍了MODis框架，通过优化多个用户定义的模型性能衡量标准来发现数据集。MODis从一组数据源和模型出发，将数据源选择和集成到一个天际线数据集中，以确保模型在所有性能测量中达到期望表现。我们将MODis形式化为一个多目标有限状态转导器，并提出了三种可行的天际线数据集生成算法。第一种算法采用“从通用减少”策略，基于通用模式并逐步剪除不乐观的数据。第二种算法通过相互交织的数据增强和减少进一步降低成本。同时，我们引入了一种多样化算法以减轻天际线数据集中的偏见。实验验证了我们的天际线数据发现算法的效率和有效性，并展示了其在优化数据科学流程中的应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11262" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 21:19:35 GMT</pubDate>
</item>
<item>
<title>S-VCO：提升大规模视觉语言模型对细粒度图像细节的敏感性</title>
<link>https://arxiv.org/abs/2502.13928</link>
<guid>https://arxiv.org/abs/2502.13928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S-VCO通过细粒度图像细节训练，显著提升视觉语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 最新研究表明，大规模视觉语言模型（VLMs）在视觉基础任务中常常忽略图像内容，过度依赖语言模型先验，导致错误和幻觉。为了解决这一问题，本文提出了一种新的微调目标S-VCO（对称视觉对比优化），旨在增强VLM训练中的视觉反馈，帮助模型捕捉重要的视觉细节并与相应的文本标记对齐。我们还介绍了MVC，一个通过自动过滤和增强视觉反事实数据建立的配对图像-文本数据集，以挑战模型并实现更为精准的对比训练。实验结果显示，我们的方法在多个基准测试中一致提升VLM性能，尤其在视觉依赖性较高的测试中，幻觉减少了最高达22%，并且在视觉中心和一般任务中也取得显著进展。总之，S-VCO不仅显著提升了VLM在视觉任务中的表现，还保留或改善了模型的通用能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:42:50 GMT</pubDate>
</item>
<item>
<title>Generating $π$-Functional Molecules Using STGG+ with Active Learning</title>
<link>https://arxiv.org/abs/2502.14842</link>
<guid>https://arxiv.org/abs/2502.14842</guid>
<content:encoded><![CDATA[
Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explore new chemical spaces but often conducts 'reward-hacking' and generates non-synthesizable molecules. In this work, we address this problem by integrating a state-of-the-art supervised learning method, STGG+, in an active learning loop. Our approach iteratively generates, evaluates, and fine-tunes STGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We apply STGG+AL to the design of organic pi-functional materials, specifically two challenging tasks: 1) generating highly absorptive molecules characterized by high oscillator strength and 2) designing absorptive molecules with reasonable oscillator strength in the near-infrared (NIR) range. The generated molecules are validated and rationalized in-silico with time-dependent density functional theory. Our results demonstrate that our method is highly effective in generating novel molecules with high oscillator strength, contrary to existing methods such as reinforcement learning (RL) methods. We open-source our active-learning code along with our Conjugated-xTB dataset containing 2.9 million pi-conjugated molecules and the function for approximating the oscillator strength and absorption wavelength (based on sTDA-xTB).
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:05:36 GMT</pubDate>
</item>
<item>
<title>CHASE框架：无人工参与的挑战性问题生成</title>
<link>https://arxiv.org/abs/2502.14678</link>
<guid>https://arxiv.org/abs/2502.14678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了CHASE框架，以无人工参与的方式合成具有挑战性的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CHASE，一个新的框架，用于合成生成具有挑战性的问题，以应对大型语言模型（LLMs）评估的不断变化的需求。传统的人力标注方法在面对复杂、高质量问题时变得难以实施，因此我们开发了一个无人工干预的自下而上的生成方法，通过更简单的组件构建复杂问题。同时，CHASE框架将生成过程拆分为可独立验证的子任务，以确保高质量和正确性。我们在三个不同领域（文档问答、代码补全和数学推理）验证了CHASE的有效性，结果显示最先进的LLMs在这些合成基准上的准确率在40-60%之间，突显了该框架生成挑战性问题的能力。我们会公开发布这些基准和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models</title>
<link>https://arxiv.org/abs/2502.14191</link>
<guid>https://arxiv.org/abs/2502.14191</guid>
<content:encoded><![CDATA[
Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:34:53 GMT</pubDate>
</item>
<item>
<title>LServe：基于混合稀疏注意力的长序列大语言模型高效服务</title>
<link>https://arxiv.org/abs/2502.14866</link>
<guid>https://arxiv.org/abs/2502.14866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LServe通过混合稀疏注意力加速长序列大语言模型的服务效率。</p><br /><br /><p><strong>摘要：</strong> 大语言模型在处理长序列时表现出色，但由于前填充阶段的注意力计算复杂度及解码阶段的KV缓存内存开销，服务这些模型仍面临挑战。为了解决这些问题，我们提出了LServe，一个高效系统，通过混合稀疏注意力加速长序列大语言模型的服务。该方法将不同的硬件友好结构稀疏模式统一到一个框架内，省略对不重要标记的计算，显示出静态和动态稀疏在长上下文模型中兼容性。通过将一半的注意力头转换为几乎免费的流式头，LServe在前填充和解码阶段实现乘法加速。此外，我们发现保持长上下文能力只需固定数量的KV页面，并设计了一个层级KV页面选择策略，根据查询的相似性动态修剪KV页面。总体而言，LServe在保持长上下文准确度的同时，使LLM的前填充加速达到2.9倍，解码加速达到1.3-2.1倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 09:39:36 GMT</pubDate>
</item>
<item>
<title>提升大型多模态模型的视觉推理与可解释性的框架</title>
<link>https://arxiv.org/abs/2502.14044</link>
<guid>https://arxiv.org/abs/2502.14044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，增强大型多模态模型的视觉推理与解释能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视觉拒绝采样框架，旨在提高大型多模态模型（LMMs）在视觉任务中的认知能力和可解释性。当前LMMs在细粒度视觉推理中表现不佳，常常无法识别特定领域的目标，也无法对其预测提供合理的解释。我们的框架通过自合成数据来解决这些问题，具体方法包括合成可解释的答案，答案中包含可供人验证的视觉特征，这些特征基于专家定义的概念，经过精心选择以与图像内容对齐。在每次微调后，我们应用无奖励模型的过滤机制，筛选出最高质量的可解释答案用于下一轮的调优。实验结果显示，该方法在提高专业视觉分类任务的准确性和可解释性方面显著有效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:26:31 GMT</pubDate>
</item>
<item>
<title>NaviClues数据集与Navig框架推动影像地理定位进步</title>
<link>https://arxiv.org/abs/2502.14638</link>
<guid>https://arxiv.org/abs/2502.14638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出NaviClues数据集和Navig框架，提升影像地理定位精度。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦影像地理定位任务，提出了新的高质量数据集NaviClues，来源于流行的地理游戏GeoGuessr，旨在提供语言方面专家推理的示例。我们还提出了Navig，一个综合的影像地理定位框架，该框架整合了全球和细粒度的影像信息。通过语言推理，Navig相较于以往的最先进模型减少了14%的平均距离误差，同时训练样本不足1000。本文贡献不仅提升了影像地理定位的精确性，也为相关研究提供了数据集和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:18:34 GMT</pubDate>
</item>
<item>
<title>HippoRAG 2: 近似人类长期记忆的高效检索增强生成框架</title>
<link>https://arxiv.org/abs/2502.14802</link>
<guid>https://arxiv.org/abs/2502.14802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HippoRAG 2 提出了一种新框架，提升了长短期记忆任务的表现。</p><br /><br /><p><strong>摘要：</strong> HippoRAG 2 是针对持续学习中的大语言模型（LLMs）优化的一种新框架，旨在增强其知识获取与组织能力。传统的检索增强生成（RAG）方法由于依赖于向量检索，未能有效模拟人类动态的长期记忆。为解决这一问题，HippoRAG 2 引入了个性化的 PageRank 算法并改进了段落集成和 LLM 的在线使用。实验结果显示，HippoRAG 2 在事实、认知和关联记忆任务上的表现均优于传统的 RAG 方法，尤其在关联记忆任务上实现了 7% 的提升。此外，该框架拓展了非参数持续学习的可能性，推动了大语言模型的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:00:41 GMT</pubDate>
</item>
<item>
<title>CLIPPER：用于叙述主张验证的合成数据生成方法</title>
<link>https://arxiv.org/abs/2502.14854</link>
<guid>https://arxiv.org/abs/2502.14854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLIPPER通过压缩方法生成更高质量的叙述主张验证合成数据。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型开发者越来越依赖合成数据，生成高质量的数据以应对复杂的长上下文推理任务仍然具有挑战性。本文介绍了CLIPPER，这是一种基于压缩的方法，专门用于生成适合叙述主张验证的合成数据。CLIPPER首先将书籍压缩为章节大纲和书籍摘要，然后利用这些中间表示生成复杂的主张及其推理链。与直接从原文生成主张相比，CLIPPER能够生成更有效、扎实和复杂的主张。我们利用CLIPPER构建了19K条合成书籍主张的数据集，并将其与源文本和推理链配对，进一步调整了三个开放权重模型，使得最佳模型在叙述主张验证任务上取得了突破性的结果，将准确率从28%提升至76%。此外，我们的分析表明，模型生成的推理链更加详尽和扎实，同时在其他叙述理解任务上（如NarrativeQA）性能也有所提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:52:55 GMT</pubDate>
</item>
<item>
<title>LLM-based User Profile Management for Recommender System</title>
<link>https://arxiv.org/abs/2502.14541</link>
<guid>https://arxiv.org/abs/2502.14541</guid>
<content:encoded><![CDATA[
The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title>
<link>https://arxiv.org/abs/2502.14502</link>
<guid>https://arxiv.org/abs/2502.14502</guid>
<content:encoded><![CDATA[
The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:29:18 GMT</pubDate>
</item>
<item>
<title>How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild</title>
<link>https://arxiv.org/abs/2502.12769</link>
<guid>https://arxiv.org/abs/2502.12769</guid>
<content:encoded><![CDATA[
In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:28:42 GMT</pubDate>
</item>
<item>
<title>S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12853</link>
<guid>https://arxiv.org/abs/2502.12853</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S^2R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\% to 81.6\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S^2R. Our code and data are available at https://github.com/NineAbyss/S2R.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:00:18 GMT</pubDate>
</item>
<item>
<title>改进长文本摘要的无结构证据引用方法</title>
<link>https://arxiv.org/abs/2502.14409</link>
<guid>https://arxiv.org/abs/2502.14409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出改进长文本摘要的方法，通过无结构证据引用提升透明度与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前大语言模型（LLMs）在长文本上下文中生成摘要时的局限性，尤其是在证据引用方面。现有研究主要集中在使用预定义的粒度（如句子、段落等）进行证据引用，而我们提出了一种新的任务——侧重于用户查询的长文本摘要，结合无结构证据引用。通过创建一个名为SUnsET的合成数据集，我们展示了不同规模的LLMs在经过该数据适应后，能更有效地引用长文本中的证据，提取更广泛的信息位置，从而生成更具相关性和事实一致性的摘要。实验结果表明，适应后模型在多个测试集上表现优于基础模型，显示出显著的提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:40 GMT</pubDate>
</item>
<item>
<title>提升图像地理定位的综合框架与新方法</title>
<link>https://arxiv.org/abs/2502.13759</link>
<guid>https://arxiv.org/abs/2502.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架，通过大规模数据集和推理方法提升图像地理定位精度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种综合的地理定位框架，旨在解决当前地理定位方法中存在的精度低、解释性差的问题。框架由三个核心组件组成：GeoComp（大规模数据集）、GeoCoT（新型推理方法）和GeoEval（评估指标）。GeoComp是一个由740K用户在两年内通过地理定位游戏平台收集的大规模数据集，包含2500万条元数据和300万个地理标签位置，提供各种难度的样本以供详细分析。基于此数据集，GeoCoT是一个多步骤推理框架，旨在提升大规模视觉模型在地理定位任务中的推理能力。通过整合上下文和空间线索，GeoCoT的推理方式更接近人类思维，最终表明GeoEval指标显示其精度提高了多达25%，并提升了模型的可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:28 GMT</pubDate>
</item>
<item>
<title>基于强化学习的量子码权重优化方法研究</title>
<link>https://arxiv.org/abs/2502.14372</link>
<guid>https://arxiv.org/abs/2502.14372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了基于强化学习的量子码权重优化方法，显著提高了编码效率。</p><br /><br /><p><strong>摘要：</strong> 随着可扩展容错量子计算的实现日益依赖量子错误纠正码，测量权重的优化变得至关重要。本文介绍了一种基于强化学习的有效方法，旨在降低稳定器码的测量权重，结果显示这些低权重代码在实际相关参数领域的表现显著优于当前技术的最优解，尤其是在小距离编码中实现了更大幅度的提升。例如，对于权重为6的码，我们的方法比现有结果节省了1到2个数量级的物理量子位开销，使得开销进入未来实验可行范围。此外，我们还利用强化学习框架研究了码参数之间的相互作用，为实践中可行的编码策略提供了新见解。本研究结果展示了强化学习在解决量子码发现中的潜力，对于推进容错量子技术的实际应用具有重要意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14372" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 01:11:34 GMT</pubDate>
</item>
<item>
<title>语言模型的时间知识处理及其局部特征分析</title>
<link>https://arxiv.org/abs/2502.14258</link>
<guid>https://arxiv.org/abs/2502.14258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型中的特定注意力头负责处理时间相关知识。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了语言模型如何处理时变事实，特别是识别出特定的注意力头（Temporal Heads），它们在处理时间知识方面起重要作用。通过电路分析，我们确认这些头在多个模型中均存在，其具体位置可能不同，且对不同类型知识和年份的响应有所差异。禁用这些头会降低模型对时间特定知识的回忆能力，但不会影响模型在时间不变和问答任务上的表现。此外，这些头不仅响应数字条件（如“在2004年”），还对文本别名（如“在那一年...”）做出反应，表明它们编码了超越简单数字表示的时间维度。我们还展示了如何通过调整这些头的值来编辑时间知识，拓展了研究的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 23:02:42 GMT</pubDate>
</item>
<item>
<title>Set-and-Sequence：动态概念个性化的视频生成框架</title>
<link>https://arxiv.org/abs/2502.14844</link>
<guid>https://arxiv.org/abs/2502.14844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Set-and-Sequence框架，旨在个性化生成视频模型中的动态概念。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的框架Set-and-Sequence，旨在个性化基于Diffusion Transformers的生成视频模型，以捕捉动态概念。与静态概念不同，动态概念不仅由外观定义，还包含运动信息。该框架通过在一个不明确分离空间和时间特征的架构内施加时空权重空间，实现个性化生成。具体而言，框架分为两个关键阶段：首先，通过无序帧集微调低秩适应（LoRA）层，以学习表示外观的身份LoRA基础；其次，冻结身份LoRA后，对其系数进行运动残差增强，进一步微调完整视频序列以捕捉运动动态。最终，Set-and-Sequence框架有效嵌入动态概念，实现前所未有的可编辑性和组合性，成为个性化动态概念的新基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:41:47 GMT</pubDate>
</item>
<item>
<title>PC-Agent: 复杂交互环境下的分层代理框架</title>
<link>https://arxiv.org/abs/2502.14282</link>
<guid>https://arxiv.org/abs/2502.14282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PC-Agent框架旨在改善PC场景下的复杂用户指令处理能力。</p><br /><br /><p><strong>摘要：</strong> PC-Agent是一个专为PC场景设计的分层代理框架，旨在克服当前大规模语言模型(MLLM)在复杂交互环境中的局限性。文章首先介绍了主动感知模块（APM），以提升对截图内容的感知能力。其次，提出了一种层次化多代理协作架构，将决策过程细分为指令、子任务和行动三个层面，设立了管理、进度和决策三个代理以优化指令分解、进度跟踪和逐步决策。此外，反思代理的引入能够实现及时的自下而上的错误反馈和调整。通过新基准PC-Eval进行实证测试，PC-Agent在任务成功率上较之前的最先进方法提高了32%。代码将会公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:48 GMT</pubDate>
</item>
<item>
<title>LongWriter-V-22k: 提升长文本生成能力的视觉语言模型</title>
<link>https://arxiv.org/abs/2502.14834</link>
<guid>https://arxiv.org/abs/2502.14834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongWriter-V-22k解决了LVLM生成超千字文本的能力不足问题。</p><br /><br /><p><strong>摘要：</strong> 现有的大型视觉语言模型（LVLM）能处理最长达128k的文本和视觉令牌输入，但在生成超千字的连贯输出时却存在困难。研究发现，主要限制因素是缺乏长输出的监督微调（SFT）示例。为此，我们引入了LongWriter-V-22k数据集，包含22,158个示例，每个示例都有多张输入图像、指令和对应的输出（范围从0到10,000字）。此外，为确保生成的长输出高保真于输入图像，我们对SFT模型采用了直接偏好优化（DPO）。由于收集人类反馈成本高，我们提出了IterDPO方法，通过将长输出进行分段处理并进行迭代修正生成偏好对。我们还开发了MMLongBench-Write基准，评估VLM的长生成能力。在使用LongWriter-V-22k和IterDPO训练的7B参数模型上，我们在该基准上表现出色，优于更大的专有模型如GPT-4o。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:21 GMT</pubDate>
</item>
<item>
<title>CoSyn框架：自动生成文本丰富的多模态数据</title>
<link>https://arxiv.org/abs/2502.14846</link>
<guid>https://arxiv.org/abs/2502.14846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSyn框架利用LLM自动生成文本丰富的图像数据，提升VLM性能。</p><br /><br /><p><strong>摘要：</strong> CoSyn是一个框架，旨在解决视觉语言模型(VLMs)在处理文本丰富图像（如图表和文档）时面临的数据稀缺问题。通过输入描述目标领域的文本，例如“营养成分标签”，CoSyn可以促使大语言模型(LLM)生成用于渲染合成图像的代码（如Python、HTML和LaTeX）。这些代码作为合成图像的文本表示，进而产生高质量的指令调优数据。基于CoSyn构建的数据集包含40万张图像和270万行的视觉语言指令调优数据。经过在七个基准上的全面实验，使用我们合成数据训练的模型在性能上超过了包括Llama 3.2在内的竞争性开源模型，并且优于GPT-4V和Gemini 1.5 Flash等专有模型。此外，CoSyn还能生成合成指向数据，使VLM能够在输入图像中定位信息，展示其在开发能够在真实环境中行动的多模态代理的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:38:36 GMT</pubDate>
</item>
<item>
<title>SigLIP 2：新一代多语言视觉-语言编码器</title>
<link>https://arxiv.org/abs/2502.14786</link>
<guid>https://arxiv.org/abs/2502.14786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SigLIP 2在多语言视觉-语言任务中的性能显著提升。</p><br /><br /><p><strong>摘要：</strong> SigLIP 2是基于原始SigLIP成功的一系列新多语言视觉-语言编码器。通过引入不同的训练目标和技术，包括基于标注的预训练、自监督损失和在线数据管理，SigLIP 2在零样本分类、图像-文本检索及视觉表示提取能力上超越了前作。此外，新模型在定位和密集预测任务上也有显著改进，且支持多种分辨率与输入原始长宽比的保留。通过使用更加多样化的数据混合和去偏见技术，SigLIP 2在多语言理解和公平性方面表现出色。最终，我们还发布了四个不同大小的模型检查点，用户可以根据推理成本与性能需求进行选择。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:33:22 GMT</pubDate>
</item>
<item>
<title>RelaCtrl：基于相关性指导的可控生成框架</title>
<link>https://arxiv.org/abs/2502.14377</link>
<guid>https://arxiv.org/abs/2502.14377</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RelaCtrl框架通过优化控制信号集成提升了Diffusion Transformer的效率。</p><br /><br /><p><strong>摘要：</strong> RelaCtrl是一个旨在提高Diffusion Transformer在文本到图像和文本到视频生成中的效率和资源利用率的可控生成框架。我们通过评估每个Transformer层与控制信息的相关性，提出了“ControlNet相关性评分”来量化控制层的重要性，从而优化控制层的布局、参数规模和建模能力，减少不必要的参数和计算量。此外，RelaCtrl还通过引入双维混合器（TDSM）替代传统的自注意力和前馈神经网络模块，实现了令牌混合器和通道混合器的高效实现。实验结果显示，与PixArt-delta相比，RelaCtrl在参数和计算复杂度方面仅占15%，且性能显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14377" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:30:51 GMT</pubDate>
</item>
<item>
<title>基于规则的强化学习在大型推理模型中的应用研究</title>
<link>https://arxiv.org/abs/2502.14768</link>
<guid>https://arxiv.org/abs/2502.14768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究基于规则的强化学习在推理模型中的应用，取得显著成效。</p><br /><br /><p><strong>摘要：</strong> 本研究受DeepSeek-R1的启发，探讨规则基础的强化学习在大型推理模型中的潜力。我们使用合成逻辑难题作为训练数据，以便分析推理动态，其复杂性可控且答案验证简单。我们提出了关键的技术贡献，以实现有效且稳定的强化学习训练，包括强调思考与回答过程的系统提示、对走捷径的输出进行惩罚的严格格式奖励函数，以及实现稳定收敛的简单训练食谱。经过训练，我们的7B模型发展出反思、验证和总结等高级推理技能，虽然这些能力在逻辑语料库中并不存在。令人瞩目的是，在仅训练5K个逻辑问题后，该模型在挑战性数学基准AIME和AMC上展示出了良好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14768" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:19:05 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在多学科领域的知识和推理能力</title>
<link>https://arxiv.org/abs/2502.14739</link>
<guid>https://arxiv.org/abs/2502.14739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SuperGPQA基准以评估大语言模型在285个学科的表现。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）在数学、物理和计算机科学等学术领域表现出色，但在其他200多个专业学科上的能力评估尚显不足。为填补这一空白，本文提出SuperGPQA，一个全面的评估基准，旨在测试研究生水平的知识和推理能力，涵盖285个学科。该基准采用新的人类-LLM协作过滤机制，通过基于LLM响应和专家反馈的迭代优化，消除琐碎或模糊的问题。实验结果显示，当前最先进的LLMs在各知识领域的表现仍有显著提升空间，最高准确率仅为61.82%。此外，本文还分享了在大型注释过程中管理80多位专家评审者的经验，为未来类似研究提供了方法论指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:15:33 GMT</pubDate>
</item>
<item>
<title>增强语言模型的视觉空间推理能力的两阶段训练框架</title>
<link>https://arxiv.org/abs/2502.14669</link>
<guid>https://arxiv.org/abs/2502.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升语言模型的视觉空间推理能力，成功用于迷宫导航。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的两阶段训练框架，旨在增强标准大型语言模型（LLMs）在迷宫导航中的视觉推理能力。第一阶段采用监督微调（SFT），利用经过处理的迷宫表示数据集教会模型逐步预测移动命令。第二阶段利用深度强化学习中的群体相对策略优化（GRPO）技术，通过精心设计的奖励函数改善模型的决策制定能力。实验结果显示，基线模型无法完成迷宫导航，而经过SFT训练的模型准确率达到86%，进一步的GRPO微调后，准确率提升至93%。定性分析表明，GRPO促进了模型更强的自我修正推理能力，突显了我们的方案在将语言模型与视觉空间任务相结合方面的潜力，这些发现对机器人、自动导航等领域具有重要的应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:11:45 GMT</pubDate>
</item>
<item>
<title>Meta MLGym：评估与开发 LLM 代理的新框架与基准</title>
<link>https://arxiv.org/abs/2502.14499</link>
<guid>https://arxiv.org/abs/2502.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了 Meta MLGym 和 MLGym-Bench，旨在评估 LLM 代理在 AI 研究任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Meta MLGym 和 MLGym-Bench，这是一个用于评估和开发大语言模型（LLM）代理在 AI 研究任务上的新框架和基准。MLGym-Bench 包含来自计算机视觉、自然语言处理、强化学习和博弈论等多个领域的13个多样化且开放式的 AI 研究任务。解决这些任务需要真实的 AI 研究技能，包括生成新想法、数据处理、实现机器学习方法、训练模型及实验分析等。通过对多个前沿的 LLM 进行评估，尽管它们通常能通过找到更好的超参数来改善已有基线，但并未能生成新的假设或显著改进。为促进未来 LLM 代理 AI 研究能力的进步，本文将框架和基准开源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.14499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:08:38 GMT</pubDate>
</item>
<item>
<title>S*: Test Time Scaling for Code Generation</title>
<link>https://arxiv.org/abs/2502.14382</link>
<guid>https://arxiv.org/abs/2502.14382</guid>
<content:encoded><![CDATA[
Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:04:42 GMT</pubDate>
</item>
<item>
<title>基于真实对话的长效聊天机器人情感智能研究</title>
<link>https://arxiv.org/abs/2502.13270</link>
<guid>https://arxiv.org/abs/2502.13270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究真实对话数据以增强聊天机器人情感智能和长效记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了REALTALK，一个为期21天的真实聊天记录数据集，旨在填补现有研究对人机对话理解的空白。通过分析数据集的情感智能属性和个性一致性，探讨真实对话所面临的挑战。与生成的对话数据相比，真实对话展示了更丰富的情感表达和个性稳定性。基于这些研究成果，提出了两个基准任务：个性模拟和记忆探测。研究发现，目前的模型在仅依赖对话历史进行用户模拟时表现不佳，而对特定用户聊天进行微调可以有效提升个性仿真能力。此外，现有模型在真实对话中回忆和利用长期上下文方面依然面临重大挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 16:00:25 GMT</pubDate>
</item>
<item>
<title>记忆代码：探索大型语言模型在长期互动中的局限性</title>
<link>https://arxiv.org/abs/2502.13791</link>
<guid>https://arxiv.org/abs/2502.13791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型在长期交互中面临信息整合的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MemoryCode，一个合成的多会话数据集，旨在评估大型语言模型（LLMs）在面对长时间交互时，跟踪和执行简单编码指令的能力。尽管所有测试的模型在处理孤立指令时表现良好，甚至包括最先进的模型如GPT-4o，但在指令分散于多个会话时其性能显著下降。我们的分析表明，这主要是由于当前模型无法有效检索和整合长指令链中的信息。这一发现揭示了目前LLMs在长期互动中合作的根本性限制，影响了其在实际工作环境中的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 14:34:52 GMT</pubDate>
</item>
<item>
<title>大语言模型在相关性评估中的应用与挑战</title>
<link>https://arxiv.org/abs/2502.13908</link>
<guid>https://arxiv.org/abs/2502.13908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大语言模型在信息检索相关性评估中的应用与研究进展。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了大语言模型（LLMs）在相关性评估中的潜力，特别是在信息检索（IR）和自然语言处理（NLP）领域的应用。研究表明，LLMs能够显著减少人工评估所需的时间和精力，尤其是在面对新主题和低资源情况时。作者回顾了在SIGIR 2024上进行的大规模自动相关性评估基准测试——LLMJudge挑战，介绍了42个由国际团队生成的相关性标签。通过对这些自动生成标签的分析，可以研究LLMs引发的系统性偏见、集成模型的有效性、以及不同模型与人工评估者之间的权衡。这项工作为改进自动化评估技术的方法提供了重要的资源和基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 13:47:47 GMT</pubDate>
</item>
<item>
<title>推出大规模多语言文本嵌入基准（MMTEB）</title>
<link>https://arxiv.org/abs/2502.13595</link>
<guid>https://arxiv.org/abs/2502.13595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMTEB基准，以丰富文本嵌入模型的多语言评估任务。</p><br /><br /><p><strong>摘要：</strong> 本文引入了大规模多语言文本嵌入基准（MMTEB），旨在克服现有文本嵌入评估在语言、领域和任务多样性方面的局限性。该基准涵盖了500多项经过质量控制的评估任务，分布在250多种语言上，包含了丰富而新颖的任务，如指令执行、长文档检索和代码检索。我们发现，尽管具有数十亿参数的大型语言模型在特定语言和任务上表现优异，但最佳公开模型multilingual-e5-large-instruct的参数仅为5.6亿。为降低计算成本，我们采用了一种基于任务间相关性的下采样方法，确保选取的任务多样且保持模型排名。此外，我们通过采样困难负例优化检索任务，创建了较小但有效的分割，从而使基准显著降低计算需求。这些改进使得我们的零-shot英语基准在计算成本较小的情况下，仍能维持与完整版本相似的排名顺序。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:26:53 GMT</pubDate>
</item>
<item>
<title>AI驱动探索：优化机器学习工程的创新方法</title>
<link>https://arxiv.org/abs/2502.13138</link>
<guid>https://arxiv.org/abs/2502.13138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIDE利用大型语言模型优化机器学习工程，提升研发效率。</p><br /><br /><p><strong>摘要：</strong> 机器学习是现代人工智能的基础，驱动着世界的创新，但其背后却是一个复杂且耗时的过程，工程师与科学家们在试错任务上耗费大量时间。为了解决这一挑战，本文介绍了AI驱动探索（AIDE），这是一个由大型语言模型支持的机器学习工程代理。AIDE将机器学习工程视为代码优化问题，利用树搜索方法框架进行试错，战略性地重用和完善有前景的解决方案，从而有效地用计算资源换取更优的性能。AIDE在多个机器学习工程基准测试中取得了领先的结果，包括Kaggle评估、OpenAI的MLE-Bench以及METRs的RE-Bench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:23:27 GMT</pubDate>
</item>
<item>
<title>MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching</title>
<link>https://arxiv.org/abs/2502.12852</link>
<guid>https://arxiv.org/abs/2502.12852</guid>
<content:encoded><![CDATA[
Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:09:53 GMT</pubDate>
</item>
<item>
<title>PGMR框架：提升自然语言生成SPARQL查询的准确性</title>
<link>https://arxiv.org/abs/2502.13369</link>
<guid>https://arxiv.org/abs/2502.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PGMR框架以提高基于LLM的SPARQL查询生成准确性，减少URI幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PGMR（后生成记忆检索）框架，该框架通过引入非参数化记忆模块，提升了大型语言模型（LLMs）生成SPARQL查询的准确性及效率。尽管LLMs在知识图谱中的SPARQL查询生成应用广泛，但通常会出现URI幻觉和超出分布的错误，导致生成的内容虽看似合理但却事实错误，从而影响在实际信息检索中的应用。PGMR框架通过有效检索知识图谱元素，显著降低了URI幻觉现象。在不同数据集和LLMs的实验中，PGMR展现出一致的强劲性能，几乎在多个场景下消除了URI幻觉问题，证明其在知识图谱信息检索应用中的潜力和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:07:02 GMT</pubDate>
</item>
<item>
<title>SplatDiff：基于像素喷溅指导的视频扩散模型</title>
<link>https://arxiv.org/abs/2502.12752</link>
<guid>https://arxiv.org/abs/2502.12752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SplatDiff通过像素喷溅引导来合成高保真新视图，有效解决纹理幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出的SplatDiff是一种基于像素喷溅的引导视频扩散模型，旨在从单张图像生成高保真的新视图。该模型使用对齐合成策略精确控制目标视角和几何一致的视图合成。为解决纹理幻觉问题，设计了一种纹理桥接模块，能够通过自适应特征融合实现高保真纹理生成。实验结果表明，SplatDiff在单视图新视图合成任务中表现出色，超越了现有方法。此外，在无需额外训练的情况下，SplatDiff在稀疏视图新视图合成和立体视频转换等多样任务中表现出显著的零-shot性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:53:49 GMT</pubDate>
</item>
<item>
<title>TESS 2：提升指令跟随能力的扩散语言模型</title>
<link>https://arxiv.org/abs/2502.13917</link>
<guid>https://arxiv.org/abs/2502.13917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TESS 2 是一种优于现有模型的指令跟随扩散语言模型。</p><br /><br /><p><strong>摘要：</strong> TESS 2 是一款通用的指令跟随扩散语言模型，它在性能上超越了当前的指令调优扩散模型，并且在某些情况下与强大的自回归模型相抗衡甚至超过它们。TESS 2 的训练首先是对一个强大的自回归模型进行继续预训练，使用交叉熵作为扩散损失，随后进行进一步的指令调优。研究表明，适应训练和基础模型的选择对于训练优秀的指令跟随扩散模型至关重要。此外，我们提出了一种名为奖励引导的新颖推理时间指导程序，以便在不需要训练基础模型的情况下对模型输出进行对齐。最后，结果显示，TESS 2 在增加推理时间计算资源后能进一步改善，这突显了扩散语言模型在推理阶段对计算资源使用的精细控制能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13917" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:46:55 GMT</pubDate>
</item>
<item>
<title>REFIND框架：改进大语言模型输出中的幻觉检测</title>
<link>https://arxiv.org/abs/2502.13622</link>
<guid>https://arxiv.org/abs/2502.13622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REFIND框架通过提取文档检测大语言模型的幻觉现象，提高可靠性。</p><br /><br /><p><strong>摘要：</strong> 针对大语言模型（LLM）输出中的幻觉现象，REFIND（检索增强事实性幻觉检测）框架应运而生。该框架通过直接利用检索到的文档，检测LLM输出中的幻觉段落。REFIND引入了上下文敏感度比（CSR）这一新指标，量化LLM输出对检索证据的敏感度，使得幻觉检测更为高效准确。在评估过程中，REFIND在九种语言中展现出良好的鲁棒性，尤其是在低资源环境下，相比于基线模型，其在识别幻觉段落时显著提高了IoU分数。本研究强调了量化上下文敏感度在幻觉检测中的有效性，为不同语言环境下更可靠的LLM应用铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 07:25:12 GMT</pubDate>
</item>
<item>
<title>LoRAM：高效的低秩适应训练方案</title>
<link>https://arxiv.org/abs/2502.13533</link>
<guid>https://arxiv.org/abs/2502.13533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAM通过小模型训练提高LLM的内存效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的内存高效的低秩适应训练方案LoRAM，旨在解决传统Low-Rank Adaption (LoRA)在内存占用方面的限制。LoRAM的核心思想是针对过度参数化的大型语言模型，许多神经元的训练效用较低但在推理中必不可少。该方案通过训练一个经过剪枝的小模型，以获得剪枝的低秩矩阵，然后将其恢复并与原始大型模型结合用于推理。此外，为了调和剪枝模型和原模型之间的知识差异，模型发布者在训练前进行最小成本的持续预训练。通过大量实验，LoRAM在多种剪枝策略和下游任务中表现出色，特别适用于具有700亿参数的模型，使得在只有20G HBM的GPU上进行训练成为可能，替代了用于LoRA训练的A100-80G GPU以及全微调所需的15个GPU。实现的QLoRAM通过结构化剪枝结合4位量化，显著减少了低秩矩阵训练中占主导地位的参数存储成本，且实现了优于原始和LoRA训练模型的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 06:45:40 GMT</pubDate>
</item>
<item>
<title>半监督异构领域适应中的可转移知识探讨</title>
<link>https://arxiv.org/abs/2502.13573</link>
<guid>https://arxiv.org/abs/2502.13573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明在半监督异构领域适应中，源样本的类别和特征信息对目标领域性能影响不大。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了半监督异构领域适应（SHDA）中可转移知识的性质，通过对约330个SHDA任务的广泛实验，使用了两种监督学习方法和七种代表性的SHDA方法。结果表明，源样本的类别和特征信息对目标领域的性能影响不显著。此外，简单分布的噪声作为源样本时，也可能蕴含可转移知识。基于这一发现，我们设计了统一的知识转移框架（KTF）用于SHDA，并发现可转移知识的主要来源是源域的可转移性和可区分性。因此，确保源样本具备这些属性，无论其来源（如图像、文本或噪声），都能提高SHDA任务中的知识转移效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:38:39 GMT</pubDate>
</item>
<item>
<title>全球文化知识评估：GIMMICK多模态基准研究</title>
<link>https://arxiv.org/abs/2502.13766</link>
<guid>https://arxiv.org/abs/2502.13766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GIMMICK是一个评估全球文化知识的多模态基准，涵盖144个国家的文化特色。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GIMMICK，一个旨在评估大型视觉语言模型（LVLM）在不同文化知识方面表现的多模态基准。GIMMICK包含六项任务及三个新数据集，覆盖144个国家的728个独特的文化事件。我们对20个LVLM和11个大型语言模型进行了系统评估。分析发现，模型表现出强烈的西方文化偏见，同时模型大小与性能之间存在显著关联。研究还显示，多模态输入和外部地理线索显著提升了模型的表现。此外，模型在识别具体文化特征（如食物）方面优于理解抽象概念（如仪式），反映出它们更擅长识别广泛文化起源而在细微理解上存在挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:19:11 GMT</pubDate>
</item>
<item>
<title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title>
<link>https://arxiv.org/abs/2502.11573</link>
<guid>https://arxiv.org/abs/2502.11573</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 04:32:22 GMT</pubDate>
</item>
<item>
<title>ActionPiece: 通过上下文增强的动作序列标记方法</title>
<link>https://arxiv.org/abs/2502.13581</link>
<guid>https://arxiv.org/abs/2502.13581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ActionPiece，通过上下文增强提高动作序列的推荐性能。</p><br /><br /><p><strong>摘要：</strong> 生成推荐（GR）是一种新兴的推荐范式，旨在将用户动作标记为离散的模式并进行自回归预测。现有的GR模型独立标记每个动作，未考虑上下文关系，导致相同动作用于不同上下文时表现不佳。为了解决这一问题，本文提出了ActionPiece，将动作与特征集相结合进行标记，构建上下文感知的动作序列。ActionPiece通过合并特征模式生成新的标记，并引入集合排列正则化，以处理特征集的无序性。实验结果表明，ActionPiece在多个公共数据集上显著优于现有的方法，使NDCG@10指标提高了6.00%至12.82%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 03:56:54 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Memories: 提升线性序列建模的新架构</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mixture-of-Memories架构，提升线性序列建模的记忆能力。</p><br /><br /><p><strong>摘要：</strong> 线性序列建模方法如线性注意力、状态空间建模和线性RNN在训练和推理中显著提升了效率，但通常将整个输入序列压缩为单一固定大小的记忆状态，导致在需回忆能力强的下游任务上的性能不佳。本文提出了一种新架构Mixture-of-Memories（MoM），灵感源自神经科学，采用多个独立记忆状态，由路由网络将输入令牌分配给特定记忆状态。这一方案极大增强了记忆容量，减少了记忆干扰，从而使MoM在记忆需求高的任务上表现优异，超越了现有的线性序列建模技术。尽管集成了多个记忆状态，MoM在训练中保持线性复杂度，在推理中保持恒定复杂度，使其保持了计算优势。实验结果表明，MoM在下游语言任务中显著优于当前线性序列模型，在需回忆的任务中甚至表现出与Transformer模型相当的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13685" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 02:40:09 GMT</pubDate>
</item>
<item>
<title>名字与身份：大型语言模型的偏见研究</title>
<link>https://arxiv.org/abs/2502.11995</link>
<guid>https://arxiv.org/abs/2502.11995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨名字对人类身份的影响及其在大型语言模型中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨名字与人类身份之间的深刻联系，分析名字在文化遗产、个体历史和个人化中的作用。尽管名字可以作为身份的标记，但简单地依赖于名字可能会导致对复杂身份的过于简化。在与大型语言模型（LLMs）交互时，用户名字是个人化的重要信息，可能通过直接输入、任务上下文以及存储的用户信息呈现。我们的研究通过测量文化假设，观察在常见建议查询中LLMs生成的反应，发现LLMs对名字 предполагают 强烈的文化身份假设。该研究为设计更加细致的个人化系统提供了重要启示，以避免增强刻板印象，同时保持有意义的定制。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:20:46 GMT</pubDate>
</item>
<item>
<title>SongGen：基于文本的可控歌曲生成模型</title>
<link>https://arxiv.org/abs/2502.13128</link>
<guid>https://arxiv.org/abs/2502.13128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SongGen是一种新型的文本到歌曲生成模型，支持细粒度控制音乐属性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SongGen的开放源代码单阶段自回归变换器，用于可控歌曲生成。该模型实现了对歌词、乐器、流派、情绪和音色等各种音乐属性的细粒度控制，并提供可选的三秒参考音频片段以进行声纹克隆。SongGen在统一的自回归框架中支持混合模式和双轨模式两种输出方式，使得生成的声音合唱和伴奏能够直接混合或分别合成，提供了更大的下游应用灵活性。作者还探索了多种令牌模式策略，显著提高了生成效果并提供了有价值的见解。此外，设计了一套自动数据预处理管道，实现了有效的质量控制。该项目的模型权重、训练代码、注释数据和预处理管道将向社区发布，以促进未来的研究和互动。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:07:44 GMT</pubDate>
</item>
<item>
<title>大型语言模型的安全对齐漏洞分析</title>
<link>https://arxiv.org/abs/2502.13946</link>
<guid>https://arxiv.org/abs/2502.13946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨模板锚定安全对齐对大型语言模型的影响及其脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文分析了大型语言模型（LLMs）在安全对齐方面的脆弱性，认为其初始行为易受到简单攻击的影响。我们提出了“模板锚定安全对齐”的概念，认为模板区域的信息聚合过度影响了模型的安全决策，导致其在面临推理时的越狱攻击时易受影响。经过广泛实验，我们验证了这一现象在多种对齐LLMs中普遍存在。机械分析表明，这种安全对齐方式使模型对攻击高度敏感。此外，本文提出将安全机制与模板区域分离的方案，有望减轻本质上的安全弱点。我们呼吁后续研究以减少对模板区域的依赖，开发更稳健的安全对齐技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:54:57 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-VL：视觉语言系列最新旗舰模型</title>
<link>https://arxiv.org/abs/2502.13923</link>
<guid>https://arxiv.org/abs/2502.13923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-VL在视觉理解与交互方面取得重大进展。</p><br /><br /><p><strong>摘要：</strong> Qwen2.5-VL是Qwen视觉语言系列的最新旗舰模型，在基础能力和创新功能上均取得了显著进展。该模型通过增强的视觉识别、精准的物体定位、强大的文档解析及长视频理解，实现了对世界的深刻理解与互动。其独特的物体定位功能能够通过边界框或点实现精确定位，并提供来自发票、表单和表格的结构化数据提取，此外还支持图表、图示和布局的详细分析。为处理复杂输入，Qwen2.5-VL引入动态分辨率处理和绝对时间编码，使得其能够处理不同尺寸的图像和最长可达数小时的视频，具备秒级事件定位能力。通过从零开始训练本地动态分辨率的视觉变换器（ViT），并结合窗口注意机制，Qwen2.5-VL在节省计算资源的同时，保持原始分辨率，展现出在静态图像和文档理解方面的优异表现，可以在实际场景中作为互动视觉代理进行推理、工具使用及任务执行。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:35:06 GMT</pubDate>
</item>
<item>
<title>提高大语言模型推理时效的信心评估</title>
<link>https://arxiv.org/abs/2502.13962</link>
<guid>https://arxiv.org/abs/2502.13962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨模型推理时的置信度问题，并提出评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在推理过程中大语言模型的计算扩展能力和置信度评估。现有测试时间扩展评估通常假设推理系统应对每个问题提供答案，这忽视了模型对答案信心及提供答案的适宜性。在此基础上，我们提取了推理过程中的置信度评分，以便在模型回答时进行阈值判断。研究发现，增加推理预算不仅能提升正确回答的数量，还能增强正确回答的置信度。此外，本文还扩展了现有的零风险响应评估范式，考虑了非零响应风险的设定，并建议在这些情况下报告评估结果的方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:34:43 GMT</pubDate>
</item>
<item>
<title>Thinking Preference Optimization：提升长链推理能力的有效方法</title>
<link>https://arxiv.org/abs/2502.13173</link>
<guid>https://arxiv.org/abs/2502.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ThinkPO方法以提升模型的长链推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Thinking Preference Optimization（ThinkPO），一种用于提高小型语言模型（LLMs）长链推理能力的后期优化方法。Supervised Fine-Tuning（SFT）虽有效提升推理能力，但获取新数据成本高且重复训练常导致性能停滞。ThinkPO通过利用易得的短链推理回答作为拒绝答案，以及长链推理回答作为选择答案，实施直接偏好优化，鼓励模型倾向于产生更长的推理输出。实验结果表明，ThinkPO显著提升了SFT模型的推理性能，其中数学推理准确率提高了8.6%，输出长度增加了25.9%。此外，ThinkPO还能够持续提升公开精炼的SFT模型性能，如DeepSeek-R1-Distill-Qwen-7B在MATH500数据集上的性能从87.4%提升至91.2%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:31:36 GMT</pubDate>
</item>
<item>
<title>NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2502.12638</link>
<guid>https://arxiv.org/abs/2502.12638</guid>
<content:encoded><![CDATA[
3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:18:32 GMT</pubDate>
</item>
<item>
<title>AdaptiveStep：基于自适应步长的过程奖励模型训练方法</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaptiveStep提供了一种新方法，提高过程奖励模型的训练效率。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的过程奖励模型（PRMs）训练方法，称为AdaptiveStep。现有方法往往依赖于固定的推理步骤和基于规则的技术，但未能有效捕捉文本中的决策点。AdaptiveStep通过模型在预测下一个单词时的置信度来划分推理步骤，从而在每个步骤中提供更多决策信息，这有助于提升下游任务的性能，例如奖励模型学习。实验结果表明，使用AdaptiveStep训练的PRMs在数学推理和代码生成任务中表现出色，超越了贪婪搜索策略和现有开源PRMs，在Best-of-N性能上达到了最新水平，同时减少了超过30%的构建成本。此外，文章还对PRMs的性能、迁移能力和泛化能力进行了深入分析和案例研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:07:01 GMT</pubDate>
</item>
<item>
<title>Crawl4LLM：基于优先评分的高效网页爬取方法</title>
<link>https://arxiv.org/abs/2502.13347</link>
<guid>https://arxiv.org/abs/2502.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crawl4LLM有效提高大型语言模型的预训练数据质量，减少爬取浪费。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了Crawl4LLM，一种高效的网页爬取方法，旨在改善大型语言模型（LLMs）预训练的数据质量。传统的网页抓取方法因其低质量数据而抛弃了大部分爬取的网页，而Crawl4LLM利用网页在LLMs预训练中的影响力，将其作为网页爬虫调度器的优先评分，取代了基于标准图连接的优先级。在对来自商业搜索引擎索引的9亿个网页的实验中，Crawl4LLM以仅爬取21%的URL达到了与之前更大规模爬虫相同的下游性能，显著减少了爬取的浪费，并减轻了对网站的负担。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:57:23 GMT</pubDate>
</item>
<item>
<title>Autellix：优化LLM服务系统的高效调度方法</title>
<link>https://arxiv.org/abs/2502.13965</link>
<guid>https://arxiv.org/abs/2502.13965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Autellix通过优化调度算法显著提升LLM程序的执行效率。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型(LLM)应用的发展，其功能已超越简单的聊天机器人，转向动态的通用代理程序。这一转变要求更高效的LLM服务系统，然而现有的系统忽视了程序与调用之间的依赖关系，导致显著的优化机会缺失。我们的分析指出，LLM请求和程序在处理时会出现长时间的累计等待，主要由于头排阻塞现象。为了解决这一问题，我们提出了Autellix，通过将程序作为一等公民来最小化端到端的延迟。Autellix拦截提交的LLM调用，并为调度器提供程序级上下文，提出了两种调度算法。这些算法能够基于程序之前的调用结果，对LLM请求进行优先级处理。实验表明，与现有最先进的系统，如vLLM相比，Autellix在多种LLM和代理工作负载下，能够以相同延迟提高程序的吞吐量4到15倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:42:06 GMT</pubDate>
</item>
<item>
<title>SearchRAG：提升医疗问答准确性的实时检索框架</title>
<link>https://arxiv.org/abs/2502.13233</link>
<guid>https://arxiv.org/abs/2502.13233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SearchRAG，通过实时检索提升医疗问答准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的SearchRAG框架，旨在提高大型语言模型（LLMs）在医疗问答中的准确性。传统的检索增强生成（RAG）技术通常依赖静态知识库，难以提供最新或详尽的医学信息。SearchRAG通过利用实时搜索引擎，生成合成查询，将复杂的医疗问题转化为更适合搜索引擎处理的查询。同时，该方法还应用不确定性知识选择，筛选和整合最相关的医学知识，确保LLM输入的信息质量。实验结果表明，SearchRAG显著提升了医疗问答任务的响应准确率，尤其在涉及复杂和详细知识的问题上表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:27:22 GMT</pubDate>
</item>
<item>
<title>基于3DGS的闭环强化学习在自动驾驶中的应用</title>
<link>https://arxiv.org/abs/2502.13144</link>
<guid>https://arxiv.org/abs/2502.13144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了一个基于3DGS的闭环强化学习方法提升自动驾驶性能。</p><br /><br /><p><strong>摘要：</strong> 现有的端到端自动驾驶算法通常采用模仿学习（IL）范式，但面临因果混淆和开放环路差距等挑战。本文建立了基于3DGS的闭环强化学习（RL）训练范式，通过利用3DGS技术，我们构建了现实物理世界的光学真实数字复制品，使自动驾驶策略能够广泛探索状态空间，并通过大规模试错来应对分布外场景。为了提高安全性，我们设计了专门的奖励，以指导策略有效应对安全关键事件并理解现实世界中的因果关系。此外，我们在RL训练中将IL纳入作为正则化项，以更好地与人类驾驶行为对齐。最后，我们引入了一个闭环评估基准，由多样化且之前未见过的3DGS环境组成。与基于IL的方法相比，RAD在大多数闭环指标上表现更强，尤其是碰撞率降低了3倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:13:49 GMT</pubDate>
</item>
<item>
<title>克服小模型学习差距的混合蒸馏策略</title>
<link>https://arxiv.org/abs/2502.12143</link>
<guid>https://arxiv.org/abs/2502.12143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出混合蒸馏策略以提高小模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究揭示了小模型学习能力的特殊现象，即小模型在长链推理或来自大模型的蒸馏中并未稳定受益，反而在较短、简单的推理链上表现更佳。为应对这一问题，本文提出了混合蒸馏（Mix Distillation）策略，该策略通过结合长短推理示例，或从大模型与小模型的推理中进行平衡，取得了显著的效果。实验表明，混合蒸馏能有效提升小模型的推理表现，远超单独训练长或短数据的表现。这些发现强调了直接强模型蒸馏的局限性，并突出了在有效转移推理能力时，适应推理复杂性的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:38:13 GMT</pubDate>
</item>
<item>
<title>通过LongPO提升短文档LLM在长文档任务中的表现</title>
<link>https://arxiv.org/abs/2502.13922</link>
<guid>https://arxiv.org/abs/2502.13922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongPO方法使短文档LLM在长文档任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> Large Language Models (LLMs)在短文档任务中表现出色，但在长文档场景中常常由于长文档对齐不足而导致性能下降。本文提出的LongPO方法，通过自我演化，使短文档LLM能够在长文档任务中表现出色。该方法通过生成的短到长偏好数据，包含对于相同指令的长文档输入和其压缩短文档对应的成对响应，帮助LLM在长文档任务中学习和适应。同时，LongPO采用短到长的KL约束，旨在减轻长文档对齐过程中短文档性能的下降。经过应用于Mistral-7B-Instruct-v0.2模型，LongPO在128K到512K上下文长度的实验中，充分保留了短文档性能，并在长短文档任务中表现显著优于传统策略，取得的长文档基准结果可与需大量长文档标注的高级LLM（如GPT-4-128K）相媲美，甚至在某些情况下超越其表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:35:20 GMT</pubDate>
</item>
<item>
<title>提升大型语言模型决策能力的自动化奖励模型框架</title>
<link>https://arxiv.org/abs/2502.12130</link>
<guid>https://arxiv.org/abs/2502.12130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无人工标注学习奖励模型框架，以改善LLM代理的决策能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在文本生成任务上表现优异，但在需要多步骤决策和环境反馈的任务上仍显不足。为了解决LLM代理的局限性，本文提出了一种自动学习奖励模型的框架，该模型无需人工标注数据。通过让一个LLM代理随机导航环境，生成多种动作轨迹，随后利用另一个LLM为每个轨迹分配任务意图，并生成正向和负向响应，构建任务意图-正向响应-负向响应的三元组，作为训练数据来优化奖励模型。此模型能够评分动作轨迹，进而为任务规划提供启发式指导。研究表明该框架在不同代理基准测试中的有效性和普遍性，标志着在复杂互动环境中提升LLM代理决策能力的重要进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 18:20:05 GMT</pubDate>
</item>
<item>
<title>基于注意力机制的YOLOv12框架提升目标检测性能</title>
<link>https://arxiv.org/abs/2502.12524</link>
<guid>https://arxiv.org/abs/2502.12524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOv12实现了速度与注意力机制性能的最佳结合，超越传统CNN模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的YOLO框架YOLOv12，该框架借助注意力机制，实现了与传统CNN模型相同的推理速度，同时提升了目标检测的准确性。与以往的YOLOv10-N和YOLOv11-N相比，YOLOv12-N在T4 GPU上实现了40.6% mAP，仅需1.64毫秒的推理延迟，分别超越了2.1%和1.2%的mAP。此外，YOLOv12还在其他模型规模中展现出显著优势，比如在与RT-DETR系列模型的比较中，YOLOv12-S在运行速度上快42%，计算量及参数量均仅为后者的36%和45%。这些结果表明，YOLOv12在准确性和速度之间达成了优良的平衡，推动了实时目标检测技术的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 13:39:32 GMT</pubDate>
</item>
<item>
<title>视觉模型在时间序列分析中的优势与未来研究方向</title>
<link>https://arxiv.org/abs/2502.08869</link>
<guid>https://arxiv.org/abs/2502.08869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了视觉模型在时间序列分析中的应用及其优势。</p><br /><br /><p><strong>摘要：</strong> 时间序列分析经历了从传统自回归模型、深度学习模型，到最近的变换器和大型语言模型（LLMs）的发展。然而，尽管已有一些利用视觉模型进行时间序列分析的努力，这一领域仍较少被关注。本文讨论了视觉模型在时间序列分析中相较于LLMs的优势，提供了现有方法的全面概述，并回答了如何将时间序列编码为图像以及如何为各种任务建模图像时间序列的关键研究问题。此外，文章还探讨了该框架中预处理和后处理步骤所面临的挑战，并概述了未来研究的方向，以进一步推动视觉模型在时间序列分析中的应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 10:33:08 GMT</pubDate>
</item>
<item>
<title>创新推理方法Flow-of-Options在AutoML中的应用</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flow-of-Options提高了大型语言模型在自动机器学习任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的推理方法Flow-of-Options（FoO），旨在解决大型语言模型（LLMs）中的内在偏见。FoO能够系统地探索各种推理的可能性，本文展示了基于FoO的自主系统在处理机器学习任务（AutoML）方面的应用。该框架在标准数据科学任务上相比于最先进的基线提高了38.2%至69.2%，在治疗化学任务上提高了37.4%至47.9%。整体操作成本每个任务不超过1美元，适合成本敏感的应用场景。除了分类和回归，本文还展示了FoO系统在强化学习和图像生成等任务中的广泛适用性。通过增强LLM解决方案的多样性，FoO提供了显著的进步，并且在结合案例推理时支持长期记忆，带来了更好的可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 08:03:59 GMT</pubDate>
</item>
<item>
<title>Text2World: 基于语言模型的符号世界模型生成新基准</title>
<link>https://arxiv.org/abs/2502.13092</link>
<guid>https://arxiv.org/abs/2502.13092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Text2World基准，通过新的评估方式提升语言模型的世界建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了近年来利用大型语言模型（LLMs）从文本描述生成符号世界模型的研究。针对以往研究中遇到的评估随机性、间接指标依赖和领域范围有限等问题，作者提出了一个新基准Text2World，该基准基于规划领域定义语言（PDDL），涵盖数百个多样化的领域，并采用多标准、执行基础的评估指标，以提供更可靠的评估。通过Text2World基准测试当前的语言模型，发现经过大规模强化学习训练的推理模型表现优异，但即便是表现最佳的模型在世界建模能力上仍显局限。基于这些发现，作者审视了几种有前景的策略，以提升LLMs的世界建模能力，包括测试时扩展、代理训练等。希望Text2World能够为未来的相关研究提供重要资源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 07:53:04 GMT</pubDate>
</item>
<item>
<title>Atom of Thoughts: 通过原子问题提升推理能力的框架</title>
<link>https://arxiv.org/abs/2502.12018</link>
<guid>https://arxiv.org/abs/2502.12018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Atom of Thoughts通过分解问题提升大型语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架Atom of Thoughts (AoT)，旨在解决大型语言模型在推理过程中因历史信息积累而造成的资源浪费和推理干扰问题。AoT通过将当前问题分解为依赖性的无向图，从而形成新的原子问题状态，每个原子问题都是自包含且可验证的。这一迭代的分解-收缩过程直至达到可直接解决的原子问题，从而实现类似于马尔可夫过程的状态转移。实验表明，AoT能够有效提升推理能力，无论作为独立框架还作为现有测试时间扩展方法的插件，均表现优异。尤其在HotpotQA基准测试中，当应用于gpt-4o-mini时，AoT达到了80.6%的F1分数，比o3-mini高出3.4%，比DeepSeek-R1高出10.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:51:04 GMT</pubDate>
</item>
<item>
<title>优化分布式训练的通信与计算重叠技术研究</title>
<link>https://arxiv.org/abs/2502.12996</link>
<guid>https://arxiv.org/abs/2502.12996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了通过重叠通信与计算来优化分布式训练速度的方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了使用分布式优化方法（如DiLoCo）训练大型模型时的通信效率问题。在分布式工作环境下，传统的数据并行训练需要大量通信，尽管DiLoCo的更新拆分为内部优化和外部优化两个阶段减少了通信需求，但在数据中心场景下，外部优化步骤的阻塞仍会导致显著的延迟。为了解决这一问题，本文提出了一种重叠通信与计算的技术，使外部优化步骤与内部优化阶段能够完全重叠。研究表明，名为“急切更新”的特定变体在低带宽的工作环境中，其性能与标准DiLoCo相当，展现出有效的优化潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:13:51 GMT</pubDate>
</item>
<item>
<title>金融领域文本嵌入基准测试与模型评估</title>
<link>https://arxiv.org/abs/2502.10990</link>
<guid>https://arxiv.org/abs/2502.10990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出金融领域的大规模文本嵌入基准FinMTEB及其评估结果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的进展，嵌入模型在自然语言处理（NLP）应用中发挥着至关重要的作用。尽管这些模型通常在通用数据集上进行基准测试，但现实应用迫切需要领域特定的评估。本文介绍了金融大规模文本嵌入基准（FinMTEB），这是一个针对金融领域的专用基准，涵盖64个金融领域特定的嵌入数据集，涉及7个任务，包括金融新闻、公司年报、环境、社会和治理（ESG）报告、监管文件及财报电话会议记录。我们还基于角色数据合成方法开发了一个金融适应模型FinPersona-E5。通过对15个嵌入模型的广泛评估，包括FinPersona-E5，我们的研究发现：通用基准测试的性能与金融领域任务的相关性有限，领域适应模型始终优于通用模型，此外，简单的词袋模型在金融语义文本相似性任务中超越了复杂的密集嵌入技术，突显出密集嵌入方法的当前局限性。我们的工作为金融NLP应用建立了一个稳健的评估框架，并为开发领域特定的嵌入模型提供了关键见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:54:27 GMT</pubDate>
</item>
<item>
<title>序列令牌压缩的极限研究及优化潜力</title>
<link>https://arxiv.org/abs/2502.13063</link>
<guid>https://arxiv.org/abs/2502.13063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示序列令牌的压缩比可达1500，揭示优化空间巨大。</p><br /><br /><p><strong>摘要：</strong> 近年来的研究关注于将令牌序列压缩成较短的实值向量序列，以替代令牌嵌入或键值缓存，这些方法能够减少现有语言模型的计算量。尽管基于强大的编码模型，现有方案的最大无损压缩比通常仅为10。这一现象引人深思，因为理论上即使是16位精度和适中向量大小，大型实值向量的最大信息容量远超此速率。本研究通过用逐样本优化程序替换编码器，探究压缩的极限，结果显示压缩比高达1500，这凸显出现有方法与实用解决方案之间的两个数量级的差距。此外，我们还实证表明，压缩的极限并非由输入长度决定，而是由需减少的不确定性，即该序列的交叉熵损失，这一发现强调了输入嵌入的理论能力与实际利用之间的显著差距，表明在模型设计中存在显著优化空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:43:42 GMT</pubDate>
</item>
<item>
<title>提升变换器性能的层集成记忆方法研究</title>
<link>https://arxiv.org/abs/2502.09245</link>
<guid>https://arxiv.org/abs/2502.09245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出层集成记忆方法，以提升变换器的表示能力和性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对标准变换器在处理历史信息时只使用相邻层的表示，导致表示崩溃和性能欠佳的问题，提出了一种新的解决方案——层集成记忆（LIMe）。该方法允许模型访问早期层的隐藏状态，从而扩展表示能力，而仍然保持整体内存占用。通过在多种架构和查找机制下进行广泛实验，我们展示了在多项任务上表现出一致的性能提升。此外，我们对学习到的表示动态进行分析，并探索了深度电路的应用，揭示了LIMe在层间集成信息的机制，为未来研究指明了有前景的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 03:03:51 GMT</pubDate>
</item>
<item>
<title>增强大型语言模型的领域知识方法综述</title>
<link>https://arxiv.org/abs/2502.10708</link>
<guid>https://arxiv.org/abs/2502.10708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨如何通过知识集成提升大型语言模型的领域适应性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在自然语言理解、文本摘要和机器翻译等任务中取得了显著成功，但其通用特性限制了在特定领域应用中的有效性。为此，研究人员探索了多种方法来增强大型语言模型的领域知识。本综述将这些方法归纳为四种关键途径：动态知识注入、静态知识嵌入、模块化适配器和提示优化。这些方法为大型语言模型提供了领域专业知识的独特机制，平衡了灵活性、可扩展性和效率之间的权衡。文中还讨论了这些方法在特化任务中的应用，比较了领域特定的大型语言模型与通用模型的优缺点，同时指出了这一新兴领域的挑战与机遇。此外，还总结了常用的数据集和基准，以帮助研究者更深入了解这一领域。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10708" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:56:09 GMT</pubDate>
</item>
<item>
<title>增强的钙钛矿太阳能电池知识管理系统</title>
<link>https://arxiv.org/abs/2502.12669</link>
<guid>https://arxiv.org/abs/2502.12669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种集成钙钛矿太阳能电池的知识管理系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种集成化的钙钛矿太阳能电池(PSC)知识增强系统，旨在提高该领域研究的知识管理与推理效率。系统包括三个主要组成部分：首先，构建了Perovskite-KG知识图谱，从1517篇研究论文中提取了23789个实体和22272个关系；其次，创建了两个互补数据集：Perovskite-Chat，涵盖了55101个高质量问答对，以及Perovskite-Reasoning，包含2217个精心策划的材料科学问题；最后，引入了两个专门的大型语言模型：用于领域特定知识辅助的Perovskite-Chat-LLM和用于科学推理任务的Perovskite-Reasoning-LLM。实验结果表明，该系统在领域特定知识检索和科学推理任务上显著优于现有模型，为PSCs领域的研究人员提供了有效的文献回顾、实验设计及复杂问题解决工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:47:33 GMT</pubDate>
</item>
<item>
<title>OctoTools：一个面向多领域复杂推理任务的开放源框架</title>
<link>https://arxiv.org/abs/2502.11271</link>
<guid>https://arxiv.org/abs/2502.11271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OctoTools提供了一个高效的框架，用于解决多领域的复杂推理任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OctoTools，这是一个无需训练、用户友好且易于扩展的开源代理框架，旨在解决多领域的复杂推理任务。OctoTools通过标准化工具卡来封装工具功能，设有高层次和低层次的规划器，以及执行器来实现工具的使用。我们在16个不同任务（包括MathVista、MMLU-Pro、MedQA和GAIA-Text）上验证了OctoTools的广泛适用性，平均准确率提高了9.3%，超越了GPT-4o。此外，OctoTools在相同工具集合下的表现，比AutoGen、GPT-Functions和LangChain高出10.6%。通过全面分析与消融实验，OctoTools在任务规划、工具有效使用和多步骤问题解决上展现出明显的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:27:36 GMT</pubDate>
</item>
<item>
<title>ARM4R：基于人类视频数据的自动回归机器人模型</title>
<link>https://arxiv.org/abs/2502.13142</link>
<guid>https://arxiv.org/abs/2502.13142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ARM4R模型，利用4D表示提升机器人控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ARM4R，一个基于人类视频数据的自动回归机器人模型，通过利用低级4D表示，旨在提升机器人的预训练效果。具体而言，ARM4R从视频中提取的3D点追踪表示，通过单目深度估计将2D表示提升为3D，使得这些4D表示在点与机器人状态表示之间保持共享几何结构，仅需线性变换即可实现高效的迁移学习。实验结果表明，ARM4R能够有效地将人类视频数据转移至机器人控制任务中，并在多种机器人环境和配置下显著改进任务表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:24:26 GMT</pubDate>
</item>
<item>
<title>基于动态提示的无提示微调方法研究</title>
<link>https://arxiv.org/abs/2502.12859</link>
<guid>https://arxiv.org/abs/2502.12859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的无提示微调方法，以增强大型语言模型的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种称为无提示微调（PAFT）的方法，旨在提高大型语言模型（LLMs）在微调后的提示鲁棒性。研究表明，微调后，LLMs能够适应下游任务，但这种适应性常常导致提示鲁棒性降低，细微的提示变化会显著影响模型性能。PAFT 通过在微调过程中动态调整提示，激励模型学习任务的基本原则，而不是过于依赖特定的提示表达。该方法分为两个阶段：首先构建多样化的合成候选提示集；其次在微调过程中从该集合中随机采样提示，生成动态训练输入。通过对多个数据集和 LLMs 的广泛实验，证明使用 PAFT 训练的模型在各种提示（包括未见过的提示）下展现出强大的鲁棒性和泛化能力。此外，这一增强的鲁棒性还提高了模型的性能和推理速度，同时保持了训练效率。消融研究进一步确认了 PAFT 的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:21:54 GMT</pubDate>
</item>
<item>
<title>Soundwave: 一种高效的语音到文本大语言模型训练方法</title>
<link>https://arxiv.org/abs/2502.12900</link>
<guid>https://arxiv.org/abs/2502.12900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Soundwave是一种提高语音到文本模型训练效率的新方法，表现优于前作。</p><br /><br /><p><strong>摘要：</strong> 现有的端到端语音大语言模型通常依赖于大规模的标注数据进行训练，但对于数据高效训练的讨论较少。本文聚焦于语音与文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，结合高效的训练策略和新颖的架构，成功解决了这些问题。实验结果表明，Soundwave在语音翻译和AIR-Bench语音任务中，使用仅为训练数据的五十分之一的情况下，超越了先进的Qwen2-Audio模型。此外，进一步分析显示，Soundwave在对话中仍保持其智能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 00:22:36 GMT</pubDate>
</item>
<item>
<title>Magma：多模态AI代理任务的基础模型</title>
<link>https://arxiv.org/abs/2502.13130</link>
<guid>https://arxiv.org/abs/2502.13130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magma是一个新型多模态基础模型，具备数字与物理世界的智能代理能力。</p><br /><br /><p><strong>摘要：</strong> Magma是一个前沿的基础模型，旨在执行多模态AI代理任务，涵盖数字和物理世界。相较于传统的视觉-语言(VL)模型，Magma不仅具备VL理解能力（语言智能），同时具备在视觉-空间世界中进行规划和行动的能力（时空智能）。为了实现智能代理功能，Magma在大量异构数据集上进行了预训练，这些数据集包括图像、视频和机器人数据。具体而言，图像中的可操作视觉对象通过Set-of-Mark (SoM)进行标记，而视频中的对象运动通过Trace-of-Mark (ToM)进行标记，从而为行动提供基础。实验结果显示，SoM和ToM的结合显著提升了Magma的时空智能，使其在UI导航和机器人操作等任务上打破了以往的记录，优于专门为这些任务设计的模型。同时，在图像和视频相关的多模态任务中，Magma也与其他训练在更大数据集上的大型多模态模型相比较，表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:51:36 GMT</pubDate>
</item>
<item>
<title>测试时间缩放在大型语言模型中的应用与效果研究</title>
<link>https://arxiv.org/abs/2502.12215</link>
<guid>https://arxiv.org/abs/2502.12215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了大型语言模型的测试时间缩放及其对推理能力的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在推理过程中测试时间缩放的能力，尤其是OpenAI的o1系列模型。尽管后续模型如QwQ、Deepseek-R1（R1）和LIMO也声称具备类似能力，但其实际效果仍需进一步探讨。研究发现，更长的chain of thought（CoT）并不总能提高准确性，反而对于同一问题，正确答案的长度往往短于错误答案。深入分析后发现，这一现象与模型的自我修正能力密切相关，长CoT中包含较多的自我修正，常导致性能的降低。本文进一步比较了QwQ、R1和LIMO的顺序与并行缩放策略，结果显示并行缩放在覆盖性和可扩展性上更优。基于此发现，提出了一种Shortest Majority Vote的方法，将并行缩放策略与CoT长度特征相结合，显著提高了模型的测试时间可扩展性，较传统多数投票方法表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:37:46 GMT</pubDate>
</item>
<item>
<title>SafeRoute：高效的安全守卫模型自适应路由方案</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeRoute通过自适应路由提升了安全守卫模型的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 部署大型语言模型需要高效的安全守卫模型来检测和阻止有害用户提示。虽然大型安全守卫模型性能强大，但计算成本高昂。为此，研究提出了SafeRoute，一种双重路由器，旨在区分困难示例和简单示例。该方法通过仅在路由器认定为困难的输入上应用大型安全守卫模型，提高了模型选择的效率，同时保持了较高的准确性。实验表明，相比单独使用大型模型，自适应选择显著提升了计算成本与安全性能之间的平衡，并在多个基准数据集上超越了相关基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:23:34 GMT</pubDate>
</item>
<item>
<title>MUDD连接：提升Transformer跨层信息流动的有效方法</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUDD连接改善了Transformer的残差连接，显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种简单有效的MUDD连接方法，以解决残差连接的局限性，并增强Transformer中的跨层信息流动。与现有的静态共享连接权重方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接能够无缝地集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformers，表现出相当于经过1.8X-2.4X计算训练的Transformers的性能。值得注意的是，MUDDPythia-2.8B在预训练的每个词困惑度和下游任务中与Pythia-6.9B相匹配，并在五次少样本设置中甚至与Pythia-12B相媲美，同时仅增加了0.23%的参数和0.4%的计算量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12170" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:59:16 GMT</pubDate>
</item>
<item>
<title>XLM-SWCM：低资源语言文本生成的新框架</title>
<link>https://arxiv.org/abs/2502.10852</link>
<guid>https://arxiv.org/abs/2502.10852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出XLM-SWCM框架以提升低资源语言的文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的框架XLM-SWCM，用于在极低资源语言中适应多语言编码器以进行文本生成。尽管现有的多语言模型如XLM-R在自然语言处理上获得了进展，但在极低资源语言的表现仍然较差。此外，现代大型语言模型支持的语言种类远少于XLM-R，导致许多语言缺乏文本生成模型。通过重用编码器和解码器之间的权重，XLM-SWCM框架充分利用了编码器学习到的语义空间，从而实现了在低资源语言中的高效学习与有效泛化。我们将此框架应用于四种中国少数民族语言，并在多项下游任务中展示了其优越的性能，甚至超越了更大模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:46:16 GMT</pubDate>
</item>
<item>
<title>基于几何性质的连续扩散模型用于语言建模</title>
<link>https://arxiv.org/abs/2502.11564</link>
<guid>https://arxiv.org/abs/2502.11564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的连续扩散模型，针对语言建模中的离散数据。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的连续扩散模型，旨在解决传统离散扩散模型在语言建模中的局限性。现有的离散扩散模型在信号转换过程中易丢失信息，而现有的连续模型在离散数据上表现不佳，限制了扩散模型的发展。我们通过建立离散扩散与连续流动之间的联系，引入一种简化设计的扩散过程，能够更好地利用底层类别分布的几何结构。此外，基于辐射对称性，我们提出了一种无仿真训练框架，以应对流形的高维性。通过对语言建模基准和其他领域的全面实验，显示我们的方法在性能上优于现有的离散扩散模型，并接近自回归模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:43:02 GMT</pubDate>
</item>
<item>
<title>HealthGPT：融合医疗视觉的强大语言模型</title>
<link>https://arxiv.org/abs/2502.09838</link>
<guid>https://arxiv.org/abs/2502.09838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HealthGPT 是一款强大的医疗视觉语言模型，具有优异的性能与可扩展性。</p><br /><br /><p><strong>摘要：</strong> HealthGPT 是一款强大的医疗大型视觉语言模型（Med-LVLM），能够在统一的自回归范式内整合医疗视觉理解与生成能力。其核心是逐步适应异构理解与生成知识到预训练的大语言模型（LLMs），采用新颖的异构低秩适应（H-LoRA）技术，结合量身定制的分层视觉感知方法和三阶段学习策略。为了有效训练 HealthGPT，我们开发了一个名为 VL-Health 的综合医学领域特定理解与生成数据集。实验结果显示，HealthGPT 在医疗视觉统一任务中表现出色且具有良好的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:35:23 GMT</pubDate>
</item>
<item>
<title>mmMamba：一种线性复杂度的多模态状态空间模型框架</title>
<link>https://arxiv.org/abs/2502.13145</link>
<guid>https://arxiv.org/abs/2502.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mmMamba框架通过知识蒸馏实现线性复杂度的多模态模型，提升效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）展现了卓越的性能，但在部署时面临计算复杂度高、缓存需求增长等挑战。为此，我们提出了mmMamba框架，通过对现有MLLM的逐步知识蒸馏，开发线性复杂度的多模态状态空间模型。该方法允许直接将经过训练的解码器-仅模型转化为线性复杂度架构，无需预训练的基于RNN的LLM或视觉编码器。通过提出播种策略和三阶段蒸馏流程，我们有效地将知识从Transformer转移到Mamba，同时保持多模态能力。经过Transformer的解码器-仅模型HoVLE蒸馏的mmMamba-linear在性能方面与现有的线性和二次复杂度视觉语言模型相竞争，而mmMamba-hybrid的性能进一步显著提升，接近HoVLE的能力。在103K tokens时，mmMamba-linear实现了20.6倍的加速和75.8%的GPU内存减少，而mmMamba-hybrid则实现了13.5倍的加速及60.2%的内存节省。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:08:27 GMT</pubDate>
</item>
<item>
<title>FLAG-Trader：一种融合语言处理与强化学习的金融交易模型</title>
<link>https://arxiv.org/abs/2502.11433</link>
<guid>https://arxiv.org/abs/2502.11433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FLAG-Trader，通过强化学习优化金融交易决策，提高多步骤任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FLAG-Trader，一种结合语言处理与强化学习的金融交易模型。大型语言模型（LLMs）在处理多模态金融数据时，展现出卓越的推理能力，但在需复杂决策的多步骤互动金融市场（如交易）中，表现尚不理想。FLAG-Trader通过将部分微调的LLM作为策略网络，利用预训练知识与金融领域的参数高效微调相结合，提升了决策过程的表现。借助政策梯度优化，在交易奖励的驱动下，FLAG-Trader不仅优化了交易性能，还显著改善了其他金融领域任务的表现。我们提供了大量实证数据来验证这些提升效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:06:19 GMT</pubDate>
</item>
<item>
<title>Decomposed Reward Models: 提取人类偏好的新方法</title>
<link>https://arxiv.org/abs/2502.13131</link>
<guid>https://arxiv.org/abs/2502.13131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRMs通过二元比较提取人类偏好，为个性化AI提供了新视角。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的奖赏模型——分解奖赏模型（DRMs），旨在有效提取多样化的人类偏好。传统奖励模型在捕捉偏好的复杂性方面存在局限性。DRMs不需要细粒度的注释，而是通过二元比较来分析人类偏好，并将其表示为向量。采用主成分分析（PCA）对偏好的数据进行分析，构造了偏好与拒绝响应之间的嵌入差异数据集。通过识别正交基向量，DRMs能够捕捉偏好的不同维度，如有帮助性、安全性和幽默感等。这些分解的奖励可以灵活组合，以适应不同用户的需求，从而提供一个可解释且可扩展的替代方案。我们的结果表明，DRMs不仅有效提取偏好维度，还能在无额外训练的情况下适应新用户，展示了其在个性化和可解释性大型语言模型对齐中的强大能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:59:45 GMT</pubDate>
</item>
<item>
<title>HEADINFER: 一种高效的长序列推理策略</title>
<link>https://arxiv.org/abs/2502.12574</link>
<guid>https://arxiv.org/abs/2502.12574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HEADINFER通过头部级别的KV缓存卸载，显著降低推理内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了HEADINFER，一种优化长序列生成模型的推理内存使用的新策略。通过将关键值缓存（KV缓存）卸载到CPU RAM，HEADINFER避免了在GPU上完全存储Transformer层的KV缓存。该方法采用细粒度的头部级别卸载策略，仅在GPU上维护选择性的注意力头的KV缓存，同时动态计算注意力输出。通过Roofline分析，我们展示了HEADINFER在保持计算效率的同时，大幅降低了内存占用。在对Llama-3-8B模型进行评估时，HEADINFER能够将KV缓存的GPU内存占用从128 GB减少到1 GB，总体GPU内存使用从207 GB减少到17 GB，相比BF16基线推理实现了92%的减少。值得一提的是，HEADINFER使得在单个24GB显存的消费级GPU（如NVIDIA RTX 4090）上实现4百万令牌的推理成为可能，而无需采用近似方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:57:00 GMT</pubDate>
</item>
<item>
<title>Phantom: Subject-consistent video generation via cross-modal alignment</title>
<link>https://arxiv.org/abs/2502.11079</link>
<guid>https://arxiv.org/abs/2502.11079</guid>
<content:encoded><![CDATA[
The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:56:39 GMT</pubDate>
</item>
<item>
<title>基于人群比较评估的LLM自动评价方法的改进</title>
<link>https://arxiv.org/abs/2502.12501</link>
<guid>https://arxiv.org/abs/2502.12501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人群比较的评估方法以提升LLM判断的可靠性。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-as-a-Judge逐渐成为自动评估的主流方法，其基于链式推理（CoT）所产生的判断却存在可靠性不足的问题，主要由于CoT推理无法捕捉到深入且全面的细节而导致的输出不完整。现有方法多依赖于多数投票或标准扩展，这未能有效解决CoT的局限性。为此，我们提出了一种人群比较评估方法，增加了额外的人群反馈来与候选回应进行比较，从而揭示候选回应中的更深层次和更全面的细节。这一过程有效地引导LLM-as-a-Judge提供更详尽的CoT判断。通过大量实验，我们的方法在五个基准测试中平均提高了6.7%的评估准确性，并产生了更高质量的CoT，进一步推动了判断蒸馏与在监督微调过程中的更高效表现。我们的分析表明，所生成的CoT在全面性和质量上优于现有方法，并且随着推理规模的扩大，评估准确性不断改善。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:55:26 GMT</pubDate>
</item>
<item>
<title>RealSyn：用于视觉-语言表示学习的真实与合成文本数据集</title>
<link>https://arxiv.org/abs/2502.12513</link>
<guid>https://arxiv.org/abs/2502.12513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealSyn数据集通过真实与合成文本增强视觉-语言表示学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RealSyn数据集的构建，该数据集结合了高质量的真实文本和合成文本，以提升视觉-语言表示学习的性能。通过建立一条真实世界数据提取管道，提取高质量图像和文本，并设计层次检索方法有效关联图像与多个语义相关的文本，本文充分利用了未配对的数据。此外，为了增强细粒度视觉信息，提出了图像语义增强生成模块用于合成文本的生产，同时采用了语义均衡抽样策略以提高数据集多样性，从而更好地学习长尾概念。RealSyn可在15M、30M和100M三个规模上使用，实验表明，基于RealSyn预训练的模型在多个下游任务上达到了最新的性能，极大推动了视觉-语言表示学习的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:52:22 GMT</pubDate>
</item>
<item>
<title>利用自然语言定义物体方向以增强机器人操作能力</title>
<link>https://arxiv.org/abs/2502.13143</link>
<guid>https://arxiv.org/abs/2502.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过自然语言定义物体方向，以提升机器人的操作能力。</p><br /><br /><p><strong>摘要：</strong> 空间智能是具身AI的关键组成部分，使机器人能够理解并与环境互动。尽管现有视觉语言模型在理解物体位置和关系方面已取得进展，但仍缺乏对物体方向的精确理解，尤其是在细致操作任务中的需求。为解决这一限制，本文提出通过自然语言来定义语义方向，形成了一种更灵活的表示方式。我们构建了OrienText300K数据集，包含带有语义方向的3D模型，旨在将几何理解与功能语义连接。通过整合语义方向进VLM系统，我们的研究使机器人能够在操作中同时考虑位置和方向的约束，实验表明，模型在仿真和真实场景中显著提升了操作准确率，如在Open6DOR上达到48.7%的准确率，SIMPLER上达到74.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:51:33 GMT</pubDate>
</item>
<item>
<title>EQ-VAE：提升潜在生成模型的等变性</title>
<link>https://arxiv.org/abs/2502.09509</link>
<guid>https://arxiv.org/abs/2502.09509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出EQ-VAE，提升潜在生成模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 潜在生成模型已成为高质量图像合成的领先方法，然而现有自编码器在应对尺度和旋转等语义保留变换时缺乏等变性，导致潜在空间复杂性增加，从而影响生成性能。为此，本文提出了EQ-VAE，这是一种简单的正则化方法，旨在在潜在空间内强制实现等变性，降低其复杂性而不损害重构质量。通过对预训练自编码器进行EQ-VAE微调，本文显著提升了包括DiT、SiT、REPA和MaskGIT在内的多种最先进生成模型的性能，其中DiT-XL/2在五个SD-VAE微调周期内实现了7倍的加速。EQ-VAE适用于连续和离散自编码器，从而为多种潜在生成模型提供了灵活的增强工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 14:56:45 GMT</pubDate>
</item>
<item>
<title>多模态检索增强生成系统综述</title>
<link>https://arxiv.org/abs/2502.08826</link>
<guid>https://arxiv.org/abs/2502.08826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本综述分析了多模态检索增强生成系统的挑战和进展。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型因依赖静态训练数据而面临幻觉和过时知识的问题。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，进而提高输出的真实性和时效性。近期的多模态学习进展促成了多模态RAG的发展，将文本、图像、音频和视频等多种模态结合以增强生成效果。然而，跨模态对齐和推理为多模态RAG带来了独特挑战。本文综述了多模态RAG系统，涵盖数据集、指标、基准、评估、方法论及创新等方面，详细审视训练策略、鲁棒性增强及损失函数，并探讨多样化的多模态RAG场景及未来研究方向。该综述为构建更强大、可靠的人工智能系统奠定了基础，旨在有效利用多模态动态外部知识库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>IHEval：评估语言模型指令层级遵循能力的新基准</title>
<link>https://arxiv.org/abs/2502.08745</link>
<guid>https://arxiv.org/abs/2502.08745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IHEval基准评估语言模型在指令层级遵循中的表现与挑战。</p><br /><br /><p><strong>摘要：</strong> 指令层级在语言模型（LMs）中至关重要，确保系统消息、用户消息、对话历史和工具输出之间的优先顺序。然而，这一领域的研究相对较少，缺乏全面的评估基准。为此，我们推出了IHEval，一个新基准，包含3,538个示例，涵盖九项任务，特别是对优先级不同的指令的处理。我们的评估显示，流行的语言模型在识别指令优先级方面表现不佳，尤其在面对相互冲突的指令时，性能显著下降。最具竞争力的开源模型在解决此类冲突中仅获得48%的准确率。这些结果强调了未来在语言模型开发中需要针对性优化的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:21:05 GMT</pubDate>
</item>
<item>
<title>高效影响值估计的神经网络方法</title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为NN-CIFT的小型神经网络方法，以降低影响值估计的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用小型神经网络（称为InfluenceNetwork）来估计模型培训中的影响值，显著降低了计算成本，达到了99%的节约。传统的影响函数计算方法由于高昂的计算需求和内存消耗，在处理大型模型和数据集时效果不理想；而我们的方法能够以仅占全语言模型0.0027%的小型模型，进行有效的影响值估计。我们将此算法（NN-CIFT）应用于针对指令细化的子集选择任务中，结果显示在速度显著提升的情况下，无需牺牲性能，与四个先进的影响函数方法对比，均表现出良好的效果。此外，我们还对NN-CIFT进行了深入的超参数分析，证明其有效性和通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:04:04 GMT</pubDate>
</item>
<item>
<title>合成多模态网络任务数据集和探索者代理的研究</title>
<link>https://arxiv.org/abs/2502.11357</link>
<guid>https://arxiv.org/abs/2502.11357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文开发了一个多模态网络任务数据集，提升了代理的性能。</p><br /><br /><p><strong>摘要：</strong> 近期大型多模态模型（LMM）的成功应用已展现出自主完成复杂网络任务的潜力。尽管开源LMM代理在离线评估基准上取得了显著进展，但在更真实的在线环境中仍远未达到人类水平。本文提出了一种可扩展的方法，合成了迄今为止最大的多样化网络任务轨迹数据集，包含超过94K的成功任务轨迹，跨越49K个独特URL、720K张屏幕截图和33M个网页元素。本研究还介绍了“Explorer”多模态网络代理，并在多个基准测试中表现出色，验证了数据扩展对提高网络代理能力的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:57:43 GMT</pubDate>
</item>
<item>
<title>ILIAS：用于实例级图像检索的新测试数据集</title>
<link>https://arxiv.org/abs/2502.11748</link>
<guid>https://arxiv.org/abs/2502.11748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ILIAS数据集旨在评估图像检索模型对特定对象的识别能力。</p><br /><br /><p><strong>摘要：</strong> ILIAS是一个新推出的测试数据集，专为评估当前及未来的基础模型与检索技术在实例级图像检索能力而设计。与现有数据集相比，ILIAS的优势在于其大规模、多样化的领域，以及准确的真实标注，且性能尚未饱和。该数据集包含1,000个对象实例的查询和正面图像，这些图像经过手动收集，旨在捕捉具有挑战性的条件和多样化的领域。检索任务涉及1亿张来自YFCC100M的数据干扰图像。在避免假阴性并减少额外标注工作量的前提下，ILIAS仅包含确认在2014年后出现的查询对象。通过广泛的基准测试，结果显示：针对特定领域的模型表现优异，但在ILIAS上的效果有限；通过多领域类监督训练线性适配层可以提高性能；局部描述符在重排检索中的作用依然重要，特别是在存在严重背景干扰的情况下；此外，视觉-语言基础模型在文本到图像的性能与图像到图像的性能接近。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:42:58 GMT</pubDate>
</item>
<item>
<title>CALM：结合对话与智能能力的统一语言模型</title>
<link>https://arxiv.org/abs/2502.08820</link>
<guid>https://arxiv.org/abs/2502.08820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CALM统一模型提升了对话系统与任务导向的能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过API调用功能，推动了语言智能体（LA）和传统任务导向对话（TOD）范式的变革。然而，现有方法在支持多轮对话时面临重大困境。任务导向系统通常在有限的目标API上训练，需要新数据来维持与新服务的接口质量，而语言智能体在多轮对话中则难以维持用户意图。为了解决这一问题，本文提出CALM（Conversational Agentic Language Model），一种结合对话和智能能力的统一方法。我们创建了CALM-IT，一个精心构建的多任务数据集，以交错多轮ReAct推理与复杂的API使用。使用CALM-IT训练的CALM系列模型（CALM 8B, CALM 70B, CALM 405B），在三个流行基准（MultiWOZ 2.4, BFCL V3, API-Bank）上均超越了包括GPT-4o在内的顶尖领域专用模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 08:59:34 GMT</pubDate>
</item>
<item>
<title>模型编辑在问答系统中的评估与实践研究</title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明当前模型编辑方法在实际应用中的有效性不足。</p><br /><br /><p><strong>摘要：</strong> 尽管现有模型编辑方法在人工评估中表现良好，但其实用性仍待证实。本研究通过建立QAEdit基准和标准化评估框架，探讨模型编辑在问答（QA）中的有效性。实验结果显示，现有编辑方法在真实应用中的表现远低于预期（38.5%对比~96%），分析指出，主要原因在于以往研究中的评估实践不当，特别是教师强迫使用不当，导致错误无法传播。此外，通过模拟真实场景的连续编辑，发现现有方法在仅进行1000次编辑的情况下效果显著下降。我们的分析对现有模型编辑方法的实际应用及其评估实践进行了重新审视，并提出了改进建议，以推动可靠且实用的模型编辑研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:33:17 GMT</pubDate>
</item>
<item>
<title>MIKASA：增强记忆能力的强化学习基准</title>
<link>https://arxiv.org/abs/2502.10550</link>
<guid>https://arxiv.org/abs/2502.10550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIKASA基准为记忆强化学习提供了统一的评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MIKASA（内存密集技能评估套件），旨在为强化学习中的记忆能力提供全面的评估基准。目前，尽管许多强化学习算法采用了记忆机制，但缺乏统一标准来评估其在各种场景中的表现。在台面机器人操控领域，记忆是解决部分可观测任务和确保系统稳定性的关键因素。为此，MIKASA包括三个主要贡献：首先，提出了一种针对记忆密集型强化学习任务的全面分类框架；其次，收集了MIKASA-Base——一个统一的基准，支持对增强记忆智能体进行系统评估；最后，开发了MIKASA-Robo，这是一个包含32个精心设计的内存密集型任务的新基准，评估台面机器人操控中的记忆能力。这些贡献为推动记忆强化学习研究提供了统一框架，助力更可靠的现实应用系统开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:16:07 GMT</pubDate>
</item>
<item>
<title>Dyve：基于动态过程验证的语言模型错误检测增强工具</title>
<link>https://arxiv.org/abs/2502.11157</link>
<guid>https://arxiv.org/abs/2502.11157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dyve通过动态验证提升语言模型的错误检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Dyve，一个动态过程验证工具，通过结合快速与慢速思维（基于Kahneman的系统理论）来增强大型语言模型中的推理错误检测。Dyve根据任务性质，智能地应用即时的token级确认（系统1）处理简单步骤，而对复杂任务则采用全面分析（系统2）。此外，Dyve引入了一种新颖的逐步共识过滤过程监督技术，利用蒙特卡罗估计与基于语言模型的评估相结合，从嘈杂数据中提取高质量的监督信号。在ProcessBench和MATH数据集上的实验结果表明，Dyve在过程验证方面显著优于现有工具，并在最佳选择设置中提升了性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:33:31 GMT</pubDate>
</item>
<item>
<title>NSA：高效的长上下文稀疏注意力机制</title>
<link>https://arxiv.org/abs/2502.11089</link>
<guid>https://arxiv.org/abs/2502.11089</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NSA机制通过高效稀疏注意力实现长上下文建模，提高计算效率。</p><br /><br /><p><strong>摘要：</strong> 长上下文建模对于下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著挑战。我们提出的NSA机制是一种可原生训练的稀疏注意力解决方案，结合算法创新和硬件优化，实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的令牌压缩与细粒度的令牌选择，既保持全局上下文感知又确保局部精度。通过算术强度平衡的算法设计与现代硬件的实现优化，我们显著提升了计算速度，并实现了端到端的训练，减少了预训练计算量而不影响模型性能。实验表明，使用NSA预训练的模型在各项基准测试、长上下文任务和基于指令的推理中表现与全注意力模型持平或更优，同时在64k长度序列上，NSA在解码、前向传播和反向传播中都显著快于全注意力模型，验证了其在模型生命周期中的高效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11089" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:07:36 GMT</pubDate>
</item>
<item>
<title>改进Adam优化器以缓解大语言模型中嵌入的各向异性问题</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Coupled Adam优化器，显著改善大语言模型的嵌入质量。</p><br /><br /><p><strong>摘要：</strong> 尽管大语言模型具有显著的能力，但它们学习的词表示往往表现出各向异性这一不理想且尚不清楚的问题。本文认为，Adam优化器中的二阶矩是导致嵌入各向异性的一个原因，并提出了一种名为Coupled Adam的改进优化器，以减轻这一问题。实验结果表明，Coupled Adam能够显著提高嵌入质量，同时在大规模数据集上也能改善后续和前期性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 05:28:54 GMT</pubDate>
</item>
<item>
<title>提升自动化事实核查工具的有效性</title>
<link>https://arxiv.org/abs/2502.09083</link>
<guid>https://arxiv.org/abs/2502.09083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究自动化事实核查的解释需求，改善信息核查流程。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型和生成性AI在网络媒体中的广泛应用，自动化事实核查的需求日益增强，以帮助事实核查员应对日益增加的信息失真。然而，自动化核查系统的解释如何与事实核查员的决策与推理过程相结合仍然不明确。通过对事实核查专业人士进行半结构化访谈，本研究阐明了事实核查员评估证据、做出决策及解释其过程的方式，探讨了事实核查员在实践中如何使用自动化工具，并识别了他们对这些工具的解释需求。研究结果显示，当前的解释需求未得到满足，并识别了可复制的事实核查解释的重要标准，包括模型的推理路径、具体证据的引用、以及强调不确定性和信息缺口的要求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:37:21 GMT</pubDate>
</item>
<item>
<title>MagicArticulate：自动将静态3D模型转化为可动画资产的有效框架</title>
<link>https://arxiv.org/abs/2502.12135</link>
<guid>https://arxiv.org/abs/2502.12135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicArticulate框架可以自动将静态3D模型转换为支持动画的版本。</p><br /><br /><p><strong>摘要：</strong> 随着3D内容创作的快速发展，对将静态3D模型自动转化为支持真实动画的关节化版本的需求日益增加。传统方法依赖手动注释，效率低下且耗时。为此，我们提出了MagicArticulate框架，能够有效自动转化静态3D模型为可动画资产。我们的主要贡献包括：第一，建立Articulation-XL，这是一项包含超过33,000个高质量关节注释的3D模型的大型基准数据集。第二，提出了一种新颖的骨架生成方法，将任务表述为序列建模问题，利用自回归变换器自然处理骨骼和关节的变化及其依赖关系。第三，使用功能扩散过程预测皮肤加权，结合顶点与关节之间的体积测地距离先验。实验结果表明，MagicArticulate在不同对象类别上显著优于现有方法，能够实现高质量的关节化，支持真实动画效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:34:15 GMT</pubDate>
</item>
<item>
<title>ThinkDiff：增强图文扩散模型的多模态推理能力</title>
<link>https://arxiv.org/abs/2502.10458</link>
<guid>https://arxiv.org/abs/2502.10458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkDiff通过多模态对齐提升图像生成模型的理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的对齐范式ThinkDiff，旨在通过整合视觉语言模型(VLMs)的优势，赋予文本到图像的扩散模型多模态的上下文理解和推理能力。当前的多模态扩散微调方法大多聚焦于像素级重建，而忽略了上下文推理，且受限于推理数据集的复杂性和可用性。ThinkDiff通过将视觉语言训练作为代理任务，简化了与编码-解码大型语言模型(LLM)解码器的对齐过程，有效提升了扩散模型的理解、推理和构成能力。实验证明，ThinkDiff在多模态上下文推理生成的挑战性CoBSAT基准上，准确率从19.2%提升至46.3%，仅需在4个A100 GPU上训练5小时。此外，ThinkDiff在将多张图像和文本组合成逻辑一致的图像方面表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:33:41 GMT</pubDate>
</item>
<item>
<title>深度神经网络模型中的直觉物理理解研究</title>
<link>https://arxiv.org/abs/2502.11831</link>
<guid>https://arxiv.org/abs/2502.11831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明深度神经网络能通过视频预测学习直觉物理知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了普遍深度神经网络模型在自然视频中预测缺失区域的过程中，如何逐步获得直觉物理理解。基于期望违背框架的实验表明，训练在学习表示空间中的视频预测模型能展示对象延续性和形状一致性等直觉物理特性。而在像素空间中的视频预测和通过文本推理的多模态大型语言模型，表现更接近于随机概率。研究比较不同结构显示，联合学习抽象表示空间并预测感官输入的缺失部分，类似于预测编码，足以培养对直觉物理的理解。这一发现挑战了固有知识的概念，即理解世界所需的核心知识并不一定需要内置到模型中，即便是训练一周的独特视频模型也能超越随机表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:20:25 GMT</pubDate>
</item>
<item>
<title>量子属性预测中的预训练质量优于量</title>
<link>https://arxiv.org/abs/2502.11085</link>
<guid>https://arxiv.org/abs/2502.11085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究表明，优质数据集在量子属性预测中优于大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了量子属性预测中近期的范式，认为进展与数据集规模和计算资源的增长相关。研究表明，在精心选择的任务相关数据集上进行预训练，能够匹敌甚至超越大规模预训练，而计算成本仅为1/24。同时，引入了一种新指标——化学相似性指数（CSI），用于量化上游预训练数据集与下游任务的对齐程度。通过选择最相关的数据集，最低化CSI距离，结果表明，基于较小、聚焦数据集的预训练模型在性能上始终优于那些基于大量混合数据集（如JMP）的模型。这一结果表明，数据增加并不总是提升性能，反而可能因低相关性数据的加入而恶化。这一发现突显出在量子属性预测中，预训练的质量常常优于数量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:16:28 GMT</pubDate>
</item>
<item>
<title>PhysReason：评估大语言模型物理推理能力的新基准</title>
<link>https://arxiv.org/abs/2502.12054</link>
<guid>https://arxiv.org/abs/2502.12054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysReason是一个评估大语言模型物理推理能力的新基准，涵盖1200个问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysReason，一个包含1200道问题的新基准，旨在评估大语言模型在物理推理方面的能力。该基准问题分为知识型（25%）和推理型（75%）两类，并依据难度分为简单、中等和困难三级，其中困难问题的平均解题步骤达到15.6步。我们还提出了物理解题自动评分框架，进行高效的答案级和步骤级评估。尽管一些顶尖模型如Deepseek-R1和Gemini-2.0-Flash-Thinking在答案级评估中得分不到60%，为何从知识型问题（75.11%）到困难问题（31.95%）的性能显著下降，借助步骤级评估，我们发现了四个主要瓶颈：物理定理应用、物理过程理解、计算和物理条件分析。这些发现使PhysReason成为评估语言模型物理推理能力的一个新颖且全面的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 03:53:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型的数学推理能力研究</title>
<link>https://arxiv.org/abs/2502.11574</link>
<guid>https://arxiv.org/abs/2502.11574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型在数学推理中存在逻辑缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）的数学推理能力，使用50个新构建的高中水平文字题进行分析。研究不仅关注模型的最终答案正确性，还深入分析了解题步骤，以识别推理失败。评估了八种最新模型，包括Mixtral、Llama、Gemini和GPT系列，发现虽然一些新模型（如o3-mini、deepseek-r1）在准确度上表现较高，但所有模型在空间推理、战略规划和算术方面均存在错误。有些模型通过错误的逻辑得出了正确的答案。常见的失败模式包括不当假设、对数字模式的过度依赖，以及将物理直觉转化为数学步骤的困难。手动分析表明，尽管模型具备广泛的数学知识，它们在处理需多步推理或现实世界知识的问题上表现不佳。研究强调仅仅关注答案而忽视推理过程的评估是有风险的，凸显出LLMs在广泛性和推理能力上的持续差距，需要针对性改进结构化推理和约束处理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:26:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型在语言复杂性测量任务中的表现研究</title>
<link>https://arxiv.org/abs/2502.11578</link>
<guid>https://arxiv.org/abs/2502.11578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示ChatGPT-o1-mini在语言复杂性测量中表现最佳。</p><br /><br /><p><strong>摘要：</strong> 本文研究了当前大型语言模型（LLMs）在语言复杂性测量任务中的表现，重点关注LIX可读性指标和平均依赖距离（ADD）的计算。通过分析瑞典高中的论文和大学级别的论文，我们评估了模型计算LIX分数和进行依赖解析的能力，并将其结果与已建立的基准进行比较。研究发现，所有模型在这些任务上均展现出一定的能力，其中ChatGPT-o1-mini在LIX计算和依赖解析中表现最为稳定，取得了最高的准确率。此外，我们观察到模型在计算LIX时的准确性与其在大规模多任务语言理解基准（MMLU）上的整体表现之间存在显著相关性（-0.875，p = 0.026，N=6）。这些结果表明，语言复杂性测量能力可以作为评估LLMs一般能力的有效零-shot代理，提供了一种无需大量基准数据集的模型评估方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:23:29 GMT</pubDate>
</item>
<item>
<title>SysGen: 提升语言模型响应的系统消息生成管道</title>
<link>https://arxiv.org/abs/2502.11330</link>
<guid>https://arxiv.org/abs/2502.11330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SysGen改善了语言模型响应与系统消息的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SysGen，一个用于生成系统消息的流程，旨在提高大型语言模型（LLMs）响应的对齐度。当前，公开可用的数据常常缺乏系统消息，且在行业中受到严格的许可限制，手动标注需要大量资源。通过对没有系统消息的有监督微调数据集进行训练，SysGen显著提升了模型响应与系统消息及用户指令的一致性，且在多种开源模型的Multifacet基准测试中表现出显著改善，同时对未见的基准测试，如Open LLM Leaderboard 2，的影响极小。定性分析则强调了多样化系统消息在不同环境中的适应性的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:45:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型知识电路演化研究</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型如何内化新知识并优化知识存储过程。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在知识密集型任务中表现卓越，但它们在理解新知识的内化过程方面存在重大缺口。本文通过知识电路演化的视角，识别出促进知识存储和处理的计算子图。我们的系统分析显示，新知识的获取受既有知识相关性的影响，知识电路的演化经历从形成到优化的明显相变，并且遵循从深到浅的演化模式。这些发现不仅深化了我们对LLMs中新知识获取机制的理论理解，也为改善持续预训练策略以提升模型性能提供了潜在的启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:02:25 GMT</pubDate>
</item>
<item>
<title>探索大型语言模型作为代码执行替代者的能力</title>
<link>https://arxiv.org/abs/2502.11167</link>
<guid>https://arxiv.org/abs/2502.11167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究大型语言模型在代码执行预测中的有效性与局限性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）作为通用代码执行替代者的潜力，尤其是预测程序输出和行为而不实际运行代码。我们提出了SURGE基准，涵盖了八个关键方面，包括多语言编程任务、竞争级编程问题和高成本科学计算等。通过对多种开源和专有LLMs的评估及模型规模与训练数据规模对替代执行准确性的影响分析，我们发现LLMs在某些情况下能够预测代码执行结果，但在通用替代执行方面存在显著限制。研究还对模型预测错误进行了分类，并探讨了潜在的改进领域，提供了使用LLMs作为代码执行替代者的可行性实证见解。代码和数据集已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:01:24 GMT</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:58:24 GMT</pubDate>
</item>
<item>
<title>基于学习框架的人形机器人自动起立控制研究</title>
<link>https://arxiv.org/abs/2502.12152</link>
<guid>https://arxiv.org/abs/2502.12152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种学习框架帮助人形机器人从跌倒状态成功起立。</p><br /><br /><p><strong>摘要：</strong> 本文针对人形机器人在跌倒后自动起立的控制问题，提出了一种学习框架。由于人形机器人跌倒后可能处于多种不同配置，并需要在复杂地形上操作，手动设计控制器面临巨大挑战。研究利用两阶段的课程学习方法，首先在松弛约束条件下发现良好的起立轨迹，其后将这些轨迹精炼为适合部署的平滑、缓慢的运动，确保在不同的初始配置和地形下的稳健性。实验表明，该方法使得一款真实的人形机器人能够从仰卧和俯卧两种姿态成功起立，验证了在真实环境中的有效性。这是首个在人形机器人身上成功演示的学习起立策略。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:49:53 GMT</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:28:31 GMT</pubDate>
</item>
<item>
<title>提升视频理解能力的开源多模态LLM video-SALMONN-o1</title>
<link>https://arxiv.org/abs/2502.11775</link>
<guid>https://arxiv.org/abs/2502.11775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出video-SALMONN-o1，提升视频理解与推理能力的开源多模态语言模型。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了video-SALMONN-o1，这是首个专为一般视频理解任务设计的开源推理增强音视频语言模型。为提升推理能力，我们开发了一个包含具有挑战性音视频问题及逐步解决方案的推理密集型数据集，并提出了过程直接偏好优化（pDPO）方法，利用对比步骤选择实现针对多模态输入的高效步级奖励建模。此外，本文还引入了RivaBench，这是首个理由密集型视频理解基准，包含4000多个高质量专家策划的问题-答案对，覆盖单口喜剧、学术演讲和合成视频检测等场景。与LLaVA-OneVision基线相比，video-SALMONN-o1在不同视频推理基准上实现了3-8%的准确性提升，而pDPO在RivaBench上相对于监督微调模型则提高了6-8%的准确率。增强的推理能力使video-SALMONN-o1具备零次合成视频检测能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11775" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:06:55 GMT</pubDate>
</item>
<item>
<title>TalkHier：一种新型LLM-MA系统的结构化沟通框架</title>
<link>https://arxiv.org/abs/2502.11098</link>
<guid>https://arxiv.org/abs/2502.11098</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TalkHier框架，解决LLM-MA系统中的沟通和优化问题。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-MA系统研究的进展，各代理在复杂任务协作中面临沟通管理和输出优化的挑战。本文提出了Talk Structurally, Act Hierarchically (TalkHier)框架，旨在引入结构化沟通协议和层级优化机制，以解决输出错误、虚假信息及偏见等问题。通过在多种任务上的测试，包括开放域问答、领域特定选择性提问和实际广告文本生成，TalkHier在性能上超过了多种前沿技术，如OpenAI的推理缩放模型和AgentVerse等开源多代理模型。这一新框架展示了其成为LLM-MA系统新标准的潜力，推动了更有效、适应性强的多代理协作框架的发展。相关代码已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11098" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:51:50 GMT</pubDate>
</item>
<item>
<title>通过对例增强数学大语言模型的证明能力研究</title>
<link>https://arxiv.org/abs/2502.10454</link>
<guid>https://arxiv.org/abs/2502.10454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明现有数学大语言模型的证明能力受训练数据的影响，并提出通过对例提高其数学推理能力的方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数学大语言模型（LLMs）在证明生成中的能力，认为现有模型的证明效果主要依赖于其训练中是否遇到相关的证明过程。这一依赖限制了模型对数学定理及概念的深入理解。我们受到人类数学教育中常用的“反例证明”方法的启发，致力于通过反例增强LLMs的数学推理和证明能力。为此，手动创建了CounterMATH，一个高质量的大学水平数学基准，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的掌握情况。此外，我们还开发了数据工程框架，以自动获取训练数据以进一步提升模型性能。广泛的实验和详细的分析表明，CounterMATH具有挑战性，显示出LLMs（如OpenAI o1）在反例驱动的证明能力方面不足。我们认为，增强LLMs的反例驱动概念推理能力对提高其整体数学能力至关重要，提出了对数学大语言模型社区的新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:37:16 GMT</pubDate>
</item>
<item>
<title>Diffusion-Sharpening：一种优化采样轨迹的微调方法</title>
<link>https://arxiv.org/abs/2502.12146</link>
<guid>https://arxiv.org/abs/2502.12146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Diffusion-Sharpening方法，通过优化采样轨迹提升微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Diffusion-Sharpening的微调方法，旨在通过优化采样轨迹提升下游任务的对齐效果。现有的基于强化学习的微调方法往往关注单个训练时间步，而忽视了轨迹级别的对齐；而最近的采样轨迹优化方法则带来了显著的推理NFE成本。Diffusion-Sharpening通过路径积分框架在训练过程中选择最优轨迹，利用奖励反馈来克服这些问题，并摊销推理成本。实验结果表明，该方法在训练效率（二次收敛速度）和推理效率（不需要额外的NFE）方面均表现出色，相较于传统的基于强化学习的微调方法和采样轨迹优化方法，Diffusion-Sharpening在文本对齐、组合能力和人类偏好等多项指标上均取得了更好的效果，提供了一种可扩展且高效的扩散模型微调方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:30:53 GMT</pubDate>
</item>
<item>
<title>HermesFlow：弥合多模态大语言模型理解与生成能力的差距</title>
<link>https://arxiv.org/abs/2502.12148</link>
<guid>https://arxiv.org/abs/2502.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HermesFlow有效弥合了多模态大语言模型的理解与生成能力差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HermesFlow，一个旨在优化多模态大语言模型（MLLMs）理解与生成能力的框架。研究表明，MLLMs的理解能力通常强于其生成能力，二者之间存在显著差距。HermesFlow通过输入同源数据，构建理解与生成的同源偏好数据，利用Pair-DPO和自我对抗优化机制，将理解能力与生成能力有效对齐。实验结果显示，HermesFlow在缩小多模态理解与生成的差距方面，显著优于之前的方法。这一发现突显了HermesFlow作为下代多模态基础模型的一般性对齐框架的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:29:29 GMT</pubDate>
</item>
<item>
<title>SAFE-SQL：自增强上下文学习提升Text-to-SQL性能</title>
<link>https://arxiv.org/abs/2502.11438</link>
<guid>https://arxiv.org/abs/2502.11438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAFE-SQL通过自生成示例提升Text-to-SQL的执行准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架SAFE-SQL，旨在将自然语言问题转换为可执行的SQL查询。传统方法如骨架掩码选择在获取相似训练示例以指导大型语言模型时表现出色，但在实际应用中面临示例缺失的挑战。为此，SAFE-SQL采用自增强上下文学习，通过生成并过滤自增强示例，显著提升SQL生成效果。该框架首先引导大型语言模型生成与测试输入相关的多个Text-to-SQL示例，并通过三种相关性评估进行过滤，构建高质量的上下文学习示例。最终，SAFE-SQL在零-shot和少-shot的Text-to-SQL任务中超越了传统框架，尤其在困难和未见场景中表现出额外的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:06:03 GMT</pubDate>
</item>
<item>
<title>CRANE：一种增强推理能力的约束解码算法</title>
<link>https://arxiv.org/abs/2502.09061</link>
<guid>https://arxiv.org/abs/2502.09061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRANE算法在约束生成中平衡了语法和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保证语法和语义正确性的同时，增强大型语言模型（LLMs）的推理能力。我们首先理论上解释了限制LLM输出为严格语法形式为何会减弱其推理能力。接着，我们提出通过增强输出语法、增加设计良好的额外规则，能够在确保输出合规的同时维持推理能力。基于这些理论见解，我们开发了CRANE算法，它在约束生成的正确性与非约束生成的灵活性之间取得有效平衡。通过对多个开源LLM和基准进行实验，结果显示CRANE在严苛的符号推理基准GSM-symbolic和FOLIO上，相较于最先进的约束解码策略和标准的非约束解码，准确性提升达10%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:43:51 GMT</pubDate>
</item>
<item>
<title>利用高质量LLM数据提升信息提取模型性能</title>
<link>https://arxiv.org/abs/2502.11275</link>
<guid>https://arxiv.org/abs/2502.11275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cuckoo模型展示了如何利用LLM数据提升信息提取效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在信息提取（IE）领域中，将大型语言模型（LLM）的数据应用于IE模型的可能性。我们提出了一种新的NTE（下一标记提取）范式，通过将预测下一个标记的过程重新设计为对已存在上下文中标记的提取，从而构建出Cuckoo模型，该模型基于来自LLM的102.6M抽取数据进行训练。在少量样本环境下，Cuckoo能有效适应传统和复杂指令下的信息提取任务，表现优于现有的预训练IE模型。作为一种“搭便车”方案，Cuckoo能随着LLM数据准备的持续进展而自然演化，无需额外的手动干预，即可从LLM训练管道的改进中获益。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11275" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:10:49 GMT</pubDate>
</item>
<item>
<title>合成数据增强在项目级证明导向编程中的应用</title>
<link>https://arxiv.org/abs/2502.11901</link>
<guid>https://arxiv.org/abs/2502.11901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种合成数据增强方法以解决证明导向编程中的数据稀缺问题。</p><br /><br /><p><strong>摘要：</strong> 现有的语言模型在证明导向编程中面临数据稀缺的问题，主要表现为缺乏足够的相关语料库和项目级实现。本文首次提出了一种基于合成数据增强的方法，旨在通过生成和修复项目级的证明导向编程问题来应对这一挑战。该方法通过合成基本的证明导向编程问题提升语言模型在特定编程语言中的熟练程度，同时引入多样化的编码数据，以增强推理能力，并在现有代码库中创建新的证明和修复数据。这一方法使得语言模型能够在函数级和代码库级别生成及修复证明。我们展示了我们微调后的14B参数模型PoPilot，在项目级证明导向编程中超越了GPT-4o模型64%的性能，并能够通过修复其输出，比GPT-4o的自我修复提高54%的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:05:54 GMT</pubDate>
</item>
<item>
<title>深度分析大型推理模型中的过度思考现象</title>
<link>https://arxiv.org/abs/2502.08235</link>
<guid>https://arxiv.org/abs/2502.08235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，过度思考限制了大型推理模型在互动环境中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型推理模型（LRMs）在互动环境中的过度思考现象，这种现象表现为模型倾向于使用冗长的内部推理链而不是进行环境互动。通过在SWE Bench Verified软件工程任务上的实验，我们识别了三种常见行为模式：分析瘫痪、鲁莽行动和过早脱离。分析结果显示，过度思考得分较高与模型表现下降相关，推理模型表现出比非推理模型更强的过度思考倾向。通过采取一些简单措施，如选择过度思考得分较低的解决方案，我们发现可将模型性能提升近30%，同时计算成本降低43%。这些发现表明，缓解过度思考在实际应用中具有重要意义。我们建议可通过利用原生功能调用能力和选择性的强化学习来减轻过度思考倾向，并开源了我们的评估框架和数据集，以推动该研究方向的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08235" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 17:09:38 GMT</pubDate>
</item>
<item>
<title>选择性自我监督微调方法（S3FT）提升大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2502.08130</link>
<guid>https://arxiv.org/abs/2502.08130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S3FT方法在保持性能的同时，改善了大语言模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的微调方法——选择性自我监督微调（S3FT），该方法旨在提高大语言模型（LLMs）在特定任务上的性能，同时改善其泛化能力。S3FT的方法通过利用多个对同一查询的有效响应来减少模型在微调过程中的过拟合，从而避免过度专注于训练数据的特征。具体而言，S3FT首先通过部署适当的评估者，识别训练集中的正确模型响应，然后利用这些正确响应与目标响应（或其同义句）微调模型。实验结果显示，与标准监督微调（SFT）相比，S3FT在数学推理、Python编程和阅读理解任务上显著提升了性能，并将标准SFT导致的平均性能下降（最高达4.4）减少了一半，说明S3FT在提高模型任务性能的同时，具备更好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 12:27:43 GMT</pubDate>
</item>
<item>
<title>CLaMP 3: 一种跨模态与跨语言音乐信息检索统一框架</title>
<link>https://arxiv.org/abs/2502.10362</link>
<guid>https://arxiv.org/abs/2502.10362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaMP 3通过对比学习实现跨模态及语言的音乐信息检索。</p><br /><br /><p><strong>摘要：</strong> CLaMP 3是一个为解决音乐信息检索中跨模态和跨语言泛化挑战而开发的统一框架。它通过对比学习将主要音乐模态（包括乐谱、表演信号和音频记录）与多语言文本对齐至共享表示空间，从而实现以文本为桥梁的各模态检索。该框架具有可适应新语言的多语言文本编码器，显示了强大的跨语言泛化能力。我们利用增强检索生成技术，创建了M4-RAG数据集，包含231万对音乐文本对，并附有丰富的元数据，涵盖广泛的全球音乐传统。为推动未来研究，我们还发布了WikiMT-X基准数据集，包含1000个乐谱、音频以及多样化文本描述的三元组。实验结果显示，CLaMP 3在多个音乐信息检索任务上达到领先性能，显著超越之前的强基线，展现出在多模态和多语言音乐上下文中的优秀泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 10:18:04 GMT</pubDate>
</item>
<item>
<title>高效多级卷积架构在3D视觉定位中的应用</title>
<link>https://arxiv.org/abs/2502.10392</link>
<guid>https://arxiv.org/abs/2502.10392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效的多级卷积架构，优化3D视觉定位性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种高效的多级卷积架构，用于3D视觉定位，旨在克服传统方法因双阶段或点基础架构导致实时推理困难的问题。受3D物体检测中稀疏卷积架构成功的启发，结合文本特征，使3D场景表示与文本特征有效互动，采用文本引导剪枝（TGP）与基于补全的加法（CBA）方法，通过逐渐区域剪枝与目标补全高度融合信息。TGP通过交叉注意力有效地稀疏化3D场景表示，CBA则解决了过度剪枝对几何信息影响的问题，以微小的计算开销修复被过度剪枝区域。实验显示，与以往单阶段方法相比，该方法在推理速度上领先，且在ScanRefer、NR3D和SR3D上存在显著的准确率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 09:25:39 GMT</pubDate>
</item>
<item>
<title>DarwinLM: Evolutionary Structured Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2502.07780</link>
<guid>https://arxiv.org/abs/2502.07780</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>ImageRAG：基于检索增强生成的图像合成方法</title>
<link>https://arxiv.org/abs/2502.09411</link>
<guid>https://arxiv.org/abs/2502.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ImageRAG通过动态检索图像提高了图像生成质量，特别是在稀有概念的合成方面。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ImageRAG的方法，它结合了图像生成模型与检索增强生成技术，以解决现有扩散模型在生成稀有或未见概念时的挑战。ImageRAG会根据给定的文本提示动态检索相关图像，并将这些图像作为上下文来指导图像生成过程。与之前专门针对检索生成训练的模型不同，ImageRAG不需要专门的训练，而是利用现有图像条件模型的能力。这种方法具有高度的适应性，能够在多种基础模型中应用，显著提高了稀有和细粒度概念的生成效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:41:41 GMT</pubDate>
</item>
<item>
<title>小型多语言模型在低资源语言处理中的适应性研究</title>
<link>https://arxiv.org/abs/2502.10140</link>
<guid>https://arxiv.org/abs/2502.10140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究 explores 小型多语言模型在低资源语言下的适应性与性能提升。</p><br /><br /><p><strong>摘要：</strong> 本研究系统探讨了如何有效地使用参数高效的适配器方法，将小型多语言模型（mLMs）适应于低资源语言（LRLs）。研究评估了三种适配器架构：顺序瓶颈、可逆瓶颈和低秩适配方法。结果显示，使用来自GlotCC的非结构化文本和ConceptNet的结构化知识的小型适配数据集（如最多1GB的自由文本或几MB的知识图数据）能显著提升模型在内部（掩码语言建模）和外部任务（主题分类、情感分析和命名实体识别）的表现。研究发现，顺序瓶颈适配器在语言建模方面表现出色，而可逆瓶颈适配器在下游任务上由于更好的嵌入对齐和更多的参数数量略有凌驾于其他方法之上。适配器方法在使用更少参数的情况下，实现了与完整微调相当或更优的性能，而与大型语言模型（如LLaMA-3、GPT-4及DeepSeek-R1基于的蒸馏模型）相比，小型mLMs在低资源语言任务中更为有效。尽管适应性提高了性能，但预训练数据规模仍是决定因素，特别是对于预训练覆盖面广的语言。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10140" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:29:25 GMT</pubDate>
</item>
<item>
<title>CAPI：一种基于聚类的纯MIM框架及其在图像识别中的应用</title>
<link>https://arxiv.org/abs/2502.08769</link>
<guid>https://arxiv.org/abs/2502.08769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAPI框架通过聚类预测提升了自监督学习的表现，达到了高准确率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CAPI，一个新颖的纯蒙面图像建模（MIM）框架，旨在提升自监督表示学习的性能。我们系统分析了目标表示、损失函数和架构，提出了一种基于聚类的损失函数，以提高模型的训练稳定性和扩展性。CAPI使用ViT-L骨干网络，在ImageNet数据集上取得了83.8%的准确率，ADE20K数据集上实现了32.1%的mIoU，相较于现有的MIM方法大幅提升，并接近当前最先进的方法DINOv2。我们将所有代码和模型发布，促进后续研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 07:24:28 GMT</pubDate>
</item>
<item>
<title>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.10235</link>
<guid>https://arxiv.org/abs/2502.10235</guid>
<content:encoded><![CDATA[
Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:36:23 GMT</pubDate>
</item>
<item>
<title>VibeGen：基于生成AI的蛋白质动态设计框架</title>
<link>https://arxiv.org/abs/2502.10173</link>
<guid>https://arxiv.org/abs/2502.10173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VibeGen是一种基于动态特性的蛋白质生成AI设计框架。</p><br /><br /><p><strong>摘要：</strong> VibeGen是一个新颖的蛋白质生成AI框架，旨在通过正常模式振动进行蛋白质的端到端设计。它采用双模型架构，包括一个基于指定振动模式生成序列候选者的蛋白质设计器和一个评估其动态准确性的蛋白质预测器。通过全原子分子模拟确认，设计的蛋白质能够准确再现规定的正常模式幅度，并在此基础上采用多种稳定、功能相关的结构。显著的是，生成的序列为全新设计，与自然蛋白质没有显著相似性，拓展了可及的蛋白质空间，突破了进化限制。本研究将蛋白质动态性引入生成设计过程，建立了序列与振动行为之间的双向联系，为工程化具有定制动态和功能特性的生物分子开辟了新的路径，具有重要的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:09:33 GMT</pubDate>
</item>
<item>
<title>通过新词开发理解人工智能的语言</title>
<link>https://arxiv.org/abs/2502.07586</link>
<guid>https://arxiv.org/abs/2502.07586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">为有效理解AI，需构建新词汇以弥补现有语言的不足。</p><br /><br /><p><strong>摘要：</strong> 本文立论认为，要理解人工智能（AI），我们不能仅依赖现有的人类词汇。相反，我们应努力开发新词汇，以准确表达人类概念或机器概念，从而实现更好的理解。人类与机器的概念不同，因此可将可解释性视为一种沟通问题：人类需要能够参考和控制机器概念，同时将人类概念传达给机器。通过创建共享的人机语言，借助新词汇的发展，可解决这一沟通难题。成功的新词汇应当在抽象程度上适中，既不过于细化，以便于在多个语境中重用，又不太高层，以便于传达精确信息。作为概念验证，文章展示了如何通过“长度新词”控制大型语言模型的回答长度，以及使用“多样性新词”促进更加多变的响应。总体来说，我们认为，无法仅用现有词汇理解AI，而通过新词汇的扩展可为控制和理解机器创造新的机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 04:28:55 GMT</pubDate>
</item>
<item>
<title>通过局部化关注层提升扩散模型文本生成能力</title>
<link>https://arxiv.org/abs/2502.09935</link>
<guid>https://arxiv.org/abs/2502.09935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨如何通过局部化扩散模型的注意力层来优化图像中的文本生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何在扩散模型中通过局部化局部注意力层来提高文本生成的效率和性能。研究表明，扩散模型参数的不到1%影响图像中文本内容的生成，主要集中在注意力层中。通过只对这些局部注意力层进行LoRA微调，可以显著提升大型扩散模型的文本生成能力，并且在保持图像生成质量和多样性的同时，应用于图像中文本编辑和防止有害文本生成。该方法跨多种扩散模型架构（如U-Net和Transformer）均具有广泛适用性，能够兼容多种文本编码器，如CLIP和T5。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 03:06:17 GMT</pubDate>
</item>
<item>
<title>MR采样器：加速可控生成中的扩散模型采样过程</title>
<link>https://arxiv.org/abs/2502.07856</link>
<guid>https://arxiv.org/abs/2502.07856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MR采样器算法，显著加速了MR扩散模型的采样过程。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型应用的不断增长，可控生成的重要性和挑战并存。目前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归(MR)扩散则直接修改随机微分方程(SDE)的结构，使得图像条件的结合更加简便自然。然而，现有的无训练快速采样器并不适用于MR扩散，因此在获得高质量样本时，MR扩散需要数百次函数评估(NFEs)。本文提出了一种新的算法MR采样器(MRS)，旨在减少MR扩散的采样NFEs。该算法解决了与MR扩散相关的逆向时间SDE和概率流普通微分方程(PF-ODE)，并推导出半解析解，包括一个解析函数和一个由神经网络参数化的积分。基于这一解法，我们能够在更少的步骤中生成高质量样本。本方法无需训练，支持所有主流参数化，包括噪声预测、数据预测和速度预测。大量实验表明，MR采样器在十个不同的图像恢复任务中以10到20倍的加速维持高采样质量，极大提高了MR扩散的可控生成实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 02:03:05 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的车辆间合作感知与规划研究</title>
<link>https://arxiv.org/abs/2502.09980</link>
<guid>https://arxiv.org/abs/2502.09980</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出基于大语言模型的车辆间合作感知与规划新方法。</p><br /><br /><p><strong>摘要：</strong> 当前的自动驾驶车辆主要依赖各自的传感器理解周围环境并规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法的可靠性下降。为了解决这一问题，本文提出了一种将大语言模型应用于车辆间合作的创新设定，并构建了车辆间问答数据集（V2V-QA）和基准测试。我们的基线方法，即车辆间大语言模型（V2V-LLM），使用大语言模型融合多个连接自动驾驶车辆的感知信息并回答与驾驶相关的问题，如物体识别和规划。实验结果显示，V2V-LLM在执行不同任务方面表现优越，优于使用其他融合方法的基线。这项研究为未来自动驾驶系统的安全性提供了新的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09980" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 01:33:15 GMT</pubDate>
</item>
<item>
<title>利用LLM进行反监测的创新方法及其潜在风险</title>
<link>https://arxiv.org/abs/2502.09638</link>
<guid>https://arxiv.org/abs/2502.09638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了通过人类干预实现LLM自我越狱的创新方式及其安全隐患。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的LLM作为红队员的方法，通过人类干预使拒绝训练的LLM能够自我越狱或越狱其他LLM。我们将被越狱的LLM称为J_2攻击者，这些模型能够使用各种红队策略系统性地评估目标模型，并通过从以往失败中学习来提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5作为J_2在Harmbench上的攻击成功率分别达到了93.0%和91.0%，显著优于其他LLM。我们的研究不仅展示了一种受人类红队员启发的战略红队方法的可扩展性，还指出了越狱对越狱的被忽视的失败模式，强调了LLM能通过一个越狱版本自身来绕过自身的安全措施。为防止J_2的直接误用并推动AI安全研究，我们分享了我们的研究方法，并对具体提示细节进行了保密。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:04:19 GMT</pubDate>
</item>
<item>
<title>LLaDA：突破自回归模型的扩散模型探索</title>
<link>https://arxiv.org/abs/2502.09992</link>
<guid>https://arxiv.org/abs/2502.09992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA模型挑战自回归模型，展示了扩散模型的潜力。</p><br /><br /><p><strong>摘要：</strong> 本研究推出LLaDA，一个全新训练的扩散模型，旨在挑战自回归模型在大型语言模型中的主导地位。LLaDA通过前向数据掩蔽过程和反向过程来模型化分布，采用简单的Transformer预测被掩蔽的标记，通过优化似然界限提供了一种原则性生成推断方法。在多个基准测试中，LLaDA显示出强大的可扩展性，超越了自构建的自回归模型基线。特别是，LLaDA 8B在上下文学习中与强大的LLM如LLaMA3 8B竞争，并在多轮对话等案例研究中表现出令人印象深刻的指令跟随能力。此外，LLaDA还克服了逆转诅咒，在逆转诗填空任务中超越了GPT-4o。研究结果确定扩散模型作为自回归模型的有效替代方案，挑战了上述关键语言模型能力与自回归模型内在联系的假设。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:03:18 GMT</pubDate>
</item>
<item>
<title>多模型推理方法提升LLM在高级数学和编码任务中的表现</title>
<link>https://arxiv.org/abs/2502.09955</link>
<guid>https://arxiv.org/abs/2502.09955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多模型推理方法，显著提升LLM在高级数学问题上的解题能力。</p><br /><br /><p><strong>摘要：</strong> 尽管推理语言模型如OpenAI的o1、o3和DeepSeek R1在数学和编码方面取得了显著进展，但在国际数学奥林匹克（IMO）组合问题、抽象与推理库（ARC）难题以及人类最后考试（HLE）问题等高级任务中仍面临挑战。我们提出了一种多元推理方法，在测试时结合多种模型和方法，发现自动验证数学和编码问题的正确性，以及其他问题的拒绝采样，简单而有效。具体而言，我们通过Lean自动验证IMO问题的正确性，通过代码验证ARC难题的解，并发现“最佳-N”有效回答HLE问题，使IMO组合问题的解答准确率从33.3%提升至77.8%，HLE问题的准确率从8%提升至37%。我们的实验表明，该方法在解决948名人类无法解出的ARC难题中成功率达80%，并在o3高计算下未解的ARC难题中解决率为26.5%。通过测试模拟、强化学习和带推理反馈的元学习，我们提高了模型的泛化能力，适应图形表示和不同的提示、代码及数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:57:43 GMT</pubDate>
</item>
<item>
<title>傅里叶数字嵌入方法及其在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2502.09741</link>
<guid>https://arxiv.org/abs/2502.09741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出傅里叶数字嵌入法，以提高数字任务的效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为傅里叶数字嵌入（FoNE）的新方法，旨在解决大语言模型在处理数字时的高效性问题。传统上，大语言模型通过多个标记表示数字，这种碎片化的表示方式在训练和推理中降低了效率，影响了解析数字的能力。FoNE通过将每个数字直接映射到傅里叶特征的嵌入空间，允许每个数字作为单个标记进行编码，并仅为每个数字的每个数字提供两个嵌入维度。这种紧凑的表示方式显著加快了训练和推理过程。在数字任务，尤其是加法、减法和乘法中，FoNE的性能远超传统的子词和数字嵌入，使用70万次数据实现99%准确度比子词和数字嵌入少64倍，对每个数字使用的标记分别减少3倍和6倍。此外，FoNE在超过100,000个测试样例上实现了100%的准确率，展示了其优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:07:53 GMT</pubDate>
</item>
<item>
<title>MM-RLHF: 提升多模态大语言模型对人类偏好的对齐研究</title>
<link>https://arxiv.org/abs/2502.10391</link>
<guid>https://arxiv.org/abs/2502.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MM-RLHF数据集，推动多模态大语言模型对人类偏好的对齐。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLM）取得了显著进展，但大多数最先进的模型未能与人类偏好进行彻底对齐。为此，我们引入了MM-RLHF数据集，包含12万对细粒度人类注释的偏好比较，具有更大的规模、更多样性和更高质量。该数据集帮助我们提出多项创新，如基于批评的奖励模型和动态奖励缩放，旨在提升奖励模型的质量和对齐算法的效率。我们的实验显示，通过MM-RLHF和对齐算法微调LLaVA-ov-7B，模型的对话能力提高了19.5%，安全性提高了60%。我们已开源数据集和训练代码，更多细节可查阅我们的项目页面。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:51:55 GMT</pubDate>
</item>
<item>
<title>Step-Video-T2V：先进的文本生成视频预训练模型</title>
<link>https://arxiv.org/abs/2502.10248</link>
<guid>https://arxiv.org/abs/2502.10248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Video-T2V是一款具有30B参数的文本到视频生成模型。</p><br /><br /><p><strong>摘要：</strong> Step-Video-T2V是一款先进的文本生成视频预训练模型，拥有30B个参数，能够生成长达204帧的视频。该模型使用深度压缩变分自编码器(Video-VAE)，实现了16x16空间和8x时间压缩比，同时确保视频重建质量卓越。为了处理英语和汉语的用户提示，采用了两种双语文本编码器。模型结合3D全注意力的DiT，通过Flow Matching去噪输入噪声，转换为潜在帧，利用视频基础的DPO方法减少伪影和提高生成视频的视觉质量。通过新开发的视频生成基准Step-Video-T2V-Eval进行性能评估，Step-Video-T2V在文本到视频生成质量上表现出色。文章还讨论了当前扩散模型的局限性，并指出了未来视频基础模型的发展方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:50:38 GMT</pubDate>
</item>
<item>
<title>RAS：一种高效的动态采样策略以加速扩散模型</title>
<link>https://arxiv.org/abs/2502.10389</link>
<guid>https://arxiv.org/abs/2502.10389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAS通过动态采样区域显著提高扩散模型的实时性能。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多种生成任务中表现出色，但其依赖于多个顺序前向传播的特性限制了实时性能。传统的加速方法主要集中于减少采样步骤或重用中间结果，未能利用图像中空间区域的差异。本文提出的RAS（区域自适应采样）策略，利用扩散变换器在处理变数量标记的灵活性，为图像的不同区域动态分配采样比例。通过观察模型在每个采样步骤专注于语义重要区域的现象，RAS仅更新当前集中区域，而将其他区域使用上一步的缓存噪声进行更新。实验结果显示，RAS在Stable Diffusion 3和Lumina-Next-T2I上分别实现了2.36倍和2.51倍的加速，同时生成质量几乎不受影响。用户研究表明，RAS在人工评估中表现相当，并实现了1.6倍的速度提升，推动了扩散变换器在实时应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:22:08 GMT</pubDate>
</item>
<item>
<title>ZeroBench：一项全新的视觉推理基准挑战大型多模态模型</title>
<link>https://arxiv.org/abs/2502.09696</link>
<guid>https://arxiv.org/abs/2502.09696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroBench旨在挑战大型多模态模型的视觉推理能力，结果显示其表现较差。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型（LMMs）在图像解释上存在显著不足，其空间认知能力甚至不及幼儿或动物。尽管如此，这些模型在许多流行的视觉基准上得分颇高，然而其进展空间正在迅速缩小。因此，急需一些更具挑战性的基准以保持其长期相关性。为此，研究者推出了ZeroBench，这是一项完全不能被现代前沿LMMs解决的轻量级视觉推理基准，由100个精心编制的问题和334个较易于答复的子问题组成。对20个LMMs进行ZeroBench评估，结果显示它们的得分均为0.0%。此基准的推出旨在促进视觉理解的进展，并已公开发布以供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:20:53 GMT</pubDate>
</item>
<item>
<title>基于时空记忆的智能代理框架STMA</title>
<link>https://arxiv.org/abs/2502.10177</link>
<guid>https://arxiv.org/abs/2502.10177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STMA框架通过时空记忆提升智能代理在动态环境中的决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的代理框架——时空记忆代理(STMA)，旨在提高智能代理在动态环境中执行长远任务的能力。STMA集成了三大关键组件：实时捕捉历史和环境变化的时空记忆模块、支持自适应空间推理的动态知识图谱，以及迭代优化任务策略的规划-评估机制。我们在TextWorld环境中评估了STMA在32个任务上的表现，通过多步规划和探索，结果显示STMA相比最先进模型的成功率提高了31.25%，平均得分提高了24.7%。这些结果突显了时空记忆在提升智能代理记忆能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 21:31:11 GMT</pubDate>
</item>
<item>
<title>新框架提升二维潜在空间的三维重建效果</title>
<link>https://arxiv.org/abs/2502.09613</link>
<guid>https://arxiv.org/abs/2502.09613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出新框架，提升二维潜在空间在三维重建中的效果。</p><br /><br /><p><strong>摘要：</strong> 针对现有三维重建方法在二维特征空间与三维表示之间存在的领域差距，本文提出了一种新颖的框架，旨在将三维意识整合到二维潜在空间中。该框架由三个阶段组成，首先通过一种考虑对应关系的自编码方法，增强二维潜在表示的三维一致性；其次，利用潜在辐射场（LRF）将这些具备三维意识的二维表示提升到三维空间；最后，采用VAE-Radiance Field（VAE-RF）对齐策略，提升从渲染的二维表示中解码图像的效果。通过广泛实验，结果表明该方法在合成性能和跨数据集泛化能力方面优于当前最先进的潜在三维重建方法，是首次展示基于二维潜在表示构建的辐射场可以实现逼真的三维重建性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 21:20:14 GMT</pubDate>
</item>
<item>
<title>GSM-Ranges：评估大语言模型数学推理能力的新方法</title>
<link>https://arxiv.org/abs/2502.08680</link>
<guid>https://arxiv.org/abs/2502.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍GSM-Ranges，用于评估大语言模型在不同数值范围下的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大语言模型（LLMs）在数学推理方面的评估局限性，现有基准测试通常使用有限的数值范围，未能真实反映模型在多样化规模下的问题解决能力。为了解决这些问题，我们引入了GSM-Ranges，这是一个基于GSM8K生成的数据集，旨在系统性地扰动数学问题中的数值，以评估模型在不同数值复杂性下的稳健性。此外，我们提出了一种新颖的评分方法，能够区分逻辑错误和非逻辑错误，从而更精准地评估推理过程。实验显示，随着数值复杂性的增加，模型的逻辑错误率显著上升，高达14个百分点，表明模型在处理超出分布的数值时存在普遍弱点。同时，模型在独立的算术任务上表现良好，但在处理嵌入了文字问题的计算时，其性能显著下降。这些发现为进一步研究LMMs的数学推理能力及其数值泛化的改进提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 09:18:18 GMT</pubDate>
</item>
<item>
<title>VFX Creator: 基于AI的可控视觉效果生成新范式</title>
<link>https://arxiv.org/abs/2502.05979</link>
<guid>https://arxiv.org/abs/2502.05979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于文本描述和静态图像的可控视觉效果生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着电影制作中特效技术的发展，视觉效果（VFX）成为实现魔法与幻觉的重要工具。本文提出了一种新的动画视觉效果生成范式，通过用户友好的文本描述和静态参考图像生成动态效果。主要贡献包括：第一，建立了Open-VFX数据集，这是首个高质量的视觉效果视频数据集，涵盖15种多样化的效果类别并附有详细标注；第二，开发了VFX Creator框架，利用视频扩散变换器实现可控VFX生成。该模型具备空间和时间控制的LoRA适配器，支持实例级的空间操控与精准的时间控制。通过在Open-VFX测试集上的广泛实验，证明了该系统在生成真实动态效果上优于现有技术，并引入了一种专门的度量标准以评估时间控制的精确度。VFX Creator通过结合传统特效与生成方法，拓展了高质量视频特效生成的新可能性，使得先进的视觉效果可被更广泛的受众所掌握。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 08:47:33 GMT</pubDate>
</item>
<item>
<title>通用神经追踪控制器的开发与应用</title>
<link>https://arxiv.org/abs/2502.09614</link>
<guid>https://arxiv.org/abs/2502.09614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了从人类参考中开发的通用神经追踪控制器，以实现灵活的操作。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何开发一个通用的神经追踪控制器，以实现灵活的机器人手部操控，能够应对多种物体及其不同的操控需求。我们提出了一种方法，利用大规模成功机器人追踪示例，结合人类参考和机器人动作，来训练一个神经控制器。通过数据飞轮的方式，我们不断提升控制器的性能以及成功追踪示例的数量和质量。同时，采用强化学习与模仿学习相结合的策略，以增强控制器在动态环境中的表现。此外，为了获得高质量的追踪示例，我们还优化了每条轨迹的追踪，通过同伦优化方法，解决复杂的轨迹追踪问题，从而增加示例的多样性。实验表明，我们训练的通用神经控制器在仿真和实际环境中实现了超过10%的成功率提升，优于现有的先进基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:50:27 GMT</pubDate>
</item>
<item>
<title>3CAD：用于工业缺陷检测的新型大规模数据集与检测框架</title>
<link>https://arxiv.org/abs/2502.05761</link>
<guid>https://arxiv.org/abs/2502.05761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出3CAD数据集及其用于工业缺陷检测的CFRG框架，提升检测准确性。</p><br /><br /><p><strong>摘要：</strong> 为提高工业缺陷检测精度，本文提出了一个新型大规模数据集3CAD，源自真实的3C生产线。该数据集包含27,039幅高分辨率图像，涵盖八种不同类型的制造部件，标注了像素级的异常，具备多种异常类型和多异常区域的特征。我们还提出了一种简单有效的无监督异常检测框架——细化检测范式与恢复引导（CFRG），通过粗定位和细定位相结合，以捕捉小缺陷异常。实验结果表明，CFRG框架在3CAD数据集上的表现强劲，为异常检测领域的发展提供了一个极具挑战性的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:00:29 GMT</pubDate>
</item>
<item>
<title>ProbeLog：提高分类模型检索效率的新方法</title>
<link>https://arxiv.org/abs/2502.09619</link>
<guid>https://arxiv.org/abs/2502.09619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ProbeLog，一种高效检索分类模型的方法。</p><br /><br /><p><strong>摘要：</strong> 随着公开可用模型数量的增加，用户对 pretrained 模型的需求不断上升，但现有的模型搜索方法主要依赖于文档中的文本搜索，限制了用户找到相关模型的能力。本文介绍了一种新方法ProbeLog，它能够在没有访问模型元数据或训练数据的情况下，检索识别目标概念（如“狗”）的分类模型。与以往的探测方法不同，ProbeLog通过观测每个模型的输出维度（logit）对固定输入集（探针）的响应，计算出一个描述符。该方法支持基于 logit 的检索和零-shot、基于文本的检索。为降低编码存储库的成本，本文还开发了一种基于协同过滤的方法，使得编码成本降低三倍。实验结果表明，ProbeLog在实际应用和细粒度搜索任务中都实现了高检索准确率，并且能扩展到全尺寸的模型存储库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:58:25 GMT</pubDate>
</item>
<item>
<title>CoSER: 高质量角色扮演语言模型数据集及评估协议</title>
<link>https://arxiv.org/abs/2502.09082</link>
<guid>https://arxiv.org/abs/2502.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSER提供高质量角色扮演语言模型的数据集及评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CoSER，一个为角色扮演语言代理(RPLA)提供的高质量数据集、开放模型及评估协议。CoSER数据集包含来自771部著名书籍的17,966个角色，并提供真实对话及多样化的数据类型，如对话设置、角色体验和内心想法。我们引入了基于表演方法的给定情境表演，来训练和评估角色扮演的语言模型，LLMs能顺序展现书中多个角色。通过该数据集，我们开发了基于LLaMA-3.1的CoSER 8B和CoSER 70B先进的开放角色扮演语言模型。大量实验表明，CoSER数据集在RPLA的训练、评估和检索中具有重要价值，其中CoSER 70B在我们的评估及现有三个基准上表现优异，超越或匹配了GPT-4o，在InCharacter和LifeChoice基准上分别达到了75.80%和93.47%的准确率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:50:35 GMT</pubDate>
</item>
<item>
<title>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09390</link>
<guid>https://arxiv.org/abs/2502.09390</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:35:53 GMT</pubDate>
</item>
<item>
<title>无编码器架构在3D理解中的应用探索</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次探讨无编码器架构在3D理解中的有效性。</p><br /><br /><p><strong>摘要：</strong> 本文对无编码器架构在3D理解的应用潜力进行了全面研究，解决了现有基于编码器的3D大多模态模型所面临的挑战，例如无法适应不同的点云分辨率以及编码器输出的特征无法满足大型语言模型的语义需求。我们提出了两个关键策略：一是在预训练阶段采用LLM嵌入的语义编码策略，并使用混合语义损失提取高级语义；二是在指令调优阶段引入层次化几何聚合策略，帮助LLM关注点云的局部细节。最终，我们提出了首个无编码器的3D大多模态模型ENEL，表现出色，与当前最先进的模型ShapeLLM-13B相竞争，分别在分类、描述和视觉问答任务中取得55.0%、50.92%和42.7%的成绩。这些结果表明，无编码器架构在3D理解领域具有替代基于编码器的架构的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:27:45 GMT</pubDate>
</item>
<item>
<title>MME-CoT：评估大型多模态模型的链式思维推理性能</title>
<link>https://arxiv.org/abs/2502.09621</link>
<guid>https://arxiv.org/abs/2502.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MME-CoT基准，评估多模态模型的链式思维推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MME-CoT，一个专门评估大型多模态模型（LMMs）链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和常规场景等六个领域。作为该领域的首个综合研究，MME-CoT包含三项新颖的评估指标，细致评估推理质量、鲁棒性和效率。通过高质量数据和独特的评估策略，我们深入分析了最先进的LMMs，发现几个关键见解：拥有反思机制的模型在CoT质量上表现优异，其中Kimi k1.5表现优于GPT-4o并取得最高质量结果；CoT提示在感知性任务中通常会降低LMM性能，这表明可能存在有害的过度思考行为；尽管CoT质量较高，但拥有反思机制的LMMs在正常响应和自我修正阶段效率明显不足。我们希望MME-CoT能够为推进LMMs的多模态推理奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:34:58 GMT</pubDate>
</item>
<item>
<title>Typhoon T1：开放的泰语推理模型开发</title>
<link>https://arxiv.org/abs/2502.09042</link>
<guid>https://arxiv.org/abs/2502.09042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Typhoon T1，一个新型泰语推理模型的开放开发项目。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Typhoon T1，这是一个旨在开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）之上，通过生成长链思维来得到最终答案，已被发现能够提高复杂任务的性能。然而，关于如何开发此类模型的细节相对有限，特别是在低资源语言的推理模型方面。Typhoon T1以更具成本效益的方式进行开发，采用监督微调和开放数据集，而非强化学习。文章分享了合成数据生成与训练的细节，以及我们的数据集和模型权重。此外，我们提供了跨领域通用推理模型的开发见解，能够生成低资源语言中的推理痕迹，以泰语为例。我们希望这一开放努力能为该领域的进一步研究奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09042" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:29:44 GMT</pubDate>
</item>
<item>
<title>CoT-Valve: 动态控制推理链长度的方法</title>
<link>https://arxiv.org/abs/2502.09601</link>
<guid>https://arxiv.org/abs/2502.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT-Valve方法动态调节推理链长度，优化推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法CoT-Valve，用于动态控制推理链的长度，以应对不同任务的难度，降低推理模型的推理成本。研究表明，在简单任务中推理路径容易压缩，而在困难任务中则存在挑战。为此，我们引入了一种新的调整和推理策略，使模型能够生成不同长度的推理链。我们在参数空间中识别出一个能够有效控制生成CoT长度的方向，并构建了从长到短的推理链数据集。实验结果表明，CoT-Valve在控制能力和压缩能力上表现优越，相较于传统的提示控制方法，在减少GSM8K和AIME任务中的推理链长短时，仅带来了轻微的性能下降。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 00:16:30 GMT</pubDate>
</item>
<item>
<title>高质量合成多模态数据及其在mmE5模型中的应用</title>
<link>https://arxiv.org/abs/2502.08468</link>
<guid>https://arxiv.org/abs/2502.08468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了高质量合成多模态数据的标准及其在mmE5模型中的应用。</p><br /><br /><p><strong>摘要：</strong> 多模态嵌入模型因能够将文本和图像等不同模态的数据映射到统一的表示空间而备受关注。然而，有限的标注多模态数据常常限制了嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。我们基于这些原则合成了涵盖多任务、多模态和多语言的高质量数据集，并通过多模态大语言模型进行深度思考生成。同时，该数据集结合了真实世界的图像与准确的文本，确保保真度，经过自我评估和精炼。利用这些高质量合成和标注数据，我们训练了多模态多语言E5模型mmE5，并在MMEB基准测试上取得了最先进的表现，在XTD基准测试中展现了卓越的多语言性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
</item>
<item>
<title>构建评估框架以提升多模态大型语言模型在体感代理中的应用</title>
<link>https://arxiv.org/abs/2502.09560</link>
<guid>https://arxiv.org/abs/2502.09560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EmbodiedBench评估框架以提升多模态代理的实际应用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmbodiedBench，一个旨在评估基于多模态大型语言模型（MLLM）的体感代理的全面评估框架。虽然语言为中心的体感代理得到了较多关注，但基于MLLM的代理仍未得到充分探索。EmbodiedBench覆盖了1,128个任务，任务内容多样，从高层的语义任务到低层的原子操作，包括空间意识、视觉感知等六个核心能力的评估。通过对13个领先的MLLM进行测试，我们发现尽管MLLM在高层任务中表现良好，但在低层操作中面临挑战，最佳模型GPT-4o的平均得分仅为28.9%。EmbodiedBench为研究人员提供了一个标准化的评估平台，不仅揭示了当前的挑战，也为提升MLLM-based体感代理的研究提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09560" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
</item>
<item>
<title>Skrr: 提高文本编码器在T2I扩散模型中的内存效率</title>
<link>https://arxiv.org/abs/2502.08690</link>
<guid>https://arxiv.org/abs/2502.08690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skrr通过选择性跳过和重用层来优化文本编码器的内存使用。</p><br /><br /><p><strong>摘要：</strong> 在文本到图像（T2I）扩散模型中，文本编码器在从文本提示生成高质量图像方面表现出色，但其内存消耗却是去噪模块的八倍。本研究提出了一种名为Skip and Re-use layers (Skrr)的修剪策略，旨在针对T2I任务优化文本编码器的内存使用。Skrr通过选择性地跳过或重用变换器块中的某些层，利用其固有冗余，显著减少内存占用而不影响性能。实验结果表明，Skrr在高稀疏级别下仍能维持与原始模型相当的图像质量，且在多个评估指标（包括FID、CLIP、DreamSim和GenEval分数）上实现了最先进的内存效率，超越了现有的逐块修剪方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
</item>
<item>
<title>InfiniteHiP：高效的长序列推理框架</title>
<link>https://arxiv.org/abs/2502.08910</link>
<guid>https://arxiv.org/abs/2502.08910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InfiniteHiP框架，提高长序列处理速度和效率。</p><br /><br /><p><strong>摘要：</strong> 在现代的大型语言模型中，处理超长上下文面临诸多挑战，如推理速度缓慢和内存消耗增加。为此，我们提出了InfiniteHiP，一个新的高效推理框架，采用模块化的层次化token剪枝算法，动态去除无关的上下文token，从而加速处理。此外，该方法允许通过根据内部注意模式选择性地应用不同的RoPE调整方法，来实现更长序列的概括。我们还在推理过程中将键值缓存转移至主内存，显著降低了GPU内存压力，实现了在单个L40s 48GB GPU上处理高达300万token的能力，相比之下是原本的3倍，无任何上下文信息的永久丢失。InfinityHiP在处理100万token上下文时，实现了18.95倍的注意力解码加速，并且不需要额外的训练。通过在SGLang框架中的实现，我们通过广泛评估展示了其有效性和实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
</item>
<item>
<title>TripoSG：高保真3D形状生成的新流行扩散模型</title>
<link>https://arxiv.org/abs/2502.06608</link>
<guid>https://arxiv.org/abs/2502.06608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TripoSG通过扩散技术实现高保真3D形状生成，提升生成质量与通用性。</p><br /><br /><p><strong>摘要：</strong> 随着扩散技术的进步，图像和视频生成质量得以显著提升，但3D形状生成技术仍面临规模和复杂性限制。本文提出TripoSG，一个新型的形状扩散范式，能生成高保真的3D网格，并精准对应输入图像。TripoSG的关键创新包括：1) 一种针对3D形状生成的大规模规范流变换器，基于大量高质量数据进行训练，以达到最佳保真度；2) 结合SDF、法向量和Eikonal损失的混合监督训练策略，显著提升3D重建性能；3) 一条生成200万高质量3D样本的数据处理管道，强调数据质量和数量在训练3D生成模型中的重要性。实验验证了各组成部分的有效性，使TripoSG在3D形状生成方面实现了领先性能，3D形状细节更为丰富，且对输入图像的保真度极高。此外，TripoSG能从多样的图像风格和内容中生成3D模型，展示了强大的通用性。为了推动3D生成领域的发展，我们将公开该模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
</item>
<item>
<title>提升泰语大语言模型推理能力的方法研究</title>
<link>https://arxiv.org/abs/2502.09056</link>
<guid>https://arxiv.org/abs/2502.09056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了增强泰语大语言模型推理能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入语言特定的大语言模型（LLMs），特别关注泰语LLM。我们的目标是在保持语言特性的同时，提升语言特定LLMs的推理能力。DeepSeek R1在推理方面表现出色，但主要集中于英语和中文等高资源语言，导致低资源语言的表现受限。文章展示了仅使用公开数据集和120美元的计算预算，就能提升语言特定LLMs的推理能力，使其水平与DeepSeek R1相当，同时不影响其在目标语言任务上的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
</item>
<item>
<title>对大型语言模型理解能力的系统评估</title>
<link>https://arxiv.org/abs/2502.08946</link>
<guid>https://arxiv.org/abs/2502.08946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，LLMs在理解物理概念任务上落后于人类约40%。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地探讨了大型语言模型（LLMs）是否真正理解其所表达的内容，提出了一项名为PhysiCo的物理概念理解任务。该任务通过网格格式输入减轻了记忆化问题，网格表示不同层次的理解，包括核心现象、应用示例以及与其他抽象模式的类比。研究结果表明，尽管当前最先进的LLMs（如GPT-4o、o1和Gemini 2.0）在自然语言中能够描述和识别相关概念，但在此网格任务中表现显著低于人类，落后约40%。此外，LLMs的表现不佳源于任务的内在难度，而非网格格式的陌生性，因为在相同格式的数据上进行的上下文学习和微调对其表现几乎没有提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的逻辑推理能力研究</title>
<link>https://arxiv.org/abs/2502.09100</link>
<guid>https://arxiv.org/abs/2502.09100</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了大型语言模型在逻辑推理方面的最新进展。</p><br /><br /><p><strong>摘要：</strong> 随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展示了显著的推理能力，但其进行严谨逻辑推理的能力仍然存在疑问。本文综述了LLMs中逻辑推理的最新进展，探讨了其理论基础及评估推理能力的基准。我们分析了在不同推理范式（包括演绎、归纳、溯因和类比）下的现有能力，并评估了提升推理表现的策略，包括数据中心调优、强化学习、解码策略和神经-符号方法。最后，本文提出了未来的研究方向，强调进一步探索以增强人工智能系统中的逻辑推理能力的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09100" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
</item>
<item>
<title>SelfCite：一种自监督方法生成高质量句子级引用</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SelfCite以自监督方式提升LLMs生成高质量引用的能力。</p><br /><br /><p><strong>摘要：</strong> SelfCite是一种新颖的自监督方法，旨在提高大型语言模型(LLMs)生成高质量细粒度句子级引用的能力。该方法依赖于LLM自身提供的奖励信号，通过上下文的消融实验来判断引用的必要性。当引用文本被移除时，若应答变化则说明引用必要；而保留引用文本时，应答不变则说明已提供足够的信息。这一奖励信号可以有效指导推理过程中最佳抽样策略的实施，从而显著改善引用质量。此外，该信号也可以用于偏好优化，直接对模型进行微调，以生成更好的引用。在LongBench-Cite基准测试中，SelfCite在五个长文本问答任务上使引用F1值提升了多达5.3个百分点，展现了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
</item>
<item>
<title>基于GEMINI学习的医疗图像密集对比表示学习</title>
<link>https://arxiv.org/abs/2502.05282</link>
<guid>https://arxiv.org/abs/2502.05282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GEMINI学习以增强医疗图像的密集对比表示效率。</p><br /><br /><p><strong>摘要：</strong> 密集对比表示学习（DCRL）在医疗图像密集预测任务中显著提高了学习效率，但由于医疗图像的特殊性，往往会导致不可靠的对应关系发现，从而产生大量的错误匹配对（假阳性和假阴性）。为了解决这一问题，本文提出了一种名为GEMINI的学习框架，通过将同胚性先验嵌入到DCRL中，实现有效的对应关系发现。我们设计了可形变同胚学习（DHL），该方法通过建模医疗图像的同胚性来学习可变形映射，以在保持拓扑结构的前提下预测像素对应关系，有效减少匹配空间。还提出几何语义相似性（GSS），用于提取特征中的语义信息，从而量化对应学习的对齐度。通过这两种方法，GEMINI不仅提高了学习效率，同时构建可靠的正向匹配对。在多项实验中，我们在七个数据集上实现了比现有方法更优的结果，证明了我们方法的有效性和优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 14:57:40 GMT</pubDate>
</item>
<item>
<title>PDE-Controller: 利用大型语言模型控制偏微分方程系统</title>
<link>https://arxiv.org/abs/2502.00963</link>
<guid>https://arxiv.org/abs/2502.00963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PDE-Controller框架使LLMs能有效控制偏微分方程系统。</p><br /><br /><p><strong>摘要：</strong> PDE-Controller是一个新框架，旨在利用大型语言模型（LLMs）来控制偏微分方程（PDE）系统，充分利用其在应用数学中的潜力。该框架能够将非正式的自然语言指令转化为正式规范，并执行推理和规划步骤，从而提升PDE控制的实用性。为了实现这一目标，我们构建了一个综合解决方案，包含人类撰写的案例及200万条合成样本的数据集、数学推理模型和创新的评估指标，付出了相当大的努力。我们的实验表明，PDE-Controller在推理、自我形式化和程序合成方面，显著优于使用最新开源和GPT模型的提示方法，实现了PDE控制实用性提高62%的显著进展。通过缩小语言生成与PDE系统之间的差距，我们展示了LLMs在解决复杂科学与工程问题方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.00963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 11:41:16 GMT</pubDate>
</item>
<item>
<title>通过增强交叉注意机制实现大型模型知识传输至小型模型</title>
<link>https://arxiv.org/abs/2502.08213</link>
<guid>https://arxiv.org/abs/2502.08213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过增强交叉注意机制实现大模型向小模型的知识传输。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种LLM模块架构，利用增强交叉注意机制将知识从大型预训练模型传递给小型模型。具体而言，通过冻结Qwen2-1.5B模型，并将其表示通过特制注意力层传递给GPT-Neo-125M模型，从而在有限的计算资源下进行训练。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量可与蒸馏方法相媲美。文中讨论了模块化方法的优势，提供了输入查询示例和比较分析，并展望了该方法的进一步扩展前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 05:48:33 GMT</pubDate>
</item>
<item>
<title>改进长效目标优化的语言模型探索方法</title>
<link>https://arxiv.org/abs/2502.06533</link>
<guid>https://arxiv.org/abs/2502.06533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习改进语言模型在长效目标上的探索能力。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）发展过程中，实现长效目标是一项重要挑战。本文研究了如何通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。探索过程中需权衡发现新方案与维持预训练模型基本能力的平衡，通常通过Kullback-Leibler（KL）惩罚来控制。我们通过对小型语言模型在简单算术任务上的探索动态进行研究，发现预训练程度对探索的影响，并强调了“关键标记”的重要性，这些标记对最终结果有显著影响。此外，我们提出了一种对KL惩罚的简单修改，旨在促进关键标记的探索，提升RL微调阶段的效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:47:28 GMT</pubDate>
</item>
<item>
<title>Animate Anyone 2: 结合环境语义的角色动画生成</title>
<link>https://arxiv.org/abs/2502.06145</link>
<guid>https://arxiv.org/abs/2502.06145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍Animate Anyone 2，提升角色与环境的动画一致性与合理性。</p><br /><br /><p><strong>摘要：</strong> 随着基于扩散模型的角色图像动画方法的发展，Animate Anyone 2应运而生，旨在改善角色与其环境之间的关系。与以往仅提取源视频的运动信号不同，Animate Anyone 2还捕捉环境的表现作为条件输入，这样可以在角色与环境之间建立更合理的关联。文章提出了一种形状无关的掩码策略，以更有效地描述角色与环境的关系。同时，引入了对象引导器用于提取交互对象的特征，并通过空间混合来增强特征注入，以提高对象交互的真实感。此外，作者还提出了姿势调节策略，使模型能够处理更多样化的运动模式。实验结果表明，该方法在动画生成方面具有显著的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:45:43 GMT</pubDate>
</item>
<item>
<title>BenchMAX：一种多语言评估基准以测量语言模型的高级能力</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchMAX是一个新兴的多语言评估基准，专注于大型语言模型的高级能力测量。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型(LLMs)的快速发展，传统的多语言基准主要集中于简单理解任务，未能充分评估它们在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。为了解决这一问题，我们引入了BenchMAX，它是一个多方式的多语言评估基准，能公平比较这些关键能力。该基准经过三个不同的母语评审者对所有任务中的样本进行独立标注，并在从英语机器翻译到另外16种语言后进行测试。全面的实验表明，核心能力在不同语言间的有效性存在差异，这些差距不能仅通过扩大模型规模来弥补。BenchMAX为多语言模型的发展提供了一个综合评估平台，数据集和代码均已公开访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:34:47 GMT</pubDate>
</item>
<item>
<title>优化模型合并提升大语言模型性能</title>
<link>https://arxiv.org/abs/2502.04411</link>
<guid>https://arxiv.org/abs/2502.04411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过分层合并与任务级路由技术提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究针对将不同任务微调后的大语言模型（LLMs）合并为更强模型时参数冲突导致性能下降的问题，提出了一种新的优化方法。研究发现，不同层级的模型存在不同程度的参数冲突。因此，本文提出对参数冲突较小的层进行平均，而对参数冲突明显的层采用新颖的任务级专家路由。同时，为了降低存储成本，借鉴任务算术稀疏性，本文将多个微调专家解耦成一个稠密专家和若干个稀疏专家。在处理分布外样本时，依据任务不确定性选择并合并合适的专家。通过在LLaMA和Qwen等多个参数规模的模型上进行大规模实验，结果表明，该方法在真实世界推理任务中始终能实现显著的性能提升，并且所需的系统成本低于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:30:35 GMT</pubDate>
</item>
<item>
<title>动态安全框架优化语言模型推理安全</title>
<link>https://arxiv.org/abs/2502.07985</link>
<guid>https://arxiv.org/abs/2502.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种在推理时优化语言模型安全性的动态安全框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的动态安全框架，用于在推理时优化语言模型的安全性 reasoning，且不需修改模型权重。该方法基于近期自我批评技术的进展，利用一种元批评机制，迭代地更新安全提示（称为规范），以自适应地驱动批评和修正过程。此种测试时优化不仅提高了模型应对对抗性越狱请求的能力，还在避免道德伤害和追求诚实回答等各种安全相关任务中表现出色。通过在多个语言模型上的实证评估，结果显示动态优化的安全提示明显优于固定系统提示和静态自我批评防御策略，显著提高了安全评分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:47:30 GMT</pubDate>
</item>
<item>
<title>WorldGUI：一种新颖的GUI基准用于真实用户交互评估</title>
<link>https://arxiv.org/abs/2502.08047</link>
<guid>https://arxiv.org/abs/2502.08047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WorldGUI基准，评估GUI任务中的初始状态敏感性及其影响。</p><br /><br /><p><strong>摘要：</strong> 当前的GUI代理在元素定位方面表现出色，但规划依然面临巨大挑战，尤其是对于环境初始状态的敏感性。些微的初始状态差异，如目标软件未打开或界面未处于默认状态，往往导致规划错误，这在真实用户场景中普遍存在，但现有基准未能对此进行评估。为此，本文提出WorldGUI，这是一种新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。该基准涵盖了10款流行软件应用的多种任务，包括PowerPoint、VSCode和Adobe Acrobat。此外，为应对动态GUI自动化任务的挑战，我们提出了GUI-Thinker，一个整体框架，利用批判机制，能够有效管理GUI交互的不确定性和复杂性。实验结果显示，GUI-Thinker在WorldGUI任务上相比Claude-3.5（计算机使用）成功率提高了14.9%，突显了基于批判性思维的框架在提升GUI自动化中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:39:08 GMT</pubDate>
</item>
<item>
<title>建立值得信赖的检索增强生成（RAG）系统的综合路线图</title>
<link>https://arxiv.org/abs/2502.06872</link>
<guid>https://arxiv.org/abs/2502.06872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升RAG系统可信度的五大关键视角。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）是一种先进技术，旨在解决人工智能生成内容（AIGC）的挑战，通过将上下文检索集成到内容生成中，RAG提供可靠且最新的外部知识，减少幻觉，并确保跨任务的一致相关性。然而，尽管RAG潜力巨大，最新研究显示其也引入了新的风险，如鲁棒性问题、隐私关注、对抗攻击和责任问题。为应对这些风险，本文提出了一个关于构建值得信赖的RAG系统的综合路线图，围绕可靠性、隐私、安全性、公平性、可解释性和责任感五大关键视角展开讨论，提供一般框架和分类法，以帮助理解当前挑战、评估现有解决方案及确定未来研究方向。同时，强调值得信赖的RAG系统在实际应用中的重要影响，以鼓励更广泛的采用和创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:06:04 GMT</pubDate>
</item>
<item>
<title>NoLiMa基准评估长文本环境下大语言模型的检索能力</title>
<link>https://arxiv.org/abs/2502.05167</link>
<guid>https://arxiv.org/abs/2502.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过NoLiMa基准评估LLMs在长文本中信息检索的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NoLiMa基准，旨在评估当前大型语言模型(LLMs)在长文本环境中的信息检索能力。NIAH测试是一种常用的方法，通过在冗长的上下文中检索相关信息来衡量模型性能。与传统方法不同，NoLiMa设计了一个针集，其中问题与信息的词汇重叠最小，这要求模型推断潜在的关联以定位信息。研究评估了12种声称支持至少128K标记上下文的流行LLMs，结果显示在短文本(<1K)中它们表现良好，但随着上下文长度的增加，表现显著下降。在32K上下文中，10个模型的表现低于50%的短文本基准，并且即便是表现最好的GPT-4o，基准从99.3%降至69.7%。分析表明，长文本中缺乏字面匹配使注意力机制面临更大困难，进而影响信息的检索能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:04:29 GMT</pubDate>
</item>
<item>
<title>TextAtlas5M：评估长文本条件下的图像生成的新数据集</title>
<link>https://arxiv.org/abs/2502.07870</link>
<guid>https://arxiv.org/abs/2502.07870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextAtlas5M是一个用于评估长文本条件下图像生成的新数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本条件下的图像生成受到了广泛关注，尤其是在处理复杂的长文本提示方面。尽管取得了一定进展，现有数据集主要专注于短文本，使得长文本图像生成仍然面临挑战。为了填补这一空白，本文提出了TextAtlas5M，这是一个专门设计用于评估长文本渲染的新数据集，涵盖500万张长文本生成及收集的图像。数据集内容多样，支持对大型生成模型在长文本图像生成上的综合评估。此外，我们精心策划了3000个经过人工改进的测试集TextAtlasEval，建立了长文本条件生成方面的重要基准。评估结果显示，TextAtlasEval基准给当前最先进的专有模型（如GPT4o与DallE-3）带来了显著挑战，而开源模型的性能差距更大。这些证据使TextAtlas5M成为未来文本条件图像生成模型训练和评估的宝贵数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:50:07 GMT</pubDate>
</item>
<item>
<title>Light-A-Video：无训练的视频重光照方法</title>
<link>https://arxiv.org/abs/2502.08590</link>
<guid>https://arxiv.org/abs/2502.08590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-A-Video 提供了一种无训练的视频重光照解决方案，提升了时间一致性。</p><br /><br /><p><strong>摘要：</strong> 近日，图像重光照模型的进步主要得益于大规模数据集和预训练扩散模型，使得一致性照明得以实现。然而，视频重光照仍面临训练成本过高和高质量多样化视频数据集稀缺的挑战。本研究提出了 Light-A-Video，这是一种无训练的方法，旨在实现时间平滑的视频重光照。通过设计一致性光照注意模块（CLA），加强了自注意力层内帧间的交互，从而稳定背景光源的生成。此外，我们利用光传输独立性的物理原则，采用渐进光融合（PLF）策略，在源视频的外观和重光照外观之间进行线性混合，以确保照明的平滑过渡。实验证明，Light-A-Video在保持图像质量的同时，提高了重光照视频的时间一致性，确保了帧间光照的连贯性过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:56 GMT</pubDate>
</item>
<item>
<title>LASP-2: 提升线性注意力变换器模型的序列并行ism方法</title>
<link>https://arxiv.org/abs/2502.07563</link>
<guid>https://arxiv.org/abs/2502.07563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LASP-2 提高了线性注意力模型的训练速度和并行性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LASP-2，这是一种新型序列并行（SP）方法，旨在提升线性注意力变换器模型在处理超长输入序列时的通信和计算并行性。与之前的LASP相比，LASP-2重新审视了线性注意力层对于SP的最小通信需求，并重组了整体的通信-计算工作流。此方法仅需对中间内存状态进行一次集体通信，显著提高了通信与计算的并行性及其重叠。另外，LASP-2延伸至LASP-2H，对标准注意力模块进行类似的通信重设计，为融合线性与标准注意力层的混合模型提供了高效的SP解决方案。在对Linear-Llama3模型的评估中，LASP-2实现了相对LASP快15.2%的训练速度提升，相比Ring Attention则提升了36.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:31 GMT</pubDate>
</item>
<item>
<title>CoCoMix：结合离散的下一个标记预测与连续概念的预训练框架</title>
<link>https://arxiv.org/abs/2502.08524</link>
<guid>https://arxiv.org/abs/2502.08524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoCoMix是一种新颖的预训练框架，通过概念学习提高语言模型性能。</p><br /><br /><p><strong>摘要：</strong> CoCoMix是一种新提出的预训练框架，它结合离散的下一个标记预测与连续概念学习。通过使用预训练的稀疏自编码器，CoCoMix能够预测连续概念，并将其与模型的隐藏状态混合。在多个基准测试中，包括语言建模和下游推理任务，实验结果表明CoCoMix在样本效率上表现更佳，并且在性能上一致优于传统的下一个标记预测、知识蒸馏以及插入暂停标记的方法。研究发现，概念学习和交错处理的结合对于性能提升至关重要。此外，CoCoMix也增强了模型的可解释性和可引导性，可以直接检查和修改预测的概念，从而提供一种透明的方式来指导模型的内部推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:42:44 GMT</pubDate>
</item>
<item>
<title>基于计算预算的模型蒸馏性能估计研究</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一个蒸馏规模法，优化了模型性能与计算预算的分配。</p><br /><br /><p><strong>摘要：</strong> 本研究提供了一种蒸馏规模法，以估算基于计算预算的蒸馏模型性能，并优化了计算在教师和学生模型之间的分配。研究成果降低了大规模使用蒸馏的风险，确保在分配计算时能够最大化学生模型的性能。我们提供了计算最优的蒸馏方案，适用于存在教师模型或需要训练教师模型的情况。如果有多个学生模型进行蒸馏且已有教师模型，则蒸馏的效果优于监督预训练，直到计算水平随着学生规模的增加而可预测性地增长；而如果只有一个学生需要蒸馏且教师也需要训练，则应选择监督学习。此外，基于大规模研究结果，我们提供了关于蒸馏的新见解，增进了对蒸馏的理解并为实验设计提供了指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:41:41 GMT</pubDate>
</item>
<item>
<title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title>
<link>https://arxiv.org/abs/2502.08168</link>
<guid>https://arxiv.org/abs/2502.08168</guid>
<content:encoded><![CDATA[
In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:57:30 GMT</pubDate>
</item>
<item>
<title>CineMaster：3D感知可控文本到视频生成框架</title>
<link>https://arxiv.org/abs/2502.08639</link>
<guid>https://arxiv.org/abs/2502.08639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineMaster框架实现了3D感知与可控的文本到视频生成。</p><br /><br /><p><strong>摘要：</strong> CineMaster是一个创新的3D感知可控文本到视频生成框架，旨在为用户提供与专业导演相似的控制能力，包括场景中的物体精确放置、对象和相机在3D空间中的灵活操控，以及对渲染帧的直观布局控制。该框架分为两个阶段：第一阶段通过交互式工作流程帮助用户在3D空间内构建条件信号；第二阶段利用生成的深度图、相机轨迹和对象类别标签，指导文本到视频扩散模型生成用户意图的视频内容。此外，CineMaster还建立了自动化数据注释管道，以从大规模视频数据中提取3D边界框和相机轨迹，克服野外数据集稀缺的问题。实验结果表明，CineMaster在3D感知文本到视频生成方面明显优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:55:44 GMT</pubDate>
</item>
<item>
<title>基于下一块预测的半自回归视频生成框架</title>
<link>https://arxiv.org/abs/2502.07737</link>
<guid>https://arxiv.org/abs/2502.07737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种半自回归框架，显著提升视频生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为下一块预测（NBP）的半自回归（semi-AR）框架，用于视频生成。通过将视频内容均匀分解成等大小的块（如行或帧），我们将生成单位从单个令牌转变为块，使当前块中的每个令牌可以同时预测下一个块中对应的令牌。这种框架在每个块内应用双向注意力，捕捉到更强的空间依赖性。通过并行预测多个令牌，NBP显著减少了生成步骤，从而提高了推理速度和效率。我们的模型在UCF101和K600数据集上的FVD分数分别达到103.3和25.5，平均超越传统NTP模型4.4。此外，由于推理步骤减少，NBP模型的生成速度达到每秒8.89帧（128x128分辨率），实现了11倍的加速。我们还探索了从700M到3B参数的模型规模，发现生成质量显著提升，UCF101和K600上的FVD分数分别从103.3降至55.3和25.5降至19.5，展示了我们方法的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:48:00 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型在金融推理中的能力与改进</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型在金融推理任务中的表现及改进方法。</p><br /><br /><p><strong>摘要：</strong> 本研究综合评估了16种强大推理和通用大型语言模型(LLMs)在三项复杂金融任务上的表现，包括金融文本、表格数据和方程的处理，重点考察数值推理、表格解读、金融术语理解、长上下文处理与基于方程的问题解决能力。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但如CoT微调等一般性增强并不总是有效。此外，所有推理策略在长上下文和多表格任务中的性能提升面临挑战。为了解决这些限制，研究开发了基于Llama-3.1-8B-Instruct的金融推理增强模型，通过CoT微调和领域特定推理路径的强化学习，简单的单金融数据集微调使模型在任务中平均实现了10%的一致性提升，超越了所有8B模型及Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究强调了金融任务中适应特定领域的必要性，并指明了未来的研究方向，如多表格推理和金融术语理解。所有数据集、模型和代码都已公开，并引入了一个基准排行榜以促进未来的数据集和模型的评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:45:28 GMT</pubDate>
</item>
<item>
<title>DPO-Shift: Shifting the Distribution of Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.07599</link>
<guid>https://arxiv.org/abs/2502.07599</guid>
<content:encoded><![CDATA[
Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:43:42 GMT</pubDate>
</item>
<item>
<title>TransMLA：提升语言模型通信效率的新方法</title>
<link>https://arxiv.org/abs/2502.07864</link>
<guid>https://arxiv.org/abs/2502.07864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransMLA方法有效提升语言模型的通信效率与推理速度。</p><br /><br /><p><strong>摘要：</strong> 现代的大型语言模型在当前硬件上经常面临沟通瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在关键值（KV）层使用低秩矩阵，允许压缩的潜在KV状态被缓存，从而大幅度减少KV缓存大小，提升推理速度。尽管MLA在Deepseek V2/V3/R1中表现出效率和有效性，许多主要模型供应商仍然依赖于组查询注意力（GQA）。本文展示了GQA可以通过MLA进行始终保持相同KV缓存开销的表示，但反之则不成立。为促进MLA的广泛应用，我们引入了**TransMLA**，一种将广泛使用的基于GQA的预训练模型（如LLaMA、Qwen、Mixtral）转换为基于MLA模型的后训练方法。转换后的模型可以在不增加KV缓存大小的情况下经过额外训练来提高表达能力。此外，我们还计划开发MLA特定的推理加速技术，以保持转化模型的低延迟，进一步提升Deepseek R1的提炼效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:41:19 GMT</pubDate>
</item>
<item>
<title>可学习的合规性放弃：提高大规模语言模型的决策可靠性</title>
<link>https://arxiv.org/abs/2502.06884</link>
<guid>https://arxiv.org/abs/2502.06884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的合规性放弃方法，以提升LLM/VLM的可靠决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为可学习合规性放弃的方法，旨在提高安全关键应用中大规模语言模型（LLM）和视觉-语言模型（VLM）的决策可靠性。传统的合规性预测方法在阈值设定上过于静态，难以适应任务复杂性和数据分布的变化。为此，本文将强化学习融入合规性预测，动态优化放弃阈值，从而在最小化预测集大小的同时，确保可靠的覆盖率。经过在多个LLM/VLM基准上的广泛评估，研究表明，该方法在准确性、幻觉检测的AUROC和不确定性引导选择生成方面均优于现有方法，并显著降低了校准误差。这些改进在多种模型和数据集上均表现出色，且始终满足90%的覆盖目标，确立了该方法作为安全关键应用中可靠决策的更有效和灵活的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 18:40:34 GMT</pubDate>
</item>
<item>
<title>Pippo: 从单张照片生成高分辨率密集视频的多视角扩散模型</title>
<link>https://arxiv.org/abs/2502.07785</link>
<guid>https://arxiv.org/abs/2502.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pippo模型可从一张照片生成高分辨率的人物视频，展现出色的多视角表现。</p><br /><br /><p><strong>摘要：</strong> Pippo是一种先进的生成模型，能够仅通过一张随意拍摄的照片生成分辨率达到1000的密集人物视频。该模型采用了多视角扩散变换器，且无需额外输入，如参数模型或图像拍摄的相机参数。欲使模型有效学习，Pippo在与3亿幅无标签人像图像预训练后，进行多视角的中期训练与后期训练，快速吸收工作室数据集。中期训练中，用于去噪的低分辨率视图数量可达48个；后期训练则使用像素对齐控制，提升3D一致性的生成。在推理阶段，Pippo通过注意力偏置技巧，能够生成超过训练时5倍的视图数量。此外，团队还提出了一种改进的3D一致性评估标准，表明Pippo在单图像多视角生成人物方面的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 13:41:46 GMT</pubDate>
</item>
<item>
<title>Hypencoder：一种新型请求编码器提升文档检索性能</title>
<link>https://arxiv.org/abs/2502.05364</link>
<guid>https://arxiv.org/abs/2502.05364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Hypencoder，一个基于神经网络的请求编码器，显著提升文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 传统的检索模型通常依赖向量内积来生成查询与文档之间的相关性评分，限制了评分的表现力。本文提出一种新范式，使用小型神经网络作为学习的相关性函数，而非生成向量来表示查询。该神经网络以文档的表示为输入，输出标量相关性评分。我们应用超网络（hypernetwork）生成该神经网络的权重，称之为Hypencoder。通过在领域内的搜索任务进行实验，Hypencoder的表现显著超越了强大的稠密检索模型，并在重排序模型和规模更大的模型中取得了更高的指标。此外，Hypencoder在领域外检索任务中也展现出良好的泛化能力。为进一步评估其能力，我们对一系列困难检索任务进行了评测，包括“想不起来的检索”和“遵循指令的检索”，结果表明，与标准检索任务相比，性能差距显著扩大。最后，我们实现了一种近似搜索算法，展示该模型能够在60毫秒内搜索880万份文档。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 10:35:08 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover：开源自动化数学证明生成的最优语言模型</title>
<link>https://arxiv.org/abs/2502.07640</link>
<guid>https://arxiv.org/abs/2502.07640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover是一个实现数学证明生成最佳性能的开源语言模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover是一种开源大型语言模型，专门用于自动化数学问题的形式化证明生成，达到了该领域的最优性能。为了解决形式化数学语句和证明稀缺的问题，研究团队训练了语句形式化工具，将自然语言数学问题转化为形式语言（Lean 4），创建了一个包含164万个形式语句的数据集。利用大型语言模型，检查这些形式语句准确保留了原始自然语言问题的内容。然后，团队通过训练一系列证明者，迭代构建了一个大型形式证明数据集。每个新的证明者都成功证明了前一个证明者无法解决的许多语句，并将这些新证明追加到下一轮训练集中。最终的证明者在整体证明生成中超越了现有所有开源模型，在miniF2F基准测试中达到了57.6%的成功率，领先于之前的最佳开源模型7.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:56:56 GMT</pubDate>
</item>
<item>
<title>通过稀疏自编码器理解与控制视觉模型</title>
<link>https://arxiv.org/abs/2502.06755</link>
<guid>https://arxiv.org/abs/2502.06755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，通过稀疏自编码器理解和控制视觉模型。</p><br /><br /><p><strong>摘要：</strong> 为了深入理解视觉模型，我们不仅需要解释其学习的特征，还需通过控制实验验证这些解释。当前的方法要么提供可解释的特征但不能测试其因果影响，要么允许模型编辑但没有可解释的控制。我们提出了一个统一框架，利用稀疏自编码器（SAEs）弥补这一空白，能够发现可人类解释的视觉特征，并精确操纵这些特征以测试模型行为的假设。通过对最先进的视觉模型应用该方法，我们揭示了不同预训练目标的模型在语义抽象学习上的关键差异，并展示了我们框架在多个视觉任务中的实际应用。我们的研究表明，SAEs能够可靠地识别和操纵可解释的视觉特征，而无需对模型进行重新训练，这为理解和控制视觉模型行为提供了强大的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:54:39 GMT</pubDate>
</item>
<item>
<title>基于链式选片的长视频理解优化</title>
<link>https://arxiv.org/abs/2502.06428</link>
<guid>https://arxiv.org/abs/2502.06428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出链式选片方法以优化长视频的任务相关镜头选择。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大语言模型在处理长视频时面临的视觉token过多问题，提出了链式选片（CoS）方法。传统的视频采样方法难以平衡关键细节与冗余内容的选择，导致模型对视频理解的偏差。CoS方法将镜头选择视为测试时视觉提示的优化，通过优化镜头与任务的对齐来选择适合视频理解的镜头。其核心包括：一个二元视频摘要机制，用于发现任务相关镜头，以及一个视频共推理模块，利用二元编码将相关镜头与不相关镜头进行学习对齐。实验结果表明，CoS在多个基线和数据集上的有效性和适应性得到验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:51:18 GMT</pubDate>
</item>
<item>
<title>深入探讨模型架构与超参数对缩放法则的影响</title>
<link>https://arxiv.org/abs/2502.06857</link>
<guid>https://arxiv.org/abs/2502.06857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨模型架构及超参数对缩放法则的影响，发布开放数据集Gemstones。</p><br /><br /><p><strong>摘要：</strong> 本文研究使用广泛的模型架构和超参数选择来探讨缩放法则的适用性，强调这些选择对结果中的建议产生的重要影响。作为研究的主要成果，我们发布了Gemstones，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个检查点，这些变换器模型的参数量高达20亿，采用不同的学习率、冷却周期和架构形状进行训练。我们的数据集支持更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。通过分析模型组合的各个方面，我们发现缩放法则的建议对实验设计过程和使用的具体模型检查点非常敏感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:18:08 GMT</pubDate>
</item>
<item>
<title>基于检索增强生成的金融时间序列预测框架</title>
<link>https://arxiv.org/abs/2502.05878</link>
<guid>https://arxiv.org/abs/2502.05878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的框架，通过有效检索提升金融时间序列预测精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的检索增强生成（RAG）框架，用于金融时间序列预测，解决了现有检索方法在处理复杂金融分析中的不足。该框架的核心创新包括：以一亿参数的大型语言模型（StockLLM）作为基础，采用新颖的候选选择方法并利用LLM反馈，以及通过最大化查询与历史重要序列相似度的训练目标，来提高检索效果。新构建的数据集融合了金融指标和历史股价，确保了FinSeer的有效训练与评估。实验结果表明，该RAG框架在BIGDATA22上实现了比StockLLM和随机检索更高的预测精度，FinSeer在现有检索方法中表现优异，准确率提高了8%。此研究突显了定制化检索模型在金融预测中的重要性，并为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:10:24 GMT</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction：提升大型语言模型信息检索能力</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MEAP方法，显著提升大型语言模型的关键信息检索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Mask-Enhanced Autoregressive Prediction（MEAP）的方法，通过将遮蔽语言建模（MLM）与下一词预测（NTP）结合，显著改善大型语言模型在关键信息检索和长上下文推理任务中的表现。MEAP随机遮蔽输入令牌的一小部分，然后利用解码器Transformer进行自回归的下一词预测，避免了对双向注意力或编码-解码架构的依赖，从而在预训练和推理阶段没有额外的计算开销。实验表明，MEAP在关键检索和长上下文推理任务上优于NTP，并且在常识推理任务上表现相当或更优。在有监督微调中，MEAP在“中间缺失”情境下表现优异，超越NTP达11.77个百分点。分析显示，MEAP能够提升可区分的注意力评分，增强了模型对与任务相关信号的关注，同时减少了外围上下文的影响。这些结果表明，MEAP是大型语言模型训练中的一种有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 06:55:30 GMT</pubDate>
</item>
<item>
<title>参数化技能扩展与组合框架PSEC的研究</title>
<link>https://arxiv.org/abs/2502.05932</link>
<guid>https://arxiv.org/abs/2502.05932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出参数化技能扩展与组合框架PSEC，用于提高自主智能体的技能扩展效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了参数化技能扩展与组合框架PSEC，旨在提升自主智能体在应对新挑战时的技能扩展效率。传统方法在扩展新技能时训练效率低下，未能充分利用已有知识。PSEC通过维护可管理的技能库，以低秩适配（LoRA）模块的形式逐步集成技能原语，实现参数高效的微调，促进灵活的技能扩展。此外，该框架通过合并不同技能的LoRA模块在参数空间中直接进行技能组合，利用技能间的共享信息高效编程新技能。文章还提出了一种上下文感知模块，能够动态激活不同技能，以协同处理新任务。经过在D4RL、DSRL基准和DeepMind控制套件上的实验，结果表明PSEC在有效利用已有知识以应对新挑战以及扩大技能库方面表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:53:50 GMT</pubDate>
</item>
<item>
<title>Eclair: 一种高效的文档级光学字符识别工具</title>
<link>https://arxiv.org/abs/2502.04223</link>
<guid>https://arxiv.org/abs/2502.04223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eclair是一款优化的文本提取工具，旨在提高文档的理解和处理能力。</p><br /><br /><p><strong>摘要：</strong> Eclair是一种新型的光学字符识别（OCR）工具，专门设计用于处理多种文档类型。它不仅能从图像中提取文本，还能识别文档的结构和语义信息，如格式、公式、表格以及多个块的阅读顺序。这些功能对于文档查询、问题回答及训练大语言模型和视觉语言模型至关重要。通过引入人类标注的基准测试，Eclair在文档级OCR及语义分类上达到了最先进的准确率，超越了其他方法。此外，Eclair在多个现有基准上表现出色，展现了其广泛的应用潜力和强大的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:25:54 GMT</pubDate>
</item>
<item>
<title>FailSafeQA：评估金融领域LLM的鲁棒性与上下文意识的新基准</title>
<link>https://arxiv.org/abs/2502.06329</link>
<guid>https://arxiv.org/abs/2502.06329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新的金融基准FailSafeQA，旨在测试LLM的鲁棒性与上下文意识。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个新的长上下文金融基准FailSafeQA，旨在测试大型语言模型（LLM）在面对六种人机交互变化时的鲁棒性和上下文意识。我们聚焦于查询失败和上下文失败这两个案例研究，分别通过改变查询的领域专业性、完整性和语言准确性，及模拟上传降级、无关和空文档来进行测试。使用LLM-as-a-Judge方法及Qwen2.5-72B-Instruct模型，定义并计算24种现成模型的鲁棒性、上下文基础和合规性评分。结果表明，尽管某些模型在应对输入扰动方面表现出色，但在提供鲁棒答案时必须谨慎避免虚构信息的出现。Palmyra-Fin-128k-Instruct在合规性上表现最优，但在17%的测试案例中难以维持鲁棒预测；而OpenAI o3-mini则在41%案例中虚构了信息。这些结果表明，即使是高表现的模型仍有显著改进空间，并强调了FailSafeQA在金融应用中优化LLM可靠性的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 02:51:41 GMT</pubDate>
</item>
<item>
<title>FocalCodec：一种高效的低比特率语音编解码器</title>
<link>https://arxiv.org/abs/2502.04465</link>
<guid>https://arxiv.org/abs/2502.04465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FocalCodec是一种低比特率语音编解码器，解决现有方法的信息损失问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过自监督预训练在海量数据集上推动了自然语言处理的革命。受此启发，研究者开始将这种方法应用于语音，通过使用神经音频编解码器将连续音频离散为令牌。然而，现有方法面临高比特率、语义或声学信息损失等限制，并且为了同时捕获这两者，常常依赖于多代码簿设计，增加了下游任务的复杂性。为此，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，利用单一二进制代码簿在0.16到0.65 kbps之间压缩语音。FocalCodec在语音重合成和声音转换方面提供与当前最先进技术相媲美的性能，同时有效处理多语言语音和噪声环境。在下游任务的评估中，FocalCodec能够保持足够的语义和声学信息，且非常适合生成建模。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 01:31:44 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型的代码生成能力</title>
<link>https://arxiv.org/abs/2502.03492</link>
<guid>https://arxiv.org/abs/2502.03492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出CTRL框架以提升代码生成模型的输出效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在代码生成中的自我批评能力，提出了一种名为CTRL的框架，用于通过强化学习训练批评模型生成反馈，旨在最大化针对固定生成模型的修正效果，而无需人工干预。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并有效减轻了基础和更强生成模型的错误叠加。此外，这些批评模型还作为准确的生成奖励模型，支持测试时通过迭代批评修正进行扩展，在挑战性的代码生成基准上实现了高达106.1%的相对提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:55:37 GMT</pubDate>
</item>
<item>
<title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title>
<link>https://arxiv.org/abs/2502.07701</link>
<guid>https://arxiv.org/abs/2502.07701</guid>
<content:encoded><![CDATA[
In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:27:13 GMT</pubDate>
</item>
<item>
<title>Chameleon Benchmark Overfit Detector：评估大型语言模型的真实理解能力</title>
<link>https://arxiv.org/abs/2502.07445</link>
<guid>https://arxiv.org/abs/2502.07445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">C-BOD框架揭示LLM依赖于表面线索而非真实理解的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Chameleon Benchmark Overfit Detector (C-BOD)，一种通过参数化变换系统性扭曲基准提示的元评估框架，以检测大型语言模型（LLMs）的过拟合现象。通过对输入的重新措辞，同时保持其语义内容和标签，C-BOD可以揭示模型的性能是否源自记忆模式。我们对26个领先的LLM在MMLU基准上的评估显示，经过适度扰动后，平均表现下降2.15%，其中20个模型表现出统计显著差异。研究发现，基线准确率较高的模型在扰动下表现差异较大，而更大的LLM对重新措辞表现出更高的敏感性，这表明它们可能过于依赖固定提示模式。相比之下，Llama系列模型和较低基线准确率的模型显示出微不足道的降幅，暗示其对表面线索的依赖减少。此外，C-BOD具有数据集和模型无关的设计，便于集成到训练流程中，促进更强的语言理解。我们的发现挑战社区超越排行榜分数，重视LLM评估中的抗干扰性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:22:50 GMT</pubDate>
</item>
<item>
<title>VidCRAFT3: 一种精准控制多视觉元素的图像到视频生成框架</title>
<link>https://arxiv.org/abs/2502.07531</link>
<guid>https://arxiv.org/abs/2502.07531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidCRAFT3框架实现了对多个视觉元素的精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VidCRAFT3的创新框架，旨在实现精准的图像到视频生成，同时控制摄像机运动、物体运动和光照方向。通过提出空间三重注意力变换器，VidCRAFT3能够对每个视觉元素进行松耦合控制。由于大多数现实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），其中包含光照方向标注和多样化的物体外观，确保VidCRAFT3有效处理强光传输和反射效应。此外，我们提出了一种三阶段训练策略，消除了对多视觉元素同时注释的训练数据的需求。广泛的实验结果显示，VidCRAFT3在生成高质量视频内容方面的表现优于现有的最先进方法，尤其在控制精度和视觉一致性上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:21:13 GMT</pubDate>
</item>
<item>
<title>CAD-Editor: 基于文本的计算机辅助设计编辑框架</title>
<link>https://arxiv.org/abs/2502.03997</link>
<guid>https://arxiv.org/abs/2502.03997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAD-Editor 是一个创新的基于文本的 CAD 编辑框架，提升了设计模型修改的效率。</p><br /><br /><p><strong>摘要：</strong> CAD-Editor 是一种创新的框架，专注于文本驱动的计算机辅助设计（CAD）模型编辑，旨在克服传统方法在文本控制和现有CAD模型应用中的局限性。该框架通过自动化数据合成管道生成原始与编辑模型的配对，并利用大型视觉语言模型（LVLMs）将它们的差异总结为编辑指令。此外，CAD-Editor 采用了定位-再填充的方法，将任务拆分为两大子任务：定位待修改区域与填充适当编辑。依赖于大型语言模型（LLMs）的强大自然语言理解和CAD知识，实验结果表明，CAD-Editor 在定量和定性性能上均表现优异，证明了其在文本式CAD编辑领域的潜力和应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:16:28 GMT</pubDate>
</item>
<item>
<title>Enhance-A-Video: 一种提升DiT生成视频的一体化方法</title>
<link>https://arxiv.org/abs/2502.07508</link>
<guid>https://arxiv.org/abs/2502.07508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种训练无关的方法，提升DiT视频生成的连贯性与质量。</p><br /><br /><p><strong>摘要：</strong> DiT基础的视频生成已经取得显著成果，但现有模型的增强研究仍较为欠缺。本文介绍了一种名为Enhance-A-Video的无训练方法，旨在提升DiT生成视频的连贯性和质量。其核心思想是增强基于非对角时间注意力分布的跨帧相关性。由于设计简单，该方法可轻松应用于大多数DiT视频生成框架，而无需任何重新训练或微调。在多种DiT视频生成模型上，我们的方法在时间一致性和视觉质量方面均表现出良好的改善。我们希望这项研究能激发未来在视频生成增强方面的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:14:10 GMT</pubDate>
</item>
<item>
<title>大语言模型中的提示缓存引发的隐私泄露风险</title>
<link>https://arxiv.org/abs/2502.07776</link>
<guid>https://arxiv.org/abs/2502.07776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提示缓存导致的时间差异可能引发隐私泄露问题。</p><br /><br /><p><strong>摘要：</strong> 在大语言模型(LLMs)中，提示缓存产生的数据依赖性时延差异可能导致隐私泄露，这意味着缓存的提示处理速度快于非缓存的提示。若缓存在不同用户之间共享，攻击者能够通过快速的API响应时间识别出缓存提示，从而获取其他用户的信息。为此，我们开展了对现实世界LLM API提供者的统计审计，以检测提示缓存。结果发现，七家API提供者（包括OpenAI）之间存在全球性缓存共享，可能导致用户提示的隐私泄露。此外，由于提示缓存导致的时延变化还可能泄露模型架构的信息，我们发现OpenAI的嵌入模型是一个仅解码的Transformer，这一信息之前并未公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:11:49 GMT</pubDate>
</item>
<item>
<title>Nature语言模型：跨领域科学发现的基础模型</title>
<link>https://arxiv.org/abs/2502.07527</link>
<guid>https://arxiv.org/abs/2502.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NatureLM是一个跨科学领域的基础模型，推动科学发现的潜力。</p><br /><br /><p><strong>摘要：</strong> NatureLM是一个新的序列基础科学模型，旨在推动科学发现，通过在多个科学领域的数据上进行预训练，实现了跨领域的生成与设计应用。该模型能够生成和优化小分子、蛋白质、RNA和材料，支持蛋白质到小分子和蛋白质到RNA的跨领域生成，以及在SMILES到IUPAC翻译及USPTO-50k的逆合成任务中实现最先进的性能。NatureLM以不同的模型规模（10亿、80亿和467亿参数）进行开发，且随着模型规模的增大，性能明显改进，展现出在药物发现、材料设计和治疗蛋白或核苷酸开发等领域的广泛应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:10:26 GMT</pubDate>
</item>
<item>
<title>Hephaestus-Forge：提升LLM代理的预训练数据集</title>
<link>https://arxiv.org/abs/2502.06589</link>
<guid>https://arxiv.org/abs/2502.06589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hephaestus-Forge是首个针对LLM代理的预训练数据集，显著提升其基本能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强大型语言模型（LLM）代理在API功能调用、内在推理和规划能力方面的基本能力。Hephaestus-Forge包含1030亿个代理特定数据，涵盖76,537个API，提供工具文档以传授API功能知识及功能调用轨迹以强化内在推理。通过研究规模法则以确定最佳数据混合比率，持续在Hephaestus-Forge上进行预训练，Hephaestus在三个代理基准测试中超越小型到中型开源LLM，并与商业LLM相抗衡，证明了该预训练语料库在提升LLM代理基本能力及其对新任务或环境的泛化能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:04:08 GMT</pubDate>
</item>
<item>
<title>超大规模预训练视觉语言模型的实证研究</title>
<link>https://arxiv.org/abs/2502.07617</link>
<guid>https://arxiv.org/abs/2502.07617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨1000亿实例的预训练对多文化任务的影响。</p><br /><br /><p><strong>摘要：</strong> 本文对在前所未有的1000亿实例规模上预训练视觉语言模型的潜力进行了实证研究。研究发现，尽管传统的西方分类和检索基准（如COCO Captions）在这一规模上模型表现趋于饱和，但在文化多样性相关任务中，利用1000亿规模的网络数据取得了显著提升，特别是对长尾概念的覆盖。此外，研究分析了模型的多语言能力，低资源语言也表现出提升。值得注意的是，通过使用CLIP等质量过滤器减少预训练数据集的规模，虽然通常被认为有助于提升性能，但却可能无意中降低了大规模数据集中所表现的文化多样性。结果表明，尽管传统基准未能显著受益于扩大到1000亿实例的噪声原始网络数据，这一数据规模在构建真正包容的多模态系统中至关重要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:03:08 GMT</pubDate>
</item>
<item>
<title>CodeI/O：提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2502.07316</link>
<guid>https://arxiv.org/abs/2502.07316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeI/O 提出新方法，通过代码输入输出预测提升推理能力。</p><br /><br /><p><strong>摘要：</strong> CodeI/O 是一种新颖的方法，旨在系统性地提炼多样的推理模式，以应对大语言模型在推理任务中的挑战。通过将原始代码转化为代码输入-输出预测格式，并训练模型在自然语言下预测这些输入和输出，CodeI/O 能够帮助模型掌握通用的推理原理，如逻辑流规划、状态空间搜索、决策树遍历和模块化分解。这种方法有效地将结构化推理与特定代码的语法解耦，确保了程序的严格性。实验结果表明，CodeI/O 在多个推理任务上取得了一致的提升，包括符号、科学、逻辑、数学与数字推理以及常识推理。通过匹配现有的真实输出或重新执行代码以验证预测，CodeI/O++ 实现了更高的性能，促进了多轮修订的思考链。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:00:20 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中长链推理的训练与结构探索</title>
<link>https://arxiv.org/abs/2502.07374</link>
<guid>https://arxiv.org/abs/2502.07374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现长链推理在大规模语言模型中依赖于结构而非内容。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大规模语言模型（LLM）如何通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）实现长链推理（Long CoT）。通过仅使用17,000个长链推理培训样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中较大幅度提升了性能，与竞争对手的模型相比表现优异。研究表明，长链推理的结构在学习过程中至关重要，而单个推理步骤的内容对性能影响有限。这意味着纠错样本或去除推理关键词等内容扰动对准确性影响不大，相较之下，破坏逻辑一致性的结构性修改会显著降低准确性。这些发现为观察和提升LLM推理能力提供了新的视角，并为未来推理模型的高效训练提供了关键考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:58:37 GMT</pubDate>
</item>
<item>
<title>大语言模型强化学习在编码与推理任务中的应用</title>
<link>https://arxiv.org/abs/2502.06807</link>
<guid>https://arxiv.org/abs/2502.06807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习提升大语言模型在复杂编码推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习应用于大语言模型（LLMs）对复杂编码和推理任务表现的显著提升。通过比较两种通用推理模型——OpenAI o1和o3的早期检查点，以及一个特定领域系统o1-ioi，后者采用手工设计的推理策略，旨在参加2024国际信息学奥林匹克竞赛（IOI）。o1-ioi在IOI 2024现场比赛中，通过手工测试策略，取得了第49百分位的成绩，而在放宽的比赛约束下，获得了金牌。然而，更新的模型o3在没有手工领域特定策略或放宽约束的情况下，同样获得金牌。这些发现表明，虽然像o1-ioi这样的专用管道显著提高了表现，但扩大规模的通用o3模型在没有依赖手工推理启发式的情况下，能够超越这些成果。整体结果显示，扩大通用强化学习的规模，而不是依赖特定领域的技术，是实现推理领域尖端人工智能的可靠路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:53:19 GMT</pubDate>
</item>
</channel>
</rss>