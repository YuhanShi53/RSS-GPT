<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>Phi-Ground模型提升GUI接地性能，推动计算机使用代理发展</title>
<link>https://arxiv.org/abs/2507.23779</link>
<guid>https://arxiv.org/abs/2507.23779</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Phi-Ground在GUI接地任务中取得突破性进展。</p><br><br><p><strong>摘要：</strong> 随着多模态推理模型的发展，计算机使用代理（CUAs）正逐渐成为现实。GUI接地是实现实际操作的核心技术，直接影响系统成败。当前的端到端接地模型在ScreenSpot-pro和UI-Vision等挑战性基准上的准确率不足65%，难以部署。本文通过实证研究，从数据收集到模型训练进行了深入探讨，最终提出了Phi-Ground模型家族，在所有五个接地基准测试中表现优异，尤其在10B参数以下的代理设置中达到最先进水平。在端到端模型设置中，Phi-Ground在ScreenSpot-pro和UI-Vision上的得分分别为43.2和27.2，展示了其在实际应用中的潜力。论文还总结了模型构建的经验与教训，对其他感知任务具有借鉴意义。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23779 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:59:09 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 13:59:09 GMT</pubDate>
</item>
<item>
<title>强化学习在3D环境中的空间推理与泛化能力研究</title>
<link>https://arxiv.org/abs/2507.23698</link>
<guid>https://arxiv.org/abs/2507.23698</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>强化学习提升3D环境中视觉运动代理的泛化能力。</p><br><br><p><strong>摘要：</strong> 本文探讨了强化学习（RL）在提升视觉运动代理空间推理和交互能力方面的潜力，特别是在Minecraft等3D环境中的应用。研究提出通过跨视角目标规范构建统一的多任务目标空间，并利用Minecraft的高可定制性实现自动化任务生成，以解决多任务RL中的表示挑战。同时，构建了一个高效的分布式RL框架用于大规模训练。实验结果表明，RL显著提升了交互成功率，实现了对未见过环境的零样本泛化，展示了其在增强视觉运动代理空间推理能力方面的巨大潜力。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23698 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 12:20:02 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 12:20:02 GMT</pubDate>
</item>
<item>
<title>ViLLA框架提升机器人操作策略的泛化能力</title>
<link>https://arxiv.org/abs/2507.23682</link>
<guid>https://arxiv.org/abs/2507.23682</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>ViLLA框架提升机器人操作策略的泛化能力。</p><br><br><p><strong>摘要：</strong> 本文介绍了ViLLA-X，一种基于视觉-语言-隐动作（latent action）的新型框架，用于学习具有泛化能力的机器人操作策略。该框架改进了隐动作的学习方式及其在预训练中的整合方法，在多个模拟环境和真实机器人平台上表现出色。研究认为，ViLLA范式具有重要潜力，为未来相关研究提供了坚实基础。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23682 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:57:46 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 11:57:46 GMT</pubDate>
</item>
<item>
<title>软最大化注意力机制的递归形式及其表达能力分析</title>
<link>https://arxiv.org/abs/2507.23632</link>
<guid>https://arxiv.org/abs/2507.23632</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究软最大化注意力的递归形式以解释其优越性。</p><br><br><p><strong>摘要：</strong> 本文通过推导软最大化注意力的递归形式，揭示了其与线性注意力之间的差异。研究表明，软最大化注意力在查询和键的内积上具有更优的非线性特性，从而提升了模型的表达能力。通过将软最大化注意力描述为递归神经网络的形式，可以更好地理解其各个组件的作用及相互关系，进而解释为何软最大化注意力在下游任务中表现更优。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23632 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 11:10:03 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 11:10:03 GMT</pubDate>
</item>
<item>
<title>基于KAN的双教师知识蒸馏艺术风格分类方法</title>
<link>https://arxiv.org/abs/2507.23436</link>
<guid>https://arxiv.org/abs/2507.23436</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>使用KAN改进双教师框架，提升艺术风格分类性能。</p><br><br><p><strong>摘要：</strong> 本文针对艺术风格分类中数据标注不足和风格特征复杂的问题，提出一种基于Kolmogorov-Arnold Networks (KANs) 的双教师知识蒸馏方法。该方法通过替换传统MLP投影层为KANs，增强对非线性特征关联的建模能力，同时保留两个教师网络的互补信息：一个关注局部纹理与笔触，另一个捕捉整体风格层次。实验表明，该方法在WikiArt和Pandora18k数据集上优于原有双教师架构，提升了Top-1准确率和线性探针性能。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23436 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 07:16:00 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>面向阿拉伯语的增强型密集段落检索框架</title>
<link>https://arxiv.org/abs/2507.23404</link>
<guid>https://arxiv.org/abs/2507.23404</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出一种针对阿拉伯语的增强DPR框架，提升问答系统性能。</p><br><br><p><strong>摘要：</strong> 阿拉伯语由于其复杂的形态学、可选变音符号以及现代标准阿拉伯语与方言共存的特点，给自然语言处理和信息检索带来了挑战。尽管阿拉伯语在全球的重要性日益增加，但在NLP研究和基准资源中仍处于劣势。本文提出了一种专为阿拉伯语设计的增强型密集段落检索（DPR）框架，核心是引入了注意力相关性评分（ARS）机制，以更有效地建模问题与段落之间的语义相关性。该方法结合了预训练阿拉伯语语言模型和架构优化，显著提升了阿拉伯语问答任务的排名准确性。相关代码已公开在GitHub上。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23404 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 06:18:28 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 06:18:28 GMT</pubDate>
</item>
<item>
<title>NeRF-GS：融合NeRF与3D高斯散射的新型框架</title>
<link>https://arxiv.org/abs/2507.23374</link>
<guid>https://arxiv.org/abs/2507.23374</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>NeRF-GS结合NeRF与3DGS，提升3D场景表示性能。</p><br><br><p><strong>摘要：</strong> 本文提出NeRF-GS，一种将神经辐射场（NeRF）与3D高斯散射（3DGS）联合优化的新框架。该框架利用NeRF的连续空间表示，解决3DGS在高斯初始化敏感性、空间感知有限和高斯间相关性弱等方面的问题，从而提升其性能。NeRF-GS通过逐步对齐3DGS的空间特征与NeRF，使两者在共享3D空间信息的基础上共同优化。同时，通过优化隐式特征和高斯位置的残差向量，增强3DGS的个性化能力。实验结果表明，NeRF-GS在基准数据集上表现优于现有方法，达到最先进水平，验证了NeRF与3DGS的互补性，为高效3D场景表示提供了新思路。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.23374 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 05:43:31 GMT</pubDate>
<pubDate>Thu, 31 Jul 2025 05:43:31 GMT</pubDate>
</item>
<item>
<title>TARS：一种改进多模态大语言模型幻觉的偏好优化方法</title>
<link>https://arxiv.org/abs/2507.21584</link>
<guid>https://arxiv.org/abs/2507.21584</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>TARS提升多模态模型推理可靠性，减少幻觉。</p><br><br><p><strong>摘要：</strong> 本文提出TARS，一种基于token自适应的偏好优化策略，旨在解决多模态大语言模型在视觉-语言推理中产生的幻觉问题。传统直接偏好优化方法容易过拟合文本线索，导致对视觉信息的依赖不足。TARS通过最小最大优化框架，在保持语义约束的前提下，增强模型对因果视觉信息的感知能力，从而有效降低幻觉率。实验表明，TARS仅使用4800个偏好样本即可显著提升模型表现，优于标准DPO并接近GPT-4o水平。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.21584 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 04:39:19 GMT</pubDate>
<pubDate>Tue, 29 Jul 2025 04:39:19 GMT</pubDate>
</item>
<item>
<title>基于人格向量的大型语言模型行为分析与控制</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究通过人格向量分析和控制语言模型的行为变化。</p><br><br><p><strong>摘要：</strong> 本文探讨了大型语言模型在部署过程中行为变化的潜在原因，通过分析模型激活空间中的人格向量（persona vectors），识别出如邪恶、奉承和幻觉倾向等特质。研究发现，这些向量可用于监控模型人格的变化，并预测和控制训练过程中的性格转变。研究还提出了一种预防性引导方法，以减少或避免不期望的性格变化。此外，人格向量还能用于检测可能导致不良性格变化的训练数据。该方法自动化程度高，只需自然语言描述即可提取人格向量。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.21509 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 01:20:14 GMT</pubDate>
<pubDate>Tue, 29 Jul 2025 01:20:14 GMT</pubDate>
</item>
<item>
<title>农业视觉语言模型评估基准AgroBench的引入与分析</title>
<link>https://arxiv.org/abs/2507.20519</link>
<guid>https://arxiv.org/abs/2507.20519</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>AgroBench用于评估农业VLM模型性能，揭示其在细粒度识别中的不足。</p><br><br><p><strong>摘要：</strong> 本文介绍了AgroBench，一个用于评估视觉语言模型（VLM）在农业任务中表现的基准测试平台。该基准涵盖七个农业主题，包括203种作物和682种病害分类，由专家农艺师标注。研究发现，尽管VLM在某些任务中表现良好，但在细粒度识别如杂草识别方面仍存在较大提升空间。作者分析了VLM的错误类型，并提出了未来改进方向。相关数据和代码已公开。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.20519 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 00:58:29 GMT</pubDate>
<pubDate>Mon, 28 Jul 2025 00:58:29 GMT</pubDate>
</item>
<item>
<title>时间对称性在序列模型中的应用研究</title>
<link>https://arxiv.org/abs/2507.14793</link>
<guid>https://arxiv.org/abs/2507.14793</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究时间对称性在序列模型中的应用及优势。</p><br><br><p><strong>摘要：</strong> 本文探讨了如何将等变性理论应用于序列模型，特别是针对时间参数化的对称性。传统RNN在处理动态变换时表现不佳，而引入时间流等变性后，模型在训练速度、长度泛化和速度泛化方面均有显著提升。该研究为构建更符合现实世界时间对称性的序列模型提供了新思路。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.14793 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 22:52:21 GMT</pubDate>
<pubDate>Sat, 19 Jul 2025 22:52:21 GMT</pubDate>
</item>

<item>
<title>Seed-Prover：基于形式验证的数学定理证明模型</title>
<link>https://arxiv.org/abs/2507.23726</link>
<guid>https://arxiv.org/abs/2507.23726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed-Prover在数学定理证明中取得重大突破。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Seed-Prover，一个基于形式验证和长链式推理的数学定理证明模型。该模型通过Lean反馈、已证明引理和自我总结不断优化证明过程，并设计了三种测试时推理策略以应对IMO级别的竞赛问题。Seed-Prover成功证明了78.1%的过去IMO问题，超越了之前的最先进水平。此外，作者还开发了Seed-Geometry几何推理引擎，提升了Lean在几何领域的支持能力。该系统在IMO 2025中成功解决了5道题目，展示了自动化数学推理的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.23726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 13:00:30 GMT</pubDate>
</item>
<item>
<title>构建多语言语音对话模型评估基准</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一个包含中英文的语音对话模型评估数据集。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于语音对话模型（SDMs）的实际效果，指出其在理解与模拟人类对话方面仍存在研究不足。相较于文本大语言模型，语音交互更具复杂性，如歧义、语境依赖等问题。为推动SDM发展，作者构建了一个包含1079个实例的中英文数据集，并引入基于大语言模型的评估方法，以更贴近人类判断的方式全面评估SDM的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>基于用户意图的下一代推荐系统RecGPT</title>
<link>https://arxiv.org/abs/2507.22879</link>
<guid>https://arxiv.org/abs/2507.22879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecGPT通过用户意图驱动提升推荐系统效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种以用户意图为核心的下一代推荐系统RecGPT，旨在解决传统推荐系统依赖历史交互数据导致的过滤气泡和长尾问题。RecGPT通过整合大语言模型，在用户兴趣挖掘、物品检索和解释生成等关键阶段实现意图驱动的推荐。该系统采用多阶段训练策略，并引入人机协作的评估机制，有效提升了推荐系统的多样性与用户满意度。目前RecGPT已在淘宝App全面部署，实验结果表明其在用户、商家和平台三方均取得显著性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Step-3：面向解码效率优化的超大规模视觉语言模型</title>
<link>https://arxiv.org/abs/2507.19427</link>
<guid>https://arxiv.org/abs/2507.19427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-3通过硬件感知设计显著提升长上下文推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Step-3，一个拥有321B参数的视觉语言模型，采用硬件感知的模型与系统协同设计，以最小化解码成本。该模型在两个关键方面进行创新：一是多矩阵分解注意力机制（MFA），有效降低KV缓存和计算量；二是注意力-前馈网络解耦（AFD），将注意力层与前馈网络分离为专用子系统。实验表明，Step-3在长上下文任务中相比DeepSeek-V3和Qwen3 MoE 235B等模型表现出更高的成本效率，并在Hopper GPU上实现了每秒4,039个token的解码吞吐量，创下新纪录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 12:53:13 GMT</pubDate>
</item>
<item>
<title>DreamScene：基于自然语言的高质量可编辑3D场景生成框架</title>
<link>https://arxiv.org/abs/2507.13985</link>
<guid>https://arxiv.org/abs/2507.13985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamScene实现从文本生成高质量、一致且可编辑的3D场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DreamScene，一个端到端的3D场景生成框架，能够根据自然语言或对话生成高质量、可编辑的3D场景。该框架首先通过GPT-4代理进行场景规划，构建混合图结构；随后利用基于图的布局算法生成无碰撞的结构化布局。接着，通过Formation Pattern Sampling（FPS）生成物体几何，并采用渐进式相机采样策略确保全局一致性。最后，系统支持对象移动、外观修改和4D动态运动等精细编辑功能。实验表明，DreamScene在质量、一致性和灵活性方面优于现有方法，为开放域3D内容创作提供实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 10:45:54 GMT</pubDate>
</item>
<item>
<title>MetaCLIP 2：在多语言网络数据上训练的对比语言-图像预训练模型</title>
<link>https://arxiv.org/abs/2507.22062</link>
<guid>https://arxiv.org/abs/2507.22062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaCLIP 2提升多语言图像文本任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MetaCLIP 2，这是首个从全球网络规模图像-文本对中从头训练的对比语言-图像预训练模型。针对非英语数据的筛选和多语言模型性能下降的问题，研究者通过最小改动进行严格消融实验，提出一种能够同时利用英语和非英语数据的训练方法。在零样本ImageNet分类任务中，MetaCLIP 2 ViT-H/14优于其仅限英语的版本和mSigLIP，且在多语言基准测试中取得了新的最先进结果，如CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600图像到文本检索达到64.3%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>MixGRPO：提升图像生成中人类偏好对齐效率的新框架</title>
<link>https://arxiv.org/abs/2507.21802</link>
<guid>https://arxiv.org/abs/2507.21802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MixGRPO通过混合采样策略提升图像生成模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MixGRPO，一种结合随机微分方程（SDE）和常微分方程（ODE）的新型框架，用于改进图像生成中的人类偏好对齐。该方法引入滑动窗口机制，仅在窗口内使用SDE采样和GRPO引导优化，减少优化开销并加速收敛。同时支持高阶求解器，进一步提升了训练效率。实验表明，MixGRPO在多个维度上优于现有方法，训练时间减少了近50%，而其变体MixGRPO-Flash更是将训练时间降低了71%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:40:09 GMT</pubDate>
</item>
<item>
<title>VL-Cogito：基于多阶段渐进式强化学习的多模态推理模型</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VL-Cogito通过PCuRL框架提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出VL-Cogito，一个基于多阶段渐进式强化学习（PCuRL）框架的多模态推理模型。该框架通过逐步增加任务难度，有效提升了模型在不同领域和复杂度下的推理能力。PCuRL引入了在线难度软加权机制和动态长度奖励机制，使模型能根据任务复杂度自适应调整推理路径长度，从而提高推理效率与准确性。实验结果表明，VL-Cogito在数学、科学、逻辑和通用理解等主流多模态基准测试中表现优异，验证了其方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 08:23:21 GMT</pubDate>
</item>
<item>
<title>基于强化学习的差分隐私优化框架RLDP提升语言模型性能</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLDP通过强化学习优化差分隐私，提升语言模型效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLDP框架，将差分隐私优化视为闭环控制问题，利用深度强化学习动态调整梯度裁剪阈值和噪声大小。该方法在多个语言模型上实现显著的困惑度降低和下游任务性能提升，同时保持隐私保护承诺，并减少训练所需的梯度更新次数。实验表明，RLDP在保证隐私的前提下提升了模型效果和训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 06:46:53 GMT</pubDate>
</item>
<item>
<title>提出OmniAVS数据集与OISA模型推动多模态指代音频视觉分割研究</title>
<link>https://arxiv.org/abs/2507.22886</link>
<guid>https://arxiv.org/abs/2507.22886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniAVS提升多模态音频视觉分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了OmniAVS数据集，包含2098个视频和59458条多模态指代表达，具有8种多模态表达形式、强调音频内容理解以及融入复杂推理与常识知识的特点。同时引入了OISA模型，利用多模态大语言模型进行细粒度音频视觉内容理解和推理分割。实验表明，OISA在OmniAVS上表现优于现有方法，并在其他相关任务中取得良好效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>基于测试前置的自动化程序修复方法 Repair-R1</title>
<link>https://arxiv.org/abs/2507.22853</link>
<guid>https://arxiv.org/abs/2507.22853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Repair-R1通过测试前置提升程序修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Repair-R1 的自动化程序修复方法，该方法将测试用例引入模型训练阶段，并在修复前生成区分性测试用例，以提高缺陷定位和修复效果。该方法采用强化学习联合优化测试生成与修复过程，在四个基准测试中表现出色，相比传统方法，修复成功率提升了2.68%至48.29%，测试覆盖率提升了0.78%至53.96%。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 13:24:05 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的UI到代码自动化转换方法</title>
<link>https://arxiv.org/abs/2507.22827</link>
<guid>https://arxiv.org/abs/2507.22827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多阶段UI-to-code框架，提升代码生成准确性和可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于多智能体框架的UI到前端代码自动化转换方法，该框架分为三个可解释的阶段：接地、规划和生成。接地代理通过视觉语言模型识别UI组件，规划代理利用前端工程先验构建层次布局，生成代理通过自适应提示合成HTML/CSS代码。该方法相比端到端黑盒方法更具鲁棒性、可解释性和准确性。此外，研究还扩展了该框架为可扩展的数据引擎，自动生成大规模图像-代码对，并用于微调开源视觉语言模型，显著提升了UI理解与代码质量。实验表明，该方法在布局准确性、结构连贯性和代码正确性方面均达到最新水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 12:41:21 GMT</pubDate>
</item>
<item>
<title>Falcon-H1：混合架构大语言模型系列提升性能与效率</title>
<link>https://arxiv.org/abs/2507.22448</link>
<guid>https://arxiv.org/abs/2507.22448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Falcon-H1采用混合架构，性能超越多款大模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Falcon-H1系列大语言模型，该模型采用Transformer与状态空间模型（SSM）的混合架构，优化了性能与效率。Falcon-H1提供了多种参数规模的版本，包括0.5B、1.5B、3B、7B和34B等，并支持量化版本。其性能在多个任务中表现优异，甚至在较小参数规模下超越了更大模型。Falcon-H1支持多语言和长上下文处理，适用于广泛的应用场景。所有模型均以开源方式发布，推动AI研究的开放与共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 03:55:33 GMT</pubDate>
</item>
<item>
<title>BANG：一种基于生成爆炸动态的3D对象分解方法</title>
<link>https://arxiv.org/abs/2507.21493</link>
<guid>https://arxiv.org/abs/2507.21493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BANG实现3D对象的直观分解与生成，提升3D设计效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出BANG，一种新型的生成式方法，用于3D对象的部件级分解。其核心是“生成爆炸动态”，通过平滑的爆炸状态序列逐步分离物体部件，同时保持几何和语义一致性。BANG利用预训练的扩散模型，并结合轻量级爆炸视图适配器和时间注意力模块，实现精确控制和流畅过渡。用户可通过空间提示（如边界框和表面区域）指定分解内容，还可结合多模态模型如GPT-4进行2D到3D操作。BANG支持详细部件生成、功能描述关联及组件感知的3D制造流程，适用于3D打印等领域，使创意概念更易转化为详细3D资产。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:21:21 GMT</pubDate>
</item>
<item>
<title>基于生成AI的航空图像车辆检测域适应方法</title>
<link>https://arxiv.org/abs/2507.20976</link>
<guid>https://arxiv.org/abs/2507.20976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用生成AI合成数据提升航空图像车辆检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于生成AI的新型方法，用于改善航空图像中车辆检测模型在不同地理区域间的泛化能力。通过使用微调的潜在扩散模型（LDMs）生成高质量的合成图像及其标注，实现多阶段、多模态的知识迁移，从而缩小源域与目标域之间的分布差异。实验表明，该方法在多个航空图像数据集上显著优于监督学习、弱监督适应、无监督域适应和开放集检测方法。此外，研究者还发布了两个新的新西兰和犹他州的航空图像数据集，以支持相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:38:06 GMT</pubDate>
</item>
<item>
<title>ChemDFM-R：提升化学领域推理能力的大型语言模型</title>
<link>https://arxiv.org/abs/2507.21990</link>
<guid>https://arxiv.org/abs/2507.21990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChemDFM-R在化学领域实现最先进的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对化学领域中大型语言模型理解深度和推理能力不足的问题，提出ChemDFM-R模型。该模型通过构建原子化知识数据集增强化学基础理解，并采用混合来源的知识蒸馏与领域特定强化学习策略，显著提升了化学推理能力。实验表明，ChemDFM-R在多个化学基准测试中表现优异，且输出具有可解释性和逻辑性，增强了人机协作中的可靠性与实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 12:40:49 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型诚实行为的系统评估与基准测试</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大模型在无法回答视觉问题时的诚实行为。</p><br /><br /><p><strong>摘要：</strong> 本文首次对多模态大语言模型（MLLMs）的诚实行为进行了系统评估。研究基于模型对无法回答的视觉问题的响应行为，定义了四种代表性类型的问题，并构建了一个包含12000多个样本的多模态诚实基准（MoHoBench），通过多阶段过滤和人工验证确保数据质量。实验发现，大多数模型在面对无法回答的问题时未能正确拒绝回答，且模型的诚实行为不仅受语言建模影响，还与视觉信息密切相关。为此，研究者尝试使用监督学习和偏好学习方法进行初步对齐，以提升模型的诚实表现，为未来构建可信的多模态大语言模型奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21503" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:55:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的CUDA自动优化框架CUDA-L1</title>
<link>https://arxiv.org/abs/2507.14111</link>
<guid>https://arxiv.org/abs/2507.14111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CUDA-L1通过强化学习实现高效CUDA优化，提升多GPU架构性能。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型对GPU计算资源需求的激增，传统CUDA优化方法已难以满足需求。本文提出CUDA-L1，一个基于强化学习的自动化CUDA优化框架。该框架在NVIDIA A100上训练，平均提升了KernelBench中250个CUDA内核的性能达17.7倍，最高可达449倍。CUDA-L1不仅在多种GPU架构（如H100、RTX 3090等）上表现出色，还能发现多种优化技术并进行策略性组合，揭示CUDA优化的基本原则，并识别潜在性能瓶颈。该方法无需人工干预，仅通过速度奖励信号即可将低效的LLM转化为高效的CUDA优化器，具有广泛的适用性和推广价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:43:56 GMT</pubDate>
</item>
<item>
<title>基于运动引导的少样本视频目标分割研究</title>
<link>https://arxiv.org/abs/2507.22061</link>
<guid>https://arxiv.org/abs/2507.22061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MOVE数据集与DMA方法提升运动引导的视频目标分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对运动引导的少样本视频目标分割（FSVOS）问题，提出一个大规模的MOVE数据集，以弥补现有数据集在动态运动理解上的不足。通过评估多种先进方法，发现当前技术在处理运动引导任务时存在明显不足。为此，作者提出一种新的基线方法DMA，实验表明该方法在少样本运动理解任务中表现优异，为后续研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>基于强化学习的离散自回归模型在图像与语言生成中的应用</title>
<link>https://arxiv.org/abs/2507.22058</link>
<guid>https://arxiv.org/abs/2507.22058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升离散自回归模型的图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Omni的框架，结合了语义图像分词器、统一的自回归模型以及离线扩散解码器，利用强化学习有效减少了图像生成中的伪影，显著提升了生成质量。该方法在使用7B语言模型时表现出色，在图像生成任务中达到了最先进的性能，能够高质量地生成图像，并具备良好的指令遵循能力和长文本渲染能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.22058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>HunyuanWorld 1.0：融合文本与图像生成沉浸式3D场景的新框架</title>
<link>https://arxiv.org/abs/2507.21809</link>
<guid>https://arxiv.org/abs/2507.21809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HunyuanWorld 1.0实现高质量3D场景生成，支持交互与兼容现有图形管线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HunyuanWorld 1.0，一种结合视频与3D方法优势的新框架，用于从文本和图像生成沉浸式、可探索和交互的3D场景。该框架具有三个核心优势：通过全景世界代理实现360度沉浸体验、支持网格导出以兼容现有图形管线、以及分离的对象表示增强交互性。其核心是语义分层的3D网格表示，利用全景图像作为语义感知的世界分解和重建工具，从而生成多样化的3D世界。实验表明，该方法在生成连贯、可探索和交互的3D场景方面表现优异，并适用于虚拟现实、物理模拟、游戏开发和互动内容创作等多个领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 09:43:35 GMT</pubDate>
</item>
<item>
<title>深度学习在非洲野生动物图像分类中的应用与评估</title>
<link>https://arxiv.org/abs/2507.21364</link>
<guid>https://arxiv.org/abs/2507.21364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较了多种深度学习模型在非洲野生动物分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了深度学习技术在非洲野生动物图像分类中的应用，重点评估了DenseNet-201、ResNet-152、EfficientNet-B4和Vision Transformer ViT-H/14等模型的性能。实验结果显示，DenseNet-201在卷积网络中表现最佳，准确率为67%，而ViT-H/14虽然准确率高达99%，但计算成本较高。研究强调了准确率、资源消耗和部署可行性之间的权衡，并将DenseNet-201集成到Hugging Face Gradio Space中，展示了轻量级模型在野外应用的可行性。该工作为非洲本土的AI研究提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 18:18:13 GMT</pubDate>
</item>
<item>
<title>AnimalClue：首个基于间接证据的物种识别大规模数据集</title>
<link>https://arxiv.org/abs/2507.20240</link>
<guid>https://arxiv.org/abs/2507.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimalClue是首个用于从间接证据中识别物种的大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnimalClue，这是首个专注于通过动物足迹、粪便、卵、骨骼和羽毛等间接证据进行物种识别的大规模数据集。该数据集包含159,605个边界框，涵盖5类间接证据，覆盖968个物种、200个科和65个目。每张图像均带有物种级标签、边界框或分割掩码，以及详细的特征信息。与以往主要关注直接视觉特征的数据集不同，AnimalClue在分类、检测和实例分割任务中面临更复杂的挑战，因其需要识别更细微的视觉特征。研究团队对多个视觉模型进行了评估，并指出了从痕迹中识别动物的关键挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 07:48:03 GMT</pubDate>
</item>
<item>
<title>基于最大后验估计的偏好优化方法MaPPO</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaPPO提升语言模型与人类偏好的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Maximum a Posteriori Preference Optimization (MaPPO)的框架，用于从偏好中学习，该方法通过将先验奖励知识整合到优化目标中，改进了传统偏好优化方法。相比DPO及其变体，MaPPO不仅扩展了这一范式，还通过避免过于简化的二分类响应方式提升了对齐效果。此外，MaPPO无需额外超参数，支持离线和在线设置，并可作为插件与SimPO、IPO、CPO等方法结合使用。实验表明，在多个基准测试中，MaPPO在保持计算效率的同时显著提升了对齐性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 01:26:50 GMT</pubDate>
</item>
<item>
<title>基于用户目标状态追踪的对话模拟器研究</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGST框架提升对话模拟器的目标对齐能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对对话人工智能中用户模拟器在多轮对话中难以保持目标导向行为的问题，提出了用户目标状态追踪（UGST）框架。该框架通过跟踪用户目标进展，结合三阶段方法开发能够自主追踪目标并生成符合目标响应的用户模拟器。研究还建立了全面的评估指标，并在两个基准数据集上验证了方法的有效性，显著提升了模拟器的表现。该成果填补了对话AI领域的关键空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 03:07:12 GMT</pubDate>
</item>
<item>
<title>SAND-Math：提升数学推理大语言模型性能的合成数据生成方法</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAND-Math提升数学推理LLM性能，显著优于现有数据集。</p><br /><br /><p><strong>摘要：</strong> 随着对具备复杂数学推理能力的大语言模型（LLMs）需求增加，训练数据的稀缺成为发展瓶颈。本文提出SAND-Math，一种通过生成高质量数学问题并逐步提升难度的合成数据生成管道。实验表明，使用SAND-Math数据可使模型在AIME25基准测试中性能提升17.85个百分点，并通过难度提升步骤将平均问题难度从5.02提高到5.98，进一步提升模型表现。该方法为构建更强大、高效的数学推理LLM提供了实用且可扩展的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 01:17:48 GMT</pubDate>
</item>
<item>
<title>提升主观推理能力的多角色增强框架研究</title>
<link>https://arxiv.org/abs/2507.20187</link>
<guid>https://arxiv.org/abs/2507.20187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多角色框架提升主观推理准确性和多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出MultiRole-R1，一个通过引入多角色视角来增强大型推理模型在主观任务中表现的框架。该框架采用无监督数据生成方法，结合多样化的角色推理链，并利用强化学习中的组相对策略优化技术，将多样性作为奖励信号，以提高推理的准确性和多样性。实验表明，该方法在多个基准测试中均表现出色，展示了多样性增强训练在大型推理模型中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Jul 2025 05:07:42 GMT</pubDate>
</item>
<item>
<title>自演化智能体：从静态模型到动态适应的范式转变</title>
<link>https://arxiv.org/abs/2507.21046</link>
<guid>https://arxiv.org/abs/2507.21046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨自演化智能体的发展与应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统回顾了自演化智能体的研究进展，聚焦于其在任务、时机和方法上的演化机制。文章分析了智能体组件（如模型、记忆、工具）的进化方式，分类讨论了不同阶段的适应方法，并探讨了算法与架构设计对演化的影响。此外，文章还评估了相关指标与基准，展示了在编程、教育和医疗等领域的应用，并指出了安全性、可扩展性和协同演化等关键挑战。该研究为构建自适应智能系统提供了框架和方向，助力实现人工智能超级智能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>GenoMAS：结合LLM的基因表达分析新方法</title>
<link>https://arxiv.org/abs/2507.21035</link>
<guid>https://arxiv.org/abs/2507.21035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenoMAS提升基因表达分析效率与准确性。</p><br /><br /><p><strong>摘要：</strong> GenoMAS是一种基于大型语言模型（LLM）的基因表达分析系统，通过集成多个专业化LLM代理，结合结构化工作流与自主适应能力，有效处理复杂的转录组数据。该系统在GenoTEX基准测试中表现出色，数据预处理的综合相似性相关系数达到89.13%，基因识别的F_1值为60.48%，优于现有方法。此外，GenoMAS能发现具有生物学意义的基因-表型关联，并调整潜在混杂因素，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>基于超长输出强化学习的大型语言模型推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UloRL提升LLM推理能力，显著提高训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UloRL的超长输出强化学习方法，旨在提升大型语言模型的推理能力。针对传统强化学习框架在处理超长输出时的低效问题，UloRL通过将输出解码分为短段来提高训练效率，并引入动态掩码机制防止熵崩溃。实验结果表明，该方法在Qwen3-30B-A3B模型上提升了2.06倍的训练速度，并在AIME2025和BeyondAIME任务中分别提高了14.2%和11.2%的性能，效果优于更大型的Qwen3-235B-A22B模型。研究展示了UloRL在超长序列生成中的潜力，并计划开源代码和模型供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 23:42:33 GMT</pubDate>
</item>
<item>
<title>ScenePainter：解决3D场景生成语义漂移问题的新框架</title>
<link>https://arxiv.org/abs/2507.19058</link>
<guid>https://arxiv.org/abs/2507.19058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScenePainter提升3D场景生成的语义一致性与连贯性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScenePainter，一种用于长期且语义一致的3D场景生成框架。现有方法在生成连续视角序列时存在语义漂移问题，主要由于出图模块的累积偏差。ScenePainter通过引入层次化场景概念图（SceneConceptGraph）来构建多层级场景概念之间的关系，指导出图模块生成一致的新视角，并可动态优化以增强多样性。实验表明，该框架有效解决了语义漂移问题，提升了3D视图序列的一致性和沉浸感。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 04:21:12 GMT</pubDate>
</item>
<item>
<title>基于隐式两阶段训练的多变量天气预测方法</title>
<link>https://arxiv.org/abs/2507.17189</link>
<guid>https://arxiv.org/abs/2507.17189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法提升天气预测准确性。</p><br /><br /><p><strong>摘要：</strong> 文章针对全球气候变化导致极端天气频发的问题，提出一种基于隐式两阶段训练的多变量天气预测方法。该方法通过为每个变量配置独立的编码器和解码器，在第一阶段学习共享潜在空间，第二阶段由翻译器捕捉变量间交互关系，并引入自注意力机制进行多变量融合，显著提升了预测性能。实验表明，该方法在近地面气温和相对湿度预测上分别降低了28.82%和23.39%的均方误差。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:26:56 GMT</pubDate>
</item>
<item>
<title>基于校准奖励的强化学习提升语言模型推理可靠性</title>
<link>https://arxiv.org/abs/2507.16806</link>
<guid>https://arxiv.org/abs/2507.16806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLCR方法提升语言模型推理准确性与置信度校准。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLCR（Reinforcement Learning with Calibration Rewards）方法，通过在强化学习中引入Brier评分来优化语言模型的置信度估计，从而提升模型的准确性和置信度校准。该方法在生成预测的同时输出数值化置信度，并利用结合二进制正确性评分和Brier评分的奖励函数进行训练。实验表明，RLCR在多个数据集上显著提升了置信度校准效果，且不影响准确性，优于传统强化学习和事后置信度分类器。此外，测试时可通过置信度加权方法进一步提高准确性和校准度。结果表明，显式优化置信度有助于构建更可靠的推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于表示空间的多任务学习方法Rep-MTL</title>
<link>https://arxiv.org/abs/2507.21049</link>
<guid>https://arxiv.org/abs/2507.21049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rep-MTL通过表示空间优化提升多任务学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的多任务学习方法Rep-MTL，该方法通过分析任务间的表示空间交互，利用熵惩罚和样本级跨任务对齐来优化任务间的信息共享。与传统依赖损失缩放和梯度调整的方法不同，Rep-MTL关注任务特定优化与共享表示学习之间的相互作用，旨在减少负迁移并增强互补信息的传递。实验表明，即使在简单的等权重策略下，Rep-MTL也能在多个多任务学习基准上取得良好的性能，且具有较高的效率。此外，幂律指数分析进一步验证了其在平衡任务学习与跨任务共享方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:28 GMT</pubDate>
</item>
<item>
<title>4D空间智能重建的多层级方法综述</title>
<link>https://arxiv.org/abs/2507.21045</link>
<guid>https://arxiv.org/abs/2507.21045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出5层4D空间智能重建框架，分析各阶段挑战与发展方向。</p><br /><br /><p><strong>摘要：</strong> 本文综述了从视觉观测中重建4D空间智能的研究进展，提出了一个五层结构的方法体系，涵盖低级3D属性、场景组件、动态场景、交互建模以及物理约束的重建。文章分析了每一层级的关键挑战，并指出了未来研究方向。作者还维护了一个项目页面以跟踪该领域的最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GPT-IMAGE-EDIT-1.5M：推动指令引导图像编辑的开源数据集</title>
<link>https://arxiv.org/abs/2507.21033</link>
<guid>https://arxiv.org/abs/2507.21033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开源图像编辑数据集提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GPT-IMAGE-EDIT-1.5M，一个包含150万组高质量指令-源图-编辑图的公开图像编辑数据集。该数据集通过GPT-4o生成并优化了三个主流图像编辑数据集，提升了视觉质量和指令对齐度。实验表明，基于该数据集微调的模型在多个基准测试中表现优异，显著缩小了与专有模型的差距，为开放研究提供了重要支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.21033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>SmallThinker：本地设备上高效运行的大语言模型</title>
<link>https://arxiv.org/abs/2507.20984</link>
<guid>https://arxiv.org/abs/2507.20984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SmallThinker在本地设备上实现高性能大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SmallThinker，一种专为本地设备设计的大型语言模型家族。与传统依赖GPU云基础设施的模型不同，SmallThinker从零开始构建，以适应计算能力弱、内存有限和存储速度慢的限制。其创新点包括两层稀疏结构、预注意力路由器和NoPE-RoPE混合稀疏注意力机制，显著提升了计算效率和内存利用率。SmallThinker-4B-A0.6B和SmallThinker-21B-A3B在性能上达到或超过更大的模型，并且在普通CPU上即可高效运行，无需昂贵的GPU硬件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 12:45:14 GMT</pubDate>
</item>
<item>
<title>ARC-Hunyuan-Video：提升短视频多模态理解能力的模型</title>
<link>https://arxiv.org/abs/2507.20939</link>
<guid>https://arxiv.org/abs/2507.20939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARC-Hunyuan-Video提升短视频多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ARC-Hunyuan-Video，一个能够处理视频中视觉、音频和文本信息的多模态模型，旨在解决真实世界短视频理解中的挑战。该模型支持多粒度时间戳视频摘要、开放式视频问答、时间视频定位和视频推理等功能。通过高质量数据和多种训练策略，模型在多个任务上表现出色，并在实际部署中提升了用户参与度和满意度。其高效性也得到验证，一分钟后视频的推理时间仅需10秒。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 11:52:36 GMT</pubDate>
</item>
<item>
<title>Music Arena：开放平台推动文本到音乐模型的人类偏好评估</title>
<link>https://arxiv.org/abs/2507.20900</link>
<guid>https://arxiv.org/abs/2507.20900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Music Arena提供实时人类偏好评估，提升文本到音乐模型的评价标准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Music Arena，一个用于文本到音乐（TTM）模型大规模人类偏好评估的开放平台。通过让用户输入文本提示并比较不同系统的输出，平台收集真实用户的偏好数据，形成排行榜。Music Arena不仅遵循其他AI领域的评估趋势，还特别设计了适合音乐的特性，如基于LLM的路由系统和详细偏好数据收集。此外，平台采用滚动数据发布政策，确保用户隐私和数据透明性。该平台旨在解决TTM领域缺乏统一评估标准的问题，并展示如何将实时评估适配到特定AI领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:52:57 GMT</pubDate>
</item>
<item>
<title>基于流匹配的JAM模型实现歌词到歌曲的精细控制</title>
<link>https://arxiv.org/abs/2507.20880</link>
<guid>https://arxiv.org/abs/2507.20880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM模型实现歌词到歌曲的词级时序控制，提升音乐生成质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种基于流匹配的歌词到歌曲生成模型JAM，该模型首次实现了对歌词中每个词的时序和持续时间的精细控制，满足音乐创作中的需求。为了提升生成歌曲的质量，研究者采用了直接偏好优化方法进行美学对齐，无需人工标注即可迭代优化模型。此外，作者还构建了公开评估数据集JAME，以标准化此类模型的评价体系。实验结果显示，JAM在音乐特定属性上优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 10:34:02 GMT</pubDate>
</item>
<item>
<title>GMPO：一种更稳定的大型语言模型策略优化方法</title>
<link>https://arxiv.org/abs/2507.20673</link>
<guid>https://arxiv.org/abs/2507.20673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GMPO通过几何均值优化提升模型稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何均值策略优化（GMPO）的方法，作为Group Relative Policy Optimization (GRPO) 的改进版本。GMPO通过最大化token级奖励的几何均值，减少了对异常值的敏感性，从而提升了策略更新的稳定性。实验表明，GMPO在多个数学和多模态推理基准测试中表现优于GRPO，如AIME24、AMC、MATH500等，平均性能提升4.1%。研究还提供了理论分析和实验验证，证明了GMPO的设计优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Jul 2025 05:54:05 GMT</pubDate>
</item>
<item>
<title>RICE：提升区域级视觉与OCR能力的对比学习方法</title>
<link>https://arxiv.org/abs/2507.20025</link>
<guid>https://arxiv.org/abs/2507.20025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RICE提升区域级视觉和OCR能力，适用于多种密集预测任务。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RICE的新方法，旨在增强区域级别的视觉和OCR能力。通过构建大规模候选区域数据集，并引入区域Transformer层来提取丰富的区域语义，RICE设计了一个统一的区域聚类判别损失，支持对象和OCR学习。该方法在分割、密集检测和多模态大语言模型的视觉感知任务中表现出色，且已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.20025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 13:47:09 GMT</pubDate>
</item>
<item>
<title>基于可验证奖励的强化学习算法ARPO提升多轮语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARPO算法提升LLM在多轮工具交互中的推理表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Agentic Reinforced Policy Optimization (ARPO)的新型强化学习算法，旨在提升大语言模型（LLM）在多轮工具交互任务中的表现。研究发现，LLM在使用外部工具后会产生更高的生成不确定性，因此ARPO引入了基于熵的自适应采样机制，动态平衡全局轨迹采样与步骤级采样，以增强高不确定性步骤的探索能力。同时，通过优势归因估计，使LLM能够更好地理解多轮工具使用中的策略差异。实验表明，ARPO在多个计算推理、知识推理和深度搜索任务中优于现有轨迹级强化学习算法，并且仅需一半的工具使用预算即可实现更优性能，为LLM代理在实时动态环境中的对齐提供了可扩展解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 03:53:11 GMT</pubDate>
</item>
<item>
<title>基于前景感知的文档图像校正方法研究</title>
<link>https://arxiv.org/abs/2507.19804</link>
<guid>https://arxiv.org/abs/2507.19804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ForCenNet模型提升文档图像几何校正效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Foreground-Centric Network (ForCenNet) 的新方法，用于消除文档图像中的几何变形，以提高文本识别的准确性。该方法通过提取前景元素作为几何参考，并引入前景感知的标签生成、掩码机制和曲率一致性损失，有效提升了模型对布局元素如文本行和表格边框的校正能力。实验表明，ForCenNet在多个真实世界基准数据集上取得了最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Jul 2025 01:36:48 GMT</pubDate>
</item>
<item>
<title>GEPA：利用自然语言反思提升LLM任务优化的提示优化器</title>
<link>https://arxiv.org/abs/2507.19457</link>
<guid>https://arxiv.org/abs/2507.19457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEPA通过自然语言反思实现高效任务优化，显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出GEPA（Genetic-Pareto）提示优化器，利用自然语言反思来提升大型语言模型（LLM）的任务适应能力。与传统基于稀疏奖励的强化学习方法相比，GEPA通过分析系统级轨迹并进行自然语言诊断，能够高效地生成和测试提示更新。实验表明，GEPA在四个任务中平均比GRPO提升10%，最高达20%，且使用 rollouts 数量减少35倍。同时，GEPA在两个LLM上优于MIPROv2，展现出在代码优化中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:42:32 GMT</pubDate>
</item>
<item>
<title>前沿人工智能模型的风险评估与管理</title>
<link>https://arxiv.org/abs/2507.16534</link>
<guid>https://arxiv.org/abs/2507.16534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">前沿AI模型风险评估揭示关键风险领域。</p><br /><br /><p><strong>摘要：</strong> 本文基于Frontier AI Risk Management Framework，通过E-T-C分析方法评估了前沿人工智能模型的七类关键风险，包括网络攻击、生物化学风险、说服与操控、自主AI研发、战略欺骗、自我复制和合谋。通过AI-45°法则设定红黄线阈值，将风险划分为绿色（可管理）、黄色（需加强控制）和红色（需暂停开发）。实验结果显示，当前所有前沿AI模型均处于绿色和黄色区域，未触及红色警戒线。其中，多数模型在自我复制和战略欺骗方面仍处于绿色区域，而说服与操控风险则普遍处于黄色区域。文章呼吁共同应对AI发展带来的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:44:38 GMT</pubDate>
</item>
<item>
<title>MMBench-GUI：跨平台GUI自动化代理评估基准</title>
<link>https://arxiv.org/abs/2507.19478</link>
<guid>https://arxiv.org/abs/2507.19478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMBench-GUI评估GUI自动化代理的多平台能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMBench-GUI，这是一个用于评估跨Windows、macOS、Linux、iOS、Android和Web平台的GUI自动化代理的分层基准。该基准包含四个层次：GUI内容理解、元素定位、任务自动化和任务协作，涵盖了GUI代理的核心技能。文章还提出了一种新的效率-质量面积（EQA）指标，用于评估在线自动化场景中的执行效率。研究发现，准确的视觉定位是任务成功的关键，模块化框架结合专用定位模块具有显著优势。此外，可靠的GUI自动化需要强大的任务规划和跨平台泛化能力，长上下文记忆、广泛的操作空间和长期推理至关重要。任务效率仍是一个被忽视的维度，所有模型都存在显著低效问题。精确定位、有效规划和早期停止策略的整合对实现高效可扩展的GUI自动化不可或缺。相关代码、数据和环境将公开在GitHub上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.19478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Jul 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>GPTQ与格点算法的数学等价性研究</title>
<link>https://arxiv.org/abs/2507.18553</link>
<guid>https://arxiv.org/abs/2507.18553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPTQ被证明与格点最近平面算法等价，具有理论保障。</p><br /><br /><p><strong>摘要：</strong> 本文揭示了GPTQ在反向执行时与Babai最近平面算法在数学上的等价性。这种等价性基于线性层输入的Hessian矩阵定义的格点问题，为GPTQ提供了几何解释和误差上界保证。这一发现不仅深化了对GPTQ的理解，还为未来大规模语言模型的量化算法设计提供了理论基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:22:18 GMT</pubDate>
</item>
<item>
<title>基于LLM的错误分析工具CLEAR提升模型评估深度</title>
<link>https://arxiv.org/abs/2507.18392</link>
<guid>https://arxiv.org/abs/2507.18392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEAR提供交互式错误分析，揭示模型性能背后的具体原因。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型的评估多依赖其他模型进行打分或排序，但这种方式仅能判断哪个模型更好，无法解释原因。为解决这一问题，研究者提出了CLEAR，一个开源的LLM错误分析工具。CLEAR能够生成逐实例的文本反馈，并识别系统级别的错误问题，同时量化各类问题的出现频率。该工具还提供了交互式仪表盘，支持通过可视化图表、过滤器和实例分析进行深入错误探究。研究展示了CLEAR在RAG和数学基准测试中的应用，并通过用户案例验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:15:21 GMT</pubDate>
</item>
<item>
<title>Specification Self-Correction: 提升语言模型对规范漏洞的自我修正能力</title>
<link>https://arxiv.org/abs/2507.18742</link>
<guid>https://arxiv.org/abs/2507.18742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SSC框架帮助语言模型在推理过程中自我修正规范漏洞，提升任务准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Specification Self-Correction (SSC) 的新框架，使语言模型能够在推理过程中识别并修正自身所依据的规范中的缺陷。该方法通过多步骤推理：首先基于可能有误的规范生成响应，然后对该响应进行批判性分析，并最终修改规范以消除可被利用的漏洞。实验表明，使用 SSC 后，模型在创意写作和代理编码任务中因规范漏洞而偏离用户意图的情况减少了超过 90%。该方法无需修改模型权重，仅在推理阶段完成动态修复，显著提升了模型行为的鲁棒性和对齐度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 14:44:28 GMT</pubDate>
</item>
<item>
<title>基于纯视觉的高效端到端自动驾驶架构PRIX</title>
<link>https://arxiv.org/abs/2507.17596</link>
<guid>https://arxiv.org/abs/2507.17596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRIX通过纯视觉实现高效自动驾驶，无需LiDAR和BEV表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PRIX的端到端自动驾驶架构，仅依赖摄像头数据，无需昂贵的LiDAR传感器和复杂的BEV特征表示。该架构采用视觉特征提取器和生成式规划头，直接从原始像素预测安全轨迹。核心组件Context-aware Recalibration Transformer (CaRT) 提升了多级视觉特征的鲁棒性。实验表明，PRIX在NavSim和nuScenes基准测试中表现优异，性能接近大型多模态扩散规划器，但推理速度更快、模型更小，适合大规模部署。项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 11:28:23 GMT</pubDate>
</item>
<item>
<title>TTD-DR：基于扩散过程的深度研究生成框架</title>
<link>https://arxiv.org/abs/2507.16075</link>
<guid>https://arxiv.org/abs/2507.16075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTD-DR通过迭代优化提升长文本研究报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Test-Time Diffusion Deep Researcher (TTD-DR) 的新框架，旨在解决大型语言模型在生成复杂、长篇研究报告时性能受限的问题。该框架模仿人类研究的迭代过程，将报告生成视为一个扩散过程，从初步草稿出发，通过不断‘去噪’和引入外部信息进行逐步优化。同时，其核心流程结合了自进化算法，以提高生成内容的质量与连贯性。实验表明，TTD-DR在多个需要深度搜索和多跳推理的任务中表现优异，超越了现有研究代理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:23:21 GMT</pubDate>
</item>
<item>
<title>AI视频聊天：实时通信的新范式与优化框架</title>
<link>https://arxiv.org/abs/2507.10510</link>
<guid>https://arxiv.org/abs/2507.10510</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI视频聊天面临延迟挑战，Artic框架提升实时性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了AI视频聊天作为实时通信新范式的潜力，指出由于多模态大语言模型（MLLM）推理耗时长，导致视频传输延迟成为瓶颈。为此，研究提出了Artic框架，通过上下文感知视频流和抗丢包自适应帧率技术，减少带宽消耗并提升AI理解能力。同时，构建了首个Degraded Video Understanding Benchmark（DeViBench）评估视频质量对MLLM的影响，并讨论了未来的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10510" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:34:49 GMT</pubDate>
</item>
<item>
<title>基于双空间建模的局部相关视频检索方法</title>
<link>https://arxiv.org/abs/2507.17402</link>
<guid>https://arxiv.org/abs/2507.17402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HLFormer框架提升视频与文本的局部相关性匹配。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频与文本查询之间部分内容匹配的问题，提出了一种基于双空间建模的局部相关视频检索方法HLFormer。该方法利用双曲空间学习弥补欧几里得空间在层次结构建模上的不足，通过集成洛伦兹注意力模块和欧几里得注意力模块，结合动态特征融合机制，增强了跨模态匹配效果。同时引入部分序保留损失函数，强化文本与视频内容之间的局部相关性。实验表明，该方法优于现有最佳模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 06:59:46 GMT</pubDate>
</item>
<item>
<title>提升多模态上下文学习能力的动态注意力重分配方法</title>
<link>https://arxiv.org/abs/2507.15807</link>
<guid>https://arxiv.org/abs/2507.15807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出DARA方法提升多模态模型的上下文学习能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了当前多模态大语言模型在多模态上下文学习（MICL）中的局限性，指出其过度依赖文本模式而忽视视觉信息。为解决这一问题，作者提出了动态注意力重分配（DARA）策略，以增强模型对视觉信息的关注。同时，构建了TrueMICL数据集，专门用于评估多模态任务中的真实上下文学习能力。实验表明，该方法显著提升了模型的多模态适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:08:18 GMT</pubDate>
</item>
<item>
<title>Iwin Transformer：一种无需位置嵌入的层次化视觉Transformer</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Iwin Transformer通过创新机制实现高效图像处理与任务表现。</p><br /><br /><p><strong>摘要：</strong> Iwin Transformer是一种无需位置嵌入的层次化视觉Transformer，能够从低分辨率直接微调到高分辨率。其核心在于结合交错窗口注意力和深度可分离卷积，使全局信息在单一模块内交换，克服了Swin Transformer需要两个连续块才能近似全局注意力的限制。实验表明，Iwin Transformer在图像分类、语义分割和视频动作识别等任务中表现出色，尤其在ImageNet-1K上达到87.4%的top-1准确率。此外，其核心组件可作为独立模块用于条件图像生成，并为未来研究如Iwin 3D Attention提供启发。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 09:45:48 GMT</pubDate>
</item>
<item>
<title>Group Sequence Policy Optimization: 提升大语言模型训练效率与稳定性的强化学习算法</title>
<link>https://arxiv.org/abs/2507.18071</link>
<guid>https://arxiv.org/abs/2507.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSPO提升大语言模型训练效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Group Sequence Policy Optimization (GSPO)，一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型。GSPO不同于以往基于token级重要性比率的算法，而是基于序列似然定义重要性比率，并进行序列级裁剪、奖励和优化。实验表明，GSPO在训练效率和性能上优于GRPO算法，尤其在稳定Mixture-of-Experts（MoE）强化学习训练方面表现突出，同时简化了强化学习基础设施的设计。GSPO的优势显著提升了最新Qwen3模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 23:50:32 GMT</pubDate>
</item>
<item>
<title>Agentar-Fin-R1系列金融大模型提升推理与可信度</title>
<link>https://arxiv.org/abs/2507.16802</link>
<guid>https://arxiv.org/abs/2507.16802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agentar-Fin-R1提升金融场景下的推理能力与可信度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了基于Qwen3的Agentar-Fin-R1系列金融大语言模型（8B和32B参数），旨在增强金融应用中的推理能力、可靠性和领域专业化。通过高质量的金融任务标签系统和多层可信保障框架，结合自动化难度感知优化、两阶段训练流程和动态归属系统，显著提升了训练效率。模型在Fineva、FinEval、FinanceIQ等主流金融基准以及MATH-500、GPQA-diamond等通用推理数据集上表现优异，并提出了新的Finova评估基准以测试实际部署能力。实验结果表明，Agentar-Fin-R1在金融任务和通用推理方面均表现出色，具备高可信度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16802" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>NABLA：提升视频生成效率的自适应块级注意力机制</title>
<link>https://arxiv.org/abs/2507.13546</link>
<guid>https://arxiv.org/abs/2507.13546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NABLA通过动态自适应块级注意力提升视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出NABLA，一种基于Transformer的视频生成注意力机制，能够动态适应视频中的稀疏模式，从而降低计算复杂度。该方法无需定制低级操作，可与PyTorch的Flex Attention无缝集成。实验表明，NABLA在几乎不损失生成质量的前提下，使训练和推理速度提升了2.7倍。代码和模型权重已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:36:36 GMT</pubDate>
</item>
<item>
<title>基于深度学习的面部年龄与性别联合分类方法研究</title>
<link>https://arxiv.org/abs/2507.18565</link>
<guid>https://arxiv.org/abs/2507.18565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种联合年龄与性别分类的深度学习模型，提升广告精准度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于深度学习的面部图像年龄与性别联合分类方法，旨在提高目标广告投放的效果。该方法采用定制化的卷积神经网络（CNN）架构，通过共享特征表示同时处理两个任务，优于传统独立处理的方式。模型在大规模多样化数据集上进行训练，并经过预处理以增强对光照、姿态和图像质量变化的鲁棒性。实验结果显示，性别分类准确率达到95%，年龄估计的平均绝对误差为5.77年。研究还分析了不同年龄段的表现差异，指出年轻群体的年龄估计存在挑战，并建议通过数据增强和模型优化来减少偏差。此外，还探讨了不同CNN结构和超参数设置对性能的影响，为未来研究提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:41:26 GMT</pubDate>
</item>
<item>
<title>GLiNER2：统一的高效信息抽取框架</title>
<link>https://arxiv.org/abs/2507.18546</link>
<guid>https://arxiv.org/abs/2507.18546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLiNER2实现多任务信息抽取，提升部署效率。</p><br /><br /><p><strong>摘要：</strong> GLiNER2是一种统一的信息抽取框架，能够在单一模型中支持命名实体识别、文本分类和结构化数据提取。该框架基于预训练的Transformer架构，在保持CPU效率和模型紧凑性的同时，通过直观的模式接口实现多任务组合。实验表明，GLiNER2在抽取和分类任务中表现出色，并在部署便捷性上优于基于大语言模型的方案。项目已开源，提供预训练模型和文档。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:11:14 GMT</pubDate>
</item>
<item>
<title>DriftMoE：一种应对概念漂移的在线专家混合模型</title>
<link>https://arxiv.org/abs/2507.18464</link>
<guid>https://arxiv.org/abs/2507.18464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DriftMoE通过协同训练提升在线学习中的概念漂移适应能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DriftMoE，一种基于在线专家混合（MoE）架构的模型，用于处理非平稳数据流中的概念漂移问题。该模型通过一个紧凑的神经路由器与增量Hoeffding树专家池协同训练，形成共生学习循环。路由器根据预测选择最合适的专家，专家在获得真实标签后进行增量更新，同时路由器利用多热正确性掩码优化参数，从而加速专家专业化。实验表明，DriftMoE在多个数据流学习基准中表现优异，提供了一种高效且系统的方法来应对概念漂移。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 10:39:20 GMT</pubDate>
</item>
<item>
<title>2024年更新的英文GloVe模型评估报告</title>
<link>https://arxiv.org/abs/2507.18103</link>
<guid>https://arxiv.org/abs/2507.18103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2024年更新的GloVe模型在语言和文化上更具相关性。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了2024年新发布的英文GloVe模型，旨在提升语言模型对现代文化和语言变化的适应能力。相比2014年的原始模型，新模型基于Wikipedia、Gigaword和Dolma数据集进行训练，并详细记录了数据版本和预处理过程。通过词汇比较、直接测试和命名实体识别（NER）任务评估，结果显示新模型在包含非西方新闻数据等时间依赖性任务中表现更优，同时在结构任务如类比和相似性任务中保持良好性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 01:29:18 GMT</pubDate>
</item>
<item>
<title>TeleChat系列模型升级：性能显著提升的多版本发布</title>
<link>https://arxiv.org/abs/2507.18013</link>
<guid>https://arxiv.org/abs/2507.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeleChat系列新版本在训练策略上优化，提升推理与任务表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TeleChat系列的最新版本——TeleChat2、TeleChat2.5和T1，相较于前代模型实现了显著的性能提升。尽管模型架构变化不大，但通过改进的预训练和后训练策略，新模型在代码生成、数学推理等任务中表现出色。T1特别注重复杂推理能力，支持长链式思维，而TeleChat2.5则强调推理速度。两款旗舰模型均为115B参数的密集型Transformer架构，且T1-115B在多项指标上超越了如OpenAI的o1-mini和GPT-4o等专有模型。所有版本已公开发布，供开发者和研究人员使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 21:00:48 GMT</pubDate>
</item>
<item>
<title>基于Spelke对象的视觉分割方法研究</title>
<link>https://arxiv.org/abs/2507.16038</link>
<guid>https://arxiv.org/abs/2507.16038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SpelkeNet，用于识别物理运动关系下的视觉对象。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了计算机视觉中基于语义的分割与人类感知的Spelke对象之间的差异。Spelke对象是根据物理因果运动关系定义的，而非特定类别。作者引入了SpelkeBench数据集，并构建了SpelkeNet模型，通过预测未来运动分布来提取Spelke对象。该模型利用运动可及性图和预期位移图进行统计反事实探测，从而定义Spelke段。实验表明，SpelkeNet在SpelkeBench上优于现有方法，并在物理对象操作任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 16:11:57 GMT</pubDate>
</item>
<item>
<title>基于扩散变换器的皮肤病变分割模型SegDT</title>
<link>https://arxiv.org/abs/2507.15595</link>
<guid>https://arxiv.org/abs/2507.15595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SegDT在皮肤病变分割中表现优异，适用于医疗应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SegDT的新分割模型，基于扩散变换器（DiT），旨在提高皮肤病变分割的准确性和效率。该模型采用Rectified Flow技术，在降低推理步骤的同时保持生成质量，并在低功耗硬件上运行。SegDT在三个基准数据集上进行了评估，结果优于现有方法，且推理速度快，适用于实际医疗场景。研究推动了深度学习在医学图像分析中的应用，为医疗诊断提供了更快速、精准的工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:18:05 GMT</pubDate>
</item>
<item>
<title>基于动量不确定性的高效语言模型推理优化方法</title>
<link>https://arxiv.org/abs/2507.14958</link>
<guid>https://arxiv.org/abs/2507.14958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUR方法提升LLM推理效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Momentum Uncertainty-guided Reasoning (MUR)的新型推理优化方法，旨在提高大型语言模型（LLMs）在推理任务中的效率和准确性。该方法受物理学中动量概念启发，通过跟踪和聚合每一步的不确定性来动态分配思考预算，从而减少冗余计算。研究还引入了gamma-control机制，通过单一超参数调节推理预算。实验结果表明，MUR在多个基准测试中平均减少50%以上的计算量，同时提升准确率0.62-3.37%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 09:36:19 GMT</pubDate>
</item>
<item>
<title>Captain Cinema：基于文本生成高质量短片的框架</title>
<link>https://arxiv.org/abs/2507.18634</link>
<guid>https://arxiv.org/abs/2507.18634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Captain Cinema通过关键帧规划与视频合成生成高质量短片。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Captain Cinema，一个用于生成短片的框架。该框架根据详细的文本描述生成关键帧序列，确保故事和视觉的一致性，随后利用视频合成模型生成时空动态内容。为提升多场景长叙事视频的生成效果，作者引入了针对长上下文数据的交错训练策略，并在专门构建的电影数据集上进行训练。实验表明，Captain Cinema能够在高质量和高效率的前提下实现视觉连贯且叙事一致的短片生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>TTS-VAR：一种高效的视觉自回归模型测试时扩展框架</title>
<link>https://arxiv.org/abs/2507.18537</link>
<guid>https://arxiv.org/abs/2507.18537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TTS-VAR提升视觉生成模型性能，提高8.7%得分。</p><br /><br /><p><strong>摘要：</strong> 本文提出TTS-VAR，首个针对视觉自回归（VAR）模型的测试时扩展框架，将生成过程建模为路径搜索问题。通过自适应下降批量大小调度平衡计算效率与探索能力，并引入基于聚类的多样性搜索和基于重采样的潜力选择机制。实验表明，在Infinity模型上提升了8.7%的GenEval分数，揭示早期结构特征对最终质量的影响及不同尺度下重采样效果的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18537" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>TeEFusion：一种高效的文本到图像生成蒸馏方法</title>
<link>https://arxiv.org/abs/2507.18192</link>
<guid>https://arxiv.org/abs/2507.18192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeEFusion提升图像生成效率，保持高质量输出。</p><br /><br /><p><strong>摘要：</strong> 本文提出TeEFusion，一种高效的文本到图像生成蒸馏方法。通过将引导强度直接融入文本嵌入中，TeEFusion在不增加额外参数的情况下，使学生模型能够学习教师模型的复杂采样策略。实验表明，该方法显著提升了推理速度，达到教师模型的6倍，同时保持了相近的图像质量。该方法已在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.18192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Jul 2025 04:45:40 GMT</pubDate>
</item>
<item>
<title>大规模地球3D生成技术的创新与应用</title>
<link>https://arxiv.org/abs/2507.16535</link>
<guid>https://arxiv.org/abs/2507.16535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EarthCrafter实现千平方公里级3D地球建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出EarthCrafter，一种用于大规模3D地球生成的框架，结合了Aerial-Earth3D数据集和稀疏解耦扩散模型。该方法通过分离结构与纹理生成，有效降低计算成本并保持地理合理性。实验表明其在超大规模生成任务中表现优异，并支持多种应用场景，如语义引导的城市布局生成和无条件地形合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16535" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 08:46:48 GMT</pubDate>
</item>
<item>
<title>Hierarchical Budget Policy Optimization提升推理效率与能力</title>
<link>https://arxiv.org/abs/2507.15844</link>
<guid>https://arxiv.org/abs/2507.15844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HBPO提升模型推理效率同时保持准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Hierarchical Budget Policy Optimization (HBPO)框架，通过分层预算探索和差异化奖励机制，使模型根据问题复杂度自动调整推理深度。该方法有效解决传统方法在效率训练中探索空间塌陷的问题，减少平均token使用量达60.6%，同时提升准确率3.14%。HBPO无需外部约束或离散模式选择，展现出模型自适应调整推理深度的能力，证明推理效率与能力可以协同优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:52:34 GMT</pubDate>
</item>
<item>
<title>LAPO：通过自适应策略优化实现高效推理的框架</title>
<link>https://arxiv.org/abs/2507.15758</link>
<guid>https://arxiv.org/abs/2507.15758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LAPO提升模型推理效率并减少token使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Length-Adaptive Policy Optimization (LAPO)的新框架，旨在将推理长度控制从外部约束转化为模型的内在能力。该框架通过两阶段强化学习过程，使模型能够学习自然的推理模式，并在推理过程中动态调整计算资源。实验表明，LAPO在数学推理基准测试中可减少高达40.9%的token使用量，同时提升2.3%的准确性。分析显示，LAPO训练的模型能根据问题复杂度有效分配计算资源，实现高效且高质量的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:14:41 GMT</pubDate>
</item>
<item>
<title>DMOSpeech 2：通过强化学习优化语音合成的持续预测器</title>
<link>https://arxiv.org/abs/2507.14988</link>
<guid>https://arxiv.org/abs/2507.14988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DMOSpeech 2优化了语音合成中的持续预测，提升整体性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DMOSpeech 2，这是一种基于扩散模型的文本转语音系统，通过强化学习方法优化了持续预测器，提高了语音合成的整体质量。该系统采用了一种新的持续策略框架，利用说话人相似性和词错误率作为奖励信号。此外，引入了教师引导采样方法，在保持效率的同时提升了输出多样性。实验结果表明，DMOSpeech 2在所有指标上均优于之前系统，并减少了采样步骤而未影响质量，标志着语音合成系统在多组件优化方面的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 10:48:48 GMT</pubDate>
</item>
<item>
<title>Promptomatix：自动提示优化框架提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.14241</link>
<guid>https://arxiv.org/abs/2507.14241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Promptomatix实现自动提示优化，提升LLM性能。</p><br /><br /><p><strong>摘要：</strong> Promptomatix是一个自动提示优化框架，能够将自然语言任务描述转换为高质量提示，无需手动调整或领域知识。该框架支持基于元提示的优化器和DSPy驱动的编译器，具备模块化设计，便于未来扩展。系统通过分析用户意图、生成合成训练数据、选择提示策略并使用成本感知目标优化提示，已在五个任务类别中表现出色，优于现有库，同时减少提示长度和计算开销，提高效率和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 14:18:20 GMT</pubDate>
</item>
<item>
<title>Pusa：基于向量化时间步适应的视频扩散模型新范式</title>
<link>https://arxiv.org/abs/2507.16116</link>
<guid>https://arxiv.org/abs/2507.16116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pusa通过VTA实现高效视频生成与多任务能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Pusa，一种基于向量化时间步适应（VTA）的视频扩散模型新范式。该方法在保持基础模型能力的同时，实现了精细的时间控制，显著提升了图像到视频生成的效率和性能。实验表明，Pusa在VBench-I2V基准上得分87.32%，远超现有模型，且具备零样本多任务能力，如起始帧、视频扩展等，同时支持文本到视频生成。VTA技术有效避免了计算冗余和灾难性遗忘问题，为下一代视频合成提供了可扩展、高效且通用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 20:09:37 GMT</pubDate>
</item>
<item>
<title>Yume: An Interactive World Generation Model</title>
<link>https://arxiv.org/abs/2507.17744</link>
<guid>https://arxiv.org/abs/2507.17744</guid>
<content:encoded><![CDATA[
Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>DesignLab：通过迭代优化提升幻灯片设计质量</title>
<link>https://arxiv.org/abs/2507.17202</link>
<guid>https://arxiv.org/abs/2507.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DesignLab通过角色分离实现幻灯片设计的持续优化。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为DesignLab的系统，旨在帮助非专业人士设计高质量的幻灯片。该系统将设计过程分为设计审查者和设计贡献者两个角色，审查者负责识别设计问题，贡献者则进行修正，形成一个迭代优化的流程。通过微调大语言模型并引入受控扰动模拟中间草稿，DesignLab能够有效学习设计错误和修复方法。实验表明，该方法在设计质量上优于现有工具，能生成更专业、精美的幻灯片。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 00:49:48 GMT</pubDate>
</item>
<item>
<title>RAVine：面向代理式搜索的现实对齐评估框架</title>
<link>https://arxiv.org/abs/2507.16725</link>
<guid>https://arxiv.org/abs/2507.16725</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAVine提升代理式搜索系统的评估准确性与实用性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RAVine，一个面向代理式大语言模型搜索的现实对齐评估框架。针对现有评估体系在复杂查询、细粒度评估和迭代过程分析方面的不足，RAVine通过多点查询和长文本回答更贴近用户意图，并引入可追溯的地面真实数据构建策略，提高评估精度。同时，RAVine关注模型在搜索工具交互中的表现及效率因素。实验结果揭示了代理式搜索系统的发展方向，相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16725" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:08:12 GMT</pubDate>
</item>
<item>
<title>文本到图像扩散模型中的记忆与隐私问题研究</title>
<link>https://arxiv.org/abs/2507.16880</link>
<guid>https://arxiv.org/abs/2507.16880</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到图像模型易复制训练数据，现有防御措施效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了文本到图像扩散模型在数据隐私和知识产权方面的潜在风险。尽管已有方法尝试通过剪枝来减少模型对训练数据的复制，但实验表明，仅需微调输入提示的文本嵌入即可重新触发数据复制，说明现有方法并不稳固。此外，研究挑战了记忆局部化的假设，证明复制行为可以在文本嵌入空间的不同位置被触发，并且路径各异。这表明当前的缓解策略不足以解决问题，应寻求真正删除记忆内容的方法。为此，作者提出了一种新的对抗性微调方法，以提升模型的鲁棒性。研究为构建更可信、合规的生成式AI提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16880" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:02:38 GMT</pubDate>
</item>
<item>
<title>基于形式语言的大型语言模型验证方法研究</title>
<link>https://arxiv.org/abs/2507.16331</link>
<guid>https://arxiv.org/abs/2507.16331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用形式语言提升LLM验证可靠性与可扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于形式语言（如Dafny）的大型语言模型（LLM）验证方法，以解决传统基于自然语言的验证不可靠、不可扩展的问题。通过引入自动数据整理流程和结合形式语言验证器反馈的强化学习设计，研究团队构建了DafnyComp基准测试集，并在监督微调阶段使小型模型生成可验证的Dafny代码，效果优于现有专有模型。进一步的正则化强化学习提升了模型在跨域任务中的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 04:13:01 GMT</pubDate>
</item>
<item>
<title>高效3D生成框架Ultra3D提升稀疏体素建模速度</title>
<link>https://arxiv.org/abs/2507.17745</link>
<guid>https://arxiv.org/abs/2507.17745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ultra3D通过优化注意力机制实现高速高质3D建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出Ultra3D，一种高效的3D生成框架，旨在解决现有方法在稀疏体素建模中的计算效率问题。该框架利用VecSet表示法在第一阶段快速生成粗略物体布局，减少令牌数量并加速体素坐标预测。第二阶段引入Part Attention机制，仅在语义一致的部分区域进行注意力计算，从而保持结构连续性并避免不必要的全局注意力，实现高达6.7倍的生成速度提升。此外，构建了可扩展的部件注释流程，将原始网格转换为带标签的稀疏体素。实验表明，Ultra3D支持1024分辨率的高质量3D生成，并在视觉保真度和用户偏好方面达到领先水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>多领域推理在强化学习中的系统研究</title>
<link>https://arxiv.org/abs/2507.17512</link>
<guid>https://arxiv.org/abs/2507.17512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多领域推理在强化学习中的交互机制与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统探讨了在强化学习框架下，数学推理、代码生成和逻辑谜题解决等多领域推理的交互机制。通过GRPO算法和Qwen-2.5-7B模型进行实验，分析了单领域训练下的性能提升与跨领域泛化能力，同时研究了多领域联合训练中出现的相互促进与冲突现象。此外，还比较了基础模型与指令微调模型在相同强化学习配置下的表现差异，并深入探讨了课程学习、奖励设计及语言特性对训练效果的影响。实验结果揭示了领域间互动的关键因素，为提升大语言模型的多领域推理能力提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.17512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Jul 2025 09:51:04 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的感知能力评估与Turing Eye Test基准</title>
<link>https://arxiv.org/abs/2507.16863</link>
<guid>https://arxiv.org/abs/2507.16863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示当前多模态模型在感知任务中表现不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在感知能力方面与人类的差距，提出了一种名为Turing Eye Test（TET）的感知导向基准测试。该基准包含四个诊断任务，用于评估模型在合成图像上的表现。研究发现，尽管最先进的MLLMs在语言推理任务中表现良好，但在感知任务上却出现严重失败，而仅通过语言模型的微调无法提升其性能。只有对视觉模块进行微调才能显著改善表现，表明当前MLLMs在视觉泛化能力上仍存在重大不足。作者发布了部分TET任务，并计划在未来引入更多任务以提升视觉泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 17:50:16 GMT</pubDate>
</item>
<item>
<title>Elevate3D：提升低质量3D模型质量的新框架</title>
<link>https://arxiv.org/abs/2507.11465</link>
<guid>https://arxiv.org/abs/2507.11465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Elevate3D提升低质量3D模型的纹理与几何质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Elevate3D，一种用于提升低质量3D模型质量的新框架。该框架通过HFS-SDEdit方法显著改善纹理质量，同时保留原始外观和几何结构，并修复其退化问题。Elevate3D采用逐视角优化策略，结合先进的单目几何预测器，实现纹理与几何的协同优化。相比现有方法，Elevate3D在3D模型精修方面达到最新水平，有效缓解高质量3D资源短缺的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 12:36:20 GMT</pubDate>
</item>
<item>
<title>PrefPalette：基于属性分解的人类偏好建模框架</title>
<link>https://arxiv.org/abs/2507.13541</link>
<guid>https://arxiv.org/abs/2507.13541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefPalette提升AI个性化能力，增强可解释性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了PrefPalette，一个将人类偏好分解为属性维度并根据不同社会群体价值观进行个性化预测的框架。该方法通过生成合成数据和注意力机制，使AI更准确地理解用户偏好，并在Reddit的45个社区中表现出比GPT-4o更高的预测准确率。此外，它揭示了不同社区的偏好特征，如学术群体重视详尽与刺激，冲突导向群体偏好讽刺与直接表达，支持型群体强调同理心。PrefPalette不仅提升了偏好建模效果，还提供了透明、可解释的洞察，为更可信的个性化应用奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 17:21:54 GMT</pubDate>
</item>
<item>
<title>面向目标检测的新型零样本量化框架</title>
<link>https://arxiv.org/abs/2507.16782</link>
<guid>https://arxiv.org/abs/2507.16782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种任务特定的零样本量化方法提升目标检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种面向目标检测的新型零样本量化框架（ZSQ），通过引入边界框和类别采样策略生成任务相关的校准集，并将任务特定训练融入知识蒸馏过程，从而恢复量化检测网络的性能。实验表明该方法在MS-COCO和Pascal VOC数据集上表现出色，具有较高的效率和先进性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:28:29 GMT</pubDate>
</item>
<item>
<title>ExpTeach：通过自我生成记忆实现视觉语言模型与机器人的有效融合</title>
<link>https://arxiv.org/abs/2507.16713</link>
<guid>https://arxiv.org/abs/2507.16713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ExpTeach提升机器人任务成功率，增强空间理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出ExpTeach框架，通过构建真实世界经验的自我生成记忆，将原本在互联网数据上训练的视觉语言模型（VLMs）有效地应用于物理机器人。ExpTeach使VLM能够自主规划动作、验证结果、反思失败并调整行为，同时将经验总结为长期记忆，用于未来任务的检索增强生成（RAG）。此外，该框架还引入按需图像标注模块，提升VLM的空间理解能力。实验表明，ExpTeach显著提高了机器人任务的成功率，从36%提升至84%，并在12个真实场景中展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>区域自适应潜在上采样提升扩散模型推理效率</title>
<link>https://arxiv.org/abs/2507.08422</link>
<guid>https://arxiv.org/abs/2507.08422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RALU方法提升扩散模型推理速度且保持图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为区域自适应潜在上采样（RALU）的训练-free 框架，用于加速基于扩散的图像和视频生成模型。RALU通过在空间维度上进行混合分辨率采样，包括低分辨率去噪、特定区域的自适应上采样以及全分辨率细节优化，显著提升了推理效率。同时，通过噪声-时间步重新调度技术，确保不同分辨率之间的生成稳定性。实验表明，RALU在FLUX和Stable Diffusion 3上分别实现了7.0倍和3.0倍的速度提升，且图像质量损失极小。该方法还可与现有时间维度加速技术兼容，进一步降低推理延迟而不影响生成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:07:43 GMT</pubDate>
</item>
<item>
<title>ThinkAct：基于视觉潜在规划的多模态推理与动作执行框架</title>
<link>https://arxiv.org/abs/2507.16815</link>
<guid>https://arxiv.org/abs/2507.16815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkAct提升多模态AI任务的长程规划与自修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ThinkAct框架，通过结合高层推理与低层动作执行，实现更高效的多模态任务处理。该框架利用强化学习生成与动作对齐的视觉奖励，指导多模态大模型生成具有一致性和目标导向的推理计划，并将其压缩为视觉潜在表示以指导后续动作执行。实验表明，ThinkAct在复杂环境任务中展现出少样本适应、长程规划和自我修正的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SOPHIA提升视觉语言模型的慢思考推理能力</title>
<link>https://arxiv.org/abs/2507.16814</link>
<guid>https://arxiv.org/abs/2507.16814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SOPHIA增强LVLM的多模态推理能力，效果显著。</p><br /><br /><p><strong>摘要：</strong> 本文提出SOPHIA，一种用于视觉语言模型（LVLM）的半离策略强化学习方法，旨在提升其慢思考推理能力。通过结合可训练LVLM的视觉理解与语言模型的离策略推理，SOPHIA生成基于结果的奖励，并将视觉奖励反向传播，使LVLM能够从推理轨迹中学习。实验表明，SOPHIA在多个多模态推理基准测试中表现优异，尤其在InternVL3.0-38B上提升了8.5%，甚至超越部分闭源模型。分析显示，SOPHIA优于监督微调和直接在线策略方法，为后续在线策略训练提供了更好的策略初始化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于多模态大模型的人-物交互合成方法研究</title>
<link>https://arxiv.org/abs/2507.16813</link>
<guid>https://arxiv.org/abs/2507.16813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HOComp方法实现自然人-物交互合成。</p><br /><br /><p><strong>摘要：</strong> 本文针对图像中人与物体交互合成的挑战，提出HOComp方法，通过MLLMs驱动的区域姿态引导和细节一致外观保持机制，实现更自然、和谐的人-物交互合成。研究还构建了首个交互感知的人-物合成数据集IHOC，实验表明该方法在视觉质量和一致性上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>构建科学推理数据集与模型提升AI在自然科学中的表现</title>
<link>https://arxiv.org/abs/2507.16812</link>
<guid>https://arxiv.org/abs/2507.16812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">构建高质量科学推理数据集，提升AI在自然科学中的研究能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出TextbookReasoning和MegaScience两个大型科学推理数据集，旨在填补人工智能在自然科学领域研究的空白。TextbookReasoning包含12,000本大学教材中的650,000道推理题，涵盖7个科学领域；MegaScience则整合了1.25百万条高质量数据，通过系统性实验优化数据选择方法。研究还构建了覆盖15个基准测试的评估体系，确保准确衡量模型性能。实验表明，基于MegaScience训练的Llama3.1、Qwen2.5和Qwen3系列模型在平均性能上优于官方指令模型，并展现出规模效应。作者已公开数据集、评估系统及训练模型，推动科学推理研究发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>无需修改训练数据的大型语言模型泛化控制方法</title>
<link>https://arxiv.org/abs/2507.16795</link>
<guid>https://arxiv.org/abs/2507.16795</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAFT技术通过概念消融提升模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为概念消融微调（CAFT）的新方法，用于控制大型语言模型在微调后的泛化行为，而无需修改训练数据。该方法利用可解释性工具，在微调过程中通过线性投影消除模型中的不良概念，从而避免不必要的泛化。实验表明，CAFT在三个微调任务中有效减少了错误泛化现象，且不损害模型在训练分布上的性能。这种方法为控制模型泛化提供了一种新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16795" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:45:04 GMT</pubDate>
</item>
<item>
<title>突破大语言模型推理瓶颈的Thread Inference Model</title>
<link>https://arxiv.org/abs/2507.16784</link>
<guid>https://arxiv.org/abs/2507.16784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TIM与TIMRUN提升大模型推理能力与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出Thread Inference Model (TIM) 和 TIMRUN 推理运行时，以解决大语言模型（LLM）在推理准确性和效率上的限制。TIM 是一种用于递归和分解问题解决的 LLM 家族，而 TIMRUN 支持超越上下文限制的长周期结构化推理。通过将自然语言建模为推理树，TIM 实现了几乎无限的工作内存和多跳工具调用，克服了输出限制、位置嵌入约束和 GPU 内存瓶颈。实验表明，该系统在处理数学任务和需要长周期推理的信息检索中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 13:30:04 GMT</pubDate>
</item>
<item>
<title>Zebra-CoT数据集提升多模态视觉链式推理能力</title>
<link>https://arxiv.org/abs/2507.16746</link>
<guid>https://arxiv.org/abs/2507.16746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra-CoT提升视觉链式推理模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Zebra-CoT，一个包含182,384个样本的大规模多模态数据集，用于训练视觉链式推理（Visual CoT）模型。该数据集涵盖科学问题、二维和三维推理任务以及逻辑与策略游戏等场景。实验表明，在Zebra-CoT上微调Anole-7B模型可使测试集准确率提升12%，在标准VLM基准测试中性能提高最多13%。Bagel-7B模型在该数据集上微调后能生成高质量的多模态推理链，证明了Zebra-CoT在提升多模态推理能力方面的有效性。作者开源了数据集和模型以支持相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16746" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 12:35:36 GMT</pubDate>
</item>
<item>
<title>Step-Audio 2：面向工业级音频理解与语音对话的端到端多模态大语言模型</title>
<link>https://arxiv.org/abs/2507.16632</link>
<guid>https://arxiv.org/abs/2507.16632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Audio 2 提升了语音识别与音频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Step-Audio 2，这是一个面向工业级音频理解和语音对话的端到端多模态大语言模型。通过整合潜在音频编码器和以推理为中心的强化学习（RL），Step-Audio 2 在自动语音识别（ASR）和音频理解方面表现出色。该模型还引入了离散音频标记生成，增强了对语调、情感等副语言信息的响应能力。此外，Step-Audio 2 结合了检索增强生成（RAG）技术，并支持调用外部工具如网络搜索和音频搜索，以减少幻觉并实现音色切换。经过数百万小时的语音和音频数据训练，Step-Audio 2 在多种对话场景中展现出强大的智能和表现力。评估结果表明，其在多个音频理解和对话基准测试中均优于其他开源和商业解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.16632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Jul 2025 10:23:55 GMT</pubDate>
</item>
<item>
<title>推理时计算增强模型鲁棒性的安全风险分析</title>
<link>https://arxiv.org/abs/2507.15974</link>
<guid>https://arxiv.org/abs/2507.15974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时计算提升模型鲁棒性，但可能暴露安全风险。</p><br /><br /><p><strong>摘要：</strong> 本文研究发现，小型开源模型也能通过推理时的预算强制策略提升鲁棒性。然而，研究揭示了先前研究中的一个隐含假设：中间推理步骤对攻击者是隐藏的。当这一假设被打破，即中间步骤可被访问时，推理时计算反而会降低模型的鲁棒性，呈现出反向缩放规律。此外，文章指出在工具集成和高级推理提取攻击等场景下，即使推理链被隐藏，模型仍可能受到攻击。因此，作者强调在实际应用中需权衡推理时计算带来的安全与性能之间的微妙平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 14:08:38 GMT</pubDate>
</item>
<item>
<title>ObjectGS：融合语义理解的3D场景重建框架</title>
<link>https://arxiv.org/abs/2507.15454</link>
<guid>https://arxiv.org/abs/2507.15454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectGS实现对象级3D重建与语义理解的统一。</p><br /><br /><p><strong>摘要：</strong> 本文提出ObjectGS，一个将3D场景重建与语义理解相结合的对象感知框架。不同于传统方法将场景视为整体，ObjectGS通过将每个物体建模为局部锚点，生成神经高斯分布并共享对象ID，从而实现精确的对象级重建。训练过程中动态调整锚点并优化特征，同时使用one-hot ID编码和分类损失确保语义清晰。实验表明，ObjectGS在开放词汇和全景分割任务中优于现有方法，并支持网格提取和场景编辑等应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:06:23 GMT</pubDate>
</item>
<item>
<title>SPAR：基于多智能体框架的学术文献检索新方法</title>
<link>https://arxiv.org/abs/2507.15245</link>
<guid>https://arxiv.org/abs/2507.15245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPAR提升学术文献检索性能，优于现有基线。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SPAR，一种基于多智能体框架的学术文献检索系统，结合RefChain进行查询分解和演化，以提高搜索的灵活性和有效性。为了系统评估，研究者构建了SPARBench基准测试集，包含专家标注的相关性标签。实验结果表明，SPAR在AutoScholar和SPARBench数据集上分别比最佳基线提升了56%和23%的F1分数。SPAR与SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 01:06:53 GMT</pubDate>
</item>
<item>
<title>基于强化学习的RefCritic模块提升语言模型批判能力</title>
<link>https://arxiv.org/abs/2507.15024</link>
<guid>https://arxiv.org/abs/2507.15024</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefCritic通过双重奖励机制提升模型批判与优化能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型中批评模块开发的挑战，提出了一种基于强化学习的长链思维批评模块RefCritic。该模块采用双重规则奖励机制，分别评估解决方案的实例级正确性和策略模型的细化准确性，以生成高质量且具有行动指导性的反馈。在多个基准测试中，RefCritic表现出显著优势，尤其在数学推理任务中优于传统步骤级监督方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15024" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:19:51 GMT</pubDate>
</item>
<item>
<title>基于MCP的LLM智能体评估框架MCPEval</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCPEval实现LLM智能体自动化评估与标准化测试。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MCPEval，一个基于Model Context Protocol (MCP) 的开源框架，用于自动化生成任务并深度评估大型语言模型（LLM）智能体在多个领域中的表现。该框架通过标准化指标、与原生工具集成，减少了人工构建评估流程的工作量。实验结果表明，MCPEval在五个现实场景中有效揭示了模型在特定领域的性能特征，并已公开发布以促进可复现和标准化的LLM评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12806" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 01:46:27 GMT</pubDate>
</item>
<item>
<title>LLM生成学生风格代码的系统研究</title>
<link>https://arxiv.org/abs/2507.12674</link>
<guid>https://arxiv.org/abs/2507.12674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究LLM生成类似学生代码的能力与方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出ParaStudent，研究大型语言模型在编程课程中生成类似学生代码的能力。通过分析多个学期的学生提交记录，设计低分辨率和高分辨率实验，评估代码在语义、功能和风格维度的表现。结果表明，微调能显著提升与真实学生代码轨迹的一致性，更准确地捕捉错误模式、逐步改进和风格变化。研究强调了通过上下文感知生成、时间建模和多维评估来模拟真实学生代码的重要性。代码和实验数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 19:12:14 GMT</pubDate>
</item>
<item>
<title>机器学习中的串行计算挑战与未来发展方向</title>
<link>https://arxiv.org/abs/2507.12549</link>
<guid>https://arxiv.org/abs/2507.12549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">串行计算问题限制了当前机器学习的进展。</p><br /><br /><p><strong>摘要：</strong> 文章指出，尽管机器学习在大规模并行化方面取得了进展，但某些问题本质上是顺序的，如数学推理、物理模拟和序列决策等，这些任务需要依赖性的计算步骤，无法并行处理。基于复杂性理论，作者明确了这一区别，并展示了当前以并行为中心的架构在处理此类任务时存在根本性局限。文章强调，识别计算的串行性质对机器学习、模型设计和硬件发展具有深远影响，认为在AI面对更复杂的推理任务时，应更加重视串行计算的扩展，而不仅仅是并行计算。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12549" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 14:01:26 GMT</pubDate>
</item>
<item>
<title>基于去噪的潜在令牌化器设计研究</title>
<link>https://arxiv.org/abs/2507.15856</link>
<guid>https://arxiv.org/abs/2507.15856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型去噪令牌化器，提升生成模型效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉令牌化器在生成建模中的有效性问题，指出现代生成模型都具有从噪声或遮蔽输入中重建清晰信号的训练目标，这一过程称为去噪。受此启发，作者提出将令牌化器嵌入直接与下游去噪目标对齐，使潜在嵌入在严重干扰下仍易于重建。为此，引入了Latent Denoising Tokenizer（l-DeTok），该令牌化器通过插值噪声和随机遮蔽来重建清晰图像。实验表明，在ImageNet 256x256数据集上，l-DeTok在六种代表性生成模型中均优于标准令牌化器。研究强调去噪是令牌化器设计的重要原则，希望为未来设计提供新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于概念驱动的视频目标分割框架SeC及其性能评估</title>
<link>https://arxiv.org/abs/2507.15852</link>
<guid>https://arxiv.org/abs/2507.15852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeC提升视频目标分割效果，实现更高精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于概念驱动的视频目标分割框架SeC，旨在解决传统方法在处理视觉变化、遮挡和复杂场景时的不足。SeC利用大视觉语言模型构建高阶、以对象为中心的表示，增强语义理解能力，并在推理过程中动态调整计算资源。为评估该方法，研究者还构建了SeCVOS基准数据集，包含160个精心标注的多场景视频。实验表明，SeC在SeCVOS上比SAM 2.1提升了11.8个百分点，成为当前最先进的概念感知视频目标分割方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>GUI-G^2：基于高斯分布的图形用户界面定位奖励框架</title>
<link>https://arxiv.org/abs/2507.15846</link>
<guid>https://arxiv.org/abs/2507.15846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI-G^2通过高斯分布提升GUI交互的精确性与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为GUI Gaussian Grounding Rewards (GUI-G^2) 的新型奖励框架，用于改进图形用户界面（GUI）中的自然语言指令定位任务。传统方法依赖二值奖励，导致信号稀疏且忽略空间连续性。而GUI-G^2通过将GUI元素建模为连续高斯分布，引入了两种协同机制：高斯点奖励用于精确定位，覆盖奖励用于评估空间对齐度。此外，该框架还包含自适应方差机制，以处理不同尺寸的界面元素。实验表明，GUI-G^2在多个基准测试中显著优于现有方法，特别是在ScreenSpot-Pro上提升了24.7%。研究显示，连续建模增强了模型对界面变化的鲁棒性和对未知布局的泛化能力，为GUI交互任务提供了新的范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:53:42 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的经济政策模拟框架</title>
<link>https://arxiv.org/abs/2507.15815</link>
<guid>https://arxiv.org/abs/2507.15815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM Economist通过代理建模实现经济政策设计与评估。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为LLM Economist的新框架，利用基于代理的建模方法，在具有层级决策的战略环境中设计和评估经济政策。底层由有限理性的工人代理组成，这些代理基于美国人口普查数据生成，以最大化文本定义的效用函数。上层的规划代理则通过上下文强化学习提出分段线性边际税率方案。该框架具备优化异质效用、生成大规模真实人口代理以及完全用自然语言表达机制设计的能力。实验表明，该框架在一百个交互代理中能够接近斯塔克尔伯格均衡，提升社会福利，并在去中心化治理下通过定期投票进一步提高效果。结果证明，基于大语言模型的代理可以共同建模、模拟和治理复杂经济系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 13:21:14 GMT</pubDate>
</item>
<item>
<title>基于熵感知的强化学习方法提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.15778</link>
<guid>https://arxiv.org/abs/2507.15778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Archer方法提升LLM推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Archer的熵感知强化学习方法，旨在提升大语言模型（LLM）的推理能力。传统RLVR方法对所有token应用相同的训练信号，而忽略了低熵知识类token与高熵推理类token的不同作用。Archer通过双token约束和同步更新机制，对推理token施加较弱的KL正则化和较高的裁剪阈值以鼓励探索，同时对知识token施加强约束以保持事实准确性。实验结果表明，该方法在多个数学推理和代码生成基准上表现优异，达到或超过同规模模型的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 12:34:01 GMT</pubDate>
</item>
<item>
<title>TokensGen：基于压缩标记的长视频生成框架</title>
<link>https://arxiv.org/abs/2507.15728</link>
<guid>https://arxiv.org/abs/2507.15728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokensGen通过压缩标记提升长视频生成的连贯性与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出TokensGen，一种两阶段框架，旨在解决长视频生成中的记忆瓶颈和长期不一致问题。该方法将长视频生成分解为内片段语义控制、长期一致性控制和片段间平滑过渡三个核心任务。首先训练To2V模型，利用文本和视频标记生成短视频；其次引入T2To模型，一次性生成所有标记以确保全局一致性；最后在推理阶段采用自适应FIFO-Diffusion策略实现片段间的无缝连接。实验表明，该方法在保持计算效率的同时显著提升了长视频的时间和内容连贯性，为叙事、电影制作和沉浸式模拟提供了可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 11:37:33 GMT</pubDate>
</item>
<item>
<title>基于数据混合代理的持续预训练方法</title>
<link>https://arxiv.org/abs/2507.15640</link>
<guid>https://arxiv.org/abs/2507.15640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出数据混合代理模型，提升语言模型在新领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Data Mixing Agent的端到端框架，用于在持续预训练中优化源领域和目标领域数据的混合比例，以避免模型遗忘原有能力。该方法通过强化学习从大量数据混合轨迹中学习通用的重加权策略，无需人工设定。实验表明，该方法在数学推理任务中优于现有基线，并能跨领域、跨模型泛化。此外，它在代码生成等不同任务中也表现出良好的适应性，且在较少源领域数据的情况下仍能保持高性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 10:01:54 GMT</pubDate>
</item>
<item>
<title>基于离散SDF的3D高斯点云逆渲染方法</title>
<link>https://arxiv.org/abs/2507.15629</link>
<guid>https://arxiv.org/abs/2507.15629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种离散SDF增强的3D高斯点云逆渲染方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于离散符号距离场（SDF）的3D高斯点云逆渲染方法，旨在解决传统方法在几何约束应用上的不足。通过将SDF编码到每个高斯中，并利用SDF到透明度的转换实现高效渲染，避免了光线追踪计算。为保证离散样本与真实SDF的一致性，引入基于投影的对齐损失。实验表明，该方法在光照重建质量上优于现有方法，且无需额外内存和复杂优化设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:52:33 GMT</pubDate>
</item>
<item>
<title>Being-H0：基于人类视频的多模态机器人操作模型</title>
<link>https://arxiv.org/abs/2507.15597</link>
<guid>https://arxiv.org/abs/2507.15597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-H0通过人类视频训练，提升机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> Being-H0是一种新型的视觉-语言-动作模型（VLA），利用大规模人类视频进行训练，旨在解决现有模型在复杂操作任务中的不足。该模型采用物理指令微调方法，结合大规模预训练、三维空间对齐和任务适配，提升了机器人在真实环境中的表现。同时，研究团队构建了一个包含多种数据源的大规模数据集，以支持模型训练与优化。实验表明，Being-H0在手部运动生成和指令遵循方面表现出色，并具备良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 09:19:09 GMT</pubDate>
</item>
<item>
<title>PhysGym：评估大语言模型科学发现能力的新基准</title>
<link>https://arxiv.org/abs/2507.15550</link>
<guid>https://arxiv.org/abs/2507.15550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysGym用于评估大语言模型在物理环境中的科学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysGym，一个用于评估基于大语言模型的科学推理能力的新基准套件和仿真平台。该平台通过控制提供给代理的先验知识水平，使研究人员能够分析模型在不同问题复杂度下的表现。PhysGym包含一系列交互式模拟，要求代理主动探测环境、在约束条件下收集数据并提出关于物理规律的假设。平台提供了标准化的评估协议和指标，用于衡量假设的准确性和模型的真实性。实验结果展示了该基准在区分不同先验知识和任务复杂度下的模型能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 08:28:10 GMT</pubDate>
</item>
<item>
<title>GR-3：通用机器人策略的进展与ByteMini集成</title>
<link>https://arxiv.org/abs/2507.15493</link>
<guid>https://arxiv.org/abs/2507.15493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GR-3具备强大泛化能力，适用于多种任务和环境。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了GR-3，一种大规模视觉-语言-动作（VLA）模型，具有在新物体、环境和抽象指令中进行泛化的出色能力。GR-3可通过少量人类轨迹数据高效微调，实现快速适应新场景。它还能处理长时序和精细操作任务，包括双臂操作和移动任务。该模型通过多阶段训练方法实现，包括网络规模的视觉-语言数据联合训练、VR设备收集的人类轨迹数据微调以及机器人轨迹数据的模仿学习。同时，文章介绍了与GR-3集成的ByteMini机器人，展示了其在多种任务中的灵活性和可靠性。实验表明，GR-3在多个挑战性任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 06:54:13 GMT</pubDate>
</item>
<item>
<title>Stitch：实现语音模型同步思考与回答的新方法</title>
<link>https://arxiv.org/abs/2507.15375</link>
<guid>https://arxiv.org/abs/2507.15375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Stitch让语音模型在说话时同步生成内部思考过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Stitch的新方法，用于改进语音语言模型（SLMs）的性能。传统SLMs在生成回应前缺乏内部思考过程，而人类在交流前通常会进行内部推理。Stitch通过交替生成未发声的推理片段和语音回应片段，实现了在语音输出过程中同步进行内部思考。这种方法有效减少了额外延迟，同时在数学推理数据集上比基线模型高出15%的性能，并在非推理任务中表现相当。项目页面提供了相关演示和动画。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Jul 2025 04:30:03 GMT</pubDate>
</item>
<item>
<title>基于知识投影的网页信息检索数据合成框架WebShaper</title>
<link>https://arxiv.org/abs/2507.15061</link>
<guid>https://arxiv.org/abs/2507.15061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebShaper提升IS代理性能，实现更精准的信息检索。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为WebShaper的数据合成框架，旨在解决信息检索（IS）代理训练数据不足的问题。该框架通过集合论对IS任务进行形式化，并引入知识投影（KP）概念，以精确控制推理结构。WebShaper通过多步骤扩展过程生成复杂任务，提升了IS代理在GAIA和WebWalkerQA基准测试中的表现，达到了当前开源IS代理的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 13:53:37 GMT</pubDate>
</item>
<item>
<title>视频理解测试：评估视频大语言模型的准确性和鲁棒性</title>
<link>https://arxiv.org/abs/2507.15028</link>
<guid>https://arxiv.org/abs/2507.15028</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">视频大语言模型在视频理解上与人类存在显著差距。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为Video-TT的视频理解测试，用于评估视频大语言模型（video LLMs）在真实视频中的表现是否接近人类。该测试包含1000个YouTube Shorts视频，每个视频配有1个开放性问题和4个对抗性问题，以检验模型在视觉和叙事复杂性方面的理解能力。实验结果表明，当前视频大语言模型在准确性和鲁棒性方面与人类仍有明显差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.15028" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 12:30:33 GMT</pubDate>
</item>
<item>
<title>RLVR在推理边界扩展中的局限性研究</title>
<link>https://arxiv.org/abs/2507.14843</link>
<guid>https://arxiv.org/abs/2507.14843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVR可能限制AI发现新解，而非真正扩展推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习与可验证奖励（RLVR）在提升AI解决复杂逻辑任务中的作用。研究指出，RLVR受限于基础模型的初始概率分布，仅能对已有高奖励输出进行优化，而难以发现全新的解决方案。同时，RLVR在提高精度的同时，可能会缩小探索范围，导致遗漏原本可被基础模型找到的正确答案。实验表明，在更大的采样预算下，RLVR的实证支持范围反而缩小。此外，尽管RLVR增加了逐标记的不确定性，但最终答案的多样性却下降，表明其可能局限于少数答案。研究建议未来需通过显式探索机制或混合策略来突破这一限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Jul 2025 03:04:08 GMT</pubDate>
</item>
<item>
<title>开源数学推理语言模型MiroMind-M1的开发与性能评估</title>
<link>https://arxiv.org/abs/2507.14683</link>
<guid>https://arxiv.org/abs/2507.14683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiroMind-M1系列模型在数学推理任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MiroMind-M1系列开源推理语言模型，旨在提升数学推理任务的透明度和可复现性。该模型基于Qwen-2.5架构，在719,000个经过验证的数学推理问题上进行监督微调，并在62,000个挑战性问题上进行强化学习训练。为提高训练效率，作者提出了一种上下文感知的多阶段策略优化算法。实验结果显示，MiroMind-M1在AIME24、AIME25和MATH等基准测试中达到或超越现有开源模型的性能，并具有更高的token效率。研究团队还公开了完整的模型、数据集及训练配置，以支持后续研究和社区发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 12:21:23 GMT</pubDate>
</item>
<item>
<title>长推理下大推理模型性能下降现象研究</title>
<link>https://arxiv.org/abs/2507.14417</link>
<guid>https://arxiv.org/abs/2507.14417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">长推理导致大模型性能下降，揭示五种失败模式。</p><br /><br /><p><strong>摘要：</strong> 本文构建了评估任务，发现延长大推理模型的推理长度会降低性能，呈现出测试时计算与准确率之间的反向关系。评估任务涵盖四种类型：带干扰项的计数任务、含虚假特征的回归任务、约束跟踪的演绎任务以及高级AI风险。研究发现了五种模型在长时间推理中的失败模式，包括Claude模型被无关信息分散注意力、OpenAI o系列模型过度依赖问题框架、模型从合理先验转向虚假相关性、所有模型在复杂演绎任务中难以保持专注，以及推理延长可能加剧不良行为。研究强调了在不同推理长度下评估模型的重要性，以识别和解决这些失败模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 20:06:13 GMT</pubDate>
</item>
<item>
<title>基于单值反馈的多轮强化学习方法提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2507.14295</link>
<guid>https://arxiv.org/abs/2507.14295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多轮强化学习结合单值反馈提升模型推理与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多轮问题解决对大型推理模型（LRMs）的重要性，并指出传统强化学习方法在多轮任务中表现不足。研究提出一种基于单值反馈的强化学习框架（UFO），利用简单的用户反馈（如“再试一次”）来改进模型的多轮推理能力。实验表明，该方法不仅保持了单轮性能，还提升了多轮推理准确率高达14%。此外，通过设计合理的奖励机制，模型在减少回答轮次的同时，也能在出错时产生更细致和多样化的推理过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 14:07:38 GMT</pubDate>
</item>
<item>
<title>自动化生成高质量图像编辑数据集提升AI图像处理能力</title>
<link>https://arxiv.org/abs/2507.14119</link>
<guid>https://arxiv.org/abs/2507.14119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化管道生成高精度图像编辑数据，提升AI图像处理效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种自动化、模块化的图像编辑数据挖掘系统，能够跨领域、多分辨率、多风格地生成高质量的图像编辑三元组。该系统基于公开生成模型，无需人工干预，通过任务调优的Gemini验证器直接评估指令遵循度和美学质量，无需分割或定位模型。通过逆向和组合引导方法，数据量扩大约2.2倍，为大规模训练提供支持。研究团队发布了NHR-Edit数据集和Bagel-NHR-Edit模型，显著提升了图像编辑任务的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:50:00 GMT</pubDate>
</item>
<item>
<title>基于不确定性引导的渐进学习框架在CT图像分类中的应用</title>
<link>https://arxiv.org/abs/2507.14102</link>
<guid>https://arxiv.org/abs/2507.14102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UGPL提升CT图像分类准确率，优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为UGPL的不确定性引导渐进学习框架，用于提高CT图像的分类准确性。该方法通过全局到局部的分析策略，首先识别诊断模糊区域，再对关键区域进行详细分析。利用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取具有空间多样性的信息片段。结合自适应融合机制，UGPL能够有效整合上下文信息与细粒度特征。实验表明，UGPL在三个CT数据集上分别提升了3.29%、2.46%和8.08%的准确率，证明其在肾脏异常、肺癌和新冠检测中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.14102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Jul 2025 13:30:56 GMT</pubDate>
</item>
<item>
<title>PhyWorldBench：评估视频生成模型物理模拟能力的基准测试</title>
<link>https://arxiv.org/abs/2507.13428</link>
<guid>https://arxiv.org/abs/2507.13428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出PhyWorldBench，用于评估视频生成模型的物理真实性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhyWorldBench，这是一个用于评估视频生成模型在物理现象模拟方面表现的全面基准。该基准涵盖从基础物理规律到复杂物体交互等多个层次的物理场景，并引入了“反物理”类别以测试模型在违反现实物理规则时的逻辑一致性。研究团队评估了12个先进的文本到视频生成模型，分析它们在不同物理场景下的表现，识别出模型在遵循真实物理规律方面的关键挑战，并提出了优化提示词以提升物理真实性的建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:54:09 GMT</pubDate>
</item>
<item>
<title>基于流式处理的4D时空几何重建方法</title>
<link>https://arxiv.org/abs/2507.11539</link>
<guid>https://arxiv.org/abs/2507.11539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种实时4D几何重建模型，提升推理速度与空间一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种流式4D视觉几何变换器，借鉴自回归大语言模型的设计理念。该模型采用因果Transformer架构，通过时间因果注意力机制和历史键值缓存实现高效的在线长期4D重建。为提升训练效率，引入从密集双向视觉几何接地Transformer（VGGT）中知识蒸馏的方法，并支持迁移高效注意力算子（如FlashAttention）。实验表明，该模型在保持性能的同时显著提升了在线场景下的推理速度，为可扩展的交互式4D视觉系统提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>RoMaP：基于3D高斯编辑的精准局部3D内容修改方法</title>
<link>https://arxiv.org/abs/2507.11061</link>
<guid>https://arxiv.org/abs/2507.11061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoMaP实现精准3D高斯局部编辑，提升3D内容质量与灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoMaP框架，解决3D高斯点云局部编辑中的挑战。通过引入3D-Geometry Aware Label Prediction模块生成鲁棒的3D掩码，结合正则化SDS损失函数，有效提升局部编辑精度和上下文一致性。实验表明，RoMaP在重建和生成场景中均达到最先进的3D局部编辑效果，为更灵活、可靠的3D内容创作提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 03:54:11 GMT</pubDate>
</item>
<item>
<title>基于几何引导的弱监督自蒸馏框架GeoDistill用于跨视角定位</title>
<link>https://arxiv.org/abs/2507.10935</link>
<guid>https://arxiv.org/abs/2507.10935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GeoDistill提升跨视角定位精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出GeoDistill，一种基于几何引导的弱监督自蒸馏框架，用于提升跨视角定位的性能。该方法通过教师-学生模型结构，利用视场角（FoV）掩码生成有限视场图像，使学生模型专注于关键特征如车道线，忽略无纹理区域。实验表明，GeoDistill在不同框架下均显著提升定位效果，并引入一种无需精确平面位置标注的相对方向估计网络，为实际应用提供高效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 23:00:15 GMT</pubDate>
</item>
<item>
<title>基于4D扩散模型的高保真人体视角合成方法</title>
<link>https://arxiv.org/abs/2507.13344</link>
<guid>https://arxiv.org/abs/2507.13344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出滑动迭代去噪方法提升4D扩散模型时空一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对稀疏视角视频输入下的人体高保真视角合成问题，提出一种新的滑动迭代去噪过程。通过在潜在网格中编码图像、相机姿态和人体姿态信息，并使用滑动窗口在空间和时间维度上交替去噪，从而增强4D扩散模型的时空一致性。该方法在DNA-Rendering和ActorsHQ数据集上表现出色，显著优于现有方法，同时保持GPU内存消耗可控。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>Voxtral Mini和Voxtral Small多模态音频聊天模型发布</title>
<link>https://arxiv.org/abs/2507.13264</link>
<guid>https://arxiv.org/abs/2507.13264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voxtral模型在音频和文本任务中表现优异，支持长对话与大上下文。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Voxtral Mini和Voxtral Small两款多模态音频聊天模型，它们能够理解和处理语音和文本信息，在多个音频基准测试中表现出色。Voxtral Small模型体积较小，可在本地运行，并支持长达40分钟的音频文件和多轮对话。研究团队还发布了三个用于评估语音理解模型的基准测试。两款模型均采用Apache 2.0许可证开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:17:37 GMT</pubDate>
</item>
<item>
<title>提升多模态大模型安全性：AutoSteer技术研究</title>
<link>https://arxiv.org/abs/2507.13255</link>
<guid>https://arxiv.org/abs/2507.13255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoSteer提升多模态大模型安全性，无需微调。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为AutoSteer的模块化、自适应推理时干预技术，旨在提升多模态大语言模型（MLLMs）在面对对抗性多模态输入时的安全性。该技术包含三个核心组件：安全意识评分（SAS）、自适应安全探测器和轻量级拒绝头。实验表明，AutoSteer在多个安全关键基准测试中显著降低了攻击成功率，同时保持了模型的通用能力。该方法为多模态AI系统的安全部署提供了一个实用、可解释且有效的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 12:04:55 GMT</pubDate>
</item>
<item>
<title>基于残差学习的稀疏自编码器改进方法</title>
<link>https://arxiv.org/abs/2507.12990</link>
<guid>https://arxiv.org/abs/2507.12990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过残差学习提升稀疏自编码器在特定领域的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于残差学习的方法，用于改进稀疏自编码器（SAE）在特定领域中的表现。该方法通过训练一个辅助SAE来建模主SAE在特定文本上的重构误差，从而捕捉主模型未识别的特征。在推理阶段，将两个模型的输出相加，显著提升了跨熵和解释方差指标。实验表明，该方法能够在不牺牲通用任务性能的前提下，有效融入新领域知识，增强SAE的可解释性，为LLM的定向机制解释提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 06:57:49 GMT</pubDate>
</item>
<item>
<title>基于黎曼几何的LoRA优化方法研究</title>
<link>https://arxiv.org/abs/2507.12142</link>
<guid>https://arxiv.org/abs/2507.12142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RiemannLoRA提升LoRA收敛速度与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于黎曼几何的新型LoRA优化方法，旨在同时解决低秩矩阵分解中的过参数化问题和初始化策略问题。该方法将固定秩的LoRA矩阵视为一个光滑流形，并通过在流形上寻找损失函数下降最快的方向来实现有效初始化。实验结果表明，RiemannLoRA在大语言模型和扩散模型中均表现出更快的收敛速度和更好的最终性能，优于传统LoRA及其改进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 07:17:12 GMT</pubDate>
</item>
<item>
<title>Einstein Fields：基于神经张量场的四维时空压缩方法</title>
<link>https://arxiv.org/abs/2507.11589</link>
<guid>https://arxiv.org/abs/2507.11589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Einstein Fields通过神经张量场压缩四维相对论模拟，提升计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Einstein Fields的神经表示方法，用于将计算密集的四维数值相对论模拟压缩为紧凑的隐式神经网络权重。该方法通过建模广义相对论的核心张量场——度规，利用自动微分推导物理量。与传统的神经场（如距离场、占用场或辐射场）不同，Einstein Fields是神经张量场，在将广义相对论的时空几何编码为神经场表示时，动态特性自然地作为副产品出现。该方法在连续时空建模、无网格性、存储效率、导数精度和易用性方面展现出巨大潜力。研究团队在多个广义相对论的标准测试案例中验证了其性能，并发布了基于JAX的开源库，为数值相对论提供了更高效和表达力更强的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:55:39 GMT</pubDate>
</item>
<item>
<title>动态视觉令牌压缩方法VisionThink提升视觉语言模型效率</title>
<link>https://arxiv.org/abs/2507.13348</link>
<guid>https://arxiv.org/abs/2507.13348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisionThink通过动态调整图像分辨率提升VLM效率与精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的视觉令牌压缩方法VisionThink，该方法根据任务复杂度动态选择图像分辨率，从而在保持高精度的同时减少视觉令牌数量。相比传统固定压缩策略，VisionThink能智能判断是否需要更高分辨率图像，并通过强化学习优化决策过程。实验表明，该方法在OCR任务中表现优异，同时在一般视觉问答任务中显著节省计算资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>pi^3：一种无需固定参考视角的视觉几何重建方法</title>
<link>https://arxiv.org/abs/2507.13347</link>
<guid>https://arxiv.org/abs/2507.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">pi^3实现无参考视角的视觉几何重建，性能领先。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了pi^3，这是一种基于前馈神经网络的新方法，用于视觉几何重建，不再依赖传统的固定参考视角。传统方法常以特定视角作为基准，若该视角不佳可能导致不稳定或失败。pi^3采用全排列等变架构，预测仿射不变的相机位姿和尺度不变的局部点图，无需参考帧。这种设计使模型对输入顺序具有鲁棒性且易于扩展，其简单无偏的方法在多种任务中达到最先进水平，包括相机位姿估计、单目/视频深度估计和密集点图重建。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13347" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>上下文工程：大型语言模型推理的系统优化方法</title>
<link>https://arxiv.org/abs/2507.13334</link>
<guid>https://arxiv.org/abs/2507.13334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">上下文工程优化LLM推理，提升复杂情境理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了上下文工程这一系统性方法，用于优化大型语言模型在推理过程中的信息输入。文章将上下文工程分解为四个基础组件：上下文检索与生成、上下文处理和上下文管理，并探讨了这些组件如何集成到智能系统中，如检索增强生成、记忆系统、工具整合推理和多智能体系统。通过对1300多篇论文的系统分析，文章不仅构建了该领域的技术路线图，还揭示了一个关键研究缺口：尽管当前模型在复杂情境理解上表现出色，但在生成高质量长文本方面仍存在明显不足。因此，未来的研究应优先解决这一不对称问题，以推动更先进的上下文感知AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:36 GMT</pubDate>
</item>
<item>
<title>基于图灵机模拟的Transformer模型长度泛化方法</title>
<link>https://arxiv.org/abs/2507.13332</link>
<guid>https://arxiv.org/abs/2507.13332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TAIL方法提升Transformer模型对长序列问题的解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Transformer模型在处理比训练时更长序列问题时的挑战，并提出一种基于图灵机模拟的模仿学习方法TAIL，以增强模型的长度泛化能力。TAIL通过生成模仿图灵机执行过程的思维链数据，扩展推理步骤并引入显式内存访问机制，从而缓解短路学习和动态数据访问难题。实验表明，TAIL在多个任务中表现优异，证明图灵机的核心概念对于模型的长度泛化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:50:07 GMT</pubDate>
</item>
<item>
<title>AbGen：评估大语言模型设计消融实验能力的基准</title>
<link>https://arxiv.org/abs/2507.13300</link>
<guid>https://arxiv.org/abs/2507.13300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AbGen是首个评估LLM设计消融实验能力的基准，揭示模型与人类专家的差距。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了AbGen，这是首个用于评估大语言模型（LLM）在科学研究中设计消融实验能力的基准。AbGen包含1500个由专家标注的示例，来源于807篇自然语言处理论文。该基准要求LLM根据给定的研究背景生成详细的消融实验设计。对DeepSeek-R1-0528和o4-mini等先进模型的评估表明，它们在重要性、忠实性和合理性方面与人类专家存在显著差距。此外，文章指出当前自动化评估方法不可靠，与人工评估存在明显差异。为此，作者开发了AbGen-Eval，一个元评估基准，用于评估常用自动化评估系统在测量LLM性能方面的可靠性，并为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.13300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 13:09:22 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的多角色面部表情动画生成方法</title>
<link>https://arxiv.org/abs/2507.12956</link>
<guid>https://arxiv.org/abs/2507.12956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FantasyPortrait实现高保真多角色面部表情动画生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出FantasyPortrait，一种基于扩散Transformer的框架，能够生成高质量且富有情感的单角色和多角色面部动画。该方法通过引入表达增强学习策略，利用隐式表示捕捉与身份无关的面部动态，提升模型对细微情绪的渲染能力。针对多角色控制，设计了掩码交叉注意力机制，确保独立且协调的表情生成，避免特征干扰。为推动该领域研究，作者构建了Multi-Expr数据集和ExprBench基准测试。实验表明，FantasyPortrait在定量和定性评估中均优于现有方法，尤其在跨角色再现和多角色场景中表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 05:50:43 GMT</pubDate>
</item>
<item>
<title>AnyCap项目提升多模态生成的可控性与评估可靠性</title>
<link>https://arxiv.org/abs/2507.12841</link>
<guid>https://arxiv.org/abs/2507.12841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyCap提升多模态生成的可控性和评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AnyCap项目，旨在解决现有模型在多模态生成中缺乏精细控制和可靠评估的问题。项目包括AnyCapModel（ACM），一种轻量级框架，可在不重新训练基础模型的情况下增强其可控性；AnyCapDataset（ACD），一个包含三种模态、28种用户指令类型和30万条高质量数据的数据集；以及AnyCapEval，一个新的评估基准，提供更可靠的评估指标。实验表明，ACM显著提升了多种基础模型的生成质量，在GPT-4o上内容得分提升45%，风格得分提升12%，并在多个基准测试中取得显著成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Jul 2025 03:04:05 GMT</pubDate>
</item>
<item>
<title>可学习分词器提升语言模型适应性</title>
<link>https://arxiv.org/abs/2507.12720</link>
<guid>https://arxiv.org/abs/2507.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出FLEXITOKENS，提升语言模型的分词灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文针对语言模型在面对新数据分布时适应性差的问题，提出一种基于字节级的可学习分词器方法。传统子词分词器在适应过程中保持固定，导致分词效率低下。本文引入可学习的边界预测模块，使分词过程更具灵活性。相比现有方法，FLEXITOKENS通过简化训练目标，有效减少分词过度碎片化，并在多个多语言和形态多样任务中取得显著性能提升，最高提升达10%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 21:55:41 GMT</pubDate>
</item>
<item>
<title>MindJourney：通过世界模型提升视觉语言模型的3D空间推理能力</title>
<link>https://arxiv.org/abs/2507.12508</link>
<guid>https://arxiv.org/abs/2507.12508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MindJourney提升VLM在3D空间推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出MindJourney，一种无需微调即可增强视觉语言模型（VLM）3D空间推理能力的测试时扩展框架。该方法通过将VLM与基于视频扩散的世界模型结合，使VLM能够生成相机轨迹并合成多视角图像，从而进行更准确的空间推理。实验表明，MindJourney在SAT基准测试中平均提升了8%的性能，展示了其在提升VLM 3D理解能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>基于时序感知扩散模型的视频帧插值方法</title>
<link>https://arxiv.org/abs/2507.04984</link>
<guid>https://arxiv.org/abs/2507.04984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TLB-VFI模型，提升视频帧插值效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频帧插值任务，提出一种名为Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI) 的高效视频扩散模型。该模型通过3D波浪门控和时序感知自编码器提取丰富的时序信息，在挑战性数据集上相比最新图像扩散模型提升了20%的FID表现。同时，模型参数减少3倍，推理速度提升2.3倍，并通过光流引导显著降低训练数据需求，参数量比视频扩散模型少20倍以上。项目代码和结果已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:25:32 GMT</pubDate>
</item>
<item>
<title>GitChameleon：面向库版本的代码生成评估基准</title>
<link>https://arxiv.org/abs/2507.12367</link>
<guid>https://arxiv.org/abs/2507.12367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GitChameleon提供执行验证的代码生成基准，提升AI代码生成适应性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了GitChameleon，这是一个包含328个Python代码补全问题的数据集，每个问题都基于特定的库版本，并附带可执行单元测试。该数据集用于评估大型语言模型、代码助手等系统在特定版本下生成功能正确的代码的能力。实验表明，当前最先进的系统在此任务上表现不佳，成功率为48%-51%，凸显了该任务的复杂性。GitChameleon通过执行验证的方式，帮助研究人员更清晰地理解代码生成挑战，并推动更具适应性和可靠性的AI代码生成方法的发展。相关数据和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 12:10:42 GMT</pubDate>
</item>
<item>
<title>基于超网络的多模态模型对齐方法</title>
<link>https://arxiv.org/abs/2507.10015</link>
<guid>https://arxiv.org/abs/2507.10015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hyma提升多模态模型选择与连接训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Hyma的新方法，用于优化多模态模型的选择和连接模块训练。该方法利用超网络的参数预测能力，能够同时为N乘M种单模态模型组合生成连接模块，从而显著降低搜索最佳模型对的成本。实验表明，Hyma在多个多模态基准测试中实现了与网格搜索相当的性能，但计算成本降低了10倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 03:51:01 GMT</pubDate>
</item>
<item>
<title>跨模态知识蒸馏框架MST-Distill的提出与实验验证</title>
<link>https://arxiv.org/abs/2507.07015</link>
<guid>https://arxiv.org/abs/2507.07015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">跨模态知识蒸馏面临路径选择和知识漂移问题，MST-Distill有效提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对跨模态知识蒸馏中存在的挑战，提出了MST-Distill框架。该框架通过引入多个专用教师模型和实例级路由网络，实现动态自适应的知识迁移。同时，设计了独立训练的掩码模块，以减少模态差异并增强知识传递效果。在五个多模态数据集上的实验表明，该方法显著优于现有方法，具有良好的泛化能力和实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:28 GMT</pubDate>
</item>
<item>
<title>对比编码器与解码器语言模型的性能与适应性</title>
<link>https://arxiv.org/abs/2507.11412</link>
<guid>https://arxiv.org/abs/2507.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比编码器和解码器模型性能，发现任务适配性差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了编码器-only 和解码器-only 语言模型的性能差异。尽管大多数工作集中在解码器模型上，但编码器模型在分类和检索任务中表现更优。研究团队推出了 Ettin 套件，包含从 1700 万到 10 亿参数的成对模型，使用相同训练方法实现了 SOTA 结果。实验表明，将模型从一种架构转换到另一种任务效果不佳，且小规模编码器在分类任务中优于大规模解码器。研究开源了所有训练数据和模型检查点，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:31:51 GMT</pubDate>
</item>
<item>
<title>AI Wizards在CLEF 2025主题性检测任务中的表现</title>
<link>https://arxiv.org/abs/2507.11764</link>
<guid>https://arxiv.org/abs/2507.11764</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI Wizards通过融合情感特征提升新闻主观性检测效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AI Wizards团队在CLEF 2025 CheckThat! Lab Task 1主题性检测任务中的研究成果，旨在识别新闻文章中的主观/客观句子。该任务覆盖了单语、多语和零样本设置，并在训练/开发数据集中包含阿拉伯语、德语、英语、意大利语和保加利亚语，最终评估还引入了希腊语、罗马尼亚语、波兰语和乌克兰语等未见语言以测试模型泛化能力。研究采用基于Transformer的分类器，并通过整合情感评分与句法表示来增强模型性能。为应对类别不平衡问题，团队优化了决策阈值。实验结果表明，情感特征的引入显著提升了模型表现，尤其在主观性F1分数上效果明显，最终在希腊语任务中获得第一名。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11764" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 18:10:20 GMT</pubDate>
</item>
<item>
<title>MMHU：大规模人类行为分析基准数据集</title>
<link>https://arxiv.org/abs/2507.12463</link>
<guid>https://arxiv.org/abs/2507.12463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMHU基准，用于评估自动驾驶中的人类行为理解。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MMHU，一个大规模的人类行为分析基准数据集，包含57,000个动作片段和173万帧数据，涵盖多种来源如Waymo、YouTube视频和自采集数据。该数据集提供丰富的标注信息，包括动作轨迹、文本描述、意图标签等，旨在支持自动驾驶中的行为预测、生成和问答任务。通过构建人机协作的标注流程，确保数据质量，并为相关研究提供全面的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>SpatialTrackerV2：一种高效的单目视频3D点跟踪方法</title>
<link>https://arxiv.org/abs/2507.12462</link>
<guid>https://arxiv.org/abs/2507.12462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialTrackerV2提升3D点跟踪性能，运行速度更快。</p><br /><br /><p><strong>摘要：</strong> 本文提出SpatialTrackerV2，一种基于单目视频的前馈3D点跟踪方法。该方法将点跟踪、单目深度估计和相机位姿估计统一在一个端到端的架构中，能够分解世界空间中的3D运动为场景几何、相机自运动和像素级物体运动。通过在多种数据集上进行可扩展训练，SpatialTrackerV2在性能上优于现有3D跟踪方法30%，同时在动态3D重建任务中达到领先水平，且运行速度提高了50倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>SWE-Perf：首个针对代码性能优化的基准测试</title>
<link>https://arxiv.org/abs/2507.12415</link>
<guid>https://arxiv.org/abs/2507.12415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-Perf评估LLM在真实代码库中优化性能的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SWE-Perf，这是首个专门用于评估大型语言模型（LLMs）在真实代码仓库中进行代码性能优化任务的基准测试。该基准包含140个精心挑选的实例，来源于GitHub上性能改进的拉取请求。每个实例包括代码库、目标函数、性能测试、专家编写的补丁和可执行环境。通过评估不同方法，研究发现现有LLMs在性能优化方面与专家水平存在显著差距，揭示了该领域的重要研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:05:17 GMT</pubDate>
</item>
<item>
<title>基于空间音频的人类运动生成方法研究</title>
<link>https://arxiv.org/abs/2507.11949</link>
<guid>https://arxiv.org/abs/2507.11949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAM数据集和MOSPA框架，实现空间音频驱动的逼真人类运动生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对虚拟人类对多样化听觉刺激的动态、真实响应这一挑战，提出了首个全面的空间音频驱动人类运动（SAM）数据集，并开发了基于扩散模型的MOSPA框架。该框架通过有效的融合机制，准确捕捉人体运动与空间音频之间的关系，能够根据不同的空间音频输入生成多样且逼真的运动。实验表明，该方法在该任务上达到了最先进的性能。相关模型和数据集将在论文接受后开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 02:33:11 GMT</pubDate>
</item>
<item>
<title>RLEP：一种用于大语言模型的强化学习框架</title>
<link>https://arxiv.org/abs/2507.07451</link>
<guid>https://arxiv.org/abs/2507.07451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLEP通过经验回放提升大语言模型的训练效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RLEP的强化学习框架，旨在解决大语言模型训练过程中不稳定和策略漂移的问题。该框架分为两个阶段：首先收集验证过的轨迹，然后在后续训练中进行回放。每次更新时，策略在混合新生成数据和回放成功案例的mini-batch上进行优化。RLEP通过回放高质量示例，引导模型避免无效探索，专注于有潜力的推理路径，从而加快收敛并提升最终性能。实验结果显示，在Qwen2.5-Math-7B模型上，RLEP在更少更新次数下达到基线峰值准确率，并在多个数学竞赛数据集上显著提升了准确率。相关代码、数据集和检查点已公开，便于复现和进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 01:58:55 GMT</pubDate>
</item>
<item>
<title>基于推理时扩展计算的大型语言模型训练方法</title>
<link>https://arxiv.org/abs/2507.05065</link>
<guid>https://arxiv.org/abs/2507.05065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过工具交互增强推理计算，提升代码修复能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的机器学习范式，利用推理和训练阶段的计算扩展来提升大型语言模型的能力。不同于传统的监督微调和强化学习方法，该研究将推理过程中的计算以多轮交互形式与状态化工具结合，模型通过自定义领域特定语言控制工具。实验表明，这种设置提高了经验采样速度和奖励信号密度，使参数量达30亿的模型也能高效执行任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 10:49:18 GMT</pubDate>
</item>
<item>
<title>AnyI2V：一种无需训练的视频生成框架</title>
<link>https://arxiv.org/abs/2507.02857</link>
<guid>https://arxiv.org/abs/2507.02857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyI2V支持多种条件图像和运动轨迹生成高质量视频。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为AnyI2V的新型视频生成框架，该框架无需训练即可根据用户定义的运动轨迹动画化任意条件图像。与现有方法相比，AnyI2V支持更广泛的模态输入，如网格和点云，并具备混合条件输入、风格迁移和编辑功能。实验表明，AnyI2V在空间和运动控制方面表现出色，为视频生成提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>物理引导的3D资产生成方法PhysX</title>
<link>https://arxiv.org/abs/2507.12465</link>
<guid>https://arxiv.org/abs/2507.12465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysX实现物理属性驱动的3D资产生成。</p><br /><br /><p><strong>摘要：</strong> 文章提出PhysX，一种基于物理属性的3D资产生成框架，旨在解决现有3D生成模型忽视物理特性的不足。PhysX包含两个核心部分：PhysXNet是一个首次系统标注五维物理属性的3D数据集，并通过人机协同标注流程提升效率；PhysXGen则是一个前馈式图像到3D资产生成框架，通过双分支结构建模3D结构与物理属性的关系，确保生成结果在保持几何质量的同时具备合理的物理特性。实验表明该框架性能优越且具有良好的泛化能力，相关代码、数据和模型将公开以推动物理生成AI的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.12465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Jul 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>面向土木工程的技术图纸修订评估基准 DrafterBench</title>
<link>https://arxiv.org/abs/2507.11527</link>
<guid>https://arxiv.org/abs/2507.11527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DrafterBench用于评估LLM在技术图纸修订中的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了 DrafterBench，一个针对土木工程中技术图纸修订任务的全面评估基准。该基准包含12种任务类型、46个自定义工具和1920个任务，旨在系统评估大型语言模型代理在理解复杂指令、利用先验知识和适应动态指令质量方面的能力。DrafterBench 提供了详细的准确率分析和错误统计，以帮助深入理解AI代理的能力并为工程应用中的LLM集成提供改进方向。该基准已开源，可在GitHub和Hugging Face上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 13:56:04 GMT</pubDate>
</item>
<item>
<title>融合检索与推理的先进方法研究</title>
<link>https://arxiv.org/abs/2507.09477</link>
<guid>https://arxiv.org/abs/2507.09477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了RAG与推理结合的最新进展及未来方向。</p><br /><br /><p><strong>摘要：</strong> 本文对检索增强生成（RAG）与推理相结合的方法进行了全面综述，探讨了如何通过推理优化RAG的各个阶段，以及如何利用不同类型的检索知识支持复杂推理。文章还介绍了融合RAG与推理的框架，这些框架通过迭代搜索和推理实现高性能表现。作者分类整理了相关方法、数据集和挑战，并提出了未来在更有效、多模态适应、可信和以用户为中心的系统方面的研究方向。相关资源可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 23:29:41 GMT</pubDate>
</item>
<item>
<title>Lizard：一种用于无限上下文生成的线性化框架</title>
<link>https://arxiv.org/abs/2507.09025</link>
<guid>https://arxiv.org/abs/2507.09025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lizard提升LLM的长文本生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lizard，一种将预训练Transformer-based大型语言模型（LLM）转换为灵活、次二次架构的线性化框架，以实现无限上下文生成。Lizard通过引入次二次注意力机制，有效缓解了Transformer模型在长上下文场景下的内存和计算瓶颈。该框架结合门控线性注意力与增强元记忆的滑动窗口注意力，实现了全局上下文压缩与局部细节捕捉的平衡。同时，Lizard支持常量内存推理和更强的长度泛化能力，并通过硬件感知算法加速训练。实验表明，Lizard在标准语言建模任务中几乎恢复了教师模型的性能，并在5-shot MMLU基准上比之前方法提升了18分，在关联回忆任务中也表现出显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 17:19:18 GMT</pubDate>
</item>
<item>
<title>无需微调的视频光流提取方法</title>
<link>https://arxiv.org/abs/2507.09082</link>
<guid>https://arxiv.org/abs/2507.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于生成模型的零样本光流提取方法取得新突破。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何从视频中提取光流，而无需进行特定任务的微调。受自监督视频模型启发，作者提出了一种新的测试时方法——KL-tracing，通过注入局部扰动并计算预测分布的Kullback-Leibler散度来获取光流。该方法在真实世界和合成数据集上均优于现有方法，展示了其在高质量光流估计中的潜力。研究发现，模型的三个关键特性有助于实现零样本光流提取：未来帧的分布预测、独立处理时空块的因子化潜在表示以及可随机访问的解码机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:59:38 GMT</pubDate>
</item>
<item>
<title>基于离散扩散模型的音频补全方法研究</title>
<link>https://arxiv.org/abs/2507.08333</link>
<guid>https://arxiv.org/abs/2507.08333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型音频补全方法，适用于长间隙恢复。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于离散扩散建模的音频补全方法，该方法利用预训练音频分词器生成的离散音频表示进行重建。与以往基于波形或频谱图的方法不同，该方法在离散潜在空间中直接建模生成过程，从而实现更稳定和语义连贯的音频恢复。实验在MusicNet和MTG数据集上进行，覆盖长达500毫秒的缺失片段，结果表明该方法在长间隙恢复任务中表现优于现有基线，为音乐录音修复提供了可靠方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 02:25:49 GMT</pubDate>
</item>
<item>
<title>BYOKG-RAG：一种增强知识图谱问答的框架</title>
<link>https://arxiv.org/abs/2507.04127</link>
<guid>https://arxiv.org/abs/2507.04127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BYOKG-RAG提升知识图谱问答性能，适用于自定义图谱。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BYOKG-RAG的框架，用于增强知识图谱问答（KGQA）任务。该框架结合大型语言模型（LLM）与专门的图检索工具，通过生成关键图结构元素（如问题实体、候选答案、推理路径和OpenCypher查询），并利用图工具进行链接和上下文检索，从而提高问答的准确性和泛化能力。实验表明，BYOKG-RAG在五个不同类型的基准数据集上表现优于现有方法，并展现出对自定义知识图谱的良好适应性。框架已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 14:47:14 GMT</pubDate>
</item>
<item>
<title>面向用户生成视频的多模态字幕生成基准与模型</title>
<link>https://arxiv.org/abs/2507.11336</link>
<guid>https://arxiv.org/abs/2507.11336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UGC-VideoCap，提升多模态视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对用户生成视频（如TikTok）中音频与视觉信息融合不足的问题，提出了UGC-VideoCap基准和模型框架。该基准包含1000个经过三阶段人工标注的短视频，强调音频与视觉信息的平衡整合，并配有4000个QA对用于评估多模态理解能力。同时，作者开发了UGC-VideoCaptioner(3B)模型，基于Gemini 2.5 Flash进行轻量化训练，采用两阶段策略提升数据效率与性能。该研究为真实场景下的多模态视频字幕生成提供了高质量基础和解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 10:08:29 GMT</pubDate>
</item>
<item>
<title>基于多代理架构的视觉分类框架提升零样本场景下的AI可信度</title>
<link>https://arxiv.org/abs/2507.10571</link>
<guid>https://arxiv.org/abs/2507.10571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信任感知的多代理AI框架，提升零样本场景下的分类准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型模块化Agentic AI视觉分类框架，结合通用多模态代理、非视觉推理协调器和检索增强生成（RAG）模块，用于提升AI在零样本场景下的可信度。该框架在苹果叶片疾病诊断任务中进行了测试，结果显示使用信任感知协调器和RAG可使零样本设置下的准确率提高77.94%，达到85.63%。研究还比较了不同模型的校准效果，并通过图像-RAG技术增强了预测的可靠性。该系统将感知与元推理分离，提高了多代理AI的可扩展性和可解释性，适用于医疗诊断、生物学等对信任要求高的领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:39:29 GMT</pubDate>
</item>
<item>
<title>EXAONE 4.0：融合推理与非推理模式的多语言AI模型</title>
<link>https://arxiv.org/abs/2507.11407</link>
<guid>https://arxiv.org/abs/2507.11407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EXAONE 4.0支持多语言，具备推理与非推理模式。</p><br /><br /><p><strong>摘要：</strong> EXAONE 4.0 是一款结合了非推理模式和推理模式的AI模型，旨在提升用户体验并增强推理能力。该模型支持英语、韩语和西班牙语，包含32B和1.2B两个版本，分别适用于高性能计算和设备端应用。EXAONE 4.0在同类模型中表现优异，甚至可与前沿模型竞争，且已公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11407" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 11:24:51 GMT</pubDate>
</item>
<item>
<title>MISS-QA：评估模型解读科学文献示意图能力的基准测试</title>
<link>https://arxiv.org/abs/2507.10787</link>
<guid>https://arxiv.org/abs/2507.10787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MISS-QA是首个评估模型解读科学文献示意图的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MISS-QA，这是首个专门用于评估模型在科学文献中解读示意图能力的基准测试。该基准包含465篇科学论文中的1,500个专家标注示例。模型需根据示意图和论文上下文回答相关问题。研究评估了18种前沿多模态基础模型的表现，发现这些模型与人类专家之间存在显著差距。通过分析模型在无法回答问题上的表现及详细错误分析，揭示了当前模型在理解多模态科学文献方面的优缺点，为未来改进提供了关键见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 16:35:25 GMT</pubDate>
</item>
<item>
<title>LLM在恶意软件变种生成中的应用研究</title>
<link>https://arxiv.org/abs/2507.09411</link>
<guid>https://arxiv.org/abs/2507.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM可有效生成恶意软件变种，降低检测率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在修改恶意软件源代码以生成变种方面的可行性。作者提出了一种名为LLMalMorph的半自动化框架，该框架利用LLMs对代码的语义和语法理解能力，通过自定义提示和代码转换策略生成新的恶意软件变种，而无需资源密集型微调。实验中收集了10个不同类型的Windows恶意软件样本，并生成了618个变种。结果表明，这些变种在一定程度上降低了杀毒软件的检测率，同时保持了原始功能。此外，尽管未针对机器学习检测器进行优化，部分变种仍对基于ML的分类器表现出较高的攻击成功率。文章还讨论了当前LLM在生成恶意软件变种方面的局限性，并评估了该技术在恶意软件变种生成领域的现状。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 18:11:10 GMT</pubDate>
</item>
<item>
<title>基于缩放定律的大型基础模型数据混合优化方法</title>
<link>https://arxiv.org/abs/2507.09404</link>
<guid>https://arxiv.org/abs/2507.09404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种系统方法优化模型训练数据比例，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于缩放定律的方法，用于确定大型基础模型在不同目标领域下的最优数据混合比例。该方法能够准确预测模型在不同规模和数据权重下的损失表现，并在语言模型、多模态模型和视觉模型等多种大规模预训练场景中得到验证。研究还表明，这些缩放定律可以推广到新的数据组合和模型规模，仅需少量小规模训练即可估计大尺度性能，为模型训练提供了一种更高效、更系统的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Jul 2025 17:16:08 GMT</pubDate>
</item>
<item>
<title>OpenCodeReasoning-II数据集与代码生成及评估的改进</title>
<link>https://arxiv.org/abs/2507.09075</link>
<guid>https://arxiv.org/abs/2507.09075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCodeReasoning-II提升代码生成与评估性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OpenCodeReasoning-II数据集，包含250万条问题-解法-评注三元组，是目前最大的公开代码推理数据集。研究采用两阶段微调策略，分别优化代码生成和代码评估能力。实验表明，微调后的Qwen2.5-Instruct模型在代码生成方面表现优于或等同于现有最佳模型，并显著提升了竞赛编程性能。此外，还扩展了LiveCodeBench基准以支持C++语言，增强LLM评估的全面性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:35:54 GMT</pubDate>
</item>
<item>
<title>多智能体系统在复杂网络中的协作与推理能力评估</title>
<link>https://arxiv.org/abs/2507.08616</link>
<guid>https://arxiv.org/abs/2507.08616</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多智能体系统的协作与推理能力，提出AgentsNet新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多智能体系统在复杂网络中的自组织与协作能力，提出了一种新的基准测试框架AgentsNet。该基准借鉴分布式系统和图论的经典问题，评估多智能体在不同网络拓扑下的策略制定、自我组织和通信能力。研究发现，尽管一些先进的大语言模型在小规模网络中表现良好，但随着网络规模扩大，性能显著下降。与现有基准最多支持2-5个智能体不同，AgentsNet可扩展至更大规模，甚至支持100个智能体的测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08616" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 10:13:22 GMT</pubDate>
</item>
<item>
<title>探究大语言模型中认知偏见的成因与影响</title>
<link>https://arxiv.org/abs/2507.07186</link>
<guid>https://arxiv.org/abs/2507.07186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大模型偏见主要源于预训练而非微调。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）中认知偏见的来源，发现这些偏见主要由预训练阶段决定，而非微调过程。研究通过两次实验方法：首先多次微调模型以观察训练随机性对偏见的影响；其次引入跨微调方法，交换不同数据集以测试偏见是否依赖于数据源。结果表明，尽管训练随机性会带来一定变化，但模型的偏见模式主要由其预训练背景决定。这一发现强调在评估和减轻模型偏见时，需关注其预训练基础，而不仅仅是微调数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:01:14 GMT</pubDate>
</item>
<item>
<title>基于预训练模型的高效视觉语言模型构建方法</title>
<link>https://arxiv.org/abs/2507.07104</link>
<guid>https://arxiv.org/abs/2507.07104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效构建视觉语言模型的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Vision-Language-Vision (VLV)的自动编码器框架，通过结合预训练的视觉编码器、文本到图像扩散模型的解码器以及大语言模型，实现高效的视觉语言模型训练。该方法通过引入信息瓶颈机制，利用连续嵌入进行知识蒸馏，实现了高质量的语义理解与图像重建。通过微调预训练语言模型生成详细描述，构建出性能接近GPT-4o和Gemini 2.0 Flash的图像描述系统。该方法大幅降低了训练成本和数据需求，总训练费用低于1000美元。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>NeuralMark：一种鲁棒的神经网络水印方法</title>
<link>https://arxiv.org/abs/2507.11137</link>
<guid>https://arxiv.org/abs/2507.11137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralMark通过哈希水印滤波提升神经网络所有权保护。</p><br /><br /><p><strong>摘要：</strong> 本文提出NeuralMark，一种基于哈希水印滤波的神经网络水印方法，旨在提高深度神经网络的所有权保护能力。该方法利用哈希函数从密钥生成不可逆二进制水印，并作为过滤器选择模型参数进行嵌入，从而增强对伪造和覆盖攻击的防御能力。同时引入平均池化以抵抗微调和剪枝攻击，且可适配多种神经网络架构。理论分析与实验验证表明，NeuralMark在13种卷积和Transformer架构中表现出良好的有效性和鲁棒性，适用于图像分类和文本生成任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.11137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Jul 2025 05:38:11 GMT</pubDate>
</item>
<item>
<title>CoDi框架实现文本到图像的主体一致性与姿态多样性生成</title>
<link>https://arxiv.org/abs/2507.08396</link>
<guid>https://arxiv.org/abs/2507.08396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoDi框架提升文本到图像生成的主体一致性和姿态多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为CoDi的文本到图像生成框架，旨在解决现有方法在保持主体一致性的同时缺乏姿态和布局多样性的问题。CoDi采用两阶段策略：身份传输（IT）和身份精炼（IR）。IT在早期去噪步骤中通过最优传输将身份特征以姿态感知的方式转移到目标图像，从而保持主体一致性并保留姿态多样性；IR在后期去噪步骤中选择最显著的身份特征来进一步细化主体细节。实验结果表明，CoDi在主体一致性、姿态多样性和提示保真度方面均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 04:15:56 GMT</pubDate>
</item>
<item>
<title>将大语言模型集成到非一致性逻辑的形式语义中</title>
<link>https://arxiv.org/abs/2507.09751</link>
<guid>https://arxiv.org/abs/2507.09751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种方法，将LLM融入非一致性逻辑形式语义，提升推理一致性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保持逻辑一致性的前提下，利用大语言模型（LLM）的广泛知识进行形式化推理。作者提出了一种直接将LLM集成到非一致性逻辑形式语义解释函数中的方法，并通过多个短文本事实性基准数据集验证了该方法的可行性。与以往工作不同，该方法提供了一个理论框架，使神经符号推理能够有效利用LLM的知识，同时保留逻辑系统的可靠性与完备性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 15:05:43 GMT</pubDate>
</item>
<item>
<title>REST：一种评估大模型多任务推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.10541</link>
<guid>https://arxiv.org/abs/2507.10541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REST框架提升大模型多任务推理评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出REST（Reasoning Evaluation through Simultaneous Testing）框架，用于评估大型推理模型在多任务压力下的表现。传统评估方法存在数据污染和无法模拟真实场景的问题，而REST通过同时测试多个问题，更全面地评估模型的上下文优先级分配、跨问题干扰抵抗和动态认知负荷管理能力。实验表明，即使是最先进的模型在REST下也会出现性能下降，且REST比现有基准更具区分力。研究还发现，采用“long2short”训练技术的模型在REST中表现更优，揭示了模型在复杂任务中的关键机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:58:47 GMT</pubDate>
</item>
<item>
<title>强化学习在大语言模型推理能力中的作用与评估挑战</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升LLM推理能力，但需关注数据污染问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）对大型语言模型（LLMs）推理能力的提升作用，指出尽管Qwen2.5等模型在特定基准测试中表现优异，但其结果可能因数据污染而不具普遍性。研究发现，仅准确的奖励信号能有效提升性能，而随机或错误信号则无效。为解决这一问题，作者提出一个完全合成的算术问题生成器，构建了无泄漏数据集RandomCalculation，并建议在未污染基准上进行跨模型评估以确保结论可靠性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10532" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Recursions：提升语言模型效率的新框架</title>
<link>https://arxiv.org/abs/2507.10524</link>
<guid>https://arxiv.org/abs/2507.10524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture-of-Recursions 提升语言模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为 Mixture-of-Recursions (MoR) 的新框架，旨在同时实现参数共享和自适应计算，以提高语言模型的效率。MoR 在一个递归 Transformer 中结合了两种效率策略，通过复用共享层实现参数效率，并利用轻量级路由器动态分配不同递归深度给不同 token，从而优化计算和内存访问效率。此外，还引入了 KV 共享变体以减少预填充延迟和内存占用。实验表明，MoR 在不同模型规模下均表现出色，能够在保持较低训练 FLOPs 和较小模型尺寸的同时，显著降低验证困惑度并提升少样本准确性，同时具备更高的吞吐量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:49:00 GMT</pubDate>
</item>
<item>
<title>MoVieS：一种高效的4D动态视图合成模型</title>
<link>https://arxiv.org/abs/2507.10065</link>
<guid>https://arxiv.org/abs/2507.10065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoVieS实现单帧视频中4D动态视图的快速合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoVieS，一种新型的前馈模型，能够在一秒内从单目视频中合成4D动态新视角。该模型通过像素对齐的高斯基元网格表示动态3D场景，并显式监督其随时间变化的运动。这使得首次在单一学习框架中统一建模外观、几何和运动，实现了视图合成、重建和3D点跟踪。MoVieS通过将新视角合成与动态几何重建相结合，支持大规模训练并减少对任务特定监督的依赖，从而自然支持多种零样本应用，如场景流估计和移动物体分割。实验验证了MoVieS在多个任务中的有效性和效率，性能优异且速度显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 04:49:57 GMT</pubDate>
</item>
<item>
<title>利用ICO图像透明通道的可执行隐写术研究</title>
<link>https://arxiv.org/abs/2507.09074</link>
<guid>https://arxiv.org/abs/2507.09074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过ICO图像透明通道隐藏JavaScript代码实现隐蔽攻击。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型可执行隐写术方法，利用ICO图像文件的alpha透明通道嵌入自解压JavaScript负载。该方法通过修改非透明区域的最低有效位，在不影响视觉效果的前提下隐藏压缩后的JavaScript代码。实验表明，64x64 ICO图像可嵌入最多512字节未压缩数据或0.8KB压缩数据。浏览器在加载页面时会默认获取favicon，从而触发嵌入的加载脚本，利用原生JavaScript API和canvas像素访问在内存中执行payload，形成无需额外网络请求的隐蔽通道。测试验证了该方法在多种浏览器环境中的有效性，并分析了其对内容安全策略和杀毒软件的规避能力。文章还讨论了现有隐写分析和清理防御的局限性，揭示了静态图像与可执行内容之间界限模糊的安全风险。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 19:29:04 GMT</pubDate>
</item>
<item>
<title>构建韩国专业级大语言模型评估基准</title>
<link>https://arxiv.org/abs/2507.08924</link>
<guid>https://arxiv.org/abs/2507.08924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍两个韩国专业级大语言模型评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两个针对韩国市场的专业级大语言模型评估基准——KMMLU-Redux和KMMLU-Pro。KMMLU-Redux基于韩国国家技术资格考试题目，去除关键错误以提高可靠性；KMMLU-Pro则基于韩国国家职业执照考试，反映专业领域知识。实验表明，这些基准能够全面体现韩国工业领域的知识水平，并已公开发布数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:56:32 GMT</pubDate>
</item>
<item>
<title>结合SFT与GRPO提升大语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2507.08267</link>
<guid>https://arxiv.org/abs/2507.08267</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过延长SFT和GRPO优化提升数学模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种结合监督微调（SFT）和在线推理强化学习（GRPO）的训练方法，以提升大语言模型在数学推理任务中的准确性和效率。研究发现，延长SFT至10个epoch有助于模型达到最佳性能，而GRPO则主要用于优化解题长度。实验表明该方法在多个高难度基准测试中表现优异，包括在AI数学奥林匹克竞赛中排名靠前。作者开源了全部代码和配置，为后续研究提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08267" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 22:26:01 GMT</pubDate>
</item>
<item>
<title>DreamPoster：基于文本和图像生成高质量海报的框架</title>
<link>https://arxiv.org/abs/2507.04218</link>
<guid>https://arxiv.org/abs/2507.04218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamPoster可生成高质量海报，支持灵活布局与分辨率。</p><br /><br /><p><strong>摘要：</strong> DreamPoster是一种文本到图像生成框架，能够从用户提供的图像和文本提示中智能合成高质量海报，同时保持内容一致性，并支持灵活的分辨率和布局输出。该框架基于Seedream3.0模型构建，统一处理多种海报生成任务。在数据集构建方面，提出了系统化的数据标注流程，精确标注海报中的文字内容和排版层次信息，并构建了包含原始素材和最终海报输出的配对数据集。此外，采用渐进式训练策略，使模型逐步获得多任务生成能力，同时保持高质量生成效果。测试结果表明，DreamPoster在可用性上优于GPT-4o和SeedEdit3.0，达到88.55%。DreamPoster将上线字节跳动旗下应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 23:06:45 GMT</pubDate>
</item>
<item>
<title>EmRACE-3K数据集推动视觉语言模型在具身环境中的推理能力研究</title>
<link>https://arxiv.org/abs/2507.10548</link>
<guid>https://arxiv.org/abs/2507.10548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmRACE-3K提升VLM在具身环境中的交互能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmRACE-3K数据集，旨在解决视觉语言模型（VLMs）在具身环境中的交互与推理能力不足的问题。该数据集包含3000多个语言引导任务，涵盖导航、物体操作和多阶段目标执行等挑战。通过EmRACE-3K，研究者建立了评估VLM在探索、动态空间语义推理和多阶段目标执行三个维度上的基准。实验表明，现有模型在零样本设置下的成功率低于20%，凸显了该领域的挑战。研究进一步通过微调Qwen2.5-VL-7B模型，显著提升了其在各项任务中的表现，验证了EmRACE-3K的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.10548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Jul 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>SpeakerVid-5M数据集推动音视频双人交互虚拟人研究</title>
<link>https://arxiv.org/abs/2507.09862</link>
<guid>https://arxiv.org/abs/2507.09862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpeakerVid-5M是首个大规模音视频交互虚拟人数据集。</p><br /><br /><p><strong>摘要：</strong> 随着大模型的快速发展，数字人类领域取得了显著进展。为推动音视频双人交互虚拟人的研究，本文发布了SpeakerVid-5M数据集，包含超过8,743小时的视频片段，涵盖多种互动类型。该数据集按互动类型和数据质量分为四个类别和两个子集，适用于2D虚拟人任务。同时，还提供了基于自回归的视频聊天基线模型及评估指标，旨在为未来研究提供基准。数据集和代码将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Jul 2025 22:22:47 GMT</pubDate>
</item>
<item>
<title>CompassJudger-2：提升大语言模型评估能力的通用判官模型</title>
<link>https://arxiv.org/abs/2507.09104</link>
<guid>https://arxiv.org/abs/2507.09104</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CompassJudger-2提升LLM评估性能，具备跨领域判断能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CompassJudger-2，一种新型的通用判官模型，旨在克服现有判官模型在专业性和鲁棒性方面的不足。该模型通过任务驱动的多领域数据收集策略，结合可验证奖励监督和拒绝采样，增强内在批判性推理能力。同时引入改进的学习目标，提升了模型表现。实验表明，CompassJudger-2在多个基准测试中表现优异，其7B版本在判断准确性上与大型模型相当。此外，作者还提出了JudgerBenchV2，用于标准化判官模型的评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.09104" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 21:34:24 GMT</pubDate>
</item>
<item>
<title>基于令牌感知与层局部对比解码的真相生成方法</title>
<link>https://arxiv.org/abs/2507.04404</link>
<guid>https://arxiv.org/abs/2507.04404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的对比解码方法提升大模型事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需额外训练或模型修改的解码方法，通过将特定类型的令牌与其最相关的Transformer层进行对齐，提高事实性生成。研究发现标点符号在早期层中获得主要关注，而概念性令牌则在中间层主导语义推理。通过有选择地抑制这些令牌在各自深度的关注度，实现可控的事实退化，并利用对比信号引导最终的事实解码。实验表明该方法在多个大语言模型和基准测试中均有效提升了事实准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 10:35:43 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的视觉推理能力提升研究</title>
<link>https://arxiv.org/abs/2507.05255</link>
<guid>https://arxiv.org/abs/2507.05255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过两阶段训练提升多模态模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究如何将大型语言模型的推理能力迁移至多模态大语言模型（MLLMs），以增强其视觉推理能力。作者提出了一种两阶段范式，首先进行大规模语言冷启动微调，然后进行近1000步的多模态强化学习，超越了以往开源模型的规模。研究揭示了三个关键发现：冷启动阶段早期就能产生行为迁移，冷启动广泛记忆视觉行为，而强化学习则能识别并扩展有效模式。最终模型Open-Vision-Reasoner（OVR）在多个推理基准测试中表现优异，如MATH500达到95.3%，MathVision达到51.8%。研究公开了模型、数据和训练过程，以推动更强大的多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>Gemini 2.X 模型家族发布：提升代码与推理能力</title>
<link>https://arxiv.org/abs/2507.06261</link>
<guid>https://arxiv.org/abs/2507.06261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini 2.5 Pro 具备超强编码与推理能力，支持3小时视频处理。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了 Gemini 2.X 模型家族，包括 Gemini 2.5 Pro、Gemini 2.5 Flash、Gemini 2.0 Flash 和 Flash-Lite。其中，Gemini 2.5 Pro 是目前最强大的模型，具备前沿的代码和推理能力，并能处理长达3小时的视频内容。它在多模态理解和推理方面表现出色，可支持复杂的自主工作流。Gemini 2.5 Flash 在计算资源和延迟要求较低的情况下仍保持优秀的推理能力，而 Gemini 2.0 Flash 和 Flash-Lite 则提供了高性能且低延迟、低成本的解决方案。整体上，Gemini 2.X 模型覆盖了从性能到成本的完整优化边界。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:36:04 GMT</pubDate>
</item>
<item>
<title>基于缓存引导的语言模型隐式控制方法</title>
<link>https://arxiv.org/abs/2507.08799</link>
<guid>https://arxiv.org/abs/2507.08799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">缓存引导提升小模型链式推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为缓存引导的轻量级方法，通过一次性的干预直接作用于键值缓存，实现对语言模型的隐式控制。该方法利用GPT-4o生成的推理轨迹构建引导向量，使模型行为更偏向于显式的多步骤推理，无需微调或修改提示。实验表明，缓存引导在多种推理基准测试中提升了模型推理质量和任务性能。相比需要持续干预的激活引导技术，缓存引导在超参数稳定性、推理效率和集成便捷性方面具有显著优势，是一种更稳健且实用的可控生成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:36 GMT</pubDate>
</item>
<item>
<title>BlockFFN：一种高效的稀疏激活MoE架构及其加速技术</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlockFFN提升模型稀疏性并实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的混合专家（MoE）架构BlockFFN，旨在解决传统MoE在计算负载和稀疏性方面的不足。通过引入带有ReLU激活和RMSNorm的路由机制，实现了可微且灵活的路由策略。同时，设计了兼顾令牌级和块级稀疏性的训练目标，提升了模型在低资源环境下的加速性能。此外，结合激活稀疏性和推测解码技术，BlockFFN在实际端侧设备上实现了3.67倍的加速效果，实验结果表明其在稀疏性与性能方面均优于其他MoE基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08771" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:28:56 GMT</pubDate>
</item>
<item>
<title>基于视觉基础模型的图像分词器设计与优化</title>
<link>https://arxiv.org/abs/2507.08441</link>
<guid>https://arxiv.org/abs/2507.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新型图像分词器VFMTok，提升图像重建与生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文探索了利用预训练视觉基础模型构建图像分词器的新方向。通过冻结视觉模型作为编码器，并引入区域自适应量化框架和语义重建目标，提升了分词器的效率和语义一致性。所提出的VFMTok在图像重建和生成任务中表现出色，显著提高了生成质量与token效率，同时加速了模型收敛，实现了高保真类别条件合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 05:32:45 GMT</pubDate>
</item>
<item>
<title>评估基础模型的归纳偏置与世界模型的对齐性</title>
<link>https://arxiv.org/abs/2507.06952</link>
<guid>https://arxiv.org/abs/2507.06952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基础模型在任务中表现良好，但可能缺乏对底层世界模型的归纳偏置。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种评估基础模型的方法，通过检查其在合成数据集上的适应能力来判断其是否具备与预设世界模型一致的归纳偏置。研究发现，尽管基础模型在训练任务中表现优异，但在面对新任务时，往往未能发展出与世界模型相符的归纳偏置。特别地，在轨道轨迹训练的模型在新物理任务中无法应用牛顿力学，表现出依赖任务特定启发式的倾向，缺乏泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 11:36:15 GMT</pubDate>
</item>
<item>
<title>基于神经信号的无手图像编辑方法LoongX</title>
<link>https://arxiv.org/abs/2507.05397</link>
<guid>https://arxiv.org/abs/2507.05397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoongX利用神经信号实现无手图像编辑，性能优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出LoongX，一种基于多模态神经生理信号的无手图像编辑方法。该方法结合脑机接口和生成模型，利用EEG、fNIRS、PPG和头部运动信号捕捉用户意图，并通过跨尺度状态空间模块和动态门控融合模块整合多模态信息。实验表明，LoongX在文本驱动方法上表现相当甚至更好，尤其在结合语音时更具优势。研究展示了神经驱动生成模型在无障碍图像编辑中的潜力，并为认知驱动的创意技术提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:31:50 GMT</pubDate>
</item>
<item>
<title>基于LLM架构的自回归视频生成模型Lumos-1</title>
<link>https://arxiv.org/abs/2507.08801</link>
<guid>https://arxiv.org/abs/2507.08801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumos-1实现高效自回归视频生成，保持LLM架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出Lumos-1，一个基于大语言模型（LLM）架构的自回归视频生成模型。该模型通过引入MM-RoPE机制增强时空相关性，并采用令牌依赖策略优化帧内和帧间关系。为解决帧级损失不平衡问题，提出AR-DF方法，在训练中引入时间管屏蔽策略以提升生成质量。Lumos-1仅使用48块GPU即可达到与EMU3、COSMOS-Video2World和OpenSoraPlan相当的性能，展示了其在视频生成任务中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>NeuralOS：基于神经网络的GUI模拟框架</title>
<link>https://arxiv.org/abs/2507.08800</link>
<guid>https://arxiv.org/abs/2507.08800</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuralOS通过预测用户输入生成GUI界面，提升人机交互体验。</p><br /><br /><p><strong>摘要：</strong> NeuralOS是一个基于神经网络的框架，能够根据用户输入（如鼠标移动、点击和键盘事件）直接生成图形用户界面。该框架结合了循环神经网络（RNN）和基于扩散的神经渲染器，用于跟踪系统状态并生成屏幕图像。训练数据来自Ubuntu XFCE的大量记录，包括随机交互和AI代理生成的真实交互。实验表明，NeuralOS能够生成逼真的GUI序列，准确捕捉鼠标交互，并可靠预测应用启动等状态转换。尽管在精确建模键盘交互方面仍存在挑战，但NeuralOS为未来自适应、生成式的人机交互系统提供了重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08800" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>生成式奖励模型的脆弱性与改进方法</title>
<link>https://arxiv.org/abs/2507.08794</link>
<guid>https://arxiv.org/abs/2507.08794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式奖励模型易受表面操纵，研究提出增强方法提升鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 生成式奖励模型（LLMs-as-judges）在强化学习中被广泛用于评估答案质量，但研究发现它们对非单词符号或推理引导语句容易产生错误奖励。这种漏洞在多种模型、数据集和提示格式中普遍存在，威胁到依赖此类模型的核心算法。为解决此问题，研究引入一种数据增强策略，并训练出更稳健的奖励模型。研究强调了改进基于LLM的评估方法的重要性，并公开了相关模型和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:55:22 GMT</pubDate>
</item>
<item>
<title>基于压缩光场令牌的神经渲染方法</title>
<link>https://arxiv.org/abs/2507.08776</link>
<guid>https://arxiv.org/abs/2507.08776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效神经渲染方法CLiFT，实现高质量场景重建与视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于“压缩光场令牌（CLiFTs）”的神经渲染方法，能够保留场景的丰富外观和几何信息。该方法通过多视角编码器将图像转换为令牌，并利用潜在空间K均值算法选择关键光线作为聚类中心。随后，通过多视角“压缩器”将所有令牌的信息压缩到中心令牌中构建CLiFTs。在测试阶段，根据目标视图和计算预算收集相应数量的邻近令牌，并使用自适应计算渲染器生成新视图。实验表明，该方法在RealEstate10K和DL3DV数据集上实现了显著的数据压缩，同时保持了与现有方法相当的渲染质量，并提供了数据大小、渲染质量和速度之间的权衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:38:52 GMT</pubDate>
</item>
<item>
<title>CoPart：基于部件感知的3D生成框架</title>
<link>https://arxiv.org/abs/2507.08772</link>
<guid>https://arxiv.org/abs/2507.08772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoPart提升3D生成的细节和可控性。</p><br /><br /><p><strong>摘要：</strong> 本文提出CoPart，一种基于部件感知的3D生成框架，通过将3D对象分解为上下文相关的部件潜在表示，实现更精确的多部件生成。该方法解决了传统单潜变量表示在复杂几何结构上的不足，并增强了部件间关系建模与细粒度控制能力。研究团队还构建了Partverse数据集，支持大规模训练。实验表明，CoPart在部件级编辑、关节物体生成和场景构建方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.08772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Jul 2025 13:33:18 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型中的模态冲突与幻觉研究</title>
<link>https://arxiv.org/abs/2507.07151</link>
<guid>https://arxiv.org/abs/2507.07151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在模态冲突下的幻觉问题及缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文从模态冲突的角度探讨多模态大语言模型（MLLMs）在现实场景中产生幻觉的现象。不同于以往关注模型输出与输入之间冲突的研究，本文聚焦于不同模态输入之间的内在冲突，这些冲突直接导致了幻觉的出现。作者提出了一个名为Multimodal Modality Conflict（MMMC）的数据集来模拟这一现象，并设计了三种基于提示工程、监督微调和强化学习的方法来缓解由模态冲突引起的幻觉。实验结果表明，强化学习方法在减少幻觉方面表现最佳，而监督微调方法则表现出良好的稳定性和潜力。该研究为理解MLLMs的鲁棒性提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 07:18:38 GMT</pubDate>
</item>
<item>
<title>MetaStone-S1：基于自监督奖励模型的生成式模型</title>
<link>https://arxiv.org/abs/2507.01951</link>
<guid>https://arxiv.org/abs/2507.01951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaStone-S1通过SPRM实现与OpenAI o3相当的性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了MetaStone-S1，这是首个基于自监督过程奖励模型（SPRM）的生成式模型。该模型通过共享主干网络并使用任务特定头部进行下一步预测和过程评分，将策略模型与过程奖励模型整合到统一接口中，大幅减少了PRM参数量，提升了推理效率。MetaStone-S1支持测试时缩放（TTS），提供三种推理努力模式，并实证建立了总思考计算与TTS性能之间的缩放规律。实验表明，MetaStone-S1在仅32B参数规模下，性能可与OpenAI-o3-mini系列媲美。研究团队已开源该项目。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:01 GMT</pubDate>
</item>
<item>
<title>基于动态分块机制的端到端语言模型研究</title>
<link>https://arxiv.org/abs/2507.07955</link>
<guid>https://arxiv.org/abs/2507.07955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">H-Nets实现端到端语言建模，提升数据效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的动态分块机制，通过将内容和上下文依赖的分割策略与模型联合学习，构建了一个显式的分层网络（H-Net），从而替代传统的分词-语言模型-反分词流程。实验表明，H-Nets在计算和数据匹配的情况下，优于基于BPE分词的Transformer模型，并且多级分层结构进一步提升了性能，表现出更好的数据扩展性。预训练的H-Nets在字符层面更具鲁棒性，无需启发式规则或监督即可学习有意义的分块策略。在中文、代码和DNA序列等分词较弱的语言和模态中，H-Nets表现出显著的数据效率提升，展示了端到端模型从原始数据中学习和扩展的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:39:37 GMT</pubDate>
</item>
<item>
<title>基于Re-Bottleneck的神经音频编码器结构优化</title>
<link>https://arxiv.org/abs/2507.07867</link>
<guid>https://arxiv.org/abs/2507.07867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重构瓶颈提升音频模型在不同任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Re-Bottleneck的框架，用于优化预训练神经音频编码器的潜在空间结构。该方法通过引入一个专门通过潜在空间损失训练的内部瓶颈，实现对潜在通道的排序、与语义嵌入对齐以及引入等变性，从而提升模型在不同下游任务中的性能。实验表明，该框架能够在不牺牲重建质量的前提下，有效增强模型的灵活性和适应性，适用于多种音频应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 11:47:43 GMT</pubDate>
</item>
<item>
<title>基于固定嵌入的模块化与分层扩展方法提升大语言模型性能</title>
<link>https://arxiv.org/abs/2507.07129</link>
<guid>https://arxiv.org/abs/2507.07129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出一种新型模型扩展方法，提升大模型灵活性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种不同于传统端到端训练的大语言模型扩展方法，利用不可训练的确定性输入嵌入作为基础，实现了模块化组合和分层增长。实验表明，不同数据集训练的专家模型可通过简单平均输出logits合并为更强大的混合专家模型，且在推理任务中表现优于单一模型。同时，通过逐层构建深度Transformer，验证了模型深度与复杂推理能力之间的关系。该方法为资源高效扩展、持续学习和更民主化的AI系统开发提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 16:01:15 GMT</pubDate>
</item>
<item>
<title>非语义嵌入在Transformer模型中的有效性研究</title>
<link>https://arxiv.org/abs/2507.04886</link>
<guid>https://arxiv.org/abs/2507.04886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">非语义嵌入可替代语义嵌入并提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了传统观点，即输入嵌入是语义表示的基础。研究构建了冻结嵌入层的Transformer模型，使用基于Unicode字形结构的预计算视觉嵌入。尽管这些嵌入不包含语义信息，但模型仍能收敛、生成连贯文本，并在MMLU推理基准上优于具有可训练嵌入的模型。研究认为，传统模型中嵌入层同时学习结构和语义特征导致了表征干扰，而高阶语义是Transformer架构和数据规模的涌现属性。该研究重新定义了嵌入的作用，将其从语义容器转变为结构基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title>通过动态调整预训练模型结构提升推理效率与准确性</title>
<link>https://arxiv.org/abs/2507.07996</link>
<guid>https://arxiv.org/abs/2507.07996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">预训练模型可通过动态调整结构提升推理效率和准确率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，利用预训练大型语言模型（LLM）的层作为可组合模块，构建针对每个测试样本的定制化浅层模型。通过跳过、重复或重新排列层，形成链式结构（CoLa），并采用蒙特卡洛树搜索（MCTS）优化每种样本的最佳结构。实验表明，CoLa在保持高准确率的同时，能显著提升推理效率，并在部分原本预测错误的样本中实现正确预测，展示了预训练模型在不同输入下动态适应的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视频大模型中的时空令牌合并方法STTM</title>
<link>https://arxiv.org/abs/2507.07990</link>
<guid>https://arxiv.org/abs/2507.07990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STTM提升视频理解效率，减少计算负担。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的时空令牌合并方法STTM，旨在解决视频大语言模型中因令牌数量增加而导致的计算复杂度问题。STTM通过利用视频数据中的局部空间和时间冗余，采用分层空间令牌生成和定向时间合并策略，在多个视频问答基准测试中表现出色。实验结果显示，在50%的令牌预算下，STTM实现了两倍的速度提升且仅损失0.5%的准确性；在30%预算下，速度提升三倍，仅损失2%的准确性。此外，STTM具有查询无关性，支持同一视频不同问题间的键值缓存复用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>在线时空理解基准OST-Bench推动多模态大语言模型发展</title>
<link>https://arxiv.org/abs/2507.07984</link>
<guid>https://arxiv.org/abs/2507.07984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OST-Bench评估多模态模型在线时空推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OST-Bench，一个用于评估多模态大语言模型在在线场景中进行时空理解的基准。该基准强调模型在动态环境中逐步获取信息并结合历史记忆进行推理的能力，与真实世界的沉浸式感知更贴近。基于ScanNet、Matterport3D和ARKitScenes数据集，OST-Bench包含1.4k场景和10k问答对。实验发现，现有模型在需要复杂时空推理的任务中表现不佳，随着探索范围扩大和记忆增长，准确率下降明显。研究还揭示了模型在空间线索推理和长期记忆检索方面的性能瓶颈，为未来研究提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>视觉-语言模型的线性推理瓶颈与对齐优化研究</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示VLMs在抽象推理中的线性分离瓶颈及解决方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前最先进的视觉-语言模型（VLMs）在抽象推理任务中受到的线性分离瓶颈问题。通过引入线性可分性上限（LSC），研究发现这一瓶颈并非源于感知能力不足，而是语言模型推理路径的失败。研究指出该问题可通过针对性对齐解决，简单语义概念只需激活现有路径，而复杂关系推理则需要调整核心模型权重。研究还发现，尽管后缀微调能激活模型中潜在的推理路径，但在需要深度适应的复杂任务中，提升表示质量反而会导致模型在新提示格式下失效。该研究为VLM分析提供了新的视角，强调稳健推理的关键在于精准对齐而非单纯提升表示学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 05:23:32 GMT</pubDate>
</item>
<item>
<title>基于时间感知的视觉表示学习方法ToBo</title>
<link>https://arxiv.org/abs/2507.06543</link>
<guid>https://arxiv.org/abs/2507.06543</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToBo通过紧凑令牌预测动态场景，提升序列理解任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Token Bottleneck (ToBo) 的自监督学习方法，旨在从动态场景中提取紧凑且具有时间感知能力的视觉表示。该方法通过将场景压缩为一个瓶颈令牌，并利用少量目标图像块作为提示来预测后续场景，从而学习序列场景的表示。ToBo在视频标签传播和机器人操作等任务中表现出色，并在真实机器人环境中验证了其鲁棒性和有效性。此外，研究还验证了该方法在不同模型规模下的可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06543" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 00:57:29 GMT</pubDate>
</item>
<item>
<title>基于单图定制的扩散模型微调方法T-LoRA</title>
<link>https://arxiv.org/abs/2507.05964</link>
<guid>https://arxiv.org/abs/2507.05964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">T-LoRA解决单图定制下的过拟合问题，提升模型泛化与多样性。</p><br /><br /><p><strong>摘要：</strong> 本文提出T-LoRA，一种针对扩散模型个性化定制的时步依赖低秩微调框架。在数据有限的情况下，传统微调方法容易过拟合，影响模型的泛化能力和生成多样性。T-LoRA通过动态调整不同时间步的微调策略，并采用正交初始化确保适配器组件独立性，有效缓解了这一问题。实验表明，T-LoRA在概念保真度和文本对齐之间取得了更好的平衡，适用于数据和资源受限的场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:14:10 GMT</pubDate>
</item>
<item>
<title>SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?</title>
<link>https://arxiv.org/abs/2507.05241</link>
<guid>https://arxiv.org/abs/2507.05241</guid>
<content:encoded><![CDATA[
The rapid advancements of AI agents have ignited the long-held ambition of leveraging them to accelerate scientific discovery. Achieving this goal requires a deep understanding of the frontiers of human knowledge. As such, Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for evaluating scientific AI agents. In this work, we aim to construct the foundational architecture for general-purpose agents and validate the capabilities through leading performance on HLE. To achieve this, we introduce X-Master, a tool-augmented reasoning agent designed to emulate human researchers by interacting flexibly with external tools during its reasoning process. This agent, guided by the conceptualization of code as an interaction language, can flexibly leverage built-in Python libraries and our customized tools to augment the reasoning. We further scale its capabilities through X-Masters, a scattered-and-stacked agentic workflow that systematically enhances breadth and depth of reasoning. Our open-source solution, X-Masters, sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to exceed the 30% threshold. This work allows us to gain a deeper understanding of complex task-solving and accumulates valuable experience that can inform future advancements, guiding subsequent model training.
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:50:52 GMT</pubDate>
</item>
<item>
<title>TreeBench与TreeVGR：推动视觉基础推理的新基准与训练方法</title>
<link>https://arxiv.org/abs/2507.07999</link>
<guid>https://arxiv.org/abs/2507.07999</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TreeBench评估视觉推理能力，引入TreeVGR提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉基础推理模型缺乏全面评估的问题，提出了TreeBench基准，该基准基于三个原则：对复杂场景中细微目标的精准感知、通过边界框评估的可追溯证据，以及对物体交互和空间层次的二阶推理。TreeBench包含405个具有挑战性的视觉问答对，即使最先进的模型如OpenAI-o3也仅达到54.87%的准确率。同时，文章介绍了TreeVGR训练范式，结合强化学习实现定位与推理的联合监督，显著提升了多个基准测试的性能，证明了可追溯性在视觉基础推理中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07999" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>PyVision：动态工具生成框架提升视觉推理能力</title>
<link>https://arxiv.org/abs/2507.07998</link>
<guid>https://arxiv.org/abs/2507.07998</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyVision通过动态生成工具提升视觉推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PyVision，一个交互式、多轮的框架，使大语言模型能够自主生成、执行和优化基于Python的工具，从而实现灵活且可解释的问题解决。研究团队对PyVision生成的工具进行了分类，并在多个基准测试中分析了其应用效果。实验结果显示，PyVision显著提升了GPT-4.1和Claude-4.0-Sonnet的性能，表明动态工具使用不仅增强了模型的推理能力，还使其具备创造新工具的能力，推动视觉推理向更智能的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07998" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>几何引导提升视频扩散模型的3D一致性</title>
<link>https://arxiv.org/abs/2507.07982</link>
<guid>https://arxiv.org/abs/2507.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过几何引导提升视频生成模型的3D结构一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为几何引导（Geometry Forcing）的方法，旨在增强视频扩散模型对3D结构的理解。该方法通过将模型的中间表示与预训练几何基础模型的特征对齐，引导模型学习具有几何感知能力的潜在表示。具体包括角度对齐和尺度对齐两个目标，分别通过余弦相似度和归一化几何特征回归实现。实验结果表明，该方法在相机视角和动作条件下的视频生成任务中显著提升了视觉质量和3D一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型长视频推理能力的全栈框架</title>
<link>https://arxiv.org/abs/2507.07966</link>
<guid>https://arxiv.org/abs/2507.07966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种提升VLM长视频推理能力的全栈框架及实验验证。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全栈框架，用于扩展视觉语言模型（VLMs）在长视频上的推理能力，通过强化学习实现。该框架包含三个关键组件：一个大规模的长视频问答数据集LongVideo-Reason，一个两阶段训练流程，以及一个名为MR-SP的长视频强化学习训练基础设施。实验表明，LongVILA-R1-7B在多个长视频QA基准测试中表现优异，并在多个推理任务上超越了现有模型。此外，MR-SP系统实现了2.1倍的训练加速，支持在单个A100节点上进行长达一小时的视频强化学习训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 13:47:40 GMT</pubDate>
</item>
<item>
<title>机器谎言：大语言模型中虚假陈述的机制与评估</title>
<link>https://arxiv.org/abs/2507.07484</link>
<guid>https://arxiv.org/abs/2507.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型中虚假陈述现象及其机制。</p><br /><br /><p><strong>摘要：</strong> 本文提出“机器谎言”作为描述大语言模型中失去真实性的新概念，引入“谎言指数”作为衡量模型对真实性漠不关心的指标，并分析四种形式的谎言：空洞修辞、模糊言辞、含糊其辞和未经验证的声明。通过在多个数据集上的实验，研究发现强化学习微调和推理时的思维链提示会加剧谎言现象，尤其在政治语境中，模糊言辞成为主要策略。研究揭示了AI对齐中的系统性挑战，并为提升模型的真实性提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Jul 2025 03:11:57 GMT</pubDate>
</item>
<item>
<title>长视频生成技术的现状与分类研究</title>
<link>https://arxiv.org/abs/2507.07202</link>
<guid>https://arxiv.org/abs/2507.07202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究分析了长视频生成技术的挑战与进展。</p><br /><br /><p><strong>摘要：</strong> 本文综述了当前视频生成模型在长视频创作中的局限性，如16秒以上的视频难以保持角色一致性与场景连贯性。尽管有方法可生成长达150秒的视频，但存在帧冗余和时间多样性低的问题。研究分析了32篇相关论文，总结了关键架构组件和训练策略，并构建了一个新的分类体系，以帮助理解不同方法的设计与性能特点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 14:20:33 GMT</pubDate>
</item>
<item>
<title>LangSplatV2：提升3D语言场推理速度与精度的高效方法</title>
<link>https://arxiv.org/abs/2507.07136</link>
<guid>https://arxiv.org/abs/2507.07136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangSplatV2实现高速3D文本查询与特征投射。</p><br /><br /><p><strong>摘要：</strong> 本文提出LangSplatV2，通过引入稀疏系数场和CUDA优化，显著提升了3D语言场的推理速度和查询精度。相比之前的LangSplat，LangSplatV2在高分辨率图像下实现了476.2 FPS的高维特征投射和384.6 FPS的3D开放词汇文本查询，分别提升了42倍和47倍。该方法通过将每个高斯视为全局字典中的稀疏编码，消除了重型解码器的需要，从而大幅提高效率。实验结果表明，LangSplatV2不仅在查询准确性上表现优异，还在实际应用中具备更高的可行性。代码和演示可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 20:19:58 GMT</pubDate>
</item>
<item>
<title>利用扩散模型提升多模态大语言模型的视觉理解能力</title>
<link>https://arxiv.org/abs/2507.07106</link>
<guid>https://arxiv.org/abs/2507.07106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型可作为更优的视觉编码器，提升多模态模型的视觉理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将预训练文本到图像扩散模型作为指令感知的视觉编码器，以弥补传统CLIP模型在细粒度信息捕捉上的不足。通过分析扩散模型内部表示，发现其具有丰富的语义和强图像-文本对齐能力。研究还发现可通过文本条件引导模型关注与问题相关的区域，并探索了如何将这些特征与大语言模型对齐。同时揭示了信息泄露现象，并提出缓解策略。最终通过融合CLIP与扩散特征，在通用VQA和专业MLLM基准测试中验证了扩散模型在视觉任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于门控记忆单元的高效序列建模架构SambaY</title>
<link>https://arxiv.org/abs/2507.06607</link>
<guid>https://arxiv.org/abs/2507.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SambaY通过GMU提升解码效率与长上下文性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SambaY的新型解码器-混合解码器架构，引入门控记忆单元（GMU）实现跨层记忆共享，显著提升解码效率并保持线性预填充时间复杂度。该模型在不依赖显式位置编码的情况下，提升了长上下文处理能力，并在大规模计算条件下表现出更低的不可约损失。实验表明，SambaY在数学推理任务中优于现有基线模型，同时在vLLM框架下实现了10倍的解码吞吐量提升。训练代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 03:27:00 GMT</pubDate>
</item>
<item>
<title>Video-RTS：提升视频推理能力的高效强化学习方法</title>
<link>https://arxiv.org/abs/2507.06485</link>
<guid>https://arxiv.org/abs/2507.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Video-RTS通过高效RL和TTS策略提升视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出Video-RTS，一种基于强化学习（RL）和视频自适应测试时缩放（TTS）策略的视频推理方法，旨在提高数据效率并减少对大规模监督微调的依赖。该方法跳过资源密集的微调步骤，采用基于输出奖励的纯RL训练，并引入稀疏到密集的TTS策略以提升推理效果。实验表明，在多个视频推理基准上，Video-RTS仅使用3.6%的训练样本便实现了比现有模型平均高出2.4%的准确率，例如在Video-Holmes和MMVU基准上分别提升了4.2%和2.6%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 22:06:13 GMT</pubDate>
</item>
<item>
<title>PERK：一种高效长上下文推理方法</title>
<link>https://arxiv.org/abs/2507.06415</link>
<guid>https://arxiv.org/abs/2507.06415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PERK提升长上下文推理性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出PERK（Parameter Efficient Reasoning over Knowledge），一种用于在测试时通过轻量级模型适配器进行参数更新的高效长上下文推理方法。PERK采用两层嵌套优化循环，在元训练阶段，内层快速将上下文编码为低秩适配器，作为基础模型的参数高效记忆模块；外层则学习利用更新后的适配器准确回忆和推理相关信息。实验表明，PERK在多个长上下文推理任务中表现优异，相比标准提示方法，小模型GPT-2平均提升90%，大模型Qwen-2.5-0.5B平均提升27%。PERK在推理复杂度、长度外推和相关信息位置方面更具鲁棒性，且推理效率优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 17:38:45 GMT</pubDate>
</item>
<item>
<title>基于分层框架的自主外科手术研究</title>
<link>https://arxiv.org/abs/2505.10251</link>
<guid>https://arxiv.org/abs/2505.10251</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出分层框架实现长期、精细的自主手术操作。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自主外科手术，针对现实世界中复杂、长时间的操作需求，提出了一种分层框架。该框架包括高层任务规划策略和低层轨迹生成策略，高层通过语言空间进行任务指导，低层则负责机器人运动控制。通过离体胆囊切除实验验证了方法的有效性，实现了100%的自主成功率，标志着向临床应用迈出重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10251" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 09:04:53 GMT</pubDate>
</item>
<item>
<title>4KAgent：一种统一的图像超分辨率通用系统</title>
<link>https://arxiv.org/abs/2507.07105</link>
<guid>https://arxiv.org/abs/2507.07105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4KAgent可将低分辨率图像提升至4K甚至更高。</p><br /><br /><p><strong>摘要：</strong> 4KAgent是一种统一的图像超分辨率通用系统，能够将任何图像提升至4K分辨率甚至更高。该系统由三个核心组件构成：用于定制处理流程的Profile模块、利用视觉语言模型和图像质量评估专家进行分析的感知代理，以及执行修复计划并遵循质量驱动策略的修复代理。此外，系统还包含专门的人脸修复管道，显著提升了肖像和自拍照片的面部细节。在11个任务类别和26个基准测试中进行了严格评估，覆盖自然图像、人像、AI生成内容、卫星图像、荧光显微镜和医学影像等广泛领域，展示了其在感知和保真度指标上的优越性能。4KAgent旨在推动低级视觉任务中自主代理的研究与创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>面向包容性内容审核的多视角毒性语言检测研究</title>
<link>https://arxiv.org/abs/2507.05455</link>
<guid>https://arxiv.org/abs/2507.05455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新数据集和模型以提升毒性语言检测准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了毒性语言检测在构建安全在线环境中的重要性，指出现有模型因依赖单一标注而忽视了社区规范和语境影响。为此，研究团队创建了包含6.8K社交媒体帖子和40K标注的MODELCITIZENS数据集，并通过LLM生成对话场景增强数据。实验表明，当前主流工具在该数据集上表现不佳，而基于该数据集微调的LLAMACITIZEN-8B和GEMMACITIZEN-12B模型在分布内评估中优于GPT-o4-mini。研究强调了社区参与标注和建模对包容性内容审核的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 16:15:18 GMT</pubDate>
</item>
<item>
<title>基于多智能体的mLLM有害模因评估框架AdamMeme</title>
<link>https://arxiv.org/abs/2507.01702</link>
<guid>https://arxiv.org/abs/2507.01702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdamMeme动态评估mLLM对有害模因的理解能力。</p><br /><br /><p><strong>摘要：</strong> 随着社交媒体上多模态模因的普及，需要更有效的评估方法来衡量多模态大语言模型（mLLMs）对有害模因的理解能力。现有基准依赖静态数据集和准确率评估，难以适应快速变化的在线模因。为此，本文提出AdamMeme，一个基于多智能体的自适应评估框架，通过协作更新挑战性模因样本，全面评估mLLMs在识别有害模因方面的表现，揭示不同模型的具体弱点。实验表明，该框架能系统分析模型性能并提供细粒度分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 09:32:30 GMT</pubDate>
</item>
<item>
<title>推动文本到动作生成的零样本泛化能力</title>
<link>https://arxiv.org/abs/2507.07095</link>
<guid>https://arxiv.org/abs/2507.07095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionMillion数据集与评估框架，提升文本生成动作的零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于基于文本描述生成多样且自然的人类运动序列这一挑战性任务。尽管已有进展，但现有方法在零样本泛化方面仍存在不足，主要受限于训练数据量和缺乏全面评估体系。为此，研究团队构建了目前最大的人体运动数据集MotionMillion，包含2000小时、200万条高质量运动序列，并提出了MotionMillion-Eval作为最全面的评估基准。通过扩展模型至7B参数规模，实验验证了其在跨领域和复杂组合运动上的强大泛化能力，标志着向零样本人类运动生成迈出了重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 13:52:04 GMT</pubDate>
</item>
<item>
<title>FR3E框架提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2507.07017</link>
<guid>https://arxiv.org/abs/2507.07017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FR3E增强LLM推理稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FR3E（First Return, Entropy-Eliciting Explore）框架，用于改进大语言模型（LLM）的推理能力。该方法通过识别推理轨迹中的高不确定性决策点，并进行针对性的回溯，以构建语义基础的中间反馈，从而在无需密集监督的情况下提供更精准的指导。实验结果表明，FR3E能够提升训练稳定性、生成更长且连贯的响应，并提高完全正确推理路径的比例，验证了其在增强LLM推理能力方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.07017" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 12:45:48 GMT</pubDate>
</item>
<item>
<title>提升代码生成评估的测试用例生成方法研究</title>
<link>https://arxiv.org/abs/2507.06920</link>
<guid>https://arxiv.org/abs/2507.06920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAGA方法提升测试用例质量与覆盖率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前代码生成评估基准中测试用例数量有限且同质化的问题，提出了一种多维度测试用例生成方法SAGA，结合人类编程经验和大语言模型推理能力，显著提升了测试用例的覆盖范围和质量。实验表明，该方法在TCGBench上的检测率达到90.62%，验证准确率为32.58%，并使合成评估基准的验证准确率比LiveCodeBench-v6高出10.78%。研究旨在构建更可靠的LLM代码评估体系，推动强化学习框架中的奖励估计及自动化对抗测试生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 10:58:47 GMT</pubDate>
</item>
<item>
<title>基于多模态光谱数据的分子结构生成框架DiffSpectra</title>
<link>https://arxiv.org/abs/2507.06853</link>
<guid>https://arxiv.org/abs/2507.06853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffSpectra通过扩散模型从多模态光谱数据中生成分子结构。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiffSpectra，一个基于扩散模型的生成框架，能够直接从多模态光谱数据中推断出分子的2D和3D结构。该方法将结构解析问题建模为条件生成过程，采用SE(3)-等变架构的Diffusion Molecule Transformer来整合拓扑与几何信息，并通过SpecFormer编码器捕捉多模态光谱中的依赖关系。实验表明，DiffSpectra在结构恢复任务中表现出色，具有较高的准确率。该工作是首个将多模态光谱推理与联合2D/3D生成建模相结合的分子结构解析框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 09 Jul 2025 09:57:20 GMT</pubDate>
</item>
<item>
<title>混合线性注意力机制在长序列建模中的研究与优化</title>
<link>https://arxiv.org/abs/2507.06457</link>
<guid>https://arxiv.org/abs/2507.06457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究混合线性注意力模型在长序列任务中的表现与优化方法。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了多种线性注意力模型在独立和混合架构中的表现，训练并开源了72个模型，涵盖不同参数规模和混合比例。实验表明，优秀的独立线性模型在混合架构中未必表现最佳，而增加全注意力层能显著提升召回性能，尤其在3:1以下比例时效果更明显。研究强调了选择性门控、层次递归和可控遗忘的重要性，并推荐HGRN-2或GatedDeltaNet等架构在3:1至6:1的混合比例下实现高效的Transformer级召回性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:54:11 GMT</pubDate>
</item>
<item>
<title>PAPO：一种增强多模态推理的感知意识强化学习方法</title>
<link>https://arxiv.org/abs/2507.06448</link>
<guid>https://arxiv.org/abs/2507.06448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAPO提升多模态任务中的视觉感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PAPO，一种基于GRPO的感知意识策略优化方法，通过引入隐式感知损失提升模型在多模态任务中的表现。PAPO无需额外数据或外部奖励模型，仅依赖内部监督信号，在多个多模态基准测试中取得显著提升，尤其在高视觉依赖任务中效果更佳。研究还发现并解决了损失黑客问题，通过双熵损失进行缓解，为视觉驱动的强化学习提供了新框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 19:22:34 GMT</pubDate>
</item>
<item>
<title>AutoTriton：基于强化学习的Triton编程模型</title>
<link>https://arxiv.org/abs/2507.05687</link>
<guid>https://arxiv.org/abs/2507.05687</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoTriton利用强化学习优化Triton编程，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AutoTriton，这是一个基于强化学习（RL）的Triton编程模型，旨在优化深度学习中的内核开发。通过监督微调和GRPO算法，AutoTriton能够自动调整关键参数，如tile大小和内存访问模式，从而提高计算效率。实验表明，AutoTriton在多个基准测试中表现优异，接近主流大模型。研究还验证了各个模块对性能提升的重要性，展示了强化学习在生成高性能内核方面的潜力，为构建更高效的AI系统奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05687" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 01:38:24 GMT</pubDate>
</item>
<item>
<title>解耦推理与证明：提升自动化定理证明的新框架</title>
<link>https://arxiv.org/abs/2507.06804</link>
<guid>https://arxiv.org/abs/2507.06804</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出解耦推理与证明的新框架，显著提升数学难题的自动证明能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对自动化定理证明（ATP）中大型语言模型在形式化证明方面的表现不足问题，提出一种将高层推理与底层证明分离的新框架。该框架采用两个专门模型：一个用于生成多样化的子目标引理，另一个用于严格验证这些引理。实验表明，该方法在2000年后国际数学奥林匹克竞赛（IMO）难题集上成功解决了5道题目，展示了在复杂数学挑战中实现自动化推理的重要进展。研究还发布了包含大量生成和验证引理的数据集，以促进未来相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06804" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 18:38:49 GMT</pubDate>
</item>
<item>
<title>Nova Premier模型的安全评估与公开发布</title>
<link>https://arxiv.org/abs/2507.06260</link>
<guid>https://arxiv.org/abs/2507.06260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nova Premier通过安全评估，符合公开发布标准。</p><br /><br /><p><strong>摘要：</strong> Nova Premier是亚马逊最强大的多模态基础模型，支持文本、图像和视频处理，具有100万词的上下文窗口。文章介绍了基于前沿模型安全框架对Nova Premier进行的全面风险评估，重点考察了化学、生物、放射性和核（CBRN）、进攻性网络操作以及自动化AI研发等高风险领域。评估结合了自动化基准测试、专家红队测试和提升研究，结果显示Nova Premier符合2025年巴黎人工智能安全峰会的承诺，可安全公开发布。未来将继续完善安全评估和缓解流程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 09:33:35 GMT</pubDate>
</item>
<item>
<title>面向自动驾驶的视觉-语言-行动模型综述</title>
<link>https://arxiv.org/abs/2506.24044</link>
<guid>https://arxiv.org/abs/2506.24044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述自动驾驶中视觉-语言-行动模型的发展与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文首次全面回顾了面向自动驾驶的视觉-语言-行动（VLA4AD）模型。文章梳理了VLA模型的架构组成，追溯了从早期解释器到以推理为核心的模型演进过程，并对比分析了20多个代表性模型。同时，文章整合了现有数据集和评估标准，强调了驾驶安全、准确性和解释质量的综合评估。最后，文章指出了VLA4AD面临的开放性挑战，如鲁棒性、实时效率和形式化验证，并提出了未来研究方向，为可解释且符合社会规范的自动驾驶系统提供了参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 12:50:02 GMT</pubDate>
</item>
<item>
<title>Agent KB：提升智能体错误纠正与跨领域知识复用的框架</title>
<link>https://arxiv.org/abs/2507.06229</link>
<guid>https://arxiv.org/abs/2507.06229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent KB提升智能体任务解决能力，实现跨领域知识共享。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Agent KB，这是一个基于Reason-Retrieve-Refine管道的分层经验框架，旨在提高智能体在复杂任务中的表现。该框架通过捕捉高层策略和详细执行日志，构建共享知识库，实现跨智能体的知识迁移。在GAIA基准测试中，Agent KB使成功率达到最高提升16.28个百分点，显著提升了Claude-3和GPT-4等模型的性能。此外，在SWE-bench代码修复任务中也表现出色。研究结果表明，Agent KB为智能体提供了一个模块化、框架无关的学习基础设施，使其能够从过往经验中学习并推广成功策略到新任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>NeoBabel：多语言图像生成的新范式</title>
<link>https://arxiv.org/abs/2507.06137</link>
<guid>https://arxiv.org/abs/2507.06137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBabel支持六种语言，提升图像生成的多语言性能与包容性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了NeoBabel，一种新型多语言图像生成框架，支持英语、中文、荷兰语、法语、印地语和波斯语。该模型通过大规模多语言预训练和高分辨率指令调优进行训练，在扩展的多语言基准测试中表现出色，同时保持强大的英语能力。NeoBabel在多语言任务上优于现有模型，并且模型规模更小。研究还引入了新的评估指标，以衡量多语言对齐和代码混合提示的鲁棒性。作者开源了工具包，包括代码、模型检查点、数据集和标准化评估协议，旨在推动包容性AI研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:19:45 GMT</pubDate>
</item>
<item>
<title>MedGemma：医疗视觉-语言基础模型的开发与应用</title>
<link>https://arxiv.org/abs/2507.05201</link>
<guid>https://arxiv.org/abs/2507.05201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedGemma在医疗任务中表现出色，提升AI在医疗领域的应用潜力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedGemma，一个基于Gemma 3 4B和27B的医疗视觉-语言基础模型集合。MedGemma在医学图像和文本理解方面表现出色，性能优于同类模型并接近专用模型水平，同时保持了Gemma 3基础模型的通用能力。在分布外任务中，MedGemma在多模态问答、胸部X光诊断分类和代理评估中分别提升了2.6%-10%、15.5%-18.1%和10.8%。微调后进一步提升了子领域的表现，如电子健康记录信息检索错误率降低50%，并在气胸分类和组织病理学图像分类中达到现有先进方法的水平。此外，MedSigLIP作为医疗优化的视觉编码器，也展现出与专业医学图像编码器相当或更优的性能。MedGemma为医疗研究和下游应用提供了强大的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:01:44 GMT</pubDate>
</item>
<item>
<title>世界模型的理论探讨与新型架构设计</title>
<link>https://arxiv.org/abs/2507.05169</link>
<guid>https://arxiv.org/abs/2507.05169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨世界模型的定义与构建，提出新型多层级架构。</p><br /><br /><p><strong>摘要：</strong> 本文从科幻作品《沙丘》出发，结合心理学中的假设性思维概念，对当前世界模型的研究进行了批判性分析。文章指出，世界模型的核心目标是模拟现实世界的可行动可能性，以支持有目的的推理与行为。基于此，作者提出了一种基于分层、多级、连续/离散混合表示的新架构，并引入生成式自监督学习框架，展望了由该模型驱动的物理、代理和嵌套（PAN）通用人工智能系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:23:46 GMT</pubDate>
</item>
<item>
<title>无监督语义场景补全方法SceneDINO的研究</title>
<link>https://arxiv.org/abs/2507.06230</link>
<guid>https://arxiv.org/abs/2507.06230</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SceneDINO实现无监督3D场景语义补全，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为SceneDINO的无监督语义场景补全方法，无需依赖昂贵的标注数据。该方法结合自监督表示学习和2D无监督场景理解技术，通过多视角一致性自监督进行训练，仅凭单张图像即可推断出3D几何结构和语义特征。通过创新的3D特征蒸馏方法，实现了无监督3D语义分割。实验表明，SceneDINO在3D和2D无监督场景理解任务中均达到最先进水平，其3D特征线性探测性能与当前有监督方法相当，并展现出良好的领域泛化能力和多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06230" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>改进Mamba模型的注意力分配机制</title>
<link>https://arxiv.org/abs/2507.06204</link>
<guid>https://arxiv.org/abs/2507.06204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何优化Mamba模型的注意力分配以提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将针对Transformer的微分设计方法应用于Mamba模型，该模型基于选择性状态空间层，具有更高的效率。研究发现，直接应用传统方法效果不佳，需进行针对性调整。为此，作者提出了一种新的微分机制，并在语言建模基准上进行了验证，结果表明该方法提升了Mamba的检索能力和整体性能。此外，通过广泛的消融实验和分析，验证了设计的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06204" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:30:14 GMT</pubDate>
</item>
<item>
<title>SingLoRA：一种更稳定且参数更少的低秩微调方法</title>
<link>https://arxiv.org/abs/2507.05566</link>
<guid>https://arxiv.org/abs/2507.05566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SingLoRA提升模型微调稳定性并减少参数使用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SingLoRA的低秩微调方法，通过将权重更新分解为单个低秩矩阵与其转置的乘积，解决了传统LoRA中矩阵尺度不一致导致的训练不稳定问题。该设计不仅消除了矩阵间的尺度冲突，还减少了约50%的参数量。理论分析表明，SingLoRA在无限宽度神经网络框架下能保证稳定的特征学习。实验结果表明，在MNLI数据集上，SingLoRA在使用更少参数的情况下实现了比LoRA和LoRA+更高的准确率；在图像生成任务中，SingLoRA在DreamBooth数据集上的表现优于DoRA和LoRA。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:11:30 GMT</pubDate>
</item>
<item>
<title>AXLearn：模块化深度学习系统的设计与实现</title>
<link>https://arxiv.org/abs/2507.05411</link>
<guid>https://arxiv.org/abs/2507.05411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AXLearn支持异构硬件，提升模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AXLearn，一个专注于模块化和异构硬件支持的深度学习系统。相比其他先进系统，AXLearn通过严格的组件封装，实现了高效的模型开发与实验。文章提出了一种基于代码量复杂度的模块化评估方法，证明了其在扩展时保持稳定复杂度的优势。例如，在集成Rotary Position Embeddings功能时，仅需10行代码即可完成，而其他系统则需要数百行。同时，AXLearn在性能上与当前最先进的系统相当。最后，作者分享了AXLearn开发与运营的经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 14:50:58 GMT</pubDate>
</item>
<item>
<title>StreamVLN：一种高效的多模态视觉语言导航框架</title>
<link>https://arxiv.org/abs/2507.05240</link>
<guid>https://arxiv.org/abs/2507.05240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamVLN提升实时视觉语言导航效率与稳定性。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamVLN，一种用于现实世界中视觉语言导航（VLN）的流式框架。该框架采用混合慢速-快速上下文建模策略，结合视觉、语言和动作输入进行多模态推理。快速对话流支持响应式动作生成，而慢速更新记忆流通过3D感知的token剪枝策略压缩历史视觉状态。这种设计实现了多轮对话的连贯性，同时保持有限的上下文大小和计算成本。实验表明，StreamVLN在VLN-CE基准测试中表现出色，具有稳定的低延迟，适用于实际部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:49:41 GMT</pubDate>
</item>
<item>
<title>LOOM-Scope：一种高效且全面的长上下文评估框架</title>
<link>https://arxiv.org/abs/2507.04723</link>
<guid>https://arxiv.org/abs/2507.04723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOOM-Scope提供标准化的长上下文评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出LOOM-Scope，一个用于评估大语言模型长上下文能力的高效且全面的框架。该框架统一了不同基准的评估设置，支持高效的长上下文推理加速方法，并引入了一个轻量级但全面的基准套件，以帮助研究者更准确地评估模型性能。由于长上下文评估的计算成本较高，LOOM-Scope旨在降低社区进行综合评估的难度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 03:33:24 GMT</pubDate>
</item>
<item>
<title>Nile-Chat系列模型：支持阿拉伯语和拉丁语的埃及方言大语言模型</title>
<link>https://arxiv.org/abs/2507.04569</link>
<guid>https://arxiv.org/abs/2507.04569</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Nile-Chat系列模型提升埃及方言文本处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Nile-Chat-4B、3x4B-A6B和12B等针对埃及方言的大语言模型，这些模型能够理解和生成阿拉伯语和拉丁语文本。其中，Nile-Chat-3x4B-A6B采用Branch-Train-MiX策略，将不同脚本的专业模型融合为一个MoE模型。实验表明，这些模型在新推出的埃及语评估基准上显著优于LLaMa、Jais和ALLaM等主流模型，12B版本在拉丁语基准上比Qwen2.5-14B-Instruct高出14.4%。所有资源均已公开，为双语脚本语言的模型适配提供了系统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04569" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:53:41 GMT</pubDate>
</item>
<item>
<title>基于属性切换的公平图生成框架FAROS</title>
<link>https://arxiv.org/abs/2507.03728</link>
<guid>https://arxiv.org/abs/2507.03728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FAROS通过属性切换实现图生成的公平性与准确性平衡。</p><br /><br /><p><strong>摘要：</strong> 本文提出FAROS，一种基于属性切换机制的公平图生成框架，直接在预训练图扩散模型的生成过程中运行。该方法通过调整节点的敏感属性，在保持原始图结构特征的同时，提升生成图的公平性。实验表明，FAROS在链接预测任务中有效减少公平性差异，并在准确性和公平性之间取得更好的权衡，优于其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:31:41 GMT</pubDate>
</item>
<item>
<title>基于FLOPs的LLM重排序器效率评估方法研究</title>
<link>https://arxiv.org/abs/2507.06223</link>
<guid>https://arxiv.org/abs/2507.06223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出E2R-FLOPs评估LLM重排序器效率与效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLM）在信息检索中的重排序任务中计算资源消耗过高的问题，提出了一种新的评估方法E2R-FLOPs。该方法通过每PetaFLOP的排名指标（RPP）和每PetaFLOP的查询吞吐量（QPP）来衡量模型的效率与效果，克服了传统指标受硬件和运行设置影响的局限性。同时，构建了一个可解释的FLOPs估算器，无需实际运行即可预估模型计算量。通过广泛实验，评估了多种架构的LLM重排序器，探讨了效率与效果之间的权衡关系，旨在提升该领域的研究透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:56:28 GMT</pubDate>
</item>
<item>
<title>数据多样性在机器人操作中的关键作用研究</title>
<link>https://arxiv.org/abs/2507.06219</link>
<guid>https://arxiv.org/abs/2507.06219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据多样性对机器人学习影响显著，任务多样性更重要。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据多样性在机器人学习中的作用，重点分析了任务、机器人本体和专家演示三个维度。研究发现，任务多样性比每项任务的示范数量更为重要，有助于模型在不同场景下的迁移。此外，高质量的单一本体数据预训练模型在微调时表现出更优的扩展性，而专家多样性可能对策略学习产生干扰，尤其是速度多模态是主要因素。基于此，作者提出一种分布去偏方法，提升了模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:52:44 GMT</pubDate>
</item>
<item>
<title>潜层推理：大语言模型的多步骤推理新范式</title>
<link>https://arxiv.org/abs/2507.06203</link>
<guid>https://arxiv.org/abs/2507.06203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">潜层推理提升大语言模型的推理效率与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文综述了潜层推理这一新兴领域，探讨了如何通过模型的连续隐藏状态进行多步骤推理，从而避免依赖自然语言的限制。文章首先分析了神经网络层在推理中的基础作用，接着介绍了多种潜层推理方法，包括基于激活的递归、隐藏状态传播和微调策略。最后，讨论了通过掩码扩散模型实现无限深度潜层推理等先进范式，旨在统一潜层推理的概念框架，并为大语言模型的认知研究提供未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:29:07 GMT</pubDate>
</item>
<item>
<title>CriticLean：提升形式化语义准确性的强化学习框架</title>
<link>https://arxiv.org/abs/2507.06181</link>
<guid>https://arxiv.org/abs/2507.06181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CriticLean提升形式化语义准确性，优化自动定理证明。</p><br /><br /><p><strong>摘要：</strong> 本文提出CriticLean，一个基于强化学习的批判性评估框架，旨在提升自然语言数学陈述到形式化代码的语义准确性。通过训练CriticLeanGPT模型评估Lean 4形式化语义一致性，并构建CriticLeanBench基准测试模型区分正确与错误形式化。研究还创建了包含28.5万题的FineLeanCorpus数据集，验证了CriticLean在提升形式化可靠性方面的有效性。结果表明，优化批判阶段对生成可靠形式化至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 13:03:39 GMT</pubDate>
</item>
<item>
<title>OmniPart：支持语义解耦与结构一致的3D对象生成框架</title>
<link>https://arxiv.org/abs/2507.06165</link>
<guid>https://arxiv.org/abs/2507.06165</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniPart实现3D对象的可编辑部件生成，提升交互应用性能。</p><br /><br /><p><strong>摘要：</strong> OmniPart是一种新型的3D对象生成框架，旨在实现组件间的高语义解耦和结构一致性。该方法将任务分为两个协同阶段：第一阶段通过自回归结构规划模块生成可控制的3D部件边界框序列，并利用灵活的2D部件掩码进行引导；第二阶段则通过空间条件校正流模型，从预训练的3D生成器中高效合成所有部件。OmniPart支持用户定义的部件粒度和精确定位，适用于多种下游应用，实验表明其性能达到当前最优水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06165" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:46:15 GMT</pubDate>
</item>
<item>
<title>Code Triangle框架评估大语言模型的编程能力</title>
<link>https://arxiv.org/abs/2507.06138</link>
<guid>https://arxiv.org/abs/2507.06138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过Code Triangle框架评估LLM的编程能力，发现其存在局限性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Code Triangle框架，从编辑分析、代码实现和测试用例生成三个维度系统评估大语言模型（LLMs）的编程能力。通过在竞赛编程基准上的实验，发现虽然LLMs能够形成自洽的系统，但其解决方案缺乏人类程序员的多样性和鲁棒性。研究揭示了模型认知与人类专家之间的显著分布差异，模型错误往往因训练数据偏差和推理迁移有限而聚集。文章建议引入人类生成的编辑指南、解决方案和多样化测试用例，以及使用模型混合方法，以显著提升LLMs的性能和鲁棒性。此外，研究还展示了LLMs在认知上的一致性与不一致性，为自我反思和自我改进提供了潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.06138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 12:20:43 GMT</pubDate>
</item>
<item>
<title>Tora2：多实体视频生成的运动引导模型改进</title>
<link>https://arxiv.org/abs/2507.05963</link>
<guid>https://arxiv.org/abs/2507.05963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tora2提升视频生成中外观与运动的多实体定制能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tora2，这是Tora模型的增强版本，旨在提升视频生成中外观和运动的多实体定制能力。Tora2引入了解耦个性化提取器，能够为多个开放集实体生成更细致的个性化嵌入。同时，设计了门控自注意力机制，整合轨迹、文本描述和视觉信息，减少多模态条件对齐问题。此外，通过对比损失函数优化运动动态与实体一致性。实验结果表明，Tora2在保持竞争力的同时，实现了更高级的运动控制，标志着多条件视频生成的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 09:11:40 GMT</pubDate>
</item>
<item>
<title>基于多轮定位的策略优化方法提升大模型视觉理解能力</title>
<link>https://arxiv.org/abs/2507.05920</link>
<guid>https://arxiv.org/abs/2507.05920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MGPO方法提升大模型在高分辨率图像中的视觉定位能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Multi-turn Grounding-based Policy Optimization (MGPO)的端到端强化学习框架，使大模型能够通过多轮对话自动聚焦关键视觉区域，无需额外标注。与监督微调相比，MGPO利用最终答案的正确性作为二元奖励信号，有效提升了模型的视觉定位能力。实验表明，MGPO在标准数据集上表现出色，在分布内和分布外任务中均优于现有方法，甚至超越了OpenAI的o1和GPT-4o模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 08:05:05 GMT</pubDate>
</item>
<item>
<title>GUI代理任务规划与视觉定位的优化方法</title>
<link>https://arxiv.org/abs/2507.05791</link>
<guid>https://arxiv.org/abs/2507.05791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GTA1模型解决GUI任务规划和视觉定位问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对GUI代理在任务规划和视觉定位中的两个主要挑战，提出了GTA1模型。该模型通过测试时缩放方法，在每一步采样多个动作提案，并利用判断模型选择最优方案，提升决策质量。同时，通过强化学习实现更精确的视觉元素定位。实验表明，GTA1在多个基准测试中表现优异，准确率分别达到50.1%、92.4%和67.7%，并展示了出色的代理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 04:52:18 GMT</pubDate>
</item>
<item>
<title>医学视频生成新进展：MedVideoCap-55K数据集与MedGen模型</title>
<link>https://arxiv.org/abs/2507.05675</link>
<guid>https://arxiv.org/abs/2507.05675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">医学视频生成面临挑战，MedGen模型表现优异。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成技术在开放领域取得显著进展，但医学视频生成仍处于探索阶段。医学视频在临床培训、教育和模拟中具有重要作用，需要高视觉保真度和严格的医学准确性。然而，现有模型在处理医学提示时常产生不现实或错误内容，主要原因是缺乏大规模高质量的医学数据集。为此，研究者推出了MedVideoCap-55K，这是首个大规模、多样化且带有丰富描述的医学视频数据集，包含超过5.5万条精心筛选的视频片段。基于该数据集，研究人员开发了MedGen模型，在多个基准测试中表现出色，其性能可与商业系统媲美。该研究希望推动医学视频生成领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 08 Jul 2025 00:58:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的数据记忆现象研究</title>
<link>https://arxiv.org/abs/2507.05578</link>
<guid>https://arxiv.org/abs/2507.05578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨LLM的记忆现象及其检测与缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文综述了大型语言模型（LLMs）在训练过程中对数据的记住现象，分析了影响记忆的关键因素，如训练数据重复、训练动态和微调过程。文章还介绍了多种检测方法，包括基于前缀的提取、成员推理和对抗性提示，并评估了它们的有效性。此外，讨论了记忆带来的法律和伦理问题，以及数据清洗、差分隐私和后训练遗忘等缓解策略。最后，指出了在减少有害记忆与保持模型性能之间平衡的挑战，并提出了未来研究的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 21:30:46 GMT</pubDate>
</item>
<item>
<title>PRING：首个基于图级别的蛋白质相互作用预测基准</title>
<link>https://arxiv.org/abs/2507.05101</link>
<guid>https://arxiv.org/abs/2507.05101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PRING是首个从图级视角评估PPI预测的基准，推动生物研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PRING，这是首个从图级别评估蛋白质-蛋白质相互作用（PPI）预测的综合基准。PRING包含21,484个蛋白质和186,818个相互作用的高质量多物种数据集，并设计了应对数据冗余和泄露的策略。该基准提出了两种互补的评估范式：拓扑导向任务和功能导向任务，涵盖PPI网络构建、蛋白复合体路径预测、GO模块分析及关键蛋白识别等。实验表明，现有PPI模型在恢复结构和功能特性方面存在局限，PRING为开发更有效的PPI预测模型提供了可靠平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>any4：一种无需预处理的4位权重量化方法</title>
<link>https://arxiv.org/abs/2507.04610</link>
<guid>https://arxiv.org/abs/2507.04610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">any4在不预处理权重或激活的情况下实现高精度4位量化。</p><br /><br /><p><strong>摘要：</strong> 本文提出any4，一种无需预处理权重或激活的4位权重量化方法，能够提供任意数值表示。实验表明，any4在多个模型（如Llama 2、Llama 3、Mistral和Mixtral）上相比int4、fp4和nf4等其他4位表示方法具有更高的准确性。此外，any4在不需要预处理的情况下仍能与需要预处理的技术（如AWQ和GPTQ）相媲美。研究还探索了any3和any2在更低比特下的表现，并展示了仅需一个精心挑选的多样化样本即可进行校准，而非传统方法所需的数百个样本。同时，作者开源了tinygemm，一个针对LLM优化的GPU矩阵乘法库，支持any4及其他常见量化方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 21:59:47 GMT</pubDate>
</item>
<item>
<title>基于LLM的网络代理计算资源优化研究</title>
<link>https://arxiv.org/abs/2507.04103</link>
<guid>https://arxiv.org/abs/2507.04103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出优化LLM网络代理的计算分配策略，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大语言模型（LLM）的网络代理在开放源代码系统中的发展滞后问题，提出了一种统计基础的计算资源分配方法。通过两阶段流程，使用Llama 3.1 8B模型模仿Llama 3.3 70B教师模型，并结合监督微调和在线策略强化学习，显著提升了性能。研究发现该方法对超参数高度敏感，因此通过采样1,370个配置并利用自助法估计有效参数，避免了昂贵的试错过程。实验表明，该策略在WorkArena和MiniWob++上优于单独使用SFT或RL，且仅需55%的计算资源即可达到纯SFT的最佳性能，有效缩小了与闭源模型的差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 13:12:33 GMT</pubDate>
</item>
<item>
<title>SAMed-2：基于SAM-2架构的医学图像分割基础模型</title>
<link>https://arxiv.org/abs/2507.03698</link>
<guid>https://arxiv.org/abs/2507.03698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAMed-2提升医学图像分割性能，解决数据噪声与任务迁移问题。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAMed-2，一个基于SAM-2架构的医学图像分割基础模型。为应对医学数据复杂性、标注噪声及多模态学习挑战，SAMed-2引入时间适配器和置信度驱动的记忆机制，以捕捉图像相关性并存储高置信特征。研究构建了MedBank-100k数据集，涵盖七种成像模态和21项分割任务。实验表明，SAMed-2在多任务场景中优于现有最佳方法，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:30:38 GMT</pubDate>
</item>
<item>
<title>基于可验证情感奖励的强化学习框架提升语言模型情感智能</title>
<link>https://arxiv.org/abs/2507.03112</link>
<guid>https://arxiv.org/abs/2507.03112</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLVER框架提升LLM情感智能，增强对话能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLVER，一个端到端的强化学习框架，利用模拟用户的可验证情感奖励来提升大型语言模型的情感智能。通过与自我一致的情感模拟用户进行对话轮次，生成确定性情感评分作为奖励信号，引导模型学习。在Qwen2.5-7B-Instruct模型上使用PPO微调后，Sentient-Benchmark得分从13.3提升至79.2，同时保持数学和编码能力。实验表明，RLVER能持续提升多种对话能力，思考型模型在共情和洞察力上表现更优，而非思考型模型则偏向行动导向。不同优化算法（如GRPO和PPO）对性能有不同影响，且中等难度环境可能带来更优结果。研究证明RLVER是构建情感智能、全面能力语言代理的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03112" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 14:33:18 GMT</pubDate>
</item>
<item>
<title>面向记忆代理的基准测试MemoryAgentBench</title>
<link>https://arxiv.org/abs/2507.05257</link>
<guid>https://arxiv.org/abs/2507.05257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MemoryAgentBench评估LLM代理的记忆能力。</p><br /><br /><p><strong>摘要：</strong> 本文指出，当前大型语言模型代理的基准测试主要关注推理、规划和执行能力，而对记忆能力的评估不足。作者定义了记忆代理的四项核心能力：准确检索、测试时学习、长程理解与冲突解决，并指出现有数据集无法满足这些需求。为此，作者提出了MemoryAgentBench，一个专门针对记忆代理的新基准，结合了重构和新构建的数据集，全面覆盖上述四项能力，为评估记忆质量提供系统化测试平台。实验表明，现有方法在掌握所有能力方面仍有不足，凸显了进一步研究综合记忆机制的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于强化学习的实体关系抽取方法研究</title>
<link>https://arxiv.org/abs/2507.04642</link>
<guid>https://arxiv.org/abs/2507.04642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-RE提升关系抽取的跨领域泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文将关系抽取任务重新定义为受标注指南引导的推理过程，提出R1-RE框架，利用可验证奖励的强化学习方法提升模型的跨领域泛化能力。实验结果显示，R1-RE-7B模型在Sem-2010和MDKG数据集上的平均OOD准确率约为70%，与GPT-4o等先进模型相当。研究还深入分析了RLVR范式在关系抽取中的训练动态和涌现推理行为，提供了新的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 23:50:59 GMT</pubDate>
</item>
<item>
<title>基于Llama 3.2 1B的隐私保护医疗转录系统研究</title>
<link>https://arxiv.org/abs/2507.03033</link>
<guid>https://arxiv.org/abs/2507.03033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种隐私保护的医疗转录系统，提升临床记录效率。</p><br /><br /><p><strong>摘要：</strong> 本文旨在开发一种隐私保护、可在设备端运行的医疗转录系统，采用微调后的Llama 3.2 1B模型，将医学转录内容转化为结构化医疗笔记。通过参数高效微调方法（LoRA）在1,500对合成数据上训练，并在两个数据集上评估其性能。结果显示，该系统在ROUGE和BERTScore等指标上显著优于基础模型，同时减少了重大幻觉问题并提高了事实准确性。该方法有助于解决医疗AI应用中的隐私、成本和可访问性问题，适用于资源有限的环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 21:51:49 GMT</pubDate>
</item>
<item>
<title>视觉嵌入模型中的有序属性捕捉研究</title>
<link>https://arxiv.org/abs/2507.03683</link>
<guid>https://arxiv.org/abs/2507.03683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现视觉嵌入模型可捕捉连续有序属性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉嵌入模型是否能通过线性方向（称为rank axes）捕捉连续的有序属性。研究发现，许多流行的编码器在多个数据集上具备这种能力，即通过投影嵌入可以保持属性的顺序。令人惊讶的是，只需少量样本或两个极端例子即可恢复有意义的rank axes，而无需全面监督。这一发现为图像排序在向量数据库中的应用提供了新可能，并推动了对可排序嵌入结构和学习方法的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 12:03:31 GMT</pubDate>
</item>
<item>
<title>UnMix-NeRF：结合光谱解混的神经辐射场方法</title>
<link>https://arxiv.org/abs/2506.21884</link>
<guid>https://arxiv.org/abs/2506.21884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UnMix-NeRF通过光谱解混提升材料分割与视图合成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UnMix-NeRF，一种将光谱解混引入神经辐射场（NeRF）的框架，实现联合高光谱新视角合成与无监督材料分割。该方法通过建模光谱反射率，利用全局端元字典表示纯材料特征，并通过每点丰度捕捉其分布。材料分割基于学习端元的光谱预测，实现无监督聚类。此外，UnMix-NeRF支持场景编辑，通过修改端元字典实现灵活的材料外观操控。实验表明，该方法在光谱重建和材料分割方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:42:49 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的图像编辑系统X-Planner</title>
<link>https://arxiv.org/abs/2507.05259</link>
<guid>https://arxiv.org/abs/2507.05259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Planner提升文本引导图像编辑的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出X-Planner，一个基于多模态大语言模型的图像编辑系统，旨在解决现有方法在理解复杂指令、保持身份一致性和避免意外编辑方面的不足。X-Planner通过链式思维推理将复杂指令分解为简单子指令，并自动生成精确的编辑类型和分割掩码，无需人工干预。此外，作者还构建了一个大规模数据生成管道，使X-Planner在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于策略判别器的奖励建模方法POLAR及其性能提升</title>
<link>https://arxiv.org/abs/2507.05197</link>
<guid>https://arxiv.org/abs/2507.05197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">POLAR通过策略判别提升奖励模型性能，显著优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的奖励建模方法POLAR，将奖励建模视为策略判别器，通过量化两个策略之间的差异来生成奖励信号，从而引导训练策略向目标策略靠拢。与依赖绝对偏好的传统方法不同，POLAR关注策略间的相对差异，适用于通用排序关系建模。实验表明，POLAR在多个任务中显著提升了奖励模型的性能，如在STEM任务中偏好准确率从54.8%提升至81.0%，在创意写作任务中从57.9%提升至85.5%。此外，POLAR在RLHF中的表现也十分稳健，提升了多个大模型的性能。研究还发现计算量与性能之间存在明显的幂律关系，表明POLAR具有良好的扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:56:31 GMT</pubDate>
</item>
<item>
<title>基于低帧率相机的高速4D捕捉系统</title>
<link>https://arxiv.org/abs/2507.05163</link>
<guid>https://arxiv.org/abs/2507.05163</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需高速相机的高帧率4D重建方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于低帧率相机的高速4D捕捉系统，通过异步拍摄和生成模型提升重建效果。在拍摄端，采用异步采集方案，通过分组相机和基础帧率实现等效100-200 FPS的高帧率。在处理端，引入基于视频扩散的修复模型，解决稀疏视角重建中的伪影问题，提升细节精度和时间一致性。实验表明，该方法在高速4D重建上优于同步采集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05163" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 12:18:35 GMT</pubDate>
</item>
<item>
<title>自动化历史文献修复方法与全页数据集研究</title>
<link>https://arxiv.org/abs/2507.05108</link>
<guid>https://arxiv.org/abs/2507.05108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoHDR方法提升历史文献修复效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种全页历史文献修复数据集（FPHDR）和自动化修复方法（AutoHDR），旨在解决传统修复方法在多模态和大规模修复上的不足。FPHDR包含真实和合成图像，涵盖不同损伤等级的字符和行级标注。AutoHDR通过OCR辅助定位损伤、视觉语言文本预测和补丁自回归修复三个阶段模拟历史学家的工作流程，并支持人机协作。实验表明，该方法显著提升了OCR识别准确率，从46.83%提升至84.05%，协作后更达94.25%。该研究对文化遗产保护具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.05108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 11:26:17 GMT</pubDate>
</item>
<item>
<title>ArtifactsBench：自动化评估视觉代码生成的新基准</title>
<link>https://arxiv.org/abs/2507.04952</link>
<guid>https://arxiv.org/abs/2507.04952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtifactsBench实现对视觉代码生成的多模态自动评估。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ArtifactsBench，这是一个用于自动化、多模态评估视觉代码生成的新基准和范式。该框架通过程序化渲染生成的视觉制品并捕获其动态行为，结合源代码由多模态大语言模型进行评估。研究构建了1,825个多样化任务，并评估了30多个领先的大语言模型。实验表明，ArtifactsBench在排名一致性上与WebDev Arena高度一致，且与人类专家有超过90%的匹配度，证明其可大规模可靠评估用户感知质量。分析显示通用模型通常优于领域特定模型。作者开源了ArtifactsBench及其相关资源，以推动以用户为中心的生成模型发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 07 Jul 2025 08:53:00 GMT</pubDate>
</item>
<item>
<title>VLM2Vec-V2：跨多种视觉形式的统一嵌入框架</title>
<link>https://arxiv.org/abs/2507.04590</link>
<guid>https://arxiv.org/abs/2507.04590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLM2Vec-V2支持多模态输入并提升嵌入性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出VLM2Vec-V2，一种支持文本、图像、视频和视觉文档输入的统一嵌入框架。为弥补现有模型对非自然图像支持不足的问题，研究团队构建了MMEB-V2基准，涵盖视觉文档检索、视频检索等五种新任务。实验表明，VLM2Vec-V2在视频和文档检索任务中表现优异，并优于现有基线模型。该研究为多模态嵌入学习提供了有效策略，推动了更广泛的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 20:51:57 GMT</pubDate>
</item>
<item>
<title>大型语言模型在预测任务中的表现评估</title>
<link>https://arxiv.org/abs/2507.04562</link>
<guid>https://arxiv.org/abs/2507.04562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型预测能力仍不及超级预测者。</p><br /><br /><p><strong>摘要：</strong> 本文评估了当前最先进的大型语言模型在464个Metaculus预测问题上的表现，发现尽管这些模型的Brier得分看似超过普通人群，但与超级预测者相比仍有显著差距。这表明虽然大型语言模型在多种任务中表现出色，但在未来事件预测方面仍需进一步提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 18:26:59 GMT</pubDate>
</item>
<item>
<title>DreamVLA：融合世界知识预测的视觉-语言-动作框架</title>
<link>https://arxiv.org/abs/2507.04447</link>
<guid>https://arxiv.org/abs/2507.04447</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamVLA提升机器人操作的泛化与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DreamVLA，一种新型的视觉-语言-动作框架，通过整合全面的世界知识预测，实现逆动力学建模，构建感知-预测-行动循环。该框架引入动态区域引导的世界知识预测，并结合空间和语义线索，提供紧凑而全面的动作规划表示。为减少训练过程中动态、空间和语义信息的干扰，采用块状结构注意力机制，防止信息泄露。此外，使用基于扩散的Transformer模型对未来的动作分布进行建模。实验表明，DreamVLA在真实机器人任务中取得76.7%的成功率，在CALVIN ABC-D基准测试中平均长度为4.44。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04447" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 12:14:29 GMT</pubDate>
</item>
<item>
<title>MOD-X：面向异构智能体的模块化开放去中心化交换框架</title>
<link>https://arxiv.org/abs/2507.04376</link>
<guid>https://arxiv.org/abs/2507.04376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOD-X 提供一种新型智能体互操作架构，提升异构系统集成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出 MOD-X（Modular Open Decentralized eXchange）框架，旨在解决当前人工智能系统中智能体之间通信协议不足的问题。MOD-X 采用分层架构，包含通用消息总线、状态管理、翻译能力和基于区块链的安全机制，支持不同架构、厂商和知识表示的异构智能体之间的集成。其核心创新包括发布-订阅通信模型、语义能力发现和动态工作流编排，为实现真正去中心化、可扩展的智能体生态系统提供了可行方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 08:46:57 GMT</pubDate>
</item>
<item>
<title>SeqTex：基于视频预训练模型的端到端3D纹理生成框架</title>
<link>https://arxiv.org/abs/2507.04285</link>
<guid>https://arxiv.org/abs/2507.04285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeqTex直接生成UV纹理图，提升3D一致性与真实感。</p><br /><br /><p><strong>摘要：</strong> 本文提出SeqTex，一个端到端的3D纹理生成框架，利用预训练视频模型的视觉知识直接生成完整的UV纹理图。不同于以往方法依赖多视角图像和后处理，SeqTex将任务转化为序列生成问题，学习多视角渲染与UV纹理的联合分布，从而有效迁移图像空间先验至UV域。通过解耦多视角与UV分支、几何引导注意力以及自适应令牌分辨率等创新设计，SeqTex在无需后处理的情况下生成高质量UV纹理，实验表明其在图像和文本条件下的3D纹理生成任务中均达到最优性能，具有更强的3D一致性、纹理-几何对齐性和现实泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 06 Jul 2025 03:58:36 GMT</pubDate>
</item>
<item>
<title>PresentAgent：将长文档转化为同步视听演示的多模态代理</title>
<link>https://arxiv.org/abs/2507.04036</link>
<guid>https://arxiv.org/abs/2507.04036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PresentAgent生成同步视听演示视频，提升信息传达效果。</p><br /><br /><p><strong>摘要：</strong> PresentAgent是一种多模态代理，能够将长文档转化为同步的视觉和语音演示视频。与现有方法仅生成静态幻灯片或文本摘要不同，PresentAgent通过模块化流程生成高质量的视觉帧和上下文相关的语音叙述，并实现精确的音画同步。为评估其输出质量，研究者引入了PresentEval框架，该框架基于视觉-语言模型对视频内容进行综合评分。实验表明，PresentAgent在多项指标上接近人类水平，展示了可控多模态代理在动态展示静态文本材料方面的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 09:24:15 GMT</pubDate>
</item>
<item>
<title>基于GUI的统一数据集合成框架Easy Dataset提升领域语言模型性能</title>
<link>https://arxiv.org/abs/2507.04009</link>
<guid>https://arxiv.org/abs/2507.04009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Easy Dataset通过GUI合成高质量数据，提升LLM领域适应性。</p><br /><br /><p><strong>摘要：</strong> 本文提出Easy Dataset，一个通过图形用户界面从非结构化文档中合成微调数据的统一框架。用户可配置文本提取模型和分块策略，将原始文档转化为连贯文本片段，并利用基于角色的提示方法生成多样化的问答对。过程中的人机交互界面有助于审查和优化中间结果，确保数据质量。实验表明，使用该框架生成的数据集能显著提升语言模型在金融问答任务中的表现，同时保持其通用知识。项目已在GitHub上开源，获得超过9000星标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.04009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 05 Jul 2025 07:38:59 GMT</pubDate>
</item>
<item>
<title>StreamDiT：实现实时视频生成的流式模型</title>
<link>https://arxiv.org/abs/2507.03745</link>
<guid>https://arxiv.org/abs/2507.03745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamDiT实现实时视频生成，支持交互与流媒体应用。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamDiT，一种基于流式处理的视频生成模型，解决了现有模型仅能生成短片段的问题。通过引入流动缓冲区和混合训练策略，提升了内容一致性和视觉质量。模型采用自适应层归一化DiT结构，并结合多步蒸馏方法，显著降低了计算量，使模型在单块GPU上达到16 FPS的实时性能，支持512p分辨率的视频流生成。实验表明，该模型在定量指标和人类评估中均表现优异，适用于实时视频生成和交互场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 14:00:01 GMT</pubDate>
</item>
<item>
<title>MemOS：面向持续学习与个性化建模的内存操作系统</title>
<link>https://arxiv.org/abs/2507.03724</link>
<guid>https://arxiv.org/abs/2507.03724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MemOS提升LLM长期推理与知识一致性。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在长期上下文推理、持续个性化和知识一致性方面的挑战，指出现有模型依赖静态参数和短期上下文状态，难以有效管理用户偏好和更新知识。尽管RAG引入了外部知识，但缺乏生命周期管理和持久化整合。为此，作者提出MemOS，一个将内存作为可管理资源的操作系统，统一表示、调度和演化文本、激活和参数级记忆，通过MemCube实现灵活的知识迁移与融合，提升计算效率并支持持续学习和个性化建模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 13:21:46 GMT</pubDate>
</item>
<item>
<title>基于Transformer的软件漏洞严重性预测模型VLAI</title>
<link>https://arxiv.org/abs/2507.03607</link>
<guid>https://arxiv.org/abs/2507.03607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLAI模型准确预测软件漏洞严重性，提升漏洞分类效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VLAI，一个基于Transformer架构的模型，能够直接从文本描述中预测软件漏洞的严重性等级。该模型基于RoBERTa进行微调，使用超过60万条真实漏洞数据进行训练，预测准确率超过82%，可显著提升漏洞优先级排序的效率，减少对人工CVSS评分的依赖。VLAI模型和相关数据集已开源，并集成到Vulnerability-Lookup服务中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 10:28:14 GMT</pubDate>
</item>
<item>
<title>BMMR：多语言、多模态、跨学科推理数据集的构建与应用</title>
<link>https://arxiv.org/abs/2507.03483</link>
<guid>https://arxiv.org/abs/2507.03483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BMMR是一个跨学科的多模态推理数据集，用于评估和训练大型多模态模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BMMR，这是一个大规模的双语、多模态、跨学科推理数据集，旨在支持大型多模态模型（LMMs）的研究与评估。该数据集包含11万道大学水平的题目，覆盖300个联合国教科文组织定义的学科，涵盖多种题型，并从纸质和数字媒体中获取。数据经过人工与自动化框架筛选，每个实例都配有高质量的推理路径。数据集分为BMMR-Eval（20,458个高质量实例）和BMMR-Train（88,991个实例），用于评估和研究。此外，作者还提出了BMMR-Verifier，用于精确评估推理过程。实验表明，即使是最先进的模型在BMMR-Eval上仍有提升空间，且不同学科表现差异明显。开源模型仍落后于专有模型，但通过微调可缩小差距。研究揭示了当前LMMs在跨学科推理中的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 07:20:09 GMT</pubDate>
</item>
<item>
<title>DiaFORGE提升企业API调用成功率的对话框架研究</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiaFORGE提高企业API调用成功率27%-49%</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DiaFORGE，一种以消除歧义为核心的三阶段对话框架，旨在解决大型语言模型在调用企业API时因工具相似或参数不明确而失败的问题。该框架通过生成多轮对话、进行监督微调以及动态评估模型表现，显著提升了工具调用的成功率。在动态基准测试中，使用DiaFORGE训练的模型比GPT-4o和Claude-3.5-Sonnet分别高出27个百分点和49个百分点。同时，作者发布了包含5000个企业级API规范及验证过的对话数据集，为构建可靠的工具调用代理提供了实用蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 04 Jul 2025 02:49:02 GMT</pubDate>
</item>
<item>
<title>RefineX：大规模预训练数据的精准优化框架</title>
<link>https://arxiv.org/abs/2507.03253</link>
<guid>https://arxiv.org/abs/2507.03253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefineX提升预训练数据质量，增强模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RefineX，一种用于大规模预训练数据精准优化的框架。该方法通过程序化编辑任务实现细粒度数据修正，同时保持文本的多样性和自然性。RefineX将高质量的专家指导结果提炼为最小化的删除指令，构建高效可靠的精炼模型，可在不同规模模型上显著提升下游任务表现。实验表明，RefineX在多个基准测试中优于原始数据、过滤数据或替代方法，具有高效率和精确度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.03253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 22:19:58 GMT</pubDate>
</item>
<item>
<title>OmniDraft：一种支持多模型协同的高效推测解码框架</title>
<link>https://arxiv.org/abs/2507.02659</link>
<guid>https://arxiv.org/abs/2507.02659</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniDraft实现单一模型适配多种目标模型并提升推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出OmniDraft，一种统一框架，使单一draft模型能够与任何目标模型协作，并动态适应用户数据。通过在线n-gram缓存和混合蒸馏微调解决词汇不匹配问题，并利用自适应推测技术提升解码速度。该框架适用于设备端大语言模型应用，展示了在数学推理、编程和文本生成任务中的有效性，可使Llama-68M模型与多个目标模型如Vicuna-7B、Qwen2-7B和Llama3-8B进行推测解码，并带来1.5-2倍的速度提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02659" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:20:41 GMT</pubDate>
</item>
<item>
<title>RoboBrain 2.0：面向物理环境的多模态AI模型</title>
<link>https://arxiv.org/abs/2507.02029</link>
<guid>https://arxiv.org/abs/2507.02029</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboBrain 2.0是先进的多模态AI模型，支持空间与时间推理。</p><br /><br /><p><strong>摘要：</strong> RoboBrain 2.0是最新一代的具身视觉-语言基础模型，旨在统一感知、推理和规划以完成复杂的物理环境任务。该模型包含两个版本：轻量级7B模型和全尺寸32B模型，采用异构架构，结合视觉编码器和语言模型。尽管体积较小，RoboBrain 2.0在多种具身推理任务中表现出色。32B版本在空间和时间基准测试中均取得领先结果，超越了之前的开源和专有模型。它支持关键的现实世界具身AI能力，如空间理解（例如功能预测、空间指代、轨迹预测）和时间决策（例如闭环交互、多智能体长期规划、场景图更新）。本文详细介绍了模型架构、数据构建、多阶段训练策略、基础设施及实际应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02029" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:05:33 GMT</pubDate>
</item>
<item>
<title>对比MLM与CLM在文本表示学习中的效果与优化策略</title>
<link>https://arxiv.org/abs/2507.00994</link>
<guid>https://arxiv.org/abs/2507.00994</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究比较MLM与CLM在文本表示上的表现及效率。</p><br /><br /><p><strong>摘要：</strong> 本文通过大规模实验对比了基于MLM和CLM的预训练方法在文本表示任务中的表现。虽然MLM通常表现更优，但CLM在数据效率和微调稳定性方面更具优势。研究还提出了一种结合CLM和MLM的双阶段训练策略，在计算资源有限的情况下取得了最佳效果。此外，利用现有的CLM预训练模型可以降低训练高性能编码器模型的计算成本。所有实验结果和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00994" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:45:48 GMT</pubDate>
</item>
<item>
<title>扩散模型在动态系统模拟中的潜在应用</title>
<link>https://arxiv.org/abs/2507.02608</link>
<guid>https://arxiv.org/abs/2507.02608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型在潜空间中模拟动态系统表现良好。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在动态系统模拟中使用扩散模型的可行性，特别是在潜空间中进行生成而非像素空间。研究发现，潜空间模拟在压缩率高达1000倍的情况下仍保持较高的准确性。此外，基于扩散的模拟器比非生成方法更精确，并能通过预测的多样性来补偿不确定性。文章还讨论了训练潜空间模拟器的关键设计选择，包括架构和优化器等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 09:32:50 GMT</pubDate>
</item>
<item>
<title>LitBench：首个用于创意写作评估的标准化基准与数据集</title>
<link>https://arxiv.org/abs/2507.00769</link>
<guid>https://arxiv.org/abs/2507.00769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LitBench提升创意写作模型评估准确性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LitBench，这是首个用于创意写作验证的标准化基准和配对数据集。该数据集包含从Reddit中提取的2,480个去偏见、人工标注的故事比较测试集，以及43,827对人工偏好标签的训练语料。通过LitBench，研究者评估了零样本语言模型作为评判者的性能，并训练了Bradley-Terry和生成式奖励模型。实验表明，训练后的模型在准确率上优于所有现成模型，并通过在线人类研究进一步验证了其与人类偏好的一致性。研究结果为创意写作系统的自动化评估和优化提供了可靠资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 10:10:36 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在计算机视觉任务中的性能评估</title>
<link>https://arxiv.org/abs/2507.01955</link>
<guid>https://arxiv.org/abs/2507.01955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态模型在视觉任务中表现一般，但具备一定通用性。</p><br /><br /><p><strong>摘要：</strong> 本文评估了GPT-4o、o4-mini、Gemini 1.5 Pro等多模态基础模型在图像分类、目标检测等标准计算机视觉任务中的表现。由于这些模型主要训练用于文本输出，难以直接处理视觉任务，研究通过提示链技术将其转化为文本可处理的任务。结果显示，尽管这些模型在专业视觉模型面前仍有差距，但在语义任务上表现较好，且对提示变化不敏感的模型表现更优。GPT-4o在非推理模型中表现最佳，而具有图像生成能力的模型则存在幻觉和空间错位问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>EKA-EVAL：面向多语言大模型的统一评估框架</title>
<link>https://arxiv.org/abs/2507.01853</link>
<guid>https://arxiv.org/abs/2507.01853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EKA-EVAL提供多语言大模型评估，覆盖35个基准。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了EKA-EVAL，这是一个面向多语言大模型的统一评估框架，整合了超过35个基准测试，包括10个针对印度语言的数据集。该框架支持推理、数学、工具使用、长文本理解和阅读理解等任务，并具备分布式推理、量化和多GPU支持等功能。与现有工具相比，EKA-EVAL具有更广泛的基准覆盖，是首个端到端、可扩展的评估套件，旨在降低多语言模型评估的门槛。该框架已开源，并计划扩展至100多个基准，构建一个强大的多语言评估生态系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:07:54 GMT</pubDate>
</item>
<item>
<title>基于文本描述的多器官医学分割模型CRISP-SAM2</title>
<link>https://arxiv.org/abs/2506.23121</link>
<guid>https://arxiv.org/abs/2506.23121</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRISP-SAM2提升多器官医学图像分割精度与细节表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CRISP-SAM2的新模型，用于改进多器官医学图像分割。该模型通过跨模态交互和语义提示机制，增强对视觉信息的理解，并减少对几何提示的依赖。此外，引入了相似性排序自更新策略和掩码优化过程，以提高局部细节的准确性。实验结果表明，CRISP-SAM2在多个公开数据集上优于现有模型，显示出其在解决当前分割模型不足方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23121" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 03:05:27 GMT</pubDate>
</item>
<item>
<title>视觉语言分割中的幻觉评估基准研究</title>
<link>https://arxiv.org/abs/2506.21546</link>
<guid>https://arxiv.org/abs/2506.21546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HalluSegBench评估视觉语言分割中的幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言分割模型中常见的幻觉问题，提出了首个专门用于评估视觉接地幻觉的基准HalluSegBench。该基准包含1340对反事实实例对，覆盖281个独特物体类别，并引入了新的度量标准来量化在视觉一致场景修改下的幻觉敏感性。实验表明，基于视觉的幻觉比基于标签的幻觉更为普遍，强调了反事实推理在诊断接地准确性中的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>InnerControl：提升扩散模型空间控制精度的新方法</title>
<link>https://arxiv.org/abs/2507.02321</link>
<guid>https://arxiv.org/abs/2507.02321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InnerControl通过全阶段空间一致性训练提升图像生成精度。</p><br /><br /><p><strong>摘要：</strong> 尽管文本到图像的扩散模型取得了显著进展，但实现精确的空间控制仍具挑战。ControlNet及其改进版本ControlNet++通过引入条件模块和循环一致性损失来提升控制效果，但忽略了中间生成阶段。为此，InnerControl提出一种训练策略，在所有扩散步骤中强制空间一致性。该方法通过轻量级卷积探针从UNet中间特征中重建输入控制信号（如边缘、深度），即使在高噪声潜变量下也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测与目标条件之间的差异，InnerControl提升了控制精度和生成质量，并结合ControlNet++等现有技术实现了多种条件方法的最先进性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 01:25:53 GMT</pubDate>
</item>
<item>
<title>大型语言模型的自我纠正盲点研究</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs存在自我纠正盲点，影响其可靠性。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）已取得显著进展，但它们仍会犯错并陷入无效推理路径。自我纠正能力对可信的LLM至关重要，尤其是自回归模型。然而，LLMs在用户输入中能识别错误，却无法修正自身输出中的相同错误，这种现象被称为‘自我纠正盲点’。为系统研究这一问题，研究人员提出了Self-Correction Bench框架，通过在三个复杂度级别上注入错误进行测试。实验显示，14个模型平均有64.5%的盲点率。研究发现，这一限制与训练数据组成有关：人类演示数据多为无错误响应，而非纠错序列，而强化学习训练的模型则通过反馈学习纠错。令人惊讶的是，仅添加“Wait”即可减少89.3%的盲点，表明该能力存在但需激活。该研究揭示了当前LLMs的关键缺陷，并为提升其可靠性和可信度提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:41:30 GMT</pubDate>
</item>
<item>
<title>利用LLM辅助科学论文局限性识别的基准研究</title>
<link>https://arxiv.org/abs/2507.02694</link>
<guid>https://arxiv.org/abs/2507.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在识别科学论文局限性方面具有潜力，LimitGen基准提升其反馈能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在科学论文同行评审中的应用，特别是在识别论文局限性方面的潜力。作者提出了一个全面的局限性分类体系，并基于此构建了LimitGen基准，包含合成数据集和真实人类撰写的局限性数据集。通过引入文献检索增强LLM的识别能力，该方法提升了LLM生成具体且有建设性反馈的能力，为早期研究反馈和补充人工评审提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:04:38 GMT</pubDate>
</item>
<item>
<title>基于能量模型的系统2思维推理方法研究</title>
<link>https://arxiv.org/abs/2507.02092</link>
<guid>https://arxiv.org/abs/2507.02092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EBTs通过无监督学习实现系统2推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的基于能量模型的推理方法（Energy-Based Transformers, EBTs），旨在通过无监督学习实现类似人类系统2思维的推理能力。与传统方法相比，EBTs无需额外监督或训练，而是通过显式验证输入与候选预测的兼容性，并将预测问题重新建模为以验证器为基准的优化问题。实验表明，EBTs在文本和视觉等多模态任务中均表现出色，训练速度更快，推理性能优于Transformer++和Diffusion Transformers。此外，EBTs在多数下游任务中表现更优，表明其具有更好的泛化能力，是一种有前景的模型扩展范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>IntFold：一种可控制的生物分子结构预测基础模型</title>
<link>https://arxiv.org/abs/2507.02025</link>
<guid>https://arxiv.org/abs/2507.02025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IntFold在结构预测方面达到AlphaFold3水平，支持多种定制化任务。</p><br /><br /><p><strong>摘要：</strong> IntFold是一种可控制的基础模型，用于通用和专业生物分子结构预测。其预测精度与AlphaFold3相当，并采用优化的注意力核。除了标准结构预测外，IntFold还可通过适配器预测别构状态、约束结构和结合亲和力。此外，研究团队引入了一种新的置信度头，以更精细地评估对接质量，特别是在抗体-抗原复合物等复杂目标上表现突出。文章还分享了训练该计算密集型模型的经验与见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:09:47 GMT</pubDate>
</item>
<item>
<title>AsyncFlow：一种高效的异步流式强化学习框架</title>
<link>https://arxiv.org/abs/2507.01663</link>
<guid>https://arxiv.org/abs/2507.01663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AsyncFlow提升大语言模型后训练效率，支持灵活扩展。</p><br /><br /><p><strong>摘要：</strong> 本文提出AsyncFlow，一种用于大语言模型后训练的异步流式强化学习框架。针对传统框架在可扩展性、数据流复杂性和资源闲置方面的不足，AsyncFlow引入了分布式数据存储与传输模块，实现统一的数据管理和细粒度调度。其架构支持自动流水线重叠和动态负载均衡，并通过生产者-消费者异步工作流减少计算空闲。此外，AsyncFlow与底层训练和推理引擎解耦，提供模块化用户界面。实验表明，该框架在吞吐量上平均提升1.59倍，为下一代强化学习系统设计提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:45:34 GMT</pubDate>
</item>
<item>
<title>ZeCO：实现线性注意力模型高效序列并行的新方法</title>
<link>https://arxiv.org/abs/2507.01004</link>
<guid>https://arxiv.org/abs/2507.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeCO提升长序列训练效率，消除通信开销。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZeCO（Zero Communication Overhead）序列并行方法，用于优化线性注意力机制在大型语言模型中的应用。传统序列并行方法因通信开销大而成为瓶颈，而ZeCO通过引入All-Scan通信原语，有效减少通信负担，实现近线性扩展。实验表明，在256块GPU上处理8M长度的序列时，ZeCO相比现有最优方法提升了60%的速度。理论与实证均证明了ZeCO的高效性与可行性，为训练超长序列的下一代LLM提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:54:53 GMT</pubDate>
</item>
<item>
<title>多模态推理中‘思考与图像’范式的演进与展望</title>
<link>https://arxiv.org/abs/2506.23918</link>
<guid>https://arxiv.org/abs/2506.23918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨AI从‘思考图像’到‘与图像共思’的范式转变。</p><br /><br /><p><strong>摘要：</strong> 本文综述了多模态推理领域中‘思考与图像’范式的最新进展。传统方法将视觉视为静态输入，导致感知数据与符号推理之间的语义鸿沟。而人类认知常利用视觉作为动态思维工具，这一理念正推动AI从仅‘思考图像’向‘与图像共思’演进。文章提出该范式发展的三个阶段：外部工具探索、程序化操作和内在想象，并总结了核心方法、评估基准、应用前景及未来挑战，为构建更强大且符合人类认知的多模态AI提供路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 10:48:35 GMT</pubDate>
</item>
<item>
<title>动态选择与合并专家模型提升跨领域信息抽取性能</title>
<link>https://arxiv.org/abs/2506.22813</link>
<guid>https://arxiv.org/abs/2506.22813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SaM框架通过动态选择和合并专家模型提升信息抽取效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SaM框架，通过在推理阶段动态选择和合并预训练的专家模型来优化信息抽取任务。该方法基于目标领域的相似性和采样实例的表现选择合适的专家模型，并将其合并以生成针对特定领域的优化模型。这种方法无需额外训练即可提高跨领域泛化能力，并具备良好的可扩展性。实验结果表明，该框架在多个基准测试中平均优于统一模型10%。文章还探讨了框架的潜在改进方向和实际应用经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 04:28:52 GMT</pubDate>
</item>
<item>
<title>基于语言理解的3D场景重建框架LangScene-X</title>
<link>https://arxiv.org/abs/2507.02813</link>
<guid>https://arxiv.org/abs/2507.02813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangScene-X通过多模态生成实现从稀疏视角的高质量3D场景重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LangScene-X的新生成框架，用于从2D图像中恢复一致的3D结构并实现开放词汇场景理解。该方法通过TriMap视频扩散模型生成RGB、法线和语义分割图，并结合语言量化压缩器（LQC）实现跨场景的语言嵌入编码，从而在仅有稀疏视角的情况下构建可泛化的3D语言嵌入场景。最后，通过将语言信息对齐到3D场景表面，支持开放式语言查询。实验表明，LangScene-X在质量和泛化能力上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:21:23 GMT</pubDate>
</item>
<item>
<title>2-单纯形Transformer提升token效率的研究</title>
<link>https://arxiv.org/abs/2507.02754</link>
<guid>https://arxiv.org/abs/2507.02754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2-单纯形Transformer在token效率上优于传统Transformer。</p><br /><br /><p><strong>摘要：</strong> 本文研究了2-单纯形Transformer架构，该架构通过高效的Triton内核实现三线性函数的注意力机制，相较于标准的点积注意力，其在数学、编程、推理和逻辑任务中表现出更高的token效率。实验表明，在固定token预算下，2-单纯形Transformer模型性能更优，并且改变了知识和推理任务的缩放定律指数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:16:34 GMT</pubDate>
</item>
<item>
<title>基于自生成目标条件MDPs的自动定理证明方法</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新框架提升LLM在复杂推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为自生成目标条件马尔可夫决策过程（sG-MDP）的新框架，旨在解决大型语言模型在自动定理证明中面临的挑战。该框架通过让智能体根据不断变化的证明状态生成并追求子目标，使问题更易于搜索。研究者将这种方法应用于Bourbaki（7B）系统，该系统可以集成多个7B参数的LLM进行子目标生成和策略合成。在PutnamBench基准测试中，Bourbaki（7B）成功解决了26个问题，取得了当前同类模型的最佳成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:41:38 GMT</pubDate>
</item>
<item>
<title>HiRA：一种分层框架提升复杂信息检索与推理效率</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HiRA通过分层规划与执行提升复杂搜索任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出HiRA，一种分层信息检索框架，将战略规划与专业执行分离。该方法将复杂搜索任务分解为子任务，并分配给具备外部工具和推理能力的领域特定代理。通过结构化集成机制协调结果，避免执行细节干扰高层推理，从而提升系统效率与答案质量。在四个跨模态深度搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:18:08 GMT</pubDate>
</item>
<item>
<title>提升大模型信息检索能力的WebSailor方法</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebSailor提升大模型在复杂信息任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出WebSailor，一种后训练方法，旨在增强大模型在复杂信息搜索任务中的能力。通过生成高不确定性任务、RFT冷启动和DUPO算法，WebSailor显著提升了开源模型的表现，使其接近专有系统水平。该方法解决了传统模型在处理海量信息时的不确定性问题，推动了大语言模型在信息检索领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 08:59:07 GMT</pubDate>
</item>
<item>
<title>提升奖励模型性能：基于高质量数据集的Skywork-Reward-V2研究</title>
<link>https://arxiv.org/abs/2507.01352</link>
<guid>https://arxiv.org/abs/2507.01352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork-Reward-V2通过高质量数据提升奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文指出当前开放奖励模型在评估基准上表现不佳，主要由于偏好数据集存在局限。为解决这一问题，作者提出了包含4000万对偏好的SynPref-40M数据集，并设计了人机协同的数据筛选流程。基于该数据集，训练出8个参数规模从0.6B到8B的Skywork-Reward-V2模型，在多个基准测试中取得最优成绩。实验表明，模型性能提升不仅得益于数据量，更得益于高质量的数据筛选。该研究展示了人机协作在数据质量提升中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:40:29 GMT</pubDate>
</item>
<item>
<title>多尺度多模态大语言模型在自动放射学报告生成中的应用</title>
<link>https://arxiv.org/abs/2507.00316</link>
<guid>https://arxiv.org/abs/2507.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出mu^2LLM模型提升CT影像报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自动化放射学报告生成（RRG）技术，旨在通过临床影像数据生成详细文本报告以提高诊断准确性和效率。文章指出该技术面临两大挑战：从影像数据中提取相关信息的复杂性以及模型生成报告与专家报告之间差异的客观评估困难。为解决这些问题，作者提出了mu^2LLM模型，结合多尺度和多模态特征，并通过直接偏好优化提升报告质量。实验结果表明，该方法在多个大型CT图像-报告数据集上优于现有方法，显示出在有限数据下进行RRG任务的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 19:14:49 GMT</pubDate>
</item>
<item>
<title>MARVIS：一种无需训练的多模态推理方法</title>
<link>https://arxiv.org/abs/2507.01544</link>
<guid>https://arxiv.org/abs/2507.01544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARVIS使小型视觉语言模型能高效预测多种数据模态。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARVIS的无需训练的方法，使小型视觉语言模型能够以高精度预测任何数据模态。该方法通过将潜在嵌入空间转换为可视化表示，并利用视觉语言模型的空间和细粒度推理能力进行解释和利用。MARVIS在视觉、音频、生物和表格领域表现出色，使用单一3B参数模型即可达到优于Gemini 16%的平均性能，并接近专用方法，同时不涉及个人身份信息且无需领域特定训练。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 05:56:24 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的实时交互式头部生成方法</title>
<link>https://arxiv.org/abs/2507.00472</link>
<guid>https://arxiv.org/abs/2507.00472</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIG框架实现更真实的实时交互头部生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了面对面交流中的交互式头部生成问题，针对传统方法在实时性和交互真实性上的不足，提出了一种基于自回归（AR）的逐帧生成框架ARIG。该框架通过非向量量化AR过程进行运动预测，并利用扩散过程表示运动分布以提高连续空间预测精度。同时，引入交互行为理解（IBU）和详细对话状态理解（CSU），通过双模态信号分析和上下文建模提升交互真实感。实验验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00472" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:38:14 GMT</pubDate>
</item>
<item>
<title>Locality-aware Parallel Decoding加速自回归图像生成</title>
<link>https://arxiv.org/abs/2507.01957</link>
<guid>https://arxiv.org/abs/2507.01957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPD技术提升自回归图像生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Locality-aware Parallel Decoding (LPD)方法，用于加速自回归图像生成。传统方法依赖逐块预测，导致高延迟。现有研究虽尝试通过多块预测实现并行化，但效果有限。LPD引入两种关键技术：灵活并行自回归建模和局部感知生成顺序，分别实现任意生成顺序和减少组内依赖，从而显著降低生成步数并提升效率。实验表明，在保持生成质量的前提下，LPD将ImageNet图像生成步数从256降至20（256×256分辨率）和1024降至48（512×512分辨率），延迟降低至少3.4倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>FreeMorph：无需微调的高效图像形态转换方法</title>
<link>https://arxiv.org/abs/2507.01953</link>
<guid>https://arxiv.org/abs/2507.01953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeMorph实现无需训练的高质量图像形态转换，速度快且效果优。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeMorph，一种无需微调的图像形态转换方法，能够处理不同语义或布局的输入。与现有依赖微调扩散模型的方法不同，FreeMorph通过引入引导感知球面插值和步进变化趋势，解决了非线性去噪过程中的质量下降问题，实现了更高效、更一致的图像过渡效果。实验表明，FreeMorph在速度和性能上均优于现有方法，提升达10倍至50倍，并建立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:20 GMT</pubDate>
</item>
<item>
<title>视觉-语言-动作模型中的动作标记化研究综述</title>
<link>https://arxiv.org/abs/2507.01925</link>
<guid>https://arxiv.org/abs/2507.01925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述分析VLA模型中动作标记的分类与作用。</p><br /><br /><p><strong>摘要：</strong> 本文综述了视觉-语言-动作（VLA）模型的研究进展，指出当前模型在处理视觉和语言输入后生成一系列动作标记，最终输出可执行动作。文章强调，VLA模型的核心设计差异在于动作标记的构建方式，包括语言描述、代码、可操作性、轨迹等多种形式。然而，对动作标记的理解仍不充分，阻碍了VLA的发展。本文通过分析不同动作标记的优缺点，提出未来研究方向，旨在推动VLA向通用智能迈进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:34:52 GMT</pubDate>
</item>
<item>
<title>STR-Match：一种无需训练的视频编辑算法</title>
<link>https://arxiv.org/abs/2506.22868</link>
<guid>https://arxiv.org/abs/2506.22868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STR-Match通过潜空间优化实现高质量视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为STR-Match的视频编辑算法，该算法无需训练即可生成视觉吸引人且时空一致的视频。其核心在于利用新颖的STR分数，结合2D空间注意力和1D时间模块，捕捉相邻帧之间的时空像素相关性，避免了计算成本高昂的3D注意力机制。通过集成潜空间优化框架和潜空间掩码，STR-Match在保持源视频关键视觉属性的同时，实现了在显著领域变换下的强性能表现。大量实验表明，该方法在视觉质量和时空一致性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 08:36:19 GMT</pubDate>
</item>
<item>
<title>Kwai Keye-VL：面向短视频理解的多模态大模型</title>
<link>https://arxiv.org/abs/2507.01949</link>
<guid>https://arxiv.org/abs/2507.01949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kwai Keye-VL提升短视频理解能力，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Kwai Keye-VL，一款80亿参数的多模态基础模型，旨在提升对短视频的理解能力。该模型基于超过6000亿token的高质量数据集和创新的训练方法，包括四阶段预训练和两阶段后训练。第二阶段引入五种模式的数据混合，增强模型的推理能力。通过强化学习和对齐优化，模型在多个视频基准测试中表现优异，并发布了针对真实短视频场景的KC-MMBench基准，进一步验证了其优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>基于动态全局-局部范式的长动画上色方法研究</title>
<link>https://arxiv.org/abs/2507.01945</link>
<guid>https://arxiv.org/abs/2507.01945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LongAnimation框架，提升动画上色的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 动画上色是动画产业的重要环节，传统方法成本高且效率低。现有研究多集中于短期上色，依赖局部特征融合，难以保持长期颜色一致性。本文提出LongAnimation框架，采用动态全局-局部范式，结合SketchDiT、DGLM和Color Consistency Reward模块，有效提升长视频段的颜色一致性。实验表明，该方法在短时（14帧）和长时（平均500帧）动画任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>DepthAnything-AC：一种适应多种环境的单目深度估计模型</title>
<link>https://arxiv.org/abs/2507.01634</link>
<guid>https://arxiv.org/abs/2507.01634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DepthAnything-AC在复杂环境下表现出色，具备零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DepthAnything-AC的单目深度估计模型，能够在多种复杂环境中保持高精度。与以往模型相比，该模型在光照变化、恶劣天气和传感器畸变等条件下表现更优。研究者引入了无监督一致性正则化微调方法，仅需少量未标记数据即可提升性能，并通过空间距离约束增强模型对局部关系的学习能力。实验结果表明，DepthAnything-AC在多个基准测试中展现出强大的零样本能力，包括真实世界恶劣天气数据集、合成噪声数据集和通用数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:05:57 GMT</pubDate>
</item>
<item>
<title>JAM-Flow：统一生成面部动作与语音的框架</title>
<link>https://arxiv.org/abs/2506.23552</link>
<guid>https://arxiv.org/abs/2506.23552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM-Flow实现面部动作与语音的联合生成，提升多模态合成效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出JAM-Flow，一个统一的框架，用于同时生成和条件化面部动作与语音。该方法结合流匹配和多模态扩散Transformer（MM-DiT）架构，包含专门的Motion-DiT和Audio-DiT模块，并通过选择性联合注意力层进行耦合。JAM-Flow采用类似修复的目标进行训练，支持文本、参考音频和参考动作等多种输入，适用于同步生成说话头像、音频驱动动画等任务，显著提升了多模态生成模型的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:51:40 GMT</pubDate>
</item>
<item>
<title>Mixture of Reasoning：提升大语言模型推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.00606</link>
<guid>https://arxiv.org/abs/2507.00606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture of Reasoning 提升大语言模型推理性能，无需外部提示。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Mixture of Reasoning (MoR) 框架，通过将多种推理策略嵌入大语言模型中，实现自主、任务自适应的推理能力。该框架包含两个阶段：首先生成推理链模板，然后通过监督微调提升性能。实验表明，MoR 在多个基准测试中显著提升表现，且无需依赖任务特定的提示，为跨任务的鲁棒推理提供了通用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 05:39:04 GMT</pubDate>
</item>
<item>
<title>基于频率修正的神经材质表示方法FreNBRDF</title>
<link>https://arxiv.org/abs/2507.00476</link>
<guid>https://arxiv.org/abs/2507.00476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreNBRDF提升材质建模精度与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为FreNBRDF的频率修正神经材质表示方法，旨在提高材质建模的准确性和可解释性。传统方法依赖于表格化的BRDF数据，而现代方法则采用隐式神经表示，但其在频域中的行为仍不明确。该研究通过引入球谐函数，将频域考虑整合到神经BRDF建模中，并设计了一种新的频率修正损失函数。该框架提升了材质重建和编辑的保真度、适应性和效率。实验表明，FreNBRDF在材质外观重建和编辑方面优于现有方法，支持更结构化和可解释的下游任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:48:50 GMT</pubDate>
</item>
<item>
<title>MOVi-MC-AC：首个多摄像头视图的非模态分割与内容数据集</title>
<link>https://arxiv.org/abs/2507.00339</link>
<guid>https://arxiv.org/abs/2507.00339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOVi-MC-AC是首个支持多摄像头视图的非模态分割数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MOVi-MC-AC，这是目前最大的非模态分割和首个非模态内容数据集。该数据集通过多摄像头模拟复杂家庭场景中的物体遮挡情况，提供了约580万实例的标签，并首次引入了真实非模态内容的地面实况。相比以往依赖慢速拼接生成伪标签的方法，MOVi-MC-AC在合成视频中实现了跨帧和多视角的一致性对象标识，为计算机视觉中的目标检测、跟踪和分割研究提供了新的挑战和机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 20:36:56 GMT</pubDate>
</item>
<item>
<title>MusiXQA：推动多模态大模型理解乐谱的基准数据集</title>
<link>https://arxiv.org/abs/2506.23009</link>
<guid>https://arxiv.org/abs/2506.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MusiXQA是首个用于评估音乐乐谱理解的多模态大模型数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MusiXQA，这是首个针对多模态大语言模型（MLLMs）在音乐乐谱理解方面的综合数据集。该数据集通过MusiXTeX生成高质量的合成乐谱，并包含结构化的注释，涵盖音符音高与时值、和弦、谱号、调号与拍号等信息，支持多种视觉问答任务。实验表明当前最先进的MLLMs在该领域存在显著不足，为此作者开发了Phi-3-MusiX模型，在该数据集上取得了优于GPT类方法的性能。该数据集和模型为未来MLLMs在音乐乐谱理解领域的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 16:46:47 GMT</pubDate>
</item>
<item>
<title>基于置信度的3D高斯点云压缩方法</title>
<link>https://arxiv.org/abs/2506.22973</link>
<guid>https://arxiv.org/abs/2506.22973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于置信度的3D高斯点云压缩方法，提升渲染效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于可学习置信度分数的损失函数，用于压缩3D高斯点云。该方法通过优化重建感知损失来调整每个点的置信度，从而修剪低置信度点，同时保持视觉质量。该方法与架构无关，适用于任何高斯点云渲染变体，并引入平均置信度作为场景质量的新评估指标。实验表明，该方法在压缩率和保真度之间取得了良好平衡。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 14:11:30 GMT</pubDate>
</item>
<item>
<title>FreeLong++：提升长视频生成质量的训练无关框架</title>
<link>https://arxiv.org/abs/2507.00162</link>
<guid>https://arxiv.org/abs/2507.00162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeLong++通过多频段融合提升长视频生成的时序一致性和视觉质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeLong和FreeLong++，用于解决长视频生成中时间一致性下降和视觉质量退化的问题。FreeLong通过在去噪过程中融合全局低频特征与局部高频特征，平衡长视频的频率分布。FreeLong++进一步扩展为多分支架构，支持多尺度时间窗口的多频段融合，从而增强语义连贯性和细节动态。该方法无需额外训练即可集成到现有模型中，显著提升长视频生成效果，并支持多提示生成、平滑场景切换及可控视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 14:11:21 GMT</pubDate>
</item>
<item>
<title>IR3D-Bench：通过主动创造评估视觉语言模型的场景理解能力</title>
<link>https://arxiv.org/abs/2506.23329</link>
<guid>https://arxiv.org/abs/2506.23329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IR3D-Bench挑战VLMs通过主动创造理解场景，推动其生成能力发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了IR3D-Bench，一个用于评估视觉语言模型（VLMs）场景理解能力的新基准。该基准要求模型通过主动使用编程和渲染工具来重建输入图像的3D结构，从而实现“通过创造理解”的方法。不同于传统基于被动识别的评估方式，IR3D-Bench强调工具使用和生成能力。研究提供了多种指标来评估几何准确性、空间关系、外观属性和整体合理性。实验表明，当前VLMs在视觉精度方面仍存在局限。IR3D-Bench包含数据和评估协议，旨在促进工具使用型VLMs的发展，以实现真正的场景理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 13:02:57 GMT</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking：多模态推理模型的进展与性能评估</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.1V-Thinking在多模态任务中表现卓越，超越多个基准。</p><br /><br /><p><strong>摘要：</strong> GLM-4.1V-Thinking是一款面向通用多模态推理的视觉语言模型。通过大规模预训练和基于课程采样的强化学习（RLCS），该模型在STEM问题解决、视频理解、内容识别、编程、定位、GUI代理和长文档理解等多个任务中展现出全面的能力提升。开源版本GLM-4.1V-9B-Thinking在28个公共基准测试中表现出色，超越Qwen2.5-VL-7B，并在多数任务上优于更大的Qwen2.5-VL-72B模型。此外，它在长文档理解和STEM推理等挑战性任务上也表现出与GPT-4o相当或更优的性能。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>SciArena：科学文献任务的开放协作评估平台</title>
<link>https://arxiv.org/abs/2507.01001</link>
<guid>https://arxiv.org/abs/2507.01001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciArena通过社区投票评估基础模型在科学文献任务中的表现。</p><br /><br /><p><strong>摘要：</strong> SciArena是一个开放协作平台，用于评估基础模型在科学文献任务中的表现。与传统基准不同，它采用社区投票方式，让研究人员直接参与模型比较。该平台已支持23个开源和专有模型，并收集了超过13,000份来自不同领域研究者的投票。分析显示，提交的问题多样且符合实际需求，研究人员在评估中表现出高度一致性和准确性。此外，团队还发布了SciArena-Eval，一个基于偏好数据的元评估基准，用于衡量模型判断答案质量的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>迈向通用人工智能：跨学科视角下的认知与架构分析</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AGI发展需整合记忆、推理与多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能向通用人工智能（AGI）发展的关键问题，指出当前模型如GPT-4.5、DeepSeek等虽具备多模态能力和部分推理能力，但仍受限于基于token的预测机制和缺乏具身代理。文章从人工智能、认知神经科学、心理学等多个领域出发，分析了AGI的架构与认知基础，强调模块化推理、持久记忆和多智能体协作的重要性。同时，Agentic RAG框架、信息压缩与测试时适应等策略被视为实现灵活智能的关键路径。此外，视觉语言模型被重新定义为具身理解与协作任务的接口。作者认为，真正的智能源于记忆与推理的融合，而非单纯依赖规模。最后，文章讨论了AGI发展中的科学、技术和伦理挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 12:52:25 GMT</pubDate>
</item>
<item>
<title>AI生成内容激增与新型水印技术PECCAVI的提出</title>
<link>https://arxiv.org/abs/2506.22960</link>
<guid>https://arxiv.org/abs/2506.22960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI生成内容将达90%，PECCAVI应对水印攻击。</p><br /><br /><p><strong>摘要：</strong> 欧洲联盟执法机构报告预测，到2026年，高达90%的在线内容可能由生成式AI创建，引发政策制定者的担忧。加州AB 3211法案要求对AI生成内容进行水印标记，但现有技术易被篡改。本文提出PECCAVI，一种针对视觉重述攻击的安全且无失真的图像水印技术。该技术在图像的核心语义区域嵌入水印，并利用多通道频域水印和噪声烧制技术增强抗逆向工程能力。PECCAVI具备模型无关性，相关资源将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 13:34:08 GMT</pubDate>
</item>
<item>
<title>提升语言模型训练效果的数据效能研究</title>
<link>https://arxiv.org/abs/2506.21545</link>
<guid>https://arxiv.org/abs/2506.21545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据效能优化可显著提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据效能（Data Efficacy）在语言模型训练中的作用，提出了一种名为DELT的通用范式，包含数据评分、数据选择和数据排序三个组件。其中，Learnability-Quality Scoring（LQS）通过梯度一致性评估样本的可学习性和质量，Folding Ordering（FO）则解决模型遗忘和数据分布偏差问题。实验表明，DELT能有效提升模型性能，且与数据效率结合使用效果更佳，证明数据效能是语言模型训练的重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>扩散语言模型在代码生成中的应用与优化</title>
<link>https://arxiv.org/abs/2506.20639</link>
<guid>https://arxiv.org/abs/2506.20639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型在代码生成中的解码行为与强化学习训练方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散语言模型（dLLMs）在代码生成中的潜力，分析了其与自回归模型的不同之处，如生成的因果性控制和采样温度对生成顺序的影响。作者训练了一个7B参数的dLLM模型DiffuCoder，并提出了一种新的采样方案coupled-GRPO，以提升强化学习训练效率。实验结果显示，该方法在代码生成基准测试中提升了4.4%，并减少了对自回归因果性的依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:35:47 GMT</pubDate>
</item>
<item>
<title>基于时空能量衰减的径向注意力机制提升视频生成效率</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出径向注意力机制，提升视频生成效率与长度。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视频扩散模型中的时空能量衰减现象，并提出了径向注意力机制。该机制通过稀疏注意力计算，将能量衰减转化为计算密度的指数衰减，从而显著降低计算复杂度。实验表明，该方法在多个视频生成模型上保持高质量的同时，提升了生成速度并减少了训练成本。相比传统密集注意力，其效率更高，且支持更长视频的生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>数学推理模型的泛化能力与训练方法研究</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数学模型在特定任务上表现优异，但泛化能力有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型在数学推理任务上的进展，并探讨其是否具备更广泛的问题解决能力。通过对20多个模型的评估发现，多数数学表现优秀的模型在其他领域如科学问答、编程和指令遵循上表现不佳。通过对比强化学习和监督微调两种训练方法，发现强化学习模型在跨领域任务中表现更好，而监督微调模型容易失去通用能力。分析表明，监督微调会导致表示和输出分布的变化，影响模型的泛化能力，提示需要重新考虑当前的训练策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 01:23:05 GMT</pubDate>
</item>
<item>
<title>MoCa：提升多模态嵌入模型性能的两阶段框架</title>
<link>https://arxiv.org/abs/2506.23115</link>
<guid>https://arxiv.org/abs/2506.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoCa通过两阶段方法提升多模态嵌入模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoCa，一种将预训练视觉语言模型转化为高效双向多模态嵌入模型的两阶段框架。第一阶段为模态感知持续预训练，引入联合重建目标以增强双向上下文感知推理；第二阶段为异构对比微调，利用多样化的多模态数据提升泛化能力和对齐效果。该方法解决了现有模型在注意力机制、数据依赖性和训练目标多样性方面的不足，并在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 02:41:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2506.21277</link>
<guid>https://arxiv.org/abs/2506.21277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出方法增强多模态模型理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要。本文指出当前多模态推理模型存在全局上下文理解不足和依赖捷径的问题，并提出通过引入上下文奖励、格式奖励和逻辑奖励来提升模型的推理能力。同时，作者构建了IntentBench基准测试，用于评估模型在理解复杂人类意图和情感方面的表现。实验结果表明，该方法在多个多模态基准上优于现有开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 10:01:03 GMT</pubDate>
</item>
<item>
<title>MEMFOF：高效多帧光流估计方法</title>
<link>https://arxiv.org/abs/2506.23151</link>
<guid>https://arxiv.org/abs/2506.23151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEMFOF在高分辨率下实现高效光流估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出MEMFOF，一种内存高效的多帧光流估计方法，在保持高精度的同时显著降低GPU内存消耗。该方法在1080p输入下仅需2.09GB显存运行，训练时为28.5GB，无需裁剪或降采样即可在原生分辨率下训练。通过优化RAFT架构设计，结合减少的卷积体积和高分辨率训练策略，MEMFOF在多个基准测试中取得最佳性能，包括Spring、Sintel和KITTI-2015数据集，展现出出色的准确性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 05:01:42 GMT</pubDate>
</item>
<item>
<title>基于多路径扩散的可调金属镜头摄影方法</title>
<link>https://arxiv.org/abs/2506.22753</link>
<guid>https://arxiv.org/abs/2506.22753</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型金属镜头成像方法，提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于退化建模的多路径扩散方法，用于可调金属镜头摄影。该方法利用预训练模型中的自然图像先验，而非依赖大规模数据集，通过正向、中性与负向提示路径平衡高频细节生成、结构保真度和金属镜头特有退化的抑制。同时引入伪数据增强和可调解码器，实现保真度与感知质量的可控权衡。此外，设计了空间变化的退化感知注意力模块，以适应复杂的光学和传感器引起的退化。最终构建了毫米级MetaCamera进行实际验证，实验结果表明该方法优于现有技术，实现了高保真和清晰的图像重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22753" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 00:48:37 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型代理在研究扩展任务中的能力</title>
<link>https://arxiv.org/abs/2506.22598</link>
<guid>https://arxiv.org/abs/2506.22598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM代理在自主实现研究扩展任务上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了基于大型语言模型（LLMs）的智能体在自主执行软件工程和科研任务方面的潜力。研究引入了RExBench，一个包含12个真实研究实验任务的基准测试，用于评估智能体在扩展已有研究成果方面的能力。每个任务都基于现有论文和代码库，并附有专家指导说明。尽管RExBench具备抗数据污染能力和自动化评估系统，但测试结果显示，使用不同框架开发的九个LLM代理在没有大量人工提示的情况下，仅能完成不到40%的任务。这表明当前智能体仍需大量人工干预才能处理现实中的研究扩展任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 15:41:41 GMT</pubDate>
</item>
<item>
<title>Tower+：在翻译与多语言通用能力之间实现性能平衡的模型</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tower+在翻译和多语言通用任务中表现出色，兼顾专业与泛用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tower+，一个在机器翻译和多语言通用文本处理方面均表现优异的模型系列。通过引入一种新的训练方法，包括持续预训练、监督微调、偏好优化和基于可验证奖励的强化学习，Tower+实现了翻译专业化与多语言通用能力之间的帕累托最优。研究团队在多个规模（2B、9B、72B）上构建了模型，并在代码生成、数学问题解决和指令遵循等任务中提升了性能。实验结果显示，Tower+在高资源语言翻译中表现卓越，并在多语言Arena Hard评估和IF-MT基准测试中取得领先。该研究证明，在优化特定业务领域（如翻译和本地化）的同时，仍可达到前沿模型的通用能力水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:30:06 GMT</pubDate>
</item>
<item>
<title>基于自对弈的强化学习框架SPIRAL提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIRAL通过自对弈训练提升语言模型推理能力，无需人工监督。</p><br /><br /><p><strong>摘要：</strong> 本文提出SPIRAL，一种基于自对弈的强化学习框架，使语言模型通过与不断进化的自我版本进行零和博弈来学习，从而无需依赖人工标注的数据或领域特定奖励工程。该框架生成持续进阶的问题课程，推动模型适应更强对手。研究中引入了角色条件优势估计（RAE）以稳定多智能体训练，并在Kuhn扑克等游戏中验证了其有效性。实验表明，SPIRAL可显著提升数学和通用推理能力，且多游戏训练进一步增强模型表现。结果表明，零和博弈是发展可迁移推理能力的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>基于运动不变图融合的ToF深度去噪网络</title>
<link>https://arxiv.org/abs/2506.23542</link>
<guid>https://arxiv.org/abs/2506.23542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型ToF深度去噪方法，提升时间稳定性和空间锐度。</p><br /><br /><p><strong>摘要：</strong> 本文针对ToF传感器获取的深度图像易受噪声影响的问题，提出了一种基于运动不变图融合的深度去噪网络。该方法利用跨帧几何注意力机制，结合图像平滑先验和ToF噪声分布的数据保真项，构建最大后验问题进行去噪。通过将解法展开为自适应学习的迭代滤波器，实现了高精度且可解释的去噪效果。实验结果表明，该方法在合成数据集和真实数据集上均表现出色，具有良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:29:24 GMT</pubDate>
</item>
<item>
<title>多语言模型工具调用能力提升方法研究</title>
<link>https://arxiv.org/abs/2506.23394</link>
<guid>https://arxiv.org/abs/2506.23394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过持续训练提升多语言模型的工具调用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，使多语言语言模型能够在非英语语言中实现可靠的工具调用。以保加利亚语为例，研究者对BgGPT模型系列进行了持续训练，使用了一个包含10,035个函数调用示例的双语数据集。该方法引入了TUCAN模型，在保加利亚语基准测试中实现了28.75%的函数调用准确率提升，并保持了核心语言理解能力。TUCAN模型还表现出更规范、可解析的输出格式，优于基础模型。研究提供了模型、评估框架和数据集，支持其他语言的复现与扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 16:47:27 GMT</pubDate>
</item>
<item>
<title>UrbanLLaVA：面向城市研究的多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.23219</link>
<guid>https://arxiv.org/abs/2506.23219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UrbanLLaVA提升城市多模态任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UrbanLLaVA，一个专为城市研究设计的多模态大语言模型。该模型能够同时处理多种城市数据，并在多个城市任务中表现出色。研究团队构建了一个涵盖单模态与跨模态数据的城市指令数据集，并设计了分阶段训练框架以提升空间推理和领域知识学习的兼容性。此外，还扩展了城市研究基准以评估模型性能。实验结果表明，UrbanLLaVA在多个城市任务中优于现有开源和商业模型，展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 09:04:27 GMT</pubDate>
</item>
<item>
<title>RoboScape：一种融合物理知识的统一世界模型</title>
<link>https://arxiv.org/abs/2506.23135</link>
<guid>https://arxiv.org/abs/2506.23135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboScape通过物理信息联合训练提升机器人视频生成的物理合理性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboScape，一个融合物理知识的统一世界模型，能够同时学习RGB视频生成和物理知识。该模型引入了时间深度预测和关键点动力学学习两个关键任务，以增强视频生成的3D几何一致性和复杂运动建模能力。实验表明，RoboScape在多种机器人场景中生成的视频具有更高的视觉质量和物理合理性，并在机器人策略训练和评估中展现出实用价值。研究为构建高效的物理感知世界模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 04:19:45 GMT</pubDate>
</item>
<item>
<title>Ovis-U1：一款融合多模态理解与生成能力的大型统一模型</title>
<link>https://arxiv.org/abs/2506.23044</link>
<guid>https://arxiv.org/abs/2506.23044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis-U1是一款30亿参数的多模态统一模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Ovis-U1，一款拥有30亿参数的统一模型，具备多模态理解、文本到图像生成和图像编辑能力。该模型基于Ovis系列，采用基于扩散的视觉解码器和双向标记精修器，在多个基准测试中表现优异，如OpenCompass多模态学术基准得分69.6，文本到图像生成在DPG-Bench和GenEval分别获得83.72和0.89分，图像编辑在ImgEdit-Bench和GEdit-Bench-EN分别获得4.00和6.42分。相比以往模型，Ovis-U1通过统一训练方法提升了性能，展示了多任务整合的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 20:40:17 GMT</pubDate>
</item>
<item>
<title>提出MARBLE基准测试，推动多模态推理模型发展</title>
<link>https://arxiv.org/abs/2506.22992</link>
<guid>https://arxiv.org/abs/2506.22992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARBLE挑战多模态推理模型的逐步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MARBLE，一个用于评估多模态语言模型（MLLMs）在复杂多模态问题和环境中进行逐步推理能力的基准测试。MARBLE包含两个高难度任务M-Portal和M-Cube，要求模型在空间、视觉和物理约束下制定并理解多步骤计划。实验结果显示，当前12个先进模型在M-Portal任务中表现接近随机，在M-Cube任务中准确率为0%。只有在简化子任务中部分模型优于随机基线，表明复杂推理仍是MLLMs的挑战。此外，研究发现感知能力仍是瓶颈，模型在从视觉输入中提取信息时存在困难。作者希望通过MARBLE推动下一代多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 15:44:32 GMT</pubDate>
</item>
<item>
<title>基于监听器增强的GRPO框架提升视觉语言模型对齐效果</title>
<link>https://arxiv.org/abs/2506.22832</link>
<guid>https://arxiv.org/abs/2506.22832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监听器增强的GRPO方法提升视觉语言模型的对齐性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于监听器增强的Group Relative Policy Optimization (GRPO)框架，用于提升视觉语言模型与人类偏好的对齐效果。传统奖励模型在泛化性方面存在不足，而监督微调容易导致记忆现象。尽管RL方法如GRPO有所改进，但当模型推理过程与独立的冻结视觉-语言模型（“监听器”）不一致时，推理准确性会显著下降。为此，本文引入监听器重新评估推理链，提供密集且校准的置信度评分，从而优化强化学习奖励信号。该方法不仅提高了准确率，还在大规模人类偏好数据集上提升了分布外性能，并减少了推理矛盾。实验结果表明，监听器奖励机制是一种高效、可扩展的对齐路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 05:53:17 GMT</pubDate>
</item>
<item>
<title>基于语言模型头的无训练优化方法提升推测解码性能</title>
<link>https://arxiv.org/abs/2506.22694</link>
<guid>https://arxiv.org/abs/2506.22694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VocabTrim技术优化推测解码速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需训练的优化方法，用于提升基于草稿模型的推测解码（SpD）性能。该方法在草稿生成过程中引入语言模型头（LM head），通过限制草稿模型的词汇量，仅保留目标模型中高频采样的词，从而降低内存瓶颈下的推理延迟。尽管接受率略有下降，但显著提升了生成速度，尤其在边缘设备上效果明显。实验表明，该方法在Llama-3.2-3B-Instruct模型上可提升内存瓶颈速度约16%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 20:26:40 GMT</pubDate>
</item>
<item>
<title>ThinkSound：基于思维链推理的视频到音频生成框架</title>
<link>https://arxiv.org/abs/2506.21448</link>
<guid>https://arxiv.org/abs/2506.21448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkSound通过思维链推理实现高质量视频到音频生成。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ThinkSound，一个利用思维链（CoT）推理的视频到音频生成框架。该框架将过程分为三个阶段：基础音效生成、交互式对象优化和自然语言指导的定向编辑。每个阶段都由多模态大语言模型生成上下文相关的CoT推理，以引导统一的音频基础模型。同时，研究者还发布了AudioCoT数据集，用于连接视觉内容、文本描述和声音合成。实验表明，ThinkSound在多个音频指标和CoT指标上均达到领先水平，并在Movie Gen Audio基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:32:06 GMT</pubDate>
</item>
<item>
<title>基于随机演示剪枝的新型提示设计范式</title>
<link>https://arxiv.org/abs/2506.17930</link>
<guid>https://arxiv.org/abs/2506.17930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过剪枝随机演示提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种颠覆传统思路的提示设计方法，通过剪除随机演示生成看似无意义的“乱码”提示，反而显著提升多种任务表现。该方法在不同任务和模型上均优于现有自动提示优化技术。为解决有效剪枝策略的发现难题，作者提出PromptQuine框架，利用进化搜索在低数据条件下自动寻找最优策略。该框架模仿自然界的复杂性演化机制，在仅使用上下文内token的情况下，生成高效且非传统的提示。实验表明其在分类、问答、生成和数学推理等任务中表现优异，同时具备良好的运行效率。研究希望推动对上下文学习的机制研究，并鼓励开发更开放的搜索算法以提升大语言模型提示效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 03:53:07 GMT</pubDate>
</item>
<item>
<title>SparseLoRA：通过上下文稀疏性加速大模型微调</title>
<link>https://arxiv.org/abs/2506.16500</link>
<guid>https://arxiv.org/abs/2506.16500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SparseLoRA通过稀疏性提升大模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SparseLoRA，一种利用上下文稀疏性加速大语言模型微调的方法。该方法引入轻量级、无需训练的SVD稀疏性估计器，动态选择部分权重进行损失和梯度计算，从而降低计算成本。实验表明，SparseLoRA在保持准确性的前提下，将计算成本减少最多2.2倍，并实现1.6倍的速度提升，适用于多种下游任务如常识推理、数学运算、代码生成等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:53:34 GMT</pubDate>
</item>
<item>
<title>Calligrapher：基于扩散模型的数字书法与设计框架</title>
<link>https://arxiv.org/abs/2506.24123</link>
<guid>https://arxiv.org/abs/2506.24123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Calligrapher通过创新技术实现精准风格控制和高质量字体生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Calligrapher，一个基于扩散模型的新型框架，结合文本定制与艺术排版，用于数字书法和设计应用。该框架解决了风格控制和数据依赖性的挑战，包含三个关键技术贡献：自蒸馏机制、局部风格注入框架以及上下文生成机制。这些技术提升了目标风格的精确对齐和字形定位。实验结果表明，Calligrapher在多种字体和设计场景中能够准确再现复杂的风格细节，优于传统模型，为数字艺术、品牌设计和上下文排版提供了强大支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>视频扩散模型中的稀疏注意力机制VMoBA</title>
<link>https://arxiv.org/abs/2506.23858</link>
<guid>https://arxiv.org/abs/2506.23858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VMoBA提升视频扩散模型效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对视频扩散模型的新型稀疏注意力机制——Video Mixture of Block Attention (VMoBA)。该方法通过分析预训练视频Transformer中的注意力模式，引入了三层递归块划分、全局块选择和基于阈值的块选择策略，以优化视频数据的时空特征捕捉。实验表明，VMoBA在保持生成质量的同时，显著提升了训练效率，实现了更高的FLOPs和延迟加速效果，并在无训练推理中也表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 09:52:31 GMT</pubDate>
</item>
<item>
<title>推理时技术在视觉语言模型中的有效性研究</title>
<link>https://arxiv.org/abs/2506.17417</link>
<guid>https://arxiv.org/abs/2506.17417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时技术提升VLM推理能力，但自验证能力仍不足。</p><br /><br /><p><strong>摘要：</strong> 本文研究了推理时计算技术在视觉语言模型（VLMs）中的应用效果，特别是基于强化学习（RL）训练的模型。实验表明，如多数投票和最佳N选择等解码策略能有效提升VLM的推理表现，其中生成依赖方法优于验证依赖方法。然而，与RL调优模型相关的自我修正行为（如顿悟时刻）并未带来显著提升。研究发现，RL训练的VLM在视觉和文本模态上的自我验证能力仍较弱，这是影响推理性能的关键原因。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 14:23:48 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在世界建模能力上的系统评估</title>
<link>https://arxiv.org/abs/2506.21876</link>
<guid>https://arxiv.org/abs/2506.21876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在基础世界建模能力上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于认知科学的两阶段框架，用于评估视觉语言模型（VLMs）作为内部世界模型的能力，涵盖感知和预测两个方面。研究引入了WM-ABench基准，包含23个细粒度维度，在6个模拟环境中进行测试。通过对15个最新商业和开源VLMs的660次实验，发现这些模型在基本世界建模能力上表现不佳，如无法准确区分运动轨迹，且缺乏对变量的解耦理解。结果揭示了VLMs与人类水平之间存在显著差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:24:29 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型的空间想象能力：MindCube基准与新方法</title>
<link>https://arxiv.org/abs/2506.21458</link>
<guid>https://arxiv.org/abs/2506.21458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出MindCube评估VLM空间推理能力，通过认知地图提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）是否能像人类一样从少量视角中想象完整场景。作者提出了MindCube基准，包含3,268张图像和21,154个问题，揭示现有VLM在空间推理方面表现不佳。研究评估了VLM构建空间心理模型的能力，包括位置、视角和动态模拟。通过引入未见中间视角、自然语言推理链和认知地图等方法，特别是“先生成地图再推理”的协同策略，显著提升了模型性能，准确率从37.8%提升至60.8%，进一步结合强化学习后达到70.7%。研究强调构建结构化内部空间表示对理解不可见空间的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:38:19 GMT</pubDate>
</item>
<item>
<title>TAPAS：基于多智能体的复杂任务求解框架</title>
<link>https://arxiv.org/abs/2506.19592</link>
<guid>https://arxiv.org/abs/2506.19592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAPAS通过LLM与符号规划结合，实现复杂任务自动化求解。</p><br /><br /><p><strong>摘要：</strong> TAPAS是一种多智能体框架，将大型语言模型（LLMs）与符号规划相结合，用于解决不需要手动定义环境模型的复杂任务。该框架利用专门的LLM代理协作生成和调整领域模型、初始状态和目标规范，并通过结构化工具调用机制进行交互。下游代理可以向上游代理请求修改，从而适应新的属性和约束。TAPAS还引入了类似ReAct风格的执行代理和自然语言计划翻译，以连接动态生成的计划与实际机器人能力。实验表明，TAPAS在基准规划领域和虚拟家庭模拟环境中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 09:02:06 GMT</pubDate>
</item>
<item>
<title>Fractional Reasoning：动态调整推理强度提升大语言模型性能</title>
<link>https://arxiv.org/abs/2506.15882</link>
<guid>https://arxiv.org/abs/2506.15882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fractional Reasoning通过动态调整推理强度提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练且与模型无关的Fractional Reasoning框架，能够在推理阶段动态控制推理强度。该方法通过提取与深度推理相关的潜在引导向量，并以可调缩放因子重新应用，使模型能根据输入复杂度自适应调整推理过程。该方法支持两种测试时扩展模式：提高广度策略（如Best-of-N）的输出质量，以及增强深度策略（如自省）的正确性。实验表明，Fractional Reasoning在GSM8K、MATH500和GPQA等任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 17:15:59 GMT</pubDate>
</item>
<item>
<title>基于3D代理的视频编辑框架Shape-for-Motion</title>
<link>https://arxiv.org/abs/2506.22432</link>
<guid>https://arxiv.org/abs/2506.22432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Shape-for-Motion框架，实现精准视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Shape-for-Motion的新框架，通过将视频中的目标物体转换为时间一致的3D网格代理，实现对视频内容的精确和一致编辑。该框架采用双传播策略，允许用户在单帧上进行编辑，并自动传播到其他帧。随后，3D网格被投影到2D空间生成编辑后的几何和纹理渲染图，输入到解耦视频扩散模型中生成最终结果。该方法支持多种精确且物理一致的视频编辑操作，如姿态调整、旋转、缩放、平移、纹理修改和对象合成，展示了其在高质量可控视频编辑中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>GPAS提升预归一化Transformer的训练效果</title>
<link>https://arxiv.org/abs/2506.22049</link>
<guid>https://arxiv.org/abs/2506.22049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPAS技术有效缓解Pre-LN模型激活方差问题，提升训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Gradient-Preserving Activation Scaling (GPAS) 的技术，旨在解决Pre-LN Transformer架构中激活值方差随层数增加而指数增长的问题。该技术通过缩放中间激活值但保持梯度不变，避免了梯度消失问题，同时保留激活信息。实验表明，GPAS在多种模型规模（71M到1B参数）下均能提升性能，并且对Sandwich-LN和DeepNorm等其他架构也表现出良好的适应性，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 05:45:15 GMT</pubDate>
</item>
<item>
<title>基于文本到文本回归的系统资源效率预测方法</title>
<link>https://arxiv.org/abs/2506.21718</link>
<guid>https://arxiv.org/abs/2506.21718</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到文本回归在系统资源预测中表现优异，优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 在许多行业中，预测大型系统的指标结果是一个核心问题，传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时面临挑战。本文提出了一种通用且可扩展的文本到文本回归方法。该方法在Google的Borg调度系统上取得了显著效果，使用60M参数的编码器-解码器模型，在整个集群中实现了接近完美的0.99（平均0.9）排名相关性，并且均方误差比传统方法低100倍。此外，模型仅需500个少样本示例即可适应新任务，并能捕捉复杂结果分布的密度。消融实验表明，使用编码器、增加序列长度以及模型的不确定性量化能力至关重要。这些成果为现实世界结果的通用模拟器奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21718" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 15:10:08 GMT</pubDate>
</item>
<item>
<title>基于RCME框架的视觉-语言模型层次结构学习</title>
<link>https://arxiv.org/abs/2506.21476</link>
<guid>https://arxiv.org/abs/2506.21476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RCME框架提升视觉-语言模型层次结构建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为径向跨模态嵌入（RCME）的框架，用于在视觉-语言模型中显式建模蕴含关系的传递性。该框架优化了概念在表示空间中的偏序关系，从而构建出能够表达生命树层次结构的视觉-语言基础模型。实验表明，该模型在层次分类和检索任务中优于现有最先进模型。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:05:06 GMT</pubDate>
</item>
<item>
<title>多模态上下文学习在医学任务中的挑战与评估</title>
<link>https://arxiv.org/abs/2506.21355</link>
<guid>https://arxiv.org/abs/2506.21355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大模型在医学任务中表现有限，上下文学习效果不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在医学任务中进行多模态上下文学习（ICL）的潜力与局限性。研究引入了SMMILE基准，包含111个医学问题和517个问答图像对，覆盖6个医学专科和13种影像技术。进一步扩展的SMMILE++包含1038个排列问题。实验表明，大多数模型在医学任务中表现出中等至较差的多模态ICL能力，上下文学习仅带来约8%-9.4%的性能提升。此外，研究发现不相关示例会显著降低性能，而示例顺序存在近期偏差，最后出现的相关示例可大幅提升表现。结果揭示了当前MLLM在医学多模态任务中面临的关键限制和偏见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:08:18 GMT</pubDate>
</item>
<item>
<title>Confucius3-Math：面向中国K-12数学教育的高效大语言模型</title>
<link>https://arxiv.org/abs/2506.18330</link>
<guid>https://arxiv.org/abs/2506.18330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Confucius3-Math在单块消费级GPU上实现高效运行并取得SOTA数学推理性能。</p><br /><br /><p><strong>摘要：</strong> Confucius3-Math是一款开源的大语言模型，拥有140亿参数，能够在单块消费级GPU上高效运行，并在多项数学推理任务中表现优异，超越了许多参数量更大的模型。该模型专为提升中国K-12阶段数学教育和知识传播而设计，通过大规模强化学习微调，与国家课程标准对齐，擅长解决主流数学问题。研究中提出了三项技术创新，包括目标熵正则化、近期样本恢复和策略特定难度加权，显著提升了训练稳定性、数据效率和模型性能。项目已开源，代码和模型可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 02:23:53 GMT</pubDate>
</item>
<item>
<title>基于贝叶斯框架的上下文学习策略分析</title>
<link>https://arxiv.org/abs/2506.17859</link>
<guid>https://arxiv.org/abs/2506.17859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了上下文学习中策略选择的贝叶斯机制。</p><br /><br /><p><strong>摘要：</strong> 本文通过贝叶斯预测器统一解释了上下文学习（ICL）中的不同策略，提出了一种层次化贝叶斯框架，能够准确预测Transformer模型的下一个词预测行为。该框架将预训练视为策略后验概率的更新过程，并在推理时对不同策略进行加权平均。研究强调了策略选择中损失与复杂度之间的权衡，解释了已知的ICL现象并提出了新的预测，如任务多样性增加时从泛化到记忆的过渡时间呈超线性增长。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 19:49:08 GMT</pubDate>
</item>
<item>
<title>基于视觉对比的链式推理方法研究</title>
<link>https://arxiv.org/abs/2506.22434</link>
<guid>https://arxiv.org/abs/2506.22434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过视觉对比训练模型进行链式推理，无需人工标注数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的方法，利用视觉对比任务来增强模型的链式推理能力。该方法基于自监督学习，构建图像三元组，包括同一图像的两个增强视图和一个相似但不同的图像。模型在训练过程中被提示生成推理过程以比较这些图像，并通过规则强化学习进行优化。由于图像间的高度相似性和增强操作，模型必须关注细微的视觉变化并进行逻辑推理。实验表明，尽管仅在视觉对比任务上训练，该方法在多图像推理基准测试中表现出色，并在通用视觉任务中也展现出强大性能，且无需依赖人工标注的问答对。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在科学再现任务中的能力</title>
<link>https://arxiv.org/abs/2506.22419</link>
<guid>https://arxiv.org/abs/2506.22419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大语言模型在再现已有研究成果方面仍存在挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Automated LLM Speedrunning Benchmark，用于评估AI代理在科学领域再现已有成果的能力。该基准基于NanoGPT速度竞赛，包含19个任务，提供训练脚本和不同形式的提示。尽管使用最新的推理模型和最先进的工具，AI仍难以复现已知创新，表明科学再现仍是AI研究的重要挑战。该基准为衡量AI自动化科学再现能力提供了有效手段。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>基于视觉-语言预训练的RetFiner提升OCT图像分类性能</title>
<link>https://arxiv.org/abs/2506.22149</link>
<guid>https://arxiv.org/abs/2506.22149</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RetFiner通过视觉-语言预训练提升OCT图像分类效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RetFiner的自监督学习视觉-语言优化方法，用于改进现有视网膜OCT基础模型的表示能力。该方法利用文本数据中的丰富监督信号，提升模型在多种复杂任务上的表现。实验结果显示，RetFiner在七个多样化OCT分类任务中分别提升了5.8、3.9和2.1个百分点。研究展示了该方法在无需额外标注的情况下，有效提升模型适应特定人群和应用场景的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22149" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 07:53:54 GMT</pubDate>
</item>
<item>
<title>XVerse：实现多主体精细控制的文本生成模型</title>
<link>https://arxiv.org/abs/2506.21416</link>
<guid>https://arxiv.org/abs/2506.21416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XVerse提升多主体图像生成的可控性与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型多主体可控生成模型XVerse，解决文本到图像生成中多主体身份和语义属性（如姿态、风格、光照）精细控制的问题。通过将参考图像转换为针对特定标记的文本流调制偏移量，XVerse实现了对特定主体的精确独立控制，同时不影响图像潜在表示或特征。该方法提升了多主体图像合成的保真度和可编辑性，增强了个性化和复杂场景生成能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:04:16 GMT</pubDate>
</item>
<item>
<title>ShotBench与ShotVL：推动电影语言理解的AI基准与模型</title>
<link>https://arxiv.org/abs/2506.21356</link>
<guid>https://arxiv.org/abs/2506.21356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ShotBench和ShotVL，提升AI对电影语言的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ShotBench，一个专为电影语言理解设计的全面基准，包含3.5k专家标注的问答对，覆盖8个关键摄影维度。评估显示现有VLM在理解电影细节方面存在明显不足，最高准确率低于60%。为此，研究构建了70k规模的ShotQA数据集，并基于此训练出ShotVL模型，在ShotBench上取得最佳表现。研究开源了模型、数据和代码，以促进AI在电影理解与生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:09:21 GMT</pubDate>
</item>
<item>
<title>DenseDiT：基于生成模型的密集预测方法在真实场景中的应用</title>
<link>https://arxiv.org/abs/2506.20279</link>
<guid>https://arxiv.org/abs/2506.20279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DenseDiT通过生成模型提升真实场景下的密集预测性能。</p><br /><br /><p><strong>摘要：</strong> 密集预测任务在计算机视觉中具有重要意义，旨在为输入图像学习像素级标注。然而，现有方法主要针对理想条件设计，难以适应真实世界场景，且面临真实数据稀缺的问题。为此，研究者提出了DenseWorld基准，涵盖25个密集预测任务，并引入DenseDiT方法，利用生成模型的视觉先验，通过统一策略执行多种真实场景下的密集预测任务。DenseDiT采用参数复用机制和两个轻量分支，仅需不到0.1%的额外参数即可有效集成多尺度上下文信息。实验表明，DenseDiT在DenseWorld上表现优于现有基线，使用不到0.01%的训练数据即可取得优异结果，展现出其在实际部署中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 05:40:50 GMT</pubDate>
</item>
<item>
<title>ARK：一个面向自主机器人的Python优先开源框架</title>
<link>https://arxiv.org/abs/2506.21628</link>
<guid>https://arxiv.org/abs/2506.21628</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARK通过Python统一机器人与AI实践，加速自主机器人研发。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ARK，一个面向自主机器人的开源Python框架。当前机器人软件栈面临学习曲线陡峭、工具碎片化等问题，而ARK旨在解决这些瓶颈。它提供类似Gym的环境接口，支持数据收集、预处理和模仿学习算法训练，并可无缝切换仿真与真实机器人。ARK采用轻量级客户端-服务器架构，支持C/C++绑定以保证实时性能。框架包含控制、SLAM、运动规划等模块，并兼容ROS。丰富的文档和案例展示了其在快速原型设计和端到端流程方面的优势，有助于降低进入门槛并推动机器人研究与商业化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21628" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 16:23:39 GMT</pubDate>
</item>
<item>
<title>基于噪声一致性的高效可控生成方法NCT</title>
<link>https://arxiv.org/abs/2506.19741</link>
<guid>https://arxiv.org/abs/2506.19741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NCT实现高效可控内容生成，无需重训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为噪声一致性训练（NCT）的新方法，用于在不重新训练基础模型的情况下，将新的控制信号直接整合到预训练的一步生成器中。NCT通过引入适配模块和噪声空间中的噪声一致性损失，使生成模型在不同条件下的行为保持一致，从而实现对新控制条件的适应。该方法具有模块化、数据效率高和易于部署的优点，实验表明其在单次前向传播中即可实现最先进的可控生成效果，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 11:58:55 GMT</pubDate>
</item>
<item>
<title>BlenderFusion：一种生成式视觉合成框架</title>
<link>https://arxiv.org/abs/2506.17450</link>
<guid>https://arxiv.org/abs/2506.17450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlenderFusion通过重组对象、相机和背景生成新场景。</p><br /><br /><p><strong>摘要：</strong> BlenderFusion是一种生成式视觉合成框架，能够通过重组对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程：首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D对齐控制编辑，最后通过生成式合成器将它们融合成连贯场景。该框架扩展了预训练扩散模型，支持同时处理原始和编辑后的场景，并通过源掩码和模拟物体抖动等训练策略提升合成效果。实验表明，BlenderFusion在复杂合成场景编辑任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 15:38:34 GMT</pubDate>
</item>
<item>
<title>Gazal-R1：医疗推理领域的高性能语言模型</title>
<link>https://arxiv.org/abs/2506.21594</link>
<guid>https://arxiv.org/abs/2506.21594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gazal-R1在医疗推理任务中表现卓越，提供透明解释。</p><br /><br /><p><strong>摘要：</strong> Gazal-R1是一款基于Qwen3 32B的320亿参数语言模型，在医疗推理任务中表现出色，并能提供清晰、分步的临床决策解释。通过两阶段训练方法，包括监督微调和强化学习，该模型在多个医学基准测试中超越了更大规模的模型。研究还揭示了在专业领域训练推理模型所面临的挑战，如奖励欺骗、训练不稳定性和事实记忆与详细推理之间的权衡。该方法为开发高性能、高效且可解释的领域专用语言模型提供了可复现的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 05:44:21 GMT</pubDate>
</item>
<item>
<title>Mixture of Grouped Experts 提升大模型推理效率与负载均衡</title>
<link>https://arxiv.org/abs/2505.21411</link>
<guid>https://arxiv.org/abs/2505.21411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoGE优化专家负载，提升大模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Mixture of Grouped Experts (MoGE)，这是一种改进的专家混合架构，通过将专家分组并限制每个组内激活的专家数量，实现更均衡的计算负载。相比传统MoE，MoGE在分布式设备上能显著提升吞吐量，特别是在推理阶段。基于MoGE构建的Pangu Pro MoE模型拥有720亿参数，其中160亿参数在每token中被激活，经过优化后在Ascend NPUs上表现出色，推理速度达1148 tokens/s，经推测加速后可达1528 tokens/s，优于同类模型。实验表明，MoGE在训练和推理中均表现出更高的效率和性价比。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:40:21 GMT</pubDate>
</item>
<item>
<title>LLaVA-Scissor：一种用于视频多模态大语言模型的无训练令牌压缩策略</title>
<link>https://arxiv.org/abs/2506.21862</link>
<guid>https://arxiv.org/abs/2506.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-Scissor通过语义连通组件实现高效视频令牌压缩。</p><br /><br /><p><strong>摘要：</strong> 本文提出LLaVA-Scissor，一种无需训练的视频多模态大语言模型令牌压缩策略。与以往依赖注意力得分的方法不同，该方法利用语义连通组件（SCC）将令牌分配到不同的语义区域，确保全面的语义覆盖。该策略在时空域中应用SCC，有效压缩令牌并以非重叠语义令牌表示整个视频。在多个视频理解基准测试中，LLaVA-Scissor表现出色，尤其在低保留率下表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 22:29:58 GMT</pubDate>
</item>
<item>
<title>SpatialReasoner-R1：提升视觉语言模型空间推理能力的新方法</title>
<link>https://arxiv.org/abs/2506.21656</link>
<guid>https://arxiv.org/abs/2506.21656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialReasoner-R1通过新方法提升空间推理性能。</p><br /><br /><p><strong>摘要：</strong> 当前视觉语言模型在细粒度空间推理方面存在不足，尤其是在多步骤逻辑和精确空间对齐方面。本文提出SpatialReasoner-R1模型，并引入M3CTS方法生成多样且逻辑一致的LongCoT推理轨迹，同时提出fDPO方法，通过空间奖励机制提升描述性定位和逻辑推理能力。实验表明，fDPO在空间质量任务中提升4.1%，在空间数量任务中提升9.0%。SpatialReasoner-R1在SPATIALRGPT-Bench上达到新SOTA，平均准确率超过最强基线9.8%，并在通用视觉语言任务中保持竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
<title>基于人体动作的自我中心视频预测模型PEVA</title>
<link>https://arxiv.org/abs/2506.21552</link>
<guid>https://arxiv.org/abs/2506.21552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PEVA模型通过人体姿态预测第一视角视频，提升环境模拟能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PEVA模型，旨在根据过去视频和由相对3D人体姿态表示的动作，预测自我中心视频。通过基于身体关节层次结构的运动轨迹进行条件建模，该模型学习如何从第一人称视角模拟人类行为对环境的影响。研究在Nymeria大规模真实世界自我中心视频与人体姿态数据集上训练了一个自回归条件扩散Transformer，并设计了分层评估协议以全面分析模型的具身预测与控制能力。该工作是首次尝试从人类视角出发，建模复杂现实环境和具身代理行为的视频预测方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>大语言模型预训练中的grokking现象与泛化机制研究</title>
<link>https://arxiv.org/abs/2506.21551</link>
<guid>https://arxiv.org/abs/2506.21551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大模型预训练中存在grokking现象，揭示了泛化能力的形成机制。</p><br /><br /><p><strong>摘要：</strong> 本文首次在7B参数的大语言模型OLMoE的一次性预训练过程中观察到grokking现象，即测试性能在训练损失收敛后仍持续提升。研究通过评估多种基准任务（如数学推理、代码生成和常识知识检索）验证了这一现象，并进一步分析了模型内部动态。发现样本路径从随机、实例特定逐渐演变为结构化且可共享，同时路径复杂度降低，表明模型经历了从记忆到泛化的转变。研究还提出了两个新指标，用于量化路径距离和复杂度，能够有效预测下游任务的泛化表现，具有实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>SAM4D：多模态时序基础模型用于相机与激光雷达的可提示分割</title>
<link>https://arxiv.org/abs/2506.21547</link>
<guid>https://arxiv.org/abs/2506.21547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAM4D实现相机与激光雷达的跨模态分割与高效数据标注。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAM4D，一个面向相机和激光雷达流的多模态时序基础模型，支持可提示分割。通过引入统一多模态位置编码（UMPE）将相机和激光雷达特征对齐到共享3D空间，实现跨模态提示与交互。同时，提出运动感知跨模态记忆注意力机制（MCMA），利用自运动补偿提升时间一致性和长时程特征检索能力。为避免标注瓶颈，开发了多模态自动化数据引擎，结合视频掩码生成、时空4D重建和跨模态掩码融合，以极高速度生成高质量伪标签，保留语义信息。实验表明SAM4D在Waymo-4DSeg数据集上展现出强大的跨模态分割能力和数据标注潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>WorldVLA：统一动作与图像理解的自回归世界模型</title>
<link>https://arxiv.org/abs/2506.21539</link>
<guid>https://arxiv.org/abs/2506.21539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldVLA通过整合动作与图像理解提升环境预测与行动生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出WorldVLA，一个将动作与图像理解及生成统一的自回归世界模型。该模型结合视觉-语言-动作（VLA）框架与世界模型，通过动作和图像理解预测未来图像，以学习环境物理规律并优化动作生成。同时，动作模型根据图像观测生成后续动作，辅助视觉理解和图像生成。实验表明，WorldVLA优于独立的动作和世界模型，但自回归生成动作序列时性能下降，原因在于模型对动作预测的泛化能力有限，导致误差累积。为此，作者提出注意力掩码策略，在生成当前动作时屏蔽先前动作，显著提升了动作块生成的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:55:40 GMT</pubDate>
</item>
<item>
<title>MADrive：基于记忆增强的自动驾驶场景重建方法</title>
<link>https://arxiv.org/abs/2506.21520</link>
<guid>https://arxiv.org/abs/2506.21520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MADrive通过外部记忆库实现更真实的自动驾驶场景合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MADrive，一种基于记忆增强的场景重建框架，旨在提升自动驾驶环境的逼真度。该方法利用大规模外部记忆库中的3D资产替换原始观测中的车辆，实现更逼真的场景合成。研究团队发布了包含约7万段360度汽车视频的MAD-Cars数据集，并开发了检索模块，用于在记忆库中找到相似车辆实例，进行3D重建并整合到目标场景中。实验表明，该方法可生成多视角完整车辆表示，支持显著改变或新驾驶场景的高质量合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:41:07 GMT</pubDate>
</item>
<item>
<title>Mind2Web 2：面向智能搜索系统的长期任务基准与评估框架</title>
<link>https://arxiv.org/abs/2506.21506</link>
<guid>https://arxiv.org/abs/2506.21506</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mind2Web 2为智能搜索系统提供长期任务基准与评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mind2Web 2，一个包含130个高质、长周期任务的基准测试集，旨在评估自主网络搜索系统的能力。该基准通过超过1000小时的人工劳动构建，支持实时浏览和信息整合。研究提出了一种新的Agent-as-a-Judge评估框架，利用树状评分标准自动判断答案准确性和来源归属。实验评估了九个前沿系统及人类表现，并分析了错误原因，为未来研究提供了方向。结果显示，OpenAI Deep Research已能实现人类50%-70%的性能，且耗时更少。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21506" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:32:50 GMT</pubDate>
</item>
<item>
<title>FairyGen：基于单幅儿童画生成叙事动画视频的系统</title>
<link>https://arxiv.org/abs/2506.21272</link>
<guid>https://arxiv.org/abs/2506.21272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FairyGen从单张儿童画生成风格一致、叙事连贯的动画视频。</p><br /><br /><p><strong>摘要：</strong> FairyGen是一个自动系统，能够从一张儿童的绘画中生成具有叙事性的卡通视频，并忠实保留其艺术风格。该系统通过分离角色建模与风格化背景生成，并引入电影镜头设计来增强表达和连贯性。它首先利用多模态大语言模型生成结构化的分镜脚本，再通过风格传播适配器保持角色视觉风格的一致性。此外，系统还包含镜头设计模块和两阶段运动定制适配器，以提升视频的多样性和电影感。实验表明，FairyGen能够生成风格忠实、叙事自然的动画，具有个性化和互动性强的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:58:16 GMT</pubDate>
</item>
<item>
<title>DiLoCoX：一种用于超大规模模型的低通信去中心化训练框架</title>
<link>https://arxiv.org/abs/2506.21263</link>
<guid>https://arxiv.org/abs/2506.21263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiLoCoX实现107B模型在1Gbps网络上的高效分布式训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiLoCoX，一个适用于超大规模语言模型（超过1000亿参数）的低通信去中心化训练框架。该框架结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案，显著提升了训练规模和速度。理论分析验证了通信与训练重叠及梯度压缩的有效性。实验表明，在1Gbps网络下，DiLoCoX可成功预训练107B模型，并比传统AllReduce方法快357倍，同时保持模型收敛性几乎不变。这是首个成功应用于1000亿参数以上模型的去中心化训练框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:45:04 GMT</pubDate>
</item>
<item>
<title>动态跳过中间层的Transformer架构优化研究</title>
<link>https://arxiv.org/abs/2506.21103</link>
<guid>https://arxiv.org/abs/2506.21103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态跳过中间层的Transformer架构，提升效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的Transformer架构，通过动态跳过中间层来提高计算效率。该方法基于输入内容决定是否跳过对称的中间块，并引入门控机制防止后续token访问被跳过的token位置。同时采用残差归一化方案和自适应正则化损失控制门控稀疏性。尽管旨在降低简单token的计算需求并促进多层级表征结构，但实验结果显示在所研究规模下，该方法在验证交叉熵与估计FLOPs之间的权衡上未优于较少层数的密集基线模型。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 05:01:19 GMT</pubDate>
</item>
<item>
<title>PhysRig：基于物理的可微分皮肤绑定与骨骼框架</title>
<link>https://arxiv.org/abs/2506.20936</link>
<guid>https://arxiv.org/abs/2506.20936</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PhysRig，解决传统LBS的变形缺陷，提升动画真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出PhysRig，一种基于物理的可微分皮肤绑定与骨骼框架，旨在克服传统线性混合皮肤（LBS）在模拟软组织、毛发等弹性材料时的局限性。通过将刚性骨骼嵌入体积表示（如四面体网格），并将其作为可变形软体结构进行模拟，PhysRig结合连续力学与粒子离散化方法，实现对材料属性和骨骼运动的可微分建模。同时引入材料原型以降低学习空间复杂度。实验表明，该方法在多个数据集上优于传统LBS方法，生成更真实、物理合理的动画效果，并展示了其在姿态迁移任务中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20936" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 21:58:09 GMT</pubDate>
</item>
<item>
<title>基于神经符号的高效图像编辑代理FaSTA^*</title>
<link>https://arxiv.org/abs/2506.20911</link>
<guid>https://arxiv.org/abs/2506.20911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaSTA^*通过结合LLM和A^*搜索实现高效图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FaSTA^*的神经符号代理，用于处理复杂的多轮图像编辑任务。该方法结合了大语言模型（LLMs）的快速高层次子任务规划与每个子任务中的慢速、精确的工具使用和局部A^*搜索，以找到成本高效的工具路径。通过LLMs对以往成功工具路径进行归纳推理，提取并优化常用子程序，并将其作为新工具在未来的任务中重复使用，从而显著降低了相似子任务的探索成本。FaSTA^*首先由LLMs进行快速子任务规划和规则子程序选择，仅在遇到新颖或困难子任务时才激活慢速A^*搜索。实验表明，FaSTA^*在计算效率上优于现有方法，同时保持了与最先进基线相当的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 20:33:43 GMT</pubDate>
</item>
<item>
<title>基于生成块的世界：通过几何抽象交互生成图像场景</title>
<link>https://arxiv.org/abs/2506.20703</link>
<guid>https://arxiv.org/abs/2506.20703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过3D几何体编辑生成高质量图像，提升可编辑性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Generative Blocks World的方法，通过操纵简单的几何抽象来与生成图像的场景进行交互。该方法将场景表示为凸3D基本形状的组合，并允许通过不同数量的形状进行结构或细节的编辑。编辑后的场景通过流模型生成图像，且图像生成条件包括深度信息和纹理提示。该纹理提示考虑了修改后的3D形状，优于现有键值缓存技术的纹理一致性。实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态搜索框架MMSearch-R1</title>
<link>https://arxiv.org/abs/2506.20670</link>
<guid>https://arxiv.org/abs/2506.20670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMSearch-R1实现端到端多模态搜索，提升信息获取效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MMSearch-R1，首个基于强化学习的多模态搜索框架，使大型多模态模型能够在真实网络环境中进行按需、多轮搜索。该框架整合图像和文本搜索工具，并通过基于结果的奖励机制引导模型决策。研究构建了一个多模态搜索VQA数据集，并筛选出搜索平衡子集以优化搜索行为。实验表明，该模型在知识密集型任务中表现优于现有RAG方法，且减少30%以上的搜索调用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>DeepRare：基于大语言模型的罕见病诊断系统</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepRare利用大语言模型实现罕见病精准诊断，准确率达100%。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepRare，一个基于大语言模型的罕见病诊断系统。该系统能够处理多种临床输入，并生成带有透明推理链的诊断假设，提升罕见病的诊断准确性。DeepRare包含三个核心组件：具备长期记忆的中央主机、负责领域特定分析的代理服务器以及集成40多个工具和最新医学知识的模块化设计。在8个数据集上的评估显示，DeepRare在2919种疾病中实现了100%的准确率，HPO评估中Recall@1达到57.18%，显著优于其他方法。此外，系统已上线为网页应用，便于临床使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 09:42:26 GMT</pubDate>
</item>
<item>
<title>MuseControlLite：轻量级文本到音乐生成模型微调机制</title>
<link>https://arxiv.org/abs/2506.18729</link>
<guid>https://arxiv.org/abs/2506.18729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MuseControlLite提升音乐生成模型的控制精度与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MuseControlLite，一种轻量级机制，用于微调文本到音乐生成模型，以实现对时间变化音乐属性和参考音频信号的精确控制。研究发现，位置嵌入在时间相关条件中至关重要。实验表明，在解耦交叉注意力层中添加旋转位置嵌入可将控制准确率从56.6%提升至61.1%，且参数量仅为现有方法的1/6.75。该方法在旋律控制、音频修复和扩展等方面表现优于MusicGen-Large和Stable Audio Open ControlNet，仅需85M可训练参数。相关代码、模型和示例已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:08:03 GMT</pubDate>
</item>
<item>
<title>DuaShepherd：融合正确性与潜力的奖励建模框架提升大语言模型数学推理能力</title>
<link>https://arxiv.org/abs/2506.17533</link>
<guid>https://arxiv.org/abs/2506.17533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuaShepherd通过结合正确性和潜力信号提升LLM数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuaShepherd，一种融合正确性与潜力奖励信号的新型奖励建模框架，旨在增强大语言模型（LLM）的数学推理能力。正确性信号关注步骤错误识别，潜力信号则侧重最终答案的可达性。研究构建了包含这两种信号的大规模奖励建模数据集，并采用多头统一架构在多任务设置中同时训练两个奖励模型。通过将两种信号合并为复合概率，模型在多个基准测试中均表现出色，尤其在MATH500和ProcessBench上优于仅使用单一奖励信号的模型，达到当前最优性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:11:01 GMT</pubDate>
</item>
<item>
<title>基于用户偏好的大语言模型路由框架</title>
<link>https://arxiv.org/abs/2506.16655</link>
<guid>https://arxiv.org/abs/2506.16655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种匹配用户偏好的模型路由方法，提升模型选择的灵活性和透明度。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的快速发展，不同模型在性能、风格和成本上各有优势，模型路由成为关键操作技术。然而，现有方法在评估性能时依赖不反映用户主观偏好的基准，并且模型选择范围有限。本文提出一种偏好对齐的路由框架，通过将查询与用户定义的领域或操作类型匹配，实现更灵活和透明的模型选择。我们引入了Arch-Router，一个1.5B参数的小型模型，能够学习将查询映射到对应的领域-操作偏好。该方法支持无缝添加新模型而无需重新训练或修改架构。实验表明，该方法在对话数据集上优于顶级专有模型，能更好地捕捉主观评价标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 19:57:41 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的超启发式框架HeurAgenix在组合优化中的应用</title>
<link>https://arxiv.org/abs/2506.15196</link>
<guid>https://arxiv.org/abs/2506.15196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeurAgenix通过LLM自动演化和选择启发式算法，提升组合优化效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出HeurAgenix，一个基于大语言模型（LLM）的两阶段超启发式框架，用于解决组合优化问题。该框架首先利用LLM比较种子启发式解与高质量解，提取可复用的演化策略；随后在求解过程中动态选择最合适的启发式方法。为提高灵活性，可以选择高性能LLM或轻量级模型。为应对监督信号不足的问题，采用双奖励机制进行微调，提升在噪声标注下的鲁棒性。实验表明，HeurAgenix在多个基准测试中表现优于现有方法，甚至超越专用求解器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 03:20:01 GMT</pubDate>
</item>
<item>
<title>AnimaX：一种基于视频扩散模型的3D动画生成框架</title>
<link>https://arxiv.org/abs/2506.19851</link>
<guid>https://arxiv.org/abs/2506.19851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimaX通过视频扩散模型实现跨骨架的3D动画生成。</p><br /><br /><p><strong>摘要：</strong> AnimaX是一种新型的前馈式3D动画框架，能够将视频扩散模型中的运动先验与基于骨骼的动画控制结构相结合。该方法克服了传统运动合成方法在固定骨骼拓扑或高维变形空间优化方面的限制，支持任意骨骼结构的3D网格动画生成。通过多视角、多帧2D姿态图表示3D运动，并结合模板渲染和文本运动提示进行联合视频-姿态扩散，实现了视频先验到运动生成任务的有效迁移。该框架在16万条带绑定序列的数据集上训练，取得了VBench基准测试中在泛化性、运动保真度和效率方面的最佳表现，为无类别限制的3D动画提供了一种可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ScaleCap：一种可扩展的图像描述生成策略</title>
<link>https://arxiv.org/abs/2506.19848</link>
<guid>https://arxiv.org/abs/2506.19848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ScaleCap，提升图像描述的准确性和平衡性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScaleCap，一种在推理阶段可扩展的图像描述生成策略，旨在解决大型视觉语言模型（LVLMs）中存在的多模态偏见和语言偏见问题。通过引入启发式问答和对比句评分两个新组件，ScaleCap能够逐步丰富并校准描述内容，提升描述的准确性、平衡性和信息量。实验表明，使用ScaleCap标注45万张图像进行预训练，可在11个基准测试中取得一致的性能提升，并在VQA任务和从描述重建图像的任务中展现出优秀的描述质量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于级联视频超分辨率的高效视频生成方法研究</title>
<link>https://arxiv.org/abs/2506.19838</link>
<guid>https://arxiv.org/abs/2506.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效视频生成框架，提升高分辨率输出效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了级联视频超分辨率（VSR）模型在视频生成中的关键设计原则。通过分离语义内容生成与细节合成两个阶段，先使用低分辨率基础模型生成内容，再通过轻量级VSR模型提升分辨率。作者提出了两种退化策略以增强训练数据的匹配性，并分析了时间步采样和噪声增强对低分辨率输入的影响。此外，引入了交错时间单元和稀疏局部注意力机制，显著降低了计算开销。实验表明，该框架优于现有方法，为高效级联视频生成提供了有效基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:57:26 GMT</pubDate>
</item>
<item>
<title>基于知识增强的强化学习缓解大语言模型幻觉问题</title>
<link>https://arxiv.org/abs/2506.19807</link>
<guid>https://arxiv.org/abs/2506.19807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KnowRL通过事实奖励提升模型推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）在推理过程中出现的严重幻觉问题，提出了一种名为KnowRL的知识增强强化学习方法。该方法在强化学习训练中引入基于知识验证的事实性奖励，引导模型进行以事实为基础的慢思考，从而帮助模型识别其知识边界。实验结果表明，KnowRL有效减少了慢思考模型的幻觉现象，同时保持了其原有的强推理能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:17:17 GMT</pubDate>
</item>
<item>
<title>提升开源大语言模型数据分析能力的研究</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何增强开源大语言模型的数据分析能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何提升开源大语言模型在数据密集型任务中的表现。通过构建多样化的现实场景数据集，从数据理解、代码生成和战略规划三个维度评估模型性能，发现战略规划质量、交互设计与任务复杂度以及数据质量是影响模型表现的关键因素。基于这些发现，作者提出了一种数据合成方法，显著提升了开源大语言模型的分析推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:04:23 GMT</pubDate>
</item>
<item>
<title>SRFT：统一SFT与RL的单阶段语言模型微调方法</title>
<link>https://arxiv.org/abs/2506.19767</link>
<guid>https://arxiv.org/abs/2506.19767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SRFT提升数学推理任务性能，优于传统两阶段方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了监督微调（SFT）和强化学习（RL）在大型语言模型中的整合问题。通过分析令牌分布、学习动态和熵指标，发现SFT带来全局性的策略变化，而RL则进行细粒度优化。基于此，作者提出了一种单阶段的监督强化微调方法（SRFT），结合SFT与RL的优势，直接利用演示数据和自探索轨迹进行优化。实验表明，SRFT在五项数学推理基准上平均准确率达到59.1%，比零RL方法高出9.0%，在三项分布外基准上高出10.9%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 12:31:37 GMT</pubDate>
</item>
<item>
<title>自动化数据集构建提升LLM在软件工程任务中的表现</title>
<link>https://arxiv.org/abs/2506.19290</link>
<guid>https://arxiv.org/abs/2506.19290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化数据集提升LLM在SWE任务中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种自动化的软件工程（SWE）数据集构建流程，旨在解决传统手动标注和环境设置耗时的问题。该数据集包含来自2531个GitHub仓库的10,169个真实Python任务实例，并附带自然语言描述和运行环境镜像。通过在这些数据上微调Skywork-SWE模型，研究发现模型性能随着数据量增加而持续提升，未出现饱和现象。在SWE-bench Verified基准测试中，Skywork-SWE模型达到38.0%的pass@1准确率，成为Qwen2.5-Coder-32B模型中的新SOTA。结合测试时缩放技术后，准确率进一步提升至47.0%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 23:53:36 GMT</pubDate>
</item>
<item>
<title>统一音频表示学习方法USAD在多类型音频任务中表现优异</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">USAD实现语音与音频的统一表示学习，性能接近最先进水平。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为通用语音与音频蒸馏（USAD）的统一音频表示学习方法，能够整合语音、声音和音乐等多种音频类型。USAD通过从特定领域的自监督学习模型中进行高效层间蒸馏，训练一个单一模型以处理多种音频任务。该方法在多个基准测试中表现出色，包括语音处理、音频标记和声音分类等任务，在SUPERB和HEAR基准上取得了接近最先进的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的智能照片修图系统 JarvisArt</title>
<link>https://arxiv.org/abs/2506.17612</link>
<guid>https://arxiv.org/abs/2506.17612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JarvisArt通过AI实现高效、精准的照片修图，提升用户体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 JarvisArt，一个基于多模态大语言模型（MLLM）的智能照片修图系统。该系统能够理解用户意图，模拟专业艺术家的思考过程，并协调 Lightroom 中超过 200 种修图工具进行自动化操作。通过两阶段训练方法（Chain-of-Thought 监督微调和 GRPO-R 策略优化），JarvisArt 在全局与局部调整上表现出色，具备出色的泛化能力和精细控制。研究还构建了 MMArt-Bench 基准测试集以评估性能，结果显示 JarvisArt 在内容保真度方面优于 GPT-4o，提升了 60% 的像素级指标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 02:36:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理一致性的强化学习方法研究</title>
<link>https://arxiv.org/abs/2506.16141</link>
<guid>https://arxiv.org/abs/2506.16141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRPO-CARE提升多模态模型推理一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型（MLLMs）的推理能力不足问题，提出了一种新的强化学习框架GRPO-CARE。该框架通过引入双层奖励机制，在提升答案正确性的同时增强推理步骤与答案之间的逻辑一致性。研究还构建了SEED-Bench-R1基准，用于评估MLLMs在复杂视频任务中的泛化能力。实验表明，GRPO-CARE在多个挑战性场景中均优于传统GRPO方法，特别是在最困难的评估级别上提升了6.7%，并显著提高了推理一致性。该方法为开发更可解释、更稳健的多模态模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:49:13 GMT</pubDate>
</item>
<item>
<title>代码转换对大语言模型理解能力的影响研究</title>
<link>https://arxiv.org/abs/2506.14012</link>
<guid>https://arxiv.org/abs/2506.14012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">代码转换影响LLM理解，嵌入外语可提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了代码转换（CSW）对大型语言模型（LLMs）理解能力的影响。研究表明，在多语言社区和在线内容中，代码转换现象日益普遍，而LLMs在处理此类混合语言文本时表现出不同的性能变化。当外语词汇干扰英语文本时，模型表现有所下降，但在将英语嵌入其他语言的情况下，理解能力反而提高。尽管提示方法效果不一，但微调策略在缓解性能下降方面更为稳定有效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 17:19:27 GMT</pubDate>
</item>
<item>
<title>4D-LRM：大规模时空重建模型实现任意视角与时间的高质量渲染</title>
<link>https://arxiv.org/abs/2506.18890</link>
<guid>https://arxiv.org/abs/2506.18890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4D-LRM实现从少量视角到任意视角和时间的高质量4D重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出4D-LRM，这是首个大规模4D重建模型，能够从非受限视角和时间戳输入中学习统一的时空表示，并直接预测每像素的4D高斯基元，从而实现高速、高质量的任意视角-时间组合渲染。相比传统方法，4D-LRM在效率、泛化性和真实性方面表现更优。实验表明，该模型能泛化到新物体、跨时间插值，并适应多种相机设置，在单块A100 GPU上可于1.5秒内完成24帧序列的重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:57:47 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态大语言模型个性化图像描述方法</title>
<link>https://arxiv.org/abs/2506.18369</link>
<guid>https://arxiv.org/abs/2506.18369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种强化学习框架提升MLLM个性化图像描述能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在生成个性化图像描述时的不足，提出了一种基于强化学习的后训练框架。尽管现有方法通过监督微调已取得一定进展，但在复杂场景下仍存在描述不准确的问题。由于高质量标注数据获取困难，作者采用强化学习策略，有效提升了模型的视觉识别和个性化生成能力，并在多概念图像描述任务中表现优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:55:52 GMT</pubDate>
</item>
<item>
<title>基于LLM代理的复杂规格到RTL代码生成系统</title>
<link>https://arxiv.org/abs/2506.13905</link>
<guid>https://arxiv.org/abs/2506.13905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Spec2RTL-Agent，实现从复杂规格自动生成RTL代码。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前LLM在生成硬件RTL代码时存在的实际应用与需求之间的差距，提出了一种名为Spec2RTL-Agent的LLM代理系统。该系统通过多代理协作框架，包括推理理解模块、逐步编码与提示优化模块以及自适应反思模块，直接处理复杂的规格文档并生成对应的RTL代码。与传统方法不同，该系统先生成可综合的C++代码再进行HLS优化，提高了代码的正确性和兼容性。实验表明，该系统在三个规格文档上的测试中，减少了75%的人工干预，成为首个完全自动化的从非结构化规格生成RTL代码的多代理系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 14:33:25 GMT</pubDate>
</item>
<item>
<title>基于两阶段优化的长视频照明编辑方法TC-Light</title>
<link>https://arxiv.org/abs/2506.18904</link>
<guid>https://arxiv.org/abs/2506.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TC-Light方法，实现高效且时间一致的视频照明编辑。</p><br /><br /><p><strong>摘要：</strong> 本文针对长视频中复杂动态场景的照明编辑问题，提出了一种名为TC-Light的新方法。该方法采用两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照，第二阶段优化提出的唯一视频张量（UVT）以对齐细粒度纹理和光照。为全面评估性能，研究者还构建了一个长而高度动态的视频基准数据集。实验结果表明，该方法在物理合理性和时间一致性方面表现优异，同时计算成本较低。相关代码和视频演示已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>RealPlay：基于神经网络的实时交互视频生成引擎</title>
<link>https://arxiv.org/abs/2506.18901</link>
<guid>https://arxiv.org/abs/2506.18901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealPlay实现从用户控制信号生成逼真、连贯的视频序列。</p><br /><br /><p><strong>摘要：</strong> RealPlay是一种基于神经网络的实时交互游戏引擎，能够根据用户控制信号生成逼真且时间一致的视频。与以往专注于游戏风格视觉效果的研究不同，RealPlay旨在模拟真实世界画面。它通过用户观察场景、发出控制指令、接收视频片段的交互循环工作。为实现高质量生成，研究解决了低延迟反馈、时间一致性及准确控制响应等挑战。训练数据包括标注的游戏数据和未标注的真实视频，无需真实动作标注。实验显示其具备控制迁移和实体迁移能力，可将虚拟控制应用于现实场景，并控制多种实体。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多面板故事可视化中的协作多智能体框架</title>
<link>https://arxiv.org/abs/2506.18900</link>
<guid>https://arxiv.org/abs/2506.18900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出协作多智能体框架提升故事视觉一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对故事可视化中多面板场景的视觉一致性问题，提出了一种协作多智能体框架。该框架能够自动识别、修正并优化多面板故事中的不一致之处，通过迭代循环实现细粒度的面板级更新，而无需重新生成整个序列。该方法与多种扩散模型兼容，包括Flux和Stable Diffusion等。实验结果表明，该方法在多面板一致性方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:29 GMT</pubDate>
</item>
<item>
<title>基于梯度优化的隐空间激活控制方法提升科学代码生成语言偏向性</title>
<link>https://arxiv.org/abs/2506.18887</link>
<guid>https://arxiv.org/abs/2506.18887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过G-ACT框架提升LLM生成代码时对特定语言的偏好。</p><br /><br /><p><strong>摘要：</strong> 本文研究了通过激活语言模型中的潜在子空间来引导科学代码生成向特定编程语言偏移的可行性。在四种编程语言上评估了五种因果语言模型的基线偏差，发现静态神经元归因方法效果有限。为此，作者提出了一种基于梯度优化的自适应激活控制框架（G-ACT），通过聚类每提示的激活差异并在线训练轻量级探测器，实现更有效的语言控制。实验表明，在LLaMA-3.2 3B模型中，该方法提升了15%的分类准确率，早期层甚至提高了61.5%。对于更大的LLaMA-3.3 70B模型，关键层注入仍有效。尽管引入轻微推理开销，但仅对部分层进行控制仍具实用性，展示了可扩展、可解释且高效的语义级控制机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:56:34 GMT</pubDate>
</item>
<item>
<title>3D Arena平台：基于人类偏好的生成式3D模型评估</title>
<link>https://arxiv.org/abs/2506.18787</link>
<guid>https://arxiv.org/abs/2506.18787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D Arena通过大规模用户偏好数据评估生成式3D模型质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了3D Arena平台，这是一个用于评估图像到3D生成模型的开放平台，通过大规模的人类偏好收集进行评估。自2024年6月推出以来，已收集超过12万次投票，涵盖19个最先进的模型。平台提供了iso3d数据集，并通过统计欺诈检测确保用户真实性。ELO排名系统为模型评估提供了可靠依据。分析显示，用户更偏好视觉呈现特征，Gaussian splat输出比网格模型有16.6 ELO优势，带纹理模型比无纹理模型高144.1 ELO。研究提出了多标准评估、任务导向评估和格式感知比较等改进建议，推动了生成式3D领域的人类中心评估发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:57:10 GMT</pubDate>
</item>
<item>
<title>DIP：一种用于提升密集图像表示的无监督后训练方法</title>
<link>https://arxiv.org/abs/2506.18463</link>
<guid>https://arxiv.org/abs/2506.18463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DIP通过伪上下文任务提升视觉编码器的密集表示性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DIP的新型无监督后训练方法，旨在提升大规模预训练视觉编码器在上下文场景理解中的密集图像表示能力。与以往依赖复杂自蒸馏架构的方法不同，DIP通过伪任务模拟下游上下文场景进行训练，借鉴了元学习原理。为了在未标注数据上进行后训练，该方法结合预训练扩散模型和视觉编码器自身，自动生成上下文任务。DIP具有简单、无监督和计算高效的特点，仅需单块A100 GPU不到9小时即可完成训练。实验表明，DIP在多种真实场景的上下文理解任务中表现优异，优于初始编码器和现有方法，提供了一种实用有效的密集表示优化方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 06:01:14 GMT</pubDate>
</item>
<item>
<title>基于视觉定位的医学视觉问答方法研究</title>
<link>https://arxiv.org/abs/2506.17939</link>
<guid>https://arxiv.org/abs/2506.17939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ThinkVG数据集提升医学问答模型的可解释性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过改进医学视觉问答模型的可解释性和可靠性，以支持临床决策。研究提出了一个名为ThinkVG的数据集，将答案生成过程分解为具有视觉定位的中间推理步骤，从而提供更细粒度的解释。同时引入了一种可验证的奖励机制，用于强化学习训练，提高模型推理过程与最终答案的一致性。实验表明，该方法仅需1/8的训练数据即可达到相当性能，展示了其高效性与有效性。数据集已公开在Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 04:09:58 GMT</pubDate>
</item>
<item>
<title>大语言模型输出稳定性的概率集中现象研究</title>
<link>https://arxiv.org/abs/2506.17871</link>
<guid>https://arxiv.org/abs/2506.17871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了对齐模型输出稳定性与概率集中有关。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了对齐大型语言模型（LLMs）输出缺乏多样性的原因，通过分析输出分布的概率集中现象，引入了分支因子（BF）作为衡量生成过程中可能下一步数量的指标。研究发现，随着生成过程推进，BF通常下降，表明模型变得更可预测。对齐调优显著减少了BF，使输出更稳定。此外，长推理链（CoT）模型通过进入低BF阶段实现更稳定的输出。研究认为对齐并未改变模型行为，而是引导其使用风格化词汇，从而减少熵值。实验表明，基础模型也可通过提示此类词汇降低BF。该研究为理解并控制LLM输出提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 22:00:37 GMT</pubDate>
</item>
<item>
<title>多文化音乐基础模型CultureMERT-95M提升跨文化音乐表示学习</title>
<link>https://arxiv.org/abs/2506.17818</link>
<guid>https://arxiv.org/abs/2506.17818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多文化音乐模型CultureMERT-95M提升非西方音乐分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多文化适应的音乐基础模型CultureMERT-95M，旨在增强跨文化音乐表示学习。通过两阶段持续预训练策略，在650小时多文化音乐数据上训练，显著提升了非西方音乐自动标记任务的性能，平均ROC-AUC和AP提高4.9%。同时，研究还探索了任务算术方法，与多文化训练模型效果相当。实验表明，单文化模型在不同音乐传统中迁移效果不一，而多文化模型表现最佳。为促进世界音乐表示学习研究，模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 17:16:39 GMT</pubDate>
</item>
<item>
<title>TPTT框架提升大语言模型效率与准确性</title>
<link>https://arxiv.org/abs/2506.17671</link>
<guid>https://arxiv.org/abs/2506.17671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPTT通过线性注意力机制提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TPTT（Transforming Pretrained Transformer into Titans）框架，该框架通过引入高效的线性化注意力机制和先进的内存管理技术，如Memory as Gate (MaG) 和混合线性化注意力 (LiZA)，提升了预训练Transformer模型的性能。TPTT兼容Hugging Face Transformers库，支持通过LoRA参数高效微调，无需完全重训练。实验结果显示，在MMLU基准测试中，Titan-Llama-3.2-1B模型在准确率上提升了20%。统计分析和对比表明，TPTT具有良好的可扩展性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:06:07 GMT</pubDate>
</item>
<item>
<title>基于前馈架构的4D视频与3D高斯粒子联合生成框架</title>
<link>https://arxiv.org/abs/2506.18839</link>
<guid>https://arxiv.org/abs/2506.18839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个4D视频与3D高斯粒子联合生成框架，提升视觉质量与重建能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的框架，能够使用前馈架构在每个时间步计算视频帧和3D高斯粒子的4D时空网格。该框架包含两个主要部分：4D视频模型和4D重建模型。第一部分分析了现有的4D视频扩散架构，并指出其局限性，提出了一种融合架构，在单一层中同时进行空间和时间注意力计算。关键在于稀疏注意力模式，使得标记在相同帧、同一时间戳或同一视角内进行交互。第二部分通过引入高斯头、相机标记替换算法以及额外的动态层和训练方法，扩展了现有的3D重建算法。整体上，该方法在4D生成任务中达到了新的技术水平，显著提升了视觉质量和重建能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 19:44:59 GMT</pubDate>
</item>
<item>
<title>视觉质量对多模态大语言模型性能的影响及优化方法</title>
<link>https://arxiv.org/abs/2506.15645</link>
<guid>https://arxiv.org/abs/2506.15645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像质量不直接影响MLLM表现，VQ-TTT提升模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了图像视觉质量对多模态大语言模型（MLLM）性能的影响，发现图像质量与模型表现之间存在视觉质量悖论，即某些情况下偏离人类感知的图像反而能提升模型性能。为解决这一问题，作者提出VQ-TTT方法，在不依赖外部模型或数据的情况下，通过轻量级适配模块动态调整输入图像，显著提升了多个基准测试中的准确率。该研究重新定义了MLLM所需的视觉输入标准，强调适应性图像的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 13:14:07 GMT</pubDate>
</item>
<item>
<title>基于Surfel索引视图记忆的视频生成方法</title>
<link>https://arxiv.org/abs/2506.18903</link>
<guid>https://arxiv.org/abs/2506.18903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型记忆机制提升视频生成环境探索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的记忆机制，用于构建能够交互式探索环境的视频生成器。现有方法要么通过扩展2D视图重建3D几何导致误差累积，要么依赖短上下文窗口难以维持场景连贯性。为此，作者引入了Surfel-Indexed View Memory (VMem)，通过基于3D表面元素（surfels）对过去视图进行几何索引，实现高效检索相关视图。该方法在生成新视图时仅关注关键视图，从而以较低计算成本保持场景一致性与相机控制能力。实验表明，在长时场景合成基准测试中，该方法优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于统一离散语义表示的多模态框架Tar</title>
<link>https://arxiv.org/abs/2506.18898</link>
<guid>https://arxiv.org/abs/2506.18898</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tar通过共享语义空间实现视觉与文本的统一理解和生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多模态框架Tar，旨在通过共享的离散语义表示统一视觉理解和生成。核心是Text-Aligned Tokenizer (TA-Tok)，它利用大型语言模型（LLM）词汇的文本对齐代码本将图像转换为离散标记。Tar通过扩展词汇表将视觉和文本整合到统一空间中，支持跨模态输入和输出，无需特定模态设计。研究还引入了适应规模的编码和解码方法，以及生成性解码器以提高视觉输出质量。为了满足不同的解码需求，采用了两种互补的解码器：快速自回归模型和基于扩散的模型。实验表明，Tar在多个基准测试中表现优异，收敛更快、训练效率更高。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18898" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>ReasonFlux-PRM：一种新型轨迹感知的奖励模型框架</title>
<link>https://arxiv.org/abs/2506.18896</link>
<guid>https://arxiv.org/abs/2506.18896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonFlux-PRM提升大模型推理轨迹评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型轨迹感知的奖励模型（ReasonFlux-PRM），用于更精准地评估大语言模型在推理过程中的中间步骤。与以往仅依赖最终输出的奖励模型不同，ReasonFlux-PRM结合了步骤级和轨迹级监督，能够对结构化思维链数据进行细粒度奖励分配。该模型支持离线和在线两种设置，适用于模型蒸馏、强化学习和测试时的缩放优化。实验结果表明，ReasonFlux-PRM-7B在多个基准任务中表现优于现有强基线模型，并实现了显著的性能提升。此外，作者还发布了轻量版ReasonFlux-PRM-1.5B，适用于资源受限场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>通用光度立体技术中的光照与表面法线耦合问题研究</title>
<link>https://arxiv.org/abs/2506.18882</link>
<guid>https://arxiv.org/abs/2506.18882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">解决光照与表面法线耦合难题，提升复杂表面几何细节恢复质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通用光度立体（PS）技术在任意光照条件下恢复高质量表面法线所面临的两大挑战。首先，光照变化与表面法线特征之间存在深度耦合，导致观察到的亮度变化难以区分是由于光照变化还是表面朝向变化。其次，复杂表面上的高频几何细节难以保留，自阴影、多次反射和细微法线变化使传统特征处理方法难以准确捕捉。文章分析了这些问题，并指出其对现有方法如SDM-UniPS和Uni MS-PS的限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:53:11 GMT</pubDate>
</item>
<item>
<title>基于Commutative Vector Quantization的长上下文大语言模型优化方法</title>
<link>https://arxiv.org/abs/2506.18879</link>
<guid>https://arxiv.org/abs/2506.18879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Commutative Vector Quantization技术，显著降低KV缓存内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在处理长上下文时KV缓存导致的内存瓶颈问题，提出了一种名为Commutative Vector Quantization (CommVQ) 的量化方法。该方法通过轻量级编码器和码本对KV缓存进行加法量化压缩，并利用矩阵乘法解码。为降低解码计算成本，设计了与RoPE兼容的码本，并通过EM算法训练。实验表明，该方法在保持高精度的同时，将FP16 KV缓存大小减少了87.5%，并支持1位量化，使LLaMA-3.1 8B模型在单块RTX 4090 GPU上实现128K上下文长度的推理。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:50:11 GMT</pubDate>
</item>
<item>
<title>OmniGen2：一种多功能生成模型的开源实现</title>
<link>https://arxiv.org/abs/2506.18871</link>
<guid>https://arxiv.org/abs/2506.18871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGen2是一个多功能生成模型，支持文本到图像等多种任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniGen2，一个开源的多功能生成模型，旨在为多种生成任务提供统一解决方案，包括文本到图像、图像编辑和上下文生成。与前代版本相比，OmniGen2采用独立的解码路径和非共享参数设计，提升了生成效果并保留了原始文本生成能力。文章还描述了用于训练OmniGen2的数据构建流程、针对图像生成的反射机制以及新提出的OmniContext基准测试。尽管参数规模较小，OmniGen2在多个任务上表现优异，并已公开模型、代码和数据集以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:38:54 GMT</pubDate>
</item>
<item>
<title>Phantom-Data数据集提升文本到视频生成的准确性</title>
<link>https://arxiv.org/abs/2506.18851</link>
<guid>https://arxiv.org/abs/2506.18851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phantom-Data提升文本到视频生成的一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本到视频生成取得了显著进展，但现有模型在忠实遵循文本指令方面仍面临挑战，尤其是‘复制粘贴问题’。该问题源于传统的成对训练方式，导致主体身份与背景属性混淆。为解决此问题，研究者提出了Phantom-Data，这是首个通用的跨对主体到视频一致性数据集，包含约一百万对身份一致的数据。该数据集通过三阶段流程构建：主体检测、跨上下文主体检索以及先验引导的身份验证。实验表明，使用Phantom-Data训练可显著提升提示对齐度和视觉质量，同时保持身份一致性与传统方法相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:11:56 GMT</pubDate>
</item>
<item>
<title>无需合成数据的超长文本生成方法研究</title>
<link>https://arxiv.org/abs/2506.18841</link>
<guid>https://arxiv.org/abs/2506.18841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于强化学习的超长文本生成方法超越传统微调技术。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需依赖合成数据的超长文本生成方法，通过强化学习训练模型，提升其生成长文本的质量和结构控制能力。实验表明，该方法在多个基准测试中表现优异，优于传统的监督微调方法，并且在性能上超越了更大规模的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:59:02 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的动态新视角合成方法ViDAR</title>
<link>https://arxiv.org/abs/2506.18792</link>
<guid>https://arxiv.org/abs/2506.18792</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViDAR利用扩散模型生成多视角监督信号，提升动态场景的视图合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出ViDAR，一种基于扩散模型的4D重建框架，用于从单目视频中生成高质量动态新视角。通过个性化扩散模型生成伪多视角监督信号，结合场景特定特征，ViDAR能够恢复精细外观细节并减少单目模糊带来的伪影。为解决扩散监督的时空不一致性，引入扩散感知损失函数和相机位姿优化策略，使合成视图与场景几何对齐。在极端视角变化的DyCheck基准上，ViDAR在视觉质量和几何一致性方面均优于现有方法，并在动态区域表现显著提升。项目页面提供更多信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18792" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:01:15 GMT</pubDate>
</item>
<item>
<title>ReDit：通过奖励抖动提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2506.18631</link>
<guid>https://arxiv.org/abs/2506.18631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReDit通过添加随机噪声改善离散奖励，提升模型训练效率和性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReDit的方法，用于解决大语言模型在使用离散奖励时出现的梯度异常、优化不稳定和收敛缓慢问题。ReDit通过向离散奖励信号中添加简单随机噪声，使训练过程中持续提供探索性梯度，从而实现更平滑的梯度更新和更快的收敛速度。实验表明，ReDit在多种任务中表现优异，仅需约10%的训练步数即可达到与传统GRPO相当的性能，并在相似训练时间内仍能提升4%的性能。可视化结果和理论分析进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 09:36:24 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的多视角图像生成方法</title>
<link>https://arxiv.org/abs/2506.18527</link>
<guid>https://arxiv.org/abs/2506.18527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MV-AR方法，实现多视角图像一致性生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D内容创作中多视角图像生成的挑战，提出了一种基于自回归模型的多视角图像生成方法（MV-AR）。该方法通过自回归模型逐步生成一致的多视角图像，并利用前序视图提取有效参考信息。为适应多种提示条件，设计了统一模型架构并引入条件注入模块，支持文本、相机姿态、图像和形状等多种输入。采用渐进式训练策略提升模型性能，并通过“Shuffle View”数据增强技术缓解数据不足问题。实验表明，MV-AR在多种条件下均能生成高质量且一致的多视角图像，性能与主流扩散模型相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 07:28:37 GMT</pubDate>
</item>
<item>
<title>SlimMoE：高效压缩大型Mixture of Experts模型的方法</title>
<link>https://arxiv.org/abs/2506.18349</link>
<guid>https://arxiv.org/abs/2506.18349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SlimMoE通过分阶段压缩提升MoE模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SlimMoE的多阶段压缩框架，用于将大型Mixture of Experts (MoE)模型转换为更小、高效的版本，而无需从头开始训练。该方法通过逐步减少参数数量并利用中间阶段的知识迁移，有效缓解了单次剪枝导致的性能下降问题。使用该框架，作者将Phi 3.5-MoE模型压缩为Phi-mini-MoE和Phi-tiny-MoE，分别仅需400B个token进行训练，且可在单块GPU上微调，适用于资源受限环境。实验表明，这些压缩模型在性能上优于同类模型，并与更大的模型相当。研究证明，结构化剪枝结合分阶段蒸馏是构建高质量紧凑MoE模型的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:15:59 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的自适应用户画像框架LettinGo</title>
<link>https://arxiv.org/abs/2506.18309</link>
<guid>https://arxiv.org/abs/2506.18309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LettinGo提升推荐系统的准确性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LettinGo，一种基于大语言模型的自适应用户画像生成框架。传统嵌入式画像缺乏可解释性，而现有方法受限于固定格式。LettinGo通过多阶段流程，结合直接偏好优化（DPO）和下游任务反馈，生成多样且适应性强的用户画像，显著提升推荐系统的准确性、灵活性和上下文感知能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 01:51:52 GMT</pubDate>
</item>
<item>
<title>无需验证器的强化学习框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.18254</link>
<guid>https://arxiv.org/abs/2506.18254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLPR通过LLM自身概率评分实现无验证器强化学习，提升推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLPR，一种无需依赖领域特定验证器的强化学习框架，利用大语言模型（LLM）生成答案时的token概率作为奖励信号，从而提升模型在多个通用领域和数学领域的推理能力。研究发现，处理概率奖励的高方差是关键，因此引入prob-to-reward和稳定化方法以确保奖励的准确性与稳定性。实验表明，RLPR在四个通用领域和三个数学基准测试中均表现优异，显著优于现有方法如VeriFree和General-Reasoner。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 22:56:36 GMT</pubDate>
</item>
<item>
<title>FaithfulSAE提升稀疏自编码器的稳定性与模型内部特征捕捉能力</title>
<link>https://arxiv.org/abs/2506.17673</link>
<guid>https://arxiv.org/abs/2506.17673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaithfulSAE通过使用模型自身数据提升SAE稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FaithfulSAE方法，旨在解决稀疏自编码器（SAEs）在不同初始化种子下不稳定以及无法准确捕捉模型内部特征的问题。传统SAEs通常在外部数据集上训练，可能引入分布外（OOD）数据，导致生成虚假特征。FaithfulSAE则使用模型自身生成的数据进行训练，实验表明其在多个模型中表现出更高的稳定性，并在SAE探测任务中优于基于网络数据的SAEs，同时降低了虚假特征比例。该方法减少了对外部数据集的依赖，提升了模型解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:18:25 GMT</pubDate>
</item>
<item>
<title>ConsumerBench：评估端侧GenAI系统效率的基准框架</title>
<link>https://arxiv.org/abs/2506.17538</link>
<guid>https://arxiv.org/abs/2506.17538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ConsumerBench评估端侧GenAI模型的效率与响应时间。</p><br /><br /><p><strong>摘要：</strong> 本文提出ConsumerBench，一个用于评估端用户设备上生成式AI（GenAI）模型系统效率和响应时间的基准框架。不同于传统基准假设模型独占GPU资源，ConsumerBench模拟了真实多应用并发运行的场景，支持自定义工作流以模拟复杂任务。该框架收集应用级指标（如延迟和SLO达成率）和系统级指标（如CPU/GPU利用率和内存带宽）。实验揭示了资源共享低效、贪婪分配下的不公平调度以及静态模型服务器配置的性能问题，并为模型开发者和系统设计者提供了实用建议，包括定制化内核和SLO感知调度策略的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:32:22 GMT</pubDate>
</item>
<item>
<title>基于MICS的医学多模态大语言模型推理路径优化</title>
<link>https://arxiv.org/abs/2506.16962</link>
<guid>https://arxiv.org/abs/2506.16962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MICS方法提升医学MLLM推理能力，构建新数据集和模型。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学领域多模态大语言模型（MLLM）推理能力不足的问题，提出一种名为Mentor-Intern Collaborative Search（MICS）的新型推理路径搜索方案。该方法通过导师模型引导推理路径，并由多个实习模型进行验证与优化，最终根据综合表现选择最优路径。为评估推理质量，引入MICS-Score指标。研究还构建了MMRP医学推理数据集和Chiron-o1模型，后者在多项医学视觉问答和推理任务中取得最佳性能。实验表明，基于MICS生成的CoT数据能显著提升模型表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 08:51:19 GMT</pubDate>
</item>
<item>
<title>基于LSTM的新生儿死亡风险预测研究</title>
<link>https://arxiv.org/abs/2506.16929</link>
<guid>https://arxiv.org/abs/2506.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LSTM模型在新生儿死亡预测中表现最佳，准确率达99%。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了如何通过机器学习技术预测新生儿死亡风险，以减少新生儿死亡率。研究使用了140万份历史数据，应用了逻辑回归、K近邻、随机森林、XGBoost、卷积神经网络和LSTM等多种算法。结果显示，XGBoost和随机森林分类器准确率为94%，而LSTM模型准确率最高，达到99%。因此，LSTM被推荐为预测新生儿是否需要采取预防措施的最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 07:44:48 GMT</pubDate>
</item>
<item>
<title>Crome：一种因果鲁棒的奖励建模框架以防止奖励黑客</title>
<link>https://arxiv.org/abs/2506.16507</link>
<guid>https://arxiv.org/abs/2506.16507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crome通过因果和中性增强提升奖励模型鲁棒性，有效防止奖励黑客。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Crome的新型奖励建模框架，旨在解决大型语言模型在对齐过程中因奖励黑客而导致的偏差问题。Crome基于显式的因果模型，通过两种合成增强方式——因果增强和中性增强，分别强化对因果属性的敏感性和对虚假属性的不变性。这些增强仅通过查询一个权威语言模型来识别因果标准，无需预先了解虚假因素。实验结果显示，Crome在多个基准测试中显著优于传统基线，提升了平均准确率，并在不同任务和设置中表现出一致的性能优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FinCoT：基于领域专家推理的结构化思维链提示方法</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinCoT提升金融问答性能并优化推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出FinCoT，一种结合领域专家金融推理的结构化思维链提示方法。研究对比了FinNLP中的三种提示风格：标准提示、非结构化思维链提示和结构化思维链提示，并发现结构化提示在性能和可解释性上具有优势。FinCoT在十个金融领域的CFA风格问题上表现出色，将准确率从63.2%提升至80.5%，同时显著减少生成的token数量。结果表明，与领域对齐的结构化提示能有效提升模型性能并降低推理成本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:18:55 GMT</pubDate>
</item>
<item>
<title>基于CodeT5的LLM代码作者归属识别研究</title>
<link>https://arxiv.org/abs/2506.17323</link>
<guid>https://arxiv.org/abs/2506.17323</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新型模型用于识别AI生成的C语言代码来源。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型生成的代码日益增多，识别其来源变得尤为重要。本文提出了CodeT5-Authorship模型，该模型仅使用CodeT5的编码器部分进行分类任务。通过引入LLM-AuthorBench基准测试集，评估了模型在区分不同LLM生成代码方面的性能。实验结果显示，该模型在二分类和多分类任务中均表现出色，准确率分别达到97.56%和95.40%。研究还提供了开源代码和数据集以支持开放科学。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17323" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 15:49:41 GMT</pubDate>
</item>
<item>
<title>Agentic AI研究中的标准化与评估协议改进</title>
<link>https://arxiv.org/abs/2506.15741</link>
<guid>https://arxiv.org/abs/2506.15741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究指出Agentic AI缺乏标准评估，提出新框架OAgents提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前Agentic AI研究中缺乏标准化和科学严谨性的问题，导致方法间难以公平比较。通过在GAIA基准和BrowseComp上的系统实证研究，发现以往工作因评估协议不统一而难以复现。为此，作者提出了更稳健的评估协议，并基于研究结果开发了OAgents框架，该框架在开源项目中表现优异，具备模块化设计以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>LLM jailbreak攻击防护机制的系统性分析</title>
<link>https://arxiv.org/abs/2506.10597</link>
<guid>https://arxiv.org/abs/2506.10597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次系统分析LLM的jailbreak防护机制，提出多维分类和评估框架。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLMs）在部署过程中暴露的关键漏洞，尤其是jailbreak攻击对安全机制的突破。为应对这一问题，研究提出了外部防御机制——guardrails，并指出当前该领域缺乏统一的分类和评估体系。本文作为知识系统化（SoK）论文，首次对LLM的jailbreak防护机制进行了全面分析，提出了一种六维分类体系，并设计了一个结合安全性、效率与实用性的评估框架。通过实验与分析，研究揭示了现有防护方法的优势与局限性，探讨了其在不同攻击类型中的适用性，并为未来优化防护组合提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:42:40 GMT</pubDate>
</item>
<item>
<title>基于隐式视觉标记的多模态推理框架 Mirage</title>
<link>https://arxiv.org/abs/2506.17218</link>
<guid>https://arxiv.org/abs/2506.17218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mirage框架提升多模态推理能力，无需生成显式图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Mirage 的多模态推理框架，旨在提升视觉语言模型（VLMs）在不需要生成显式图像的情况下进行视觉推理的能力。受人类通过心理意象进行推理的启发，Mirage 在文本解码过程中引入隐式视觉标记，使模型能够在不生成像素级图像的情况下继续多模态推理流程。该方法首先通过从真实图像嵌入中蒸馏监督隐式标记，随后切换为纯文本监督以对齐任务目标，并通过强化学习进一步增强多模态推理能力。实验结果表明，Mirage 在多个基准测试中表现出更强的多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>MEXA：一种无需训练的多模态推理框架</title>
<link>https://arxiv.org/abs/2506.17113</link>
<guid>https://arxiv.org/abs/2506.17113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEXA通过聚合专家模型实现跨领域的多模态推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MEXA的训练-free 框架，用于在不同领域中进行多模态推理。MEXA能够根据输入模态和任务需求动态选择专家模型，并利用大型推理模型对这些模型的输出进行整合与推理，从而生成最终答案。该方法无需额外训练，具有模块化设计，适用于视频推理、音频推理、3D理解及医疗问答等多种任务。实验表明，MEXA在多个多模态基准测试中均优于现有方法，展示了其在多模态推理中的有效性与广泛适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 12:14:13 GMT</pubDate>
</item>
<item>
<title>基于对数概率序列的提示逆向方法研究</title>
<link>https://arxiv.org/abs/2506.17090</link>
<guid>https://arxiv.org/abs/2506.17090</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PILS方法，提升隐藏提示恢复准确率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了语言模型逆向问题，旨在通过模型输出恢复隐藏提示。作者提出了一种新方法——从对数概率序列中进行提示逆向（PILS），利用模型在多个生成步骤中的下一个词概率来恢复隐藏提示。该方法基于一个关键洞察：语言模型的向量输出位于低维子空间中，这使得可以无损压缩多步生成的概率分布。实验结果显示，PILS在恢复隐藏提示方面比现有方法有显著提升，恢复率提高了2到3.5倍。此外，该方法在恢复隐藏系统消息任务中也表现出色，并展示了良好的泛化能力。研究还分析了重复内容在提示恢复中的作用，并提出了基于逻辑的跨模型迁移方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17090" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:53:51 GMT</pubDate>
</item>
<item>
<title>图像生成模型输出的令牌级水印方法</title>
<link>https://arxiv.org/abs/2506.16349</link>
<guid>https://arxiv.org/abs/2506.16349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种图像生成模型的令牌级水印技术，提升检测可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对自回归图像生成模型输出的令牌级水印方法，这是该领域的首次尝试。研究发现，由于缺乏反向循环一致性（RCC），重新分词会破坏水印。为解决此问题并增强对常见图像变换、神经压缩和移除攻击的鲁棒性，作者引入了定制的分词器-反分词器微调流程和互补的水印同步层。实验表明，该方法在理论上具有可靠的p值，能够实现稳健的水印检测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 10:25:51 GMT</pubDate>
</item>
<item>
<title>Vision-Language-Action模型的泛化能力评估与基准测试</title>
<link>https://arxiv.org/abs/2506.09930</link>
<guid>https://arxiv.org/abs/2506.09930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">评估VLA模型在模拟任务中的泛化能力，揭示其感知与执行差距。</p><br /><br /><p><strong>摘要：</strong> 本文针对Vision-Language-Action (VLA)模型在机器人领域中的泛化能力进行了系统评估。尽管VLA模型基于大型视觉-语言模型（VLM）展现出强大的感知理解和高层规划能力，但在实际动作执行中表现不稳定，尤其在面对分布外观察时。研究引入了一个包含50个模拟任务的统一评估套件，覆盖10个子类别。实验结果表明，微调动作数据可能削弱VLM的通用推理能力。作者开源了任务套件和代码，旨在为未来VLA研究提供标准化基准，推动感知到动作的差距缩小。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:52:18 GMT</pubDate>
</item>
<item>
<title>多语言语音合成中文化敏感情感与口音建模的新方法</title>
<link>https://arxiv.org/abs/2506.16310</link>
<guid>https://arxiv.org/abs/2506.16310</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新TTS系统提升印度语和英语口音及情感准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型文本转语音（TTS）架构，专门针对印地语和印度英语的口音与情感建模。该系统在Parler-TTS基础上进行了改进，引入了语言特定的音素对齐编码器-解码器结构、基于本地语料库训练的文化敏感情感嵌入层，以及动态口音代码切换机制。实验结果显示，该系统在口音准确率上提升了23.7%，情感识别准确率达85.3%，优于现有基线模型。此外，系统支持实时口音切换，保持情感一致性，用户主观评价得分为4.2/5，显著优于现有系统。该研究为跨语言语音合成提供了可行方案，适用于南亚教育科技和无障碍软件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16310" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 09:35:05 GMT</pubDate>
</item>
<item>
<title>InfGen：一种用于长期交通仿真的统一模型</title>
<link>https://arxiv.org/abs/2506.17213</link>
<guid>https://arxiv.org/abs/2506.17213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfGen实现长期交通仿真，性能优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfGen，一种统一的下一标记预测模型，能够同时进行封闭回路运动模拟和场景生成。该模型在短期（9秒）交通仿真中达到最先进水平，并在长期（30秒）仿真中显著优于其他方法。InfGen可以自动切换模拟模式，实现稳定、真实的长期交通仿真，适用于自动驾驶系统部署中的实际场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>UniFork：一种新型的统一图像理解与生成架构</title>
<link>https://arxiv.org/abs/2506.17202</link>
<guid>https://arxiv.org/abs/2506.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniFork架构，提升图像理解与生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了统一图像理解与生成模型的架构设计问题，发现理解任务和生成任务在模态对齐模式上存在显著差异。理解任务需要随着网络深度增加逐步增强模态对齐以提升语义理解，而生成任务则在浅层增强对齐、深层减弱以恢复空间细节。这种差异导致传统共享Transformer架构难以兼顾两者。为此，作者提出UniFork架构，采用Y型结构，在浅层共享表示学习，深层引入任务专用分支，有效平衡了共享学习与任务专精，实验表明其性能优于传统架构和任务专用模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:52:31 GMT</pubDate>
</item>
<item>
<title>基于提示的参数生成方法DnD实现高效大语言模型微调</title>
<link>https://arxiv.org/abs/2506.16406</link>
<guid>https://arxiv.org/abs/2506.16406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DnD通过提示生成参数，大幅提升LLM微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Drag-and-Drop LLMs（DnD）的新方法，该方法通过将少量未标记的任务提示直接映射为LoRA权重更新，从而避免了每个下游数据集都需要单独优化的过程。DnD使用轻量级文本编码器将提示批次压缩为条件嵌入，并通过级联超卷积解码器生成完整的LoRA矩阵。训练完成后，DnD可在几秒内生成特定任务的参数，相比全量微调降低了12,000倍的开销，并在多个基准测试中表现出30%的性能提升。此外，DnD在跨领域任务中也展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 11:38:21 GMT</pubDate>
</item>
<item>
<title>PAROAttention：通过重新排序提升视觉生成中的注意力效率</title>
<link>https://arxiv.org/abs/2506.16054</link>
<guid>https://arxiv.org/abs/2506.16054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAROAttention优化视觉生成中的注意力机制，提升效率与性能。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成中，注意力机制的二次复杂度导致高内存和计算成本，尤其在高分辨率图像或视频生成时更为显著。为解决此问题，研究者尝试了稀疏化和量化等技术，但效果受限。本文指出，视觉注意力模式的分散性和不规则性是核心难题，因此提出一种新策略——重新组织注意力模式。受视觉特征提取局部聚合特性的启发，设计了Pattern-Aware token ReOrdering (PARO)技术，将多样化的注意力模式统一为硬件友好的块状模式，从而简化并提升稀疏化和量化效果。实验表明，PAROAttention在保持与全精度模型相近性能的同时，显著降低密度和位宽（INT8/INT4），实现1.9x至2.7倍的端到端延迟加速。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 02:25:02 GMT</pubDate>
</item>
<item>
<title>基于多模态模型的文档分块方法提升RAG系统性能</title>
<link>https://arxiv.org/abs/2506.16035</link>
<guid>https://arxiv.org/abs/2506.16035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态文档分块方法提升RAG系统准确性和结构保留能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大型多模态模型（LMMs）的新型多模态文档分块方法，用于处理PDF文档中的复杂结构，如跨页表格、嵌入图表和上下文依赖。该方法通过配置化的页面批次处理方式，保持语义连贯性和结构完整性，并在手动构建的PDF数据集上验证了其有效性，结果显示该方法在分块质量和下游RAG性能方面均有显著提升，相比传统RAG系统具有更高的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 01:11:43 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.1：3D AI生成内容的全面教程</title>
<link>https://arxiv.org/abs/2506.15442</link>
<guid>https://arxiv.org/abs/2506.15442</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.1提供3D生成模型的完整训练与评估指南。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Hunyuan3D 2.1，这是一个用于3D人工智能生成内容（AIGC）的先进系统，旨在帮助用户处理3D数据、训练生成模型并进行性能评估。该系统包含两个核心组件：Hunyuan3D-DiT用于形状生成，Hunyuan3D-Paint用于纹理合成。文章详细讲解了从数据准备、模型架构、训练策略到评估指标和部署的全流程，适合希望在游戏、虚拟现实和工业设计中应用3D生成技术的开发者和研究人员。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15442" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:14:46 GMT</pubDate>
</item>
<item>
<title>InfiniPot-V：突破视频流理解的内存瓶颈</title>
<link>https://arxiv.org/abs/2506.15745</link>
<guid>https://arxiv.org/abs/2506.15745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniPot-V实现视频流实时处理且不占用过多内存。</p><br /><br /><p><strong>摘要：</strong> InfiniPot-V是一种无需训练、与查询无关的框架，能够在视频流处理中保持固定内存上限。该方法在视频编码过程中监控缓存，当达到用户设定阈值时，通过时间轴冗余度（TaR）和值范数（VaN）对缓存进行轻量级压缩，有效去除冗余信息并保留语义重要信息。实验表明，InfiniPot-V可减少高达94%的GPU峰值内存，同时保持实时生成能力和与全缓存相当的准确性，适用于多种多模态大模型和视频基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 22:22:14 GMT</pubDate>
</item>
<item>
<title>基于多平面同步的3D全景图扩散模型DreamCube</title>
<link>https://arxiv.org/abs/2506.17206</link>
<guid>https://arxiv.org/abs/2506.17206</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多平面同步扩展2D基础模型至全景域，实现高质量3D全景图生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DreamCube的多平面RGB-D扩散模型，用于3D全景图生成。该模型通过在2D基础模型的操作器上应用多平面同步技术，有效克服了2D图像先验与3D全景图不兼容的问题，从而实现了对全景视觉外观和几何结构的有效建模。实验表明，DreamCube在全景图像生成、全景深度估计及3D场景生成方面均表现出色，显著提升了生成内容的质量和多样性，同时保证了多视角一致性。这项工作为3D全景合成提供了新的思路，有望推动相关领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17206" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Hunyuan-GameCraft：面向游戏环境的高动态交互视频生成框架</title>
<link>https://arxiv.org/abs/2506.17201</link>
<guid>https://arxiv.org/abs/2506.17201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Hunyuan-GameCraft框架解决现有游戏视频生成方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来扩散模型驱动的可控视频生成技术取得了显著进展，但当前方法在动态表现、通用性、长期一致性及效率方面仍存在不足，限制了其在游戏视频生成中的应用潜力。为弥补这些缺陷，我们引入了Hunyuan-GameCraft，这是一种针对游戏环境中高动态交互视频生成的新框架。该框架通过统一键盘和鼠标输入至共享相机表示空间实现精细动作控制，并采用混合历史条件训练策略以扩展视频序列并保留场景信息。此外，为了提升推理效率与可玩性，框架实现了模型蒸馏，减少计算开销的同时保持长时间序列的一致性。模型基于涵盖超过100款AAA级游戏的百万量级游戏录像数据集进行训练，并在精心标注的合成数据集上微调以增强精度与控制能力。实验表明，Hunyuan-GameCraft在视觉逼真度、真实性及动作控制方面均显著优于现有方法，极大推动了交互式游戏视频生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:50:37 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.5：高保真3D资产生成的新突破</title>
<link>https://arxiv.org/abs/2506.16504</link>
<guid>https://arxiv.org/abs/2506.16504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.5通过新模型LATTICE和PBR技术提升3D形状与纹理生成质量。</p><br /><br /><p><strong>摘要：</strong> Hunyuan3D 2.5是一款强大的3D扩散模型套件，专注于生成高质量且细节丰富的纹理化3D资产。它沿用了Hunyuan3D 2.0的两阶段流程，但在形状和纹理生成方面取得了显著进步。在形状生成方面，引入了新的基础模型LATTICE，该模型通过扩展高质量数据集、模型规模和计算资源进行训练。最大模型参数达到10B，能够生成清晰、详细的3D形状，并精确匹配图像与3D模型，同时保持网格表面干净平滑，大幅缩小生成形状与手工制作形状之间的差距。在纹理生成方面，通过基于物理的渲染(PBR)技术，借助从Hunyuan3D 2.0 Paint模型扩展的多视图架构实现升级。广泛评估显示，Hunyuan3D 2.5在形状生成和端到端纹理生成方面均显著优于先前方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在无偏视角摘要中的应用与评估</title>
<link>https://arxiv.org/abs/2506.15925</link>
<guid>https://arxiv.org/abs/2506.15925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出改进的评估方法提升视角摘要质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中的政治视角摘要这一重要应用，指出当前评价框架对覆盖度和忠实性等属性的传统度量方法缺乏适用性验证的问题。为填补这一空白，我们首先通过人工注释构建测试集来评估度量指标的可靠性，发现基于语言模型的度量方法优于传统指标。进一步地，我们证明基于重排序的方法效果显著，并且利用合成数据进行偏好微调可进一步提升性能。这些研究成果有助于推动视角摘要方法的可靠评估与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 20:01:43 GMT</pubDate>
</item>
<item>
<title>VIKI-Bench与VIKI-R：多智能体协作新基准与框架</title>
<link>https://arxiv.org/abs/2506.09049</link>
<guid>https://arxiv.org/abs/2506.09049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VIKI-Bench基准和VIKI-R框架，提升具身多智能体视觉推理合作能力。</p><br /><br /><p><strong>摘要：</strong> 在动态环境中协调多个具身代理是人工智能中的核心挑战，涉及感知驱动推理与可扩展合作策略。尽管已有研究利用大型语言模型进行多智能体规划，但基于视觉-语言模型(VLM)的视觉推理探索尚少且支持多样性有限。本文引入VIKI-Bench，首个针对具身多智能体协作设计的分层基准，涵盖代理激活、任务规划和轨迹感知三个层次。VIKI-Bench包含多样化的机器人形态、多视角视觉观测及结构化监督信号。为展示其价值，我们提出VIKI-R框架，通过链式思维标注演示微调预训练VLM，再结合多层级奖励信号强化学习。实验表明，VIKI-R在所有任务层次上显著优于基线方法，并促进异构代理间的组合式合作模式涌现。VIKI-Bench与VIKI-R共同构成推动具身AI系统多智能体视觉驱动合作发展的统一测试平台与方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Show-o2：基于流匹配与自回归建模的多模态统一模型</title>
<link>https://arxiv.org/abs/2506.15564</link>
<guid>https://arxiv.org/abs/2506.15564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合流匹配与自回归建模的多模态统一模型Show-o2。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Show-o2的改进型原生统一多模态模型，该模型通过空间（时空）融合构建统一视觉表示，并利用因果变分自编码器空间实现跨图像和视频模态的扩展。Show-o2在语言头和流头分别应用自回归建模和流匹配技术，用于文本标记预测和图像/视频生成。设计了两阶段训练方法以有效学习并扩展到更大模型，最终模型在多种模态（如文本、图像和视频）的多模态理解和生成任务中表现出色。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 11:39:15 GMT</pubDate>
</item>
<item>
<title>RE-IMAGINE框架评估大型语言模型的推理能力</title>
<link>https://arxiv.org/abs/2506.15455</link>
<guid>https://arxiv.org/abs/2506.15455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出RE-IMAGINE框架，评估大型语言模型是否依赖统计记忆进行推理。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明大型语言模型（LLMs）在推理基准测试中表现优异，但其推理能力的真实来源尚不明确。本文受因果阶梯理论启发，提出RE-IMAGINE框架，通过生成不同层次的问题变体，系统性地评估LLMs的推理能力。该框架基于中间符号表示生成问题，避免单纯依赖训练集的记忆能力。实验结果显示，在不同推理领域（如数学、代码、逻辑）中，模型性能随问题复杂度提升而下降，表明现有模型对统计记忆存在一定程度的依赖。这一发现为未来研究如何提升LLMs的高层次推理能力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:35:47 GMT</pubDate>
</item>
<item>
<title>SonicVerse：融合多任务特征检测的音乐描述生成模型</title>
<link>https://arxiv.org/abs/2506.15154</link>
<guid>https://arxiv.org/abs/2506.15154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合音频特征检测的音乐描述生成模型，提升音乐AI研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SonicVerse的多任务音乐描述生成模型，该模型不仅生成音乐片段的描述，还通过集成关键检测、人声检测等辅助任务捕捉低级声学细节和高级音乐属性。其创新之处在于基于投影的架构，将音频输入转化为语言标记的同时进行特征检测。此框架不仅能为短音乐片段生成丰富的描述，还能通过时间信息链式生成长音乐片段的详细描述。为训练模型，作者扩展了MusicBench数据集，使用MIRFLEX标注音乐特征，最终实验表明，这种方法显著提升了生成描述的质量和细节水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 01:51:36 GMT</pubDate>
</item>
<item>
<title>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</title>
<link>https://arxiv.org/abs/2506.14965</link>
<guid>https://arxiv.org/abs/2506.14965</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 16:24:00 GMT</pubDate>
</item>
<item>
<title>基于迭代精化的图表到代码生成方法</title>
<link>https://arxiv.org/abs/2506.14837</link>
<guid>https://arxiv.org/abs/2506.14837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于结构化指令的迭代精化方法提升图表到代码生成性能。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）因其强大的视觉理解能力而备受关注，但在图表到代码生成任务上表现欠佳。这项任务不仅需要精确的视觉理解，还需要将视觉元素准确转化为结构化代码。本文提出的{ChartIR}通过区分视觉理解和代码翻译两个子任务，设计描述和差异两种结构化指令，将视觉特征转化为语言表示，从而优化后续代码翻译过程。此外，该方法将整个生成流程分解为初始代码生成和迭代精化两个阶段，逐步提升最终输出质量。实验结果显示，无论是在开源模型Qwen2-VL还是闭源模型GPT-4o上，{ChartIR}均优于其他方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 10:10:16 GMT</pubDate>
</item>
<item>
<title>EmoNet-Voice：基于新基准的数据集推动语音情感识别发展</title>
<link>https://arxiv.org/abs/2506.09827</link>
<guid>https://arxiv.org/abs/2506.09827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmoNet-Voice推出包含大规模预训练数据集和专家标注的新基准，提升AI语音情感识别能力。</p><br /><br /><p><strong>摘要：</strong> 随着文本转语音及音频生成模型的进步，评估AI系统情感理解能力的需求愈发迫切。然而，当前的语音情感识别（SER）数据集存在情感粒度不足、隐私问题或依赖演员表演等局限性。本文介绍EmoNet-Voice，一种新的语音情感检测资源，它包括EmoNet-Voice Big（涵盖超过4500小时的跨语言、多情绪语音数据）和EmoNet-Voice Bench（具有人工专家注释的新型基准数据集）。该资源旨在通过精细的情感分类评估SER模型，并利用先进的语音生成技术创建模拟演员表演的合成音频片段，同时由心理学专家验证并标注感知强度。此外，我们提出Empathic Insight Voice模型，在情感识别上达到与人类专家高度一致的新标准。我们的研究发现表明，高唤醒情绪如愤怒比低唤醒状态如专注更容易被检测到。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 11:06:59 GMT</pubDate>
</item>
<item>
<title>基于视觉Transformer的寿命预测模型</title>
<link>https://arxiv.org/abs/2506.13430</link>
<guid>https://arxiv.org/abs/2506.13430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用预训练视觉Transformer模型从图像预测剩余寿命。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，通过利用预训练的视觉Transformer基础模型，结合面部和全身图像估计个体的剩余寿命，并实现了对预测不确定性的量化。研究显示，预测不确定性系统性地随真实剩余寿命变化，且可以通过学习高斯分布来有效建模。该方法在现有数据集上达到了7.48年的平均绝对误差（MAE），并在两个新发布的高质量数据集上进一步优化至4.79年和5.07年。虽然此模型尚未达到临床部署标准，但其结果显示了从图像中提取医学相关信息的潜力。所有代码和数据集均公开，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:47:37 GMT</pubDate>
</item>
<item>
<title>提升小规模推理语言模型性能的研究</title>
<link>https://arxiv.org/abs/2506.13404</link>
<guid>https://arxiv.org/abs/2506.13404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索训练策略以提高0.5B参数量语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型的发展，尽管这些模型在多种任务上表现出色，但其高昂的计算成本和潜在隐私问题限制了其应用范围。相比之下，具有约0.5亿参数的小型推理语言模型（SRLMs）因其高效性和经济性成为一种有吸引力的选择，尤其是在资源受限的环境中。然而，这类小型模型由于容量限制，在处理复杂任务如数学推理和代码生成时面临挑战。本研究探讨了监督微调（SFT）、知识蒸馏（KD）和强化学习（RL）等不同的训练策略及其组合方法，以缩小小型模型与大型模型之间的性能差距。通过广泛的实验验证和分析，我们提出了优化的训练管道建议，旨在最大化0.5B参数量模型的推理能力，为相关领域的实际应用提供指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:18:11 GMT</pubDate>
</item>
<item>
<title>SeqPE：一种统一且完全可学习的位置编码框架</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SeqPE框架，解决传统位置编码在扩展性和多模态适应性上的局限。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeqPE的统一且完全可学习的位置编码框架，该框架通过将每个n维位置索引表示为符号序列，并采用轻量级顺序位置编码器以端到端方式学习嵌入。为了规范SeqPE的嵌入空间，引入了对比目标和知识蒸馏损失两种互补的目标函数。实验表明，SeqPE在语言建模、长上下文问答和二维图像分类等任务上表现优异，特别是在上下文长度外推方面超越了强大的基线模型，并且能够在无需手动架构重设计的情况下无缝泛化到多维输入。此外，作者开源了代码、数据和检查点。关键词：Transformer、位置编码、SeqPE。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 05:16:40 GMT</pubDate>
</item>
<item>
<title>DoTA-RAG：面向大规模网络知识索引的高效检索增强生成系统</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DoTA-RAG通过三阶段管道优化检索增强生成系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DoTA-RAG（动态聚合检索增强生成系统）的新方法，该系统专门针对高吞吐量、大规模网络知识索引进行了优化。传统检索增强生成（RAG）系统在处理海量多样化数据时通常面临高延迟和有限准确性的问题。为解决这些问题，DoTA-RAG采用了三阶段管道策略，包括查询重写、动态路由到专用子索引以及多阶段检索和排名。此外，通过评估并选择更优的嵌入模型并对FineWeb-10BT语料库进行重新嵌入，进一步增强了系统的检索能力。同时，构建了一个包含500个问题的多样化问答数据集，涵盖了广泛的WebOrganizer主题和格式。实验结果显示，DoTA-RAG将答案正确性得分从基线的0.752提高到了1.478，同时保持了低延迟，在Live Challenge Day上达到了0.929的正确性得分。这些成果表明，DoTA-RAG在需要快速可靠访问大型且不断发展的知识源的领域具有实际部署潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 12:56:00 GMT</pubDate>
</item>
<item>
<title>利用大型语言模型评估新闻媒体可信度与政治偏见的研究</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法利用大型语言模型评估新闻媒体的整体可信度与政治倾向。</p><br /><br /><p><strong>摘要：</strong> 在当今充斥着虚假信息的网络环境中，帮助读者理解所读内容至关重要。传统上，事实核查主要依赖手动或自动方式，但对新兴话题或信息有限的情况难以应对。本研究聚焦于评估新闻媒体整体的可靠性及政治偏见，而非单篇文章。我们设计了一系列基于专业事实核查员标准的提示，并通过大型语言模型（LLMs）获取回应，最终整合预测结果。实验表明，该方法显著优于现有基线模型，并深入分析了媒体流行度和地区对模型表现的影响。此外，还进行了消融研究以确定数据集的关键要素。为促进后续研究，我们公开了数据集和代码。这项研究填补了新闻媒体整体评估领域的空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 11:49:20 GMT</pubDate>
</item>
<item>
<title>QGuard：一种基于问题提示的大语言模型安全防护方法</title>
<link>https://arxiv.org/abs/2506.12299</link>
<guid>https://arxiv.org/abs/2506.12299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QGuard方法，利用问题提示零样本防御大语言模型的有害提示攻击。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）面临的有害提示和越狱提示带来的恶意攻击风险，提出了名为QGuard的安全防护方法。QGuard通过引入问题提示，在无需微调的情况下，以零样本方式有效阻止文本和多模态有害提示的攻击。实验结果显示，该方法在文本和多模态有害数据集上表现出色，同时通过对问题提示的分析，实现了用户输入的白盒分析。我们认为，此方法为实际LLMs服务中的安全风险缓解提供了有价值的参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12299" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 21:23:50 GMT</pubDate>
</item>
<item>
<title>EgoPrivacy：第一人称视角隐私风险评估基准</title>
<link>https://arxiv.org/abs/2506.12258</link>
<guid>https://arxiv.org/abs/2506.12258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示第一人称视频中可推断出的穿戴者隐私信息。</p><br /><br /><p><strong>摘要：</strong> 随着可穿戴摄像头的普及，尽管已有研究关注旁观者隐私，但对穿戴者隐私威胁的研究相对不足。本文引入EgoPrivacy，这是首个针对第一人称视角视觉隐私风险的大规模基准测试工具，涵盖三类隐私（人口统计、个体特征和情境信息），并定义了七个任务来恢复从精细到粗粒度的私密信息。此外，我们提出了一种新颖的检索增强攻击策略，通过外部旁观者视频池中的检索来提高人口统计隐私攻击的效果。实验表明，即使在零样本设置下，基础模型也能有效泄露穿戴者的身份、场景、性别和种族等属性，准确率可达70%-80%。我们的代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 18:19:54 GMT</pubDate>
</item>
<item>
<title>语言模型在动态仇恨言论检测中的时间敏感性评估</title>
<link>https://arxiv.org/abs/2506.12148</link>
<guid>https://arxiv.org/abs/2506.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示静态基准无法准确评估语言模型处理动态仇恨言论的能力。</p><br /><br /><p><strong>摘要：</strong> 随着社会动态和文化变迁，仇恨言论的语言也在迅速演变，这对自然语言处理（NLP）领域提出了挑战。尽管已有研究探讨了语言演化对模型训练的影响并提出了一些解决方案，但其对模型基准测试的影响仍未得到充分探索。作为保障模型安全的关键工具，仇恨言论基准的重要性不言而喻。本文通过实证研究评估了20种语言模型在两个动态仇恨言论实验中的鲁棒性，揭示了静态与时间敏感性评估之间的时间错配问题。我们的发现强调了构建时间敏感型语言基准的必要性，以便更可靠地评价仇恨言论领域的语言模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 14:08:19 GMT</pubDate>
</item>
<item>
<title>VGR：增强视觉感知能力的多模态链式推理大模型</title>
<link>https://arxiv.org/abs/2506.11991</link>
<guid>https://arxiv.org/abs/2506.11991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新多模态大语言模型VGR，提升复杂视觉推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有多模态链式推理方法主要依赖纯语言空间且局限于数学或科学领域的局限性，提出了一种名为VGR的新模型。VGR是一种具有增强细粒度视觉感知能力的多模态大语言模型，它通过检测有助于解决问题的相关区域并基于这些区域提供精确答案来实现视觉和语言推理的结合。为了训练该模型，我们构建了一个包含视觉定位和语言推理混合数据的大规模SFT数据集VGR-SFT。实验表明，在LLaVA-NeXT-7B基线上，VGR在需要全面理解图像细节的多模态基准测试中表现出色，例如在MMStar上得分提高了+4.1，在AI2D上提高了+7.1，在ChartQA上则提升了+12.9，同时使用的图像token数量仅为基线模型的30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11991" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:47:43 GMT</pubDate>
</item>
<item>
<title>TaskCraft：自动化生成可扩展的多工具交互型任务</title>
<link>https://arxiv.org/abs/2506.10055</link>
<guid>https://arxiv.org/abs/2506.10055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaskCraft通过自动化生成多工具交互型任务提升AI模型性能。</p><br /><br /><p><strong>摘要：</strong> 随着多步骤问题解决等自主性任务在自然语言处理（NLP）和人工智能（AI）中的重要性增加，现有的指令数据缺乏工具交互能力，而当前的基准测试依赖昂贵的人工标注，限制了其扩展性。本文介绍TaskCraft，这是一种自动化的任务生成工作流，能够创建难度可调节、支持多工具交互且可验证的任务，并包含执行轨迹。TaskCraft通过深度和宽度扩展原子任务，构建结构和层次复杂的挑战。实证结果显示，这些任务优化了生成工作流中的提示，并提升了对自主基础模型的监督微调效果。此外，研究还提供了一个包含约36,000个任务的大规模合成数据集，以支持未来关于代理调优和评估的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:14 GMT</pubDate>
</item>
<item>
<title>基于LLM辅助系统的自主学习能力培养研究</title>
<link>https://arxiv.org/abs/2506.09968</link>
<guid>https://arxiv.org/abs/2506.09968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过游戏化和AI支持系统提升大学生自主学习技能。</p><br /><br /><p><strong>摘要：</strong> 自主学习能力（SRL）对大学生适应学术挑战至关重要。本研究针对59名大学生面临的自主学习技能发展难题，如目标设定、时间管理和反思性学习困难，开发了SRLAgent系统。该系统基于Zimmerman的三阶段SRL框架，利用大型语言模型（LLM）提供实时反馈和适应性支持，在游戏化环境中促进学生的自主学习。实验结果显示，SRLAgent组学生的自主学习技能显著提高（p < .001, 效应值d=0.234），且用户参与度高于对照组。这项研究强调了将自主学习支架和实时AI支持嵌入游戏化环境中的价值，为教育技术的设计提供了重要启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:45:03 GMT</pubDate>
</item>
<item>
<title>TransDiff：结合Transformer与扩散模型的图像生成新方法</title>
<link>https://arxiv.org/abs/2506.09482</link>
<guid>https://arxiv.org/abs/2506.09482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransDiff结合AR Transformer与扩散模型，在ImageNet上性能超越其他方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TransDiff，一种首次将自回归（AR）Transformer与扩散模型相结合的图像生成模型。在该联合建模框架下，TransDiff通过编码标签和图像到高层次语义特征，并利用扩散模型估计图像样本分布。在ImageNet 256x256基准测试中，TransDiff显著优于基于独立AR Transformer或扩散模型的其他图像生成模型。具体而言，TransDiff实现了Fréchet Inception Distance (FID) 为1.61和Inception Score (IS) 为293.4的性能，并且相比最先进的AR Transformer方法推理延迟快2倍，比纯扩散模型方法快112倍。此外，基于TransDiff模型，我们提出了一种新的图像生成范式——多参考自回归（MRAR），它通过预测下一个图像进行自回归生成，允许模型引用多个先前生成的图像，从而促进学习更丰富的表示并提高后续迭代生成图像的质量。应用MRAR后，TransDiff的FID从1.61降至1.42。我们预计TransDiff将在图像生成领域开辟新的研究前沿。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 03:50:31 GMT</pubDate>
</item>
<item>
<title>MATTER：结合材料知识的新型分词方法提升科学文本处理性能</title>
<link>https://arxiv.org/abs/2506.11115</link>
<guid>https://arxiv.org/abs/2506.11115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MATTER，一种结合材料知识的分词方法，改善材料科学领域语言模型的表现。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型在材料科学中的应用日益广泛，传统的基于频率的分词方法因无法维持材料概念的结构和语义完整性而受到限制。本文提出了一种名为MATTER的新方法，通过集成材料知识并优先考虑材料概念，有效解决了现有分词方法的问题。实验表明，MATTER在生成和分类任务中分别提升了4%和2%的平均性能，强调了领域知识在科学文本处理中的重要性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:59:13 GMT</pubDate>
</item>
<item>
<title>DeepEDM：结合深度学习与非线性动力学系统的时序预测框架</title>
<link>https://arxiv.org/abs/2506.06454</link>
<guid>https://arxiv.org/abs/2506.06454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种融合深度神经网络与非线性动力学建模的新框架DeepEDM。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中复杂非线性动态时间序列的预测问题，提出了DeepEDM框架。该框架基于Takens定理，通过延迟嵌入学习潜在空间，并利用核回归逼近隐藏的动力学机制，同时采用高效的Softmax注意力实现精准的未来时间步预测。实验表明，DeepEDM在合成数据及跨领域真实数据上均表现出较强的鲁棒性和预测准确性，优于现有方法。研究代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:24:12 GMT</pubDate>
</item>
<item>
<title>离散扩散语言模型与多模态语言模型综述</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">离散扩散模型在并行生成和推理加速方面优于自回归模型。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs），这些模型采用多令牌并行解码范式，与自回归模型相比具有生成速度快、可控性强等优势。文章追溯了dLLMs和dMLLMs的历史发展，总结了训练与推理的关键技术，并分析了其在语言、视觉-语言及生物领域的应用前景。此外，本文还讨论了未来研究方向和部署挑战，强调了这些模型在学术界和工业界的快速发展趋势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>预算引导：通过轻量级预测控制大语言模型推理长度</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出预算引导方法，有效控制大语言模型推理长度并提升效率。</p><br /><br /><p><strong>摘要：</strong> 近期深度大型语言模型通常通过大量推理来提高性能，但这种方法可能导致过高的推理成本且性能提升有限。因此，在有限的推理预算下控制推理长度至关重要，但具有挑战性。本文提出了一种名为预算引导的新方法，无需对语言模型进行微调即可指导其推理过程达到目标预算。该方法引入了一个轻量级预测器，在每次生成下一个令牌时预测剩余推理长度的伽马分布。这一信号被用于以软方式、令牌级别的方式指导生成过程，确保整体推理符合指定的预算限制。实验表明，预算引导方法在数学基准测试中显著提高了令牌效率，例如在MATH-500基准测试中，与基线方法相比，在严格预算下准确率提高了26%，同时仅使用全推理模型63%的令牌即保持竞争力。此外，该方法在更广泛的任务领域表现出色，并展现出估计问题难度等新能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>Test3R：通过测试时学习显著提升3D重建几何精度</title>
<link>https://arxiv.org/abs/2506.13750</link>
<guid>https://arxiv.org/abs/2506.13750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Test3R技术，利用图像三元组优化网络，大幅提升3D重建几何一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Test3R的简单但有效的测试时学习技术，用于增强密集匹配方法在3D重建中的几何准确性。传统方法依赖于成对点图回归，但存在全局几何一致性不足的问题。Test3R通过使用图像三元组(I_1, I_2, I_3)，分别生成基于(I_1, I_2)和(I_1, I_3)的重建，并在测试时通过自监督目标优化网络，即最大化这两重建之间的几何一致性。这种方法确保了模型输出的跨对一致性，无论输入如何。实验结果显示，Test3R在3D重建和多视图深度估计任务上显著优于现有最先进的方法，且具有广泛的适用性和几乎零成本的特点。此外，该技术易于与其他模型集成，并在测试时仅需极小的训练开销和参数量。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:56:22 GMT</pubDate>
</item>
<item>
<title>Ego-R1框架：通过强化学习实现超长时间第一人称视频推理</title>
<link>https://arxiv.org/abs/2506.13654</link>
<guid>https://arxiv.org/abs/2506.13654</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Ego-R1框架，利用工具链式推理解决超长时第一人称视频理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Ego-R1的新框架，用于处理长达数天甚至数周的第一人称视频推理。该框架通过结构化的工具链式思维（CoTT）过程实现推理，由经过强化学习训练的Ego-R1智能体协调操作。受到人类解决问题策略的启发，CoTT将复杂的推理分解为模块化步骤，每一步调用特定工具完成子问题求解，如时间检索和多模态理解。为了训练该智能体，设计了包含监督微调（SFT）和强化学习（RL）两个阶段的训练范式，并构建了Ego-R1 Data数据集，其中包括Ego-CoTT-25K用于SFT，Ego-QA-4.4K用于RL。此外，Ego-R1智能体在新创建的一周视频问答基准Ego-R1 Bench上进行了评估，该基准包含来自混合来源的人类验证问答对。实验结果表明，Ego-R1智能体能够有效应对理解超长第一人称视频的独特挑战，时间覆盖范围从几小时显著扩展到一周。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13654" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 12:17:08 GMT</pubDate>
</item>
<item>
<title>MiniMax-M1：全球首个开放权重大规模混合注意力推理模型发布</title>
<link>https://arxiv.org/abs/2506.13585</link>
<guid>https://arxiv.org/abs/2506.13585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniMax-M1是一款支持超长上下文处理的高效混合注意力推理模型。</p><br /><br /><p><strong>摘要：</strong> MiniMax-M1是全球首款开放权重、大规模混合注意力推理模型，由混合专家（MoE）架构与闪电注意力机制驱动，基于MiniMax-Text-01升级而来，总参数达4560亿，单token激活459亿参数，上下文长度可达100万tokens，是DeepSeek R1的8倍。该模型通过大规模强化学习训练，在复杂任务如软件工程和工具利用方面表现出色。此外，提出的CISPO算法进一步提升了强化学习效率，使M1仅用512块H800 GPU完成训练仅需三周，成本仅为$534,700。实验显示，MiniMax-M1在标准基准测试中与强开源模型相比表现相当甚至更优。MiniMax-M1已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 11:08:02 GMT</pubDate>
</item>
<item>
<title>结构化提示对大型语言模型文本分析能力的影响研究</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，结构化提示可有效引导大型语言模型进行复杂文本分析。</p><br /><br /><p><strong>摘要：</strong> 本研究设计并评估了一组概念验证的结构化工作流提示，旨在引导大型语言模型（LLMs）完成高阶语义和语言分析任务，如检测学术论文总结中的未经证实主张（信息完整性）及模糊代词引用（语言清晰度）。通过在Gemini Pro 2.5 Pro和ChatGPT Plus o3两款前沿模型上进行多轮系统性评估，发现模型在信息完整性任务上的表现因语法角色差异而异，且在仅提供摘要而非全文的情况下，ChatGPT表现出色，而Gemini性能显著下降。这一研究揭示了结构化提示在复杂文本分析中的潜力，同时强调了模型性能高度依赖于模型类型、任务性质及上下文条件，需进行严谨的特定模型测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 03:34:31 GMT</pubDate>
</item>
<item>
<title>基于提示的大型语言模型时间序列预测方法</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示无需重训即可让大型语言模型高效进行时间序列预测。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLMs）的进步为准确且高效的时间序列分析提供了新的可能性，但以往研究通常需要大量微调或忽略了时间序列间的相关性。本文探索了一种简单灵活的基于提示的方法，使LLMs能够在不进行大规模重新训练或不依赖复杂外部架构的情况下完成时间序列预测。通过利用时间序列分解、基于块的标记化以及基于相似性的邻居增强等专门提示方法，我们发现可以提升LLM的预测质量，同时保持模型的简洁性和对数据预处理的需求最小化。为此，我们提出了PatchInstruct方法，该方法使LLMs能够做出精确有效的预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 15:42:58 GMT</pubDate>
</item>
<item>
<title>PersonaFeedback：评估大型语言模型个性化能力的新基准</title>
<link>https://arxiv.org/abs/2506.12915</link>
<guid>https://arxiv.org/abs/2506.12915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准PersonaFeedback，用于评估LLMs根据用户画像生成个性化响应的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）通用能力的提升，如何构建能提供个性化服务的系统成为重要研究课题。然而，缺乏高质量的个性化评估基准阻碍了这一领域的发展。本文介绍PersonaFeedback，这是一个直接评估LLMs生成个性化响应能力的新基准，它通过显式用户画像进行测试，而非依赖隐式推断。该基准包含8298个人类注释的测试案例，分为易、中、难三个层级。实验结果显示，即使是最先进的LLMs在难度较高的测试中也表现不佳，甚至人类评估者也可能难以区分细微差异。此外，分析表明当前的检索增强框架并非个性化任务的默认解决方案。所有数据、注释协议和评估流程都将公开，以促进未来的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 13:19:19 GMT</pubDate>
</item>
<item>
<title>面向用户界面教学视频的多模态摘要新基准</title>
<link>https://arxiv.org/abs/2506.12623</link>
<guid>https://arxiv.org/abs/2506.12623</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出针对UI教学视频的多模态摘要新基准，填补现有数据集空白。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于教学视频的多模态摘要生成问题，旨在通过文本指令和关键帧为用户提供高效技能学习方式。然而，现有视频摘要数据集主要关注通用语义层面的视频总结，无法满足提供逐步可执行说明的需求。为此，我们构建了一个名为MS4UI的新数据集，包含2413个UI教学视频，总时长达167小时，并对视频进行分割、文本摘要及视频摘要的手动标注。实验表明，目前最先进的多模态摘要方法在UI教学视频摘要任务上表现不佳，强调了开发新方法的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12623" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 16:39:32 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的表示对齐及其跨语言控制方法</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型具有自然出现的表示对齐能力，并提出了一种新的跨语言控制方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现，以及这种对齐对分离语言特定信息和语言无关信息的意义。通过实证研究，我们验证了这种对齐的存在，并将其与显式设计的对齐模型进行比较分析，展示了其在不损害语义的情况下实现语言特定操作的潜力。基于这些发现，我们提出了推理时语言控制（ITLC），一种利用潜在注入的新方法，以实现精确的跨语言控制并减轻LLMs中的语言混淆问题。实验表明，ITLC在保持目标语言语义完整性的同时展现出强大的跨语言控制能力，并有效缓解了现有大规模LLMs中存在的跨语言语言混淆问题，从而导致语言生成的一致性不足。本研究加深了我们对LLMs表示对齐的理解，并为提高其跨语言性能提供了实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 07:09:50 GMT</pubDate>
</item>
<item>
<title>大型语言模型人格解读：基于Supernova Event Dataset的事件提取与基准测试</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过新数据集评估多种语言模型的人格特质及事件提取能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法，利用Supernova Event Dataset来分析大型语言模型（LLMs）的人格特质及其对文本中关键事件提取和排序的能力。该数据集涵盖了传记、历史事件、新闻报道及科学发现等多样化内容。研究对象包括小型模型（如Phi-4、Orca 2、Qwen 2.5）和强大的大型模型（如Claude 3.7、Gemini 2.5、OpenAI o3）。通过让另一款LLM作为裁判，根据模型对事件的选择和分类推断其人格特征。研究揭示了各模型的独特人格特质，例如Orca 2专注于人际动态的情感推理，而Qwen 2.5则表现出战略性分析风格。此外，在处理科学发现事件时，Claude Sonnet 3.7侧重概念框架构建，Gemini 2.5 Pro强调实证验证，o3倾向于逐步因果推理。这项工作提高了模型的可解释性，使其更适合广泛多样的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 15:31:52 GMT</pubDate>
</item>
<item>
<title>DeepResearch Bench：LLM驱动的研究代理能力评估基准</title>
<link>https://arxiv.org/abs/2506.11763</link>
<guid>https://arxiv.org/abs/2506.11763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepResearch Bench基准用于评估基于LLM的研究代理能力。</p><br /><br /><p><strong>摘要：</strong> 深度研究代理（DRAs）作为大型语言模型（LLMs）驱动的代理类别，能够通过多步骤网络探索、目标检索和高级合成将海量在线信息转化为高质量的研究报告。然而，目前缺乏系统评估这些代理能力的综合基准。为填补这一空白，我们发布了DeepResearch Bench，包含由22个领域专家设计的100项博士级研究任务。由于评估DRAs复杂且耗时，我们提出了两种新方法，一种是参考导向法，采用自适应标准评估生成报告的质量；另一种框架则通过有效引用数量和总体引用准确性来衡量信息检索能力。DeepResearch Bench及相关组件已开源，以推动实用LLM代理的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 09:17:32 GMT</pubDate>
</item>
<item>
<title>科学家首次考试（SFE）基准评测科学多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准SFE评估科学多模态大语言模型的认知能力，发现当前顶级模型表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 随着科学研究对复杂多模态推理的需求增加，科学多模态大语言模型（MLLMs）被寄予厚望，可以显著提升科学发现的效率。然而，现有的科学基准主要集中在评估MLLMs的知识理解能力，对其感知和推理能力的评估不足。为弥补这一缺陷，我们提出了科学家的首次考试（SFE）基准，通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLMs的科学认知能力。SFE包含830个由专家验证的视觉问答对，涵盖五个高价值学科中的66个多模态任务。实验结果显示，最先进的GPT-o3和InternVL-3模型在SFE上的得分分别为34.08%和26.52%，表明这些模型在科学领域还有很大的改进空间。我们希望SFE提供的见解能够促进人工智能辅助科学发现的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 05:29:16 GMT</pubDate>
</item>
<item>
<title>ALE-Bench：评估AI系统在算法工程中的表现</title>
<link>https://arxiv.org/abs/2506.09050</link>
<guid>https://arxiv.org/abs/2506.09050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准ALE-Bench用于评估AI在优化问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ALE-Bench的新基准，旨在评估人工智能系统在解决诸如包裹配送路径规划、机组排班、工厂生产计划及电网平衡等复杂优化问题时的表现。该基准基于AtCoder启发式竞赛的实际任务构建，包含计算上难以处理且无已知精确解的问题。不同于短期通过/失败的编码测试，ALE-Bench强调长时间跨度内的迭代式解决方案改进。研究采用的软件框架支持交互式代理架构，允许利用运行反馈和可视化工具。实验表明，尽管前沿的大语言模型在特定问题上表现出色，但与人类相比，在跨问题的一致性及长期问题解决能力方面仍存在显著差距。这一发现凸显了设立此类基准的重要性，以推动未来AI技术的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>NoWait：高效多模态推理的插件式解决方案</title>
<link>https://arxiv.org/abs/2506.08343</link>
<guid>https://arxiv.org/abs/2506.08343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出NoWait方法，通过抑制自省信号提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 近期大规模推理模型的进步虽实现了复杂分步推理，但常伴随冗长和重复输出的问题，降低了效率。本研究探讨显式自我反思是否对高级推理必要，并提出了名为NoWait的方法，在推理过程中抑制诸如“等一下”、“嗯”之类的自省标记。实验结果显示，NoWait在文本、视觉及视频推理的十个基准测试中，将五种R1系列模型的推理轨迹长度减少了27%-51%，且不影响模型实用性。这一简单有效的方法为多模态推理提供了高效的实用型解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 21:54:04 GMT</pubDate>
</item>
<item>
<title>BridgeVLA：一种高效的三维视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2506.07961</link>
<guid>https://arxiv.org/abs/2506.07961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合3D信号的新型VLA模型BridgeVLA，显著提升了机器人操作学习的效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，利用预训练的视觉-语言模型(VLMs)构建视觉-语言-动作(VLA)模型成为机器人操作学习的有效方法之一，但现有方法对3D信号的利用不足，导致样本效率低下。本文提出了BridgeVLA，通过将3D输入投影到多个2D图像上并与VLM骨干对齐，同时利用2D热图进行动作预测，统一了输入与输出空间。此外，还设计了一种可扩展的预训练方法，在下游策略学习前赋予VLM骨干预测2D热图的能力。实验表明，BridgeVLA在三个模拟基准测试中均优于当前最先进的基线方法，尤其在RLBench、COLOSSEUM和GemBench中表现优异。在真实机器人实验中，BridgeVLA不仅在多种分布外设置中表现出稳健的泛化能力，还实现了极高的样本效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:36:34 GMT</pubDate>
</item>
<item>
<title>From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding</title>
<link>https://arxiv.org/abs/2506.03968</link>
<guid>https://arxiv.org/abs/2506.03968</guid>
<content:encoded><![CDATA[
The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:00:47 GMT</pubDate>
</item>
<item>
<title>构建AI代理行为科学：从模型到行为的系统性研究</title>
<link>https://arxiv.org/abs/2506.06366</link>
<guid>https://arxiv.org/abs/2506.06366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AI代理行为科学，强调观察、干预与理论指导。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的进步推动了具备人类行为特征的AI代理发展，这些行为不仅依赖模型架构，还受具体情境中的环境、社会线索及交互反馈影响。本文提出AI代理行为科学这一新视角，聚焦于观察行为、设计实验验证假设及基于理论解释AI的行为、适应性和交互过程。通过整合个体代理、多代理及人机交互的研究成果，该视角被应用于提升AI的公平性、安全性、可解释性、责任性和隐私保护等特性。本研究统一现有发现并规划未来方向，将AI代理行为科学定位为传统模型驱动方法的重要补充，为评估和管理日益自主的AI系统提供必要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:12:32 GMT</pubDate>
</item>
<item>
<title>Avey：一种突破注意力与循环机制的新神经基础架构</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种不依赖注意力和循环的新神经架构Avey，显著提升长序列处理能力。</p><br /><br /><p><strong>摘要：</strong> Transformer虽在语言模型领域广泛应用，但受限于固定上下文窗口和二次复杂度问题。本文提出Avey架构，通过排名器与自回归处理器协作，仅关注相关token，实现任意长度序列的有效处理。实验表明，Avey在短距离NLP基准测试中表现优异，尤其擅长捕捉长距离依赖关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 17:11:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不确定场景下的拒绝回答能力评估</title>
<link>https://arxiv.org/abs/2506.09038</link>
<guid>https://arxiv.org/abs/2506.09038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，大型语言模型在面对不确定问题时的拒绝回答能力亟待提升。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在复杂问题求解上取得了显著进展，但在处理不确定或无法回答的问题时仍存在明显不足。本文介绍了一个名为AbstentionBench的新基准，用于评估20个多样化数据集中的拒绝回答能力，涵盖未知答案、表述不清、错误前提、主观解释及过时信息等问题。实验结果显示，即使是最新推理型LLMs在拒绝回答方面的表现也远不如预期，甚至在经过推理微调后，该能力反而下降了24%。尽管精心设计的系统提示可以在一定程度上改善这一状况，但并未解决模型根本无法有效处理不确定性的问题。本研究旨在通过发布AbstentionBench推动LLMs可靠性的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型对反馈的吸收能力研究</title>
<link>https://arxiv.org/abs/2506.11930</link>
<guid>https://arxiv.org/abs/2506.11930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现即使在理想条件下，LLMs对反馈的吸收仍存在阻力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在接收外部反馈后的改进能力，通过构建受控实验环境，让解题模型在接收接近完整的反馈后重新尝试问题解答。实验涵盖了数学推理、知识推理、科学推理以及多领域综合评估等任务，使用了如Claude 3.7等先进的语言模型。尽管条件理想，但这些模型普遍表现出对反馈的抵抗现象，即所谓的“反馈摩擦”。我们尝试通过采样策略改善这一情况，但效果有限。此外，排除了模型过度自信和数据熟悉度等因素作为主要原因。希望揭示这一局限性能为未来LLMs的自我提升研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:31:51 GMT</pubDate>
</item>
<item>
<title>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards</title>
<link>https://arxiv.org/abs/2506.11474</link>
<guid>https://arxiv.org/abs/2506.11474</guid>
<content:encoded><![CDATA[
Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 01:36:30 GMT</pubDate>
</item>
<item>
<title>通过学习继续思考令牌提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.11274</link>
<guid>https://arxiv.org/abs/2506.11274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索了通过学习专用继续思考令牌来增强语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 测试时扩展计算是一种有效提升语言模型性能的方法，通过在推理阶段利用额外计算资源实现。近期研究表明，覆盖结束思考标记（如将“”替换为“等待”）可以延长推理步骤并提高准确性。本研究尝试引入一个专门的继续思考标记，通过强化学习训练其嵌入，同时保持模型权重不变。实验表明，此方法在标准数学基准测试中优于基线模型及固定标记的测试时扩展方法。例如，在GSM8K基准测试中，固定标记方法提升了1.3%的准确率，而学习标记方法则实现了比不使用预算强迫的基线模型高4.2%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 16:28:54 GMT</pubDate>
</item>
<item>
<title>Mirage-1：基于分层多模态技能的跨平台GUI代理</title>
<link>https://arxiv.org/abs/2506.10387</link>
<guid>https://arxiv.org/abs/2506.10387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的跨平台GUI代理Mirage-1，显著提升了长周期任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型作为图形用户界面（GUI）代理在线上环境处理长周期任务时的知识不足问题，提出了一种分层多模态技能（HMS）模块，通过逐步抽象轨迹形成执行技能、核心技能及元技能，构建层次化知识结构以支持长期规划。同时，引入技能增强的蒙特卡洛树搜索（SA-MCTS）算法，利用离线环境中习得的技能缩减在线探索中的动作搜索空间，从而弥合域间差距。在此基础上开发的Mirage-1是一种多模态、跨平台的即插即用型GUI代理。为验证其性能，构建了新基准AndroidLH，实验结果显示，相比现有方法，Mirage-1在多个测试集上的表现分别提升了32%、19%、15%和79%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:21:19 GMT</pubDate>
</item>
<item>
<title>基于LoRA调优的掩码引导视频编辑方法</title>
<link>https://arxiv.org/abs/2506.10082</link>
<guid>https://arxiv.org/abs/2506.10082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于掩码的LoRA调优方法，实现灵活可控的视频编辑。</p><br /><br /><p><strong>摘要：</strong> 当前基于扩散模型的视频编辑方法多依赖大规模预训练，缺乏灵活性。为解决这一问题，本文提出了一种基于掩码的LoRA（低秩适应）调优方法，该方法通过适配预训练的图像到视频（I2V）模型，在保持背景不变的同时实现对特定区域的可控编辑传播。此外，引入参考图像作为视觉锚点，帮助模型更好地理解编辑意图。实验表明，该方法在性能上优于现有最先进方法，且无需改变模型架构，展现出高效性和适应性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 14:03:55 GMT</pubDate>
</item>
<item>
<title>基于生成-修剪-排名范式的程序验证效率与准确性权衡</title>
<link>https://arxiv.org/abs/2506.10056</link>
<guid>https://arxiv.org/abs/2506.10056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出生成-修剪-排名方法，显著提升程序验证速度同时保持较高准确性。</p><br /><br /><p><strong>摘要：</strong> 当前通过大型语言模型解决编程任务的标准范式是生成代码后进行排名，其中排名步骤依赖于验证器。普遍认为当全面验证器可用时，应优先采用而非结果奖励模型(ORM)，且较少考虑速度与准确性之间的权衡。本研究挑战这一假设，系统性探索两者之间的平衡，发现ORM在利用速度换取准确率方面至关重要，尤其是在生成-修剪-排名范式中。这种新方法比传统全面测试套件快11.65倍，仅降低8.33%的准确性。分析表明，该方法通过过滤掉高度排名但错误的解决方案实现高效验证，从而为设计可扩展且精确的程序排名系统提供了可能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>JAFAR：轻量且灵活的基础视觉编码器特征上采样方法</title>
<link>https://arxiv.org/abs/2506.11136</link>
<guid>https://arxiv.org/abs/2506.11136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为JAFAR的轻量级特征上采样方法，提升视觉特征的空间分辨率。</p><br /><br /><p><strong>摘要：</strong> Foundation Vision Encoders在密集视觉任务中至关重要，但其低分辨率特征输出限制了下游任务的表现。本文介绍了一种名为JAFAR的新方法，它通过基于注意力机制的模块，利用低级图像特征衍生的高分辨率查询和语义丰富的低分辨率键进行空间特征变换(SFT)调制，从而增强特征的空间分辨率至任意目标分辨率。尽管缺乏高分辨率监督，JAFAR在低上采样比和分辨率下学习的效果能够很好地推广到更高的输出尺度。实验表明，JAFAR在恢复细粒度空间细节方面表现出色，并在多种下游任务中显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 16:53:12 GMT</pubDate>
</item>
<item>
<title>基于自我意识弱点驱动的问题合成框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.08989</link>
<guid>https://arxiv.org/abs/2506.08989</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我意识弱点驱动问题合成框架，提高大语言模型在复杂推理任务上的性能。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励机制（RLVR）对训练大型语言模型（LLMs）解决复杂推理任务如数学问题非常有效，但现有数据集的人类标注数学问题质量和答案精确性不足限制了其效果。大多数问题合成策略不考虑模型能力，导致效率低下。为解决这些问题，我们提出了自我意识弱点驱动问题合成框架（SwS），通过系统识别模型的薄弱环节并针对性生成新问题，显著提升了模型在主流推理基准测试中的平均性能，7B和32B模型分别提高了10.0%和7.7%，且无需外部知识蒸馏。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08989" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>Infinity-Instruct：提升大语言模型基础与对话能力的新基准数据集</title>
<link>https://arxiv.org/abs/2506.11116</link>
<guid>https://arxiv.org/abs/2506.11116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Infinity-Instruct数据集，增强开源模型的基础与指令跟随能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Infinity-Instruct的高质量指令数据集，旨在通过两阶段管道增强大型语言模型（LLMs）的基础和聊天能力。第一阶段，从超过1亿个样本中筛选出740万个高质量基础指令（InfInstruct-F-7.4M）。第二阶段，通过多阶段过程合成150万个高质量对话指令（InfInstruct-G-1.5M）。通过微调多个开源模型（如Mistral、LLaMA、Qwen和Yi），实验结果显示Infinity-Instruct显著提升了模型在基础和指令跟随任务上的表现，甚至在某些任务上超过了官方指令优化的模型。特别是，基于LLaMA的InfInstruct版本在指令跟随任务中的表现比GPT-4高出8.6%，同时在基础任务上表现相当。这些结果证明了基础训练和对话训练之间的协同效应，并为全面开发LLMs提供了新视角。相关数据集和代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:37:15 GMT</pubDate>
</item>
<item>
<title>基于候选标注的大语言模型数据标注方法</title>
<link>https://arxiv.org/abs/2506.03857</link>
<guid>https://arxiv.org/abs/2506.03857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大语言模型候选标注的新方法以提升数据质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大语言模型（LLMs）在数据标注中的局限性展开研究，这些方法通常通过单一标签策略进行标注，但难以应对复杂样本导致的错误标注问题。受人类规避不确定性行为的启发，我们提出了一种新的候选标注范式，即当模型存在不确定性时，鼓励其输出所有可能的标签。为适应下游任务需求，我们设计了一个教师-学生框架CanDist，利用小语言模型（SLM）对候选标注进行蒸馏。理论分析表明，这种候选标注的蒸馏方法优于直接采用单一标注。实验结果在六个文本分类任务中验证了该方法的有效性，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 07:42:37 GMT</pubDate>
</item>
<item>
<title>多维线性循环神经网络在长距离依赖任务中的表现</title>
<link>https://arxiv.org/abs/2506.11997</link>
<guid>https://arxiv.org/abs/2506.11997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩展线性RNN至多维结构，提出pLSTM模型，适用于复杂图结构数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的多维线性循环神经网络（pLSTM），通过引入Source、Transition和Mark门，使其能够处理一般有向无环图（DAG）上的数据。pLSTM不仅能够并行化操作，还解决了长距离依赖问题，采用定向传播模式和扩散分布模式。实验表明，pLSTM在合成箭头指向外推任务及分子图和计算机视觉基准测试中表现出色，尤其在处理更大图像时优于Transformer模型。代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:51:37 GMT</pubDate>
</item>
<item>
<title>大型语言模型在编程竞赛中的表现评估</title>
<link>https://arxiv.org/abs/2506.11928</link>
<guid>https://arxiv.org/abs/2506.11928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型在编程竞赛中仍显著落后于人类专家。</p><br /><br /><p><strong>摘要：</strong> 近期报道声称大型语言模型（LLMs）在编程竞赛中已超越顶尖人类选手。然而，本研究通过分析国际算法竞赛奖牌得主的经验，重新审视这一说法，探讨LLMs与人类专家的区别及现有局限。我们引入LiveCodeBench Pro基准测试，涵盖来自Codeforces、ICPC和IOI的问题，并由金牌得主对问题进行分类和错误代码分析。结果显示，即使是最前沿的模型，在中等难度问题上的pass@1仅为53%，而在难题上则为0%，远不及人类专家。尽管LLMs在实现类问题上表现良好，但在复杂算法推理和案例分析方面存在明显不足，且常给出错误的自信解释。高绩效主要依赖于代码精确性而非更强的推理能力。因此，LiveCodeBench Pro不仅揭示了LLMs与人类大师水平之间的显著差距，还提供了详细的诊断工具，以指导未来代码型LLM推理能力的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:29:09 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的对齐新视角图像与几何生成框架</title>
<link>https://arxiv.org/abs/2506.11924</link>
<guid>https://arxiv.org/abs/2506.11924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过扭曲与填补方法生成对齐的新视角图像与几何的扩散模型框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于扩散模型的框架，该框架通过扭曲与填补的方法实现对齐的新视角图像和几何生成。不同于以往需要密集姿态图像或受限于域内视图的姿态嵌入生成模型的方法，我们的方法利用现成的几何预测器预测参考图像所看到的部分几何形状，并将新视角合成表述为图像和几何的填补任务。为了确保生成图像和几何之间的精确对齐，我们提出了跨模态注意力蒸馏，在训练和推理过程中将图像扩散分支的注意力图注入到并行的几何扩散分支中。这种多任务方法实现了协同效应，促进了具有几何鲁棒性的图像合成以及定义良好的几何预测。此外，我们引入了基于邻近性的网格条件化，以整合深度和法线线索，插值点云并过滤掉错误预测的几何形状对生成过程的影响。实证研究表明，我们的方法在多种未见场景下实现了高保真的外推视角合成，无论是图像还是几何，并且在插值设置下提供了竞争性的重建质量，还生成了几何对齐的彩色点云以实现全面的3D补全。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:19:00 GMT</pubDate>
</item>
<item>
<title>FourierAttention：一种高效的大语言模型长上下文处理方法</title>
<link>https://arxiv.org/abs/2506.11886</link>
<guid>https://arxiv.org/abs/2506.11886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的FourierAttention框架，提升大语言模型长上下文处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着上下文长度的增长，大语言模型面临日益增加的记忆需求，现有压缩方法往往牺牲精度或引入计算开销。本文提出FourierAttention，这是一种无需训练的框架，通过利用Transformer头维度的异构角色，在较低维度优先处理局部上下文的同时，较高维度捕捉远距离依赖。通过将对长上下文不敏感的维度投影到正交傅里叶基上，FourierAttention用固定长度的频谱系数近似其时间演化过程。实验表明，FourierAttention在LongBench和Needle-In-A-Haystack数据集上的长上下文准确性达到最佳。此外，还设计了一个定制的Triton内核FlashFourierAttention，优化内存使用并通过流线型读写操作实现高效部署，性能不受影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 11:35:54 GMT</pubDate>
</item>
<item>
<title>Configurable Preference Tuning (CPT): 动态调整AI行为的新框架</title>
<link>https://arxiv.org/abs/2506.11702</link>
<guid>https://arxiv.org/abs/2506.11702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入CPT框架，使语言模型可根据人类指令动态调整行为。</p><br /><br /><p><strong>摘要：</strong> 现有的AI对齐模型通常基于固定的单一偏好集，限制了适应性。本文提出Configurable Preference Tuning (CPT)，一种新颖的框架，允许语言模型根据显式的人类可解释指令动态调整行为。CPT通过利用基于结构化细粒度量表生成的合成偏好数据，这些量表定义了如写作风格等期望属性，从而实现这一目标。通过在这些量表引导的偏好上微调，LLM能够在推理时响应系统提示调整输出，而无需重新训练。这种方法不仅提供了细粒度控制，还为建模更细腻和情境依赖的人类反馈提供了机制。研究中的多个实验资源，包括训练代码、生成的数据集和微调模型，已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 08:17:38 GMT</pubDate>
</item>
<item>
<title>Duo：通过高斯扩散改进离散扩散模型以提升文本生成性能</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Duo通过引入课程学习和一致性蒸馏技术，显著提升了离散扩散模型的训练效率和采样速度。</p><br /><br /><p><strong>摘要：</strong> 离散均匀状态扩散模型因其自我校正能力而具有快速文本生成的潜力，但通常表现不如自回归模型和掩码扩散模型。本研究通过利用高斯扩散过程的洞察，提出了一种名为Duo的新方法，通过课程学习策略和离散一致性蒸馏技术，大幅提高了训练速度和采样效率。实验结果显示，采用课程学习后，Duo在零样本困惑度上超越了自回归模型，在7个基准中的3个上表现优异；而离散一致性蒸馏则使扩散语言模型的采样速度加快了两个数量级。这项工作有效缩小了离散扩散模型与更先进模型之间的性能差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:55:35 GMT</pubDate>
</item>
<item>
<title>通过视觉描述幻觉批评提升视觉语言模型的感知能力</title>
<link>https://arxiv.org/abs/2506.10128</link>
<guid>https://arxiv.org/abs/2506.10128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ViCrit任务，通过检测图像描述中的细微错误提升视觉语言模型的感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViCrit（视觉描述幻觉批评）的强化学习代理任务，旨在提升视觉语言模型（VLMs）在视觉感知方面的表现。ViCrit任务通过在人类编写的图像描述中注入微妙的合成视觉幻觉错误，训练模型定位这些错误。此方法不仅保留了视觉感知的全部难度，还提供了易于计算且明确的二元奖励机制。实验表明，使用ViCrit任务训练的模型在多种视觉语言基准测试中取得了显著改进，这些改进不仅限于自然图像数据，还能推广到抽象图像推理和视觉数学等领域。此外，为了促进评估，我们还推出了ViCrit-Bench，这是一个类别平衡的诊断基准，系统性地探测了不同图像领域和错误类型的感知错误。总体而言，我们的研究证明了细粒度幻觉批评是一种有效且可泛化的视觉感知增强目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 15:16:54 GMT</pubDate>
</item>
<item>
<title>对抗性用户对政策合规AI代理的威胁及防御策略研究</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究政策合规AI代理在客户服务中的安全性和鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 随着任务导向型基于大型语言模型（LLM）的智能体在受政策约束领域的应用增加，如退款资格或取消规则等，如何保证这些智能体始终遵守相关政策并拒绝违规请求成为一大挑战。本文提出了一种新的威胁模型，关注试图利用政策合规智能体谋取个人利益的恶意用户。为解决这一问题，我们开发了CRAFT，一个多智能体红队系统，通过采用基于政策感知的说服策略，在客户服务质量评估场景下有效削弱政策合规智能体的表现，超越了传统的越狱方法。此外，基于现有的tau-bench基准，我们引入了tau-break，一个专门用于评估智能体对操纵性用户行为的稳健性的补充基准。最后，我们测试了几种简单但有效的防御措施，虽然提供了一定程度的保护，但仍需更强有力的研究驱动的安全保障来抵御潜在的攻击。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 06:59:47 GMT</pubDate>
</item>
<item>
<title>InterSyn：基于自评估迭代精炼的大规模多模态数据集</title>
<link>https://arxiv.org/abs/2506.09427</link>
<guid>https://arxiv.org/abs/2506.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InterSyn数据集和SynJudge评估模型，提升多模态模型生成能力。</p><br /><br /><p><strong>摘要：</strong> 近年来大型多模态模型（LMMs）在多模态理解和生成方面取得显著进展，但生成紧密交织的图文输出仍具挑战性，主要受限于现有训练数据集的规模、质量和指导丰富度。为此，我们引入InterSyn数据集，利用自评估与迭代精炼（SEIR）方法构建，提供多轮指令驱动的图文对话，具备丰富的对象多样性和严格的自动化质量优化，适合作为下一代指令跟随型LMMs的训练数据。同时，针对缺乏可靠评估工具的问题，我们开发了SynJudge自动评估模型，从文本内容、图像内容、图像质量和图文协同四个维度量化评估多模态输出。实验表明，SEIR方法显著提高了数据集质量，而基于InterSyn训练的LMMs在所有评估指标上均表现更优，验证了InterSyn在推动多模态系统发展中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 02:21:20 GMT</pubDate>
</item>
<item>
<title>SkillBlender：一种用于人形机器人灵活操控的分层强化学习框架</title>
<link>https://arxiv.org/abs/2506.09366</link>
<guid>https://arxiv.org/abs/2506.09366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的分层强化学习框架SkillBlender，实现人形机器人的多样化操控。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkillBlender的新框架，旨在解决现有方法在处理人形机器人多任务适应性时存在的问题。通过预训练无任务特定条件的基础技能并动态混合这些技能，SkillBlender能够在最小化任务特定奖励工程的情况下完成复杂的操控任务。同时，我们还开发了一个名为SkillBench的模拟基准，包含三种机器人形态、四种基础技能及八个挑战性任务，用于科学评估。实验表明，SkillBlender显著优于其他基线模型，在准确性与可行性方面表现出色。未来，我们的代码和基准将开源供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:24:26 GMT</pubDate>
</item>
<item>
<title>基于自精炼框架的无标注数据增强ASR性能研究</title>
<link>https://arxiv.org/abs/2506.11130</link>
<guid>https://arxiv.org/abs/2506.11130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用无标注数据提升语音识别性能的自精炼框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为自精炼框架的方法，通过仅使用无标注数据即可提升自动语音识别（ASR）系统的性能。该框架首先利用现有的ASR模型对无标注语音生成伪标签，然后使用这些伪标签训练高保真的文本到语音（TTS）系统。接着，将合成的语音文本对反馈至原始ASR系统，完成闭合循环的自我改进过程。此方法在台湾华语语音上进行了验证，在6000小时无标注语音、少量文本数据及AI模型生成的合成内容的支持下，成功将Whisper-large-v2模型转化为专门化的Twister模型。结果显示，Twister在普通话基准测试中的错误率降低了20%，而在普通话-英语混合代码转换基准测试中的错误率降低了50%。这一框架不仅优于传统的伪标签自蒸馏方法，还为低资源或特定领域ASR性能提升提供了实际可行的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:30:32 GMT</pubDate>
</item>
<item>
<title>基于二值注意力掩码的图像预测方法</title>
<link>https://arxiv.org/abs/2506.08915</link>
<guid>https://arxiv.org/abs/2506.08915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用二值注意力掩码提升图像预测鲁棒性的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于注意力机制的方法，通过学习得到的二值注意力掩码，确保只有被关注的图像区域影响最终预测结果。这种方法旨在解决对象感知中的上下文干扰问题，尤其是在物体出现在分布外背景时可能导致的偏差表示。为了应对这一挑战，我们提出了一个双阶段框架：第一阶段处理完整图像以发现对象部分并确定任务相关的区域；第二阶段利用输入注意力掩码限制其感受野到这些区域，从而实现聚焦分析同时过滤掉潜在的虚假信息。两个阶段联合训练，使第二阶段能够优化第一阶段的表现。大量实验表明，该方法显著提高了对虚假相关性和分布外背景的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:41:22 GMT</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[
This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 05:00:33 GMT</pubDate>
</item>
<item>
<title>U-CoT+: 一种高效灵活的有害模因检测框架</title>
<link>https://arxiv.org/abs/2506.08477</link>
<guid>https://arxiv.org/abs/2506.08477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架U-CoT+，通过文本描述和指导优化有害模因检测效率与灵活性。</p><br /><br /><p><strong>摘要：</strong> 有害模因的检测对维护在线环境的完整性至关重要，但现有方法在资源效率、灵活性和可解释性方面存在局限性。本文介绍了一种名为U-CoT+的新框架，该框架通过开发高保真的模因到文本转换管道，将视觉模因转化为细节保留的文本描述，从而实现对复杂原始视觉内容的高效分解与分类。此设计解耦了模因解释与分类过程，使基于通用大型语言模型的有害模因检测更加高效。进一步结合针对性的人类指导准则，在零样本思维链提示下引导模型推理，增强了框架的适应性和可解释性。实验验证了该框架的有效性，表明其在低资源条件下利用小型语言模型进行有害模因检测具有广阔潜力。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:10:45 GMT</pubDate>
</item>
<item>
<title>基于Reg-GRPO的视频大型语言模型增强视频推理能力的研究</title>
<link>https://arxiv.org/abs/2506.07464</link>
<guid>https://arxiv.org/abs/2506.07464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种改进的强化学习算法Reg-GRPO，显著提升视频大型语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，基于强化学习的后训练方法可以有效增强大型语言模型的推理能力，其中Group Relative Policy Optimization (GRPO)表现出色。然而，GRPO在视频大型语言模型（Video LLMs）中的应用尚未深入探索。本文研究了GRPO应用于Video LLMs时存在的两个主要问题：对安全措施的依赖及优势消失的问题。为解决这些问题，我们提出了DeepVideo-R1，这是一种结合了Reg-GRPO（回归形式的GRPO）和难度感知数据增强策略的视频大型语言模型。Reg-GRPO将GRPO目标重新定义为回归任务，直接预测优势值，从而消除了对安全措施如裁剪和最小函数的需求，使模型政策指导更加直接。此外，我们设计了一种难度感知的数据增强策略，在可解难度级别动态扩充训练样本，产生多样化且信息丰富的奖励信号。实验表明，DeepVideo-R1在多个视频推理基准测试中显著提升了视频推理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:15:54 GMT</pubDate>
</item>
<item>
<title>HeadHunter: 针对扩散模型注意力扰动的细粒度控制方法</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HeadHunter框架，实现扩散模型生成质量的精细调控。</p><br /><br /><p><strong>摘要：</strong> 现有扩散模型中的注意力扰动方法在确定扰动位置时缺乏系统性，尤其是在Diffusion Transformer架构中，质量相关的计算分布在多层中。本文研究了注意力扰动的不同粒度，发现特定的注意力头控制着不同的视觉概念，如结构、风格和纹理质量。基于此，我们提出了HeadHunter框架，用于迭代选择与用户目标对齐的注意力头，从而实现生成质量与视觉属性的精细控制。此外，我们引入SoftPAG，通过线性插值注意力图向单位矩阵靠近，提供连续调节扰动强度的旋钮以抑制伪影。实验验证表明，该方法不仅缓解了现有层级扰动导致的过度平滑问题，还实现了特定视觉风格的目标操控。本研究首次对扩散模型中的注意力扰动进行了头级别分析，揭示了注意力层内的可解释专业化，并为设计有效的扰动策略提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>AutoMind：一种适应性强的知识驱动型大语言模型代理框架</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoMind提升自动化数据科学能力，优于现有最先进方法。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）代理在解决实际数据科学问题方面展现出巨大潜力，但现有框架受限于固定工作流与编码策略，难以处理复杂创新任务。本文提出AutoMind框架，通过构建领域专家知识库、采用知识驱动树搜索算法及动态自适应编码策略，克服现有缺陷。评估显示，AutoMind在两个自动化数据科学基准测试中表现优异，效率、效果及解的质量均优于当前最佳基线，标志着迈向全自动数据科学的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>SWE-Factory：自动化构建大规模GitHub问题解决数据集</title>
<link>https://arxiv.org/abs/2506.10954</link>
<guid>https://arxiv.org/abs/2506.10954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-Factory自动化流水线，显著提升GitHub问题解决数据集构建效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模GitHub问题解决数据集构建的传统挑战，提出名为SWE-Factory的自动化流水线。该流水线通过SWE-Builder多智能体系统实现评估环境的自动构建，采用基于退出码的标准化评分方法取代自定义解析器，并通过可靠退出码信号实现失败转成功的自动化验证过程。实验表明，在四种编程语言的671个问题上，SWE-Builder以最低成本构造有效实例，且基于退出码的评分方法达到100%准确性，自动验证也表现出高精度与完全召回率。我们希望此流水线能加速高质量数据集的收集，用于训练和评估大型语言模型的软件工程能力。相关代码与数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:54:17 GMT</pubDate>
</item>
<item>
<title>迈向自主网络代理的新交互范式：Agentic Web Interface 的提出</title>
<link>https://arxiv.org/abs/2506.10953</link>
<guid>https://arxiv.org/abs/2506.10953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种针对大型语言模型优化的网络界面设计，解决现有方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）及其多模态变体的发展引发了对网络代理研究的浓厚兴趣。网络代理是一种能够在网页环境中自主导航并完成任务的人工智能系统，然而当前方法因人类设计的界面与LLMs能力之间的根本不匹配而面临诸多挑战。这些问题体现在处理复杂的DOM树、依赖截图附加信息或绕过用户界面的API交互上。本文主张网络代理研究需要转变范式，而非让代理适应人为设计的界面，而是开发专门针对代理能力优化的新型交互方式。为此，我们提出了Agentic Web Interface（AWI）的概念，这是一种专为代理设计的网络界面，并制定了六项指导原则，强调安全、效率和标准化，以平衡各利益相关方的需求。这种重新定义旨在克服现有界面的根本限制，推动更高效、可靠且透明的网络代理设计，这一过程需由整个机器学习社区共同参与。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:58 GMT</pubDate>
</item>
<item>
<title>Domain2Vec：一种高效的数据集分解方法</title>
<link>https://arxiv.org/abs/2506.10952</link>
<guid>https://arxiv.org/abs/2506.10952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Domain2Vec通过分解数据集为元域向量优化语言模型预训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Domain2Vec的新方法，该方法将任意数据集分解为若干元域的线性组合。元域是一个新概念，用于捕捉数据集的关键特征。Domain2Vec通过分类器将数据集转化为对应的元域分布向量，从而无需训练即可确定最优数据混合方案。此方法基于分布对齐假设，表明当训练集和验证集的数据分布更匹配时，验证损失会更低。此外，Domain2Vec可以无缝集成到现有工作中，提升模型效率和可扩展性。实验表明，使用Domain2Vec进行预训练时，只需原计算资源的51.5%，即可达到相同的验证损失，同时在同等计算预算下，下游任务性能平均提高2.83%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:51 GMT</pubDate>
</item>
<item>
<title>基于半非负矩阵分解的大型语言模型可解释性特征提取</title>
<link>https://arxiv.org/abs/2506.10920</link>
<guid>https://arxiv.org/abs/2506.10920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过半非负矩阵分解直接分解MLP激活，提高因果导向能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中的机制可解释性问题，重点在于找到一种无监督方式来识别具有因果解释力的特征方向。传统方法依赖稀疏自动编码器（SAEs），但其在因果评估中表现不佳且缺乏内在可解释性。为此，我们引入半非负矩阵分解（SNMF）技术，将MLP激活直接分解为稀疏线性组合的神经元特征，并映射到激活输入上，从而提升特征的直观性和可解释性。实验表明，基于SNMF的方法在因果导向性能上优于SAEs及强监督基线，在Llama 3.1、Gemma 2和GPT-2等模型上表现出色。进一步分析揭示了神经元组合在语义相关特征间被复用的现象，展示了MLP激活空间中的层次结构。这些结果表明，SNMF是一种简单而有效的工具，用于识别LLMs中的可解释特征并解析概念表示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:33:29 GMT</pubDate>
</item>
<item>
<title>NoLoCo：一种无需显式同步的高效大规模语言模型训练方法</title>
<link>https://arxiv.org/abs/2506.10911</link>
<guid>https://arxiv.org/abs/2506.10911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需显式参数同步的优化方法NoLoCo，显著降低通信开销并提高收敛速度。</p><br /><br /><p><strong>摘要：</strong> 当前大规模语言模型的训练通常依赖于高度互联的计算集群，但扩展此类集群成本高昂且不切实际。近期研究提出了减少通信密集度的训练方法，但仍需模型参数的同步步骤，这在低带宽网络上可能变得昂贵。本文提出了一种名为NoLoCo的新型优化方法，该方法通过Nesterov动量优化器的变体隐式同步模型权重，无需显式同步所有模型参数，也无需集体通信。理论分析和实验结果表明，NoLoCo在各种加速器数量和模型规模（125M到6.8B参数）下表现出色，相比完全分片的数据并行训练和低通信训练方法DiLoCo，其通信开销显著减少。此外，在数百个互联网加速器上进行的同步步骤估计比DiLoCo中的all-reduce快一个数量级，且不存在全局阻塞通信，从而减少了加速器的空闲时间。实验还显示，NoLoCo对各种模型大小和加速器数量的收敛速度比DiLoCo快高达4%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:23:23 GMT</pubDate>
</item>
<item>
<title>Magistral：基于纯强化学习训练的语言模型</title>
<link>https://arxiv.org/abs/2506.10910</link>
<guid>https://arxiv.org/abs/2506.10910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magistral展示了纯强化学习训练大型语言模型的可能性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mistral的第一款推理模型Magistral及其自主研发的可扩展强化学习（RL）管道。不同于依赖现有实现或从先前模型中蒸馏出的RL跟踪，我们采取了自下而上的方法，完全依赖自身模型和基础设施。研究展示了纯强化学习训练大型语言模型的潜力，提出了一种强制模型推理语言的简单方法，并证明仅使用文本数据进行强化学习可以保持初始检查点的能力。此外，我们展示了仅使用文本的强化学习可以维持或改善多模态理解、指令跟随和功能调用能力。我们发布了Magistral Medium，它是基于Mistral Medium 3通过纯强化学习训练的模型，并开源了Magistral Small（采用Apache 2.0许可证），它还包括了来自Magistral Medium的冷启动数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:22:37 GMT</pubDate>
</item>
<item>
<title>CreatiPoster：一种支持多层可编辑图形设计的AI框架</title>
<link>https://arxiv.org/abs/2506.10890</link>
<guid>https://arxiv.org/abs/2506.10890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CreatiPoster利用自然语言指令生成高质量、可编辑的图形设计。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CreatiPoster的新框架，它可以从自然语言指令或用户提供的资产中生成可编辑的多层次图形作品。该框架通过协议模型生成包含每层布局、层级、内容和风格的JSON规范，并结合背景提示生成协调的背景。实验表明，CreatiPoster在生成图形设计方面超越了现有的开源方法和商业系统。此外，研究团队发布了一个包含100,000个多层设计的无版权数据集，以促进进一步的研究。CreatiPoster支持多种应用场景，如画布编辑、文本覆盖、响应式缩放、多语言适应和动态海报制作，推动了AI辅助图形设计的普及。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:54:39 GMT</pubDate>
</item>
<item>
<title>VRBench：首个长叙事视频基准用于评估大模型多步推理能力</title>
<link>https://arxiv.org/abs/2506.10857</link>
<guid>https://arxiv.org/abs/2506.10857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VRBench，首个长叙事视频基准，用于评估大型模型的多步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VRBench，这是一个专为评估大型模型多步推理能力而设计的首个长叙事视频基准。现有评估方法忽视了时间推理和程序有效性，而VRBench通过包含1010段长视频（平均时长1.6小时）、9468个人类标注的多步问答对以及30292个带有时间戳的推理步骤，解决了这些问题。这些视频经过多阶段过滤过程筛选，确保情节连贯性。我们开发了一种人机协作框架，生成连贯的推理链，每条链需要多个基于时间的步骤，涵盖七种类型（如事件归因、隐式推理）。VRBench还设计了一个多阶段评估管道，在结果和过程层面评估模型。除了多项选择题外，我们还提出了LLM引导的进度级评分指标，从多个维度全面评估推理链质量。通过对12个LLM和16个VLM在VRBench上的广泛评估，我们进行了深入分析并提供了有价值的见解，推动了多步推理领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:17:17 GMT</pubDate>
</item>
<item>
<title>基于文本推理模型的长视频理解框架</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过仅依赖文本推理模型实现长视频理解任务。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型在长视频理解（LVU）任务上面临复杂性和上下文窗口限制的挑战。传统观点认为需要扩展上下文窗口、增强视觉感知能力和领域专业知识的基础模型。本研究提出VideoDeepResearch框架，仅利用现有实践中的文本-only大推理模型及多模态工具包（如多模态检索器和视觉感知器），通过推理制定问题解决策略并选择性访问视频内容。实验表明，在MLVU、Video-MME和LVBench等基准测试中，该方法显著优于现有基础模型，分别提升了9.6%、6.6%和3.9%，证明了基于智能体系统的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 11:39:10 GMT</pubDate>
</item>
<item>
<title>PosterCraft：一种统一框架用于高审美海报生成</title>
<link>https://arxiv.org/abs/2506.10741</link>
<guid>https://arxiv.org/abs/2506.10741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架PosterCraft，显著提升海报生成的质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PosterCraft的新框架，旨在解决高审美海报生成中的挑战。与传统的模块化流水线不同，PosterCraft允许模型自由探索连贯且视觉吸引人的布局。该框架通过四个优化阶段实现高质量海报生成：大规模文本渲染优化、区域感知监督微调、美学文本强化学习及联合视觉语言反馈优化。这些阶段由专门设计的数据构建管道支持，无需复杂的架构修改即可进行稳健训练。实验表明，PosterCraft在文本渲染准确性、布局一致性及整体视觉吸引力方面优于开源基线，接近顶级商业系统的质量。相关代码、模型和数据集可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:28:12 GMT</pubDate>
</item>
<item>
<title>TaxoAdapt：动态适配的科学文献自动分类框架</title>
<link>https://arxiv.org/abs/2506.10737</link>
<guid>https://arxiv.org/abs/2506.10737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TaxoAdapt框架，提升科学文献分类的精度与适应性。</p><br /><br /><p><strong>摘要：</strong> 传统专家构建的科学文献分类体系耗时且成本高昂，而现有的自动方法存在特定语料库依赖性强或忽视领域动态变化的问题。为此，本文提出TaxoAdapt框架，通过迭代分层分类，在多个维度上扩展分类宽度和深度，以适应特定语料库的专题分布。实验表明，TaxoAdapt在多类计算机科学会议文献中表现优异，其生成的分类系统比现有最佳基线方法在粒度保存率和一致性上分别高出26.51%和50.41%，有效捕捉了科学领域的演变特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:26:28 GMT</pubDate>
</item>
<item>
<title>ClaimSpect：基于检索增强生成框架的复杂主张分解与视角表示</title>
<link>https://arxiv.org/abs/2506.10728</link>
<guid>https://arxiv.org/abs/2506.10728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ClaimSpect，用于自动构建主张的层级结构并分析其多维视角。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了科学和政治主张中复杂的表述通常难以简单归类为“真”或“假”，但可以通过拆解成子方面（如有效性、安全性）进行验证。为此，我们提出了ClaimSpect框架，它利用检索增强生成技术，将主张分解为其组成成分和子成分，并从特定语料库中提取相关段落来丰富这些方面。该框架不仅能够发现新的子方面，还能揭示针对某一主张方面的不同立场（支持、中立或反对）及其流行程度。通过应用到科学和政治主张的真实案例，并结合人工评估，证明了ClaimSpect在解析复杂主张和表示语料库观点方面的有效性和准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:17:45 GMT</pubDate>
</item>
<item>
<title>TeleMath：首个电信领域数学问题评估基准</title>
<link>https://arxiv.org/abs/2506.10674</link>
<guid>https://arxiv.org/abs/2506.10674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出TeleMath基准数据集，用于评估大语言模型在电信数学问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能在电信领域的广泛应用，对大语言模型（LLMs）处理特定领域数学密集型任务的能力产生了兴趣。尽管LLMs在一般数学推理方面取得了进展，但在信号处理、网络优化等专门领域的表现尚不明确。为此，我们引入TeleMath，这是一个针对电信领域数学问题设计的首个基准数据集，包含500组问答对。通过专家设计的问题种子，我们构建了该数据集并评估了多种开源LLMs的表现，发现专注于数学推理的模型表现最优，而通用模型则表现不佳。本研究还公开了数据集和评估代码，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 09:04:18 GMT</pubDate>
</item>
<item>
<title>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2506.10600</link>
<guid>https://arxiv.org/abs/2506.10600</guid>
<content:encoded><![CDATA[
Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:43:50 GMT</pubDate>
</item>
<item>
<title>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</title>
<link>https://arxiv.org/abs/2506.10568</link>
<guid>https://arxiv.org/abs/2506.10568</guid>
<content:encoded><![CDATA[
In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:58:23 GMT</pubDate>
</item>
<item>
<title>AniMaker：基于多智能体框架的文本驱动故事动画生成</title>
<link>https://arxiv.org/abs/2506.10540</link>
<guid>https://arxiv.org/abs/2506.10540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AniMaker框架，解决多场景故事动画生成中的叙事连贯性和稳定性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成模型在多场景叙事动画生成中的不足，提出了名为AniMaker的多智能体框架。该框架通过导演代理、摄影代理、审查代理及后期制作代理的协同工作，实现了从文本输入到全局一致且叙事连贯的动画生成。其中，摄影代理采用蒙特卡洛树搜索启发策略（MCTS-Gen），高效筛选高质量候选片段；审查代理则引入首个多镜头动画评估框架（AniEval），综合评估叙事一致性、动作完整性等特性。实验表明，AniMaker在多个指标上超越现有方法，显著提升生成效率，使AI生成的故事动画接近生产标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:06:21 GMT</pubDate>
</item>
<item>
<title>基于因果表示学习的语言模型能力评估框架</title>
<link>https://arxiv.org/abs/2506.10378</link>
<guid>https://arxiv.org/abs/2506.10378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种因果表示学习框架，用于有效评估语言模型的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过因果表示学习解决语言模型评估中的复杂混杂效应及高昂计算成本问题。研究利用线性变换将基准性能建模为潜在能力因素的函数，并控制基础模型作为共同混杂因子，确定这些因素间的因果关系。通过对涵盖1500多个模型的数据集进行分析，揭示了一种简洁的三节点线性因果结构，解释了性能变化。进一步解读显示了从一般问题解决能力到指令跟随能力再到数学推理能力的因果路径。结果强调了在评估中控制基础模型变异的重要性，这是准确揭示潜在模型能力间因果关系的关键步骤。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:07:42 GMT</pubDate>
</item>
<item>
<title>Optimus-3：面向Minecraft环境的多模态通用智能体</title>
<link>https://arxiv.org/abs/2506.10357</link>
<guid>https://arxiv.org/abs/2506.10357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于多模态大语言模型的通用智能体Optimus-3。</p><br /><br /><p><strong>摘要：</strong> 本文针对开放世界环境中构建具备感知、规划、行动、定位及反思能力的通用型智能体所面临的挑战，如领域特定数据不足、异构任务干扰及视觉多样性等问题，提出了三项创新解决方案。首先，设计了一种知识增强的数据生成管道，以提供可扩展且高质量的训练数据；其次，引入任务级路由的专家混合架构（MoE），减少异构任务间的干扰；最后，开发了多模态推理增强的强化学习方法，提升智能体对视觉多样性的处理能力。基于这些改进，我们推出了Optimus-3，一款针对Minecraft环境的通用智能体。实验结果表明，Optimus-3在多个任务上超越了现有的多模态大语言模型及智能体基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 01:29:40 GMT</pubDate>
</item>
<item>
<title>Discrete Audio Tokens: More Than a Survey!</title>
<link>https://arxiv.org/abs/2506.10274</link>
<guid>https://arxiv.org/abs/2506.10274</guid>
<content:encoded><![CDATA[
Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 21:35:43 GMT</pubDate>
</item>
<item>
<title>高效探针方法在自监督学习中的性能提升研究</title>
<link>https://arxiv.org/abs/2506.10178</link>
<guid>https://arxiv.org/abs/2506.10178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多查询交叉注意力机制，在多个基准测试中超越传统线性及现有注意力探针方法。</p><br /><br /><p><strong>摘要：</strong> 随着大规模微调变得不切实际，探针成为自监督学习评估的主要协议。然而标准线性探针难以充分反映掩码图像建模训练模型的潜力，因此激发了对关注探针的需求。尽管关注探针逐渐被采用，但其过度参数化和计算效率低的问题仍未解决。本文通过精度与效率权衡的角度重新审视关注探针，系统分析现有方法并引入高效探针（EP），该机制通过消除冗余投影和减少可训练参数数量实现高达10倍的速度提升。EP在七个基准测试中优于线性探针和先前的方法，同时在多样化预训练范式和低样本、分层设置中表现优异，且生成的注意力图具有可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 17:10:26 GMT</pubDate>
</item>
<item>
<title>面向文本恢复的图像修复方法研究</title>
<link>https://arxiv.org/abs/2506.09993</link>
<guid>https://arxiv.org/abs/2506.09993</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉与文本恢复的新任务及多任务扩散框架。</p><br /><br /><p><strong>摘要：</strong> 现有的基于扩散模型的图像修复方法虽在自然图像修复方面取得了成功，但在处理退化图像中的文本区域时往往表现不佳，容易产生看似合理但不正确的类似文本模式，即文本-图像幻觉现象。为解决这一问题，本文引入了一种新的任务——文本感知图像修复（TAIR），旨在同时恢复视觉内容和文本准确性。为此，我们构建了一个包含10万张高质量场景图像的大规模基准数据集SA-Text，这些图像密集标注了多样且复杂的文本实例。此外，我们还提出了一个名为TeReDiff的多任务扩散框架，将扩散模型的内部特征集成到文本检测模块中，使两者通过联合训练相互受益，从而提取丰富的文本表示，并将其作为后续去噪步骤的提示。大量实验表明，我们的方法在文本识别准确率上显著优于现有最先进的修复方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09993" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>高效激发语言模型推理能力的稀疏自编码调优方法</title>
<link>https://arxiv.org/abs/2506.09967</link>
<guid>https://arxiv.org/abs/2506.09967</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过稀疏自编码调优显著降低语言模型推理能力训练成本的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Resa的推理模型家族，该模型通过一种新颖且高效的稀疏自编码调优(SAE-Tuning)技术训练而成。该方法首先利用源模型训练一个稀疏自编码器以捕捉推理能力，然后借助此编码器引导标准监督微调过程，使目标模型获得类似推理能力，整个过程仅使用经过验证的问题-答案数据而无需任何推理痕迹。研究显示，当应用于某些基础模型后，在进一步强化学习(RL)微调之前，SAE-Tuning保留了超过97%的RL训练推理性能，同时将训练成本降低超过2000倍至约1美元，训练时间缩短超过450倍至约20分钟。此外，当应用于轻度RL训练模型时，它能在AIME24上实现43.33%的Pass@1分数，在AMC23上实现90%的Pass@1分数，只需增加约1美元的成本。令人惊讶的是，通过SAE提取的推理能力可能具有通用性和模块化特性，即从某一数据集提取的能力仍可提升更大重叠语料库的表现，并且可以从Qwen或Qwen-Math等模型中提取的能力在测试时附加到R1-Distill模型上，无需重新训练即可获得相似收益。广泛的消融实验验证了这些发现，所有成果均已完全开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09967" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:44:01 GMT</pubDate>
</item>
<item>
<title>UniPre3D：一种适用于任意尺度点云的统一预训练方法</title>
<link>https://arxiv.org/abs/2506.09952</link>
<guid>https://arxiv.org/abs/2506.09952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个可无缝应用于任意规模点云的统一预训练方法UniPre3D。</p><br /><br /><p><strong>摘要：</strong> 点云数据的尺度多样性对3D视觉统一表征学习技术的发展提出了重大挑战。目前缺乏统一的3D模型，且现有预训练方法对物体级和场景级点云的效果不均衡。本文介绍UniPre3D，这是一种针对任意尺度和架构的3D模型设计的统一预训练方法。UniPre3D通过预测高斯基元作为预训练任务，并利用可微分高斯散射渲染图像，实现精确的像素级监督和端到端优化。此外，该方法结合预训练图像模型的2D特征，引入成熟的纹理知识，进一步规范预训练任务并引导模型关注几何结构。实验验证表明，UniPre3D在多种物体级和场景级任务中表现出了普遍有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:23:21 GMT</pubDate>
</item>
<item>
<title>结合规则与大模型推理的指令跟随强化学习</title>
<link>https://arxiv.org/abs/2506.09942</link>
<guid>https://arxiv.org/abs/2506.09942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合代码验证与大语言模型验证的强化学习方法提升指令跟随性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了指令跟随领域中强化学习验证挑战，并提出了名为VerIF的新方法，该方法融合了基于规则的代码验证与基于大型推理模型（如QwQ-32B）的语言模型验证。为了支持这一方法，我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其对应的验证信号。通过在两个模型上应用带有VerIF的强化学习训练，我们在多个代表性指令跟随基准测试中取得了显著改进。训练后的模型在相同规模的模型中达到最先进的性能，并且对未见过的约束条件具有良好的泛化能力。此外，观察到这些模型的整体能力未受负面影响，表明VerIF可以整合到现有的强化学习框架中以提高整体模型性能。我们已公开数据集、代码和模型以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:10:36 GMT</pubDate>
</item>
<item>
<title>ReasonMed：大规模医学推理数据集推动LLMs在医疗问答中的性能提升</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ReasonMed数据集，显著提升LLMs在医学推理和问答中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管基于推理的大语言模型（LLMs）在数学和编程领域表现出色，但它们在知识密集型医学问答中的能力尚未得到充分探索。为此，我们引入了ReasonMed，这是目前最大的医学推理数据集，包含从170万个初始推理路径中提取的37万高质量示例。通过多智能体验证和优化过程，设计了一个错误修正器来改进推理路径，纠正由验证器标记的错误步骤。利用ReasonMed，我们系统地研究了训练医学推理模型的最佳实践，发现结合详细的因果推理和简洁的答案总结是最有效的微调策略。基于此策略，我们训练了ReasonMed-7B模型，在PubMedQA上超过了之前最好的模型，并且超过了更大的LLaMA3.1-70B模型。这一成果为医学领域的LLMs发展设立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 04:36:55 GMT</pubDate>
</item>
<item>
<title>Ming-Omni：一种支持多模态处理与生成的统一模型</title>
<link>https://arxiv.org/abs/2506.09344</link>
<guid>https://arxiv.org/abs/2506.09344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Omni是一种能够处理多种模态并生成高质量图像和语音的统一模型。</p><br /><br /><p><strong>摘要：</strong> Ming-Omni是一个创新的多模态统一模型，它不仅能够处理图像、文本、音频和视频，还具备强大的语音和图像生成能力。该模型通过专用编码器从不同模态中提取特征，并利用Ling这一混合专家架构结合模态特定路由进行高效处理与融合。与传统多模态模型相比，Ming-Omni的独特之处在于支持音频和图像的生成任务，这得益于先进的音频解码器和Ming-Lite-Uni图像生成模块的支持。此外，该模型还能实现上下文感知聊天、文本转语音转换以及多样化的图像编辑功能。实验结果表明，Ming-Omni在多模态感知与生成方面表现卓越。值得一提的是，这是首个我们所知开源且在模态支持上可媲美GPT-4o的模型，我们已公开所有代码和模型权重，旨在推动相关领域的进一步研究与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 22:50:49 GMT</pubDate>
</item>
<item>
<title>Token Perturbation Guidance提升扩散模型生成质量</title>
<link>https://arxiv.org/abs/2506.10036</link>
<guid>https://arxiv.org/abs/2506.10036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练且适用于条件与非条件生成的Token Perturbation Guidance方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对分类器自由引导（CFG）在现代扩散模型中的局限性，如特定训练需求及仅限条件生成，提出了一种名为Token Perturbation Guidance (TPG)的新方法。TPG通过直接对扩散网络内的中间token表示应用扰动矩阵，采用保范洗牌操作提供有效的稳定引导信号，从而提升生成质量而无需架构改动。作为一种无需训练且与输入条件无关的方法，TPG可应用于条件和非条件生成。实验表明，在SDXL和Stable Diffusion 2.1上，TPG在无条件生成中的FID得分较SDXL基线提升了近两倍，同时在提示对齐方面接近CFG的效果，确立了TPG作为通用且无条件依赖的引导方法的地位。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:25:46 GMT</pubDate>
</item>
<item>
<title>Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.09250</link>
<guid>https://arxiv.org/abs/2506.09250</guid>
<content:encoded><![CDATA[
Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N &gt; 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:16:53 GMT</pubDate>
</item>
<item>
<title>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</title>
<link>https://arxiv.org/abs/2506.08862</link>
<guid>https://arxiv.org/abs/2506.08862</guid>
<content:encoded><![CDATA[
Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 10:52:36 GMT</pubDate>
</item>
<item>
<title>Draft-based Approximate Inference for LLMs</title>
<link>https://arxiv.org/abs/2506.08373</link>
<guid>https://arxiv.org/abs/2506.08373</guid>
<content:encoded><![CDATA[
Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 22:37:46 GMT</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 17:04:14 GMT</pubDate>
</item>
<item>
<title>LLM Unlearning Should Be Form-Independent</title>
<link>https://arxiv.org/abs/2506.07795</link>
<guid>https://arxiv.org/abs/2506.07795</guid>
<content:encoded><![CDATA[
Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:21:25 GMT</pubDate>
</item>
<item>
<title>Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques</title>
<link>https://arxiv.org/abs/2506.08060</link>
<guid>https://arxiv.org/abs/2506.08060</guid>
<content:encoded><![CDATA[
Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:37:19 GMT</pubDate>
</item>
<item>
<title>LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer</title>
<link>https://arxiv.org/abs/2506.06952</link>
<guid>https://arxiv.org/abs/2506.06952</guid>
<content:encoded><![CDATA[
Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 20:15:32 GMT</pubDate>
</item>
<item>
<title>What Makes a Good Natural Language Prompt?</title>
<link>https://arxiv.org/abs/2506.06950</link>
<guid>https://arxiv.org/abs/2506.06950</guid>
<content:encoded><![CDATA[
As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 19:19:27 GMT</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 03:19:11 GMT</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 18:16:16 GMT</pubDate>
</item>
<item>
<title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title>
<link>https://arxiv.org/abs/2506.05982</link>
<guid>https://arxiv.org/abs/2506.05982</guid>
<content:encoded><![CDATA[
As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:02:01 GMT</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation中的知识冲突类型及其处理研究</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新型知识冲突分类法并构建首个高质量基准CONFLICTS，评估大型语言模型处理冲突信息的能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval Augmented Generation (RAG) 是一种常见的方法，用于通过相关且最新的信息增强大型语言模型 (LLMs)，但检索到的来源经常包含矛盾信息，模型如何解决这些矛盾仍不清楚。本研究首先提出了RAG中知识冲突类型的新型分类法，并定义了每种类型期望的模型行为。随后，引入了CONFLICTS，这是一个具有专家标注的高质量基准，在现实RAG环境中对冲突类型进行注释。CONFLICTS是首个能够追踪模型在处理多种知识冲突方面进展的基准。我们对该基准进行了广泛的实验，结果显示LLMs往往难以适当解决来源之间的冲突。虽然提示LLMs显式推理检索文档中的潜在冲突显著提高了响应质量和适当性，但仍存在很大的改进空间，未来研究仍有大量工作待完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:52:57 GMT</pubDate>
</item>
<item>
<title>Institutional Books 1.0：基于哈佛图书馆公共领域书籍的历史文本数据集发布</title>
<link>https://arxiv.org/abs/2506.08300</link>
<guid>https://arxiv.org/abs/2506.08300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍基于哈佛图书馆公共领域书籍的数据集Institutional Books 1.0。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，高质量训练数据的稀缺性成为关注焦点。本技术报告介绍了Institutional Books 1.0，这是一个由哈佛图书馆参与Google Books项目（自2006年开始）数字化的大量公共领域书籍组成的数据集。我们与哈佛图书馆合作，从原始扫描的1,075,899卷书中提取并处理出包含约2500亿标记的历史文本数据集，其中983,004卷书（约2420亿标记）的OCR提取文本及元数据已公开。该数据集涵盖超过250种语言，旨在提升历史文本对人类和机器的可访问性和可用性。报告详细描述了项目的背景、目标、方法以及分析结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 20:11:30 GMT</pubDate>
</item>
<item>
<title>Mirage：基于音频生成高质量视频的统一模型</title>
<link>https://arxiv.org/abs/2506.08279</link>
<guid>https://arxiv.org/abs/2506.08279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型Mirage，可从音频输入生成逼真的视频。</p><br /><br /><p><strong>摘要：</strong> 当前视频生成方法要么忽略声音专注于无声图像生成，要么关注视听元素但局限于特定应用领域。本文介绍的Mirage是一种音频到视频的基础模型，能从零开始根据音频输入生成真实且富有表现力的输出图像。当与现有的语音合成方法结合时，Mirage可以生成引人注目的多模态视频。其核心技术贡献是一种统一的自注意力机制音频到视频生成模型训练方法，无论是从头开始还是基于已有权重，都能保持模型的通用性并提升输出质量。该模型在处理人物演讲场景时尤其有效，能够生成可信的表演解读视频。我们鼓励读者亲自观看和聆听Mirage的成果（详见论文和链接）。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 18:56:02 GMT</pubDate>
</item>
<item>
<title>测试时间交互扩展提升智能体性能</title>
<link>https://arxiv.org/abs/2506.07976</link>
<guid>https://arxiv.org/abs/2506.07976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的测试时间扩展维度——测试时间交互，显著提升智能体性能。</p><br /><br /><p><strong>摘要：</strong> 当前测试时间扩展范式依赖于生成长推理路径以增加思考深度，但不支持智能体实时获取环境信息或动态调整行为。本研究引入测试时间交互这一新维度，使智能体能在单次运行中进行探索、回溯及动态重规划等复杂行为。通过在网页代理领域验证，即使没有训练，基于提示的交互扩展也能显著提高任务成功率。进一步提出TTI方法，采用基于课程的在线强化学习策略，根据智能体的表现动态调整其交互长度。实验表明，TTI在WebVoyager和WebArena基准上达到了开源开放数据集的最佳性能，并展示了智能体在探索与利用之间的自适应平衡能力。测试时间交互扩展为提升智能体适应性提供了全新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:02 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的可解释AI生成图像检测</title>
<link>https://arxiv.org/abs/2506.07045</link>
<guid>https://arxiv.org/abs/2506.07045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">优化后的多模态大语言模型能有效检测AI生成图像并提供合理解释。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着图像生成技术的飞速发展，对可解释且鲁棒的检测方法需求增加。尽管现有方法精度较高，但通常缺乏透明性，无法为人所理解。多模态大语言模型虽非专门设计用于伪造检测，但具备强大的分析推理能力。通过适当微调，这些模型能够有效识别AI生成图像并提供有意义的解释。然而，当前的多模态大语言模型仍存在幻觉问题，其视觉解释难以与实际图像内容及人类推理保持一致。为此，我们构建了一个包含标注边界框和描述性字幕的数据集，用于突出合成伪影，奠定人类对齐的视觉文本推理基础。随后，我们采用多阶段优化策略，逐步平衡准确检测、视觉定位和连贯文本解释的目标。最终模型在检测AI生成图像和定位视觉缺陷方面表现出色，显著优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:44 GMT</pubDate>
</item>
<item>
<title>MMRefine：多模态大语言模型误差精化能力评估基准</title>
<link>https://arxiv.org/abs/2506.04688</link>
<guid>https://arxiv.org/abs/2506.04688</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMRefine基准，评估多模态大语言模型的误差检测与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMRefine的多模态精化基准，用于评估多模态大型语言模型（MLLMs）的误差精化能力。与以往仅关注最终准确率不同，MMRefine通过六个特定场景评估MLLMs的错误检测和纠正能力，并将错误分类为六种类型进行性能分析。实验表明，不同开放及闭源MLLMs存在改进瓶颈，揭示了提升推理效果的关键领域。该基准代码和数据集已公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04688" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 03:11:36 GMT</pubDate>
</item>
<item>
<title>基于查询聚焦的定量关键点摘要模型QQSUM-RAG</title>
<link>https://arxiv.org/abs/2506.04020</link>
<guid>https://arxiv.org/abs/2506.04020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QQSUM任务及模型，改进产品问答系统对客户意见多样性的捕捉能力。</p><br /><br /><p><strong>摘要：</strong> 现有基于评论的产品问答(PQA)系统因仅提供单一视角而难以反映客户意见的多样性。本文引入Quantitative Query-Focused Summarization(QQSUM)任务，旨在通过量化意见的流行度来生成具有代表性的关键点摘要，从而有效回答用户查询。尽管检索增强生成(RAG)方法在PQA中有一定潜力，但其生成的答案仍无法充分涵盖多样的观点。为解决这一问题，我们提出了QQSUM-RAG模型，该模型通过少量样本学习同时训练关键点导向的检索器和关键点摘要生成器，从而实现能够捕捉多样化且代表性观点的关键点摘要。实验结果表明，QQSUM-RAG在文本质量和意见量化准确性方面均优于最先进的RAG基线模型。相关源代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:50:32 GMT</pubDate>
</item>
<item>
<title>PlayerOne：首个第一人称现实世界模拟器</title>
<link>https://arxiv.org/abs/2506.09995</link>
<guid>https://arxiv.org/abs/2506.09995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlayerOne可精准构建虚拟世界并生成与真实场景匹配的第一人称视频。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为PlayerOne的创新性第一人称现实世界模拟器，它能够在动态环境中实现沉浸式且无限制的探索。该系统通过输入用户的第一人称场景图像，能够精确构建对应的虚拟世界，并生成与用户实际动作严格对齐的第一人称视频。PlayerOne采用粗到细的训练管道，在大规模第一人称文本-视频对上进行预训练，随后利用同步运动-视频数据进行微调，这些数据来自由自动构建管道提取的内外视角视频数据集。此外，为了更好地控制不同部位的动作，设计了一种部分解耦的动作注入方案，并开发了一种联合重建框架，逐步建模4D场景及视频帧，保证长时间视频生成的一致性。实验结果显示，PlayerOne在控制多样化场景中的各种人体动作方面表现出强大的泛化能力，标志着第一人称现实世界模拟领域的首次尝试，为世界建模及其广泛应用开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>基于多模态条件的端到端人体动画生成框架</title>
<link>https://arxiv.org/abs/2506.09984</link>
<guid>https://arxiv.org/abs/2506.09984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架实现多概念人体为中心的高质量可控视频生成。</p><br /><br /><p><strong>摘要：</strong> 近年来，结合文本、图像和音频等多种模态条件的人体动画取得了显著进展。然而，大多数现有方法仅限于单一主体动画，并以全局方式注入条件，忽视了多概念在同一视频中的丰富交互场景。这种局限性阻碍了对人类及物体的精确个性化控制。本研究摒弃单实体假设，提出了一种新颖框架，通过区域特定绑定模态条件至各身份的空间时间足迹，实现了多概念的精确控制。实验结果和消融研究表明，该方法在多模态条件下的显式布局控制优于隐式方法及其他现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>SAFE：面向视觉语言动作模型的多任务故障检测器</title>
<link>https://arxiv.org/abs/2506.09937</link>
<guid>https://arxiv.org/abs/2506.09937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种适用于多种任务的故障检测器SAFE，用于提升机器人在新任务中的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言动作模型（VLAs）在新任务上的有限成功率问题，引入多任务故障检测问题，并设计了一种名为SAFE的故障检测器。SAFE利用VLAs内部特征，通过学习任务成功与失败的高阶知识，在模拟和真实环境中对多种策略架构进行测试，展现出卓越的故障检测性能及准确性和检测时间的最佳权衡。SAFE在多个任务上优于现有基线方法，为机器人在未知环境中的安全交互提供了保障。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:59:13 GMT</pubDate>
</item>
<item>
<title>ComfyUI-R1：首个用于自动化工作流生成的大规模推理模型</title>
<link>https://arxiv.org/abs/2506.09790</link>
<guid>https://arxiv.org/abs/2506.09790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模推理模型ComfyUI-R1，显著提升AI艺术创作的工作流生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ComfyUI-R1的新模型，该模型是首个专门用于自动化工作流生成的大规模推理模型。通过利用精心策划的数据集中的4000个工作流，我们构建了包含节点选择、工作流规划及代码级工作流表示在内的长链推理数据。ComfyUI-R1采用了两阶段框架进行训练：首先通过冷启动的链式思维微调来适应ComfyUI领域；然后利用细粒度规则-指标混合奖励机制下的强化学习，以激励推理能力并确保格式的有效性、结构的完整性以及节点级别的准确性。实验结果显示，我们的7B参数模型达到了97%的格式有效性率，并且在节点级别和图级别F1得分上表现出色，明显优于使用GPT-4o和Claude系列等领先闭源模型的方法。进一步分析表明推理过程的重要性以及将工作流转化为代码的优势。定性比较显示，我们在合成复杂且多样化的节点组合工作流方面具有优势，这突显了长链推理在AI艺术创作中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 10:35:15 GMT</pubDate>
</item>
<item>
<title>Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation</title>
<link>https://arxiv.org/abs/2506.09350</link>
<guid>https://arxiv.org/abs/2506.09350</guid>
<content:encoded><![CDATA[
Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:04:23 GMT</pubDate>
</item>
<item>
<title>Seedance 1.0：高效高质量视频生成基础模型</title>
<link>https://arxiv.org/abs/2506.09113</link>
<guid>https://arxiv.org/abs/2506.09113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedance 1.0通过多源数据精炼等技术实现了高效高质量视频生成。</p><br /><br /><p><strong>摘要：</strong> Seedance 1.0是一种高性能且推理高效的视频生成基础模型，针对当前扩散模型在视频生成中的挑战进行了多项技术改进。首先，它通过多源数据精炼结合精确的视频描述，实现跨多样化场景的全面学习；其次，采用高效的架构设计和训练范式，支持多镜头生成并联合处理文本到视频和图像到视频任务；第三，利用细粒度监督微调和多维奖励机制优化后训练，显著提升性能；最后，通过多阶段蒸馏策略和系统级优化，实现约10倍的推理加速。实验结果显示，Seedance 1.0仅需41.4秒即可生成1080p分辨率的5秒视频（基于NVIDIA-L20）。相较于现有最先进模型，该模型在时空流畅性、结构稳定性、复杂多主体场景下的指令遵循能力以及多镜头叙事连贯性方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:56:11 GMT</pubDate>
</item>
<item>
<title>Branched Schrödinger Bridge Matching：多模态分布转换的新框架</title>
<link>https://arxiv.org/abs/2506.09007</link>
<guid>https://arxiv.org/abs/2506.09007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法BranchSBM，用于多路径分布转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Branched Schrödinger Bridge Matching（BranchSBM）的新框架，该框架通过参数化多个时间相关的速度场和增长过程，解决了现有方法无法处理多模态分布转换的问题。BranchSBM不仅提高了表达能力，还在多路径表面导航、细胞命运分支建模以及细胞对扰动的发散响应模拟等任务中展现出其必要性。传统方法如流匹配和Schrödinger桥接匹配仅能处理单模态转换，而BranchSBM可以捕捉从共同起点到多个不同终点的分支或发散演化过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:29:48 GMT</pubDate>
</item>
<item>
<title>基于测试驱动开发的数据合成框架SWE-Flow</title>
<link>https://arxiv.org/abs/2506.09003</link>
<guid>https://arxiv.org/abs/2506.09003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于测试驱动开发的新数据合成框架SWE-Flow。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SWE-Flow的新型数据合成框架，该框架以测试驱动开发（TDD）为基础。与依赖人工提交问题的传统软件工程数据不同，SWE-Flow通过自动推断单元测试中的增量开发步骤来生成数据，这些测试本身就包含了高层次的需求。SWE-Flow的核心是构建运行时依赖图（RDG），它精确捕捉函数间的交互，从而生成结构化的逐步开发计划。每一步骤都会生成部分代码库、对应的单元测试及必要的代码修改，形成可验证的TDD任务。通过此方法，我们从GitHub的真实项目中生成了16,061个训练实例和2,020个测试实例，创建了SWE-Flow-Eval基准。实验表明，在该数据集上微调开源模型显著提升了基于TDD的编码性能。为了促进进一步研究，所有代码、数据集、模型和Docker镜像均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09003" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:23:33 GMT</pubDate>
</item>
<item>
<title>SeerAttention-R：面向推理模型长解码的稀疏注意力框架</title>
<link>https://arxiv.org/abs/2506.08889</link>
<guid>https://arxiv.org/abs/2506.08889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeerAttention-R通过自蒸馏门机制实现高效稀疏注意力，适用于长序列推理任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeerAttention-R的稀疏注意力框架，专为推理模型的长序列解码设计。该框架继承了SeerAttention的设计，通过自蒸馏门机制学习注意力稀疏性，并去除了查询池化以支持自回归解码。它轻量且易于集成到现有的预训练模型中，无需修改原始参数。实验表明，在AIME基准测试中，SeerAttention-R仅需0.4Btokens即可在大稀疏注意力块大小下保持接近无损的推理精度。此外，利用TileLang开发的优化稀疏解码内核，在H100 GPU上实现了高达9倍的速度提升。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:17:26 GMT</pubDate>
</item>
<item>
<title>POET：一种基于正交等价变换的大规模语言模型训练算法</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法POET，通过优化神经元显著提升大规模语言模型的训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对有效且可靠地训练大型语言模型这一人工智能领域的重大挑战，提出了一种名为POET的新算法。该算法采用正交等价变换对神经元进行重新参数化，具体而言，每个神经元通过两个可学习的正交矩阵和一个固定的随机权重矩阵表示。由于其在谱性质上的保真性，POET能够在稳定优化目标函数的同时提高泛化能力。此外，还开发了高效的近似方法，使POET适用于大规模神经网络的灵活且可扩展的训练。大量实验验证了POET在训练大型语言模型方面的有效性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于自我信心的大语言模型后训练强化学习方法</title>
<link>https://arxiv.org/abs/2506.06395</link>
<guid>https://arxiv.org/abs/2506.06395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需标注的强化学习方法RLSC，显著提升数学推理任务性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理方面表现出色，但后训练过程对调整其行为以符合任务目标至关重要。现有的强化学习（RL）方法通常依赖昂贵的人类注释或外部奖励模型。我们提出了通过自我信心进行强化学习（RLSC），利用模型自身的置信度作为奖励信号，从而避免了标签、偏好模型或奖励工程的需求。将RLSC应用于Qwen2.5-Math-7B模型，在每个问题仅使用16个样本且训练步骤为10或20的情况下，该方法在AIME2024上提升了13.4%，在MATH500上提升了21.2%，在Minerva Math上提升了21.7%，在Olympiadbench上提升了20.8%，在AMC23上提升了9.7%的准确性。RLSC提供了一种简单且可扩展的后训练方法，仅需少量样本和无标注监督即可优化推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:55:15 GMT</pubDate>
</item>
<item>
<title>引入Autoregressive Semantic Visual Reconstruction提升多模态理解</title>
<link>https://arxiv.org/abs/2506.09040</link>
<guid>https://arxiv.org/abs/2506.09040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ASVR模型，通过联合学习视觉与文本模态提高多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型视觉语言模型（LVLMs）主要依赖文本序列的自回归监督，未能充分整合视觉模态，导致无法有效利用无描述图片、遗漏关键视觉细节及难以通过文本传达特定视觉内容的问题。本研究引入Autoregressive Semantic Visual Reconstruction（ASVR），在一个统一的自回归框架下实现视觉与文本模态的联合学习。实验表明，直接自回归重建图像原始外观可能削弱多模态理解，而基于语义表示的重建显著提升性能。ASVR在多种数据规模和大语言模型骨干上均有显著改进，在14个多模态基准测试中使LLaVA-1.5平均得分提升了5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:50 GMT</pubDate>
</item>
<item>
<title>基于强化学习的小型规则推理模型的高效方法</title>
<link>https://arxiv.org/abs/2506.08672</link>
<guid>https://arxiv.org/abs/2506.08672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型强化规则推理方法，显著提升小模型在多任务中的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于规则推理的挑战，特别是针对实际应用中规则格式、类型和复杂性的变化带来的困难。虽然大型推理模型在强化学习辅助下表现出色，但小型推理模型是否能有效学习并具有跨任务和领域的稳健泛化能力尚不清楚。为解决此问题，我们提出了RuleReasoner，这是一种通过精心策划的任务集和新颖的领域感知动态采样方法实现规则推理的方法。实验表明，该方法在分布内和分布外基准测试中均优于现有前沿方法，同时具有更高的计算效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 06:31:21 GMT</pubDate>
</item>
<item>
<title>Mathesis：结合强化学习的端到端数学定理证明系统</title>
<link>https://arxiv.org/abs/2506.07047</link>
<guid>https://arxiv.org/abs/2506.07047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mathesis首次实现从自然语言问题到形式化证明的全流程自动化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Mathesis的新方法，它是一个端到端的数学定理证明流水线，能够处理非正式的问题陈述并将其自动形式化。Mathesis的关键组成部分包括Mathesis-Autoformalizer，这是一个使用强化学习增强自然语言问题形式化的工具，并通过LeanScorer框架评估形式化质量。此外，还提出了一种Mathesis-Prover，用于从形式化陈述生成形式化证明。为了评估该方法的实际应用价值，我们创建了一个名为Gaokao-Formal的新基准测试集，其中包含来自中国高考的488个复杂问题。实验结果显示，Mathesis-Autoformalizer在Gaokao-Formal上的通过率比最佳基线高出22%，整个系统在MiniF2F上达到64%的准确率，在Gaokao-Formal上实现了18%的最新性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 05:04:14 GMT</pubDate>
</item>
<item>
<title>DiscoVLA：针对视频文本检索的CLIP参数高效适配方法</title>
<link>https://arxiv.org/abs/2506.08887</link>
<guid>https://arxiv.org/abs/2506.08887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiscoVLA方法，同时减少图像到视频的视觉、语言及对齐三方面差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了CLIP模型在视频文本检索中的参数高效适应问题，发现从图像级到视频级迁移时存在视觉、语言和对齐三大关键差异。现有方法主要关注视觉差异，忽视了语言和对齐方面的改进。为此，我们提出了DiscoVLA（Discrepancy Reduction in Vision, Language, and Alignment），通过引入图像-视频特征融合来解决视觉和语言差异，并利用伪图像描述生成学习细粒度的图像级对齐，同时采用图像到视频对齐蒸馏提升视频级对齐效果。实验表明，DiscoVLA在MSRVTT数据集上的R@1指标比现有方法高出1.5%，达到50.5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:16:40 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的新训练范式：Self Forcing</title>
<link>https://arxiv.org/abs/2506.08009</link>
<guid>https://arxiv.org/abs/2506.08009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Self Forcing方法解决视频生成中的曝光偏差问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self Forcing的新型训练范式，专门针对自回归视频扩散模型设计，旨在解决长期存在的曝光偏差问题。传统方法在推理过程中依赖于真实的上下文帧，而Self Forcing通过在训练时利用之前自动生成的输出进行条件生成，显著提高了生成序列的整体质量。该方法采用整体视频级别的损失函数，而非传统的逐帧目标函数，同时结合了几步扩散模型和随机梯度截断策略，有效平衡了计算成本与性能。此外，引入滚动键值缓存机制进一步提升了效率，使得该方法能够在单GPU上实现亚秒级延迟的实时流媒体视频生成，且生成质量可媲美甚至超越更为复杂和非因果的扩散模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Aligning Text, Images, and 3D Structure Token-by-Token</title>
<link>https://arxiv.org/abs/2506.08002</link>
<guid>https://arxiv.org/abs/2506.08002</guid>
<content:encoded><![CDATA[
Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:37 GMT</pubDate>
</item>
<item>
<title>Squeeze3D：基于隐式先验知识的高效3D数据压缩框架</title>
<link>https://arxiv.org/abs/2506.07932</link>
<guid>https://arxiv.org/abs/2506.07932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用已有3D生成模型的隐式先验知识进行高比率压缩的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Squeeze3D的创新框架，该框架通过可训练映射网络连接已有的编码器和生成模型的潜在空间，利用这些模型学习到的隐式先验知识实现对网格、点云及辐射场等3D数据的极高压缩比。实验表明，Squeeze3D在保持视觉质量的同时，对纹理网格的压缩比可达2187倍，对点云为55倍，对辐射场为619倍。此外，由于不涉及对象特定网络的训练，其压缩和解压延迟极小。Squeeze3D适用于多种现有预训练3D编码器和生成模型，并支持不同格式的3D数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:52:10 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不等式证明中的挑战与研究进展</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法提升不等式证明能力，发现顶级模型在严谨推理上仍有显著差距。</p><br /><br /><p><strong>摘要：</strong> 不等式证明作为科学与数学领域的重要工具，考验着高级推理能力。然而，现有数据集的局限性阻碍了大型语言模型（LLMs）在此领域的进步。为此，研究者引入了一种新的任务形式，将不等式证明分解为可自动验证的子任务——边界估计与关系预测，并构建了一个名为IneqMath的专业级不等式数据集。该数据集包含丰富的解题步骤与定理注释，旨在推动模型性能提升。此外，研究设计了一种结合最终答案判断与逐步判断的评估框架，用于检测推理缺陷。对29个领先LLMs的测试显示，尽管顶级模型如o1在最终答案准确性上有一定表现，但在逐步推理验证下整体准确率不足10%，较仅考虑最终答案时下降了65.5%。这一发现揭示了当前LLMs在严谨证明构建上的脆弱性和局限性。研究进一步表明，单纯增加模型规模或计算资源并不能显著提高证明正确性，而是需要探索如定理引导推理及自我优化等新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07927" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:43:38 GMT</pubDate>
</item>
<item>
<title>Frame Guidance：无需训练的可控视频生成引导方法</title>
<link>https://arxiv.org/abs/2506.07177</link>
<guid>https://arxiv.org/abs/2506.07177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于帧级信号的无需训练的可控视频生成引导方法Frame Guidance。</p><br /><br /><p><strong>摘要：</strong> 扩散模型的进步显著提升了视频质量，但许多现有方法依赖于针对特定任务微调大规模视频模型，随着模型规模增长变得不切实际。本文介绍Frame Guidance，这是一种无需训练即可实现可控视频生成的引导方法，基于帧级信号如关键帧、风格参考图像、草图或深度图等。为实现实用的无需训练引导，我们提出了一种简单的潜在处理方法以大幅减少内存使用，并采用一种新颖的潜在优化策略以实现全局一致的视频生成。Frame Guidance能够在多种任务上有效控制，包括关键帧引导、风格化和循环生成，且无需任何训练，兼容所有视频模型。实验结果显示，该方法可以为广泛的任务和输入信号生成高质量的可控视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:54:41 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的地缘政治偏见研究</title>
<link>https://arxiv.org/abs/2506.06751</link>
<guid>https://arxiv.org/abs/2506.06751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大型语言模型存在显著的地缘政治偏见，且简单去偏方法效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在解释具有国家视角冲突的历史事件时所表现出的地缘政治偏见（涉及美国、英国、苏联和中国）。我们构建了一个包含中立事件描述及多国对立观点的新数据集，实验结果显示这些模型倾向于支持特定的国家叙事。尽管尝试了简单的去偏提示，但其对减少偏见的效果十分有限。此外，通过操控参与者标签的实验表明，模型对归因高度敏感，有时会放大偏见或检测到不一致性，尤其是在标签被替换的情况下。本研究揭示了LLMs中的国家叙事偏见，挑战了简单去偏方法的有效性，并为未来地缘政治偏见研究提供了框架和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 06:45:17 GMT</pubDate>
</item>
<item>
<title>异构Mixture-of-Adapters方法提升大语言模型参数高效微调性能</title>
<link>https://arxiv.org/abs/2506.05928</link>
<guid>https://arxiv.org/abs/2506.05928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出异构MoA方法解决现有MoE-LoRA的表征塌陷和负载不均问题。</p><br /><br /><p><strong>摘要：</strong> 最近的研究将低秩适配（LoRA）与专家混合（MoE）结合，进一步提升大语言模型（LLM）参数高效微调（PEFT）方法的性能。然而，现有的同构MoE-LoRA架构常面临表征塌陷和专家负载失衡的问题。为应对这些挑战，本文提出了异构Mixture-of-Adapter（MoA）方法，通过动态整合结构多样的PEFT适配器专家，利用互补的表示能力促进专家专业化，从而增强预训练知识向下游任务的有效迁移。MoA支持两种变体：Soft MoA实现细粒度集成，Sparse MoA则基于贡献稀疏激活适配器专家。实验表明，异构MoA在性能和参数效率上均优于同构MoE-LoRA方法。该项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:54:19 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的知识增强金融推理模型RKEFino1</title>
<link>https://arxiv.org/abs/2506.05700</link>
<guid>https://arxiv.org/abs/2506.05700</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合监管知识的金融推理模型RKEFino1，提升数字合规报告的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型(LLMs)在数字监管报告(DRR)中的应用挑战，提出了RKEFino1模型，该模型通过微调Fino1，在XBRL、CDM和MOF等领域的专业知识基础上进行增强。研究设计了基于知识的问答(QA)任务、数学推理QA任务以及覆盖句子和表格中财务实体的数值命名实体识别(NER)任务。实验结果表明，RKEFino1在合规相关的金融任务中表现出色且具有良好的泛化能力。目前，该模型已发布在Hugging Face平台上供公众使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05700" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 23:02:52 GMT</pubDate>
</item>
<item>
<title>基于证据性引导的检索增强生成模型ECoRAG</title>
<link>https://arxiv.org/abs/2506.05167</link>
<guid>https://arxiv.org/abs/2506.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ECoRAG，提升LLMs在开放域问答中的性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在开放域问答（ODQA）中通过检索增强生成（RAG）利用外部文档表现优异，但现有压缩方法未能有效过滤非证据性信息，限制了性能。本文提出ECoRAG框架，通过基于证据性的文档压缩提升LLMs性能，确保答案生成由正确证据支持，并在必要时补充更多内容。实验表明，ECoRAG在多个ODQA任务中超越现有压缩方法，同时显著降低延迟并减少令牌使用量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 11:43:49 GMT</pubDate>
</item>
<item>
<title>基于预操作批评机制的多模态大语言模型在GUI自动化中的应用</title>
<link>https://arxiv.org/abs/2506.04614</link>
<guid>https://arxiv.org/abs/2506.04614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的预操作批评机制显著提升了GUI自动化任务的成功率和效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）被广泛应用于图形用户界面（GUI）自动化等多模态推理任务。不同于一般的离线多模态任务，GUI自动化需要在线交互环境中逐步决策，对每一步决策的准确性要求较高。为了提高决策可靠性，我们引入了一种预操作批评机制，通过预测行动的潜在结果和正确性提供反馈。具体而言，提出了建议感知梯度相对策略优化（S-GRPO）策略构建预操作批评模型GUI-Critic-R1，并引入新颖的建议奖励以增强反馈的可靠性。此外，开发了一种基于推理引导的数据收集管道，创建GUI-Critic-Train和GUI-Critic-Test，填补了现有GUI批评数据的空白。静态实验表明，GUI-Critic-R1在批评准确性上优于当前的MLLMs；动态评估进一步证明了该模型在成功率和操作效率方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:12:36 GMT</pubDate>
</item>
<item>
<title>基于正交匹配追踪的预训练语言模型跨分词器移植方法</title>
<link>https://arxiv.org/abs/2506.06607</link>
<guid>https://arxiv.org/abs/2506.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需微调的正交匹配追踪方法，实现大型语言模型的跨分词器迁移。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种无需微调的预训练大型语言模型（LLMs）的分词器移植方法，通过正交匹配追踪（OMP）重建未见令牌嵌入。该方法将词汇外令牌近似为共享令牌的稀疏线性组合，在两个具有挑战性的跨分词器任务中展示了最佳的零样本性能保存能力。与其他零样本方法相比，OMP在多个基准测试中表现出色，且有效解决了大分词器差异问题。此外，该技术支持预训练权重的直接重用，适用于跨分词器的知识蒸馏、推测解码、集成、合并及领域特定词汇适应。我们还将此方法集成到开源工具mergekit-tokensurgeon中，用于后处理词汇对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 20:51:27 GMT</pubDate>
</item>
<item>
<title>NetPress: Dynamically Generated LLM Benchmarks for Network Applications</title>
<link>https://arxiv.org/abs/2506.03231</link>
<guid>https://arxiv.org/abs/2506.03231</guid>
<content:encoded><![CDATA[
Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:04:22 GMT</pubDate>
</item>
<item>
<title>无需权重更新的动态视图合成</title>
<link>https://arxiv.org/abs/2506.08004</link>
<guid>https://arxiv.org/abs/2506.08004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重新设计预训练视频扩散模型的噪声初始化阶段实现高保真动态视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文将动态视图合成问题视为无监督训练中的逆问题，提出了一种不需权重更新或辅助模块的方法。我们首先指出由零终端信噪比（SNR）调度引起的确定性反转基本障碍，并通过引入新的噪声表示方法——K阶递归噪声表示（K-order Recursive Noise Representation）解决该问题。此表示法具有闭合形式表达，可精确高效地对齐VAE编码与DDIM反演潜在变量。此外，针对相机运动导致的新可见区域，我们提出随机潜在调制（Stochastic Latent Modulation），在潜在空间上进行基于可见性的采样以完成被遮挡区域。实验表明，通过结构化潜在变量操作可以在噪声初始化阶段有效执行动态视图合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>tau^2-bench：引入双控环境评估对话AI代理</title>
<link>https://arxiv.org/abs/2506.07982</link>
<guid>https://arxiv.org/abs/2506.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准tau^2-bench通过双控环境模拟真实场景，评估对话AI代理的推理与用户引导能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对话AI代理评估基准多局限于单一控制环境，即仅AI代理可操作工具与外界交互，而用户作为被动的信息提供者。这种设置与现实世界中的应用场景（如技术支持）存在差异，在这些场景中用户需主动参与改变共享世界的状态。为弥合这一差距，本文提出了tau^2-bench，该基准具有四个关键贡献：首先，构建了一个新颖的电信双控领域，建模为Dec-POMDP，允许AI代理和用户共同利用工具在共享动态环境中行动，测试代理的协调与沟通能力；其次，开发了一种组合任务生成器，从原子组件编程生成多样化且可验证的任务，确保领域覆盖度和可控复杂性；第三，设计了一个紧密耦合环境的可靠用户模拟器，其行为受制于工具和可观测状态，提高仿真保真度；最后，通过多种消融分析细致评估代理性能，包括区分推理错误与沟通/协调错误。实验表明，当代理从无用户环境切换到双控环境时，性能显著下降，凸显出引导用户面临的挑战。总体而言，tau^2-bench为需要有效推理并引导用户行为的代理提供了受控测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:52:18 GMT</pubDate>
</item>
<item>
<title>CyberV：基于控制论的视频多模态大语言模型自适应框架</title>
<link>https://arxiv.org/abs/2506.07971</link>
<guid>https://arxiv.org/abs/2506.07971</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于控制论的视频多模态大语言模型自适应框架，显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大语言模型在处理长视频或复杂视频时面临计算需求高、鲁棒性差和准确性有限的问题，尤其是参数较少的模型。为解决这些问题，我们提出了名为CyberV的创新框架，该框架受到控制论原理的启发，将视频多模态大语言模型重新设计为具备自监测、自修正和动态资源分配能力的自适应系统。具体而言，CyberV引入了一个由多模态大语言模型推理系统、传感器和控制器组成的控制环路。传感器监控推理过程并收集中间解释，控制器则决定何时触发自修正并生成反馈以指导下一轮推理。此框架无需重新训练或添加额外组件即可增强冻结的多模态大语言模型。实验结果显示，在VideoMMMU基准上，CyberV使Qwen2.5-VL-7B提升了8.3%，InternVL3-8B提升了5.5%，甚至超过了竞争性的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，性能提升达10.0%，达到接近人类专家的表现。此外，该方法在VideoMME和WorldSense等通用基准测试中也显示出一致的改进，证明了其在提高动态视频理解方面模型的鲁棒性和准确性方面的有效性及泛化能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07971" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:45:18 GMT</pubDate>
</item>
<item>
<title>SAFEFLOW：构建可信大型语言模型及视觉语言模型驱动代理的新框架</title>
<link>https://arxiv.org/abs/2506.07564</link>
<guid>https://arxiv.org/abs/2506.07564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAFEFLOW框架，强化多模态代理的信息流控制与安全性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）和视觉语言模型（VLMs）的进步推动了复杂推理与工具使用的自主代理的发展。然而，现有代理框架仍存在脆弱性问题，缺乏安全的信息流动机制、可靠性以及多代理协作能力。本文引入SAFEFLOW，一种协议级框架，用于构建可信的LLM/VLM驱动代理。该框架通过细粒度的信息流控制（IFC），精确追踪代理间交换数据的来源、完整性和保密性。此外，它通过事务执行、冲突解决和共享状态的安全调度，在并发多代理环境中确保全局一致性。为了增强鲁棒性，SAFEFLOW还引入了写前日志记录、回滚和安全缓存等机制。通过构建SAFEFLOWBENCH基准套件验证其性能，实验表明，即使在敌对环境下，基于SAFEFLOW的代理也能保持优异的任务表现和安全保障，显著优于当前最先进的方法。这一成果奠定了构建原则性、鲁棒且安全的代理生态系统的基石，推动可靠自治领域的前沿发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 05:04:37 GMT</pubDate>
</item>
<item>
<title>基于自适应提升循环的机器人视觉规划模型在线学习方法</title>
<link>https://arxiv.org/abs/2506.06658</link>
<guid>https://arxiv.org/abs/2506.06658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过自我适应迭代提升视频模型性能的方法解决未知任务。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于演示训练的视频生成模型在解决机器人任务时泛化能力不足的问题，提出了Self-Adapting Improvement Loop (SAIL) 方法。该方法利用互联网规模预训练的视频模型进行领域内适应，收集自我产生的轨迹数据，并迭代更新领域内视频模型，从而持续提升特定任务的性能。研究将SAIL应用于MetaWorld任务集及真实机械臂的两个操作任务中，发现即使初始领域内演示质量较低或未经过筛选的数据，模型仍能在多个迭代后显著改善对新任务的表现。这表明通过互联网规模数据的适应性学习与在线经验累积，可以逐步构建高性能的视频模型以应对未知的机器人任务挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 00:34:37 GMT</pubDate>
</item>
<item>
<title>SynthesizeMe：基于用户交互生成合成人格以实现个性化奖励建模</title>
<link>https://arxiv.org/abs/2506.05598</link>
<guid>https://arxiv.org/abs/2506.05598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过用户交互生成合成人格以提升个性化奖励模型准确性的方法。</p><br /><br /><p><strong>摘要：</strong> 近期对大型语言模型多样对齐的需求推动了适应不同用户偏好的研究，但现有个性化奖励模型大多依赖额外的身份信息。本文介绍SynthesizeMe方法，通过用户交互生成合成人格，用于个性化奖励建模。该方法首先生成并验证解释用户偏好的推理，然后从这些推理中诱导出合成人格，并筛选出有意义的用户交互以构建个性化提示。实验表明，使用SynthesizeMe生成的提示可将Chatbot Arena上的个性化LLM作为评委的准确性提高4.4%，并与奖励模型结合在PersonalRewardBench上达到最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 17:23:16 GMT</pubDate>
</item>
<item>
<title>大型语言模型在策略规划中的自适应进化研究</title>
<link>https://arxiv.org/abs/2506.04651</link>
<guid>https://arxiv.org/abs/2506.04651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过多角色协作，LLM可以自主改进策略规划能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在面对需要长期战略规划的任务时的表现，并提出了一种多角色协作的自适应架构。该架构由分析者、研究者、编码者和玩家四个角色组成，共同迭代优化游戏策略。实验基于《卡坦岛》桌游展开，利用开源框架Catanatron评估了从基础游戏代理到能够自我重写代码的复杂系统。结果显示，相较于人工设计的静态代理，由LLMs驱动的自演化代理在Claude 3.7和GPT-4o等模型的支持下，展现出更强的适应性和策略调整能力，证明了LLMs在持续学习和策略优化方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 01:45:24 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在隐含推理场景中的表现分析</title>
<link>https://arxiv.org/abs/2506.00258</link>
<guid>https://arxiv.org/abs/2506.00258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示当前多模态大语言模型在隐含推理任务中常忽视潜在问题，但简单干预可显著提升表现。</p><br /><br /><p><strong>摘要：</strong> 多模态大语言模型（MLLMs）在开放性真实环境中应用时面临输入混乱、定义不清等问题。不同于精心设计的基准测试，这些场景中指令可能涉及缺失对象、矛盾事实或请求不可行操作。研究通过构建包含四种实际失败模式的诊断工具包，评估六种模型如o3和GPT-4o，发现尽管模型具备必要的感知和推理能力，却常常未能揭示隐藏问题。进一步研究表明，显式提示表明这些能力存在但常被用户合规性压制。实验表明，简单的推理阶段干预，例如谨慎的角色提示及提出澄清问题，能显著提高模型性能。研究揭示了当前MLLMs在推理能力和行为合规性之间的持续差距，并提出了使模型在非约束环境下更加可信的实用策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 17:47:28 GMT</pubDate>
</item>
<item>
<title>基于条件数分析的模型免疫框架及其应用</title>
<link>https://arxiv.org/abs/2505.23760</link>
<guid>https://arxiv.org/abs/2505.23760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过Hessian矩阵条件数分析模型免疫性，并设计算法实现对线性和非线性模型的有效免疫。</p><br /><br /><p><strong>摘要：</strong> 本文旨在探讨如何在预训练阶段构建对有害任务难以微调但仍保留其他任务效用的模型，即模型免疫技术。尽管先前研究表明文本到图像模型可以通过免疫处理，但免疫发生的条件及免疫模型的精确定义尚不明确。为此，我们提出了一个基于Hessian矩阵条件数的框架，用于分析线性模型的免疫性。在此基础上，设计了一种带有正则化项的算法，以控制预训练后模型的条件数。实验结果表明，该算法不仅适用于线性模型，也能有效应用于非线性深度网络的免疫处理。这项研究为模型免疫提供了理论基础和实践方法，同时代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>无需重训的视觉Transformer测试时异常值抑制方法</title>
<link>https://arxiv.org/abs/2506.08010</link>
<guid>https://arxiv.org/abs/2506.08010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重训的视觉Transformer测试时异常值处理方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉Transformer中高模令牌出现的机制及其导致的嘈杂注意力图问题。通过观察发现，某些模型中的稀疏神经元会集中高模激活到异常令牌上，从而破坏下游视觉处理。现有解决方案需要重新训练模型，而我们基于发现提出了一种无需重训的方法，通过将高模激活转移到额外的未训练令牌上来模拟注册令牌的效果。实验表明，该方法生成更清洁的注意力和特征图，在多个下游视觉任务中提升性能，并达到与显式训练注册令牌模型相当的结果。此外，还将此方法扩展到现成的视觉语言模型中，提高了其可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>通过视觉游戏学习提升多模态大型语言模型的泛化推理能力</title>
<link>https://arxiv.org/abs/2506.08011</link>
<guid>https://arxiv.org/abs/2506.08011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过玩简单街机游戏来提升多模态大型语言模型的跨领域推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究受认知科学启发，提出了一种新的后训练范式——视觉游戏学习（ViGaL），通过让多模态大型语言模型（MLLMs）玩类似街机的游戏，如贪吃蛇，从而提升其多模态推理的跨领域泛化能力。实验表明，这种强化学习方法显著提高了模型在多模态数学基准测试（如MathVista）及跨学科问题集（如MMMU）上的表现，且无需接触示例解法、方程或图表。值得注意的是，该模型在多模态推理基准测试中优于专门针对多模态推理数据微调的模型，同时保持了对一般视觉基准的表现，这是许多专门模型难以实现的。这一发现揭示了合成规则型游戏作为可控可扩展预训练任务的新潜力，可以有效解锁MLLMs中的通用多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>Dreamland：结合物理模拟器与生成模型的混合世界生成框架</title>
<link>https://arxiv.org/abs/2506.08006</link>
<guid>https://arxiv.org/abs/2506.08006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Dreamland框架，提升大规模视频生成模型的可控性并优化场景编辑与AI训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Dreamland的混合世界生成框架，该框架结合了基于物理的模拟器和大规模预训练生成模型的优点，旨在解决现有大型视觉生成模型缺乏元素级可控性的缺陷。Dreamland通过设计一种分层的世界抽象表示，将像素级和对象级语义及几何信息作为中间表示，从而实现对模拟器和生成模型的有效连接。这种方法不仅增强了模型的可控性，还降低了适配成本，并支持直接使用现有的和未来的预训练生成模型。此外，研究团队构建了一个名为D3Sim的数据集，用于支持混合生成管道的训练和评估。实验结果显示，Dreamland在图像质量方面比现有基线提高了50.8%，可控性增强了17.9%，并在增强具身智能体训练方面展现出巨大潜力。未来，代码和数据将公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Temperature-Adjusted Cross-modal Attention提升文本到图像扩散模型的对齐效果</title>
<link>https://arxiv.org/abs/2506.07986</link>
<guid>https://arxiv.org/abs/2506.07986</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TACA方法解决跨模态注意力不平衡问题，显著改善文本到图像生成的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态扩散Transformer（MM-DiT）模型如FLUX在文本提示与生成内容精确对齐上的不足，分析了跨模态注意力机制中的两大问题：视觉与文本模态间标记数量失衡导致的跨模态注意力抑制，以及缺乏时间步相关的注意力权重调整。为此，我们提出了温度调节跨模态注意力（TACA），通过温度缩放和时间步相关调整实现高效动态的多模态交互平衡。TACA结合LoRA微调后，在T2I-CompBench基准上显著提升了文本到图像生成的对齐性能，同时计算开销极小。实验表明，TACA在FLUX和SD3.5等先进模型上均有效改善了对象外观、属性绑定及空间关系的对齐效果。本研究强调了平衡跨模态注意力在提高文本到图像扩散模型语义保真度中的重要性。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07986" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>OneIG-Bench：文本到图像模型的综合性评估基准</title>
<link>https://arxiv.org/abs/2506.07977</link>
<guid>https://arxiv.org/abs/2506.07977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OneIG-Bench，用于多维度评估文本到图像模型的性能。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像（T2I）模型的发展，现有基准测试暴露出缺乏全面评估的问题，特别是对推理能力、文本渲染和风格化等方面的不足。为解决这些局限性，本文引入OneIG-Bench，这是一个经过精心设计的综合评估框架，涵盖提示-图像对齐、文本渲染精度、推理生成内容、风格化及多样性等多个维度。通过结构化的评估方式，该基准能够深入分析模型性能，帮助研究者和开发者识别模型的优势与瓶颈。此外，OneIG-Bench支持灵活的评估模式，用户可以专注于特定的评估子集，而无需针对所有提示生成图像。我们的代码库和数据集现已公开，旨在促进T2I研究领域的可重复性研究和跨模型比较。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:21 GMT</pubDate>
</item>
<item>
<title>MiniCPM4：面向端侧设备的高效大型语言模型</title>
<link>https://arxiv.org/abs/2506.07900</link>
<guid>https://arxiv.org/abs/2506.07900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniCPM4通过多维度创新成为高效的端侧大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一款名为MiniCPM4的高效大型语言模型（LLM），专为端侧设备设计。为了实现这一效率，MiniCPM4在模型架构、训练数据、训练算法及推理系统四个方面进行了系统性创新。在模型架构上，提出InfLLM v2，一种可训练的稀疏注意力机制，加速长上下文处理的预填充和解码阶段；在训练数据方面，提出UltraClean和UltraChat v2，仅需8万亿训练令牌即可获得满意性能；在训练算法上，改进了ModelTunnel v2并引入chunk-wise rollout和BitCPM，提升训练效率；在推理系统中，提出CPM.cu，结合稀疏注意力、模型量化和推测采样以实现高效预填充和解码。MiniCPM4提供两种版本，分别具有0.5B和8B参数量，并在多个基准测试中表现优异，特别是在处理长序列时，MiniCPM4-8B相比Qwen3-8B有显著速度提升。此外，通过进一步适配，MiniCPM4成功支持多种应用，如可信问卷生成和工具使用等，展示了其广泛的实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:16:50 GMT</pubDate>
</item>
<item>
<title>PolyVivid：多主体视频定制框架实现精确身份控制</title>
<link>https://arxiv.org/abs/2506.07848</link>
<guid>https://arxiv.org/abs/2506.07848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多主体视频定制框架，提升身份一致性与交互效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PolyVivid的多主体视频定制框架，解决了现有视频生成模型在精细控制方面的不足，特别是在多主体身份一致性和交互上的难题。该框架通过设计基于视觉语言大模型(VLLM)的文本-图像融合模块，将视觉身份嵌入到文本空间中，确保准确对应。同时，引入基于3D-RoPE的增强模块，促进文本与图像嵌入的双向结构化融合。此外，开发了继承注意力机制的身份注入模块，有效减轻身份漂移问题。最后，构建基于多语言大模型(MLLM)的数据管道，结合多种策略生成高质量多主体数据，显著提高主体区分度。实验表明，PolyVivid在身份保真度、视频真实感及主体对齐方面表现优异，优于现有开源和商业基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 11:11:09 GMT</pubDate>
</item>
<item>
<title>Improving large language models with concept-aware fine-tuning</title>
<link>https://arxiv.org/abs/2506.07833</link>
<guid>https://arxiv.org/abs/2506.07833</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:55:00 GMT</pubDate>
</item>
<item>
<title>通过图像重建解析视觉特征编码器</title>
<link>https://arxiv.org/abs/2506.07803</link>
<guid>https://arxiv.org/abs/2506.07803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过图像重建方法解析视觉特征编码器内部表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于图像重建的新方法，用于解释视觉特征编码器的内部表示。我们比较了SigLIP和SigLIP2两个相关模型家族，发现基于图像任务预训练的编码器保留了比对比学习等非图像任务更多的图像信息。此外，该方法还能对多种视觉编码器进行排名，并揭示特征空间的操作会导致可预测的重建图像变化，其中正交旋转而非空间变换控制颜色编码。此方法适用于任何视觉编码器，有助于揭示其特征空间的内部结构。实验代码和模型权重已公开于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:32:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型对低资源语言的漏洞分析</title>
<link>https://arxiv.org/abs/2506.07645</link>
<guid>https://arxiv.org/abs/2506.07645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型在低资源语言中的潜在安全漏洞。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在处理自然语言处理任务时面临的安全挑战，特别是其对小幅度字符和词汇级攻击的敏感性。通过仅修改少量字符并结合小型代理模型计算词的重要性，研究人员成功构建出强大的廉价攻击方式，这些攻击显著影响了多种LLMs的预测结果，表明其内部安全机制存在被绕过的隐患。研究重点验证了这种攻击在波兰语等低资源语言上的有效性，并展示了其扩展到其他语言的可能性。此外，研究团队公开了创建的数据集和代码，以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 07:09:39 GMT</pubDate>
</item>
<item>
<title>通过ReLIFT提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.07527</link>
<guid>https://arxiv.org/abs/2506.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合强化学习和监督微调的ReLIFT显著提升了大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，通过强化学习（RL），大型语言模型（LLM）能够表现出复杂的推理行为如规划和自我反思。然而，当前的强化学习方法受限于基础模型的知识范围，难以超越其现有能力。为解决这一问题，我们采用监督微调（SFT）来学习强化学习无法实现的能力，从而引入新的知识和推理模式。分析显示，强化学习擅长维持并优化模型原始能力范围内的表现，而监督微调则更适用于扩展模型能力边界。受此启发，我们提出了ReLIFT（强化学习与在线微调交替训练）的新方法。该方法主要依赖强化学习进行训练，在遇到难题时收集高质量解决方案进行微调，并交替进行两种训练方式以增强模型推理能力。实验表明，ReLIFT在多个竞争级基准测试中平均提升了超过5.2分，且仅需13%的详细演示数据便优于传统方法，证明了其优越的可扩展性与潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:11:20 GMT</pubDate>
</item>
<item>
<title>SpatialLM：基于标准多模态LLM架构的3D场景理解模型</title>
<link>https://arxiv.org/abs/2506.07491</link>
<guid>https://arxiv.org/abs/2506.07491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialLM通过标准LLM架构处理点云数据并实现3D场景理解。</p><br /><br /><p><strong>摘要：</strong> SpatialLM是一种专门设计用于处理三维点云数据的大语言模型，旨在生成结构化的3D场景理解输出，例如墙壁、门、窗户等建筑元素及带语义类别的对象框。与以往依赖特定任务网络设计的方法不同，该模型采用标准的多模态大型语言模型架构，并直接从开源模型微调而来。为了训练SpatialLM，我们收集了一个包含12,328个室内场景（54,778个房间）及其真实3D标注的大规模高质量合成数据集，并对多种建模和训练决策进行了深入研究。在公共基准测试中，该模型在布局估计方面达到了最先进的性能，在3D物体检测方面也取得了具有竞争力的结果。这一成果展示了增强现代大型语言模型空间理解能力的一种可行路径，适用于增强现实、具身机器人等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 03:10:58 GMT</pubDate>
</item>
<item>
<title>CCI4.0：构建高质量双语预训练数据集及其应用</title>
<link>https://arxiv.org/abs/2506.07463</link>
<guid>https://arxiv.org/abs/2506.07463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于大规模语言模型预训练的高质量双语数据集CCI4.0。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CCI4.0的大规模双语预训练数据集，该数据集经过精心设计以提供卓越的数据质量和多样化的人类推理轨迹。数据集由两个子集组成：CCI4.0-M2-Base和CCI4.0-M2-CoT。其中，CCI4.0-M2-Base结合了精心挑选的中文网络语料库、Nemotron-CC的英文子集以及其他多样化的数学、维基百科、arxiv和代码来源。为了保证数据质量，我们提出了一个新的流水线方法，主要基于模型通过两阶段去重、多分类器质量评分和领域感知流畅性过滤来验证数据质量。此外，我们还提取了45亿个CoT（Chain-of-Thought）模板，命名为CCI4.0-M2-CoT，这些模板展示了不同的推理模式，并显著降低了幻觉的可能性。实验评估表明，在CCI4.0上预训练的语言模型在下游任务中，尤其是在数学和代码反射任务中，表现出了一致的改进。我们的研究强调了严格的数据整理和人类思维模板在提升大型语言模型性能中的关键作用，为自动处理预训练语料库提供了启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:14:19 GMT</pubDate>
</item>
<item>
<title>弱到强解码框架提升大语言模型对齐能力</title>
<link>https://arxiv.org/abs/2506.07434</link>
<guid>https://arxiv.org/abs/2506.07434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弱到强解码框架增强大语言模型对齐能力。</p><br /><br /><p><strong>摘要：</strong> 为了改善大语言模型（LLMs）生成内容可能存在的不当、虚假或无意义的问题，近年来低资源对齐方法受到关注，但高质量且对齐良好的数据获取仍具挑战性。本研究发现解码开始阶段对生成对齐响应的难度较大，因此提出弱到强解码（WSD）框架，通过小型对齐模型引导大型基础模型，先由小型模型起草对齐开头，再由大型模型完成后续内容。此外，还构建了一个名为GenerAlign的新数据集，用于微调小型Pilot-3B模型作为起草模型。实验表明，该方法在不损害下游任务性能的情况下显著提升了多种基础模型的对齐效果。同时，研究还深入分析了WSD框架的内在机制及其设置和时间效率的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 01:21:22 GMT</pubDate>
</item>
<item>
<title>通过ConfQA策略降低大语言模型事实性陈述幻觉率</title>
<link>https://arxiv.org/abs/2506.07309</link>
<guid>https://arxiv.org/abs/2506.07309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为ConfQA的微调策略，将大语言模型的事实性幻觉率降至5%以下。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ConfQA的微调策略，旨在减少大型语言模型（LLMs）在生成事实性陈述时的幻觉现象。传统方法下，LLMs的幻觉率通常在20%-40%，而通过ConfQA策略，这一比率可降至5%以下。该策略的核心在于当模型回答正确时，训练其继续给出答案；否则则训练其承认“我不确定”。此外，通过引入“仅在自信时回答”的抑制提示，以及利用知识图谱中的简单事实属性值来校准模型的信心，进一步提升了效果。基于此，研究提出了Dual Neural Knowledge框架，该框架能够根据ConfQA的置信度，在内部参数化神经知识和外部记录的符号知识之间无缝切换，不仅使准确率提升至超过95%，还减少了超过30%不必要的外部检索操作。这种方法展现了在多个事实性基准测试上的强大泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 18:51:46 GMT</pubDate>
</item>
<item>
<title>利用预训练语言模型通过上下文学习预测隐马尔可夫模型生成序列</title>
<link>https://arxiv.org/abs/2506.07298</link>
<guid>https://arxiv.org/abs/2506.07298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示预训练语言模型可通过上下文学习有效预测隐马尔可夫模型生成的数据。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了预训练大型语言模型（LLMs）通过上下文学习（ICL）对隐马尔可夫模型（HMMs）生成数据的建模能力。实验结果显示，LLMs在多种合成HMM上达到了接近理论最优的预测精度。此外，研究揭示了受HMM属性影响的新颖规模趋势，并提出了相关理论假设。针对科学家的实际应用，本文还提供了使用ICL作为复杂数据分析诊断工具的指南。在真实动物决策任务中，ICL的表现与人类设计的模型相当。这是首次证明ICL可以学习并预测HMM生成序列的研究，这一发现深化了我们对LLMs上下文学习的理解，并展示了其作为揭示复杂科学数据隐藏结构的强大工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 17:49:38 GMT</pubDate>
</item>
<item>
<title>通过内部进度编码优化大型语言模型的显式推理过程</title>
<link>https://arxiv.org/abs/2506.07240</link>
<guid>https://arxiv.org/abs/2506.07240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLMs如何调节推理长度并提出一种减少过思考的方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在显式推理过程中理解和调控推理长度的机制。首先，我们发现LLMs会在推理过程中编码进展状态，并通过交互式进度条可视化揭示了模型的规划动态。其次，我们通过对推理阶段的内部进度编码进行操作，减少了不必要的推理步骤，从而生成更加简洁且果断的推理链条。实验结果显示，这种方法可以有效缓解过思考问题，提高答案准确性，同时降低推理延迟。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 13:54:33 GMT</pubDate>
</item>
<item>
<title>GeometryZero：通过强化学习优化几何问题求解的辅助构造</title>
<link>https://arxiv.org/abs/2506.07160</link>
<guid>https://arxiv.org/abs/2506.07160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习框架GCPO，用于训练高效结合辅助构造和几何推理的模型GeometryZero。</p><br /><br /><p><strong>摘要：</strong> 近期大规模语言模型(LLMs)在数学推理领域表现出色，但在几何问题求解方面仍具挑战性，尤其是辅助构造的重要性。现有方法要么性能欠佳，要么依赖庞大的LLMs导致高计算成本。我们提出了一种基于可验证奖励的强化学习方向，但直接应用存在局限性。为此，我们设计了Group Contrastive Policy Optimization(GCPO)，创新性地引入上下文效用自适应奖励和长度奖励，开发出GeometryZero模型家族，其在多个基准测试中显著优于现有方法，平均提升4.29%。这项研究为更经济高效的几何推理模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:18:15 GMT</pubDate>
</item>
<item>
<title>大型推理模型的性能与局限性分析</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型推理模型在复杂度超过一定阈值时会出现准确性崩溃。</p><br /><br /><p><strong>摘要：</strong> 近年来，语言模型的发展引入了大型推理模型（LRMs），这些模型在提供答案前会生成详细的推理过程，显示出在推理基准测试中的改进表现。然而，关于其基本能力、扩展特性及限制的研究仍显不足。目前的评估主要集中在数学和编码基准上，侧重最终答案的准确性，但这种评估方式容易受到污染且无法深入洞察推理路径。本研究利用可控谜题环境系统地探讨了这些空白，在保持逻辑结构一致的同时精确调整复杂度。实验结果显示，LRMs在超过特定复杂度时完全丧失准确性，且表现出反直觉的扩展极限——随着问题复杂性的增加，推理努力先上升后下降。通过与标准语言模型对比，我们发现LRMs在低、中、高复杂度任务中分别表现出不同的性能表现。此外，研究还深入分析了LRMs的计算行为和推理路径，揭示了其在精确计算和跨尺度推理中的局限性，并对模型的推理能力提出了疑问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 18:42:29 GMT</pubDate>
</item>
<item>
<title>通过元学习提升多模态大模型的小样本任务适应能力</title>
<link>https://arxiv.org/abs/2506.06905</link>
<guid>https://arxiv.org/abs/2506.06905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于元学习的方法，通过蒸馏任务相关图像特征的软提示，提升小规模多模态大模型的小样本性能。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型(LMMs)通常依赖于上下文学习(ICL)来执行新任务，但这种性能在较小的LMMs中往往不一致且非单调增长。我们假设这是由于模型被多余的图像嵌入信息所淹没，这些信息对下游任务并非必要。为解决此问题，我们提出了一种元学习方法，利用固定的任务相关图像特征集的软提示，在少量示例下进行测试时适配。为此，我们引入了一个注意力映射模块，可以轻松集成到流行的LLaVA v1.5架构中，并与软提示联合训练，使模型在低数据情况下仅需少量梯度步即可适应任务。在VL-ICL基准上的评估表明，我们的方法在面对图像扰动时始终优于ICL及相关提示微调方法，提升了视觉问答任务中的任务诱导和推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 15:37:22 GMT</pubDate>
</item>
<item>
<title>大型语言模型在上下文与记忆冲突下的表现评估框架</title>
<link>https://arxiv.org/abs/2506.06485</link>
<guid>https://arxiv.org/abs/2506.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出诊断框架评估大型语言模型在上下文与参数知识冲突中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种诊断框架，用于系统性评估大型语言模型（LLMs）在面对上下文信息与参数知识冲突时的行为。通过构建引发此类冲突的诊断数据集，并分析模型在多种任务类型上的表现，研究发现当上下文信息与模型的参数化信念一致时，模型表现更好；知识冲突对不依赖知识的任务影响较小；模型难以完全抑制内部知识，即使有明确指令；解释冲突的推理过程会增加对上下文的依赖。这些发现引发了对基于模型评估有效性的担忧，并强调了在部署LLMs时考虑知识冲突的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 15:20:23 GMT</pubDate>
</item>
<item>
<title>面向LLM推理阶段的安全保障：SAFFRON范式的提出</title>
<link>https://arxiv.org/abs/2506.06444</link>
<guid>https://arxiv.org/abs/2506.06444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对LLM推理阶段的安全性问题，本文提出了一种新的范式SAFFRON。</p><br /><br /><p><strong>摘要：</strong> 现有的安全保证研究主要集中在训练阶段对齐，以使大型语言模型（LLMs）习得安全行为。然而，近期研究表明这些方法容易受到多种越狱攻击的影响。与此同时，推理阶段的扩展显著提升了LLMs的推理能力，但其在安全保证中的应用尚未被深入探索。本研究填补了这一空白，开创性地提出了推理阶段的扩展方法，以应对新兴威胁下的LLMs安全问题。我们发现，尽管传统的推理扩展技术在推理任务中表现良好，但在安全场景下却表现欠佳，甚至不如基本的Best-of-N采样方法。这种低效性归因于频繁的过程奖励模型（PRM）评估所带来的高计算开销。为解决这一问题，我们提出了SAFFRON，一种专门针对安全保证设计的新推理扩展范式。该范式的核心在于引入多分支奖励模型（MRM），大幅减少所需奖励模型评估次数。此外，我们还设计了部分监督训练目标、保守探索约束以及基于Trie的数据缓存策略等关键技术。实验结果验证了SAFFRON的有效性，并公开了训练好的多分支奖励模型（Saffron-1）及相关的安全奖励数据集（Safety4M），以促进LLMs安全领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:05:45 GMT</pubDate>
</item>
<item>
<title>通过自学习训练KV缓存以降低大语言模型长上下文推理成本</title>
<link>https://arxiv.org/abs/2506.06266</link>
<guid>https://arxiv.org/abs/2506.06266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过自学习方法训练KV缓存（Cartridge），大幅降低大语言模型长上下文推理的成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通常通过将整个文本语料库放入上下文窗口并利用上下文学习（ICL）来回答基于大规模文本语料库的问题。然而，这种方法的内存消耗随着输入长度增加而增加，导致服务成本高昂。为解决这一问题，本文探索了一种替代方案：在线下为每个语料库训练一个较小的KV缓存。这种缓存被称为“Cartridge”，在推理时加载即可生成响应。尽管训练Cartridge的成本可以分摊到引用相同语料库的所有查询上，但直接使用下一个标记预测的方法未能超越ICL。因此，我们提出了自学习（self-study）方法，即生成关于语料库的合成对话，并以上下文化蒸馏为目标训练Cartridge。实验表明，采用自学习训练的Cartridge不仅能够复制ICL的功能，而且显著降低了推理成本。在具有挑战性的长上下文基准测试中，使用自学习训练的Cartridge在内存使用减少38.6倍的同时，吞吐量提高了26.4倍。此外，自学习还延长了模型的有效上下文长度，并意外地使推理时无需重新训练即可组合Cartridge。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:48:23 GMT</pubDate>
</item>
<item>
<title>Astra双模型架构提升机器人室内导航性能</title>
<link>https://arxiv.org/abs/2506.06205</link>
<guid>https://arxiv.org/abs/2506.06205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Astra双模型架构显著提高机器人在复杂室内环境中的导航成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Astra的全新双模型架构，用于解决现代机器人在多样化复杂室内环境中导航困难的问题。Astra由全局模型Astra-Global和局部模型Astra-Local组成，其中Astra-Global采用多模态大型语言模型处理视觉和语言输入，结合混合拓扑语义图实现自定位和目标定位，优于传统视觉位置识别方法；Astra-Local则是一个多任务网络，负责局部路径规划和里程估计，其4D时空编码器通过自监督学习生成鲁棒特征，规划头利用流匹配和新型掩码ESDF损失减少碰撞风险，里程计头通过变压器编码器整合多传感器输入预测机器人相对姿态。Astra部署在实际移动机器人上，在多种室内环境中实现了高成功率的任务完成率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 12:08:47 GMT</pubDate>
</item>
<item>
<title>基于语言表达动作的视觉-语言基础模型世界模型与动力学模型研究</title>
<link>https://arxiv.org/abs/2506.06006</link>
<guid>https://arxiv.org/abs/2506.06006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监督微调提升动力学模型较构建世界模型更容易。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉-语言基础模型在语言表达动作时，构建现实世界模型（观察-动作-观察）和动力学模型（观察-观察-动作）的能力。尽管开源模型在这两方面均表现不佳，但研究发现，通过监督微调获取动力学模型比获取世界模型更为容易。进一步地，动力学模型可通过两种策略助力世界模型的构建：一是利用合成数据进行弱监督学习，二是推理阶段的验证。具体而言，动力学模型可标注视频帧对之间的动作，从而扩充训练数据；同时，通过引入一种新目标函数，依据识别模型预测的重要性加权观察对中的图像标记。此外，动力学模型还能为世界模型的多个样本分配奖励，用于推理阶段的评分。通过Aurora-Bench上的动作中心图像编辑任务评估，我们最佳模型的表现与最先进的图像编辑模型相当，在GPT4o-as-judge评估的现实子集上提升了15%，并在Aurora-Bench的所有子集上获得了最佳的人类评价。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:50:18 GMT</pubDate>
</item>
<item>
<title>基于合成对话数据的实时感知任务引导对话系统</title>
<link>https://arxiv.org/abs/2506.05904</link>
<guid>https://arxiv.org/abs/2506.05904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种综合框架，解决实时感知任务引导对话系统的数据和评估难题。</p><br /><br /><p><strong>摘要：</strong> 近期会话人工智能取得了显著进展，但在开发实时感知任务引导系统方面仍面临挑战。这类系统需要根据流式视觉输入提供交互性和主动性帮助，但其开发受到昂贵且耗时的数据收集和系统评估过程的限制。为了解决这些局限性，本文提出了一个全面的框架，包含三个关键贡献：首先，引入一种新颖的数据整理管道，从标注的主观视角视频中合成对话，生成跨越多个领域的大型合成对话数据集；其次，开发了一组自动评估指标，并通过广泛的用户研究验证其有效性；最后，提出一个端到端模型，处理流式视频输入以生成上下文相关的响应，同时采用创新技术处理数据不平衡和长时间视频问题。这项工作为开发实时主动AI助手奠定了基础，能够指导用户完成多样化任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:23:29 GMT</pubDate>
</item>
<item>
<title>大型语言模型在辩论演讲评估中的表现分析</title>
<link>https://arxiv.org/abs/2506.05062</link>
<guid>https://arxiv.org/abs/2506.05062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型在评估辩论演讲中的能力表现。</p><br /><br /><p><strong>摘要：</strong> 本文引入了辩论演讲评估作为评估大型语言模型（LLMs）判断能力的新基准，该任务需要对演讲在多个层面进行深入理解，包括论证强度、连贯性及风格等。通过分析超过600份精心标注的辩论演讲数据集，我们发现虽然较大的模型在某些方面可以接近人类的判断，但在整体判断行为上存在显著差异。此外，研究还探讨了前沿LLMs生成有说服力演讲的能力，表明这些模型在这项任务上的表现可能达到人类水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 10:06:51 GMT</pubDate>
</item>
<item>
<title>MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character Recognition with over 97K Categories</title>
<link>https://arxiv.org/abs/2506.04807</link>
<guid>https://arxiv.org/abs/2506.04807</guid>
<content:encoded><![CDATA[
Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 05:33:06 GMT</pubDate>
</item>
<item>
<title>gamma-PO算法提升大语言模型对齐效率</title>
<link>https://arxiv.org/abs/2506.03690</link>
<guid>https://arxiv.org/abs/2506.03690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出gamma-PO算法优化大语言模型对齐，显著提高性能且不影响训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为gamma-PO的新算法，该算法是一种动态目标边界偏好优化方法，通过引入实例特定的边界校准，在每对偏好数据中调整奖励边界。这种方法优先处理高置信度数据对，同时抑制噪声影响，从而有效提升大语言模型（LLMs）的安全性和可靠性。gamma-PO兼容多种基于奖励边界的直接偏好优化（DPO）变体，并在AlpacaEval2和Arena-Hard等基准测试中实现了平均4.4%的性能提升，成为当前最先进的技术。此外，gamma-PO仅需少量代码改动，对训练效率几乎没有负面影响，证明了其作为增强LLMs对齐工具的稳健性。相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:19:37 GMT</pubDate>
</item>
<item>
<title>ExpertLongBench：面向专家级任务的大规模语言模型评估基准</title>
<link>https://arxiv.org/abs/2506.01241</link>
<guid>https://arxiv.org/abs/2506.01241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出包含11项任务的ExpertLongBench基准，评估现有大模型在专家级任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为ExpertLongBench的专家级基准，该基准包含来自9个领域的11项任务，这些任务反映了真实的专家工作流程和应用。不同于传统的问答任务，ExpertLongBench中的任务需要超过5000个标记的长篇输出，并且要求严格遵守领域特定的要求。每个任务都有由领域专家设计或验证的任务说明表，用于指定任务需求并指导输出评估。此外，我们提出了CLEAR评估框架，支持对本基准中长篇模型输出的准确评估。为了实现细粒度的专家对齐评估，CLEAR通过从任务特定说明表中提取信息，从模型输出和参考中衍生出检查清单。然后将模型输出的检查清单项目与参考输出的对应项目进行比较，以评估其正确性，从而实现基于事实的评估。我们对11个大型语言模型进行了基准测试，并分析了CLEAR的组成部分，结果显示现有大型语言模型在专家级任务上的表现仍需显著改进，其中顶级性能仅达到26.8%的F1得分；尽管如此，模型确实可以生成符合要求的内容，但准确性仍有待提高；同时，开放权重模型在CLEAR中准确提取和比较检查清单方面具有可扩展性和低成本的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 21:39:02 GMT</pubDate>
</item>
<item>
<title>EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions</title>
<link>https://arxiv.org/abs/2505.23473</link>
<guid>https://arxiv.org/abs/2505.23473</guid>
<content:encoded><![CDATA[
Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 10:26:46 GMT</pubDate>
</item>
<item>
<title>引入自省与纠错能力的端到端多模态GUI自动化框架</title>
<link>https://arxiv.org/abs/2506.08012</link>
<guid>https://arxiv.org/abs/2506.08012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GUI-Reflection框架，提升多模态模型的自省与错误恢复能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有图形用户界面(GUI)自动化模型缺乏自省和错误恢复能力的问题，提出了一种名为GUI-Reflection的新框架。该框架通过三个阶段的专用训练——特定于GUI的预训练、离线监督微调(SFT)和在线自省微调，将自我反思和错误纠正功能融入到端到端的多模态GUI模型中。这一过程完全基于自动化的数据生成和学习，无需人工标注。具体而言，我们首先设计了可扩展的数据管道，从现有的成功轨迹中自动生成用于反思和错误修正的数据。同时，我们还提出了GUI-Reflection任务套件，以显式地学习和评估这些面向反思的能力。此外，我们在移动设备上构建了一个多样化且高效的环境，用于GUI模型的在线训练和数据收集，并开发了一种迭代式的在线自省微调算法，使模型能够持续提升其自省和错误纠正能力。我们的方法为更健壮、适应性强且智能的GUI自动化铺平了道路，并承诺公开所有数据、模型、环境和工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>强化预训练（RPT）：语言模型与强化学习的新范式</title>
<link>https://arxiv.org/abs/2506.08007</link>
<guid>https://arxiv.org/abs/2506.08007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化预训练通过将预测下一token重构为推理任务，显著提升语言模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的扩展范式——强化预训练（Reinforcement Pre-Training, RPT），它将下一个词预测重新定义为一种可以通过强化学习训练的推理任务。该方法利用可验证的奖励机制，针对给定上下文正确预测下一个词，从而有效利用大量文本数据进行通用强化学习，而无需依赖特定领域的标注答案。实验表明，RPT不仅显著提高了语言建模的准确性，还为后续的强化微调奠定了坚实的基础。此外，随着计算资源的增加，训练效果持续改善，显示出RPT作为语言模型预训练有效且有前景的扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>小规模语言模型在长链式思维训练中的性能退化现象</title>
<link>https://arxiv.org/abs/2506.07712</link>
<guid>https://arxiv.org/abs/2506.07712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示小规模语言模型在长链式思维训练中会出现显著性能下降的现象。</p><br /><br /><p><strong>摘要：</strong> 长链式思维（Long CoT）监督是提升语言模型推理能力的常见策略，但在小规模语言模型（SLMs，≤3B参数）中，我们观察到一种名为“长CoT退化”的现象，即这些模型在有限长CoT数据集上训练时性能会大幅下降。通过Qwen2.5、LLaMA3和Gemma3系列模型的实验验证，我们发现这种退化普遍存在。例如，在某些情况下，仅基于8k长CoT样本训练的模型在微调前性能可能降低多达75%。即使增加训练样本量至220k，部分极小型模型也无法恢复或超越初始性能。进一步分析表明，这种退化源于错误累积效应，尽管长响应能增强多步推理能力，但也放大了错误传播风险。此外，长CoT退化可能对下游强化学习产生负面影响，但可通过充分的监督微调缓解。本研究挑战了关于长CoT训练对SLMs益处的传统认知，为构建更有效的中小规模推理模型提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 08:56:41 GMT</pubDate>
</item>
<item>
<title>GTR-Mol-VLM：基于图遍历机制的光学化学结构识别框架</title>
<link>https://arxiv.org/abs/2506.07553</link>
<guid>https://arxiv.org/abs/2506.07553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GTR-Mol-VLM模型提升复杂分子结构识别精度。</p><br /><br /><p><strong>摘要：</strong> 光学化学结构识别(OCSR)是将分子图像转换为机器可读格式的关键技术。本文介绍了一种名为GTR-Mol-VLM的新框架，通过引入Graph Traversal as Visual Chain of Thought机制和Faithfully Recognize原则解决了现有视觉语言模型在处理复杂分子结构时的不足。为了支持模型开发，构建了GTR-CoT-1.3M大规模指令调优数据集，并设计了MolRec-Bench基准测试。实验表明，GTR-Mol-VLM在涉及功能团缩写分子图像的场景中，比第二好的基线高出约14个百分点。本研究希望推动OCSR技术更好地服务于实际需求，促进化学生物信息学和科学人工智能领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:47:10 GMT</pubDate>
</item>
<item>
<title>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</title>
<link>https://arxiv.org/abs/2506.07530</link>
<guid>https://arxiv.org/abs/2506.07530</guid>
<content:encoded><![CDATA[
Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:15:11 GMT</pubDate>
</item>
<item>
<title>Lingshu：面向医学应用的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2506.07044</link>
<guid>https://arxiv.org/abs/2506.07044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的医学专用多模态大型语言模型Lingshu。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有医学多模态大型语言模型在医疗场景中的局限性，如知识覆盖不足、易产生幻觉及缺乏复杂推理能力等问题，提出了综合的数据整理程序和多阶段训练方法，构建了一个名为Lingshu的医学专用多模态大型语言模型。Lingshu不仅扩展了医学知识的范围，还通过强化学习技术增强了其推理能力。此外，我们开发了MedEvalKit评估框架，用于标准化评估模型性能。实验结果显示，Lingshu在多项医学任务上优于现有的开源多模态模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:30 GMT</pubDate>
</item>
<item>
<title>MIRIAD：构建高质量医疗知识库以提升大语言模型可靠性</title>
<link>https://arxiv.org/abs/2506.06091</link>
<guid>https://arxiv.org/abs/2506.06091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入MIRIAD医学问答数据集，显著提高大语言模型在医疗领域的准确性与可信度。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）有望通过高级决策支持和灵活的聊天助手革新医疗领域，但其生成的医疗内容可能存在不准确性。为解决这一问题，本文提出了MIRIAD，这是一个大规模、经过精心筛选的医学问答对数据集，包含5,821,948个问答对，这些问答对源自同行评审的医学文献并经过半自动化处理。与依赖原始未结构化文本的传统方法相比，MIRIAD以操作化的查询-回答格式组织医疗知识，从而实现更精确的知识检索。实验表明，在具有相同源数据和检索文本量的情况下，结合MIRIAD的大语言模型在困难的医学问答基准测试中的准确率提高了多达6.7%，同时在检测医疗幻觉方面的能力提升了22.5至37%（F1分数增加）。此外，还推出了MIRIAD-Atlas，这是一个交互式的知识地图，覆盖了56个医学学科，便于临床用户探索、搜索和细化医学知识。MIRIAD及其相关工具为医疗信息检索器、增强型RAG应用和基于知识的聊天界面等下游应用奠定了基础，最终推动医疗领域更可靠的LLM应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 09:52:32 GMT</pubDate>
</item>
<item>
<title>从模型中心到数据中心：AI图像生成领域的范式转变</title>
<link>https://arxiv.org/abs/2506.05673</link>
<guid>https://arxiv.org/abs/2506.05673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型领域转向数据质量驱动的发展，引入高质量图像数据集DSD。</p><br /><br /><p><strong>摘要：</strong> 现代人工智能模型，特别是在计算机视觉和图像生成任务中的扩散模型，正在经历开发方法的重大转变。过去主要依赖于复杂模型架构和超参数优化的“模型中心”方法，正逐渐被更注重数据质量、结构和相关性的“数据中心”方法取代。本文介绍了DataSeeds.AI样本数据集（DSD），该数据集包含约10,610张经过高质量人类评分和多层注释的摄影图片，旨在推动商业图像数据集的新标准。作为DataSeed.AI超过1亿张图像目录的一小部分，DSD为稳健的商业和多模态AI开发提供了可扩展的基础。通过深入分析，我们展示了DSD对特定模型在已知基准上的定量改进，并公开了评估过程中使用的代码和训练模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 21:50:28 GMT</pubDate>
</item>
<item>
<title>AI推理知识转移能力的评估与优化</title>
<link>https://arxiv.org/abs/2506.05579</link>
<guid>https://arxiv.org/abs/2506.05579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现AI模型性能与人类理解间存在不一致性，需专门优化知识转移。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能推理能力的进步显著提升了多项任务的表现，但其是否有助于更佳的知识转移尚存疑问。知识转移涉及模型以人类可理解的方式传达推理，从而促进学习和应用。为探讨这一问题，我们开发了知识集成与转移评估(KITE)框架，并开展了首个大规模人类实验(N=118)，以衡量AI与人类协作中的知识传递效果。实验分为两个阶段：首先，人类与AI共同构思解决方案策略；随后，人类独立实施解决方案。结果显示，尽管AI基准性能与合作成果之间存在一定关联，但这种关系并不稳定，存在显著异常值，表明知识转移需要针对性优化。进一步分析揭示了影响成功知识转移的行为和战略因素。我们公开了代码、数据集及评估框架，以支持未来研究交流对齐模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 16:48:16 GMT</pubDate>
</item>
<item>
<title>缓解视觉场景文本理解中的语义幻觉问题</title>
<link>https://arxiv.org/abs/2506.05551</link>
<guid>https://arxiv.org/abs/2506.05551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的框架解决大规模多模态模型的语义幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 大规模多模态模型（LMMs）在视觉感知与推理方面取得显著进展，但在处理视觉模糊或非语义场景文本时易产生语义幻觉。本文分析了语义幻觉的根本原因，并发现具有更强注意力焦点的Transformer层更不易出现此问题。为此，我们提出了一种无需训练的缓解框架，包含ZoomText策略和基于接地层修正的方法。此外，我们构建了TextHalu-Bench基准测试集，包含1730多个样本，用于评估模型的语义幻觉表现。实验表明，该方法不仅有效减少语义幻觉，还在公共基准测试中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:53:19 GMT</pubDate>
</item>
<item>
<title>基于AI工业资产生命周期管理的下一代自动化系统</title>
<link>https://arxiv.org/abs/2506.03828</link>
<guid>https://arxiv.org/abs/2506.03828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端自动化解决方案，整合工业资产全生命周期管理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了传统人工智能（AI）和机器学习（ML）方法在工业资产管理中的局限性，即仅专注于单一任务而非整体流程。通过引入AI代理和大型语言模型（LLMs），提出了实现工业资产全生命周期管理的下一代自动化机会。文中介绍了一个名为AssetOpsBench的统一框架和环境，旨在指导开发适用于第四次工业革命应用的领域特定代理。该框架强调感知、推理和控制的集成，以支持现实世界中的工业操作。AssetOpsBench软件已开源，有助于推动这一领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:57:35 GMT</pubDate>
</item>
<item>
<title>Simba: 通过层次化稀疏化提升状态空间模型性能</title>
<link>https://arxiv.org/abs/2505.20698</link>
<guid>https://arxiv.org/abs/2505.20698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Simba方法，通过层次化稀疏化改进状态空间模型，在相同计算预算下优于Mamba基线。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何通过稀疏化技术增强状态空间模型（SSMs）的性能，提出了Simba方法，该方法基于令牌剪枝对SSMs进行分层稀疏化处理。Simba在上层稀疏化更多，使上层更像信息高速公路，同时在底层保留更多细节信息。实验表明，Simba在多种自然语言任务中超越了具有相同浮点运算（FLOPS）的基线模型Mamba，不仅提高了效率，还改善了长序列中的信息流。此外，Simba引入了一种新颖的令牌剪枝标准，通过累积局部递归测量令牌对最终输出的全局影响。研究展示了Simba在自然语言处理任务中的优越性，并提供了开源代码以供进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:07:23 GMT</pubDate>
</item>
<item>
<title>STARFlow：基于归一化流的高分辨率图像合成生成模型</title>
<link>https://arxiv.org/abs/2506.06276</link>
<guid>https://arxiv.org/abs/2506.06276</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STARFlow结合归一化流和Transformer实现高效高分辨率图像合成。</p><br /><br /><p><strong>摘要：</strong> STARFlow是一种基于归一化流的可扩展生成模型，通过引入Transformer自回归流（TARFlow），在高分辨率图像合成任务上表现出色。TARFlow结合了归一化流的强大表达能力和自回归Transformer的结构建模能力，并证明了其在连续分布建模中的理论通用性。为了提升模型的可扩展性，STARFlow采用了深浅结合的设计架构，利用预训练自动编码器的潜在空间进行建模，并提出了一种新颖的引导算法以提高样本质量。作为端到端的归一化流模型，STARFlow支持连续空间下的精确最大似然训练。实验结果显示，STARFlow在条件图像生成任务中表现优异，接近当前最先进的扩散模型。本研究首次成功展示了归一化流在大规模和高分辨率图像生成中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06276" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:58:39 GMT</pubDate>
</item>
<item>
<title>Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision</title>
<link>https://arxiv.org/abs/2506.06253</link>
<guid>https://arxiv.org/abs/2506.06253</guid>
<content:encoded><![CDATA[
Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:25:48 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的编程竞赛测试用例生成系统</title>
<link>https://arxiv.org/abs/2506.05817</link>
<guid>https://arxiv.org/abs/2506.05817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用大语言模型生成高质量的编程竞赛测试用例。</p><br /><br /><p><strong>摘要：</strong> 编程竞赛因其高推理难度和精确反馈成为评估大语言模型推理能力的重要工具，但获取问题的测试用例具有挑战性。本文提出了一种基于大语言模型的代理系统，用于生成高质量的测试用例，并将其应用于CodeContests数据集，创建了改进版本CodeContests+。通过分析172万次提交记录，发现CodeContests+的测试用例准确性显著提高，尤其是True Positive Rate大幅提升。此外，在大语言模型强化学习实验中进一步验证了测试用例质量提升带来的显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 03:29:01 GMT</pubDate>
</item>
<item>
<title>PartCrafter：基于单张RGB图像的多部件3D网格联合生成模型</title>
<link>https://arxiv.org/abs/2506.05573</link>
<guid>https://arxiv.org/abs/2506.05573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PartCrafter是首个可同时生成多个语义明确且几何上不同的3D网格的结构化生成模型。</p><br /><br /><p><strong>摘要：</strong> PartCrafter是一种创新的3D生成模型，它能够从一张RGB图像中同时生成多个语义明确且几何上不同的3D网格。不同于现有的方法，PartCrafter采用了一种统一的组合生成架构，无需预先分割输入图像。该模型通过引入组合潜在空间和分层注意力机制，在生成过程中实现了全局一致性与部件细节的平衡。此外，为了支持部件级别的监督学习，研究团队创建了一个新的数据集，从中挖掘大规模3D对象数据集中的部件级标注。实验表明，PartCrafter在生成可分解的3D网格方面超越了现有方法，甚至能够生成输入图像中未直接可见的部分，展示了部件感知生成先验在3D理解和合成中的优势。未来，相关代码和训练数据将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 16:30:28 GMT</pubDate>
</item>
<item>
<title>MORSE-500：多模态推理新基准推动视觉语言模型发展</title>
<link>https://arxiv.org/abs/2506.05523</link>
<guid>https://arxiv.org/abs/2506.05523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MORSE-500视频基准测试，涵盖六类推理任务，显著提升对多模态智能的评估能力。</p><br /><br /><p><strong>摘要：</strong> 现有多模态推理基准存在三大不足：过度依赖静态图像、局限于数学问题解决、且容易饱和。本文引入MORSE-500，这是一个由500段脚本生成的视频数据集，包含嵌入式问题，覆盖抽象、物理、规划、空间及时间等六大推理类别。通过程序化生成技术，MORSE-500可精细控制视觉复杂度和时间动态，支持难度的系统性扩展。初步实验显示，当前最先进的视觉语言模型在各推理任务中存在明显性能差距，尤其在抽象和规划任务上表现欠佳。该数据集及相关工具已公开，旨在促进透明且可重复的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:12:45 GMT</pubDate>
</item>
<item>
<title>Sentinel: SOTA model to protect against prompt injections</title>
<link>https://arxiv.org/abs/2506.05446</link>
<guid>https://arxiv.org/abs/2506.05446</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 10:07:15 GMT</pubDate>
</item>
<item>
<title>Prefix Grouper：一种高效增强版Group Relative Policy Optimization算法</title>
<link>https://arxiv.org/abs/2506.05433</link>
<guid>https://arxiv.org/abs/2506.05433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Prefix Grouper通过消除冗余前缀计算提升GRPO训练效率。</p><br /><br /><p><strong>摘要：</strong> Group Relative Policy Optimization (GRPO)是一种有效的策略学习方法，但其处理长共享前缀时会引入大量冗余计算，导致训练效率低下。本文提出Prefix Grouper，通过Shared-Prefix Forward策略重构自注意力机制，仅需一次编码共享前缀，从而大幅降低计算开销。实验表明，Prefix Grouper与标准GRPO在训练等效性上表现一致，同时显著提升了长前缀场景下的训练效率。该方法具有良好的兼容性，可无缝集成到现有GRPO架构中，支持更大组规模，适用于更复杂的任务和更大模型的训练。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 05:13:37 GMT</pubDate>
</item>
<item>
<title>通过认知激活潜力优化多模态大语言模型推理能力的数据选择方法</title>
<link>https://arxiv.org/abs/2506.04755</link>
<guid>https://arxiv.org/abs/2506.04755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于认知激活潜力的新方法，显著提升多模态推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大型语言模型（MLLMs）在复杂推理任务中的训练数据需求问题，挑战了广泛认为需要大量数据的观点。研究发现，仅需一小部分具有认知价值的样本即可触发有意义的多模态推理，而其余大部分样本贡献有限。为此，我们提出了Reasoning Activation Potential（RAP）方法，通过因果差异估计器（CDE）和注意力置信估计器（ACE）识别这些认知样本，并引入难度感知替换模块（DRM）增强数据挑战性。实验结果显示，RAP方法仅使用9.3%的训练数据即取得最佳性能，同时降低了43%以上的计算成本。该研究为高效训练多模态推理模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 04:40:24 GMT</pubDate>
</item>
<item>
<title>一种结合光场渲染与物理模拟的实时机器人仿真框架</title>
<link>https://arxiv.org/abs/2506.04120</link>
<guid>https://arxiv.org/abs/2506.04120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合Gaussian Splatting与物体网格的混合场景表示方法，优化真实到模拟的转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的真实到模拟框架，旨在解决基于真实机器人运动创建精确物理模拟中的多个挑战，例如遮挡、相机姿态噪声及动态场景元素等。通过将3D高斯点绘图的光场渲染与适合物理模拟的显式物体网格融合于单一表示中，我们提出了一个端到端的优化管道，利用MuJoCo中的可微渲染与可微物理技术，联合优化场景的所有组成部分，包括物体几何形状、外观、机器人姿态和物理参数。这种方法允许同时实现高保真物体网格重建、生成逼真的新视角以及进行无需标注的机器人姿态校准。实验表明，该方法在模拟环境及实际复杂场景下均表现出色，显著提升了真实到模拟转换的实用性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:14:31 GMT</pubDate>
</item>
<item>
<title>探究多模态语言模型向全模态扩展的可行性</title>
<link>https://arxiv.org/abs/2506.01872</link>
<guid>https://arxiv.org/abs/2506.01872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态语言模型能否实现真正的全模态扩展。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前多模态语言模型在向全模态扩展时所面临的挑战，通过实验分析了扩展模态是否会影响核心的语言能力，独立训练的模态特定模型合并能否有效实现全模态能力，以及全模态扩展相比顺序扩展是否能更好地实现知识共享和泛化。研究发现，尽管现有模型在特定模态对上表现良好，但在真正实现全模态能力方面仍有显著差距，需要进一步优化以达到理想效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:01:40 GMT</pubDate>
</item>
<item>
<title>HASHIRU：一种灵活且高效的多智能体系统框架</title>
<link>https://arxiv.org/abs/2506.04255</link>
<guid>https://arxiv.org/abs/2506.04255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为HASHIRU的新多智能体系统框架，提升了灵活性与资源利用效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HASHIRU（Hierarchical Agent System for Hybrid Intelligent Resource Utilization）的新型多智能体系统（MAS）框架，旨在通过动态层级控制、资源感知的混合智能以及自主功能扩展，提升多智能体系统的灵活性、资源效率和适应性。HASHIRU采用“CEO”代理管理多个“员工”代理的方式，这些代理根据任务需求和资源限制动态实例化。其混合智能优先使用较小的本地大型语言模型（LLM），同时灵活调用外部API和更大规模的模型。该框架还设计了一个经济模型，通过雇佣/解雇成本促进团队稳定性和资源高效分配，并具备自主API工具创建及记忆功能。在学术论文评审、安全评估及复杂推理等任务中的测试表明，HASHIRU表现出色，例如在GSM8K任务上比Gemini 2.0 Flash高出35个百分点。此外，案例研究展示了系统通过自主成本模型生成、工具集成和预算管理实现自我改进的能力。HASHIRU的源代码和基准测试分别托管于GitHub和HashiruAgentX网站，提供开源访问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 13:33:16 GMT</pubDate>
</item>
<item>
<title>GUIDEX：提升零样本信息抽取性能的新方法</title>
<link>https://arxiv.org/abs/2506.00649</link>
<guid>https://arxiv.org/abs/2506.00649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUIDEX通过自动定义领域特定模式提升大模型跨域信息抽取表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GUIDEX的新方法，该方法通过自动定义领域特定的模式、推导指南并生成合成标注实例，从而实现更好的跨域泛化能力。GUIDEX方法显著提升了零样本命名实体识别任务的表现，在七个基准测试中达到了新的技术水平。与之前的方法相比，使用GUIDEX训练的模型在没有人工标注数据的情况下提高了多达7个F1点数，而结合人工标注数据后进一步提升了近2个F1点数。此外，GUIDEX还增强了模型对复杂领域特定标注模式的理解能力。目前，GUIDEX的相关代码、模型及合成数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 13:36:18 GMT</pubDate>
</item>
<item>
<title>EverGreenQA：首个带时间稳定性标签的多语言问答数据集及其应用</title>
<link>https://arxiv.org/abs/2505.21115</link>
<guid>https://arxiv.org/abs/2505.21115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个带有时间稳定性标签的多语言问答数据集，评估大模型对问题时间性的编码能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在问答任务中经常出现幻觉现象，而问题的时间性（永恒性或变化性）是导致这一问题的关键但尚未充分探索的因素。本文介绍了一个名为EverGreenQA的新数据集，该数据集首次为多语言问答任务提供了永恒性标签，支持评估和训练。通过此数据集，我们对12个现代LLM进行基准测试，以评估它们是否明确（通过口头判断）或隐含（通过不确定性信号）地编码了问题的时间性。此外，我们还训练了一个轻量级多语言分类器EG-E5，在该任务上实现了最先进的性能。最后，我们在三个实际应用中展示了永恒性分类的实用性，包括提升自我知识估计、过滤问答数据集以及解释GPT-4o检索行为。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:35:13 GMT</pubDate>
</item>
<item>
<title>基于3D光流的世界模型实现跨机器人操控技能迁移</title>
<link>https://arxiv.org/abs/2506.06199</link>
<guid>https://arxiv.org/abs/2506.06199</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过学习人类和机器人的操作数据，构建3D光流世界模型提升机器人操控能力。</p><br /><br /><p><strong>摘要：</strong> 长期以来，机器人操控复杂物体任务面临挑战，而人类却能轻松完成类似将杯子挂到架子上的动作，主要原因是缺乏大规模统一的数据集用于训练机器人操控技能。现有数据集多局限于简单场景中的单一动作空间记录，难以让机器人学习适用于多样化场景的不同设备统一且稳健的动作表示。受人类理解操作任务方式的启发，我们发现理解物体在三维空间中的移动方式是指导动作的关键线索，这一线索与具体形态无关，适用于人类及不同机器人。基于此，我们提出从人类和机器人操控数据中学习3D光流世界模型的方法，该模型可预测交互对象在三维空间中的未来运动轨迹，从而指导操控动作规划。为此，我们合成了一套大规模3D光流数据集ManiFlow-110k，并利用基于视频扩散的世界模型从中学习操控物理规律，生成根据语言指令条件下的3D光流轨迹。通过引入基于光流引导渲染机制，机器人能够预测最终状态并验证光流是否符合任务描述，进而获得闭环规划能力。最后，我们将预测的3D光流作为优化策略的约束条件，确定一系列机器人操控动作。大量实验表明，该方法在多种机器人操控任务中展现出强大的泛化能力和可靠的跨形态适应性，无需针对特定硬件进行训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06199" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 12:00:31 GMT</pubDate>
</item>
<item>
<title>基于音频感知大语言模型的演讲风格自动评估</title>
<link>https://arxiv.org/abs/2506.05984</link>
<guid>https://arxiv.org/abs/2506.05984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索音频感知大语言模型作为自动评委评估演讲风格的能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了利用音频感知大语言模型（ALLMs）作为自动评判工具来评估语音合成模型（SLMs）生成演讲的风格一致性，包括情感、音量、语速、词语重音、音高控制及非言语元素等方面。通过在语音指令遵循和角色扮演两项任务中的实验，发现Gemini-2.5-pro与人类评价者之间的协议程度与人类评价者之间相当，表明ALLMs可以有效评估SLMs生成的对话自然度与风格控制能力。然而，当前SLMs，即使如GPT-4o-audio，仍需改进以更好地实现对演讲风格的精准控制与自然对话生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:05:48 GMT</pubDate>
</item>
<item>
<title>基于输入依赖软提示的参数高效微调方法</title>
<link>https://arxiv.org/abs/2506.05629</link>
<guid>https://arxiv.org/abs/2506.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的软提示微调技术，提升大模型领域特定任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过软提示实现参数高效微调的方法，以解决领域特定任务中大语言模型微调成本高和技术难度大的问题。我们提出了一种名为ID-SPAM的新技术，该技术结合自注意力机制，根据输入生成动态软提示，并对不同重要性令牌进行关注。实验表明，ID-SPAM方法不仅简单高效，而且在多个任务上超越了现有最先进的方法，同时显著提高了零样本跨域迁移能力。这种方法通过少量可训练参数实现了对下游任务的灵活适配，具有重要的研究价值和实际应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 19:13:22 GMT</pubDate>
</item>
<item>
<title>FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion</title>
<link>https://arxiv.org/abs/2506.01111</link>
<guid>https://arxiv.org/abs/2506.01111</guid>
<content:encoded><![CDATA[
High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 14:29:17 GMT</pubDate>
</item>
<item>
<title>重新审视测试时间扩展定律：基于稀疏注意力的新范式</title>
<link>https://arxiv.org/abs/2506.05333</link>
<guid>https://arxiv.org/abs/2506.05333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现更大模型在测试时间扩展中的有效性被低估，提出新的Kinetics扩展定律。</p><br /><br /><p><strong>摘要：</strong> 本文从实际效率角度重新审视测试时间扩展定律，揭示小模型的有效性被高估的问题。传统研究基于计算最优性，忽略了推理策略（如Best-of-N、长CoTs）引入的内存访问瓶颈。通过分析0.6B到32B参数范围内的模型，我们提出了新的Kinetics扩展定律，该定律结合计算和内存访问成本指导资源分配。研究表明，测试时间计算在大模型上比小模型更有效，因为注意力而非参数数量成为主要成本因素。受此启发，我们提出了一种基于稀疏注意力的新扩展范式，显著提升了问题解决的准确性，并在低成本和高成本场景下均表现出色。实验结果表明，稀疏注意力模型优于密集模型，在AIME上的准确性提升超过60分（低成本）和5分（高成本）。这些发现表明稀疏注意力对于实现测试时间扩展的全部潜力至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:24 GMT</pubDate>
</item>
<item>
<title>FlowDirector：无需反演的数据空间文本驱动视频编辑框架</title>
<link>https://arxiv.org/abs/2506.05046</link>
<guid>https://arxiv.org/abs/2506.05046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种无需反演的视频编辑框架FlowDirector，显著提升视频编辑的时序一致性与结构保真度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FlowDirector的新颖文本驱动视频编辑框架，旨在解决现有基于反演技术的方法中存在的时序不一致及结构保真度下降的问题。FlowDirector通过将编辑过程建模为数据空间中的直接演化，利用常微分方程(ODE)引导视频沿其固有的时空流形平滑过渡，从而保持时间上的连贯性和结构细节。此外，为了实现局部化的可控编辑，引入了注意力引导的掩码机制来调节ODE的速度场；同时，受无分类器指导思想的启发，提出了增强指导策略，通过多候选流之间的差分信号来引导编辑轨迹，以强化语义对齐而不损害结构一致性。大量实验表明，FlowDirector在指令遵从性、时序一致性以及背景保存方面均达到了当前最佳性能，为高效且连贯的视频编辑建立了新的范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:54:40 GMT</pubDate>
</item>
<item>
<title>Search Arena: Analyzing Search-Augmented LLMs</title>
<link>https://arxiv.org/abs/2506.05334</link>
<guid>https://arxiv.org/abs/2506.05334</guid>
<content:encoded><![CDATA[
Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>基于CLIP空间的材料编辑方法MARBLE</title>
<link>https://arxiv.org/abs/2506.05313</link>
<guid>https://arxiv.org/abs/2506.05313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用CLIP空间进行材料混合与重组的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MARBLE的方法，用于通过在CLIP空间中找到材料嵌入来执行精细材料属性的混合和重新组合。该方法通过定位去噪UNet中的材料属性相关层，实现了基于示例图像的材料编辑改进。此外，通过浅层网络预测方向，可以实现对粗糙度、金属感、透明度和发光等精细材料属性的参数化控制。实验表明，MARBLE不仅能在单次前向传递中完成多种编辑，还适用于绘画场景。项目页面提供了更多细节和演示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:55:16 GMT</pubDate>
</item>
<item>
<title>FEAT：一种高效的全维度注意力Transformer用于动态医学视频合成</title>
<link>https://arxiv.org/abs/2506.04956</link>
<guid>https://arxiv.org/abs/2506.04956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FEAT模型，通过创新机制解决现有Transformer在动态医学视频合成中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文针对动态医学视频合成中空间一致性与时间动态建模的需求，提出了FEAT（Full-dimensional Efficient Attention Transformer）。该模型通过引入序列化的时空通道注意力机制、线性复杂度的注意力设计以及残差值引导模块，有效解决了现有基于Transformer方法中通道交互不足、自注意力计算复杂度高及去噪指导粗糙的问题。实验表明，FEAT-S参数量仅为当前最优模型Endora的23%，却展现出相当甚至更高的性能；而FEAT-L在多个数据集上超越所有对比方法，证明了其卓越的有效性和可扩展性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 08:31:02 GMT</pubDate>
</item>
<item>
<title>基于缩放定律的语言视觉模型CLIP与MaMMUT的比较研究</title>
<link>https://arxiv.org/abs/2506.04598</link>
<guid>https://arxiv.org/abs/2506.04598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过缩放定律对CLIP和MaMMUT进行跨尺度比较，揭示MaMMUT在扩展性和样本效率上的优势。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用缩放定律对基础模型及数据集进行比较的方法，特别是在语言-视觉学习领域，首次基于密集测量推导出对比学习方法CLIP和结合对比与描述性文本生成损失的MaMMUT的完整缩放定律。研究表明，随着规模扩大，MaMMUT展现出更强的性能提升及更高的样本效率。通过对多个下游任务（分类、检索、分割）和开放数据集（DataComp、DFN、Re-LAION）的一致性分析，进一步验证了这一结论。此外，我们展示了在恒定学习率下推导缩放定律的可能性，从而降低计算成本。本研究提供了系统评估和改进开放基础模型及数据集的重要工具，同时发布了所有预训练模型及其中间检查点，包括开放版MaMMUT-L/14，其在ImageNet-1k上的零样本准确率为80.3%。相关代码和实验数据可在指定GitHub仓库获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 23:35:59 GMT</pubDate>
</item>
<item>
<title>通过推理对齐的视觉描述优化提升多模态大语言模型性能</title>
<link>https://arxiv.org/abs/2506.04559</link>
<guid>https://arxiv.org/abs/2506.04559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法RACRO，通过优化视觉描述增强多模态大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，慢思考语言模型在复杂推理任务中展现了接近人类的反思认知能力。然而，将这些能力扩展到多模态大语言模型面临挑战，尤其是在升级底层推理器时需要高昂的重新训练成本。一种直接解决方案是解耦感知与推理，即将视觉输入转换为语言表示（如描述），然后传递给强大的文本推理器。但这种方法引入了一个关键挑战：视觉提取器生成的描述必须既忠实于图像又足够丰富，以支持准确的下游推理。为了解决这一问题，我们提出了通过描述奖励优化实现推理对齐的视觉解耦（RACRO），这是一种推理引导的强化学习策略，使提取器的描述行为与推理目标对齐。通过基于奖励的优化，RACRO显著增强了视觉定位并提取了推理优化的表示。在多模态数学和科学基准测试中，RACRO方法达到了最先进的平均性能，同时实现了卓越的可扩展性和对更先进推理语言模型的即插即用适应性，而无需昂贵的多模态重新对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 22:28:07 GMT</pubDate>
</item>
<item>
<title>大型语言模型水印技术对对齐属性的影响及改进方法</title>
<link>https://arxiv.org/abs/2506.04462</link>
<guid>https://arxiv.org/abs/2506.04462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示两种主流水印方法对大型语言模型对齐属性的影响，并提出采样方法恢复对齐。</p><br /><br /><p><strong>摘要：</strong> 本文系统分析了两种流行的大语言模型水印方法（Gumbel 和 KGW）对四个对齐模型的核心对齐属性（真实性、安全性、实用性）的影响。实验发现两种降级模式：守卫衰减（增强实用性损害安全性）和守卫放大（过度谨慎降低实用性）。这些模式源于水印引起的标记分布变化，揭示了对齐目标之间的基本矛盾。为缓解这些问题，我们提出了对齐重采样（AR），一种利用外部奖励模型恢复对齐的推理时采样方法。理论分析表明增加样本数量可提高期望奖励分数，实验证明仅需重采样2-4次即可恢复或超越基线对齐分数。此外，通过牺牲严格无失真性，我们的修改版本保证了与AR的兼容性。实验结果证明AR成功恢复了两种水印方法的基线对齐性能，同时保持了强大的水印检测能力。这项工作揭示了水印强度与模型对齐之间的重要平衡，并提供了实用的推理时解决方案，以负责任地部署水印大语言模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 17:29:07 GMT</pubDate>
</item>
<item>
<title>DOVE：一种动态视觉编码器实现高效语义特征提取</title>
<link>https://arxiv.org/abs/2506.03643</link>
<guid>https://arxiv.org/abs/2506.03643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态视觉编码器DOVE，根据图像复杂度自适应生成视觉tokens。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视觉编码器忽视图像信息量差异的问题，提出了DOVE（Dynamic Vision Encoder），它能够根据图像的复杂程度动态生成不同数量的视觉tokens以重建图像。实验表明，DOVE不仅显著减少了平均token数量，同时保持了高质量的图像重建效果。在多个线性探测和下游多模态任务中，DOVE在使用更少token的情况下表现优于现有的基于自动编码器的方法，捕捉到更具表达性的语义特征。此外，通过引入查询条件化token化，DOVE进一步提升了目标导向的语义提取效率。代码和模型权重已公开。关键词：动态编码、视觉tokens、语义特征。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 03:40:33 GMT</pubDate>
</item>
<item>
<title>同步扩散框架生成高保真手物交互视频与运动序列</title>
<link>https://arxiv.org/abs/2506.02444</link>
<guid>https://arxiv.org/abs/2506.02444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合视觉先验和动态约束的同步扩散框架，实现手物交互视频与运动的联合生成。</p><br /><br /><p><strong>摘要：</strong> 当前三维手物交互（HOI）运动生成方法依赖预定义模型和实验室捕捉数据，限制了泛化能力；而HOI视频生成方法虽注重像素级视觉保真，却常牺牲物理合理性。鉴于视觉外观与运动模式在现实世界中共享基本物理定律，本文提出一种新颖框架，在同步扩散过程中结合视觉先验和动态约束，同时生成HOI视频与运动。通过三模态自适应调制整合异构语义特征，结合3D全注意力建模模态间依赖关系，引入视觉感知的3D交互扩散模型直接生成显式交互序列，并反馈形成闭环。该架构无需依赖预定义对象模型或明确姿态指导，显著提升视频-运动一致性。实验表明，该方法在生成高保真、动态合理的HOI序列方面优于现有技术，尤其在未知真实场景中展现出显著的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 01:04:29 GMT</pubDate>
</item>
<item>
<title>自监督模型学习的语言特异性语音表征研究</title>
<link>https://arxiv.org/abs/2506.00981</link>
<guid>https://arxiv.org/abs/2506.00981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，Wav2Vec2模型在荷兰语预训练比英语或多语言预训练更能有效表征荷兰语的语音特征。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了自监督模型（如Wav2Vec2）学习到的语音表征在多大程度上具有语言特异性。尽管已有研究表明，从仅基于语音录音训练的端到端模型中可以成功解码多种语言学特征，但这些模型在特定语言上的预训练对增强语言特异性语言信息的作用尚不明确。本研究通过测试Wav2Vec2模型内部表征中荷兰语的语音和词汇信息，发现仅用荷兰语进行预训练相较于用相似量级的英语或更大规模的多语言数据进行预训练，在表征荷兰语语言特征方面表现更优。这种语言特异性优势可以通过经过训练的聚类或分类探针显著检测到，部分也可以通过零样本指标观察到。此外，这种语言特异性优势与自动语音识别下游任务的表现相一致。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:25:13 GMT</pubDate>
</item>
<item>
<title>VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos</title>
<link>https://arxiv.org/abs/2506.05349</link>
<guid>https://arxiv.org/abs/2506.05349</guid>
<content:encoded><![CDATA[
Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>FreeTimeGS：一种用于复杂动态场景重建的4D表示方法</title>
<link>https://arxiv.org/abs/2506.05348</link>
<guid>https://arxiv.org/abs/2506.05348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的4D表示方法，解决复杂运动场景的动态视图合成问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了复杂运动场景动态三维重建的挑战。近期一些研究通过在规范空间定义三维高斯基元并利用变形场将规范基元映射到观测空间，实现了实时动态视图合成。然而，这些方法在优化变形场时常常难以处理复杂运动场景。为了解决这一问题，我们提出了FreeTimeGS，这是一种新颖的4D表示方法，允许高斯基元出现在任意时间和位置。与规范高斯基元相比，我们的表示具有更强的灵活性，从而提高了对动态三维场景建模的能力。此外，我们为每个高斯基元赋予了一个运动函数，使其能够在时间上移动到相邻区域，这减少了时间冗余。实验结果表明，我们的方法在多个数据集上的渲染质量显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>通过动态内存稀疏化提升Transformer大模型推理精度</title>
<link>https://arxiv.org/abs/2506.05345</link>
<guid>https://arxiv.org/abs/2506.05345</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过压缩KV缓存实现推理时间超缩放，显著提升Transformer大模型推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在推理阶段通过增加生成序列长度或并行度来提高Transformer大语言模型推理精度的方法，但发现生成成本受制于键值（KV）缓存大小而非生成令牌数量。因此，我们探索了推理时间的超缩放方法，即通过压缩KV缓存，在相同的计算预算下生成更多令牌，从而进一步提高推理精度。然而，这种方法的成功依赖于压缩方法在高压缩比下仍能保持准确性。为使超缩放实用化，我们提出了动态内存稀疏化（DMS），这是一种新颖的KV缓存稀疏化方法，仅需1K训练步骤即可实现8倍压缩，同时优于无需训练的稀疏注意力方法。DMS通过延迟缓存令牌的淘汰，隐式合并表示并保留关键信息。实验表明，结合DMS的推理时间超缩放在多个LLM家族上有效提升了准确性，例如在AIME 24、GPQA和LiveCodeBench上分别提高了Qwen-R1 32B的平均分数9.1、7.6和9.6分。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05345" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>SeedVR2：基于扩散模型的一阶段高分辨率视频修复方法</title>
<link>https://arxiv.org/abs/2506.05301</link>
<guid>https://arxiv.org/abs/2506.05301</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种一阶段扩散模型SeedVR2，显著提升视频修复效率并保持高质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeedVR2的新一代基于扩散模型的视频修复方法，旨在解决现有视频修复技术计算成本高昂的问题。SeedVR2通过引入自适应窗口注意力机制和一系列损失函数优化，实现了高效且高质量的一阶段视频修复。实验表明，该模型在处理高分辨率视频时表现出色，性能可媲美甚至超越现有方法。研究还展示了如何利用对抗训练稳定模型，并通过特征匹配损失改进了训练过程。这些创新使得SeedVR2成为高分辨率视频修复领域的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05301" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:51:05 GMT</pubDate>
</item>
<item>
<title>基于几何引导长期空间记忆的视频世界模型一致性增强</title>
<link>https://arxiv.org/abs/2506.05284</link>
<guid>https://arxiv.org/abs/2506.05284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入几何引导的空间记忆机制提升视频世界模型的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有世界模型在处理长时间视频时因时间窗口限制导致场景一致性差的问题，受到人类记忆机制的启发，提出了一种新颖的框架，利用几何引导的长期空间记忆来增强视频世界模型的长期一致性。该框架设计了存储和检索长期空间记忆的机制，并构建了定制的数据集对具有显式存储三维记忆机制的世界模型进行训练和评估。实验结果显示，与相关基线相比，所提出的方法在生成质量、一致性以及上下文长度方面均有显著改善，为实现长期一致的视频生成奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:42:34 GMT</pubDate>
</item>
<item>
<title>Diagonal Batching优化循环记忆Transformer模型的长上下文推理</title>
<link>https://arxiv.org/abs/2506.05229</link>
<guid>https://arxiv.org/abs/2506.05229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Diagonal Batching方法，大幅提升循环记忆Transformer在长上下文推理中的效率。</p><br /><br /><p><strong>摘要：</strong> Transformer模型在处理长上下文时面临时间复杂度和内存使用的挑战，而循环记忆Transformer（RMTs）通过降低计算成本解决了这一问题，但其内存更新机制导致执行顺序受限。本文引入Diagonal Batching调度方案，在保留精确递归的同时解锁了段间并行性，显著提升了GPU推理性能。该技术无需重新训练即可应用于现有RMT模型，实验证明其在LLaMA-1B上的加速效果显著，尤其适用于超长序列输入。Diagonal Batching消除了执行瓶颈，降低了推理成本和延迟，使RMT成为实际应用中处理长上下文问题的有效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:43:48 GMT</pubDate>
</item>
<item>
<title>基于开源许可文本的大规模语言模型训练数据集Common Pile v0.1发布</title>
<link>https://arxiv.org/abs/2506.05209</link>
<guid>https://arxiv.org/abs/2506.05209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究团队发布了一个8TB开源许可文本数据集Common Pile v0.1用于训练大规模语言模型。</p><br /><br /><p><strong>摘要：</strong> 目前，大型语言模型（LLMs）的训练主要依赖大量未授权文本，这引发了知识产权侵权和伦理问题的关注。尽管利用开放许可文本进行训练是解决问题的第一步，但之前的数据收集工作得到的数据集要么规模过小，要么质量不佳，难以支持高性能LLMs的开发。为了填补这一空白，本文介绍并发布了Common Pile v0.1，这是一个由30个来源组成的8TB开源许可文本集合，涵盖了科研论文、代码、书籍、百科全书、教育材料、音频转录等多种领域内容。该数据集经过验证后，用于训练两个参数量为70亿的LLMs——Comma v0.1-1T和Comma v0.1-2T，分别基于1万亿和2万亿标记进行训练。实验表明，这两个模型在性能上可与采用相似计算预算的未经许可文本训练的LLMs（如Llama 1和Llama 2 7B）相媲美。此外，除了发布Common Pile v0.1数据集外，研究团队还公开了数据集创建所使用的代码、训练混合数据集以及Comma v0.1模型的检查点。这些成果为推动LLMs的可持续发展提供了重要的资源和支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:21:30 GMT</pubDate>
</item>
<item>
<title>基于技能感知的时间采样方法提升运动技能自动化评估</title>
<link>https://arxiv.org/abs/2506.04996</link>
<guid>https://arxiv.org/abs/2506.04996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的时间采样策略PATS，提升多视角运动技能评估精度。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前视频采样方法破坏动作连续性的问题，提出了一种名为Proficiency-Aware Temporal Sampling（PATS）的新策略。PATS通过自适应分割视频，在多个连续片段中捕捉完整的动作模式，从而实现多视角技能评估。实验表明，PATS在EgoExo4D基准测试中超越现有技术，在多种场景下显著提高评估准确性，尤其在攀岩（+26.22%）、音乐表演（+2.39%）和篮球（+1.13%）等挑战性领域表现优异。此外，PATS能灵活适应不同活动特性，展现出强大的实用性，为真实世界中的技能自动化评估提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:05:23 GMT</pubDate>
</item>
<item>
<title>Surfer-H：结合视觉语言模型的高效网络代理</title>
<link>https://arxiv.org/abs/2506.02865</link>
<guid>https://arxiv.org/abs/2506.02865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">集成VLM的Surfer-H在WebVoyager上达到92.2%的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Surfer-H，一种成本效益高的网络代理系统，它通过整合视觉语言模型（Vision-Language Models, VLM）来执行用户定义的任务。Surfer-H与Holo1协同工作，Holo1是一个新的开放权重集合的VLM，专门用于网络导航和信息提取。Holo1经过精心策划的数据源训练，包括开放访问的网络内容、合成示例和自动生成的自主数据。Holo1在通用用户界面（UI）基准测试以及新的网络UI本地化基准测试WebClick中表现出色。当由Holo1驱动时，Surfer-H在WebVoyager上的表现达到了92.2%的状态-of-the-art水平，实现了准确性与成本效益之间的帕累托最优平衡。为了加速自主系统的研发进展，我们开源了WebClick评估数据集和Holo1模型权重。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 09:29:03 GMT</pubDate>
</item>
<item>
<title>RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS</title>
<link>https://arxiv.org/abs/2506.02751</link>
<guid>https://arxiv.org/abs/2506.02751</guid>
<content:encoded><![CDATA[
3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 07:13:48 GMT</pubDate>
</item>
<item>
<title>基于鸟瞰图特征的LiDAR-相机标定模型BEVCALIB</title>
<link>https://arxiv.org/abs/2506.02587</link>
<guid>https://arxiv.org/abs/2506.02587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个利用鸟瞰图特征实现原始数据标定的模型，显著提升自动驾驶和机器人系统的多模态感知融合。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BEVCALIB的新模型，通过分别提取相机和激光雷达的鸟瞰图特征并融合至共享特征空间，实现了从原始数据进行LiDAR-相机标定的目标。为了充分利用几何信息，模型引入了新颖的特征选择器，优化了变换解码器的效率。在KITTI、NuScenes及自建数据集上的实验表明，BEVCALIB在多种噪声条件下显著优于现有方法，尤其在平移和旋转误差上平均提升了47.08%和82.32%（KITTI）以及78.17%和68.29%（NuScenes）。此外，在开源领域，该模型的性能较最佳可复现基线高出一个数量级。代码和演示结果已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:07:18 GMT</pubDate>
</item>
<item>
<title>面向自回归图像生成模型的抗再生攻击水印框架</title>
<link>https://arxiv.org/abs/2506.01011</link>
<guid>https://arxiv.org/abs/2506.01011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种针对自回归模型的新型水印框架，有效抵抗再生攻击。</p><br /><br /><p><strong>摘要：</strong> 自回归（AR）图像生成模型因其高质量合成效果而受到关注，但其潜在的滥用问题需要强大的水印技术。然而，现有的生成过程中嵌入水印的方法主要针对扩散模型设计，在潜空间嵌入水印的方式难以直接适应AR模型。此外，扩散模型的再生攻击可以有效消除这些水印。为解决这些问题，本文提出了Lexical Bias Watermarking（LBW），这是一种专门设计用于AR模型的新型水印框架，能够抵御再生攻击。LBW通过在生成过程中偏向预先定义的“绿名单”来直接将水印嵌入到标记映射中，从而实现与现有AR模型的无缝集成，并可扩展到后处理水印。为了提高对白盒攻击的安全性，每个图像的绿名单从多个绿名单池中随机采样。水印检测通过标记分布的量化和统计分析完成。大量实验表明，LBW在抵抗再生攻击方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 09:44:20 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的3D占用预测在自动驾驶中的应用</title>
<link>https://arxiv.org/abs/2505.23115</link>
<guid>https://arxiv.org/abs/2505.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用扩散模型重新定义3D占用预测任务，提升预测精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法，将3D占用网格预测视为一种生成建模任务，通过采用扩散模型来学习数据分布并结合3D场景先验知识，从而有效应对噪声数据、不完整观测及复杂结构问题。实验表明，基于扩散模型的方法不仅在预测一致性、抗噪能力上优于当前最先进的判别方法，而且在遮挡或低可见度区域的表现尤为突出。此外，改进后的预测结果显著提升了下游规划任务的效果，展示了该方法在实际自动驾驶应用中的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 01:34:22 GMT</pubDate>
</item>
<item>
<title>SparseMM：通过视觉注意力稀疏性优化多模态大语言模型推理</title>
<link>https://arxiv.org/abs/2506.05344</link>
<guid>https://arxiv.org/abs/2506.05344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLMs中仅少数注意力头参与视觉理解，并提出SparseMM加速多模态推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）如何处理视觉输入，发现仅不到5%的注意力头（称为视觉头）对视觉理解有贡献。我们设计了一个无需训练的框架，通过目标响应分析量化各注意力头的视觉相关性，从而高效识别这些视觉头。基于此发现，我们提出了SparseMM，这是一种KV缓存优化策略，根据注意力头的视觉评分分配非对称计算预算，利用视觉头的稀疏性加速MLLMs推理。与忽视视觉特性的现有KV缓存加速方法相比，SparseMM在解码过程中优先保持视觉语义。广泛的主流多模态基准评估表明，SparseMM在实现1.38倍实时加速和52%内存减少的同时，保持了性能一致性。我们的项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>MINT-CoT：引入数学嵌入标记的视觉推理链式思维方法</title>
<link>https://arxiv.org/abs/2506.05331</link>
<guid>https://arxiv.org/abs/2506.05331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MINT-CoT模型，通过嵌入视觉标记提升多模态数学问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型在多模态数学问题解决中的局限性，提出了一种名为MINT-CoT的新方法。MINT-CoT通过引入数学嵌入标记（Mathematical INterleaved Tokens），实现了视觉信息与文本推理的动态整合，从而改进了数学问题的视觉推理能力。该方法利用Interleave Token动态选择数学图形中的任意形状区域，有效解决了传统方法依赖粗粒度图像区域、视觉编码器对数学内容感知有限以及对外部视觉修改能力的依赖等问题。为了支持这一能力，我们构建了一个包含54K道数学问题的数据集MINT-CoT，并设计了三阶段训练策略。实验表明，MINT-CoT在MathVista、GeoQA和MMStar等基准测试中显著优于基线模型，分别提升了34.08%、28.78%和23.2%的性能。我们的代码和数据已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>CG-AV-Counting基准与AV-Reasoner模型提升视频计数能力</title>
<link>https://arxiv.org/abs/2506.05328</link>
<guid>https://arxiv.org/abs/2506.05328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CG-AV-Counting基准和AV-Reasoner模型，解决现有视频理解模型在计数任务中的不足。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大规模语言模型在计数任务上表现欠佳，而现有的评估基准存在视频长度有限、查询集封闭、缺乏线索标注及弱多模态覆盖等问题。为了解决这些问题，本文引入了CG-AV-Counting，这是一个包含1027个多模态问题和5845个注释线索的长视频计数基准。该基准支持黑盒和白盒评估，可作为端到端和基于推理计数方法的全面测试平台。同时，我们提出了AV-Reasoner模型，通过GRPO和课程学习训练，从相关任务中泛化计数能力。实验表明，AV-Reasoner在多个基准上取得了最先进的成果，但跨领域基准上的性能提升仍需进一步研究。代码和基准现已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>基于PM-Loss的深度图优化提升3D高斯点云渲染质量</title>
<link>https://arxiv.org/abs/2506.05327</link>
<guid>https://arxiv.org/abs/2506.05327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PM-Loss正则化损失，优化深度图以提升3D高斯点云渲染效果。</p><br /><br /><p><strong>摘要：</strong> 深度图在前馈3D高斯点云（3DGS）管道中通过反投影生成3D点云，用于新视图合成，具有高效训练、已知相机姿态利用及几何估计准确等优势。然而，物体边界处的深度不连续性常导致点云碎片化或稀疏化，影响渲染质量。为此，我们引入了基于预训练Transformer预测点映射的PM-Loss新型正则化损失，尽管点映射可能不如深度图精确，但能有效增强几何平滑性，尤其是在物体边界区域。借助改进的深度图，我们的方法显著提升了多种架构和场景下的前馈3DGS性能，提供了更优质的渲染结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:58:23 GMT</pubDate>
</item>
<item>
<title>EOC-Bench：面向动态第一人称场景的对象中心认知评估基准</title>
<link>https://arxiv.org/abs/2506.05287</link>
<guid>https://arxiv.org/abs/2506.05287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EOC-Bench基准，系统评估多模态大语言模型在动态第一人称场景中的对象中心认知能力。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）的兴起推动了第一人称视觉应用的发展，但现有具身基准主要关注静态场景探索，忽视了用户交互引发的动态变化评估。为此，我们引入EOC-Bench，这是一个创新性基准，包含3,277个精心标注的问题对，分为过去、现在和未来三个时间类别，涵盖11个细粒度评价维度和3种视觉对象引用类型。我们开发了一种混合格式的人在回路标注框架，并设计了新的多尺度时间准确性指标进行开放时间评估。基于EOC-Bench，我们对多种专有、开源和对象级MLLM进行了全面评估。EOC-Bench为提升MLLMs的具身对象认知能力提供了重要工具，并为构建可靠的具身系统核心模型奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:44:12 GMT</pubDate>
</item>
<item>
<title>Rectified Point Flow: Generic Point Cloud Pose Estimation</title>
<link>https://arxiv.org/abs/2506.05282</link>
<guid>https://arxiv.org/abs/2506.05282</guid>
<content:encoded><![CDATA[
We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:36:03 GMT</pubDate>
</item>
<item>
<title>Micro-Act框架解决RAG系统中的知识冲突问题</title>
<link>https://arxiv.org/abs/2506.05278</link>
<guid>https://arxiv.org/abs/2506.05278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Micro-Act框架，通过层次化动作空间自动感知上下文复杂性，显著提升问答任务的准确性。</p><br /><br /><p><strong>摘要：</strong>  Retrieval-Augmented Generation (RAG) 系统常面临知识冲突问题，即外部检索的知识与大语言模型的内在知识相矛盾，影响下游任务如问答的表现。现有方法通常通过直接对比两种知识源来缓解冲突，但可能因过多冗长的上下文导致模型无法有效识别和解决不一致性。为了解决这一问题，本文提出Micro-Act框架，该框架具有分层动作空间，能够自动感知上下文复杂性并自适应地将每个知识源分解为一系列细粒度比较，这些比较被表示为可操作步骤，从而实现超越表面上下文的推理。通过在五个基准数据集上的大量实验表明，Micro-Act在所有数据集和三种冲突类型上均显著提高了问答准确性，特别是在时间型和语义型冲突中，表现尤为突出，而其他基线方法则表现不佳。此外，Micro-Act在非冲突问题上也表现出稳健的性能，凸显了其在实际RAG应用中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05278" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:33:02 GMT</pubDate>
</item>
<item>
<title>基于流模型的可学习潜在空间对齐框架</title>
<link>https://arxiv.org/abs/2506.05240</link>
<guid>https://arxiv.org/abs/2506.05240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用流模型作为先验的潜在空间对齐新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的框架，通过利用基于流的生成模型作为先验，实现可学习潜在空间与任意目标分布的对齐。首先，该方法通过在目标特征上预训练流模型捕获潜在分布；随后，固定流模型通过对齐损失正则化潜在空间，将流匹配目标重新表述为优化潜在的目标。理论证明显示，最小化此对齐损失提供了一个计算可行的代理目标，用于最大化潜在变量在目标分布下的变分下界。值得注意的是，该方法避免了昂贵的似然性评估和优化过程中的常微分方程求解。实验验证表明，所提出的对齐损失景观近似于目标分布的负对数似然，并在ImageNet上的大规模图像生成实验中展示了其有效性，同时进行了详细的讨论和消融研究。本框架为潜在空间对齐开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:59:53 GMT</pubDate>
</item>
<item>
<title>Qwen3 Embedding系列：文本嵌入与重排序能力的重大突破</title>
<link>https://arxiv.org/abs/2506.05176</link>
<guid>https://arxiv.org/abs/2506.05176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen3 Embedding系列显著提升多语言文本理解和生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Qwen3 Embedding系列，这是对GTE-Qwen系列的重要升级，基于Qwen3基础模型，在文本嵌入和重排序方面展现出更强的能力。通过结合大规模无监督预训练和高质量数据集上的有监督微调，该系列利用Qwen3大语言模型在多语言文本理解和生成方面的强大能力，采用创新的多阶段训练管道。此外，有效的模型合并策略进一步增强了Qwen3 Embedding系列的鲁棒性和适应性。在训练过程中，Qwen3大语言模型不仅作为骨干模型，还在多个领域和语言中合成高质量、丰富且多样化的训练数据。该系列提供了多种规模的模型（0.6B、4B、8B），适用于嵌入和重排序任务，满足不同的部署需求。实证评估表明，Qwen3 Embedding系列在多个基准测试中达到最先进水平，尤其在MTEB多语言评估基准和代码检索、跨语言检索及多语言检索等任务中表现出色。为了促进可重复研究并推动社区驱动的发展，Qwen3 Embedding模型已公开发布，采用Apache 2.0许可。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 11:49:48 GMT</pubDate>
</item>
<item>
<title>ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development</title>
<link>https://arxiv.org/abs/2506.05010</link>
<guid>https://arxiv.org/abs/2506.05010</guid>
<content:encoded><![CDATA[
We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:20:50 GMT</pubDate>
</item>
<item>
<title>开放源码社区广泛采用的Deepseek-R1-Distill系列模型性能评估波动性研究</title>
<link>https://arxiv.org/abs/2506.04734</link>
<guid>https://arxiv.org/abs/2506.04734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明Deepseek-R1-Distill系列模型性能评价受多种因素影响存在显著波动。</p><br /><br /><p><strong>摘要：</strong> 近年来，由Deepseek-R1-Distill系列模型为代表的推理模型因其在数学、科学和编程等领域的卓越表现，在开源社区中得到了广泛应用。然而，我们的研究发现，这些模型的基准评估结果受到诸多因素的影响，表现出较大的波动性。例如，微小的评估条件差异就可能导致结果出现显著变化。类似的现象也在基于Deepseek-R1-Distill系列微调的其他开源推理模型，以及QwQ-32B模型中被观察到，这使得这些模型所声称的性能提升难以可靠复现。因此，我们呼吁建立更为严格的模型性能评估范式，并提出了针对Deepseek-R1-Distill系列模型的经验性评估方法，以期为后续研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 04:09:11 GMT</pubDate>
</item>
<item>
<title>STARE基准测试：评估多模态大语言模型的空间认知能力</title>
<link>https://arxiv.org/abs/2506.04633</link>
<guid>https://arxiv.org/abs/2506.04633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STARE基准测试专注于评估AI在复杂视觉模拟推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 现有AI基准测试主要侧重于语言推理，忽视了非语言多步视觉模拟的重要性。为填补这一空白，我们开发了STARE（Spatial Transformations and Reasoning Evaluation）基准测试，用于评估多模态大型语言模型在涉及几何变换、空间推理等任务上的表现。STARE包含超过4000个任务，涵盖基础二维和三维几何变换、立方体网折叠及七巧板拼图等。实验表明，模型在简单的二维变换任务上表现良好，但在复杂的三维任务上接近随机猜测。人类在这些任务上表现出色但耗时较长，而提供中间视觉模拟可以显著提高效率。然而，模型对视觉信息的利用并不一致，部分情况下反而表现下降，显示了当前AI在处理复杂视觉推理时的局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 01:09:46 GMT</pubDate>
</item>
<item>
<title>MedAgentGYM：首个提升医学推理能力的LLM训练环境</title>
<link>https://arxiv.org/abs/2506.04405</link>
<guid>https://arxiv.org/abs/2506.04405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedAgentGYM提供129类医学任务，显著提升LLM在医疗场景中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedAgentGYM，这是首个公开可用的大型语言模型（LLM）训练环境，旨在增强基于编码的医学推理能力。该平台包含来自真实生物医学场景的72,413个任务实例，分为129个类别。每个任务都封装在一个可执行的编码环境中，具备详细的任务描述、交互反馈机制、可验证的真实标注及可扩展的训练轨迹生成。通过对超过30个LLM的广泛基准测试发现，基于商业API的模型与开源模型之间存在明显性能差距。利用MedAgentGYM，Med-Copilot-7B通过监督微调和持续强化学习实现了显著性能提升，成为一种经济实惠且保护隐私的竞争性解决方案。MedAgentGYM不仅提供了全面的基准测试，还提供了统一执行环境中的综合培训资源，有助于开发先进的基于LLM的编码助手，支持生物医学研究和实践。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 15:38:55 GMT</pubDate>
</item>
<item>
<title>RoboRefer：结合深度编码和强化微调的3D空间指代理解模型</title>
<link>https://arxiv.org/abs/2506.04308</link>
<guid>https://arxiv.org/abs/2506.04308</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboRefer模型，通过深度编码和强化学习提升机器人3D空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RoboRefer的3D感知视觉语言模型，旨在解决现有方法在复杂三维场景中精确理解空间关系及动态推理交互位置的问题。RoboRefer通过监督微调集成专用深度编码器实现精确的空间理解，并通过强化微调优化多步空间推理能力。此外，引入了一个包含2000万问答对的大规模数据集RefSpatial，用于支持模型训练。实验表明，经过监督微调的RoboRefer在空间理解方面达到最新技术水平，而强化微调进一步显著提升了性能，在RefSpatial-Bench基准测试中表现优于其他所有基线模型。RoboRefer还可与多种控制策略结合，在拥挤的真实世界场景中实现跨机器人平台的长期动态任务执行。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04308" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>基于固定大型语言模型的语言-图像对齐方法</title>
<link>https://arxiv.org/abs/2506.04209</link>
<guid>https://arxiv.org/abs/2506.04209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现仅训练图像编码器即可实现高效的语言-图像对齐。</p><br /><br /><p><strong>摘要：</strong> 当前主流的语言-图像对齐方法通常通过联合训练文本和图像编码器实现，如CLIP及其变体。然而，本文提出了一种新的方法——LIFT（Language-Image alignment with a Fixed Text encoder），即使用预训练的固定大型语言模型作为文本编码器，仅训练图像编码器。通过广泛的基准测试和消融研究，我们发现LIFT框架在涉及组合理解及长描述符场景下表现优于CLIP，同时显著提高了计算效率。这项工作首次系统性地探索了大型语言模型的文本嵌入如何指导视觉学习，并提出了学习语言对齐视觉表示的一种替代设计方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:51:56 GMT</pubDate>
</item>
<item>
<title>基于多平面和全身扫描的CT图像自动异常定位与描述方法</title>
<link>https://arxiv.org/abs/2506.03238</link>
<guid>https://arxiv.org/abs/2506.03238</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型OminiAbnorm-CT，用于自动解读CT影像中的异常情况。</p><br /><br /><p><strong>摘要：</strong> 本文旨在解决临床放射学中自动化解读CT图像特别是多平面和全身扫描中异常定位和描述的重大挑战，通过构建分类系统、贡献大规模标注数据集、开发OminiAbnorm-CT模型及建立基准评估任务，实现了对多种临床场景下异常情况的高效自动解析，显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03238" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:57:34 GMT</pubDate>
</item>
<item>
<title>StreamBP：一种高效且精确的长序列反向传播方法</title>
<link>https://arxiv.org/abs/2506.03077</link>
<guid>https://arxiv.org/abs/2506.03077</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种内存高效的反向传播方法StreamBP，显著降低长序列训练中的内存成本。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为StreamBP的新型反向传播算法，该算法通过沿序列维度进行线性分解，有效降低了存储激活值所需的内存成本，特别适用于复杂任务如长链推理。相较于传统梯度检查点技术，StreamBP在相同或更少的时间内将最大可处理序列长度提升了2.8到5.5倍。此外，StreamBP还能提升计算效率并加速多GPU分布式训练。该方法支持多种常见训练目标，例如SFT、GRPO和DPO，并已开源代码便于集成至任意Transformer模型的训练流程中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03077" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 12:54:15 GMT</pubDate>
</item>
<item>
<title>FlexPainter：一种灵活多模态引导的高质量纹理生成方法</title>
<link>https://arxiv.org/abs/2506.02620</link>
<guid>https://arxiv.org/abs/2506.02620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FlexPainter方法解决现有纹理生成中的控制灵活性和一致性问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FlexPainter的新颖纹理生成管道，旨在通过提供灵活的多模态条件引导和实现高度一致的纹理生成来克服扩散基方法的局限性。FlexPainter构建了一个共享条件嵌入空间以在不同输入模态之间进行灵活聚合，并采用基于图像的CFG方法分解结构和风格信息，从而实现基于参考图像的样式化。利用图像扩散先验中的3D知识，该框架首先使用网格表示同时生成多视角图像以增强全局理解，并在扩散采样过程中提出视图同步和自适应加权模块以进一步确保局部一致性。最后，结合3D感知纹理完成模型和纹理增强模型生成无缝且高分辨率的纹理贴图。实验表明，该框架在灵活性和生成质量方面显著优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:36:03 GMT</pubDate>
</item>
<item>
<title>基于多模态输入的高保真语音驱动虚拟人像生成框架</title>
<link>https://arxiv.org/abs/2506.00830</link>
<guid>https://arxiv.org/abs/2506.00830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架SkyReels-Audio，实现高保真长时语音驱动的人脸视频生成与编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkyReels-Audio的统一框架，用于合成高质量且时间上连贯的语音驱动人脸视频。该框架基于预训练的视频扩散变换器，支持无限长度的生成与编辑，并通过文本、图像和视频等多种模态输入实现多样化且可控的条件设置。我们采用混合课程学习策略逐步对齐音频与面部动作，从而在长视频序列中实现精细的多模态控制。为了增强局部面部一致性，引入了面部掩码损失和基于音频的分类器自由引导机制。滑动窗口去噪方法进一步融合了时间片段中的潜在表示，确保了长时间跨度和多样化身份下的视觉保真度和时间一致性。此外，构建了一个专门的数据管道来整理同步的音频、视频和文本描述三元组。综合基准评估表明，SkyReels-Audio在唇形同步准确性、身份一致性及逼真的面部动态方面表现出色，尤其是在复杂和具有挑战性的条件下。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 00:27:13 GMT</pubDate>
</item>
<item>
<title>自主代理中的上下文完整性研究：基于LLMs与强化学习的方法</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种结合LLMs与强化学习的方法，提升自主代理在上下文完整性上的表现。</p><br /><br /><p><strong>摘要：</strong> 随着自主代理在用户决策中的应用扩展，确保上下文完整性（CI）成为关键问题。本文首先通过提示大型语言模型（LLMs）显式推理CI来决定信息共享内容，随后开发了一种强化学习框架进一步培养模型实现CI所需的推理能力。实验采用合成数据集验证方法，在减少不适当信息披露的同时保持任务性能，且改进效果可迁移到具有人工标注的基准测试中，如PrivacyLens。本研究为提升AI助手隐私保护提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 17:26:21 GMT</pubDate>
</item>
<item>
<title>VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models</title>
<link>https://arxiv.org/abs/2505.23656</link>
<guid>https://arxiv.org/abs/2505.23656</guid>
<content:encoded><![CDATA[
Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:06:44 GMT</pubDate>
</item>
<item>
<title>引入可编辑几何和保真外观扩散模型用于目标对象合成</title>
<link>https://arxiv.org/abs/2505.20914</link>
<guid>https://arxiv.org/abs/2505.20914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型DGAD，实现目标对象几何编辑与外观细节保存的平衡。</p><br /><br /><p><strong>摘要：</strong> 现有通用对象合成（GOC）方法通过高级语义嵌入结合扩散模型进行几何可编辑生成，但这些嵌入仅能捕捉高层次语义线索，无法保留细粒度外观细节。本文提出解耦几何可编辑与外观保存扩散模型（DGAD），先利用语义嵌入隐式捕获所需几何变换，再通过跨注意检索机制对齐细粒度外观特征与几何编辑表示，从而实现在对象合成中的精确几何编辑和忠实外观保存。具体而言，DGAD基于CLIP/DINO衍生网络提取语义嵌入与外观保存表示，并将其解耦式集成到编码和解码管道中。首先将语义嵌入融入具备强空间推理能力的预训练扩散模型，以隐式捕获对象几何，实现灵活的对象操作并确保可编辑性；随后设计密集跨注意机制，利用隐式学习的物体几何检索并空间对齐外观特征，确保外观一致性。大量实验表明，该框架在公共基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 05:05:28 GMT</pubDate>
</item>
<item>
<title>Sounding that Object: Interactive Object-Aware Image to Audio Generation</title>
<link>https://arxiv.org/abs/2506.04214</link>
<guid>https://arxiv.org/abs/2506.04214</guid>
<content:encoded><![CDATA[
Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:57:26 GMT</pubDate>
</item>
<item>
<title>OpenThoughts项目推动开源推理模型发展</title>
<link>https://arxiv.org/abs/2506.04178</link>
<guid>https://arxiv.org/abs/2506.04178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过创建开源数据集，OpenThoughts项目训练出性能优异的推理模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，推理模型在数学、代码和科学相关基准测试中取得了显著进展，但最佳训练方法仍存争议，且多依赖私有数据集。为解决这一问题，OpenThoughts项目致力于构建开源推理数据集。其开发的OpenThoughts2-1M数据集促成了首个匹配DeepSeek-R1-Distill-32B的公共推理数据模型OpenThinker2-32B，并在AIME和LiveCodeBench等标准推理基准上表现优异。进一步优化后，OpenThoughts3数据集规模扩展至120万样本，结合QwQ-32B作为教师模型，成功训练出性能更优的OpenThinker3-7B模型，在AIME 2025、LiveCodeBench 06/24-01/25和GPQA Diamond等多个评测中取得领先成绩。所有数据集和模型均开放获取，助力推理模型研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:25:39 GMT</pubDate>
</item>
<item>
<title>HTSC-2025：基于AI的高温超导材料基准数据集发布</title>
<link>https://arxiv.org/abs/2506.03837</link>
<guid>https://arxiv.org/abs/2506.03837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布HTSC-2025基准数据集以推动AI预测高温超导材料的研究。</p><br /><br /><p><strong>摘要：</strong> 近年来，利用人工智能预测超导转变温度的研究受到广泛关注，但由于缺乏广泛接受的基准数据集，导致不同算法间的公平比较难以实现，阻碍了该领域的发展。本文介绍了一个名为HTSC-2025的新基准数据集，该数据集包含了2023年至2025年间理论物理学家基于BCS超导理论预测的高温超导材料，涵盖了多个重要的超导体系如X_2YH_6系统、MXH_3系统等。HTSC-2025数据集已开源并持续更新，为加速通过AI方法发现高温超导材料提供了重要支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 07:14:00 GMT</pubDate>
</item>
<item>
<title>CRAWLDoc：基于上下文关联的多源学术文档元数据提取方法</title>
<link>https://arxiv.org/abs/2506.03822</link>
<guid>https://arxiv.org/abs/2506.03822</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法CRAWLDoc，用于从网页链接文档中提取布局无关的元数据。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CRAWLDoc的新方法，该方法通过上下文关联对链接的网页文档进行排序，以解决因网络布局和数据格式多样性导致的元数据提取挑战。CRAWLDoc从出版物的URL开始，获取着陆页及所有相关联的资源（如PDF、ORCID资料和补充材料），并将其嵌入统一表示中。为了验证该方法的有效性，我们创建了一个包含600篇来自计算机科学领域六大顶级出版社的手动标注数据集。实验表明，CRAWLDoc在跨出版商和数据格式的情况下能够稳健地对相关文档进行排名，为从具有多种布局和格式的网页文档中改进元数据提取奠定了基础。此外，我们的源代码和数据集可通过指定链接获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03822" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:52:55 GMT</pubDate>
</item>
<item>
<title>Orak：面向多类型游戏的大规模语言模型智能体基准测试平台</title>
<link>https://arxiv.org/abs/2506.03610</link>
<guid>https://arxiv.org/abs/2506.03610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Orak基准测试平台，支持大规模语言模型在多种游戏中的训练与评估。</p><br /><br /><p><strong>摘要：</strong> 现有游戏基准测试无法满足实际需求，缺乏对不同类型游戏中多样化大规模语言模型能力的评估、复杂游戏场景中重要自主模块的研究以及预训练模型微调数据集。为此，我们提出了Orak，这是一个专注于训练和评估大规模语言模型智能体的基准测试平台。Orak涵盖了12款流行视频游戏，横跨主要游戏类型，提供了一个全面的评价框架，包括通用游戏分数排行榜、智能体战斗场所以及深入分析视觉输入状态、自主策略和微调效果等。此外，Orak引入了一种基于模型上下文协议的即插即用接口，使大规模语言模型能够无缝连接游戏并操控自主模块，同时提供了一个多样游戏类型的微调数据集。这一平台为构建通用游戏智能体奠定了基础。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:40:33 GMT</pubDate>
</item>
<item>
<title>基于位置专家的推测解码优化大语言模型推理</title>
<link>https://arxiv.org/abs/2506.03566</link>
<guid>https://arxiv.org/abs/2506.03566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出位置专家方法提升大语言模型推测解码后期预测质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有推测解码方法在后期位置上因错误累积导致草案标记预测质量下降的问题，提出了位置专家（Position Specialists, PosS）方法。该方法通过多个专注于特定位置的草案层生成标记，有效提高了后期位置的标记接受率。实验结果表明，PosS方法在Llama-3-8B-Instruct和Llama-2-13B-chat模型上，相较于基线模型平均接受了更长的标记序列并提升了加速比。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:30:30 GMT</pubDate>
</item>
<item>
<title>Video-SKoT：一种领域自适应视频推理框架</title>
<link>https://arxiv.org/abs/2506.03525</link>
<guid>https://arxiv.org/abs/2506.03525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Video-SKoT框架，通过技能感知的Chain-of-Thought推理提升复杂视频理解。</p><br /><br /><p><strong>摘要：</strong> 近期关于Chain-of-Thought（CoT）推理的研究推动了复杂视频理解的进步，但现有方法难以适应视频内容中的多样化领域特定技能（如事件检测、空间关系理解和情感理解）。针对这一问题，我们提出了Video-Skill-CoT（简称Video-SKoT），这是一种自动构建并利用技能感知的CoT监督进行领域自适应视频推理的框架。首先，我们构建基于技能的CoT注释：从训练问题中提取领域相关的推理技能，将其聚类到共享的技能分类中，并为每对视频-问题创建详细的多步CoT推理理由用于训练。其次，我们引入了技能特定的专家学习框架，每个专家模块专注于一组推理技能，并通过收集的CoT监督使用轻量级适配器进行训练。实验表明，在三个视频理解基准上，Video-SKoT始终优于强大的基线模型。此外，我们还深入分析了不同CoT注释管道和多种视频域中学到的技能之间的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03525" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:18:01 GMT</pubDate>
</item>
<item>
<title>IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2506.03150</link>
<guid>https://arxiv.org/abs/2506.03150</guid>
<content:encoded><![CDATA[
Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>CapSpeech：面向风格标注文本到语音合成的新基准</title>
<link>https://arxiv.org/abs/2506.02863</link>
<guid>https://arxiv.org/abs/2506.02863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入CapSpeech，这是首个大规模风格标注文本到语音合成相关任务的数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CapSpeech的新基准，旨在推动风格标注文本到语音合成（CapTTS）的发展。该基准包含超过1000万机器注释的音频-描述对和近36万人工注释的音频-描述对，并新增了两个专业录制的数据集用于特定任务。实验表明，无论采用自回归还是非自回归模型，CapSpeech都能实现高质量、高可懂度的语音合成。CapSpeech是目前最大的CapTTS相关任务全面标注数据集，为CapTTS系统的开发提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 09:28:55 GMT</pubDate>
</item>
<item>
<title>FLAIR：基于流模型的无训练变分框架用于逆向成像问题</title>
<link>https://arxiv.org/abs/2506.02680</link>
<guid>https://arxiv.org/abs/2506.02680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLAIR利用流模型作为先验，在逆向成像问题上优于现有扩散和流方法。</p><br /><br /><p><strong>摘要：</strong> 基于流模型的潜在生成模型（如Stable Diffusion 3）虽能生成高质量图像，但在逆向成像问题中的表现却不尽人意。主要障碍包括非线性映射、数据似然项难以处理及罕见模式恢复困难。本文提出FLAIR，一种无需训练的变分框架，通过引入无类型依赖的流匹配变分目标并结合确定性轨迹调整，有效解决上述问题。此外，FLAIR通过解耦数据保真度与正则化优化、引入时间依赖校准方案，进一步提升重建质量和样本多样性。实验表明，FLAIR在标准成像基准测试中表现卓越。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 05:29:47 GMT</pubDate>
</item>
<item>
<title>FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning</title>
<link>https://arxiv.org/abs/2506.02515</link>
<guid>https://arxiv.org/abs/2506.02515</guid>
<content:encoded><![CDATA[
Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:44:42 GMT</pubDate>
</item>
<item>
<title>小语言模型在自主AI系统中的未来前景</title>
<link>https://arxiv.org/abs/2506.02153</link>
<guid>https://arxiv.org/abs/2506.02153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">小语言模型更适合自主AI系统的多任务重复应用。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型虽表现接近人类，但在自主AI系统中，小语言模型因其高效性、经济性和特定任务适应性更具优势。本文论证了小语言模型在自主AI系统中的潜力，探讨了其在专用任务中的部署成本效益，并提出了从大型语言模型向小语言模型转换的算法。此外，文中强调了混合模型架构在需要广泛对话能力场景中的必要性，并讨论了推广小语言模型面临的障碍，呼吁行业对资源优化使用的进一步讨论。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 14:35:16 GMT</pubDate>
</item>
<item>
<title>RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents</title>
<link>https://arxiv.org/abs/2506.00618</link>
<guid>https://arxiv.org/abs/2506.00618</guid>
<content:encoded><![CDATA[
With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce RiOSWorld, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on RiOSWorld demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 12:04:59 GMT</pubDate>
</item>
<item>
<title>Segment Policy Optimization: 中文标题</title>
<link>https://arxiv.org/abs/2505.23564</link>
<guid>https://arxiv.org/abs/2505.23564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习框架SPO，优化大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用强化学习提升大型语言模型推理能力这一重要挑战。传统方法分为细粒度的Token级和粗粒度的轨迹级，前者因评价模型不准确导致估计困难，后者则面临精确归因问题。为解决这些问题，我们提出了Segment Policy Optimization (SPO)，这是一种介于两者之间的中间粒度优势估计新框架。SPO通过灵活分段、准确的段级优势估计和基于段优势的策略优化，实现了更好的平衡。具体实例化为SPO-chain和SPO-tree，分别针对短链式思维和长链式思维场景，在GSM8K和MATH500数据集上显著提升了性能，分别达到6-12和7-11个百分点的改进。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:38:19 GMT</pubDate>
</item>
<item>
<title>LayerFlow：一种统一的层感知视频生成框架</title>
<link>https://arxiv.org/abs/2506.04228</link>
<guid>https://arxiv.org/abs/2506.04228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种支持多种层感知视频生成任务的统一框架LayerFlow。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LayerFlow的统一解决方案，用于层感知的视频生成任务。该方法通过提供每层提示，生成透明前景、干净背景和混合场景的视频，并支持分解混合视频或根据给定前景生成背景等变体。基于文本到视频扩散变换器，将不同层的视频组织为子片段，并利用层嵌入区分每个片段及其对应的层感知提示。此外，针对高质量分层训练视频的缺乏，设计了一种多阶段训练策略，先用低质量视频数据训练模型，再调整运动LoRA使其兼容静态帧，最后在高质量分层图像和复制粘贴视频混合数据上训练内容LoRA。在推理过程中移除运动LoRA以生成所需层的平滑视频。这种方法在多个层感知视频生成任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Voyager：基于单张图像生成世界一致3D点云序列的视频扩散框架</title>
<link>https://arxiv.org/abs/2506.04225</link>
<guid>https://arxiv.org/abs/2506.04225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Voyager，用于从单张图像生成具有用户自定义相机路径的世界一致3D点云序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Voyager的创新视频扩散框架，该框架可以从单张图像生成世界一致的3D点云序列，并支持用户定义的相机路径进行场景探索。与现有方法不同，Voyager实现了端到端的场景生成与重建，同时保证跨帧的一致性，无需依赖传统的3D重建管道（如运动结构或多视图立体视觉）。Voyager由三个关键组件组成：1）世界一致的视频扩散模型，通过联合生成对齐的RGB和深度视频序列，在现有世界观测条件下确保全局一致性；2）长范围世界探索功能，采用高效的点剔除世界缓存和自动回归推断技术，实现迭代场景扩展并保持上下文感知的一致性；3）可扩展的数据引擎，自动化摄像机姿态估计和度量深度预测，从而无需人工标注即可生成大规模多样化的训练数据。这些设计使Voyager在视觉质量和几何准确性方面显著优于现有方法，并具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>ReVisual-R1：通过分阶段训练提升多模态大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.04207</link>
<guid>https://arxiv.org/abs/2506.04207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出分阶段训练方法显著提升了多模态大语言模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大型语言模型（MLLMs）在复杂推理任务中的表现瓶颈，分析现有强化学习（RL）方法的局限性。研究表明，有效初始化对增强推理至关重要，仅使用文本数据即可超越许多近期模型；标准GRPO方法在多模态RL中易出现梯度停滞；后续文本-only RL进一步优化推理能力。基于这些发现，我们提出了ReVisual-R1模型，在多个挑战性基准测试中达到开源7B规模MLLM的新SOTA性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:51:08 GMT</pubDate>
</item>
<item>
<title>SuperWriter-Agent：提升大语言模型长文本生成质量的新框架</title>
<link>https://arxiv.org/abs/2506.04180</link>
<guid>https://arxiv.org/abs/2506.04180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于代理的SuperWriter-Agent框架，显著提高长文本生成的连贯性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SuperWriter-Agent的新框架，旨在解决大型语言模型在长文本生成中面临的一致性、逻辑性和质量下降等问题。该框架通过引入结构化思考和规划阶段，模拟专业作家的创作过程。研究团队还构建了一个监督微调数据集训练了一个7B参数规模的SuperWriter-LM，并采用层次化的直接偏好优化方法结合蒙特卡洛树搜索进行优化。实验表明，SuperWriter-LM在多个基准测试中表现优异，超越了更大规模的基线模型，且通过消融研究验证了层次化优化的有效性，强调了引入结构化思考步骤的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:27:42 GMT</pubDate>
</item>
<item>
<title>Image Editing As Programs with Diffusion Models</title>
<link>https://arxiv.org/abs/2506.04158</link>
<guid>https://arxiv.org/abs/2506.04158</guid>
<content:encoded><![CDATA[
While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:57:24 GMT</pubDate>
</item>
<item>
<title>Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis</title>
<link>https://arxiv.org/abs/2506.04142</link>
<guid>https://arxiv.org/abs/2506.04142</guid>
<content:encoded><![CDATA[
The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:33:44 GMT</pubDate>
</item>
<item>
<title>MMR-V：视频多模态深度推理基准测试</title>
<link>https://arxiv.org/abs/2506.04141</link>
<guid>https://arxiv.org/abs/2506.04141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于评估视频多模态推理能力的新基准MMR-V。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大型语言模型在处理视频中的多帧证据定位和跨模态推理时面临挑战，因为现有视频基准主要关注理解任务，无法充分测试模型的推理能力。为了解决这一问题，我们提出了MMR-V，这是一个针对视频中多模态深度推理设计的基准测试集。该基准具有以下特点：支持长距离多帧推理、超越感知能力的推理需求、人工标注以保证任务的可靠性、以及精心设计的干扰项以减少模型捷径。MMR-V包含317个视频和1257个任务。实验结果显示，当前最先进的模型在多模态推理方面表现仍然有限，即使最佳模型o4-mini的准确率也只有52.5%。此外，当前的推理增强策略如思维链和测试时计算扩展带来的提升有限。进一步分析表明，视频多模态推理所需的思维链与文本推理中的有所不同，这也是性能提升受限的原因之一。我们希望MMR-V能激发更多关于提升多模态推理能力的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:33:41 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的自主多智能体系统的信任、风险与安全管理</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述了基于大型语言模型的自主多智能体系统中的信任、风险与安全管理。</p><br /><br /><p><strong>摘要：</strong> 本文通过结构化分析，探讨了基于大型语言模型（LLMs）的自主多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）。首先，文章概述了自主AI的概念基础及其架构差异，并介绍了支持可扩展工具使用自主性的新兴系统设计。接着，从治理、可解释性、ModelOps和隐私/安全四个支柱详细阐述了自主AI框架下的TRiSM，并提出了针对自主LLMs的独特威胁向量及全面的风险分类法，辅以案例研究展示实际应用中的漏洞。此外，还调查了分布式LLM代理系统的信任构建机制、透明度和监督技术以及最新的可解释性策略。最后，文章讨论了用于评估信任、可解释性和以人为本性能的指标，同时审视了开放基准挑战，并通过加密、对抗防御和遵守不断发展的AI法规解决安全和隐私问题，最终提出了负责任的自主AI发展路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:26:11 GMT</pubDate>
</item>
<item>
<title>Rectified Sparse Attention (ReSA)：高效长序列生成的新方法</title>
<link>https://arxiv.org/abs/2506.04108</link>
<guid>https://arxiv.org/abs/2506.04108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合稀疏注意力与密集修正的ReSA方法，大幅提升长序列生成效率且保持高质量。</p><br /><br /><p><strong>摘要：</strong> 高效生成长序列是大型语言模型面临的重要挑战。虽然近期的稀疏解码方法提高了效率，但KV缓存的不匹配问题导致误差累积，降低了生成质量。本研究提出了Rectified Sparse Attention (ReSA)，一种简单而有效的方法，通过将块稀疏注意力与周期性密集修正相结合，在固定间隔刷新KV缓存，从而控制误差累积并维持与预训练分布的一致性。实验表明，ReSA在数学推理、语言建模及检索等任务上实现了接近无损的生成质量，同时显著提升了效率，尤其在256K序列长度下的端到端速度提升达2.42倍，成为可扩展长上下文推理的实用解决方案。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:01:48 GMT</pubDate>
</item>
<item>
<title>AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment</title>
<link>https://arxiv.org/abs/2506.04089</link>
<guid>https://arxiv.org/abs/2506.04089</guid>
<content:encoded><![CDATA[
As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 11:47:07 GMT</pubDate>
</item>
<item>
<title>Rex-Thinker：通过显式推理提升物体指代任务的可解释性和可靠性</title>
<link>https://arxiv.org/abs/2506.04034</link>
<guid>https://arxiv.org/abs/2506.04034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型Rex-Thinker，将物体指代任务转化为显式推理任务，提升预测的可解释性和准确性。</p><br /><br /><p><strong>摘要：</strong> 物体指代旨在检测图像中所有符合给定自然语言描述的对象。本文认为，稳健的物体指代模型应具备可解释性与视觉内容的一致性，即能够验证预测并拒绝不匹配表达的场景。然而，现有方法多直接预测边界框，缺乏解释能力且难以拒绝无效表达。为此，我们提出了Rex-Thinker模型，将其视为显式连续推理任务。该模型首先确定候选对象实例，然后逐步评估每个候选是否满足给定描述，最终做出预测。为支持此框架，我们基于HumanRef数据集构建了大规模的CoT风格数据集HumanRef-CoT，使模型能够学习分解和可解释的推理过程。Rex-Thinker采用两阶段训练策略：先通过监督微调学习结构化推理，再利用GRPO强化学习优化精度和泛化能力。实验表明，该方法在域内任务中优于基线模型，在域外设置中也展现出更强的泛化能力和拒绝幻觉输出的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:56:57 GMT</pubDate>
</item>
<item>
<title>Adapting预训练模型以解决连续学习中的稳定性-可塑性权衡问题</title>
<link>https://arxiv.org/abs/2506.03956</link>
<guid>https://arxiv.org/abs/2506.03956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种在连续学习前适应预训练模型的新框架，平衡稳定性与可塑性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了连续学习（CL）中神经网络如何在保持现有知识（稳定性）的同时获取新知识（可塑性）的问题。尽管预训练模型（PTMs）在CL中至关重要，但冻结PTM主干以保证稳定性会限制其可塑性，而对整个PTM进行顺序微调则可能引发灾难性遗忘。为了解决这一稳定性-可塑性权衡问题，我们提出了在核心CL过程之前通过插拔式适应阶段调整PTM主干的方法（ACL）。该方法在学习新任务时通过与原始类别原型对齐嵌入并远离其他类别来增强可塑性。理论和实证研究表明，ACL在多个基准数据集和集成方法上显著提高了CL性能，提供了一个灵活的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:46:33 GMT</pubDate>
</item>
<item>
<title>Dual-Arch框架：解决连续学习中的稳定性与可塑性权衡问题</title>
<link>https://arxiv.org/abs/2506.03951</link>
<guid>https://arxiv.org/abs/2506.03951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Dual-Arch框架，在架构层面解决连续学习的稳定性与可塑性难题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了连续学习(CL)领域中稳定性与可塑性之间的矛盾，并揭示了深度网络更适合可塑性而宽网络则在稳定性上表现更佳的规律。为了解决这一架构级的矛盾，我们引入了一种名为Dual-Arch的新框架，该框架作为插件组件用于CL方法中。Dual-Arch利用两个独立且互补的子网络，分别专注于稳定性与可塑性，每个子网络都采用轻量化的专门设计。实验表明，Dual-Arch不仅提升了现有CL方法的性能，还实现了高达87%的参数紧凑性提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:40:41 GMT</pubDate>
</item>
<item>
<title>VisCode-200K：基于Python的可视化及自修正大规模指令调优数据集</title>
<link>https://arxiv.org/abs/2506.03930</link>
<guid>https://arxiv.org/abs/2506.03930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VisCode-200K数据集，显著提升可视化代码生成性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在绘图等可视化任务上常因代码正确性和视觉语义问题表现不佳。现有指令调优数据集缺乏执行导向监督且对迭代代码修正支持有限，导致绘图生成效果脆弱不可靠。本文介绍VisCode-200K，这是一个包含超过200,000个示例的大规模Python可视化及自修正指令调优数据集。该数据集来源于两个部分：一是来自开源存储库的验证绘图代码及其配对的自然语言说明和渲染图表；二是Code-Feedback中的45,000个多轮修正对话，使模型能够利用运行时反馈修正错误代码。通过在VisCode-200K上微调Qwen2.5-Coder-Instruct，我们创建了VisCoder，并在PandasPlotBench上进行评估，结果显示VisCoder在绘图生成性能上显著优于强大的开源基线模型，接近GPT-4o-mini等专有模型的表现。此外，我们采用自我调试评估协议来评估迭代修复能力，证明了基于反馈的学习在生成可执行且视觉准确代码中的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:24:44 GMT</pubDate>
</item>
<item>
<title>主动学习中的超参数空间挑战与优化研究</title>
<link>https://arxiv.org/abs/2506.03817</link>
<guid>https://arxiv.org/abs/2506.03817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨主动学习中复杂超参数空间对实验结果的影响并提出改进建议。</p><br /><br /><p><strong>摘要：</strong> 主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标注样本进行人工标注来减少人力成本的技术，但其在实际应用中仍较少被采用。主要原因在于设置过程的复杂性以及对其效果的信任不足。本文假设这些障碍的根源在于AL的庞大且未充分探索的超参数空间，该空间可能导致误导性和不可重复的结果。研究首先构建了一个包含超过460万种组合的大型超参数网格，其次记录了迄今为止最大规模AL研究的所有组合性能，并分析了每个超参数对实验结果的影响。最终，本文提供了关于各超参数影响的建议，揭示了具体AL策略实现的意外影响力，并设计了一种最小化计算开销的可重复AL实验框架，从而推动更可靠和可信的AL研究发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:41:37 GMT</pubDate>
</item>
<item>
<title>视觉拼接能力对视觉语言模型安全性的挑战</title>
<link>https://arxiv.org/abs/2506.03614</link>
<guid>https://arxiv.org/abs/2506.03614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉语言模型存在通过视觉拼接学习有害内容的风险。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型(VLMs)在训练过程中面临的安全风险，特别是在有害图像被分割成看似无害的小块并散布在多个样本中时。即使进行了数据清洗，这些片段仍可能被模型学习并通过视觉拼接能力重新组合，导致推理阶段产生不当响应。实验表明，许多开源VLMs具备这种视觉拼接能力，能够在不同粒度上从碎片化数据中重构完整信息。基于此，我们模拟了一种数据投毒攻击场景，展示了如何利用这一能力绕过常规的数据审核机制，将有害内容隐藏在看似正常的文本描述中，从而对VLM的安全性构成威胁。本研究旨在提高对这类潜在安全隐患的认识，并促进更有效的防护措施开发。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:46:06 GMT</pubDate>
</item>
<item>
<title>MiMo-VL Technical Report</title>
<link>https://arxiv.org/abs/2506.03569</link>
<guid>https://arxiv.org/abs/2506.03569</guid>
<content:encoded><![CDATA[
We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:32:54 GMT</pubDate>
</item>
<item>
<title>基于不对称双3D高斯点喷涂的野外图像三维重建</title>
<link>https://arxiv.org/abs/2506.03538</link>
<guid>https://arxiv.org/abs/2506.03538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Asymmetric Dual 3DGS，利用视觉伪影的随机性实现稳定可靠的场景几何重建。</p><br /><br /><p><strong>摘要：</strong> 野外图像的三维重建面临光照条件不一致和瞬态干扰等挑战，现有方法通常依赖启发式策略处理低质量训练数据，容易产生不稳定且不一致的重建结果及视觉伪影。本文提出Asymmetric Dual 3DGS框架，通过训练两个3D高斯点喷涂模型并施加一致性约束，鼓励可靠场景几何的收敛同时抑制伪影。为避免确认偏差导致模型陷入相似失败模式，引入分异掩码策略，应用互补掩码促进模型间非对称训练。此外，设计轻量级动态指数移动平均代理，提高训练效率。实验表明，该方法在多个具有挑战性的真实世界数据集上表现优于现有方法。代码和训练模型将公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:40:33 GMT</pubDate>
</item>
<item>
<title>DenseDPO：提升文本到视频扩散模型训练的数据效率与性能</title>
<link>https://arxiv.org/abs/2506.03517</link>
<guid>https://arxiv.org/abs/2506.03517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DenseDPO方法，优化文本到视频扩散模型的训练数据标注方式。</p><br /><br /><p><strong>摘要：</strong> 近期，Direct Preference Optimization (DPO) 被用于文本到视频扩散模型的后训练技术。然而，传统DPO方法因采用低分辨率视频对比导致运动偏见，影响模型训练效果。本文提出DenseDPO，通过生成对齐的视频对、细化标注单元及利用视觉语言模型实现自动标注，显著提升了模型在运动生成上的表现，同时保持其他指标不下降。实验表明，DenseDPO仅需三分之一的标注数据即可超越原始DPO，且自动标注的性能接近人工标注。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:06:08 GMT</pubDate>
</item>
<item>
<title>RefEdit：基于指令的复杂场景图像编辑模型</title>
<link>https://arxiv.org/abs/2506.03448</link>
<guid>https://arxiv.org/abs/2506.03448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefEdit解决了现有图像编辑方法在复杂场景中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RefEdit的新模型，专门用于处理包含多个对象的复杂场景图像编辑任务。尽管现有的基于反转和指令的图像编辑技术在单一显著对象的编辑上表现出色，但在处理复杂场景时却表现不佳。为了衡量这一差距，研究者创建了一个名为RefEdit-Bench的真实世界基准测试集，发现即使是经过大量样本训练的基础模型也难以应对。为了解决这个问题，研究团队开发了RefEdit，该模型通过可扩展的合成数据生成管道进行训练。令人印象深刻的是，RefEdit仅使用了20,000个编辑三元组就超过了那些基于Flux/SD3模型且训练于数百万数据的基线模型。广泛的评估显示，RefEdit不仅在指代表达任务上表现优异，还在传统基准测试中提升了性能，达到了开源方法中的最先进水平。我们还公开了数据集和检查点以促进可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 19:20:24 GMT</pubDate>
</item>
<item>
<title>LEAF: 提升CLIP文本编码器对抗鲁棒性的方法</title>
<link>https://arxiv.org/abs/2506.03355</link>
<guid>https://arxiv.org/abs/2506.03355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LEAF方法，显著提升CLIP文本编码器的零样本对抗精度。</p><br /><br /><p><strong>摘要：</strong> 对抗性输入攻击可能显著改变CLIP嵌入，影响依赖CLIP模型下游任务的鲁棒性，特别是文本编码器的鲁棒性尚未被充分研究。本研究填补了这一空白，提出了LEAF（Efficient Adversarial Fine-tuning Method），一种针对文本域的高效对抗微调方法，适用于大规模CLIP模型。实验表明，LEAF不仅大幅提升了文本域的零样本对抗精度，还保持了视觉域的性能。结合文本到图像扩散模型时，可提高在对抗噪声下的生成质量；应用于多模态检索任务时，能改善对抗噪声下的召回率。此外，鲁棒的文本编码器有助于通过直接优化恢复输入文本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 15:57:09 GMT</pubDate>
</item>
<item>
<title>通过一次微调释放大型语言模型的推理潜力</title>
<link>https://arxiv.org/abs/2506.03295</link>
<guid>https://arxiv.org/abs/2506.03295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示一次微调即可显著提升LLMs的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何高效释放强大多语言模型（如Qwen-Math、MiMo和Phi-4）的推理潜力。尽管强化学习（RL）能大幅提升这些模型的推理性能，但其成本高昂且不稳定。本文提出一种名为Critique Fine-Tuning（CFT）的方法，仅需针对单一问题进行微调，通过收集模型生成的多样化解决方案并由教师模型提供详尽反馈来构建批评数据集。实验结果显示，Qwen和Llama系列模型在经过CFT训练后，在多种推理任务上取得了显著进步。例如，Qwen-Math-7B-CFT在六个数学基准测试中平均提升了15%，在三个逻辑推理基准测试中提升了16%，且所需计算资源仅为传统RL方法的1/20。消融研究进一步证明了一次性CFT方法的鲁棒性。这些发现表明，CFT是一种简单、通用且高效的策略，可有效释放现代大型语言模型的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 14:35:52 GMT</pubDate>
</item>
<item>
<title>SVGenius：面向SVG处理的大规模基准测试</title>
<link>https://arxiv.org/abs/2506.03139</link>
<guid>https://arxiv.org/abs/2506.03139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVGenius提出综合基准测试以评估大型语言模型在SVG处理中的能力。</p><br /><br /><p><strong>摘要：</strong> 现有的SVG处理基准存在现实覆盖不足、复杂度分层缺乏及评估范式碎片化的问题。本文介绍SVGenius，这是一个包含2,377个查询的全面基准测试，涵盖理解、编辑和生成三个维度。SVGenius基于来自24个应用领域的现实数据，系统性地分层复杂度，并通过8个任务类别和18个指标进行评估。研究评估了22种主流模型，发现闭源模型显著优于开源模型，所有模型在复杂度增加时性能均下降，表明现有方法的根本局限性。然而，增强推理训练比单纯扩展更有效，而样式转换对所有模型类型而言最具挑战性。SVGenius为矢量图形模型开发和自动化图形设计应用提供了重要的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>Critique-GRPO：结合自然语言反馈的强化学习优化框架</title>
<link>https://arxiv.org/abs/2506.03106</link>
<guid>https://arxiv.org/abs/2506.03106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过结合自然语言与数值反馈，提出Critique-GRPO框架提升大模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了仅依赖数值反馈的强化学习在复杂推理任务中的局限性，如性能瓶颈、自我反思效果有限及持续失败等问题。研究发现，即使在性能停滞的情况下，强化学习微调模型仍可通过自然语言形式的批评生成正确改进。基于此，我们提出了Critique-GRPO框架，该框架融合自然语言与数值反馈进行策略优化，使大型语言模型在保持探索的同时，从初始响应和批评引导的改进中同步学习。实验表明，该方法在数学、STEM及通用推理任务上显著优于监督学习和传统强化学习微调方法，平均提升约4.5%-5%的pass@1分数。此外，进一步分析揭示了策略探索的两个关键见解：高熵并不总能保证高效学习，较长响应不一定带来更有效的探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:39:02 GMT</pubDate>
</item>
<item>
<title>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models</title>
<link>https://arxiv.org/abs/2506.03099</link>
<guid>https://arxiv.org/abs/2506.03099</guid>
<content:encoded><![CDATA[
In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:29:28 GMT</pubDate>
</item>
<item>
<title>基于回归模型的LLM自动评估框架</title>
<link>https://arxiv.org/abs/2506.02945</link>
<guid>https://arxiv.org/abs/2506.02945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过回归模型提升LLM评估能力的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LLM-as-a-judge的框架，该框架利用大型语言模型（LLM）自动评估另一个LLM的输出。我们提出了四种定量LLM评估器，分别针对绝对反馈和相对反馈的不同类型，展示了此框架的通用性和灵活性。这些评估器通过回归模型对现有评估器的评分进行量化调整，使其更接近人类评分。与监督微调相比，我们的框架在计算效率上更高，并且当可用的人类反馈有限时，统计效率也更强。我们在四个数据集上使用两种基础评估器进行了验证实验，结果表明，通过后处理建模，定量评估器能够有效增强现有评估器的预测能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:44:23 GMT</pubDate>
</item>
<item>
<title>LongBioBench：一种用于评估长上下文语言模型的新基准</title>
<link>https://arxiv.org/abs/2506.02921</link>
<guid>https://arxiv.org/abs/2506.02921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于人工生成传记的长上下文语言模型评估新框架LongBioBench。</p><br /><br /><p><strong>摘要：</strong> 现有的长上下文语言模型评估框架主要分为现实世界任务和合成任务两类，但两者均存在局限性。现实世界任务复杂且易受数据污染影响，而合成任务则因缺乏针与干草堆之间的连贯性而削弱其作为实际应用代理的有效性。为应对这些挑战，我们提出理想的长上下文评估框架应具备无缝上下文、可控设置和可靠评估三大特征。本研究引入LongBioBench，这是一个利用人工生成传记作为控制环境的新基准，用于评估长上下文语言模型的理解、推理和可信度。实验评估显示，大多数模型在扩展上下文长度时仍存在语义理解和基础推理的缺陷，且可信度下降。进一步分析表明，现有合成基准的设计选择如上下文非连贯性、数值型问题及缺乏干扰项等，限制了对模型长上下文能力的测试。此外，我们还发现长上下文连续预训练主要通过调整RoPE嵌入来适应扩展的上下文长度。综上所述，相比之前的合成基准，LongBioBench在模拟真实语言任务与保持可控性之间取得了更好的平衡，具有更高的可解释性和配置性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:23:06 GMT</pubDate>
</item>
<item>
<title>Beyond the Surface: Measuring Self-Preference in LLM Judgments</title>
<link>https://arxiv.org/abs/2506.02592</link>
<guid>https://arxiv.org/abs/2506.02592</guid>
<content:encoded><![CDATA[
Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:12:47 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的数据增强策略解决知识蒸馏中的协变量偏移问题</title>
<link>https://arxiv.org/abs/2506.02294</link>
<guid>https://arxiv.org/abs/2506.02294</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法利用扩散模型生成数据增强样本，提升知识蒸馏中小模型的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 大型基础模型在多种领域展现了强大的零样本能力，但在数据和模型规模受限的情况下，知识蒸馏成为将知识从大模型迁移到小模型的有效工具。然而，知识蒸馏的效果受到可用训练数据的严重限制。本文针对知识蒸馏中的常见实际问题——协变量偏移展开研究，在这种情况下，训练过程中会出现测试时不存在的虚假特征。我们探讨了当这些虚假特征未知但存在鲁棒教师时，学生模型是否也能对这些特征具有鲁棒性。为了解决这个问题，我们引入了一种基于扩散模型的新颖数据增强策略，通过最大化教师和学生之间的分歧来生成图像，从而有效地创建学生模型难以应对的挑战性样本。实验表明，我们的方法显著提高了CelebA、SpuCo Birds以及在协变量偏移下的spurious ImageNet的最差组和平均组准确性，超越了现有的基于扩散模型的数据增强基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02294" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 18:15:59 GMT</pubDate>
</item>
<item>
<title>通过神经符号代理解决流图解释难题</title>
<link>https://arxiv.org/abs/2506.01344</link>
<guid>https://arxiv.org/abs/2506.01344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的方法来提高大语言模型处理流图的可靠性。</p><br /><br /><p><strong>摘要：</strong> 流图作为决策过程可视化的重要工具，因其非线性结构和复杂的图文关系，在利用大型语言模型（LLMs）进行解析时面临挑战。现有模型常出现不存在的连接和路径，影响了物流、医疗和工程等关键领域的自动化处理可靠性。本文引入细粒度流图归因任务，通过将模型响应与流图组件关联来验证预测并增强可解释性。为此，我们开发了FlowPathAgent，这是一种神经符号代理，通过基于图的推理实现细粒度后验归因。它首先分割流图，将其转换为结构化符号图，然后采用代理方法动态交互生成归因路径。此外，我们还提出了FlowExplainBench基准，用于评估不同风格、领域和问题类型的流图归因。实验表明，FlowPathAgent显著减少了流图问答中的视觉幻觉，在FlowExplainBench数据集上比强基线高出10-14个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 02:02:41 GMT</pubDate>
</item>
<item>
<title>Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title>
<link>https://arxiv.org/abs/2506.01320</link>
<guid>https://arxiv.org/abs/2506.01320</guid>
<content:encoded><![CDATA[
We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 01:02:33 GMT</pubDate>
</item>
<item>
<title>BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.00482</link>
<guid>https://arxiv.org/abs/2506.00482</guid>
<content:encoded><![CDATA[
As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 05:24:32 GMT</pubDate>
</item>
<item>
<title>TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence</title>
<link>https://arxiv.org/abs/2505.24500</link>
<guid>https://arxiv.org/abs/2505.24500</guid>
<content:encoded><![CDATA[
Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:01:06 GMT</pubDate>
</item>
<item>
<title>DLP: Dynamic Layerwise Pruning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.23807</link>
<guid>https://arxiv.org/abs/2505.23807</guid>
<content:encoded><![CDATA[
Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 03:35:00 GMT</pubDate>
</item>
<item>
<title>DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.21541</link>
<guid>https://arxiv.org/abs/2505.21541</guid>
<content:encoded><![CDATA[
Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 12:08:04 GMT</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:48:53 GMT</pubDate>
</item>
<item>
<title>Subject Fidelity Optimization：一种提升零样本主体驱动生成主体保真度的新框架</title>
<link>https://arxiv.org/abs/2506.03621</link>
<guid>https://arxiv.org/abs/2506.03621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Subject Fidelity Optimization框架，通过对比学习增强生成模型的主体保真度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Subject Fidelity Optimization（SFO）的新颖比较学习框架，用于零样本主体驱动生成任务，旨在提高生成结果的主体保真度。不同于仅依赖正样本目标的传统监督微调方法，SFO引入合成负样本，并通过成对比较引导模型优先选择正样本。此外，为了生成有区分度且信息丰富的负样本，我们提出了条件退化负采样（CDNS），该方法通过故意破坏视觉和文本线索来实现，无需昂贵的人工标注。同时，我们重新加权扩散时间步长，使微调集中在主体细节显现的中间阶段。实验表明，结合CDNS的SFO在主体保真度和文本对齐方面显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:59:25 GMT</pubDate>
</item>
<item>
<title>基于推理控制场的大规模推理模型可控长链推理研究</title>
<link>https://arxiv.org/abs/2506.00189</link>
<guid>https://arxiv.org/abs/2506.00189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出推理控制场方法提升大规模推理模型的长链推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模推理模型（LRMs）在长链推理中面临的欠思考与过度思考问题，提出了推理控制场（RCF），这是一种测试时通过注入结构化控制信号引导推理的新方法。RCF允许模型根据给定条件调整推理努力以解决复杂任务。此外，我们构建了Control-R-4K数据集，其中包含带有详细推理过程及对应控制字段的难题。为了进一步增强推理控制，我们还提出了条件蒸馏微调（CDF）方法，训练模型（如Control-R-32B）在测试时有效调整推理努力。实验表明，该方法在AIME2024和MATH500等基准测试中达到了最先进的性能，并实现了可控的长链推理过程。这项工作为测试时可扩展推理提供了一个有效的范例。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 15:59:44 GMT</pubDate>
</item>
<item>
<title>PoseFuse3D-KI：基于3D人体引导的可控关键帧插值框架</title>
<link>https://arxiv.org/abs/2506.03119</link>
<guid>https://arxiv.org/abs/2506.03119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PoseFuse3D-KI框架，结合3D人体引导信号改进视频插值效果。</p><br /><br /><p><strong>摘要：</strong> 现有关键帧插值方法主要依赖预训练的视频扩散先验，但缺乏三维几何引导，在处理复杂的人体运动时难以生成可信的结果，且对合成动态的控制有限。本文引入PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI)，这是一种创新框架，通过将三维人体引导信号整合到扩散过程中实现以人为中心的关键帧插值(CHKI)。PoseFuse3D包含一个新颖的SMPL-X编码器，可将三维几何形状转换至二维潜在条件空间，并结合融合网络将这些三维线索与二维姿态嵌入相结合。为了评估该方法，我们构建了CHKI-Video数据集，其中标注了二维姿态和三维SMPL-X参数。实验表明，PoseFuse3D-KI在CHKI-Video上显著优于现有最先进方法，PSNR提升了9%，LPIPS降低了38%。此外，全面的消融研究证明了PoseFuse3D模型在提高插值保真度方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:50:05 GMT</pubDate>
</item>
<item>
<title>基于动态比例训练的高效语言推理方法</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需复杂标注的动态比例训练方法，显著减少推理输出的令牌数并保持准确性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过强化学习和扩展的链式思维技术取得了显著进展，但高效的语言推理，特别是在极长输出的推理阶段，仍然是研究领域的关注焦点。本文提出了一种动态比例训练管道，该方法不依赖复杂的注释或多个模型之间的插值。通过持续平衡模型系统-1和系统-2数据的权重，减少了冗余推理过程，同时保留了推理能力。我们在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上验证了这种方法，并在不同难度级别的基准测试中进行了评估。实验结果显示，该方法将输出令牌数减少了近40%，同时保持了推理的准确性。我们的代码和数据将在近期公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 05:23:41 GMT</pubDate>
</item>
<item>
<title>Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals</title>
<link>https://arxiv.org/abs/2506.02281</link>
<guid>https://arxiv.org/abs/2506.02281</guid>
<content:encoded><![CDATA[
Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 17:40:38 GMT</pubDate>
</item>
<item>
<title>LongGuide：通过任务分布引导提升长文本生成中的上下文学习性能</title>
<link>https://arxiv.org/abs/2506.01265</link>
<guid>https://arxiv.org/abs/2506.01265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示上下文学习在长文本生成任务中的不足，并提出LongGuide提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 上下文学习（ICL）是预训练大语言模型的一项重要能力，但在长文本生成任务如摘要生成中表现欠佳。本文通过实证与理论分析表明，仅依赖示例无法有效教会模型生成所需的语言和格式分布。为此，我们提出了LongGuide，它生成两种指导流：度量指南（MGs）和输出约束指南（OCGs），用于优化模型性能。实验显示，LongGuide显著提升了多种开源与闭源模型的表现，同时具备泛化性和与其他提示优化器的协同效应。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 22:35:24 GMT</pubDate>
</item>
<item>
<title>MoCA-Video：无需训练的视频语义混合框架</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的视频语义混合方法MoCA-Video。</p><br /><br /><p><strong>摘要：</strong> MoCA-Video（Motion-Aware Concept Alignment in Video）是一种无需训练的框架，旨在弥合图像领域语义混合与视频之间的差距。通过将用户提供的参考图像的语义特征注入到视频中的特定对象，同时保持原始运动和视觉背景，该方法利用对角去噪调度和类别无关分割技术，在潜在空间中检测和跟踪对象并精确控制混合对象的空间位置。此外，通过引入基于动量的语义校正和伽马残差噪声稳定技术确保帧间时间一致性。实验表明，MoCA-Video在SSIM、LPIPS等标准指标上表现优异，并提出了新的CASS（概念对齐偏移分数）度量方法，即使没有训练或微调，其性能也显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 09:28:04 GMT</pubDate>
</item>
<item>
<title>Ctrl-Crash：一种可控汽车碰撞视频生成模型</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于扩散技术的可控汽车碰撞视频生成模型Ctrl-Crash。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频扩散技术取得了显著进展，但在生成逼真的汽车碰撞场景方面仍面临挑战，主要由于大多数驾驶数据集中此类事件稀缺。为了提升交通安全研究，需要真实且可控的事故模拟。本文介绍的Ctrl-Crash是一种可控制的汽车碰撞视频生成模型，它通过边界框、碰撞类型及初始图像帧等信号进行条件生成。该方法支持反事实场景生成，使得输入的微小变化可以导致完全不同的碰撞结果。此外，在推理阶段，我们利用无分类器引导机制，对每种条件信号独立调节缩放比例，以实现精细控制。实验表明，Ctrl-Crash在定量视频质量指标（如FVD和JEDi）及定性物理真实性的人类评估中均达到最先进的性能，超越了先前基于扩散的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 17:04:38 GMT</pubDate>
</item>
<item>
<title>REAL: 通过强化学习提升大型语言模型代码生成质量</title>
<link>https://arxiv.org/abs/2505.22704</link>
<guid>https://arxiv.org/abs/2505.22704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出REAL框架，利用程序分析指导反馈提高大模型代码安全性与可维护性。</p><br /><br /><p><strong>摘要：</strong> 当前基于大型语言模型（LLMs）的代码生成技术（即vibe coding）在生产环境中应用广泛，但无法充分保障代码质量，特别是在安全性和可维护性方面存在显著不足。传统方法如监督微调和基于规则的后处理需要大量人工标注或易碎的启发式规则，难以实现规模化应用。本文提出REAL框架，采用强化学习方法，借助程序分析检测安全缺陷和可维护性问题，同时结合单元测试确保功能正确性，从而激励模型生成高质量代码。与现有方法不同，REAL无需依赖特定提示或参考代码，实现了无手动干预的自动化监督。实验表明，REAL在多个数据集和模型规模上的功能与代码质量评估中均优于最先进的方法，有效弥合了原型开发与生产级代码之间的差距，使LLMs能够在速度和质量之间取得平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:57:47 GMT</pubDate>
</item>
<item>
<title>MERIT与Coral：多条件语义检索的新突破</title>
<link>https://arxiv.org/abs/2506.03144</link>
<guid>https://arxiv.org/abs/2506.03144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出首个多语言交错多条件语义检索数据集MERIT，并设计新框架Coral改进检索性能。</p><br /><br /><p><strong>摘要：</strong> 现有语义检索研究受限于单一语言或条件，无法充分利用视觉信息的表达能力。实际应用中常涉及多条件查询与多图像检索。针对此问题，本文引入MERIT数据集，包含5种语言、13.5万产品和32万查询，覆盖7类商品。实验揭示现有模型仅关注全局语义而忽略具体条件元素的缺陷。为此，我们提出Coral框架，通过嵌入重构保存细粒度条件信息并结合对比学习提取全局语义，使性能较传统方法提升45.9%，并在8个基准测试中展现强泛化能力。本研究贡献包括新型数据集、现有模型局限性分析及创新微调框架，为未来相关研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>Dual-Expert Consistency Model：加速视频扩散模型采样并提升视觉质量</title>
<link>https://arxiv.org/abs/2506.03123</link>
<guid>https://arxiv.org/abs/2506.03123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种双专家一致性模型解决视频扩散模型蒸馏中的时间一致性问题。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视频合成领域取得了显著成果，但其迭代去噪过程导致计算开销大。尽管一致性模型在加速扩散模型方面取得进展，但直接应用于视频扩散模型时，常出现时间一致性差和细节丢失的问题。本文通过分析一致性模型的训练动态，发现蒸馏过程中存在显著的时间步优化梯度差异，阻碍了学生模型达到最优状态。为此，我们提出了参数高效的Dual-Expert Consistency Model (DCM)，其中语义专家专注于学习语义布局和运动，细节专家则专门负责细节优化。此外，我们引入时间一致性损失以提高语义专家的运动一致性，同时采用GAN和特征匹配损失增强细节专家的合成质量。实验表明，该方法在大幅减少采样步骤的同时实现了最先进的视觉质量。我们的代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>FuseLIP：基于早期融合的多模态嵌入架构</title>
<link>https://arxiv.org/abs/2506.03096</link>
<guid>https://arxiv.org/abs/2506.03096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多模态嵌入架构FuseLIP，实现文本图像统一表示。</p><br /><br /><p><strong>摘要：</strong> 对比语言图像预训练方法通常通过独立编码器对齐文本图像特征，但在处理多模态输入时需额外模块整合特征。本文介绍FuseLIP，利用离散图像标记器，采用单一Transformer模型操作扩展词汇表中的文本和图像标记，实现早期融合，使各模态在编码过程中互动并获得更丰富的表示。我们构建了新的多模态预训练和评估数据集，并设计了具有挑战性的任务。实验表明，FuseLIP在视觉问答(VQA)和文本引导图像变换检索等多模态嵌入任务上优于其他方法，同时在单模态任务上表现与基线相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:27:12 GMT</pubDate>
</item>
<item>
<title>OThink-R1：优化大型推理模型中的冗余推理</title>
<link>https://arxiv.org/abs/2506.02397</link>
<guid>https://arxiv.org/abs/2506.02397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出OThink-R1方法，通过区分冗余推理和必要推理，显著减少推理步骤并提升效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）通过扩展的链式思考（CoT）推理技术，在解决复杂任务方面取得了卓越表现。然而，我们发现这些模型在处理简单任务时所采用的复杂推理可能并非必要，因为非推理型大语言模型（LLMs）也能用较少的token解决类似问题。基于此，本文系统分析了LRMs的推理轨迹，提出了利用已识别范式和LLM-Judge对这些轨迹进行分类的方法，将推理轨迹分为冗余推理和必要推理两类。同时，我们引入了OThink-R1方法，该方法能够在保持逻辑有效性的同时修剪掉冗余的推理步骤。具体而言，OThink-R1动态切换至非推理模式（快思考）处理简单问题，而在面对复杂问题时则启用深思熟虑的推理模式（慢思考）。实验表明，OThink-R1在数学和问答任务上平均减少了23%的推理冗余，且不降低准确性。这项工作为构建高效的推理模型提供了实用指导，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 23:31:30 GMT</pubDate>
</item>
<item>
<title>Qari-OCR：基于Qwen2-VL优化的阿拉伯文光学字符识别新突破</title>
<link>https://arxiv.org/abs/2506.02295</link>
<guid>https://arxiv.org/abs/2506.02295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qari-OCR通过迭代优化，在阿拉伯文OCR领域取得显著进展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Qari-OCR的视觉语言模型系列，该系列模型基于Qwen2-VL-2B-Instruct，经过针对阿拉伯文特性的逐步迭代微调，特别是在合成数据集上的优化，成功提升了阿拉伯文光学字符识别（OCR）的性能。其中，QARI v0.2模型在带有重音符号的文本上实现了0.160的词错误率（WER）、0.061的字符错误率（CER）和0.737的BLEU评分，表现出对阿拉伯文特有的重音标记、多样字体及文档布局的强大适应能力，同时在低分辨率图像处理方面也有出色表现。进一步的研究（QARI v0.3版本）展示了该模型在结构化文档理解和手写文本识别方面的潜力。本研究显著提高了阿拉伯文OCR的准确性和效率，并开放了所有模型和数据集以推动后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 18:21:06 GMT</pubDate>
</item>
<item>
<title>基于自挑战框架的大语言模型工具使用能力增强</title>
<link>https://arxiv.org/abs/2506.01716</link>
<guid>https://arxiv.org/abs/2506.01716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自挑战框架，利用自身生成高质量任务训练模型，显著提升工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为自挑战（Self-Challenging）的框架，用于通过模型自身生成的高质量任务进行训练，从而提高智能代理的工具使用能力。该框架首先让模型扮演挑战者角色，通过与工具交互生成任务，这些任务属于代码作为任务（Code-as-Task）的新问题类别，由指令、验证函数及成功与失败案例组成，以筛选高质量任务。随后，模型切换到执行者角色，通过强化学习在这些任务上进行训练，并根据评估反馈优化性能。实验表明，即使仅使用自生成的训练数据，该框架在M3ToolEval和TauBench两个现有多轮工具使用代理基准测试中，对Llama-3.1-8B-Instruct模型的表现提升了两倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 10:23:33 GMT</pubDate>
</item>
<item>
<title>SHARE: 基于分层动作修正的文本转SQL自纠错方法</title>
<link>https://arxiv.org/abs/2506.00391</link>
<guid>https://arxiv.org/abs/2506.00391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于小语言模型的分层SQL查询修正方法，提高LLMs的错误检测与修正能力。</p><br /><br /><p><strong>摘要：</strong> 当前文本转SQL的自纠错方法存在递归计算开销大及难以有效检测和修正声明式SQL查询错误的问题。为此，本文提出名为SHARE的分层动作修正辅助工具，通过三个专门的小语言模型按序处理，将声明式SQL转换为逐步动作轨迹以揭示推理路径，并进行两阶段精细优化。此外，还设计了一种高效的数据自适应训练策略。实验表明，SHARE显著提升了多种大型语言模型的自纠错性能，尤其在低资源训练场景下表现稳健，对受数据隐私限制的文本转SQL应用具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:51:12 GMT</pubDate>
</item>
<item>
<title>GUI-Actor: 一种无需坐标定位的视觉语言模型驱动GUI动作引导方法</title>
<link>https://arxiv.org/abs/2506.03143</link>
<guid>https://arxiv.org/abs/2506.03143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GUI-Actor方法，通过注意力机制实现无需坐标定位的GUI动作引导，显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GUI-Actor的视觉语言模型（VLM）驱动的GUI动作引导方法，解决了传统基于文本坐标生成方法的局限性，如空间语义对齐弱、无法处理模棱两可的监督目标等。GUI-Actor的核心是一个基于注意力的动作头，它学习将专用标记与所有相关的视觉补丁标记对齐，从而在一个前向传递中提出一个或多个动作区域。此外，设计了一个定位验证器，用于评估并选择最有可能执行动作的区域。实验表明，GUI-Actor在多个GUI动作定位基准测试上超过了现有最先进的方法，并且在未见过的屏幕分辨率和布局上有更好的泛化能力。值得注意的是，在ScreenSpot-Pro基准测试中，GUI-Actor-7B的表现甚至超过了UI-TARS-72B。通过引入验证器，我们发现只需微调新引入的动作头即可实现与先前最先进的模型相当的性能，而无需对整个VLM骨干进行微调，这表明GUI-Actor可以赋予底层VLM有效的定位能力而不损害其通用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>OmniSpatial：面向空间推理的认知心理学基准测试</title>
<link>https://arxiv.org/abs/2506.03135</link>
<guid>https://arxiv.org/abs/2506.03135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniSpatial基准测试，评估视觉语言模型的空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniSpatial，这是一个基于认知心理学的空间推理综合基准测试，涵盖动态推理、复杂空间逻辑、空间交互和视角转换四大类别及50个细粒度子类别。通过网络数据爬取和人工标注，构建超过1500个问答对。实验表明现有视觉语言模型和推理模型在全面空间理解上存在显著局限性，同时分析了失败案例并提出了未来研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:29 GMT</pubDate>
</item>
<item>
<title>AnimeShooter：基于参考图像的多镜头动画数据集及生成模型</title>
<link>https://arxiv.org/abs/2506.03126</link>
<guid>https://arxiv.org/abs/2506.03126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AnimeShooter数据集和生成模型，提升动画视频跨镜头一致性。</p><br /><br /><p><strong>摘要：</strong> 近年来，AI生成内容技术显著加速了动画制作流程。然而，现有公开数据集多聚焦于现实场景且缺乏角色指导的参考图像。为此，我们开发了AnimeShooter，这是一个具备视觉一致性并带有分层注释的多镜头动画数据集。该数据集不仅提供故事级注释（如情节、关键场景和角色参考图像），还通过镜头级注释分解故事为连续片段。此外，AnimeShooter-audio子集提供了同步音频轨道。为了验证数据集效果，我们设计了AnimeShooterGen模型，结合多模态大语言模型和视频扩散模型，实现了基于参考图像的多镜头动画生成。实验表明，该模型在跨镜头视觉一致性方面表现优异，展示了AnimeShooter的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:55:18 GMT</pubDate>
</item>
<item>
<title>ORV：基于占用场的机器人视频生成框架</title>
<link>https://arxiv.org/abs/2506.03079</link>
<guid>https://arxiv.org/abs/2506.03079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ORV，提升机器人模拟视频生成精度和泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对通过远程操作获取真实世界机器人模拟数据耗时且费力的问题，提出了一种名为ORV（Occupancy-centric Robot Video generation framework）的新框架。该框架利用4D语义占用序列作为细粒度表示，提供更精确的语义和几何指导，从而克服现有方法因全局粗略对齐而导致的控制精度低和泛化能力差的问题。实验表明，ORV在多个数据集和子任务上均优于现有基线方法，同时支持多视角机器人抓取操作视频的同步生成，有助于下游机器人学习任务。此外，ORV实现了仿真数据到逼真机器人视频的无缝转换，确保了高时间一致性与精确可控性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:00:32 GMT</pubDate>
</item>
<item>
<title>Sparse-vDiT：通过结构稀疏性加速视频扩散Transformer</title>
<link>https://arxiv.org/abs/2506.03065</link>
<guid>https://arxiv.org/abs/2506.03065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Sparse-vDiT框架，显著降低视频扩散Transformer的计算复杂度。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频生成领域中的扩散Transformer模型（DiTs），因自注意力机制导致的二次复杂度问题展开研究。通过对Video Diffusion Transformer (vDiT) 中注意力图的详细分析，发现三种常见的稀疏模式：对角线、多对角线和垂直条纹结构，并验证这些模式在不同层深和头位置具有强相关性，且较少依赖输入内容。基于此，我们提出Sparse-vDiT框架，包括针对每种稀疏模式设计的优化稀疏核替代密集注意力计算，以及一种硬件感知成本建模的离线稀疏扩散搜索算法。该方法在不影响视觉保真度的情况下显著降低了FLOPs并提升了推理速度。实验表明，在不同基准模型上，Sparse-vDiT分别实现了2.09倍、2.38倍和1.67倍的理论浮点运算减少，实际推理速度提升分别为1.76倍、1.85倍和1.58倍，证明了系统性利用潜在结构稀疏性的可行性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 12:42:37 GMT</pubDate>
</item>
<item>
<title>基于运动引导的长视频生成框架LumosFlow</title>
<link>https://arxiv.org/abs/2506.02497</link>
<guid>https://arxiv.org/abs/2506.02497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架LumosFlow，用于生成具有连贯性和吸引力的长视频。</p><br /><br /><p><strong>摘要：</strong> 长视频生成在娱乐和模拟等领域应用广泛，但合成长时间连贯且视觉吸引的长序列仍是挑战。传统方法如顺序生成短片段或分层生成关键帧常导致时间重复或不自然过渡问题。本文重新审视分层生成管道，提出LumosFlow框架，通过显式引入运动引导解决上述问题。首先利用LMTV-DM生成具有较大动作间隔的关键帧，确保内容多样性；然后将中间帧插值分解为运动生成和后处理优化。LOF-DM生成复杂的大动作光流，MotionControlNet进一步优化扭曲结果并指导中间帧生成。相比传统插值方法，LumosFlow实现15倍插值，确保相邻帧间合理连续运动。实验表明，该方法可生成一致性高且视觉效果优秀的长视频。代码和模型将在接受后公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:25:00 GMT</pubDate>
</item>
<item>
<title>基于非推理规模训练的大规模推理模型长链思维数据集构建</title>
<link>https://arxiv.org/abs/2506.02338</link>
<guid>https://arxiv.org/abs/2506.02338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索不依赖现有模型独立开发大规模推理模型的方法。</p><br /><br /><p><strong>摘要：</strong> 随着R1等公开可用的大规模推理模型的发布，研究人员通常通过在其长链思维推理上训练语言模型来开发新的模型。然而，这种对现有模型的依赖限制了领域的发展。本文作为独立开发大规模推理模型的第一步，探讨利用未经推理时间扩展训练的语言模型构建长链思维数据集的可能性。为此，我们提出了Long CoT Collection数据集，该数据集包含10万条由现有短链思维语言模型注释的推理理由。我们开发了一种管道，将o1的新推理策略引入短链思维语言模型，使它们能够进行更长的推理并更好地管理过度推理问题。广泛的分析验证了我们的数据集质量可与R1媲美或略低。此外，实验表明，在我们的数据集上训练不仅增强了通用推理能力，还为强化学习提供了坚实的基础，初始化在我们数据上的模型在RLVR上取得了2-3倍更大的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 20:29:15 GMT</pubDate>
</item>
<item>
<title>基于Layer-wise Relevance Propagation的Transformer可解释性改进</title>
<link>https://arxiv.org/abs/2506.02138</link>
<guid>https://arxiv.org/abs/2506.02138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的LRP方法，解决Transformer位置编码忽略问题，提升视觉和NLP任务的可解释性。</p><br /><br /><p><strong>摘要：</strong> Transformer模型的可解释性工具开发是深度学习研究的重要方向之一。本文聚焦于Layer-wise Relevance Propagation (LRP) 方法，指出现有基于LRP的Transformer可解释性方法忽视了位置编码这一关键组件，导致违背了守恒属性并丢失了与结构和位置相关的独特相关性。为解决此局限，我们重新定义了Transformer的输入空间为位置-标记对的集合，并提出了专门设计的理论基础LRP规则，用于传播各种位置编码方法（如Rotary、Learnable和Absolute PE）的归因。实验表明，该方法在经过微调的分类器和零样本基础模型（如LLaMA 3）上显著优于现有最先进的视觉和自然语言处理可解释性任务方法。代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 14:07:55 GMT</pubDate>
</item>
<item>
<title>Hanfu-Bench：探索文化的时间维度</title>
<link>https://arxiv.org/abs/2506.01565</link>
<guid>https://arxiv.org/abs/2506.01565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Hanfu-Bench数据集，聚焦文化的时间维度理解。</p><br /><br /><p><strong>摘要：</strong> 现有基于视觉语言模型(VLMs)的文化理解研究多关注地理多样性而忽视时间维度。为弥补这一不足，我们推出了Hanfu-Bench，一个由专家精心策划的多模态数据集。该数据集以汉服为载体，通过文化视觉理解和图像再创作两大核心任务，评估了封闭式和开放式VLMs在跨时空文化理解与创意适应中的表现。结果显示，尽管封闭式VLMs在视觉文化理解上接近非专家水平，但在与人类专家对比时仍有约10%的差距，而开放式的则表现更差。在图像再创作任务中，最佳模型的成功率仅为42%。Hanfu-Bench为未来研究提供了重要的测试平台，揭示了时间文化理解与创造性适应领域的重大挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 07:43:46 GMT</pubDate>
</item>
<item>
<title>ReFoCUS：通过强化学习优化视频帧选择提升多模态模型推理能力</title>
<link>https://arxiv.org/abs/2506.01274</link>
<guid>https://arxiv.org/abs/2506.01274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReFoCUS框架，通过强化学习优化视频帧选择提升视频问答性能。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型在视觉语言推理方面取得了显著进展，但对视频内容的理解仍受限于次优的帧选择策略。现有方法通常依赖静态启发式算法或外部检索模块，可能导致不相关的信息输入。本文介绍ReFoCUS（基于强化学习的上下文理解帧优化），这是一种新颖的帧级策略优化框架，将优化目标从文本响应转移到视觉输入选择上。ReFoCUS利用参考大模型的奖励信号通过强化学习训练帧选择策略，同时采用条件自回归架构提高效率并保证时序一致性。该方法无需帧级显式监督，在多个视频问答基准测试中均表现出色，证明了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 23:08:07 GMT</pubDate>
</item>
<item>
<title>FlowMo：无需训练的文本到视频扩散模型时间一致性增强方法</title>
<link>https://arxiv.org/abs/2506.01144</link>
<guid>https://arxiv.org/abs/2506.01144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的方法FlowMo，通过利用预训练模型自身的预测提升文本到视频生成中的运动连贯性。</p><br /><br /><p><strong>摘要：</strong> 当前的文本到视频扩散模型在建模运动、物理及动态交互等时间特性方面存在显著局限。传统方法通过重新训练模型或引入外部条件信号来解决这一问题。本研究探索是否可以直接从预训练模型的预测中提取有意义的时间表示，而无需额外训练或辅助输入。为此，我们提出了FlowMo，这是一种无需训练的引导方法，仅基于模型自身在每个扩散步的预测即可增强运动连贯性。FlowMo首先通过计算连续帧对应潜在空间的距离，获得去外观偏置的时间表示，揭示模型隐含的时间结构。随后，通过测量时间维度上补丁级别的方差来估计运动连贯性，并在采样过程中动态指导模型减少该方差。大量实验表明，FlowMo显著提高了运动连贯性，同时保持视觉质量和提示对齐，为增强预训练视频扩散模型的时间保真度提供了一种有效的即插即用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 15:55:33 GMT</pubDate>
</item>
<item>
<title>ActiveKD：结合主动学习与知识蒸馏的框架</title>
<link>https://arxiv.org/abs/2506.00910</link>
<guid>https://arxiv.org/abs/2506.00910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ActiveKD框架，利用大规模视觉语言模型实现主动学习与知识蒸馏的结合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ActiveKD的新框架，旨在将主动学习（AL）与知识蒸馏（KD）相结合，特别是在数据稀缺场景下，通过利用大规模视觉语言模型（VLMs）的零样本和少样本能力，克服传统知识蒸馏需要大量标注数据的限制。ActiveKD的关键在于VLMs的结构化预测偏置，即其预测在概率空间中形成聚类，可视为一种归纳偏置，有助于学生模型学习到可泛化的输出模式。为充分利用这种偏置，我们提出了Probabilistic CoreSet（PCoreSet）选择策略，该策略专注于最大化概率空间中的覆盖率而非特征空间。实验评估显示，PCoreSet在11个数据集上的表现优于现有方法，推动了主动学习与知识蒸馏交叉领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 04:54:37 GMT</pubDate>
</item>
<item>
<title>自适应并行解码提升扩散大语言模型生成速度</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法实现扩散大语言模型的高效并行解码。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统自回归解码在大规模语言模型生成中的瓶颈问题，提出了一种名为自适应并行解码(APD)的新方法。该方法通过动态调整并行采样的令牌数量，在保持质量的同时显著提高了生成速度。具体而言，APD通过定义扩散大语言模型边缘概率与小辅助自回归模型序列联合概率之间的乘法混合，颠覆了推测性解码的标准设置。此外，还优化了KV缓存和掩码输入大小。实验结果显示，APD在下游基准测试中提供了明显更高的吞吐量，且质量损失极小，实现了效率与性能的良好平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 02:10:10 GMT</pubDate>
</item>
<item>
<title>Visual Embodied Brain (VeBrain): 统一多模态大型语言模型的机器人应用框架</title>
<link>https://arxiv.org/abs/2506.00123</link>
<guid>https://arxiv.org/abs/2506.00123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架VeBrain，将机器人控制统一为文本任务以提升多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Visual Embodied Brain (VeBrain) 的统一框架，用于现实世界中的感知、推理和控制。VeBrain通过将机器人控制问题重新表述为基于文本的多模态大型语言模型(MLLMs)任务，实现了不同任务目标和映射空间的统一。此外，还提出了一个新颖的机器人适配器，可将MLLMs生成的文本控制信号转换为实际机器人的运动策略。为了支持这一框架，我们构建了VeBrain-600k数据集，包含了大量高质量指令数据，涵盖了多种能力并通过多模态链式思维方法混合不同能力。实验表明，VeBrain在13个多模态基准测试和5个空间智能基准测试中均表现出色，相较于现有模型如Qwen2.5-VL有显著改进。当应用于腿足机器人和机械臂时，VeBrain展示了更强的适应性、灵活性和组合能力。例如，在MMVet测试中，VeBrain比Qwen2.5-VL提升了5.6%，而在腿足机器人任务中更是提高了50%的平均性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 14:00:34 GMT</pubDate>
</item>
<item>
<title>基于自我反思与强化学习的大语言模型性能提升方法</title>
<link>https://arxiv.org/abs/2505.24726</link>
<guid>https://arxiv.org/abs/2505.24726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自我反思与强化学习改进大语言模型在复杂任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种通过自我反思和强化学习提升大型语言模型性能的方法。当模型回答错误时，通过激励其生成更好的自我反思评论，即使无法生成合成数据且仅能获得二元反馈的情况下，也能显著提高解决复杂验证任务的能力。该框架分为两个阶段：首先，在任务失败后生成自我反思评论；其次，在反思的基础上再次尝试任务。若第二次尝试成功，则对反思阶段生成的标记进行奖励。实验结果显示，不同架构模型均取得显著性能提升，例如数学公式写作提高了34.7%，函数调用提高了18.1%。值得注意的是，较小规模的微调模型（1.5亿至70亿参数）在某些任务上优于同家族更大规模的模型（10倍大小）。这一新范式为构建更实用、可靠的自优化语言模型提供了令人兴奋的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 11:49:42 GMT</pubDate>
</item>
<item>
<title>零样本链式思维推理过程成功预测研究</title>
<link>https://arxiv.org/abs/2505.24362</link>
<guid>https://arxiv.org/abs/2505.24362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，基于大型语言模型表示的探测分类器可在推理初始阶段预测链式思维推理的成功。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨是否可以在零样本链式思维（CoT）推理完成前预测其成功与否。实验表明，基于大型语言模型（LLM）表示的探测分类器即使在生成第一个标记之前也能表现良好，这表明推理过程的关键信息在初始步骤的表示中已经存在。相比之下，依赖生成标记的强BERT基线表现较差，可能是因为它更多依赖于浅层的语言线索而非深层的推理动态。令人惊讶的是，使用后续推理步骤并不总是提高分类性能，在某些情况下，较早的表示与后期表示更为相似，表明LLMs早期就编码了关键信息，这意味着推理可以较早停止而不会造成损失。为了验证这一点，我们进行了提前终止实验，结果显示截断CoT推理仍能提高性能，但与完整推理相比仍有差距。然而，通过监督学习或强化学习等方法缩短CoT链条时，可以利用我们的分类器指导来判断何时进行有效提前终止。这些发现为优化CoT效率提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 04:54:28 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中回溯技术对推理能力提升的研究</title>
<link>https://arxiv.org/abs/2505.24273</link>
<guid>https://arxiv.org/abs/2505.24273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示回溯技术如何显著改善大规模语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地探讨了监督微调（SFT）与强化学习（RL）结合时回溯技术对八类推理任务的影响。实验发现，较短的思维链序列在简单任务中对RL训练有适度贡献，但随着任务难度增加，这种作用减弱。通过构建具有不同回溯步数的数据集，我们发现较长的思维链且包含回溯通常能带来更好的RL训练效果，且复杂问题需要更多回溯。此外，蒸馏数据的实验表明，RL训练对长思维链序列的正确性依赖较低，更注重结构模式。这些结果为优化大规模语言模型的推理能力提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24273" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 02:49:00 GMT</pubDate>
</item>
<item>
<title>CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs</title>
<link>https://arxiv.org/abs/2505.24120</link>
<guid>https://arxiv.org/abs/2505.24120</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:34:25 GMT</pubDate>
</item>
<item>
<title>Robot-R1：通过强化学习提升机器人视觉语言模型的具身推理能力</title>
<link>https://arxiv.org/abs/2506.00070</link>
<guid>https://arxiv.org/abs/2506.00070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于强化学习的新框架Robot-R1，用于提升机器人视觉语言模型的具身推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型视觉语言模型（LVLMs）近年来在结合具身推理与机器人控制方面展现了巨大潜力。然而，传统的监督微调（SFT）方法存在数据构建不优化及泛化性能下降等问题。为解决这些局限性，我们提出了Robot-R1，这是一种利用强化学习增强机器人控制中具身推理的新框架。Robot-R1通过预测任务完成所需的下一个关键点状态，基于场景图像和环境元数据进行条件学习，这些数据源自专家演示。受DeepSeek-R1方法启发，Robot-R1对基于推理的响应进行采样并强化那些产生更准确预测的响应。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法，即使参数量仅为7B，也超越了GPT-4o在低级动作控制相关的推理任务上的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:41:12 GMT</pubDate>
</item>
<item>
<title>DINGO：一种高效且分布保持的约束解码策略</title>
<link>https://arxiv.org/abs/2505.23061</link>
<guid>https://arxiv.org/abs/2505.23061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DINGO方法解决扩散语言模型无法满足用户指定约束的问题。</p><br /><br /><p><strong>摘要：</strong> 扩散语言模型（Diffusion LLMs）因其显著的运行效率提升而备受关注，但缺乏对用户指定形式化约束（如正则表达式）的支持，这限制了其在需要结构化输出的任务中的应用。与自回归模型不同，扩散LLMs并行预测一组token，使得传统的约束解码算法失效。为了解决这一问题，我们提出了DINGO，这是一种基于动态规划的约束解码策略，既高效又能保证输出分布的真实性。实验表明，DINGO在标准符号数学和JSON生成基准测试中比无约束推理提高了多达68个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:04:54 GMT</pubDate>
</item>
<item>
<title>基于深度视频发现代理的长视频理解方法</title>
<link>https://arxiv.org/abs/2505.18079</link>
<guid>https://arxiv.org/abs/2505.18079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用强化学习策略处理长视频理解问题的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对长视频理解中存在的时空复杂性和上下文问答难题，提出了一种名为Deep Video Discovery (DVD)代理的方法。传统的大语言模型（LLMs）虽在视频分析方面有所进步，但在处理长达数小时的信息密集型视频时仍显局限。DVD代理通过在分割后的视频片段上采用自主搜索策略，结合多粒度视频数据库中的工具集，利用LLMs的高级推理能力进行状态观察、工具选择及参数优化，从而实现对视频内容的有效解析。实验结果显示，该方法在多个长视频理解基准测试中表现出色，特别是在LVBench数据集上的表现超越了现有技术。此外，文中还进行了消融研究和工具分析，为未来面向长视频理解任务的智能代理发展提供了宝贵见解。代码将在之后发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:37:36 GMT</pubDate>
</item>
<item>
<title>UniWorld：基于语义特征的统一生成框架</title>
<link>https://arxiv.org/abs/2506.03147</link>
<guid>https://arxiv.org/abs/2506.03147</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于语义特征的统一生成框架UniWorld，在图像编辑任务上超越BAGEL。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniWorld的统一生成框架，该框架利用强大的视觉语言模型和对比语义编码器提供的语义特征，旨在解决现有统一模型在图像感知和操作任务上的局限性。通过实验发现，GPT-4o-Image不依赖传统的VAE，而是使用语义编码器提取特征，这一观察启发了我们开发UniWorld。UniWorld仅使用BAGEL数据量的1%，却在图像编辑基准测试中始终优于BAGEL，并且在图像理解和生成能力上表现强劲，适用于多种图像感知任务。我们的研究不仅展示了UniWorld在图像操作领域的潜力，还强调了语义特征在跨模态任务中的重要性。此外，我们将模型、训练和评估脚本以及数据集完全开源，以促进相关领域的进一步研究和应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03147" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.03136</link>
<guid>https://arxiv.org/abs/2506.03136</guid>
<content:encoded><![CDATA[
We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:42 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的任意分辨率图像合成</title>
<link>https://arxiv.org/abs/2506.03131</link>
<guid>https://arxiv.org/abs/2506.03131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法实现任意分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为native-resolution image synthesis的新生成建模范式，它能够以任意分辨率和宽高比合成图像。通过引入Native-resolution diffusion Transformer（NiT），该架构能够在去噪过程中显式建模变化的分辨率和宽高比。NiT摆脱了固定格式的限制，从多种分辨率和宽高比的图像中学习内在视觉分布。实验结果显示，单一NiT模型在ImageNet-256x256和512x512基准测试中均达到最先进的性能，并且在未见过的高分辨率（如1536 x 1536）和多样化的宽高比（如16:9、3:1、4:3）上表现出色的零样本泛化能力，类似于先进的大型语言模型。这项研究揭示了native-resolution建模作为视觉生成建模与先进LLM方法之间桥梁的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:57:33 GMT</pubDate>
</item>
<item>
<title>基于视觉提示的可泛化图像编辑新范式</title>
<link>https://arxiv.org/abs/2506.02528</link>
<guid>https://arxiv.org/abs/2506.02528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RelationAdapter模块提升图像编辑模型对非刚性变换的理解能力。</p><br /><br /><p><strong>摘要：</strong> 受大型语言模型上下文学习机制的启发，一种新的基于视觉提示的通用图像编辑范式正在兴起。现有单参考方法多集中于风格或外观调整，难以处理非刚性变换问题。为此，我们通过利用源目标图像对提取并转移内容感知的编辑意图至查询图像。为此，引入RelationAdapter轻量级模块，使基于Diffusion Transformer的模型能够有效捕捉并应用来自极少量示例的视觉变换。同时，构建Relation252K综合数据集评估模型在视觉提示驱动场景中的泛化性和适应性。实验表明，RelationAdapter显著提升了模型理解及传递编辑意图的能力，大幅提高了生成质量和整体编辑性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 03:06:35 GMT</pubDate>
</item>
<item>
<title>M^3FinMeeting：多语言金融会议理解基准的开创性研究</title>
<link>https://arxiv.org/abs/2506.02510</link>
<guid>https://arxiv.org/abs/2506.02510</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为M^3FinMeeting的新基准，用于评估大型语言模型在金融会议理解中的表现。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步促使开发新的金融领域性能评估基准，但现有基准多依赖新闻或财报等静态文本，难以反映真实金融会议的动态特性。为弥补这一不足，本研究提出了M^3FinMeeting，这是一个支持多语言、多行业和多任务的金融会议理解数据集。该数据集包含英语、中文和日语三种语言，覆盖全球行业分类标准(GICS)定义的多个行业部门，并设计了摘要生成、问答对提取及问答三个任务。实验结果显示，即使是最先进的长上下文模型在M^3FinMeeting上的表现仍有显著提升空间，表明该基准在评估LLMs金融会议理解能力方面具有重要价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02510" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:41:09 GMT</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher：结合文本与可视化的大语言模型深度研究框架</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Multimodal DeepResearcher框架，实现文本与可视化报告的高效生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度研究框架主要关注文本生成而忽视图文混排的问题，提出了Formal Description of Visualization (FDV)，一种结构化的图表文本表示方法，使大语言模型能够学习并生成高质量的可视化内容。在此基础上，设计了Multimodal DeepResearcher框架，将任务分解为四个阶段：研究、示例报告文本化、规划及多模态报告生成。为了评估生成的多模态报告，构建了MultimodalReportBench基准数据集，包含100个多样化主题，并采用五个专用指标进行评估。实验表明，该框架在多个模型和评估方法下均优于基线方法，尤其使用Claude 3.7 Sonnet模型时，整体胜率可达82%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 01:18:19 GMT</pubDate>
</item>
<item>
<title>Visual Strategic Bench (VS-Bench): 多智能体环境中视觉语言模型的战略推理评估</title>
<link>https://arxiv.org/abs/2506.02387</link>
<guid>https://arxiv.org/abs/2506.02387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多模态基准VS-Bench，用于评估视觉语言模型在多智能体环境中的战略推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉语言模型（VLMs）的能力已扩展到交互代理任务，但现有基准大多局限于单代理或纯文本环境，无法反映真实世界中涉及多智能体、复杂视觉和语言交互的场景。为解决这一问题，我们引入了Visual Strategic Bench (VS-Bench)，这是一个多模态基准，旨在评估VLMs在多智能体环境中的战略推理和决策能力。VS-Bench包含八个视觉相关的环境，涵盖合作、竞争及混合动机交互，通过预测未来动作准确性和归一化回合回报等指标进行评估。实验结果显示当前模型与最优性能之间存在显著差距，同时深入分析了多模态观察、测试时扩展性、社会行为及模型失败案例。我们希望VS-Bench成为未来多模态战略代理研究的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 22:57:38 GMT</pubDate>
</item>
<item>
<title>基于合成数据增强的视觉语言模型强化学习研究</title>
<link>https://arxiv.org/abs/2506.02096</link>
<guid>https://arxiv.org/abs/2506.02096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SynthRL方法，通过合成数据提升视觉语言模型强化学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过合成强化学习数据进一步优化具有可验证奖励的视觉语言模型强化学习方法。为此，我们设计了一个名为SynthRL的可扩展且可靠的推理导向强化学习训练自动数据扩展管道。该方法包含三个关键阶段：选择分布合理的种子问题、生成更具挑战性的变体同时保留原始答案、确保接近完美的正确性和难度增强的验证阶段。实验表明，SynthRL在MMK12数据集上可生成超过3.3K个高质量问题，显著提升了跨五个域外视觉数学推理基准的表现，尤其在最具挑战性的样本上表现更为突出。这证明了SynthRL在激发更深层次推理模式方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:45:16 GMT</pubDate>
</item>
<item>
<title>构建高质量数据集的挑战与系统性评估方法</title>
<link>https://arxiv.org/abs/2506.01789</link>
<guid>https://arxiv.org/abs/2506.01789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过引入评估量规解决数据集质量评价问题。</p><br /><br /><p><strong>摘要：</strong> 高质量数据集对机器学习模型至关重要，但其创建过程尤其是人工标注面临诸多挑战。当前数据集论文提交存在原创性不足、多样性缺乏及质量控制不严等问题，而现有工具如数据表单虽促进透明度，却未能提供标准化评估方法。为应对这些局限，本文主张在数据集评审过程中整合系统化的量规评估指标，同时探讨合成数据生成的高效且经济的方法。作为行动号召，我们提出了DataRubrics框架，该框架基于LLM技术，可对人工和模型生成的数据集进行结构化评估，实现可重复、可扩展且实用的质量评估方案，并开源代码支持评估复现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 11:31:52 GMT</pubDate>
</item>
<item>
<title>MotionSight：零样本细粒度视频运动理解的新方法</title>
<link>https://arxiv.org/abs/2506.01674</link>
<guid>https://arxiv.org/abs/2506.01674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的零样本方法MotionSight，提升多模态大语言模型的细粒度视频运动理解能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大型语言模型（MLLMs）取得了显著进展，但其在细粒度视频运动理解方面的能力仍然有限。现有模型通常缺乏帧间差异处理，且倾向于忽略微妙的视觉线索。此外，虽然视觉提示在静态图像中的应用已显示出潜力，但在视频的时间复杂性上的应用，特别是针对细粒度运动理解，仍鲜有研究。本文介绍了一种名为MotionSight的新方法，通过引入对象中心的视觉聚光灯和运动模糊作为视觉提示，在不进行训练的情况下有效改善了细粒度运动理解。为支持这一方法，我们创建了MotionVid-QA，这是首个用于细粒度视频运动理解的大规模数据集，包含层次化的注释，如SFT和偏好数据，以及约40K个视频片段和87K个问答对。实验表明，MotionSight在开源模型中达到了最先进的性能，并在竞争力上接近商业模型。这项研究展示了零样本技术在视频运动理解中的新可能性，并提供了高质量的数据集和公开代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:44:56 GMT</pubDate>
</item>
<item>
<title>金融领域多模态大型语言模型评估基准FinMME发布</title>
<link>https://arxiv.org/abs/2505.24714</link>
<guid>https://arxiv.org/abs/2505.24714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinMME填补金融领域的多模态评估数据集空白。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FinMME的金融领域多模态评估数据集，包含超过11,000个高质量样本，覆盖18个金融领域和6类资产，支持10种主要图表类型及其子类型。通过精心设计的验证机制和20名标注员确保数据质量。同时，开发了FinScore评估系统，采用幻觉惩罚和多维能力评估，以提供公正的评价。实验表明，即使是最先进的模型如GPT-4o，在FinMME上的表现也不尽人意，凸显了该数据集的挑战性。此外，FinMME具有高鲁棒性，不同提示下的预测变化低于1%，优于现有数据集。该数据集和评估协议可通过Hugging Face和GitHub获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24714" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 11:36:19 GMT</pubDate>
</item>
<item>
<title>RRec：具有内在推理能力的统一推荐模型</title>
<link>https://arxiv.org/abs/2505.16994</link>
<guid>https://arxiv.org/abs/2505.16994</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RRec模型，通过融合推理和推荐优化提升推荐系统性能。</p><br /><br /><p><strong>摘要：</strong> 当前大型推荐模型通常将大型语言模型(LLMs)作为外部推理模块，用于增强传统推荐管道。然而，这种解耦设计存在显著资源成本和次优联合优化问题。为解决这些问题，我们提出了RRec，这是一种具备内在推理能力的统一大型推荐模型。首先，重新设计模型架构以促进自回归过程中的交错推理和推荐；其次，提出RecPO强化学习框架，在单一策略更新中同时优化推理和推荐能力。RecPO引入融合奖励方案，仅依赖推荐标签模拟推理能力，无需专门的推理注释。在三个数据集上的实验验证了RRec的有效性，相较于基线模型，Hit@5指标提升了68.67%，NDCG@20指标提升了45.21%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16994" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:55:43 GMT</pubDate>
</item>
<item>
<title>多编程语言与英语在大语言模型概念空间中的关系研究</title>
<link>https://arxiv.org/abs/2506.01074</link>
<guid>https://arxiv.org/abs/2506.01074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨大语言模型如何在概念空间中表示多种编程语言与英语的关系。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大语言模型（LLMs）在处理编程语言（PLs）时的机制，特别是它们如何在多语言环境下表示英语。通过在两个基于Llama的模型上进行少量样本翻译任务，我们观察到概念空间更接近英语（包括PL关键字），且中间层后半部分对英语标记赋予高概率。进一步分析显示，虽然语言特定神经元主要集中在底层，但每种PL特有的神经元往往出现在顶层。对于与其他PL高度对齐的PL，无法识别语言特定神经元，这类PL通常具有更大的关键字集，并在翻译任务中无论输入/输出PL如何，都更接近模型的概念空间。我们的发现揭示了LLMs内部表示PL的结构模式，有助于深入理解模型的工作原理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 12:24:13 GMT</pubDate>
</item>
<item>
<title>SealQA：评估搜索增强语言模型的新基准</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SealQA包含三种版本，用于测试模型在复杂事实查询中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SealQA，这是一个针对搜索增强语言模型在面对冲突性、噪声或无用搜索结果时处理事实查询能力的新基准。SealQA分为三个版本：Seal-0（主版）和Seal-Hard，侧重于评估模型的事实准确性与推理能力，其中Seal-0专注于最具挑战性的问题；LongSeal则扩展了SealQA，用于测试长上下文、多文档推理能力。研究发现当前模型存在显著局限性，即使是最前沿的语言模型在所有SealQA版本中表现也较差。例如，在Seal-0上，配备工具如o3和o4-mini的前沿代理模型分别仅达到17.1%和6.3%的最佳推理准确率。尽管高级推理模型对搜索结果的噪声敏感，但增加测试时间计算并未带来可靠提升。此外，虽然新模型较少受到“迷失中间”问题的影响，但在LongSeal中仍难以有效识别相关文档。为促进未来研究，SealQA已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 12:04:34 GMT</pubDate>
</item>
<item>
<title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title>
<link>https://arxiv.org/abs/2506.00979</link>
<guid>https://arxiv.org/abs/2506.00979</guid>
<content:encoded><![CDATA[
The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:20:22 GMT</pubDate>
</item>
<item>
<title>RAG系统在动态语料库上的鲁棒性评估</title>
<link>https://arxiv.org/abs/2506.00789</link>
<guid>https://arxiv.org/abs/2506.00789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RARE框架揭示了RAG系统对实时噪声和事实变化的脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RARE的统一框架和大规模基准测试，用于评估基于检索增强生成（RAG）模型在处理动态时间敏感型语料库时的鲁棒性。RARE通过知识图谱驱动的合成管道自动生成多级问题集，构建了一个包含400份金融、经济和政策文档及48,322个问题的数据集。研究发现，无论生成器大小或架构如何，RAG系统在面对查询、文档或实际检索结果的变化时，文档层面的鲁棒性始终最弱，并且在多跳问题上的表现明显低于单跳问题。这一研究表明现有RAG系统的现实应用能力仍有待提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 22:42:36 GMT</pubDate>
</item>
<item>
<title>Neuro2Semantic：基于iEEG信号的语言语义解码新框架</title>
<link>https://arxiv.org/abs/2506.00381</link>
<guid>https://arxiv.org/abs/2506.00381</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种新型框架Neuro2Semantic实现从神经信号重建语言语义。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Neuro2Semantic的新框架，该框架可以从颅内脑电图(iEEG)记录中重建感知语音的语义内容。Neuro2Semantic分为两个阶段：首先，基于LSTM的适配器将神经信号与预训练文本嵌入对齐；其次，校正模块直接从这些对齐的嵌入中生成连续自然文本。这种方法克服了先前解码方法的局限性，支持不受约束的文本生成。实验表明，即使只有30分钟的神经数据，Neuro2Semantic也能在低数据场景下超越最新的同类方法，显示出在脑机接口和神经解码技术中的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00381" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:17:19 GMT</pubDate>
</item>
<item>
<title>源无关域自适应中的增强技术与伪标签重加权策略</title>
<link>https://arxiv.org/abs/2505.24216</link>
<guid>https://arxiv.org/abs/2505.24216</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法提升源无关域适应性能，在多个基准数据集上取得最佳结果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了源无关域适应（SFDA），即模型在不访问源域数据的情况下适应目标域的问题。文中引入了一种新的数据增强技术——Shuffle PatchMix（SPM）以及一种新颖的伪标签重加权策略，旨在提高模型性能。SPM通过打乱并混合图像块生成多样化且具有挑战性的增强样本，而伪标签重加权策略则优先考虑可靠的伪标签以减轻标签噪声的影响。这些技术在较小的数据集如PACS上表现尤为突出，因为这类数据集更容易出现过拟合和伪标签噪声问题。实验结果显示，该方法在三个主要基准数据集PACS、VisDA-C和DomainNet-126上均达到了最先进的性能。特别是在PACS数据集上，单目标设置下的准确性从79.4%提升至86.7%，多目标设置下提高了7.2%；而在DomainNet-126和VisDA-C上的准确率分别提升了2.8%和0.7%。这一结合先进增强技术和稳健伪标签重加权的方法为SFDA领域设定了新的标杆。相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24216" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 01:02:42 GMT</pubDate>
</item>
<item>
<title>达尔文Gödel机器：一种自我进化的AI系统</title>
<link>https://arxiv.org/abs/2505.22954</link>
<guid>https://arxiv.org/abs/2505.22954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">达尔文Gödel机器通过迭代改进自身代码实现自主进化。</p><br /><br /><p><strong>摘要：</strong> 当前的人工智能系统依赖固定架构且无法自主持续优化，而达尔文Gödel机器（DGM）作为一种自我进化的AI系统，通过迭代修改自身代码并验证改进效果，在编码能力上显著提升。DGM受达尔文进化论启发，维护一个代码代理档案，并通过采样和基于基础模型的创新生成新版本，形成多样化高质量的搜索空间探索树。实验表明，DGM不仅提升了SWE-bench和Polyglot的表现，还在安全措施下展现出超越传统基线的能力，标志着迈向自主人工智能的重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 20:26:15 GMT</pubDate>
</item>
<item>
<title>基于流匹配的双耳语音合成框架BinauralFlow</title>
<link>https://arxiv.org/abs/2505.22865</link>
<guid>https://arxiv.org/abs/2505.22865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合流匹配和因果U-Net的双耳语音合成方法。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于双耳音频渲染问题，针对现有方法在渲染质量和实时推断上的不足，提出了一种名为BinauralFlow的新框架。该框架将双耳渲染视为生成问题而非回归问题，设计条件流匹配模型提升音频质量，并通过因果U-Net架构实现流式推断。此外，引入连续推理管道优化渲染连续性和速度。实验表明，BinauralFlow在客观评估和主观感知测试中均优于现有技术，其生成的音频接近真实录音效果，混淆率为42%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 16:59:15 GMT</pubDate>
</item>
<item>
<title>Plan-and-Budget：提升大语言模型推理效率的框架</title>
<link>https://arxiv.org/abs/2505.16122</link>
<guid>https://arxiv.org/abs/2505.16122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Plan-and-Budget框架解决大语言模型过思考和计算低效问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在复杂推理任务中的显著成功与其推理过程的计算低效之间的矛盾。通过分析发现，许多流行的大模型存在过度推理的问题，即对简单查询生成冗长且偏离主题的推理轨迹。尽管已有工作尝试通过固定令牌预算来缓解这一问题，但这可能导致欠思考，尤其是在更难的问题上。为此，我们开发了一个理论模型BBAM（贝叶斯预算分配模型），将推理建模为具有不同不确定性的子问题序列，并引入E³指标来捕捉正确性和计算效率之间的权衡。基于BBAM的理论成果，我们提出了Plan-and-Budget，这是一种模型不可知的测试时间框架，它将复杂的查询分解为子问题，并根据估计的复杂性使用自适应调度分配令牌预算。Plan-and-Budget在各种任务和模型上提高了推理效率，在某些情况下实现了高达+70%的准确性提升、-39%的令牌减少以及+187.5%的E³改进。特别值得注意的是，它使较小的模型（DS-Qwen-32B）的表现接近更大的模型（DS-LLaMA-70B），展示了Plan-and-Budget缩小性能差距的能力，而无需重新训练。我们的代码可在anonymous.4open.science/r/P-and-B-6513/获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 21:56:29 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型推理机制研究：基于视觉矛盾数据集的分析</title>
<link>https://arxiv.org/abs/2505.17127</link>
<guid>https://arxiv.org/abs/2505.17127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多模态模型推理时如何权衡世界知识与视觉信息。</p><br /><br /><p><strong>摘要：</strong> 本文探讨多模态大型语言模型（MLLMs）在视觉问答等任务中的推理机制，是否依赖记忆的知识还是输入图像中的视觉信息。为此，我们引入了一个名为Visual CounterFact的新数据集，其中包含现实感强的反事实样本，使世界知识先验（如红色草莓）与视觉输入（如蓝色草莓）产生冲突。实验表明，模型预测最初反映记忆的先验，但在中间到晚期层逐渐转向视觉证据，揭示了两种模态之间的竞争，最终视觉输入会覆盖先验。为控制这种行为，我们提出了Pixels Versus Priors (PvP)引导向量，通过激活层面干预来控制模型输出倾向于世界知识还是视觉输入。平均而言，PvP成功将92.5%的颜色预测和74.6%的大小预测从先验转移到反事实。这些发现为解释和控制多模态模型的事实行为提供了新工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:56:55 GMT</pubDate>
</item>
<item>
<title>WebChoreArena：衡量大型语言模型处理复杂网络任务的能力</title>
<link>https://arxiv.org/abs/2506.01952</link>
<guid>https://arxiv.org/abs/2506.01952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出WebChoreArena基准测试平台，评估LLMs在复杂网络任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为WebChoreArena的新基准测试平台，该平台由532项精心策划的任务组成，旨在扩展WebArena的功能，涵盖更多劳动密集型和繁琐的任务。WebChoreArena集成了三大挑战：大量记忆需求任务、精确数学推理任务和长期记忆任务。基于可完全重现的四个WebArena仿真环境构建，它保证了严格的可重复性并支持与现有基准进行公平直接比较。实验结果显示，随着GPT-4o、Claude 3.7 Sonnet和Gemini 2.5 Pro等LLMs的发展，WebChoreArena上的性能显著提升。然而，即使使用最先进的Gemini 2.5 Pro，与WebArena相比仍有较大改进空间，表明WebChoreArena更能体现LLMs的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>融合自回归与掩码扩散模型的Eso-LMs提升语言建模效率</title>
<link>https://arxiv.org/abs/2506.01928</link>
<guid>https://arxiv.org/abs/2506.01928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eso-LMs通过融合AR与MDM实现高效并行生成且提升推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的模型家族Eso-LMs，它结合了自回归（AR）模型和平行生成的掩码扩散模型（MDM），实现了两者困惑度的平滑插值，同时克服了各自的局限性。实验表明，Eso-LMs在标准语言建模基准上达到了新的性能高度。更重要的是，我们首次为MDM引入了KV缓存技术，同时保持了并行生成的能力，显著提升了推理效率。结合优化后的采样调度，我们的方法比标准MDMs快达65倍，比之前的半自回归方法快4倍。此外，项目代码和模型检查点已在项目页面公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:47:27 GMT</pubDate>
</item>
<item>
<title>阿拉伯语语言模型评估的理论指南与新框架</title>
<link>https://arxiv.org/abs/2506.01920</link>
<guid>https://arxiv.org/abs/2506.01920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的阿拉伯语语言模型评估框架，揭示现有模型在文化理解和专业知识上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文通过分析现有的阿拉伯语评价数据集，发现其在语言准确性、文化适应性和方法严谨性方面的重大问题。为解决这些问题，我们提出了阿拉伯深度微型数据集(ADMD)，包含十个主要领域的490个挑战性问题。通过ADMD对五个领先语言模型进行评估，结果显示不同领域模型表现差异显著，特别是在需要深厚文化理解和专业知识的领域。Claude 3.5 Sonnet在整体上表现出最高的准确率30%，尤其在数学理论、阿拉伯语和伊斯兰教相关领域表现突出。本研究为改进阿拉伯语语言模型的评估提供了理论基础和实用见解，强调了文化胜任力和技术能力同等重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:39:50 GMT</pubDate>
</item>
<item>
<title>探索压缩表示中的规模定律：统一预测模型性能</title>
<link>https://arxiv.org/abs/2506.01863</link>
<guid>https://arxiv.org/abs/2506.01863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究压缩格式对大规模机器学习模型性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了规模定律与压缩格式之间的相互作用，验证了一种通用的规模定律公式，并展示了其在不同压缩类型中的适用性。研究表明，基于表示能力的“容量”指标可以可靠地预测多种压缩表示下的参数效率。此外，我们还扩展了该公式，用于比较不同压缩格式的精度潜力并优化稀疏量化格式的训练算法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:52:51 GMT</pubDate>
</item>
<item>
<title>基于组相对策略优化的多模态自反思增强推理方法</title>
<link>https://arxiv.org/abs/2506.01713</link>
<guid>https://arxiv.org/abs/2506.01713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段强化学习框架SRPO，显著提升多模态大模型的推理与反思能力。</p><br /><br /><p><strong>摘要：</strong> 现有研究显示，多模态大型语言模型（MLLMs）在推理任务中虽有潜力，但难以应对需要明确自我反思和修正的复杂问题。针对这一挑战，本文提出了一种名为Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO)的两阶段强化学习框架，旨在通过引入高质量的反思数据集及创新奖励机制，显著提高多模态大模型的推理与反思质量。实验结果表明，在MathVista、MathVision等多模态推理基准测试中，SRPO相较于当前最先进的模型在推理准确性和反思质量上均有显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 10:21:44 GMT</pubDate>
</item>
<item>
<title>基于多模态去噪扩散模型的量子运算高效编译方法</title>
<link>https://arxiv.org/abs/2506.01666</link>
<guid>https://arxiv.org/abs/2506.01666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合离散门选择与连续参数预测的量子电路生成新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为多模态去噪扩散模型的方法，用于同时生成量子电路的结构及其连续参数，从而实现目标酉矩阵的编译。该模型通过两个独立的扩散过程分别处理离散门选择和参数预测。实验表明，该方法在不同量子比特数量、电路深度及可调门比例下均表现出较高的准确性。此外，利用其快速电路生成能力，我们构建了针对特定操作的大规模电路数据集，并从中提取了有价值的启发式规则，为量子电路综合研究提供了新见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:35:33 GMT</pubDate>
</item>
<item>
<title>基于LLM的自动化仇恨言论去毒化研究</title>
<link>https://arxiv.org/abs/2506.01484</link>
<guid>https://arxiv.org/abs/2506.01484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用GPT-4o-mini实现自动化仇恨言论去毒化，构建大规模平行数据集PARADEHATE。</p><br /><br /><p><strong>摘要：</strong> 随着网络上有害内容的增多，去毒化任务变得尤为重要，但由于标注成本和敏感性，高质量的去毒化数据集稀缺。本文提出了一种新的LLM循环管道，通过使用GPT-4o-mini替代人工标注员，复制并改进了ParaDetox流程，证明了LLM标注的效果可媲美人工。在此基础上，我们构建了一个名为PARADEHATE的大规模平行数据集，包含超过8000对仇恨言论及其非仇恨版本的文本对。实验表明，在PARADEHATE上微调的BART模型在风格准确性、内容保留和流畅度方面表现优异，验证了LLM生成的去毒化文本作为规模化替代人工标注的有效性。这项工作为仇恨言论的自动化处理提供了新思路和基准数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 05:45:05 GMT</pubDate>
</item>
<item>
<title>个性化场景认知对齐的视觉语言模型评估基准与框架</title>
<link>https://arxiv.org/abs/2506.00930</link>
<guid>https://arxiv.org/abs/2506.00930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出个性化场景认知对齐框架PCogAlign及基准PCogAlignBench。</p><br /><br /><p><strong>摘要：</strong> 随着愿景-语言模型(VLMs)在人类视觉任务中的广泛应用，如何使这些模型满足多样化用户的需求成为亟待解决的问题。本文通过社会学中的角色集(Role-Set)概念简化个体特征描述，并构建了一个包含18k实例和20个具有不同角色集的个体的基准数据集PCogAlignBench。此外，我们还提出了一个名为PCogAlign的框架，该框架基于认知意识和行动导向的奖励模型实现个性化对齐。实验结果和人工评估证明了PCogAlignBench的可靠性和PCogAlign的有效性。我们的工作将开源构建的数据集和代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 05:50:54 GMT</pubDate>
</item>
<item>
<title>LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00772</link>
<guid>https://arxiv.org/abs/2506.00772</guid>
<content:encoded><![CDATA[
Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 21:31:50 GMT</pubDate>
</item>
<item>
<title>大型语言模型在预测任务中的表现评估挑战</title>
<link>https://arxiv.org/abs/2506.00723</link>
<guid>https://arxiv.org/abs/2506.00723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在预测任务中的性能评估问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）被广泛应用于预测任务，一些研究声称其表现可媲美甚至超越人类。然而，我们指出，作为研究社区，我们需要谨慎对待这些结论，因为评估LLM预测器存在独特挑战。首先，由于多种时间泄漏形式的存在，难以信任评价结果；其次，在实际应用中，从评估表现到真实世界预测的外推也存在困难。通过系统分析和引用先前工作的具体例子，我们展示了评估中的缺陷如何引发对当前及未来性能声明的担忧。因此，我们主张需要更严格的评估方法，以自信地评估LLMs的预测能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 17:49:17 GMT</pubDate>
</item>
<item>
<title>CityLens：评估大语言-视觉模型预测城市社会经济指标的能力</title>
<link>https://arxiv.org/abs/2506.00530</link>
<guid>https://arxiv.org/abs/2506.00530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CityLens基准测试评估大语言-视觉模型在预测城市社会经济指标方面的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CityLens的综合基准测试，用于评估大型语言-视觉模型（LLVMs）从卫星图像和街景图像预测城市社会经济指标的能力。研究团队构建了一个包含17个全球分布城市的多模态数据集，涵盖经济、教育、犯罪、交通、健康和环境六大领域。基于此数据集，定义了11项预测任务，并采用三种评估范式：直接度量预测、归一化度量估计和基于特征的回归。研究对17个最先进的LLVM进行了基准测试，结果显示尽管这些模型展示了良好的感知和推理能力，但在预测城市社会经济指标方面仍存在局限性。CityLens提供了一个统一框架，可用于诊断这些局限性并指导未来利用LLVMs理解和预测城市社会经济模式的努力。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00530" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 08:25:33 GMT</pubDate>
</item>
<item>
<title>SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation</title>
<link>https://arxiv.org/abs/2506.00523</link>
<guid>https://arxiv.org/abs/2506.00523</guid>
<content:encoded><![CDATA[
The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 07:59:02 GMT</pubDate>
</item>
<item>
<title>对抗性攻击对机器生成文本检测器性能的影响研究</title>
<link>https://arxiv.org/abs/2505.24523</link>
<guid>https://arxiv.org/abs/2505.24523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有检测器易受对抗样本影响，检测性能显著下降。</p><br /><br /><p><strong>摘要：</strong> 近期生成式人工智能和大型语言模型的进步催生了高度逼真的合成内容，但这也引发了关于恶意用途（如虚假信息传播）的担忧。尽管如此，由于缺乏评估真实场景泛化能力的稳健基准，检测机器生成文本依然面临挑战。本研究提出了一套测试方法，针对当前最先进的机器生成文本检测工具（如Mage、Radar、LLM-DetectAIve），评估其在面对基于语言学的对抗性攻击时的表现。我们通过直接偏好优化微调语言模型，使生成文本更接近人类书写风格，从而揭示检测器对特定语言特征的依赖。实验结果显示，即使少量对抗样本即可大幅降低检测器的准确性。这一发现强调了提升检测技术鲁棒性的紧迫性，尤其是应对未知领域的文本检测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:33:30 GMT</pubDate>
</item>
<item>
<title>ComposeAnything：无需重新训练的文本到图像复合生成框架</title>
<link>https://arxiv.org/abs/2505.24086</link>
<guid>https://arxiv.org/abs/2505.24086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ComposeAnything，提升复杂物体布局的文本到图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 当前文本到图像(T2I)模型在生成涉及复杂及新颖物体排列的图像时面临重大挑战。尽管先前基于布局的方法通过二维布局的空间约束改进了物体排列，但它们往往难以捕捉三维位置并牺牲了图像质量和连贯性。本研究引入ComposeAnything，这是一种无需重新训练现有T2I模型的新框架。该方法首先利用大型语言模型(LLMs)的链式思维推理能力从文本生成2.5D语义布局，其中包括带有深度信息的二维物体边界框和详细描述。基于此布局，生成具有空间和深度感知的粗略复合物体，作为强且可解释的先验，取代扩散型T2I模型中的随机噪声初始化。这一先验通过对象先验增强和空间控制去噪引导去噪过程，实现复合物体和背景的无缝生成，同时允许对不准确的先验进行优化。ComposeAnything在T2I-CompBench和NSR-1K基准测试中超越了最先进的方法，特别是在具有二维/三维空间排列、高物体数量和超现实组合的提示下。此外，人类评估显示我们的模型生成的高质量图像忠实反映了文本内容。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 20:13:36 GMT</pubDate>
</item>
<item>
<title>OmniResponse：一种多模态大语言模型用于在线对话反馈生成</title>
<link>https://arxiv.org/abs/2505.21724</link>
<guid>https://arxiv.org/abs/2505.21724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniResponse模型解决在线多模态对话反馈生成中的同步问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Online Multimodal Conversational Response Generation (OMCRG) 的新任务，旨在根据发言人的多模态输入实时生成同步的口头及非口头反馈。为了解决音频与面部反馈之间的同步挑战，我们创新性地引入文本作为中间模态进行桥梁连接，并提出了OmniResponse，这是一种利用预训练语言模型增强的多模态大语言模型（MLLM），它能够自回归地生成高质量的多模态听众反馈。OmniResponse通过引入Chrono-Text和TempoVoice两个新组件来提高生成质量，其中Chrono-Text用于时间锚定生成的文本标记，而TempoVoice则是一个可控的在线TTS模块，用于生成与面部表情同步的声音。为了支持进一步的研究，我们还发布了ResponseNet数据集，该数据集包含696组高质量的双向交互视频、多通道音频、转录文本和面部行为标注。在ResponseNet上的综合评估表明，OmniResponse在语义语音内容、音视频同步和生成质量方面显著优于基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 16:12:46 GMT</pubDate>
</item>
<item>
<title>R1-Code-Interpreter：通过代码生成提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.21668</link>
<guid>https://arxiv.org/abs/2505.21668</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出R1-Code-Interpreter模型，显著提高大语言模型在复杂推理任务中的准确性。</p><br /><br /><p><strong>摘要：</strong> 尽管大规模语言模型（LLMs）在推理和规划方面取得了进展，但在需要精确计算、符号操作、优化及算法推理的任务上仍表现不足。本文介绍R1-Code-Interpreter，这是一种文本模型扩展，通过多轮监督微调（SFT）和强化学习（RL）训练，使模型能够自主生成多个代码查询以辅助推理过程。通过在144个推理和规划任务上的实验，该模型在测试集上实现了从44.0%到64.1%的准确率提升，优于GPT-4o的文本模式，并接近配备了代码解释器的GPT-4o。此外，我们还探讨了不同的训练策略和输出格式对模型性能的影响，强调了SFT阶段的关键作用。最终模型及其相关资源已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21668" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 14:47:33 GMT</pubDate>
</item>
<item>
<title>Normalized Attention Guidance (NAG)：一种高效的扩散模型负向引导机制</title>
<link>https://arxiv.org/abs/2505.21179</link>
<guid>https://arxiv.org/abs/2505.21179</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型负向引导方法NAG，解决扩散模型在少步采样中的负向指导失效问题。</p><br /><br /><p><strong>摘要：</strong> 负向引导（抑制不想要的属性）一直是扩散模型的一个基本挑战，尤其是在少步采样设置下。虽然Classifier-Free Guidance (CFG) 在标准条件下表现良好，但在激进的采样步数压缩时会因正负分支预测分歧而失效。本文介绍了一种名为Normalized Attention Guidance (NAG) 的高效、无需训练的机制，它通过注意力空间中的外推、L1范数归一化和细化来实现负向引导。NAG不仅在CFG崩溃的地方恢复了有效的负向引导，还保持了保真度，并且可以跨架构（如UNet、DiT）、采样方式（少步、多步）以及模态（图像、视频）通用使用，具有极低的计算开销。通过广泛的实验，我们展示了NAG在文本对齐(CLIP分数)、保真度(FID、PFID) 和人类感知质量(ImageReward) 方面的一致改进。消融研究验证了每个设计组件的有效性，用户研究也确认了人们对NAG引导输出的显著偏好。作为一种模型不可知的推理时间方法，NAG无需重新训练即可为所有现代扩散框架提供轻松的负向引导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21179" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:30:46 GMT</pubDate>
</item>
<item>
<title>MaskSearch：通过预训练提升大语言模型的通用搜索能力</title>
<link>https://arxiv.org/abs/2505.20285</link>
<guid>https://arxiv.org/abs/2505.20285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的预训练框架MaskSearch，增强大语言模型的检索和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MaskSearch的新预训练框架，该框架旨在通过引入检索增强的掩码预测（RAMP）任务，使大型语言模型（LLMs）具备更强的通用搜索能力。在预训练阶段，模型学习如何利用检索工具填补大量预训练数据中的掩码部分，从而获得检索和推理能力。随后，模型通过监督微调（SFT）和强化学习（RL）进行下游任务训练。实验表明，MaskSearch显著提升了基于LLMs的搜索代理在域内和域外下游任务上的性能，特别是在开放领域多跳问答任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:50 GMT</pubDate>
</item>
<item>
<title>Frankentexts：LLMs生成的一种新型叙事文本研究</title>
<link>https://arxiv.org/abs/2505.18128</link>
<guid>https://arxiv.org/abs/2505.18128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究LLMs生成的Frankentexts，探索可控文本生成的新挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种由大型语言模型（LLMs）生成的新类型长篇叙事文本——Frankentexts，这些文本在生成过程中有高达90%的词汇需直接来自人类写作，因此对可控文本生成提出了严峻挑战。为了生成此类文本，模型需要根据提示选择并整合不同的人类写作片段，并在迭代修改时维持特定的复制比例。实验结果显示Gemini-2.5-Pro在这一任务上表现优异，其生成的Frankentexts中有81%具备连贯性且100%与提示相关，但仍有59%的生成内容被误认为是人类创作。此外，生成的文本因段落间语气突变及语法不一致而可能被人工标注者辨识。这项研究不仅探讨了构建有效检测器以应对新作者身份灰色地带的必要性，还为混合作者身份检测提供了训练数据，并作为研究人机协同写作的试验场。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:38:47 GMT</pubDate>
</item>
<item>
<title>MIKU-PAL：基于多模态自动化管道的情绪语音合成系统</title>
<link>https://arxiv.org/abs/2505.15772</link>
<guid>https://arxiv.org/abs/2505.15772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIKU-PAL系统，实现高一致性情绪语音自动标注。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MIKU-PAL的全自动化多模态管道，用于从无标注视频数据中提取高一致性的情绪语音。通过结合人脸检测与跟踪算法及多模态大语言模型（MLLM），该系统实现了接近人类水平的准确性（MELD数据集上68.5%）和极高的一致性（Fleiss Kappa评分0.93），同时显著降低了成本并提升了效率。此外，MIKU-PAL能够对多达26种细粒度的语音情感类别进行标注，并通过人工验证确认其合理性达到83%。基于此系统，我们还发布了MIKU-EmoBench数据集（131.2小时），作为情感文本转语音及视觉声音克隆的新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:23:12 GMT</pubDate>
</item>
<item>
<title>SmolVLA：高效社区驱动的视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2506.01844</link>
<guid>https://arxiv.org/abs/2506.01844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种小型高效的视觉-语言-动作模型SmolVLA，大幅降低训练和推理成本。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言-动作（VLA）模型通常规模庞大，参数量可达数十亿，导致高昂的训练成本且难以实际部署。这些模型主要依赖学术和工业界的数据集，而忽视了社区收集的廉价机器人平台数据。本文介绍了一种名为SmolVLA的小型、高效且社区驱动的VLA模型，该模型可以在单一GPU上进行训练，并能在消费级GPU甚至CPU上部署，同时保持竞争力的表现。为了提高响应速度，我们还引入了一个异步推理堆栈，将感知和动作预测与执行分离，实现了更高的控制速率和分块动作生成。尽管SmolVLA模型体积较小，但其性能与大10倍的传统VLA模型相当。我们在多种模拟和真实世界机器人基准上评估了SmolVLA，并发布了所有代码、预训练模型和训练数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:30:19 GMT</pubDate>
</item>
<item>
<title>EarthMind：面向多粒度多传感器地球观测数据的理解框架</title>
<link>https://arxiv.org/abs/2506.01667</link>
<guid>https://arxiv.org/abs/2506.01667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EarthMind框架，结合空间注意力提示和跨模态融合，提升大规模视觉语言模型对地球观测数据的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EarthMind的新颖视觉语言框架，旨在解决大规模多模态模型（LMMs）在处理地球观测（EO）数据时面临的挑战。EarthMind通过引入空间注意力提示（SAP）增强像素级理解，并利用跨模态融合将异构模态对齐到共享空间，从而实现高效的信息融合。为了评估多传感器融合的效果，我们构建了EarthMind-Bench基准数据集，包含超过2000个人工标注的多传感器图像问题对。实验结果显示，EarthMind在EarthMind-Bench上达到最先进的性能，且在多个公开的地球观测基准测试中表现优异，证明了其在统一框架下应对多粒度和多传感器挑战的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:36:05 GMT</pubDate>
</item>
<item>
<title>zip2zip：通过动态词汇表优化大语言模型推理效率</title>
<link>https://arxiv.org/abs/2506.01084</link>
<guid>https://arxiv.org/abs/2506.01084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出zip2zip框架，使大语言模型在推理时动态调整词汇表，减少生成标记数并加速推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为zip2zip的新框架，旨在提升大型语言模型（LLMs）的推理效率。传统静态分词器由于固定词汇表无法很好地适应特定领域或语言的输入，导致生成较长的标记序列及更高的计算成本。zip2zip由三个关键部分组成：基于Lempel-Ziv-Welch（LZW）压缩算法的增量式分词器，用于实时生成可重用的“超标记”；运行时计算新生成超标记嵌入的嵌入层；以及一种因果语言建模变体，训练模型处理经过超标记化的压缩序列。通过参数高效的微调，现有LLM可在10个GPU小时内完成zip2zip改造。实验表明，改造后的zip2zip LLM在推理时有效学习使用超标记，将输入和输出序列长度减少了20%-60%，显著降低了推理延迟。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01084" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 13:03:02 GMT</pubDate>
</item>
<item>
<title>基于渐进视图范式的文本引导3D编辑方法</title>
<link>https://arxiv.org/abs/2506.00512</link>
<guid>https://arxiv.org/abs/2506.00512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Pro3D-Editor，通过渐进视图传播实现更一致的3D编辑。</p><br /><br /><p><strong>摘要：</strong> 文本引导的3D编辑技术在游戏和影视制作等领域有广泛应用潜力，但现有方法因忽视多视角间的依赖关系导致编辑不一致。本文提出一种新的渐进视图编辑范式，通过主视图采样、关键视图渲染及全视图优化三个模块，有效提升编辑精度和空间一致性。实验表明，该方法显著优于现有技术。关键词：3D编辑、渐进视图、多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 07:11:55 GMT</pubDate>
</item>
<item>
<title>大规模多语言连续预训练中的平行数据研究</title>
<link>https://arxiv.org/abs/2506.00469</link>
<guid>https://arxiv.org/abs/2506.00469</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究平行数据对Llama3家族模型多语言适应的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模多语言连续预训练中的一个重要设计决策——平行数据的引入。研究重点在于双语翻译数据对Llama3家族模型（如EMMA-500）在500种语言上的多语言适应效果。为此，构建了一个包含超过2500种语言对的MaLA双语文本数据集，并开发了四种基于Llama3基础模型的多语言模型，通过混合数据进行连续预训练，总训练量达6710亿tokens。评估结果显示，双语数据显著提升了低资源语言的迁移能力和性能表现。所有相关数据集、模型及代码均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00469" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 04:37:17 GMT</pubDate>
</item>
<item>
<title>蒸馏模型对抗性偏见注入漏洞及传播机制研究</title>
<link>https://arxiv.org/abs/2505.24842</link>
<guid>https://arxiv.org/abs/2505.24842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示蒸馏模型易受训练阶段对抗性偏见注入影响且传播加剧。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了蒸馏语言模型在训练过程中因轻微数据投毒而引入偏见的脆弱性。通过两种传播模式——目标性和非目标性传播，实验表明即使仅使用0.25%的污染数据，学生模型也会在目标场景下生成偏见响应的概率高达76.9%，超过教师模型的69.4%。非目标传播中，学生模型在未见过的任务上显示出6到29倍更高的偏见频率。研究覆盖六类偏见类型及多种蒸馏方法和模态，验证了当前防御措施如困惑度过滤、偏见检测系统和大语言模型自动评分框架的不足，揭示了蒸馏模型显著的安全隐患，并提出了针对性防护设计原则。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:41:58 GMT</pubDate>
</item>
<item>
<title>AReaL：一种用于大规模语言模型强化学习的全异步系统</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种全异步的强化学习系统AReaL，大幅提升了GPU利用率并加快训练速度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AReaL的全新强化学习系统，专门针对大规模语言模型的推理任务设计。传统的同步强化学习方法由于需要等待批次中最长生成完成而造成GPU资源浪费，而AReaL通过完全解耦生成和训练过程实现了全异步操作，从而显著提高了GPU利用率。此外，AReaL还引入了一系列系统级优化措施，例如动态平衡生成和训练的工作负载以控制数据陈旧度，并采用增强型PPO算法处理过时样本。实验表明，AReaL在数学和代码推理基准测试中的训练速度比最佳同步系统快2.57倍，同时最终性能持平或有所提升。该系统的代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 03:18:25 GMT</pubDate>
</item>
<item>
<title>CodeV-R1：基于强化学习带验证奖励的硬件描述语言自动生成框架</title>
<link>https://arxiv.org/abs/2505.24183</link>
<guid>https://arxiv.org/abs/2505.24183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CodeV-R1框架，解决EDA领域自然语言到Verilog代码生成的三大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CodeV-R1的强化学习带验证奖励（RLVR）框架，用于训练生成硬件描述语言（HDL）如Verilog的大型语言模型（LLMs）。在电子设计自动化（EDA）领域，将RLVR扩展到从自然语言规范自动生成Verilog代码面临缺乏自动化且精确的验证环境、高质量自然语言-代码对稀缺性以及RLVR计算成本高昂等三大挑战。为应对这些挑战，CodeV-R1首先开发了一种基于规则的测试平台生成器，可针对黄金参考进行稳健的等效性检查；其次提出了一种往返数据合成方法，将开源Verilog代码片段与LLM生成的自然语言描述配对，并通过生成的测试平台验证代码-自然语言-代码一致性，筛选出不等价的例子以获得高质量数据集；最后采用两阶段“先蒸馏后强化学习”训练管道：蒸馏用于推理能力的冷启动，随后使用自适应DAPO算法进一步优化，该算法可根据需要动态调整采样率从而降低训练成本。最终生成的模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上的pass@1分别达到了68.6%和72.9%，比前人工作提升了12~20%，并达到甚至超过了671B参数规模的DeepSeek-R1的表现。研究团队计划公开发布此模型、训练管道及数据集，以推动EDA和LLM领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 23:51:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型主观倾向评估：Preference, Opinion, and Belief 调查</title>
<link>https://arxiv.org/abs/2505.19621</link>
<guid>https://arxiv.org/abs/2505.19621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型的主观偏好、意见和信念，揭示其一致性下降及偏见增加的趋势。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Preference, Opinion, and Belief（POBs）的新基准，用于评估大型语言模型（LLMs）在社会、文化、伦理和个人领域的主观倾向。通过对开放源码和闭源LLMs的测试，我们衡量了可靠性、中立性和一致性等特性。此外，我们还探讨了通过推理和自我反思机制提高计算能力对这些指标的影响。尽管这些机制在其他任务中有效，但我们的结果显示其在本领域中的提升有限。进一步分析表明，较新的模型版本表现出较低的一致性并更倾向于特定观点，这凸显了一个值得关注的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:41:21 GMT</pubDate>
</item>
<item>
<title>RoboMaster：一种基于协作轨迹建模的多对象交互视频扩散模型</title>
<link>https://arxiv.org/abs/2506.01943</link>
<guid>https://arxiv.org/abs/2506.01943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboMaster框架，解决现有方法无法有效捕捉多对象交互的问题。</p><br /><br /><p><strong>摘要：</strong> 近期视频扩散模型在生成机器人决策数据方面展现了巨大潜力，但现有的轨迹导向方法主要针对单个物体运动，难以捕获复杂操作中的多物体交互。此问题源于重叠区域的多特征纠缠，导致视觉保真度下降。为解决这一局限性，我们提出了RoboMaster框架，通过协作轨迹公式化建模物体间动力学。不同于以往分解物体的方法，我们将其交互过程分解为三个子阶段：预交互、交互和后交互，分别利用主导物体（如机械臂或操作对象）的特征进行建模。此外，为了确保视频中物体的主体语义一致性，我们引入了外观和形状感知的潜在表示。在Bridge V2数据集上的实验及野外评估表明，该方法优于现有技术，达到了轨迹控制视频生成的新高度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:57:06 GMT</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:54:39 GMT</pubDate>
</item>
<item>
<title>STORM框架：任务型对话系统中的非对称信息处理与意图形成建模</title>
<link>https://arxiv.org/abs/2506.01881</link>
<guid>https://arxiv.org/abs/2506.01881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STORM框架通过UserLLM和AgentLLM协作，解决任务型对话系统中意图定义不明确的问题。</p><br /><br /><p><strong>摘要：</strong> 任务型对话系统面临用户表达看似完整却缺乏必要结构信息的问题，这源于用户需求不明晰且系统需要精确意图定义。当前基于大型语言模型的代理无法有效区分语法完整与语境触发的表达，缺乏协作意图形成框架。我们提出STORM框架，通过UserLLM（全内部访问）与AgentLLM（仅可观察行为）之间的对话动态建模，生成捕捉表达轨迹和潜在认知转变的注释语料库，从而系统分析协作理解的发展。STORM的主要贡献包括：(1)形式化对话系统中的非对称信息处理；(2)建模意图形成并追踪协作理解演化；(3)设计衡量内部认知改进及任务表现的评估指标。实验显示，在特定场景下适度不确定性（40%-60%）优于完全透明性，模型特定模式表明需重新考虑人机协作中的最优信息完整性。这些发现有助于理解非对称推理动态，并为校准不确定性的对话系统设计提供指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:11:10 GMT</pubDate>
</item>
<item>
<title>ShapeLLM-Omni：一种支持文本与3D资产双向交互的原生大型语言模型</title>
<link>https://arxiv.org/abs/2506.01853</link>
<guid>https://arxiv.org/abs/2506.01853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种可理解和生成3D资产的原生大型语言模型ShapeLLM-Omni。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ShapeLLM-Omni的新型原生三维大型语言模型，该模型能够处理文本与三维资产之间的双向交互任务。首先，通过训练一个基于3D矢量量化变分自编码器（VQVAE），实现了对三维物体的有效离散化表示及其高效重构。在此基础上，构建了一个大规模连续训练数据集3D-Alpaca，用于生成、理解和编辑3D内容，为后续研究提供了丰富的资源。最后，在3D-Alpaca数据集上对Qwen-2.5-vl-7B-Instruct模型进行了指令微调，进一步增强了模型的多模态能力。这项工作为扩展多模态模型的基本三维功能奠定了基础，有助于推动三维原生人工智能的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:40:50 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型处理复杂指令的能力</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种结合分解与强化学习的方法，有效提升大语言模型处理复杂指令的能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型（LLMs）在面对复杂指令时面临挑战，尤其是在多重约束并行、链式及分支结构组织的情况下。尽管链式思维（CoT）被普遍认为可以改善LLMs能力，但传统CoT方法由于简单的重复指令模式导致性能下降。为解决此问题，本文提出了一种系统性方法，通过激励测试时计算扩展的推理过程来增强LLMs的指令处理能力。首先，我们基于现有分类对复杂指令进行分解，并提出可重现的数据获取方法；其次，利用基于可验证规则的奖励信号进行强化学习，专门培养模型的指令跟随推理能力。通过样本对比优化CoT执行，同时采用专家行为克隆技术引导快速思维LLMs向技能型推理器转变。在七个综合基准测试中的广泛评估显示，该方法使1.5B规模的LLM取得了相当于8B规模LLM的性能，提升了11.74%。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 04:11:44 GMT</pubDate>
</item>
<item>
<title>Scaling with Gradient Grouping (SGG): 改进大规模语言模型优化的新方法</title>
<link>https://arxiv.org/abs/2506.01049</link>
<guid>https://arxiv.org/abs/2506.01049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过动态分组和特定缩放提升自适应学习率估计的优化器包装器SGG。</p><br /><br /><p><strong>摘要：</strong> 训练大规模语言模型面临参数规模大和架构异构性等挑战，现有的自适应优化器如AdamW在梯度变化处理上仍显不足，导致训练不稳定、收敛缓慢且与参数高效微调技术兼容性差。本文介绍了一种名为Scaling with Gradient Grouping (SGG) 的优化器包装器，通过动态分组和分组特定缩放改进自适应学习率估计。SGG首先将每一层中的梯度统计分为若干簇，然后对各参数应用簇特定的缩放以校准学习率，从而在维持每参数精确适应的同时施加集体群组约束。实验表明，SGG与现有优化器无缝集成，在多种大规模语言模型基准测试中表现出一致性增益和更快收敛速度，适用于不同模型大小。此外，SGG在不同批量大小和学习率下均表现稳定，成为大规模语言模型优化的稳健选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 11:30:37 GMT</pubDate>
</item>
<item>
<title>Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00996</link>
<guid>https://arxiv.org/abs/2506.00996</guid>
<content:encoded><![CDATA[
Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:57:43 GMT</pubDate>
</item>
<item>
<title>大型语言模型在多选题中的局限性及改进方法</title>
<link>https://arxiv.org/abs/2506.00643</link>
<guid>https://arxiv.org/abs/2506.00643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现当前大型语言模型在识别所有正确答案时存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在单选题评估中表现良好，但在实际应用中需要识别所有正确答案的能力却未得到充分研究。本研究引入了SATA-BENCH，这是首个专注于评估LLMs在跨领域多选题上的表现的基准，包括阅读理解、法律和生物医学等领域。通过对27个开源和专有模型的测试显示，最强模型也只能达到41.8%的精确匹配率，揭示了LLMs在这方面的不足。问题主要源于选择偏差和计数偏差两个核心挑战。为解决这些问题，我们提出了Choice Funnel解码策略，该策略结合了令牌去偏和自适应阈值化，使模型在精确匹配上提高了高达29%，同时降低了超过64%的推理成本。本研究揭示了当前LLMs的根本局限性，并提出了一种新的诊断和改进多答案推理的框架。我们发布了SATA-BENCH和Choice Funnel，以促进LLMs在现实多答案应用场景中的稳健发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 13:14:21 GMT</pubDate>
</item>
<item>
<title>通过后训练技术提升大型语言模型在多智能体系统中的经济推理能力</title>
<link>https://arxiv.org/abs/2506.00577</link>
<guid>https://arxiv.org/abs/2506.00577</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示后训练技术可有效增强大型语言模型在多智能体经济推理场景中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了监督微调(SFT)和基于可验证奖励的强化学习(RLVR)是否能有效提升大型语言模型(LLMs)在多智能体系统(MAS)中的泛化能力。以经济学推理作为测试平台，研究团队开发了Recon，一款基于70亿参数的开源LLM，该模型经过精心策划的2100个高质量经济学推理问题的数据集后训练而成。实验结果显示，在经济学推理基准测试和多智能体游戏中，Recon展现出显著增强的结构化推理能力和经济理性。这些发现表明领域对齐的后训练方法在提升推理能力和模型对齐方面具有巨大潜力，同时揭示了SFT和RL在塑造模型行为中的重要作用。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00577" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 10:22:40 GMT</pubDate>
</item>
<item>
<title>ARIA：通过意图空间奖励聚合提升语言模型强化学习效能</title>
<link>https://arxiv.org/abs/2506.00539</link>
<guid>https://arxiv.org/abs/2506.00539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIA方法，解决开放域语言环境中奖励稀疏问题，显著提升强化学习效果。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型(LLMs)使智能体能够通过自然语言交互执行复杂推理和决策。然而，在开放域语言行动环境（如谈判或问答游戏）中，动作空间可能被表述为令牌联合分布，导致动作空间呈指数级增长。这种情况下采样动作会导致极端的奖励稀疏性，增加奖励方差，阻碍有效强化学习(RL)。为了解决这个问题，我们提出了ARIA方法，即通过在意图空间中聚合奖励，实现高效且有效的语言智能体训练。ARIA旨在将自然语言动作从高维联合令牌分布空间投影到低维意图空间，其中语义相似的动作被聚类并分配共享奖励。这种意图感知的奖励聚合通过密集化奖励信号降低了奖励方差，促进了更好的策略优化。大量实验表明，ARIA不仅显著减少了策略梯度方差，还在四个下游任务中平均带来了9.95%的性能提升，始终优于离线和在线RL基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 08:54:49 GMT</pubDate>
</item>
<item>
<title>LoHoVLA：一种针对长时序任务的统一视觉语言动作框架</title>
<link>https://arxiv.org/abs/2506.00411</link>
<guid>https://arxiv.org/abs/2506.00411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的统一视觉语言动作框架LoHoVLA，提升长时序任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对具身代理在处理长时序任务时面临的挑战，提出了一种名为LoHoVLA的新型统一视觉语言动作框架。该框架结合了大型预训练视觉语言模型作为主干网络，用于同时生成子任务的语言描述和机器人动作预测的令牌，从而实现高效的任务分解与动作规划。此外，LoHoVLA引入了一种分层闭环控制机制，以缓解高阶规划和低阶控制中的误差问题。为了验证框架的有效性，构建了一个包含20种长时序任务及1000个专家演示样本的数据集LoHoSet。实验结果显示，LoHoVLA在Ravens模拟器中显著优于现有的分层方法和标准视觉语言动作模型，展示了统一架构在提升可泛化具身智能方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 02:01:03 GMT</pubDate>
</item>
<item>
<title>MagiCodec：一种基于Transformer的高效音频编解码器</title>
<link>https://arxiv.org/abs/2506.00385</link>
<guid>https://arxiv.org/abs/2506.00385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型单层Transformer音频编解码器，提升编码语义表达同时保持高重建质量。</p><br /><br /><p><strong>摘要：</strong> 神经音频编解码器近年来在将原始音频波形高效映射为离散标记表示方面取得了显著进展，但现有编解码器多侧重于重建质量，而忽视了对下游模型可用性的优化。针对这一瓶颈，本文提出了MagiCodec，这是一种新颖的单层、流式Transformer架构的音频编解码器。通过引入多阶段训练流程，包括高斯噪声注入和潜在正则化技术，MagiCodec旨在增强生成代码的语义表达能力，同时保持高水平的重建精度。实验表明，MagiCodec在重建质量和下游任务性能上均优于现有最先进方法，其生成的标记表现出类似自然语言的Zipf分布，提升了与基于语言模型的生成架构的兼容性。此外，我们还分析了频率域中噪声注入的效果，证明其有助于抑制高频成分并促进稳健的标记化过程。源代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00385" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:31:02 GMT</pubDate>
</item>
<item>
<title>基于YODAS扩展的Open Whisper-style Speech Models V4</title>
<link>https://arxiv.org/abs/2506.00338</link>
<guid>https://arxiv.org/abs/2506.00338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过整合大规模网络爬取数据集YODAS提升OWSM模型性能。</p><br /><br /><p><strong>摘要：</strong> Open Whisper-style Speech Models (OWSM)项目利用学术资源开发了一系列完全开源的语音基础模型，但其训练数据量不足。本研究通过引入Creative Commons许可的YODAS数据集增强OWSM，但由于YODAS的野性数据特性（如语言标签错误和音频文本错配），带来了诸多挑战。为此，我们开发了一套可扩展的数据清洗流水线，最终获得涵盖75种语言的166,000小时语音数据集。基于此清洗后的数据训练的新版OWSM v4模型，在多语言基准测试中显著超越旧版本，甚至在多个场景中达到或超过了工业前沿模型Whisper和MMS的表现。我们将公开发布清理后的YODAS数据、预训练模型及相关脚本，均通过ESPnet工具包提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 21:44:44 GMT</pubDate>
</item>
<item>
<title>MiCRo：基于大规模二元偏好数据的学习框架提升个性化奖励建模</title>
<link>https://arxiv.org/abs/2505.24846</link>
<guid>https://arxiv.org/abs/2505.24846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiCRo通过两阶段框架增强个性化奖励建模，无需显式细粒度标注。</p><br /><br /><p><strong>摘要：</strong> 奖励建模是应用强化学习从人类反馈（RLHF）构建安全基础模型的关键步骤，但在利用Bradley-Terry（BT）模型时，假设单一全局奖励函数无法捕捉人类偏好的多样性。这种简化限制了大语言模型（LLMs）支持个性化和多元对齐的能力。理论上，当人类偏好符合多样子群的混合分布时，单一BT模型存在不可减少的误差。尽管已有解决方案如多目标学习结合细粒度注释有所改善，但成本高昂且受限于预定义属性，未能充分反映人类价值观的丰富性。本文提出MiCRo，一种两阶段框架，在不依赖显式细粒度注释的情况下，通过大规模二元偏好数据提升个性化偏好学习。第一阶段引入上下文感知的混合建模方法捕获多样化的人类偏好；第二阶段整合在线路由策略，动态调整混合权重以解决歧义，实现高效可扩展的偏好适配。多项偏好数据集上的实验表明，MiCRo有效捕捉多样化的人类偏好并显著提升下游个性化性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:44:28 GMT</pubDate>
</item>
<item>
<title>Reasoning Gym：基于可验证奖励的强化学习环境库</title>
<link>https://arxiv.org/abs/2505.24760</link>
<guid>https://arxiv.org/abs/2505.24760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的推理环境库，支持生成无限复杂度训练数据。</p><br /><br /><p><strong>摘要：</strong> Reasoning Gym (RG) 是一种专为强化学习设计的推理环境库，提供超过100个跨多个领域的数据生成器和验证器，涵盖代数、算术、几何、逻辑等多个领域。与传统固定的数据集不同，RG 的创新之处在于可以通过程序化生成方式提供几乎无限的训练数据，并且可以根据需求调整复杂度。这种特性使得模型可以在不同的难度等级下进行持续评估。实验结果表明，RG 在评估和提升推理模型方面具有显著效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:20:18 GMT</pubDate>
</item>
<item>
<title>基于视频的3D几何大语言模型在场景理解中的应用</title>
<link>https://arxiv.org/abs/2505.24625</link>
<guid>https://arxiv.org/abs/2505.24625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需3D输入的视频到3D几何大语言模型，提升场景理解和空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究通过引入视频到3D几何大语言模型（VG LLM），使多模态大语言模型能够在没有全面3D数据输入的情况下直接理解3D场景。我们设计了一个3D视觉几何编码器，从视频序列中提取3D先验信息，并将其与视觉标记结合输入到多模态大语言模型中。实验表明，该方法在多个3D场景理解和空间推理任务中表现优异，尤其是在VSI-Bench评估中，我们的4B参数模型甚至超过了现有的最先进方法Gemini-1.5-Pro。这一进展显著降低了对复杂3D数据的需求，提升了模型的通用性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 10:16:41 GMT</pubDate>
</item>
<item>
<title>统一预算感知学习率调度器UBA的研究</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种理论支持的预算感知学习率调度器UBA，优化多种架构和任务的训练表现。</p><br /><br /><p><strong>摘要：</strong> 随着计算成本的增加和资源限制，预算迭代训练成为实现最优学习的关键需求。然而，现有的学习率调度设计主要基于经验法则，缺乏理论依据，且需要大量试错，导致效率低下。本研究提出了统一预算感知（UBA）调度器，这是一种理论上成立的学习率调度方法，在不同的网络架构和任务中均优于常用调度方案。通过构建新的预算感知优化框架，UBA消除了对每种网络进行数值优化的需求，并通过理论分析建立了超参数φ与条件数之间的联系。此外，我们证明了不同φ值下的收敛性，并提供了选择φ的实用指南。实验结果显示，UBA在多种视觉和语言任务中表现出色，适用于不同规模的网络架构和训练迭代预算。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24452" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 06:38:03 GMT</pubDate>
</item>
<item>
<title>VisualSphinx：首个大规模合成视觉逻辑推理训练数据集</title>
<link>https://arxiv.org/abs/2505.23977</link>
<guid>https://arxiv.org/abs/2505.23977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模合成视觉逻辑推理数据集VisualSphinx，提升视觉语言模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉语言模型在多模态推理方面表现不足，主要由于缺乏大规模且结构良好的训练数据集。为解决这一问题，本文提出了VisualSphinx，这是一个专门针对视觉逻辑推理设计的大规模合成训练数据集。通过引入一种基于规则的图像合成管道，该方法可以从基础问题中提取并扩展出谜题规则，并生成具有定位答案的合成图像。实验表明，利用VisualSphinx训练的视觉语言模型在逻辑推理任务中表现出更高的逻辑连贯性和可读性，并且在代数推理、算术推理和几何推理等其他推理任务中也显示出显著性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 16:08:36 GMT</pubDate>
</item>
<item>
<title>Cora：一种基于语义对应的新图像编辑框架</title>
<link>https://arxiv.org/abs/2505.23907</link>
<guid>https://arxiv.org/abs/2505.23907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cora通过引入对应感知噪声校正和插值注意力图改进图像编辑效果。</p><br /><br /><p><strong>摘要：</strong> 图像编辑在计算机图形学、视觉和视觉特效领域至关重要，但涉及显著结构变化的编辑仍具挑战性。现有方法常产生纹理不相关等伪影或难以保留源图像的关键属性。本文提出Cora框架，通过语义对应实现精确的纹理转移并生成新内容，同时提供对生成与保留平衡的控制。实验表明，Cora在多种编辑任务中表现优异，用户研究进一步验证其优越性。关键词：图像编辑、语义对应、扩散模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:00:56 GMT</pubDate>
</item>
<item>
<title>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles</title>
<link>https://arxiv.org/abs/2505.23590</link>
<guid>https://arxiv.org/abs/2505.23590</guid>
<content:encoded><![CDATA[
The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:01:22 GMT</pubDate>
</item>
<item>
<title>VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.23504</link>
<guid>https://arxiv.org/abs/2505.23504</guid>
<content:encoded><![CDATA[
Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 10:48:10 GMT</pubDate>
</item>
<item>
<title>From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval</title>
<link>https://arxiv.org/abs/2505.23059</link>
<guid>https://arxiv.org/abs/2505.23059</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:04:25 GMT</pubDate>
</item>
<item>
<title>DyePack：通过后门攻击检测大语言模型对基准测试集的依赖</title>
<link>https://arxiv.org/abs/2505.23001</link>
<guid>https://arxiv.org/abs/2505.23001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DyePack框架，利用后门攻击检测训练中是否使用过基准测试集。</p><br /><br /><p><strong>摘要：</strong> 开放基准对于评估和改进大型语言模型至关重要，但其易访问性使其容易受到测试集污染的影响。本文介绍了一种名为DyePack的框架，它通过后门攻击在不访问模型损失、logits或内部细节的情况下识别出是否在训练过程中使用了基准测试集。DyePack的设计包括多个具有随机目标的后门，可精确计算误报率(FPR)，从而有效防止错误指控并提供确凿证据。我们在三个数据集上的五种模型上进行了评估，涵盖了多项选择题和开放式生成任务，结果显示DyePack在多种任务中均成功检测到受污染模型，并保证了极低的误报率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 22:22:14 GMT</pubDate>
</item>
<item>
<title>多模态大模型强化学习框架提升泛化能力</title>
<link>https://arxiv.org/abs/2505.24871</link>
<guid>https://arxiv.org/abs/2505.24871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态语言模型通过强化学习实现跨领域优化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种针对多模态大型语言模型（MLLMs）的强化学习框架，名为RLVR，通过整合多种具有可验证答案的视觉语言任务数据集进行后训练。该框架解决了多数据集训练过程中目标冲突的问题，提出了优化的数据混合策略。实验表明，采用这种策略的多领域强化学习显著提升了模型的泛化能力和推理性能，在分布外基准测试中的准确率平均提高了5.24%，相较均匀数据混合的模型提升了20.74%。此外，该研究还开发了在线强化学习模块，支持不同领域的可验证奖励机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>SiLVR: A Simple Language-based Video Reasoning Framework</title>
<link>https://arxiv.org/abs/2505.24869</link>
<guid>https://arxiv.org/abs/2505.24869</guid>
<content:encoded><![CDATA[
Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>EXP-Bench：评估AI代理完成完整研究实验的能力</title>
<link>https://arxiv.org/abs/2505.24785</link>
<guid>https://arxiv.org/abs/2505.24785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EXP-Bench评估AI代理在设计、执行和分析完整研究实验中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EXP-Bench，这是一个用于评估AI代理在完整研究实验中能力的新基准。EXP-Bench从顶级AI研究论文及其开源代码中提取并结构化关键实验细节，创建了461个复杂的任务。通过测试基于大型语言模型的领先AI代理，发现其在单个实验方面（如设计或实施正确性）偶尔可达20-35%，但整体可执行实验的成功率仅为0.5%。EXP-Bench揭示了现有AI代理在科学研究自动化中的瓶颈，并为未来改进提供了方向。该基准已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:46:29 GMT</pubDate>
</item>
<item>
<title>DINO-R1：通过强化学习实现视觉基础模型的上下文推理能力</title>
<link>https://arxiv.org/abs/2505.24025</link>
<guid>https://arxiv.org/abs/2505.24025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DINO-R1，首个利用强化学习增强视觉基础模型推理能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DINO-R1，这是首个尝试通过强化学习激励视觉基础模型上下文推理能力的工作。DINO-R1引入了组相对查询优化（GRQO），这是一种专为基于查询的表示模型设计的新型强化风格训练策略，通过组归一化的对齐质量计算查询级奖励。此外，还应用了KL正则化来稳定物体分布，减少训练不稳定性。这种联合优化实现了跨查询的密集且表达性监督，同时减轻了过拟合和分布漂移问题。基于Grounding-DINO，我们训练了一系列DINO-R1家族模型，这些模型集成了视觉提示编码器和视觉引导的查询选择机制。在COCO、LVIS和ODinW上的广泛实验表明，DINO-R1显著优于监督微调基线，在开放词汇和闭集视觉提示场景中表现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 17:58:06 GMT</pubDate>
</item>
<item>
<title>OmNIGUARD：一种多语言跨模态有害提示检测方法</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmNIGUARD显著提升了多语言和跨模态有害提示的检测准确率。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的能力不断提升，其潜在的有害滥用问题引发了广泛关注。现有检测方法存在不足，尤其容易受到利用模型能力不匹配的攻击影响。为解决这一挑战，本文提出OmNIGUARD，这是一种用于检测多种语言和模态下有害提示的方法。OmNIGUARD通过识别模型内部表示中跨语言或跨模态对齐的部分，构建出一种语言无关或模态无关的分类器。实验结果显示，在多语言环境下，OmNIGUARD比最强基线提高了11.57%的分类准确率；在基于图像的提示检测中提高了20.44%；并且在音频提示检测中达到了新的最佳性能。此外，由于重用了生成过程中计算的嵌入，OmNIGUARD还具有很高的效率，大约比下一个最快的基线快120倍。代码和数据已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 01:25:27 GMT</pubDate>
</item>
<item>
<title>ReasonGen-R1：结合推理与强化学习的生成视觉模型</title>
<link>https://arxiv.org/abs/2505.24875</link>
<guid>https://arxiv.org/abs/2505.24875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReasonGen-R1框架，将推理能力引入生成视觉模型并优化图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReasonGen-R1的两阶段框架，旨在将基于文本的推理能力整合到生成视觉模型中。首先通过在新生成的书面理由数据集上进行有监督微调，使自回归图像生成器获得显式的“思考”技能；然后利用Group Relative Policy Optimization (GRPO) 对其输出进行进一步优化。为了支持模型在生成图像前通过文本进行推理，我们自动创建并发布了一个由模型生成的理由与视觉提示配对的语料库，从而实现对象布局、风格和场景构成的可控规划。GRPO算法使用预训练的视觉语言模型的奖励信号来评估整体视觉质量，并在每次更新中优化策略。在GenEval、DPG和T2I基准测试中的评估表明，ReasonGen-R1在性能上显著优于强大的基线模型和先前的最先进模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>Context is Gold to find the Gold Passage: Evaluating and Training Contextual Document Embeddings</title>
<link>https://arxiv.org/abs/2505.24782</link>
<guid>https://arxiv.org/abs/2505.24782</guid>
<content:encoded><![CDATA[
A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:43:28 GMT</pubDate>
</item>
<item>
<title>基于Matryoshka表征学习的阿拉伯语文本语义相似度模型</title>
<link>https://arxiv.org/abs/2505.24581</link>
<guid>https://arxiv.org/abs/2505.24581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的阿拉伯语文本嵌入模型，在STS任务中表现超越大型预训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GATE（General Arabic Text Embedding）的新模型，该模型在阿拉伯语文本的语义相似度（STS）任务上取得了最先进的性能。由于高质量数据集和预训练模型的缺乏，阿拉伯语的STS研究一直受到限制。GATE通过利用Matryoshka表征学习方法和基于阿拉伯语三元组数据集的混合损失训练策略，显著提升了模型在细粒度语义理解任务上的表现。实验结果显示，GATE在STS基准测试中比包括OpenAI在内的更大模型高出20-25%的性能，成功捕捉了阿拉伯语独特的语义特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 09:29:03 GMT</pubDate>
</item>
<item>
<title>小语言模型在特定领域任务中的质量优势</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，即使大语言模型成本降低，小语言模型在结构化输出任务中仍具10%的质量优势。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）如GPT-4o能够通过适当的提示处理复杂任务，而随着其令牌成本的下降，小语言模型（SLMs）在实际应用中的速度和成本优势可能不再明显。本文通过对比小语言模型微调与直接提示大型语言模型生成JSON形式低代码工作流的表现，发现微调方法在领域特定任务中平均提高了10%的质量。此外，我们进行了系统性错误分析以揭示模型局限性，进一步支持小语言模型在特定场景下的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 23:59:35 GMT</pubDate>
</item>
<item>
<title>LLM安全研究中的语言多样性分析</title>
<link>https://arxiv.org/abs/2505.24119</link>
<guid>https://arxiv.org/abs/2505.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLM安全领域存在显著的语言差距且多聚焦英语。</p><br /><br /><p><strong>摘要：</strong> 本文通过系统回顾2020年至2024年间*ACL主要会议及研讨会近300篇论文，分析大型语言模型（LLM）安全性研究的语言多样性现状，发现该领域具有明显的英语中心倾向，对高资源非英语语言的关注度极低。研究还指出，非英语语言鲜少被单独研究，而英语安全研究的文档记录实践也存在不足。基于调查，我们提出多语言安全研究的若干建议，并提出了三个具体未来方向：安全性评估、训练数据生成和跨语言安全性泛化。本研究旨在推动构建更稳健、包容的AI安全措施，以适应全球多样化人群的需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:32:44 GMT</pubDate>
</item>
<item>
<title>基于角色的自适应奖励模型提升对话系统真实性</title>
<link>https://arxiv.org/abs/2505.23923</link>
<guid>https://arxiv.org/abs/2505.23923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型ChARM，显著提高角色扮演语言代理的学习效率和评价性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统奖励模型在角色扮演语言代理(RPLAs)中的可扩展性和适应性不足问题，提出了ChARM模型。该模型通过引入基于角色的自适应边距和利用大规模无标注数据的自我进化机制，解决了现有方法的瓶颈。此外，还构建了首个大规模偏好数据集RoleplayPref和专用评估基准RoleplayEval，包含1,108个角色及16,888段双语对话。实验表明，ChARM相比传统的Bradley-Terry模型在偏好排名上提升了13%，并在CharacterEval和RoleplayEval中取得了最佳性能。相关代码和数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:15:18 GMT</pubDate>
</item>
<item>
<title>LEGAR BENCH与LegalSearchLM：解决法律案例检索难题</title>
<link>https://arxiv.org/abs/2505.23832</link>
<guid>https://arxiv.org/abs/2505.23832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模韩语法律案例检索基准LEGAR BENCH及新模型LegalSearchLM。</p><br /><br /><p><strong>摘要：</strong> Legal Case Retrieval (LCR) 是法律专业人士研究和决策的基础任务，但现有研究存在小规模语料库和有限表示能力的问题。为此，我们推出了LEGAR BENCH，这是首个涵盖120万案件、411种犯罪类型的韩语法律案例检索基准。同时，开发了LegalSearchLM模型，通过法律元素推理和约束解码生成目标案件内容，显著提升了检索性能，在LEGAR BENCH上比基线模型高出6-20%，并展示了出色的跨领域泛化能力。实验表明，该模型不仅在性能上达到最新水平，还能有效避免无关匹配问题，为法律检索提供全新解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 05:02:41 GMT</pubDate>
</item>
<item>
<title>双向线性运算在循环神经网络中的作用及其对记忆建模的影响</title>
<link>https://arxiv.org/abs/2505.21749</link>
<guid>https://arxiv.org/abs/2505.21749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示双向线性运算如何增强循环神经网络的记忆建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了循环神经网络中隐藏单元的作用，提出它们不仅仅是被动存储器，而是积极参与计算的重要组件。通过理论分析和实证研究，我们发现双向线性操作（即隐藏单元与输入嵌入之间的乘法交互）自然地为状态跟踪任务中隐藏状态的演变提供了偏差。此外，我们展示了双向线性状态更新形成了一种自然的层次结构，其中流行的线性递归网络如Mamba位于该层次结构的最低复杂度中心。这一发现为理解隐藏单元在复杂任务中的动态行为提供了新的视角，同时推动了对循环神经网络设计的进一步探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 16:38:19 GMT</pubDate>
</item>
<item>
<title>基于协调扩散噪声优化框架的全身操作合成</title>
<link>https://arxiv.org/abs/2505.21437</link>
<guid>https://arxiv.org/abs/2505.21437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架解决人体与物体交互中的运动协调与精度问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究全身影操控合成问题，包括身体、手部及物体的运动协调，这是虚拟人和机器人领域的重要挑战。主要难题在于手部与身体其他部分的紧密协作需求以及对高自由度物体操控的精确性要求。为解决这些问题，我们提出了一种新的协调扩散噪声优化框架。通过三个专门的扩散模型分别处理身体、左手和右手的运动，这些模型各自训练特定的数据集以提升泛化能力。协调性自然地通过人体运动链上的梯度流动实现，使得全局身体姿势能够高度忠实于手部动作目标进行适应。此外，采用基于基点集(BPS)的统一表示法增强手-物交互的精确性，该方法将末端执行器位置编码为与物体几何相同的BPS距离，从而捕捉手部与物体部件间的细微空间关系。实验表明，我们的方法在运动质量和物理真实性方面优于现有技术，并支持多种功能如物体姿态控制、行走与操控同时进行以及仅凭手部数据生成全身动作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:11:50 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型的模态偏好研究与调控方法</title>
<link>https://arxiv.org/abs/2505.20977</link>
<guid>https://arxiv.org/abs/2505.20977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现多模态大型语言模型普遍表现出模态偏好，并提出一种无需微调的方法来控制这种偏好。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）在处理复杂多模态任务时表现优异，但其模态偏好（即倾向于优先使用某一模态的信息）尚未得到充分研究。本文构建了一个名为MC²的基准测试集，通过受控冲突场景评估模型的模态偏好。实验显示，所有被测的18种MLLMs均表现出显著的模态偏见，且这种偏好可通过外部干预改变。进一步分析表明，模态偏好可体现在模型的潜在表示中。基于此，我们提出了基于表征工程的探针与引导方法，无需额外微调或精心设计提示即可显式控制模态偏好。该方法在幻觉抑制和多模态机器翻译等下游任务上取得了令人鼓舞的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 06:07:59 GMT</pubDate>
</item>
<item>
<title>面向形式化验证的大语言模型不确定性量化研究</title>
<link>https://arxiv.org/abs/2505.20047</link>
<guid>https://arxiv.org/abs/2505.20047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究解决大语言模型生成形式化规范时的确定性问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自动化推理的形式化规范生成方面展现出巨大潜力，但其概率性质与形式验证所需的确定性保证之间存在根本矛盾。本文系统评估五种前沿LLMs生成的形式化工件，揭示基于可满足模理论（SMT）的自动形式化对逻辑任务和事实任务准确性的影响差异（+34.8%至-44.5%），并发现传统不确定性量化技术难以识别这些错误。我们提出一种基于概率上下文无关文法（PCFG）的框架，建立细化的不确定性分类体系，并发现不确定性信号具有任务依赖性。最终，通过轻量级融合这些信号实现选择性验证，大幅减少错误率（14%-100%），同时保持较低的弃权率，将LLM驱动的形式化转变为可靠的工程学科。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:34:04 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型推理链长度对视觉接地的影响研究</title>
<link>https://arxiv.org/abs/2505.21523</link>
<guid>https://arxiv.org/abs/2505.21523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多模态数学推理中推理链越长模型越倾向于脱离图像内容而增加幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在多模态大型语言模型中，随着推理链条的增长，模型逐渐偏离图像引导内容并更多依赖语言先验的现象。通过注意力分析发现，较长的推理链条会降低对视觉输入的关注度，从而加剧幻觉问题。为了系统研究这一现象，我们提出了RH-AUC指标，用于量化模型感知准确性随推理长度的变化，同时发布了RH-Bench诊断基准，涵盖多种多模态任务，以评估推理能力与幻觉之间的权衡关系。研究结果表明，较大的模型通常能在推理和感知之间取得更好的平衡，而这种平衡更多取决于训练数据的类型和领域，而非数据总量。这些发现强调了需要综合考虑推理质量和感知保真度的评估框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 01:08:40 GMT</pubDate>
</item>
<item>
<title>引入RPEval：评估大型语言模型角色扮演能力的新基准</title>
<link>https://arxiv.org/abs/2505.13157</link>
<guid>https://arxiv.org/abs/2505.13157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RPEval基准以全面评估大型语言模型的角色扮演能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RPEval的新基准，用于评估大型语言模型（LLMs）的角色扮演能力。该基准涵盖了情感理解、决策制定、道德一致性及角色内在一致性四个关键维度。通过构建RPEval，我们旨在解决传统人工评估资源消耗大且自动化评估可能有偏见的问题。文章还提供了初步的基准测试结果，并公开了代码和数据集，供进一步研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:18:16 GMT</pubDate>
</item>
<item>
<title>Open CaptchaWorld：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出CaptchaWorld，用于评估多模态LLMs解决CAPTCHA的能力。</p><br /><br /><p><strong>摘要：</strong> CAPTCHAs长期以来阻碍了网络机器人在实际应用中的部署，而现代多模态大型语言模型（MLLMs）虽在静态感知任务中表现优异，但其处理交互性和多步推理挑战的能力尚未得到充分测试。本文介绍了一个名为Open CaptchaWorld的新基准平台，该平台通过多样化的动态CAPTCHA谜题来评估MLLM驱动代理的视觉推理和交互能力。CaptchaWorld涵盖了20种现代CAPTCHA类型，总计225个CAPTCHA，并引入了新的度量标准“CAPTCHA推理深度”，量化解决每个谜题所需的认知和动作步骤。实验表明，人类在该测试中接近满分，而最先进的MLLM代理成功率仅为40.0%，远低于人类水平的93.3%。这表明CaptchaWorld是一个重要的基准，可用于诊断当前多模态代理系统的局限性，并指导开发更强大的多模态推理系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Time Blindness: Why Video-Language Models Can't See What Humans Can?</title>
<link>https://arxiv.org/abs/2505.24867</link>
<guid>https://arxiv.org/abs/2505.24867</guid>
<content:encoded><![CDATA[
Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>强化学习如何扩展语言模型的推理边界</title>
<link>https://arxiv.org/abs/2505.24864</link>
<guid>https://arxiv.org/abs/2505.24864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，强化学习可揭示基模型无法触及的新推理策略。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，强化学习（RL）在引导语言模型实现可验证奖励方面具有潜力。然而，RL是否真正提升了模型的推理能力，还是仅仅放大了基模型分布中已存在的高奖励输出，仍存争议。此外，持续增加RL计算资源是否能可靠提升推理性能也未有定论。本研究通过引入持续强化学习（ProRL）方法，证明即使在广泛采样的情况下，ProRL训练也能挖掘出基模型无法触及的新推理策略。ProRL方法结合了KL散度控制、参考策略重置及多样化任务套件。实证分析表明，RL训练模型在多项pass@k评估中始终优于基模型，且在某些基模型完全失败的场景下表现优异。进一步研究表明，推理边界的改进与基模型的任务能力和训练时长密切相关，表明RL能够随着时间推移探索并填充新的解空间区域。这些发现为未来长期RL在推理领域的研究奠定了基础，并提供了新的见解。研究模型权重已公开，支持进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>ViStoryBench：故事可视化评估基准的引入</title>
<link>https://arxiv.org/abs/2505.24862</link>
<guid>https://arxiv.org/abs/2505.24862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ViStoryBench基准，用于评估故事可视化模型性能。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的进步，故事可视化领域取得了显著进展。为了进一步提升模型在实际场景中的表现，我们引入了ViStoryBench，这是一个综合性的评估基准。该基准集成了多样化的数据集，涵盖了多种故事类型和艺术风格，通过多维度测试模型能力，包括情节类型（如喜剧、恐怖）和视觉美学（如动漫、3D渲染）。ViStoryBench精心设计，平衡叙事结构与视觉元素，包含单主角和多主角的故事，以及复杂的情节和世界构建。此外，它采用广泛的评价指标进行全面比较。这一系统化框架有助于研究人员深入分析模型优劣，推动针对性改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>大型语言模型的忠实置信校准研究</title>
<link>https://arxiv.org/abs/2505.24858</link>
<guid>https://arxiv.org/abs/2505.24858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有大型语言模型在传达不确定性时表现不佳，提出新方法MetaFaith显著提升校准效果。</p><br /><br /><p><strong>摘要：</strong> 可靠不确定性沟通对大型语言模型的信任至关重要，但这些模型常以肯定语气传达错误信息，导致用户过度依赖并削弱信任。本研究首次系统评估了多种模型、数据集及提示策略下的忠实置信校准能力，发现当前方法成效有限，标准提示仅带来微小改进，而基于事实性的校准技术甚至可能损害准确性。为此，我们开发了MetaFaith，一种受人类元认知启发的新型提示校准方法，在多个模型和任务领域显著提升了校准的忠实性，使忠实度提高最多达61%，并获得人类评估83%的胜率。这项工作填补了大型语言模型在不确定性表达上的关键空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:54:08 GMT</pubDate>
</item>
<item>
<title>通过强化蒸馏优化大规模语言模型推理性能</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架REDI，有效利用正负推理样本提升LLM推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，模型蒸馏技术的进步表明，来自高级推理模型的数据可以有效地将复杂推理能力转移到较小的高效学生模型中。然而，标准做法采用拒绝采样方法，丢弃了错误的推理示例——这些数据虽然有价值但通常未被充分利用。本文探讨了如何在离线环境中有效利用正负蒸馏推理轨迹以最大化大型语言模型（LLM）的推理性能。为此，我们提出了强化蒸馏（REDI），这是一种两阶段框架。第一阶段通过监督微调（SFT）学习正向轨迹；第二阶段则通过我们提出的REDI目标函数进一步优化模型，该目标函数是一种简单的无参考损失函数，在这种蒸馏上下文中优于已建立的方法如DPO和SimPO。我们的实证评估显示，REDI在数学推理任务上优于基线拒绝采样SFT或SFT结合DPO/SimPO。特别是，Qwen-REDI-1.5B模型仅使用开放可用的Open-R1数据集中的131k个正负示例进行后训练，在MATH-500（pass@1）上获得了83.1%的分数。其在各种数学推理基准测试中的表现与DeepSeek-R1-Distill-Qwen-1.5B（使用800k专有数据后训练）相当或更好，确立了在仅使用公开可用数据进行离线后训练的1.5B规模模型的新技术水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24850" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:47:17 GMT</pubDate>
</item>
<item>
<title>Harnessing Large Language Models for Scientific Novelty Detection</title>
<link>https://arxiv.org/abs/2505.24615</link>
<guid>https://arxiv.org/abs/2505.24615</guid>
<content:encoded><![CDATA[
In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 10:08:13 GMT</pubDate>
</item>
<item>
<title>利用扩散模型先验进行跨帧一致性几何估计</title>
<link>https://arxiv.org/abs/2505.24521</link>
<guid>https://arxiv.org/abs/2505.24521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过扩散模型的内在一致性实现视频全局几何属性的跨帧一致预测。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，通过合理设计和微调扩散模型，有效利用视频生成模型的内在一致性，用于一致性的单目几何估计。具体而言，我们选择共享相同对应关系的全局坐标系中的几何属性作为预测目标，引入基于位置编码重用的高效条件方法，并通过联合训练多个共享相同对应关系的几何属性提升性能。实验结果显示，我们的方法在视频全局几何属性预测上表现优异，并可以直接应用于重建任务。即使仅在静态视频数据上训练，该方法也展现出对动态视频场景的潜在泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:31:59 GMT</pubDate>
</item>
<item>
<title>un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</title>
<link>https://arxiv.org/abs/2505.24517</link>
<guid>https://arxiv.org/abs/2505.24517</guid>
<content:encoded><![CDATA[
Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:29:38 GMT</pubDate>
</item>
<item>
<title>大型语言模型的近似线性分解及其语义结构解析</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型可被映射为等效线性系统。</p><br /><br /><p><strong>摘要：</strong> 本文展示了多种开源权重的大型语言模型（LLMs）的推理操作可以映射到与输入序列等效的线性系统，而无需修改模型权重或改变预测输出。通过借鉴图像扩散模型的技术，我们战略性地调整了针对下一个词预测的梯度计算，使得模型的雅可比矩阵几乎精确地再现了前向预测的线性系统。该方法适用于多种模型（如Llama 3、Gemma 3、Qwen 3等），并通过奇异值分解显示这些LLMs在极低维子空间中运行，其中许多最大的奇异向量解码出的概念与最可能的输出词相关。此外，这种方法还允许我们将每一层的操作视为近似的线性系统，并观察到语义概念的出现。尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过近乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察并揭示下一个词预测过程中的可解释语义结构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 03:08:33 GMT</pubDate>
</item>
<item>
<title>CLaSp：一种基于上下文层跳过的自推测解码策略</title>
<link>https://arxiv.org/abs/2505.24196</link>
<guid>https://arxiv.org/abs/2505.24196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaSp通过跳过验证模型的中间层实现高效解码，加速大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CLaSp的新方法，用于提升大语言模型（LLMs）的解码速度。与传统推测解码（SD）需要额外模块训练不同，CLaSp采用即插即用的方式，通过跳过验证模型的部分中间层构建压缩版草案模型，无需额外训练。该方法利用动态规划算法优化层跳过过程，根据每次验证阶段后的完整隐藏状态动态调整策略。实验表明，在LLaMA3系列模型上，CLaSp实现了1.3到1.7倍的加速，且不影响生成文本的原始分布。这项研究为加速LLMs的推理提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:15:06 GMT</pubDate>
</item>
<item>
<title>HardTests: Synthesizing High-Quality Test Cases for LLM Coding</title>
<link>https://arxiv.org/abs/2505.24098</link>
<guid>https://arxiv.org/abs/2505.24098</guid>
<content:encoded><![CDATA[
Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:00:34 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在计数任务中的偏见研究</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现最先进的视觉语言模型在计数和识别任务中表现出显著偏见。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过互联网学习大量先验知识，但这些知识可能导致它们产生错误或有偏见的答案。本研究专注于测试这些知识对视觉语言模型（VLMs）在标准视觉任务（如计数和识别）中的准确性的影响。实验结果显示，最先进的VLMs在处理涉及流行主题的任务时表现不佳，例如，在计数带有附加条纹的阿迪达斯标志条纹数量时，平均准确率仅为17.05%，涵盖动物、商标、国际象棋、棋盘游戏、视觉错觉和图案网格等多个领域。当向图像插入描述性文本时，准确率进一步下降。即使指导模型重新检查答案或依赖图像细节，计数准确性仅提高约2个百分点。这项工作揭示了VLMs的一种有趣失败模式，并提出了一种自动化框架用于检测模型偏差。相关代码和数据可在vlmsarebiased.github.io获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:47:58 GMT</pubDate>
</item>
<item>
<title>Point-MoE：实现大规模跨域3D点云理解的Mixture-of-Experts架构</title>
<link>https://arxiv.org/abs/2505.23926</link>
<guid>https://arxiv.org/abs/2505.23926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的Mixture-of-Experts架构Point-MoE，用于提升3D点云跨域理解能力。</p><br /><br /><p><strong>摘要：</strong> 尽管缩放定律已在自然语言处理和计算机视觉领域取得了显著成果，但3D点云理解尚未达到类似阶段。这一差距主要归因于3D数据集规模较小且来源多样化，导致扫描模式、采样密度及语义偏差各异。这种领域异质性严重阻碍了统一模型的大规模训练。本研究提出了Point-MoE，这是一种专门设计的Mixture-of-Experts架构，旨在实现3D感知中的大规模跨域泛化。实验表明，标准点云主干在混合域数据上表现明显下降，而Point-MoE通过简单的top-k路由策略能够自动专业化专家，即使没有领域标签。研究还证明，Point-MoE不仅优于强大的多域基线模型，而且对未见过的领域具有更好的泛化能力。这项工作强调了一种可扩展的3D理解路径：让模型自行发现多样化3D数据中的结构，而非通过手动整理或领域监督强加。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:21:47 GMT</pubDate>
</item>
<item>
<title>EmergentTTS-Eval：语音合成模型的综合评估基准</title>
<link>https://arxiv.org/abs/2505.23009</link>
<guid>https://arxiv.org/abs/2505.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新基准EmergentTTS-Eval，涵盖六个复杂场景以评估语音合成模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EmergentTTS-Eval的新基准，该基准扩展自EmergentTTS，专注于评估文本到语音（TTS）模型在处理微妙和语义复杂文本时的表现。它涵盖了六个具有挑战性的场景，包括情绪表达、副语言特征、外来词、句法复杂性、复杂发音以及问句处理。通过利用大型语言模型（LLMs）迭代生成测试案例，最终构建了包含1645个多样化测试案例的数据集。此外，采用模型作为裁判的方法，利用大型音频语言模型（LALM）从多个维度评估语音质量，如情感表达、韵律、语调和发音准确性。实验结果显示，这种方法不仅能揭示不同TTS系统间的细微性能差异，还与人类偏好高度相关。研究开源了评价代码和数据集，为未来的研究提供了宝贵的资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 22:36:24 GMT</pubDate>
</item>
<item>
<title>DexUMI：通过人类手部接口学习灵巧操作技能的框架</title>
<link>https://arxiv.org/abs/2505.21864</link>
<guid>https://arxiv.org/abs/2505.21864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DexUMI框架通过人类手部接口转移灵巧操作技能到机器人手上，实验成功率达86%。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DexUMI的数据收集和策略学习框架，该框架利用人类手作为自然接口，将灵巧操作技能转移到不同的机器人手上。DexUMI包含硬件和软件适应性调整，以最小化人体手与机器人手之间的差异。硬件上，通过可穿戴外骨骼桥接运动学差距，并提供直接触觉反馈；软件上，通过高保真机器人手图像修复技术解决视觉差异。实验证明，在两个不同硬件平台上的平均任务成功率达到了86%，展示了DexUMI的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>无需额外训练的音频-视觉大语言模型平衡模态理解方法</title>
<link>https://arxiv.org/abs/2505.20873</link>
<guid>https://arxiv.org/abs/2505.20873</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为Fork-Merge Decoding的方法，无需额外训练即可减少模态偏差。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过解决模态偏差问题来提升音频-视觉大语言模型（AV-LLMs）的平衡多模态理解能力，而无需进行额外训练。当前的AV-LLMs通常通过解码器联合处理音频和视频特征，虽然促进了统一的多模态理解，但可能引入模态偏差，即模型倾向于过度依赖某一模态。为了解决这一问题，我们提出了Fork-Merge Decoding（FMD），这是一种在推理阶段简单有效的策略，不需要额外的训练或架构修改。FMD首先通过早期解码层对仅音频和仅视频输入进行模态特定推理（分叉阶段），然后在剩余层合并隐藏状态以进行联合推理（合并阶段）。这种方法促进了模态贡献的平衡并利用了跨模态的互补信息。我们在两个代表性AV-LLMs（VideoLLaMA2和video-SALMONN）上使用三个基准数据集评估了该方法。实验结果显示，在专注于音频、视频和组合音频-视觉推理的任务中，性能得到了一致的改善，证明了推理时干预的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20873" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 04:22:56 GMT</pubDate>
</item>
<item>
<title>v1模型：多模态大语言模型的轻量级视觉重访扩展</title>
<link>https://arxiv.org/abs/2505.18842</link>
<guid>https://arxiv.org/abs/2505.18842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">v1模型通过点选-复制机制实现推理过程中的动态视觉访问。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为v1的轻量级扩展，用于多模态大型语言模型（MLLMs），使模型能够在推理过程中选择性地重新访问视觉信息。不同于传统MLLMs仅一次性处理视觉输入并完全依赖内部记忆，v1引入了一个简单的点选-复制机制，允许模型在整个推理过程中动态检索相关的图像区域。该机制通过最小修改增强了现有架构，基于模型不断发展的假设提供上下文访问视觉标记的能力。为了训练这种能力，我们构建了v1g数据集，包含30万个多模态推理跟踪样本及交错的视觉定位注释。实验表明，在三个多模态数学推理基准测试（MathVista、MathVision和MathVerse）上，v1相比同类基线模型表现更为出色，尤其是在需要精细视觉参考和多步推理的任务中。我们的研究结果表明，动态视觉访问是提升基于事实的多模态推理性能的一个有前景的方向。代码、模型和数据将被公开发布，以支持未来的相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 15:30:47 GMT</pubDate>
</item>
<item>
<title>LLMSynthor：利用大语言模型实现高保真数据合成</title>
<link>https://arxiv.org/abs/2505.14752</link>
<guid>https://arxiv.org/abs/2505.14752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMSynthor通过引入结构感知模拟器，提升基于大语言模型的数据合成效率与统计一致性。</p><br /><br /><p><strong>摘要：</strong> 数据建模中的一个重要挑战是生成能够忠实反映现实世界分布统计特性的合成数据。传统方法依赖强参数假设或手动结构设计，在高维或异构领域表现不佳。尽管大型语言模型(LLMs)展现出作为灵活高维先验的强大潜力，但其标准采样方法存在效率低、上下文限制固定且难以保证统计对齐的问题。为解决这些问题，我们提出了LLMSynthor框架，它将LLMs转化为由分布反馈引导的结构感知模拟器。该框架利用LLM作为非参数copula模拟器来建模高阶依赖关系，并通过LLM提议采样生成接地提议分布，从而提高采样效率而不需拒绝采样。通过迭代合成循环，LLMSynthor逐步揭示并优化潜在生成结构，使真实数据和合成数据保持统计一致性。我们在隐私敏感领域的异构数据集（如电子商务、人口和移动性）上进行了控制实验和实际应用评估，结果显示LLMSynthor生成的合成数据具有高统计保真度、实用性和跨数据适应性，可广泛应用于经济学、社会科学、城市研究等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:35:38 GMT</pubDate>
</item>
<item>
<title>AlphaOne：一种用于大模型推理过程动态调控的通用框架</title>
<link>https://arxiv.org/abs/2505.24863</link>
<guid>https://arxiv.org/abs/2505.24863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AlphaOne框架，通过参数化思考阶段提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AlphaOne的新框架，旨在测试时对大规模推理模型（LRMs）的推理进程进行灵活调控。AlphaOne首先引入了“阿尔法时刻”这一概念，通过一个通用参数α来表示扩展的思考阶段。在此过程中，它通过将推理转换标记的插入建模为伯努利随机过程，动态调度缓慢推理的过渡。在“阿尔法时刻”结束后，AlphaOne通过终止符确定性地结束缓慢推理，从而促进快速推理和高效答案生成。这种方法统一并推广了现有的单调缩放方法，实现了缓慢到快速推理的灵活且密集的调控。在数学、编码和科学等多个领域的具有挑战性的基准测试中，AlphaOne展示了其卓越的推理能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:36 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的多语言文本生成框架EasyText</title>
<link>https://arxiv.org/abs/2505.24417</link>
<guid>https://arxiv.org/abs/2505.24417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DiT的多语言文本渲染框架EasyText。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EasyText的新框架，该框架基于扩散Transformer（DiT），旨在实现精确的多语言文本生成。通过结合去噪潜变量与多语言字符标记编码，EasyText利用字符位置编码和位置编码插值技术，实现了可控且精确的文本渲染。此外，构建了一个包含百万级多语言图像文本标注的大规模合成文本图像数据集及高质量的2万张标注图像数据集，用于预训练和微调。实验结果表明，EasyText在多语言文本渲染、视觉质量和布局感知的文本集成方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 05:55:39 GMT</pubDate>
</item>
<item>
<title>一种自适应知识集成框架用于增强大型语言模型</title>
<link>https://arxiv.org/abs/2505.23844</link>
<guid>https://arxiv.org/abs/2505.23844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应知识集成框架，解决传统方法内存消耗大及性能下降问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型（LLMs）改进中的挑战，如传统微调方法的局限性和集成其他专用模型时的问题，提出了一个自适应知识集成框架。该框架通过设计一个自适应选择网络，根据源模型的分数选择最相关的模型，减少了知识干扰，并采用动态加权融合策略和反馈驱动损失函数，提升了模型的稳定性和可扩展性。实验表明，该方法相比现有方法将知识干扰降低了50%，同时保持了较高的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:24:50 GMT</pubDate>
</item>
<item>
<title>GSO基准测试：评估语言模型在高性能软件开发中的能力</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出GSO基准测试，评估语言模型在优化代码性能上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为GSO的新基准测试，用于评估语言模型在开发高性能软件方面的表现。通过自动化管道生成并执行性能测试，从10个代码库的历史提交记录中提取出102个优化任务，涵盖多个领域和编程语言。实验中，代理被要求提高代码运行效率，并与专家开发者的表现进行对比。定量分析显示，领先的语言模型表现不佳，成功率不足5%，且随着推理时间扩展改善有限。定性分析揭示了主要失败模式，如对低级语言处理困难、懒惰优化策略及瓶颈定位挑战。本研究还公开了基准测试代码及相关数据，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:14:55 GMT</pubDate>
</item>
<item>
<title>Yet Another Quantization Algorithm (YAQA) 改进大语言模型后量化性能</title>
<link>https://arxiv.org/abs/2505.22988</link>
<guid>https://arxiv.org/abs/2505.22988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法YAQA，显著提升大语言模型后量化压缩效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Yet Another Quantization Algorithm (YAQA) 的新型自适应舍入算法，用于优化大规模语言模型（LLMs）的后训练量化（PTQ）。传统方法通过独立最小化立即激活误差来量化线性层，但忽略了后续层的影响，导致优化目标局部化。YAQA利用每一层线性层相对于全模型KL散度的Hessian矩阵的Kronecker分解近似值，从而更好地捕获全局影响。该算法由两部分组成：可高效计算的大规模LLMs层间Hessian的Kronecker分解近似，以及与具体量化器无关的具有理论保证的舍入方法。实验表明，在多种模型和量化器上，YAQA可将KL散度降低约30%，同时在下游任务上达到最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:53:00 GMT</pubDate>
</item>
<item>
<title>Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14599</link>
<guid>https://arxiv.org/abs/2505.14599</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:49:40 GMT</pubDate>
</item>
<item>
<title>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.23764</link>
<guid>https://arxiv.org/abs/2505.23764</guid>
<content:encoded><![CDATA[
Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在解码字谜中的能力评估</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，视觉语言模型在解码简单视觉线索时表现良好，但在抽象推理方面存在不足。</p><br /><br /><p><strong>摘要：</strong> 字谜（Rebus puzzles）是一种通过图像、空间排列和符号替代来编码语言的视觉谜题，对当前的视觉语言模型（VLMs）提出了独特的挑战。不同于传统的图像描述或问答任务，字谜的解答需要多模态抽象、符号推理以及对文化、语音及语言双关的理解。本文构建了一个由多样化的英文字谜组成的基准测试集，涵盖从简单的图画替代到依赖空间提示的复杂谜题。通过对多种VLMs的性能分析，我们发现尽管这些模型在解析简单视觉线索时表现出一定的能力，但在涉及抽象推理、横向思维以及理解视觉隐喻的任务上仍存在显著困难。这项研究揭示了现有VLMs在跨模态任务上的局限性及其未来改进的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于关键帧的音乐同步动物舞蹈视频生成框架</title>
<link>https://arxiv.org/abs/2505.23738</link>
<guid>https://arxiv.org/abs/2505.23738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于关键帧的动物舞蹈视频生成方法，结合图优化和扩散模型实现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于关键帧的框架，用于生成与音乐同步且符合编舞意识的动物舞蹈视频。该框架通过将舞蹈合成建模为图优化问题，寻找满足指定编舞模式的最佳关键帧结构，这些模式可以从参考舞蹈视频中自动估计。此外，还引入了一种镜像姿态图像生成方法，以捕捉舞蹈中的对称性。通过视频扩散模型生成中间帧，在仅提供六个输入关键帧的情况下，可以生成长达30秒的跨多种动物和音乐轨道的舞蹈视频。这一方法展示了从文本到图像提示或GPT-4生成的关键帧开始，实现高质量动物舞蹈视频的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>ZPressor：通过信息瓶颈优化提升3D Gaussian Splatting模型的多视角扩展性</title>
<link>https://arxiv.org/abs/2505.23734</link>
<guid>https://arxiv.org/abs/2505.23734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ZPressor模块，压缩多视角输入至紧凑潜在状态Z，显著提高3DGS模型的扩展能力。</p><br /><br /><p><strong>摘要：</strong> 本文分析了基于前馈的3D Gaussian Splatting (3DGS) 模型在处理多视角输入时面临的扩展性限制问题，即随着输入视图数量增加，性能下降或内存消耗过高。我们基于信息瓶颈原理设计了ZPressor，这是一种轻量级且架构无关的模块，用于高效压缩多视角输入到一个保留关键场景信息的紧凑潜在状态Z。具体来说，ZPressor通过将视图划分为锚点集和支持集，并利用交叉注意力机制，将支持视图的信息压缩到锚点视图中，从而构建压缩后的潜在状态Z。实验表明，集成ZPressor后，多个最先进的前馈3DGS模型在适度输入视图下性能得到提升，在密集视图设置下的鲁棒性也有所增强。该方法在DL3DV-10K和RealEstate10K两个大规模基准数据集上表现优异，同时在80GB GPU上实现了超过100个480P分辨率输入视图的处理能力。相关视频结果、代码及训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>ViGoRL：通过视觉引导强化学习提升模型的视觉推理能力</title>
<link>https://arxiv.org/abs/2505.23678</link>
<guid>https://arxiv.org/abs/2505.23678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViGoRL通过视觉定位强化学习提升语言模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViGoRL（Visually Grounded Reinforcement Learning）的视觉语言模型，该模型通过强化学习将每一步推理明确锚定到特定的视觉坐标上，从而实现空间推理路径的生成。ViGoRL受到人类视觉决策的启发，能够指导视觉注意力聚焦于相关区域。在一系列视觉推理基准测试中，包括SAT-2、BLINK、V*bench等任务，ViGoRL的表现显著优于传统的监督微调和缺乏显式定位机制的常规强化学习基线。特别是在需要精细探索的任务中，多轮强化学习框架结合动态缩放功能进一步提升了模型性能。此外，研究表明，显式定位不仅提高了模型在局部元素定位和视觉搜索上的表现，还增强了其他视觉行为，如区域探索和子目标设定。最后，人类评估表明，ViGoRL的视觉参考不仅空间准确，而且有助于理解模型的推理过程。这些结果表明，视觉引导的强化学习是一种强大的方法，可以赋予模型通用的视觉推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:20:26 GMT</pubDate>
</item>
<item>
<title>基于轨迹输入的统一视频运动控制框架</title>
<link>https://arxiv.org/abs/2505.22944</link>
<guid>https://arxiv.org/abs/2505.22944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种运动类型的统一视频生成控制框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种用于视频生成的统一运动控制框架，该框架通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间中，实现了相机移动、对象级平移和精细局部运动的无缝集成。与以往针对不同运动类型采用独立模块的方法不同，我们的方法通过单一框架实现了对局部形变、物体整体运动、虚拟相机动态或其组合的精确控制。实验表明，该方法在多个视频运动控制任务上表现出色，包括风格化运动效果、动态视点变化及局部运动操作，同时在可控性和视觉质量上显著优于现有方法和商业解决方案，且兼容多种先进的视频生成模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:49:18 GMT</pubDate>
</item>
<item>
<title>AIDSAFE：通过多智能体迭代推敲提升LLMs安全推理能力</title>
<link>https://arxiv.org/abs/2505.21784</link>
<guid>https://arxiv.org/abs/2505.21784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AIDSAFE方法，利用多智能体协作生成高质量的安全推理链，显著提升大模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AIDSAFE的新方法，旨在解决现有安全措施如过度拒绝和越狱漏洞的问题。AIDSAFE通过多智能体迭代推敲的方式生成嵌入安全政策的链式思维（CoT）数据集，同时引入数据精炼阶段以消除重复、冗余及误导性思维。实验表明，基于AIDSAFE生成的CoT进行监督微调可大幅提升开源大模型的安全泛化能力和越狱鲁棒性，同时保持良好的实用性和拒绝精度。此外，为了满足对齐阶段的偏好数据需求，该方法还设计了一种补充方案，通过信念增强技术创建区分选择和拒绝样本的CoT数据。最终评估显示，AIDSAFE生成的CoT在政策遵守度和推理质量方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 17:34:40 GMT</pubDate>
</item>
<item>
<title>LUNGUAGE：基于多研究纵向评估的胸部X光报告生成基准数据集</title>
<link>https://arxiv.org/abs/2505.21190</link>
<guid>https://arxiv.org/abs/2505.21190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持单次与纵向评估的胸部X光报告生成基准数据集及评价方法。</p><br /><br /><p><strong>摘要：</strong> 现有放射学报告评估方法局限于单一报告环境且依赖粗略指标，无法捕捉精细临床语义和时间依赖性。本文引入LUNGUAGE，这是一个结构化胸部X光报告生成基准数据集，支持单次报告评估和跨多个研究的患者级别纵向评估。数据集包含1,473份由专家注释的胸片报告，其中80份具有纵向注释以捕获疾病进展。此外，开发了一个两阶段框架将生成的报告转换为细粒度的结构化表示，实现纵向解释。同时，提出了LUNGUAGESCORE，一种可解释的指标，在实体、关系和属性层面比较结构化输出的同时建模患者时间线的一致性。这些贡献建立了首个针对序列放射学报告的基准数据集、结构化框架和评估指标，实证结果显示LUNGUAGESCORE有效支持结构化报告评估。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:40:00 GMT</pubDate>
</item>
<item>
<title>大型语言模型与知识图谱结合用于复杂问答任务的研究综述</title>
<link>https://arxiv.org/abs/2505.20099</link>
<guid>https://arxiv.org/abs/2505.20099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型与知识图谱结合解决复杂问答任务的方法及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在问答（QA）任务中表现出色，但由于推理能力不足、知识过时及幻觉问题，在处理复杂QA任务时面临挑战。一些研究尝试将LLMs与知识图谱（KGs）相结合，以克服这些限制。本文提出了一种新的结构化分类法，根据QA类型和KG在与LLMs集成时的角色对方法进行分类。我们系统性地回顾了相关领域的最新进展，并从优势、局限性和KG需求等方面比较分析了这些方法。此外，我们还探讨了这些方法如何应对不同类型复杂QA的主要挑战，并总结了现有研究的成果、评估指标和基准数据集，同时指出了开放的问题与未来机会。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:08:23 GMT</pubDate>
</item>
<item>
<title>系统1.5推理：高效且适应性的大语言模型推理框架</title>
<link>https://arxiv.org/abs/2505.18962</link>
<guid>https://arxiv.org/abs/2505.18962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出系统1.5推理方法，显著提高大语言模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 当前基于链式思维（CoT）的大语言模型推理面临效率低下的问题，而潜在空间推理虽提高了效率，但未能区分关键推理步骤与辅助步骤，导致计算资源利用不充分。本文提出系统1.5推理框架，通过潜在空间中的动态捷径路径，在推理步骤间动态分配计算资源。该框架包括模型深度捷径（DS）和步骤捷径（SS），前者允许非关键标记提前退出轻量级适配器分支，后者则跨解码步骤重用隐藏状态以跳过简单步骤。通过两阶段自蒸馏过程训练，系统1.5推理在GSM8K等推理任务上展现了卓越性能，推理速度提升超过20倍，平均减少92.31%的标记生成，同时保持与传统CoT微调方法相当的推理效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 23:35:49 GMT</pubDate>
</item>
<item>
<title>视觉表征压缩对细粒度特征还原的影响及基准评测</title>
<link>https://arxiv.org/abs/2505.18142</link>
<guid>https://arxiv.org/abs/2505.18142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉分词器和VAE在保存细节特征上的局限性，并提出评估文本与人脸重建性能的新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉分词器和变分自编码器（VAE）在图像压缩过程中丢失细节信息的问题，特别是在处理小尺度的文本和人脸图像时的局限性。尽管这些技术通过提供高效的图像压缩和量化表示推动了视觉生成与多模态建模的发展，但它们在减少计算负担的同时也限制了视觉生成质量的上限。为了评估这一上限，我们聚焦于文本和面部特征的重建质量，因为这些特征通常具有密集纹理、易塌陷且对人类视觉高度敏感的特点。我们收集并整理了来自现有数据集的高质量文本和人脸图像，并采用成熟的OCR和人脸识别模型进行评估，这种方法不仅准确而且轻量级，仅需2GB内存和4分钟即可完成。通过我们的基准测试，分析了不同图像分词器和VAE在各种尺度下的重建质量。结果显示，现代视觉分词器在保存细粒度特征方面仍有不足，尤其是在小尺度下表现欠佳。此外，我们将此评估框架扩展到视频领域，对多种视频分词器进行了综合分析，并证明传统指标无法准确反映人脸和文本的重建效果，而我们提出的指标则是一个有效的补充。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>REOrder：通过优化补丁顺序提升视觉Transformer性能</title>
<link>https://arxiv.org/abs/2505.23751</link>
<guid>https://arxiv.org/abs/2505.23751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示补丁排列对现代Transformer模型表现有显著影响，提出REOrder框架优化补丁顺序。</p><br /><br /><p><strong>摘要：</strong> 当前基于Transformer的视觉模型通常将图像展平为一维序列，常用固定行优先（栅格扫描）顺序。尽管完全自注意力机制具有置换等变性，但现代长序列Transformer倾向于采用破坏这种不变性的架构近似方法，导致对补丁排列敏感。本文表明，在这些设置下补丁顺序显著影响模型性能，例如列优先或希尔伯特曲线等简单替代方案可带来明显的准确性变化。受此启发，我们提出了REOrder，这是一种两阶段框架，用于发现任务最优的补丁排列。首先，通过评估各种补丁序列的压缩性得出信息论先验；然后，利用REINFORCE优化Plackett-Luce策略学习排列策略。该方法能够在组合排列空间中实现高效学习。实验证明，REOrder在ImageNet-1K上相比行优先排列提升了高达3.01%的top-1准确率，并在Functional Map of the World数据集上提高了13.35%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>基于强化学习的大语言模型代码效率优化框架</title>
<link>https://arxiv.org/abs/2505.23387</link>
<guid>https://arxiv.org/abs/2505.23387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过强化学习显著提升大语言模型代码执行效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）虽然能够生成功能正确的代码，但在代码效率方面存在不足，成为实际应用中的瓶颈。本文介绍了一种测试时迭代优化框架，利用闭环系统让LLMs根据执行沙箱反馈迭代改进代码。研究探索了三种训练策略：监督微调（SFT）、直接偏好优化（DPO）和分组相对策略优化（GRPO）。实验显示，SFT和DPO在效率提升上很快达到饱和，而采用强化学习的GRPO持续优化代码性能，在Venus数据集和APPS基准测试中分别将pass@1提升至62%并使效率优于人类提交的概率从31%提高到45%。本研究证明了测试时代码效率优化的有效性，并揭示了强化学习在指导LLMs自我提升代码效率方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:14:29 GMT</pubDate>
</item>
<item>
<title>基于语言模型解释性和不确定性量化提升机器翻译质量评估效率</title>
<link>https://arxiv.org/abs/2505.23183</link>
<guid>https://arxiv.org/abs/2505.23183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用语言模型解释性与不确定性量化提升机器翻译质量评估效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过语言模型的解释性和不确定性量化技术，以更高效的方式识别机器翻译中的错误片段，从而替代传统昂贵的质量评估方法。这些传统方法通常依赖大型语言模型的提示或大量人工标注数据的训练。研究对12种翻译方向下的14项指标进行了评估，发现人类标签变化对评估性能有显著影响。实验结果表明，无监督评估方法具有未被充分挖掘的潜力，而当面临标签不确定性时，监督方法存在不足，单一标注者评价实践也显得脆弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 03:20:36 GMT</pubDate>
</item>
<item>
<title>Re-ttention：通过利用时间冗余实现视觉生成模型的极高稀疏注意力</title>
<link>https://arxiv.org/abs/2505.22918</link>
<guid>https://arxiv.org/abs/2505.22918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Re-ttention方法，在不影响视觉质量的前提下实现极高的稀疏注意力。</p><br /><br /><p><strong>摘要：</strong> 扩散Transformer（DiT）已成为生成高质量视频和图像的主流模型，但其计算瓶颈在于注意力机制，其复杂度随分辨率和视频长度呈平方级增长。现有稀疏注意力技术在极高稀疏水平下无法保持视觉质量且可能带来显著计算开销。为解决此问题，本文提出Re-ttention，利用扩散模型的时间冗余来克服注意力机制中的概率归一化偏移，通过重塑注意力得分来维持全量二次注意力的视觉质量。实验表明，Re-ttention在推理过程中仅需3.1%的tokens，优于FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，该方法在H100 GPU上实现了超过45%的端到端延迟和超过92%的自注意力延迟减少，且开销可忽略不计。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:39:12 GMT</pubDate>
</item>
<item>
<title>When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy</title>
<link>https://arxiv.org/abs/2505.22888</link>
<guid>https://arxiv.org/abs/2505.22888</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, their ability to think in other languages is less studied. This capability is as important as answer accuracy for real world applications because users may find the reasoning trace useful for oversight only when it is expressed in their own language. We comprehensively evaluate two leading families of LRMs on our XReasoning benchmark and find that even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing a substantial gap in multilingual reasoning. Prompt based interventions that force models to reason in the users language improve readability and oversight but reduce answer accuracy, exposing an important trade off. We further show that targeted post training on just 100 examples mitigates this mismatch, though some accuracy loss remains. Our results highlight the limited multilingual reasoning capabilities of current LRMs and outline directions for future work. Code and data are available at https://github.com/Betswish/mCoT-XReasoning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 17:44:12 GMT</pubDate>
</item>
<item>
<title>CLIPGaussians：一种多模态风格迁移框架</title>
<link>https://arxiv.org/abs/2505.22854</link>
<guid>https://arxiv.org/abs/2505.22854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个支持文本和图像引导的多模态风格迁移框架。</p><br /><br /><p><strong>摘要：</strong> Gaussian Splatting (GS) 是一种高效的 3D 场景渲染方法，但其风格迁移仍具挑战性。本文介绍 CLIPGaussians，这是一种针对 2D 图像、视频、3D 对象和 4D 场景的统一风格迁移框架。该方法直接作用于高斯基元，无需大型生成模型或重新训练即可集成到现有 GS 流程中。CLIPGaussians 能实现 3D 和 4D 环境下的颜色和几何联合优化，并在视频中保持时间一致性，同时保持模型大小。实验表明，它在所有任务中表现出卓越的风格保真度和一致性，验证了其作为通用高效多模态风格迁移解决方案的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 16:41:24 GMT</pubDate>
</item>
<item>
<title>VidText：视频文本理解的新基准</title>
<link>https://arxiv.org/abs/2505.22810</link>
<guid>https://arxiv.org/abs/2505.22810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VidText新基准，评估视频文本理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视频理解基准大多忽视文本信息，而OCR特定基准仅限于静态图像，无法充分捕捉文本与动态视觉背景间的交互。为填补这一空白，我们提出了VidText，这是一个针对视频文本理解进行全面深入评估的新基准。VidText涵盖广泛的真实场景，支持多语言内容，提供多层次评估框架，并引入配对感知推理任务。实验显示当前模型在大多数任务上表现不佳，存在显著改进空间。分析表明模型内在因素（如输入分辨率、OCR能力）和外部因素（如辅助信息使用、推理策略）的影响。我们希望VidText能弥补现有基准的不足，并为未来动态环境中多模态推理研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 15:39:35 GMT</pubDate>
</item>
<item>
<title>FAMA：首个开源科学语音基础模型</title>
<link>https://arxiv.org/abs/2505.22759</link>
<guid>https://arxiv.org/abs/2505.22759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布首个开源语音基础模型FAMA，性能媲美现有模型且速度提升8倍。</p><br /><br /><p><strong>摘要：</strong> 随着像Whisper和SeamlessM4T这样的语音基础模型的发展，语音处理领域取得了显著进步，但其封闭性质限制了可复现性和公平评估。尽管其他研究领域已通过开放科学取得进展，但语音领域的类似努力仍显不足。为填补这一空白，我们推出了FAMA，这是首个面向英语和意大利语的开源科学语音基础模型家族，训练数据超过15万小时的开源语音数据。此外，我们还发布了包含1.6万小时清理后伪标签语音的新数据集。实验结果显示，FAMA在性能上可与现有模型相媲美，同时运行速度提高了8倍。所有代码、数据集和模型均以符合开源许可的方式发布，推动了语音技术研究的开放性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:19:34 GMT</pubDate>
</item>
<item>
<title>KronSAE：通过Kronecker分解提升稀疏自编码器效率</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出KronSAE架构，利用Kronecker分解减少稀疏自编码器的计算开销。</p><br /><br /><p><strong>摘要：</strong> 稀疏自编码器(SAEs)在解释语言模型隐藏状态方面表现出巨大潜力，但其训练在大规模场景下具有挑战性，尤其是当字典规模较大时。尽管解码器可以采用稀疏感知核以提高效率，但编码器仍需进行高维线性运算，带来较高的计算成本。为解决这一问题，我们提出了KronSAE，这是一种新颖的架构，通过Kronecker乘积分解来大幅降低内存和计算负担。此外，我们引入了mAND，一种近似二元AND操作的不同iable激活函数，它不仅提升了因子化框架的可解释性，还提高了性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 07:41:11 GMT</pubDate>
</item>
<item>
<title>Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting</title>
<link>https://arxiv.org/abs/2505.19716</link>
<guid>https://arxiv.org/abs/2505.19716</guid>
<content:encoded><![CDATA[
Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose a difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, a large teacher model first judges each problem's difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate a distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled a new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that a student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms a model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches 74.2% Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:04:44 GMT</pubDate>
</item>
<item>
<title>VBenchComp：用于评估视频大模型时间推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.14321</link>
<guid>https://arxiv.org/abs/2505.14321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VBenchComp，解决现有视频理解基准混淆知识和图像问题的问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频理解基准通常混淆知识型和纯图像型问题，未能明确区分模型的时间推理能力，这是视频理解区别于其他模态的关键方面。我们发现两个主要问题：强语言先验和时间不变性，导致高分未必真正反映对动态内容的理解。为解决这些问题，我们提出了VBenchComp，通过自动化管道将问题分类为LLM可回答、语义型和时间型，其余归为其他类。这种方法能够更精细地评估视频大模型的不同能力。我们的分析揭示了传统总体分数掩盖的模型弱点，并提供了对未来基准设计的见解和建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:07:55 GMT</pubDate>
</item>
<item>
<title>ZeroGUI：无需人工成本的图形用户界面自动化训练框架</title>
<link>https://arxiv.org/abs/2505.23762</link>
<guid>https://arxiv.org/abs/2505.23762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种零人工成本的在线学习框架ZeroGUI，提升图形用户界面代理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有纯视觉图形用户界面（GUI）代理训练方法依赖手工标注和适应性差的问题，提出了ZeroGUI，这是一种可扩展的在线学习框架，通过基于大型视觉语言模型的任务自动生成功能、奖励评估功能及两阶段强化学习机制，在零人工干预下显著提升了两个先进GUI代理(UI-TARS和Aguvis)在OSWorld和AndroidLab环境中的表现。实验表明，ZeroGUI克服了传统方法的局限性，提高了模型的泛化能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于差分信息分布的直接偏好优化理论分析</title>
<link>https://arxiv.org/abs/2505.23761</link>
<guid>https://arxiv.org/abs/2505.23761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">揭示直接偏好优化的理论基础及其与差分信息分布的关系。</p><br /><br /><p><strong>摘要：</strong> Direct Preference Optimization (DPO) 是一种标准技术，用于以监督方式对齐语言模型与人类偏好。尽管其实证成功，但其对数比率奖励参数化的理论依据仍不完整。本文通过利用差分信息分布 (DID) 解决这一问题，该分布捕捉策略更新过程中的信息增益。首先证明当偏好标签编码将参考策略转换为目标策略所需的差分信息时，DPO 的对数比率奖励成为学习目标策略的最佳形式。其次发现偏好编码差分信息的条件与隐含假设密切相关。最后，通过分析 DID 的熵，我们描述了学习低熵差分信息如何增强策略分布，而高熵差分信息诱导平滑效果。我们在合成实验和真实世界指令跟随数据集中验证了这些理论发现，表明学习高熵差分信息对一般指令跟随至关重要，而学习低熵差分信息有利于知识密集型问答。本文为 DPO 目标、偏好数据结构及由此产生的策略行为提供了统一视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>LoRAShop：基于LoRA的多概念图像编辑框架</title>
<link>https://arxiv.org/abs/2505.23758</link>
<guid>https://arxiv.org/abs/2505.23758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAShop利用LoRA模型实现多概念图像编辑，提升身份保真度。</p><br /><br /><p><strong>摘要：</strong> LoRAShop是一种创新的多概念图像编辑框架，专为LoRA模型设计。它基于Flux风格扩散变换器内的特征交互模式观察，通过前期前向传递获取每个概念的解耦潜空间掩码，并仅在限定区域内融合相应的LoRA权重，从而实现多个主体或风格的无缝整合，同时保留全局上下文和细节。实验表明，该方法在身份保真度上优于现有基线。LoRAShop无需重新训练且不受外部约束，将个性化扩散模型转变为实用的‘LoRA Photoshop’工具，推动了视觉叙事和创意迭代的新发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>DeepTheorem：利用自然语言增强大语言模型数学推理能力的综合框架</title>
<link>https://arxiv.org/abs/2505.23754</link>
<guid>https://arxiv.org/abs/2505.23754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepTheorem框架，通过强化学习提升大语言模型在非形式定理证明中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DeepTheorem的新框架，旨在利用自然语言提高大型语言模型（LLMs）在非形式定理证明中的数学推理能力。该框架包含一个大规模的基准数据集，由121,000个高质量的国际数学奥林匹克水平的非形式定理及其证明组成，覆盖多个数学领域，并经过严格标注，同时伴随有系统构建的可验证定理变体。我们还设计了一种专门针对非形式定理证明的新型强化学习策略（RL-Zero），利用这些可验证的定理变体激励稳健的数学推理。此外，我们提出了全面的结果和过程评估指标，考察证明的正确性和推理步骤的质量。广泛的实验分析表明，DeepTheorem在现有数据集和监督微调协议上显著提高了LLMs的定理证明性能，达到了最先进的准确度和推理质量。我们的研究结果强调了DeepTheorem在根本上推进自动非形式定理证明和数学探索方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>基于2D观测的空间多模态大语言模型</title>
<link>https://arxiv.org/abs/2505.23747</link>
<guid>https://arxiv.org/abs/2505.23747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需3D数据即可提升空间智能的多模态大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Spatial-MLLM的新框架，用于仅基于2D观测的视觉空间推理。与依赖CLIP视觉编码器的传统视频多模态大语言模型不同，Spatial-MLLM利用前馈视觉几何基础模型的强大结构先验知识。该模型采用双编码器架构：一个预训练的2D视觉编码器用于提取语义特征，一个初始化自视觉几何模型主干的空问编码器用于提取3D结构特征。通过连接器将两者整合为统一的视觉标记以增强空间理解能力。此外，在推理阶段引入了空间感知帧采样策略，确保模型聚焦于对空间推理至关重要的关键帧。除架构改进外，我们构建了Spatial-MLLM-120k数据集，并通过监督微调和GRPO方法进行训练。实验证明，该模型在多种实际数据集上实现了视觉空间理解和推理任务的最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>To Trust Or Not To Trust Your Vision-Language Model's Prediction</title>
<link>https://arxiv.org/abs/2505.23745</link>
<guid>https://arxiv.org/abs/2505.23745</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>MAGREF：基于掩码引导的任意参考多主体视频生成框架</title>
<link>https://arxiv.org/abs/2505.23742</link>
<guid>https://arxiv.org/abs/2505.23742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架MAGREF，实现高质量多主体视频合成。</p><br /><br /><p><strong>摘要：</strong> 近年来，深度生成模型尤其是扩散模型推动了视频生成技术的进步。然而，基于多个参考主体的视频生成仍面临多主体一致性保持和高质量生成的挑战。本文提出MAGREF框架，通过引入掩码引导机制，在多样参考图像和文本提示条件下实现连贯的多主体视频合成。MAGREF框架包含两个关键创新：区域感知动态掩码机制，使单一模型能够灵活处理多种主体推理；像素级通道连接机制，更好地保留外观特征。实验表明，该方法在复杂多主体场景中表现优于现有开源和商业基线。此外，我们还构建了一个全面的多主体视频基准用于评估。结果表明，MAGREF实现了可扩展、可控且高保真的多主体视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:15 GMT</pubDate>
</item>
<item>
<title>ATLAS：一种高效的长时记忆模块增强Transformer模型</title>
<link>https://arxiv.org/abs/2505.23735</link>
<guid>https://arxiv.org/abs/2505.23735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ATLAS模块，显著提升长上下文理解和序列建模性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统Transformer模型在长序列建模中的瓶颈问题，提出了ATLAS，这是一种具有高容量长时记忆模块，通过优化当前及历史标记来改进记忆管理。基于此设计，我们引入了一组新的深度Transformer架构DeepTransformers，它严格扩展了原始Transformer架构。实验表明，在语言建模、常识推理、召回密集型任务及长上下文理解任务中，ATLAS不仅超越了标准Transformer和近期线性递归模型的表现，还在10M上下文长度的BABILong基准测试中提升了80%的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title>
<link>https://arxiv.org/abs/2505.23716</link>
<guid>https://arxiv.org/abs/2505.23716</guid>
<content:encoded><![CDATA[
We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:49:56 GMT</pubDate>
</item>
<item>
<title>提出新基准VF-Eval评估多模态大语言模型在AI生成内容视频中的能力</title>
<link>https://arxiv.org/abs/2505.23693</link>
<guid>https://arxiv.org/abs/2505.23693</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新基准VF-Eval评估多模态大语言模型在AI生成视频中的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视频问答方面得到了广泛研究，但现有评估主要集中在自然视频上，忽视了合成视频如AI生成内容（AIGC）。一些视频生成工作依赖MLLMs评估生成质量，然而MLLMs对解释AIGC视频的能力尚未得到充分探索。为此，我们提出了一个新的基准VF-Eval，引入了连贯性验证、错误意识、错误类型检测和推理评估四项任务，以全面评估MLLMs在AIGC视频上的能力。我们在VF-Eval上评估了13个前沿MLLMs，发现即使表现最好的模型GPT-4.1，在所有任务上也难以保持一致的良好性能，这凸显了我们基准的挑战性。此外，为了调查VF-Eval在提升视频生成方面的实际应用，我们进行了重新提示实验（RePrompt），表明使MLLMs更紧密地符合人类反馈可以有助于视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23693" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:31:13 GMT</pubDate>
</item>
<item>
<title>Diffusion via Autoregressive模型：一种新的图像扩散建模范式</title>
<link>https://arxiv.org/abs/2505.23660</link>
<guid>https://arxiv.org/abs/2505.23660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种将图像扩散过程重新定义为标准自回归预测的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Diffusion via Autoregressive models (D-AR)的新范式，通过将图像扩散过程视为标准的下一个标记预测过程，实现了对图像的自回归建模。首先设计了一种将图像转换为离散标记序列的分词器，这些标记可以解码为像素空间中的去噪扩散步骤。得益于扩散特性，这些标记自然遵循粗到细的顺序，非常适合自回归建模。通过对这些标记进行标准的下一个标记预测，无需修改任何底层设计，就可以实现图像空间中扩散过程的镜像。实验表明，在ImageNet基准上，使用775M Llama骨干网络和256个离散标记的方法达到了2.09的FID分数。该方法支持一致的预览生成部分标记，并且能够在零样本布局控制合成方面表现出色。我们希望这项工作能够激发未来关于视觉合成统一自回归架构的研究，特别是结合大型语言模型的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23660" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:09:25 GMT</pubDate>
</item>
<item>
<title>大型推理模型中的幻觉现象研究</title>
<link>https://arxiv.org/abs/2505.23646</link>
<guid>https://arxiv.org/abs/2505.23646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型推理模型在事实寻求任务中的幻觉现象表现存在争议。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）因其强大的长链推理能力在复杂任务中表现出色。然而，这些模型是否能通过推理能力减少事实寻求任务中的幻觉现象仍存争议。例如，DeepSeek-R1在SimpleQA任务中报告了性能提升，而OpenAI-o3却发现幻觉现象更加严重。本文从三个方面探讨了这一问题：首先，我们对LRMs的幻觉现象进行了全面评估，发现冷启动监督微调和可验证奖励强化学习可以减轻幻觉，而仅使用蒸馏或无冷启动微调的强化学习会引入更多微妙的幻觉；其次，我们分析了不同的后训练管道如何影响LRMs的幻觉现象，发现表面推理重复错误和思考与答案不匹配是影响事实准确性的重要认知行为；最后，我们从模型不确定性角度研究了LRMs的幻觉机制，发现模型不确定性与事实准确性之间的不匹配通常会导致更高的幻觉率。本研究为理解LRMs的幻觉现象提供了初步认识。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23646" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:53:41 GMT</pubDate>
</item>
<item>
<title>基于文本引导扩散模型的零样本音频源分离方法</title>
<link>https://arxiv.org/abs/2505.23625</link>
<guid>https://arxiv.org/abs/2505.23625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过配置良好的预训练扩散模型实现无需微调的音频源分离。</p><br /><br /><p><strong>摘要：</strong> 当前监督深度学习方法在音频源分离任务中受限于大量标注数据的需求且泛化能力有限，而本文受生成基础模型成功的启发，研究了是否可以利用预训练的文本引导音频扩散模型克服这些限制。令人惊讶的是，在适当配置下，纯文本引导的扩散模型能够实现零样本的音频源分离。所提出的方法名为ZeroSep，它通过将混合音频反向投影到扩散模型的潜在空间，并利用文本条件指导去噪过程以恢复单个源信号。ZeroSep无需特定任务的训练或微调，直接重新利用生成扩散模型进行判别性分离任务，并通过丰富的文本先验支持开放集场景。该方法与多种预训练的文本引导音频扩散模型兼容，在多个分离基准测试中表现出色，甚至超过了监督方法的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:31:45 GMT</pubDate>
</item>
<item>
<title>推理时扩展的表格推理研究：基于蒸馏与可验证奖励强化学习的方法</title>
<link>https://arxiv.org/abs/2505.23621</link>
<guid>https://arxiv.org/abs/2505.23621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出两种后训练策略以实现推理时扩展，表R1-Zero模型性能媲美GPT-4.1。</p><br /><br /><p><strong>摘要：</strong> 本文首次探索了表格推理任务中的推理时扩展问题，开发并评估了两种后训练策略：前沿模型推理轨迹蒸馏和可验证奖励强化学习（RLVR）。通过DeepSeek-R1生成的大规模推理轨迹数据集，我们对大型语言模型进行了微调，得到Table-R1-SFT模型；而在RLVR方法中，我们提出了特定任务的可验证奖励函数，并应用GRPO算法获得了Table-R1-Zero模型。这些模型在短形式问答、事实验证和自由形式问答等多样化表格推理任务上表现出色，其中Table-R1-Zero模型在仅使用7B参数的情况下达到了与GPT-4.1和DeepSeek-R1相当甚至更高的性能，并且在跨领域数据集上展现了强大的泛化能力。进一步的消融分析和定性分析揭示了指令微调、模型架构选择以及跨任务泛化的益处，同时显示了在强化学习训练过程中表格推理技能的涌现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:28:50 GMT</pubDate>
</item>
<item>
<title>Muddit：基于离散扩散的统一文本图像生成模型</title>
<link>https://arxiv.org/abs/2505.23606</link>
<guid>https://arxiv.org/abs/2505.23606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Muddit模型，实现文本和图像模态的快速并行生成。</p><br /><br /><p><strong>摘要：</strong> Unified generation models致力于通过单一架构处理跨模态任务，但现有模型存在推理速度慢或泛化能力弱的问题。本文介绍Muddit，一种基于离散扩散的Transformer模型，通过整合预训练的文本到图像骨干网络与轻量级文本解码器，在文本和图像生成方面实现了高效且高质量的多模态生成。实验表明，Muddit在质量和效率上可媲美甚至超越更大规模的自回归模型，展示了纯粹离散扩散模型结合强视觉先验的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title>EvoScale：通过进化提升小规模语言模型在软件工程任务中的性能</title>
<link>https://arxiv.org/abs/2505.23604</link>
<guid>https://arxiv.org/abs/2505.23604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为EvoScale的方法，提升小规模语言模型在软件工程任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EvoScale的新方法，旨在通过将生成过程视为进化过程来提高小规模语言模型在实际软件工程任务中的表现。传统方法如监督微调需要高质量但昂贵的数据集，而测试时扩展策略虽然有效但成本高。EvoScale通过迭代选择和变异优化生成输出，显著减少所需样本数量。此外，该方法利用强化学习训练模型自我进化，从而在推理阶段无需依赖外部验证器。实验结果显示，采用EvoScale后，32B参数的Satori-SWE-32B模型在SWE-Bench-Verified基准上达到了超过100B参数模型的性能水平，同时仅需少量样本。代码、数据和模型均计划开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:36 GMT</pubDate>
</item>
<item>
<title>基于最优奖励基准的对策略强化学习算法</title>
<link>https://arxiv.org/abs/2505.23585</link>
<guid>https://arxiv.org/abs/2505.23585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习算法OPO，解决大语言模型训练不稳定和计算效率低的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为On-Policy RL with Optimal reward baseline (OPO)的新算法，旨在解决当前强化学习算法在大语言模型训练中的稳定性不足和计算效率低的问题。OPO通过强调精确的对策略训练和引入最优奖励基准来减少梯度方差，从而提高训练稳定性和探索能力。实验表明，OPO在数学推理基准测试中表现出色，且无需额外的辅助模型或正则化项。此外，OPO还减少了策略漂移并提高了输出熵，使得生成的响应更加多样且重复性更低。这些结果表明，OPO为大语言模型的对齐和推理任务提供了一个有前景的方向。相关实现已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:58:04 GMT</pubDate>
</item>
<item>
<title>SafeScientist：强化AI科学家框架的安全性与伦理责任</title>
<link>https://arxiv.org/abs/2505.23559</link>
<guid>https://arxiv.org/abs/2505.23559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SafeScientist框架提升AI驱动科研探索中的安全性和伦理责任。</p><br /><br /><p><strong>摘要：</strong> 本文介绍SafeScientist，一种专门设计用于增强AI科学家框架安全性的创新方法。SafeScientist通过主动拒绝不道德或高风险任务，在整个研究过程中强调安全措施，包括引入多个防御机制如提示监控、协作监控、工具使用监控及伦理审查组件。此外，我们还提出了SciSafetyBench，这是一个用于评估科学领域AI安全性的新基准，涵盖六个领域的240项高风险科学任务及相关工具。实验表明，SafeScientist在保持科研产出质量的同时，将安全性提高了35%，并经受住了多种对抗性攻击测试。该框架的代码和数据将在指定GitHub页面上提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:35:58 GMT</pubDate>
</item>
<item>
<title>SWE-bench-Live：面向动态软件修复的大规模可更新基准</title>
<link>https://arxiv.org/abs/2505.23419</link>
<guid>https://arxiv.org/abs/2505.23419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-bench-Live，解决现有基准静态、过时等问题。</p><br /><br /><p><strong>摘要：</strong> SWE-bench-Live是一个全新的可更新基准，由1319个来自GitHub真实问题的任务组成，涵盖93个存储库。它通过自动化管道简化实例创建和环境设置，解决了传统基准如SWE-bench的局限性。在多个最先进的模型上测试表明，SWE-bench-Live中的性能显著优于静态基准，特别是在实时环境中。通过对存储库来源、问题时效性和任务难度的深入分析，揭示了性能差异的原因。SWE-bench-Live为评估大语言模型和智能体在动态软件开发场景中的能力提供了可靠工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:09:44 GMT</pubDate>
</item>
<item>
<title>KVzip：一种高效的Transformer语言模型KV缓存压缩方法</title>
<link>https://arxiv.org/abs/2505.23416</link>
<guid>https://arxiv.org/abs/2505.23416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KVzip通过压缩KV缓存提高Transformer模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为KVzip的查询无关的KV缓存淘汰方法，它利用底层的语言模型量化KV对的重要性并淘汰不重要的KV对，从而实现高效复用压缩后的KV缓存。实验表明，KVzip可将KV缓存大小减少3到4倍，将FlashAttention解码延迟降低约2倍，在问答、检索、推理和代码理解等任务中性能损失可以忽略不计。KVzip适用于多种模型如LLaMA3.1-8B、Qwen2.5-14B和Gemma3-12B，且在上下文长度高达17万tokens时表现优异。相比之下，现有的查询感知KV淘汰方法即使在90%缓存预算下多查询场景中也会导致性能下降。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:05:47 GMT</pubDate>
</item>
<item>
<title>UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23380</link>
<guid>https://arxiv.org/abs/2505.23380</guid>
<content:encoded><![CDATA[
Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:00:15 GMT</pubDate>
</item>
<item>
<title>VideoReasonBench：评估视觉为中心的复杂视频推理能力</title>
<link>https://arxiv.org/abs/2505.23359</link>
<guid>https://arxiv.org/abs/2505.23359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入VideoReasonBench评估多模态大模型的复杂视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，长链-of-thought（CoT）推理可显著提升大型语言模型（LLMs）在复杂任务中的表现，但在视频理解领域尚未得到验证，因为大多数现有基准缺乏足够的推理深度。本文提出VideoReasonBench，这是一个旨在评估视觉为中心的复杂视频推理的基准。该基准通过设计富含视觉细节且具有高推理复杂度的视频问题，涵盖回忆观察到的视觉信息、推断潜在状态内容及预测视频外信息三个层次。通过对18种最先进的多模态LLMs进行测试，发现大多数模型在此类任务上的表现不佳，而增强思维的Gemini-2.5-Pro表现最佳，达到56.0%的准确率。此外，研究显示扩展思考预算对现有视频基准影响有限，但在VideoReasonBench上至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 07:33:43 GMT</pubDate>
</item>
<item>
<title>UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes</title>
<link>https://arxiv.org/abs/2505.23253</link>
<guid>https://arxiv.org/abs/2505.23253</guid>
<content:encoded><![CDATA[
We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 04:58:41 GMT</pubDate>
</item>
<item>
<title>引入Theory of Mind增强的说服模型ToMAP</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ToMAP，通过增强理论思维提升语言模型的说服力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在说服方面展现出潜力，但现有训练方法仍处于初步阶段。人类擅长动态建模对方的想法，而当前LLMs在理论思维（ToM）推理上存在不足，导致说服多样性有限。为解决此问题，我们提出了ToMAP（Theory of Mind Augmented Persuader），通过两个ToM模块增强对对手心理状态的认知与分析能力。实验显示，尽管ToMAP仅含3B参数，但在多个说服对象模型和语料库上的表现优于更大规模的基线模型GPT-4o，相对提升了39.4%。ToMAP展示了复杂的推理链并减少了重复，使说服更具多样性和有效性。此外，其对手感知特性使其适用于长时间对话，并能采用更逻辑化且有针对性的策略。这些结果验证了ToMAP方法的有效性，并为开发更具说服力的语言代理提供了参考。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:03:41 GMT</pubDate>
</item>
<item>
<title>Multimodal Adversarial Compositionality (MAC)基准测试提升多模态模型鲁棒性</title>
<link>https://arxiv.org/abs/2505.22943</link>
<guid>https://arxiv.org/abs/2505.22943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MAC基准测试，评估多模态模型的组合性漏洞并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Multimodal Adversarial Compositionality (MAC)，这是一个利用大型语言模型生成欺骗性文本样本的新基准，旨在检测和评估跨多种模态的预训练多模态表示（如CLIP）中的组合性脆弱性。通过样本级攻击成功率和组级基于熵的多样性进行评估，MAC揭示了现有模型在处理复杂组合问题时的不足。为改善零样本方法，我们提出了自我训练方法，结合拒绝采样微调和多样性促进过滤技术，显著提升了攻击成功率和样本多样性。实验表明，在较小的语言模型（如Llama-3.1-8B）上，该方法在发现图像、视频和音频等多模态表示中的组合性脆弱性方面表现出色，为提高多模态模型的鲁棒性和安全性提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:45:55 GMT</pubDate>
</item>
<item>
<title>多模态CAD重建模型结合视觉语言与强化学习</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种输入模态的多模态CAD重建模型。</p><br /><br /><p><strong>摘要：</strong> 计算机辅助设计（CAD）在工程和制造领域至关重要，但现有方法通常只专注于单一输入模态（如点云、图像或文本），限制了其通用性和鲁棒性。本文利用视觉语言模型（VLM）的最新进展，提出了一个多模态CAD重建模型，同时处理三种输入模态。该模型采用两阶段管道：首先在大规模程序生成的数据上进行监督微调（SFT），然后通过在线反馈进行强化学习（RL）微调。我们首次探索了针对CAD任务的LLM在线RL微调，发现如组相对偏好优化（GRPO）等在线算法优于离线替代方案。在DeepCAD基准测试中，我们的SFT模型在所有三种输入模态上均优于现有的单模态方法。经过RL微调后，cadrille在三个具有挑战性的数据集上，包括一个真实世界的数据集，创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:32:31 GMT</pubDate>
</item>
<item>
<title>基于合成数据提升语音语言模型对句子重音的理解能力</title>
<link>https://arxiv.org/abs/2505.22765</link>
<guid>https://arxiv.org/abs/2505.22765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入StressTest基准测试，评估现有语音语言模型对句子重音解释的能力并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 句子重音是指在口语表达中对特定词汇施加强调，以突出或对比某种想法，或者引入新信息。最近，语音感知语言模型(SLMs)的发展使得直接处理音频成为可能，从而绕过转录过程并利用语音信号的丰富性进行音频推理任务，如口语问答。然而，尽管重音在塑造意义和说话者意图方面起着关键作用，在这类模型的评估与开发中却往往被忽视。本研究通过引入StressTest基准测试填补这一空白，评估了几种领先SLMs的表现，发现它们在这类任务上的表现不佳。为解决这一问题，我们提出了一个新的合成数据生成管道，创建了Stress17k训练集，该数据集模拟了由重音变化所暗示的意义改变。实证研究表明，优化这些模型可以很好地适应真实录音，并有效微调SLMs。结果显示，我们的微调模型StresSLM在句子重音推理和检测任务上显著优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:32:56 GMT</pubDate>
</item>
<item>
<title>大型语言模型后训练中奖励噪声的影响研究</title>
<link>https://arxiv.org/abs/2505.22653</link>
<guid>https://arxiv.org/abs/2505.22653</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大型语言模型对显著奖励噪声具有强鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在后训练大型语言模型时，奖励噪声对模型推理能力的影响。研究发现，即使在数学任务中人为翻转40%的奖励输出，基于Qwen-2.5-7B模型仍能在任务性能上快速收敛至72%的准确率，接近无噪声奖励模型的75%表现。令人惊讶的是，仅通过奖励关键推理短语（如“首先，我需要”）的出现而非答案准确性，模型在下游任务上的表现峰值超过70%，与严格验证正确性的模型相当。结合推理模式奖励（RPR）与噪声奖励模型，可校准奖励模型并减少潜在的误判，提升开放性任务的表现。本研究强调了预训练阶段基础能力的重要性，并为后训练技术的发展提供了新见解。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22653" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>基于双向扩散模型的高效非自回归文本生成</title>
<link>https://arxiv.org/abs/2505.22618</link>
<guid>https://arxiv.org/abs/2505.22618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的块级近似KV缓存机制，大幅提升扩散语言模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有开源扩散大型语言模型（Diffusion LLMs）在实际推理速度上的不足，尤其是与自回归模型相比的延迟问题，提出了两项创新性改进。首先，我们设计了一种专为双向扩散模型定制的块级近似Key-Value（KV）缓存机制，显著提升了并行解码时的效率，同时保持了极小的性能下降。其次，我们揭示了并行解码导致生成质量下降的根本原因——条件独立假设破坏了令牌依赖关系，并通过引入置信度感知的并行解码策略解决了这一问题，该策略有选择性地对超过置信阈值的令牌进行解码，有效缓解了依赖关系的破坏，从而保证了生成质量。实验结果显示，在LLaDA和Dream等模型上，我们的方法实现了高达27.6倍的吞吐量提升，同时仅造成微小的准确性损失，成功缩小了与自回归模型的性能差距，为扩散语言模型的实际部署铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:39:15 GMT</pubDate>
</item>
<item>
<title>GeoDrive：提升自动驾驶世界模型的空间感知与安全性</title>
<link>https://arxiv.org/abs/2505.22421</link>
<guid>https://arxiv.org/abs/2505.22421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入3D几何条件，GeoDrive显著提高自动驾驶场景建模的准确性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 近期动态环境模拟领域的发展推动了世界模型的进步，这些模型在自动驾驶中的应用可以预测其他道路使用者的行为并进行风险评估。然而，现有方法存在3D几何一致性不足及遮挡处理中产生伪影的问题，影响了安全评估的可靠性。为解决这些问题，我们提出了GeoDrive，它将鲁棒的3D几何条件整合到驾驶世界模型中，以增强空间理解和行动可控性。具体来说，GeoDrive首先从输入帧中提取3D表示，并基于指定的主车轨迹生成2D渲染。在训练过程中，我们还设计了一个动态编辑模块，通过调整车辆位置来优化渲染效果。实验表明，GeoDrive在动作精度和3D空间感知方面均优于现有模型，从而实现了更真实、灵活且可靠的场景建模，显著提升了自动驾驶的安全性。此外，该模型具备泛化能力，并支持交互式场景编辑功能，如对象编辑和轨迹控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 10:46:51 GMT</pubDate>
</item>
<item>
<title>SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</title>
<link>https://arxiv.org/abs/2505.22126</link>
<guid>https://arxiv.org/abs/2505.22126</guid>
<content:encoded><![CDATA[
Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 04:51:01 GMT</pubDate>
</item>
<item>
<title>Differentiable Solver Search for Fast Diffusion Sampling</title>
<link>https://arxiv.org/abs/2505.21114</link>
<guid>https://arxiv.org/abs/2505.21114</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:33:43 GMT</pubDate>
</item>
<item>
<title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title>
<link>https://arxiv.org/abs/2505.20755</link>
<guid>https://arxiv.org/abs/2505.20755</guid>
<content:encoded><![CDATA[
In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a theory-driven framework which we name the \emph{Uni-Instruct}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the f-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded f-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded f-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \emph{1.46} for unconditional generation and \emph{1.38} for conditional generation. On the ImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \emph{1.02}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 01:55:45 GMT</pubDate>
</item>
<item>
<title>One-shot Entropy Minimization</title>
<link>https://arxiv.org/abs/2505.20282</link>
<guid>https://arxiv.org/abs/2505.20282</guid>
<content:encoded><![CDATA[
We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:30 GMT</pubDate>
</item>
<item>
<title>Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking</title>
<link>https://arxiv.org/abs/2505.20199</link>
<guid>https://arxiv.org/abs/2505.20199</guid>
<content:encoded><![CDATA[
Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 12:40:22 GMT</pubDate>
</item>
<item>
<title>Multi-Domain Explainability of Preferences</title>
<link>https://arxiv.org/abs/2505.20088</link>
<guid>https://arxiv.org/abs/2505.20088</guid>
<content:encoded><![CDATA[
Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:01:56 GMT</pubDate>
</item>
<item>
<title>ChartLens: Fine-grained Visual Attribution in Charts</title>
<link>https://arxiv.org/abs/2505.19360</link>
<guid>https://arxiv.org/abs/2505.19360</guid>
<content:encoded><![CDATA[
The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 19:17:32 GMT</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 15:34:15 GMT</pubDate>
</item>
<item>
<title>Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator</title>
<link>https://arxiv.org/abs/2505.19236</link>
<guid>https://arxiv.org/abs/2505.19236</guid>
<content:encoded><![CDATA[
Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:25:23 GMT</pubDate>
</item>
<item>
<title>CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays</title>
<link>https://arxiv.org/abs/2505.18087</link>
<guid>https://arxiv.org/abs/2505.18087</guid>
<content:encoded><![CDATA[
Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:44:21 GMT</pubDate>
</item>
<item>
<title>PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions</title>
<link>https://arxiv.org/abs/2505.17818</link>
<guid>https://arxiv.org/abs/2505.17818</guid>
<content:encoded><![CDATA[
Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:34:48 GMT</pubDate>
</item>
<item>
<title>IQBench：评估视觉语言模型在人类智商测试中的推理能力</title>
<link>https://arxiv.org/abs/2505.12000</link>
<guid>https://arxiv.org/abs/2505.12000</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入IQBench基准，评估视觉语言模型在标准化视觉智商测试中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型视觉语言模型(VLMs)在多模态任务中表现优异，但其在人类智商测试中的真正推理能力尚未充分探索。本文介绍了IQBench，这是一个新设计的基准，专注于评估VLMs在标准化视觉智商测试中的推理能力。与以往仅关注最终预测准确性不同，该基准通过评估模型的解释和解题模式，结合最终预测的准确性及人工评价来衡量推理能力。实验结果显示，尽管某些模型如`o4-mini`、`gemini-2.5-flash`和`claude-3.7-sonnet`在平均准确率上表现较好，但在三维空间和变位词推理任务中仍显不足，表明当前VLMs的通用推理能力存在显著局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12000" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 09:24:08 GMT</pubDate>
</item>
<item>
<title>零样本嫁接技术降低视觉语言模型训练成本</title>
<link>https://arxiv.org/abs/2505.22664</link>
<guid>https://arxiv.org/abs/2505.22664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过小模型迁移策略降低视觉语言模型训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“零样本嫁接”的方法，用于减少视觉语言模型（VLMs）的训练开销。传统方法通常将大型语言模型（LLMs）作为解码器，但这种方法计算负担重且成本高昂。为了解决这个问题，研究者们设计了小型“替代模型”，这些模型继承了目标LLM的浅层网络并共享相同的嵌入空间和表示语言。经过替代模型训练的视觉编码器可以直接迁移到更大的模型上。实验结果显示，这种嫁接方法不仅在某些基准测试中表现优于直接使用小模型，甚至可以达到与完整解码器训练相当的效果。此外，当以Llama-70B作为解码器时，该方法可使总体训练成本降低约45%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>FastTD3：加速人形机器人强化学习训练的高效算法</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FastTD3通过简单修改显著提升人形机器人在多环境中的训练速度。</p><br /><br /><p><strong>摘要：</strong> 强化学习在机器人领域取得了显著进展，但其复杂性和长时间训练仍是主要瓶颈。本文介绍了一种名为FastTD3的新算法，它通过并行模拟、大批次更新、分布式批评者及精心调优的超参数，在HumanoidBench、IsaacLab和MuJoCo Playground等环境中大幅缩短了人形机器人训练时间，某些任务仅需不到3小时即可完成，且训练过程稳定。此外，我们还提供了FastTD3的轻量化实现，以促进机器人领域的强化学习研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:55:26 GMT</pubDate>
</item>
<item>
<title>PISCES：一种精确擦除语言模型概念知识的新框架</title>
<link>https://arxiv.org/abs/2505.22586</link>
<guid>https://arxiv.org/abs/2505.22586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PISCES框架，通过直接编辑参数空间中的方向，有效擦除语言模型中的特定概念。</p><br /><br /><p><strong>摘要：</strong> 现有方法在擦除大型语言模型中的不当知识时存在粗略、浅显或无效的问题。本文提出PISCES框架，利用解缠模型将MLP向量分解为可解释特征，并通过自动化可解释技术定位并移除目标概念相关的特征。实验表明，PISCES在Gemma 2和Llama 3.1上对多种概念的擦除效果优于领先方法，将目标概念的准确率降至7.7%，同时显著提高了擦除特异性和鲁棒性，分别提升了31%和38%。整体来看，基于特征的参数内编辑为语言模型中概念知识的更精确和可靠擦除提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:58:23 GMT</pubDate>
</item>
<item>
<title>HLIP：一种针对3D医学影像的语言-图像预训练框架</title>
<link>https://arxiv.org/abs/2505.21862</link>
<guid>https://arxiv.org/abs/2505.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HLIP通过分层注意力机制提升了3D医学影像语言-图像预训练的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HLIP（Hierarchical attention for Language-Image Pre-training）的可扩展预训练框架，用于3D医学影像。HLIP采用轻量级分层注意力机制，模仿放射学数据的自然层次结构（切片、扫描和研究）。该方法在Rad-ChestCT基准测试上展示了强泛化能力，在CT-RATE数据集上预训练后，宏观AUC提高了4.3%。此外，HLIP的计算效率允许直接在未经处理的数据集上进行训练。它在脑部MRI和头部CT数据集上的表现达到当前最佳水平，例如在Pub-Brain-5基准上平衡准确率提升32.4%，在RSNA和CQ500头部CT基准上分别提高1.4%和6.9%的宏观AUC。这些结果表明，HLIP使得直接在未经处理的临床数据集上进行预训练成为3D医学影像领域的一种可行且有效的方法。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:16:34 GMT</pubDate>
</item>
<item>
<title>DORI基准测试：多模态系统物体方向感知能力评估</title>
<link>https://arxiv.org/abs/2505.21649</link>
<guid>https://arxiv.org/abs/2505.21649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DORI基准测试揭示现有视觉语言模型在物体方向理解上的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为DORI（辨别性方向推理智能）的新基准测试，专门用于评估物体方向感知能力。DORI通过四个维度（正面对齐、旋转变换、相对方向关系和典型方向理解）来衡量模型的表现。经过对11个数据集的精细设计任务分析，发现最先进的视觉语言模型在粗粒度任务上的准确率仅为54.2%，而在细粒度方向判断上仅为33.0%，且在涉及参考框架转换或复合旋转的任务中表现明显下降。这些结果表明现有模型在内部三维空间表示方面存在严重缺陷，提示需要改进方向表示机制。DORI作为首个针对多模态系统方向意识的诊断框架，具有提升机器人控制、3D场景重建及人机物理交互的重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 14:22:44 GMT</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Mon, 26 May 2025 19:50:36 GMT</pubDate>
</item>
<item>
<title>HoPE：提升视觉语言模型长上下文能力的混合位置嵌入方法</title>
<link>https://arxiv.org/abs/2505.20444</link>
<guid>https://arxiv.org/abs/2505.20444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出HoPE，一种针对长视频理解的混合位置嵌入方法。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型(VLMs)在多模态任务中取得了显著进展，但在长上下文场景（如长视频）中的表现通常会下降。尽管旋转位置嵌入(RoPE)已被广泛用于大语言模型(LLMs)的长度泛化，但将其扩展到捕捉视频复杂的时空依赖关系仍是一个未解决的挑战。现有方法通常通过不同的频率分配策略来编码三维位置信息，但这些策略主要基于启发式方法，缺乏深入的理论分析。本研究首先探讨了不同分配策略对VLMs长上下文能力的影响，发现当前的多模态RoPE无法可靠地捕获扩展上下文中的语义相似性。为此，我们提出了HoPE，这是一种混合位置嵌入方法，旨在提高VLMs的长上下文能力。HoPE引入了一种混合频率分配策略，以在任意长的上下文中进行可靠的语义建模，并采用动态时间缩放机制，促进在不同上下文长度下的稳健学习和灵活推理。广泛的实验表明，在四个视频基准上的长视频理解和检索任务中，HoPE始终优于现有方法，证明了其有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:37:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在真实文本因果推理中的挑战</title>
<link>https://arxiv.org/abs/2505.18931</link>
<guid>https://arxiv.org/abs/2505.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示顶级大型语言模型在真实文本因果推理上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 理解并推断文本中的因果关系是人类认知的核心方面，也是推动大型语言模型（LLMs）向通用人工智能发展的关键。现有研究多集中于合成文本中明确提到的简单因果关系，未能反映现实任务的复杂性。本文探讨了LLMs是否能在真实世界文本中推断因果关系，并开发了一个来自实际学术文献的数据集，涵盖多样化的文本长度、因果关系复杂性及领域。这是首个针对此任务的真实世界数据集。实验表明，最先进的LLMs在该基准测试中面临重大挑战，最佳模型的平均F1分数仅为0.477。分析揭示了常见问题，如隐含信息的理解困难、区分相关因果因素与上下文细节的能力不足，以及连接分散在长文本中的因果相关信息的难题。通过系统地识别这些缺陷，本研究为LLMs因果推理能力的进一步发展提供了有针对性的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 21:50:05 GMT</pubDate>
</item>
</channel>
</rss>