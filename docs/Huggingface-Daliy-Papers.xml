<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>动态词汇头（DyVo）提升稀疏检索模型的实体识别效果</title>
<link>https://arxiv.org/abs/2410.07722</link>
<guid>https://arxiv.org/abs/2410.07722</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究通过动态词汇头（DyVo）与维基百科概念结合，提升了稀疏检索模型对实体的识别与检索能力。</p><br><br><p><strong>摘要：</strong> 学习型稀疏检索（LSR）模型使用预训练变换器的词汇，但往往将实体拆分成不合理的片段，从而降低检索准确性，并限制模型吸纳最新的世界知识。为此，本研究通过结合维基百科的概念和实体，增强了LSR的词汇，从而更有效地解决歧义并保持与不断发展的知识保持同步。我们的方法核心在于动态词汇（DyVo）头，该头利用现有实体嵌入和实体检索组件，识别与查询或文档相关的实体。DyVo头用于生成实体权重，随后将这些权重与词片权重合并，以创建高效编码和检索的联合表征，并使用倒排索引进行检索。实验结果显示，在三个富含实体的文档排序数据集上，DyVo模型的表现显著优于最先进的基线模型。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.07722 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 10:24:44 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 10:24:44 GMT</pubDate>
</item>
<item>
<title>特征在不同文本领域之间的稳定性与转变研究</title>
<link>https://arxiv.org/abs/2410.12391</link>
<guid>https://arxiv.org/abs/2410.12391</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究特征如何在不同文本领域的模型中出现、消失和持续。</p><br><br><p><strong>摘要：</strong> 本文研究了特征在不同领域文本的模型中的表现及其稳定性，聚焦于对一个基础的一层Transformer语言模型进行适应和测试。该基础模型是基于BabyLM语料库和来自The Stack的Python代码集训练的。我们将该基础模型适应于两个新的文本领域：TinyStories和Lua编程语言。随后，通过球面线性插值方法将这两个模型合并。我们的探索旨在揭示在小规模模型和稀疏自编码器的典型迁移学习场景中，特征的稳定性和转变过程。这为理解不同领域间特征的演变提供了更深入的见解，进而有助于提高迁移学习和模型适应能力。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.12391 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 10:10:35 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 10:10:35 GMT</pubDate>
</item>
<item>
<title>通过逆向强化学习解读大型语言模型的隐性奖励函数</title>
<link>https://arxiv.org/abs/2410.12491</link>
<guid>https://arxiv.org/abs/2410.12491</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究运用逆向强化学习解析毒性对齐的LLM隐性奖励，探讨模型大小与可解释性等关键问题。</p><br><br><p><strong>摘要：</strong> 本文提出了一种新的方法，通过逆向强化学习(Inverse Reinforcement Learning, IRL)恢复大型语言模型(LLMs)的隐性奖励函数，以理解它们的决策过程。研究聚焦于毒性对齐的LLM，采用多种规模的模型进行实验，提取奖励模型的预测准确率达到80.40%。分析中揭示了奖励函数的非可识别性、模型规模与可解释性之间的关系以及RLHF过程中的潜在问题。结果表明，通过IRL提取的奖励模型能够用于新LLM的微调，在毒性基准测试上实现了相当或更优的性能。这项工作为理解和改进大型语言模型的对齐提供了新的视角，并且对这些强大系统的负责任开发与部署具有重要意义。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.12491 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 08:37:56 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 08:37:56 GMT</pubDate>
</item>
<item>
<title>神经形态变换（NeuMeta）：自适应神经网络的连续权重学习</title>
<link>https://arxiv.org/abs/2410.11878</link>
<guid>https://arxiv.org/abs/2410.11878</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>NeuMeta提出了一种新学习范式，实现自适应神经网络，并在不同配置下无缝生成权重。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种新的学习范式，即神经形态变换（NeuMeta），旨在构建可自我变换的神经网络。与为不同架构或规模制造单独模型不同，NeuMeta直接学习神经网络的连续权重流形。训练后，可以直接从流形中为任何大小的网络采样权重，甚至在先前未见过的配置中，无需重新训练。为了实现这一雄心勃勃的目标，NeuMeta将神经隐式函数作为超网络进行训练，接受模型空间内的坐标作为输入，并生成相应的不变权重值。训练过程中，发现最终性能与学习流形的平滑性密切相关。为提高平滑性，我们采用了两种策略：首先，通过解决最短哈密顿路径问题对权重矩阵进行置换以实现模型内部平滑性。其次，在训练隐式函数时对输入坐标添加噪声，以确保不同大小的模型均表现一致。因此，NeuMeta在合成多种网络配置的参数方面显示出良好效果。我们的广泛测试表明，NeuMeta在图像分类、语义分割和图像生成任务中，即使在75%的压缩率下，也能保持全尺寸性能。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.11878 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 08:23:01 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 08:23:01 GMT</pubDate>
</item>
<item>
<title>基于连续时间的生成模型的稳定训练与快速采样</title>
<link>https://arxiv.org/abs/2410.11081</link>
<guid>https://arxiv.org/abs/2410.11081</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新方法，优化基于扩散的生成模型，在连续时间上实现稳定快速采样。</p><br><br><p><strong>摘要：</strong> 一致性模型（Consistency Models, CMs）是一类强大的基于扩散的生成模型，旨在实现快速采样。虽然现有的CMs大多采用离散时间步训练，这带来了额外的超参数并易受到离散化误差影响。但连续时间形式可以缓解这些问题，然而其成功受到训练不稳定性的限制。为了解决这一问题，我们提出了一个简化的理论框架，用于统一以前的扩散模型和CMs的参数化，找出了不稳定性的根本原因。在此分析的基础上，我们引入了扩散过程参数化、网络架构和训练目标的关键改进。这些改变使我们能够在前所未有的规模上训练连续时间CMs，达到1.5亿参数在ImageNet 512x512上。我们的训练算法仅使用两个采样步骤，即在CIFAR-10上实现了2.06的FID分数，在ImageNet 64x64上为1.48，在ImageNet 512x512上为1.88，FID分数与最佳扩散模型之间的差距缩小到10%以内。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.11081 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 07:42:04 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 07:42:04 GMT</pubDate>
</item>
<item>
<title>WorldMedQA-V：多语言多模态医疗基准测试数据集</title>
<link>https://arxiv.org/abs/2410.12722</link>
<guid>https://arxiv.org/abs/2410.12722</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>WorldMedQA-V是一个更新的多语言多模态基准数据集，旨在评估医疗领域的视觉语言模型。</p><br><br><p><strong>摘要：</strong> 随着多模态/视觉语言模型（VLMs）在全球医疗环境中的广泛应用，确保其安全性、有效性和公平性变得日益重要。本研究提出了WorldMedQA-V，一个更新的多语言多模态基准测试数据集，旨在评估VLMs在医疗领域的表现。该数据集包括568个标记的多项选择题和568个医学图像，覆盖来自巴西、以色列、日本和西班牙的四个国家，提供了原语言及经过本地临床医生验证的英文翻译。我们还提供了常见开源和闭源模型的基准性能测试结果，涵盖本地语言和英文翻译，并针对模型在有无图像的情况下的表现进行了对比。WorldMedQA-V基准旨在更好地匹配AI系统与其所处的多样化医疗环境，促进更公平、高效和具代表性的应用。该数据集为未来在多语言和多模态的医疗场景中的模型发展和应用提供了重要的基准依据。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.12722 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 07:19:55 GMT</pubDate>
<pubDate>Thu, 17 Oct 2024 07:19:55 GMT</pubDate>
</item>

<item>
<title>Large Language Model Evaluation via Matrix Nuclear-Norm</title>
<link>https://arxiv.org/abs/2410.10672</link>
<guid>https://arxiv.org/abs/2410.10672</guid>
<content:encoded><![CDATA[
As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \( L_{1,2}-norm \) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 05:44:04 GMT</pubDate>
</item>
<item>
<title>VidEgoThink：评估自我视角视频理解能力的综合基准</title>
<link>https://arxiv.org/abs/2410.11623</link>
<guid>https://arxiv.org/abs/2410.11623</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidEgoThink基准评估自我视角视频理解能力，揭示多模态大语言模型在此领域的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）的进步为具身人工智能应用开辟了新方向。在之前工作EgoThink的基础上，我们提出了VidEgoThink，这是一个用于评估自我视角视频理解能力的综合基准。为了弥合MLLMs与具身AI低级控制之间的差距，我们设计了四个相互关联的关键任务：视频问答、层次规划、视觉定位和奖励建模。为降低手动标注成本，我们基于Ego4D数据集开发了一种自动数据生成管道，利用GPT-4o的先验知识和多模态能力，随后三位人工标注者过滤生成的数据，以确保多样性和质量，最终形成了VidEgoThink基准。我们对三种类型的模型进行了广泛实验：基于API的MLLMs、开源图像基MLLMs和开源视频基MLLMs。实验结果表明，包括GPT-4o在内的所有MLLMs在自我视角视频理解相关任务上表现较差。这些发现表明，基础模型仍需重大进展才能有效应用于具身人工智能中的第一人称场景。总体而言，VidEgoThink反映了将MLLMs应用于自我视角视觉的研究趋势，旨在实现与人类能力相似的主动观察与互动，探究复杂现实环境中的行为。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11623" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 05:23:48 GMT</pubDate>
</item>
<item>
<title>ZipVL：针对大规模视觉语言模型的高效推理框架</title>
<link>https://arxiv.org/abs/2410.08584</link>
<guid>https://arxiv.org/abs/2410.08584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZipVL通过动态重要令牌比例分配策略，提高大规模视觉语言模型的推理效率，减小计算和内存瓶颈。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ZipVL，一个高效的推理框架，旨在解决大规模视觉语言模型（LVLMs）在预填充阶段的计算瓶颈和解码阶段的内存瓶颈。ZipVL采用动态重要令牌比例分配策略，该比例依据层特定的注意力分数分布自适应确定，以优化较低复杂性任务的效率，同时保持高复杂性任务的表现。我们通过标准化注意力分数选择重要令牌，并仅对这些重要令牌执行注意力机制，从而加速预填充阶段。此外，在解码阶段，我们对键值（KV）缓存采用混合精度量化，对重要令牌的缓存使用高位量化，对不太重要的缓存应用低位量化。实验结果显示，ZipVL能够将预填充阶段的速度提升2.6倍，同时将GPU内存使用减少50.0%，在LongVA-7B模型上，在Video-MME基准上仅有0.2%的准确率下降，从而有效增强了LVLMs的生成效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 03:40:12 GMT</pubDate>
</item>
<item>
<title>多模态模型中的幻觉问题研究：挑战与前景</title>
<link>https://arxiv.org/abs/2410.12787</link>
<guid>https://arxiv.org/abs/2410.12787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究系统分析了大规模多模态模型中的幻觉现象，提出评估基准并探讨解决方案。</p><br /><br /><p><strong>摘要：</strong> 近期大规模多模态模型（LMMs）的进展显著提升了在多种任务中的表现，且仍在持续研究如何整合视频和音频等附加模态。然而，现有LMMs仍然易受幻觉影响，即实际的多模态输入与生成的文本输出之间的差异，这限制了它们在各种实际场景中的应用。本研究首次系统性地调查了涉及语言、视觉和音频这三种最常见模态的LMMs中的幻觉现象。我们的研究揭示了两个关键原因：对单一模态先验的过度依赖及模态间虚假的关联。为了解决这些挑战，我们推出了基准The Curse of Multi-Modalities (CMM)，全面评估LMMs中的幻觉，并深入分析其根本问题。研究结果突显出关键脆弱点，包括模态整合的不平衡和来自训练数据的偏见，强调了平衡的跨模态学习和增强幻觉减轻策略的必要性。基于我们的观察和发现，我们提出了可能增强LMMs可靠性的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 02:21:58 GMT</pubDate>
</item>
<item>
<title>HumanEval-V：评估大型多模态模型的视觉理解与编程能力的基准</title>
<link>https://arxiv.org/abs/2410.12381</link>
<guid>https://arxiv.org/abs/2410.12381</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HumanEval-V是一个新基准，评估大型多模态模型的编码与视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在编码任务上的评估成为人工智能进步的重要工具，视觉感知能力的引入使得大型多模态模型（LMMs）变得愈发重要。然而，目前对于LMMs在视觉推理和代码生成方面的评估基准相对缺乏，为此我们提出了HumanEval-V。该基准包含108个精心设计的初级Python编码任务，来源于CodeForces和Stack Overflow，并经过适当调整以保证其独创性。每个任务都包括视觉元素和预定义的函数签名，模型需要基于这些内容生成代码解决方案，并通过手工编写的测试用例进行评估。在对19个先进的LMM进行评测后，我们发现目前LMM在视觉推理和编码能力上存在显著挑战，与商用模型如GPT-4o相比，其通过率仅为13%（pass@1）和36.4%（pass@10），而一些开源模型的通过率更低于4%。这些结果为未来的研究指明了增强LMM能力的关键方向。我们的代码和基准已开源，链接为：https://github.com/HumanEval-V/HumanEval-V-Benchmark。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12381" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:49:58 GMT</pubDate>
</item>
<item>
<title>跨模态时间理解的新模型与数据集研究</title>
<link>https://arxiv.org/abs/2410.12109</link>
<guid>https://arxiv.org/abs/2410.12109</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出OCTAV数据集及OMCAT模型，提升音视频跨模态时间理解的能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在文本生成与理解方面取得了显著进展，最近的研究扩展到了整合视觉和音频输入的多模态LLMs。然而，这些模型在细粒度的跨模态时间理解，特别是在音频与视频流之间相关事件的关联上仍然面临挑战。为了解决这些问题，我们提出了两个关键贡献：一个新的数据集OCTAV（Omni Context and Temporal Audio Video），旨在捕捉音视频之间的事件过渡；以及OMCAT（Omni Context Aware Transformer）模型，该模型利用RoTE（Rotary Time Embeddings），对RoPE进行创新扩展，提升了时间锚定任务的时效性和计算效率。通过强大的三阶段训练管道——特征对齐、指令调优和OCTAV特定训练——OMCAT在跨模态时间理解上表现卓越。我们的模型在音频视觉问答（AVQA）任务和OCTAV基准上展现了先进的性能，并通过全面的实验和消融研究验证了其在时间推理和跨模态对齐方面的显著提升。我们的数据集和代码将公开发布，演示页面链接为：https://om-cat.github.io。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12109" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:18:30 GMT</pubDate>
</item>
<item>
<title>ProSA：评估大型语言模型提示敏感性的框架</title>
<link>https://arxiv.org/abs/2410.12405</link>
<guid>https://arxiv.org/abs/2410.12405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProSA框架评估大型语言模型在不同提示下的性能敏感性，提出新的度量指标并揭示其内在机制。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在多项任务中展现出令人印象深刻的能力，但其表现对所使用的提示高度敏感。这种可变性给准确评估和用户满意度带来了挑战。目前的研究常常忽视实例级提示变异及其对主观评估的影响。为了解决这些问题，我们提出了ProSA框架，该框架旨在评估和理解LLMs的提示敏感性。ProSA引入了一种新的敏感性度量指标PromptSensiScore，并利用解码置信度来阐明其内在机制。我们的广泛研究涵盖多项任务，揭示提示敏感性在不同数据集和模型间的波动状况，发现较大的模型表现出更强的鲁棒性。此外，我们观察到少量示例能够缓解这一敏感性问题，主观评估也受到提示敏感性的影响，特别是在复杂的推理任务中。值得注意的是，模型的更高置信度与提示鲁棒性之间呈正相关。我们相信这项工作将成为研究LLMs提示敏感性的重要工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 01:00:15 GMT</pubDate>
</item>
<item>
<title>基于模型亲缘关系的高效语言模型合并策略</title>
<link>https://arxiv.org/abs/2410.12613</link>
<guid>https://arxiv.org/abs/2410.12613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出模型亲缘关系，优化大语言模型合并策略，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了模型亲缘关系（Model Kinship）的概念，旨在衡量大型语言模型（LLMs）之间的相似性和相关性。通过综合实证分析，我们发现模型亲缘关系与模型合并后性能增益之间存在一定的关系，这可以帮助我们在选择候选模型时做出更明智的判断。基于这一发现，我们提出了一种新的合并策略——基于模型亲缘关系的Top-k贪婪合并。这种方法在基准数据集上的表现更佳，并且我们发现利用模型亲缘关系作为标准可以促进持续的模型合并，缓解模型进化过程中的性能退化（局部最优）问题。此外，模型亲缘关系还可以帮助我们摆脱陷阱，实现更有效的模型演化。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:55:42 GMT</pubDate>
</item>
<item>
<title>DocLayout-YOLO：一种高速高准确率的文档布局分析方法</title>
<link>https://arxiv.org/abs/2410.12628</link>
<guid>https://arxiv.org/abs/2410.12628</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种名为DocLayout-YOLO的新方法，旨在提升文档布局分析的速度和准确性。</p><br /><br /><p><strong>摘要：</strong> 在文档理解系统中，文档布局分析至关重要，但面临速度与准确性之间的权衡。针对这一问题，本文提出DocLayout-YOLO，一种通过文档特定优化在预训练和模型设计上提高准确性并保持速度优势的新方法。我们提出Mesh-candidate BestFit算法，将文档合成视为二维装箱问题，从而生成大规模、多样化的DocSynth-300K数据集，实现稳健的文档预训练。使用DocSynth-300K进行预训练显著提升了各种文档类型的微调性能。在模型优化方面，我们提出了Global-to-Local可控感受野模块，能够更好地处理文档元素的多尺度变化。此外，为验证不同文档类型的性能，我们引入了复杂且具挑战性的基准测试DocStructBench。大量实验证明，DocLayout-YOLO在速度和准确性方面均表现优异。代码、数据和模型可在https://github.com/opendatalab/DocLayout-YOLO获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12628" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:44:41 GMT</pubDate>
</item>
<item>
<title>长文本对齐的文本到图像生成模型的优化方法 LongAlign</title>
<link>https://arxiv.org/abs/2410.11817</link>
<guid>https://arxiv.org/abs/2410.11817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LongAlign方法，通过分段编码和偏好优化提高长文本的图像生成对齐效果。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像（T2I）扩散模型的快速发展，它们能够根据给定文本生成前所未有的结果。然而，对于长文本的输入，现有的编码方法（如CLIP）面临限度，导致生成的图像与长文本的对齐变得具有挑战性。为了解决这些问题，本文提出了LongAlign方法，包括一种分段级编码方法，用于处理长文本，以及一种分解的偏好优化方法，以实现有效的对齐训练。在分段级编码中，长文本被拆分为多个段落并单独处理，从而克服了预训练编码模型的最大输入长度限制。在偏好优化方面，我们提供了基于CLIP的分解偏好模型来微调扩散模型。我们深入研究了CLIP-based偏好模型的评分机制，并发现偏好分数可以分解为两个组件：一个与文本相关的部分用于测量T2I对齐程度，另一个与文本无关的部分评估人类偏好的其他视觉方面。通过提出重加权策略，分配这两个组件不同的权重，我们减少了过拟合现象，提高了对齐效果。经过大约20小时的微调，512x512的Stable Diffusion (SD) v1.5显著超越了PixArt-alpha和Kandinsky v2.2等更强大的基础模型。相关代码已发布于https://github.com/luping-liu/LongAlign。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:20:12 GMT</pubDate>
</item>
<item>
<title>限制因素与问题影响：语言智能体规划能力的挑战</title>
<link>https://arxiv.org/abs/2410.12409</link>
<guid>https://arxiv.org/abs/2410.12409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究语言智能体在自主规划中面临的限制因素及其影响，包括约束条件作用有限与问题效应减弱。</p><br /><br /><p><strong>摘要：</strong> 自主规划自人工智能诞生以来一直是一个重要研究领域。早期的规划智能体主要针对特定任务提供精准解决方案，但缺乏通用能力。随着大型语言模型（LLMs）及其强大推理能力的出现，自动生成合适方案的兴趣重新涌现。然而，现有研究表明，目前的语言智能体仍未达到人类级别的规划能力，OpenAI的o1模型在复杂真实世界规划基准上仅获得15.6%的成绩。本文探讨了阻碍语言智能体实现人类级规划的深层次原因，并通过特征归因研究识别了两个关键因素：约束条件的有限作用和问题的减弱影响。虽然目前的解决策略在一定程度上减轻了这些挑战，但并未完全解决，显示出智能体在达到人类级智能方面仍然任重道远。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.12409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Oct 2024 00:09:59 GMT</pubDate>
</item>
<item>
<title>MultiVENT 2.0：多语言事件驱动的视频检索基准</title>
<link>https://arxiv.org/abs/2410.11619</link>
<guid>https://arxiv.org/abs/2410.11619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MultiVENT 2.0是一个大型多语言视频检索基准，涵盖218,000个新闻视频和3,906个事件查询，挑战现有的多模态视频检索系统。</p><br /><br /><p><strong>摘要：</strong> 随着从大规模多模态集合中有效提取和综合信息的需求日益增长，现有的视频检索数据集受到范围限制，主要关注将模糊描述性的查询与小规模、专业编辑的英语视频进行匹配。为了解决这一问题，我们推出了MultiVENT 2.0，这是一个大型多语言事件驱动的视频检索基准，包含超过218,000个新闻视频和3,906个针对特定世界事件的查询。这些查询特别针对视频中的视觉内容、音频、嵌入文本和文本元数据的信息，要求系统充分利用所有这些来源以成功完成任务。初步结果表明，当前最先进的视觉-语言模型在这一任务上面临很大挑战，而其他方法虽然显示出一定的潜力，但仍不足以有效应对这一问题。这些发现突显了建立更强大的多模态检索系统的必要性，有效的视频检索是实现多模态内容理解和生成任务的关键一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 16:08:59 GMT</pubDate>
</item>
<item>
<title>EchoPrime：一种多视角视频基础模型用于全面心脏超声学解读</title>
<link>https://arxiv.org/abs/2410.09704</link>
<guid>https://arxiv.org/abs/2410.09704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EchoPrime通过多视角视频分析，实现精确的心脏超声自动化评估。</p><br /><br /><p><strong>摘要：</strong> Echocardiography是评估心脏结构和功能的最广泛使用的影像学方法，而人工智能（AI）在其应用中具有潜在优势。然而，目前大多数echocardiography AI模型仅限于单一视角和单一任务，未能利用全面检查中多个视角的互补信息，从而限制了其性能和应用范围。为了解决这一问题，我们提出了EchoPrime，这是一种经过训练的多视角、视角信息驱动的视频基础模型，基于1200万个视频-报告对进行训练。EchoPrime使用对比学习建立针对所有标准视角的统一嵌入模型，并具备对常见和罕见疾病的表示。它通过视角分类与视角驱动的解剖注意模型整合视频特定解释，准确映射超声波视角与解剖结构之间的关系。通过增强信息检索，EchoPrime整合全面研究中的所有超声波视频，实现全面的临床解读。在来自两个独立医疗系统的数据集中，EchoPrime在23个心脏形态和功能的多样化基准上达到了最先进的性能，超越了任务特定方法和以前的基础模型。经过严格的临床评估，EchoPrime可以帮助医生自动化地初步评估全面超声心动图。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 13:31:06 GMT</pubDate>
</item>
<item>
<title>NesTools：评估大型语言模型的嵌套工具学习能力的新基准</title>
<link>https://arxiv.org/abs/2410.11805</link>
<guid>https://arxiv.org/abs/2410.11805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出NesTools，评估大型语言模型的嵌套工具学习能力，用于填补相关研究空白。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）与工具学习的结合在现实应用中取得了显著成果，嵌套工具学习的能力成为研究的重点。然而，目前的研究仍然比较薄弱，现有基准缺乏相关的数据实例。为了应对这一问题，本文提出了NesTools，以填补在综合嵌套工具学习评估中的空白。NesTools采用了一种新颖的自动数据生成方法，构建了大规模的嵌套工具调用，涵盖多种嵌套结构。通过人工审查和细化，生成的数据集质量高，且与真实场景密切相关。因此，NesTools可以作为评估LLMs嵌套工具学习能力的新基准。我们在22个LLMs上进行了广泛的实验，并提供了详细的分析，结果显示当前LLMs在复杂的嵌套工具学习任务中的表现仍然存在不足之处。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 12:59:44 GMT</pubDate>
</item>
<item>
<title>Agent-as-a-Judge框架：针对智能体系统的新评估方法</title>
<link>https://arxiv.org/abs/2410.10934</link>
<guid>https://arxiv.org/abs/2410.10934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Agent-as-a-Judge框架，针对智能体系统的评估提供新的思路和方法。</p><br /><br /><p><strong>摘要：</strong> 当今的评估技术对智能体系统的评估显得不足，常常只关注最终结果，而忽略了智能体系统解决问题的逐步过程，或者需要过多的人工劳动。为此，我们提出了Agent-as-a-Judge框架，利用智能体系统相互评估的方式，作为LLM-as-a-Judge框架的自然延伸，结合了能够为整个任务解决过程提供中间反馈的智能体特征。我们将该框架应用于代码生成任务，开发了名为DevAI的新基准，包含55个真实的自动化AI开发任务以及365个分层用户需求的详细手动注释。通过Agent-as-a-Judge基准测试了三种流行的智能体系统，结果显示其表现远超LLM-as-a-Judge，与人类评估基线同样可靠。总体而言，Agent-as-a-Judge为现代智能体系统的发展提供了富有意义和可靠的奖励信号，实现动态和可扩展的自我改进，标志着该领域的一个重要进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10934" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 12:41:48 GMT</pubDate>
</item>
<item>
<title>基于COCO的互动图像抠图数据集与方法研究</title>
<link>https://arxiv.org/abs/2410.06593</link>
<guid>https://arxiv.org/abs/2410.06593</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出COCO-Matting数据集及SEMat方法，以改善交互式自然图像抠图性能。</p><br /><br /><p><strong>摘要：</strong> 近期的研究尝试将强大的交互式分割模型如SAM适应于交互式抠图，并通过合成抠图数据集进行微调。然而，基于合成数据训练的模型在复杂及遮挡场景中表现不佳。为了解决这一挑战，本文提出了一个基于COCO数据集的新抠图数据集COCO-Matting。具体来说，COCO-Matting的构建包括配件融合和从语义分割掩码到抠图标签的转换，选取复杂的真实世界图像并将其语义分割掩码转换为抠图标签。构建的COCO-Matting包含38,251个在复杂自然场景下的人物实例级α遮罩。此外，现有的基于SAM的抠图方法从冻结的SAM中提取中间特征和掩码，仅通过端到端的抠图损失训练一个轻量级的抠图解码器，未能充分挖掘预训练SAM的潜力。因此，本文提出了SEMat，通过改革网络架构和训练目标以提升性能。新提出的特征对齐变压器能够提取细粒度的边缘和透明特征，而抠图对齐解码器则旨在细分抠图特定对象，并将粗糙掩码转换为高精度的抠图。通过在七个多样化数据集上的广泛实验，证明了本方法的卓越性能，验证了其在交互自然图像抠图中的有效性。此外，我们将代码、模型和数据集开源于 https://github.com/XiaRho/SEMat。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06593" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 11:19:09 GMT</pubDate>
</item>
<item>
<title>基于LLMtimesMapReduce框架的长文本处理研究</title>
<link>https://arxiv.org/abs/2410.09342</link>
<guid>https://arxiv.org/abs/2410.09342</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LLMtimesMapReduce框架，通过分块和聚合策略提升长文本处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着长文本处理需求的增加，扩大大型语言模型（LLMs）的上下文窗口成为重要的研究领域。本文提出了一种新颖的训练无关框架——LLMtimesMapReduce，利用分而治之的策略实现全面的文档理解。该框架将整个文档分割为多个部分供大语言模型处理，然后将中间答案聚合以生成最终输出。面对长文本处理中的难点，特别是分割文本可能导致重要的长距离信息丢失的问题，提出了两类干扰信息的分类，包括块间依赖和块间冲突。为此，本文设计了一个结构化信息协议来处理块间依赖，并引入了上下文置信度校准机制以解决块间冲突。实验结果表明，LLMtimesMapReduce在性能上优于多个代表性的开源和商业长上下文LLMs，并适用于多种模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09342" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 10:23:09 GMT</pubDate>
</item>
<item>
<title>互惠增强效应：文本分类中词级与文本级分类的协同关系</title>
<link>https://arxiv.org/abs/2410.09745</link>
<guid>https://arxiv.org/abs/2410.09745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过实验证实互惠增强效应在文本分类中的有效性，展示词级信息对文本级分类的提升作用。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了互惠增强效应（Mutual Reinforcement Effect, MRE），阐述词级与文本级分类在文本分类任务中的协同关系及其相互促进的可能性。以往研究中未能充分证明这一机制，因此我们通过实验证实了MRE理论的有效性。我们在21个MRE混合数据集上进行实验，发现模型的表现受到MRE影响。在实验中，我们对比了不同的模型微调实验，结果确认了MRE的存在。此外，我们将MRE扩展到提示学习中，利用词级信息作为语言化工具来提升模型对文本级分类标签的预测能力。在最后的实验中，F1-score在21个MRE混合数据集中有18个超过了基线值，这进一步验证了词级信息提高了语言模型对整体文本理解的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 09:54:44 GMT</pubDate>
</item>
<item>
<title>SimBa：通过注入简约偏差来提升深度强化学习的网络规模</title>
<link>https://arxiv.org/abs/2410.09754</link>
<guid>https://arxiv.org/abs/2410.09754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SimBa架构通过简约偏差提高深度强化学习的样本效率，表现超过多种深度RL算法。</p><br /><br /><p><strong>摘要：</strong> 近年来，计算机视觉（CV）和自然语言处理（NLP）的进展主要得益于网络参数的扩展，虽然传统理论认为大规模网络容易过拟合。然而，这些大型网络通过集成简约偏差组件来指导模型趋向于简单且可泛化的解决方案。在深度强化学习（RL）领域，针对网络设计与扩展的研究相对较少。因此，我们提出了SimBa，一种通过注入简约偏差来扩展深度RL参数的架构。SimBa包含三个关键组件：（i）观察标准化层，通过运行统计量标准化输入，（ii）残差前馈块，为输入到输出提供线性路径，以及（iii）层归一化，用于控制特征幅度。通过将SimBa集成到各类深度RL算法中，包括离策略、在策略和无监督方法，样本效率得到了显著提高。此外，仅仅将SimBa架构应用于软演员评论家（SAC），即能在DMC、MyoSuite和HumanoidBench等环境中匹敌或超越当前最先进的深度RL方法，展现出SimBa在多样化强化学习算法和环境中的广泛适应性与有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 09:31:13 GMT</pubDate>
</item>
<item>
<title>MoE LLMs在嵌入模型中的应用研究</title>
<link>https://arxiv.org/abs/2410.10814</link>
<guid>https://arxiv.org/abs/2410.10814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明Mixture-of-Experts LLMs在嵌入任务中表现出色，无需进一步微调。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Mixture-of-Experts（MoE）大语言模型（LLMs）作为嵌入模型的潜力。尽管传统的解码器架构限制了LLMs在嵌入任务上的表现，我们的研究显示MoE中的专家路由器可以作为一种即插即用的嵌入模型，能够在多个嵌入任务上取得令人满意的效果，而无需进行进一步的微调。通过对MoE路由权重（RW）和隐藏状态（HS）的深入分析，我们发现RW在处理输入提示时更具鲁棒性，并且更侧重于高层语义。借此分析动机，我们提出了MoEE模型，该模型结合了RW和HS，这一组合在性能上优于单独使用其中任何一个。我们还探索了它们的结合方式及提示策略，发现RW和HS的相似性加权和优于它们连接后的相似性。最后，我们在Massive Text Embedding Benchmark（MTEB）的20个数据集上进行了6项嵌入任务的实验，结果表明MoEE在LLM基础的嵌入任务上实现了显著的性能提升，无需进一步微调。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10814" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 07:49:01 GMT</pubDate>
</item>
<item>
<title>扩散模型的高效性综述：理论与实践</title>
<link>https://arxiv.org/abs/2410.11795</link>
<guid>https://arxiv.org/abs/2410.11795</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述扩散模型的设计原则与高效应用，助力模型应用与研究。</p><br /><br /><p><strong>摘要：</strong> 扩散模型作为近年来备受关注的生成模型，展示了在图像合成、视频生成、分子设计、3D场景渲染及多模态生成等多种生成任务中的卓越优势。其成功归功于渐进式设计原则以及高效的架构、训练、推理和部署方法论。然而，目前尚缺乏一项综合深入的综述来总结这些原则和实践，为快速理解与应用扩散模型提供帮助。因此，本文从高效性角度出发，梳理了现有的研究和实践，重点关注架构设计、模型训练、快速推理和可靠部署的深刻原则和高效做法，旨在以读者友好的方式指导进一步理论研究、算法迁移及新场景下的模型应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11795" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 07:46:08 GMT</pubDate>
</item>
<item>
<title>LVD-2M：用于长视频生成的新型长拍视频数据集</title>
<link>https://arxiv.org/abs/2410.10816</link>
<guid>https://arxiv.org/abs/2410.10816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一个新数据集LVD-2M，旨在促进长视频生成模型的研究。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成模型的不断发展，长视频生成的研究受到越来越多的关注。现有的视频生成模型通常依赖于短视频数据集，而获取高质量的长视频数据集则成为这一领域发展的障碍。本文提出了LVD-2M数据集，旨在推动长视频生成研究。该数据集的四个关键特性包括：每个视频至少持续10秒、无场景切割的长拍视频、丰富的动态内容和临时密集的字幕。为了实现这一目标，本文介绍了一种新颖的筛选高质量长拍视频的流程与临时密集字幕生成的分层视频标注管道。通过定义一系列定量评估视频质量的指标，如场景切割、动态程度和语义质量，成功过滤出高质量的长拍视频。在此基础上，创建了包括200万个视频的LVD-2M数据集。每个视频的时长超过10秒，且配有临时密集的字幕。进一步通过微调视频生成模型，验证了LVD-2M在长视频生成中的有效性。希望该工作能为长视频生成的未来研究作出重大贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 06:15:43 GMT</pubDate>
</item>
<item>
<title>RoboDual：协同的通用与专业政策机器人系统</title>
<link>https://arxiv.org/abs/2410.08001</link>
<guid>https://arxiv.org/abs/2410.08001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboDual通过通用与专业政策的结合，提升了多任务处理的效率与精度。</p><br /><br /><p><strong>摘要：</strong> 随着对多功能机器人系统需求的增加，RoboDual应运而生，结合了通用策略和专业策略的优势。通用策略利用大量跨体数据实现广泛适应和高层次推理，而专业策略则针对特定领域数据进行优化，拥有更高的任务精准度和效率。然而，通用策略在推理效率和训练成本上存在不足。RoboDual通过采用基于扩散变压器的专业政策，进行多步动作预测，依赖于视觉-语言-行动（VLA）通用政策所提供的高层任务理解和离散化动作输出。在实际应用中，RoboDual较OpenVLA提高了26.7%的性能，在CALVIN任务上提高了12%。尽管只使用了5%的示范数据，RoboDual依然保持强劲表现，并在实际部署中实现了3.8倍的控制频率提升。代码将公开分享，项目页面可访问：https://opendrivelab.com/RoboDual/。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 04:20:04 GMT</pubDate>
</item>
<item>
<title>What Matters in Transformers? Not All Attention is Needed</title>
<link>https://arxiv.org/abs/2406.15786</link>
<guid>https://arxiv.org/abs/2406.15786</guid>
<content:encoded><![CDATA[
While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\% speedup with only a 2.4\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/Shwai-He/LLM-Drop.
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 03:14:52 GMT</pubDate>
</item>
<item>
<title>动态修正解码方法（DeCo）在多模态大型语言模型中的应用</title>
<link>https://arxiv.org/abs/2410.11779</link>
<guid>https://arxiv.org/abs/2410.11779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种动态修正解码方法（DeCo），有效减少多模态大型语言模型的幻觉现象。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）常常出现幻觉现象，但其背后的原因尚不明确。本文通过实证分析发现，尽管MLLMs在最终输出中错误生成对象，但它们能在前几层识别视觉对象。我们推测这是由于语言模型强烈的知识优先性抑制了视觉信息，从而导致幻觉现象。基于此，我们提出了一种新颖的动态修正解码方法（DeCo），该方法可以自适应地选择合适的前几层，并将知识按比例整合到最终层，以调整输出的logits。DeCo是模型无关的，可以与多种经典解码策略无缝结合，并应用于不同的MLLMs。在广泛使用的基准上，我们评估了DeCo，结果表明其能够显著降低幻觉率，相比基线表现出改善，展示了其减轻幻觉的潜力。代码可在 https://github.com/zjunlp/DeCo 获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:56:29 GMT</pubDate>
</item>
<item>
<title>MTU-Bench：一种多粒度的大语言模型工具使用基准</title>
<link>https://arxiv.org/abs/2410.11710</link>
<guid>https://arxiv.org/abs/2410.11710</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MTU-Bench通过多样化场景和无需GPT或人类评估的基础指标，提升大语言模型的工具使用能力评估。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多粒度工具使用基准，称为MTU-Bench，旨在针对大语言模型（LLMs）的工具使用能力进行评估。现有的工具使用基准存在评估场景不足和高昂的评估成本等局限性。为了克服这些问题，MTU-Bench覆盖了五种工具使用场景，包括单轮单工具、单轮多工具、多轮单工具、多轮多工具以及超出分布的任务。此外，MTU-Bench的所有评估指标均基于预测结果与真实值的比较，无需依赖GPT或人类评估。该基准数据集通过转换现有高质量数据集，模拟了真实世界的工具使用场景。我们还提出了一个指令数据集MTU-Instruct，以增强现有LLMs的工具使用能力。通过实验结果表明，MTU-Bench在评估工具使用能力方面有效，相关的代码和数据将会在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11710" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:14:22 GMT</pubDate>
</item>
<item>
<title>SecCodePLT：全面评估代码生成AI安全风险的平台</title>
<link>https://arxiv.org/abs/2410.11096</link>
<guid>https://arxiv.org/abs/2410.11096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SecCodePLT平台，全面评估代码生成AI的安全风险，包括不安全编码和网络攻击的可行性。</p><br /><br /><p><strong>摘要：</strong> 已有研究确立了多个基准，揭示了代码生成AI（Code GenAI）的安全风险，主要集中在两个方面：模型生成不安全代码的潜力和其在网络攻击中的应用效用。虽然现有基准取得了显著进展，但仍有进一步改进的机会。目前的一些基准主要关注模型提供攻击建议的能力，而忽视了生成可执行攻击的能力。此外，大多数基准过于依赖静态评估指标，而动态指标如通过测试案例的能力可能更为准确。而专家验证的基准虽然提供了高质量数据，但多以小规模运作。为了解决这些问题，我们开发了SecCodePLT，这是一个统一且全面的评估平台，用于检测代码生成AI的风险。在不安全编码方面，我们推出了一种新数据创建方法，将专家与自动生成结合，确保数据质量的同时实现大规模生成。我们还将样本与测试案例关联，以进行代码相关的动态评估。在网络攻击有用性方面，我们设置了真实的评估环境，并构建样本以引导模型生成实际攻击，同时在该环境中应用动态指标。经过广泛实验，我们展示了SecCodePLT在安全相关性上优于现有的SOTA基准CyberSecEval，并能更好地识别SOTA模型面临的不安全编码和网络攻击有用性方面的风险。最后，我们将SecCodePLT应用于SOTA代码代理Cursor，首次识别出该高级编码代理的非平凡安全风险。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:10:40 GMT</pubDate>
</item>
<item>
<title>利用多语言大模型解决低资源语言医疗数据稀缺问题</title>
<link>https://arxiv.org/abs/2410.10626</link>
<guid>https://arxiv.org/abs/2410.10626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出了一种新型的MoE路由方法，以提高多语言医疗模型对低资源语言的适应性。</p><br /><br /><p><strong>摘要：</strong> 在医疗领域，多语言大模型（LLMs）可以打破语言障碍，提高医疗服务的获取率，但数据稀缺问题依然突出，特别是在低资源语言中。为了解决这一挑战，本文首先构建了一个高质量的医疗数据集，并进行了质量分析。接着，我们从多语言视角探索LLMs的内部信息流，采用专家混合（MoE）模块化。技术上，我们提出了一种新型的MoE路由方法，利用语言特定的专家与跨语言路由。受电路理论的启发，我们的路由分析揭示了一种信息流机制：早期层集中跨语言信息流，而后期层则表现出语言特定的分歧。基于此，我们发展了后MoE架构，仅在后期层应用稀疏路由，同时保持其他层的密集性。实验结果表明，该方法提升了多语言模型对其他语言的泛化能力，同时保持了解释性。最后，为了高效扩展模型至50种语言，我们引入了语言家族专家的概念，借助语言学先验，能够在不增加额外参数的情况下扩展语言数量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 01:01:31 GMT</pubDate>
</item>
<item>
<title>基于空间和角度高斯表示的实时高质量照明与视图合成</title>
<link>https://arxiv.org/abs/2410.11419</link>
<guid>https://arxiv.org/abs/2410.11419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种结合高斯表示和三重溅射过程的技术，实现多视角图像的实时照明与视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于空间和角度高斯表示的技术，以及一种三重溅射过程，旨在实现实时、高质量的新照明和视图合成。为了描述复杂的外观，我们为每个空间高斯引入了一种效果良好的反射函数，即兰伯特反射与角度高斯的混合模型。生成自阴影的过程是通过将所有空间高斯溅射向光源，并计算阴影值，随后利用小型多层感知网络进行精细化处理。此外，为了补偿全局照明等其他效果，我们训练了另一个网络，以计算和添加每个空间高斯对应的RGB元组。本文展示的结果在30个样本上进行了验证，样本涵盖了几何形状（从坚固到松散）和外观（从半透明到各向异性）的广泛变化，并使用了多种输入数据，包括合成/重建对象的渲染图像、手持相机拍摄的照片、以及来自专业灯台的图像。我们在单个普通GPU上的训练时间为40-70分钟，并达到了每秒90帧的渲染速度。与现有技术相比，我们的结果在质量和性能上均表现优异。代码和数据已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.11419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Oct 2024 00:26:35 GMT</pubDate>
</item>
<item>
<title>探索去规范化解码器中激活函数的优化</title>
<link>https://arxiv.org/abs/2410.09637</link>
<guid>https://arxiv.org/abs/2410.09637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究发现ReLU在去LayerNorm模型中超越GELU，改善学习动态和信息保留。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了在去LayerNorm的解码器中选择激活函数的问题，发现ReLU在性能上显著优于GELU。传统上，变换器模型偏爱GELU，但我们的实证研究显示ReLU在这类架构中提供了8.2%的困惑度改善。我们分析了GELU在早期层中所产生的熵过载现象，导致注意力头的表现能力未能充分利用。这表明，像GELU这样平滑的激活函数不适合去LayerNorm架构，而ReLU的几何特性则在缺乏LayerNorm的情况下改善学习动态和信息保留。这项研究为优化具有显著挑战的变换器架构提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 20:33:34 GMT</pubDate>
</item>
<item>
<title>大语言模型中的语言结构与内部电路的对应关系研究</title>
<link>https://arxiv.org/abs/2410.09223</link>
<guid>https://arxiv.org/abs/2410.09223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨大语言模型如何处理不同语言的形态句法过程，发现共享电路与语言特异组件的共存。</p><br /><br /><p><strong>摘要：</strong> 本研究通过对大语言模型(LLMs)的机制解释工具，探讨其内部结构与语言的形态句法过程之间的关系。我们主要提出两个问题：第一，当两种语言使用相同的形态句法过程时，LLMs是否使用共享的内部电路处理这些过程？第二，当两种语言采用不同的形态句法过程时，LLMs是否使用不同的内部电路？针对英语和中文的多语言及单语言模型，我们分析了涉及两项任务的内部电路。研究结果表明，模型在处理相同句法过程时，无论是在何种语言中，均使用相同的电路，甚至在完全独立训练的单语言模型中也是如此。此外，我们还发现多语言模型在处理某些特有的语言过程（例如形态标记）时，会运用语言特定的组件（如注意力头和前馈网络）。这些结果为大语言模型在同时建模多种语言时，如何在利用共通结构与保留语言差异之间进行权衡提供了新的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 14:32:40 GMT</pubDate>
</item>
<item>
<title>VisRAG：面向多模态文档的视觉-语言检索增强生成</title>
<link>https://arxiv.org/abs/2410.10594</link>
<guid>https://arxiv.org/abs/2410.10594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了VisRAG，一个基于视觉-语言模型的检索增强生成系统，用于多模态文档的处理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VisRAG，一个解决现有基于文本的检索增强生成（RAG）系统局限的新方法。传统RAG系统无法有效利用多模态文档中的视觉信息，因此我们提出了VisRAG，通过直接使用视觉-语言模型（VLM）处理文档。这一新颖的流程避免了在文本解析过程中可能引起的信息丢失，从而最大限度地保留了原始文档中的信息。我们收集了开放源代码和合成数据来训练VisRAG中的检索器，探索多种生成方法。实验结果显示，VisRAG在检索和生成两个阶段均优于传统的文本基础RAG，整体性能提升幅度达到25-39%。进一步的分析表明，VisRAG在利用训练数据方面表现出色，并展现出强大的泛化能力，成为多模态文档RAG的有前景的解决方案。我们的代码和数据已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 13:58:17 GMT</pubDate>
</item>
<item>
<title>提升大型语言模型推理能力的训练方法</title>
<link>https://arxiv.org/abs/2410.10630</link>
<guid>https://arxiv.org/abs/2410.10630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新的训练方法，使大型语言模型具备推理能力，从而提高指令响应效果。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在指令跟随能力的广泛应用，如何使其具备更好的推理能力成为研究的重点。本文提出了一种新的训练方法，通过迭代搜索和优化程序，探索可能的思维生成空间，使现有模型在没有额外人类数据的情况下学习思考能力。对此，每个指令的思维候选方案使用一个判别模型进行评分，并通过偏好优化进行改进。研究显示，这种新方法在AlpacaEval和Arena-Hard等基准测试中表现优越，同时还在非推理类任务（如营销、健康和一般知识）上取得改善。该方法为大型语言模型的训练提供了新的视角，证明了思考能力在多种任务中的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 13:51:05 GMT</pubDate>
</item>
<item>
<title>MMCOMPOSITION：评估大规模视觉-语言模型的组合能力的新基准</title>
<link>https://arxiv.org/abs/2410.09733</link>
<guid>https://arxiv.org/abs/2410.09733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MMCOMPOSITION基准，以评估视觉-语言模型的组合能力，发现GPT-4o在此方面表现不佳。</p><br /><br /><p><strong>摘要：</strong> 随着大规模视觉-语言模型（VLM）的出现，跨模态理解取得了显著进步，促进了图像和视频标注、视觉问答和跨模态检索等任务的精确集成。尽管VLM具有优越的能力，研究者对其组合能力的理解仍然不够全面。现有基准仅从对象、关系和属性的粗略角度评估组合性，忽视了对对象交互、计数和复杂构图的深层次推理。然而，组合能力是促进VLM跨模态推理和理解的关键能力。为此，我们提出了MMCOMPOSITION，一个新的人类标注基准，旨在全面和准确地评估VLM的组合能力。通过MMCOMPOSITION，我们能够量化和探索主流VLM的组合能力。令人惊讶的是，实验发现GPT-4o的组合能力低于最佳开源模型，同时分析了其背后的原因。我们的实验分析揭示了VLM在细粒度组合感知和推理方面的局限性，并指出了VLM设计和训练的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 12:32:30 GMT</pubDate>
</item>
<item>
<title>LiveXiv：基于科学ArXiv论文的可扩展实时基准测试</title>
<link>https://arxiv.org/abs/2410.10783</link>
<guid>https://arxiv.org/abs/2410.10783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiveXiv是一个自动生成科学论文视觉问答对的基准测试，评估多模态模型的真实能力。</p><br /><br /><p><strong>摘要：</strong> LiveXiv是一个新的基准测试，旨在通过科学ArXiv论文评估多模态模型的能力。它通过实时访问领域特定的手稿，在没有人工干预的情况下，自动生成视觉问答对（VQA）。提议的方案利用手稿中的图表和数据向量，采用高效的评估方法，使用有限数量模型的结果来估计所有模型的表现，从而降低整体评估费用。该基准的首个版本对多种开放和私有的大型多模态模型（LMMs）进行了基准测试，展现了独特的挑战性并揭示了模型的真实能力，避免了测试数据污染。为了保证高质量，研究团队还收集并评估了经过人工验证的子集，与自动注释的结果进行对比，发现性能差异小于2.5%。该数据集已在HuggingFace上公开，并且代码也将提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10783" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 04:40:56 GMT</pubDate>
</item>
<item>
<title>问题树（ToP）：解决复杂推理任务的新方法</title>
<link>https://arxiv.org/abs/2410.06634</link>
<guid>https://arxiv.org/abs/2410.06634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出问题树（ToP），在复杂任务中表现优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本研究提出一种新方法——问题树（Tree of Problems，ToP），旨在处理可分解为相同子任务的复杂问题。尽管大语言模型（LLMs）在许多任务中表现卓越，尤其是在上下文学习方面，链式思考（Chain-of-Thought，CoT）提示在复杂推理中也取得了良好结果，但仍存在一些挑战。为了解决这些困难，树状思维（Tree of Thoughts，ToT）和图形思维（Graph of Thoughts，GoT）提出了将复杂问题划分为子问题的思路。我们假设，问题树作为ToT的简化版本，能够更有效地处理可以划分为相同子任务的复杂任务。我们的实证结果显示，ToP在多个复杂推理任务上超越了ToT和GoT的表现，同时也优于CoT。研究所用的所有代码已公开，方便其他研究者使用和验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 04:34:39 GMT</pubDate>
</item>
<item>
<title>基于动态最优控制的矩形流模型图像反演与编辑</title>
<link>https://arxiv.org/abs/2410.10792</link>
<guid>https://arxiv.org/abs/2410.10792</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新方法，通过动态最优控制进行矩形流模型的图像反演和编辑。</p><br /><br /><p><strong>摘要：</strong> 本文针对生成模型中的两个关键任务：图像反演与编辑，提出了一种基于矩形流（Rectified Flows, RFs）的框架。尽管扩散模型在图像生成领域取得了显著成功，但其反演过程中由于漂移和扩散的非线性特性导致了忠实性和可编辑性问题。现有的扩散模型反演方法往往需要额外参数的训练或测试时对潜变量的优化，这在实际操作中成本高昂。矩形流提供了一种有前景的替代方案，但其反演方法尚未得到充分研究。我们提出的一种RF反演方法，基于动态最优控制，通过线性二次调节器导出。我们证明了所得到的向量场等价于一个矩形的随机微分方程。此外，我们将该框架扩展为Flux的随机采样器。我们的反演方法在零-shot反演和编辑任务中表现出色，在笔触到图像合成和语义图像编辑方面优于现有方法，且大规模人类评估结果验证了其用户偏好的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10792" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 03:10:41 GMT</pubDate>
</item>
<item>
<title>长时记忆评估框架：提升聊天助手的记忆能力</title>
<link>https://arxiv.org/abs/2410.10813</link>
<guid>https://arxiv.org/abs/2410.10813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LongMemEval基准，评估聊天助手的长时记忆能力，探索优化设计以提升记忆召回与问答表现。</p><br /><br /><p><strong>摘要：</strong> 近期的大型语言模型（LLM）驱动的聊天助手系统已集成内存组件，以跟踪用户与助手之间的对话历史，从而实现更准确和个性化的响应。然而，它们在持续交互中的长期记忆能力仍未得到充分探索。本文介绍了LongMemEval，这是一个综合性基准，旨在评估聊天助手五种核心的长期记忆能力：信息提取、多会话推理、时间推理、知识更新和弃权。LongMemEval包含500个精心策划的问题，嵌入可自由扩展的用户-助手对话历史中，给现有的长期记忆系统带来了重大挑战。实验结果显示，商业聊天助手和长上下文LLM在持续交互中的信息记忆准确率下降了30%。我们提出一个统一框架，将长期记忆设计分解为四个设计选择，涵盖索引、检索和读写阶段。基于关键实验洞察，提出几种内存设计方案，包括会话分解以优化值粒度、事实增强键扩展以增强索引结构，以及时间感知查询扩展以改进搜索范围。实验结果表明，这些优化显著提高了在LongMemEval上的记忆召回率和后续问答表现。总体而言，本研究为推动基于LLM的聊天助手长期记忆能力提供了宝贵的资源和指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 03:01:29 GMT</pubDate>
</item>
<item>
<title>TemporalBench：评估视频中的细粒度时间理解的新基准</title>
<link>https://arxiv.org/abs/2410.10818</link>
<guid>https://arxiv.org/abs/2410.10818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了TemporalBench，一个新的基准，用于评估视频的细粒度时间理解能力。</p><br /><br /><p><strong>摘要：</strong> 理解细粒度时间动态对于多模态视频的理解和生成至关重要。由于缺乏细粒度时间标注，现有视频基准多类似静态图像基准，无法有效评估模型的时间理解能力。为此，本文介绍了TemporalBench，一个专为评估视频的细粒度时间理解而设计的新基准。TemporalBench包含约10,000对视频问答对，源于约2,000个高质量的人类标注，这些标注详细描述了视频片段中的时间动态。该基准提供了一个独特的测试平台，以评估各种时间理解和推理能力，如动作频率、运动幅度、事件顺序等，支持视频问答、视频字幕生成、短视频和长视频理解等多项任务，以及多种模型，如多模态视频嵌入模型和文本生成模型。结果显示，诸如GPT-4o等最先进模型在TemporalBench上的问答准确率仅为38.5%，表明人类与AI在时间理解上存在显著差距（约30%）。此外，我们注意到多选QA的一个关键缺陷，即大型语言模型（LLMs）能够检测负面描述中的微妙变化，并将中心化描述作为预测线索，因此我们提出了多个二元准确度（MBA）来纠正这种偏差。我们希望TemporalBench能够促进研究，提高模型的时间推理能力。数据集和评估代码将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 02:16:22 GMT</pubDate>
</item>
<item>
<title>自主操作的改进型3D扩散政策（iDP3）在多样化环境下的应用</title>
<link>https://arxiv.org/abs/2410.10803</link>
<guid>https://arxiv.org/abs/2410.10803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种改进型3D扩散政策（iDP3），使人形机器人在多样环境中自主执行技能。</p><br /><br /><p><strong>摘要：</strong> 自主操作的人形机器人一直是机器人研究者追求的目标。然而，由于获取可泛化技能的难度，自主操作的人形机器人往往只能在特定的场景中进行有效操作。近年来，3D视动政策（如3D Diffusion Policy, DP3）的进展为拓展这些能力到多样化环境提供了希望。然而，3D视动政策通常依赖于相机标定和点云分割，这对移动机器人（如人形机器人）的应用造成了挑战。为了解决这些问题，本文提出了一种新的3D视动政策——改进型3D扩散政策（iDP3），它利用自我中心的3D视觉表示，消除了上述限制。我们展示了iDP3能够使全尺寸人形机器人在多样的真实场景中自主执行技能，并使用仅在实验室收集的数据进行训练。相关视频可以在: https://humanoid-manipulation.github.io 获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 02:14:57 GMT</pubDate>
</item>
<item>
<title>Cavia：一种可控相机的多视角视频生成框架</title>
<link>https://arxiv.org/abs/2410.10774</link>
<guid>https://arxiv.org/abs/2410.10774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cavia框架实现了从输入图像生成多个时空一致的视频，支持精确控制相机运动与物体运动。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像到视频生成领域取得了显著突破，但生成帧在3D一致性和相机可控性方面仍存在未解决的问题。尽管有研究尝试将相机控制融入生成过程中，但结果往往仅限于简单轨迹，或缺乏对同一场景的多条独特相机路径生成一致视频的能力。为了解决这些限制，我们提出了Cavia，这是一种新颖的相机可控、多视角视频生成框架，能够将输入图像转换为多个时空一致的视频。我们的框架将空间和时间注意力模块扩展为视角集成注意力模块，从而提高视角和时间一致性。这种灵活的设计允许与多种策划数据源进行联合训练，包括场景级静态视频、对象级合成多视角动态视频以及现实世界单目动态视频。到目前为止，Cavia是首个允许用户精确指定相机运动的同时获得物体运动的系统。大量实验表明，Cavia在几何一致性和感知质量上优于最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:58:29 GMT</pubDate>
</item>
<item>
<title>Animate-X：针对各种角色类型的通用动画框架</title>
<link>https://arxiv.org/abs/2410.10306</link>
<guid>https://arxiv.org/abs/2410.10306</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Animate-X通用动画框架，以提高对人形和拟人角色的动画效果。</p><br /><br /><p><strong>摘要：</strong> 随着过去几年动画技术的进步，通过参考图像和目标姿势序列生成高质量视频的角色图像动画取得了显著进展。然而，现有大部分方法仅适用于人类形象，难以在游戏和娱乐行业使用的拟人角色上进行有效泛化。深入分析认为，这一局限性源于对运动建模的不足，导致无法理解驱动视频的动作模式，从而将姿势序列僵硬地施加于目标角色。为此，本文提出了Animate-X，一个基于LDM的通用动画框架，能够处理各种角色类型（统称为X），包括拟人角色。为增强运动表现，我们引入了Pose Indicator，旨在通过隐式与显式方式捕捉驱动视频的综合运动模式。隐式方式利用CLIP视觉特征提取运动的整体模式和动作间的时间关系，而显式方式则通过预先模拟推理过程中可能出现的输入来增强LDM的泛化能力。此外，我们还提出了一个新的动画拟人基准（A^2Bench）来评估Animate-X在通用动画图像上的性能。广泛实验结果验证了Animate-X相对于现有最先进方法的优越性和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10306" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:34:40 GMT</pubDate>
</item>
<item>
<title>MEGA-Bench：一种大规模多模态评估套件</title>
<link>https://arxiv.org/abs/2410.10563</link>
<guid>https://arxiv.org/abs/2410.10563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEGA-Bench 是一项涵盖 505 个现实任务的多模态评估套件， 提供多样化的输出格式和细致的模型评估。</p><br /><br /><p><strong>摘要：</strong> MEGA-Bench 是一个新建立的评估套件，旨在将多模态评估扩展到超过 500 个现实世界任务，以应对用户日常使用中的高度异质性需求。我们的目标是优化高质量数据样本的收集，涵盖多样且丰富的多模态任务，同时实现成本有效且准确的模型评估。我们从 16 位专家注释员处收集了 505 个现实任务，共计超过 8000 个样本，以广泛覆盖多模态任务空间。不同于 MMMU、MMBench 和 MMT-Bench 通过标准多项选择题统一这些问题，MEGA-Bench 接纳了多种输出格式，包括数字、短语、代码、LaTeX、坐标、JSON 和自由格式等。为适应这些格式，我们开发了超过 40 种指标来评估这些任务。MEGA-Bench 还提供跨多个维度（如应用、输入类型、输出格式和技能）进行细致的能力报告，使用户能够深入交互和可视化模型能力。我们在 MEGA-Bench 上评估了多种前沿的视觉语言模型，以了解它们在这些维度上的能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:21:36 GMT</pubDate>
</item>
<item>
<title>TVBench: Redesigning Video-Language Evaluation</title>
<link>https://arxiv.org/abs/2410.07752</link>
<guid>https://arxiv.org/abs/2410.07752</guid>
<content:encoded><![CDATA[
Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline.
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 01:06:10 GMT</pubDate>
</item>
<item>
<title>大规模数据选择在监督微调中的关键性研究</title>
<link>https://arxiv.org/abs/2410.09335</link>
<guid>https://arxiv.org/abs/2410.09335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大量数据选择在监督微调中的重要性，强调多样性优于单纯的高质量数据。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了在大规模数据集中进行监督微调（SFT）时，如何选择代表性的训练数据以提高模型性能。我们在200万样本的数据集中复现了多种自评分方法，发现这些方法普遍无法显著优于随机选择。特别是在 SFT 过程中，数据选择的多样性显得尤为重要，而不仅仅是关注数据的高质量。此外，我们还探讨了当前方法的局限性，解释了它们在大规模数据集中的低效表现。经过分析，我们发现通过令牌长度对数据进行过滤是一种稳定且高效的改进方法，尤其对相对较弱的基础模型（如 Llama3）来说，训练长文本数据时表现出显著的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:50:02 GMT</pubDate>
</item>
<item>
<title>LOKI: 评估大型多模态模型Synthetic Data检测能力的新基准</title>
<link>https://arxiv.org/abs/2410.09732</link>
<guid>https://arxiv.org/abs/2410.09732</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOKI是一个新颖的基准，评估大型多模态模型在合成数据检测中的能力和局限性。</p><br /><br /><p><strong>摘要：</strong> 随着AI生成内容的快速发展，互联网将面临大量合成数据的挑战，真实与合成内容的区分变得愈发困难。因此，合成数据检测受到了广泛关注，尤其是大型多模态模型（LMMs）在此任务中的表现引起了极大的兴趣。LMMs能够为其真实性判断提供自然语言解释，从而增强合成内容检测的可解释性。为此，我们提出了LOKI这一新基准，旨在评估LMMs在多种模态中检测合成数据的能力。LOKI涵盖视频、图像、3D、文本和音频等多种模态，包含18K个细致策划的问题，涵盖26个子类别并设有明确的难度等级。该基准不仅包括粗粒度判断和多项选择题，还包含细粒度的异常选择与解释任务，从而对LMMs进行全面分析。我们在LOKI上对22个开源LMM和6个闭源模型进行了评估，突显了它们作为合成数据检测器的潜力，同时也揭示了LMM能力开发中的一些局限性。更多信息请访问 https://opendatalab.github.io/LOKI/ 。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09732" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:49:53 GMT</pubDate>
</item>
<item>
<title>VIF-RAG：提高检索增强生成系统指令跟随对齐的自动化框架</title>
<link>https://arxiv.org/abs/2410.09584</link>
<guid>https://arxiv.org/abs/2410.09584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VIF-RAG 提出了一个用于提升 RAG 系统指令跟随对齐的新方法，包含自动化、可扩展的合成管道和 FollowRAG 基准。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型（LLMs）的发展，检索增强生成（RAG）系统在多种应用中表现出色，但指令跟随（IF）对齐的研究尚不充分。为了解决这个问题，我们提出了 VIF-RAG，这是首个自动化、可扩展且可验证的合成管道，旨在提升 RAG 系统中的指令跟随对齐。该方法首先手动构建一组较小的原子指令，并发展组合规则以合成和验证复杂指令。随后，我们利用监督模型进行指令重写，同时生成代码以自动化验证指令质量。最后，我们整合指令和大量 RAG 数据样本，自动化生成超过 10 万条高质量的 VIF-RAG-QA 数据集。此外，为了填补 RAG 系统指令跟随自动评估的空白，我们推出了 FollowRAG 基准，包括约 3000 个测试样本，涵盖 22 类一般指令约束和四个知识密集型 QA 数据集。我们展示了 VIF-RAG 在多个通用指令约束下显著提升 LLM 性能，并就实现 RAG 系统中的 IF 对齐提供了实用见解。我们的代码和数据集已公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09584" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:40:06 GMT</pubDate>
</item>
<item>
<title>MMIE：评估大型视觉语言模型的交错多模态理解与生成的基准</title>
<link>https://arxiv.org/abs/2410.10139</link>
<guid>https://arxiv.org/abs/2410.10139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MMIE，一个大型知识密集型基准，用于评估大型视觉语言模型的交错多模态理解与生成能力。</p><br /><br /><p><strong>摘要：</strong> 在交错多模态理解与生成（Interleaved Multimodal Comprehension and Generation）的研究领域，尽管已有显著进展，但评估这一能力仍显不足。现有基准在数据规模、范围和评估深度上存在局限，评估指标往往成本高、偏见明显，缺乏实用性的可靠性。为了解决这些挑战，我们提出MMIE，这是一套大型知识密集型基准，旨在评估大型视觉语言模型（LVLMs）在交错多模态理解与生成方面的能力。MMIE包含20K精心策划的多模态查询，涵盖3个类别、12个领域和102个子领域，包括数学、编程、物理、文学、健康和艺术等。它支持交错的输入与输出，提供多项选择和开放式问题格式，以评估不同的能力。此外，我们提出了一种可靠的自动评估指标，利用经过人类标注数据和系统评估标准微调的评分模型，旨在减少偏见，提高评估准确性。大量实验展示了我们的基准和指标在全面评估交错LVLMs方面的有效性。具体而言，我们评估了八个LVLMs，结果表明，甚至是最好的模型也还有显著改进的空间，多数仅取得中等结果。我们相信MMIE将推动交错LVLMs的发展。我们将公开发布我们的基准和代码，网址为https://mmie-bench.github.io/。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.10139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:30:53 GMT</pubDate>
</item>
<item>
<title>针对大型语言模型的数学推理能力的奥林匹亚级基准测试</title>
<link>https://arxiv.org/abs/2410.07985</link>
<guid>https://arxiv.org/abs/2410.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个专门评估大型语言模型奥林匹亚级数学推理能力的基准，包含4428个高难度数学问题。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在数学推理能力方面取得显著突破，现有基准如GSM8K和MATH在模型解决这些问题时精确度已达到94.8%，这表明这些基准已无法有效挑战模型。为此，我们提出了一个全面且复杂的基准，专门设计用于评估LLMs在奥林匹亚级数学推理方面的能力。与现有的奥林匹亚相关基准不同，我们的数据集专注于数学，包含4428个经过严格人类标注的竞赛级问题。这些问题被细致地分类为33个子领域，并跨越10个不同的难度等级，使得我们能够全面评估模型在奥林匹亚数学推理表现方面的能力。此外，我们还基于这一基准进行了深入分析。实验结果显示，即使是最先进的模型，如OpenAI o1-mini和OpenAI o1-preview，在处理高度挑战性的奥林匹亚级问题时仍面临显著困难，其准确率分别为60.54%和52.55%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Oct 2024 00:06:51 GMT</pubDate>
</item>
<item>
<title>基于计划去噪的离散扩散框架DDPD</title>
<link>https://arxiv.org/abs/2410.06264</link>
<guid>https://arxiv.org/abs/2410.06264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了DDPD框架，通过规划与去噪结合，实现了更高效的生成过程。</p><br /><br /><p><strong>摘要：</strong> 离散扩散方法在标准基准上取得了最佳性能，接近或超越了自回归模型。本文介绍了一种新颖的离散扩散框架——计划去噪的离散扩散（DDPD），它将生成过程分为两个模型：规划者和去噪器。在推理时，规划者通过识别最需要去噪的位置（包括初始损坏位置和需要进一步细化的位置）来选择下一个去噪位置。这种计划与去噪的方法使得在生成过程中能够更高效地重建，通过迭代识别和以最佳顺序去噪来处理损坏。DDPD在语言建模基准上表现优于传统的仅有去噪器的掩蔽扩散方法，在text8、OpenWebText和ImageNet 256x256的基于令牌的生成任务中取得了优越结果。值得注意的是，在语言建模中，DDPD显著缩小了扩散和自回归方法在生成困惑度上的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 20:35:31 GMT</pubDate>
</item>
<item>
<title>Synth-SONAR：基于扩散模型与GPT提示的声纳图像合成框架</title>
<link>https://arxiv.org/abs/2410.08612</link>
<guid>https://arxiv.org/abs/2410.08612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Synth-SONAR框架，利用扩散模型和GPT提示生成高质量声纳图像，提升数据多样性和现实感。</p><br /><br /><p><strong>摘要：</strong> 声纳图像合成对水下探索、海洋生物学和国防等应用至关重要。然而，传统方法往往依赖昂贵且大量的数据采集，影响数据质量和多样性。为了解决这些问题，本研究提出了一种新的声纳图像合成框架，Synth-SONAR，结合了扩散模型和GPT提示。Synth-SONAR的创新点主要体现在三个方面：首先，利用基于生成的AI风格注入技术和公开可用的真实/模拟数据，生成最大规模的声纳数据集。其次，构建了一个双重文本条件的声纳扩散模型层次，可以合成高质量和多样性的粗略与精细声纳图像。第三，采用高级（粗略）和低级（详细）文本基础的声纳生成方法，有效利用视觉语言模型（VLMs）和GPT提示中的高级语义信息。在推理过程中，该方法从文本提示中生成多样化且真实的声纳图像，首次将GPT提示应用于声纳图像合成。Synth-SONAR在生产高质量合成声纳数据集方面实现了最新的领先成绩，显著提升了数据的多样性和真实感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 18:14:02 GMT</pubDate>
</item>
<item>
<title>GenARM：一种有效的自回归奖励模型用于无重训练的大型语言模型对齐</title>
<link>https://arxiv.org/abs/2410.08193</link>
<guid>https://arxiv.org/abs/2410.08193</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenARM通过自回归奖励模型实现有效的无训练对齐，支持多目标和弱到强的指导。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）展现了令人印象深刻的能力，但需要与人类偏好进行谨慎对齐。传统的训练时方法使用人类偏好数据集对LLMs进行微调，成本高昂且难以处理多样化用户偏好。而测试时对齐方法则通过奖励模型（RMs）指导冻结的LLMs，避免了重新培训的需求。目前的测试时方法依赖于轨迹级奖励模型，旨在评估完整响应，这在需要从部分响应计算下一个标记奖励的自回归文本生成中显得不够合适。为此，我们提出了GenARM，这一测试时对齐方法利用自回归奖励模型——一种新型的奖励参数化设计，能够预测下一个标记的奖励，从而促进高效且有效的自回归生成。我们从理论上证明，这种参数化可以在KL正则化的强化学习框架内可证明地引导冻结LLMs朝向由传统RMs能够实现的任何分布。实验结果表明，GenARM显著优于现有的测试时对齐基准，并且与训练时方法的性能相匹配。此外，GenARM支持高效的弱到强指导，使得较大的LLMs能够与较小的RMs进行对齐，而无需承担训练大模型的高成本。同时，GenARM还支持多目标对齐，实时调整偏好维度之间的权衡，满足多样化用户偏好的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08193" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 17:04:55 GMT</pubDate>
</item>
<item>
<title>利用简单分层方法提高大型语言模型生成的多样性</title>
<link>https://arxiv.org/abs/2410.09038</link>
<guid>https://arxiv.org/abs/2410.09038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SimpleStrat方法，利用语言模型对输出进行分层，从而提高生成内容的多样性和质量。</p><br /><br /><p><strong>摘要：</strong> 在生成多样化响应方面，大型语言模型（LLMs）扮演着重要角色，尤其在规划搜索和合成数据生成等应用中。传统方法依赖于提高温度来增强多样性，然而，研究表明这不仅导致个体生成质量下降，而且其效果依赖于模型预测概率与真实答案分布的相似性。为了克服这一问题，本文提出了一种新的方法——SimpleStrat，通过语言模型自身对生成空间进行分层，在推理时随机选择一个层级并从中抽取样本。此外，为了衡量生成结果的多样性，本文引入了CoverageQA数据集，该数据集由多个同样合理的答案组成的模糊问题构成，利用Kullback-Leibler散度（KL散度）来评估输出分布与有效答案的均匀分布之间的差异。在对比评估中，使用SimpleStrat方法在召回率方面比GPT-4o提高了0.05，而相较于Llama 3则在KL散度上减少了0.36，展现了该方法在提高生成多样性和质量上的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 16:35:17 GMT</pubDate>
</item>
<item>
<title>MiRAGeNews数据集：对抗AI生成假新闻的多模态检测</title>
<link>https://arxiv.org/abs/2410.09045</link>
<guid>https://arxiv.org/abs/2410.09045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究推出MiRAGeNews数据集，旨在检测AI生成的假新闻，提高内容真实性。使用此数据集训练的多模态检测器效果显著。</p><br /><br /><p><strong>摘要：</strong> 随着虚假新闻内容的泛滥和AI生成图像技术的迅速发展，AI生成的假新闻内容变得更加危险。为此，我们提出了MiRAGeNews数据集，该数据集包含12,500对高质量的真实和AI生成的图像-标题配对，使用了最先进的生成模型。我们的研究表明，这一数据集对人类的检测能力构成了显著挑战（F-1为60%），同时对当前最先进的多模态大语言模型的检测性能也很低（F-1小于24%）。基于该数据集，我们训练了一种多模态检测器（MiRAGe），在来自域外图像生成器和新闻发布者的图像-标题配对任务上，F-1提高了5.1%，超越了现有的基线方法。我们将代码和数据公开发布，以助力后续对AI生成内容的检测研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 13:41:24 GMT</pubDate>
</item>
<item>
<title>I-Max框架：提升文本到图像RFTs的分辨率潜力</title>
<link>https://arxiv.org/abs/2410.07536</link>
<guid>https://arxiv.org/abs/2410.07536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍I-Max框架，利用新的Projected Flow策略和推理工具，提升RFTs在分辨率扩展中的稳定性和图像细节质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Rectified Flow Transformers（RFTs）在扩展生成分辨率方面的挑战，尤其是数据质量和训练成本的问题。针对现有的无调优分辨率外推方法，往往在生成稳定性方面存在不足，我们提出了I-Max框架。I-Max框架包括两大特性：首先是最新的Projected Flow策略，旨在实现更稳定的分辨率外推；其次是一个先进的推理工具包，可帮助模型知识向更高分辨率的泛化。通过在Lumina-Next-2K和Flux.1-dev数据集上的实验，结果表明I-Max能够有效提升分辨率外推的稳定性，并带来图像细节的增强和伪影的修正，证实了无调优分辨率外推的实际应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 12:22:47 GMT</pubDate>
</item>
<item>
<title>ZeroComp：一种有效的零样本3D物体合成方法</title>
<link>https://arxiv.org/abs/2410.08168</link>
<guid>https://arxiv.org/abs/2410.08168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroComp通过使用ControlNet和Stable Diffusion实现无配对图像的3D物体合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ZeroComp的有效零样本3D物体合成方法，该方法在训练过程中不需要配对的复合场景图像。ZeroComp利用ControlNet对内在图像进行条件处理，并结合Stable Diffusion模型以利用其场景先验，这两者共同运作为一个高效的渲染引擎。在训练阶段，ZeroComp使用基于几何、反射率和遮蔽的内在图像，完全不依赖于含有复合物体的场景的图像配对。训练完成后，ZeroComp能够无缝地将虚拟3D物体集成到场景中，并调整光照，使复合效果更加真实。为了验证ZeroComp的效果，我们开发了一个高质量评估数据集，结果显示其在定量和人类感知基准测试中均优于依赖显式照明估计和生成技术的方法。此外，ZeroComp在真实和户外图像合成上也展现出潜力，即使其训练数据仅限于合成的室内数据，依然能够有效实现图像合成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 11:29:03 GMT</pubDate>
</item>
<item>
<title>Mentor-KD：多步推理能力的知识蒸馏方法</title>
<link>https://arxiv.org/abs/2410.09037</link>
<guid>https://arxiv.org/abs/2410.09037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Mentor-KD方法，通过中介模型增强CoT标注与软标签提供，以有效蒸馏LLMs的推理能力。 </p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）在复杂任务中表现出的卓越性能，Chain-of-Thought（CoT）提示法已成为主要研究方向。近年来，有研究提出了一种推理蒸馏（Reasoning Distillation）的方法，通过对LLM教师生成的多步推理进行微调，以转移其推理能力。然而，现有方法在蒸馏集中存在两大挑战：1）数据质量不足，2）软标签提供不充分。为了解决这些问题，本文提出了Mentor-KD方法，旨在有效蒸馏LLMs的多步推理能力至小型语言模型。具体而言，我们利用一个中介模型，进行特定任务的微调，以增强CoT注释的数量和质量，并在推理蒸馏过程中为学生模型提供软标签。通过广泛的实验，我们验证了Mentor-KD在各种模型和复杂推理任务中的有效性，显示出该方法在提升推理能力方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 09:26:42 GMT</pubDate>
</item>
<item>
<title>SAE Match：基于稀疏自编码器的神经网络层间特征对齐</title>
<link>https://arxiv.org/abs/2410.07656</link>
<guid>https://arxiv.org/abs/2410.07656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SAE Match，通过折叠参数最小化均方误差，实现神经网络层间特征的有效对齐。</p><br /><br /><p><strong>摘要：</strong> 理解深度神经网络中特征在不同层之间的演变是机械可解释性中的一个基本挑战，尤其是由于多义性和特征叠加使得这一过程更加复杂。尽管稀疏自编码器（SAEs）已被用来从单个层中提取可解释特征，但不同层之间的特征对齐仍然是一个开放的问题。在本文中，我们提出了一种新颖的数据无关的方法SAE Match，用于对齐神经网络不同层的SAE特征。我们的方法通过最小化SAEs的折叠参数之间的均方误差来匹配特征，这一技术在编码器和解码器权重中引入激活阈值，以解决特征尺度差异的问题。通过对Gemma 2语言模型的广泛实验，我们证明了我们的方法能够有效捕捉特征在不同层间的演变，从而提高特征匹配质量。我们还展示了特征在多个层中持续存在，并且我们的方法能够近似不同层之间的隐藏状态。我们的工作推动了对神经网络中特征动态的理解，并为机械可解释性研究提供了一种新的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 08:07:48 GMT</pubDate>
</item>
<item>
<title>DA-Code：针对代理的数据科学任务的代码生成基准</title>
<link>https://arxiv.org/abs/2410.07331</link>
<guid>https://arxiv.org/abs/2410.07331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DA-Code是一个评估LLMs在数据科学任务上的代码生成能力的基准，涵盖复杂数据处理的实际任务。</p><br /><br /><p><strong>摘要：</strong> 我们介绍了DA-Code，一个专门设计用来评估LLMs在代理数据科学任务中的代码生成能力的基准。DA-Code的设计包含三个核心要素。首先，任务具有内在挑战性，要求高级编码技能，特别是在上下文处理和计划方面。其次，DA-Code中的所有示例均基于真实且多样的数据，涵盖各种复杂的数据整理和分析任务。最后，为了完成这些任务，模型必须使用复杂的数据科学编程语言，以进行精细化的数据处理并得出结论。我们在一个可控和可执行的环境中搭建了这个基准，确保它与现实世界的数据分析场景相一致，并具备可扩展性。评估套件经过注释者的精心设计，以确保评估的准确性和稳健性。我们开发了DA-Agent基准线并进行了实验，结果表明，尽管基线在现有框架中表现更好，但当前最佳的LLMs准确率仅为30.5%，仍大有提升空间。我们将在https://da-code-bench.github.io发布我们的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 07:27:37 GMT</pubDate>
</item>
<item>
<title>多智能体协作数据选择机制在大型语言模型预训练中的应用</title>
<link>https://arxiv.org/abs/2410.08102</link>
<guid>https://arxiv.org/abs/2410.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多智能体协作框架，以提高大型语言模型预训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）预训练的有效数据选择至关重要。虽然已有多种方法致力于提升数据效率，但很少有研究关注这些方法之间的固有冲突，以实现最优数据选择。为了解决这一问题，我们提出了一种新颖的多智能体协作数据选择机制。在该框架中，每种数据选择方法作为独立代理工作，设计了一个代理控制台，以动态整合所有代理在整个LLM训练过程中的信息。我们通过广泛的实证研究评估了我们的多智能体框架。实验结果表明，我们的方法显著提高了数据效率，加速了LLM训练的收敛，并在多个语言模型基准上实现了平均10.5%的性能提升，相比于最先进的方法。有了这样的机制，数据选择的冲突问题得以更好地解决，进一步推动了大型语言模型的研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:56:36 GMT</pubDate>
</item>
<item>
<title>StructRAG：基于结构化知识增强大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2410.08815</link>
<guid>https://arxiv.org/abs/2410.08815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出StructRAG框架，通过结构化知识增强LLMs在知识密集任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval-augmented generation (RAG) 是一种增强大型语言模型 (LLMs) 在知识密集任务中表现的关键方法，但现有RAG方法在处理知识密集推理任务时面临挑战。这是因为相关信息往往散布不均，使得现有方法难以准确识别关键信息并进行全局推理。为了解决这个问题，本文提出了一种新框架StructRAG，它基于人类在处理知识密集推理任务时将原始信息转换为多种结构化知识的认知理论，能够识别适合特定任务的最佳结构类型，将原始文档重构为该结构格式，并基于结果结构推理出答案。在各种知识密集任务上的广泛实验表明，StructRAG在复杂情境中表现出色，达到了最先进的性能，展示了其在复杂现实世界应用中增强LLMs作为有效解决方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:14:19 GMT</pubDate>
</item>
<item>
<title>通过KV预测减少变换器模型的首次输出时间</title>
<link>https://arxiv.org/abs/2410.08391</link>
<guid>https://arxiv.org/abs/2410.08391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出KV预测方法，通过辅助模型生成KV缓存，提高变换器模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 推理过程中的prompt处理步骤通常会消耗大量时间，尤其是在边缘设备上使用亿级参数模型时。为了解决首次输出时间（TTFT）过长的问题，我们提出了一种名为KV预测的新方法。通过使用一个小型辅助模型处理prompt，生成近似的KV缓存，从而减少基模型在自回归生成时对计算资源的需求。该方法在维持相对准确性的同时显著提高了效率，相比基线方法，KV预测在TriviaQA上准确度提升了15%-50%，在HumanEval的Python代码补全任务中提升了最多30%。此外，我们在Apple M2 Pro CPU上进行了基准测试，验证了FLOPs的改善直接转化为TTFT的速度提升。本研究展示了在多种TTFT计算预算下的有效性，为变换器模型的应用提供了新的可能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:41:21 GMT</pubDate>
</item>
<item>
<title>VITask：提升大型视觉语言模型任务适应性的框架</title>
<link>https://arxiv.org/abs/2410.06456</link>
<guid>https://arxiv.org/abs/2410.06456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VITask通过集成任务特定模型，提升视觉语言模型的任务适应能力，展示在医学诊断中的有效性。</p><br /><br /><p><strong>摘要：</strong> 该研究提出了一种名为VITask的创新框架，旨在提升大型视觉语言模型（VLMs）在特定任务中的适应能力。由于预训练与微调之间的领域差异，传统VLMs在任务特定应用中往往表现不佳。VITask通过引入任务特定模型（TSMs）并实施三种关键策略，即示例提示（EP）、响应分布对齐（RDA）和对比响应调整（CRT），有效改善任务特定表现。EP使TSM特征能够引导VLMs，而RDA则在推理过程中通过借鉴示例提示模型的经验，使VLMs能够适应而不需TSMs。CRT进一步优化了正确图像-响应对的排名，从而降低了生成不当响应的风险。在12个医学诊断数据集和9种成像模式下的实验结果表明，VITask的表现超越了传统的指令调优VLMs和TSMs，证明了其有效整合两种模型互补特性的能力。此外，VITask在TSM集成的灵活性和对不完整指令的鲁棒性方面也展现了实际优势，成为任务特定VLM微调的多功能和高效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:10:18 GMT</pubDate>
</item>
<item>
<title>增强大型语言模型的长度控制与复制粘贴能力</title>
<link>https://arxiv.org/abs/2410.07035</link>
<guid>https://arxiv.org/abs/2410.07035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的方法提升大型语言模型的长度控制与复制粘贴能力，显著改进性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在角色扮演、创意写作、数学推理和编码等领域显示出强大的能力，然而在长度控制方面仍然面临挑战。这一问题主要源于模型在生成文本时缺乏位置意识，导致难以遵循特定的长度限制。为了解决这一问题，我们提出了两种新方法：PositionID Prompting和PositionID Fine-Tuning，使模型能够持续监控和管理生成文本的长度。此外，我们还引入了PositionID CP Prompting，使LLMs能够准确地执行复制和粘贴操作。为了评估长度控制和复制粘贴功能，我们开发了两个基准测试。实验结果表明，我们的方法显著提高了模型对长度限制的遵守程度和复制粘贴的准确性，同时不牺牲响应质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:03:18 GMT</pubDate>
</item>
<item>
<title>Meissonic：高效的非自回归文本到图像建模</title>
<link>https://arxiv.org/abs/2410.08261</link>
<guid>https://arxiv.org/abs/2410.08261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Meissonic 提升非自回归图像建模，生成高质量、高分辨率图像。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成领域，扩散模型如 Stable Diffusion 取得了重要进展，但其和自回归语言模型的根本不同，使得统一语言-视觉模型的开发面临挑战。针对这一问题，我们提出了 Meissonic，它将非自回归的遮蔽图像建模（MIM）文本到图像生成技术提升至与先进扩散模型（如 SDXL）相当的水平。通过综合采用一系列架构创新、先进的位置信息编码策略以及优化的采样条件，Meissonic 显著提高了 MIM 的性能和效率。同时，我们利用高质量的训练数据，集成了基于人类偏好评分的信息微调条件，并采用特征压缩层来进一步提升图像的真实感和分辨率。我们的模型不仅在生成高质量、高分辨率图像方面与现有模型如 SDXL 相匹配，且往往超过其性能。大量实验验证了 Meissonic 的能力，展示了它作为文本到图像合成新标准的潜力。我们发布了一个能够生成 1024x1024 分辨率图像的模型检查点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 01:19:46 GMT</pubDate>
</item>
<item>
<title>Baichuan-Omni：首个开源7B多模态大语言模型</title>
<link>https://arxiv.org/abs/2410.08565</link>
<guid>https://arxiv.org/abs/2410.08565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Baichuan-Omni，这是一个开源的7B多模态大语言模型，具备图像、视频、音频与文本的处理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们介绍了Baichuan-Omni，首个开源的7B多模态大语言模型（MLLM），它能够同时处理和分析图像、视频、音频和文本等多种信息模式，以提供先进的多模态交互体验及出色的性能。我们提出了一种有效的多模态训练方案，从7B模型开始，经过两个阶段的多模态对齐和多任务微调，以处理音频、图像、视频和文本等多种模式。这一方法使语言模型具备有效处理视觉和音频数据的能力。Baichuan-Omni在多个全模态和多模态基准测试中表现出色。我们的目标是希望这一贡献为开放源代码社区提供一个具有竞争力的基准，促进多模态理解和实时交互的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:29:48 GMT</pubDate>
</item>
<item>
<title>基于语义得分蒸馏采样的复杂3D内容生成研究</title>
<link>https://arxiv.org/abs/2410.09009</link>
<guid>https://arxiv.org/abs/2410.09009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的SDS方法，SemanticSDS，通过语义嵌入提高复杂3D场景生成的表现力和准确性。</p><br /><br /><p><strong>摘要：</strong> 生成高质量的3D资产仍然是计算机图形学和视觉研究中的关键挑战。由于3D数据的稀缺，当前的前沿方法利用预训练的2D扩散先验，通过得分蒸馏采样（SDS）进行优化。尽管技术有所进步，然而，制作複杂3D场景及多个物体或复杂交互仍然困难。为了解决这一问题，近期方法逐渐引入了框或布局引导，但这些布局引导的组合方法通常在提供细粒度控制方面表现不佳，较为粗糙且缺乏表现力。为此，我们提出了一种新的得分蒸馏采样方法——语义得分蒸馏采样（SemanticSDS），旨在有效提高组合文本到3D生成的表现力与准确性。我们的做法整合了新的语义嵌入，这些嵌入在不同渲染视图间保持一致，并且能够清晰地区分不同的物体和部分。通过将这些嵌入转化为一个语义图，我们引导了区域特定的SDS过程，从而实现精确的优化与组合生成。通过利用显式的语义引导，我们的方法解锁了现有预训练扩散模型的组合能力，在复杂物体和场景的3D内容生成方面达到了优秀的质量。实验结果表明，我们的SemanticSDS框架在生成尖端复杂3D内容方面非常有效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:24:03 GMT</pubDate>
</item>
<item>
<title>SuperCorrect：一种改进小型模型推理能力的两阶段框架</title>
<link>https://arxiv.org/abs/2410.09008</link>
<guid>https://arxiv.org/abs/2410.09008</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出SuperCorrect框架，通过大模型的监督，提高小模型的推理与自我纠错能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）如GPT-4和PaLM在推理任务中表现出显著的改进。然而，较小的模型例如Llama-3-8B和DeepSeekMath-Base在复杂数学推理中仍面临挑战，尤其在独立检测和纠正推理错误方面。为了解决这一问题，我们提出了一个名为SuperCorrect的创新两阶段框架，利用大型教师模型来监督和纠正小型学生模型的推理和反思过程。在第一阶段，我们从教师模型中提取层次化的高层次和详细思维模板，以指导学生模型更好地引导细致的推理思考。在第二阶段，我们引入跨模型的协作直接偏好优化（DPO）方法，以增强学生模型的自我纠错能力，学生模型在训练过程中跟随教师的纠正轨迹。这个跨模型DPO方法教会学生模型有效定位和解决错误思维，突破思维瓶颈，获取新的技能和知识，从而应对挑战性的问题。大量实验证明，SuperCorrect优于以前的方法，尤其是我们的SuperCorrect-7B模型在MATH和GSM8K基准上分别比DeepSeekMath-7B提升了7.8%/5.3%和比Qwen2.5-Math-7B提升了15.1%/6.3%，在所有7B模型中达到了新的SOTA性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09008" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:22:21 GMT</pubDate>
</item>
<item>
<title>EvolveDirector：基于公共资源训练文本到图像生成模型的框架</title>
<link>https://arxiv.org/abs/2410.07133</link>
<guid>https://arxiv.org/abs/2410.07133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvolveDirector框架利用公共API生成的数据训练文本到图像生成模型，显著降低数据需求，实现优越的生成能力。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的快速进步，文本到图像生成表现出惊人的内容创作能力。然而，大多数先进模型依赖于专有数据，并提供有限的开放API，限制了其在下游任务中的应用。为探讨利用公共资源训练一个可与先进模型媲美的文本到图像生成模型的可行性，我们提出了EvolveDirector框架。该框架通过公共API与先进模型交互，获得文本-图像数据对以训练基础模型。实验显示，尽管通过生成数据训练的模型能接近先进模型的生成能力，但需要大量的样本（1000万个或更多），这会带来高昂的时间和计算资源成本，尤其是API调用费用。为了解决这一问题，EvolveDirector利用预训练的大型视觉-语言模型（VLM）来指导基础模型的演化，持续评估基础模型，并通过判别、扩展、删除和变异等操作动态更新和优化训练数据集。实验结果表明，这种方法显著减少了所需的数据量。EvolveDirector还能够在面对多个高级模型时选择其生成的最佳样本，以学习强大且均衡的能力。最终训练出的模型Edgen显示出优于这些先进模型的效果。代码和模型权重可在https://github.com/showlab/EvolveDirector获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:17:02 GMT</pubDate>
</item>
<item>
<title>利用加速偏好优化加快人类反馈下的强化学习</title>
<link>https://arxiv.org/abs/2410.06293</link>
<guid>https://arxiv.org/abs/2410.06293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种加速偏好优化框架，结合动量技术加速大语言模型的对齐。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的人类反馈（RLHF）正在成为对齐大型语言模型（LLM）的重要工具。直接偏好优化（DPO）是一种流行的方法，它将RLHF视为一个政策优化问题，而不显式估计奖励函数。此方法克服了两步法所面临的稳定性和效率问题，并展示了可以通过动量技术加速RLHF的潜力。本文首次表明，迭代偏好优化方法可以视为一种近端点方法。基于此观察，提出了一种通用的加速偏好优化（APO）框架，统一了许多现有的偏好优化算法，并采用Nesterov动量技术加速LLM的对齐过程。理论上，APO能够实现比标准迭代偏好优化方法（包括DPO和自我博弈偏好优化（SPPO））更快的收敛速度。实证结果显示，APO在AlpacaEval 2.0基准上相较于DPO、迭代DPO及其他强基线表现出更强的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 23:06:15 GMT</pubDate>
</item>
<item>
<title>Data Advisor：提升数据生成质量的增强LLM方法</title>
<link>https://arxiv.org/abs/2410.05269</link>
<guid>https://arxiv.org/abs/2410.05269</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Data Advisor，优化LLM生成数据，提高数据质量与覆盖率，特别针对安全对齐问题。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型 (LLM) 对齐中，数据是至关重要的元素。尽管近期研究探讨了使用LLM进行高效数据收集，但LLM生成的数据常常存在质量问题，如缺乏代表性、某些方面的缺失以及低质量数据点。为了解决这些问题，我们提出了Data Advisor，这是一种增强的LLM方法，用于生成符合特定特征的数据集。Data Advisor基于一组预定义原则，监控生成数据的状态，识别当前数据集的弱点，并据此建议下一轮数据生成的策略。Data Advisor可轻松集成到现有的数据生成方法中，以提升数据的质量和覆盖率。在对三种代表性LLM（即Mistral、Llama2和Falcon）进行的安全对齐实验中，Data Advisor显示了提升模型安全性与应对各种细粒度安全问题的有效性，同时不牺牲模型的实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05269" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 19:05:54 GMT</pubDate>
</item>
<item>
<title>基于神经符号学习的LLM世界模型对齐与探索</title>
<link>https://arxiv.org/abs/2410.07484</link>
<guid>https://arxiv.org/abs/2410.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WALL-E代理，通过规则学习对LLM进行环境对齐，显著提升在Minecraft和ALFWorld等开放世界中的探索效率与成功率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）作为模型基础代理的世界模型的潜力，尽管存在LLMs的先验知识与特定环境动态之间的差距，我们的研究表明，这些差距可以通过对LLM与其部署环境进行对齐来弥补。这种“世界对齐”可以通过在LLMs上进行规则学习高效实现。由于LLMs具有丰富的先验知识，通常只需少量额外规则即可使LLM预测与实定环境动态一致。为此，本文提出了一种神经符号方法，通过比较代理探索的轨迹与世界模型预测，诱导、更新和修剪规则，从而以无梯度的方式学习这些规则。最终的世界模型由LLM及学习的规则组成。我们的具身LLM代理“WALL-E”基于模型预测控制（MPC）构建，通过基于精确世界模型优化前瞻性动作，MPC显著提高了探索和学习效率。与现有LLM代理相比，WALL-E的推理只需少量主规则，而不是将冗长的缓冲轨迹纳入LLM输入。在Minecraft和ALFWorld的开放世界挑战中，WALL-E相比现有方法有更高的成功率，且重规划时间和使用的tokens更少。在Minecraft中，WALL-E的成功率超过基线15-30%，重规划轮次减少8-20轮，使用的tokens仅为60-80%。在ALFWorld中，其成功率在仅6次迭代后飙升至95%的新纪录。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 17:14:49 GMT</pubDate>
</item>
<item>
<title>向量-内联学习：扩展大型语言模型的能力</title>
<link>https://arxiv.org/abs/2410.05629</link>
<guid>https://arxiv.org/abs/2410.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型如何通过轻量级投影器处理连续向量，实现向量-内联学习能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在文本数据的上下文学习（ICL）能力方面表现出色。本文探索了这些能力是否可以扩展到来自不同领域的连续向量，这些向量是通过黑箱预训练编码器获得的。我们通过轻量级投影器将输入数据与LLM的嵌入空间对齐，发现LLMs能够有效地处理和学习这些投影向量，称之为向量-内联学习（Vector-ICL）。尤其值得注意的是，使用通用语言建模目标对投影器进行预训练可以实现Vector-ICL，而任务特定的微调则进一步提升了性能。我们在各种任务和模态上进行了实验，包括文本重建、数值函数回归、文本分类、摘要生成、分子描述、时间序列分类、图分类和fMRI解码。结果表明，Vector-ICL在许多情况下超越了少量样本的ICL和领域特定模型或微调。我们还进行了分析和案例研究，表明LLMs在处理向量表示方面的潜力超越了传统的基于标记的范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 16:04:02 GMT</pubDate>
</item>
<item>
<title>Zebra：一种新型生成自回归变换器用于解决时间依赖性参数偏微分方程</title>
<link>https://arxiv.org/abs/2410.03437</link>
<guid>https://arxiv.org/abs/2410.03437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra 是一款不需梯度适应的新型变换器，能够灵活应对各种参数 PDE，展示卓越的性能。</p><br /><br /><p><strong>摘要：</strong> 解决时间依赖性参数偏微分方程（PDEs）是一项具有挑战性的任务，因为模型必须适应系数、强迫项和边界条件等参数的变化。数据驱动的神经网络求解器通常依赖于从PDE参数分布中采样的数据进行训练，以期在新的实例上实现泛化，或依靠基于梯度的适应和元学习，隐式编码来自观察的数据动态。然而，这通常会增加推理复杂性。受到大型语言模型（LLMs）在上下文学习能力的启发，我们提出了Zebra，一种新型生成自回归变换器，旨在解决参数PDEs，而无需在推理时进行梯度适应。Zebra通过在预训练和推理过程中利用上下文信息，能够动态适应新任务，条件依赖于包含上下文轨迹或先前状态的输入序列。这种方法使得Zebra能够灵活处理任意大小的上下文输入，并通过采样多个解轨迹支持不确定性量化。我们在多种挑战性的PDE场景中评估了Zebra，展示了其适应性、鲁棒性以及相较于现有方法的优越性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 15:11:34 GMT</pubDate>
</item>
<item>
<title>DART：一种新型的非马尔可夫扩散模型</title>
<link>https://arxiv.org/abs/2410.08159</link>
<guid>https://arxiv.org/abs/2410.08159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DART提出了一种统一自回归与扩散框架的图像生成模型，提升了训练与推理效率。</p><br /><br /><p><strong>摘要：</strong> 扩散模型已成为视觉生成的主流方法，但其基于马尔可夫过程的特性限制了模型充分利用生成轨迹的能力，导致在训练和推理时效率低下。为此，本文提出了一种新的模型DART，基于变换器架构，将自回归（AR）和扩散过程结合在一个非马尔可夫的框架内。DART通过空间和光谱的方式迭代去噪图像块，采用与标准语言模型相同的架构。DART不依赖于图像量化，增强了图像建模的有效性，保持了灵活性。同时，DART能够在一个统一模型中无缝地训练文本和图像数据。我们的实验结果表明，DART在类别条件和文本到图像生成任务上表现出色，提供了一种可扩展、高效的替代方案。通过这一统一框架，DART为高质量图像合成设立了新的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:56:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型的任务超叠现象及其内在机制研究</title>
<link>https://arxiv.org/abs/2410.05603</link>
<guid>https://arxiv.org/abs/2410.05603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在单次推理过程中同时执行多个不同任务的能力，称为任务超叠现象。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在上下文学习（ICL）方面的一个惊人现象：模型能够在单次推理调用中同时执行多个计算上独立的ICL任务，这一能力被称为‘任务超叠’。我们提供了针对不同LLM家族和规模的实证证据，表明即使在模型训练时仅学习一个任务，也能出现这一现象。此外，我们提供了理论解释，认为这一能力在变换器的表达能力范围内。我们还探讨了LLMs如何在超叠过程中内部组合任务向量的机制。研究表明，较大的模型能够并行解决更多ICL任务，并更好地校准其输出分布。这些发现为LLMs潜在能力提供了新的见解，进一步支持了‘LLMs作为模拟器的超叠’的观点，并引发了关于实现同时任务执行的机制的思考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:27:35 GMT</pubDate>
</item>
<item>
<title>自动化基准测试中的作弊现象及其影响</title>
<link>https://arxiv.org/abs/2410.07137</link>
<guid>https://arxiv.org/abs/2410.07137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，常量输出模型可在自动化基准测试中作弊，表现异常优异，呼吁开发反作弊机制。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在自动化大语言模型（LLM）基准测试中存在的作弊问题。研究表明，即使是一个始终输出固定响应（与输入无关）的“空模型”，也能在多个基准测试上取得高分，如在 AlpacaEval 2.0 上获得 86.5% 的 LC 胜率，在 Arena-Hard-Auto 上获得 83.0 分，而在 MT-Bench 上获得 9.55 分。这些作弊输出展示出了可转移性，因为假设测试指令是私有且不可访问的。尽管本研究主要作为概念验证，但某些对手可以利用 LLM 生成更不易察觉的作弊回复，从而从高胜率和推广影响中不道德地获利。因此，本文呼吁开发可靠的反作弊机制，以确保自动化基准测试的可信度及其在评估语言模型时的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:28:45 GMT</pubDate>
</item>
<item>
<title>LPZero：自动设计零成本代理的框架</title>
<link>https://arxiv.org/abs/2410.04808</link>
<guid>https://arxiv.org/abs/2410.04808</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPZero是一个框架，能够自动设计零成本（ZC）代理，提升NLP任务中的性能和排名一致性。</p><br /><br /><p><strong>摘要：</strong> 在神经架构搜索（NAS）面临的大量计算开销的背景下，零成本（ZC）代理作为一种有前途的方法逐渐受到关注。然而，现有的ZC代理往往依赖于专家知识，且存在显著的试错成本，尤其在自然语言处理（NLP）任务中，许多ZC代理无法超越简单基线表现。为了解决这些问题，我们提出了一个新颖的框架LPZero，这是首个能够为各种任务自动设计ZC代理的系统，且其排名一致性优于人工设计的代理。具体而言，我们将ZC代理建模为符号方程，并整合了一个统一的代理搜索空间，该空间涵盖了由预定义的数学符号构成的现有ZC代理。LPZero利用遗传编程进行启发式搜索，以找到最佳的符号组合。同时，我们提出了一种基于规则的剪枝策略（RPS），该策略可以预先消除不太有前景的代理，从而降低代理降低性能的风险。通过对FlexiBERT、GPT-2和LLaMA-7B的广泛实验，LPZero在下游任务中的排名能力和性能均优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04808" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:09:02 GMT</pubDate>
</item>
<item>
<title>GLOV：利用大语言模型优化视觉语言模型的隐式优化方法</title>
<link>https://arxiv.org/abs/2410.06154</link>
<guid>https://arxiv.org/abs/2410.06154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GLOV方法，利用大语言模型作为隐式优化器提升视觉任务性能。</p><br /><br /><p><strong>摘要：</strong> 在这项研究中，我们提出了一种新颖的方法（GLOV），使大语言模型（LLMs）能够作为隐式优化器，提升视觉语言模型（VLMs）在下游视觉任务中的表现。通过对下游任务描述的元提示，GLOV查询出适合的VLM提示（如使用CLIP进行零-shot分类），并根据适应性函数获得的纯度度量对这些提示进行排名。在每个优化步骤中，排名后的提示及其准确性作为上下文示例被送入LLM，帮助LLM了解下游VLM所偏爱的文本提示类型。此外，我们在每个优化步骤中明确引导LLM生成过程，通过将来自上一步中正负解的嵌入差异向量添加至网络的中间层，从而在下一步的生成过程中引导LLM朝向下游VLM所偏好的语言类型。这一策略显著提升了下游视觉任务的表现。我们在16个多样化的数据集上全面评估了GLOV，使用两类VLM（即双编码器模型如CLIP和编码器-解码器模型如LLaVa），结果显示，所发现的解决方案能在识别性能上提升高达15.0%和57.5%（平均分别提升3.8%和21.6%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 07:34:00 GMT</pubDate>
</item>
<item>
<title>WorFBench：一个用于评估工作流生成能力的统一基准</title>
<link>https://arxiv.org/abs/2410.07869</link>
<guid>https://arxiv.org/abs/2410.07869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了WorFBench工作流生成基准及WorFEval评估协议，揭示LLM在序列和图规划间的能力差异。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在解决推理和规划任务方面取得了显著进展，其中将复杂问题分解为可执行工作流是关键步骤。现有的工作流评估框架存在局限，不能全面反映LLM的能力。为此，我们引入了WorFBench，这是一种统一的工作流生成基准，涵盖多样化场景和复杂图形工作流结构。同时，我们提出了WorFEval，一种系统的评估协议，利用子序列和子图匹配算法来准确量化LLM代理的工作流生成能力。通过对不同类型的LLMs进行综合评估，我们发现LLM代理在序列规划能力和图规划能力之间存在显著差距，即使是GPT-4，二者之间的差距约为15%。此外，我们训练了两个开源模型，并评估了它们在持出任务上的泛化能力。值得注意的是，生成的工作流能够提高下游任务的表现，使得推理过程所需时间减少，从而达到更优秀的性能。代码和数据集将发布于https://github.com/zjunlp/WorFBench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:48:20 GMT</pubDate>
</item>
<item>
<title>基于运动先验的变形3D高斯点云重建框架MotionGS</title>
<link>https://arxiv.org/abs/2410.07707</link>
<guid>https://arxiv.org/abs/2410.07707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了MotionGS，一个通过运动先验引导变形的3D高斯点云重建框架，显著提升动态场景重建效果。</p><br /><br /><p><strong>摘要：</strong> 动态场景重建在3D视觉领域一直是一个长期挑战。最近，3D高斯点云技术的兴起为这一问题提供了新的视角。尽管后续工作迅速将静态3D高斯扩展到动态场景，但往往缺乏对物体运动的明确约束，导致优化困难和性能下降。为了解决这些问题，本文提出了一个新颖的变形3D高斯点云框架——MotionGS，旨在探索明确的运动先验以引导3D高斯的变形。具体而言，我们首先引入了一种光流解耦模块，该模块将光流解耦为相机流和运动流，分别对应于相机移动和物体运动。然后，运动流可以有效约束3D高斯的变形，从而模拟动态物体的运动。此外，还提出了一种相机姿态优化模块，以交替优化3D高斯和相机姿态，减轻不准确相机姿态的影响。在单目动态场景中的广泛实验验证了MotionGS在定性和定量结果上都超越了最新的技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:17:59 GMT</pubDate>
</item>
<item>
<title>基于数学推理和代码生成的数学继续预训练方法</title>
<link>https://arxiv.org/abs/2410.08196</link>
<guid>https://arxiv.org/abs/2410.08196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的数学继续预训练方法，通过生成代码和推理步骤提升语言模型的数学能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的数学继续预训练方法，通过生成数学代码及其相应的推理步骤，提升大型语言模型的数学推理能力。最初，我们构建了一个高质量的数学继续预训练数据集，融合了数学相关的网络数据、使用数学包的代码、数学教科书以及合成数据。接着，我们通过提取LaTeX表达式、这些表达式所需的条件及其结果，生成推理步骤。基于提取的信息，我们生成了相应的代码，以准确捕捉数学推理过程。在每个推理步骤后附加生成的代码，形成了自然语言推理步骤与其对应代码的配对数据。将这些数据与原始数据集结合，形成了一个包含19.2B标记的高性能数学预训练语料库，命名为MathCode-Pile。使用该语料库训练几种流行的基础模型显著提升了它们的数学能力，最终形成了MathCoder2模型系列。为了确保透明性和易于重现，我们将所有数据处理和训练代码开源，代码可在https://github.com/mathllm/MathCoder2获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 04:16:58 GMT</pubDate>
</item>
<item>
<title>SFTMix: 基于Mixup的指令调优方法研究</title>
<link>https://arxiv.org/abs/2410.05248</link>
<guid>https://arxiv.org/abs/2410.05248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SFTMix，通过Mixup正则化提高指令调优性能，降低对高质量数据集的依赖。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）交互驱动任务的指令调优阶段，通常通过下一个标记预测（NTP）损失对指令-响应对进行训练。以往旨在提升指令调优表现的研究往往强调需要更高质量的监督微调（SFT）数据集，这通常伴随高昂的数据过滤成本或人工标注的劳动力。然而，这些方法未能充分利用数据集的内在特性，导致高计算和劳动成本，限制了可扩展性和性能提升。本文提出了SFTMix，一种新颖的策略，通过Mixup正则化提升指令调优性能，而无需精心策划的数据集。我们观察到LLMs在语义表示空间中的信心不均匀，认为不同信心水平的示例在指令调优过程中应发挥不同角色。在此基础上，SFTMix利用训练动态识别不同信心水平的示例，减轻对高信心示例的过拟合，同时增强对低信心示例的学习信号。这一方法显著提升了在多种指令跟随和医疗领域特定SFT任务中的表现，证明了SFTMix对不同LLM家族的适应性以及对任何规模数据集的可扩展性。全面的消融研究进一步验证了SFTMix设计选择的稳健性，强调其在语言处理应用中提升性能的通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:19:15 GMT</pubDate>
</item>
<item>
<title>Agent S: 基于多模态大语言模型的自主交互框架</title>
<link>https://arxiv.org/abs/2410.08164</link>
<guid>https://arxiv.org/abs/2410.08164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent S 是一种开放的自主代理框架，可通过GUI自动化复杂任务，实现人机交互的变革。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Agent S，一个开放的自主代理框架，旨在通过图形用户界面与计算机进行自主交互，从而实现人机交互的变革，自动化复杂和多步骤的任务。Agent S解决了自动化计算机任务的三个主要挑战：获取领域特定知识、规划长期任务及处理动态非均匀界面。为此，Agent S 引入了经验增强层次规划，能够在多个层次上从外部知识搜索和内部经验检索中学习，从而促进高效的任务规划和子任务执行。此外，Agent S 采用了代理-计算机接口（ACI），更好地充分发挥基于多模态大语言模型（MLLMs）的GUI代理的推理和控制能力。根据OSWorld基准的评估结果，Agent S 在成功率上超过了基线水平9.37%，实现了83.6%的相对改进，并且取得了新的最先进成果。全面分析展示了各个组件的有效性，并为未来的改进提供了洞见。此外，Agent S 在新发布的WindowsAgentArena基准上展现出良好的广泛泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:06:26 GMT</pubDate>
</item>
<item>
<title>大语言与视觉模型（LLVMs）的感知能力研究</title>
<link>https://arxiv.org/abs/2410.04751</link>
<guid>https://arxiv.org/abs/2410.04751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统探讨了LLVMs在感知任务中的表现及其内在机制。</p><br /><br /><p><strong>摘要：</strong> 本文系统地研究了大语言与视觉模型（LLVMs），特别是它们在基础感知任务（如MMVP）上的低表现。通过对几种LLVMs家族（如LLaVA）的10个评估基准进行评估，我们发现多个有趣的特性：1）即便视觉块序列随机排列，它们仍以全局方式处理图像；2）在解决数学问题时，模型并不总是需要详细的数字信息；3）交叉模态的对齐在复杂推理任务中存在过拟合现象，从而导致模型失去部分视觉编码器的原始感知能力；4）模型较低层次的表示空间（低于25%）在性能和增强视觉理解中起着关键作用。基于这些观察，本文提出了未来改进LLVMs及构建更具挑战性评估基准的潜在方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:01:45 GMT</pubDate>
</item>
<item>
<title>AlphaLLM-CPL：一种基于MCTS行为蒸馏的自我改进框架</title>
<link>https://arxiv.org/abs/2410.06508</link>
<guid>https://arxiv.org/abs/2410.06508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AlphaLLM-CPL框架，利用MCTS生成的轨迹对LLM进行自我改进，显著提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们提出了一种新的框架，AlphaLLM-CPL，用于通过蒙特卡洛树搜索（MCTS）行为蒸馏来提升大语言模型（LLM）的推理能力。尽管现有的蒸馏方法利用MCTS生成的轨迹，但仍然未能充分利用这些丰富的信息，限制了LLM推理性能的提升。AlphaLLM-CPL引入了两个关键创新：首先，它从共享同一父节点的子节点构造逐步轨迹对，以提供更有效的逐步信息用于MCTS行为蒸馏。其次，AlphaLLM-CPL采用了课程偏好学习，在每个离线训练周期中动态调整轨迹对的训练顺序，优先考虑关键学习步骤，从而减轻过拟合。通过在数学推理任务上的实验结果表明，AlphaLLM-CPL显著优于以往的MCTS行为蒸馏方法，极大地提升了LLM的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:06:42 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的进展及应用</title>
<link>https://arxiv.org/abs/2410.08151</link>
<guid>https://arxiv.org/abs/2410.08151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出扩展现有视频扩散模型的方法，实现长达1分钟的视频生成。</p><br /><br /><p><strong>摘要：</strong> 当前最前沿的视频扩散模型在生成高质量视频方面表现出色。然而，由于计算限制，这些模型通常只能生成时长约为10秒（240帧）的视频。本文展示了如何在不改变现有架构的情况下，将这些模型自然扩展为自回归视频扩散模型。我们的关键思想是为潜在帧分配逐渐增加的噪声水平，而不是使用单一的噪声水平，从而允许潜在帧之间的细粒度条件及注意窗口之间的大重叠。这种渐进的视频去噪方法使我们的模型在自回归生成视频帧时，能够避免质量下降或突发场景变化。我们在长视频生成任务上取得了最先进的结果，生成了长达1分钟（1440帧、24帧每秒）的视频。本文视频可在https://desaixie.github.io/pa-vdm/上查看。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:02:23 GMT</pubDate>
</item>
<item>
<title>大卷积核在现代卷积神经网络设计中的应用</title>
<link>https://arxiv.org/abs/2410.08049</link>
<guid>https://arxiv.org/abs/2410.08049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出大卷积核作为设计现代卷积神经网络（ConvNets）的新范式，并引入UniRepLKNet架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出了大卷积核作为现代卷积神经网络（ConvNets）设计的新范式。研究表明，使用少量大卷积核，而非堆叠多个小卷积核，可能是一种更优的设计策略。我们提出了一套专门针对大卷积核ConvNets的架构设计指南，以优化其效率和性能。UniRepLKNet架构的设计原则特别强调大卷积核ConvNets在无需深层堆叠的情况下捕捉广泛空间信息的能力。实验结果表明，该模型在ImageNet上达到了88.0%的准确率，ADE20K数据集上获得了55.6%的mIoU，以及在COCO检测上取得了56.4%的AP，表现出显著的可扩展性及在多种场景下的优异性能，包括时间序列预测、音频、点云及视频识别。与视觉Transformer相比，大卷积核ConvNets具有更大的有效感受野和更高的形状偏置，克服了小卷积核CNN的纹理偏置。这些结果展示了大卷积核ConvNets的通用建模能力。所有代码和模型已在https://github.com/AILab-CVC/UniRepLKNet上公开，促进社区的进一步研究与发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:49:57 GMT</pubDate>
</item>
<item>
<title>简化和扩展扩散模型 rectification 的新策略</title>
<link>https://arxiv.org/abs/2410.07303</link>
<guid>https://arxiv.org/abs/2410.07303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出了 Rectified Diffusion，简化了 rectification 方法并验证其在 Stable Diffusion 上的效果。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视觉生成方面取得了显著进展，但由于解决生成常微分方程（ODE）的计算密集性，生成速度仍然较慢。本文提出的 Rectified Diffusion 方法，通过使用预训练的扩散模型获取噪声和样本的匹配对，简化了训练程序。我们认为，以往方法中包含的流匹配和 v 预测等组件并非必要，主要目标应是实现一阶近似 ODE 路径，而非强求路径的直线性。我们的方法不再局限于流匹配模型，而是广泛适用于各种扩散模型。经过在 Stable Diffusion v1-5 和 Stable Diffusion XL 上的验证，我们的方法不仅降低了训练成本，还提升了性能。我们的代码已开放在 GitHub 上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07303" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:27:35 GMT</pubDate>
</item>
<item>
<title>基于偏好学习的多模态轨迹检索增强方法</title>
<link>https://arxiv.org/abs/2410.03450</link>
<guid>https://arxiv.org/abs/2410.03450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MART方法，通过偏好学习优化轨迹检索，提升机器人在未见场景中的任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法MLLM as ReTriever (MART)，旨在提升机器人执行复杂任务的能力。当前的检索方法多关注表面相似性，未充分考虑轨迹对特定任务的有效性。MART利用交互数据，通过偏好学习对大型语言模型（MLLM）进行微调，使检索过程能够更好地评估和优先选择适用于未见任务的轨迹。为进一步加强理解，文章引入Trajectory Abstraction机制，利用MLLM的摘要能力将轨迹用更少的符号表示，并保留关键身分信息，这帮助代理更好地抓住轨迹中的重要里程碑。实验结果显示，在不同环境下，MART方法显著提升了代理在未见场景中的任务成功率，表明其在多模态轨迹检索与代理行为的研究中具有重要意义。所有基准任务集和模拟器代码修改将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:21:00 GMT</pubDate>
</item>
<item>
<title>PrefixQuant：一种高效的稀疏化量化技术用于大型语言模型的推理加速</title>
<link>https://arxiv.org/abs/2410.05265</link>
<guid>https://arxiv.org/abs/2410.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefixQuant通过离线隔离高频稀疏令牌，实现了高效的静态量化，显著提升推理速度与准确性。</p><br /><br /><p><strong>摘要：</strong> 量化在大型语言模型（LLM）部署中尤为重要，因为它能提高内存效率和推理速度。目前的激活量化方法主要处理通道级别的离群点，往往忽视令牌级别的离群点，因此依赖于昂贵的每令牌动态量化技术。为了解决这个问题，我们提出了PrefixQuant，一种独特的技术，能够离线识别高频离群令牌，并将其前缀存储在KV缓存中，从而防止推理时生成离群令牌，并简化量化。根据我们的知识，PrefixQuant首次实现了高效的每张量静态量化，超过了昂贵的每令牌动态量化。以W4A4KV4（4位权重、4位激活和4位KV缓存）上的Llama-3-8B为例，PrefixQuant结合每张量静态量化实现了7.43的WikiText2困惑度以及71.08%的常识推理任务平均准确率，分别比之前的动态量化方法QuaRot提升了0.98困惑度与5.98准确率。此外，使用PrefixQuant的W4A4量化模型的推理速度比FP16模型快1.60至2.81倍，相较于QuaRot模型快1.2至1.3倍。我们的代码已发布在https://github.com/ChenMnZ/PrefixQuant。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:13:37 GMT</pubDate>
</item>
<item>
<title>Optima：提升大语言模型多智能体系统通信效率与任务有效性的框架</title>
<link>https://arxiv.org/abs/2410.08115</link>
<guid>https://arxiv.org/abs/2410.08115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Optima通过提高通信效率和任务有效性，解决大语言模型多智能体系统中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Optima，一个新颖的框架，旨在解决大语言模型（LLM）基础的多智能体系统（MAS）面临的低通信效率、较差的可扩展性和缺乏有效参数更新优化方法等关键挑战。Optima通过对LLM进行训练，采用迭代的生成、排名、选择和训练模式，结合一个平衡任务性能、 tokens 效率和交流可读性的奖励函数，大幅增强了通信效率和任务有效性。我们探索了多种强化学习算法，包括监督微调（Supervised Fine-Tuning）、直接偏好优化（Direct Preference Optimization）及其混合方法，提供了它们的有效性与效率权衡的深入见解。此外，我们结合受蒙特卡洛树搜索启发的技术生成DPO数据，将对话回合视为树节点，以探索多样的互动路径。在信息不对称问答和复杂推理等常见多智能体任务上评估后，Optima相较于单智能体基线和基于Llama 3 8B的传统MAS表现出一致而显著的提升，在需要大量信息交换的任务中实现了多达2.8倍的性能提升，同时消耗的tokens少于10%。此外，Optima的效率提升为更有效利用推理计算开辟了新可能，推动了推理时间的规模法则的改进。通过解决大语言模型基础多智能体系统中的基本挑战，Optima展示了朝着可扩展、高效和有效的多智能体系统发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:11:50 GMT</pubDate>
</item>
<item>
<title>重复训练示例在变压器模型中的效益研究</title>
<link>https://arxiv.org/abs/2410.07041</link>
<guid>https://arxiv.org/abs/2410.07041</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，在固定训练步数下，重复训练的示例效果优于单次使用的示例。</p><br /><br /><p><strong>摘要：</strong> 本文研究了变压器模型在算法生成数据集上的性能表现，重点关注训练示例重复使用的问题。在处理最大公约数、模乘法和矩阵特征值这三种数学问题时，我们发现，在固定的训练步数下，使用重复的示例的小规模训练集的模型性能优于使用单次示例的大规模训练集的模型。这表明，重复训练的益处超越了数据多样性带来的优势。此外，我们还展示了两集训练的方法，即对小随机子集的重复使用与对训练集其余部分的正常采样相结合，可以加速学习并提升模型性能。这项研究在受控环境下提供了关于深度学习中泛化与记忆之间尚不清晰的相互关系的洞察。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07041" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:59:17 GMT</pubDate>
</item>
<item>
<title>基于局部对抗负例损失的视觉语言模型增强方法</title>
<link>https://arxiv.org/abs/2410.05210</link>
<guid>https://arxiv.org/abs/2410.05210</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新方法FSC-CLIP，通过局部硬负例损失提升视觉语言模型的组合理解能力，保持多模态任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法——Fine-grained Selective Calibrated CLIP (FSC-CLIP)，旨在提升预训练视觉语言模型（VLMs）的组合理解能力，同时不影响零-shot多模态任务的性能。传统的微调方法通常在提高组合推理的同时降低多模态能力，其主要原因是使用全局硬负例（HN）损失，导致图像和文本的全局表示受到影响。这种全局HN损失会推送与原始文本高度相似的HN文本，从而损害模型的多模态表示。为克服这一限制，FSC-CLIP整合了局部硬负例损失和选择性校准正则化。这些创新提供了精细的负向监督，同时保持了模型的表示完整性。我们的广泛评估显示，FSC-CLIP在多项组合性和多模态任务基准上不仅达到了与最先进模型相当的组合能力，而且保持了强大的多模态能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05210" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:47:13 GMT</pubDate>
</item>
<item>
<title>DICE：用于可控编辑的离散反演方法</title>
<link>https://arxiv.org/abs/2410.08207</link>
<guid>https://arxiv.org/abs/2410.08207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICE是首个用于离散扩散模型的精确反演方法，支持灵活的内容编辑。</p><br /><br /><p><strong>摘要：</strong> 离散扩散模型在图像生成和掩蔽语言建模等任务中取得了成功，但在可控内容编辑方面存在局限性。我们提出了DICE（Discrete Inversion for Controllable Editing），这是第一个支持离散扩散模型（包括多项式扩散和掩蔽生成模型）的精确反演方法。通过在反向扩散过程中记录噪声序列和掩蔽模式，DICE实现了对离散数据的准确重建和灵活编辑，无需预定义掩蔽或注意力操作。我们在图像和文本领域展示了DICE的有效性，并在VQ-Diffusion、Paella和RoBERTa等模型上进行了评估。结果显示，DICE在提高编辑能力的同时，也保持了高数据保真度，为离散空间中的精细内容操控提供了新的机遇。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:32:24 GMT</pubDate>
</item>
</channel>
</rss>