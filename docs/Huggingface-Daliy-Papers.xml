<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>ZClip: Adaptive Spike Mitigation for LLM Pre-Training</title>
<link>https://arxiv.org/abs/2504.02507</link>
<guid>https://arxiv.org/abs/2504.02507</guid>
<content:encoded><![CDATA[

  Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.

]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 07:41:55 GMT</pubDate>
<pubDate>Thu, 03 Apr 2025 07:41:55 GMT</pubDate>
</item>
<item>
<title>Scaling Analysis of Interleaved Speech-Text Language Models</title>
<link>https://arxiv.org/abs/2504.02398</link>
<guid>https://arxiv.org/abs/2504.02398</guid>
<content:encoded><![CDATA[

  Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.

]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 04:46:56 GMT</pubDate>
<pubDate>Thu, 03 Apr 2025 04:46:56 GMT</pubDate>
</item>
<item>
<title>Instruction-Guided Autoregressive Neural Network Parameter Generation</title>
<link>https://arxiv.org/abs/2504.02012</link>
<guid>https://arxiv.org/abs/2504.02012</guid>
<content:encoded><![CDATA[

  Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.

]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:50:19 GMT</pubDate>
<pubDate>Wed, 02 Apr 2025 01:50:19 GMT</pubDate>
</item>
<item>
<title>GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning</title>
<link>https://arxiv.org/abs/2504.00891</link>
<guid>https://arxiv.org/abs/2504.00891</guid>
<content:encoded><![CDATA[

  Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.

]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:21:05 GMT</pubDate>
<pubDate>Tue, 01 Apr 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers</title>
<link>https://arxiv.org/abs/2504.00502</link>
<guid>https://arxiv.org/abs/2504.00502</guid>
<content:encoded><![CDATA[

  Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV

]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:47:55 GMT</pubDate>
<pubDate>Tue, 01 Apr 2025 03:47:55 GMT</pubDate>
</item>

<item>
<title>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing</title>
<link>https://arxiv.org/abs/2504.02826</link>
<guid>https://arxiv.org/abs/2504.02826</guid>
<content:encoded><![CDATA[
Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation</title>
<link>https://arxiv.org/abs/2504.02782</link>
<guid>https://arxiv.org/abs/2504.02782</guid>
<content:encoded><![CDATA[
The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:23:16 GMT</pubDate>
</item>
<item>
<title>Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme</title>
<link>https://arxiv.org/abs/2504.02587</link>
<guid>https://arxiv.org/abs/2504.02587</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 09:53:28 GMT</pubDate>
</item>
<item>
<title>Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</title>
<link>https://arxiv.org/abs/2504.02542</link>
<guid>https://arxiv.org/abs/2504.02542</guid>
<content:encoded><![CDATA[
Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 08:44:41 GMT</pubDate>
</item>
<item>
<title>SkyReels-A2: Compose Anything in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.02436</link>
<guid>https://arxiv.org/abs/2504.02436</guid>
<content:encoded><![CDATA[
This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 05:50:50 GMT</pubDate>
</item>
<item>
<title>Efficient Model Selection for Time Series Forecasting via LLMs</title>
<link>https://arxiv.org/abs/2504.02119</link>
<guid>https://arxiv.org/abs/2504.02119</guid>
<content:encoded><![CDATA[
Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 16:33:27 GMT</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 03:20:58 GMT</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 17:10:39 GMT</pubDate>
</item>
<item>
<title>Target-Aware Video Diffusion Models</title>
<link>https://arxiv.org/abs/2503.18950</link>
<guid>https://arxiv.org/abs/2503.18950</guid>
<content:encoded><![CDATA[
We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>Medical large language models are easily distracted</title>
<link>https://arxiv.org/abs/2504.01201</link>
<guid>https://arxiv.org/abs/2504.01201</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:34:01 GMT</pubDate>
</item>
<item>
<title>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis</title>
<link>https://arxiv.org/abs/2502.18924</link>
<guid>https://arxiv.org/abs/2502.18924</guid>
<content:encoded><![CDATA[
While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 03:22:00 GMT</pubDate>
</item>
<item>
<title>DASH: Detection and Assessment of Systematic Hallucinations of VLMs</title>
<link>https://arxiv.org/abs/2503.23573</link>
<guid>https://arxiv.org/abs/2503.23573</guid>
<content:encoded><![CDATA[
Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 15:45:09 GMT</pubDate>
</item>
<item>
<title>Towards Physically Plausible Video Generation via VLM Planning</title>
<link>https://arxiv.org/abs/2503.23368</link>
<guid>https://arxiv.org/abs/2503.23368</guid>
<content:encoded><![CDATA[
Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 05:03:09 GMT</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:05:03 GMT</pubDate>
</item>
<item>
<title>Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations</title>
<link>https://arxiv.org/abs/2503.18817</link>
<guid>https://arxiv.org/abs/2503.18817</guid>
<content:encoded><![CDATA[
Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:00:21 GMT</pubDate>
</item>
<item>
<title>Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback</title>
<link>https://arxiv.org/abs/2405.20216</link>
<guid>https://arxiv.org/abs/2405.20216</guid>
<content:encoded><![CDATA[
The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.
]]></content:encoded>
<pubDate>Thu, 30 May 2024 12:18:05 GMT</pubDate>
</item>
<item>
<title>VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</title>
<link>https://arxiv.org/abs/2504.01956</link>
<guid>https://arxiv.org/abs/2504.01956</guid>
<content:encoded><![CDATA[
Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement</title>
<link>https://arxiv.org/abs/2504.01934</link>
<guid>https://arxiv.org/abs/2504.01934</guid>
<content:encoded><![CDATA[
We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:45:00 GMT</pubDate>
</item>
<item>
<title>Understanding R1-Zero-Like Training: A Critical Perspective</title>
<link>https://arxiv.org/abs/2503.20783</link>
<guid>https://arxiv.org/abs/2503.20783</guid>
<content:encoded><![CDATA[
DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>PaperBench: Evaluating AI's Ability to Replicate AI Research</title>
<link>https://arxiv.org/abs/2504.01848</link>
<guid>https://arxiv.org/abs/2504.01848</guid>
<content:encoded><![CDATA[
We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 11:55:24 GMT</pubDate>
</item>
<item>
<title>DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</title>
<link>https://arxiv.org/abs/2504.01724</link>
<guid>https://arxiv.org/abs/2504.01724</guid>
<content:encoded><![CDATA[
While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 09:30:32 GMT</pubDate>
</item>
<item>
<title>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</title>
<link>https://arxiv.org/abs/2504.01308</link>
<guid>https://arxiv.org/abs/2504.01308</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 22:35:19 GMT</pubDate>
</item>
<item>
<title>Articulated Kinematics Distillation from Video Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01204</link>
<guid>https://arxiv.org/abs/2504.01204</guid>
<content:encoded><![CDATA[
We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:37:57 GMT</pubDate>
</item>
<item>
<title>AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction</title>
<link>https://arxiv.org/abs/2504.01014</link>
<guid>https://arxiv.org/abs/2504.01014</guid>
<content:encoded><![CDATA[
Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:57:18 GMT</pubDate>
</item>
<item>
<title>MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</title>
<link>https://arxiv.org/abs/2504.00999</link>
<guid>https://arxiv.org/abs/2504.00999</guid>
<content:encoded><![CDATA[
Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:39:19 GMT</pubDate>
</item>
<item>
<title>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</title>
<link>https://arxiv.org/abs/2504.00883</link>
<guid>https://arxiv.org/abs/2504.00883</guid>
<content:encoded><![CDATA[
Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:11:11 GMT</pubDate>
</item>
<item>
<title>ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations</title>
<link>https://arxiv.org/abs/2504.00824</link>
<guid>https://arxiv.org/abs/2504.00824</guid>
<content:encoded><![CDATA[
Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:12:14 GMT</pubDate>
</item>
<item>
<title>LSNet: See Large, Focus Small</title>
<link>https://arxiv.org/abs/2503.23135</link>
<guid>https://arxiv.org/abs/2503.23135</guid>
<content:encoded><![CDATA[
Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:00:54 GMT</pubDate>
</item>
<item>
<title>Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models</title>
<link>https://arxiv.org/abs/2503.22165</link>
<guid>https://arxiv.org/abs/2503.22165</guid>
<content:encoded><![CDATA[
Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 02:09:51 GMT</pubDate>
</item>
<item>
<title>Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL</title>
<link>https://arxiv.org/abs/2503.23157</link>
<guid>https://arxiv.org/abs/2503.23157</guid>
<content:encoded><![CDATA[
Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 13:29:30 GMT</pubDate>
</item>
<item>
<title>MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing</title>
<link>https://arxiv.org/abs/2503.24219</link>
<guid>https://arxiv.org/abs/2503.24219</guid>
<content:encoded><![CDATA[
We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:36:41 GMT</pubDate>
</item>
<item>
<title>MixerMDM: Learnable Composition of Human Motion Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01019</link>
<guid>https://arxiv.org/abs/2504.01019</guid>
<content:encoded><![CDATA[
Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Scaling Language-Free Visual Representation Learning</title>
<link>https://arxiv.org/abs/2504.01017</link>
<guid>https://arxiv.org/abs/2504.01017</guid>
<content:encoded><![CDATA[
Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:15 GMT</pubDate>
</item>
<item>
<title>DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.24210</link>
<guid>https://arxiv.org/abs/2503.24210</guid>
<content:encoded><![CDATA[
Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:27:07 GMT</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 22:18:51 GMT</pubDate>
</item>
<item>
<title>Towards Trustworthy GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2503.23434</link>
<guid>https://arxiv.org/abs/2503.23434</guid>
<content:encoded><![CDATA[
GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 09:26:00 GMT</pubDate>
</item>
<item>
<title>OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts</title>
<link>https://arxiv.org/abs/2503.22952</link>
<guid>https://arxiv.org/abs/2503.22952</guid>
<content:encoded><![CDATA[
The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 22:46:58 GMT</pubDate>
</item>
<item>
<title>Multi-Token Attention</title>
<link>https://arxiv.org/abs/2504.00927</link>
<guid>https://arxiv.org/abs/2504.00927</guid>
<content:encoded><![CDATA[
Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:59:32 GMT</pubDate>
</item>
<item>
<title>m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.00869</link>
<guid>https://arxiv.org/abs/2504.00869</guid>
<content:encoded><![CDATA[
Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:57:43 GMT</pubDate>
</item>
<item>
<title>Z1: Efficient Test-time Scaling with Code</title>
<link>https://arxiv.org/abs/2504.00810</link>
<guid>https://arxiv.org/abs/2504.00810</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., . . . ) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:01:50 GMT</pubDate>
</item>
<item>
<title>Command A: An Enterprise-Ready Large Language Model</title>
<link>https://arxiv.org/abs/2504.00698</link>
<guid>https://arxiv.org/abs/2504.00698</guid>
<content:encoded><![CDATA[
In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 08:08:07 GMT</pubDate>
</item>
<item>
<title>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources</title>
<link>https://arxiv.org/abs/2504.00595</link>
<guid>https://arxiv.org/abs/2504.00595</guid>
<content:encoded><![CDATA[
The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:54:00 GMT</pubDate>
</item>
<item>
<title>Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features</title>
<link>https://arxiv.org/abs/2504.00557</link>
<guid>https://arxiv.org/abs/2504.00557</guid>
<content:encoded><![CDATA[
Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:10:32 GMT</pubDate>
</item>
<item>
<title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
<link>https://arxiv.org/abs/2504.00509</link>
<guid>https://arxiv.org/abs/2504.00509</guid>
<content:encoded><![CDATA[
The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:57:58 GMT</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead</title>
<link>https://arxiv.org/abs/2504.00294</link>
<guid>https://arxiv.org/abs/2504.00294</guid>
<content:encoded><![CDATA[
Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 19:40:28 GMT</pubDate>
</item>
<item>
<title>Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs</title>
<link>https://arxiv.org/abs/2504.00072</link>
<guid>https://arxiv.org/abs/2504.00072</guid>
<content:encoded><![CDATA[
We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:41:29 GMT</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:50:39 GMT</pubDate>
</item>
<item>
<title>GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors</title>
<link>https://arxiv.org/abs/2504.01016</link>
<guid>https://arxiv.org/abs/2504.01016</guid>
<content:encoded><![CDATA[
Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.01005</link>
<guid>https://arxiv.org/abs/2504.01005</guid>
<content:encoded><![CDATA[
Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:41:57 GMT</pubDate>
</item>
<item>
<title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
<link>https://arxiv.org/abs/2504.00906</link>
<guid>https://arxiv.org/abs/2504.00906</guid>
<content:encoded><![CDATA[
Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:40:27 GMT</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.24377</link>
<guid>https://arxiv.org/abs/2503.24377</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1</title>
<link>https://arxiv.org/abs/2503.24376</link>
<guid>https://arxiv.org/abs/2503.24376</guid>
<content:encoded><![CDATA[
Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:55:23 GMT</pubDate>
</item>
<item>
<title>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</title>
<link>https://arxiv.org/abs/2503.23733</link>
<guid>https://arxiv.org/abs/2503.23733</guid>
<content:encoded><![CDATA[
Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:13:02 GMT</pubDate>
</item>
<item>
<title>Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base</title>
<link>https://arxiv.org/abs/2503.23361</link>
<guid>https://arxiv.org/abs/2503.23361</guid>
<content:encoded><![CDATA[
Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 04:33:56 GMT</pubDate>
</item>
<item>
<title>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</title>
<link>https://arxiv.org/abs/2503.21860</link>
<guid>https://arxiv.org/abs/2503.21860</guid>
<content:encoded><![CDATA[
Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:50:30 GMT</pubDate>
</item>
<item>
<title>DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</title>
<link>https://arxiv.org/abs/2503.22677</link>
<guid>https://arxiv.org/abs/2503.22677</guid>
<content:encoded><![CDATA[
Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>ActionStudio: A Lightweight Framework for Data and Training of Large Action Models</title>
<link>https://arxiv.org/abs/2503.22673</link>
<guid>https://arxiv.org/abs/2503.22673</guid>
<content:encoded><![CDATA[
Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>AvatarArtist: Open-Domain 4D Avatarization</title>
<link>https://arxiv.org/abs/2503.19906</link>
<guid>https://arxiv.org/abs/2503.19906</guid>
<content:encoded><![CDATA[
This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>PAVE: Patching and Adapting Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.19794</link>
<guid>https://arxiv.org/abs/2503.19794</guid>
<content:encoded><![CDATA[
Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:02:37 GMT</pubDate>
</item>
<item>
<title>Understanding Co-speech Gestures in-the-wild</title>
<link>https://arxiv.org/abs/2503.22668</link>
<guid>https://arxiv.org/abs/2503.22668</guid>
<content:encoded><![CDATA[
Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:55:52 GMT</pubDate>
</item>
<item>
<title>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</title>
<link>https://arxiv.org/abs/2503.22655</link>
<guid>https://arxiv.org/abs/2503.22655</guid>
<content:encoded><![CDATA[
Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:43:00 GMT</pubDate>
</item>
<item>
<title>Entropy-Based Adaptive Weighting for Self-Training</title>
<link>https://arxiv.org/abs/2503.23913</link>
<guid>https://arxiv.org/abs/2503.23913</guid>
<content:encoded><![CDATA[
The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 06:04:35 GMT</pubDate>
</item>
<item>
<title>MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs</title>
<link>https://arxiv.org/abs/2503.23022</link>
<guid>https://arxiv.org/abs/2503.23022</guid>
<content:encoded><![CDATA[
In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 05:21:50 GMT</pubDate>
</item>
<item>
<title>Decoupling Angles and Strength in Low-rank Adaptation</title>
<link>https://arxiv.org/abs/2503.18225</link>
<guid>https://arxiv.org/abs/2503.18225</guid>
<content:encoded><![CDATA[
Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 18:00:56 GMT</pubDate>
</item>
<item>
<title>Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</title>
<link>https://arxiv.org/abs/2503.24391</link>
<guid>https://arxiv.org/abs/2503.24391</guid>
<content:encoded><![CDATA[
Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data</title>
<link>https://arxiv.org/abs/2503.21694</link>
<guid>https://arxiv.org/abs/2503.21694</guid>
<content:encoded><![CDATA[
It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 12:59:15 GMT</pubDate>
</item>
<item>
<title>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</title>
<link>https://arxiv.org/abs/2503.19901</link>
<guid>https://arxiv.org/abs/2503.19901</guid>
<content:encoded><![CDATA[
Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:57:46 GMT</pubDate>
</item>
<item>
<title>UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2503.14941</link>
<guid>https://arxiv.org/abs/2503.14941</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&amp;A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 03:15:41 GMT</pubDate>
</item>
<item>
<title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</title>
<link>https://arxiv.org/abs/2503.24388</link>
<guid>https://arxiv.org/abs/2503.24388</guid>
<content:encoded><![CDATA[
Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Query and Conquer: Execution-Guided SQL Generation</title>
<link>https://arxiv.org/abs/2503.24364</link>
<guid>https://arxiv.org/abs/2503.24364</guid>
<content:encoded><![CDATA[
We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:43:36 GMT</pubDate>
</item>
<item>
<title>Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model</title>
<link>https://arxiv.org/abs/2503.24290</link>
<guid>https://arxiv.org/abs/2503.24290</guid>
<content:encoded><![CDATA[
We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 12:36:05 GMT</pubDate>
</item>
<item>
<title>What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:46:15 GMT</pubDate>
</item>
<item>
<title>Expanding RL with Verifiable Rewards Across Diverse Domains</title>
<link>https://arxiv.org/abs/2503.23829</link>
<guid>https://arxiv.org/abs/2503.23829</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 04:22:49 GMT</pubDate>
</item>
<item>
<title>KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language</title>
<link>https://arxiv.org/abs/2503.23730</link>
<guid>https://arxiv.org/abs/2503.23730</guid>
<content:encoded><![CDATA[
The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:04:25 GMT</pubDate>
</item>
<item>
<title>TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes</title>
<link>https://arxiv.org/abs/2503.23461</link>
<guid>https://arxiv.org/abs/2503.23461</guid>
<content:encoded><![CDATA[
This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 10:36:55 GMT</pubDate>
</item>
<item>
<title>SketchVideo: Sketch-based Video Generation and Editing</title>
<link>https://arxiv.org/abs/2503.23284</link>
<guid>https://arxiv.org/abs/2503.23284</guid>
<content:encoded><![CDATA[
Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 22:44:09 GMT</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 09:27:46 GMT</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 10:06:17 GMT</pubDate>
</item>
<item>
<title>MoCha: Towards Movie-Grade Talking Character Synthesis</title>
<link>https://arxiv.org/abs/2503.23307</link>
<guid>https://arxiv.org/abs/2503.23307</guid>
<content:encoded><![CDATA[
Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 00:22:09 GMT</pubDate>
</item>
<item>
<title>Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization</title>
<link>https://arxiv.org/abs/2503.20286</link>
<guid>https://arxiv.org/abs/2503.20286</guid>
<content:encoded><![CDATA[
Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 03:30:23 GMT</pubDate>
</item>
<item>
<title>Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code</title>
<link>https://arxiv.org/abs/2503.18809</link>
<guid>https://arxiv.org/abs/2503.18809</guid>
<content:encoded><![CDATA[
In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:50:20 GMT</pubDate>
</item>
<item>
<title>SWI: Speaking with Intent in Large Language Models</title>
<link>https://arxiv.org/abs/2503.21544</link>
<guid>https://arxiv.org/abs/2503.21544</guid>
<content:encoded><![CDATA[
Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:34:28 GMT</pubDate>
</item>
<item>
<title>Reconstructing Humans with a Biomechanically Accurate Skeleton</title>
<link>https://arxiv.org/abs/2503.21751</link>
<guid>https://arxiv.org/abs/2503.21751</guid>
<content:encoded><![CDATA[
In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:24 GMT</pubDate>
</item>
<item>
<title>Your ViT is Secretly an Image Segmentation Model</title>
<link>https://arxiv.org/abs/2503.19108</link>
<guid>https://arxiv.org/abs/2503.19108</guid>
<content:encoded><![CDATA[
Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 15:56:02 GMT</pubDate>
</item>
<item>
<title>4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding</title>
<link>https://arxiv.org/abs/2503.17827</link>
<guid>https://arxiv.org/abs/2503.17827</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\% accuracy compared to the human baseline of 91\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 13:55:53 GMT</pubDate>
</item>
<item>
<title>OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning</title>
<link>https://arxiv.org/abs/2503.16081</link>
<guid>https://arxiv.org/abs/2503.16081</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have gained significant traction for their ability to process diverse input data types and generate coherent, contextually relevant outputs across various applications. While supervised fine-tuning (SFT) has been the predominant approach to enhance MLLM capabilities in task-specific optimization, it often falls short in fostering crucial generalized reasoning abilities. Although reinforcement learning (RL) holds great promise in overcoming these limitations, it encounters two significant challenges: (1) its generalized capacities in multimodal tasks remain largely unexplored, and (2) its training constraints, including the constant Kullback-Leibler divergence or the clamp strategy, often result in suboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an advanced MLLM equipped with profound comprehension and reasoning capabilities across multimodal tasks. Specifically, we introduce Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly enhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct, GRPO-D achieves a relative improvement of more than 5.72% over SFT and more than 13.59% over GRPO in same-task evaluation on two adapted datasets. Furthermore, GRPO-D demonstrates remarkable cross-task generalization capabilities, with an average relative improvement of more than 61.63% over SFT in cross-task evaluation. These results highlight that the MLLM trained with GRPO-D on one multimodal task can be effectively transferred to another task, underscoring the superior generalized reasoning capabilities of our proposed OThink-MR1 model.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:22:18 GMT</pubDate>
</item>
<item>
<title>A Refined Analysis of Massive Activations in LLMs</title>
<link>https://arxiv.org/abs/2503.22329</link>
<guid>https://arxiv.org/abs/2503.22329</guid>
<content:encoded><![CDATA[
Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 07:08:34 GMT</pubDate>
</item>
<item>
<title>Segment Any Motion in Videos</title>
<link>https://arxiv.org/abs/2503.22268</link>
<guid>https://arxiv.org/abs/2503.22268</guid>
<content:encoded><![CDATA[
Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 05:34:11 GMT</pubDate>
</item>
<item>
<title>Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging</title>
<link>https://arxiv.org/abs/2503.22236</link>
<guid>https://arxiv.org/abs/2503.22236</guid>
<content:encoded><![CDATA[
With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:39:20 GMT</pubDate>
</item>
<item>
<title>Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2503.22230</link>
<guid>https://arxiv.org/abs/2503.22230</guid>
<content:encoded><![CDATA[
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:26:41 GMT</pubDate>
</item>
<item>
<title>X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction</title>
<link>https://arxiv.org/abs/2503.21779</link>
<guid>https://arxiv.org/abs/2503.21779</guid>
<content:encoded><![CDATA[
Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</title>
<link>https://arxiv.org/abs/2503.21732</link>
<guid>https://arxiv.org/abs/2503.21732</guid>
<content:encoded><![CDATA[
Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:46:42 GMT</pubDate>
</item>
<item>
<title>On Large Multimodal Models as Open-World Image Classifiers</title>
<link>https://arxiv.org/abs/2503.21851</link>
<guid>https://arxiv.org/abs/2503.21851</guid>
<content:encoded><![CDATA[
Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:03:18 GMT</pubDate>
</item>
<item>
<title>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</title>
<link>https://arxiv.org/abs/2503.21614</link>
<guid>https://arxiv.org/abs/2503.21614</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback</title>
<link>https://arxiv.org/abs/2503.21332</link>
<guid>https://arxiv.org/abs/2503.21332</guid>
<content:encoded><![CDATA[
Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 06:11:41 GMT</pubDate>
</item>
<item>
<title>Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency</title>
<link>https://arxiv.org/abs/2503.20785</link>
<guid>https://arxiv.org/abs/2503.20785</guid>
<content:encoded><![CDATA[
We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2503.20308</link>
<guid>https://arxiv.org/abs/2503.20308</guid>
<content:encoded><![CDATA[
Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:18:57 GMT</pubDate>
</item>
<item>
<title>PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving</title>
<link>https://arxiv.org/abs/2503.21821</link>
<guid>https://arxiv.org/abs/2503.21821</guid>
<content:encoded><![CDATA[
We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:21:56 GMT</pubDate>
</item>
<item>
<title>AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2503.19693</link>
<guid>https://arxiv.org/abs/2503.19693</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 10:18:21 GMT</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 10:04:18 GMT</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.22194</link>
<guid>https://arxiv.org/abs/2503.22194</guid>
<content:encoded><![CDATA[
We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 03:23:12 GMT</pubDate>
</item>
<item>
<title>Tracktention Layer</title>
<link>https://arxiv.org/abs/2503.19904</link>
<guid>https://arxiv.org/abs/2503.19904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tracktention Layer</p><br /><br /><p><strong></strong> 3DTracktention LayerTracktention LayerTransformerTracktention Layer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19904" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:48 GMT</pubDate>
</item>
<item>
<title>SemLA</title>
<link>https://arxiv.org/abs/2503.21780</link>
<guid>https://arxiv.org/abs/2503.21780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SemLA</p><br /><br /><p><strong></strong> SemLASemLALoRACLIP1020SemLA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21780" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>LOCATEdit</title>
<link>https://arxiv.org/abs/2503.21541</link>
<guid>https://arxiv.org/abs/2503.21541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOCATEdit</p><br /><br /><p><strong></strong> TEXT-guided image editing LOCATEditLOCATEditPIE-Bench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21541" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:32:17 GMT</pubDate>
</item>
<item>
<title>Embodied Reasoner</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Embodied Reasoner </p><br /><br /><p><strong></strong>  Embodied Reasoner  9.3k -- 64k  90k  OpenAI o1o3-mini  Claude-3.7 </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21696" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:00:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs122024LLMsLLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21248" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 04:09:15 GMT</pubDate>
</item>
<item>
<title>FinAudio: </title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinAudio</p><br /><br /><p><strong></strong> FinAudioAudioLLMsAudioLLMsFinAudioAudioLLMsAudioLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20990" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 17:07:51 GMT</pubDate>
</item>
<item>
<title>Video-R1: </title>
<link>https://arxiv.org/abs/2503.21776</link>
<guid>https://arxiv.org/abs/2503.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Video-R1T-GRPO</p><br /><br /><p><strong></strong> Video-R1MLLMsR1GRPOT-GRPOSFTVideo-R1-COT-165kVideo-R1-260kVideo-R1VideoMMMUVSI-BenchMVBenchTempCompassVSI-benchVideo-R1-7B35.8%GPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21776" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21774</link>
<guid>https://arxiv.org/abs/2503.21774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ODE1099.4%GitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21774" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21765</link>
<guid>https://arxiv.org/abs/2503.21765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 1) 2) 3) </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21765" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>Lumina-Image 2.0</title>
<link>https://arxiv.org/abs/2503.21758</link>
<guid>https://arxiv.org/abs/2503.21758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumina-Image 2.0</p><br /><br /><p><strong></strong> Lumina-Image 2.0T2ILumina-NextUnified Next-DiTLumina-Image 2.0Unified Captioner, UniCapT2ILumina-Image 2.02.6B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21758" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21755</link>
<guid>https://arxiv.org/abs/2503.21755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VBench-2.0</p><br /><br /><p><strong></strong> VBenchVBench-2.0VLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21755" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:01 GMT</pubDate>
</item>
<item>
<title>LeX-Art: -</title>
<link>https://arxiv.org/abs/2503.21749</link>
<guid>https://arxiv.org/abs/2503.21749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LeX-Art-</p><br /><br /><p><strong></strong> LeX-Art - Deepseek-R1  LeX-10K 10,00010241024 LeX-EnhancerLeX-FLUX  LeX-Lumina LeX-Bench Pairwise Normalized Edit DistancePNEDLeX-Lumina  CreateBench 79.81% PNED  LeX-FLUX </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21749" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:15 GMT</pubDate>
</item>
<item>
<title>ReaRAG</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReaRAG</p><br /><br /><p><strong></strong> LRMsLRMsReaRAGLRMRAGReaRAGLRMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21729" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:44:18 GMT</pubDate>
</item>
<item>
<title>GUI</title>
<link>https://arxiv.org/abs/2503.21620</link>
<guid>https://arxiv.org/abs/2503.21620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI</p><br /><br /><p><strong></strong> RLMLLMsGUI136GRPOUI-R1-3BIDOODIDAndroidControl15%10.3%OOD GUIScreenSpot-Pro6.0%OS-Atlas-7B76KSFTGUI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21620" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:39:30 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21460</link>
<guid>https://arxiv.org/abs/2503.21460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21460" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 08:50:17 GMT</pubDate>
</item>
<item>
<title>OlymMATH: </title>
<link>https://arxiv.org/abs/2503.21380</link>
<guid>https://arxiv.org/abs/2503.21380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OlymMATH</p><br /><br /><p><strong></strong> OlymMATHOlymMATH200AIMEOlymMATH</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21380" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 07:20:17 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.21144</link>
<guid>https://arxiv.org/abs/2503.21144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 4090 GPU512 * 76830fps</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21144" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:18:53 GMT</pubDate>
</item>
<item>
<title>ZJUKLABSemEval-2025</title>
<link>https://arxiv.org/abs/2503.21088</link>
<guid>https://arxiv.org/abs/2503.21088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZJUKLABSemEval-2025</p><br /><br /><p><strong></strong> ZJUKLABSemEval-20254TIES260.9440.487MIAROUGE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.21088" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 22:03:25 GMT</pubDate>
</item>
<item>
<title>UniDisc</title>
<link>https://arxiv.org/abs/2503.20853</link>
<guid>https://arxiv.org/abs/2503.20853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniDisc</p><br /><br /><p><strong></strong> UniDiscARUniDiscUniDisc</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20853" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>Feature4X2D4D</title>
<link>https://arxiv.org/abs/2503.20776</link>
<guid>https://arxiv.org/abs/2503.20776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feature4X2D4D</p><br /><br /><p><strong></strong> Feature4X2D4DFeature4X4DFeature4X4DAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20776" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:56:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20578</link>
<guid>https://arxiv.org/abs/2503.20578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">bug</p><br /><br /><p><strong></strong> bugbugLLPutLLaMAQwenQwen-Coder206bug</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20578" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 10:25:01 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20822</link>
<guid>https://arxiv.org/abs/2503.20822</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20822" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 20:45:07 GMT</pubDate>
</item>
<item>
<title>RecTable: </title>
<link>https://arxiv.org/abs/2503.20731</link>
<guid>https://arxiv.org/abs/2503.20731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecTable</p><br /><br /><p><strong></strong> RecTableGANVAERecTableRecTableRecTableGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20731" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:12:20 GMT</pubDate>
</item>
<item>
<title>Gemma 3</title>
<link>https://arxiv.org/abs/2503.19786</link>
<guid>https://arxiv.org/abs/2503.19786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemma 3</p><br /><br /><p><strong></strong> Gemma 3Gemma1270128KKVGemma 3Gemma 2Gemma3-4B-ITGemma2-27B-ITGemma3-27B-ITGemini-1.5-Pro</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19786" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:52:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20641</link>
<guid>https://arxiv.org/abs/2503.20641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs12L2S12SVD55%2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20641" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 11:34:37 GMT</pubDate>
</item>
<item>
<title>TBA</title>
<link>https://arxiv.org/abs/2503.18929</link>
<guid>https://arxiv.org/abs/2503.18929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TBARL</p><br /><br /><p><strong></strong> RLLLM(TBA)LLM RLTBATBA4TBA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18929" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:51:39 GMT</pubDate>
</item>
<item>
<title>UniHDSA: </title>
<link>https://arxiv.org/abs/2503.15893</link>
<guid>https://arxiv.org/abs/2503.15893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniHDSA</p><br /><br /><p><strong></strong> HDSAUniHDSAHDSAUniHDSATransformerComp-HRDocDocLayNet</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15893" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:44:47 GMT</pubDate>
</item>
<item>
<title>RONA</title>
<link>https://arxiv.org/abs/2503.10997</link>
<guid>https://arxiv.org/abs/2503.10997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RONA</p><br /><br /><p><strong></strong> RONAMLLMRONARONA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10997" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 21:45:38 GMT</pubDate>
</item>
<item>
<title>PathoHR</title>
<link>https://arxiv.org/abs/2503.17970</link>
<guid>https://arxiv.org/abs/2503.17970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PathoHR</p><br /><br /><p><strong></strong> PathoHRVision TransformerViTPathoHR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17970" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:37:24 GMT</pubDate>
</item>
<item>
<title>Vision-Language</title>
<link>https://arxiv.org/abs/2503.20271</link>
<guid>https://arxiv.org/abs/2503.20271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViLBench</p><br /><br /><p><strong></strong> PRMVLLMORMPRMVLLMViLBenchVLLMOpenAIGPT-4o27.3%73.6K3BViLBenchChain-of-ThoughtCoT3.3%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20271" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:38:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17358</link>
<guid>https://arxiv.org/abs/2503.17358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> IMUMASt3RCOLMAP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17358" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:58:56 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-Omni</title>
<link>https://arxiv.org/abs/2503.20215</link>
<guid>https://arxiv.org/abs/2503.20215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-Omni</p><br /><br /><p><strong></strong> Qwen2.5-OmniTMRoPERoPEQwen2.5-OmniThinker-TalkerThinkerTalkerThinkerQwen2.5-VLOmni-Bench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20215" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:17:55 GMT</pubDate>
</item>
<item>
<title>LEGO-Puzzles</title>
<link>https://arxiv.org/abs/2503.19990</link>
<guid>https://arxiv.org/abs/2503.19990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LEGO-Puzzles</p><br /><br /><p><strong></strong> LEGO-PuzzlesMLLMs110011MLLMs90%LEGOGemini-2.0-FlashGPT-4oLEGO-PuzzlesMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19990" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 14:21:07 GMT</pubDate>
</item>
<item>
<title>MCTS-RAG</title>
<link>https://arxiv.org/abs/2503.20757</link>
<guid>https://arxiv.org/abs/2503.20757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCTS-RAG</p><br /><br /><p><strong></strong> MCTS-RAGRAGMCTSMCTS-RAGRAGMCTSMCTS-RAGGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20757" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:46:08 GMT</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:45:29 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20672</link>
<guid>https://arxiv.org/abs/2503.20672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> FluxIdeogram 2.0Infographics-650KBizEvalSOTAFluxSD3Infographics-650KBizEval</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20672" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 12:04:57 GMT</pubDate>
</item>
<item>
<title>Wan</title>
<link>https://arxiv.org/abs/2503.20314</link>
<guid>https://arxiv.org/abs/2503.20314</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Wan</p><br /><br /><p><strong></strong> WanWanVAEWan14B1.3BWan1.3B8.19 GB VRAMGPUWan</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20314" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:25:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20240</link>
<guid>https://arxiv.org/abs/2503.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> CFGCFGCFGCFGCFGZero-1-to-3Versatile DiffusionDiTDynamiCrafterInstructPix2Pix</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20240" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 01:11:38 GMT</pubDate>
</item>
<item>
<title>DINeMo3D</title>
<link>https://arxiv.org/abs/2503.20220</link>
<guid>https://arxiv.org/abs/2503.20220</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINeMo3D3D</p><br /><br /><p><strong></strong> DINeMo3DDINeMo3D67.3%DINeMo3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20220" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:23:53 GMT</pubDate>
</item>
<item>
<title>ODSAI</title>
<link>https://arxiv.org/abs/2503.20201</link>
<guid>https://arxiv.org/abs/2503.20201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ODSLLM</p><br /><br /><p><strong></strong> ODSAIODSLLMODSLLMLLMDeepSeek-R1ODSSimpleQAFRAMESFRAMESODSGPT-4o9.7%ODSLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20201" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:51:32 GMT</pubDate>
</item>
<item>
<title>Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</title>
<link>https://arxiv.org/abs/2503.20198</link>
<guid>https://arxiv.org/abs/2503.20198</guid>
<content:encoded><![CDATA[
Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:44:25 GMT</pubDate>
</item>
<item>
<title>Gemini Robotics--</title>
<link>https://arxiv.org/abs/2503.20020</link>
<guid>https://arxiv.org/abs/2503.20020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Robotics</p><br /><br /><p><strong></strong> Gemini RoboticsGemini 2.0--VLAGemini Robotics100Gemini Robotics-ERGemini</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20020" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 15:02:56 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.19953</link>
<guid>https://arxiv.org/abs/2503.19953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Opt-CWM</p><br /><br /><p><strong></strong> Opt-CWMOpt-CWMOpt-CWM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19953" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:52 GMT</pubDate>
</item>
<item>
<title>Attention-IoU</title>
<link>https://arxiv.org/abs/2503.19846</link>
<guid>https://arxiv.org/abs/2503.19846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Attention-IoU</p><br /><br /><p><strong></strong> Attention-IoUAttention Intersection over UnionWaterbirdsAttention-IoUCelebAAttention-IoUCelebAAttention-IoU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19846" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:11:39 GMT</pubDate>
</item>
<item>
<title>LogQuant2</title>
<link>https://arxiv.org/abs/2503.19950</link>
<guid>https://arxiv.org/abs/2503.19950</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LogQuantKV</p><br /><br /><p><strong></strong> LogQuant2LLMKVtokenstokensLogQuantKVLogQuant25%60%LogQuant40%200%GitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19950" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:24:45 GMT</pubDate>
</item>
<item>
<title>Dita</title>
<link>https://arxiv.org/abs/2503.19757</link>
<guid>https://arxiv.org/abs/2503.19757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dita</p><br /><br /><p><strong></strong> DitaTransformerDitaDita10-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19757" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:19:56 GMT</pubDate>
</item>
<item>
<title>GenHancerCLIP</title>
<link>https://arxiv.org/abs/2503.19480</link>
<guid>https://arxiv.org/abs/2503.19480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenHancerCLIP</p><br /><br /><p><strong></strong> GenHancerCLIP1) tokenstokens2) 3) GenHancerMMVP-VLMOpenAICLIP6.0%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19480" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:15:34 GMT</pubDate>
</item>
<item>
<title>AccVideo: </title>
<link>https://arxiv.org/abs/2503.19462</link>
<guid>https://arxiv.org/abs/2503.19462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AccVideo</p><br /><br /><p><strong></strong> AccVideo8.55720x128024fps</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19462" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 04:52:07 GMT</pubDate>
</item>
<item>
<title>Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs</title>
<link>https://arxiv.org/abs/2503.16870</link>
<guid>https://arxiv.org/abs/2503.16870</guid>
<content:encoded><![CDATA[
Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (&lt;10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:58:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Llama 3.0 8BLlama 3.1 8BGPQA10.7%-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.20110" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 19:24:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18712</link>
<guid>https://arxiv.org/abs/2503.18712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MLLMsEPIC-KITCHENS-100EPIC-KITCHENS-100-MQAMLLMsMLLMsEPIC-KITCHENS-100EPIC-KITCHENS-100-MQAGPT-4o 21EgoSchemaPerceptionTestLongVideoBenchVideoMMEMVBenchMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18712" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 10:24:17 GMT</pubDate>
</item>
<item>
<title>Any6D6D</title>
<link>https://arxiv.org/abs/2503.18673</link>
<guid>https://arxiv.org/abs/2503.18673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Any6D RGB-D6D</p><br /><br /><p><strong></strong> Any6D6DRGB-D6D3DAny6D2D-3DREAL275Toyota-LightHO3DYCB-INEOATLM-O</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18673" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>360</title>
<link>https://arxiv.org/abs/2503.15667</link>
<guid>https://arxiv.org/abs/2503.15667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">360</p><br /><br /><p><strong></strong> 360Diffusion360DiffPortrait3DControlNet(NeRFs)360</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15667" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 15:47:04 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.04919</link>
<guid>https://arxiv.org/abs/2503.04919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FirePlace3D</p><br /><br /><p><strong></strong> MLLMsFirePlace3DMLLMsMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04919" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 14:34:15 GMT</pubDate>
</item>
<item>
<title>ST-VLM</title>
<link>https://arxiv.org/abs/2503.19355</link>
<guid>https://arxiv.org/abs/2503.19355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ST-VLM</p><br /><br /><p><strong></strong> VLMsSTKitSTKit-Bench3D3D4DST-VLMST-VLMSTKit-BenchST-VLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19355" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:08:06 GMT</pubDate>
</item>
<item>
<title>OpenCity3D</title>
<link>https://arxiv.org/abs/2503.16776</link>
<guid>https://arxiv.org/abs/2503.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenCity3DVLMs</p><br /><br /><p><strong></strong> OpenCity3DVLMs3DOpenCity3DOpenCity3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16776" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 21:11:21 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17982</link>
<guid>https://arxiv.org/abs/2503.17982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MidAirAeroscapes20.2GitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17982" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 04:25:07 GMT</pubDate>
</item>
<item>
<title>PhysTwin: </title>
<link>https://arxiv.org/abs/2503.17973</link>
<guid>https://arxiv.org/abs/2503.17973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysTwin</p><br /><br /><p><strong></strong> PhysTwin-PhysTwinPhysTwin</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17973" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:49:19 GMT</pubDate>
</item>
<item>
<title>FullDiT</title>
<link>https://arxiv.org/abs/2503.19907</link>
<guid>https://arxiv.org/abs/2503.19907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FullDiT</p><br /><br /><p><strong></strong> FullDiTFullDiTFullBenchFullDiT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19907" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>LPOSS+</title>
<link>https://arxiv.org/abs/2503.19777</link>
<guid>https://arxiv.org/abs/2503.19777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LPOSS+VLMsVLMsVMLPOSS+</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19777" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:47:13 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2503.19356</link>
<guid>https://arxiv.org/abs/2503.19356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> AIAIQualcommIVDAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19356" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:13:12 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.19207</link>
<guid>https://arxiv.org/abs/2503.19207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3D1000-shot3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19207" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 19:20:47 GMT</pubDate>
</item>
<item>
<title>VocAgnoLM</title>
<link>https://arxiv.org/abs/2503.19123</link>
<guid>https://arxiv.org/abs/2503.19123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VocAgnoLM</p><br /><br /><p><strong></strong> VocAgnoLM127B1BQwen2.5-Math-InstructTinyLlama6%VocAgnoLM46%VocAgnoLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19123" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 16:19:31 GMT</pubDate>
</item>
<item>
<title>WikiAutoGen</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WikiAutoGen</p><br /><br /><p><strong></strong> WikiAutoGenWikiAutoGenWikiSeekWikiAutoGenWikiSeek8%-29%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19065" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:51:55 GMT</pubDate>
</item>
<item>
<title>xKV</title>
<link>https://arxiv.org/abs/2503.18893</link>
<guid>https://arxiv.org/abs/2503.18893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">xKVKV-Cache</p><br /><br /><p><strong></strong> LLMsKV-CacheKV-CachexKVSVDKV-CacheKV-CacheLLMsLlama-3.1Qwen2.5xKV6.82.7%xKV3xKVLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18893" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:37 GMT</pubDate>
</item>
<item>
<title>Towards a Unified Copernicus Foundation Model for Earth Vision</title>
<link>https://arxiv.org/abs/2503.11849</link>
<guid>https://arxiv.org/abs/2503.11849</guid>
<content:encoded><![CDATA[
Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 16:16:48 GMT</pubDate>
</item>
<item>
<title>YOLOv12BoT-SORT</title>
<link>https://arxiv.org/abs/2503.17237</link>
<guid>https://arxiv.org/abs/2503.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOv12BoT-SORT</p><br /><br /><p><strong></strong> UAVsYOLOv12BoT-SORTYOLOv5DeepSORTGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17237" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 11:40:18 GMT</pubDate>
</item>
<item>
<title>Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation</title>
<link>https://arxiv.org/abs/2503.14905</link>
<guid>https://arxiv.org/abs/2503.14905</guid>
<content:encoded><![CDATA[
With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 01:14:44 GMT</pubDate>
</item>
<item>
<title>MDocAgent</title>
<link>https://arxiv.org/abs/2503.13964</link>
<guid>https://arxiv.org/abs/2503.13964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MDocAgent</p><br /><br /><p><strong></strong> MDocAgentMDocAgentMDocAgent12.1%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13964" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 02:57:21 GMT</pubDate>
</item>
<item>
<title>CoLLM</title>
<link>https://arxiv.org/abs/2503.19910</link>
<guid>https://arxiv.org/abs/2503.19910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLLM</p><br /><br /><p><strong></strong> Composed Image Retrieval (CIR)-CoLLM-340Multi-Text CIR (MTCIR)CIRCIRRFashion-IQCoLLMCIRMTCIR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19910" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>PS3</title>
<link>https://arxiv.org/abs/2503.19903</link>
<guid>https://arxiv.org/abs/2503.19903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PS3</p><br /><br /><p><strong></strong> PS3CLIP4KPS3VILA-HDAnyResS^24.3VILA-HD4K4KProVILA-HD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19903" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:37 GMT</pubDate>
</item>
<item>
<title>Multi-round Thinking</title>
<link>https://arxiv.org/abs/2503.19855</link>
<guid>https://arxiv.org/abs/2503.19855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsQwQ-32BDeepSeek-R1AIME 2024MATH-500QwQ-32BAIME 202480.3%82.1%DeepSeek-R179.7%82.0%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19855" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:19:38 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.19622</link>
<guid>https://arxiv.org/abs/2503.19622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LMMsHAVENLMMs60007SRFTTDPO7.65%4.5%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19622" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 09:12:17 GMT</pubDate>
</item>
<item>
<title>ReSearch</title>
<link>https://arxiv.org/abs/2503.19470</link>
<guid>https://arxiv.org/abs/2503.19470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReSearchLLM</p><br /><br /><p><strong></strong> LLMsReSearchLLMsQwen2.5-7BQwen2.5-32BReSearch</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19470" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:00:58 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 1) (SDE)2) 3) Rollover Budget Forcing (RBF)SDEVP-SDERBF</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19385" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 02:30:45 GMT</pubDate>
</item>
<item>
<title>Frame AutoRegressive</title>
<link>https://arxiv.org/abs/2503.19325</link>
<guid>https://arxiv.org/abs/2503.19325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Frame AutoRegressive</p><br /><br /><p><strong></strong> Frame AutoRegressive (FAR) FAR Token ARFlexRoPEFARFAR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.19325" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 23:38:06 GMT</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:11:42 GMT</pubDate>
</item>
<item>
<title>CoMP</title>
<link>https://arxiv.org/abs/2503.18931</link>
<guid>https://arxiv.org/abs/2503.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoMP</p><br /><br /><p><strong></strong> VFMsCoMPVFMsCoMPVFMsCoMP-SigLIPChartQADocVQA66.775.9ImageNet-1K87.4%ADE20K49.5mIoU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18931" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:52:47 GMT</pubDate>
</item>
<item>
<title>Frequency Dynamic Convolution: </title>
<link>https://arxiv.org/abs/2503.18783</link>
<guid>https://arxiv.org/abs/2503.18783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FDConv </p><br /><br /><p><strong></strong> FDConvDY-ConvFDConv KSMFBMKSM FBMFDConv  ResNet-50 3.6MFDConvConvNeXtSwin-Transformer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18783" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:32:06 GMT</pubDate>
</item>
<item>
<title>LSRNA</title>
<link>https://arxiv.org/abs/2503.18446</link>
<guid>https://arxiv.org/abs/2503.18446</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LSRNA</p><br /><br /><p><strong></strong> LSRNA1KRGBLSRNALSRRNALSRNA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18446" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 04:50:15 GMT</pubDate>
</item>
<item>
<title>Gumbel-Softmax</title>
<link>https://arxiv.org/abs/2503.17361</link>
<guid>https://arxiv.org/abs/2503.17361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gumbel-Softmax</p><br /><br /><p><strong></strong> Gumbel-SoftmaxDNAGumbel-SoftmaxGumbel-SoftmaxGumbel-SoftmaxSTGFlowDNA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17361" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16965</link>
<guid>https://arxiv.org/abs/2503.16965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMsLLMsVLMsVLMVLM-VLMLLMGPT-4VLMVLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16965" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:25:23 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.18476</link>
<guid>https://arxiv.org/abs/2503.18476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> VLM3D-VLMVLM3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18476" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:21:13 GMT</pubDate>
</item>
<item>
<title>Instruct-CLIP</title>
<link>https://arxiv.org/abs/2503.18406</link>
<guid>https://arxiv.org/abs/2503.18406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Instruct-CLIP</p><br /><br /><p><strong></strong> Instruct-CLIPT2IInstruct-CLIPInstruct-CLIPInstructPix2Pix120K</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18406" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 03:25:44 GMT</pubDate>
</item>
<item>
<title>QuartDepthASIC</title>
<link>https://arxiv.org/abs/2503.16709</link>
<guid>https://arxiv.org/abs/2503.16709</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QuartDepthMDEASIC</p><br /><br /><p><strong></strong> ASICQuartDepthASICMDE4ASIC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16709" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 17:03:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> HumanML3DMotion-XLCRLCR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18674" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18071</link>
<guid>https://arxiv.org/abs/2503.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> -</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18071" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 09:40:44 GMT</pubDate>
</item>
<item>
<title>Feather-SQL: NL2SQL</title>
<link>https://arxiv.org/abs/2503.17811</link>
<guid>https://arxiv.org/abs/2503.17811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feather-SQLNL2SQL</p><br /><br /><p><strong></strong> SQL (NL2SQL) NL2SQLFeather-SQLFeather-SQLschemaSQL1+1SQLSQLFeather-SQLNL2SQL10%54.76%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17811" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 12:22:53 GMT</pubDate>
</item>
<item>
<title>CODA</title>
<link>https://arxiv.org/abs/2503.17760</link>
<guid>https://arxiv.org/abs/2503.17760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CODA</p><br /><br /><p><strong></strong> CODACODAVAECODAVQGAN6ImageNet 256256100%8160.431.34FIDrFID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17760" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 08:59:00 GMT</pubDate>
</item>
<item>
<title>DynamicVis</title>
<link>https://arxiv.org/abs/2503.16426</link>
<guid>https://arxiv.org/abs/2503.16426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DynamicVis</p><br /><br /><p><strong></strong> DynamicVisDynamicVis972048x2048833 MB GPUViT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16426" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.14774</link>
<guid>https://arxiv.org/abs/2503.14774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> WB, sRGB WB16000sRGB100%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14774" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 19:01:22 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13074</link>
<guid>https://arxiv.org/abs/2503.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SRGTSRSRGTGTRQIGTSR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13074" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:25:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18018</link>
<guid>https://arxiv.org/abs/2503.18018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsGSM8KLLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18018" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:35:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18948</link>
<guid>https://arxiv.org/abs/2503.18948</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 12256x256ImageNet</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18948" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>CFG-Zero*: </title>
<link>https://arxiv.org/abs/2503.18886</link>
<guid>https://arxiv.org/abs/2503.18886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CFG-Zero*</p><br /><br /><p><strong></strong> CFGCFGCFG-Zero*Lumina-NextStable Diffusion 3FluxWan-2.1CFG-Zero*CFG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18886" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:59:57 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18878</link>
<guid>https://arxiv.org/abs/2503.18878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsSAEsDeepSeek-R1LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18878" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:54:26 GMT</pubDate>
</item>
<item>
<title>CURA: </title>
<link>https://arxiv.org/abs/2503.18494</link>
<guid>https://arxiv.org/abs/2503.18494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURA</p><br /><br /><p><strong></strong> CURAVPSCURABigCodeBench3.65%CURAo3-miniVPSLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18494" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:48:59 GMT</pubDate>
</item>
<item>
<title>MetaSpatial3D</title>
<link>https://arxiv.org/abs/2503.18470</link>
<guid>https://arxiv.org/abs/2503.18470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSpatialRL3D</p><br /><br /><p><strong></strong> MetaSpatialRLVLM3D3DMetaSpatialVLM3DSFTRLVLMMetaSpatialRL/</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18470" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:18:01 GMT</pubDate>
</item>
<item>
<title>Diffusion-4K</title>
<link>https://arxiv.org/abs/2503.18352</link>
<guid>https://arxiv.org/abs/2503.18352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diffusion-4K</p><br /><br /><p><strong></strong> Diffusion-4KAesthetic-4K4KGPT-4o4KGLCMFIDCLIPScore4K4KDiffusion-4KSD3-2BFlux-12B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18352" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 01:25:07 GMT</pubDate>
</item>
<item>
<title>OmnimatteZero</title>
<link>https://arxiv.org/abs/2503.18033</link>
<guid>https://arxiv.org/abs/2503.18033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmnimatteZero</p><br /><br /><p><strong></strong> OmnimatteZeroOmnimatteZeroOmnimatteZeroOmnimatte</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18033" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 07:26:48 GMT</pubDate>
</item>
<item>
<title>Aether</title>
<link>https://arxiv.org/abs/2503.18945</link>
<guid>https://arxiv.org/abs/2503.18945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aether</p><br /><br /><p><strong></strong> AetherAether-AetherAether</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18945" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18942</link>
<guid>https://arxiv.org/abs/2503.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> TTSToF TTS </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18942" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>Video SimpleQA</title>
<link>https://arxiv.org/abs/2503.18923</link>
<guid>https://arxiv.org/abs/2503.18923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Video SimpleQA</p><br /><br /><p><strong></strong> LVLMsVideo SimpleQALVLMs41LVLMsGemini-1.5-Pro54.4%F1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18923" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:46:09 GMT</pubDate>
</item>
<item>
<title>FFN Fusion: </title>
<link>https://arxiv.org/abs/2503.18908</link>
<guid>https://arxiv.org/abs/2503.18908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFN FusionFFN</p><br /><br /><p><strong></strong> FFN FusionFFNLlama-3.1-405B-InstructLlama-Nemotron-Ultra-253B-Base1.71Token3549B253BFFN FusionFFN</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18908" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:20:35 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 10LLama3-8BMistral-7B/24BQwen2.5Qwen</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18892" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.18866</link>
<guid>https://arxiv.org/abs/2503.18866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> EM1B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18866" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:41:23 GMT</pubDate>
</item>
<item>
<title>CaMeL</title>
<link>https://arxiv.org/abs/2503.18813</link>
<guid>https://arxiv.org/abs/2503.18813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CaMeLLLM</p><br /><br /><p><strong></strong> CaMeLLLMLLMCaMeLLLMCaMeLLLMCaMeLAgentDojoCaMeL67%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18813" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:54:10 GMT</pubDate>
</item>
<item>
<title>Hummingbird: </title>
<link>https://arxiv.org/abs/2503.18559</link>
<guid>https://arxiv.org/abs/2503.18559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hummingbird</p><br /><br /><p><strong></strong> T2VT2VHummingbirdU-Net147HummingbirdVideoCrafter231VBenchHummingbird26U-NetGPU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18559" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 07:13:33 GMT</pubDate>
</item>
<item>
<title>AgentRxiv</title>
<link>https://arxiv.org/abs/2503.18102</link>
<guid>https://arxiv.org/abs/2503.18102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentRxiv</p><br /><br /><p><strong></strong> AgentRxivLLMAgentRxivMATH-50011.4%MATH-50013.7%AIAgentRxiv</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18102" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 11:16:42 GMT</pubDate>
</item>
<item>
<title>Vision-R1: </title>
<link>https://arxiv.org/abs/2503.18013</link>
<guid>https://arxiv.org/abs/2503.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vision-R1LVLM</p><br /><br /><p><strong></strong> LVLMsVision-R1R1-likeVision-R17B LVLMs50% improvement10</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18013" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:21:14 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17735</link>
<guid>https://arxiv.org/abs/2503.17735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ASGI2V-AdapterSimDA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17735" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 07:28:25 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17500</link>
<guid>https://arxiv.org/abs/2503.17500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LIRTVR</p><br /><br /><p><strong></strong> LLMLIRTVR10LLaMA4.6%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17500" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 15:23:08 GMT</pubDate>
</item>
<item>
<title>RISC-V CPU</title>
<link>https://arxiv.org/abs/2503.17422</link>
<guid>https://arxiv.org/abs/2503.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RISC-V CPU</p><br /><br /><p><strong></strong> LLMsGPUCPURISC-VRISC-VRISC-V CPUSophon SG2042LLMLLMDeepSeek R1 Distill Llama 8BDeepSeek R1 Distill QWEN 14B4.32/2.29 token/s6.54/3.68 token/s2.9/3.0</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17422" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:00:19 GMT</pubDate>
</item>
<item>
<title>OMG3D</title>
<link>https://arxiv.org/abs/2503.16924</link>
<guid>https://arxiv.org/abs/2503.16924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OMG3D</p><br /><br /><p><strong></strong> 3D Gaussian Splatting3DGSOMGOMG50%600+</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16924" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:41:45 GMT</pubDate>
</item>
<item>
<title>Typed-RAG: </title>
<link>https://arxiv.org/abs/2503.15879</link>
<guid>https://arxiv.org/abs/2503.15879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Typed-RAG</p><br /><br /><p><strong></strong> NFQATyped-RAGRAGNFQATyped-RAGNFQNFQTyped-RAGWiki-NFQANFQTyped-RAGNFQA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15879" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:04:12 GMT</pubDate>
</item>
<item>
<title>Bottleneck Sampling</title>
<link>https://arxiv.org/abs/2503.18940</link>
<guid>https://arxiv.org/abs/2503.18940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Bottleneck Sampling--32.5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18940" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AlphaSpace3D</title>
<link>https://arxiv.org/abs/2503.18769</link>
<guid>https://arxiv.org/abs/2503.18769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaSpace3D</p><br /><br /><p><strong></strong> AlphaSpaceLLMs3DAlphaSpaceLLMs[x, y, z]AlphaSpace66.67%GPT-4o37.5%Claude 3.5 Sonnet29.17%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.18769" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:16:51 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2503.17489</link>
<guid>https://arxiv.org/abs/2503.17489</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> MLLMMMUMMGTaskAnythingJudgeAnythingMLLMTaskAnything151500JudgeAnything5GPT-4oGemini-2.0-FlashMLLMMMU66.55%MMG53.37%OmniArena</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17489" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 14:59:20 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17359</link>
<guid>https://arxiv.org/abs/2503.17359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> IGVGGEGGEIGVGGEL0L4AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17359" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17439</link>
<guid>https://arxiv.org/abs/2503.17439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LEMMA</p><br /><br /><p><strong></strong> LEMMALEMMA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17439" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:10 GMT</pubDate>
</item>
<item>
<title>MagicComp: </title>
<link>https://arxiv.org/abs/2503.14428</link>
<guid>https://arxiv.org/abs/2503.14428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicComp</p><br /><br /><p><strong></strong> MagicCompT2VMagicCompT2VT2V-CompBenchVBenchMagicComp</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14428" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:02:14 GMT</pubDate>
</item>
<item>
<title>Can Large Vision Language Models Read Maps Like a Human?</title>
<link>https://arxiv.org/abs/2503.14607</link>
<guid>https://arxiv.org/abs/2503.14607</guid>
<content:encoded><![CDATA[
In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 14:05:38 GMT</pubDate>
</item>
<item>
<title>GAEA</title>
<link>https://arxiv.org/abs/2503.16423</link>
<guid>https://arxiv.org/abs/2503.16423</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GAEA</p><br /><br /><p><strong></strong> GPSGAEAGAEA80160OpenStreetMap4000-GAEALMMsLLaVA-OneVisionGPT-4o25.69%8.28%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16423" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FFaceNeRF3D</title>
<link>https://arxiv.org/abs/2503.17095</link>
<guid>https://arxiv.org/abs/2503.17095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFaceNeRF3D</p><br /><br /><p><strong></strong> FFaceNeRFNeRF3DFFaceNeRFFFaceNeRF3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17095" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 08:24:58 GMT</pubDate>
</item>
<item>
<title>TaoAvatar</title>
<link>https://arxiv.org/abs/2503.17032</link>
<guid>https://arxiv.org/abs/2503.17032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaoAvatar</p><br /><br /><p><strong></strong> TaoAvatar3D Gaussian Splatting (3DGS) TaoAvatarStyleUnetMLP90 FPS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17032" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 06:40:37 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.16282</link>
<guid>https://arxiv.org/abs/2503.16282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GFS-VL</p><br /><br /><p><strong></strong> GFS-VL3DGFS-PCSGFS-PCS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16282" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:10:33 GMT</pubDate>
</item>
<item>
<title>SISO</title>
<link>https://arxiv.org/abs/2503.16025</link>
<guid>https://arxiv.org/abs/2503.16025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SISO</p><br /><br /><p><strong></strong> SISOSISOSISO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16025" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:45:04 GMT</pubDate>
</item>
<item>
<title>PVChat</title>
<link>https://arxiv.org/abs/2503.17069</link>
<guid>https://arxiv.org/abs/2503.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PVChat </p><br /><br /><p><strong></strong> PVChatViLLMViLLMsPVChatMixture-of-Heads (MoH)QAPVChatReLU Routing MoHPVChatViLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17069" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 07:50:06 GMT</pubDate>
</item>
<item>
<title>ETVA</title>
<link>https://arxiv.org/abs/2503.16867</link>
<guid>https://arxiv.org/abs/2503.16867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ETVA</p><br /><br /><p><strong></strong> ETVAETVAagentLLMLLMETVA58.4731.02000120001015T2V</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16867" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:52:50 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16660</link>
<guid>https://arxiv.org/abs/2503.16660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Gumbel-SoftmaxOCR50%30%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16660" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 15:17:08 GMT</pubDate>
</item>
<item>
<title>MathFlow</title>
<link>https://arxiv.org/abs/2503.16549</link>
<guid>https://arxiv.org/abs/2503.16549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathFlow</p><br /><br /><p><strong></strong> MLLMsFlowVerseMLLMsMathFlowMathFlow-P-7BMathFlow-P-7B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16549" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 07:46:19 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12821</link>
<guid>https://arxiv.org/abs/2503.12821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVLM</p><br /><br /><p><strong></strong> LVLMLTADRDRDSDRDSDDPMsADRLLaVA 1.54.36%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12821" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 01:01:09 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11572</link>
<guid>https://arxiv.org/abs/2503.11572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsRM-IATtokensAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11572" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:40:02 GMT</pubDate>
</item>
<item>
<title>OpenVLThinker</title>
<link>https://arxiv.org/abs/2503.17352</link>
<guid>https://arxiv.org/abs/2503.17352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenVLThinker</p><br /><br /><p><strong></strong> DeepSeek-R1LVLMsSFTRLR1RLRLSFTOpenVLThinkerMathVistaMathVerseMathVision</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17352" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:52:43 GMT</pubDate>
</item>
<item>
<title>FastCuRL</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FastCuRL</p><br /><br /><p><strong></strong> FastCuRLR115FastCuRLFastCuRL-1.5B-PreviewMATH 500AIME 2024AMC 2023Minerva MathOlympiadBenchDeepScaleR-1.5B-Preview50%FastCuRL-1.5B-Preview8 GPU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17287" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 12:35:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.17126</link>
<guid>https://arxiv.org/abs/2503.17126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DPOORPODivPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.17126" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 09:21:45 GMT</pubDate>
</item>
<item>
<title>VCtrl</title>
<link>https://arxiv.org/abs/2503.16983</link>
<guid>https://arxiv.org/abs/2503.16983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VCtrl</p><br /><br /><p><strong></strong> VCtrlPP-VCtrlVCtrlCannyVCtrlPaddlePaddle</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16983" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:48:00 GMT</pubDate>
</item>
<item>
<title>DPO</title>
<link>https://arxiv.org/abs/2503.16921</link>
<guid>https://arxiv.org/abs/2503.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DPO</p><br /><br /><p><strong></strong> Diffusion-DPOAdaptive-DPODPOAdaptive-DPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16921" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:33:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16905</link>
<guid>https://arxiv.org/abs/2503.16905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MSPsMSPsBig Seven PersonalityMAPSEMMAOlympiadMathVista15.84%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16905" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:13:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16874</link>
<guid>https://arxiv.org/abs/2503.16874</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARS</p><br /><br /><p><strong></strong> MARSMARS--MARS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16874" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 02:19:55 GMT</pubDate>
</item>
<item>
<title>TokenBridgeToken</title>
<link>https://arxiv.org/abs/2503.16430</link>
<guid>https://arxiv.org/abs/2503.16430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokenBridge</p><br /><br /><p><strong></strong> TokenBridgeTokenTokenTokenTokenBridgetokenizerTokenToken</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16430" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16408</link>
<guid>https://arxiv.org/abs/2503.16408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RoboFactoryRoboFactory</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16408" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:58:38 GMT</pubDate>
</item>
<item>
<title>GASP</title>
<link>https://arxiv.org/abs/2503.15672</link>
<guid>https://arxiv.org/abs/2503.15672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GASP4D</p><br /><br /><p><strong></strong> GASPGASP3D4DGASP4D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15672" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 16:00:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.14201</link>
<guid>https://arxiv.org/abs/2503.14201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ApacheSpring136T5Code Llama60M, 750M7BGPU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14201" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 08:26:06 GMT</pubDate>
</item>
<item>
<title>Where do Large Vision-Language Models Look at when Answering Questions?</title>
<link>https://arxiv.org/abs/2503.13891</link>
<guid>https://arxiv.org/abs/2503.13891</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 00:34:43 GMT</pubDate>
</item>
<item>
<title>DeCapBenchDCScore</title>
<link>https://arxiv.org/abs/2503.07906</link>
<guid>https://arxiv.org/abs/2503.07906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeCapBenchDCScore</p><br /><br /><p><strong></strong> (VLM)DeCapBenchDCScoreDCScoreDCScoreDeCapBenchVLMFeedQuillVLMGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07906" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:53:56 GMT</pubDate>
</item>
<item>
<title>Sonata</title>
<link>https://arxiv.org/abs/2503.16429</link>
<guid>https://arxiv.org/abs/2503.16429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sonata3D</p><br /><br /><p><strong></strong> Sonata3DSonataScanNet21.8%72.5%1%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16429" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>3DSynCity</title>
<link>https://arxiv.org/abs/2503.16420</link>
<guid>https://arxiv.org/abs/2503.16420</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SynCity3D</p><br /><br /><p><strong></strong> SynCity3D3DSynCity3D2D3DSynCity3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16420" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.15299</link>
<guid>https://arxiv.org/abs/2503.15299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsLLMs40%LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15299" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:21:48 GMT</pubDate>
</item>
<item>
<title>PORTALAI3D</title>
<link>https://arxiv.org/abs/2503.13356</link>
<guid>https://arxiv.org/abs/2503.13356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PORTALAI3D</p><br /><br /><p><strong></strong> PORTAL3DLLMsDSLPORTAL-PORTAL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13356" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:42:34 GMT</pubDate>
</item>
<item>
<title>TikZero</title>
<link>https://arxiv.org/abs/2503.11509</link>
<guid>https://arxiv.org/abs/2503.11509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TikZero</p><br /><br /><p><strong></strong> TikZTikZeroTikZeroTikZeroGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11509" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:29:58 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2503.09949</link>
<guid>https://arxiv.org/abs/2503.09949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> MLLMsUVE-BenchVGMs15UVE-Bench16MLLMsMLLMsAIGVMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09949" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 21:52:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16425</link>
<guid>https://arxiv.org/abs/2503.16425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> TokenSet</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16425" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>SwD</title>
<link>https://arxiv.org/abs/2503.16397</link>
<guid>https://arxiv.org/abs/2503.16397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SwD</p><br /><br /><p><strong></strong> SwD(DMs)SwD-SwD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16397" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:54:02 GMT</pubDate>
</item>
<item>
<title>VidKVKV</title>
<link>https://arxiv.org/abs/2503.16257</link>
<guid>https://arxiv.org/abs/2503.16257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidKVVideoLLMs</p><br /><br /><p><strong></strong> VideoLLMsKVVidKVKV2KVVidKVKV1.51.58VidKV</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16257" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:52:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16091</link>
<guid>https://arxiv.org/abs/2503.16091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIMI</p><br /><br /><p><strong></strong> AIMI27CNNLSTMLSTM93.2%F-193.6%GitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16091" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:32:35 GMT</pubDate>
</item>
<item>
<title>VideoRFSplat3D</title>
<link>https://arxiv.org/abs/2503.15855</link>
<guid>https://arxiv.org/abs/2503.15855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoRFSplat3D</p><br /><br /><p><strong></strong> VideoRFSplat3D3D2DVideoRFSplat3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15855" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:26:09 GMT</pubDate>
</item>
<item>
<title>BigO(Bench)</title>
<link>https://arxiv.org/abs/2503.15242</link>
<guid>https://arxiv.org/abs/2503.15242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BigO(Bench)</p><br /><br /><p><strong></strong> BigO(Bench)BigO(Bench)Python31501190250token-space</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15242" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:19:57 GMT</pubDate>
</item>
<item>
<title>Flux</title>
<link>https://arxiv.org/abs/2503.14237</link>
<guid>https://arxiv.org/abs/2503.14237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flux</p><br /><br /><p><strong></strong> FluxFluxFluxViT1/4FluxViT90%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14237" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 09:15:58 GMT</pubDate>
</item>
<item>
<title>RSD: </title>
<link>https://arxiv.org/abs/2503.13358</link>
<guid>https://arxiv.org/abs/2503.13358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RSD</p><br /><br /><p><strong></strong> RSDResShiftRSDRSDGPURealSRRealSet65DRealSRImageNetDIV2K</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13358" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:44:08 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2503.16416</link>
<guid>https://arxiv.org/abs/2503.16416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16416" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16375</link>
<guid>https://arxiv.org/abs/2503.16375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> NuiScene43</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16375" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:37:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16219</link>
<guid>https://arxiv.org/abs/2503.16219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RL1.5DeepSeek-R1-Distill-Qwen-1.5B4NVIDIA A40 GPU48 GB VRAM24Group Relative Policy OptimizationGRPOAMC2363%80%AIME2446.7%4207000</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16219" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:13:23 GMT</pubDate>
</item>
<item>
<title>Race-DiT: </title>
<link>https://arxiv.org/abs/2503.16057</link>
<guid>https://arxiv.org/abs/2503.16057</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Race-DiT</p><br /><br /><p><strong></strong> Race-DiTMoEExpert RaceImageNet</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16057" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:45:08 GMT</pubDate>
</item>
<item>
<title>SALT: </title>
<link>https://arxiv.org/abs/2503.16055</link>
<guid>https://arxiv.org/abs/2503.16055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SALT</p><br /><br /><p><strong></strong> SALTSVD5SALTPEFTLoRASVD2%5%Dice3.9%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16055" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:42:41 GMT</pubDate>
</item>
<item>
<title>Zero-1-to-A4D</title>
<link>https://arxiv.org/abs/2503.15851</link>
<guid>https://arxiv.org/abs/2503.15851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zero-1-to-A4D</p><br /><br /><p><strong></strong> Animatable head avatar generation traditionallyZero-1-to-A4DZero-1-to-A</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15851" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:07:46 GMT</pubDate>
</item>
<item>
<title>MotionStreamer: </title>
<link>https://arxiv.org/abs/2503.15451</link>
<guid>https://arxiv.org/abs/2503.15451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MotionStreamer</p><br /><br /><p><strong></strong> GPTMotionStreamer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15451" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:32:24 GMT</pubDate>
</item>
<item>
<title>DiffMoE</title>
<link>https://arxiv.org/abs/2503.14487</link>
<guid>https://arxiv.org/abs/2503.14487</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffMoE</p><br /><br /><p><strong></strong> DiffMoEDiffMoEDiffMoEImageNet</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14487" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MASMASMAS15014MASMASFTLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13657" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:04:38 GMT</pubDate>
</item>
<item>
<title>LHM</title>
<link>https://arxiv.org/abs/2503.10625</link>
<guid>https://arxiv.org/abs/2503.10625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LHM</p><br /><br /><p><strong></strong> LHMLHMLHM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10625" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>XAttention: Transformer</title>
<link>https://arxiv.org/abs/2503.16428</link>
<guid>https://arxiv.org/abs/2503.16428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XAttentionTransformer</p><br /><br /><p><strong></strong> TransformerLCTMsXAttentionTransformerXAttentionRULERLongBenchVideoMMEVBenchXAttention13.5XAttention</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16428" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.16422</link>
<guid>https://arxiv.org/abs/2503.16422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4DGS-1K</p><br /><br /><p><strong></strong> 4D Gaussian Splatting (4DGS) 4DGS-1KGPU1000 FPS-4DGS419</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16422" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>MagicMotion</title>
<link>https://arxiv.org/abs/2503.16421</link>
<guid>https://arxiv.org/abs/2503.16421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicMotion</p><br /><br /><p><strong></strong> MagicMotionMagicMotionMagicDataMagicBenchMagicMotion</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16421" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLRMsOpenAI o1DeepSeek-R1LLMs123</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16419" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>InfiniteYou</title>
<link>https://arxiv.org/abs/2503.16418</link>
<guid>https://arxiv.org/abs/2503.16418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniteYou</p><br /><br /><p><strong></strong> InfiniteYou (InfU)  (DiTs) InfUInfuseNetDiT (SPMS) SFTInfU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16418" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16365</link>
<guid>https://arxiv.org/abs/2503.16365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLAAct from Visual Language Post-TrainingVLMsMinecraft1,000VLA40%Minecraft</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16365" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:21:58 GMT</pubDate>
</item>
<item>
<title>URAE</title>
<link>https://arxiv.org/abs/2503.16322</link>
<guid>https://arxiv.org/abs/2503.16322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">URAE</p><br /><br /><p><strong></strong> URAEFLUX1URAE30002000FLUX1.12K4K</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16322" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:44:43 GMT</pubDate>
</item>
<item>
<title>FlashVDM3D</title>
<link>https://arxiv.org/abs/2503.16302</link>
<guid>https://arxiv.org/abs/2503.16302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlashVDMVAEDiT3D</p><br /><br /><p><strong></strong> FlashVDM3DVAEDiTVecset Diffusion Model (VDM)3DFlashVDMProgressive Flow DistillationDiT5KVFlashVDMVAE3D4532</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16302" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:23:44 GMT</pubDate>
</item>
<item>
<title>MathFusion: </title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathFusion</p><br /><br /><p><strong></strong> MathFusionMathFusionMathFusionQA18.045K</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16212" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:00:41 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16194</link>
<guid>https://arxiv.org/abs/2503.16194</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> CTFImageNetInception Score59</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16194" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:41:29 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MLLMs-shotCLS-RLMLLMsCLS-RLSFTNo-Thinking-CLS-RL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16188" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:37:45 GMT</pubDate>
</item>
<item>
<title>BalGrad</title>
<link>https://arxiv.org/abs/2503.13834</link>
<guid>https://arxiv.org/abs/2503.13834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BalGrad</p><br /><br /><p><strong></strong> VLBalGradKLUPMC Food-101Hateful MemesMM-IMDbBalGrad</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13834" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 22:17:41 GMT</pubDate>
</item>
<item>
<title>MagicID: </title>
<link>https://arxiv.org/abs/2503.12689</link>
<guid>https://arxiv.org/abs/2503.12689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicID</p><br /><br /><p><strong></strong> MagicIDMagicIDFrontierMagicID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12689" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 19:15:09 GMT</pubDate>
</item>
<item>
<title>M3</title>
<link>https://arxiv.org/abs/2503.16413</link>
<guid>https://arxiv.org/abs/2503.16413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3</p><br /><br /><p><strong></strong> M3M3M3M3-M3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16413" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>CaKELLM</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CaKELLM</p><br /><br /><p><strong></strong> KELLMsKEKEMEMITWISECaKECircuit-aware Knowledge EditingCaKEKEMQuAKE20%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16356" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:14:34 GMT</pubDate>
</item>
<item>
<title>Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens</title>
<link>https://arxiv.org/abs/2503.16278</link>
<guid>https://arxiv.org/abs/2503.16278</guid>
<content:encoded><![CDATA[
Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:07:04 GMT</pubDate>
</item>
<item>
<title>Fin-R1</title>
<link>https://arxiv.org/abs/2503.16252</link>
<guid>https://arxiv.org/abs/2503.16252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fin-R1</p><br /><br /><p><strong></strong> Fin-R1Fin-R1DeepSeek-R1SFTRLFin-R1DeepSeek-R170FinQAConvFinQASOTAFin-R1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16252" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:46:18 GMT</pubDate>
</item>
<item>
<title>DHD</title>
<link>https://arxiv.org/abs/2503.16031</link>
<guid>https://arxiv.org/abs/2503.16031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DHD</p><br /><br /><p><strong></strong> DHDDHDChatGPT-4o13DHD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.16031" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:58:02 GMT</pubDate>
</item>
<item>
<title>Unified Variational Auto-Encoder3D</title>
<link>https://arxiv.org/abs/2503.15567</link>
<guid>https://arxiv.org/abs/2503.15567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3D3DUAE-3D3DDiffusion TransformerUAE-3DGEOM-DrugsQM93D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15567" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 04:56:13 GMT</pubDate>
</item>
<item>
<title>Cosmos-Reason1AI</title>
<link>https://arxiv.org/abs/2503.15558</link>
<guid>https://arxiv.org/abs/2503.15558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cosmos-Reason1AI</p><br /><br /><p><strong></strong> Cosmos-Reason1AIAICosmos-Reason1-8BCosmos-Reason1-56BAIAIAINVIDIA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15558" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 18:06:58 GMT</pubDate>
</item>
<item>
<title>SWEET-RL</title>
<link>https://arxiv.org/abs/2503.15478</link>
<guid>https://arxiv.org/abs/2503.15478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWEET-RLLLM</p><br /><br /><p><strong></strong> RLLLMColBenchLLMSWEET-RLSWEET-RLColBench6%Llama-3.1-8BGPT4-o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15478" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>SkyLadder</title>
<link>https://arxiv.org/abs/2503.15450</link>
<guid>https://arxiv.org/abs/2503.15450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SkyLadder</p><br /><br /><p><strong></strong> LLMtokenSkyLadderSkyLadder100B tokens1B32K3B8K3.7%22%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15450" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:31:15 GMT</pubDate>
</item>
<item>
<title>DP-Recon: 3D</title>
<link>https://arxiv.org/abs/2503.14830</link>
<guid>https://arxiv.org/abs/2503.14830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DP-Recon3D</p><br /><br /><p><strong></strong> DP-Recon3DDP-ReconScore Distillation Sampling (SDS) SDSDP-ReconReplicaScanNet++10100UV</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14830" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 22:11:31 GMT</pubDate>
</item>
<item>
<title>LLM-FE: </title>
<link>https://arxiv.org/abs/2503.14434</link>
<guid>https://arxiv.org/abs/2503.14434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM-FE</p><br /><br /><p><strong></strong> LLM-FELLMLLM-FELLMLLM-FE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14434" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:11:24 GMT</pubDate>
</item>
<item>
<title>VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity</title>
<link>https://arxiv.org/abs/2503.11557</link>
<guid>https://arxiv.org/abs/2503.11557</guid>
<content:encoded><![CDATA[
Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:26:11 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.15354</link>
<guid>https://arxiv.org/abs/2503.15354</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 0.070.120-1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15354" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:56:21 GMT</pubDate>
</item>
<item>
<title>MetaLadder</title>
<link>https://arxiv.org/abs/2503.14891</link>
<guid>https://arxiv.org/abs/2503.14891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaLadder</p><br /><br /><p><strong></strong> MetaLadderLLMsMetaLadderMetaLadderLLMsCoT10.3%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14891" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 00:36:35 GMT</pubDate>
</item>
<item>
<title>CURIE</title>
<link>https://arxiv.org/abs/2503.13517</link>
<guid>https://arxiv.org/abs/2503.13517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURIE</p><br /><br /><p><strong></strong> CURIE580Gemini Flash 2.0Claude-3GPT-4ocommand-R+32%CURIE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13517" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:53:03 GMT</pubDate>
</item>
<item>
<title>KDTalker3D</title>
<link>https://arxiv.org/abs/2503.12963</link>
<guid>https://arxiv.org/abs/2503.12963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KDTalker3D</p><br /><br /><p><strong></strong> KDTalker3DKDTalkerKDTalker</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12963" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:18:31 GMT</pubDate>
</item>
<item>
<title>SynthScars: LEGION</title>
<link>https://arxiv.org/abs/2503.15264</link>
<guid>https://arxiv.org/abs/2503.15264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SynthScarsLEGION</p><br /><br /><p><strong></strong> SynthScars12,236LEGIONLEGIONSynthScarsmIoU3.31%F17.75%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15264" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:37:21 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13360</link>
<guid>https://arxiv.org/abs/2503.13360</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 2%TVCTVC3.4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13360" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:45:12 GMT</pubDate>
</item>
<item>
<title>-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation</title>
<link>https://arxiv.org/abs/2503.13288</link>
<guid>https://arxiv.org/abs/2503.13288</guid>
<content:encoded><![CDATA[
Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:38:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11227</link>
<guid>https://arxiv.org/abs/2503.11227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GKG2915OODOOD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11227" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 05:23:22 GMT</pubDate>
</item>
<item>
<title>TULIPCLIP</title>
<link>https://arxiv.org/abs/2503.15485</link>
<guid>https://arxiv.org/abs/2503.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TULIP</p><br /><br /><p><strong></strong> TULIPCLIP-CLIPSigLIPTULIP/10-shot3SigLIPMMVP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15485" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>3DRoblox</title>
<link>https://arxiv.org/abs/2503.15475</link>
<guid>https://arxiv.org/abs/2503.15475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Roblox3D</p><br /><br /><p><strong></strong> Roblox3DRoblox3D3DLLMs3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15475" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:52:17 GMT</pubDate>
</item>
<item>
<title>FluxFlow: </title>
<link>https://arxiv.org/abs/2503.15417</link>
<guid>https://arxiv.org/abs/2503.15417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FluxFlow</p><br /><br /><p><strong></strong> FluxFlowFluxFlowUCF-101VBenchFluxFlowU-NetDiTAR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15417" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 12:59:32 GMT</pubDate>
</item>
<item>
<title>DeepMesh</title>
<link>https://arxiv.org/abs/2503.15265</link>
<guid>https://arxiv.org/abs/2503.15265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepMesh</p><br /><br /><p><strong></strong> Triangle meshes3DDeepMesh3D3DDeepMesh</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15265" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:39:30 GMT</pubDate>
</item>
<item>
<title>ELTEX</title>
<link>https://arxiv.org/abs/2503.15055</link>
<guid>https://arxiv.org/abs/2503.15055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ELTEX</p><br /><br /><p><strong></strong> ELTEXLLMLLMsELTEXELTEXELTEXGemma-2BELTEXGPT-4</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.15055" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 05:46:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.14868</link>
<guid>https://arxiv.org/abs/2503.14868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 8.2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14868" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 23:45:37 GMT</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ViSpeak</title>
<link>https://arxiv.org/abs/2503.12769</link>
<guid>https://arxiv.org/abs/2503.12769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LMMsViSpeak-InstructViSpeak-BenchViSpeakGPT-4oViSpeak-InstructViSpeak</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12769" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 23:05:31 GMT</pubDate>
</item>
<item>
<title>STEVE</title>
<link>https://arxiv.org/abs/2503.12532</link>
<guid>https://arxiv.org/abs/2503.12532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STEVE</p><br /><br /><p><strong></strong> STEVEGPT-4oKahnemanTverskySTEVESTEVE7B-WinAgentArena</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12532" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:43 GMT</pubDate>
</item>
<item>
<title>PyGDA</title>
<link>https://arxiv.org/abs/2503.10284</link>
<guid>https://arxiv.org/abs/2503.10284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyGDAPython</p><br /><br /><p><strong></strong> PyGDAPython20PyGDAPyGDAAPIPyGDAMIT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10284" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 07:52:23 GMT</pubDate>
</item>
<item>
<title>MeshFleet</title>
<link>https://arxiv.org/abs/2503.14002</link>
<guid>https://arxiv.org/abs/2503.14002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MeshFleet</p><br /><br /><p><strong></strong> MeshFleetObjaverse-XLObjaverseDINOv2SigLIPCaptionCaptionSV3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14002" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 04:09:24 GMT</pubDate>
</item>
<item>
<title>Multi-Scale AttentionAtlas</title>
<link>https://arxiv.org/abs/2503.12355</link>
<guid>https://arxiv.org/abs/2503.12355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Multi-Scale AttentionAtlas</p><br /><br /><p><strong></strong> Multi-Scale Attention, MSAMSAO(log N)MSAAtlasImageNet 1001024pxAtlas-B91.04%ConvNext-B91.92%4.3AtlasFasterViTLongViT2.954.96%MambaVision-SAtlas-S</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12355" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:52:13 GMT</pubDate>
</item>
<item>
<title>AdaLLaVA</title>
<link>https://arxiv.org/abs/2503.10905</link>
<guid>https://arxiv.org/abs/2503.10905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaLLaVA</p><br /><br /><p><strong></strong> MLLMMLLMAdaLLaVAMLLMAdaLLaVAAdaLLaVAMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10905" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 17:39:38 GMT</pubDate>
</item>
<item>
<title>AudioX</title>
<link>https://arxiv.org/abs/2503.10522</link>
<guid>https://arxiv.org/abs/2503.10522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AudioX</p><br /><br /><p><strong></strong> AudioXAudioXvggsound-capsVGGSound19V2M-capsV2M600AudioX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10522" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:30:59 GMT</pubDate>
</item>
<item>
<title>EvalTree: </title>
<link>https://arxiv.org/abs/2503.08893</link>
<guid>https://arxiv.org/abs/2503.08893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvalTree</p><br /><br /><p><strong></strong> EvalTreeMATHWildChatEvalTreeEvalTreeEvalTreeChatbot ArenaEvalTree</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08893" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 17:12:48 GMT</pubDate>
</item>
<item>
<title>CoLMDriver: </title>
<link>https://arxiv.org/abs/2503.08683</link>
<guid>https://arxiv.org/abs/2503.08683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLMDriver  V2V </p><br /><br /><p><strong></strong> CoLMDriver - LLM  InterDrive10 CARLA  V2V CoLMDriver  V2V 11%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08683" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:58:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12303</link>
<guid>https://arxiv.org/abs/2503.12303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SIcog</p><br /><br /><p><strong></strong> MLLMsCoTSIcogMLLMsChain-of-DescriptionSIcogCoTMLLMs213KSIcogMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12303" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 20:25:13 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13661</link>
<guid>https://arxiv.org/abs/2503.13661</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs2,000Pensez 7BAIME2520%MATH 512%LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13661" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:09:11 GMT</pubDate>
</item>
<item>
<title>FlexWorld: 3D</title>
<link>https://arxiv.org/abs/2503.13265</link>
<guid>https://arxiv.org/abs/2503.13265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexWorld3D</p><br /><br /><p><strong></strong> FlexWorld3D360FlexWorldV2V3DV2VFlexWorld3DFlexWorld3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13265" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:18:38 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.13111</link>
<guid>https://arxiv.org/abs/2503.13111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> MLLMs3D3D supervised fine-tuning Cubify Anything VQACA-VQA3DCA-VQAMM-SpatialMLLM3D3DSFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13111" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 08:34:22 GMT</pubDate>
</item>
<item>
<title>-HySAC</title>
<link>https://arxiv.org/abs/2503.12127</link>
<guid>https://arxiv.org/abs/2503.12127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HySAC-</p><br /><br /><p><strong></strong> -CLIPHySACHyperbolic Safety-Aware CLIPHySAC-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12127" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 09:18:04 GMT</pubDate>
</item>
<item>
<title>Florenz: </title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Florenz(VLM)FlorenzVLM Florence-2Gemma-20.4B11.2B-FlorenzFlorenz</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09443" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:41:10 GMT</pubDate>
</item>
<item>
<title>Concat-ID</title>
<link>https://arxiv.org/abs/2503.14151</link>
<guid>https://arxiv.org/abs/2503.14151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Concat-ID</p><br /><br /><p><strong></strong> Concat-ID3DConcat-IDConcat-ID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14151" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.14504</link>
<guid>https://arxiv.org/abs/2503.14504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> (MLLMs)</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14504" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2503.14499</link>
<guid>https://arxiv.org/abs/2503.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">50%- quantifies AI</p><br /><br /><p><strong></strong> AIAI50%-AI50%RE-BenchHCASTAIClaude 3.7 Sonnet50%502019AI2024AIAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14499" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>Temporal Consistency for LLM Reasoning Process Error Identification</title>
<link>https://arxiv.org/abs/2503.14495</link>
<guid>https://arxiv.org/abs/2503.14495</guid>
<content:encoded><![CDATA[
Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:58:28 GMT</pubDate>
</item>
<item>
<title>Cosmos-Transfer: </title>
<link>https://arxiv.org/abs/2503.14492</link>
<guid>https://arxiv.org/abs/2503.14492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cosmos-Transfer</p><br /><br /><p><strong></strong> Cosmos-TransferCosmos-TransferSim2RealSim2RealAINVIDIA GB200 NVL72</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14492" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:54 GMT</pubDate>
</item>
<item>
<title>Creation-MMBench: </title>
<link>https://arxiv.org/abs/2503.14478</link>
<guid>https://arxiv.org/abs/2503.14478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Creation-MMBench </p><br /><br /><p><strong></strong>  Creation-MMBenchMLLMs76551 MLLMs  MLLMs LLMCreation-MMBench  MLLM </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14478" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:51:34 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2503.14476</link>
<guid>https://arxiv.org/abs/2503.14476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DAPORLLLM</p><br /><br /><p><strong></strong> DAPORLQwen2.5-32BAIME 202450LLMRLverlLLM RL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14476" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:49:06 GMT</pubDate>
</item>
<item>
<title>DeepPerception: </title>
<link>https://arxiv.org/abs/2503.12797</link>
<guid>https://arxiv.org/abs/2503.12797</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepPerception</p><br /><br /><p><strong></strong> DeepPerceptionMLLMMLLMKVGDeepPerceptionKVG-Bench101.3KDeepPerceptionKVG-Bench8.08%4.60%MLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12797" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 00:06:34 GMT</pubDate>
</item>
<item>
<title>CapArena</title>
<link>https://arxiv.org/abs/2503.12329</link>
<guid>https://arxiv.org/abs/2503.12329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsVLMsCapArena6000GPT-4oVLM-as-a-JudgeCapArena-Auto94.3%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12329" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 22:56:09 GMT</pubDate>
</item>
<item>
<title>Reflect-DiT</title>
<link>https://arxiv.org/abs/2503.12271</link>
<guid>https://arxiv.org/abs/2503.12271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Reflect-DiT</p><br /><br /><p><strong></strong> Reflect-DiTbest-of-NReflect-DiTDiffusion TransformersReflect-DiTGenEval0.81SANA-1.5-4.8B0.8020</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12271" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 17:58:12 GMT</pubDate>
</item>
<item>
<title>RWKV-7 "Goose" with Expressive Dynamic State Evolution</title>
<link>https://arxiv.org/abs/2503.14456</link>
<guid>https://arxiv.org/abs/2503.14456</guid>
<content:encoded><![CDATA[
We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:31:05 GMT</pubDate>
</item>
<item>
<title>IPV-Bench</title>
<link>https://arxiv.org/abs/2503.14378</link>
<guid>https://arxiv.org/abs/2503.14378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IPV-Bench</p><br /><br /><p><strong></strong> IPV-BenchIPV-Bench414LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14378" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 12:10:24 GMT</pubDate>
</item>
<item>
<title>Frac-Connections</title>
<link>https://arxiv.org/abs/2503.14125</link>
<guid>https://arxiv.org/abs/2503.14125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Frac-Connections</p><br /><br /><p><strong></strong> Frac-Connections7B MoE3Frac-Connections</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.14125" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 06:37:50 GMT</pubDate>
</item>
<item>
<title>Infinite Mobility</title>
<link>https://arxiv.org/abs/2503.13424</link>
<guid>https://arxiv.org/abs/2503.13424</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> Infinite Mobility</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13424" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:53:56 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12545</link>
<guid>https://arxiv.org/abs/2503.12545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PEBench</p><br /><br /><p><strong></strong> MLLMsMUMUMLLMsPEBenchMUMLLMsPEBench6MUMUMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12545" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 11:26:20 GMT</pubDate>
</item>
<item>
<title>MPBench</title>
<link>https://arxiv.org/abs/2503.12505</link>
<guid>https://arxiv.org/abs/2503.12505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPBench</p><br /><br /><p><strong></strong> LLMsPRMsLLMsPRMsMPBenchPRMsMPBenchPRMsMPBenchPRMsPRMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12505" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 09:50:38 GMT</pubDate>
</item>
<item>
<title>KUDA</title>
<link>https://arxiv.org/abs/2503.10546</link>
<guid>https://arxiv.org/abs/2503.10546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KUDA</p><br /><br /><p><strong></strong> LLMs-VLMsKUDAVLMsKUDARGBVLMKUDA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10546" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:59:17 GMT</pubDate>
</item>
<item>
<title>RoCo-Sim</title>
<link>https://arxiv.org/abs/2503.10410</link>
<guid>https://arxiv.org/abs/2503.10410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoCo-Sim</p><br /><br /><p><strong></strong> RoCo-SimRoCo-SimMOASSAM3D83.7483.12AP70RoCo-Sim</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10410" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:33:42 GMT</pubDate>
</item>
<item>
<title>Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models</title>
<link>https://arxiv.org/abs/2503.06269</link>
<guid>https://arxiv.org/abs/2503.06269</guid>
<content:encoded><![CDATA[
Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:29:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12964</link>
<guid>https://arxiv.org/abs/2503.12964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VFMVFMsVFMNVIDIA NeMoVFM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12964" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:19:12 GMT</pubDate>
</item>
<item>
<title>GenStereo</title>
<link>https://arxiv.org/abs/2503.12720</link>
<guid>https://arxiv.org/abs/2503.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenStereo</p><br /><br /><p><strong></strong> GenStereo11GenStereo</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12720" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 21:19:28 GMT</pubDate>
</item>
<item>
<title>WISA: </title>
<link>https://arxiv.org/abs/2503.08153</link>
<guid>https://arxiv.org/abs/2503.08153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISA</p><br /><br /><p><strong></strong> WISAT2VWISAWISA-32K32,00017WISAT2VVideoPhy</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08153" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 04:10:03 GMT</pubDate>
</item>
<item>
<title>SPIN-Bench: </title>
<link>https://arxiv.org/abs/2503.12349</link>
<guid>https://arxiv.org/abs/2503.12349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIN-BenchAI</p><br /><br /><p><strong></strong> SPIN-BenchAISPIN-BenchPDDLAISPIN-Bench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12349" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:10:53 GMT</pubDate>
</item>
<item>
<title>Sightation</title>
<link>https://arxiv.org/abs/2503.13369</link>
<guid>https://arxiv.org/abs/2503.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sightation</p><br /><br /><p><strong></strong> BLVBLVVLMBLVSightation5000137,000</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13369" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:52:46 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13082</link>
<guid>https://arxiv.org/abs/2503.13082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">-</p><br /><br /><p><strong></strong> -VLMsFreeGraspVLMsFreeGraspGPT-4oFreeGraspDataMetaGraspNetV2FreeGrasp</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13082" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:41:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12605</link>
<guid>https://arxiv.org/abs/2503.12605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MCoTMLLMs3DMCoTMCoTAGI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12605" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 14:39:13 GMT</pubDate>
</item>
<item>
<title>LVAS-Agent: </title>
<link>https://arxiv.org/abs/2503.10719</link>
<guid>https://arxiv.org/abs/2503.10719</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVAS-Agent </p><br /><br /><p><strong></strong> LVAS-Agent/-LVAS-Bench207LVAS-Agent</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10719" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 03:58:23 GMT</pubDate>
</item>
<item>
<title>BlobCtrl</title>
<link>https://arxiv.org/abs/2503.13434</link>
<guid>https://arxiv.org/abs/2503.13434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlobCtrlBlob</p><br /><br /><p><strong></strong> BlobCtrlBlobBlob1) 2) 3) dropoutBlobDataBlobBenchBlobCtrl</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13434" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.06053</link>
<guid>https://arxiv.org/abs/2503.06053</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DropletVideo-10M1000206DropletVideoDropletVideo</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06053" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 23:37:38 GMT</pubDate>
</item>
<item>
<title>VideoMind</title>
<link>https://arxiv.org/abs/2503.13444</link>
<guid>https://arxiv.org/abs/2503.13444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoMind</p><br /><br /><p><strong></strong> VideoMindVideoMindChain-of-LoRALoRA14VideoMind365</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13444" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>WideRange4D: 4D</title>
<link>https://arxiv.org/abs/2503.13435</link>
<guid>https://arxiv.org/abs/2503.13435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4DWideRange4DProgress4D</p><br /><br /><p><strong></strong> 4D4DWideRange4D4D4D4D4DProgress4D4DWideRange4DProgress4D4D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13435" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>MicroVQA</title>
<link>https://arxiv.org/abs/2503.13399</link>
<guid>https://arxiv.org/abs/2503.13399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MicroVQA</p><br /><br /><p><strong></strong> MicroVQAVQA1,042MLLMsMicroVQA53%LLMsMicroVQAAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13399" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:33:10 GMT</pubDate>
</item>
<item>
<title>Edit Transfer: </title>
<link>https://arxiv.org/abs/2503.13327</link>
<guid>https://arxiv.org/abs/2503.13327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Edit Transfer</p><br /><br /><p><strong></strong> Edit Transfer-Edit Transfer-DiTLoRA42Edit TransferTIERIE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13327" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:04:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.13070</link>
<guid>https://arxiv.org/abs/2503.13070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R0</p><br /><br /><p><strong></strong> (AIGC)R0R0</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.13070" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:21:43 GMT</pubDate>
</item>
<item>
<title>Step-wise Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2503.12937</link>
<guid>https://arxiv.org/abs/2503.12937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> StepGRPOMLLMsStepGRPOStepRARStepRVRStepRARStepRVRStepGRPOMLLMsR1-VL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12937" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 04:51:44 GMT</pubDate>
</item>
<item>
<title>DreamRenderer</title>
<link>https://arxiv.org/abs/2503.12885</link>
<guid>https://arxiv.org/abs/2503.12885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRenderer</p><br /><br /><p><strong></strong> DreamRendererFLUXT5FLUXDreamRendererCOCO-POSCOCO-MIGFLUX17.7%GLIGEN3DIS26.8%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12885" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 03:30:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12590</link>
<guid>https://arxiv.org/abs/2503.12590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">-free</p><br /><br /><p><strong></strong> (DiTs)Personalize Anything</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12590" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>Being-0: </title>
<link>https://arxiv.org/abs/2503.12533</link>
<guid>https://arxiv.org/abs/2503.12533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-0</p><br /><br /><p><strong></strong> Being-0FMFMBeing-0Connector-VLMBeing-0</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12533" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12530</link>
<guid>https://arxiv.org/abs/2503.12530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMsLlama 3.2 Vision Instruct11BMolmo 7B-D-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12530" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:50:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.12528</link>
<guid>https://arxiv.org/abs/2503.12528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> top-k</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.12528" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:45:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11751</link>
<guid>https://arxiv.org/abs/2503.11751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> reWordBench59%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11751" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>V-STaR</title>
<link>https://arxiv.org/abs/2503.11495</link>
<guid>https://arxiv.org/abs/2503.11495</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">V-STaR</p><br /><br /><p><strong></strong> Video-LLMsV-STaRRSTRGPT-4</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11495" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:21:44 GMT</pubDate>
</item>
<item>
<title>MTV-Inpaint</title>
<link>https://arxiv.org/abs/2503.11412</link>
<guid>https://arxiv.org/abs/2503.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MTV-Inpaint</p><br /><br /><p><strong></strong> (T2V)MTV-InpaintMTV-InpaintMTV-Inpaint(I2V)MTV-InpaintMTV-Inpaint</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11412" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 09:54:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10704</link>
<guid>https://arxiv.org/abs/2503.10704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ARVDMMeta-ARVDMMeta-ARVDMKLARVDMDMLabMinecraftPareto</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10704" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 11:32:44 GMT</pubDate>
</item>
<item>
<title>CHOrD3D</title>
<link>https://arxiv.org/abs/2503.11958</link>
<guid>https://arxiv.org/abs/2503.11958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CHOrD3D</p><br /><br /><p><strong></strong> CHOrD3DCHOrD2DCHOrD3D-FRONTCHOrD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11958" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 22:05:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11207</link>
<guid>https://arxiv.org/abs/2503.11207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LRMsOpenAIo3-miniDeepSeek R1RavenI-RAVENI-RAVEN-XI-RAVEN-XOpenAIo3-miniI-RAVEN86.6%I-RAVEN-X17.0%DeepSeek R180.6%23.2%ARLC98.6%88.0%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11207" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 04:52:25 GMT</pubDate>
</item>
<item>
<title>VGGT3D</title>
<link>https://arxiv.org/abs/2503.11651</link>
<guid>https://arxiv.org/abs/2503.11651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VGGT3D3D</p><br /><br /><p><strong></strong> VGGT3D3D3DVGGT3D3DVGGT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11651" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>TreeMeshGPT</title>
<link>https://arxiv.org/abs/2503.11629</link>
<guid>https://arxiv.org/abs/2503.11629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TreeMeshGPT</p><br /><br /><p><strong></strong> TreeMeshGPTTreeMeshGPT22%TreeMeshGPTTreeMeshGPT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11629" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:48:06 GMT</pubDate>
</item>
<item>
<title>VAMBA</title>
<link>https://arxiv.org/abs/2503.11579</link>
<guid>https://arxiv.org/abs/2503.11579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VAMBA</p><br /><br /><p><strong></strong> VAMBAMamba-TransformerMamba-2VAMBATokenGPU1024640x360256VAMBAGPU50%VAMBALVBench4.3%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11579" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:45:23 GMT</pubDate>
</item>
<item>
<title>SmolDocling</title>
<link>https://arxiv.org/abs/2503.11576</link>
<guid>https://arxiv.org/abs/2503.11576</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SmolDocling</p><br /><br /><p><strong></strong> SmolDoclingDocTagsSmolDocling256MSmolDocling27</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11576" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:44:14 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10620</link>
<guid>https://arxiv.org/abs/2503.10620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsTOWERSPIRETOWER</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10620" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:57:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09330</link>
<guid>https://arxiv.org/abs/2503.09330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MIUMIUMIU</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09330" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 08:24:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10684</link>
<guid>https://arxiv.org/abs/2503.10684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SBDSBDMinecraftSBD63.7%52.1%11.3%20.8%YouTube</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10684" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 14:51:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10781</link>
<guid>https://arxiv.org/abs/2503.10781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> HowToGround1MGROVEiGround3500iGroundVidSTGActivityNet-Entities</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10781" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 14:21:07 GMT</pubDate>
</item>
<item>
<title>ETC</title>
<link>https://arxiv.org/abs/2503.10624</link>
<guid>https://arxiv.org/abs/2503.10624</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ETCSE(3)</p><br /><br /><p><strong></strong> ETCHSE(3)CAPE4D-DressETCH16.7%69.5%49.9%67.2%89.8%ETCH</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10624" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10696</link>
<guid>https://arxiv.org/abs/2503.10696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> NARNAR-NARImageNetUCF101NAR2.48.6PAR-4XFID/FVDGenEval0.8BNAR0.4Chameleon-7B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10696" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 01:52:27 GMT</pubDate>
</item>
<item>
<title>MaRI3D</title>
<link>https://arxiv.org/abs/2503.08111</link>
<guid>https://arxiv.org/abs/2503.08111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaRI</p><br /><br /><p><strong></strong> 3DMaRIMaRIMaRI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08111" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:23:11 GMT</pubDate>
</item>
<item>
<title>ProJudgeBench</title>
<link>https://arxiv.org/abs/2503.06553</link>
<guid>https://arxiv.org/abs/2503.06553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProJudgeBench</p><br /><br /><p><strong></strong> MLLMsProJudgeBenchMLLMs240050118ProJudge-173k</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06553" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 06:55:51 GMT</pubDate>
</item>
<item>
<title>ARMOR</title>
<link>https://arxiv.org/abs/2503.06542</link>
<guid>https://arxiv.org/abs/2503.06542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARMOR</p><br /><br /><p><strong></strong> ARMORMLLMs-ARMORMLLMsARMORMLLMsARMORMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06542" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 06:15:39 GMT</pubDate>
</item>
<item>
<title>ReCamMaster</title>
<link>https://arxiv.org/abs/2503.11647</link>
<guid>https://arxiv.org/abs/2503.11647</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReCamMaster</p><br /><br /><p><strong></strong> ReCamMasterUnreal Engine 5ReCamMaster</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11647" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11646</link>
<guid>https://arxiv.org/abs/2503.11646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ADCADCADCADC20%ADC-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11646" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11224</link>
<guid>https://arxiv.org/abs/2503.11224</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SSMsSSMsSSMSSMSSMS4SSMMambaSSMSSM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11224" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 05:20:31 GMT</pubDate>
</item>
<item>
<title>APIGUI</title>
<link>https://arxiv.org/abs/2503.11069</link>
<guid>https://arxiv.org/abs/2503.11069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">APIGUI</p><br /><br /><p><strong></strong> APIGUIAPIGUI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11069" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 00:26:21 GMT</pubDate>
</item>
<item>
<title>TxAgentAI</title>
<link>https://arxiv.org/abs/2503.10970</link>
<guid>https://arxiv.org/abs/2503.10970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TxAgentAI</p><br /><br /><p><strong></strong> TxAgentAI211TxAgent92.1%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10970" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 20:28:15 GMT</pubDate>
</item>
<item>
<title>FlowTok</title>
<link>https://arxiv.org/abs/2503.10772</link>
<guid>https://arxiv.org/abs/2503.10772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlowTok</p><br /><br /><p><strong></strong> FlowTokFlowTok1D3.3FlowTok</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10772" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 14:06:13 GMT</pubDate>
</item>
<item>
<title>Kolmogorov-ArnoldTransformer</title>
<link>https://arxiv.org/abs/2503.10632</link>
<guid>https://arxiv.org/abs/2503.10632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kolmogorov-ArnoldTransformer</p><br /><br /><p><strong></strong> Kolmogorov-ArnoldKArAtMLPTransformerViTKolmogorov-Arnold-KArAtCIFAR-10CIFAR-100ImageNet-1KViTViTsKANs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10632" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.11514</link>
<guid>https://arxiv.org/abs/2503.11514</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GIAGIAGIAOP-GIAGEN-GIAANA-GIAGIAOP-GIAGEN-GIAANA-GIA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.11514" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 04:08:44 GMT</pubDate>
</item>
<item>
<title>Cockatiel</title>
<link>https://arxiv.org/abs/2503.09279</link>
<guid>https://arxiv.org/abs/2503.09279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cockatiel</p><br /><br /><p><strong></strong> (VDC)CockatielVDC-Cockatiel-13BCockatiel-13BCockatiel-8BVDCSCORE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09279" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 07:25:04 GMT</pubDate>
</item>
<item>
<title>PLADIS</title>
<link>https://arxiv.org/abs/2503.07677</link>
<guid>https://arxiv.org/abs/2503.07677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PLADIS</p><br /><br /><p><strong></strong> PLADISU-NetTransformerNFEPLADIS-PLADISPLADIS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07677" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:23:19 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.06674</link>
<guid>https://arxiv.org/abs/2503.06674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TDM</p><br /><br /><p><strong></strong> TDMTDMPixArt-alpha40.01%TDMAIGC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06674" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 11:53:49 GMT</pubDate>
</item>
<item>
<title>GoalFlow</title>
<link>https://arxiv.org/abs/2503.05689</link>
<guid>https://arxiv.org/abs/2503.05689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GoalFlow</p><br /><br /><p><strong></strong> GoalFlowGoalFlowGoalFlowFlow MatchingGoalFlowNavsimDauner2024_navsimPDMS90.3GoalFlow</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05689" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:52:08 GMT</pubDate>
</item>
<item>
<title>PoseLess: </title>
<link>https://arxiv.org/abs/2503.07111</link>
<guid>https://arxiv.org/abs/2503.07111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PoseLess2D</p><br /><br /><p><strong></strong> PoseLess2D-shotPoseLessPoseLess</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07111" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:34:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10638</link>
<guid>https://arxiv.org/abs/2503.10638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10638" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10635</link>
<guid>https://arxiv.org/abs/2503.10635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVLM</p><br /><br /><p><strong></strong> LVLMLVLMLVLMGPT-4.5Claude-3.7-sonnet90%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10635" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>ConsisLoRA: </title>
<link>https://arxiv.org/abs/2503.10614</link>
<guid>https://arxiv.org/abs/2503.10614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ConsisLoRA</p><br /><br /><p><strong></strong> ConsisLoRALoRALoRAConsisLoRA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10614" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:55:58 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10602</link>
<guid>https://arxiv.org/abs/2503.10602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LVLMsOHLVLM token LVLMTruthPrIntLVLMComnHalluLVLMTruthPrIntLVLMsOH</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10602" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:46:06 GMT</pubDate>
</item>
<item>
<title>IP-PriorLoRA</title>
<link>https://arxiv.org/abs/2503.10365</link>
<guid>https://arxiv.org/abs/2503.10365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> IP-Adapter+IP-PriorLoRAIP-Adapter+</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10365" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:46:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10242</link>
<guid>https://arxiv.org/abs/2503.10242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsAILLMMinorBenchLLMLLMAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10242" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 06:34:43 GMT</pubDate>
</item>
<item>
<title>DiLoCo</title>
<link>https://arxiv.org/abs/2503.09799</link>
<guid>https://arxiv.org/abs/2503.09799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiLoCo</p><br /><br /><p><strong></strong> DiLoCoDiLoCoDiLoCoDiLoCo</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09799" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 16:04:38 GMT</pubDate>
</item>
<item>
<title>Transformer</title>
<link>https://arxiv.org/abs/2503.09046</link>
<guid>https://arxiv.org/abs/2503.09046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Transformer</p><br /><br /><p><strong></strong> TransformerTransformer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09046" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 00:10:46 GMT</pubDate>
</item>
<item>
<title>OmniPaint</title>
<link>https://arxiv.org/abs/2503.08677</link>
<guid>https://arxiv.org/abs/2503.08677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniPaint</p><br /><br /><p><strong></strong> Diffusion-basedOmniPaintCFD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08677" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:55:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10357</link>
<guid>https://arxiv.org/abs/2503.10357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> WordNet129GPT-4Playground-v2FLUX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10357" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:37:54 GMT</pubDate>
</item>
<item>
<title>VisualPRM</title>
<link>https://arxiv.org/abs/2503.10291</link>
<guid>https://arxiv.org/abs/2503.10291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualPRM</p><br /><br /><p><strong></strong> VisualPRM80MLLMsMLLMsInternVL2.5-78B5.9VisualPRMBest-of-NBoNVisualPRM400KVisualProcessBenchPRMsMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10291" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 08:03:37 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09837</link>
<guid>https://arxiv.org/abs/2503.09837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMsOpenAICLIPGoogleSigLIP/Flickr8k</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09837" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 16:58:16 GMT</pubDate>
</item>
<item>
<title>CoRe: </title>
<link>https://arxiv.org/abs/2503.09662</link>
<guid>https://arxiv.org/abs/2503.09662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoRe </p><br /><br /><p><strong></strong>  CoReCoRe CollectReflect  RefineCoRe  Z-Sampling </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09662" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 11:15:25 GMT</pubDate>
</item>
<item>
<title>PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling</title>
<link>https://arxiv.org/abs/2503.09368</link>
<guid>https://arxiv.org/abs/2503.09368</guid>
<content:encoded><![CDATA[
We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 09:14:51 GMT</pubDate>
</item>
<item>
<title>Hugging Face</title>
<link>https://arxiv.org/abs/2503.10633</link>
<guid>https://arxiv.org/abs/2503.10633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hugging Face</p><br /><br /><p><strong></strong> Hugging Face</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10633" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>Tanh</title>
<link>https://arxiv.org/abs/2503.10622</link>
<guid>https://arxiv.org/abs/2503.10622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tanh</p><br /><br /><p><strong></strong> TanhDyTDyTDyT(x) = tanh(alpha x)SDyT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10622" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>Diffusion Transformers</title>
<link>https://arxiv.org/abs/2503.10618</link>
<guid>https://arxiv.org/abs/2503.10618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diffusion Transformers</p><br /><br /><p><strong></strong> Diffusion TransformersDiTsDiTPixArtMMDiTDiTDiTMMDiT66%DiT-AirDiT-Air-LiteDiT-AirGenEvalT2I CompBenchDiT-Air-Lite</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10618" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:57:25 GMT</pubDate>
</item>
<item>
<title>SANA-Sprint</title>
<link>https://arxiv.org/abs/2503.09641</link>
<guid>https://arxiv.org/abs/2503.09641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SANA-Sprint</p><br /><br /><p><strong></strong> SANA-SprintT2I201-4SANA-Sprint1-4SANA-SprintControlNetSANA-SprintPareto17.59FID0.74GenEvalFLUX-schnell10H1001024 x 10240.1T2I0.25ControlNetRTX 40900.31AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09641" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 00:53:07 GMT</pubDate>
</item>
<item>
<title>Generation Chain-of-Thought: </title>
<link>https://arxiv.org/abs/2503.10639</link>
<guid>https://arxiv.org/abs/2503.10639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GoT</p><br /><br /><p><strong></strong> Generation Chain-of-ThoughtGoTGoT900-Qwen2.5-VLGoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10639" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10637</link>
<guid>https://arxiv.org/abs/2503.10637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Concept SlidersLoRAsDiffusion Target (DT) DT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10637" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation</title>
<link>https://arxiv.org/abs/2503.10636</link>
<guid>https://arxiv.org/abs/2503.10636</guid>
<content:encoded><![CDATA[
Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10630</link>
<guid>https://arxiv.org/abs/2503.10630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> UniGoal</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10630" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>R1-Onevision</title>
<link>https://arxiv.org/abs/2503.10615</link>
<guid>https://arxiv.org/abs/2503.10615</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-Onevision</p><br /><br /><p><strong></strong> R1-OnevisionR1-OnevisionR1-OnevisionR1-OnevisionR1-Onevision-BenchR1-OnevisionGPT-4oQwen2.5-VL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10615" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:56:05 GMT</pubDate>
</item>
<item>
<title>LLMsCoSTA*</title>
<link>https://arxiv.org/abs/2503.10613</link>
<guid>https://arxiv.org/abs/2503.10613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSTA*LLMs</p><br /><br /><p><strong></strong> CoSTA*LLMsAIA*CoSTA*-CoSTA*</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10613" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:55:45 GMT</pubDate>
</item>
<item>
<title>GroundingSuite</title>
<link>https://arxiv.org/abs/2503.10596</link>
<guid>https://arxiv.org/abs/2503.10596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GroundingSuite</p><br /><br /><p><strong></strong> Pixel groundingRESGroundingSuite1-VLM295633,800GroundingSuitegRefCOCO68.9cIoURefCOCOm55.3gIoUGroundingSuiteGLaMM4.5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10596" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:43:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10589</link>
<guid>https://arxiv.org/abs/2503.10589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LCTLCTKVLCT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10589" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:40:07 GMT</pubDate>
</item>
<item>
<title>VisualWebInstruct</title>
<link>https://arxiv.org/abs/2503.10582</link>
<guid>https://arxiv.org/abs/2503.10582</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualWebInstruct</p><br /><br /><p><strong></strong> VLMsVisualWebInstruct3000070URLHTML9040%VisualWebInstructMAmmoTH-VL210BMMM-pro-std40.7%MathVerse42.6%DynaMath55.7%VLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10582" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:32:48 GMT</pubDate>
</item>
<item>
<title>ARPG</title>
<link>https://arxiv.org/abs/2503.10568</link>
<guid>https://arxiv.org/abs/2503.10568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARPG-shot</p><br /><br /><p><strong></strong> ARPG-shotARPGARPG-shotKVImageNet-1K 256ARPG641.94FID2075%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10568" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:19:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10480</link>
<guid>https://arxiv.org/abs/2503.10480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LVLMsD^2POLVLMsVoTa-BenchD^2POGPT-4oQwen2-VL7BLLaVA-1.67BLLaMA-3.211B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10480" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 11:49:56 GMT</pubDate>
</item>
<item>
<title>Light-R1</title>
<link>https://arxiv.org/abs/2503.10460</link>
<guid>https://arxiv.org/abs/2503.10460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-R1</p><br /><br /><p><strong></strong> Light-R1COTCOTSFTDPOLight-R1-32BDeepSeek-R1-Distill-Qwen-32BLight-R1-32BSFT3kDeepSeek-R1-Distilled7B14BSOTAGRPOCOTLight-R1-14B-DS14BSOTAAIME242574.060.232BCOTSFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10460" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 11:29:22 GMT</pubDate>
</item>
<item>
<title>4D LangSplat</title>
<link>https://arxiv.org/abs/2503.10437</link>
<guid>https://arxiv.org/abs/2503.10437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4D LangSplat</p><br /><br /><p><strong></strong> 4D LangSplat4DCLIP4D LangSplat4D LangSplat</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10437" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:58:22 GMT</pubDate>
</item>
<item>
<title>CINEMA</title>
<link>https://arxiv.org/abs/2503.10391</link>
<guid>https://arxiv.org/abs/2503.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CINEMA</p><br /><br /><p><strong></strong> CINEMAMLLMMLLMCINEMA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10391" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:07:58 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10351</link>
<guid>https://arxiv.org/abs/2503.10351</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LRMsCoTMTLRMsMTLLMsMTLRMs1) 2) 3) LRMsLRMsLRMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10351" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:27:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.10072</link>
<guid>https://arxiv.org/abs/2503.10072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GitHub203</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.10072" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 01:39:29 GMT</pubDate>
</item>
<item>
<title>Whisper</title>
<link>https://arxiv.org/abs/2503.09905</link>
<guid>https://arxiv.org/abs/2503.09905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Whisper</p><br /><br /><p><strong></strong> (ASR)WhisperWhisperLibriSpeechwhispercpp(INT4INT5INT8)(WER)19%45%WhisperGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09905" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 19:50:35 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09669</link>
<guid>https://arxiv.org/abs/2503.09669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09669" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:21:57 GMT</pubDate>
</item>
<item>
<title>Open-Sora 2.0</title>
<link>https://arxiv.org/abs/2503.09642</link>
<guid>https://arxiv.org/abs/2503.09642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Open-Sora 2.0</p><br /><br /><p><strong></strong> Open-Sora 2.020VBenchOpen-Sora 2.0HunyuanVideoRunway Gen-3 AlphaOpen-Sora 2.0GitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09642" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 01:00:07 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04723</link>
<guid>https://arxiv.org/abs/2503.04723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsNLPLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04723" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 13:59:37 GMT</pubDate>
</item>
<item>
<title>Search-R1</title>
<link>https://arxiv.org/abs/2503.09516</link>
<guid>https://arxiv.org/abs/2503.09516</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Search-R1LLM</p><br /><br /><p><strong></strong> Search-R1DeepSeek-R1LLMsSearch-R1Search-R126%Qwen2.5-7B21%Qwen2.5-3B10%LLaMA3.2-3BLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09516" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 12:26:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08674</link>
<guid>https://arxiv.org/abs/2503.08674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MLFFsMLFFsMLFFsMLFFsMLFFs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08674" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:54:29 GMT</pubDate>
</item>
<item>
<title>PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?</title>
<link>https://arxiv.org/abs/2503.05333</link>
<guid>https://arxiv.org/abs/2503.05333</guid>
<content:encoded><![CDATA[
The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 06:19:13 GMT</pubDate>
</item>
<item>
<title>BIMBA</title>
<link>https://arxiv.org/abs/2503.09590</link>
<guid>https://arxiv.org/abs/2503.09590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BIMBA</p><br /><br /><p><strong></strong> VQABIMBALLMBIMBA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09590" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:57:32 GMT</pubDate>
</item>
<item>
<title>RANSAC</title>
<link>https://arxiv.org/abs/2503.09410</link>
<guid>https://arxiv.org/abs/2503.09410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RANSAC</p><br /><br /><p><strong></strong> RANSACRANSACScanNetMegaDepthRANSAC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09410" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:01:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08681</link>
<guid>https://arxiv.org/abs/2503.08681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STaSC</p><br /><br /><p><strong></strong> STaSCSTaSC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08681" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:57:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05397</link>
<guid>https://arxiv.org/abs/2503.05397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LAMsQwen Code Instruct 2.5 7BRougeL85.596.5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05397" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 08:20:12 GMT</pubDate>
</item>
<item>
<title>RewardSDS: </title>
<link>https://arxiv.org/abs/2503.09601</link>
<guid>https://arxiv.org/abs/2503.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RewardSDS2D</p><br /><br /><p><strong></strong> Score Distillation SamplingSDS2D3DRewardSDSSDSSDSRewardVSD-2D-3DRewardSDSRewardVSDSDSVSD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09601" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09600</link>
<guid>https://arxiv.org/abs/2503.09600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAG</p><br /><br /><p><strong></strong> RAGLLMLLMMoCMoCRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09600" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09579</link>
<guid>https://arxiv.org/abs/2503.09579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09579" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:50:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08525</link>
<guid>https://arxiv.org/abs/2503.08525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GTR</p><br /><br /><p><strong></strong> RLVRVLMALFWorldVLMGTRGTRLLaVA-7b3-5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08525" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 11:17:02 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07588</link>
<guid>https://arxiv.org/abs/2503.07588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RSIsDIPRFMDIPLRS-VQA7333QA27328</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07588" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2503.07103</link>
<guid>https://arxiv.org/abs/2503.07103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> (LLM)16BLLM32834BLLM2470%32</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07103" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:26:08 GMT</pubDate>
</item>
<item>
<title>WildIFEval: </title>
<link>https://arxiv.org/abs/2503.06573</link>
<guid>https://arxiv.org/abs/2503.06573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WildIFEval12000LLM</p><br /><br /><p><strong></strong> WildIFEval12000WildIFEvalLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06573" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:06:29 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04388</link>
<guid>https://arxiv.org/abs/2503.04388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAGLLM</p><br /><br /><p><strong></strong> RAGLLMRAGLLMGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04388" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:38:17 GMT</pubDate>
</item>
<item>
<title>TPDiff: </title>
<link>https://arxiv.org/abs/2503.09566</link>
<guid>https://arxiv.org/abs/2503.09566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPDiff</p><br /><br /><p><strong></strong> TPDiffTPDiffODE50%1.5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09566" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:33:22 GMT</pubDate>
</item>
<item>
<title>AF-LDM</title>
<link>https://arxiv.org/abs/2503.09419</link>
<guid>https://arxiv.org/abs/2503.09419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LDM</p><br /><br /><p><strong></strong> LDMsLDMLDMsVAEU-NetLDMAF-LDMAF-LDMLDM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09419" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:16:30 GMT</pubDate>
</item>
<item>
<title>VLog: </title>
<link>https://arxiv.org/abs/2503.09402</link>
<guid>https://arxiv.org/abs/2503.09402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLog</p><br /><br /><p><strong></strong> VLogVLogGPT-2narration pair encodingEgoSchemaCOINHiRESTVLog</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09402" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 09:53:30 GMT</pubDate>
</item>
<item>
<title>Reangle-A-Video</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Reangle-A-Video</p><br /><br /><p><strong></strong> Reangle-A-Video4DDUSt3RReangle-A-Video</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09151" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 04:26:15 GMT</pubDate>
</item>
<item>
<title>Motion Anything</title>
<link>https://arxiv.org/abs/2503.06955</link>
<guid>https://arxiv.org/abs/2503.06955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Motion Anything</p><br /><br /><p><strong></strong> Conditional motion generationMotion AnythingText-Music-Dance (TMD)2153AIST++Motion AnythingHumanML3DFID15%AIST++TMD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06955" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:04:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.09573</link>
<guid>https://arxiv.org/abs/2503.09573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> KV</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09573" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:43:40 GMT</pubDate>
</item>
<item>
<title>scMMGPT</title>
<link>https://arxiv.org/abs/2503.09427</link>
<guid>https://arxiv.org/abs/2503.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">scMMGPT</p><br /><br /><p><strong></strong> PLMsPLMsRNAPLMsscMMGPTPLMsscMMGPT270084%20.5%k-NN4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.09427" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:26:16 GMT</pubDate>
</item>
<item>
<title>PlainQAFact</title>
<link>https://arxiv.org/abs/2503.08890</link>
<guid>https://arxiv.org/abs/2503.08890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlainQAFact </p><br /><br /><p><strong></strong> PlainQAFactPLSPLSPlainQAFactPlainFactPlainQAFactPLS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08890" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 16:59:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07565</link>
<guid>https://arxiv.org/abs/2503.07565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">(IMM)</p><br /><br /><p><strong></strong> (Inductive Moment Matching, IMM)IMMIMMImageNet-256x2561.99FID8CIFAR-10IMM1.982FID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07565" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:37:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07154</link>
<guid>https://arxiv.org/abs/2503.07154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> IMM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07154" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 06:27:30 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05066</link>
<guid>https://arxiv.org/abs/2503.05066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MoEMixtral-8times7B-Instruct0.2%1.94</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05066" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 20:11:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05037</link>
<guid>https://arxiv.org/abs/2503.05037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RAGRe-DocREDDragon+Contriever3%RAGLLMs34%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05037" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 18:23:13 GMT</pubDate>
</item>
<item>
<title>OTTER--</title>
<link>https://arxiv.org/abs/2503.03734</link>
<guid>https://arxiv.org/abs/2503.03734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OTTER</p><br /><br /><p><strong></strong> --VLAOTTERVLAOTTER-OTTERVLA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03734" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 13:44:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08644</link>
<guid>https://arxiv.org/abs/2503.08644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> NV-EmbedLLM2Vec50%LLM2Vec61.35%LLMLlama3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08644" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:36:53 GMT</pubDate>
</item>
<item>
<title>ObjectMover</title>
<link>https://arxiv.org/abs/2503.08037</link>
<guid>https://arxiv.org/abs/2503.08037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectMover</p><br /><br /><p><strong></strong> ObjectMoverObjectMover</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08037" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 00:42:59 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07587</link>
<guid>https://arxiv.org/abs/2503.07587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Robusto-1VQAVLMsRSA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07587" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:50:04 GMT</pubDate>
</item>
<item>
<title>CineBrainEEGfMRI</title>
<link>https://arxiv.org/abs/2503.06940</link>
<guid>https://arxiv.org/abs/2503.06940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineBrainEEGfMRI</p><br /><br /><p><strong></strong> CineBrainEEGfMRIEEGfMRICineBrainCineSyncEEGfMRICine-BenchmarkCineSyncfMRIEEG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06940" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 01:39:43 GMT</pubDate>
</item>
<item>
<title>PLM</title>
<link>https://arxiv.org/abs/2503.08684</link>
<guid>https://arxiv.org/abs/2503.08684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PLM</p><br /><br /><p><strong></strong> PLMPLMCDCCDC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08684" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:00 GMT</pubDate>
</item>
<item>
<title>AnyMoLe</title>
<link>https://arxiv.org/abs/2503.08417</link>
<guid>https://arxiv.org/abs/2503.08417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyMoLe</p><br /><br /><p><strong></strong> AnyMoLeAnyMoLeICAdapt2D3DAnyMoLe</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08417" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 09:28:59 GMT</pubDate>
</item>
<item>
<title>RexSeekHumanRef</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RexSeek</p><br /><br /><p><strong></strong> HumanRefRexSeekRefCOCOHumanRefRexSeek</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08507" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:57:14 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08478</link>
<guid>https://arxiv.org/abs/2503.08478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> -free</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08478" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:29:37 GMT</pubDate>
</item>
<item>
<title>Transformer</title>
<link>https://arxiv.org/abs/2503.08307</link>
<guid>https://arxiv.org/abs/2503.08307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Transformer</p><br /><br /><p><strong></strong> TransformerAIAVGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08307" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 07:18:47 GMT</pubDate>
</item>
<item>
<title>SECOND ME</title>
<link>https://arxiv.org/abs/2503.08102</link>
<guid>https://arxiv.org/abs/2503.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SECOND ME</p><br /><br /><p><strong></strong> SECOND ME(LLMs)AISECOND MESECOND MELLMAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08102" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:05:52 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.06594</link>
<guid>https://arxiv.org/abs/2503.06594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsNMTNMTLLMsWMT2.46.5KV75%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06594" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:54:05 GMT</pubDate>
</item>
<item>
<title>VisualSimpleQA</title>
<link>https://arxiv.org/abs/2503.06492</link>
<guid>https://arxiv.org/abs/2503.06492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualSimpleQA</p><br /><br /><p><strong></strong> LVLMsVisualSimpleQALVLMsVisualSimpleQA-hard15LVLMGPT-4oVisualSimpleQA60%VisualSimpleQA-hard30%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06492" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 03:25:32 GMT</pubDate>
</item>
<item>
<title>MagicInfinite</title>
<link>https://arxiv.org/abs/2503.05978</link>
<guid>https://arxiv.org/abs/2503.05978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicInfinite</p><br /><br /><p><strong></strong> MagicInfiniteTransformer13D23cfg20MagicInfinite</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05978" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 18:21:11 GMT</pubDate>
</item>
<item>
<title>AI4SEBenchScoutBenchFrame</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI4SEBenchScoutBenchFrame</p><br /><br /><p><strong></strong> 173204AI4SEBenchScoutBenchFrameHumanEvalHumanEvalNextHumanEvalNext</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05860" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>QuoTA</title>
<link>https://arxiv.org/abs/2503.08689</link>
<guid>https://arxiv.org/abs/2503.08689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QuoTA</p><br /><br /><p><strong></strong> QuoTAQuoTAQuoTAQuoTALLaVA-Video-7BVideo-MMEMLVU3.2%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08689" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>OmniMamba</title>
<link>https://arxiv.org/abs/2503.08686</link>
<guid>https://arxiv.org/abs/2503.08686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniMamba</p><br /><br /><p><strong></strong> OmniMambatokenMamba-2LoRAOmniMamba200-OmniMambaJanusFlowShow-oTransformer119.2GPU63%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08686" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>YuE</title>
<link>https://arxiv.org/abs/2503.08638</link>
<guid>https://arxiv.org/abs/2503.08638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YuE</p><br /><br /><p><strong></strong> YuELLaMA2tokenYuEYuEYuEMARBLE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08638" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:26:50 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08625</link>
<guid>https://arxiv.org/abs/2503.08625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HLMATMLLM</p><br /><br /><p><strong></strong> MLLMsVQAHLMATMLLMsHLMATMLLMsSegAgentHLMATMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08625" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:08:54 GMT</pubDate>
</item>
<item>
<title>LightGen</title>
<link>https://arxiv.org/abs/2503.08619</link>
<guid>https://arxiv.org/abs/2503.08619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LightGen</p><br /><br /><p><strong></strong> LightGenKDDPO0.7B2LightGenGPU88GPUDPOLightGen</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08619" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:58:02 GMT</pubDate>
</item>
<item>
<title>BiasEdit</title>
<link>https://arxiv.org/abs/2503.08588</link>
<guid>https://arxiv.org/abs/2503.08588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BiasEdit</p><br /><br /><p><strong></strong> BiasEditBiasEditBiasEditStereoSetCrows-PairsBiasEdit</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08588" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:25:36 GMT</pubDate>
</item>
<item>
<title>Gemini Embedding</title>
<link>https://arxiv.org/abs/2503.07891</link>
<guid>https://arxiv.org/abs/2503.07891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Embedding </p><br /><br /><p><strong></strong> Gemini EmbeddingGeminiGemini EmbeddingGeminiMMTEBGemini EmbeddingMMTEBGemini Embedding</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07891" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:16:45 GMT</pubDate>
</item>
<item>
<title>RayFlow: </title>
<link>https://arxiv.org/abs/2503.07699</link>
<guid>https://arxiv.org/abs/2503.07699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RayFlow</p><br /><br /><p><strong></strong> RayFlowTime SamplerRayFlow</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07699" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:20:52 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18858</link>
<guid>https://arxiv.org/abs/2502.18858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 10^{26}70AIAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18858" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:59:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.08685</link>
<guid>https://arxiv.org/abs/2503.08685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> PCA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08685" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>SynCoS</title>
<link>https://arxiv.org/abs/2503.08605</link>
<guid>https://arxiv.org/abs/2503.08605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Synchronized Coupled SamplingSynCoSSynCoSSynCoS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08605" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:43:45 GMT</pubDate>
</item>
<item>
<title>UniF^2ace</title>
<link>https://arxiv.org/abs/2503.08120</link>
<guid>https://arxiv.org/abs/2503.08120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniF^2ace</p><br /><br /><p><strong></strong> UniF^2aceUniF^2ace-130K130K-UniF^2aceUniF^2ace</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.08120" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:34:59 GMT</pubDate>
</item>
<item>
<title>SEA-VL</title>
<link>https://arxiv.org/abs/2503.07920</link>
<guid>https://arxiv.org/abs/2503.07920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEA-VL-</p><br /><br /><p><strong></strong> -SEA-VL85%128SEA-VL-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07920" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 19:54:52 GMT</pubDate>
</item>
<item>
<title>VidDiff</title>
<link>https://arxiv.org/abs/2503.07860</link>
<guid>https://arxiv.org/abs/2503.07860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidDiff</p><br /><br /><p><strong></strong> Video Action DifferencingVidDiffVidDiffBench54944692075VidDiffBenchGPT-4oQwen2-VLVidDiffBenchVidDiff</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07860" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 17:18:32 GMT</pubDate>
</item>
<item>
<title>Seedream 2.0</title>
<link>https://arxiv.org/abs/2503.07703</link>
<guid>https://arxiv.org/abs/2503.07703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedream 2.0 </p><br /><br /><p><strong></strong> Seedream 2.0 -Seedream 2.0 ByT5SFTRLHFSeedream 2.0</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07703" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07604</link>
<guid>https://arxiv.org/abs/2503.07604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GPT-2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07604" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07572</link>
<guid>https://arxiv.org/abs/2503.07572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> MRTLLMsMRT2-31.5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07572" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:40:43 GMT</pubDate>
</item>
<item>
<title>LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL</title>
<link>https://arxiv.org/abs/2503.07536</link>
<guid>https://arxiv.org/abs/2503.07536</guid>
<content:encoded><![CDATA[
Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \method achieves 4.83\% and 4.5\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>MoE-X</title>
<link>https://arxiv.org/abs/2503.07639</link>
<guid>https://arxiv.org/abs/2503.07639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoE-X</p><br /><br /><p><strong></strong> MoE-XMoEMoE-XMoEMLPMoE-XMoE-XGPT-2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07639" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 12:40:54 GMT</pubDate>
</item>
<item>
<title>PhiloBERTA</title>
<link>https://arxiv.org/abs/2503.05265</link>
<guid>https://arxiv.org/abs/2503.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhiloBERTA</p><br /><br /><p><strong></strong> PhiloBERTA etymologicallyepistmedikaiosynp0.012</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05265" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:30:16 GMT</pubDate>
</item>
<item>
<title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
<link>https://arxiv.org/abs/2503.02819</link>
<guid>https://arxiv.org/abs/2503.02819</guid>
<content:encoded><![CDATA[
While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 12:46:51 GMT</pubDate>
</item>
<item>
<title>VACE</title>
<link>https://arxiv.org/abs/2503.07598</link>
<guid>https://arxiv.org/abs/2503.07598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VACE</p><br /><br /><p><strong></strong> VACEVACEVACE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07598" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.06698</link>
<guid>https://arxiv.org/abs/2503.06698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06698" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 13:29:01 GMT</pubDate>
</item>
<item>
<title>Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries</title>
<link>https://arxiv.org/abs/2502.20475</link>
<guid>https://arxiv.org/abs/2502.20475</guid>
<content:encoded><![CDATA[
To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:23:15 GMT</pubDate>
</item>
<item>
<title>REF-VLM</title>
<link>https://arxiv.org/abs/2503.07413</link>
<guid>https://arxiv.org/abs/2503.07413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REF-VLM</p><br /><br /><p><strong></strong> REF-VLMTRPVTInstruct125REF-VLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07413" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:59:14 GMT</pubDate>
</item>
<item>
<title>TRCE</title>
<link>https://arxiv.org/abs/2503.07389</link>
<guid>https://arxiv.org/abs/2503.07389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TRCE</p><br /><br /><p><strong></strong> TRCETRCETRCETRCE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07389" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:37:53 GMT</pubDate>
</item>
<item>
<title>ARRA</title>
<link>https://arxiv.org/abs/2503.07334</link>
<guid>https://arxiv.org/abs/2503.07334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARRA</p><br /><br /><p><strong></strong> ARRALLMsARRALLMARRALLMARRALLMChameleonLlamaGen25.5%FIDMIMIC-CXR8.8%DeepEyeNet7.5%ImageNetARRALLMMIMIC-CXR18.6%FID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07334" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:49:28 GMT</pubDate>
</item>
<item>
<title>SlotMIM</title>
<link>https://arxiv.org/abs/2503.06960</link>
<guid>https://arxiv.org/abs/2503.06960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SlotMIM</p><br /><br /><p><strong></strong> PVMsDINOiBOTMAENOCSlotMIM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06960" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:18:31 GMT</pubDate>
</item>
<item>
<title>DiffCLIP-</title>
<link>https://arxiv.org/abs/2503.06626</link>
<guid>https://arxiv.org/abs/2503.06626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffCLIP-</p><br /><br /><p><strong></strong> DiffCLIP-CLIPCLIPDiffCLIP-shotCLIP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06626" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 10:04:09 GMT</pubDate>
</item>
<item>
<title>Symbolic-MoELLM</title>
<link>https://arxiv.org/abs/2503.05641</link>
<guid>https://arxiv.org/abs/2503.05641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Symbolic-MoELLM</p><br /><br /><p><strong></strong> Symbolic-MoEMixture-of-ExpertsLLMLLMGPU16MMLU-ProGPQAAIMEMedMCQASymbolic-MoE8.15%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05641" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:03:13 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07597</link>
<guid>https://arxiv.org/abs/2503.07597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3DHMR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07597" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:03 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07274</link>
<guid>https://arxiv.org/abs/2503.07274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> AGDCFGNFEAGDCFGAGD2%AGDCFGAGDCFGFIDNFEGPU2.6B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07274" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:55:08 GMT</pubDate>
</item>
<item>
<title>WISE</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISE</p><br /><br /><p><strong></strong> (T2I)-WISEWISE251000WiScore20T2I</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07265" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:47:53 GMT</pubDate>
</item>
<item>
<title>Zero-AVSR</title>
<link>https://arxiv.org/abs/2503.06273</link>
<guid>https://arxiv.org/abs/2503.06273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zero-AVSR</p><br /><br /><p><strong></strong> Zero-AVSR(AV-Romanizer)(LLMs)Zero-AVSRAV-RomanizerLLMLLM2916(MARC)82Zero-AVSR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06273" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:40:13 GMT</pubDate>
</item>
<item>
<title>Novel Object 6D Pose Estimation with a Single Reference View</title>
<link>https://arxiv.org/abs/2503.05578</link>
<guid>https://arxiv.org/abs/2503.05578</guid>
<content:encoded><![CDATA[
Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 12:00:41 GMT</pubDate>
</item>
<item>
<title>Mixture of Large Language Model Agents</title>
<link>https://arxiv.org/abs/2503.05856</link>
<guid>https://arxiv.org/abs/2503.05856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoALLM</p><br /><br /><p><strong></strong> Mixture of Large Language Model Agents (MoA)LLMMoAAlpacaEval 2.0MoAAlpacaEval 2.049.2%37.9%QuALITY48.5%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05856" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 09:46:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04973</link>
<guid>https://arxiv.org/abs/2503.04973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> KVLLMsRAGLongBench v2RAG7300.430.16RAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04973" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:07:41 GMT</pubDate>
</item>
<item>
<title>YOLOE</title>
<link>https://arxiv.org/abs/2503.07465</link>
<guid>https://arxiv.org/abs/2503.07465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOE</p><br /><br /><p><strong></strong> YOLOERe-parameterizable Region-Text AlignmentRepRTASemantic-Activated Visual Prompt EncoderSAVPELazy Region-Prompt ContrastLRPCYOLOELVISYOLO-Worldv2-S1.4COCOYOLOE-v8-LYOLOv8-L0.6 AP^b0.4 AP^mYOLOE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07465" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:42:59 GMT</pubDate>
</item>
<item>
<title>ReLURePO</title>
<link>https://arxiv.org/abs/2503.07426</link>
<guid>https://arxiv.org/abs/2503.07426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RePO</p><br /><br /><p><strong></strong> Aligning large language models (LLMs) with human preferences is faced with challenges in computational efficiency and stability. Existing methods, such as reinforcement learning from human feedback (RLHF), often struggle with complex parameters. In contrast, we propose a novel approach called ReLU-based Preference Optimization (RePO), which streamlines the alignment process by eliminating the need for a beta hyperparameter. This is achieved through two main innovations: retaining reference-free margins while utilizing gradient analysis to remove beta, and employing a ReLU-based max-margin loss to filter trivial pairs effectively. Theoretically, RePO is positioned as a limiting case of SimPO where logistic weighting simplifies to binary thresholding. Empirical evaluations on datasets like AlpacaEval 2 and Arena-Hard demonstrate that RePO consistently outperforms existing methods DPO and SimPO, achieving effective alignment while requiring only a single hyperparameter adjustment.</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07426" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:11:07 GMT</pubDate>
</item>
<item>
<title>Llama-MTSK</title>
<link>https://arxiv.org/abs/2503.06362</link>
<guid>https://arxiv.org/abs/2503.06362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-MTSK</p><br /><br /><p><strong></strong> AVSRLLMAVSRLLMLlama-MTSKryoshkaLLMLoRAryoshkaLlama-MTSKAVSR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06362" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 19:02:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05283</link>
<guid>https://arxiv.org/abs/2503.05283</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05283" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:51:56 GMT</pubDate>
</item>
<item>
<title>WritingBench</title>
<link>https://arxiv.org/abs/2503.05244</link>
<guid>https://arxiv.org/abs/2503.05244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Introducing WritingBench, a benchmark to evaluate LLMs in diverse writing domains.</p><br /><br /><p><strong></strong> LLMsWritingBenchLLMsLLMs7BLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05244" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 03:56:20 GMT</pubDate>
</item>
<item>
<title>AlphaDrive</title>
<link>https://arxiv.org/abs/2503.07608</link>
<guid>https://arxiv.org/abs/2503.07608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaDrive</p><br /><br /><p><strong></strong> AlphaDriveRLVLMAlphaDriveGRPORLSFTAlphaDriveGRPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07608" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07603</link>
<guid>https://arxiv.org/abs/2503.07603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMs1080%2%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07603" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:19 GMT</pubDate>
</item>
<item>
<title>DreamRelation</title>
<link>https://arxiv.org/abs/2503.07602</link>
<guid>https://arxiv.org/abs/2503.07602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRelation</p><br /><br /><p><strong></strong> DreamRelationMM-DiTLoRADreamRelation</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07602" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>MedAgentsBench: </title>
<link>https://arxiv.org/abs/2503.07459</link>
<guid>https://arxiv.org/abs/2503.07459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedAgentsBench</p><br /><br /><p><strong></strong> MedAgentsBenchLLMsMedAgentsBenchDeepSeek R1OpenAI o3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07459" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:38:44 GMT</pubDate>
</item>
<item>
<title>DistiLLM-2</title>
<link>https://arxiv.org/abs/2503.07067</link>
<guid>https://arxiv.org/abs/2503.07067</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DistiLLM-2</p><br /><br /><p><strong></strong> DistiLLM-2DistiLLM-2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07067" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:51:32 GMT</pubDate>
</item>
<item>
<title>ProBench: </title>
<link>https://arxiv.org/abs/2503.06885</link>
<guid>https://arxiv.org/abs/2503.06885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProBench</p><br /><br /><p><strong></strong> ProBenchProBench40001056MLLM24ProBenchAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06885" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:29:18 GMT</pubDate>
</item>
<item>
<title>Vision-R1</title>
<link>https://arxiv.org/abs/2503.06749</link>
<guid>https://arxiv.org/abs/2503.06749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vision-R1</p><br /><br /><p><strong></strong> DeepSeek-R1-ZeroLLMsMLLMsMLLMsVision-R1MLLMDeepSeek-R1CoTVision-R1-coldPTSTGRPO6%Vision-R1-7BMathVista73.5%OpenAI O10.4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06749" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 16:06:45 GMT</pubDate>
</item>
<item>
<title>SurveyForge</title>
<link>https://arxiv.org/abs/2503.04629</link>
<guid>https://arxiv.org/abs/2503.04629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyForgeLLMs</p><br /><br /><p><strong></strong> (LLMs)LLMSurveyForgeSurveyForgeSurveyBench100AISurveyForgeAutoSurvey</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04629" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03601</link>
<guid>https://arxiv.org/abs/2503.03601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ATDSAEGemma-2-2bATDLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03601" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 10:33:52 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07605</link>
<guid>https://arxiv.org/abs/2503.07605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SEAPSEAPLLMSEAP50%SEAPWandAFLAP 20%20%2.2%SEAPLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07605" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.07595</link>
<guid>https://arxiv.org/abs/2503.07595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DetectGPTBERT90%DetectGPT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07595" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:56:25 GMT</pubDate>
</item>
<item>
<title>PE3R3D</title>
<link>https://arxiv.org/abs/2503.07507</link>
<guid>https://arxiv.org/abs/2503.07507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PE3R2D3D</p><br /><br /><p><strong></strong> 2D3DPerception-Efficient 3D Reconstruction (PE3R)PE3R3D-shotPE3R3D9</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07507" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 12:29:10 GMT</pubDate>
</item>
<item>
<title>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.07365</link>
<guid>https://arxiv.org/abs/2503.07365</guid>
<content:encoded><![CDATA[
We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:23:12 GMT</pubDate>
</item>
<item>
<title>Automated Movie Generation via Multi-Agent CoT Planning</title>
<link>https://arxiv.org/abs/2503.07314</link>
<guid>https://arxiv.org/abs/2503.07314</guid>
<content:encoded><![CDATA[
Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:33:27 GMT</pubDate>
</item>
<item>
<title>FedRand</title>
<link>https://arxiv.org/abs/2503.07216</link>
<guid>https://arxiv.org/abs/2503.07216</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FedRand</p><br /><br /><p><strong></strong> FedRandFLFLVLMsFedRandLoRAFedRandMIAs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07216" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:55:50 GMT</pubDate>
</item>
<item>
<title>eMIGM</title>
<link>https://arxiv.org/abs/2503.07197</link>
<guid>https://arxiv.org/abs/2503.07197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">eMIGMImageNet</p><br /><br /><p><strong></strong> eMIGMeMIGMImageNet256x256NFEseMIGMVARNFEeMIGMNFE40%ImageNet 512x512eMIGM60%NFE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07197" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:27:12 GMT</pubDate>
</item>
<item>
<title>EasyControl: </title>
<link>https://arxiv.org/abs/2503.07027</link>
<guid>https://arxiv.org/abs/2503.07027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyControl</p><br /><br /><p><strong></strong> EasyControlLoRAKVEasyControl</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07027" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:07:17 GMT</pubDate>
</item>
<item>
<title>MMDiag: DiagNote</title>
<link>https://arxiv.org/abs/2503.07002</link>
<guid>https://arxiv.org/abs/2503.07002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMDiagDiagNote</p><br /><br /><p><strong></strong> MMDiagGPTDiagNoteDeliberateGazeDiagNote</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.07002" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:32:53 GMT</pubDate>
</item>
<item>
<title>FEA-Bench: </title>
<link>https://arxiv.org/abs/2503.06680</link>
<guid>https://arxiv.org/abs/2503.06680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FEA-BenchLLMs</p><br /><br /><p><strong></strong> FEA-BenchLLMs83GitHubLLMsLLMsFEA-Bench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06680" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 12:11:57 GMT</pubDate>
</item>
<item>
<title>AutoCoA</title>
<link>https://arxiv.org/abs/2503.06580</link>
<guid>https://arxiv.org/abs/2503.06580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoCoA</p><br /><br /><p><strong></strong> AutoCoALAMsSFTRLAutoCoAAutoCoAReAct</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06580" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:19:47 GMT</pubDate>
</item>
<item>
<title>Seg-Zero: -shot </title>
<link>https://arxiv.org/abs/2503.06520</link>
<guid>https://arxiv.org/abs/2503.06520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seg-Zero-shot</p><br /><br /><p><strong></strong> Seg-ZeroSeg-ZeroSeg-Zero-shotSeg-Zero-7BReasonSeg57.5-shotLISA-7B18%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.06520" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 04:48:51 GMT</pubDate>
</item>
<item>
<title>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling</title>
<link>https://arxiv.org/abs/2503.06121</link>
<guid>https://arxiv.org/abs/2503.06121</guid>
<content:encoded><![CDATA[
Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 03:31:18 GMT</pubDate>
</item>
<item>
<title>NeuGrasp</title>
<link>https://arxiv.org/abs/2503.03511</link>
<guid>https://arxiv.org/abs/2503.03511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuGrasp</p><br /><br /><p><strong></strong> NeuGraspNeuGrasp extensiveNeuGrasp</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03511" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:57:37 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03499</link>
<guid>https://arxiv.org/abs/2503.03499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SSM</p><br /><br /><p><strong></strong> SSMsTransformerPEFTSSMsSSMsPEFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03499" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:44:42 GMT</pubDate>
</item>
<item>
<title>LLaVE</title>
<link>https://arxiv.org/abs/2503.04812</link>
<guid>https://arxiv.org/abs/2503.04812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVE</p><br /><br /><p><strong></strong> LLaVELMMLLaVEMMEB36LLaVE7BLLaVE-2BSOTALLaVE-7B6.2LLaVE--</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04812" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:21:57 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02199</link>
<guid>https://arxiv.org/abs/2503.02199</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMsVLMsVLMsVLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02199" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 21:21:07 GMT</pubDate>
</item>
<item>
<title>SafeArena</title>
<link>https://arxiv.org/abs/2503.04957</link>
<guid>https://arxiv.org/abs/2503.04957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeArena</p><br /><br /><p><strong></strong> LLMSafeArena250250Agent Risk AssessmentLLMGPT-4oQwen-2GPT-4oQwen-234.7%27.3%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04957" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 15:43:14 GMT</pubDate>
</item>
<item>
<title>S2S-Arena</title>
<link>https://arxiv.org/abs/2503.05085</link>
<guid>https://arxiv.org/abs/2503.05085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S2S-Arena</p><br /><br /><p><strong></strong> speech2speech, S2SS2S-ArenaS2S154TTS21GPT-4o-ASRLLMTTSS2SLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05085" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 21:07:00 GMT</pubDate>
</item>
<item>
<title>LONGCODEU</title>
<link>https://arxiv.org/abs/2503.04359</link>
<guid>https://arxiv.org/abs/2503.04359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LONGCODEU</p><br /><br /><p><strong></strong> LONGCODEU96332K128K-1M</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04359" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:02:31 GMT</pubDate>
</item>
<item>
<title>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</title>
<link>https://arxiv.org/abs/2503.01840</link>
<guid>https://arxiv.org/abs/2503.01840</guid>
<content:encoded><![CDATA[
The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.21263</link>
<guid>https://arxiv.org/abs/2502.21263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 10,0001,500ICDBERTLoRALLaMARAGPubMedUMLSICD20172021EHR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.21263" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 12:40:24 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18968</link>
<guid>https://arxiv.org/abs/2502.18968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> USPUSPLLMUSPUSP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18968" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 04:26:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04824</link>
<guid>https://arxiv.org/abs/2503.04824</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> vSDv1.54FID10.7032DDIMFID=10.05</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04824" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 06:34:36 GMT</pubDate>
</item>
<item>
<title>STILL</title>
<link>https://arxiv.org/abs/2503.04548</link>
<guid>https://arxiv.org/abs/2503.04548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STILL</p><br /><br /><p><strong></strong> STILLRLRLRLRLQwen2.5-32BDeepSeek-R1-Distill-Qwen-1.5BRLAIME 202439.33%RLAIME 202486.67% STILL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04548" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:23:26 GMT</pubDate>
</item>
<item>
<title>Linear-MoE</title>
<link>https://arxiv.org/abs/2503.05447</link>
<guid>https://arxiv.org/abs/2503.05447</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Linear-MoELSMMoE</p><br /><br /><p><strong></strong> Linear-MoELSMMoELinear-MoELSMMoELSMLinear-MoELinear-MoETransformer-MoEA0.3B-2BA1B-7BLinear-MoE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05447" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:05:22 GMT</pubDate>
</item>
<item>
<title>VideoPainter</title>
<link>https://arxiv.org/abs/2503.05639</link>
<guid>https://arxiv.org/abs/2503.05639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoPainter</p><br /><br /><p><strong></strong> VideoPainter6%IDVideoPainterVPDataVPBenchVideoPainter</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05639" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:30:00 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04504</link>
<guid>https://arxiv.org/abs/2503.04504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VADC-VADAnyAnomalyAnyAnomalyC-VADVADUBnormal</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04504" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:06:49 GMT</pubDate>
</item>
<item>
<title>EuroBERT: </title>
<link>https://arxiv.org/abs/2503.05500</link>
<guid>https://arxiv.org/abs/2503.05500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EuroBERT</p><br /><br /><p><strong></strong> EuroBERTEuroBERT8192EuroBERTEuroBERT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05500" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:42:45 GMT</pubDate>
</item>
<item>
<title>SAGE</title>
<link>https://arxiv.org/abs/2503.01713</link>
<guid>https://arxiv.org/abs/2503.01713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAGE</p><br /><br /><p><strong></strong> RAGSAGESAGESAGESAGE61.25%49.41%RAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01713" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:23:51 GMT</pubDate>
</item>
<item>
<title>TrajectoryCrafter: </title>
<link>https://arxiv.org/abs/2503.05638</link>
<guid>https://arxiv.org/abs/2503.05638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> TrajectoryCrafter4D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05638" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:24:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05315</link>
<guid>https://arxiv.org/abs/2503.05315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LoRACodeBERTUniXcoder2%H100 GPU225Code2CodeMRR9.1%Text2Code86.69%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05315" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 00:51:02 GMT</pubDate>
</item>
<item>
<title>R1-Searcher</title>
<link>https://arxiv.org/abs/2503.05592</link>
<guid>https://arxiv.org/abs/2503.05592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-Searcher</p><br /><br /><p><strong></strong> LRMsRLLLMsR1-SearcherLLMsLLMsR1-SearcherRAGGPT-4o-mini</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05592" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:43:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.05379</link>
<guid>https://arxiv.org/abs/2503.05379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RLVROmniRLVR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05379" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:40:46 GMT</pubDate>
</item>
<item>
<title>DeepSeek R1</title>
<link>https://arxiv.org/abs/2503.05132</link>
<guid>https://arxiv.org/abs/2503.05132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepSeek R1</p><br /><br /><p><strong></strong> DeepSeek R1SFT2BQwen2-VL-2BSATCVBench59.47%30%SFTR1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05132" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:39:12 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04872</link>
<guid>https://arxiv.org/abs/2503.04872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsDeepSeek-R1DeepSeek-R1-Distill-Qwen-32BTinyR1-32B-Preview5.54.42.9AIME 2024DeepSeek-R1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04872" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:35:58 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04808</link>
<guid>https://arxiv.org/abs/2503.04808</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsRLDeepSeek R1LLM145.6%252.5%42.3%43.2%LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04808" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:29:35 GMT</pubDate>
</item>
<item>
<title>BEHAVIOR</title>
<link>https://arxiv.org/abs/2503.05652</link>
<guid>https://arxiv.org/abs/2503.05652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BEHAVIOR</p><br /><br /><p><strong></strong> BEHAVIORBRSBRS4BRS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05652" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:51:04 GMT</pubDate>
</item>
<item>
<title>Sketch-of-Thought: </title>
<link>https://arxiv.org/abs/2503.05179</link>
<guid>https://arxiv.org/abs/2503.05179</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Sketch-of-Thought</p><br /><br /><p><strong></strong> CoTSketch-of-ThoughtSoTSoT15SoT76%SoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05179" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:25:52 GMT</pubDate>
</item>
<item>
<title>UnifiedReward</title>
<link>https://arxiv.org/abs/2503.05236</link>
<guid>https://arxiv.org/abs/2503.05236</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UnifiedReward</p><br /><br /><p><strong></strong> UnifiedRewardDPO/</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.05236" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:20:09 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02130</link>
<guid>https://arxiv.org/abs/2503.02130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> FoXFoXFlashAttentionFoXMamba-2HGRN2DeltaNetProFoX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02130" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 22:02:39 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03962</link>
<guid>https://arxiv.org/abs/2503.03962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03962" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 11:38:45 GMT</pubDate>
</item>
<item>
<title>Truthfulness Separator Vector</title>
<link>https://arxiv.org/abs/2503.01917</link>
<guid>https://arxiv.org/abs/2503.01917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TSV</p><br /><br /><p><strong></strong> LLMsLLMsTruthfulness Separator Vector (TSV)LLMTSVLLMTSVLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01917" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 11:06:42 GMT</pubDate>
</item>
<item>
<title>LLMVoX: </title>
<link>https://arxiv.org/abs/2503.04724</link>
<guid>https://arxiv.org/abs/2503.04724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMVoX</p><br /><br /><p><strong></strong> LLMVoX30MTTSLLMLLMLLMVoXLLMVoXLLMLLMVoXLLMVoX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04724" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 08:19:28 GMT</pubDate>
</item>
<item>
<title>Union-of-ExpertsMoE</title>
<link>https://arxiv.org/abs/2503.02495</link>
<guid>https://arxiv.org/abs/2503.02495</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Union-of-ExpertsMixture-of-Experts</p><br /><br /><p><strong></strong> Mixture-of-Experts (MoE)MoEUnion-of-Experts (UoE)TransformerMLPUoE(SMHA)MLP(UoME)UoEMoE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02495" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 06:08:09 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04369</link>
<guid>https://arxiv.org/abs/2503.04369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLM prevalenceLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04369" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 05:25:00 GMT</pubDate>
</item>
<item>
<title>LINGOLY-TOO </title>
<link>https://arxiv.org/abs/2503.02972</link>
<guid>https://arxiv.org/abs/2503.02972</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs LINGOLY-TOO OpenAI o1-preview  DeepSeem R1  LLMs  LLMs </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02972" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:50:00 GMT</pubDate>
</item>
<item>
<title>IFIR</title>
<link>https://arxiv.org/abs/2503.04644</link>
<guid>https://arxiv.org/abs/2503.04644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IFIR2426</p><br /><br /><p><strong></strong> IFIR2426IFIR(LLM)LLM15</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04644" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:37:52 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2503.01901</link>
<guid>https://arxiv.org/abs/2503.01901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PQIReQuantLLM</p><br /><br /><p><strong></strong> LLMHessianPQIReQuantReQuant2.66Llama 3.2 1B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01901" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:23:41 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01375</link>
<guid>https://arxiv.org/abs/2503.01375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> CFM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01375" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 02:51:01 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04725</link>
<guid>https://arxiv.org/abs/2503.04725</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> L^2M</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04725" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 01:42:13 GMT</pubDate>
</item>
<item>
<title>EgoLifeAI</title>
<link>https://arxiv.org/abs/2503.03803</link>
<guid>https://arxiv.org/abs/2503.03803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoLifeAI</p><br /><br /><p><strong></strong> EgoLifeAIAI300EgoLifeEgoLifeQA-EgoButlerEgoGPTEgoRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03803" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:44:13 GMT</pubDate>
</item>
<item>
<title>Audio Flamingo 2</title>
<link>https://arxiv.org/abs/2503.03983</link>
<guid>https://arxiv.org/abs/2503.03983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Audio Flamingo 2</p><br /><br /><p><strong></strong> Audio Flamingo 2AF2ALMAIAF2CLAP3B20305LongAudioALMLongAudioAF2LongAudioBenchALM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03983" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:12:47 GMT</pubDate>
</item>
<item>
<title>GitHub</title>
<link>https://arxiv.org/abs/2503.02191</link>
<guid>https://arxiv.org/abs/2503.02191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GitHub</p><br /><br /><p><strong></strong> GitHub202696LLMs69%F1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02191" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 00:11:25 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20258</link>
<guid>https://arxiv.org/abs/2502.20258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong>  degradation </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20258" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:56:18 GMT</pubDate>
</item>
<item>
<title>PokChamp: an Expert-level Minimax Language Agent</title>
<link>https://arxiv.org/abs/2503.04094</link>
<guid>https://arxiv.org/abs/2503.04094</guid>
<content:encoded><![CDATA[
We introduce Pok\'eChamp, a minimax agent powered by Large Language Models (LLMs) for Pok\'emon battles. Built on a general framework for two-player competitive games, Pok\'eChamp leverages the generalist capabilities of LLMs to enhance minimax tree search. Specifically, LLMs replace three key modules: (1) player action sampling, (2) opponent modeling, and (3) value function estimation, enabling the agent to effectively utilize gameplay history and human knowledge to reduce the search space and address partial observability. Notably, our framework requires no additional LLM training. We evaluate Pok\'eChamp in the popular Gen 9 OU format. When powered by GPT-4o, it achieves a win rate of 76% against the best existing LLM-based bot and 84% against the strongest rule-based bot, demonstrating its superior performance. Even with an open-source 8-billion-parameter Llama 3.1 model, Pok\'eChamp consistently outperforms the previous best LLM-based bot, Pok\'ellmon powered by GPT-4o, with a 64% win rate. Pok\'eChamp attains a projected Elo of 1300-1500 on the Pok\'emon Showdown online ladder, placing it among the top 30%-10% of human players. In addition, this work compiles the largest real-player Pok\'emon battle dataset, featuring over 3 million games, including more than 500k high-Elo matches. Based on this dataset, we establish a series of battle benchmarks and puzzles to evaluate specific battling skills. We further provide key updates to the local game engine. We hope this work fosters further research that leverage Pok\'emon battle as benchmark to integrate LLM technologies with game-theoretic algorithms addressing general multiagent problems. Videos, code, and dataset available at https://sites.google.com/view/pokechamp-llm.
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:53:38 GMT</pubDate>
</item>
<item>
<title>STORM</title>
<link>https://arxiv.org/abs/2503.04130</link>
<guid>https://arxiv.org/abs/2503.04130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STORM</p><br /><br /><p><strong></strong> STORMSTORMMambaSTORM5%82.4-2.9</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04130" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:53:09 GMT</pubDate>
</item>
<item>
<title>LanDiff</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LanDiff</p><br /><br /><p><strong></strong> LanDiffT2VLanDiff3D1D14,000LanDiffVBench T2V85.43Hunyuan Video13B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04606" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:52:33 GMT</pubDate>
</item>
<item>
<title>HybridNorm: Transformer</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HybridNormPre-NormPost-NormTransformer</p><br /><br /><p><strong></strong> HybridNormTransformerHybridNormPre-NormPost-NormTransformerQKVFFNPost-NormLLMsHybridNormPre-NormPost-NormHybridNormTransformer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04598" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 23:04:06 GMT</pubDate>
</item>
<item>
<title>START</title>
<link>https://arxiv.org/abs/2503.04625</link>
<guid>https://arxiv.org/abs/2503.04625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">START</p><br /><br /><p><strong></strong> STARTOpenAI-o1DeepSeek-R1STARTSTART1) Hint-infer2) Hint RFTQwQ-32B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04625" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:35:47 GMT</pubDate>
</item>
<item>
<title>FuseChat-3.0: </title>
<link>https://arxiv.org/abs/2503.04222</link>
<guid>https://arxiv.org/abs/2503.04222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FuseChat-3.0</p><br /><br /><p><strong></strong> FuseChat-3.0Gemma-2-27B-itMistral-Large-Instruct-2407Qwen-2.5-72B-InstructLlama-3.1-70B-Instruct(SFT)(DPO)14Llama-3.1-8B-Instruct6.837.130.1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04222" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:20:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.04378</link>
<guid>https://arxiv.org/abs/2503.04378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Arena Hard92.7OpenAI o1-preview-2024-09-12DeepSeek R1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.04378" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 22:10:18 GMT</pubDate>
</item>
<item>
<title>Highlighted Chain-of-Thought Prompting</title>
<link>https://arxiv.org/abs/2503.02003</link>
<guid>https://arxiv.org/abs/2503.02003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> HoTLLMsHoTXMLHoT17CoTLLMLLMHoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02003" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 17:46:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02954</link>
<guid>https://arxiv.org/abs/2503.02954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GNN-VAE</p><br /><br /><p><strong></strong> GNN-VAEMILPGNN-VAE250</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02954" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:16:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02924</link>
<guid>https://arxiv.org/abs/2503.02924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STL</p><br /><br /><p><strong></strong> STLSTL1/17</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02924" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:10:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00307</link>
<guid>https://arxiv.org/abs/2503.00307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReMDM</p><br /><br /><p><strong></strong> ReMDMReMDMReMDM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00307" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 14:57:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03746</link>
<guid>https://arxiv.org/abs/2503.03746</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03746" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:15:20 GMT</pubDate>
</item>
<item>
<title>Shakti</title>
<link>https://arxiv.org/abs/2503.01933</link>
<guid>https://arxiv.org/abs/2503.01933</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Shakti</p><br /><br /><p><strong></strong> ShaktiSLMsShakti-100MShakti-250MShakti-500MShaktiMMLUHellaswag</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01933" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 05:49:04 GMT</pubDate>
</item>
<item>
<title>MoR</title>
<link>https://arxiv.org/abs/2502.20317</link>
<guid>https://arxiv.org/abs/2502.20317</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoR</p><br /><br /><p><strong></strong> Mixture of Structural-and-Textual RetrievalMoRMoRMoR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20317" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 03:22:14 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18860</link>
<guid>https://arxiv.org/abs/2502.18860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18860" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 03:20:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03044</link>
<guid>https://arxiv.org/abs/2503.03044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> QE4PE (QE)  (MT) 42QEMTQE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03044" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:30:17 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.03278</link>
<guid>https://arxiv.org/abs/2503.03278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLMsVLM0.23B Florence-27B LLaVAVLM1.5% </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03278" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:29:15 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01836</link>
<guid>https://arxiv.org/abs/2503.01836</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CrowdSelect</p><br /><br /><p><strong></strong> LLMCrowdSelect4MT-benchArena-HardCrowdSelectFullLoRAArena-Hard4.81%MT-bench11.1%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01836" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 02:20:38 GMT</pubDate>
</item>
<item>
<title>ToolRet</title>
<link>https://arxiv.org/abs/2503.01763</link>
<guid>https://arxiv.org/abs/2503.01763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ToolRet</p><br /><br /><p><strong></strong> ToolRet7.6k43kIRIRToolRet20IR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01763" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:12:07 GMT</pubDate>
</item>
<item>
<title>FLAME</title>
<link>https://arxiv.org/abs/2503.01729</link>
<guid>https://arxiv.org/abs/2503.01729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLAME</p><br /><br /><p><strong></strong> FLAMEFederated Learning Across Manipulation EnvironmentsFLAME16FLAME</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01729" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:11:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01449</link>
<guid>https://arxiv.org/abs/2503.01449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsSVD5LLMsSVDPythonJavaJavaScript86260LLMsSVDSVDLLMsLLMsSVD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01449" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:11:25 GMT</pubDate>
</item>
<item>
<title>CognitiveDrone--</title>
<link>https://arxiv.org/abs/2503.01378</link>
<guid>https://arxiv.org/abs/2503.01378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CognitiveDroneVLA</p><br /><br /><p><strong></strong> CognitiveDrone--VLA80004DCognitiveDrone-R1-CognitiveDroneBenchCognitiveDrone-R177.2%30%VLA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01378" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:10:56 GMT</pubDate>
</item>
<item>
<title>SwiLTra-Bench</title>
<link>https://arxiv.org/abs/2503.01372</link>
<guid>https://arxiv.org/abs/2503.01372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SwiLTra-Bench</p><br /><br /><p><strong></strong> SwiLTra-Bench18LLMClaude-3.5-Sonnet-shotSwiLTra-JudgeLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01372" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 00:10:21 GMT</pubDate>
</item>
<item>
<title>GEN3C</title>
<link>https://arxiv.org/abs/2503.03751</link>
<guid>https://arxiv.org/abs/2503.03751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEN3C3D</p><br /><br /><p><strong></strong> GEN3CGEN3CGEN3C</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.03751" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 22:13:22 GMT</pubDate>
</item>
<item>
<title>Babel</title>
<link>https://arxiv.org/abs/2503.00865</link>
<guid>https://arxiv.org/abs/2503.00865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Babel25</p><br /><br /><p><strong></strong> BabelLLMLLM2590%BabelBabel-9BBabel-83BLLMBabelLLMBabel-9B-Chat10BLLMBabel-83B-Chat</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00865" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:49:03 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00502</link>
<guid>https://arxiv.org/abs/2503.00502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> AVsHVsActor-ReasonerLLMsLLMReasonerActorActor</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00502" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:37:18 GMT</pubDate>
</item>
<item>
<title>ABC: </title>
<link>https://arxiv.org/abs/2503.00329</link>
<guid>https://arxiv.org/abs/2503.00329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ABC</p><br /><br /><p><strong></strong> ABCCLIP-basedABC-MSCOCOCtrlBenchABC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00329" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:33:37 GMT</pubDate>
</item>
<item>
<title>KodCode</title>
<link>https://arxiv.org/abs/2503.02951</link>
<guid>https://arxiv.org/abs/2503.02951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KodCode</p><br /><br /><p><strong></strong> KodCodeKodCode--KodCodeHumanEvalMBPPBigCodeBenchLiveCodeBenchKodCodeQwen2.5-Coder-32B-InstructDeepSeek-R1-Distill-Llama-70B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02951" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 21:31:01 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00069</link>
<guid>https://arxiv.org/abs/2503.00069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMLLMLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00069" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 18:39:55 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00200</link>
<guid>https://arxiv.org/abs/2503.00200</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">(UVA)</p><br /><br /><p><strong></strong> UVAUVA-UVAUVA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00200" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 16:12:24 GMT</pubDate>
</item>
<item>
<title>KVQ-Filters</title>
<link>https://arxiv.org/abs/2503.02812</link>
<guid>https://arxiv.org/abs/2503.02812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Q-FiltersKV</p><br /><br /><p><strong></strong> KVQ-FiltersQueryKeyQ-FiltersKey-ValueFlashAttentionQ-FiltersSnapKVStreaming-LLMQ-Filtersx3299%65%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02812" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:29:36 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02823</link>
<guid>https://arxiv.org/abs/2503.02823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MusicGENn=111</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02823" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:04:04 GMT</pubDate>
</item>
<item>
<title>Tabby: Transformer</title>
<link>https://arxiv.org/abs/2503.02152</link>
<guid>https://arxiv.org/abs/2503.02152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TabbyTransformer</p><br /><br /><p><strong></strong> TabbyTransformerTabbyGated Mixture-of-ExpertsTabbyPlainTabby44%TabbyJSON</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02152" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:03:42 GMT</pubDate>
</item>
<item>
<title>TokenOCR token </title>
<link>https://arxiv.org/abs/2503.02304</link>
<guid>https://arxiv.org/abs/2503.02304</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TokenOCR  token </p><br /><br /><p><strong></strong> VFMMLLM TokenOCR token  TokenOCR  token  TokenIT218 token-mask VFM MLLM TokenVL VQA  TokenOCR  TokenVL </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02304" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 11:03:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01842</link>
<guid>https://arxiv.org/abs/2503.01842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DHAL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01842" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 10:37:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00876</link>
<guid>https://arxiv.org/abs/2503.00876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SRL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00876" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:25:03 GMT</pubDate>
</item>
<item>
<title>Q-EVAL-100K</title>
<link>https://arxiv.org/abs/2503.02357</link>
<guid>https://arxiv.org/abs/2503.02357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Q-EVAL-100K</p><br /><br /><p><strong></strong> Q-EVAL-100K960K(MOS)60K40KQ-Eval-ScoreQ-EVAL-100KGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02357" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 06:09:41 GMT</pubDate>
</item>
<item>
<title>IterPref</title>
<link>https://arxiv.org/abs/2503.02783</link>
<guid>https://arxiv.org/abs/2503.02783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IterPref</p><br /><br /><p><strong></strong> IterPrefCode LLMsIterPrefDPOCodeFlowIterPrefBigCodeBenchIterPref</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02783" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:54:00 GMT</pubDate>
</item>
<item>
<title>RectifiedHR</title>
<link>https://arxiv.org/abs/2503.02537</link>
<guid>https://arxiv.org/abs/2503.02537</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RectifiedHR</p><br /><br /><p><strong></strong> RectifiedHRRectifiedHRRectifiedHR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02537" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:53:05 GMT</pubDate>
</item>
<item>
<title>LADDER: </title>
<link>https://arxiv.org/abs/2503.00735</link>
<guid>https://arxiv.org/abs/2503.00735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LADDER</p><br /><br /><p><strong></strong> LADDERLADDERLADDERLlama 3.2 3B1%82%Qwen2.5 7B Deepseek-R1MIT73%TTRLQwen2.5 7B Deepseek-R1MIT90%OpenAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00735" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 05:24:20 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02368</link>
<guid>https://arxiv.org/abs/2503.02368</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> (RLHF)</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02368" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 03:48:51 GMT</pubDate>
</item>
<item>
<title>GUI</title>
<link>https://arxiv.org/abs/2503.02268</link>
<guid>https://arxiv.org/abs/2503.02268</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMGUI</p><br /><br /><p><strong></strong> GUI(LLM)LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02268" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 01:15:42 GMT</pubDate>
</item>
<item>
<title>FR-Spec</title>
<link>https://arxiv.org/abs/2502.14856</link>
<guid>https://arxiv.org/abs/2502.14856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FR-Spec</p><br /><br /><p><strong></strong> FR-SpecLLMs-Llama-3-8B128kLLMsFR-Spec75%FR-SpecEAGLE-21.12</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14856" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 00:36:34 GMT</pubDate>
</item>
<item>
<title>SemViQA</title>
<link>https://arxiv.org/abs/2503.00955</link>
<guid>https://arxiv.org/abs/2503.00955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SemViQA</p><br /><br /><p><strong></strong> LLMsGPTGeminiSemViQASERTVCISE-DSC0178.97%ViWikiFC80.82%UITSemViQA Faster7SemViQA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00955" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 00:08:53 GMT</pubDate>
</item>
<item>
<title>UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface</title>
<link>https://arxiv.org/abs/2503.01342</link>
<guid>https://arxiv.org/abs/2503.01342</guid>
<content:encoded><![CDATA[
Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present \ours, a framework that Unifies Fine-grained visual perception tasks through an Open-ended language interface. By transforming all perception targets into the language space, \ours unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, \ours outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby enabling more challenging tasks such as reasoning segmentation. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 23:55:08 GMT</pubDate>
</item>
<item>
<title>Meta Plan Optimization</title>
<link>https://arxiv.org/abs/2503.02682</link>
<guid>https://arxiv.org/abs/2503.02682</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPOLLM</p><br /><br /><p><strong></strong> Meta Plan Optimization (MPO)LLMMPOMPOMPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02682" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:30:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02879</link>
<guid>https://arxiv.org/abs/2503.02879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsLLMsLLMs1%-2%LLMsLLMsLLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02879" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:25:53 GMT</pubDate>
</item>
<item>
<title>Mask-DPO</title>
<link>https://arxiv.org/abs/2503.02846</link>
<guid>https://arxiv.org/abs/2503.02846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mask-DPO</p><br /><br /><p><strong></strong> DPOMask-DPOLLMsAIMask-DPOMask-DPOLLMsLlama3.1-8B-InstructANAH49.19%77.53%Llama3.1-70B-Instruct</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02846" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:25:15 GMT</pubDate>
</item>
<item>
<title>ATLaS</title>
<link>https://arxiv.org/abs/2503.02197</link>
<guid>https://arxiv.org/abs/2503.02197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ATLaSLLM</p><br /><br /><p><strong></strong> LLMATLaSATLaS30%LLMLLMLLMATLaSLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02197" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:17:48 GMT</pubDate>
</item>
<item>
<title>SPIDER</title>
<link>https://arxiv.org/abs/2503.02876</link>
<guid>https://arxiv.org/abs/2503.02876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIDERAI</p><br /><br /><p><strong></strong> SPIDERSupervised Pathology Image-DEscription RepositorySPIDERHibou-LAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02876" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:08:26 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.02878</link>
<guid>https://arxiv.org/abs/2503.02878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">20%</p><br /><br /><p><strong></strong> 80gpt-4o20%37LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.02878" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 22:07:18 GMT</pubDate>
</item>
<item>
<title>MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents</title>
<link>https://arxiv.org/abs/2503.01935</link>
<guid>https://arxiv.org/abs/2503.01935</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 21:46:46 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01328</link>
<guid>https://arxiv.org/abs/2503.01328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> PPPPPPTP19%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01328" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 21:30:49 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.00031</link>
<guid>https://arxiv.org/abs/2503.00031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsBest-of-NLLMsBest-of-N16MathQA81.083.6</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00031" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 12:05:25 GMT</pubDate>
</item>
<item>
<title>Web AI</title>
<link>https://arxiv.org/abs/2502.20383</link>
<guid>https://arxiv.org/abs/2502.20383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Web AILLM</p><br /><br /><p><strong></strong> Web AILLMWeb AIWeb AI1) 2) 3) AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20383" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 10:47:26 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19402</link>
<guid>https://arxiv.org/abs/2502.19402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsAGILLMsLLMs1. 2. 3. </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19402" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 08:19:57 GMT</pubDate>
</item>
<item>
<title>PodAgent</title>
<link>https://arxiv.org/abs/2503.00455</link>
<guid>https://arxiv.org/abs/2503.00455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PodAgent</p><br /><br /><p><strong></strong> PodAgentPodAgent--(LM)PodAgentGPT-487.4%LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00455" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 08:11:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01688</link>
<guid>https://arxiv.org/abs/2503.01688</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs-MASJLLMsPhi-4MistralQwenMASJROC AUC0.73ROC AUC0.55MMLU-ProLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01688" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 06:41:49 GMT</pubDate>
</item>
<item>
<title>SampleMix: </title>
<link>https://arxiv.org/abs/2503.01506</link>
<guid>https://arxiv.org/abs/2503.01506</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SampleMixSampleMixSampleMixSampleMix1.42.1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01506" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:28:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01714</link>
<guid>https://arxiv.org/abs/2503.01714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsSemRecScoreLLMsLLMsLLMsLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01714" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:13:44 GMT</pubDate>
</item>
<item>
<title>Direct Discriminative Optimization</title>
<link>https://arxiv.org/abs/2503.01103</link>
<guid>https://arxiv.org/abs/2503.01103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DDO</p><br /><br /><p><strong></strong> DDODDODPOGANDDO1%DDOEDMCIFAR-10ImageNet-64FID1.79/1.581.30/0.97ImageNet 256x256CFGFID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01103" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:12:10 GMT</pubDate>
</item>
<item>
<title>TOKENSWIFT</title>
<link>https://arxiv.org/abs/2502.18890</link>
<guid>https://arxiv.org/abs/2502.18890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TOKENSWIFT3</p><br /><br /><p><strong></strong> 100KtokenLLMsTOKENSWIFTTOKENSWIFT1.5B, 7B, 8B, 14BMHA, GQA3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18890" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:56:33 GMT</pubDate>
</item>
<item>
<title>DiffRhythm</title>
<link>https://arxiv.org/abs/2503.01183</link>
<guid>https://arxiv.org/abs/2503.01183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffRhythm</p><br /><br /><p><strong></strong> DiffRhythm44510DiffRhythm</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01183" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:54:04 GMT</pubDate>
</item>
<item>
<title>DUSt3R</title>
<link>https://arxiv.org/abs/2502.16779</link>
<guid>https://arxiv.org/abs/2502.16779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Plane-DUSt3R</p><br /><br /><p><strong></strong> 3DDUSt3RPlane-DUSt3RStructure3DDUSt3R2DPlane-DUSt3RPlane-DUSt3R</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16779" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 04:17:23 GMT</pubDate>
</item>
<item>
<title>OneRec: </title>
<link>https://arxiv.org/abs/2502.18965</link>
<guid>https://arxiv.org/abs/2502.18965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OneRec</p><br /><br /><p><strong></strong> OneRecOneRec-session-wiseDPOOneRec1.6%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18965" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 03:56:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01063</link>
<guid>https://arxiv.org/abs/2503.01063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMscryptophasiaASCII32-126220 Hz50,175.42 Hz7.9>20 kHzABCAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01063" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 03:20:03 GMT</pubDate>
</item>
<item>
<title>Liger</title>
<link>https://arxiv.org/abs/2503.01496</link>
<guid>https://arxiv.org/abs/2503.01496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Liger </p><br /><br /><p><strong></strong>  LigerLLMsLiger  Low-Rank Adaptation (LoRA) LLMs Liger Liger Attention 0.02%  Transformer  LLM  93%  1080</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01496" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:48:58 GMT</pubDate>
</item>
<item>
<title>CLEA</title>
<link>https://arxiv.org/abs/2503.00729</link>
<guid>https://arxiv.org/abs/2503.00729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEA</p><br /><br /><p><strong></strong> LLMsCLEALLMCLEACLEA12-CLEA67.3%52.8%CLEA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00729" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:27:17 GMT</pubDate>
</item>
<item>
<title>SpeQL: </title>
<link>https://arxiv.org/abs/2503.00714</link>
<guid>https://arxiv.org/abs/2503.00714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpeQL</p><br /><br /><p><strong></strong> SpeQLSQLLLMsSpeQLSpeQLSpeQLSpeQLSpeQL2894</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00714" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:21:00 GMT</pubDate>
</item>
<item>
<title>CodeArena</title>
<link>https://arxiv.org/abs/2503.01295</link>
<guid>https://arxiv.org/abs/2503.01295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeArena</p><br /><br /><p><strong></strong> LLMsCodeArenaLLMCodeArenaAPI123API</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01295" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 02:16:25 GMT</pubDate>
</item>
<item>
<title>Qilin</title>
<link>https://arxiv.org/abs/2503.00501</link>
<guid>https://arxiv.org/abs/2503.00501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qilin</p><br /><br /><p><strong></strong> QilinQilinQilinAPPRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00501" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 01:56:03 GMT</pubDate>
</item>
<item>
<title>Kiss3DGen: 3D</title>
<link>https://arxiv.org/abs/2503.01370</link>
<guid>https://arxiv.org/abs/2503.01370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kiss3DGen2D3D</p><br /><br /><p><strong></strong> Kiss3DGenKeep It Simple and Straightforward in 3D Generation3D2D3D Bundle Image3D3D3D2DKiss3DGen3D3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01370" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 01:19:45 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2503.01774</link>
<guid>https://arxiv.org/abs/2503.01774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Difix3D+3D</p><br /><br /><p><strong></strong> Difix3D+3DDifix3DDifix3DDifix3DDifix3D+NeRF3DGSFID3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01774" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:52:22 GMT</pubDate>
</item>
<item>
<title>VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2503.01739</link>
<guid>https://arxiv.org/abs/2503.01739</guid>
<content:encoded><![CDATA[
Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:29:56 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2503.01307</link>
<guid>https://arxiv.org/abs/2503.01307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RLQwen-2.5-3BLlama-3.2-3BLlamaQwenLlama</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01307" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 00:09:04 GMT</pubDate>
</item>
<item>
<title>Large-Scale Data Selection for Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.01807</link>
<guid>https://arxiv.org/abs/2503.01807</guid>
<content:encoded><![CDATA[
Selecting high-quality training data from a larger pool is a crucial step when instruction-tuning language models, as carefully curated datasets often produce models that outperform those trained on much larger, noisier datasets. Automated data selection approaches for instruction-tuning are typically tested by selecting small datasets (roughly 10k samples) from small pools (100-200k samples). However, popular deployed instruction-tuned models often train on hundreds of thousands to millions of samples, subsampled from even larger data pools. We present a systematic study of how well data selection methods scale to these settings, selecting up to 2.5M samples from pools of up to 5.8M samples and evaluating across 7 diverse tasks. We show that many recently proposed methods fall short of random selection in this setting (while using more compute), and even decline in performance when given access to larger pools of data to select over. However, we find that a variant of representation-based data selection (RDS+), which uses weighted mean pooling of pretrained LM hidden states, consistently outperforms more complex methods across all settings tested -- all whilst being more compute-efficient. Our findings highlight that the scaling properties of proposed automated selection methods should be more closely examined. We release our code, data, and models at https://github.com/hamishivi/automated-instruction-selection.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:44:06 GMT</pubDate>
</item>
<item>
<title>Visual-RFT</title>
<link>https://arxiv.org/abs/2503.01785</link>
<guid>https://arxiv.org/abs/2503.01785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Visual-RFT</p><br /><br /><p><strong></strong> Visual-RFTVisual-RFTIoUGRPOVisual-RFT100Visual-RFT24.3%COCO21.9LVIS15.4Visual-RFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01785" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:29:27 GMT</pubDate>
</item>
<item>
<title>Introducing Phi-4-MiniPhi-4-Multimodal</title>
<link>https://arxiv.org/abs/2503.01743</link>
<guid>https://arxiv.org/abs/2503.01743</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-4-MiniPhi-4-Multimodal</p><br /><br /><p><strong></strong> Phi-4-MiniPhi-4-MultimodalPhi-4-Mini3820tokenPhi-4-Multimodal/LoRA38Phi-4-Mini</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.01743" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 23:15:05 GMT</pubDate>
</item>
<item>
<title>DuoDecoding</title>
<link>https://arxiv.org/abs/2503.00784</link>
<guid>https://arxiv.org/abs/2503.00784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuoDecoding</p><br /><br /><p><strong></strong> DuoDecodingCPUGPUDuoDecoding2.6183%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2503.00784" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 22:35:45 GMT</pubDate>
</item>
<item>
<title>muCode</title>
<link>https://arxiv.org/abs/2502.20380</link>
<guid>https://arxiv.org/abs/2502.20380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">muCode</p><br /><br /><p><strong></strong> muCodemuCodeMDPmuCodemuCodemuCode</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20380" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 11:25:57 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19731</link>
<guid>https://arxiv.org/abs/2502.19731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs36,000PsychoCounsel-PreferencePsychoCounsel-PreferenceLLMsPsychoCounsel-Llama3-8BGPT-4o87%LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19731" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 10:56:33 GMT</pubDate>
</item>
<item>
<title>EgoNormia: </title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoNormia</p><br /><br /><p><strong></strong> EgoNormia1853(VLMs)//EgoNormia45%92%EgoNormia</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20490" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 10:26:31 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.21318</link>
<guid>https://arxiv.org/abs/2502.21318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> ImageNet1/101/1000GenEvalSD-XL 2DPGBench5T2I</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.21318" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:49:10 GMT</pubDate>
</item>
<item>
<title>DexGraspVLA</title>
<link>https://arxiv.org/abs/2502.20900</link>
<guid>https://arxiv.org/abs/2502.20900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DexGraspVLA-</p><br /><br /><p><strong></strong> DexGraspVLA-DexGraspVLA90%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20900" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:44:46 GMT</pubDate>
</item>
<item>
<title>TeleRAGRAG</title>
<link>https://arxiv.org/abs/2502.20969</link>
<guid>https://arxiv.org/abs/2502.20969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TeleRAGRAGGPU</p><br /><br /><p><strong></strong> Retrieval-augmented generation (RAG)LLMRAGGPUTeleRAGGPURAGTeleRAGCPUGPURAGIVFTeleRAGTeleRAG1.72RAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20969" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 09:33:49 GMT</pubDate>
</item>
<item>
<title>MIGE</title>
<link>https://arxiv.org/abs/2502.21291</link>
<guid>https://arxiv.org/abs/2502.21291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIGE</p><br /><br /><p><strong></strong> MIGEMIGE-MIGE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.21291" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 08:13:06 GMT</pubDate>
</item>
<item>
<title>LettuceDetect</title>
<link>https://arxiv.org/abs/2502.17125</link>
<guid>https://arxiv.org/abs/2502.17125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LettuceDetectRAG</p><br /><br /><p><strong></strong> LettuceDetectRAGModernBERT8000RAGTruth30LettuceDetect--RAGTruthF179.22%Luna14.8%GPU3060RAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17125" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 07:33:14 GMT</pubDate>
</item>
<item>
<title>Optimal Brain Apoptosis</title>
<link>https://arxiv.org/abs/2502.17941</link>
<guid>https://arxiv.org/abs/2502.17941</guid>
<content:encoded><![CDATA[
The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at https://github.com/NEU-REAL/OBA.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 07:04:47 GMT</pubDate>
</item>
<item>
<title>ProtoFM</title>
<link>https://arxiv.org/abs/2502.19577</link>
<guid>https://arxiv.org/abs/2502.19577</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProtoFM</p><br /><br /><p><strong></strong> VFMSEMProtoFMVFM100ProtoFM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19577" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 04:21:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18600</link>
<guid>https://arxiv.org/abs/2502.18600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> CoDLLMsCoTCoDLLMsCoDCoT7.6%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18600" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 02:35:09 GMT</pubDate>
</item>
<item>
<title>ViDoSeekViDoRAG</title>
<link>https://arxiv.org/abs/2502.18017</link>
<guid>https://arxiv.org/abs/2502.18017</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViDoSeekViDoRAG</p><br /><br /><p><strong></strong> RAGViDoSeekRAGRAGViDoRAGRAGViDoSeekViDoRAG10%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18017" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:22:01 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20396</link>
<guid>https://arxiv.org/abs/2502.20396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20396" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:08:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20811</link>
<guid>https://arxiv.org/abs/2502.20811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> HAICTrainHAICBench126KGemini-Pro-500-1400HAICTrain</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20811" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:04:15 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20545</link>
<guid>https://arxiv.org/abs/2502.20545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs</p><br /><br /><p><strong></strong> SoS-1K1000LLM81%4SoS-7B671BDeepSeek-V3GPT-4o-mini1.8%5%LLMNP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20545" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 22:00:31 GMT</pubDate>
</item>
<item>
<title>LiteASR</title>
<link>https://arxiv.org/abs/2502.20583</link>
<guid>https://arxiv.org/abs/2502.20583</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LiteASRASR</p><br /><br /><p><strong></strong> LiteASRASRPCALiteASRWhisper large-v350%Whisper medium</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20583" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 21:48:46 GMT</pubDate>
</item>
<item>
<title>SolutionBenchSolutionRAG</title>
<link>https://arxiv.org/abs/2502.20730</link>
<guid>https://arxiv.org/abs/2502.20730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SolutionBenchSolutionRAG</p><br /><br /><p><strong></strong> SolutionBenchSolutionRAGSolutionRAGSolutionBench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20730" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 02 Mar 2025 21:35:24 GMT</pubDate>
</item>
<item>
<title>PlanGEN</title>
<link>https://arxiv.org/abs/2502.16111</link>
<guid>https://arxiv.org/abs/2502.16111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlanGEN</p><br /><br /><p><strong></strong> Recent agent frameworks and inference-time algorithms face challenges in complex planning problems due to the inability to effectively verify generated plans and adapt to varying instance complexities. To tackle these issues, we introduce PlanGEN, a scalable and model-agnostic agent framework that integrates three essential components: constraint, verification, and selection agents. Our method employs constraint-guided iterative verification to enhance the performance of inference-time algorithms like Best of N, Tree-of-Thought, and REBASE. The selection agent further optimizes the choice of algorithms based on instance complexity, improving adaptability for complex planning tasks. Experimental results indicate that PlanGEN significantly outperforms existing baselines across various benchmarks, achieving state-of-the-art improvements on NATURAL PLAN, OlympiadBench, DocFinQA, and GPQA. The findings suggest that constraint-guided verification and adaptive selection are crucial for advancing performance in complex reasoning and planning challenges.</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16111" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 16:51:51 GMT</pubDate>
</item>
<item>
<title>xAR</title>
<link>https://arxiv.org/abs/2502.20388</link>
<guid>https://arxiv.org/abs/2502.20388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">xAR</p><br /><br /><p><strong></strong> AR2DtokenxARtokenXXtokenARxARImageNet-256xAR-B172M675MDiT-XLSiT-XL20xAR-H1.24FID2.2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20388" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 13:21:13 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17355</link>
<guid>https://arxiv.org/abs/2502.17355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLlama-2iiiiii</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17355" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:54:03 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2502.16750</link>
<guid>https://arxiv.org/abs/2502.16750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> AILLMGEMINI 1.5 prolama-3.3-70B94%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16750" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 08:46:19 GMT</pubDate>
</item>
<item>
<title>Training Consistency Models with Variational Noise Coupling</title>
<link>https://arxiv.org/abs/2502.18197</link>
<guid>https://arxiv.org/abs/2502.18197</guid>
<content:encoded><![CDATA[
Consistency Training (CT) has recently emerged as a promising alternative to diffusion models, achieving competitive performance in image generation tasks. However, non-distillation consistency training often suffers from high variance and instability, and analyzing and improving its training dynamics is an active area of research. In this work, we propose a novel CT training approach based on the Flow Matching framework. Our main contribution is a trained noise-coupling scheme inspired by the architecture of Variational Autoencoders (VAE). By training a data-dependent noise emission model implemented as an encoder architecture, our method can indirectly learn the geometry of the noise-to-data mapping, which is instead fixed by the choice of the forward process in classical CT. Empirical results across diverse image datasets show significant generative improvements, with our model outperforming baselines and achieving the state-of-the-art (SoTA) non-distillation CT FID on CIFAR-10, and attaining FID on par with SoTA on ImageNet at 64 times 64 resolution in 2-step generation. Our code is available at https://github.com/sony/vct .
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:55:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20378</link>
<guid>https://arxiv.org/abs/2502.20378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> EDGSEDGS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20378" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 07:25:35 GMT</pubDate>
</item>
<item>
<title>ArtGS</title>
<link>https://arxiv.org/abs/2502.19459</link>
<guid>https://arxiv.org/abs/2502.19459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ArtGS3D</p><br /><br /><p><strong></strong> ArtGS3DArtGSArtGS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19459" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:47:08 GMT</pubDate>
</item>
<item>
<title>MedVLM-R1</title>
<link>https://arxiv.org/abs/2502.19634</link>
<guid>https://arxiv.org/abs/2502.19634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedVLM-R1</p><br /><br /><p><strong></strong> MedVLM-R1MedVLM-R1fine-tuning6002BMedVLM-R1MRICTX55.11%78.22%100AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19634" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:36:05 GMT</pubDate>
</item>
<item>
<title>Dream Engine-</title>
<link>https://arxiv.org/abs/2502.20172</link>
<guid>https://arxiv.org/abs/2502.20172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dream Engine-</p><br /><br /><p><strong></strong> -Dream Engine-GenEval0.69SD3.5FLUX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20172" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 04:02:19 GMT</pubDate>
</item>
<item>
<title>NeoBERT</title>
<link>https://arxiv.org/abs/2502.19587</link>
<guid>https://arxiv.org/abs/2502.19587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeoBERT</p><br /><br /><p><strong></strong> NeoBERTNeoBERT4096250MNeoBERTMTEBBERT largeRoBERTa largeGLUEMTEB</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19587" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 03:27:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16944</link>
<guid>https://arxiv.org/abs/2502.16944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DVPO</p><br /><br /><p><strong></strong> DVPOPPODVPOGVMDVPOGPU40%35%DVPORLHFDPOPPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16944" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:55:41 GMT</pubDate>
</item>
<item>
<title>FINEREASON: </title>
<link>https://arxiv.org/abs/2502.20238</link>
<guid>https://arxiv.org/abs/2502.20238</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FINEREASON</p><br /><br /><p><strong></strong> LLMs12FINEREASONLLMsGSM8K5.1%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20238" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 01:14:11 GMT</pubDate>
</item>
<item>
<title>Mobius</title>
<link>https://arxiv.org/abs/2502.20307</link>
<guid>https://arxiv.org/abs/2502.20307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobius</p><br /><br /><p><strong></strong> MobiusMobius</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20307" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:14:01 GMT</pubDate>
</item>
<item>
<title>FlexiDiT</title>
<link>https://arxiv.org/abs/2502.20126</link>
<guid>https://arxiv.org/abs/2502.20126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiDiT</p><br /><br /><p><strong></strong> FlexiDiTDiTFlexiDiT40%FlexiDiT75%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20126" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:10:30 GMT</pubDate>
</item>
<item>
<title>R1-Translator: </title>
<link>https://arxiv.org/abs/2502.19735</link>
<guid>https://arxiv.org/abs/2502.19735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-Translator</p><br /><br /><p><strong></strong> R1-Translator (R1-T1)R1-TranslatorFlores-101R1-Translator218015</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19735" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 28 Feb 2025 00:03:34 GMT</pubDate>
</item>
<item>
<title>UniTok</title>
<link>https://arxiv.org/abs/2502.20321</link>
<guid>https://arxiv.org/abs/2502.20321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniTok</p><br /><br /><p><strong></strong> UniTokUniTokImageNetUniTok0.38rFIDSD-VAE0.8778.6%CLIP76.2%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20321" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:34:45 GMT</pubDate>
</item>
<item>
<title>CODESYNC</title>
<link>https://arxiv.org/abs/2502.16645</link>
<guid>https://arxiv.org/abs/2502.16645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CODESYNC</p><br /><br /><p><strong></strong> APICODESYNCPythonCODESYNCBENCH220API3300LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16645" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 23:04:14 GMT</pubDate>
</item>
<item>
<title>SubtaskSoRFTissue</title>
<link>https://arxiv.org/abs/2502.20127</link>
<guid>https://arxiv.org/abs/2502.20127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SoRFT</p><br /><br /><p><strong></strong> SoRFTSoRFTCoTPPOSWE-Bench VerifiedSWE-Bench LiteSoRFTSoRFT-Qwen-7BSWE-Bench Verified21.4%SoRFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20127" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:38:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.20395</link>
<guid>https://arxiv.org/abs/2502.20395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R2-T2</p><br /><br /><p><strong></strong> LMMsLLMsMoER2-T2R2-T2R2-T2LMM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20395" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:27:24 GMT</pubDate>
</item>
<item>
<title>LongRoPE2</title>
<link>https://arxiv.org/abs/2502.20082</link>
<guid>https://arxiv.org/abs/2502.20082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongRoPE2</p><br /><br /><p><strong></strong> LongRoPE2RoPEOODRoPERoPERoPELLaMA3-8BPhi3-mini-3.8BLongRoPE2LongRoPE2LLaMA3-8B128K98.5%10BMeta80</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.20082" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:22:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19613</link>
<guid>https://arxiv.org/abs/2502.19613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs Llama-3  Qwen-2.5 </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19613" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 22:15:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18779</link>
<guid>https://arxiv.org/abs/2502.18779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MDSDMDSD findings</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18779" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:03:36 GMT</pubDate>
</item>
<item>
<title>FSPO</title>
<link>https://arxiv.org/abs/2502.19312</link>
<guid>https://arxiv.org/abs/2502.19312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FSPO</p><br /><br /><p><strong></strong> FSPOLLMFSPOLLM1001500FSPO87%72%FSPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19312" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 11:09:15 GMT</pubDate>
</item>
<item>
<title>Drop-Upcycling</title>
<link>https://arxiv.org/abs/2502.19261</link>
<guid>https://arxiv.org/abs/2502.19261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Drop-Upcycling</p><br /><br /><p><strong></strong> Mixture of Experts (MoE) upcyclingMoEDrop-UpcyclingMoEDrop-UpcyclingMoEMoE5.9B13BFLOPs1/4MoE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19261" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 10:12:03 GMT</pubDate>
</item>
<item>
<title>Rank1</title>
<link>https://arxiv.org/abs/2502.18418</link>
<guid>https://arxiv.org/abs/2502.18418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Rank1 </p><br /><br /><p><strong></strong> Rank1  OpenAI  o1  Deepseek  R160 MS MARCO  R1  RAG Rank1 </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18418" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 09:41:49 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15885</link>
<guid>https://arxiv.org/abs/2502.15885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DOEI</p><br /><br /><p><strong></strong> DOEIDual Optimization of Embedding InformationWSSSCAMDOEIDOEIRGBDOEIWSSSPASCAL VOCMS COCO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15885" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 07:31:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19413</link>
<guid>https://arxiv.org/abs/2502.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLM95%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19413" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:18:26 GMT</pubDate>
</item>
<item>
<title>GHOST 2.0: generative high-fidelity one shot transfer of heads</title>
<link>https://arxiv.org/abs/2502.18417</link>
<guid>https://arxiv.org/abs/2502.18417</guid>
<content:encoded><![CDATA[
While the task of face swapping has recently gained attention in the research community, a related problem of head swapping remains largely unexplored. In addition to skin color transfer, head swap poses extra challenges, such as the need to preserve structural information of the whole head during synthesis and inpaint gaps between swapped head and background. In this paper, we address these concerns with GHOST 2.0, which consists of two problem-specific modules. First, we introduce enhanced Aligner model for head reenactment, which preserves identity information at multiple scales and is robust to extreme pose variations. Secondly, we use a Blender module that seamlessly integrates the reenacted head into the target background by transferring skin color and inpainting mismatched regions. Both modules outperform the baselines on the corresponding tasks, allowing to achieve state of the art results in head swapping. We also tackle complex cases, such as large difference in hair styles of source and target. Code is available at https://github.com/ai-forever/ghost-2.0
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 04:15:43 GMT</pubDate>
</item>
<item>
<title>BIG-Bench Extra Hard</title>
<link>https://arxiv.org/abs/2502.19187</link>
<guid>https://arxiv.org/abs/2502.19187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BIG-Bench Extra Hard</p><br /><br /><p><strong></strong> LLMsBIG-Bench Extra HardBBEHLLMsBIG-Bench HardBBHBBEHBBEH9.8%44.8%LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19187" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:43:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19414</link>
<guid>https://arxiv.org/abs/2502.19414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LMsREFUTEOpenAI o3-miniREFUTE9%48%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19414" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 02:36:29 GMT</pubDate>
</item>
<item>
<title>CritiQ</title>
<link>https://arxiv.org/abs/2502.19279</link>
<guid>https://arxiv.org/abs/2502.19279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CritiQ</p><br /><br /><p><strong></strong> CritiQCritiQ30CritiQ FlowCritiQ FlowCritiQ ScorerLlama 3.1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19279" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:47:02 GMT</pubDate>
</item>
<item>
<title>PosterSum</title>
<link>https://arxiv.org/abs/2502.17540</link>
<guid>https://arxiv.org/abs/2502.17540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PosterSum</p><br /><br /><p><strong></strong> PosterSum16,305MLLMsSegment & SummarizeMLLMsROUGE-L3.14%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17540" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:37:24 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17955</link>
<guid>https://arxiv.org/abs/2502.17955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 10,00013</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17955" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:17:58 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18772</link>
<guid>https://arxiv.org/abs/2502.18772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NLPPlutus-benPlutus-8B</p><br /><br /><p><strong></strong> LLMsPlutus-benPlutus-8BLLMPlutus-benNLPLLM22LLMsPlutus-benNLPLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18772" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 00:08:09 GMT</pubDate>
</item>
<item>
<title>Kanana</title>
<link>https://arxiv.org/abs/2502.18934</link>
<guid>https://arxiv.org/abs/2502.18934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kanana</p><br /><br /><p><strong></strong> KananaKananaKanana2132521</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18934" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:05:13 GMT</pubDate>
</item>
<item>
<title>DeltaBencho1-like</title>
<link>https://arxiv.org/abs/2502.19361</link>
<guid>https://arxiv.org/abs/2502.19361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeltaBencho1-like</p><br /><br /><p><strong></strong> DeltaBencho1-likeQwQDeepSeek-R1Chain-of-ThoughtCoTLLMsCoTDeltaBencho1-likeCoTCoTCoTo1-likePRMsDeltaBenchCoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19361" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 23:04:47 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2502.16284</link>
<guid>https://arxiv.org/abs/2502.16284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3DSpecFormer3D3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16284" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:29:40 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2502.18864</link>
<guid>https://arxiv.org/abs/2502.18864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> Gemini 2.0AIAIAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18864" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:18:06 GMT</pubDate>
</item>
<item>
<title>AISafetyLab: AI</title>
<link>https://arxiv.org/abs/2502.16776</link>
<guid>https://arxiv.org/abs/2502.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AISafetyLabAI</p><br /><br /><p><strong></strong> AIAIAISafetyLabAISafetyLabVicunaAISafetyLabGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16776" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:16:03 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19204</link>
<guid>https://arxiv.org/abs/2502.19204</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> (MDE)RGB3D-shot MDE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19204" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:10:20 GMT</pubDate>
</item>
<item>
<title>Manim</title>
<link>https://arxiv.org/abs/2502.19400</link>
<guid>https://arxiv.org/abs/2502.19400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TheoremExplainAgent</p><br /><br /><p><strong></strong> TheoremExplainAgentManim5TheoremExplainBench2405o3-mini93.8%0.77</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19400" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:07:49 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.19328</link>
<guid>https://arxiv.org/abs/2502.19328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Agentic Reward ModelingRewardAgentRewardAgentRewardAgentDPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.19328" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:05:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.18906</link>
<guid>https://arxiv.org/abs/2502.18906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUI</p><br /><br /><p><strong></strong> GUIRLVEMVEM-VEMVEMGUIAndroid-in-the-WildVEM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18906" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 22:02:50 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17910</link>
<guid>https://arxiv.org/abs/2502.17910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GPT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17910" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 18:40:15 GMT</pubDate>
</item>
<item>
<title>LDGen</title>
<link>https://arxiv.org/abs/2502.18302</link>
<guid>https://arxiv.org/abs/2502.18302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LDGen</p><br /><br /><p><strong></strong> LDGenLLMsCLIPT5LLMsLLMsLDGen-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18302" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 16:56:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17422</link>
<guid>https://arxiv.org/abs/2502.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MLLMsMLLMsMLLMsMLLMMLLMMLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17422" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 14:46:51 GMT</pubDate>
</item>
<item>
<title>Curie: Toward Rigorous and Automated Scientific Experimentation with AI Agents</title>
<link>https://arxiv.org/abs/2502.16069</link>
<guid>https://arxiv.org/abs/2502.16069</guid>
<content:encoded><![CDATA[
Scientific experimentation, a cornerstone of human progress, demands rigor in reliability, methodical control, and interpretability to yield meaningful results. Despite the growing capabilities of large language models (LLMs) in automating different aspects of the scientific process, automating rigorous experimentation remains a significant challenge. To address this gap, we propose Curie, an AI agent framework designed to embed rigor into the experimentation process through three key components: an intra-agent rigor module to enhance reliability, an inter-agent rigor module to maintain methodical control, and an experiment knowledge module to enhance interpretability. To evaluate Curie, we design a novel experimental benchmark composed of 46 questions across four computer science domains, derived from influential research papers, and widely adopted open-source projects. Compared to the strongest baseline tested, we achieve a 3.4times improvement in correctly answering experimental questions.Curie is open-sourced at https://github.com/Just-Curieous/Curie.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:51:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17814</link>
<guid>https://arxiv.org/abs/2502.17814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17814" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 12:34:59 GMT</pubDate>
</item>
<item>
<title>WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging</title>
<link>https://arxiv.org/abs/2502.18316</link>
<guid>https://arxiv.org/abs/2502.18316</guid>
<content:encoded><![CDATA[
We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with "None of the above", a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:53:44 GMT</pubDate>
</item>
<item>
<title>P2L</title>
<link>https://arxiv.org/abs/2502.14855</link>
<guid>https://arxiv.org/abs/2502.14855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">P2L</p><br /><br /><p><strong></strong> Prompt-to-LeaderboardP2LLLMP2LLLMBradley-TerryChatbot ArenaP2LP2LLLM20251Chatbot ArenaGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14855" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 10:43:07 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16825</link>
<guid>https://arxiv.org/abs/2502.16825</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsDPO21AlpacaEval 2 - 2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16825" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 09:40:23 GMT</pubDate>
</item>
<item>
<title>LaTIM: Mamba</title>
<link>https://arxiv.org/abs/2502.15612</link>
<guid>https://arxiv.org/abs/2502.15612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LaTIMMamba</p><br /><br /><p><strong></strong> LaTIMMamba-1Mamba-2SSMsMambaMambaLaTIMMamba</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15612" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 07:28:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17425</link>
<guid>https://arxiv.org/abs/2502.17425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MLLMMLLMMLLM2B23.6%0.7087B13.4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17425" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 02:37:36 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.17535</link>
<guid>https://arxiv.org/abs/2502.17535</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> LLMsKVLLMsLLMLLMLLMLLMLLMLLMKV</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17535" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 01:04:23 GMT</pubDate>
</item>
<item>
<title>K-LoRA</title>
<link>https://arxiv.org/abs/2502.18461</link>
<guid>https://arxiv.org/abs/2502.18461</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">K-LoRALoRA</p><br /><br /><p><strong></strong> LoRALoRALoRAK-LoRAK-LoRALoRAKLoRALoRA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18461" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:56:27 GMT</pubDate>
</item>
<item>
<title>Shakti VLM</title>
<link>https://arxiv.org/abs/2502.17092</link>
<guid>https://arxiv.org/abs/2502.17092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Shakti VLM</p><br /><br /><p><strong></strong> Shakti VLM1040ShaktiQK-NormalizationShakti-VLM-1BShakti-VLM-4BOCRShakti</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17092" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:38:42 GMT</pubDate>
</item>
<item>
<title>Scale-Distribution Decoupling: </title>
<link>https://arxiv.org/abs/2502.15499</link>
<guid>https://arxiv.org/abs/2502.15499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SDD</p><br /><br /><p><strong></strong> LLMs-SDDSDDLLMSDD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15499" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:26:11 GMT</pubDate>
</item>
<item>
<title>WebGamesAI</title>
<link>https://arxiv.org/abs/2502.18356</link>
<guid>https://arxiv.org/abs/2502.18356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebGames50AI</p><br /><br /><p><strong></strong> WebGames50AIAIGPT-4oClaude Computer-UseGemini-1.5-ProQwen2-VLAIAI43.1%95.7%AIwebgames.convergence.aiWebGames</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18356" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16794</link>
<guid>https://arxiv.org/abs/2502.16794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Intention-Informed Auditory Scene Understanding (II-ASU)Auditory Attention-Driven LLM (AAD-LLM)AAD-LLM(iEEG)AAD-LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16794" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:20:08 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17262</link>
<guid>https://arxiv.org/abs/2502.17262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> CODLLMsCODLLM70B LLMCOD1.36%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17262" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:18:24 GMT</pubDate>
</item>
<item>
<title>SpargeAttn</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpargeAttn</p><br /><br /><p><strong></strong> SpargeAttnSpargeAttnsoftmaxSpargeAttn</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18137" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:04:57 GMT</pubDate>
</item>
<item>
<title>SWE-RL</title>
<link>https://arxiv.org/abs/2502.18449</link>
<guid>https://arxiv.org/abs/2502.18449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SWE-RL</p><br /><br /><p><strong></strong> SWE-RLDeepSeek-R1SWE-RLLLMs41.0%LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18449" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:03:08 GMT</pubDate>
</item>
<item>
<title>OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference</title>
<link>https://arxiv.org/abs/2502.18411</link>
<guid>https://arxiv.org/abs/2502.18411</guid>
<content:encoded><![CDATA[
Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 22:01:56 GMT</pubDate>
</item>
<item>
<title>ART</title>
<link>https://arxiv.org/abs/2502.18364</link>
<guid>https://arxiv.org/abs/2502.18364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ART</p><br /><br /><p><strong></strong> ART12ART</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.18364" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:50:19 GMT</pubDate>
</item>
<item>
<title>KV-Edit</title>
<link>https://arxiv.org/abs/2502.17363</link>
<guid>https://arxiv.org/abs/2502.17363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KV-Edit </p><br /><br /><p><strong></strong> KV-EditKVKVO(1)DiTKV-Edit</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17363" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 21:36:19 GMT</pubDate>
</item>
<item>
<title>MutaGReP: </title>
<link>https://arxiv.org/abs/2502.15872</link>
<guid>https://arxiv.org/abs/2502.15872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MutaGReP</p><br /><br /><p><strong></strong> MutaGRePLLMLLMMutaGRePLongCodeArena5%128KGPT-4oMutaGRePQwen 2.5 Coder32B72BLongCodeArenaGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15872" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 19:35:42 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2502.14247</link>
<guid>https://arxiv.org/abs/2502.14247</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3D3D3DVAE3DRGBPBR3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14247" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 17:06:48 GMT</pubDate>
</item>
<item>
<title>(LAM)</title>
<link>https://arxiv.org/abs/2502.15919</link>
<guid>https://arxiv.org/abs/2502.15919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">7,500</p><br /><br /><p><strong></strong> AI(LAM)4847,5000.33R^2=0.30</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15919" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 16:46:31 GMT</pubDate>
</item>
<item>
<title>Agentic Long-Context Understanding: LLMs</title>
<link>https://arxiv.org/abs/2502.15920</link>
<guid>https://arxiv.org/abs/2502.15920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgenticLULLMs</p><br /><br /><p><strong></strong> Agentic Long-Context UnderstandingAgenticLULLMsAgenticLUChain-of-ClarificationsCoCNarrativeQA97.8%CoCAgenticLULLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15920" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:50:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16810</link>
<guid>https://arxiv.org/abs/2502.16810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs listings 123 LLM </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16810" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 12:26:42 GMT</pubDate>
</item>
<item>
<title>InductionBench</title>
<link>https://arxiv.org/abs/2502.15823</link>
<guid>https://arxiv.org/abs/2502.15823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InductionBench</p><br /><br /><p><strong></strong> LLMsLLMsInductionBenchLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15823" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:58:10 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15799</link>
<guid>https://arxiv.org/abs/2502.15799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsOpenSafetyMini4LLaMAMistral442</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15799" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>XCOVID-19</title>
<link>https://arxiv.org/abs/2502.16622</link>
<guid>https://arxiv.org/abs/2502.16622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">COVID-19</p><br /><br /><p><strong></strong> XCXRCOVID-19COVIDDenseNet16180%77.3%83.9%70%ViT0.5676</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16622" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 11:02:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14429</link>
<guid>https://arxiv.org/abs/2502.14429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Instant Confidence COMETEarly-Exit COMET50%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14429" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:17:04 GMT</pubDate>
</item>
<item>
<title>MegaLoc</title>
<link>https://arxiv.org/abs/2502.17237</link>
<guid>https://arxiv.org/abs/2502.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MegaLoc</p><br /><br /><p><strong></strong> MegaLoc3DSLAMMegaLocMegaLocLaMARMegaLoc</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17237" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 09:00:19 GMT</pubDate>
</item>
<item>
<title>TAME Agent Framework</title>
<link>https://arxiv.org/abs/2502.15425</link>
<guid>https://arxiv.org/abs/2502.15425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAME Agent Framework</p><br /><br /><p><strong></strong> TAME Agent Framework (TAG) TAGLevelEnvTAGTAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15425" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:51:02 GMT</pubDate>
</item>
<item>
<title>Stable-SPAM: 4</title>
<link>https://arxiv.org/abs/2502.17055</link>
<guid>https://arxiv.org/abs/2502.17055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Stable-SPAM4</p><br /><br /><p><strong></strong> 4SPAMStable-SPAMStable-SPAM l_2-SPAMAdamStable-SPAM4AdamSPAMStable-SPAM4LLaMA-1BAdamBF16 LLaMA-1B24Stable-SPAMAdam</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17055" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 05:40:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14132</link>
<guid>https://arxiv.org/abs/2502.14132</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Twitter/XMetaTwitter/X</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14132" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:11:18 GMT</pubDate>
</item>
<item>
<title>CVS</title>
<link>https://arxiv.org/abs/2502.13074</link>
<guid>https://arxiv.org/abs/2502.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CVS</p><br /><br /><p><strong></strong> Cori-Vauquelin-Schaeffer (CVS)AldousCVS</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13074" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 04:03:39 GMT</pubDate>
</item>
<item>
<title>M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</title>
<link>https://arxiv.org/abs/2502.15167</link>
<guid>https://arxiv.org/abs/2502.15167</guid>
<content:encoded><![CDATA[
The rapid advancement of AI-generated image (AGI) models has introduced significant challenges in evaluating their quality, which requires considering multiple dimensions such as perceptual quality, prompt correspondence, and authenticity. To address these challenges, we propose M3-AGIQA, a comprehensive framework for AGI quality assessment that is Multimodal, Multi-Round, and Multi-Aspect. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) as joint text and image encoders and distills advanced captioning capabilities from online MLLMs into a local model via Low-Rank Adaptation (LoRA) fine-tuning. The framework includes a structured multi-round evaluation mechanism, where intermediate image descriptions are generated to provide deeper insights into the quality, correspondence, and authenticity aspects. To align predictions with human perceptual judgments, a predictor constructed by an xLSTM and a regression head is incorporated to process sequential logits and predict Mean Opinion Scores (MOSs). Extensive experiments conducted on multiple benchmark datasets demonstrate that M3-AGIQA achieves state-of-the-art performance, effectively capturing nuanced aspects of AGI quality. Furthermore, cross-dataset validation confirms its strong generalizability. The code is available at https://github.com/strawhatboy/M3-AGIQA.
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 03:36:50 GMT</pubDate>
</item>
<item>
<title>GCC</title>
<link>https://arxiv.org/abs/2502.17435</link>
<guid>https://arxiv.org/abs/2502.17435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GCC</p><br /><br /><p><strong></strong> GCCGCC1) 2) 3) GCC25%5.154.32</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17435" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 02:06:00 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16707</link>
<guid>https://arxiv.org/abs/2502.16707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> (VLM)VLMVLMVLM(MCTS)</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16707" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 01:02:05 GMT</pubDate>
</item>
<item>
<title>MONSTER</title>
<link>https://arxiv.org/abs/2502.15122</link>
<guid>https://arxiv.org/abs/2502.15122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MONSTER</p><br /><br /><p><strong></strong> MONSTERMONashUCRUEA217255MONSTER</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15122" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:37:53 GMT</pubDate>
</item>
<item>
<title>X-Dancer</title>
<link>https://arxiv.org/abs/2502.17414</link>
<guid>https://arxiv.org/abs/2502.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Dancer</p><br /><br /><p><strong></strong> X-Dancer-2D3DX-Dancer2D2DX-Dancer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17414" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:17:51 GMT</pubDate>
</item>
<item>
<title>VideoGrain-shot</title>
<link>https://arxiv.org/abs/2502.17258</link>
<guid>https://arxiv.org/abs/2502.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoGrain</p><br /><br /><p><strong></strong> VideoGrain-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17258" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:13:12 GMT</pubDate>
</item>
<item>
<title>RIFLEx</title>
<link>https://arxiv.org/abs/2502.15894</link>
<guid>https://arxiv.org/abs/2502.15894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RIFLEx</p><br /><br /><p><strong></strong> RIFLExRIFLEx23</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15894" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 25 Feb 2025 00:09:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17407</link>
<guid>https://arxiv.org/abs/2502.17407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MCLM55ORMORMBFQwen2.5-1.5B MathMR1-1.5BQwen2.5-1.5B MathORMMCLM35.8MR1-1.5BBF35.2NFLOPsBFAIME201.94MCLMMR1-1.5B</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17407" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:37:53 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15987</link>
<guid>https://arxiv.org/abs/2502.15987</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> AIAIWang</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15987" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:30:36 GMT</pubDate>
</item>
<item>
<title>Audio-FLAN: </title>
<link>https://arxiv.org/abs/2502.16584</link>
<guid>https://arxiv.org/abs/2502.16584</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Audio-FLAN</p><br /><br /><p><strong></strong> Recent advancements in audio tokenization have significantly improved the integration of audio capabilities into large language models (LLMs). Audio-FLAN801Audio-FLANHuggingFaceGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16584" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:20 GMT</pubDate>
</item>
<item>
<title>Slam24GPURecipe</title>
<link>https://arxiv.org/abs/2502.15814</link>
<guid>https://arxiv.org/abs/2502.15814</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SlamGPU</p><br /><br /><p><strong></strong> SlamGPU24SLMSLMSLMSLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15814" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 23:14:12 GMT</pubDate>
</item>
<item>
<title>CTM</title>
<link>https://arxiv.org/abs/2502.16922</link>
<guid>https://arxiv.org/abs/2502.16922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CTM</p><br /><br /><p><strong></strong> (CTM)CTMCTM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16922" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:48:30 GMT</pubDate>
</item>
<item>
<title>DICEPTION</title>
<link>https://arxiv.org/abs/2502.17157</link>
<guid>https://arxiv.org/abs/2502.17157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICEPTION</p><br /><br /><p><strong></strong> DICEPTIONDICEPTION0.06%600K1BDICEPTION501%DICEPTION</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17157" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:39:29 GMT</pubDate>
</item>
<item>
<title>GOATLoRA</title>
<link>https://arxiv.org/abs/2502.16894</link>
<guid>https://arxiv.org/abs/2502.16894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GOATSVDLoRA</p><br /><br /><p><strong></strong> (LoRA)(Full FT)(SVD)LOraLoRA(GOAT)SVD25GOAT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16894" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:35:41 GMT</pubDate>
</item>
<item>
<title>Mobile-Agent-V</title>
<link>https://arxiv.org/abs/2502.17110</link>
<guid>https://arxiv.org/abs/2502.17110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mobile-Agent-V</p><br /><br /><p><strong></strong> AIMobile-Agent-VMobile-Agent-VMobile-Agent-V30%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17110" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:31:17 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.17129</link>
<guid>https://arxiv.org/abs/2502.17129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsLLMsLLMsLLMs10</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.17129" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:27:11 GMT</pubDate>
</item>
<item>
<title>CodeCriticBench</title>
<link>https://arxiv.org/abs/2502.16614</link>
<guid>https://arxiv.org/abs/2502.16614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeCriticBenchLLMs</p><br /><br /><p><strong></strong> CodeCriticBench(LLMs)CodeCriticBenchLLMsCodeCriticBench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16614" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 22:17:28 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16033</link>
<guid>https://arxiv.org/abs/2502.16033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MMIRMLLMs534/</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16033" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:50 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.16701</link>
<guid>https://arxiv.org/abs/2502.16701</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.16701" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 21:59:15 GMT</pubDate>
</item>
<item>
<title>Seq2Exp</title>
<link>https://arxiv.org/abs/2502.13991</link>
<guid>https://arxiv.org/abs/2502.13991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seq2Exp</p><br /><br /><p><strong></strong> DNASeq2ExpSeq2ExpDNADNABetaSeq2ExpMACS3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13991" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 17:06:56 GMT</pubDate>
</item>
<item>
<title>RareScale</title>
<link>https://arxiv.org/abs/2502.15069</link>
<guid>https://arxiv.org/abs/2502.15069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RareScale</p><br /><br /><p><strong></strong> RareScaleLLMsLLMsRareScaleLLMsRareScale575LLMs17%Top-588.8%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15069" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 15:59:07 GMT</pubDate>
</item>
<item>
<title>Tree-of-Debate</title>
<link>https://arxiv.org/abs/2502.14767</link>
<guid>https://arxiv.org/abs/2502.14767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tree-of-Debate</p><br /><br /><p><strong></strong> Tree-of-DebateToDLLMsToDToD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14767" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:53:11 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14122</link>
<guid>https://arxiv.org/abs/2502.14122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsUN19942024UNSCLLMsUNBenchLLMsAIUNBenchGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14122" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 12:18:28 GMT</pubDate>
</item>
<item>
<title>FantasyID</title>
<link>https://arxiv.org/abs/2502.13995</link>
<guid>https://arxiv.org/abs/2502.13995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FantasyID</p><br /><br /><p><strong></strong> FantasyID3D2D2D3DDiTFantasyID</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13995" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 11:28:12 GMT</pubDate>
</item>
<item>
<title>MedHallu</title>
<link>https://arxiv.org/abs/2502.14302</link>
<guid>https://arxiv.org/abs/2502.14302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedHallu</p><br /><br /><p><strong></strong> LLMsMedHalluPubMedQA10,000LLMsGPT-4oLlama-3.1UltraMedicalF10.625F138%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14302" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:54:53 GMT</pubDate>
</item>
<item>
<title>mStyleDistance</title>
<link>https://arxiv.org/abs/2502.15168</link>
<guid>https://arxiv.org/abs/2502.15168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mStyleDistance</p><br /><br /><p><strong></strong> mStyleDistanceSTEL-or-ContentmStyleDistance</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15168" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 10:33:37 GMT</pubDate>
</item>
<item>
<title>EgoSpeak</title>
<link>https://arxiv.org/abs/2502.14892</link>
<guid>https://arxiv.org/abs/2502.14892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EgoSpeak</p><br /><br /><p><strong></strong> EgoSpeakEgoSpeakRGBYT-ConversationYouTubeEasyComEgo4DEgoSpeak</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14892" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 08:37:35 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.15086</link>
<guid>https://arxiv.org/abs/2502.15086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">U-SAFEBENCHLLM</p><br /><br /><p><strong></strong> LLMLLMU-SAFEBENCHLLM18LLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15086" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:41:08 GMT</pubDate>
</item>
<item>
<title>WHAC</title>
<link>https://arxiv.org/abs/2403.12959</link>
<guid>https://arxiv.org/abs/2403.12959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WHAC</p><br /><br /><p><strong></strong> WHACSMPL-XSMPL-XWHAC-A-Mole</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2403.12959" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 07:37:26 GMT</pubDate>
</item>
<item>
<title>Evaluating Multimodal Generative AI with Korean Educational Standards</title>
<link>https://arxiv.org/abs/2502.15422</link>
<guid>https://arxiv.org/abs/2502.15422</guid>
<content:encoded><![CDATA[
This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests. KoNET comprises four exams: the Korean Elementary General Educational Development Test (KoEGED), Middle (KoMGED), High (KoHGED), and College Scholastic Ability Test (KoCSAT). These exams are renowned for their rigorous standards and diverse questions, facilitating a comprehensive analysis of AI performance across different educational levels. By focusing on Korean, KoNET provides insights into model performance in less-explored languages. We assess a range of models - open-source, open-access, and closed APIs - by examining difficulties, subject diversity, and human error rates. The code and dataset builder will be made fully open-sourced at https://github.com/naver-ai/KoNET.
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:58:46 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14975</link>
<guid>https://arxiv.org/abs/2502.14975</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs</p><br /><br /><p><strong></strong> LLMs1156LLMsGPT-4oClaude-3.5 SonnetMistral-largeClaude-3.58.69/1086.5125.62< 0.2243.20%< 1%LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14975" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 06:44:41 GMT</pubDate>
</item>
<item>
<title>KITAB-Bench: OCR</title>
<link>https://arxiv.org/abs/2502.14949</link>
<guid>https://arxiv.org/abs/2502.14949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KITAB-BenchOCR</p><br /><br /><p><strong></strong> (RAG)OCRKITAB-Bench8809OCR936GPT-4Gemini(CER)OCR60%OCRPDFMarkdownGemini-2.0-Flash65%OCR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14949" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:43:47 GMT</pubDate>
</item>
<item>
<title>ReQFlow</title>
<link>https://arxiv.org/abs/2502.14637</link>
<guid>https://arxiv.org/abs/2502.14637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReQFlow</p><br /><br /><p><strong></strong> ReQFlowSLERPQFlowReQFlow300RFDiffusion37Genie262</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14637" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 05:35:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13189</link>
<guid>https://arxiv.org/abs/2502.13189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoBA</p><br /><br /><p><strong></strong> AGILLMsMoBAMoBAMoEMoBAKimi</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13189" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:52:30 GMT</pubDate>
</item>
<item>
<title>JL1-CD</title>
<link>https://arxiv.org/abs/2502.13407</link>
<guid>https://arxiv.org/abs/2502.13407</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JL1-CDMTKD</p><br /><br /><p><strong></strong> (CD)CDJL1-CD50000.50.75512 x 512(MTKD)CDJL1-CDSYSU-CDMTKDCD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13407" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 04:29:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15007</link>
<guid>https://arxiv.org/abs/2502.15007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsMMLUBABILong-4kLLM-MicroscopeLogit Lens</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15007" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 02:07:41 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11663</link>
<guid>https://arxiv.org/abs/2502.11663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MAEMaskGWMMaskGWM-longMaskGWM-mviewDiffusion TransformerNusceneOpenDV-2KWaymo-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11663" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:16:03 GMT</pubDate>
</item>
<item>
<title>CrossOver</title>
<link>https://arxiv.org/abs/2502.15011</link>
<guid>https://arxiv.org/abs/2502.15011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CrossOver</p><br /><br /><p><strong></strong> CrossOverRGBCADCrossOverScanNet3RScanCrossOver</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15011" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 01:13:24 GMT</pubDate>
</item>
<item>
<title>VLM^2-Bench</title>
<link>https://arxiv.org/abs/2502.12084</link>
<guid>https://arxiv.org/abs/2502.12084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VLM^2-BenchVLMs93000VLMGPT-4oGPT-4o34.80%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12084" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:36:34 GMT</pubDate>
</item>
<item>
<title>LightThinker</title>
<link>https://arxiv.org/abs/2502.15589</link>
<guid>https://arxiv.org/abs/2502.15589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LightThinker</p><br /><br /><p><strong></strong> LightThinkerLLMsLightThinkerDepLightThinker</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15589" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:07:05 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2502.15657</link>
<guid>https://arxiv.org/abs/2502.15657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIAI</p><br /><br /><p><strong></strong> AIAIAIAIAI/actionsAIAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15657" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 24 Feb 2025 00:02:52 GMT</pubDate>
</item>
<item>
<title>StructFlowBench</title>
<link>https://arxiv.org/abs/2502.14494</link>
<guid>https://arxiv.org/abs/2502.14494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StructFlowBench </p><br /><br /><p><strong></strong> LLMsStructFlowBench13LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14494" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:43:43 GMT</pubDate>
</item>
<item>
<title>UPCORE</title>
<link>https://arxiv.org/abs/2502.15082</link>
<guid>https://arxiv.org/abs/2502.15082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UPCORE</p><br /><br /><p><strong></strong> UPCOREUtility-Preserving Coreset SelectionUPCOREUPCOREAUCUPCOREAUC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15082" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 23:17:33 GMT</pubDate>
</item>
<item>
<title>PhotoDoodle</title>
<link>https://arxiv.org/abs/2502.14397</link>
<guid>https://arxiv.org/abs/2502.14397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhotoDoodle</p><br /><br /><p><strong></strong> PhotoDoodleOmniEditorEditLoRAPhotoDoodlePhotoDoodle</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14397" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:55:04 GMT</pubDate>
</item>
<item>
<title>f</title>
<link>https://arxiv.org/abs/2502.15681</link>
<guid>https://arxiv.org/abs/2502.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">f</p><br /><br /><p><strong></strong> f-distillfKullback-Leiblerfff-distillKLJensen-Shannonf-distillJensen-Shannonf-distillImageNet64MS-COCO-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15681" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:24:55 GMT</pubDate>
</item>
<item>
<title>SIFT</title>
<link>https://arxiv.org/abs/2502.14922</link>
<guid>https://arxiv.org/abs/2502.14922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SIFT</p><br /><br /><p><strong></strong> **Stick to the Facts (SIFT)**SIFT*Sticker*SIFTStickerSIFT3B100B+GSM8KMATH-500AIME2024DeepSeek-R1pass@178.33%85.67%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14922" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:17:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14905</link>
<guid>https://arxiv.org/abs/2502.14905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMDeepSeek R11.52R1120GRPO3SFTDeepSeek R1Gemini 2.0 Flash</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14905" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 22:11:17 GMT</pubDate>
</item>
<item>
<title>Mol-LLaMA</title>
<link>https://arxiv.org/abs/2502.13449</link>
<guid>https://arxiv.org/abs/2502.13449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mol-LLaMA</p><br /><br /><p><strong></strong> Mol-LLaMAMol-LLaMA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13449" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:52:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15027</link>
<guid>https://arxiv.org/abs/2502.15027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InterFeedback</p><br /><br /><p><strong></strong> LMMInterFeedbackLMMInterFeedback-BenchMMM-ProMathVerse10LMMInterFeedback-Human120OpenAI-o1Claude-3.5-SonnetLMMOpenAI-o150%LMM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15027" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:44:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.15631</link>
<guid>https://arxiv.org/abs/2502.15631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> o1-minio3-miniOmni-MATHo3-mini(m)o3-mini(h)o3-mini(m)o3-mini(m)</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.15631" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:40:17 GMT</pubDate>
</item>
<item>
<title>SurveyX</title>
<link>https://arxiv.org/abs/2502.14776</link>
<guid>https://arxiv.org/abs/2502.14776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyX</p><br /><br /><p><strong></strong> LLMsSurveyXAttributeTreeSurveyX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14776" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 23 Feb 2025 21:39:54 GMT</pubDate>
</item>
<item>
<title>MODis</title>
<link>https://arxiv.org/abs/2502.11262</link>
<guid>https://arxiv.org/abs/2502.11262</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MODis</p><br /><br /><p><strong></strong> AIMODisMODisMODis</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11262" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 21:19:35 GMT</pubDate>
</item>
<item>
<title>S-VCO</title>
<link>https://arxiv.org/abs/2502.13928</link>
<guid>https://arxiv.org/abs/2502.13928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S-VCO</p><br /><br /><p><strong></strong> VLMsS-VCOVLMMVC-VLM22%S-VCOVLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13928" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:42:50 GMT</pubDate>
</item>
<item>
<title>Generating $$-Functional Molecules Using STGG+ with Active Learning</title>
<link>https://arxiv.org/abs/2502.14842</link>
<guid>https://arxiv.org/abs/2502.14842</guid>
<content:encoded><![CDATA[
Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explore new chemical spaces but often conducts 'reward-hacking' and generates non-synthesizable molecules. In this work, we address this problem by integrating a state-of-the-art supervised learning method, STGG+, in an active learning loop. Our approach iteratively generates, evaluates, and fine-tunes STGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We apply STGG+AL to the design of organic pi-functional materials, specifically two challenging tasks: 1) generating highly absorptive molecules characterized by high oscillator strength and 2) designing absorptive molecules with reasonable oscillator strength in the near-infrared (NIR) range. The generated molecules are validated and rationalized in-silico with time-dependent density functional theory. Our results demonstrate that our method is highly effective in generating novel molecules with high oscillator strength, contrary to existing methods such as reinforcement learning (RL) methods. We open-source our active-learning code along with our Conjugated-xTB dataset containing 2.9 million pi-conjugated molecules and the function for approximating the oscillator strength and absorption wavelength (based on sTDA-xTB).
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 13:05:36 GMT</pubDate>
</item>
<item>
<title>CHASE</title>
<link>https://arxiv.org/abs/2502.14678</link>
<guid>https://arxiv.org/abs/2502.14678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CHASE</p><br /><br /><p><strong></strong> CHASELLMsCHASECHASELLMs40-60%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14678" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models</title>
<link>https://arxiv.org/abs/2502.14191</link>
<guid>https://arxiv.org/abs/2502.14191</guid>
<content:encoded><![CDATA[
Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 11:34:53 GMT</pubDate>
</item>
<item>
<title>LServe</title>
<link>https://arxiv.org/abs/2502.14866</link>
<guid>https://arxiv.org/abs/2502.14866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LServe</p><br /><br /><p><strong></strong> KVLServeLServeKVKVKVLServeLLM2.91.3-2.1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14866" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 09:39:36 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14044</link>
<guid>https://arxiv.org/abs/2502.14044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LMMsLMMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14044" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:26:31 GMT</pubDate>
</item>
<item>
<title>NaviCluesNavig</title>
<link>https://arxiv.org/abs/2502.14638</link>
<guid>https://arxiv.org/abs/2502.14638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NaviCluesNavig</p><br /><br /><p><strong></strong> NaviCluesGeoGuessrNavigNavig14%1000</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14638" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:18:34 GMT</pubDate>
</item>
<item>
<title>HippoRAG 2: </title>
<link>https://arxiv.org/abs/2502.14802</link>
<guid>https://arxiv.org/abs/2502.14802</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HippoRAG 2 </p><br /><br /><p><strong></strong> HippoRAG 2 LLMsRAGHippoRAG 2  PageRank  LLM HippoRAG 2  RAG  7% </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14802" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 08:00:41 GMT</pubDate>
</item>
<item>
<title>CLIPPER</title>
<link>https://arxiv.org/abs/2502.14854</link>
<guid>https://arxiv.org/abs/2502.14854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLIPPER</p><br /><br /><p><strong></strong> CLIPPERCLIPPERCLIPPERCLIPPER19K28%76%NarrativeQA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14854" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:52:55 GMT</pubDate>
</item>
<item>
<title>LLM-based User Profile Management for Recommender System</title>
<link>https://arxiv.org/abs/2502.14541</link>
<guid>https://arxiv.org/abs/2502.14541</guid>
<content:encoded><![CDATA[
The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 07:16:00 GMT</pubDate>
</item>
<item>
<title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title>
<link>https://arxiv.org/abs/2502.14502</link>
<guid>https://arxiv.org/abs/2502.14502</guid>
<content:encoded><![CDATA[
The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:29:18 GMT</pubDate>
</item>
<item>
<title>How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild</title>
<link>https://arxiv.org/abs/2502.12769</link>
<guid>https://arxiv.org/abs/2502.12769</guid>
<content:encoded><![CDATA[
In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:28:42 GMT</pubDate>
</item>
<item>
<title>S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.12853</link>
<guid>https://arxiv.org/abs/2502.12853</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated the effectiveness of LLM test-time scaling. However, existing approaches to incentivize LLMs' deep thinking abilities generally require large-scale data or significant training efforts. Meanwhile, it remains unclear how to improve the thinking abilities of less powerful base models. In this work, we introduce S^2R, an efficient framework that enhances LLM reasoning by teaching models to self-verify and self-correct during inference. Specifically, we first initialize LLMs with iterative self-verification and self-correction behaviors through supervised fine-tuning on carefully curated data. The self-verification and self-correction skills are then further strengthened by both outcome-level and process-level reinforcement learning, with minimized resource requirements, enabling the model to adaptively refine its reasoning process during inference. Our results demonstrate that, with only 3.1k self-verifying and self-correcting behavior initialization samples, Qwen2.5-math-7B achieves an accuracy improvement from 51.0\% to 81.6\%, outperforming models trained on an equivalent amount of long-CoT distilled data. Extensive experiments and analysis based on three base models across both in-domain and out-of-domain benchmarks validate the effectiveness of S^2R. Our code and data are available at https://github.com/NineAbyss/S2R.
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 05:00:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14409</link>
<guid>https://arxiv.org/abs/2502.14409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsSUnsETLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14409" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13759</link>
<guid>https://arxiv.org/abs/2502.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> GeoCompGeoCoTGeoEvalGeoComp740K2500300GeoCoTGeoCoTGeoEval25%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13759" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 03:33:28 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14372</link>
<guid>https://arxiv.org/abs/2502.14372</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 612</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14372" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 21 Feb 2025 01:11:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14258</link>
<guid>https://arxiv.org/abs/2502.14258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Temporal Heads2004...</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14258" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 23:02:42 GMT</pubDate>
</item>
<item>
<title>Set-and-Sequence</title>
<link>https://arxiv.org/abs/2502.14844</link>
<guid>https://arxiv.org/abs/2502.14844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Set-and-Sequence</p><br /><br /><p><strong></strong> Set-and-SequenceDiffusion TransformersLoRALoRALoRASet-and-Sequence</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14844" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:41:47 GMT</pubDate>
</item>
<item>
<title>PC-Agent: </title>
<link>https://arxiv.org/abs/2502.14282</link>
<guid>https://arxiv.org/abs/2502.14282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PC-AgentPC</p><br /><br /><p><strong></strong> PC-AgentPC(MLLM)APMPC-EvalPC-Agent32%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14282" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:48 GMT</pubDate>
</item>
<item>
<title>LongWriter-V-22k: </title>
<link>https://arxiv.org/abs/2502.14834</link>
<guid>https://arxiv.org/abs/2502.14834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongWriter-V-22kLVLM</p><br /><br /><p><strong></strong> LVLM128kSFTLongWriter-V-22k22,158010,000SFTDPOIterDPOMMLongBench-WriteVLMLongWriter-V-22kIterDPO7BGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14834" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:39:21 GMT</pubDate>
</item>
<item>
<title>CoSyn</title>
<link>https://arxiv.org/abs/2502.14846</link>
<guid>https://arxiv.org/abs/2502.14846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSynLLMVLM</p><br /><br /><p><strong></strong> CoSyn(VLMs)CoSyn(LLM)PythonHTMLLaTeXCoSyn40270Llama 3.2GPT-4VGemini 1.5 FlashCoSynVLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14846" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:38:36 GMT</pubDate>
</item>
<item>
<title>SigLIP 2-</title>
<link>https://arxiv.org/abs/2502.14786</link>
<guid>https://arxiv.org/abs/2502.14786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SigLIP 2-</p><br /><br /><p><strong></strong> SigLIP 2SigLIP-SigLIP 2-SigLIP 2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14786" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:33:22 GMT</pubDate>
</item>
<item>
<title>RelaCtrl</title>
<link>https://arxiv.org/abs/2502.14377</link>
<guid>https://arxiv.org/abs/2502.14377</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RelaCtrlDiffusion Transformer</p><br /><br /><p><strong></strong> RelaCtrlDiffusion TransformerTransformerControlNetRelaCtrlTDSMPixArt-deltaRelaCtrl15%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14377" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:30:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14768</link>
<guid>https://arxiv.org/abs/2502.14768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DeepSeek-R17B5KAIMEAMC</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14768" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:19:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14739</link>
<guid>https://arxiv.org/abs/2502.14739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SuperGPQA285</p><br /><br /><p><strong></strong> LLMs200SuperGPQA285-LLMLLMLLMs61.82%80</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14739" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:15:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.14669</link>
<guid>https://arxiv.org/abs/2502.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsSFTGRPOSFT86%GRPO93%GRPO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14669" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:11:45 GMT</pubDate>
</item>
<item>
<title>Meta MLGym LLM </title>
<link>https://arxiv.org/abs/2502.14499</link>
<guid>https://arxiv.org/abs/2502.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"> Meta MLGym  MLGym-Bench LLM  AI </p><br /><br /><p><strong></strong>  Meta MLGym  MLGym-BenchLLM AI MLGym-Bench 13 AI  AI  LLM  LLM  AI </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.14499" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:08:38 GMT</pubDate>
</item>
<item>
<title>S*: Test Time Scaling for Code Generation</title>
<link>https://arxiv.org/abs/2502.14382</link>
<guid>https://arxiv.org/abs/2502.14382</guid>
<content:encoded><![CDATA[
Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 22:04:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13270</link>
<guid>https://arxiv.org/abs/2502.13270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> REALTALK21</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13270" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 16:00:25 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13791</link>
<guid>https://arxiv.org/abs/2502.13791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> MemoryCodeLLMsGPT-4oLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13791" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 14:34:52 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13908</link>
<guid>https://arxiv.org/abs/2502.13908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsIRNLPLLMsSIGIR 2024LLMJudge42LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13908" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 13:47:47 GMT</pubDate>
</item>
<item>
<title>MMTEB</title>
<link>https://arxiv.org/abs/2502.13595</link>
<guid>https://arxiv.org/abs/2502.13595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMTEB</p><br /><br /><p><strong></strong> MMTEB500250multilingual-e5-large-instruct5.6-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13595" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:26:53 GMT</pubDate>
</item>
<item>
<title>AI</title>
<link>https://arxiv.org/abs/2502.13138</link>
<guid>https://arxiv.org/abs/2502.13138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIDE</p><br /><br /><p><strong></strong> AIAIDEAIDEAIDEKaggleOpenAIMLE-BenchMETRsRE-Bench</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13138" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:23:27 GMT</pubDate>
</item>
<item>
<title>MVL-SIB: A Massively Multilingual Vision-Language Benchmark for Cross-Modal Topical Matching</title>
<link>https://arxiv.org/abs/2502.12852</link>
<guid>https://arxiv.org/abs/2502.12852</guid>
<content:encoded><![CDATA[
Existing multilingual vision-language (VL) benchmarks often only cover a handful of languages. Consequently, evaluations of large vision-language models (LVLMs) predominantly target high-resource languages, underscoring the need for evaluation data for low-resource languages. To address this limitation, we introduce MVL-SIB, a massively multilingual vision-language benchmark that evaluates both cross-modal and text-only topical matching across 205 languages -- over 100 more than the most multilingual existing VL benchmarks encompass. We then benchmark a range of of open-weight LVLMs together with GPT-4o(-mini) on MVL-SIB. Our results reveal that LVLMs struggle in cross-modal topic matching in lower-resource languages, performing no better than chance on languages like N'Koo. Our analysis further reveals that VL support in LVLMs declines disproportionately relative to textual support for lower-resource languages, as evidenced by comparison of cross-modal and text-only topical matching performance. We further observe that open-weight LVLMs do not benefit from representing a topic with more than one image, suggesting that these models are not yet fully effective at handling multi-image tasks. By correlating performance on MVL-SIB with other multilingual VL benchmarks, we highlight that MVL-SIB serves as a comprehensive probe of multilingual VL understanding in LVLMs.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:09:53 GMT</pubDate>
</item>
<item>
<title>PGMRSPARQL</title>
<link>https://arxiv.org/abs/2502.13369</link>
<guid>https://arxiv.org/abs/2502.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PGMRLLMSPARQLURI</p><br /><br /><p><strong></strong> PGMRLLMsSPARQLLLMsSPARQLURIPGMRURILLMsPGMRURI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13369" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 12:07:02 GMT</pubDate>
</item>
<item>
<title>SplatDiff</title>
<link>https://arxiv.org/abs/2502.12752</link>
<guid>https://arxiv.org/abs/2502.12752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SplatDiff</p><br /><br /><p><strong></strong> SplatDiffSplatDiffSplatDiff-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12752" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:53:49 GMT</pubDate>
</item>
<item>
<title>TESS 2</title>
<link>https://arxiv.org/abs/2502.13917</link>
<guid>https://arxiv.org/abs/2502.13917</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TESS 2 </p><br /><br /><p><strong></strong> TESS 2 TESS 2 TESS 2 </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13917" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 10:46:55 GMT</pubDate>
</item>
<item>
<title>REFIND</title>
<link>https://arxiv.org/abs/2502.13622</link>
<guid>https://arxiv.org/abs/2502.13622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REFIND</p><br /><br /><p><strong></strong> LLMREFINDLLMREFINDCSRLLMREFINDIoULLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13622" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 07:25:12 GMT</pubDate>
</item>
<item>
<title>LoRAM</title>
<link>https://arxiv.org/abs/2502.13533</link>
<guid>https://arxiv.org/abs/2502.13533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAMLLM</p><br /><br /><p><strong></strong> LoRAMLow-Rank Adaption (LoRA)LoRAMLoRAM70020G HBMGPULoRAA100-80G GPU15GPUQLoRAM4LoRA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13533" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 06:45:40 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13573</link>
<guid>https://arxiv.org/abs/2502.13573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> SHDA330SHDASHDAKTFSHDASHDA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13573" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:38:39 GMT</pubDate>
</item>
<item>
<title>GIMMICK</title>
<link>https://arxiv.org/abs/2502.13766</link>
<guid>https://arxiv.org/abs/2502.13766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GIMMICK144</p><br /><br /><p><strong></strong> GIMMICKLVLMGIMMICK14472820LVLM11</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13766" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 05:19:11 GMT</pubDate>
</item>
<item>
<title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title>
<link>https://arxiv.org/abs/2502.11573</link>
<guid>https://arxiv.org/abs/2502.11573</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have made significant advancements in reasoning capabilities. However, they still face challenges such as high computational demands and privacy concerns. This paper focuses on developing efficient Small Language Models (SLMs) and Multimodal Small Language Models (MSLMs) that retain competitive reasoning abilities. We introduce a novel training pipeline that enhances reasoning capabilities and facilitates deployment on edge devices, achieving state-of-the-art performance while minimizing development costs. \InfR~ aims to advance AI systems by improving reasoning, reducing adoption barriers, and addressing privacy concerns through smaller model sizes. Resources are available at https://github. com/Reallm-Labs/InfiR.
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 04:32:22 GMT</pubDate>
</item>
<item>
<title>ActionPiece: </title>
<link>https://arxiv.org/abs/2502.13581</link>
<guid>https://arxiv.org/abs/2502.13581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ActionPiece</p><br /><br /><p><strong></strong> GRGRActionPieceActionPieceActionPieceNDCG@106.00%12.82%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13581" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 03:56:54 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Memories: </title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture-of-Memories</p><br /><br /><p><strong></strong> RNNMixture-of-MemoriesMoMMoMMoMMoMTransformer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13685" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 02:40:09 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11995</link>
<guid>https://arxiv.org/abs/2502.11995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsLLMs  </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11995" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:20:46 GMT</pubDate>
</item>
<item>
<title>SongGen</title>
<link>https://arxiv.org/abs/2502.13128</link>
<guid>https://arxiv.org/abs/2502.13128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SongGen</p><br /><br /><p><strong></strong> SongGenSongGen</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13128" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 20 Feb 2025 01:07:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13946</link>
<guid>https://arxiv.org/abs/2502.13946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13946" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:54:57 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-VL</title>
<link>https://arxiv.org/abs/2502.13923</link>
<guid>https://arxiv.org/abs/2502.13923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-VL</p><br /><br /><p><strong></strong> Qwen2.5-VLQwenQwen2.5-VLViTQwen2.5-VL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13923" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:35:06 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13962</link>
<guid>https://arxiv.org/abs/2502.13962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13962" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:34:43 GMT</pubDate>
</item>
<item>
<title>Thinking Preference Optimization</title>
<link>https://arxiv.org/abs/2502.13173</link>
<guid>https://arxiv.org/abs/2502.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkPO</p><br /><br /><p><strong></strong> Thinking Preference OptimizationThinkPOLLMsSupervised Fine-TuningSFTThinkPOThinkPOSFT8.6%25.9%ThinkPOSFTDeepSeek-R1-Distill-Qwen-7BMATH50087.4%91.2%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13173" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:31:36 GMT</pubDate>
</item>
<item>
<title>NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2502.12638</link>
<guid>https://arxiv.org/abs/2502.12638</guid>
<content:encoded><![CDATA[
3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol.
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:18:32 GMT</pubDate>
</item>
<item>
<title>AdaptiveStep</title>
<link>https://arxiv.org/abs/2502.13943</link>
<guid>https://arxiv.org/abs/2502.13943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaptiveStep</p><br /><br /><p><strong></strong> PRMsAdaptiveStepAdaptiveStepAdaptiveStepPRMsPRMsBest-of-N30%PRMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13943" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 23:07:01 GMT</pubDate>
</item>
<item>
<title>Crawl4LLM</title>
<link>https://arxiv.org/abs/2502.13347</link>
<guid>https://arxiv.org/abs/2502.13347</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crawl4LLM</p><br /><br /><p><strong></strong> Crawl4LLMLLMsCrawl4LLMLLMs9Crawl4LLM21%URL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13347" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:57:23 GMT</pubDate>
</item>
<item>
<title>AutellixLLM</title>
<link>https://arxiv.org/abs/2502.13965</link>
<guid>https://arxiv.org/abs/2502.13965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutellixLLM</p><br /><br /><p><strong></strong> (LLM)LLMLLMAutellixAutellixLLMLLMvLLMAutellixLLM415</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13965" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:42:06 GMT</pubDate>
</item>
<item>
<title>SearchRAG</title>
<link>https://arxiv.org/abs/2502.13233</link>
<guid>https://arxiv.org/abs/2502.13233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SearchRAG</p><br /><br /><p><strong></strong> SearchRAGLLMsRAGSearchRAGLLMSearchRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13233" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:27:22 GMT</pubDate>
</item>
<item>
<title>3DGS</title>
<link>https://arxiv.org/abs/2502.13144</link>
<guid>https://arxiv.org/abs/2502.13144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3DGS</p><br /><br /><p><strong></strong> IL3DGSRL3DGSRLIL3DGSILRAD3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13144" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 22:13:49 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12143</link>
<guid>https://arxiv.org/abs/2502.12143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> Mix Distillation</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12143" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:38:13 GMT</pubDate>
</item>
<item>
<title>LongPOLLM</title>
<link>https://arxiv.org/abs/2502.13922</link>
<guid>https://arxiv.org/abs/2502.13922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LongPOLLM</p><br /><br /><p><strong></strong> Large Language Models (LLMs)LongPOLLMLLMLongPOKLMistral-7B-Instruct-v0.2LongPO128K512KLLMGPT-4-128K</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13922" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 21:35:20 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12130</link>
<guid>https://arxiv.org/abs/2502.12130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> LLMLLMLLM--LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12130" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 18:20:05 GMT</pubDate>
</item>
<item>
<title>YOLOv12</title>
<link>https://arxiv.org/abs/2502.12524</link>
<guid>https://arxiv.org/abs/2502.12524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOv12CNN</p><br /><br /><p><strong></strong> YOLOYOLOv12CNNYOLOv10-NYOLOv11-NYOLOv12-NT4 GPU40.6% mAP1.642.1%1.2%mAPYOLOv12RT-DETRYOLOv12-S42%36%45%YOLOv12</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12524" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 13:39:32 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08869</link>
<guid>https://arxiv.org/abs/2502.08869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08869" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 10:33:08 GMT</pubDate>
</item>
<item>
<title>Flow-of-OptionsAutoML</title>
<link>https://arxiv.org/abs/2502.12929</link>
<guid>https://arxiv.org/abs/2502.12929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Flow-of-Options</p><br /><br /><p><strong></strong> Flow-of-OptionsFoOLLMsFoOFoOAutoML38.2%69.2%37.4%47.9%1FoOLLMFoO</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12929" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 08:03:59 GMT</pubDate>
</item>
<item>
<title>Text2World: </title>
<link>https://arxiv.org/abs/2502.13092</link>
<guid>https://arxiv.org/abs/2502.13092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Text2World</p><br /><br /><p><strong></strong> LLMsText2WorldPDDLText2WorldLLMsText2World</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13092" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 07:53:04 GMT</pubDate>
</item>
<item>
<title>Atom of Thoughts: </title>
<link>https://arxiv.org/abs/2502.12018</link>
<guid>https://arxiv.org/abs/2502.12018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Atom of Thoughts</p><br /><br /><p><strong></strong> Atom of Thoughts (AoT)AoT-AoTHotpotQAgpt-4o-miniAoT80.6%F1o3-mini3.4%DeepSeek-R110.6%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12018" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:51:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12996</link>
<guid>https://arxiv.org/abs/2502.12996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DiLoCoDiLoCoDiLoCo</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12996" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 06:13:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.10990</link>
<guid>https://arxiv.org/abs/2502.10990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinMTEB</p><br /><br /><p><strong></strong> NLPFinMTEB647ESGFinPersona-E515FinPersona-E5NLP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10990" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:54:27 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13063</link>
<guid>https://arxiv.org/abs/2502.13063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">1500</p><br /><br /><p><strong></strong> 10161500</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13063" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 04:43:42 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09245</link>
<guid>https://arxiv.org/abs/2502.09245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LIMeLIMe</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09245" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 03:03:51 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.10708</link>
<guid>https://arxiv.org/abs/2502.10708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10708" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:56:09 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12669</link>
<guid>https://arxiv.org/abs/2502.12669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> (PSC)Perovskite-KG15172378922272Perovskite-Chat55101Perovskite-Reasoning2217Perovskite-Chat-LLMPerovskite-Reasoning-LLMPSCs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12669" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:47:33 GMT</pubDate>
</item>
<item>
<title>OctoTools</title>
<link>https://arxiv.org/abs/2502.11271</link>
<guid>https://arxiv.org/abs/2502.11271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OctoTools</p><br /><br /><p><strong></strong> OctoToolsOctoTools16MathVistaMMLU-ProMedQAGAIA-TextOctoTools9.3%GPT-4oOctoToolsAutoGenGPT-FunctionsLangChain10.6%OctoTools</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11271" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 02:27:36 GMT</pubDate>
</item>
<item>
<title>ARM4R</title>
<link>https://arxiv.org/abs/2502.13142</link>
<guid>https://arxiv.org/abs/2502.13142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARM4R4D</p><br /><br /><p><strong></strong> ARM4R4DARM4R3D2D3D4DARM4R</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13142" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:24:26 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12859</link>
<guid>https://arxiv.org/abs/2502.12859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> PAFTLLMsLLMsPAFT  LLMs  PAFT  PAFT </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12859" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 01:21:54 GMT</pubDate>
</item>
<item>
<title>Soundwave: </title>
<link>https://arxiv.org/abs/2502.12900</link>
<guid>https://arxiv.org/abs/2502.12900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Soundwave</p><br /><br /><p><strong></strong> SoundwaveSoundwaveAIR-BenchQwen2-AudioSoundwave</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12900" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 00:22:36 GMT</pubDate>
</item>
<item>
<title>MagmaAI</title>
<link>https://arxiv.org/abs/2502.13130</link>
<guid>https://arxiv.org/abs/2502.13130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magma</p><br /><br /><p><strong></strong> MagmaAI-(VL)MagmaVL-MagmaSet-of-Mark (SoM)Trace-of-Mark (ToM)SoMToMMagmaUIMagma</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13130" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:51:36 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12215</link>
<guid>https://arxiv.org/abs/2502.12215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsOpenAIo1QwQDeepseek-R1R1LIMOchain of thoughtCoTCoTQwQR1LIMOShortest Majority VoteCoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12215" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:37:46 GMT</pubDate>
</item>
<item>
<title>SafeRoute</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeRoute</p><br /><br /><p><strong></strong> SafeRoute</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12464" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:23:34 GMT</pubDate>
</item>
<item>
<title>MUDDTransformer</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MUDDTransformer</p><br /><br /><p><strong></strong> MUDDTransformerMUDDTransformerMUDDTransformerMUDDFormerMUDDFormerTransformers1.8X-2.4XTransformersMUDDPythia-2.8BPythia-6.9BPythia-12B0.23%0.4%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12170" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:59:16 GMT</pubDate>
</item>
<item>
<title>XLM-SWCM</title>
<link>https://arxiv.org/abs/2502.10852</link>
<guid>https://arxiv.org/abs/2502.10852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XLM-SWCM</p><br /><br /><p><strong></strong> XLM-SWCMXLM-RXLM-RXLM-SWCM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10852" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:46:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11564</link>
<guid>https://arxiv.org/abs/2502.11564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11564" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:43:02 GMT</pubDate>
</item>
<item>
<title>HealthGPT</title>
<link>https://arxiv.org/abs/2502.09838</link>
<guid>https://arxiv.org/abs/2502.09838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HealthGPT </p><br /><br /><p><strong></strong> HealthGPT Med-LVLMLLMsH-LoRA HealthGPT VL-Health HealthGPT </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09838" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:35:23 GMT</pubDate>
</item>
<item>
<title>mmMamba</title>
<link>https://arxiv.org/abs/2502.13145</link>
<guid>https://arxiv.org/abs/2502.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mmMamba</p><br /><br /><p><strong></strong> MLLMsmmMambaMLLM-RNNLLMTransformerMambaTransformer-HoVLEmmMamba-linearmmMamba-hybridHoVLE103K tokensmmMamba-linear20.675.8%GPUmmMamba-hybrid13.560.2%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13145" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:08:27 GMT</pubDate>
</item>
<item>
<title>FLAG-Trader</title>
<link>https://arxiv.org/abs/2502.11433</link>
<guid>https://arxiv.org/abs/2502.11433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLAG-Trader</p><br /><br /><p><strong></strong> FLAG-TraderLLMsFLAG-TraderLLMFLAG-Trader</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11433" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:06:19 GMT</pubDate>
</item>
<item>
<title>Decomposed Reward Models: </title>
<link>https://arxiv.org/abs/2502.13131</link>
<guid>https://arxiv.org/abs/2502.13131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRMsAI</p><br /><br /><p><strong></strong> DRMsDRMsPCADRMsDRMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13131" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:59:45 GMT</pubDate>
</item>
<item>
<title>HEADINFER: </title>
<link>https://arxiv.org/abs/2502.12574</link>
<guid>https://arxiv.org/abs/2502.12574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HEADINFERKV</p><br /><br /><p><strong></strong> HEADINFERKVCPU RAMHEADINFERGPUTransformerKVGPUKVRooflineHEADINFERLlama-3-8BHEADINFERKVGPU128 GB1 GBGPU207 GB17 GBBF1692%HEADINFER24GBGPUNVIDIA RTX 40904</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12574" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:57:00 GMT</pubDate>
</item>
<item>
<title>Phantom: Subject-consistent video generation via cross-modal alignment</title>
<link>https://arxiv.org/abs/2502.11079</link>
<guid>https://arxiv.org/abs/2502.11079</guid>
<content:encoded><![CDATA[
The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:56:39 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.12501</link>
<guid>https://arxiv.org/abs/2502.12501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> LLM-as-a-JudgeCoTCoTCoTLLM-as-a-JudgeCoT6.7%CoTCoT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12501" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:55:26 GMT</pubDate>
</item>
<item>
<title>RealSyn-</title>
<link>https://arxiv.org/abs/2502.12513</link>
<guid>https://arxiv.org/abs/2502.12513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealSyn-</p><br /><br /><p><strong></strong> RealSyn-RealSyn15M30M100MRealSyn-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12513" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:52:22 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.13143</link>
<guid>https://arxiv.org/abs/2502.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> AIOrienText300K3DVLMOpen6DOR48.7%SIMPLER74.9%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.13143" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:51:33 GMT</pubDate>
</item>
<item>
<title>EQ-VAE</title>
<link>https://arxiv.org/abs/2502.09509</link>
<guid>https://arxiv.org/abs/2502.09509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EQ-VAE</p><br /><br /><p><strong></strong> EQ-VAEEQ-VAEDiTSiTREPAMaskGITDiT-XL/2SD-VAE7EQ-VAE</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09509" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 14:56:45 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08826</link>
<guid>https://arxiv.org/abs/2502.08826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> RAGRAGRAGRAGRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08826" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>IHEval</title>
<link>https://arxiv.org/abs/2502.08745</link>
<guid>https://arxiv.org/abs/2502.08745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IHEval</p><br /><br /><p><strong></strong> LMsIHEval3,53848%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08745" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:21:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NN-CIFT</p><br /><br /><p><strong></strong> InfluenceNetwork99%0.0027%NN-CIFTNN-CIFT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09969" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:04:04 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11357</link>
<guid>https://arxiv.org/abs/2502.11357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LMMLMM94K49KURL720K33MExplorer</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11357" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:57:43 GMT</pubDate>
</item>
<item>
<title>ILIAS</title>
<link>https://arxiv.org/abs/2502.11748</link>
<guid>https://arxiv.org/abs/2502.11748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ILIAS</p><br /><br /><p><strong></strong> ILIASILIAS1,0001YFCC100MILIAS2014ILIAS-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11748" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:42:58 GMT</pubDate>
</item>
<item>
<title>CALM</title>
<link>https://arxiv.org/abs/2502.08820</link>
<guid>https://arxiv.org/abs/2502.08820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CALM</p><br /><br /><p><strong></strong> LLMsAPILATODAPICALMConversational Agentic Language ModelCALM-ITReActAPICALM-ITCALMCALM 8B, CALM 70B, CALM 405BMultiWOZ 2.4, BFCL V3, API-BankGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08820" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 08:59:34 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> QAEditQA38.5%~96%1000</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11177" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:33:17 GMT</pubDate>
</item>
<item>
<title>MIKASA</title>
<link>https://arxiv.org/abs/2502.10550</link>
<guid>https://arxiv.org/abs/2502.10550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIKASA</p><br /><br /><p><strong></strong> MIKASAMIKASAMIKASA-BaseMIKASA-Robo32</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10550" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:16:07 GMT</pubDate>
</item>
<item>
<title>Dyve</title>
<link>https://arxiv.org/abs/2502.11157</link>
<guid>https://arxiv.org/abs/2502.11157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dyve</p><br /><br /><p><strong></strong> DyveKahnemanDyvetoken12DyveProcessBenchMATHDyve</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11157" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:33:31 GMT</pubDate>
</item>
<item>
<title>NSA</title>
<link>https://arxiv.org/abs/2502.11089</link>
<guid>https://arxiv.org/abs/2502.11089</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NSA</p><br /><br /><p><strong></strong> NSANSANSA64kNSA</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11089" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:07:36 GMT</pubDate>
</item>
<item>
<title>Adam</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Coupled Adam</p><br /><br /><p><strong></strong> AdamCoupled AdamCoupled Adam</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08441" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 05:28:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09083</link>
<guid>https://arxiv.org/abs/2502.09083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09083" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:37:21 GMT</pubDate>
</item>
<item>
<title>MagicArticulate3D</title>
<link>https://arxiv.org/abs/2502.12135</link>
<guid>https://arxiv.org/abs/2502.12135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicArticulate3D</p><br /><br /><p><strong></strong> 3D3DMagicArticulate3DArticulation-XL33,0003DMagicArticulate</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12135" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:34:15 GMT</pubDate>
</item>
<item>
<title>ThinkDiff</title>
<link>https://arxiv.org/abs/2502.10458</link>
<guid>https://arxiv.org/abs/2502.10458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkDiff</p><br /><br /><p><strong></strong> ThinkDiff(VLMs)ThinkDiff-(LLM)ThinkDiffCoBSAT19.2%46.3%4A100 GPU5ThinkDiff</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10458" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:33:41 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11831</link>
<guid>https://arxiv.org/abs/2502.11831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11831" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:20:25 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11085</link>
<guid>https://arxiv.org/abs/2502.11085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 1/24CSICSIJMP</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11085" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:16:28 GMT</pubDate>
</item>
<item>
<title>PhysReason</title>
<link>https://arxiv.org/abs/2502.12054</link>
<guid>https://arxiv.org/abs/2502.12054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysReason1200</p><br /><br /><p><strong></strong> PhysReason120025%75%15.6Deepseek-R1Gemini-2.0-Flash-Thinking60%75.11%31.95%PhysReason</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12054" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 03:53:47 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11574</link>
<guid>https://arxiv.org/abs/2502.11574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMs50MixtralLlamaGeminiGPTo3-minideepseek-r1LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11574" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:26:18 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11578</link>
<guid>https://arxiv.org/abs/2502.11578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ChatGPT-o1-mini</p><br /><br /><p><strong></strong> LLMsLIXADDLIXChatGPT-o1-miniLIXLIXMMLU-0.875p = 0.026N=6LLMs-shot</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11578" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:23:29 GMT</pubDate>
</item>
<item>
<title>SysGen: </title>
<link>https://arxiv.org/abs/2502.11330</link>
<guid>https://arxiv.org/abs/2502.11330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SysGen</p><br /><br /><p><strong></strong> SysGenLLMsSysGenMultifacetOpen LLM Leaderboard 2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11330" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:45:36 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11196" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:02:25 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11167</link>
<guid>https://arxiv.org/abs/2502.11167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsSURGELLMsLLMsLLMsGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11167" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:01:24 GMT</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:58:24 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.12152</link>
<guid>https://arxiv.org/abs/2502.12152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12152" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:49:53 GMT</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:28:31 GMT</pubDate>
</item>
<item>
<title>LLM video-SALMONN-o1</title>
<link>https://arxiv.org/abs/2502.11775</link>
<guid>https://arxiv.org/abs/2502.11775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">video-SALMONN-o1</p><br /><br /><p><strong></strong> video-SALMONN-o1pDPORivaBench4000-LLaVA-OneVisionvideo-SALMONN-o13-8%pDPORivaBench6-8%video-SALMONN-o1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11775" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:06:55 GMT</pubDate>
</item>
<item>
<title>TalkHierLLM-MA</title>
<link>https://arxiv.org/abs/2502.11098</link>
<guid>https://arxiv.org/abs/2502.11098</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TalkHierLLM-MA</p><br /><br /><p><strong></strong> LLM-MATalk Structurally, Act Hierarchically (TalkHier)TalkHierOpenAIAgentVerseLLM-MAGitHub</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11098" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:51:50 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.10454</link>
<guid>https://arxiv.org/abs/2502.10454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLMsCounterMATHLLMsCounterMATHLLMsOpenAI o1LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10454" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:37:16 GMT</pubDate>
</item>
<item>
<title>Diffusion-Sharpening</title>
<link>https://arxiv.org/abs/2502.12146</link>
<guid>https://arxiv.org/abs/2502.12146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diffusion-Sharpening</p><br /><br /><p><strong></strong> Diffusion-SharpeningNFEDiffusion-SharpeningNFEDiffusion-Sharpening</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12146" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:30:53 GMT</pubDate>
</item>
<item>
<title>HermesFlow</title>
<link>https://arxiv.org/abs/2502.12148</link>
<guid>https://arxiv.org/abs/2502.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HermesFlow</p><br /><br /><p><strong></strong> HermesFlowMLLMsMLLMsHermesFlowPair-DPOHermesFlowHermesFlow</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.12148" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:29:29 GMT</pubDate>
</item>
<item>
<title>SAFE-SQLText-to-SQL</title>
<link>https://arxiv.org/abs/2502.11438</link>
<guid>https://arxiv.org/abs/2502.11438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAFE-SQLText-to-SQL</p><br /><br /><p><strong></strong> SAFE-SQLSQLSAFE-SQLSQLText-to-SQLSAFE-SQL-shot-shotText-to-SQL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11438" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:06:03 GMT</pubDate>
</item>
<item>
<title>CRANE</title>
<link>https://arxiv.org/abs/2502.09061</link>
<guid>https://arxiv.org/abs/2502.09061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRANE</p><br /><br /><p><strong></strong> LLMsLLMCRANELLMCRANEGSM-symbolicFOLIO10%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09061" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:43:51 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.11275</link>
<guid>https://arxiv.org/abs/2502.11275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CuckooLLM</p><br /><br /><p><strong></strong> IELLMIENTECuckooLLM102.6MCuckooIECuckooLLMLLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11275" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:10:49 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.11901</link>
<guid>https://arxiv.org/abs/2502.11901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 14BPoPilotGPT-4o64%GPT-4o54%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.11901" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:05:54 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08235</link>
<guid>https://arxiv.org/abs/2502.08235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LRMsSWE Bench Verified30%43%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08235" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 17:09:38 GMT</pubDate>
</item>
<item>
<title>S3FT</title>
<link>https://arxiv.org/abs/2502.08130</link>
<guid>https://arxiv.org/abs/2502.08130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S3FT</p><br /><br /><p><strong></strong> S3FTLLMsS3FTS3FTSFTS3FTPythonSFT4.4S3FT</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08130" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 12:27:43 GMT</pubDate>
</item>
<item>
<title>CLaMP 3: </title>
<link>https://arxiv.org/abs/2502.10362</link>
<guid>https://arxiv.org/abs/2502.10362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaMP 3</p><br /><br /><p><strong></strong> CLaMP 3M4-RAG231WikiMT-X1000CLaMP 3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10362" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 10:18:04 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2502.10392</link>
<guid>https://arxiv.org/abs/2502.10392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3D3D3DTGPCBATGP3DCBAScanReferNR3DSR3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10392" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 09:25:39 GMT</pubDate>
</item>
<item>
<title>DarwinLM: Evolutionary Structured Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2502.07780</link>
<guid>https://arxiv.org/abs/2502.07780</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>ImageRAG</title>
<link>https://arxiv.org/abs/2502.09411</link>
<guid>https://arxiv.org/abs/2502.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ImageRAG</p><br /><br /><p><strong></strong> ImageRAGImageRAGImageRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09411" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:41:41 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.10140</link>
<guid>https://arxiv.org/abs/2502.10140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"> explores </p><br /><br /><p><strong></strong> mLMsLRLsGlotCCConceptNet1GBMBLLaMA-3GPT-4DeepSeek-R1mLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10140" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:29:25 GMT</pubDate>
</item>
<item>
<title>CAPIMIM</title>
<link>https://arxiv.org/abs/2502.08769</link>
<guid>https://arxiv.org/abs/2502.08769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAPI</p><br /><br /><p><strong></strong> CAPIMIMCAPIViT-LImageNet83.8%ADE20K32.1%mIoUMIMDINOv2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08769" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 07:24:28 GMT</pubDate>
</item>
<item>
<title>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.10235</link>
<guid>https://arxiv.org/abs/2502.10235</guid>
<content:encoded><![CDATA[
Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:36:23 GMT</pubDate>
</item>
<item>
<title>VibeGenAI</title>
<link>https://arxiv.org/abs/2502.10173</link>
<guid>https://arxiv.org/abs/2502.10173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VibeGenAI</p><br /><br /><p><strong></strong> VibeGenAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10173" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:09:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.07586</link>
<guid>https://arxiv.org/abs/2502.07586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI</p><br /><br /><p><strong></strong> AIAI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07586" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 04:28:55 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09935</link>
<guid>https://arxiv.org/abs/2502.09935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 1%LoRAU-NetTransformerCLIPT5</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09935" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 03:06:17 GMT</pubDate>
</item>
<item>
<title>MR</title>
<link>https://arxiv.org/abs/2502.07856</link>
<guid>https://arxiv.org/abs/2502.07856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MRMR</p><br /><br /><p><strong></strong> (MR)(SDE)MRMR(NFEs)MR(MRS)MRNFEsMRSDE(PF-ODE)MR1020MR</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07856" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 02:03:05 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09980</link>
<guid>https://arxiv.org/abs/2502.09980</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> V2V-QAV2V-LLMV2V-LLM</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09980" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 01:33:15 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.09638</link>
<guid>https://arxiv.org/abs/2502.09638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> LLMLLMLLMLLMJ_2Sonnet 3.5Gemini 1.5J_2Harmbench93.0%91.0%LLMLLMJ_2AI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09638" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:04:19 GMT</pubDate>
</item>
<item>
<title>LLaDA</title>
<link>https://arxiv.org/abs/2502.09992</link>
<guid>https://arxiv.org/abs/2502.09992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA</p><br /><br /><p><strong></strong> LLaDALLaDATransformerLLaDALLaDA 8BLLMLLaMA3 8BLLaDAGPT-4o</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09992" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:03:18 GMT</pubDate>
</item>
<item>
<title>LLM</title>
<link>https://arxiv.org/abs/2502.09955</link>
<guid>https://arxiv.org/abs/2502.09955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM</p><br /><br /><p><strong></strong> OpenAIo1o3DeepSeek R1IMOARCHLELeanIMOARC-NHLEIMO33.3%77.8%HLE8%37%948ARC80%o3ARC26.5%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09955" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:57:43 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09741</link>
<guid>https://arxiv.org/abs/2502.09741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> FoNEFoNEFoNE7099%6436FoNE100,000100%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09741" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:07:53 GMT</pubDate>
</item>
<item>
<title>MM-RLHF: </title>
<link>https://arxiv.org/abs/2502.10391</link>
<guid>https://arxiv.org/abs/2502.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MM-RLHF</p><br /><br /><p><strong></strong> MLLMMM-RLHF12MM-RLHFLLaVA-ov-7B19.5%60%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10391" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:51:55 GMT</pubDate>
</item>
<item>
<title>Step-Video-T2V</title>
<link>https://arxiv.org/abs/2502.10248</link>
<guid>https://arxiv.org/abs/2502.10248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Video-T2V30B</p><br /><br /><p><strong></strong> Step-Video-T2V30B204(Video-VAE)16x168x3DDiTFlow MatchingDPOStep-Video-T2V-EvalStep-Video-T2V</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10248" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:50:38 GMT</pubDate>
</item>
<item>
<title>RAS</title>
<link>https://arxiv.org/abs/2502.10389</link>
<guid>https://arxiv.org/abs/2502.10389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAS</p><br /><br /><p><strong></strong> RASRASRASStable Diffusion 3Lumina-Next-T2I2.362.51RAS1.6</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10389" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:22:08 GMT</pubDate>
</item>
<item>
<title>ZeroBench</title>
<link>https://arxiv.org/abs/2502.09696</link>
<guid>https://arxiv.org/abs/2502.09696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroBench</p><br /><br /><p><strong></strong> LMMsZeroBenchLMMs10033420LMMsZeroBench0.0%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09696" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:20:53 GMT</pubDate>
</item>
<item>
<title>STMA</title>
<link>https://arxiv.org/abs/2502.10177</link>
<guid>https://arxiv.org/abs/2502.10177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STMA</p><br /><br /><p><strong></strong> (STMA)STMA-TextWorldSTMA32STMA31.25%24.7%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.10177" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 21:31:11 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09613</link>
<guid>https://arxiv.org/abs/2502.09613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LRFVAE-Radiance FieldVAE-RF</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09613" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 21:20:14 GMT</pubDate>
</item>
<item>
<title>GSM-Ranges</title>
<link>https://arxiv.org/abs/2502.08680</link>
<guid>https://arxiv.org/abs/2502.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GSM-Ranges</p><br /><br /><p><strong></strong> LLMsGSM-RangesGSM8K14LMMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08680" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 09:18:18 GMT</pubDate>
</item>
<item>
<title>VFX Creator: AI</title>
<link>https://arxiv.org/abs/2502.05979</link>
<guid>https://arxiv.org/abs/2502.05979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> VFXOpen-VFX15VFX CreatorVFXLoRAOpen-VFXVFX Creator</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.05979" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 08:47:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09614</link>
<guid>https://arxiv.org/abs/2502.09614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> 10%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09614" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:50:27 GMT</pubDate>
</item>
<item>
<title>3CAD</title>
<link>https://arxiv.org/abs/2502.05761</link>
<guid>https://arxiv.org/abs/2502.05761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3CADCFRG</p><br /><br /><p><strong></strong> 3CAD3C27,039CFRGCFRG3CAD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.05761" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:00:29 GMT</pubDate>
</item>
<item>
<title>ProbeLog</title>
<link>https://arxiv.org/abs/2502.09619</link>
<guid>https://arxiv.org/abs/2502.09619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProbeLog</p><br /><br /><p><strong></strong>  pretrained ProbeLogProbeLoglogit logit -shotProbeLog</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09619" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:58:25 GMT</pubDate>
</item>
<item>
<title>CoSER: </title>
<link>https://arxiv.org/abs/2502.09082</link>
<guid>https://arxiv.org/abs/2502.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSER</p><br /><br /><p><strong></strong> CoSER(RPLA)CoSER77117,966LLMsLLaMA-3.1CoSER 8BCoSER 70BCoSERRPLACoSER 70BGPT-4oInCharacterLifeChoice75.80%93.47%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09082" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:50:35 GMT</pubDate>
</item>
<item>
<title>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09390</link>
<guid>https://arxiv.org/abs/2502.09390</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:35:53 GMT</pubDate>
</item>
<item>
<title>3D</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D</p><br /><br /><p><strong></strong> 3D3DLLMLLM3DENELShapeLLM-13B55.0%50.92%42.7%3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09620" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:27:45 GMT</pubDate>
</item>
<item>
<title>MME-CoT</title>
<link>https://arxiv.org/abs/2502.09621</link>
<guid>https://arxiv.org/abs/2502.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MME-CoT</p><br /><br /><p><strong></strong> MME-CoTLMMsCoTMME-CoTLMMsCoTKimi k1.5GPT-4oCoTLMMCoTLMMsMME-CoTLMMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09621" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:34:58 GMT</pubDate>
</item>
<item>
<title>Typhoon T1</title>
<link>https://arxiv.org/abs/2502.09042</link>
<guid>https://arxiv.org/abs/2502.09042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Typhoon T1</p><br /><br /><p><strong></strong> Typhoon T1LLMsTyphoon T1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09042" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:29:44 GMT</pubDate>
</item>
<item>
<title>CoT-Valve: </title>
<link>https://arxiv.org/abs/2502.09601</link>
<guid>https://arxiv.org/abs/2502.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT-Valve</p><br /><br /><p><strong></strong> CoT-ValveCoTCoT-ValveGSM8KAIME</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09601" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 00:16:30 GMT</pubDate>
</item>
<item>
<title>mmE5</title>
<link>https://arxiv.org/abs/2502.08468</link>
<guid>https://arxiv.org/abs/2502.08468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">mmE5</p><br /><br /><p><strong></strong> E5mmE5MMEBXTD</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08468" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09560</link>
<guid>https://arxiv.org/abs/2502.09560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmbodiedBench</p><br /><br /><p><strong></strong> EmbodiedBenchMLLMMLLMEmbodiedBench1,12813MLLMMLLMGPT-4o28.9%EmbodiedBenchMLLM-based</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09560" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
</item>
<item>
<title>Skrr: T2I</title>
<link>https://arxiv.org/abs/2502.08690</link>
<guid>https://arxiv.org/abs/2502.08690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skrr</p><br /><br /><p><strong></strong> T2ISkip and Re-use layers (Skrr)T2ISkrrSkrrFIDCLIPDreamSimGenEval</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08690" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
</item>
<item>
<title>InfiniteHiP</title>
<link>https://arxiv.org/abs/2502.08910</link>
<guid>https://arxiv.org/abs/2502.08910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniteHiP</p><br /><br /><p><strong></strong> InfiniteHiPtokentokenRoPEGPUL40s 48GB GPU300token3InfinityHiP100token18.95SGLang</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08910" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
</item>
<item>
<title>TripoSG3D</title>
<link>https://arxiv.org/abs/2502.06608</link>
<guid>https://arxiv.org/abs/2502.06608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TripoSG3D</p><br /><br /><p><strong></strong> 3DTripoSG3DTripoSG1) 3D2) SDFEikonal3D3) 2003D3DTripoSG3D3DTripoSG3D3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.06608" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09056</link>
<guid>https://arxiv.org/abs/2502.09056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> DeepSeek R1LLMsLLMLLMsDeepSeek R1120LLMsDeepSeek R1</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09056" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08946</link>
<guid>https://arxiv.org/abs/2502.08946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs40%</p><br /><br /><p><strong></strong> LLMsPhysiCoLLMsGPT-4oo1Gemini 2.040%LLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08946" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.09100</link>
<guid>https://arxiv.org/abs/2502.09100</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> OpenAI o3DeepSeek-R1LLMsLLMs-</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09100" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
</item>
<item>
<title>SelfCite</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SelfCiteLLMs</p><br /><br /><p><strong></strong> SelfCite(LLMs)LLMLongBench-CiteSelfCiteF15.3</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.09604" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
</item>
<item>
<title>GEMINI</title>
<link>https://arxiv.org/abs/2502.05282</link>
<guid>https://arxiv.org/abs/2502.05282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GEMINI</p><br /><br /><p><strong></strong> DCRLGEMINIDCRLDHLGSSGEMINI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.05282" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 14:57:40 GMT</pubDate>
</item>
<item>
<title>PDE-Controller: </title>
<link>https://arxiv.org/abs/2502.00963</link>
<guid>https://arxiv.org/abs/2502.00963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PDE-ControllerLLMs</p><br /><br /><p><strong></strong> PDE-ControllerLLMsPDEPDE200PDE-ControllerGPTPDE62%PDELLMs</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.00963" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 11:41:16 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08213</link>
<guid>https://arxiv.org/abs/2502.08213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMQwen2-1.5BGPT-Neo-125MBespoke-Stratos-17k15</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08213" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 05:48:33 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.06533</link>
<guid>https://arxiv.org/abs/2502.06533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsRLLLMsKullback-LeiblerKLKLRL</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.06533" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:47:28 GMT</pubDate>
</item>
<item>
<title>Animate Anyone 2: </title>
<link>https://arxiv.org/abs/2502.06145</link>
<guid>https://arxiv.org/abs/2502.06145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Animate Anyone 2</p><br /><br /><p><strong></strong> Animate Anyone 2Animate Anyone 2</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.06145" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:45:43 GMT</pubDate>
</item>
<item>
<title>BenchMAX</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchMAX</p><br /><br /><p><strong></strong> (LLMs)BenchMAX16BenchMAX</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07346" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:34:47 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.04411</link>
<guid>https://arxiv.org/abs/2502.04411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> LLMsLLaMAQwen</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.04411" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:30:35 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.07985</link>
<guid>https://arxiv.org/abs/2502.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong>  reasoning</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07985" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:47:30 GMT</pubDate>
</item>
<item>
<title>WorldGUIGUI</title>
<link>https://arxiv.org/abs/2502.08047</link>
<guid>https://arxiv.org/abs/2502.08047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldGUIGUI</p><br /><br /><p><strong></strong> GUIWorldGUIGUIGUI10PowerPointVSCodeAdobe AcrobatGUIGUI-ThinkerGUIGUI-ThinkerWorldGUIClaude-3.514.9%GUI</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08047" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:39:08 GMT</pubDate>
</item>
<item>
<title>RAG</title>
<link>https://arxiv.org/abs/2502.06872</link>
<guid>https://arxiv.org/abs/2502.06872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAG</p><br /><br /><p><strong></strong> RAGAIGCRAGRAGRAGRAG</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.06872" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:06:04 GMT</pubDate>
</item>
<item>
<title>NoLiMa</title>
<link>https://arxiv.org/abs/2502.05167</link>
<guid>https://arxiv.org/abs/2502.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NoLiMaLLMs</p><br /><br /><p><strong></strong> NoLiMa(LLMs)NIAHNoLiMa12128KLLMs(<1K)32K1050%GPT-4o99.3%69.7%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.05167" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:04:29 GMT</pubDate>
</item>
<item>
<title>TextAtlas5M</title>
<link>https://arxiv.org/abs/2502.07870</link>
<guid>https://arxiv.org/abs/2502.07870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextAtlas5M</p><br /><br /><p><strong></strong> TextAtlas5M5003000TextAtlasEvalTextAtlasEvalGPT4oDallE-3TextAtlas5M</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07870" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:50:07 GMT</pubDate>
</item>
<item>
<title>Light-A-Video</title>
<link>https://arxiv.org/abs/2502.08590</link>
<guid>https://arxiv.org/abs/2502.08590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-A-Video </p><br /><br /><p><strong></strong>  Light-A-VideoCLAPLFLight-A-Video</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08590" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:56 GMT</pubDate>
</item>
<item>
<title>LASP-2: ism</title>
<link>https://arxiv.org/abs/2502.07563</link>
<guid>https://arxiv.org/abs/2502.07563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LASP-2 </p><br /><br /><p><strong></strong> LASP-2SPLASPLASP-2SP-LASP-2LASP-2HSPLinear-Llama3LASP-2LASP15.2%Ring Attention36.6%</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.07563" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:31 GMT</pubDate>
</item>
<item>
<title>CoCoMix</title>
<link>https://arxiv.org/abs/2502.08524</link>
<guid>https://arxiv.org/abs/2502.08524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoCoMix</p><br /><br /><p><strong></strong> CoCoMixCoCoMixCoCoMixCoCoMix</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08524" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:42:44 GMT</pubDate>
</item>
<item>
<title></title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;"></p><br /><br /><p><strong></strong> </p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08606" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:41:41 GMT</pubDate>
</item>
<item>
<title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title>
<link>https://arxiv.org/abs/2502.08168</link>
<guid>https://arxiv.org/abs/2502.08168</guid>
<content:encoded><![CDATA[
In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:57:30 GMT</pubDate>
</item>
<item>
<title>CineMaster3D</title>
<link>https://arxiv.org/abs/2502.08639</link>
<guid>https://arxiv.org/abs/2502.08639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineMaster3D</p><br /><br /><p><strong></strong> CineMaster3D3D3DCineMaster3DCineMaster3D</p><br /><br /><p><em> gpt-4o-mini  </em></p><a href="https://arxiv.org/abs/2502.08639" target="_blank"></a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:55:44 GMT</pubDate>
</item>
</channel>
</rss>