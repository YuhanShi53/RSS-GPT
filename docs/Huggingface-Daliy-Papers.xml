<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>InnerControl：提升扩散模型空间控制精度的新方法</title>
<link>https://arxiv.org/abs/2507.02321</link>
<guid>https://arxiv.org/abs/2507.02321</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>InnerControl通过全阶段空间一致性训练提升图像生成精度。</p><br><br><p><strong>摘要：</strong> 尽管文本到图像的扩散模型取得了显著进展，但实现精确的空间控制仍具挑战。ControlNet及其改进版本ControlNet++通过引入条件模块和循环一致性损失来提升控制效果，但忽略了中间生成阶段。为此，InnerControl提出一种训练策略，在所有扩散步骤中强制空间一致性。该方法通过轻量级卷积探针从UNet中间特征中重建输入控制信号（如边缘、深度），即使在高噪声潜变量下也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测与目标条件之间的差异，InnerControl提升了控制精度和生成质量，并结合ControlNet++等现有技术实现了多种条件方法的最先进性能。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2507.02321 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 01:25:53 GMT</pubDate>
<pubDate>Thu, 03 Jul 2025 01:25:53 GMT</pubDate>
</item>

<item>
<title>大型语言模型的自我纠正盲点研究</title>
<link>https://arxiv.org/abs/2507.02778</link>
<guid>https://arxiv.org/abs/2507.02778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMs存在自我纠正盲点，影响其可靠性。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）已取得显著进展，但它们仍会犯错并陷入无效推理路径。自我纠正能力对可信的LLM至关重要，尤其是自回归模型。然而，LLMs在用户输入中能识别错误，却无法修正自身输出中的相同错误，这种现象被称为‘自我纠正盲点’。为系统研究这一问题，研究人员提出了Self-Correction Bench框架，通过在三个复杂度级别上注入错误进行测试。实验显示，14个模型平均有64.5%的盲点率。研究发现，这一限制与训练数据组成有关：人类演示数据多为无错误响应，而非纠错序列，而强化学习训练的模型则通过反馈学习纠错。令人惊讶的是，仅添加“Wait”即可减少89.3%的盲点，表明该能力存在但需激活。该研究揭示了当前LLMs的关键缺陷，并为提升其可靠性和可信度提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:41:30 GMT</pubDate>
</item>
<item>
<title>利用LLM辅助科学论文局限性识别的基准研究</title>
<link>https://arxiv.org/abs/2507.02694</link>
<guid>https://arxiv.org/abs/2507.02694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM在识别科学论文局限性方面具有潜力，LimitGen基准提升其反馈能力。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLM）在科学论文同行评审中的应用，特别是在识别论文局限性方面的潜力。作者提出了一个全面的局限性分类体系，并基于此构建了LimitGen基准，包含合成数据集和真实人类撰写的局限性数据集。通过引入文献检索增强LLM的识别能力，该方法提升了LLM生成具体且有建设性反馈的能力，为早期研究反馈和补充人工评审提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:04:38 GMT</pubDate>
</item>
<item>
<title>基于能量模型的系统2思维推理方法研究</title>
<link>https://arxiv.org/abs/2507.02092</link>
<guid>https://arxiv.org/abs/2507.02092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EBTs通过无监督学习实现系统2推理，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的基于能量模型的推理方法（Energy-Based Transformers, EBTs），旨在通过无监督学习实现类似人类系统2思维的推理能力。与传统方法相比，EBTs无需额外监督或训练，而是通过显式验证输入与候选预测的兼容性，并将预测问题重新建模为以验证器为基准的优化问题。实验表明，EBTs在文本和视觉等多模态任务中均表现出色，训练速度更快，推理性能优于Transformer++和Diffusion Transformers。此外，EBTs在多数下游任务中表现更优，表明其具有更好的泛化能力，是一种有前景的模型扩展范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>IntFold：一种可控制的生物分子结构预测基础模型</title>
<link>https://arxiv.org/abs/2507.02025</link>
<guid>https://arxiv.org/abs/2507.02025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IntFold在结构预测方面达到AlphaFold3水平，支持多种定制化任务。</p><br /><br /><p><strong>摘要：</strong> IntFold是一种可控制的基础模型，用于通用和专业生物分子结构预测。其预测精度与AlphaFold3相当，并采用优化的注意力核。除了标准结构预测外，IntFold还可通过适配器预测别构状态、约束结构和结合亲和力。此外，研究团队引入了一种新的置信度头，以更精细地评估对接质量，特别是在抗体-抗原复合物等复杂目标上表现突出。文章还分享了训练该计算密集型模型的经验与见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 12:09:47 GMT</pubDate>
</item>
<item>
<title>AsyncFlow：一种高效的异步流式强化学习框架</title>
<link>https://arxiv.org/abs/2507.01663</link>
<guid>https://arxiv.org/abs/2507.01663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AsyncFlow提升大语言模型后训练效率，支持灵活扩展。</p><br /><br /><p><strong>摘要：</strong> 本文提出AsyncFlow，一种用于大语言模型后训练的异步流式强化学习框架。针对传统框架在可扩展性、数据流复杂性和资源闲置方面的不足，AsyncFlow引入了分布式数据存储与传输模块，实现统一的数据管理和细粒度调度。其架构支持自动流水线重叠和动态负载均衡，并通过生产者-消费者异步工作流减少计算空闲。此外，AsyncFlow与底层训练和推理引擎解耦，提供模块化用户界面。实验表明，该框架在吞吐量上平均提升1.59倍，为下一代强化学习系统设计提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:45:34 GMT</pubDate>
</item>
<item>
<title>ZeCO：实现线性注意力模型高效序列并行的新方法</title>
<link>https://arxiv.org/abs/2507.01004</link>
<guid>https://arxiv.org/abs/2507.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeCO提升长序列训练效率，消除通信开销。</p><br /><br /><p><strong>摘要：</strong> 本文提出ZeCO（Zero Communication Overhead）序列并行方法，用于优化线性注意力机制在大型语言模型中的应用。传统序列并行方法因通信开销大而成为瓶颈，而ZeCO通过引入All-Scan通信原语，有效减少通信负担，实现近线性扩展。实验表明，在256块GPU上处理8M长度的序列时，ZeCO相比现有最优方法提升了60%的速度。理论与实证均证明了ZeCO的高效性与可行性，为训练超长序列的下一代LLM提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:54:53 GMT</pubDate>
</item>
<item>
<title>多模态推理中‘思考与图像’范式的演进与展望</title>
<link>https://arxiv.org/abs/2506.23918</link>
<guid>https://arxiv.org/abs/2506.23918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨AI从‘思考图像’到‘与图像共思’的范式转变。</p><br /><br /><p><strong>摘要：</strong> 本文综述了多模态推理领域中‘思考与图像’范式的最新进展。传统方法将视觉视为静态输入，导致感知数据与符号推理之间的语义鸿沟。而人类认知常利用视觉作为动态思维工具，这一理念正推动AI从仅‘思考图像’向‘与图像共思’演进。文章提出该范式发展的三个阶段：外部工具探索、程序化操作和内在想象，并总结了核心方法、评估基准、应用前景及未来挑战，为构建更强大且符合人类认知的多模态AI提供路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 10:48:35 GMT</pubDate>
</item>
<item>
<title>动态选择与合并专家模型提升跨领域信息抽取性能</title>
<link>https://arxiv.org/abs/2506.22813</link>
<guid>https://arxiv.org/abs/2506.22813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SaM框架通过动态选择和合并专家模型提升信息抽取效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出SaM框架，通过在推理阶段动态选择和合并预训练的专家模型来优化信息抽取任务。该方法基于目标领域的相似性和采样实例的表现选择合适的专家模型，并将其合并以生成针对特定领域的优化模型。这种方法无需额外训练即可提高跨领域泛化能力，并具备良好的可扩展性。实验结果表明，该框架在多个基准测试中平均优于统一模型10%。文章还探讨了框架的潜在改进方向和实际应用经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 04:28:52 GMT</pubDate>
</item>
<item>
<title>基于语言理解的3D场景重建框架LangScene-X</title>
<link>https://arxiv.org/abs/2507.02813</link>
<guid>https://arxiv.org/abs/2507.02813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LangScene-X通过多模态生成实现从稀疏视角的高质量3D场景重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为LangScene-X的新生成框架，用于从2D图像中恢复一致的3D结构并实现开放词汇场景理解。该方法通过TriMap视频扩散模型生成RGB、法线和语义分割图，并结合语言量化压缩器（LQC）实现跨场景的语言嵌入编码，从而在仅有稀疏视角的情况下构建可泛化的3D语言嵌入场景。最后，通过将语言信息对齐到3D场景表面，支持开放式语言查询。实验表明，LangScene-X在质量和泛化能力上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 13:21:23 GMT</pubDate>
</item>
<item>
<title>2-单纯形Transformer提升token效率的研究</title>
<link>https://arxiv.org/abs/2507.02754</link>
<guid>https://arxiv.org/abs/2507.02754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2-单纯形Transformer在token效率上优于传统Transformer。</p><br /><br /><p><strong>摘要：</strong> 本文研究了2-单纯形Transformer架构，该架构通过高效的Triton内核实现三线性函数的注意力机制，相较于标准的点积注意力，其在数学、编程、推理和逻辑任务中表现出更高的token效率。实验表明，在固定token预算下，2-单纯形Transformer模型性能更优，并且改变了知识和推理任务的缩放定律指数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 12:16:34 GMT</pubDate>
</item>
<item>
<title>基于自生成目标条件MDPs的自动定理证明方法</title>
<link>https://arxiv.org/abs/2507.02726</link>
<guid>https://arxiv.org/abs/2507.02726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新框架提升LLM在复杂推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一种名为自生成目标条件马尔可夫决策过程（sG-MDP）的新框架，旨在解决大型语言模型在自动定理证明中面临的挑战。该框架通过让智能体根据不断变化的证明状态生成并追求子目标，使问题更易于搜索。研究者将这种方法应用于Bourbaki（7B）系统，该系统可以集成多个7B参数的LLM进行子目标生成和策略合成。在PutnamBench基准测试中，Bourbaki（7B）成功解决了26个问题，取得了当前同类模型的最佳成绩。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 11:41:38 GMT</pubDate>
</item>
<item>
<title>HiRA：一种分层框架提升复杂信息检索与推理效率</title>
<link>https://arxiv.org/abs/2507.02652</link>
<guid>https://arxiv.org/abs/2507.02652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HiRA通过分层规划与执行提升复杂搜索任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出HiRA，一种分层信息检索框架，将战略规划与专业执行分离。该方法将复杂搜索任务分解为子任务，并分配给具备外部工具和推理能力的领域特定代理。通过结构化集成机制协调结果，避免执行细节干扰高层推理，从而提升系统效率与答案质量。在四个跨模态深度搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 10:18:08 GMT</pubDate>
</item>
<item>
<title>提升大模型信息检索能力的WebSailor方法</title>
<link>https://arxiv.org/abs/2507.02592</link>
<guid>https://arxiv.org/abs/2507.02592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebSailor提升大模型在复杂信息任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 文章提出WebSailor，一种后训练方法，旨在增强大模型在复杂信息搜索任务中的能力。通过生成高不确定性任务、RFT冷启动和DUPO算法，WebSailor显著提升了开源模型的表现，使其接近专有系统水平。该方法解决了传统模型在处理海量信息时的不确定性问题，推动了大语言模型在信息检索领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.02592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 03 Jul 2025 08:59:07 GMT</pubDate>
</item>
<item>
<title>提升奖励模型性能：基于高质量数据集的Skywork-Reward-V2研究</title>
<link>https://arxiv.org/abs/2507.01352</link>
<guid>https://arxiv.org/abs/2507.01352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork-Reward-V2通过高质量数据提升奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文指出当前开放奖励模型在评估基准上表现不佳，主要由于偏好数据集存在局限。为解决这一问题，作者提出了包含4000万对偏好的SynPref-40M数据集，并设计了人机协同的数据筛选流程。基于该数据集，训练出8个参数规模从0.6B到8B的Skywork-Reward-V2模型，在多个基准测试中取得最优成绩。实验表明，模型性能提升不仅得益于数据量，更得益于高质量的数据筛选。该研究展示了人机协作在数据质量提升中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 00:40:29 GMT</pubDate>
</item>
<item>
<title>多尺度多模态大语言模型在自动放射学报告生成中的应用</title>
<link>https://arxiv.org/abs/2507.00316</link>
<guid>https://arxiv.org/abs/2507.00316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出mu^2LLM模型提升CT影像报告生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自动化放射学报告生成（RRG）技术，旨在通过临床影像数据生成详细文本报告以提高诊断准确性和效率。文章指出该技术面临两大挑战：从影像数据中提取相关信息的复杂性以及模型生成报告与专家报告之间差异的客观评估困难。为解决这些问题，作者提出了mu^2LLM模型，结合多尺度和多模态特征，并通过直接偏好优化提升报告质量。实验结果表明，该方法在多个大型CT图像-报告数据集上优于现有方法，显示出在有限数据下进行RRG任务的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 19:14:49 GMT</pubDate>
</item>
<item>
<title>MARVIS：一种无需训练的多模态推理方法</title>
<link>https://arxiv.org/abs/2507.01544</link>
<guid>https://arxiv.org/abs/2507.01544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARVIS使小型视觉语言模型能高效预测多种数据模态。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARVIS的无需训练的方法，使小型视觉语言模型能够以高精度预测任何数据模态。该方法通过将潜在嵌入空间转换为可视化表示，并利用视觉语言模型的空间和细粒度推理能力进行解释和利用。MARVIS在视觉、音频、生物和表格领域表现出色，使用单一3B参数模型即可达到优于Gemini 16%的平均性能，并接近专用方法，同时不涉及个人身份信息且无需领域特定训练。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 05:56:24 GMT</pubDate>
</item>
<item>
<title>基于自回归框架的实时交互式头部生成方法</title>
<link>https://arxiv.org/abs/2507.00472</link>
<guid>https://arxiv.org/abs/2507.00472</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIG框架实现更真实的实时交互头部生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了面对面交流中的交互式头部生成问题，针对传统方法在实时性和交互真实性上的不足，提出了一种基于自回归（AR）的逐帧生成框架ARIG。该框架通过非向量量化AR过程进行运动预测，并利用扩散过程表示运动分布以提高连续空间预测精度。同时，引入交互行为理解（IBU）和详细对话状态理解（CSU），通过双模态信号分析和上下文建模提升交互真实感。实验验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00472" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:38:14 GMT</pubDate>
</item>
<item>
<title>Locality-aware Parallel Decoding加速自回归图像生成</title>
<link>https://arxiv.org/abs/2507.01957</link>
<guid>https://arxiv.org/abs/2507.01957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPD技术提升自回归图像生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出Locality-aware Parallel Decoding (LPD)方法，用于加速自回归图像生成。传统方法依赖逐块预测，导致高延迟。现有研究虽尝试通过多块预测实现并行化，但效果有限。LPD引入两种关键技术：灵活并行自回归建模和局部感知生成顺序，分别实现任意生成顺序和减少组内依赖，从而显著降低生成步数并提升效率。实验表明，在保持生成质量的前提下，LPD将ImageNet图像生成步数从256降至20（256×256分辨率）和1024降至48（512×512分辨率），延迟降低至少3.4倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>FreeMorph：无需微调的高效图像形态转换方法</title>
<link>https://arxiv.org/abs/2507.01953</link>
<guid>https://arxiv.org/abs/2507.01953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeMorph实现无需训练的高质量图像形态转换，速度快且效果优。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeMorph，一种无需微调的图像形态转换方法，能够处理不同语义或布局的输入。与现有依赖微调扩散模型的方法不同，FreeMorph通过引入引导感知球面插值和步进变化趋势，解决了非线性去噪过程中的质量下降问题，实现了更高效、更一致的图像过渡效果。实验表明，FreeMorph在速度和性能上均优于现有方法，提升达10倍至50倍，并建立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:58:20 GMT</pubDate>
</item>
<item>
<title>视觉-语言-动作模型中的动作标记化研究综述</title>
<link>https://arxiv.org/abs/2507.01925</link>
<guid>https://arxiv.org/abs/2507.01925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述分析VLA模型中动作标记的分类与作用。</p><br /><br /><p><strong>摘要：</strong> 本文综述了视觉-语言-动作（VLA）模型的研究进展，指出当前模型在处理视觉和语言输入后生成一系列动作标记，最终输出可执行动作。文章强调，VLA模型的核心设计差异在于动作标记的构建方式，包括语言描述、代码、可操作性、轨迹等多种形式。然而，对动作标记的理解仍不充分，阻碍了VLA的发展。本文通过分析不同动作标记的优缺点，提出未来研究方向，旨在推动VLA向通用智能迈进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:34:52 GMT</pubDate>
</item>
<item>
<title>STR-Match：一种无需训练的视频编辑算法</title>
<link>https://arxiv.org/abs/2506.22868</link>
<guid>https://arxiv.org/abs/2506.22868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STR-Match通过潜空间优化实现高质量视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为STR-Match的视频编辑算法，该算法无需训练即可生成视觉吸引人且时空一致的视频。其核心在于利用新颖的STR分数，结合2D空间注意力和1D时间模块，捕捉相邻帧之间的时空像素相关性，避免了计算成本高昂的3D注意力机制。通过集成潜空间优化框架和潜空间掩码，STR-Match在保持源视频关键视觉属性的同时，实现了在显著领域变换下的强性能表现。大量实验表明，该方法在视觉质量和时空一致性方面均优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 08:36:19 GMT</pubDate>
</item>
<item>
<title>Kwai Keye-VL：面向短视频理解的多模态大模型</title>
<link>https://arxiv.org/abs/2507.01949</link>
<guid>https://arxiv.org/abs/2507.01949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Kwai Keye-VL提升短视频理解能力，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Kwai Keye-VL，一款80亿参数的多模态基础模型，旨在提升对短视频的理解能力。该模型基于超过6000亿token的高质量数据集和创新的训练方法，包括四阶段预训练和两阶段后训练。第二阶段引入五种模式的数据混合，增强模型的推理能力。通过强化学习和对齐优化，模型在多个视频基准测试中表现优异，并发布了针对真实短视频场景的KC-MMBench基准，进一步验证了其优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>基于动态全局-局部范式的长动画上色方法研究</title>
<link>https://arxiv.org/abs/2507.01945</link>
<guid>https://arxiv.org/abs/2507.01945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LongAnimation框架，提升动画上色的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 动画上色是动画产业的重要环节，传统方法成本高且效率低。现有研究多集中于短期上色，依赖局部特征融合，难以保持长期颜色一致性。本文提出LongAnimation框架，采用动态全局-局部范式，结合SketchDiT、DGLM和Color Consistency Reward模块，有效提升长视频段的颜色一致性。实验表明，该方法在短时（14帧）和长时（平均500帧）动画任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>DepthAnything-AC：一种适应多种环境的单目深度估计模型</title>
<link>https://arxiv.org/abs/2507.01634</link>
<guid>https://arxiv.org/abs/2507.01634</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DepthAnything-AC在复杂环境下表现出色，具备零样本能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DepthAnything-AC的单目深度估计模型，能够在多种复杂环境中保持高精度。与以往模型相比，该模型在光照变化、恶劣天气和传感器畸变等条件下表现更优。研究者引入了无监督一致性正则化微调方法，仅需少量未标记数据即可提升性能，并通过空间距离约束增强模型对局部关系的学习能力。实验结果表明，DepthAnything-AC在多个基准测试中展现出强大的零样本能力，包括真实世界恶劣天气数据集、合成噪声数据集和通用数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01634" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 02 Jul 2025 08:05:57 GMT</pubDate>
</item>
<item>
<title>JAM-Flow：统一生成面部动作与语音的框架</title>
<link>https://arxiv.org/abs/2506.23552</link>
<guid>https://arxiv.org/abs/2506.23552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JAM-Flow实现面部动作与语音的联合生成，提升多模态合成效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出JAM-Flow，一个统一的框架，用于同时生成和条件化面部动作与语音。该方法结合流匹配和多模态扩散Transformer（MM-DiT）架构，包含专门的Motion-DiT和Audio-DiT模块，并通过选择性联合注意力层进行耦合。JAM-Flow采用类似修复的目标进行训练，支持文本、参考音频和参考动作等多种输入，适用于同步生成说话头像、音频驱动动画等任务，显著提升了多模态生成模型的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:51:40 GMT</pubDate>
</item>
<item>
<title>Mixture of Reasoning：提升大语言模型推理能力的新框架</title>
<link>https://arxiv.org/abs/2507.00606</link>
<guid>https://arxiv.org/abs/2507.00606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mixture of Reasoning 提升大语言模型推理性能，无需外部提示。</p><br /><br /><p><strong>摘要：</strong> 本文提出 Mixture of Reasoning (MoR) 框架，通过将多种推理策略嵌入大语言模型中，实现自主、任务自适应的推理能力。该框架包含两个阶段：首先生成推理链模板，然后通过监督微调提升性能。实验表明，MoR 在多个基准测试中显著提升表现，且无需依赖任务特定的提示，为跨任务的鲁棒推理提供了通用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 05:39:04 GMT</pubDate>
</item>
<item>
<title>基于频率修正的神经材质表示方法FreNBRDF</title>
<link>https://arxiv.org/abs/2507.00476</link>
<guid>https://arxiv.org/abs/2507.00476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreNBRDF提升材质建模精度与可解释性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为FreNBRDF的频率修正神经材质表示方法，旨在提高材质建模的准确性和可解释性。传统方法依赖于表格化的BRDF数据，而现代方法则采用隐式神经表示，但其在频域中的行为仍不明确。该研究通过引入球谐函数，将频域考虑整合到神经BRDF建模中，并设计了一种新的频率修正损失函数。该框架提升了材质重建和编辑的保真度、适应性和效率。实验表明，FreNBRDF在材质外观重建和编辑方面优于现有方法，支持更结构化和可解释的下游任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 02:48:50 GMT</pubDate>
</item>
<item>
<title>MOVi-MC-AC：首个多摄像头视图的非模态分割与内容数据集</title>
<link>https://arxiv.org/abs/2507.00339</link>
<guid>https://arxiv.org/abs/2507.00339</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MOVi-MC-AC是首个支持多摄像头视图的非模态分割数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MOVi-MC-AC，这是目前最大的非模态分割和首个非模态内容数据集。该数据集通过多摄像头模拟复杂家庭场景中的物体遮挡情况，提供了约580万实例的标签，并首次引入了真实非模态内容的地面实况。相比以往依赖慢速拼接生成伪标签的方法，MOVi-MC-AC在合成视频中实现了跨帧和多视角的一致性对象标识，为计算机视觉中的目标检测、跟踪和分割研究提供了新的挑战和机遇。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00339" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 20:36:56 GMT</pubDate>
</item>
<item>
<title>MusiXQA：推动多模态大模型理解乐谱的基准数据集</title>
<link>https://arxiv.org/abs/2506.23009</link>
<guid>https://arxiv.org/abs/2506.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MusiXQA是首个用于评估音乐乐谱理解的多模态大模型数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MusiXQA，这是首个针对多模态大语言模型（MLLMs）在音乐乐谱理解方面的综合数据集。该数据集通过MusiXTeX生成高质量的合成乐谱，并包含结构化的注释，涵盖音符音高与时值、和弦、谱号、调号与拍号等信息，支持多种视觉问答任务。实验表明当前最先进的MLLMs在该领域存在显著不足，为此作者开发了Phi-3-MusiX模型，在该数据集上取得了优于GPT类方法的性能。该数据集和模型为未来MLLMs在音乐乐谱理解领域的研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 16:46:47 GMT</pubDate>
</item>
<item>
<title>基于置信度的3D高斯点云压缩方法</title>
<link>https://arxiv.org/abs/2506.22973</link>
<guid>https://arxiv.org/abs/2506.22973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于置信度的3D高斯点云压缩方法，提升渲染效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于可学习置信度分数的损失函数，用于压缩3D高斯点云。该方法通过优化重建感知损失来调整每个点的置信度，从而修剪低置信度点，同时保持视觉质量。该方法与架构无关，适用于任何高斯点云渲染变体，并引入平均置信度作为场景质量的新评估指标。实验表明，该方法在压缩率和保真度之间取得了良好平衡。代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 14:11:30 GMT</pubDate>
</item>
<item>
<title>FreeLong++：提升长视频生成质量的训练无关框架</title>
<link>https://arxiv.org/abs/2507.00162</link>
<guid>https://arxiv.org/abs/2507.00162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FreeLong++通过多频段融合提升长视频生成的时序一致性和视觉质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出FreeLong和FreeLong++，用于解决长视频生成中时间一致性下降和视觉质量退化的问题。FreeLong通过在去噪过程中融合全局低频特征与局部高频特征，平衡长视频的频率分布。FreeLong++进一步扩展为多分支架构，支持多尺度时间窗口的多频段融合，从而增强语义连贯性和细节动态。该方法无需额外训练即可集成到现有模型中，显著提升长视频生成效果，并支持多提示生成、平滑场景切换及可控视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 14:11:21 GMT</pubDate>
</item>
<item>
<title>IR3D-Bench：通过主动创造评估视觉语言模型的场景理解能力</title>
<link>https://arxiv.org/abs/2506.23329</link>
<guid>https://arxiv.org/abs/2506.23329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IR3D-Bench挑战VLMs通过主动创造理解场景，推动其生成能力发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了IR3D-Bench，一个用于评估视觉语言模型（VLMs）场景理解能力的新基准。该基准要求模型通过主动使用编程和渲染工具来重建输入图像的3D结构，从而实现“通过创造理解”的方法。不同于传统基于被动识别的评估方式，IR3D-Bench强调工具使用和生成能力。研究提供了多种指标来评估几何准确性、空间关系、外观属性和整体合理性。实验表明，当前VLMs在视觉精度方面仍存在局限。IR3D-Bench包含数据和评估协议，旨在促进工具使用型VLMs的发展，以实现真正的场景理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 13:02:57 GMT</pubDate>
</item>
<item>
<title>GLM-4.1V-Thinking：多模态推理模型的进展与性能评估</title>
<link>https://arxiv.org/abs/2507.01006</link>
<guid>https://arxiv.org/abs/2507.01006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GLM-4.1V-Thinking在多模态任务中表现卓越，超越多个基准。</p><br /><br /><p><strong>摘要：</strong> GLM-4.1V-Thinking是一款面向通用多模态推理的视觉语言模型。通过大规模预训练和基于课程采样的强化学习（RLCS），该模型在STEM问题解决、视频理解、内容识别、编程、定位、GUI代理和长文档理解等多个任务中展现出全面的能力提升。开源版本GLM-4.1V-9B-Thinking在28个公共基准测试中表现出色，超越Qwen2.5-VL-7B，并在多数任务上优于更大的Qwen2.5-VL-72B模型。此外，它在长文档理解和STEM推理等挑战性任务上也表现出与GPT-4o相当或更优的性能。代码和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>SciArena：科学文献任务的开放协作评估平台</title>
<link>https://arxiv.org/abs/2507.01001</link>
<guid>https://arxiv.org/abs/2507.01001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SciArena通过社区投票评估基础模型在科学文献任务中的表现。</p><br /><br /><p><strong>摘要：</strong> SciArena是一个开放协作平台，用于评估基础模型在科学文献任务中的表现。与传统基准不同，它采用社区投票方式，让研究人员直接参与模型比较。该平台已支持23个开源和专有模型，并收集了超过13,000份来自不同领域研究者的投票。分析显示，提交的问题多样且符合实际需求，研究人员在评估中表现出高度一致性和准确性。此外，团队还发布了SciArena-Eval，一个基于偏好数据的元评估基准，用于衡量模型判断答案质量的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.01001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>迈向通用人工智能：跨学科视角下的认知与架构分析</title>
<link>https://arxiv.org/abs/2507.00951</link>
<guid>https://arxiv.org/abs/2507.00951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AGI发展需整合记忆、推理与多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人工智能向通用人工智能（AGI）发展的关键问题，指出当前模型如GPT-4.5、DeepSeek等虽具备多模态能力和部分推理能力，但仍受限于基于token的预测机制和缺乏具身代理。文章从人工智能、认知神经科学、心理学等多个领域出发，分析了AGI的架构与认知基础，强调模块化推理、持久记忆和多智能体协作的重要性。同时，Agentic RAG框架、信息压缩与测试时适应等策略被视为实现灵活智能的关键路径。此外，视觉语言模型被重新定义为具身理解与协作任务的接口。作者认为，真正的智能源于记忆与推理的融合，而非单纯依赖规模。最后，文章讨论了AGI发展中的科学、技术和伦理挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 12:52:25 GMT</pubDate>
</item>
<item>
<title>AI生成内容激增与新型水印技术PECCAVI的提出</title>
<link>https://arxiv.org/abs/2506.22960</link>
<guid>https://arxiv.org/abs/2506.22960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AI生成内容将达90%，PECCAVI应对水印攻击。</p><br /><br /><p><strong>摘要：</strong> 欧洲联盟执法机构报告预测，到2026年，高达90%的在线内容可能由生成式AI创建，引发政策制定者的担忧。加州AB 3211法案要求对AI生成内容进行水印标记，但现有技术易被篡改。本文提出PECCAVI，一种针对视觉重述攻击的安全且无失真的图像水印技术。该技术在图像的核心语义区域嵌入水印，并利用多通道频域水印和噪声烧制技术增强抗逆向工程能力。PECCAVI具备模型无关性，相关资源将开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 13:34:08 GMT</pubDate>
</item>
<item>
<title>提升语言模型训练效果的数据效能研究</title>
<link>https://arxiv.org/abs/2506.21545</link>
<guid>https://arxiv.org/abs/2506.21545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数据效能优化可显著提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数据效能（Data Efficacy）在语言模型训练中的作用，提出了一种名为DELT的通用范式，包含数据评分、数据选择和数据排序三个组件。其中，Learnability-Quality Scoring（LQS）通过梯度一致性评估样本的可学习性和质量，Folding Ordering（FO）则解决模型遗忘和数据分布偏差问题。实验表明，DELT能有效提升模型性能，且与数据效率结合使用效果更佳，证明数据效能是语言模型训练的重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>扩散语言模型在代码生成中的应用与优化</title>
<link>https://arxiv.org/abs/2506.20639</link>
<guid>https://arxiv.org/abs/2506.20639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究扩散语言模型在代码生成中的解码行为与强化学习训练方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散语言模型（dLLMs）在代码生成中的潜力，分析了其与自回归模型的不同之处，如生成的因果性控制和采样温度对生成顺序的影响。作者训练了一个7B参数的dLLM模型DiffuCoder，并提出了一种新的采样方案coupled-GRPO，以提升强化学习训练效率。实验结果显示，该方法在代码生成基准测试中提升了4.4%，并减少了对自回归因果性的依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:35:47 GMT</pubDate>
</item>
<item>
<title>基于时空能量衰减的径向注意力机制提升视频生成效率</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出径向注意力机制，提升视频生成效率与长度。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视频扩散模型中的时空能量衰减现象，并提出了径向注意力机制。该机制通过稀疏注意力计算，将能量衰减转化为计算密度的指数衰减，从而显著降低计算复杂度。实验表明，该方法在多个视频生成模型上保持高质量的同时，提升了生成速度并减少了训练成本。相比传统密集注意力，其效率更高，且支持更长视频的生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>数学推理模型的泛化能力与训练方法研究</title>
<link>https://arxiv.org/abs/2507.00432</link>
<guid>https://arxiv.org/abs/2507.00432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">数学模型在特定任务上表现优异，但泛化能力有限。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型在数学推理任务上的进展，并探讨其是否具备更广泛的问题解决能力。通过对20多个模型的评估发现，多数数学表现优秀的模型在其他领域如科学问答、编程和指令遵循上表现不佳。通过对比强化学习和监督微调两种训练方法，发现强化学习模型在跨领域任务中表现更好，而监督微调模型容易失去通用能力。分析表明，监督微调会导致表示和输出分布的变化，影响模型的泛化能力，提示需要重新考虑当前的训练策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2507.00432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 01:23:05 GMT</pubDate>
</item>
<item>
<title>MoCa：提升多模态嵌入模型性能的两阶段框架</title>
<link>https://arxiv.org/abs/2506.23115</link>
<guid>https://arxiv.org/abs/2506.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoCa通过两阶段方法提升多模态嵌入模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出MoCa，一种将预训练视觉语言模型转化为高效双向多模态嵌入模型的两阶段框架。第一阶段为模态感知持续预训练，引入联合重建目标以增强双向上下文感知推理；第二阶段为异构对比微调，利用多样化的多模态数据提升泛化能力和对齐效果。该方法解决了现有模型在注意力机制、数据依赖性和训练目标多样性方面的不足，并在多个基准测试中取得最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 02:41:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2506.21277</link>
<guid>https://arxiv.org/abs/2506.21277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出方法增强多模态模型理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要。本文指出当前多模态推理模型存在全局上下文理解不足和依赖捷径的问题，并提出通过引入上下文奖励、格式奖励和逻辑奖励来提升模型的推理能力。同时，作者构建了IntentBench基准测试，用于评估模型在理解复杂人类意图和情感方面的表现。实验结果表明，该方法在多个多模态基准上优于现有开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 10:01:03 GMT</pubDate>
</item>
<item>
<title>MEMFOF：高效多帧光流估计方法</title>
<link>https://arxiv.org/abs/2506.23151</link>
<guid>https://arxiv.org/abs/2506.23151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEMFOF在高分辨率下实现高效光流估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出MEMFOF，一种内存高效的多帧光流估计方法，在保持高精度的同时显著降低GPU内存消耗。该方法在1080p输入下仅需2.09GB显存运行，训练时为28.5GB，无需裁剪或降采样即可在原生分辨率下训练。通过优化RAFT架构设计，结合减少的卷积体积和高分辨率训练策略，MEMFOF在多个基准测试中取得最佳性能，包括Spring、Sintel和KITTI-2015数据集，展现出出色的准确性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 05:01:42 GMT</pubDate>
</item>
<item>
<title>基于多路径扩散的可调金属镜头摄影方法</title>
<link>https://arxiv.org/abs/2506.22753</link>
<guid>https://arxiv.org/abs/2506.22753</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型金属镜头成像方法，提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于退化建模的多路径扩散方法，用于可调金属镜头摄影。该方法利用预训练模型中的自然图像先验，而非依赖大规模数据集，通过正向、中性与负向提示路径平衡高频细节生成、结构保真度和金属镜头特有退化的抑制。同时引入伪数据增强和可调解码器，实现保真度与感知质量的可控权衡。此外，设计了空间变化的退化感知注意力模块，以适应复杂的光学和传感器引起的退化。最终构建了毫米级MetaCamera进行实际验证，实验结果表明该方法优于现有技术，实现了高保真和清晰的图像重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22753" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 00:48:37 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型代理在研究扩展任务中的能力</title>
<link>https://arxiv.org/abs/2506.22598</link>
<guid>https://arxiv.org/abs/2506.22598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM代理在自主实现研究扩展任务上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了基于大型语言模型（LLMs）的智能体在自主执行软件工程和科研任务方面的潜力。研究引入了RExBench，一个包含12个真实研究实验任务的基准测试，用于评估智能体在扩展已有研究成果方面的能力。每个任务都基于现有论文和代码库，并附有专家指导说明。尽管RExBench具备抗数据污染能力和自动化评估系统，但测试结果显示，使用不同框架开发的九个LLM代理在没有大量人工提示的情况下，仅能完成不到40%的任务。这表明当前智能体仍需大量人工干预才能处理现实中的研究扩展任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 15:41:41 GMT</pubDate>
</item>
<item>
<title>Tower+：在翻译与多语言通用能力之间实现性能平衡的模型</title>
<link>https://arxiv.org/abs/2506.17080</link>
<guid>https://arxiv.org/abs/2506.17080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tower+在翻译和多语言通用任务中表现出色，兼顾专业与泛用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Tower+，一个在机器翻译和多语言通用文本处理方面均表现优异的模型系列。通过引入一种新的训练方法，包括持续预训练、监督微调、偏好优化和基于可验证奖励的强化学习，Tower+实现了翻译专业化与多语言通用能力之间的帕累托最优。研究团队在多个规模（2B、9B、72B）上构建了模型，并在代码生成、数学问题解决和指令遵循等任务中提升了性能。实验结果显示，Tower+在高资源语言翻译中表现卓越，并在多语言Arena Hard评估和IF-MT基准测试中取得领先。该研究证明，在优化特定业务领域（如翻译和本地化）的同时，仍可达到前沿模型的通用能力水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:30:06 GMT</pubDate>
</item>
<item>
<title>基于自对弈的强化学习框架SPIRAL提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.24119</link>
<guid>https://arxiv.org/abs/2506.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIRAL通过自对弈训练提升语言模型推理能力，无需人工监督。</p><br /><br /><p><strong>摘要：</strong> 本文提出SPIRAL，一种基于自对弈的强化学习框架，使语言模型通过与不断进化的自我版本进行零和博弈来学习，从而无需依赖人工标注的数据或领域特定奖励工程。该框架生成持续进阶的问题课程，推动模型适应更强对手。研究中引入了角色条件优势估计（RAE）以稳定多智能体训练，并在Kuhn扑克等游戏中验证了其有效性。实验表明，SPIRAL可显著提升数学和通用推理能力，且多游戏训练进一步增强模型表现。结果表明，零和博弈是发展可迁移推理能力的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>基于运动不变图融合的ToF深度去噪网络</title>
<link>https://arxiv.org/abs/2506.23542</link>
<guid>https://arxiv.org/abs/2506.23542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型ToF深度去噪方法，提升时间稳定性和空间锐度。</p><br /><br /><p><strong>摘要：</strong> 本文针对ToF传感器获取的深度图像易受噪声影响的问题，提出了一种基于运动不变图融合的深度去噪网络。该方法利用跨帧几何注意力机制，结合图像平滑先验和ToF噪声分布的数据保真项，构建最大后验问题进行去噪。通过将解法展开为自适应学习的迭代滤波器，实现了高精度且可解释的去噪效果。实验结果表明，该方法在合成数据集和真实数据集上均表现出色，具有良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 02:29:24 GMT</pubDate>
</item>
<item>
<title>多语言模型工具调用能力提升方法研究</title>
<link>https://arxiv.org/abs/2506.23394</link>
<guid>https://arxiv.org/abs/2506.23394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过持续训练提升多语言模型的工具调用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，使多语言语言模型能够在非英语语言中实现可靠的工具调用。以保加利亚语为例，研究者对BgGPT模型系列进行了持续训练，使用了一个包含10,035个函数调用示例的双语数据集。该方法引入了TUCAN模型，在保加利亚语基准测试中实现了28.75%的函数调用准确率提升，并保持了核心语言理解能力。TUCAN模型还表现出更规范、可解析的输出格式，优于基础模型。研究提供了模型、评估框架和数据集，支持其他语言的复现与扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 16:47:27 GMT</pubDate>
</item>
<item>
<title>UrbanLLaVA：面向城市研究的多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.23219</link>
<guid>https://arxiv.org/abs/2506.23219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UrbanLLaVA提升城市多模态任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出UrbanLLaVA，一个专为城市研究设计的多模态大语言模型。该模型能够同时处理多种城市数据，并在多个城市任务中表现出色。研究团队构建了一个涵盖单模态与跨模态数据的城市指令数据集，并设计了分阶段训练框架以提升空间推理和领域知识学习的兼容性。此外，还扩展了城市研究基准以评估模型性能。实验结果表明，UrbanLLaVA在多个城市任务中优于现有开源和商业模型，展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 09:04:27 GMT</pubDate>
</item>
<item>
<title>RoboScape：一种融合物理知识的统一世界模型</title>
<link>https://arxiv.org/abs/2506.23135</link>
<guid>https://arxiv.org/abs/2506.23135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoboScape通过物理信息联合训练提升机器人视频生成的物理合理性。</p><br /><br /><p><strong>摘要：</strong> 本文提出RoboScape，一个融合物理知识的统一世界模型，能够同时学习RGB视频生成和物理知识。该模型引入了时间深度预测和关键点动力学学习两个关键任务，以增强视频生成的3D几何一致性和复杂运动建模能力。实验表明，RoboScape在多种机器人场景中生成的视频具有更高的视觉质量和物理合理性，并在机器人策略训练和评估中展现出实用价值。研究为构建高效的物理感知世界模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Jun 2025 04:19:45 GMT</pubDate>
</item>
<item>
<title>Ovis-U1：一款融合多模态理解与生成能力的大型统一模型</title>
<link>https://arxiv.org/abs/2506.23044</link>
<guid>https://arxiv.org/abs/2506.23044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ovis-U1是一款30亿参数的多模态统一模型，性能领先。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Ovis-U1，一款拥有30亿参数的统一模型，具备多模态理解、文本到图像生成和图像编辑能力。该模型基于Ovis系列，采用基于扩散的视觉解码器和双向标记精修器，在多个基准测试中表现优异，如OpenCompass多模态学术基准得分69.6，文本到图像生成在DPG-Bench和GenEval分别获得83.72和0.89分，图像编辑在ImgEdit-Bench和GEdit-Bench-EN分别获得4.00和6.42分。相比以往模型，Ovis-U1通过统一训练方法提升了性能，展示了多任务整合的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 20:40:17 GMT</pubDate>
</item>
<item>
<title>提出MARBLE基准测试，推动多模态推理模型发展</title>
<link>https://arxiv.org/abs/2506.22992</link>
<guid>https://arxiv.org/abs/2506.22992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MARBLE挑战多模态推理模型的逐步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MARBLE，一个用于评估多模态语言模型（MLLMs）在复杂多模态问题和环境中进行逐步推理能力的基准测试。MARBLE包含两个高难度任务M-Portal和M-Cube，要求模型在空间、视觉和物理约束下制定并理解多步骤计划。实验结果显示，当前12个先进模型在M-Portal任务中表现接近随机，在M-Cube任务中准确率为0%。只有在简化子任务中部分模型优于随机基线，表明复杂推理仍是MLLMs的挑战。此外，研究发现感知能力仍是瓶颈，模型在从视觉输入中提取信息时存在困难。作者希望通过MARBLE推动下一代多模态推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 15:44:32 GMT</pubDate>
</item>
<item>
<title>基于监听器增强的GRPO框架提升视觉语言模型对齐效果</title>
<link>https://arxiv.org/abs/2506.22832</link>
<guid>https://arxiv.org/abs/2506.22832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监听器增强的GRPO方法提升视觉语言模型的对齐性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于监听器增强的Group Relative Policy Optimization (GRPO)框架，用于提升视觉语言模型与人类偏好的对齐效果。传统奖励模型在泛化性方面存在不足，而监督微调容易导致记忆现象。尽管RL方法如GRPO有所改进，但当模型推理过程与独立的冻结视觉-语言模型（“监听器”）不一致时，推理准确性会显著下降。为此，本文引入监听器重新评估推理链，提供密集且校准的置信度评分，从而优化强化学习奖励信号。该方法不仅提高了准确率，还在大规模人类偏好数据集上提升了分布外性能，并减少了推理矛盾。实验结果表明，监听器奖励机制是一种高效、可扩展的对齐路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 28 Jun 2025 05:53:17 GMT</pubDate>
</item>
<item>
<title>基于语言模型头的无训练优化方法提升推测解码性能</title>
<link>https://arxiv.org/abs/2506.22694</link>
<guid>https://arxiv.org/abs/2506.22694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VocabTrim技术优化推测解码速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需训练的优化方法，用于提升基于草稿模型的推测解码（SpD）性能。该方法在草稿生成过程中引入语言模型头（LM head），通过限制草稿模型的词汇量，仅保留目标模型中高频采样的词，从而降低内存瓶颈下的推理延迟。尽管接受率略有下降，但显著提升了生成速度，尤其在边缘设备上效果明显。实验表明，该方法在Llama-3.2-3B-Instruct模型上可提升内存瓶颈速度约16%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 20:26:40 GMT</pubDate>
</item>
<item>
<title>ThinkSound：基于思维链推理的视频到音频生成框架</title>
<link>https://arxiv.org/abs/2506.21448</link>
<guid>https://arxiv.org/abs/2506.21448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkSound通过思维链推理实现高质量视频到音频生成。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ThinkSound，一个利用思维链（CoT）推理的视频到音频生成框架。该框架将过程分为三个阶段：基础音效生成、交互式对象优化和自然语言指导的定向编辑。每个阶段都由多模态大语言模型生成上下文相关的CoT推理，以引导统一的音频基础模型。同时，研究者还发布了AudioCoT数据集，用于连接视觉内容、文本描述和声音合成。实验表明，ThinkSound在多个音频指标和CoT指标上均达到领先水平，并在Movie Gen Audio基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:32:06 GMT</pubDate>
</item>
<item>
<title>基于随机演示剪枝的新型提示设计范式</title>
<link>https://arxiv.org/abs/2506.17930</link>
<guid>https://arxiv.org/abs/2506.17930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过剪枝随机演示提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种颠覆传统思路的提示设计方法，通过剪除随机演示生成看似无意义的“乱码”提示，反而显著提升多种任务表现。该方法在不同任务和模型上均优于现有自动提示优化技术。为解决有效剪枝策略的发现难题，作者提出PromptQuine框架，利用进化搜索在低数据条件下自动寻找最优策略。该框架模仿自然界的复杂性演化机制，在仅使用上下文内token的情况下，生成高效且非传统的提示。实验表明其在分类、问答、生成和数学推理等任务中表现优异，同时具备良好的运行效率。研究希望推动对上下文学习的机制研究，并鼓励开发更开放的搜索算法以提升大语言模型提示效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 03:53:07 GMT</pubDate>
</item>
<item>
<title>SparseLoRA：通过上下文稀疏性加速大模型微调</title>
<link>https://arxiv.org/abs/2506.16500</link>
<guid>https://arxiv.org/abs/2506.16500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SparseLoRA通过稀疏性提升大模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出SparseLoRA，一种利用上下文稀疏性加速大语言模型微调的方法。该方法引入轻量级、无需训练的SVD稀疏性估计器，动态选择部分权重进行损失和梯度计算，从而降低计算成本。实验表明，SparseLoRA在保持准确性的前提下，将计算成本减少最多2.2倍，并实现1.6倍的速度提升，适用于多种下游任务如常识推理、数学运算、代码生成等。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:53:34 GMT</pubDate>
</item>
<item>
<title>Calligrapher：基于扩散模型的数字书法与设计框架</title>
<link>https://arxiv.org/abs/2506.24123</link>
<guid>https://arxiv.org/abs/2506.24123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Calligrapher通过创新技术实现精准风格控制和高质量字体生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Calligrapher，一个基于扩散模型的新型框架，结合文本定制与艺术排版，用于数字书法和设计应用。该框架解决了风格控制和数据依赖性的挑战，包含三个关键技术贡献：自蒸馏机制、局部风格注入框架以及上下文生成机制。这些技术提升了目标风格的精确对齐和字形定位。实验结果表明，Calligrapher在多种字体和设计场景中能够准确再现复杂的风格细节，优于传统模型，为数字艺术、品牌设计和上下文排版提供了强大支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.24123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>视频扩散模型中的稀疏注意力机制VMoBA</title>
<link>https://arxiv.org/abs/2506.23858</link>
<guid>https://arxiv.org/abs/2506.23858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VMoBA提升视频扩散模型效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对视频扩散模型的新型稀疏注意力机制——Video Mixture of Block Attention (VMoBA)。该方法通过分析预训练视频Transformer中的注意力模式，引入了三层递归块划分、全局块选择和基于阈值的块选择策略，以优化视频数据的时空特征捕捉。实验表明，VMoBA在保持生成质量的同时，显著提升了训练效率，实现了更高的FLOPs和延迟加速效果，并在无训练推理中也表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.23858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 30 Jun 2025 09:52:31 GMT</pubDate>
</item>
<item>
<title>推理时技术在视觉语言模型中的有效性研究</title>
<link>https://arxiv.org/abs/2506.17417</link>
<guid>https://arxiv.org/abs/2506.17417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理时技术提升VLM推理能力，但自验证能力仍不足。</p><br /><br /><p><strong>摘要：</strong> 本文研究了推理时计算技术在视觉语言模型（VLMs）中的应用效果，特别是基于强化学习（RL）训练的模型。实验表明，如多数投票和最佳N选择等解码策略能有效提升VLM的推理表现，其中生成依赖方法优于验证依赖方法。然而，与RL调优模型相关的自我修正行为（如顿悟时刻）并未带来显著提升。研究发现，RL训练的VLM在视觉和文本模态上的自我验证能力仍较弱，这是影响推理性能的关键原因。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 14:23:48 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在世界建模能力上的系统评估</title>
<link>https://arxiv.org/abs/2506.21876</link>
<guid>https://arxiv.org/abs/2506.21876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLMs在基础世界建模能力上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于认知科学的两阶段框架，用于评估视觉语言模型（VLMs）作为内部世界模型的能力，涵盖感知和预测两个方面。研究引入了WM-ABench基准，包含23个细粒度维度，在6个模拟环境中进行测试。通过对15个最新商业和开源VLMs的660次实验，发现这些模型在基本世界建模能力上表现不佳，如无法准确区分运动轨迹，且缺乏对变量的解耦理解。结果揭示了VLMs与人类水平之间存在显著差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 23:24:29 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型的空间想象能力：MindCube基准与新方法</title>
<link>https://arxiv.org/abs/2506.21458</link>
<guid>https://arxiv.org/abs/2506.21458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出MindCube评估VLM空间推理能力，通过认知地图提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型（VLMs）是否能像人类一样从少量视角中想象完整场景。作者提出了MindCube基准，包含3,268张图像和21,154个问题，揭示现有VLM在空间推理方面表现不佳。研究评估了VLM构建空间心理模型的能力，包括位置、视角和动态模拟。通过引入未见中间视角、自然语言推理链和认知地图等方法，特别是“先生成地图再推理”的协同策略，显著提升了模型性能，准确率从37.8%提升至60.8%，进一步结合强化学习后达到70.7%。研究强调构建结构化内部空间表示对理解不可见空间的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:38:19 GMT</pubDate>
</item>
<item>
<title>TAPAS：基于多智能体的复杂任务求解框架</title>
<link>https://arxiv.org/abs/2506.19592</link>
<guid>https://arxiv.org/abs/2506.19592</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TAPAS通过LLM与符号规划结合，实现复杂任务自动化求解。</p><br /><br /><p><strong>摘要：</strong> TAPAS是一种多智能体框架，将大型语言模型（LLMs）与符号规划相结合，用于解决不需要手动定义环境模型的复杂任务。该框架利用专门的LLM代理协作生成和调整领域模型、初始状态和目标规范，并通过结构化工具调用机制进行交互。下游代理可以向上游代理请求修改，从而适应新的属性和约束。TAPAS还引入了类似ReAct风格的执行代理和自然语言计划翻译，以连接动态生成的计划与实际机器人能力。实验表明，TAPAS在基准规划领域和虚拟家庭模拟环境中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19592" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 09:02:06 GMT</pubDate>
</item>
<item>
<title>Fractional Reasoning：动态调整推理强度提升大语言模型性能</title>
<link>https://arxiv.org/abs/2506.15882</link>
<guid>https://arxiv.org/abs/2506.15882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Fractional Reasoning通过动态调整推理强度提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练且与模型无关的Fractional Reasoning框架，能够在推理阶段动态控制推理强度。该方法通过提取与深度推理相关的潜在引导向量，并以可调缩放因子重新应用，使模型能根据输入复杂度自适应调整推理过程。该方法支持两种测试时扩展模式：提高广度策略（如Best-of-N）的输出质量，以及增强深度策略（如自省）的正确性。实验表明，Fractional Reasoning在GSM8K、MATH500和GPQA等任务中均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 17:15:59 GMT</pubDate>
</item>
<item>
<title>基于3D代理的视频编辑框架Shape-for-Motion</title>
<link>https://arxiv.org/abs/2506.22432</link>
<guid>https://arxiv.org/abs/2506.22432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Shape-for-Motion框架，实现精准视频编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Shape-for-Motion的新框架，通过将视频中的目标物体转换为时间一致的3D网格代理，实现对视频内容的精确和一致编辑。该框架采用双传播策略，允许用户在单帧上进行编辑，并自动传播到其他帧。随后，3D网格被投影到2D空间生成编辑后的几何和纹理渲染图，输入到解耦视频扩散模型中生成最终结果。该方法支持多种精确且物理一致的视频编辑操作，如姿态调整、旋转、缩放、平移、纹理修改和对象合成，展示了其在高质量可控视频编辑中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>GPAS提升预归一化Transformer的训练效果</title>
<link>https://arxiv.org/abs/2506.22049</link>
<guid>https://arxiv.org/abs/2506.22049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPAS技术有效缓解Pre-LN模型激活方差问题，提升训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为Gradient-Preserving Activation Scaling (GPAS) 的技术，旨在解决Pre-LN Transformer架构中激活值方差随层数增加而指数增长的问题。该技术通过缩放中间激活值但保持梯度不变，避免了梯度消失问题，同时保留激活信息。实验表明，GPAS在多种模型规模（71M到1B参数）下均能提升性能，并且对Sandwich-LN和DeepNorm等其他架构也表现出良好的适应性，具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 05:45:15 GMT</pubDate>
</item>
<item>
<title>基于文本到文本回归的系统资源效率预测方法</title>
<link>https://arxiv.org/abs/2506.21718</link>
<guid>https://arxiv.org/abs/2506.21718</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文本到文本回归在系统资源预测中表现优异，优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 在许多行业中，预测大型系统的指标结果是一个核心问题，传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时面临挑战。本文提出了一种通用且可扩展的文本到文本回归方法。该方法在Google的Borg调度系统上取得了显著效果，使用60M参数的编码器-解码器模型，在整个集群中实现了接近完美的0.99（平均0.9）排名相关性，并且均方误差比传统方法低100倍。此外，模型仅需500个少样本示例即可适应新任务，并能捕捉复杂结果分布的密度。消融实验表明，使用编码器、增加序列长度以及模型的不确定性量化能力至关重要。这些成果为现实世界结果的通用模拟器奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21718" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 15:10:08 GMT</pubDate>
</item>
<item>
<title>基于RCME框架的视觉-语言模型层次结构学习</title>
<link>https://arxiv.org/abs/2506.21476</link>
<guid>https://arxiv.org/abs/2506.21476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RCME框架提升视觉-语言模型层次结构建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为径向跨模态嵌入（RCME）的框架，用于在视觉-语言模型中显式建模蕴含关系的传递性。该框架优化了概念在表示空间中的偏序关系，从而构建出能够表达生命树层次结构的视觉-语言基础模型。实验表明，该模型在层次分类和检索任务中优于现有最先进模型。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:05:06 GMT</pubDate>
</item>
<item>
<title>多模态上下文学习在医学任务中的挑战与评估</title>
<link>https://arxiv.org/abs/2506.21355</link>
<guid>https://arxiv.org/abs/2506.21355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大模型在医学任务中表现有限，上下文学习效果不佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）在医学任务中进行多模态上下文学习（ICL）的潜力与局限性。研究引入了SMMILE基准，包含111个医学问题和517个问答图像对，覆盖6个医学专科和13种影像技术。进一步扩展的SMMILE++包含1038个排列问题。实验表明，大多数模型在医学任务中表现出中等至较差的多模态ICL能力，上下文学习仅带来约8%-9.4%的性能提升。此外，研究发现不相关示例会显著降低性能，而示例顺序存在近期偏差，最后出现的相关示例可大幅提升表现。结果揭示了当前MLLM在医学多模态任务中面临的关键限制和偏见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:08:18 GMT</pubDate>
</item>
<item>
<title>Confucius3-Math：面向中国K-12数学教育的高效大语言模型</title>
<link>https://arxiv.org/abs/2506.18330</link>
<guid>https://arxiv.org/abs/2506.18330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Confucius3-Math在单块消费级GPU上实现高效运行并取得SOTA数学推理性能。</p><br /><br /><p><strong>摘要：</strong> Confucius3-Math是一款开源的大语言模型，拥有140亿参数，能够在单块消费级GPU上高效运行，并在多项数学推理任务中表现优异，超越了许多参数量更大的模型。该模型专为提升中国K-12阶段数学教育和知识传播而设计，通过大规模强化学习微调，与国家课程标准对齐，擅长解决主流数学问题。研究中提出了三项技术创新，包括目标熵正则化、近期样本恢复和策略特定难度加权，显著提升了训练稳定性、数据效率和模型性能。项目已开源，代码和模型可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 02:23:53 GMT</pubDate>
</item>
<item>
<title>基于贝叶斯框架的上下文学习策略分析</title>
<link>https://arxiv.org/abs/2506.17859</link>
<guid>https://arxiv.org/abs/2506.17859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了上下文学习中策略选择的贝叶斯机制。</p><br /><br /><p><strong>摘要：</strong> 本文通过贝叶斯预测器统一解释了上下文学习（ICL）中的不同策略，提出了一种层次化贝叶斯框架，能够准确预测Transformer模型的下一个词预测行为。该框架将预训练视为策略后验概率的更新过程，并在推理时对不同策略进行加权平均。研究强调了策略选择中损失与复杂度之间的权衡，解释了已知的ICL现象并提出了新的预测，如任务多样性增加时从泛化到记忆的过渡时间呈超线性增长。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 19:49:08 GMT</pubDate>
</item>
<item>
<title>基于视觉对比的链式推理方法研究</title>
<link>https://arxiv.org/abs/2506.22434</link>
<guid>https://arxiv.org/abs/2506.22434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过视觉对比训练模型进行链式推理，无需人工标注数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新的方法，利用视觉对比任务来增强模型的链式推理能力。该方法基于自监督学习，构建图像三元组，包括同一图像的两个增强视图和一个相似但不同的图像。模型在训练过程中被提示生成推理过程以比较这些图像，并通过规则强化学习进行优化。由于图像间的高度相似性和增强操作，模型必须关注细微的视觉变化并进行逻辑推理。实验表明，尽管仅在视觉对比任务上训练，该方法在多图像推理基准测试中表现出色，并在通用视觉任务中也展现出强大性能，且无需依赖人工标注的问答对。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>评估大语言模型在科学再现任务中的能力</title>
<link>https://arxiv.org/abs/2506.22419</link>
<guid>https://arxiv.org/abs/2506.22419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大语言模型在再现已有研究成果方面仍存在挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Automated LLM Speedrunning Benchmark，用于评估AI代理在科学领域再现已有成果的能力。该基准基于NanoGPT速度竞赛，包含19个任务，提供训练脚本和不同形式的提示。尽管使用最新的推理模型和最先进的工具，AI仍难以复现已知创新，表明科学再现仍是AI研究的重要挑战。该基准为衡量AI自动化科学再现能力提供了有效手段。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>基于视觉-语言预训练的RetFiner提升OCT图像分类性能</title>
<link>https://arxiv.org/abs/2506.22149</link>
<guid>https://arxiv.org/abs/2506.22149</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RetFiner通过视觉-语言预训练提升OCT图像分类效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RetFiner的自监督学习视觉-语言优化方法，用于改进现有视网膜OCT基础模型的表示能力。该方法利用文本数据中的丰富监督信号，提升模型在多种复杂任务上的表现。实验结果显示，RetFiner在七个多样化OCT分类任务中分别提升了5.8、3.9和2.1个百分点。研究展示了该方法在无需额外标注的情况下，有效提升模型适应特定人群和应用场景的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.22149" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 27 Jun 2025 07:53:54 GMT</pubDate>
</item>
<item>
<title>XVerse：实现多主体精细控制的文本生成模型</title>
<link>https://arxiv.org/abs/2506.21416</link>
<guid>https://arxiv.org/abs/2506.21416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XVerse提升多主体图像生成的可控性与一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型多主体可控生成模型XVerse，解决文本到图像生成中多主体身份和语义属性（如姿态、风格、光照）精细控制的问题。通过将参考图像转换为针对特定标记的文本流调制偏移量，XVerse实现了对特定主体的精确独立控制，同时不影响图像潜在表示或特征。该方法提升了多主体图像合成的保真度和可编辑性，增强了个性化和复杂场景生成能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 12:04:16 GMT</pubDate>
</item>
<item>
<title>ShotBench与ShotVL：推动电影语言理解的AI基准与模型</title>
<link>https://arxiv.org/abs/2506.21356</link>
<guid>https://arxiv.org/abs/2506.21356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ShotBench和ShotVL，提升AI对电影语言的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ShotBench，一个专为电影语言理解设计的全面基准，包含3.5k专家标注的问答对，覆盖8个关键摄影维度。评估显示现有VLM在理解电影细节方面存在明显不足，最高准确率低于60%。为此，研究构建了70k规模的ShotQA数据集，并基于此训练出ShotVL模型，在ShotBench上取得最佳表现。研究开源了模型、数据和代码，以促进AI在电影理解与生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 11:09:21 GMT</pubDate>
</item>
<item>
<title>DenseDiT：基于生成模型的密集预测方法在真实场景中的应用</title>
<link>https://arxiv.org/abs/2506.20279</link>
<guid>https://arxiv.org/abs/2506.20279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DenseDiT通过生成模型提升真实场景下的密集预测性能。</p><br /><br /><p><strong>摘要：</strong> 密集预测任务在计算机视觉中具有重要意义，旨在为输入图像学习像素级标注。然而，现有方法主要针对理想条件设计，难以适应真实世界场景，且面临真实数据稀缺的问题。为此，研究者提出了DenseWorld基准，涵盖25个密集预测任务，并引入DenseDiT方法，利用生成模型的视觉先验，通过统一策略执行多种真实场景下的密集预测任务。DenseDiT采用参数复用机制和两个轻量分支，仅需不到0.1%的额外参数即可有效集成多尺度上下文信息。实验表明，DenseDiT在DenseWorld上表现优于现有基线，使用不到0.01%的训练数据即可取得优异结果，展现出其在实际部署中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 05:40:50 GMT</pubDate>
</item>
<item>
<title>ARK：一个面向自主机器人的Python优先开源框架</title>
<link>https://arxiv.org/abs/2506.21628</link>
<guid>https://arxiv.org/abs/2506.21628</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARK通过Python统一机器人与AI实践，加速自主机器人研发。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了ARK，一个面向自主机器人的开源Python框架。当前机器人软件栈面临学习曲线陡峭、工具碎片化等问题，而ARK旨在解决这些瓶颈。它提供类似Gym的环境接口，支持数据收集、预处理和模仿学习算法训练，并可无缝切换仿真与真实机器人。ARK采用轻量级客户端-服务器架构，支持C/C++绑定以保证实时性能。框架包含控制、SLAM、运动规划等模块，并兼容ROS。丰富的文档和案例展示了其在快速原型设计和端到端流程方面的优势，有助于降低进入门槛并推动机器人研究与商业化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21628" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 16:23:39 GMT</pubDate>
</item>
<item>
<title>基于噪声一致性的高效可控生成方法NCT</title>
<link>https://arxiv.org/abs/2506.19741</link>
<guid>https://arxiv.org/abs/2506.19741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NCT实现高效可控内容生成，无需重训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为噪声一致性训练（NCT）的新方法，用于在不重新训练基础模型的情况下，将新的控制信号直接整合到预训练的一步生成器中。NCT通过引入适配模块和噪声空间中的噪声一致性损失，使生成模型在不同条件下的行为保持一致，从而实现对新控制条件的适应。该方法具有模块化、数据效率高和易于部署的优点，实验表明其在单次前向传播中即可实现最先进的可控生成效果，优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 11:58:55 GMT</pubDate>
</item>
<item>
<title>BlenderFusion：一种生成式视觉合成框架</title>
<link>https://arxiv.org/abs/2506.17450</link>
<guid>https://arxiv.org/abs/2506.17450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlenderFusion通过重组对象、相机和背景生成新场景。</p><br /><br /><p><strong>摘要：</strong> BlenderFusion是一种生成式视觉合成框架，能够通过重组对象、相机和背景来合成新场景。它采用分层-编辑-合成的流程：首先将视觉输入分割并转换为可编辑的3D实体，然后在Blender中进行3D对齐控制编辑，最后通过生成式合成器将它们融合成连贯场景。该框架扩展了预训练扩散模型，支持同时处理原始和编辑后的场景，并通过源掩码和模拟物体抖动等训练策略提升合成效果。实验表明，BlenderFusion在复杂合成场景编辑任务中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 15:38:34 GMT</pubDate>
</item>
<item>
<title>Gazal-R1：医疗推理领域的高性能语言模型</title>
<link>https://arxiv.org/abs/2506.21594</link>
<guid>https://arxiv.org/abs/2506.21594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gazal-R1在医疗推理任务中表现卓越，提供透明解释。</p><br /><br /><p><strong>摘要：</strong> Gazal-R1是一款基于Qwen3 32B的320亿参数语言模型，在医疗推理任务中表现出色，并能提供清晰、分步的临床决策解释。通过两阶段训练方法，包括监督微调和强化学习，该模型在多个医学基准测试中超越了更大规模的模型。研究还揭示了在专业领域训练推理模型所面临的挑战，如奖励欺骗、训练不稳定性和事实记忆与详细推理之间的权衡。该方法为开发高性能、高效且可解释的领域专用语言模型提供了可复现的框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 05:44:21 GMT</pubDate>
</item>
<item>
<title>Mixture of Grouped Experts 提升大模型推理效率与负载均衡</title>
<link>https://arxiv.org/abs/2505.21411</link>
<guid>https://arxiv.org/abs/2505.21411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoGE优化专家负载，提升大模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Mixture of Grouped Experts (MoGE)，这是一种改进的专家混合架构，通过将专家分组并限制每个组内激活的专家数量，实现更均衡的计算负载。相比传统MoE，MoGE在分布式设备上能显著提升吞吐量，特别是在推理阶段。基于MoGE构建的Pangu Pro MoE模型拥有720亿参数，其中160亿参数在每token中被激活，经过优化后在Ascend NPUs上表现出色，推理速度达1148 tokens/s，经推测加速后可达1528 tokens/s，优于同类模型。实验表明，MoGE在训练和推理中均表现出更高的效率和性价比。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:40:21 GMT</pubDate>
</item>
<item>
<title>LLaVA-Scissor：一种用于视频多模态大语言模型的无训练令牌压缩策略</title>
<link>https://arxiv.org/abs/2506.21862</link>
<guid>https://arxiv.org/abs/2506.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaVA-Scissor通过语义连通组件实现高效视频令牌压缩。</p><br /><br /><p><strong>摘要：</strong> 本文提出LLaVA-Scissor，一种无需训练的视频多模态大语言模型令牌压缩策略。与以往依赖注意力得分的方法不同，该方法利用语义连通组件（SCC）将令牌分配到不同的语义区域，确保全面的语义覆盖。该策略在时空域中应用SCC，有效压缩令牌并以非重叠语义令牌表示整个视频。在多个视频理解基准测试中，LLaVA-Scissor表现出色，尤其在低保留率下表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 22:29:58 GMT</pubDate>
</item>
<item>
<title>SpatialReasoner-R1：提升视觉语言模型空间推理能力的新方法</title>
<link>https://arxiv.org/abs/2506.21656</link>
<guid>https://arxiv.org/abs/2506.21656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialReasoner-R1通过新方法提升空间推理性能。</p><br /><br /><p><strong>摘要：</strong> 当前视觉语言模型在细粒度空间推理方面存在不足，尤其是在多步骤逻辑和精确空间对齐方面。本文提出SpatialReasoner-R1模型，并引入M3CTS方法生成多样且逻辑一致的LongCoT推理轨迹，同时提出fDPO方法，通过空间奖励机制提升描述性定位和逻辑推理能力。实验表明，fDPO在空间质量任务中提升4.1%，在空间数量任务中提升9.0%。SpatialReasoner-R1在SPATIALRGPT-Bench上达到新SOTA，平均准确率超过最强基线9.8%，并在通用视觉语言任务中保持竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 14:00:00 GMT</pubDate>
</item>
<item>
<title>基于人体动作的自我中心视频预测模型PEVA</title>
<link>https://arxiv.org/abs/2506.21552</link>
<guid>https://arxiv.org/abs/2506.21552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PEVA模型通过人体姿态预测第一视角视频，提升环境模拟能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出PEVA模型，旨在根据过去视频和由相对3D人体姿态表示的动作，预测自我中心视频。通过基于身体关节层次结构的运动轨迹进行条件建模，该模型学习如何从第一人称视角模拟人类行为对环境的影响。研究在Nymeria大规模真实世界自我中心视频与人体姿态数据集上训练了一个自回归条件扩散Transformer，并设计了分层评估协议以全面分析模型的具身预测与控制能力。该工作是首次尝试从人类视角出发，建模复杂现实环境和具身代理行为的视频预测方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>大语言模型预训练中的grokking现象与泛化机制研究</title>
<link>https://arxiv.org/abs/2506.21551</link>
<guid>https://arxiv.org/abs/2506.21551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大模型预训练中存在grokking现象，揭示了泛化能力的形成机制。</p><br /><br /><p><strong>摘要：</strong> 本文首次在7B参数的大语言模型OLMoE的一次性预训练过程中观察到grokking现象，即测试性能在训练损失收敛后仍持续提升。研究通过评估多种基准任务（如数学推理、代码生成和常识知识检索）验证了这一现象，并进一步分析了模型内部动态。发现样本路径从随机、实例特定逐渐演变为结构化且可共享，同时路径复杂度降低，表明模型经历了从记忆到泛化的转变。研究还提出了两个新指标，用于量化路径距离和复杂度，能够有效预测下游任务的泛化表现，具有实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>SAM4D：多模态时序基础模型用于相机与激光雷达的可提示分割</title>
<link>https://arxiv.org/abs/2506.21547</link>
<guid>https://arxiv.org/abs/2506.21547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAM4D实现相机与激光雷达的跨模态分割与高效数据标注。</p><br /><br /><p><strong>摘要：</strong> 本文提出SAM4D，一个面向相机和激光雷达流的多模态时序基础模型，支持可提示分割。通过引入统一多模态位置编码（UMPE）将相机和激光雷达特征对齐到共享3D空间，实现跨模态提示与交互。同时，提出运动感知跨模态记忆注意力机制（MCMA），利用自运动补偿提升时间一致性和长时程特征检索能力。为避免标注瓶颈，开发了多模态自动化数据引擎，结合视频掩码生成、时空4D重建和跨模态掩码融合，以极高速度生成高质量伪标签，保留语义信息。实验表明SAM4D在Waymo-4DSeg数据集上展现出强大的跨模态分割能力和数据标注潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>WorldVLA：统一动作与图像理解的自回归世界模型</title>
<link>https://arxiv.org/abs/2506.21539</link>
<guid>https://arxiv.org/abs/2506.21539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WorldVLA通过整合动作与图像理解提升环境预测与行动生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出WorldVLA，一个将动作与图像理解及生成统一的自回归世界模型。该模型结合视觉-语言-动作（VLA）框架与世界模型，通过动作和图像理解预测未来图像，以学习环境物理规律并优化动作生成。同时，动作模型根据图像观测生成后续动作，辅助视觉理解和图像生成。实验表明，WorldVLA优于独立的动作和世界模型，但自回归生成动作序列时性能下降，原因在于模型对动作预测的泛化能力有限，导致误差累积。为此，作者提出注意力掩码策略，在生成当前动作时屏蔽先前动作，显著提升了动作块生成的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:55:40 GMT</pubDate>
</item>
<item>
<title>MADrive：基于记忆增强的自动驾驶场景重建方法</title>
<link>https://arxiv.org/abs/2506.21520</link>
<guid>https://arxiv.org/abs/2506.21520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MADrive通过外部记忆库实现更真实的自动驾驶场景合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出MADrive，一种基于记忆增强的场景重建框架，旨在提升自动驾驶环境的逼真度。该方法利用大规模外部记忆库中的3D资产替换原始观测中的车辆，实现更逼真的场景合成。研究团队发布了包含约7万段360度汽车视频的MAD-Cars数据集，并开发了检索模块，用于在记忆库中找到相似车辆实例，进行3D重建并整合到目标场景中。实验表明，该方法可生成多视角完整车辆表示，支持显著改变或新驾驶场景的高质量合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:41:07 GMT</pubDate>
</item>
<item>
<title>Mind2Web 2：面向智能搜索系统的长期任务基准与评估框架</title>
<link>https://arxiv.org/abs/2506.21506</link>
<guid>https://arxiv.org/abs/2506.21506</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mind2Web 2为智能搜索系统提供长期任务基准与评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mind2Web 2，一个包含130个高质、长周期任务的基准测试集，旨在评估自主网络搜索系统的能力。该基准通过超过1000小时的人工劳动构建，支持实时浏览和信息整合。研究提出了一种新的Agent-as-a-Judge评估框架，利用树状评分标准自动判断答案准确性和来源归属。实验评估了九个前沿系统及人类表现，并分析了错误原因，为未来研究提供了方向。结果显示，OpenAI Deep Research已能实现人类50%-70%的性能，且耗时更少。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21506" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 13:32:50 GMT</pubDate>
</item>
<item>
<title>FairyGen：基于单幅儿童画生成叙事动画视频的系统</title>
<link>https://arxiv.org/abs/2506.21272</link>
<guid>https://arxiv.org/abs/2506.21272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FairyGen从单张儿童画生成风格一致、叙事连贯的动画视频。</p><br /><br /><p><strong>摘要：</strong> FairyGen是一个自动系统，能够从一张儿童的绘画中生成具有叙事性的卡通视频，并忠实保留其艺术风格。该系统通过分离角色建模与风格化背景生成，并引入电影镜头设计来增强表达和连贯性。它首先利用多模态大语言模型生成结构化的分镜脚本，再通过风格传播适配器保持角色视觉风格的一致性。此外，系统还包含镜头设计模块和两阶段运动定制适配器，以提升视频的多样性和电影感。实验表明，FairyGen能够生成风格忠实、叙事自然的动画，具有个性化和互动性强的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:58:16 GMT</pubDate>
</item>
<item>
<title>DiLoCoX：一种用于超大规模模型的低通信去中心化训练框架</title>
<link>https://arxiv.org/abs/2506.21263</link>
<guid>https://arxiv.org/abs/2506.21263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiLoCoX实现107B模型在1Gbps网络上的高效分布式训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出DiLoCoX，一个适用于超大规模语言模型（超过1000亿参数）的低通信去中心化训练框架。该框架结合了流水线并行、双优化器策略、通信与本地训练的一步延迟重叠以及自适应梯度压缩方案，显著提升了训练规模和速度。理论分析验证了通信与训练重叠及梯度压缩的有效性。实验表明，在1Gbps网络下，DiLoCoX可成功预训练107B模型，并比传统AllReduce方法快357倍，同时保持模型收敛性几乎不变。这是首个成功应用于1000亿参数以上模型的去中心化训练框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 09:45:04 GMT</pubDate>
</item>
<item>
<title>动态跳过中间层的Transformer架构优化研究</title>
<link>https://arxiv.org/abs/2506.21103</link>
<guid>https://arxiv.org/abs/2506.21103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态跳过中间层的Transformer架构，提升效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的Transformer架构，通过动态跳过中间层来提高计算效率。该方法基于输入内容决定是否跳过对称的中间块，并引入门控机制防止后续token访问被跳过的token位置。同时采用残差归一化方案和自适应正则化损失控制门控稀疏性。尽管旨在降低简单token的计算需求并促进多层级表征结构，但实验结果显示在所研究规模下，该方法在验证交叉熵与估计FLOPs之间的权衡上未优于较少层数的密集基线模型。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.21103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 26 Jun 2025 05:01:19 GMT</pubDate>
</item>
<item>
<title>PhysRig：基于物理的可微分皮肤绑定与骨骼框架</title>
<link>https://arxiv.org/abs/2506.20936</link>
<guid>https://arxiv.org/abs/2506.20936</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PhysRig，解决传统LBS的变形缺陷，提升动画真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出PhysRig，一种基于物理的可微分皮肤绑定与骨骼框架，旨在克服传统线性混合皮肤（LBS）在模拟软组织、毛发等弹性材料时的局限性。通过将刚性骨骼嵌入体积表示（如四面体网格），并将其作为可变形软体结构进行模拟，PhysRig结合连续力学与粒子离散化方法，实现对材料属性和骨骼运动的可微分建模。同时引入材料原型以降低学习空间复杂度。实验表明，该方法在多个数据集上优于传统LBS方法，生成更真实、物理合理的动画效果，并展示了其在姿态迁移任务中的广泛应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20936" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 21:58:09 GMT</pubDate>
</item>
<item>
<title>基于神经符号的高效图像编辑代理FaSTA^*</title>
<link>https://arxiv.org/abs/2506.20911</link>
<guid>https://arxiv.org/abs/2506.20911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaSTA^*通过结合LLM和A^*搜索实现高效图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FaSTA^*的神经符号代理，用于处理复杂的多轮图像编辑任务。该方法结合了大语言模型（LLMs）的快速高层次子任务规划与每个子任务中的慢速、精确的工具使用和局部A^*搜索，以找到成本高效的工具路径。通过LLMs对以往成功工具路径进行归纳推理，提取并优化常用子程序，并将其作为新工具在未来的任务中重复使用，从而显著降低了相似子任务的探索成本。FaSTA^*首先由LLMs进行快速子任务规划和规则子程序选择，仅在遇到新颖或困难子任务时才激活慢速A^*搜索。实验表明，FaSTA^*在计算效率上优于现有方法，同时保持了与最先进基线相当的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 20:33:43 GMT</pubDate>
</item>
<item>
<title>基于生成块的世界：通过几何抽象交互生成图像场景</title>
<link>https://arxiv.org/abs/2506.20703</link>
<guid>https://arxiv.org/abs/2506.20703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过3D几何体编辑生成高质量图像，提升可编辑性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Generative Blocks World的方法，通过操纵简单的几何抽象来与生成图像的场景进行交互。该方法将场景表示为凸3D基本形状的组合，并允许通过不同数量的形状进行结构或细节的编辑。编辑后的场景通过流模型生成图像，且图像生成条件包括深度信息和纹理提示。该纹理提示考虑了修改后的3D形状，优于现有键值缓存技术的纹理一致性。实验表明，该方法在视觉保真度、可编辑性和组合泛化方面优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态搜索框架MMSearch-R1</title>
<link>https://arxiv.org/abs/2506.20670</link>
<guid>https://arxiv.org/abs/2506.20670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MMSearch-R1实现端到端多模态搜索，提升信息获取效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MMSearch-R1，首个基于强化学习的多模态搜索框架，使大型多模态模型能够在真实网络环境中进行按需、多轮搜索。该框架整合图像和文本搜索工具，并通过基于结果的奖励机制引导模型决策。研究构建了一个多模态搜索VQA数据集，并筛选出搜索平衡子集以优化搜索行为。实验表明，该模型在知识密集型任务中表现优于现有RAG方法，且减少30%以上的搜索调用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>DeepRare：基于大语言模型的罕见病诊断系统</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepRare利用大语言模型实现罕见病精准诊断，准确率达100%。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepRare，一个基于大语言模型的罕见病诊断系统。该系统能够处理多种临床输入，并生成带有透明推理链的诊断假设，提升罕见病的诊断准确性。DeepRare包含三个核心组件：具备长期记忆的中央主机、负责领域特定分析的代理服务器以及集成40多个工具和最新医学知识的模块化设计。在8个数据集上的评估显示，DeepRare在2919种疾病中实现了100%的准确率，HPO评估中Recall@1达到57.18%，显著优于其他方法。此外，系统已上线为网页应用，便于临床使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.20430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 25 Jun 2025 09:42:26 GMT</pubDate>
</item>
<item>
<title>MuseControlLite：轻量级文本到音乐生成模型微调机制</title>
<link>https://arxiv.org/abs/2506.18729</link>
<guid>https://arxiv.org/abs/2506.18729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MuseControlLite提升音乐生成模型的控制精度与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出MuseControlLite，一种轻量级机制，用于微调文本到音乐生成模型，以实现对时间变化音乐属性和参考音频信号的精确控制。研究发现，位置嵌入在时间相关条件中至关重要。实验表明，在解耦交叉注意力层中添加旋转位置嵌入可将控制准确率从56.6%提升至61.1%，且参数量仅为现有方法的1/6.75。该方法在旋律控制、音频修复和扩展等方面表现优于MusicGen-Large和Stable Audio Open ControlNet，仅需85M可训练参数。相关代码、模型和示例已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:08:03 GMT</pubDate>
</item>
<item>
<title>DuaShepherd：融合正确性与潜力的奖励建模框架提升大语言模型数学推理能力</title>
<link>https://arxiv.org/abs/2506.17533</link>
<guid>https://arxiv.org/abs/2506.17533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DuaShepherd通过结合正确性和潜力信号提升LLM数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出DuaShepherd，一种融合正确性与潜力奖励信号的新型奖励建模框架，旨在增强大语言模型（LLM）的数学推理能力。正确性信号关注步骤错误识别，潜力信号则侧重最终答案的可达性。研究构建了包含这两种信号的大规模奖励建模数据集，并采用多头统一架构在多任务设置中同时训练两个奖励模型。通过将两种信号合并为复合概率，模型在多个基准测试中均表现出色，尤其在MATH500和ProcessBench上优于仅使用单一奖励信号的模型，达到当前最优性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:11:01 GMT</pubDate>
</item>
<item>
<title>基于用户偏好的大语言模型路由框架</title>
<link>https://arxiv.org/abs/2506.16655</link>
<guid>https://arxiv.org/abs/2506.16655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种匹配用户偏好的模型路由方法，提升模型选择的灵活性和透明度。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的快速发展，不同模型在性能、风格和成本上各有优势，模型路由成为关键操作技术。然而，现有方法在评估性能时依赖不反映用户主观偏好的基准，并且模型选择范围有限。本文提出一种偏好对齐的路由框架，通过将查询与用户定义的领域或操作类型匹配，实现更灵活和透明的模型选择。我们引入了Arch-Router，一个1.5B参数的小型模型，能够学习将查询映射到对应的领域-操作偏好。该方法支持无缝添加新模型而无需重新训练或修改架构。实验表明，该方法在对话数据集上优于顶级专有模型，能更好地捕捉主观评价标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 19:57:41 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的超启发式框架HeurAgenix在组合优化中的应用</title>
<link>https://arxiv.org/abs/2506.15196</link>
<guid>https://arxiv.org/abs/2506.15196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HeurAgenix通过LLM自动演化和选择启发式算法，提升组合优化效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出HeurAgenix，一个基于大语言模型（LLM）的两阶段超启发式框架，用于解决组合优化问题。该框架首先利用LLM比较种子启发式解与高质量解，提取可复用的演化策略；随后在求解过程中动态选择最合适的启发式方法。为提高灵活性，可以选择高性能LLM或轻量级模型。为应对监督信号不足的问题，采用双奖励机制进行微调，提升在噪声标注下的鲁棒性。实验表明，HeurAgenix在多个基准测试中表现优于现有方法，甚至超越专用求解器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 03:20:01 GMT</pubDate>
</item>
<item>
<title>AnimaX：一种基于视频扩散模型的3D动画生成框架</title>
<link>https://arxiv.org/abs/2506.19851</link>
<guid>https://arxiv.org/abs/2506.19851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnimaX通过视频扩散模型实现跨骨架的3D动画生成。</p><br /><br /><p><strong>摘要：</strong> AnimaX是一种新型的前馈式3D动画框架，能够将视频扩散模型中的运动先验与基于骨骼的动画控制结构相结合。该方法克服了传统运动合成方法在固定骨骼拓扑或高维变形空间优化方面的限制，支持任意骨骼结构的3D网格动画生成。通过多视角、多帧2D姿态图表示3D运动，并结合模板渲染和文本运动提示进行联合视频-姿态扩散，实现了视频先验到运动生成任务的有效迁移。该框架在16万条带绑定序列的数据集上训练，取得了VBench基准测试中在泛化性、运动保真度和效率方面的最佳表现，为无类别限制的3D动画提供了一种可扩展的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ScaleCap：一种可扩展的图像描述生成策略</title>
<link>https://arxiv.org/abs/2506.19848</link>
<guid>https://arxiv.org/abs/2506.19848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ScaleCap，提升图像描述的准确性和平衡性。</p><br /><br /><p><strong>摘要：</strong> 本文提出ScaleCap，一种在推理阶段可扩展的图像描述生成策略，旨在解决大型视觉语言模型（LVLMs）中存在的多模态偏见和语言偏见问题。通过引入启发式问答和对比句评分两个新组件，ScaleCap能够逐步丰富并校准描述内容，提升描述的准确性、平衡性和信息量。实验表明，使用ScaleCap标注45万张图像进行预训练，可在11个基准测试中取得一致的性能提升，并在VQA任务和从描述重建图像的任务中展现出优秀的描述质量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>基于级联视频超分辨率的高效视频生成方法研究</title>
<link>https://arxiv.org/abs/2506.19838</link>
<guid>https://arxiv.org/abs/2506.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效视频生成框架，提升高分辨率输出效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了级联视频超分辨率（VSR）模型在视频生成中的关键设计原则。通过分离语义内容生成与细节合成两个阶段，先使用低分辨率基础模型生成内容，再通过轻量级VSR模型提升分辨率。作者提出了两种退化策略以增强训练数据的匹配性，并分析了时间步采样和噪声增强对低分辨率输入的影响。此外，引入了交错时间单元和稀疏局部注意力机制，显著降低了计算开销。实验表明，该框架优于现有方法，为高效级联视频生成提供了有效基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:57:26 GMT</pubDate>
</item>
<item>
<title>基于知识增强的强化学习缓解大语言模型幻觉问题</title>
<link>https://arxiv.org/abs/2506.19807</link>
<guid>https://arxiv.org/abs/2506.19807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KnowRL通过事实奖励提升模型推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）在推理过程中出现的严重幻觉问题，提出了一种名为KnowRL的知识增强强化学习方法。该方法在强化学习训练中引入基于知识验证的事实性奖励，引导模型进行以事实为基础的慢思考，从而帮助模型识别其知识边界。实验结果表明，KnowRL有效减少了慢思考模型的幻觉现象，同时保持了其原有的强推理能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:17:17 GMT</pubDate>
</item>
<item>
<title>提升开源大语言模型数据分析能力的研究</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究如何增强开源大语言模型的数据分析能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何提升开源大语言模型在数据密集型任务中的表现。通过构建多样化的现实场景数据集，从数据理解、代码生成和战略规划三个维度评估模型性能，发现战略规划质量、交互设计与任务复杂度以及数据质量是影响模型表现的关键因素。基于这些发现，作者提出了一种数据合成方法，显著提升了开源大语言模型的分析推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19794" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 13:04:23 GMT</pubDate>
</item>
<item>
<title>SRFT：统一SFT与RL的单阶段语言模型微调方法</title>
<link>https://arxiv.org/abs/2506.19767</link>
<guid>https://arxiv.org/abs/2506.19767</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SRFT提升数学推理任务性能，优于传统两阶段方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了监督微调（SFT）和强化学习（RL）在大型语言模型中的整合问题。通过分析令牌分布、学习动态和熵指标，发现SFT带来全局性的策略变化，而RL则进行细粒度优化。基于此，作者提出了一种单阶段的监督强化微调方法（SRFT），结合SFT与RL的优势，直接利用演示数据和自探索轨迹进行优化。实验表明，SRFT在五项数学推理基准上平均准确率达到59.1%，比零RL方法高出9.0%，在三项分布外基准上高出10.9%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19767" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 24 Jun 2025 12:31:37 GMT</pubDate>
</item>
<item>
<title>自动化数据集构建提升LLM在软件工程任务中的表现</title>
<link>https://arxiv.org/abs/2506.19290</link>
<guid>https://arxiv.org/abs/2506.19290</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">自动化数据集提升LLM在SWE任务中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种自动化的软件工程（SWE）数据集构建流程，旨在解决传统手动标注和环境设置耗时的问题。该数据集包含来自2531个GitHub仓库的10,169个真实Python任务实例，并附带自然语言描述和运行环境镜像。通过在这些数据上微调Skywork-SWE模型，研究发现模型性能随着数据量增加而持续提升，未出现饱和现象。在SWE-bench Verified基准测试中，Skywork-SWE模型达到38.0%的pass@1准确率，成为Qwen2.5-Coder-32B模型中的新SOTA。结合测试时缩放技术后，准确率进一步提升至47.0%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.19290" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 23:53:36 GMT</pubDate>
</item>
<item>
<title>统一音频表示学习方法USAD在多类型音频任务中表现优异</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">USAD实现语音与音频的统一表示学习，性能接近最先进水平。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为通用语音与音频蒸馏（USAD）的统一音频表示学习方法，能够整合语音、声音和音乐等多种音频类型。USAD通过从特定领域的自监督学习模型中进行高效层间蒸馏，训练一个单一模型以处理多种音频任务。该方法在多个基准测试中表现出色，包括语音处理、音频标记和声音分类等任务，在SUPERB和HEAR基准上取得了接近最先进的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的智能照片修图系统 JarvisArt</title>
<link>https://arxiv.org/abs/2506.17612</link>
<guid>https://arxiv.org/abs/2506.17612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JarvisArt通过AI实现高效、精准的照片修图，提升用户体验。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 JarvisArt，一个基于多模态大语言模型（MLLM）的智能照片修图系统。该系统能够理解用户意图，模拟专业艺术家的思考过程，并协调 Lightroom 中超过 200 种修图工具进行自动化操作。通过两阶段训练方法（Chain-of-Thought 监督微调和 GRPO-R 策略优化），JarvisArt 在全局与局部调整上表现出色，具备出色的泛化能力和精细控制。研究还构建了 MMArt-Bench 基准测试集以评估性能，结果显示 JarvisArt 在内容保真度方面优于 GPT-4o，提升了 60% 的像素级指标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 02:36:00 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型推理一致性的强化学习方法研究</title>
<link>https://arxiv.org/abs/2506.16141</link>
<guid>https://arxiv.org/abs/2506.16141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GRPO-CARE提升多模态模型推理一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型（MLLMs）的推理能力不足问题，提出了一种新的强化学习框架GRPO-CARE。该框架通过引入双层奖励机制，在提升答案正确性的同时增强推理步骤与答案之间的逻辑一致性。研究还构建了SEED-Bench-R1基准，用于评估MLLMs在复杂视频任务中的泛化能力。实验表明，GRPO-CARE在多个挑战性场景中均优于传统GRPO方法，特别是在最困难的评估级别上提升了6.7%，并显著提高了推理一致性。该方法为开发更可解释、更稳健的多模态模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:49:13 GMT</pubDate>
</item>
<item>
<title>代码转换对大语言模型理解能力的影响研究</title>
<link>https://arxiv.org/abs/2506.14012</link>
<guid>https://arxiv.org/abs/2506.14012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">代码转换影响LLM理解，嵌入外语可提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文系统评估了代码转换（CSW）对大型语言模型（LLMs）理解能力的影响。研究表明，在多语言社区和在线内容中，代码转换现象日益普遍，而LLMs在处理此类混合语言文本时表现出不同的性能变化。当外语词汇干扰英语文本时，模型表现有所下降，但在将英语嵌入其他语言的情况下，理解能力反而提高。尽管提示方法效果不一，但微调策略在缓解性能下降方面更为稳定有效。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 17:19:27 GMT</pubDate>
</item>
<item>
<title>4D-LRM：大规模时空重建模型实现任意视角与时间的高质量渲染</title>
<link>https://arxiv.org/abs/2506.18890</link>
<guid>https://arxiv.org/abs/2506.18890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4D-LRM实现从少量视角到任意视角和时间的高质量4D重建。</p><br /><br /><p><strong>摘要：</strong> 本文提出4D-LRM，这是首个大规模4D重建模型，能够从非受限视角和时间戳输入中学习统一的时空表示，并直接预测每像素的4D高斯基元，从而实现高速、高质量的任意视角-时间组合渲染。相比传统方法，4D-LRM在效率、泛化性和真实性方面表现更优。实验表明，该模型能泛化到新物体、跨时间插值，并适应多种相机设置，在单块A100 GPU上可于1.5秒内完成24帧序列的重建。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:57:47 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态大语言模型个性化图像描述方法</title>
<link>https://arxiv.org/abs/2506.18369</link>
<guid>https://arxiv.org/abs/2506.18369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种强化学习框架提升MLLM个性化图像描述能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型在生成个性化图像描述时的不足，提出了一种基于强化学习的后训练框架。尽管现有方法通过监督微调已取得一定进展，但在复杂场景下仍存在描述不准确的问题。由于高质量标注数据获取困难，作者采用强化学习策略，有效提升了模型的视觉识别和个性化生成能力，并在多概念图像描述任务中表现优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:55:52 GMT</pubDate>
</item>
<item>
<title>基于LLM代理的复杂规格到RTL代码生成系统</title>
<link>https://arxiv.org/abs/2506.13905</link>
<guid>https://arxiv.org/abs/2506.13905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Spec2RTL-Agent，实现从复杂规格自动生成RTL代码。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前LLM在生成硬件RTL代码时存在的实际应用与需求之间的差距，提出了一种名为Spec2RTL-Agent的LLM代理系统。该系统通过多代理协作框架，包括推理理解模块、逐步编码与提示优化模块以及自适应反思模块，直接处理复杂的规格文档并生成对应的RTL代码。与传统方法不同，该系统先生成可综合的C++代码再进行HLS优化，提高了代码的正确性和兼容性。实验表明，该系统在三个规格文档上的测试中，减少了75%的人工干预，成为首个完全自动化的从非结构化规格生成RTL代码的多代理系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 14:33:25 GMT</pubDate>
</item>
<item>
<title>基于两阶段优化的长视频照明编辑方法TC-Light</title>
<link>https://arxiv.org/abs/2506.18904</link>
<guid>https://arxiv.org/abs/2506.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TC-Light方法，实现高效且时间一致的视频照明编辑。</p><br /><br /><p><strong>摘要：</strong> 本文针对长视频中复杂动态场景的照明编辑问题，提出了一种名为TC-Light的新方法。该方法采用两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照，第二阶段优化提出的唯一视频张量（UVT）以对齐细粒度纹理和光照。为全面评估性能，研究者还构建了一个长而高度动态的视频基准数据集。实验结果表明，该方法在物理合理性和时间一致性方面表现优异，同时计算成本较低。相关代码和视频演示已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>RealPlay：基于神经网络的实时交互视频生成引擎</title>
<link>https://arxiv.org/abs/2506.18901</link>
<guid>https://arxiv.org/abs/2506.18901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealPlay实现从用户控制信号生成逼真、连贯的视频序列。</p><br /><br /><p><strong>摘要：</strong> RealPlay是一种基于神经网络的实时交互游戏引擎，能够根据用户控制信号生成逼真且时间一致的视频。与以往专注于游戏风格视觉效果的研究不同，RealPlay旨在模拟真实世界画面。它通过用户观察场景、发出控制指令、接收视频片段的交互循环工作。为实现高质量生成，研究解决了低延迟反馈、时间一致性及准确控制响应等挑战。训练数据包括标注的游戏数据和未标注的真实视频，无需真实动作标注。实验显示其具备控制迁移和实体迁移能力，可将虚拟控制应用于现实场景，并控制多种实体。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多面板故事可视化中的协作多智能体框架</title>
<link>https://arxiv.org/abs/2506.18900</link>
<guid>https://arxiv.org/abs/2506.18900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出协作多智能体框架提升故事视觉一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对故事可视化中多面板场景的视觉一致性问题，提出了一种协作多智能体框架。该框架能够自动识别、修正并优化多面板故事中的不一致之处，通过迭代循环实现细粒度的面板级更新，而无需重新生成整个序列。该方法与多种扩散模型兼容，包括Flux和Stable Diffusion等。实验结果表明，该方法在多面板一致性方面优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:29 GMT</pubDate>
</item>
<item>
<title>基于梯度优化的隐空间激活控制方法提升科学代码生成语言偏向性</title>
<link>https://arxiv.org/abs/2506.18887</link>
<guid>https://arxiv.org/abs/2506.18887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过G-ACT框架提升LLM生成代码时对特定语言的偏好。</p><br /><br /><p><strong>摘要：</strong> 本文研究了通过激活语言模型中的潜在子空间来引导科学代码生成向特定编程语言偏移的可行性。在四种编程语言上评估了五种因果语言模型的基线偏差，发现静态神经元归因方法效果有限。为此，作者提出了一种基于梯度优化的自适应激活控制框架（G-ACT），通过聚类每提示的激活差异并在线训练轻量级探测器，实现更有效的语言控制。实验表明，在LLaMA-3.2 3B模型中，该方法提升了15%的分类准确率，早期层甚至提高了61.5%。对于更大的LLaMA-3.3 70B模型，关键层注入仍有效。尽管引入轻微推理开销，但仅对部分层进行控制仍具实用性，展示了可扩展、可解释且高效的语义级控制机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:56:34 GMT</pubDate>
</item>
<item>
<title>3D Arena平台：基于人类偏好的生成式3D模型评估</title>
<link>https://arxiv.org/abs/2506.18787</link>
<guid>https://arxiv.org/abs/2506.18787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">3D Arena通过大规模用户偏好数据评估生成式3D模型质量。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了3D Arena平台，这是一个用于评估图像到3D生成模型的开放平台，通过大规模的人类偏好收集进行评估。自2024年6月推出以来，已收集超过12万次投票，涵盖19个最先进的模型。平台提供了iso3d数据集，并通过统计欺诈检测确保用户真实性。ELO排名系统为模型评估提供了可靠依据。分析显示，用户更偏好视觉呈现特征，Gaussian splat输出比网格模型有16.6 ELO优势，带纹理模型比无纹理模型高144.1 ELO。研究提出了多标准评估、任务导向评估和格式感知比较等改进建议，推动了生成式3D领域的人类中心评估发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 11:57:10 GMT</pubDate>
</item>
<item>
<title>DIP：一种用于提升密集图像表示的无监督后训练方法</title>
<link>https://arxiv.org/abs/2506.18463</link>
<guid>https://arxiv.org/abs/2506.18463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DIP通过伪上下文任务提升视觉编码器的密集表示性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DIP的新型无监督后训练方法，旨在提升大规模预训练视觉编码器在上下文场景理解中的密集图像表示能力。与以往依赖复杂自蒸馏架构的方法不同，DIP通过伪任务模拟下游上下文场景进行训练，借鉴了元学习原理。为了在未标注数据上进行后训练，该方法结合预训练扩散模型和视觉编码器自身，自动生成上下文任务。DIP具有简单、无监督和计算高效的特点，仅需单块A100 GPU不到9小时即可完成训练。实验表明，DIP在多种真实场景的上下文理解任务中表现优异，优于初始编码器和现有方法，提供了一种实用有效的密集表示优化方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 06:01:14 GMT</pubDate>
</item>
<item>
<title>基于视觉定位的医学视觉问答方法研究</title>
<link>https://arxiv.org/abs/2506.17939</link>
<guid>https://arxiv.org/abs/2506.17939</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ThinkVG数据集提升医学问答模型的可解释性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过改进医学视觉问答模型的可解释性和可靠性，以支持临床决策。研究提出了一个名为ThinkVG的数据集，将答案生成过程分解为具有视觉定位的中间推理步骤，从而提供更细粒度的解释。同时引入了一种可验证的奖励机制，用于强化学习训练，提高模型推理过程与最终答案的一致性。实验表明，该方法仅需1/8的训练数据即可达到相当性能，展示了其高效性与有效性。数据集已公开在Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17939" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 04:09:58 GMT</pubDate>
</item>
<item>
<title>大语言模型输出稳定性的概率集中现象研究</title>
<link>https://arxiv.org/abs/2506.17871</link>
<guid>https://arxiv.org/abs/2506.17871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了对齐模型输出稳定性与概率集中有关。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了对齐大型语言模型（LLMs）输出缺乏多样性的原因，通过分析输出分布的概率集中现象，引入了分支因子（BF）作为衡量生成过程中可能下一步数量的指标。研究发现，随着生成过程推进，BF通常下降，表明模型变得更可预测。对齐调优显著减少了BF，使输出更稳定。此外，长推理链（CoT）模型通过进入低BF阶段实现更稳定的输出。研究认为对齐并未改变模型行为，而是引导其使用风格化词汇，从而减少熵值。实验表明，基础模型也可通过提示此类词汇降低BF。该研究为理解并控制LLM输出提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 22:00:37 GMT</pubDate>
</item>
<item>
<title>多文化音乐基础模型CultureMERT-95M提升跨文化音乐表示学习</title>
<link>https://arxiv.org/abs/2506.17818</link>
<guid>https://arxiv.org/abs/2506.17818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多文化音乐模型CultureMERT-95M提升非西方音乐分类性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了多文化适应的音乐基础模型CultureMERT-95M，旨在增强跨文化音乐表示学习。通过两阶段持续预训练策略，在650小时多文化音乐数据上训练，显著提升了非西方音乐自动标记任务的性能，平均ROC-AUC和AP提高4.9%。同时，研究还探索了任务算术方法，与多文化训练模型效果相当。实验表明，单文化模型在不同音乐传统中迁移效果不一，而多文化模型表现最佳。为促进世界音乐表示学习研究，模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 17:16:39 GMT</pubDate>
</item>
<item>
<title>TPTT框架提升大语言模型效率与准确性</title>
<link>https://arxiv.org/abs/2506.17671</link>
<guid>https://arxiv.org/abs/2506.17671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPTT通过线性注意力机制提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了TPTT（Transforming Pretrained Transformer into Titans）框架，该框架通过引入高效的线性化注意力机制和先进的内存管理技术，如Memory as Gate (MaG) 和混合线性化注意力 (LiZA)，提升了预训练Transformer模型的性能。TPTT兼容Hugging Face Transformers库，支持通过LoRA参数高效微调，无需完全重训练。实验结果显示，在MMLU基准测试中，Titan-Llama-3.2-1B模型在准确率上提升了20%。统计分析和对比表明，TPTT具有良好的可扩展性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:06:07 GMT</pubDate>
</item>
<item>
<title>基于前馈架构的4D视频与3D高斯粒子联合生成框架</title>
<link>https://arxiv.org/abs/2506.18839</link>
<guid>https://arxiv.org/abs/2506.18839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个4D视频与3D高斯粒子联合生成框架，提升视觉质量与重建能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的框架，能够使用前馈架构在每个时间步计算视频帧和3D高斯粒子的4D时空网格。该框架包含两个主要部分：4D视频模型和4D重建模型。第一部分分析了现有的4D视频扩散架构，并指出其局限性，提出了一种融合架构，在单一层中同时进行空间和时间注意力计算。关键在于稀疏注意力模式，使得标记在相同帧、同一时间戳或同一视角内进行交互。第二部分通过引入高斯头、相机标记替换算法以及额外的动态层和训练方法，扩展了现有的3D重建算法。整体上，该方法在4D生成任务中达到了新的技术水平，显著提升了视觉质量和重建能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 19:44:59 GMT</pubDate>
</item>
<item>
<title>视觉质量对多模态大语言模型性能的影响及优化方法</title>
<link>https://arxiv.org/abs/2506.15645</link>
<guid>https://arxiv.org/abs/2506.15645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">图像质量不直接影响MLLM表现，VQ-TTT提升模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了图像视觉质量对多模态大语言模型（MLLM）性能的影响，发现图像质量与模型表现之间存在视觉质量悖论，即某些情况下偏离人类感知的图像反而能提升模型性能。为解决这一问题，作者提出VQ-TTT方法，在不依赖外部模型或数据的情况下，通过轻量级适配模块动态调整输入图像，显著提升了多个基准测试中的准确率。该研究重新定义了MLLM所需的视觉输入标准，强调适应性图像的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 13:14:07 GMT</pubDate>
</item>
<item>
<title>基于Surfel索引视图记忆的视频生成方法</title>
<link>https://arxiv.org/abs/2506.18903</link>
<guid>https://arxiv.org/abs/2506.18903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型记忆机制提升视频生成环境探索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的记忆机制，用于构建能够交互式探索环境的视频生成器。现有方法要么通过扩展2D视图重建3D几何导致误差累积，要么依赖短上下文窗口难以维持场景连贯性。为此，作者引入了Surfel-Indexed View Memory (VMem)，通过基于3D表面元素（surfels）对过去视图进行几何索引，实现高效检索相关视图。该方法在生成新视图时仅关注关键视图，从而以较低计算成本保持场景一致性与相机控制能力。实验表明，在长时场景合成基准测试中，该方法优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于统一离散语义表示的多模态框架Tar</title>
<link>https://arxiv.org/abs/2506.18898</link>
<guid>https://arxiv.org/abs/2506.18898</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Tar通过共享语义空间实现视觉与文本的统一理解和生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种多模态框架Tar，旨在通过共享的离散语义表示统一视觉理解和生成。核心是Text-Aligned Tokenizer (TA-Tok)，它利用大型语言模型（LLM）词汇的文本对齐代码本将图像转换为离散标记。Tar通过扩展词汇表将视觉和文本整合到统一空间中，支持跨模态输入和输出，无需特定模态设计。研究还引入了适应规模的编码和解码方法，以及生成性解码器以提高视觉输出质量。为了满足不同的解码需求，采用了两种互补的解码器：快速自回归模型和基于扩散的模型。实验表明，Tar在多个基准测试中表现优异，收敛更快、训练效率更高。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18898" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>ReasonFlux-PRM：一种新型轨迹感知的奖励模型框架</title>
<link>https://arxiv.org/abs/2506.18896</link>
<guid>https://arxiv.org/abs/2506.18896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonFlux-PRM提升大模型推理轨迹评估效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型轨迹感知的奖励模型（ReasonFlux-PRM），用于更精准地评估大语言模型在推理过程中的中间步骤。与以往仅依赖最终输出的奖励模型不同，ReasonFlux-PRM结合了步骤级和轨迹级监督，能够对结构化思维链数据进行细粒度奖励分配。该模型支持离线和在线两种设置，适用于模型蒸馏、强化学习和测试时的缩放优化。实验结果表明，ReasonFlux-PRM-7B在多个基准任务中表现优于现有强基线模型，并实现了显著的性能提升。此外，作者还发布了轻量版ReasonFlux-PRM-1.5B，适用于资源受限场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>通用光度立体技术中的光照与表面法线耦合问题研究</title>
<link>https://arxiv.org/abs/2506.18882</link>
<guid>https://arxiv.org/abs/2506.18882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">解决光照与表面法线耦合难题，提升复杂表面几何细节恢复质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通用光度立体（PS）技术在任意光照条件下恢复高质量表面法线所面临的两大挑战。首先，光照变化与表面法线特征之间存在深度耦合，导致观察到的亮度变化难以区分是由于光照变化还是表面朝向变化。其次，复杂表面上的高频几何细节难以保留，自阴影、多次反射和细微法线变化使传统特征处理方法难以准确捕捉。文章分析了这些问题，并指出其对现有方法如SDM-UniPS和Uni MS-PS的限制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:53:11 GMT</pubDate>
</item>
<item>
<title>基于Commutative Vector Quantization的长上下文大语言模型优化方法</title>
<link>https://arxiv.org/abs/2506.18879</link>
<guid>https://arxiv.org/abs/2506.18879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Commutative Vector Quantization技术，显著降低KV缓存内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型在处理长上下文时KV缓存导致的内存瓶颈问题，提出了一种名为Commutative Vector Quantization (CommVQ) 的量化方法。该方法通过轻量级编码器和码本对KV缓存进行加法量化压缩，并利用矩阵乘法解码。为降低解码计算成本，设计了与RoPE兼容的码本，并通过EM算法训练。实验表明，该方法在保持高精度的同时，将FP16 KV缓存大小减少了87.5%，并支持1位量化，使LLaMA-3.1 8B模型在单块RTX 4090 GPU上实现128K上下文长度的推理。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:50:11 GMT</pubDate>
</item>
<item>
<title>OmniGen2：一种多功能生成模型的开源实现</title>
<link>https://arxiv.org/abs/2506.18871</link>
<guid>https://arxiv.org/abs/2506.18871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGen2是一个多功能生成模型，支持文本到图像等多种任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniGen2，一个开源的多功能生成模型，旨在为多种生成任务提供统一解决方案，包括文本到图像、图像编辑和上下文生成。与前代版本相比，OmniGen2采用独立的解码路径和非共享参数设计，提升了生成效果并保留了原始文本生成能力。文章还描述了用于训练OmniGen2的数据构建流程、针对图像生成的反射机制以及新提出的OmniContext基准测试。尽管参数规模较小，OmniGen2在多个任务上表现优异，并已公开模型、代码和数据集以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:38:54 GMT</pubDate>
</item>
<item>
<title>Phantom-Data数据集提升文本到视频生成的准确性</title>
<link>https://arxiv.org/abs/2506.18851</link>
<guid>https://arxiv.org/abs/2506.18851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phantom-Data提升文本到视频生成的一致性与准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本到视频生成取得了显著进展，但现有模型在忠实遵循文本指令方面仍面临挑战，尤其是‘复制粘贴问题’。该问题源于传统的成对训练方式，导致主体身份与背景属性混淆。为解决此问题，研究者提出了Phantom-Data，这是首个通用的跨对主体到视频一致性数据集，包含约一百万对身份一致的数据。该数据集通过三阶段流程构建：主体检测、跨上下文主体检索以及先验引导的身份验证。实验表明，使用Phantom-Data训练可显著提升提示对齐度和视觉质量，同时保持身份一致性与传统方法相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 13:11:56 GMT</pubDate>
</item>
<item>
<title>无需合成数据的超长文本生成方法研究</title>
<link>https://arxiv.org/abs/2506.18841</link>
<guid>https://arxiv.org/abs/2506.18841</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">基于强化学习的超长文本生成方法超越传统微调技术。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种无需依赖合成数据的超长文本生成方法，通过强化学习训练模型，提升其生成长文本的质量和结构控制能力。实验表明，该方法在多个基准测试中表现优异，优于传统的监督微调方法，并且在性能上超越了更大规模的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18841" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:59:02 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的动态新视角合成方法ViDAR</title>
<link>https://arxiv.org/abs/2506.18792</link>
<guid>https://arxiv.org/abs/2506.18792</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViDAR利用扩散模型生成多视角监督信号，提升动态场景的视图合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出ViDAR，一种基于扩散模型的4D重建框架，用于从单目视频中生成高质量动态新视角。通过个性化扩散模型生成伪多视角监督信号，结合场景特定特征，ViDAR能够恢复精细外观细节并减少单目模糊带来的伪影。为解决扩散监督的时空不一致性，引入扩散感知损失函数和相机位姿优化策略，使合成视图与场景几何对齐。在极端视角变化的DyCheck基准上，ViDAR在视觉质量和几何一致性方面均优于现有方法，并在动态区域表现显著提升。项目页面提供更多信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18792" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 12:01:15 GMT</pubDate>
</item>
<item>
<title>ReDit：通过奖励抖动提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2506.18631</link>
<guid>https://arxiv.org/abs/2506.18631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReDit通过添加随机噪声改善离散奖励，提升模型训练效率和性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReDit的方法，用于解决大语言模型在使用离散奖励时出现的梯度异常、优化不稳定和收敛缓慢问题。ReDit通过向离散奖励信号中添加简单随机噪声，使训练过程中持续提供探索性梯度，从而实现更平滑的梯度更新和更快的收敛速度。实验表明，ReDit在多种任务中表现优异，仅需约10%的训练步数即可达到与传统GRPO相当的性能，并在相似训练时间内仍能提升4%的性能。可视化结果和理论分析进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 09:36:24 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的多视角图像生成方法</title>
<link>https://arxiv.org/abs/2506.18527</link>
<guid>https://arxiv.org/abs/2506.18527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MV-AR方法，实现多视角图像一致性生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D内容创作中多视角图像生成的挑战，提出了一种基于自回归模型的多视角图像生成方法（MV-AR）。该方法通过自回归模型逐步生成一致的多视角图像，并利用前序视图提取有效参考信息。为适应多种提示条件，设计了统一模型架构并引入条件注入模块，支持文本、相机姿态、图像和形状等多种输入。采用渐进式训练策略提升模型性能，并通过“Shuffle View”数据增强技术缓解数据不足问题。实验表明，MV-AR在多种条件下均能生成高质量且一致的多视角图像，性能与主流扩散模型相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 07:28:37 GMT</pubDate>
</item>
<item>
<title>SlimMoE：高效压缩大型Mixture of Experts模型的方法</title>
<link>https://arxiv.org/abs/2506.18349</link>
<guid>https://arxiv.org/abs/2506.18349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SlimMoE通过分阶段压缩提升MoE模型效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SlimMoE的多阶段压缩框架，用于将大型Mixture of Experts (MoE)模型转换为更小、高效的版本，而无需从头开始训练。该方法通过逐步减少参数数量并利用中间阶段的知识迁移，有效缓解了单次剪枝导致的性能下降问题。使用该框架，作者将Phi 3.5-MoE模型压缩为Phi-mini-MoE和Phi-tiny-MoE，分别仅需400B个token进行训练，且可在单块GPU上微调，适用于资源受限环境。实验表明，这些压缩模型在性能上优于同类模型，并与更大的模型相当。研究证明，结构化剪枝结合分阶段蒸馏是构建高质量紧凑MoE模型的有效路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 03:15:59 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的自适应用户画像框架LettinGo</title>
<link>https://arxiv.org/abs/2506.18309</link>
<guid>https://arxiv.org/abs/2506.18309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LettinGo提升推荐系统的准确性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 本文提出LettinGo，一种基于大语言模型的自适应用户画像生成框架。传统嵌入式画像缺乏可解释性，而现有方法受限于固定格式。LettinGo通过多阶段流程，结合直接偏好优化（DPO）和下游任务反馈，生成多样且适应性强的用户画像，显著提升推荐系统的准确性、灵活性和上下文感知能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 23 Jun 2025 01:51:52 GMT</pubDate>
</item>
<item>
<title>无需验证器的强化学习框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.18254</link>
<guid>https://arxiv.org/abs/2506.18254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RLPR通过LLM自身概率评分实现无验证器强化学习，提升推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出RLPR，一种无需依赖领域特定验证器的强化学习框架，利用大语言模型（LLM）生成答案时的token概率作为奖励信号，从而提升模型在多个通用领域和数学领域的推理能力。研究发现，处理概率奖励的高方差是关键，因此引入prob-to-reward和稳定化方法以确保奖励的准确性与稳定性。实验表明，RLPR在四个通用领域和三个数学基准测试中均表现优异，显著优于现有方法如VeriFree和General-Reasoner。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.18254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 22 Jun 2025 22:56:36 GMT</pubDate>
</item>
<item>
<title>FaithfulSAE提升稀疏自编码器的稳定性与模型内部特征捕捉能力</title>
<link>https://arxiv.org/abs/2506.17673</link>
<guid>https://arxiv.org/abs/2506.17673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FaithfulSAE通过使用模型自身数据提升SAE稳定性与准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出FaithfulSAE方法，旨在解决稀疏自编码器（SAEs）在不同初始化种子下不稳定以及无法准确捕捉模型内部特征的问题。传统SAEs通常在外部数据集上训练，可能引入分布外（OOD）数据，导致生成虚假特征。FaithfulSAE则使用模型自身生成的数据进行训练，实验表明其在多个模型中表现出更高的稳定性，并在SAE探测任务中优于基于网络数据的SAEs，同时降低了虚假特征比例。该方法减少了对外部数据集的依赖，提升了模型解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Jun 2025 06:18:25 GMT</pubDate>
</item>
<item>
<title>ConsumerBench：评估端侧GenAI系统效率的基准框架</title>
<link>https://arxiv.org/abs/2506.17538</link>
<guid>https://arxiv.org/abs/2506.17538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ConsumerBench评估端侧GenAI模型的效率与响应时间。</p><br /><br /><p><strong>摘要：</strong> 本文提出ConsumerBench，一个用于评估端用户设备上生成式AI（GenAI）模型系统效率和响应时间的基准框架。不同于传统基准假设模型独占GPU资源，ConsumerBench模拟了真实多应用并发运行的场景，支持自定义工作流以模拟复杂任务。该框架收集应用级指标（如延迟和SLO达成率）和系统级指标（如CPU/GPU利用率和内存带宽）。实验揭示了资源共享低效、贪婪分配下的不公平调度以及静态模型服务器配置的性能问题，并为模型开发者和系统设计者提供了实用建议，包括定制化内核和SLO感知调度策略的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 21:32:22 GMT</pubDate>
</item>
<item>
<title>基于MICS的医学多模态大语言模型推理路径优化</title>
<link>https://arxiv.org/abs/2506.16962</link>
<guid>https://arxiv.org/abs/2506.16962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MICS方法提升医学MLLM推理能力，构建新数据集和模型。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学领域多模态大语言模型（MLLM）推理能力不足的问题，提出一种名为Mentor-Intern Collaborative Search（MICS）的新型推理路径搜索方案。该方法通过导师模型引导推理路径，并由多个实习模型进行验证与优化，最终根据综合表现选择最优路径。为评估推理质量，引入MICS-Score指标。研究还构建了MMRP医学推理数据集和Chiron-o1模型，后者在多项医学视觉问答和推理任务中取得最佳性能。实验表明，基于MICS生成的CoT数据能显著提升模型表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 08:51:19 GMT</pubDate>
</item>
<item>
<title>基于LSTM的新生儿死亡风险预测研究</title>
<link>https://arxiv.org/abs/2506.16929</link>
<guid>https://arxiv.org/abs/2506.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LSTM模型在新生儿死亡预测中表现最佳，准确率达99%。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了如何通过机器学习技术预测新生儿死亡风险，以减少新生儿死亡率。研究使用了140万份历史数据，应用了逻辑回归、K近邻、随机森林、XGBoost、卷积神经网络和LSTM等多种算法。结果显示，XGBoost和随机森林分类器准确率为94%，而LSTM模型准确率最高，达到99%。因此，LSTM被推荐为预测新生儿是否需要采取预防措施的最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 07:44:48 GMT</pubDate>
</item>
<item>
<title>Crome：一种因果鲁棒的奖励建模框架以防止奖励黑客</title>
<link>https://arxiv.org/abs/2506.16507</link>
<guid>https://arxiv.org/abs/2506.16507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Crome通过因果和中性增强提升奖励模型鲁棒性，有效防止奖励黑客。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Crome的新型奖励建模框架，旨在解决大型语言模型在对齐过程中因奖励黑客而导致的偏差问题。Crome基于显式的因果模型，通过两种合成增强方式——因果增强和中性增强，分别强化对因果属性的敏感性和对虚假属性的不变性。这些增强仅通过查询一个权威语言模型来识别因果标准，无需预先了解虚假因素。实验结果显示，Crome在多个基准测试中显著优于传统基线，提升了平均准确率，并在不同任务和设置中表现出一致的性能优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FinCoT：基于领域专家推理的结构化思维链提示方法</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinCoT提升金融问答性能并优化推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出FinCoT，一种结合领域专家金融推理的结构化思维链提示方法。研究对比了FinNLP中的三种提示风格：标准提示、非结构化思维链提示和结构化思维链提示，并发现结构化提示在性能和可解释性上具有优势。FinCoT在十个金融领域的CFA风格问题上表现出色，将准确率从63.2%提升至80.5%，同时显著减少生成的token数量。结果表明，与领域对齐的结构化提示能有效提升模型性能并降低推理成本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 04:18:55 GMT</pubDate>
</item>
<item>
<title>基于CodeT5的LLM代码作者归属识别研究</title>
<link>https://arxiv.org/abs/2506.17323</link>
<guid>https://arxiv.org/abs/2506.17323</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新型模型用于识别AI生成的C语言代码来源。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型生成的代码日益增多，识别其来源变得尤为重要。本文提出了CodeT5-Authorship模型，该模型仅使用CodeT5的编码器部分进行分类任务。通过引入LLM-AuthorBench基准测试集，评估了模型在区分不同LLM生成代码方面的性能。实验结果显示，该模型在二分类和多分类任务中均表现出色，准确率分别达到97.56%和95.40%。研究还提供了开源代码和数据集以支持开放科学。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17323" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 15:49:41 GMT</pubDate>
</item>
<item>
<title>Agentic AI研究中的标准化与评估协议改进</title>
<link>https://arxiv.org/abs/2506.15741</link>
<guid>https://arxiv.org/abs/2506.15741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究指出Agentic AI缺乏标准评估，提出新框架OAgents提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前Agentic AI研究中缺乏标准化和科学严谨性的问题，导致方法间难以公平比较。通过在GAIA基准和BrowseComp上的系统实证研究，发现以往工作因评估协议不统一而难以复现。为此，作者提出了更稳健的评估协议，并基于研究结果开发了OAgents框架，该框架在开源项目中表现优异，具备模块化设计以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>LLM jailbreak攻击防护机制的系统性分析</title>
<link>https://arxiv.org/abs/2506.10597</link>
<guid>https://arxiv.org/abs/2506.10597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次系统分析LLM的jailbreak防护机制，提出多维分类和评估框架。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了大型语言模型（LLMs）在部署过程中暴露的关键漏洞，尤其是jailbreak攻击对安全机制的突破。为应对这一问题，研究提出了外部防御机制——guardrails，并指出当前该领域缺乏统一的分类和评估体系。本文作为知识系统化（SoK）论文，首次对LLM的jailbreak防护机制进行了全面分析，提出了一种六维分类体系，并设计了一个结合安全性、效率与实用性的评估框架。通过实验与分析，研究揭示了现有防护方法的优势与局限性，探讨了其在不同攻击类型中的适用性，并为未来优化防护组合提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:42:40 GMT</pubDate>
</item>
<item>
<title>基于隐式视觉标记的多模态推理框架 Mirage</title>
<link>https://arxiv.org/abs/2506.17218</link>
<guid>https://arxiv.org/abs/2506.17218</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mirage框架提升多模态推理能力，无需生成显式图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种名为 Mirage 的多模态推理框架，旨在提升视觉语言模型（VLMs）在不需要生成显式图像的情况下进行视觉推理的能力。受人类通过心理意象进行推理的启发，Mirage 在文本解码过程中引入隐式视觉标记，使模型能够在不生成像素级图像的情况下继续多模态推理流程。该方法首先通过从真实图像嵌入中蒸馏监督隐式标记，随后切换为纯文本监督以对齐任务目标，并通过强化学习进一步增强多模态推理能力。实验结果表明，Mirage 在多个基准测试中表现出更强的多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17218" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>MEXA：一种无需训练的多模态推理框架</title>
<link>https://arxiv.org/abs/2506.17113</link>
<guid>https://arxiv.org/abs/2506.17113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MEXA通过聚合专家模型实现跨领域的多模态推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MEXA的训练-free 框架，用于在不同领域中进行多模态推理。MEXA能够根据输入模态和任务需求动态选择专家模型，并利用大型推理模型对这些模型的输出进行整合与推理，从而生成最终答案。该方法无需额外训练，具有模块化设计，适用于视频推理、音频推理、3D理解及医疗问答等多种任务。实验表明，MEXA在多个多模态基准测试中均优于现有方法，展示了其在多模态推理中的有效性与广泛适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 12:14:13 GMT</pubDate>
</item>
<item>
<title>基于对数概率序列的提示逆向方法研究</title>
<link>https://arxiv.org/abs/2506.17090</link>
<guid>https://arxiv.org/abs/2506.17090</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PILS方法，提升隐藏提示恢复准确率。</p><br /><br /><p><strong>摘要：</strong> 本文研究了语言模型逆向问题，旨在通过模型输出恢复隐藏提示。作者提出了一种新方法——从对数概率序列中进行提示逆向（PILS），利用模型在多个生成步骤中的下一个词概率来恢复隐藏提示。该方法基于一个关键洞察：语言模型的向量输出位于低维子空间中，这使得可以无损压缩多步生成的概率分布。实验结果显示，PILS在恢复隐藏提示方面比现有方法有显著提升，恢复率提高了2到3.5倍。此外，该方法在恢复隐藏系统消息任务中也表现出色，并展示了良好的泛化能力。研究还分析了重复内容在提示恢复中的作用，并提出了基于逻辑的跨模型迁移方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17090" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 11:53:51 GMT</pubDate>
</item>
<item>
<title>图像生成模型输出的令牌级水印方法</title>
<link>https://arxiv.org/abs/2506.16349</link>
<guid>https://arxiv.org/abs/2506.16349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种图像生成模型的令牌级水印技术，提升检测可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对自回归图像生成模型输出的令牌级水印方法，这是该领域的首次尝试。研究发现，由于缺乏反向循环一致性（RCC），重新分词会破坏水印。为解决此问题并增强对常见图像变换、神经压缩和移除攻击的鲁棒性，作者引入了定制的分词器-反分词器微调流程和互补的水印同步层。实验表明，该方法在理论上具有可靠的p值，能够实现稳健的水印检测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 10:25:51 GMT</pubDate>
</item>
<item>
<title>Vision-Language-Action模型的泛化能力评估与基准测试</title>
<link>https://arxiv.org/abs/2506.09930</link>
<guid>https://arxiv.org/abs/2506.09930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">评估VLA模型在模拟任务中的泛化能力，揭示其感知与执行差距。</p><br /><br /><p><strong>摘要：</strong> 本文针对Vision-Language-Action (VLA)模型在机器人领域中的泛化能力进行了系统评估。尽管VLA模型基于大型视觉-语言模型（VLM）展现出强大的感知理解和高层规划能力，但在实际动作执行中表现不稳定，尤其在面对分布外观察时。研究引入了一个包含50个模拟任务的统一评估套件，覆盖10个子类别。实验结果表明，微调动作数据可能削弱VLM的通用推理能力。作者开源了任务套件和代码，旨在为未来VLA研究提供标准化基准，推动感知到动作的差距缩小。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:52:18 GMT</pubDate>
</item>
<item>
<title>多语言语音合成中文化敏感情感与口音建模的新方法</title>
<link>https://arxiv.org/abs/2506.16310</link>
<guid>https://arxiv.org/abs/2506.16310</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新TTS系统提升印度语和英语口音及情感准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新型文本转语音（TTS）架构，专门针对印地语和印度英语的口音与情感建模。该系统在Parler-TTS基础上进行了改进，引入了语言特定的音素对齐编码器-解码器结构、基于本地语料库训练的文化敏感情感嵌入层，以及动态口音代码切换机制。实验结果显示，该系统在口音准确率上提升了23.7%，情感识别准确率达85.3%，优于现有基线模型。此外，系统支持实时口音切换，保持情感一致性，用户主观评价得分为4.2/5，显著优于现有系统。该研究为跨语言语音合成提供了可行方案，适用于南亚教育科技和无障碍软件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16310" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 09:35:05 GMT</pubDate>
</item>
<item>
<title>InfGen：一种用于长期交通仿真的统一模型</title>
<link>https://arxiv.org/abs/2506.17213</link>
<guid>https://arxiv.org/abs/2506.17213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfGen实现长期交通仿真，性能优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出InfGen，一种统一的下一标记预测模型，能够同时进行封闭回路运动模拟和场景生成。该模型在短期（9秒）交通仿真中达到最先进水平，并在长期（30秒）仿真中显著优于其他方法。InfGen可以自动切换模拟模式，实现稳定、真实的长期交通仿真，适用于自动驾驶系统部署中的实际场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>UniFork：一种新型的统一图像理解与生成架构</title>
<link>https://arxiv.org/abs/2506.17202</link>
<guid>https://arxiv.org/abs/2506.17202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniFork架构，提升图像理解与生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了统一图像理解与生成模型的架构设计问题，发现理解任务和生成任务在模态对齐模式上存在显著差异。理解任务需要随着网络深度增加逐步增强模态对齐以提升语义理解，而生成任务则在浅层增强对齐、深层减弱以恢复空间细节。这种差异导致传统共享Transformer架构难以兼顾两者。为此，作者提出UniFork架构，采用Y型结构，在浅层共享表示学习，深层引入任务专用分支，有效平衡了共享学习与任务专精，实验表明其性能优于传统架构和任务专用模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:52:31 GMT</pubDate>
</item>
<item>
<title>基于提示的参数生成方法DnD实现高效大语言模型微调</title>
<link>https://arxiv.org/abs/2506.16406</link>
<guid>https://arxiv.org/abs/2506.16406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DnD通过提示生成参数，大幅提升LLM微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Drag-and-Drop LLMs（DnD）的新方法，该方法通过将少量未标记的任务提示直接映射为LoRA权重更新，从而避免了每个下游数据集都需要单独优化的过程。DnD使用轻量级文本编码器将提示批次压缩为条件嵌入，并通过级联超卷积解码器生成完整的LoRA矩阵。训练完成后，DnD可在几秒内生成特定任务的参数，相比全量微调降低了12,000倍的开销，并在多个基准测试中表现出30%的性能提升。此外，DnD在跨领域任务中也展现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 11:38:21 GMT</pubDate>
</item>
<item>
<title>PAROAttention：通过重新排序提升视觉生成中的注意力效率</title>
<link>https://arxiv.org/abs/2506.16054</link>
<guid>https://arxiv.org/abs/2506.16054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PAROAttention优化视觉生成中的注意力机制，提升效率与性能。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成中，注意力机制的二次复杂度导致高内存和计算成本，尤其在高分辨率图像或视频生成时更为显著。为解决此问题，研究者尝试了稀疏化和量化等技术，但效果受限。本文指出，视觉注意力模式的分散性和不规则性是核心难题，因此提出一种新策略——重新组织注意力模式。受视觉特征提取局部聚合特性的启发，设计了Pattern-Aware token ReOrdering (PARO)技术，将多样化的注意力模式统一为硬件友好的块状模式，从而简化并提升稀疏化和量化效果。实验表明，PAROAttention在保持与全精度模型相近性能的同时，显著降低密度和位宽（INT8/INT4），实现1.9x至2.7倍的端到端延迟加速。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 02:25:02 GMT</pubDate>
</item>
<item>
<title>基于多模态模型的文档分块方法提升RAG系统性能</title>
<link>https://arxiv.org/abs/2506.16035</link>
<guid>https://arxiv.org/abs/2506.16035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态文档分块方法提升RAG系统准确性和结构保留能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种基于大型多模态模型（LMMs）的新型多模态文档分块方法，用于处理PDF文档中的复杂结构，如跨页表格、嵌入图表和上下文依赖。该方法通过配置化的页面批次处理方式，保持语义连贯性和结构完整性，并在手动构建的PDF数据集上验证了其有效性，结果显示该方法在分块质量和下游RAG性能方面均有显著提升，相比传统RAG系统具有更高的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 01:11:43 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.1：3D AI生成内容的全面教程</title>
<link>https://arxiv.org/abs/2506.15442</link>
<guid>https://arxiv.org/abs/2506.15442</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.1提供3D生成模型的完整训练与评估指南。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Hunyuan3D 2.1，这是一个用于3D人工智能生成内容（AIGC）的先进系统，旨在帮助用户处理3D数据、训练生成模型并进行性能评估。该系统包含两个核心组件：Hunyuan3D-DiT用于形状生成，Hunyuan3D-Paint用于纹理合成。文章详细讲解了从数据准备、模型架构、训练策略到评估指标和部署的全流程，适合希望在游戏、虚拟现实和工业设计中应用3D生成技术的开发者和研究人员。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15442" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:14:46 GMT</pubDate>
</item>
<item>
<title>InfiniPot-V：突破视频流理解的内存瓶颈</title>
<link>https://arxiv.org/abs/2506.15745</link>
<guid>https://arxiv.org/abs/2506.15745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">InfiniPot-V实现视频流实时处理且不占用过多内存。</p><br /><br /><p><strong>摘要：</strong> InfiniPot-V是一种无需训练、与查询无关的框架，能够在视频流处理中保持固定内存上限。该方法在视频编码过程中监控缓存，当达到用户设定阈值时，通过时间轴冗余度（TaR）和值范数（VaN）对缓存进行轻量级压缩，有效去除冗余信息并保留语义重要信息。实验表明，InfiniPot-V可减少高达94%的GPU峰值内存，同时保持实时生成能力和与全缓存相当的准确性，适用于多种多模态大模型和视频基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 22:22:14 GMT</pubDate>
</item>
<item>
<title>基于多平面同步的3D全景图扩散模型DreamCube</title>
<link>https://arxiv.org/abs/2506.17206</link>
<guid>https://arxiv.org/abs/2506.17206</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多平面同步扩展2D基础模型至全景域，实现高质量3D全景图生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为DreamCube的多平面RGB-D扩散模型，用于3D全景图生成。该模型通过在2D基础模型的操作器上应用多平面同步技术，有效克服了2D图像先验与3D全景图不兼容的问题，从而实现了对全景视觉外观和几何结构的有效建模。实验表明，DreamCube在全景图像生成、全景深度估计及3D场景生成方面均表现出色，显著提升了生成内容的质量和多样性，同时保证了多视角一致性。这项工作为3D全景合成提供了新的思路，有望推动相关领域的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17206" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:55:06 GMT</pubDate>
</item>
<item>
<title>Hunyuan-GameCraft：面向游戏环境的高动态交互视频生成框架</title>
<link>https://arxiv.org/abs/2506.17201</link>
<guid>https://arxiv.org/abs/2506.17201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Hunyuan-GameCraft框架解决现有游戏视频生成方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来扩散模型驱动的可控视频生成技术取得了显著进展，但当前方法在动态表现、通用性、长期一致性及效率方面仍存在不足，限制了其在游戏视频生成中的应用潜力。为弥补这些缺陷，我们引入了Hunyuan-GameCraft，这是一种针对游戏环境中高动态交互视频生成的新框架。该框架通过统一键盘和鼠标输入至共享相机表示空间实现精细动作控制，并采用混合历史条件训练策略以扩展视频序列并保留场景信息。此外，为了提升推理效率与可玩性，框架实现了模型蒸馏，减少计算开销的同时保持长时间序列的一致性。模型基于涵盖超过100款AAA级游戏的百万量级游戏录像数据集进行训练，并在精心标注的合成数据集上微调以增强精度与控制能力。实验表明，Hunyuan-GameCraft在视觉逼真度、真实性及动作控制方面均显著优于现有方法，极大推动了交互式游戏视频生成领域的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.17201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 20 Jun 2025 13:50:37 GMT</pubDate>
</item>
<item>
<title>Hunyuan3D 2.5：高保真3D资产生成的新突破</title>
<link>https://arxiv.org/abs/2506.16504</link>
<guid>https://arxiv.org/abs/2506.16504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hunyuan3D 2.5通过新模型LATTICE和PBR技术提升3D形状与纹理生成质量。</p><br /><br /><p><strong>摘要：</strong> Hunyuan3D 2.5是一款强大的3D扩散模型套件，专注于生成高质量且细节丰富的纹理化3D资产。它沿用了Hunyuan3D 2.0的两阶段流程，但在形状和纹理生成方面取得了显著进步。在形状生成方面，引入了新的基础模型LATTICE，该模型通过扩展高质量数据集、模型规模和计算资源进行训练。最大模型参数达到10B，能够生成清晰、详细的3D形状，并精确匹配图像与3D模型，同时保持网格表面干净平滑，大幅缩小生成形状与手工制作形状之间的差距。在纹理生成方面，通过基于物理的渲染(PBR)技术，借助从Hunyuan3D 2.0 Paint模型扩展的多视图架构实现升级。广泛评估显示，Hunyuan3D 2.5在形状生成和端到端纹理生成方面均显著优于先前方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.16504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 19 Jun 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在无偏视角摘要中的应用与评估</title>
<link>https://arxiv.org/abs/2506.15925</link>
<guid>https://arxiv.org/abs/2506.15925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出改进的评估方法提升视角摘要质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中的政治视角摘要这一重要应用，指出当前评价框架对覆盖度和忠实性等属性的传统度量方法缺乏适用性验证的问题。为填补这一空白，我们首先通过人工注释构建测试集来评估度量指标的可靠性，发现基于语言模型的度量方法优于传统指标。进一步地，我们证明基于重排序的方法效果显著，并且利用合成数据进行偏好微调可进一步提升性能。这些研究成果有助于推动视角摘要方法的可靠评估与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 20:01:43 GMT</pubDate>
</item>
<item>
<title>VIKI-Bench与VIKI-R：多智能体协作新基准与框架</title>
<link>https://arxiv.org/abs/2506.09049</link>
<guid>https://arxiv.org/abs/2506.09049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VIKI-Bench基准和VIKI-R框架，提升具身多智能体视觉推理合作能力。</p><br /><br /><p><strong>摘要：</strong> 在动态环境中协调多个具身代理是人工智能中的核心挑战，涉及感知驱动推理与可扩展合作策略。尽管已有研究利用大型语言模型进行多智能体规划，但基于视觉-语言模型(VLM)的视觉推理探索尚少且支持多样性有限。本文引入VIKI-Bench，首个针对具身多智能体协作设计的分层基准，涵盖代理激活、任务规划和轨迹感知三个层次。VIKI-Bench包含多样化的机器人形态、多视角视觉观测及结构化监督信号。为展示其价值，我们提出VIKI-R框架，通过链式思维标注演示微调预训练VLM，再结合多层级奖励信号强化学习。实验表明，VIKI-R在所有任务层次上显著优于基线方法，并促进异构代理间的组合式合作模式涌现。VIKI-Bench与VIKI-R共同构成推动具身AI系统多智能体视觉驱动合作发展的统一测试平台与方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Show-o2：基于流匹配与自回归建模的多模态统一模型</title>
<link>https://arxiv.org/abs/2506.15564</link>
<guid>https://arxiv.org/abs/2506.15564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合流匹配与自回归建模的多模态统一模型Show-o2。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Show-o2的改进型原生统一多模态模型，该模型通过空间（时空）融合构建统一视觉表示，并利用因果变分自编码器空间实现跨图像和视频模态的扩展。Show-o2在语言头和流头分别应用自回归建模和流匹配技术，用于文本标记预测和图像/视频生成。设计了两阶段训练方法以有效学习并扩展到更大模型，最终模型在多种模态（如文本、图像和视频）的多模态理解和生成任务中表现出色。代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 11:39:15 GMT</pubDate>
</item>
<item>
<title>RE-IMAGINE框架评估大型语言模型的推理能力</title>
<link>https://arxiv.org/abs/2506.15455</link>
<guid>https://arxiv.org/abs/2506.15455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出RE-IMAGINE框架，评估大型语言模型是否依赖统计记忆进行推理。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明大型语言模型（LLMs）在推理基准测试中表现优异，但其推理能力的真实来源尚不明确。本文受因果阶梯理论启发，提出RE-IMAGINE框架，通过生成不同层次的问题变体，系统性地评估LLMs的推理能力。该框架基于中间符号表示生成问题，避免单纯依赖训练集的记忆能力。实验结果显示，在不同推理领域（如数学、代码、逻辑）中，模型性能随问题复杂度提升而下降，表明现有模型对统计记忆存在一定程度的依赖。这一发现为未来研究如何提升LLMs的高层次推理能力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 09:35:47 GMT</pubDate>
</item>
<item>
<title>SonicVerse：融合多任务特征检测的音乐描述生成模型</title>
<link>https://arxiv.org/abs/2506.15154</link>
<guid>https://arxiv.org/abs/2506.15154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合音频特征检测的音乐描述生成模型，提升音乐AI研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SonicVerse的多任务音乐描述生成模型，该模型不仅生成音乐片段的描述，还通过集成关键检测、人声检测等辅助任务捕捉低级声学细节和高级音乐属性。其创新之处在于基于投影的架构，将音频输入转化为语言标记的同时进行特征检测。此框架不仅能为短音乐片段生成丰富的描述，还能通过时间信息链式生成长音乐片段的详细描述。为训练模型，作者扩展了MusicBench数据集，使用MIRFLEX标注音乐特征，最终实验表明，这种方法显著提升了生成描述的质量和细节水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.15154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 18 Jun 2025 01:51:36 GMT</pubDate>
</item>
<item>
<title>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</title>
<link>https://arxiv.org/abs/2506.14965</link>
<guid>https://arxiv.org/abs/2506.14965</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 16:24:00 GMT</pubDate>
</item>
<item>
<title>基于迭代精化的图表到代码生成方法</title>
<link>https://arxiv.org/abs/2506.14837</link>
<guid>https://arxiv.org/abs/2506.14837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于结构化指令的迭代精化方法提升图表到代码生成性能。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）因其强大的视觉理解能力而备受关注，但在图表到代码生成任务上表现欠佳。这项任务不仅需要精确的视觉理解，还需要将视觉元素准确转化为结构化代码。本文提出的{ChartIR}通过区分视觉理解和代码翻译两个子任务，设计描述和差异两种结构化指令，将视觉特征转化为语言表示，从而优化后续代码翻译过程。此外，该方法将整个生成流程分解为初始代码生成和迭代精化两个阶段，逐步提升最终输出质量。实验结果显示，无论是在开源模型Qwen2-VL还是闭源模型GPT-4o上，{ChartIR}均优于其他方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.14837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 10:10:16 GMT</pubDate>
</item>
<item>
<title>EmoNet-Voice：基于新基准的数据集推动语音情感识别发展</title>
<link>https://arxiv.org/abs/2506.09827</link>
<guid>https://arxiv.org/abs/2506.09827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EmoNet-Voice推出包含大规模预训练数据集和专家标注的新基准，提升AI语音情感识别能力。</p><br /><br /><p><strong>摘要：</strong> 随着文本转语音及音频生成模型的进步，评估AI系统情感理解能力的需求愈发迫切。然而，当前的语音情感识别（SER）数据集存在情感粒度不足、隐私问题或依赖演员表演等局限性。本文介绍EmoNet-Voice，一种新的语音情感检测资源，它包括EmoNet-Voice Big（涵盖超过4500小时的跨语言、多情绪语音数据）和EmoNet-Voice Bench（具有人工专家注释的新型基准数据集）。该资源旨在通过精细的情感分类评估SER模型，并利用先进的语音生成技术创建模拟演员表演的合成音频片段，同时由心理学专家验证并标注感知强度。此外，我们提出Empathic Insight Voice模型，在情感识别上达到与人类专家高度一致的新标准。我们的研究发现表明，高唤醒情绪如愤怒比低唤醒状态如专注更容易被检测到。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 11:06:59 GMT</pubDate>
</item>
<item>
<title>基于视觉Transformer的寿命预测模型</title>
<link>https://arxiv.org/abs/2506.13430</link>
<guid>https://arxiv.org/abs/2506.13430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用预训练视觉Transformer模型从图像预测剩余寿命。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，通过利用预训练的视觉Transformer基础模型，结合面部和全身图像估计个体的剩余寿命，并实现了对预测不确定性的量化。研究显示，预测不确定性系统性地随真实剩余寿命变化，且可以通过学习高斯分布来有效建模。该方法在现有数据集上达到了7.48年的平均绝对误差（MAE），并在两个新发布的高质量数据集上进一步优化至4.79年和5.07年。虽然此模型尚未达到临床部署标准，但其结果显示了从图像中提取医学相关信息的潜力。所有代码和数据集均公开，以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:47:37 GMT</pubDate>
</item>
<item>
<title>提升小规模推理语言模型性能的研究</title>
<link>https://arxiv.org/abs/2506.13404</link>
<guid>https://arxiv.org/abs/2506.13404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索训练策略以提高0.5B参数量语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型的发展，尽管这些模型在多种任务上表现出色，但其高昂的计算成本和潜在隐私问题限制了其应用范围。相比之下，具有约0.5亿参数的小型推理语言模型（SRLMs）因其高效性和经济性成为一种有吸引力的选择，尤其是在资源受限的环境中。然而，这类小型模型由于容量限制，在处理复杂任务如数学推理和代码生成时面临挑战。本研究探讨了监督微调（SFT）、知识蒸馏（KD）和强化学习（RL）等不同的训练策略及其组合方法，以缩小小型模型与大型模型之间的性能差距。通过广泛的实验验证和分析，我们提出了优化的训练管道建议，旨在最大化0.5B参数量模型的推理能力，为相关领域的实际应用提供指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 08:18:11 GMT</pubDate>
</item>
<item>
<title>SeqPE：一种统一且完全可学习的位置编码框架</title>
<link>https://arxiv.org/abs/2506.13277</link>
<guid>https://arxiv.org/abs/2506.13277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SeqPE框架，解决传统位置编码在扩展性和多模态适应性上的局限。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeqPE的统一且完全可学习的位置编码框架，该框架通过将每个n维位置索引表示为符号序列，并采用轻量级顺序位置编码器以端到端方式学习嵌入。为了规范SeqPE的嵌入空间，引入了对比目标和知识蒸馏损失两种互补的目标函数。实验表明，SeqPE在语言建模、长上下文问答和二维图像分类等任务上表现优异，特别是在上下文长度外推方面超越了强大的基线模型，并且能够在无需手动架构重设计的情况下无缝泛化到多维输入。此外，作者开源了代码、数据和检查点。关键词：Transformer、位置编码、SeqPE。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 05:16:40 GMT</pubDate>
</item>
<item>
<title>DoTA-RAG：面向大规模网络知识索引的高效检索增强生成系统</title>
<link>https://arxiv.org/abs/2506.12571</link>
<guid>https://arxiv.org/abs/2506.12571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DoTA-RAG通过三阶段管道优化检索增强生成系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DoTA-RAG（动态聚合检索增强生成系统）的新方法，该系统专门针对高吞吐量、大规模网络知识索引进行了优化。传统检索增强生成（RAG）系统在处理海量多样化数据时通常面临高延迟和有限准确性的问题。为解决这些问题，DoTA-RAG采用了三阶段管道策略，包括查询重写、动态路由到专用子索引以及多阶段检索和排名。此外，通过评估并选择更优的嵌入模型并对FineWeb-10BT语料库进行重新嵌入，进一步增强了系统的检索能力。同时，构建了一个包含500个问题的多样化问答数据集，涵盖了广泛的WebOrganizer主题和格式。实验结果显示，DoTA-RAG将答案正确性得分从基线的0.752提高到了1.478，同时保持了低延迟，在Live Challenge Day上达到了0.929的正确性得分。这些成果表明，DoTA-RAG在需要快速可靠访问大型且不断发展的知识源的领域具有实际部署潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 12:56:00 GMT</pubDate>
</item>
<item>
<title>利用大型语言模型评估新闻媒体可信度与政治偏见的研究</title>
<link>https://arxiv.org/abs/2506.12552</link>
<guid>https://arxiv.org/abs/2506.12552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法利用大型语言模型评估新闻媒体的整体可信度与政治倾向。</p><br /><br /><p><strong>摘要：</strong> 在当今充斥着虚假信息的网络环境中，帮助读者理解所读内容至关重要。传统上，事实核查主要依赖手动或自动方式，但对新兴话题或信息有限的情况难以应对。本研究聚焦于评估新闻媒体整体的可靠性及政治偏见，而非单篇文章。我们设计了一系列基于专业事实核查员标准的提示，并通过大型语言模型（LLMs）获取回应，最终整合预测结果。实验表明，该方法显著优于现有基线模型，并深入分析了媒体流行度和地区对模型表现的影响。此外，还进行了消融研究以确定数据集的关键要素。为促进后续研究，我们公开了数据集和代码。这项研究填补了新闻媒体整体评估领域的空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 11:49:20 GMT</pubDate>
</item>
<item>
<title>QGuard：一种基于问题提示的大语言模型安全防护方法</title>
<link>https://arxiv.org/abs/2506.12299</link>
<guid>https://arxiv.org/abs/2506.12299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QGuard方法，利用问题提示零样本防御大语言模型的有害提示攻击。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）面临的有害提示和越狱提示带来的恶意攻击风险，提出了名为QGuard的安全防护方法。QGuard通过引入问题提示，在无需微调的情况下，以零样本方式有效阻止文本和多模态有害提示的攻击。实验结果显示，该方法在文本和多模态有害数据集上表现出色，同时通过对问题提示的分析，实现了用户输入的白盒分析。我们认为，此方法为实际LLMs服务中的安全风险缓解提供了有价值的参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12299" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 21:23:50 GMT</pubDate>
</item>
<item>
<title>EgoPrivacy：第一人称视角隐私风险评估基准</title>
<link>https://arxiv.org/abs/2506.12258</link>
<guid>https://arxiv.org/abs/2506.12258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示第一人称视频中可推断出的穿戴者隐私信息。</p><br /><br /><p><strong>摘要：</strong> 随着可穿戴摄像头的普及，尽管已有研究关注旁观者隐私，但对穿戴者隐私威胁的研究相对不足。本文引入EgoPrivacy，这是首个针对第一人称视角视觉隐私风险的大规模基准测试工具，涵盖三类隐私（人口统计、个体特征和情境信息），并定义了七个任务来恢复从精细到粗粒度的私密信息。此外，我们提出了一种新颖的检索增强攻击策略，通过外部旁观者视频池中的检索来提高人口统计隐私攻击的效果。实验表明，即使在零样本设置下，基础模型也能有效泄露穿戴者的身份、场景、性别和种族等属性，准确率可达70%-80%。我们的代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 18:19:54 GMT</pubDate>
</item>
<item>
<title>语言模型在动态仇恨言论检测中的时间敏感性评估</title>
<link>https://arxiv.org/abs/2506.12148</link>
<guid>https://arxiv.org/abs/2506.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示静态基准无法准确评估语言模型处理动态仇恨言论的能力。</p><br /><br /><p><strong>摘要：</strong> 随着社会动态和文化变迁，仇恨言论的语言也在迅速演变，这对自然语言处理（NLP）领域提出了挑战。尽管已有研究探讨了语言演化对模型训练的影响并提出了一些解决方案，但其对模型基准测试的影响仍未得到充分探索。作为保障模型安全的关键工具，仇恨言论基准的重要性不言而喻。本文通过实证研究评估了20种语言模型在两个动态仇恨言论实验中的鲁棒性，揭示了静态与时间敏感性评估之间的时间错配问题。我们的发现强调了构建时间敏感型语言基准的必要性，以便更可靠地评价仇恨言论领域的语言模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 14:08:19 GMT</pubDate>
</item>
<item>
<title>VGR：增强视觉感知能力的多模态链式推理大模型</title>
<link>https://arxiv.org/abs/2506.11991</link>
<guid>https://arxiv.org/abs/2506.11991</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新多模态大语言模型VGR，提升复杂视觉推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有多模态链式推理方法主要依赖纯语言空间且局限于数学或科学领域的局限性，提出了一种名为VGR的新模型。VGR是一种具有增强细粒度视觉感知能力的多模态大语言模型，它通过检测有助于解决问题的相关区域并基于这些区域提供精确答案来实现视觉和语言推理的结合。为了训练该模型，我们构建了一个包含视觉定位和语言推理混合数据的大规模SFT数据集VGR-SFT。实验表明，在LLaVA-NeXT-7B基线上，VGR在需要全面理解图像细节的多模态基准测试中表现出色，例如在MMStar上得分提高了+4.1，在AI2D上提高了+7.1，在ChartQA上则提升了+12.9，同时使用的图像token数量仅为基线模型的30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11991" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:47:43 GMT</pubDate>
</item>
<item>
<title>TaskCraft：自动化生成可扩展的多工具交互型任务</title>
<link>https://arxiv.org/abs/2506.10055</link>
<guid>https://arxiv.org/abs/2506.10055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaskCraft通过自动化生成多工具交互型任务提升AI模型性能。</p><br /><br /><p><strong>摘要：</strong> 随着多步骤问题解决等自主性任务在自然语言处理（NLP）和人工智能（AI）中的重要性增加，现有的指令数据缺乏工具交互能力，而当前的基准测试依赖昂贵的人工标注，限制了其扩展性。本文介绍TaskCraft，这是一种自动化的任务生成工作流，能够创建难度可调节、支持多工具交互且可验证的任务，并包含执行轨迹。TaskCraft通过深度和宽度扩展原子任务，构建结构和层次复杂的挑战。实证结果显示，这些任务优化了生成工作流中的提示，并提升了对自主基础模型的监督微调效果。此外，研究还提供了一个包含约36,000个任务的大规模合成数据集，以支持未来关于代理调优和评估的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:14 GMT</pubDate>
</item>
<item>
<title>基于LLM辅助系统的自主学习能力培养研究</title>
<link>https://arxiv.org/abs/2506.09968</link>
<guid>https://arxiv.org/abs/2506.09968</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过游戏化和AI支持系统提升大学生自主学习技能。</p><br /><br /><p><strong>摘要：</strong> 自主学习能力（SRL）对大学生适应学术挑战至关重要。本研究针对59名大学生面临的自主学习技能发展难题，如目标设定、时间管理和反思性学习困难，开发了SRLAgent系统。该系统基于Zimmerman的三阶段SRL框架，利用大型语言模型（LLM）提供实时反馈和适应性支持，在游戏化环境中促进学生的自主学习。实验结果显示，SRLAgent组学生的自主学习技能显著提高（p < .001, 效应值d=0.234），且用户参与度高于对照组。这项研究强调了将自主学习支架和实时AI支持嵌入游戏化环境中的价值，为教育技术的设计提供了重要启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09968" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:45:03 GMT</pubDate>
</item>
<item>
<title>TransDiff：结合Transformer与扩散模型的图像生成新方法</title>
<link>https://arxiv.org/abs/2506.09482</link>
<guid>https://arxiv.org/abs/2506.09482</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransDiff结合AR Transformer与扩散模型，在ImageNet上性能超越其他方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了TransDiff，一种首次将自回归（AR）Transformer与扩散模型相结合的图像生成模型。在该联合建模框架下，TransDiff通过编码标签和图像到高层次语义特征，并利用扩散模型估计图像样本分布。在ImageNet 256x256基准测试中，TransDiff显著优于基于独立AR Transformer或扩散模型的其他图像生成模型。具体而言，TransDiff实现了Fréchet Inception Distance (FID) 为1.61和Inception Score (IS) 为293.4的性能，并且相比最先进的AR Transformer方法推理延迟快2倍，比纯扩散模型方法快112倍。此外，基于TransDiff模型，我们提出了一种新的图像生成范式——多参考自回归（MRAR），它通过预测下一个图像进行自回归生成，允许模型引用多个先前生成的图像，从而促进学习更丰富的表示并提高后续迭代生成图像的质量。应用MRAR后，TransDiff的FID从1.61降至1.42。我们预计TransDiff将在图像生成领域开辟新的研究前沿。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09482" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 03:50:31 GMT</pubDate>
</item>
<item>
<title>MATTER：结合材料知识的新型分词方法提升科学文本处理性能</title>
<link>https://arxiv.org/abs/2506.11115</link>
<guid>https://arxiv.org/abs/2506.11115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MATTER，一种结合材料知识的分词方法，改善材料科学领域语言模型的表现。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型在材料科学中的应用日益广泛，传统的基于频率的分词方法因无法维持材料概念的结构和语义完整性而受到限制。本文提出了一种名为MATTER的新方法，通过集成材料知识并优先考虑材料概念，有效解决了现有分词方法的问题。实验表明，MATTER在生成和分类任务中分别提升了4%和2%的平均性能，强调了领域知识在科学文本处理中的重要性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 00:59:13 GMT</pubDate>
</item>
<item>
<title>DeepEDM：结合深度学习与非线性动力学系统的时序预测框架</title>
<link>https://arxiv.org/abs/2506.06454</link>
<guid>https://arxiv.org/abs/2506.06454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种融合深度神经网络与非线性动力学建模的新框架DeepEDM。</p><br /><br /><p><strong>摘要：</strong> 本文针对现实世界中复杂非线性动态时间序列的预测问题，提出了DeepEDM框架。该框架基于Takens定理，通过延迟嵌入学习潜在空间，并利用核回归逼近隐藏的动力学机制，同时采用高效的Softmax注意力实现精准的未来时间步预测。实验表明，DeepEDM在合成数据及跨领域真实数据上均表现出较强的鲁棒性和预测准确性，优于现有方法。研究代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:24:12 GMT</pubDate>
</item>
<item>
<title>离散扩散语言模型与多模态语言模型综述</title>
<link>https://arxiv.org/abs/2506.13759</link>
<guid>https://arxiv.org/abs/2506.13759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">离散扩散模型在并行生成和推理加速方面优于自回归模型。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了离散扩散语言模型（dLLMs）和离散扩散多模态语言模型（dMLLMs），这些模型采用多令牌并行解码范式，与自回归模型相比具有生成速度快、可控性强等优势。文章追溯了dLLMs和dMLLMs的历史发展，总结了训练与推理的关键技术，并分析了其在语言、视觉-语言及生物领域的应用前景。此外，本文还讨论了未来研究方向和部署挑战，强调了这些模型在学术界和工业界的快速发展趋势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>预算引导：通过轻量级预测控制大语言模型推理长度</title>
<link>https://arxiv.org/abs/2506.13752</link>
<guid>https://arxiv.org/abs/2506.13752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出预算引导方法，有效控制大语言模型推理长度并提升效率。</p><br /><br /><p><strong>摘要：</strong> 近期深度大型语言模型通常通过大量推理来提高性能，但这种方法可能导致过高的推理成本且性能提升有限。因此，在有限的推理预算下控制推理长度至关重要，但具有挑战性。本文提出了一种名为预算引导的新方法，无需对语言模型进行微调即可指导其推理过程达到目标预算。该方法引入了一个轻量级预测器，在每次生成下一个令牌时预测剩余推理长度的伽马分布。这一信号被用于以软方式、令牌级别的方式指导生成过程，确保整体推理符合指定的预算限制。实验表明，预算引导方法在数学基准测试中显著提高了令牌效率，例如在MATH-500基准测试中，与基线方法相比，在严格预算下准确率提高了26%，同时仅使用全推理模型63%的令牌即保持竞争力。此外，该方法在更广泛的任务领域表现出色，并展现出估计问题难度等新能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>Test3R：通过测试时学习显著提升3D重建几何精度</title>
<link>https://arxiv.org/abs/2506.13750</link>
<guid>https://arxiv.org/abs/2506.13750</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Test3R技术，利用图像三元组优化网络，大幅提升3D重建几何一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Test3R的简单但有效的测试时学习技术，用于增强密集匹配方法在3D重建中的几何准确性。传统方法依赖于成对点图回归，但存在全局几何一致性不足的问题。Test3R通过使用图像三元组(I_1, I_2, I_3)，分别生成基于(I_1, I_2)和(I_1, I_3)的重建，并在测试时通过自监督目标优化网络，即最大化这两重建之间的几何一致性。这种方法确保了模型输出的跨对一致性，无论输入如何。实验结果显示，Test3R在3D重建和多视图深度估计任务上显著优于现有最先进的方法，且具有广泛的适用性和几乎零成本的特点。此外，该技术易于与其他模型集成，并在测试时仅需极小的训练开销和参数量。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13750" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 13:56:22 GMT</pubDate>
</item>
<item>
<title>Ego-R1框架：通过强化学习实现超长时间第一人称视频推理</title>
<link>https://arxiv.org/abs/2506.13654</link>
<guid>https://arxiv.org/abs/2506.13654</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Ego-R1框架，利用工具链式推理解决超长时第一人称视频理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Ego-R1的新框架，用于处理长达数天甚至数周的第一人称视频推理。该框架通过结构化的工具链式思维（CoTT）过程实现推理，由经过强化学习训练的Ego-R1智能体协调操作。受到人类解决问题策略的启发，CoTT将复杂的推理分解为模块化步骤，每一步调用特定工具完成子问题求解，如时间检索和多模态理解。为了训练该智能体，设计了包含监督微调（SFT）和强化学习（RL）两个阶段的训练范式，并构建了Ego-R1 Data数据集，其中包括Ego-CoTT-25K用于SFT，Ego-QA-4.4K用于RL。此外，Ego-R1智能体在新创建的一周视频问答基准Ego-R1 Bench上进行了评估，该基准包含来自混合来源的人类验证问答对。实验结果表明，Ego-R1智能体能够有效应对理解超长第一人称视频的独特挑战，时间覆盖范围从几小时显著扩展到一周。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13654" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 12:17:08 GMT</pubDate>
</item>
<item>
<title>MiniMax-M1：全球首个开放权重大规模混合注意力推理模型发布</title>
<link>https://arxiv.org/abs/2506.13585</link>
<guid>https://arxiv.org/abs/2506.13585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniMax-M1是一款支持超长上下文处理的高效混合注意力推理模型。</p><br /><br /><p><strong>摘要：</strong> MiniMax-M1是全球首款开放权重、大规模混合注意力推理模型，由混合专家（MoE）架构与闪电注意力机制驱动，基于MiniMax-Text-01升级而来，总参数达4560亿，单token激活459亿参数，上下文长度可达100万tokens，是DeepSeek R1的8倍。该模型通过大规模强化学习训练，在复杂任务如软件工程和工具利用方面表现出色。此外，提出的CISPO算法进一步提升了强化学习效率，使M1仅用512块H800 GPU完成训练仅需三周，成本仅为$534,700。实验显示，MiniMax-M1在标准基准测试中与强开源模型相比表现相当甚至更优。MiniMax-M1已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 11:08:02 GMT</pubDate>
</item>
<item>
<title>结构化提示对大型语言模型文本分析能力的影响研究</title>
<link>https://arxiv.org/abs/2506.13172</link>
<guid>https://arxiv.org/abs/2506.13172</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，结构化提示可有效引导大型语言模型进行复杂文本分析。</p><br /><br /><p><strong>摘要：</strong> 本研究设计并评估了一组概念验证的结构化工作流提示，旨在引导大型语言模型（LLMs）完成高阶语义和语言分析任务，如检测学术论文总结中的未经证实主张（信息完整性）及模糊代词引用（语言清晰度）。通过在Gemini Pro 2.5 Pro和ChatGPT Plus o3两款前沿模型上进行多轮系统性评估，发现模型在信息完整性任务上的表现因语法角色差异而异，且在仅提供摘要而非全文的情况下，ChatGPT表现出色，而Gemini性能显著下降。这一研究揭示了结构化提示在复杂文本分析中的潜力，同时强调了模型性能高度依赖于模型类型、任务性质及上下文条件，需进行严谨的特定模型测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.13172" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 16 Jun 2025 03:34:31 GMT</pubDate>
</item>
<item>
<title>基于提示的大型语言模型时间序列预测方法</title>
<link>https://arxiv.org/abs/2506.12953</link>
<guid>https://arxiv.org/abs/2506.12953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示无需重训即可让大型语言模型高效进行时间序列预测。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLMs）的进步为准确且高效的时间序列分析提供了新的可能性，但以往研究通常需要大量微调或忽略了时间序列间的相关性。本文探索了一种简单灵活的基于提示的方法，使LLMs能够在不进行大规模重新训练或不依赖复杂外部架构的情况下完成时间序列预测。通过利用时间序列分解、基于块的标记化以及基于相似性的邻居增强等专门提示方法，我们发现可以提升LLM的预测质量，同时保持模型的简洁性和对数据预处理的需求最小化。为此，我们提出了PatchInstruct方法，该方法使LLMs能够做出精确有效的预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 15:42:58 GMT</pubDate>
</item>
<item>
<title>PersonaFeedback：评估大型语言模型个性化能力的新基准</title>
<link>https://arxiv.org/abs/2506.12915</link>
<guid>https://arxiv.org/abs/2506.12915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准PersonaFeedback，用于评估LLMs根据用户画像生成个性化响应的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）通用能力的提升，如何构建能提供个性化服务的系统成为重要研究课题。然而，缺乏高质量的个性化评估基准阻碍了这一领域的发展。本文介绍PersonaFeedback，这是一个直接评估LLMs生成个性化响应能力的新基准，它通过显式用户画像进行测试，而非依赖隐式推断。该基准包含8298个人类注释的测试案例，分为易、中、难三个层级。实验结果显示，即使是最先进的LLMs在难度较高的测试中也表现不佳，甚至人类评估者也可能难以区分细微差异。此外，分析表明当前的检索增强框架并非个性化任务的默认解决方案。所有数据、注释协议和评估流程都将公开，以促进未来的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 15 Jun 2025 13:19:19 GMT</pubDate>
</item>
<item>
<title>面向用户界面教学视频的多模态摘要新基准</title>
<link>https://arxiv.org/abs/2506.12623</link>
<guid>https://arxiv.org/abs/2506.12623</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出针对UI教学视频的多模态摘要新基准，填补现有数据集空白。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于教学视频的多模态摘要生成问题，旨在通过文本指令和关键帧为用户提供高效技能学习方式。然而，现有视频摘要数据集主要关注通用语义层面的视频总结，无法满足提供逐步可执行说明的需求。为此，我们构建了一个名为MS4UI的新数据集，包含2413个UI教学视频，总时长达167小时，并对视频进行分割、文本摘要及视频摘要的手动标注。实验表明，目前最先进的多模态摘要方法在UI教学视频摘要任务上表现不佳，强调了开发新方法的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12623" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 16:39:32 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的表示对齐及其跨语言控制方法</title>
<link>https://arxiv.org/abs/2506.12450</link>
<guid>https://arxiv.org/abs/2506.12450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型具有自然出现的表示对齐能力，并提出了一种新的跨语言控制方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中自然出现的表示对齐现象，特别是在中间层的表现，以及这种对齐对分离语言特定信息和语言无关信息的意义。通过实证研究，我们验证了这种对齐的存在，并将其与显式设计的对齐模型进行比较分析，展示了其在不损害语义的情况下实现语言特定操作的潜力。基于这些发现，我们提出了推理时语言控制（ITLC），一种利用潜在注入的新方法，以实现精确的跨语言控制并减轻LLMs中的语言混淆问题。实验表明，ITLC在保持目标语言语义完整性的同时展现出强大的跨语言控制能力，并有效缓解了现有大规模LLMs中存在的跨语言语言混淆问题，从而导致语言生成的一致性不足。本研究加深了我们对LLMs表示对齐的理解，并为提高其跨语言性能提供了实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 14 Jun 2025 07:09:50 GMT</pubDate>
</item>
<item>
<title>大型语言模型人格解读：基于Supernova Event Dataset的事件提取与基准测试</title>
<link>https://arxiv.org/abs/2506.12189</link>
<guid>https://arxiv.org/abs/2506.12189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过新数据集评估多种语言模型的人格特质及事件提取能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的方法，利用Supernova Event Dataset来分析大型语言模型（LLMs）的人格特质及其对文本中关键事件提取和排序的能力。该数据集涵盖了传记、历史事件、新闻报道及科学发现等多样化内容。研究对象包括小型模型（如Phi-4、Orca 2、Qwen 2.5）和强大的大型模型（如Claude 3.7、Gemini 2.5、OpenAI o3）。通过让另一款LLM作为裁判，根据模型对事件的选择和分类推断其人格特征。研究揭示了各模型的独特人格特质，例如Orca 2专注于人际动态的情感推理，而Qwen 2.5则表现出战略性分析风格。此外，在处理科学发现事件时，Claude Sonnet 3.7侧重概念框架构建，Gemini 2.5 Pro强调实证验证，o3倾向于逐步因果推理。这项工作提高了模型的可解释性，使其更适合广泛多样的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.12189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 15:31:52 GMT</pubDate>
</item>
<item>
<title>DeepResearch Bench：LLM驱动的研究代理能力评估基准</title>
<link>https://arxiv.org/abs/2506.11763</link>
<guid>https://arxiv.org/abs/2506.11763</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepResearch Bench基准用于评估基于LLM的研究代理能力。</p><br /><br /><p><strong>摘要：</strong> 深度研究代理（DRAs）作为大型语言模型（LLMs）驱动的代理类别，能够通过多步骤网络探索、目标检索和高级合成将海量在线信息转化为高质量的研究报告。然而，目前缺乏系统评估这些代理能力的综合基准。为填补这一空白，我们发布了DeepResearch Bench，包含由22个领域专家设计的100项博士级研究任务。由于评估DRAs复杂且耗时，我们提出了两种新方法，一种是参考导向法，采用自适应标准评估生成报告的质量；另一种框架则通过有效引用数量和总体引用准确性来衡量信息检索能力。DeepResearch Bench及相关组件已开源，以推动实用LLM代理的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11763" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 09:17:32 GMT</pubDate>
</item>
<item>
<title>科学家首次考试（SFE）基准评测科学多模态大语言模型</title>
<link>https://arxiv.org/abs/2506.10521</link>
<guid>https://arxiv.org/abs/2506.10521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准SFE评估科学多模态大语言模型的认知能力，发现当前顶级模型表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 随着科学研究对复杂多模态推理的需求增加，科学多模态大语言模型（MLLMs）被寄予厚望，可以显著提升科学发现的效率。然而，现有的科学基准主要集中在评估MLLMs的知识理解能力，对其感知和推理能力的评估不足。为弥补这一缺陷，我们提出了科学家的首次考试（SFE）基准，通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLMs的科学认知能力。SFE包含830个由专家验证的视觉问答对，涵盖五个高价值学科中的66个多模态任务。实验结果显示，最先进的GPT-o3和InternVL-3模型在SFE上的得分分别为34.08%和26.52%，表明这些模型在科学领域还有很大的改进空间。我们希望SFE提供的见解能够促进人工智能辅助科学发现的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 05:29:16 GMT</pubDate>
</item>
<item>
<title>ALE-Bench：评估AI系统在算法工程中的表现</title>
<link>https://arxiv.org/abs/2506.09050</link>
<guid>https://arxiv.org/abs/2506.09050</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准ALE-Bench用于评估AI在优化问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ALE-Bench的新基准，旨在评估人工智能系统在解决诸如包裹配送路径规划、机组排班、工厂生产计划及电网平衡等复杂优化问题时的表现。该基准基于AtCoder启发式竞赛的实际任务构建，包含计算上难以处理且无已知精确解的问题。不同于短期通过/失败的编码测试，ALE-Bench强调长时间跨度内的迭代式解决方案改进。研究采用的软件框架支持交互式代理架构，允许利用运行反馈和可视化工具。实验表明，尽管前沿的大语言模型在特定问题上表现出色，但与人类相比，在跨问题的一致性及长期问题解决能力方面仍存在显著差距。这一发现凸显了设立此类基准的重要性，以推动未来AI技术的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09050" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>NoWait：高效多模态推理的插件式解决方案</title>
<link>https://arxiv.org/abs/2506.08343</link>
<guid>https://arxiv.org/abs/2506.08343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出NoWait方法，通过抑制自省信号提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 近期大规模推理模型的进步虽实现了复杂分步推理，但常伴随冗长和重复输出的问题，降低了效率。本研究探讨显式自我反思是否对高级推理必要，并提出了名为NoWait的方法，在推理过程中抑制诸如“等一下”、“嗯”之类的自省标记。实验结果显示，NoWait在文本、视觉及视频推理的十个基准测试中，将五种R1系列模型的推理轨迹长度减少了27%-51%，且不影响模型实用性。这一简单有效的方法为多模态推理提供了高效的实用型解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 21:54:04 GMT</pubDate>
</item>
<item>
<title>BridgeVLA：一种高效的三维视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2506.07961</link>
<guid>https://arxiv.org/abs/2506.07961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合3D信号的新型VLA模型BridgeVLA，显著提升了机器人操作学习的效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，利用预训练的视觉-语言模型(VLMs)构建视觉-语言-动作(VLA)模型成为机器人操作学习的有效方法之一，但现有方法对3D信号的利用不足，导致样本效率低下。本文提出了BridgeVLA，通过将3D输入投影到多个2D图像上并与VLM骨干对齐，同时利用2D热图进行动作预测，统一了输入与输出空间。此外，还设计了一种可扩展的预训练方法，在下游策略学习前赋予VLM骨干预测2D热图的能力。实验表明，BridgeVLA在三个模拟基准测试中均优于当前最先进的基线方法，尤其在RLBench、COLOSSEUM和GemBench中表现优异。在真实机器人实验中，BridgeVLA不仅在多种分布外设置中表现出稳健的泛化能力，还实现了极高的样本效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:36:34 GMT</pubDate>
</item>
<item>
<title>From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding</title>
<link>https://arxiv.org/abs/2506.03968</link>
<guid>https://arxiv.org/abs/2506.03968</guid>
<content:encoded><![CDATA[
The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:00:47 GMT</pubDate>
</item>
<item>
<title>构建AI代理行为科学：从模型到行为的系统性研究</title>
<link>https://arxiv.org/abs/2506.06366</link>
<guid>https://arxiv.org/abs/2506.06366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AI代理行为科学，强调观察、干预与理论指导。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的进步推动了具备人类行为特征的AI代理发展，这些行为不仅依赖模型架构，还受具体情境中的环境、社会线索及交互反馈影响。本文提出AI代理行为科学这一新视角，聚焦于观察行为、设计实验验证假设及基于理论解释AI的行为、适应性和交互过程。通过整合个体代理、多代理及人机交互的研究成果，该视角被应用于提升AI的公平性、安全性、可解释性、责任性和隐私保护等特性。本研究统一现有发现并规划未来方向，将AI代理行为科学定位为传统模型驱动方法的重要补充，为评估和管理日益自主的AI系统提供必要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:12:32 GMT</pubDate>
</item>
<item>
<title>Avey：一种突破注意力与循环机制的新神经基础架构</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种不依赖注意力和循环的新神经架构Avey，显著提升长序列处理能力。</p><br /><br /><p><strong>摘要：</strong> Transformer虽在语言模型领域广泛应用，但受限于固定上下文窗口和二次复杂度问题。本文提出Avey架构，通过排名器与自回归处理器协作，仅关注相关token，实现任意长度序列的有效处理。实验表明，Avey在短距离NLP基准测试中表现优异，尤其擅长捕捉长距离依赖关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11305" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 17:11:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不确定场景下的拒绝回答能力评估</title>
<link>https://arxiv.org/abs/2506.09038</link>
<guid>https://arxiv.org/abs/2506.09038</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，大型语言模型在面对不确定问题时的拒绝回答能力亟待提升。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在复杂问题求解上取得了显著进展，但在处理不确定或无法回答的问题时仍存在明显不足。本文介绍了一个名为AbstentionBench的新基准，用于评估20个多样化数据集中的拒绝回答能力，涵盖未知答案、表述不清、错误前提、主观解释及过时信息等问题。实验结果显示，即使是最新推理型LLMs在拒绝回答方面的表现也远不如预期，甚至在经过推理微调后，该能力反而下降了24%。尽管精心设计的系统提示可以在一定程度上改善这一状况，但并未解决模型根本无法有效处理不确定性的问题。本研究旨在通过发布AbstentionBench推动LLMs可靠性的进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09038" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型对反馈的吸收能力研究</title>
<link>https://arxiv.org/abs/2506.11930</link>
<guid>https://arxiv.org/abs/2506.11930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现即使在理想条件下，LLMs对反馈的吸收仍存在阻力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在接收外部反馈后的改进能力，通过构建受控实验环境，让解题模型在接收接近完整的反馈后重新尝试问题解答。实验涵盖了数学推理、知识推理、科学推理以及多领域综合评估等任务，使用了如Claude 3.7等先进的语言模型。尽管条件理想，但这些模型普遍表现出对反馈的抵抗现象，即所谓的“反馈摩擦”。我们尝试通过采样策略改善这一情况，但效果有限。此外，排除了模型过度自信和数据熟悉度等因素作为主要原因。希望揭示这一局限性能为未来LLMs的自我提升研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:31:51 GMT</pubDate>
</item>
<item>
<title>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards</title>
<link>https://arxiv.org/abs/2506.11474</link>
<guid>https://arxiv.org/abs/2506.11474</guid>
<content:encoded><![CDATA[
Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 01:36:30 GMT</pubDate>
</item>
<item>
<title>通过学习继续思考令牌提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.11274</link>
<guid>https://arxiv.org/abs/2506.11274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索了通过学习专用继续思考令牌来增强语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 测试时扩展计算是一种有效提升语言模型性能的方法，通过在推理阶段利用额外计算资源实现。近期研究表明，覆盖结束思考标记（如将“”替换为“等待”）可以延长推理步骤并提高准确性。本研究尝试引入一个专门的继续思考标记，通过强化学习训练其嵌入，同时保持模型权重不变。实验表明，此方法在标准数学基准测试中优于基线模型及固定标记的测试时扩展方法。例如，在GSM8K基准测试中，固定标记方法提升了1.3%的准确率，而学习标记方法则实现了比不使用预算强迫的基线模型高4.2%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 16:28:54 GMT</pubDate>
</item>
<item>
<title>Mirage-1：基于分层多模态技能的跨平台GUI代理</title>
<link>https://arxiv.org/abs/2506.10387</link>
<guid>https://arxiv.org/abs/2506.10387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的跨平台GUI代理Mirage-1，显著提升了长周期任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大语言模型作为图形用户界面（GUI）代理在线上环境处理长周期任务时的知识不足问题，提出了一种分层多模态技能（HMS）模块，通过逐步抽象轨迹形成执行技能、核心技能及元技能，构建层次化知识结构以支持长期规划。同时，引入技能增强的蒙特卡洛树搜索（SA-MCTS）算法，利用离线环境中习得的技能缩减在线探索中的动作搜索空间，从而弥合域间差距。在此基础上开发的Mirage-1是一种多模态、跨平台的即插即用型GUI代理。为验证其性能，构建了新基准AndroidLH，实验结果显示，相比现有方法，Mirage-1在多个测试集上的表现分别提升了32%、19%、15%和79%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:21:19 GMT</pubDate>
</item>
<item>
<title>基于LoRA调优的掩码引导视频编辑方法</title>
<link>https://arxiv.org/abs/2506.10082</link>
<guid>https://arxiv.org/abs/2506.10082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于掩码的LoRA调优方法，实现灵活可控的视频编辑。</p><br /><br /><p><strong>摘要：</strong> 当前基于扩散模型的视频编辑方法多依赖大规模预训练，缺乏灵活性。为解决这一问题，本文提出了一种基于掩码的LoRA（低秩适应）调优方法，该方法通过适配预训练的图像到视频（I2V）模型，在保持背景不变的同时实现对特定区域的可控编辑传播。此外，引入参考图像作为视觉锚点，帮助模型更好地理解编辑意图。实验表明，该方法在性能上优于现有最先进方法，且无需改变模型架构，展现出高效性和适应性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 14:03:55 GMT</pubDate>
</item>
<item>
<title>基于生成-修剪-排名范式的程序验证效率与准确性权衡</title>
<link>https://arxiv.org/abs/2506.10056</link>
<guid>https://arxiv.org/abs/2506.10056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出生成-修剪-排名方法，显著提升程序验证速度同时保持较高准确性。</p><br /><br /><p><strong>摘要：</strong> 当前通过大型语言模型解决编程任务的标准范式是生成代码后进行排名，其中排名步骤依赖于验证器。普遍认为当全面验证器可用时，应优先采用而非结果奖励模型(ORM)，且较少考虑速度与准确性之间的权衡。本研究挑战这一假设，系统性探索两者之间的平衡，发现ORM在利用速度换取准确率方面至关重要，尤其是在生成-修剪-排名范式中。这种新方法比传统全面测试套件快11.65倍，仅降低8.33%的准确性。分析表明，该方法通过过滤掉高度排名但错误的解决方案实现高效验证，从而为设计可扩展且精确的程序排名系统提供了可能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>JAFAR：轻量且灵活的基础视觉编码器特征上采样方法</title>
<link>https://arxiv.org/abs/2506.11136</link>
<guid>https://arxiv.org/abs/2506.11136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为JAFAR的轻量级特征上采样方法，提升视觉特征的空间分辨率。</p><br /><br /><p><strong>摘要：</strong> Foundation Vision Encoders在密集视觉任务中至关重要，但其低分辨率特征输出限制了下游任务的表现。本文介绍了一种名为JAFAR的新方法，它通过基于注意力机制的模块，利用低级图像特征衍生的高分辨率查询和语义丰富的低分辨率键进行空间特征变换(SFT)调制，从而增强特征的空间分辨率至任意目标分辨率。尽管缺乏高分辨率监督，JAFAR在低上采样比和分辨率下学习的效果能够很好地推广到更高的输出尺度。实验表明，JAFAR在恢复细粒度空间细节方面表现出色，并在多种下游任务中显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 16:53:12 GMT</pubDate>
</item>
<item>
<title>基于自我意识弱点驱动的问题合成框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.08989</link>
<guid>https://arxiv.org/abs/2506.08989</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我意识弱点驱动问题合成框架，提高大语言模型在复杂推理任务上的性能。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励机制（RLVR）对训练大型语言模型（LLMs）解决复杂推理任务如数学问题非常有效，但现有数据集的人类标注数学问题质量和答案精确性不足限制了其效果。大多数问题合成策略不考虑模型能力，导致效率低下。为解决这些问题，我们提出了自我意识弱点驱动问题合成框架（SwS），通过系统识别模型的薄弱环节并针对性生成新问题，显著提升了模型在主流推理基准测试中的平均性能，7B和32B模型分别提高了10.0%和7.7%，且无需外部知识蒸馏。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08989" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:02:00 GMT</pubDate>
</item>
<item>
<title>Infinity-Instruct：提升大语言模型基础与对话能力的新基准数据集</title>
<link>https://arxiv.org/abs/2506.11116</link>
<guid>https://arxiv.org/abs/2506.11116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Infinity-Instruct数据集，增强开源模型的基础与指令跟随能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Infinity-Instruct的高质量指令数据集，旨在通过两阶段管道增强大型语言模型（LLMs）的基础和聊天能力。第一阶段，从超过1亿个样本中筛选出740万个高质量基础指令（InfInstruct-F-7.4M）。第二阶段，通过多阶段过程合成150万个高质量对话指令（InfInstruct-G-1.5M）。通过微调多个开源模型（如Mistral、LLaMA、Qwen和Yi），实验结果显示Infinity-Instruct显著提升了模型在基础和指令跟随任务上的表现，甚至在某些任务上超过了官方指令优化的模型。特别是，基于LLaMA的InfInstruct版本在指令跟随任务中的表现比GPT-4高出8.6%，同时在基础任务上表现相当。这些结果证明了基础训练和对话训练之间的协同效应，并为全面开发LLMs提供了新视角。相关数据集和代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:37:15 GMT</pubDate>
</item>
<item>
<title>基于候选标注的大语言模型数据标注方法</title>
<link>https://arxiv.org/abs/2506.03857</link>
<guid>https://arxiv.org/abs/2506.03857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大语言模型候选标注的新方法以提升数据质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大语言模型（LLMs）在数据标注中的局限性展开研究，这些方法通常通过单一标签策略进行标注，但难以应对复杂样本导致的错误标注问题。受人类规避不确定性行为的启发，我们提出了一种新的候选标注范式，即当模型存在不确定性时，鼓励其输出所有可能的标签。为适应下游任务需求，我们设计了一个教师-学生框架CanDist，利用小语言模型（SLM）对候选标注进行蒸馏。理论分析表明，这种候选标注的蒸馏方法优于直接采用单一标注。实验结果在六个文本分类任务中验证了该方法的有效性，相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 07:42:37 GMT</pubDate>
</item>
<item>
<title>多维线性循环神经网络在长距离依赖任务中的表现</title>
<link>https://arxiv.org/abs/2506.11997</link>
<guid>https://arxiv.org/abs/2506.11997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩展线性RNN至多维结构，提出pLSTM模型，适用于复杂图结构数据。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的多维线性循环神经网络（pLSTM），通过引入Source、Transition和Mark门，使其能够处理一般有向无环图（DAG）上的数据。pLSTM不仅能够并行化操作，还解决了长距离依赖问题，采用定向传播模式和扩散分布模式。实验表明，pLSTM在合成箭头指向外推任务及分子图和计算机视觉基准测试中表现出色，尤其在处理更大图像时优于Transformer模型。代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 13:51:37 GMT</pubDate>
</item>
<item>
<title>大型语言模型在编程竞赛中的表现评估</title>
<link>https://arxiv.org/abs/2506.11928</link>
<guid>https://arxiv.org/abs/2506.11928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型在编程竞赛中仍显著落后于人类专家。</p><br /><br /><p><strong>摘要：</strong> 近期报道声称大型语言模型（LLMs）在编程竞赛中已超越顶尖人类选手。然而，本研究通过分析国际算法竞赛奖牌得主的经验，重新审视这一说法，探讨LLMs与人类专家的区别及现有局限。我们引入LiveCodeBench Pro基准测试，涵盖来自Codeforces、ICPC和IOI的问题，并由金牌得主对问题进行分类和错误代码分析。结果显示，即使是最前沿的模型，在中等难度问题上的pass@1仅为53%，而在难题上则为0%，远不及人类专家。尽管LLMs在实现类问题上表现良好，但在复杂算法推理和案例分析方面存在明显不足，且常给出错误的自信解释。高绩效主要依赖于代码精确性而非更强的推理能力。因此，LiveCodeBench Pro不仅揭示了LLMs与人类大师水平之间的显著差距，还提供了详细的诊断工具，以指导未来代码型LLM推理能力的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:29:09 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的对齐新视角图像与几何生成框架</title>
<link>https://arxiv.org/abs/2506.11924</link>
<guid>https://arxiv.org/abs/2506.11924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过扭曲与填补方法生成对齐的新视角图像与几何的扩散模型框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于扩散模型的框架，该框架通过扭曲与填补的方法实现对齐的新视角图像和几何生成。不同于以往需要密集姿态图像或受限于域内视图的姿态嵌入生成模型的方法，我们的方法利用现成的几何预测器预测参考图像所看到的部分几何形状，并将新视角合成表述为图像和几何的填补任务。为了确保生成图像和几何之间的精确对齐，我们提出了跨模态注意力蒸馏，在训练和推理过程中将图像扩散分支的注意力图注入到并行的几何扩散分支中。这种多任务方法实现了协同效应，促进了具有几何鲁棒性的图像合成以及定义良好的几何预测。此外，我们引入了基于邻近性的网格条件化，以整合深度和法线线索，插值点云并过滤掉错误预测的几何形状对生成过程的影响。实证研究表明，我们的方法在多种未见场景下实现了高保真的外推视角合成，无论是图像还是几何，并且在插值设置下提供了竞争性的重建质量，还生成了几何对齐的彩色点云以实现全面的3D补全。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 12:19:00 GMT</pubDate>
</item>
<item>
<title>FourierAttention：一种高效的大语言模型长上下文处理方法</title>
<link>https://arxiv.org/abs/2506.11886</link>
<guid>https://arxiv.org/abs/2506.11886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的FourierAttention框架，提升大语言模型长上下文处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着上下文长度的增长，大语言模型面临日益增加的记忆需求，现有压缩方法往往牺牲精度或引入计算开销。本文提出FourierAttention，这是一种无需训练的框架，通过利用Transformer头维度的异构角色，在较低维度优先处理局部上下文的同时，较高维度捕捉远距离依赖。通过将对长上下文不敏感的维度投影到正交傅里叶基上，FourierAttention用固定长度的频谱系数近似其时间演化过程。实验表明，FourierAttention在LongBench和Needle-In-A-Haystack数据集上的长上下文准确性达到最佳。此外，还设计了一个定制的Triton内核FlashFourierAttention，优化内存使用并通过流线型读写操作实现高效部署，性能不受影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 11:35:54 GMT</pubDate>
</item>
<item>
<title>Configurable Preference Tuning (CPT): 动态调整AI行为的新框架</title>
<link>https://arxiv.org/abs/2506.11702</link>
<guid>https://arxiv.org/abs/2506.11702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入CPT框架，使语言模型可根据人类指令动态调整行为。</p><br /><br /><p><strong>摘要：</strong> 现有的AI对齐模型通常基于固定的单一偏好集，限制了适应性。本文提出Configurable Preference Tuning (CPT)，一种新颖的框架，允许语言模型根据显式的人类可解释指令动态调整行为。CPT通过利用基于结构化细粒度量表生成的合成偏好数据，这些量表定义了如写作风格等期望属性，从而实现这一目标。通过在这些量表引导的偏好上微调，LLM能够在推理时响应系统提示调整输出，而无需重新训练。这种方法不仅提供了细粒度控制，还为建模更细腻和情境依赖的人类反馈提供了机制。研究中的多个实验资源，包括训练代码、生成的数据集和微调模型，已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 13 Jun 2025 08:17:38 GMT</pubDate>
</item>
<item>
<title>Duo：通过高斯扩散改进离散扩散模型以提升文本生成性能</title>
<link>https://arxiv.org/abs/2506.10892</link>
<guid>https://arxiv.org/abs/2506.10892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Duo通过引入课程学习和一致性蒸馏技术，显著提升了离散扩散模型的训练效率和采样速度。</p><br /><br /><p><strong>摘要：</strong> 离散均匀状态扩散模型因其自我校正能力而具有快速文本生成的潜力，但通常表现不如自回归模型和掩码扩散模型。本研究通过利用高斯扩散过程的洞察，提出了一种名为Duo的新方法，通过课程学习策略和离散一致性蒸馏技术，大幅提高了训练速度和采样效率。实验结果显示，采用课程学习后，Duo在零样本困惑度上超越了自回归模型，在7个基准中的3个上表现优异；而离散一致性蒸馏则使扩散语言模型的采样速度加快了两个数量级。这项工作有效缩小了离散扩散模型与更先进模型之间的性能差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:55:35 GMT</pubDate>
</item>
<item>
<title>通过视觉描述幻觉批评提升视觉语言模型的感知能力</title>
<link>https://arxiv.org/abs/2506.10128</link>
<guid>https://arxiv.org/abs/2506.10128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ViCrit任务，通过检测图像描述中的细微错误提升视觉语言模型的感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViCrit（视觉描述幻觉批评）的强化学习代理任务，旨在提升视觉语言模型（VLMs）在视觉感知方面的表现。ViCrit任务通过在人类编写的图像描述中注入微妙的合成视觉幻觉错误，训练模型定位这些错误。此方法不仅保留了视觉感知的全部难度，还提供了易于计算且明确的二元奖励机制。实验表明，使用ViCrit任务训练的模型在多种视觉语言基准测试中取得了显著改进，这些改进不仅限于自然图像数据，还能推广到抽象图像推理和视觉数学等领域。此外，为了促进评估，我们还推出了ViCrit-Bench，这是一个类别平衡的诊断基准，系统性地探测了不同图像领域和错误类型的感知错误。总体而言，我们的研究证明了细粒度幻觉批评是一种有效且可泛化的视觉感知增强目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 15:16:54 GMT</pubDate>
</item>
<item>
<title>对抗性用户对政策合规AI代理的威胁及防御策略研究</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究政策合规AI代理在客户服务中的安全性和鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 随着任务导向型基于大型语言模型（LLM）的智能体在受政策约束领域的应用增加，如退款资格或取消规则等，如何保证这些智能体始终遵守相关政策并拒绝违规请求成为一大挑战。本文提出了一种新的威胁模型，关注试图利用政策合规智能体谋取个人利益的恶意用户。为解决这一问题，我们开发了CRAFT，一个多智能体红队系统，通过采用基于政策感知的说服策略，在客户服务质量评估场景下有效削弱政策合规智能体的表现，超越了传统的越狱方法。此外，基于现有的tau-bench基准，我们引入了tau-break，一个专门用于评估智能体对操纵性用户行为的稳健性的补充基准。最后，我们测试了几种简单但有效的防御措施，虽然提供了一定程度的保护，但仍需更强有力的研究驱动的安全保障来抵御潜在的攻击。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 06:59:47 GMT</pubDate>
</item>
<item>
<title>InterSyn：基于自评估迭代精炼的大规模多模态数据集</title>
<link>https://arxiv.org/abs/2506.09427</link>
<guid>https://arxiv.org/abs/2506.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InterSyn数据集和SynJudge评估模型，提升多模态模型生成能力。</p><br /><br /><p><strong>摘要：</strong> 近年来大型多模态模型（LMMs）在多模态理解和生成方面取得显著进展，但生成紧密交织的图文输出仍具挑战性，主要受限于现有训练数据集的规模、质量和指导丰富度。为此，我们引入InterSyn数据集，利用自评估与迭代精炼（SEIR）方法构建，提供多轮指令驱动的图文对话，具备丰富的对象多样性和严格的自动化质量优化，适合作为下一代指令跟随型LMMs的训练数据。同时，针对缺乏可靠评估工具的问题，我们开发了SynJudge自动评估模型，从文本内容、图像内容、图像质量和图文协同四个维度量化评估多模态输出。实验表明，SEIR方法显著提高了数据集质量，而基于InterSyn训练的LMMs在所有评估指标上均表现更优，验证了InterSyn在推动多模态系统发展中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 02:21:20 GMT</pubDate>
</item>
<item>
<title>SkillBlender：一种用于人形机器人灵活操控的分层强化学习框架</title>
<link>https://arxiv.org/abs/2506.09366</link>
<guid>https://arxiv.org/abs/2506.09366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的分层强化学习框架SkillBlender，实现人形机器人的多样化操控。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkillBlender的新框架，旨在解决现有方法在处理人形机器人多任务适应性时存在的问题。通过预训练无任务特定条件的基础技能并动态混合这些技能，SkillBlender能够在最小化任务特定奖励工程的情况下完成复杂的操控任务。同时，我们还开发了一个名为SkillBench的模拟基准，包含三种机器人形态、四种基础技能及八个挑战性任务，用于科学评估。实验表明，SkillBlender显著优于其他基线模型，在准确性与可行性方面表现出色。未来，我们的代码和基准将开源供社区使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:24:26 GMT</pubDate>
</item>
<item>
<title>基于自精炼框架的无标注数据增强ASR性能研究</title>
<link>https://arxiv.org/abs/2506.11130</link>
<guid>https://arxiv.org/abs/2506.11130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用无标注数据提升语音识别性能的自精炼框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为自精炼框架的方法，通过仅使用无标注数据即可提升自动语音识别（ASR）系统的性能。该框架首先利用现有的ASR模型对无标注语音生成伪标签，然后使用这些伪标签训练高保真的文本到语音（TTS）系统。接着，将合成的语音文本对反馈至原始ASR系统，完成闭合循环的自我改进过程。此方法在台湾华语语音上进行了验证，在6000小时无标注语音、少量文本数据及AI模型生成的合成内容的支持下，成功将Whisper-large-v2模型转化为专门化的Twister模型。结果显示，Twister在普通话基准测试中的错误率降低了20%，而在普通话-英语混合代码转换基准测试中的错误率降低了50%。这一框架不仅优于传统的伪标签自蒸馏方法，还为低资源或特定领域ASR性能提升提供了实际可行的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.11130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:30:32 GMT</pubDate>
</item>
<item>
<title>基于二值注意力掩码的图像预测方法</title>
<link>https://arxiv.org/abs/2506.08915</link>
<guid>https://arxiv.org/abs/2506.08915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用二值注意力掩码提升图像预测鲁棒性的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于注意力机制的方法，通过学习得到的二值注意力掩码，确保只有被关注的图像区域影响最终预测结果。这种方法旨在解决对象感知中的上下文干扰问题，尤其是在物体出现在分布外背景时可能导致的偏差表示。为了应对这一挑战，我们提出了一个双阶段框架：第一阶段处理完整图像以发现对象部分并确定任务相关的区域；第二阶段利用输入注意力掩码限制其感受野到这些区域，从而实现聚焦分析同时过滤掉潜在的虚假信息。两个阶段联合训练，使第二阶段能够优化第一阶段的表现。大量实验表明，该方法显著提高了对虚假相关性和分布外背景的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:41:22 GMT</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[
This work focuses on an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within the semantics, resulting in failed dense retrieval on even simple cases. To examine such behaviors, we first introduce a new evaluation dataset in Chinese, named CapRetrieval, whose passages are image captions, and queries are phrases inquiring entities or events in various forms. Zero-shot evaluation suggests that encoders may fail on these fine-grained matching, regardless of training sources or model sizes. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, which obtains the best performance on CapRetrieval. Within this process, we further identify an issue of granularity dilemma, a challenge for embeddings to express fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 05:00:33 GMT</pubDate>
</item>
<item>
<title>U-CoT+: 一种高效灵活的有害模因检测框架</title>
<link>https://arxiv.org/abs/2506.08477</link>
<guid>https://arxiv.org/abs/2506.08477</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架U-CoT+，通过文本描述和指导优化有害模因检测效率与灵活性。</p><br /><br /><p><strong>摘要：</strong> 有害模因的检测对维护在线环境的完整性至关重要，但现有方法在资源效率、灵活性和可解释性方面存在局限性。本文介绍了一种名为U-CoT+的新框架，该框架通过开发高保真的模因到文本转换管道，将视觉模因转化为细节保留的文本描述，从而实现对复杂原始视觉内容的高效分解与分类。此设计解耦了模因解释与分类过程，使基于通用大型语言模型的有害模因检测更加高效。进一步结合针对性的人类指导准则，在零样本思维链提示下引导模型推理，增强了框架的适应性和可解释性。实验验证了该框架的有效性，表明其在低资源条件下利用小型语言模型进行有害模因检测具有广阔潜力。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08477" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:10:45 GMT</pubDate>
</item>
<item>
<title>基于Reg-GRPO的视频大型语言模型增强视频推理能力的研究</title>
<link>https://arxiv.org/abs/2506.07464</link>
<guid>https://arxiv.org/abs/2506.07464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种改进的强化学习算法Reg-GRPO，显著提升视频大型语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，基于强化学习的后训练方法可以有效增强大型语言模型的推理能力，其中Group Relative Policy Optimization (GRPO)表现出色。然而，GRPO在视频大型语言模型（Video LLMs）中的应用尚未深入探索。本文研究了GRPO应用于Video LLMs时存在的两个主要问题：对安全措施的依赖及优势消失的问题。为解决这些问题，我们提出了DeepVideo-R1，这是一种结合了Reg-GRPO（回归形式的GRPO）和难度感知数据增强策略的视频大型语言模型。Reg-GRPO将GRPO目标重新定义为回归任务，直接预测优势值，从而消除了对安全措施如裁剪和最小函数的需求，使模型政策指导更加直接。此外，我们设计了一种难度感知的数据增强策略，在可解难度级别动态扩充训练样本，产生多样化且信息丰富的奖励信号。实验表明，DeepVideo-R1在多个视频推理基准测试中显著提升了视频推理性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:15:54 GMT</pubDate>
</item>
<item>
<title>HeadHunter: 针对扩散模型注意力扰动的细粒度控制方法</title>
<link>https://arxiv.org/abs/2506.10978</link>
<guid>https://arxiv.org/abs/2506.10978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HeadHunter框架，实现扩散模型生成质量的精细调控。</p><br /><br /><p><strong>摘要：</strong> 现有扩散模型中的注意力扰动方法在确定扰动位置时缺乏系统性，尤其是在Diffusion Transformer架构中，质量相关的计算分布在多层中。本文研究了注意力扰动的不同粒度，发现特定的注意力头控制着不同的视觉概念，如结构、风格和纹理质量。基于此，我们提出了HeadHunter框架，用于迭代选择与用户目标对齐的注意力头，从而实现生成质量与视觉属性的精细控制。此外，我们引入SoftPAG，通过线性插值注意力图向单位矩阵靠近，提供连续调节扰动强度的旋钮以抑制伪影。实验验证表明，该方法不仅缓解了现有层级扰动导致的过度平滑问题，还实现了特定视觉风格的目标操控。本研究首次对扩散模型中的注意力扰动进行了头级别分析，揭示了注意力层内的可解释专业化，并为设计有效的扰动策略提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>AutoMind：一种适应性强的知识驱动型大语言模型代理框架</title>
<link>https://arxiv.org/abs/2506.10974</link>
<guid>https://arxiv.org/abs/2506.10974</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoMind提升自动化数据科学能力，优于现有最先进方法。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）代理在解决实际数据科学问题方面展现出巨大潜力，但现有框架受限于固定工作流与编码策略，难以处理复杂创新任务。本文提出AutoMind框架，通过构建领域专家知识库、采用知识驱动树搜索算法及动态自适应编码策略，克服现有缺陷。评估显示，AutoMind在两个自动化数据科学基准测试中表现优异，效率、效果及解的质量均优于当前最佳基线，标志着迈向全自动数据科学的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10974" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>SWE-Factory：自动化构建大规模GitHub问题解决数据集</title>
<link>https://arxiv.org/abs/2506.10954</link>
<guid>https://arxiv.org/abs/2506.10954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-Factory自动化流水线，显著提升GitHub问题解决数据集构建效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模GitHub问题解决数据集构建的传统挑战，提出名为SWE-Factory的自动化流水线。该流水线通过SWE-Builder多智能体系统实现评估环境的自动构建，采用基于退出码的标准化评分方法取代自定义解析器，并通过可靠退出码信号实现失败转成功的自动化验证过程。实验表明，在四种编程语言的671个问题上，SWE-Builder以最低成本构造有效实例，且基于退出码的评分方法达到100%准确性，自动验证也表现出高精度与完全召回率。我们希望此流水线能加速高质量数据集的收集，用于训练和评估大型语言模型的软件工程能力。相关代码与数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:54:17 GMT</pubDate>
</item>
<item>
<title>迈向自主网络代理的新交互范式：Agentic Web Interface 的提出</title>
<link>https://arxiv.org/abs/2506.10953</link>
<guid>https://arxiv.org/abs/2506.10953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种针对大型语言模型优化的网络界面设计，解决现有方法的局限性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）及其多模态变体的发展引发了对网络代理研究的浓厚兴趣。网络代理是一种能够在网页环境中自主导航并完成任务的人工智能系统，然而当前方法因人类设计的界面与LLMs能力之间的根本不匹配而面临诸多挑战。这些问题体现在处理复杂的DOM树、依赖截图附加信息或绕过用户界面的API交互上。本文主张网络代理研究需要转变范式，而非让代理适应人为设计的界面，而是开发专门针对代理能力优化的新型交互方式。为此，我们提出了Agentic Web Interface（AWI）的概念，这是一种专为代理设计的网络界面，并制定了六项指导原则，强调安全、效率和标准化，以平衡各利益相关方的需求。这种重新定义旨在克服现有界面的根本限制，推动更高效、可靠且透明的网络代理设计，这一过程需由整个机器学习社区共同参与。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:58 GMT</pubDate>
</item>
<item>
<title>Domain2Vec：一种高效的数据集分解方法</title>
<link>https://arxiv.org/abs/2506.10952</link>
<guid>https://arxiv.org/abs/2506.10952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Domain2Vec通过分解数据集为元域向量优化语言模型预训练。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Domain2Vec的新方法，该方法将任意数据集分解为若干元域的线性组合。元域是一个新概念，用于捕捉数据集的关键特征。Domain2Vec通过分类器将数据集转化为对应的元域分布向量，从而无需训练即可确定最优数据混合方案。此方法基于分布对齐假设，表明当训练集和验证集的数据分布更匹配时，验证损失会更低。此外，Domain2Vec可以无缝集成到现有工作中，提升模型效率和可扩展性。实验表明，使用Domain2Vec进行预训练时，只需原计算资源的51.5%，即可达到相同的验证损失，同时在同等计算预算下，下游任务性能平均提高2.83%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:53:51 GMT</pubDate>
</item>
<item>
<title>基于半非负矩阵分解的大型语言模型可解释性特征提取</title>
<link>https://arxiv.org/abs/2506.10920</link>
<guid>https://arxiv.org/abs/2506.10920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过半非负矩阵分解直接分解MLP激活，提高因果导向能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中的机制可解释性问题，重点在于找到一种无监督方式来识别具有因果解释力的特征方向。传统方法依赖稀疏自动编码器（SAEs），但其在因果评估中表现不佳且缺乏内在可解释性。为此，我们引入半非负矩阵分解（SNMF）技术，将MLP激活直接分解为稀疏线性组合的神经元特征，并映射到激活输入上，从而提升特征的直观性和可解释性。实验表明，基于SNMF的方法在因果导向性能上优于SAEs及强监督基线，在Llama 3.1、Gemma 2和GPT-2等模型上表现出色。进一步分析揭示了神经元组合在语义相关特征间被复用的现象，展示了MLP激活空间中的层次结构。这些结果表明，SNMF是一种简单而有效的工具，用于识别LLMs中的可解释特征并解析概念表示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:33:29 GMT</pubDate>
</item>
<item>
<title>NoLoCo：一种无需显式同步的高效大规模语言模型训练方法</title>
<link>https://arxiv.org/abs/2506.10911</link>
<guid>https://arxiv.org/abs/2506.10911</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需显式参数同步的优化方法NoLoCo，显著降低通信开销并提高收敛速度。</p><br /><br /><p><strong>摘要：</strong> 当前大规模语言模型的训练通常依赖于高度互联的计算集群，但扩展此类集群成本高昂且不切实际。近期研究提出了减少通信密集度的训练方法，但仍需模型参数的同步步骤，这在低带宽网络上可能变得昂贵。本文提出了一种名为NoLoCo的新型优化方法，该方法通过Nesterov动量优化器的变体隐式同步模型权重，无需显式同步所有模型参数，也无需集体通信。理论分析和实验结果表明，NoLoCo在各种加速器数量和模型规模（125M到6.8B参数）下表现出色，相比完全分片的数据并行训练和低通信训练方法DiLoCo，其通信开销显著减少。此外，在数百个互联网加速器上进行的同步步骤估计比DiLoCo中的all-reduce快一个数量级，且不存在全局阻塞通信，从而减少了加速器的空闲时间。实验还显示，NoLoCo对各种模型大小和加速器数量的收敛速度比DiLoCo快高达4%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10911" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:23:23 GMT</pubDate>
</item>
<item>
<title>Magistral：基于纯强化学习训练的语言模型</title>
<link>https://arxiv.org/abs/2506.10910</link>
<guid>https://arxiv.org/abs/2506.10910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Magistral展示了纯强化学习训练大型语言模型的可能性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Mistral的第一款推理模型Magistral及其自主研发的可扩展强化学习（RL）管道。不同于依赖现有实现或从先前模型中蒸馏出的RL跟踪，我们采取了自下而上的方法，完全依赖自身模型和基础设施。研究展示了纯强化学习训练大型语言模型的潜力，提出了一种强制模型推理语言的简单方法，并证明仅使用文本数据进行强化学习可以保持初始检查点的能力。此外，我们展示了仅使用文本的强化学习可以维持或改善多模态理解、指令跟随和功能调用能力。我们发布了Magistral Medium，它是基于Mistral Medium 3通过纯强化学习训练的模型，并开源了Magistral Small（采用Apache 2.0许可证），它还包括了来自Magistral Medium的冷启动数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 13:22:37 GMT</pubDate>
</item>
<item>
<title>CreatiPoster：一种支持多层可编辑图形设计的AI框架</title>
<link>https://arxiv.org/abs/2506.10890</link>
<guid>https://arxiv.org/abs/2506.10890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CreatiPoster利用自然语言指令生成高质量、可编辑的图形设计。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CreatiPoster的新框架，它可以从自然语言指令或用户提供的资产中生成可编辑的多层次图形作品。该框架通过协议模型生成包含每层布局、层级、内容和风格的JSON规范，并结合背景提示生成协调的背景。实验表明，CreatiPoster在生成图形设计方面超越了现有的开源方法和商业系统。此外，研究团队发布了一个包含100,000个多层设计的无版权数据集，以促进进一步的研究。CreatiPoster支持多种应用场景，如画布编辑、文本覆盖、响应式缩放、多语言适应和动态海报制作，推动了AI辅助图形设计的普及。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:54:39 GMT</pubDate>
</item>
<item>
<title>VRBench：首个长叙事视频基准用于评估大模型多步推理能力</title>
<link>https://arxiv.org/abs/2506.10857</link>
<guid>https://arxiv.org/abs/2506.10857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VRBench，首个长叙事视频基准，用于评估大型模型的多步推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VRBench，这是一个专为评估大型模型多步推理能力而设计的首个长叙事视频基准。现有评估方法忽视了时间推理和程序有效性，而VRBench通过包含1010段长视频（平均时长1.6小时）、9468个人类标注的多步问答对以及30292个带有时间戳的推理步骤，解决了这些问题。这些视频经过多阶段过滤过程筛选，确保情节连贯性。我们开发了一种人机协作框架，生成连贯的推理链，每条链需要多个基于时间的步骤，涵盖七种类型（如事件归因、隐式推理）。VRBench还设计了一个多阶段评估管道，在结果和过程层面评估模型。除了多项选择题外，我们还提出了LLM引导的进度级评分指标，从多个维度全面评估推理链质量。通过对12个LLM和16个VLM在VRBench上的广泛评估，我们进行了深入分析并提供了有价值的见解，推动了多步推理领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 12:17:17 GMT</pubDate>
</item>
<item>
<title>基于文本推理模型的长视频理解框架</title>
<link>https://arxiv.org/abs/2506.10821</link>
<guid>https://arxiv.org/abs/2506.10821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过仅依赖文本推理模型实现长视频理解任务。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型在长视频理解（LVU）任务上面临复杂性和上下文窗口限制的挑战。传统观点认为需要扩展上下文窗口、增强视觉感知能力和领域专业知识的基础模型。本研究提出VideoDeepResearch框架，仅利用现有实践中的文本-only大推理模型及多模态工具包（如多模态检索器和视觉感知器），通过推理制定问题解决策略并选择性访问视频内容。实验表明，在MLVU、Video-MME和LVBench等基准测试中，该方法显著优于现有基础模型，分别提升了9.6%、6.6%和3.9%，证明了基于智能体系统的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 11:39:10 GMT</pubDate>
</item>
<item>
<title>PosterCraft：一种统一框架用于高审美海报生成</title>
<link>https://arxiv.org/abs/2506.10741</link>
<guid>https://arxiv.org/abs/2506.10741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架PosterCraft，显著提升海报生成的质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PosterCraft的新框架，旨在解决高审美海报生成中的挑战。与传统的模块化流水线不同，PosterCraft允许模型自由探索连贯且视觉吸引人的布局。该框架通过四个优化阶段实现高质量海报生成：大规模文本渲染优化、区域感知监督微调、美学文本强化学习及联合视觉语言反馈优化。这些阶段由专门设计的数据构建管道支持，无需复杂的架构修改即可进行稳健训练。实验表明，PosterCraft在文本渲染准确性、布局一致性及整体视觉吸引力方面优于开源基线，接近顶级商业系统的质量。相关代码、模型和数据集可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:28:12 GMT</pubDate>
</item>
<item>
<title>TaxoAdapt：动态适配的科学文献自动分类框架</title>
<link>https://arxiv.org/abs/2506.10737</link>
<guid>https://arxiv.org/abs/2506.10737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TaxoAdapt框架，提升科学文献分类的精度与适应性。</p><br /><br /><p><strong>摘要：</strong> 传统专家构建的科学文献分类体系耗时且成本高昂，而现有的自动方法存在特定语料库依赖性强或忽视领域动态变化的问题。为此，本文提出TaxoAdapt框架，通过迭代分层分类，在多个维度上扩展分类宽度和深度，以适应特定语料库的专题分布。实验表明，TaxoAdapt在多类计算机科学会议文献中表现优异，其生成的分类系统比现有最佳基线方法在粒度保存率和一致性上分别高出26.51%和50.41%，有效捕捉了科学领域的演变特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:26:28 GMT</pubDate>
</item>
<item>
<title>ClaimSpect：基于检索增强生成框架的复杂主张分解与视角表示</title>
<link>https://arxiv.org/abs/2506.10728</link>
<guid>https://arxiv.org/abs/2506.10728</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ClaimSpect，用于自动构建主张的层级结构并分析其多维视角。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了科学和政治主张中复杂的表述通常难以简单归类为“真”或“假”，但可以通过拆解成子方面（如有效性、安全性）进行验证。为此，我们提出了ClaimSpect框架，它利用检索增强生成技术，将主张分解为其组成成分和子成分，并从特定语料库中提取相关段落来丰富这些方面。该框架不仅能够发现新的子方面，还能揭示针对某一主张方面的不同立场（支持、中立或反对）及其流行程度。通过应用到科学和政治主张的真实案例，并结合人工评估，证明了ClaimSpect在解析复杂主张和表示语料库观点方面的有效性和准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10728" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 10:17:45 GMT</pubDate>
</item>
<item>
<title>TeleMath：首个电信领域数学问题评估基准</title>
<link>https://arxiv.org/abs/2506.10674</link>
<guid>https://arxiv.org/abs/2506.10674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出TeleMath基准数据集，用于评估大语言模型在电信数学问题中的性能。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能在电信领域的广泛应用，对大语言模型（LLMs）处理特定领域数学密集型任务的能力产生了兴趣。尽管LLMs在一般数学推理方面取得了进展，但在信号处理、网络优化等专门领域的表现尚不明确。为此，我们引入TeleMath，这是一个针对电信领域数学问题设计的首个基准数据集，包含500组问答对。通过专家设计的问题种子，我们构建了该数据集并评估了多种开源LLMs的表现，发现专注于数学推理的模型表现最优，而通用模型则表现不佳。本研究还公开了数据集和评估代码，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 09:04:18 GMT</pubDate>
</item>
<item>
<title>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2506.10600</link>
<guid>https://arxiv.org/abs/2506.10600</guid>
<content:encoded><![CDATA[
Constructing a physically realistic and accurately scaled simulated 3D world is crucial for the training and evaluation of embodied intelligence tasks. The diversity, realism, low cost accessibility and affordability of 3D data assets are critical for achieving generalization and scalability in embodied AI. However, most current embodied intelligence tasks still rely heavily on traditional 3D computer graphics assets manually created and annotated, which suffer from high production costs and limited realism. These limitations significantly hinder the scalability of data driven approaches. We present EmbodiedGen, a foundational platform for interactive 3D world generation. It enables the scalable generation of high-quality, controllable and photorealistic 3D assets with accurate physical properties and real-world scale in the Unified Robotics Description Format (URDF) at low cost. These assets can be directly imported into various physics simulation engines for fine-grained physical control, supporting downstream tasks in training and evaluation. EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object Generation, Scene Generation and Layout Generation. EmbodiedGen generates diverse and interactive 3D worlds composed of generative 3D assets, leveraging generative AI to address the challenges of generalization and evaluation to the needs of embodied intelligence related research. Code is available at https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 07:43:50 GMT</pubDate>
</item>
<item>
<title>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</title>
<link>https://arxiv.org/abs/2506.10568</link>
<guid>https://arxiv.org/abs/2506.10568</guid>
<content:encoded><![CDATA[
In e-commerce and digital marketing, generating high-fidelity human-product demonstration videos is important for effective product presentation. However, most existing frameworks either fail to preserve the identities of both humans and products or lack an understanding of human-product spatial relationships, leading to unrealistic representations and unnatural interactions. To address these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our method simultaneously preserves human identities and product-specific details, such as logos and textures, by injecting paired human-product reference information and utilizing an additional masked cross-attention mechanism. We employ a 3D body mesh template and product bounding boxes to provide precise motion guidance, enabling intuitive alignment of hand gestures with product placements. Additionally, structured text encoding is used to incorporate category-level semantics, enhancing 3D consistency during small rotational changes across frames. Trained on a hybrid dataset with extensive data augmentation strategies, our approach outperforms state-of-the-art techniques in maintaining the identity integrity of both humans and products and generating realistic demonstration motions. Project page: https://submit2025-dream.github.io/DreamActor-H1/.
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:58:23 GMT</pubDate>
</item>
<item>
<title>AniMaker：基于多智能体框架的文本驱动故事动画生成</title>
<link>https://arxiv.org/abs/2506.10540</link>
<guid>https://arxiv.org/abs/2506.10540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AniMaker框架，解决多场景故事动画生成中的叙事连贯性和稳定性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成模型在多场景叙事动画生成中的不足，提出了名为AniMaker的多智能体框架。该框架通过导演代理、摄影代理、审查代理及后期制作代理的协同工作，实现了从文本输入到全局一致且叙事连贯的动画生成。其中，摄影代理采用蒙特卡洛树搜索启发策略（MCTS-Gen），高效筛选高质量候选片段；审查代理则引入首个多镜头动画评估框架（AniEval），综合评估叙事一致性、动作完整性等特性。实验表明，AniMaker在多个指标上超越现有方法，显著提升生成效率，使AI生成的故事动画接近生产标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 06:06:21 GMT</pubDate>
</item>
<item>
<title>基于因果表示学习的语言模型能力评估框架</title>
<link>https://arxiv.org/abs/2506.10378</link>
<guid>https://arxiv.org/abs/2506.10378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种因果表示学习框架，用于有效评估语言模型的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过因果表示学习解决语言模型评估中的复杂混杂效应及高昂计算成本问题。研究利用线性变换将基准性能建模为潜在能力因素的函数，并控制基础模型作为共同混杂因子，确定这些因素间的因果关系。通过对涵盖1500多个模型的数据集进行分析，揭示了一种简洁的三节点线性因果结构，解释了性能变化。进一步解读显示了从一般问题解决能力到指令跟随能力再到数学推理能力的因果路径。结果强调了在评估中控制基础模型变异的重要性，这是准确揭示潜在模型能力间因果关系的关键步骤。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 02:07:42 GMT</pubDate>
</item>
<item>
<title>Optimus-3：面向Minecraft环境的多模态通用智能体</title>
<link>https://arxiv.org/abs/2506.10357</link>
<guid>https://arxiv.org/abs/2506.10357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于多模态大语言模型的通用智能体Optimus-3。</p><br /><br /><p><strong>摘要：</strong> 本文针对开放世界环境中构建具备感知、规划、行动、定位及反思能力的通用型智能体所面临的挑战，如领域特定数据不足、异构任务干扰及视觉多样性等问题，提出了三项创新解决方案。首先，设计了一种知识增强的数据生成管道，以提供可扩展且高质量的训练数据；其次，引入任务级路由的专家混合架构（MoE），减少异构任务间的干扰；最后，开发了多模态推理增强的强化学习方法，提升智能体对视觉多样性的处理能力。基于这些改进，我们推出了Optimus-3，一款针对Minecraft环境的通用智能体。实验结果表明，Optimus-3在多个任务上超越了现有的多模态大语言模型及智能体基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 12 Jun 2025 01:29:40 GMT</pubDate>
</item>
<item>
<title>Discrete Audio Tokens: More Than a Survey!</title>
<link>https://arxiv.org/abs/2506.10274</link>
<guid>https://arxiv.org/abs/2506.10274</guid>
<content:encoded><![CDATA[
Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks.They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 21:35:43 GMT</pubDate>
</item>
<item>
<title>高效探针方法在自监督学习中的性能提升研究</title>
<link>https://arxiv.org/abs/2506.10178</link>
<guid>https://arxiv.org/abs/2506.10178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多查询交叉注意力机制，在多个基准测试中超越传统线性及现有注意力探针方法。</p><br /><br /><p><strong>摘要：</strong> 随着大规模微调变得不切实际，探针成为自监督学习评估的主要协议。然而标准线性探针难以充分反映掩码图像建模训练模型的潜力，因此激发了对关注探针的需求。尽管关注探针逐渐被采用，但其过度参数化和计算效率低的问题仍未解决。本文通过精度与效率权衡的角度重新审视关注探针，系统分析现有方法并引入高效探针（EP），该机制通过消除冗余投影和减少可训练参数数量实现高达10倍的速度提升。EP在七个基准测试中优于线性探针和先前的方法，同时在多样化预训练范式和低样本、分层设置中表现优异，且生成的注意力图具有可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 17:10:26 GMT</pubDate>
</item>
<item>
<title>面向文本恢复的图像修复方法研究</title>
<link>https://arxiv.org/abs/2506.09993</link>
<guid>https://arxiv.org/abs/2506.09993</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉与文本恢复的新任务及多任务扩散框架。</p><br /><br /><p><strong>摘要：</strong> 现有的基于扩散模型的图像修复方法虽在自然图像修复方面取得了成功，但在处理退化图像中的文本区域时往往表现不佳，容易产生看似合理但不正确的类似文本模式，即文本-图像幻觉现象。为解决这一问题，本文引入了一种新的任务——文本感知图像修复（TAIR），旨在同时恢复视觉内容和文本准确性。为此，我们构建了一个包含10万张高质量场景图像的大规模基准数据集SA-Text，这些图像密集标注了多样且复杂的文本实例。此外，我们还提出了一个名为TeReDiff的多任务扩散框架，将扩散模型的内部特征集成到文本检测模块中，使两者通过联合训练相互受益，从而提取丰富的文本表示，并将其作为后续去噪步骤的提示。大量实验表明，我们的方法在文本识别准确率上显著优于现有最先进的修复方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09993" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>高效激发语言模型推理能力的稀疏自编码调优方法</title>
<link>https://arxiv.org/abs/2506.09967</link>
<guid>https://arxiv.org/abs/2506.09967</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过稀疏自编码调优显著降低语言模型推理能力训练成本的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Resa的推理模型家族，该模型通过一种新颖且高效的稀疏自编码调优(SAE-Tuning)技术训练而成。该方法首先利用源模型训练一个稀疏自编码器以捕捉推理能力，然后借助此编码器引导标准监督微调过程，使目标模型获得类似推理能力，整个过程仅使用经过验证的问题-答案数据而无需任何推理痕迹。研究显示，当应用于某些基础模型后，在进一步强化学习(RL)微调之前，SAE-Tuning保留了超过97%的RL训练推理性能，同时将训练成本降低超过2000倍至约1美元，训练时间缩短超过450倍至约20分钟。此外，当应用于轻度RL训练模型时，它能在AIME24上实现43.33%的Pass@1分数，在AMC23上实现90%的Pass@1分数，只需增加约1美元的成本。令人惊讶的是，通过SAE提取的推理能力可能具有通用性和模块化特性，即从某一数据集提取的能力仍可提升更大重叠语料库的表现，并且可以从Qwen或Qwen-Math等模型中提取的能力在测试时附加到R1-Distill模型上，无需重新训练即可获得相似收益。广泛的消融实验验证了这些发现，所有成果均已完全开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09967" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:44:01 GMT</pubDate>
</item>
<item>
<title>UniPre3D：一种适用于任意尺度点云的统一预训练方法</title>
<link>https://arxiv.org/abs/2506.09952</link>
<guid>https://arxiv.org/abs/2506.09952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个可无缝应用于任意规模点云的统一预训练方法UniPre3D。</p><br /><br /><p><strong>摘要：</strong> 点云数据的尺度多样性对3D视觉统一表征学习技术的发展提出了重大挑战。目前缺乏统一的3D模型，且现有预训练方法对物体级和场景级点云的效果不均衡。本文介绍UniPre3D，这是一种针对任意尺度和架构的3D模型设计的统一预训练方法。UniPre3D通过预测高斯基元作为预训练任务，并利用可微分高斯散射渲染图像，实现精确的像素级监督和端到端优化。此外，该方法结合预训练图像模型的2D特征，引入成熟的纹理知识，进一步规范预训练任务并引导模型关注几何结构。实验验证表明，UniPre3D在多种物体级和场景级任务中表现出了普遍有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:23:21 GMT</pubDate>
</item>
<item>
<title>结合规则与大模型推理的指令跟随强化学习</title>
<link>https://arxiv.org/abs/2506.09942</link>
<guid>https://arxiv.org/abs/2506.09942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合代码验证与大语言模型验证的强化学习方法提升指令跟随性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了指令跟随领域中强化学习验证挑战，并提出了名为VerIF的新方法，该方法融合了基于规则的代码验证与基于大型推理模型（如QwQ-32B）的语言模型验证。为了支持这一方法，我们构建了一个高质量的指令跟随数据集VerInstruct，包含约22,000个实例及其对应的验证信号。通过在两个模型上应用带有VerIF的强化学习训练，我们在多个代表性指令跟随基准测试中取得了显著改进。训练后的模型在相同规模的模型中达到最先进的性能，并且对未见过的约束条件具有良好的泛化能力。此外，观察到这些模型的整体能力未受负面影响，表明VerIF可以整合到现有的强化学习框架中以提高整体模型性能。我们已公开数据集、代码和模型以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:10:36 GMT</pubDate>
</item>
<item>
<title>ReasonMed：大规模医学推理数据集推动LLMs在医疗问答中的性能提升</title>
<link>https://arxiv.org/abs/2506.09513</link>
<guid>https://arxiv.org/abs/2506.09513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ReasonMed数据集，显著提升LLMs在医学推理和问答中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管基于推理的大语言模型（LLMs）在数学和编程领域表现出色，但它们在知识密集型医学问答中的能力尚未得到充分探索。为此，我们引入了ReasonMed，这是目前最大的医学推理数据集，包含从170万个初始推理路径中提取的37万高质量示例。通过多智能体验证和优化过程，设计了一个错误修正器来改进推理路径，纠正由验证器标记的错误步骤。利用ReasonMed，我们系统地研究了训练医学推理模型的最佳实践，发现结合详细的因果推理和简洁的答案总结是最有效的微调策略。基于此策略，我们训练了ReasonMed-7B模型，在PubMedQA上超过了之前最好的模型，并且超过了更大的LLaMA3.1-70B模型。这一成果为医学领域的LLMs发展设立了新的基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 04:36:55 GMT</pubDate>
</item>
<item>
<title>Ming-Omni：一种支持多模态处理与生成的统一模型</title>
<link>https://arxiv.org/abs/2506.09344</link>
<guid>https://arxiv.org/abs/2506.09344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Omni是一种能够处理多种模态并生成高质量图像和语音的统一模型。</p><br /><br /><p><strong>摘要：</strong> Ming-Omni是一个创新的多模态统一模型，它不仅能够处理图像、文本、音频和视频，还具备强大的语音和图像生成能力。该模型通过专用编码器从不同模态中提取特征，并利用Ling这一混合专家架构结合模态特定路由进行高效处理与融合。与传统多模态模型相比，Ming-Omni的独特之处在于支持音频和图像的生成任务，这得益于先进的音频解码器和Ming-Lite-Uni图像生成模块的支持。此外，该模型还能实现上下文感知聊天、文本转语音转换以及多样化的图像编辑功能。实验结果表明，Ming-Omni在多模态感知与生成方面表现卓越。值得一提的是，这是首个我们所知开源且在模态支持上可媲美GPT-4o的模型，我们已公开所有代码和模型权重，旨在推动相关领域的进一步研究与发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 22:50:49 GMT</pubDate>
</item>
<item>
<title>Token Perturbation Guidance提升扩散模型生成质量</title>
<link>https://arxiv.org/abs/2506.10036</link>
<guid>https://arxiv.org/abs/2506.10036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练且适用于条件与非条件生成的Token Perturbation Guidance方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对分类器自由引导（CFG）在现代扩散模型中的局限性，如特定训练需求及仅限条件生成，提出了一种名为Token Perturbation Guidance (TPG)的新方法。TPG通过直接对扩散网络内的中间token表示应用扰动矩阵，采用保范洗牌操作提供有效的稳定引导信号，从而提升生成质量而无需架构改动。作为一种无需训练且与输入条件无关的方法，TPG可应用于条件和非条件生成。实验表明，在SDXL和Stable Diffusion 2.1上，TPG在无条件生成中的FID得分较SDXL基线提升了近两倍，同时在提示对齐方面接近CFG的效果，确立了TPG作为通用且无条件依赖的引导方法的地位。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.10036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:25:46 GMT</pubDate>
</item>
<item>
<title>Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title>
<link>https://arxiv.org/abs/2506.09250</link>
<guid>https://arxiv.org/abs/2506.09250</guid>
<content:encoded><![CDATA[
Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit "accuracy collapse" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments systematically exceed model output token limits at reported failure points, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N &gt; 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 17:16:53 GMT</pubDate>
</item>
<item>
<title>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</title>
<link>https://arxiv.org/abs/2506.08862</link>
<guid>https://arxiv.org/abs/2506.08862</guid>
<content:encoded><![CDATA[
Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is crucial for numerous real-world applications. However, existing methods struggle to jointly address three key challenges: 1) processing uncalibrated inputs in real time, 2) accurately modeling dynamic scene evolution, and 3) maintaining long-term stability and computational efficiency. To this end, we introduce StreamSplat, the first fully feed-forward framework that transforms uncalibrated video streams of arbitrary length into dynamic 3D Gaussian Splatting (3DGS) representations in an online manner, capable of recovering scene dynamics from temporally local observations. We propose two key technical innovations: a probabilistic sampling mechanism in the static encoder for 3DGS position prediction, and a bidirectional deformation field in the dynamic decoder that enables robust and efficient dynamic modeling. Extensive experiments on static and dynamic benchmarks demonstrate that StreamSplat consistently outperforms prior works in both reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams. Code and models are available at https://github.com/nickwzk/StreamSplat.
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 10:52:36 GMT</pubDate>
</item>
<item>
<title>Draft-based Approximate Inference for LLMs</title>
<link>https://arxiv.org/abs/2506.08373</link>
<guid>https://arxiv.org/abs/2506.08373</guid>
<content:encoded><![CDATA[
Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 22:37:46 GMT</pubDate>
</item>
<item>
<title>Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2506.08234</link>
<guid>https://arxiv.org/abs/2506.08234</guid>
<content:encoded><![CDATA[
Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 17:04:14 GMT</pubDate>
</item>
<item>
<title>LLM Unlearning Should Be Form-Independent</title>
<link>https://arxiv.org/abs/2506.07795</link>
<guid>https://arxiv.org/abs/2506.07795</guid>
<content:encoded><![CDATA[
Large Language Model (LLM) unlearning aims to erase or suppress undesirable knowledge within the model, offering promise for controlling harmful or private information to prevent misuse. However, recent studies highlight its limited efficacy in real-world scenarios, hindering practical adoption. In this study, we identify a pervasive issue underlying many downstream failures: the effectiveness of existing unlearning methods heavily depends on the form of training samples and frequently fails to generalize to alternate expressions of the same knowledge. We formally characterize this problem as Form-Dependent Bias and systematically investigate its specific manifestation patterns across various downstream tasks. To quantify its prevalence and support future research, we introduce ORT, a novel benchmark designed to evaluate the robustness of unlearning methods against variations in knowledge expression. Results reveal that Form-Dependent Bias is both widespread and severe among current techniques.   We argue that LLM unlearning should be form-independent to address the endless forms of downstream tasks encountered in real-world security-critical scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR), a novel training-free method, as a promising solution path. ROCR performs unlearning by targeting the invariants in downstream tasks, specifically the activated dangerous concepts. It is capable of modifying model parameters within seconds to redirect the model's perception of a specific unlearning target concept to another harmless concept. Extensive experiments demonstrate that ROCR significantly improves unlearning effectiveness compared to traditional methods while generating highly natural outputs.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:21:25 GMT</pubDate>
</item>
<item>
<title>Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques</title>
<link>https://arxiv.org/abs/2506.08060</link>
<guid>https://arxiv.org/abs/2506.08060</guid>
<content:encoded><![CDATA[
Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size Oleft( m V{varepsilon^2} log m{delta} right) or, with bounded context, Oleft( l log V{varepsilon^2} log 1{delta} right) suffice to approximate fine-tuned behavior across m contexts within error varepsilon, where V is the vocabulary size and delta is the failure probability. For linear classification, datasets of size Oleft( d{varepsilon} right) or, with fixed context, Oleft( 1{varepsilon^2} log 1{delta} right) are sufficient, where d is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:37:19 GMT</pubDate>
</item>
<item>
<title>LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer</title>
<link>https://arxiv.org/abs/2506.06952</link>
<guid>https://arxiv.org/abs/2506.06952</guid>
<content:encoded><![CDATA[
Recent advances in multimodal foundation models unifying image understanding and generation have opened exciting avenues for tackling a wide range of vision-language tasks within a single framework. Despite progress, existing unified models typically require extensive pretraining and struggle to achieve the same level of performance compared to models dedicated to each task. Additionally, many of these models suffer from slow image generation speeds, limiting their practical deployment in real-time or resource-constrained settings. In this work, we propose Layerwise Timestep-Expert Flow-based Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image understanding and generation within a single multimodal model. LaTtE-Flow builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong multimodal understanding capabilities, and extends them with a novel Layerwise Timestep Experts flow-based architecture for efficient image generation. LaTtE-Flow distributes the flow-matching process across specialized groups of Transformer layers, each responsible for a distinct subset of timesteps. This design significantly improves sampling efficiency by activating only a small subset of layers at each sampling timestep. To further enhance performance, we propose a Timestep-Conditioned Residual Attention mechanism for efficient information reuse across layers. Experiments demonstrate that LaTtE-Flow achieves strong performance on multimodal understanding tasks, while achieving competitive image generation quality with around 6x faster inference speed compared to recent unified multimodal models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 20:15:32 GMT</pubDate>
</item>
<item>
<title>What Makes a Good Natural Language Prompt?</title>
<link>https://arxiv.org/abs/2506.06950</link>
<guid>https://arxiv.org/abs/2506.06950</guid>
<content:encoded><![CDATA[
As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 19:19:27 GMT</pubDate>
</item>
<item>
<title>Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning</title>
<link>https://arxiv.org/abs/2506.06694</link>
<guid>https://arxiv.org/abs/2506.06694</guid>
<content:encoded><![CDATA[
Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models.
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 03:19:11 GMT</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 18:16:16 GMT</pubDate>
</item>
<item>
<title>MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks</title>
<link>https://arxiv.org/abs/2506.05982</link>
<guid>https://arxiv.org/abs/2506.05982</guid>
<content:encoded><![CDATA[
As automated attack techniques rapidly advance, CAPTCHAs remain a critical defense mechanism against malicious bots. However, existing CAPTCHA schemes encompass a diverse range of modalities -- from static distorted text and obfuscated images to interactive clicks, sliding puzzles, and logic-based questions -- yet the community still lacks a unified, large-scale, multimodal benchmark to rigorously evaluate their security robustness. To address this gap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking suite that integrates heterogeneous CAPTCHA types into a single evaluation protocol. Leveraging a shared vision-language model backbone, we fine-tune specialized cracking agents for each CAPTCHA category, enabling consistent, cross-modal assessments. Extensive experiments reveal that MCA-Bench effectively maps the vulnerability spectrum of modern CAPTCHA designs under varied attack settings, and crucially offers the first quantitative analysis of how challenge complexity, interaction depth, and model solvability interrelate. Based on these findings, we propose three actionable design principles and identify key open challenges, laying the groundwork for systematic CAPTCHA hardening, fair benchmarking, and broader community collaboration. Datasets and code are available online.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:02:01 GMT</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation中的知识冲突类型及其处理研究</title>
<link>https://arxiv.org/abs/2506.08500</link>
<guid>https://arxiv.org/abs/2506.08500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新型知识冲突分类法并构建首个高质量基准CONFLICTS，评估大型语言模型处理冲突信息的能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval Augmented Generation (RAG) 是一种常见的方法，用于通过相关且最新的信息增强大型语言模型 (LLMs)，但检索到的来源经常包含矛盾信息，模型如何解决这些矛盾仍不清楚。本研究首先提出了RAG中知识冲突类型的新型分类法，并定义了每种类型期望的模型行为。随后，引入了CONFLICTS，这是一个具有专家标注的高质量基准，在现实RAG环境中对冲突类型进行注释。CONFLICTS是首个能够追踪模型在处理多种知识冲突方面进展的基准。我们对该基准进行了广泛的实验，结果显示LLMs往往难以适当解决来源之间的冲突。虽然提示LLMs显式推理检索文档中的潜在冲突显著提高了响应质量和适当性，但仍存在很大的改进空间，未来研究仍有大量工作待完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 02:52:57 GMT</pubDate>
</item>
<item>
<title>Institutional Books 1.0：基于哈佛图书馆公共领域书籍的历史文本数据集发布</title>
<link>https://arxiv.org/abs/2506.08300</link>
<guid>https://arxiv.org/abs/2506.08300</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍基于哈佛图书馆公共领域书籍的数据集Institutional Books 1.0。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，高质量训练数据的稀缺性成为关注焦点。本技术报告介绍了Institutional Books 1.0，这是一个由哈佛图书馆参与Google Books项目（自2006年开始）数字化的大量公共领域书籍组成的数据集。我们与哈佛图书馆合作，从原始扫描的1,075,899卷书中提取并处理出包含约2500亿标记的历史文本数据集，其中983,004卷书（约2420亿标记）的OCR提取文本及元数据已公开。该数据集涵盖超过250种语言，旨在提升历史文本对人类和机器的可访问性和可用性。报告详细描述了项目的背景、目标、方法以及分析结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08300" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 20:11:30 GMT</pubDate>
</item>
<item>
<title>Mirage：基于音频生成高质量视频的统一模型</title>
<link>https://arxiv.org/abs/2506.08279</link>
<guid>https://arxiv.org/abs/2506.08279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型Mirage，可从音频输入生成逼真的视频。</p><br /><br /><p><strong>摘要：</strong> 当前视频生成方法要么忽略声音专注于无声图像生成，要么关注视听元素但局限于特定应用领域。本文介绍的Mirage是一种音频到视频的基础模型，能从零开始根据音频输入生成真实且富有表现力的输出图像。当与现有的语音合成方法结合时，Mirage可以生成引人注目的多模态视频。其核心技术贡献是一种统一的自注意力机制音频到视频生成模型训练方法，无论是从头开始还是基于已有权重，都能保持模型的通用性并提升输出质量。该模型在处理人物演讲场景时尤其有效，能够生成可信的表演解读视频。我们鼓励读者亲自观看和聆听Mirage的成果（详见论文和链接）。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 18:56:02 GMT</pubDate>
</item>
<item>
<title>测试时间交互扩展提升智能体性能</title>
<link>https://arxiv.org/abs/2506.07976</link>
<guid>https://arxiv.org/abs/2506.07976</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的测试时间扩展维度——测试时间交互，显著提升智能体性能。</p><br /><br /><p><strong>摘要：</strong> 当前测试时间扩展范式依赖于生成长推理路径以增加思考深度，但不支持智能体实时获取环境信息或动态调整行为。本研究引入测试时间交互这一新维度，使智能体能在单次运行中进行探索、回溯及动态重规划等复杂行为。通过在网页代理领域验证，即使没有训练，基于提示的交互扩展也能显著提高任务成功率。进一步提出TTI方法，采用基于课程的在线强化学习策略，根据智能体的表现动态调整其交互长度。实验表明，TTI在WebVoyager和WebArena基准上达到了开源开放数据集的最佳性能，并展示了智能体在探索与利用之间的自适应平衡能力。测试时间交互扩展为提升智能体适应性提供了全新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07976" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:02 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的可解释AI生成图像检测</title>
<link>https://arxiv.org/abs/2506.07045</link>
<guid>https://arxiv.org/abs/2506.07045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">优化后的多模态大语言模型能有效检测AI生成图像并提供合理解释。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着图像生成技术的飞速发展，对可解释且鲁棒的检测方法需求增加。尽管现有方法精度较高，但通常缺乏透明性，无法为人所理解。多模态大语言模型虽非专门设计用于伪造检测，但具备强大的分析推理能力。通过适当微调，这些模型能够有效识别AI生成图像并提供有意义的解释。然而，当前的多模态大语言模型仍存在幻觉问题，其视觉解释难以与实际图像内容及人类推理保持一致。为此，我们构建了一个包含标注边界框和描述性字幕的数据集，用于突出合成伪影，奠定人类对齐的视觉文本推理基础。随后，我们采用多阶段优化策略，逐步平衡准确检测、视觉定位和连贯文本解释的目标。最终模型在检测AI生成图像和定位视觉缺陷方面表现出色，显著优于基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:44 GMT</pubDate>
</item>
<item>
<title>MMRefine：多模态大语言模型误差精化能力评估基准</title>
<link>https://arxiv.org/abs/2506.04688</link>
<guid>https://arxiv.org/abs/2506.04688</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMRefine基准，评估多模态大语言模型的误差检测与修正能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMRefine的多模态精化基准，用于评估多模态大型语言模型（MLLMs）的误差精化能力。与以往仅关注最终准确率不同，MMRefine通过六个特定场景评估MLLMs的错误检测和纠正能力，并将错误分类为六种类型进行性能分析。实验表明，不同开放及闭源MLLMs存在改进瓶颈，揭示了提升推理效果的关键领域。该基准代码和数据集已公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04688" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 03:11:36 GMT</pubDate>
</item>
<item>
<title>基于查询聚焦的定量关键点摘要模型QQSUM-RAG</title>
<link>https://arxiv.org/abs/2506.04020</link>
<guid>https://arxiv.org/abs/2506.04020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QQSUM任务及模型，改进产品问答系统对客户意见多样性的捕捉能力。</p><br /><br /><p><strong>摘要：</strong> 现有基于评论的产品问答(PQA)系统因仅提供单一视角而难以反映客户意见的多样性。本文引入Quantitative Query-Focused Summarization(QQSUM)任务，旨在通过量化意见的流行度来生成具有代表性的关键点摘要，从而有效回答用户查询。尽管检索增强生成(RAG)方法在PQA中有一定潜力，但其生成的答案仍无法充分涵盖多样的观点。为解决这一问题，我们提出了QQSUM-RAG模型，该模型通过少量样本学习同时训练关键点导向的检索器和关键点摘要生成器，从而实现能够捕捉多样化且代表性观点的关键点摘要。实验结果表明，QQSUM-RAG在文本质量和意见量化准确性方面均优于最先进的RAG基线模型。相关源代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:50:32 GMT</pubDate>
</item>
<item>
<title>PlayerOne：首个第一人称现实世界模拟器</title>
<link>https://arxiv.org/abs/2506.09995</link>
<guid>https://arxiv.org/abs/2506.09995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlayerOne可精准构建虚拟世界并生成与真实场景匹配的第一人称视频。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为PlayerOne的创新性第一人称现实世界模拟器，它能够在动态环境中实现沉浸式且无限制的探索。该系统通过输入用户的第一人称场景图像，能够精确构建对应的虚拟世界，并生成与用户实际动作严格对齐的第一人称视频。PlayerOne采用粗到细的训练管道，在大规模第一人称文本-视频对上进行预训练，随后利用同步运动-视频数据进行微调，这些数据来自由自动构建管道提取的内外视角视频数据集。此外，为了更好地控制不同部位的动作，设计了一种部分解耦的动作注入方案，并开发了一种联合重建框架，逐步建模4D场景及视频帧，保证长时间视频生成的一致性。实验结果显示，PlayerOne在控制多样化场景中的各种人体动作方面表现出强大的泛化能力，标志着第一人称现实世界模拟领域的首次尝试，为世界建模及其广泛应用开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>基于多模态条件的端到端人体动画生成框架</title>
<link>https://arxiv.org/abs/2506.09984</link>
<guid>https://arxiv.org/abs/2506.09984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架实现多概念人体为中心的高质量可控视频生成。</p><br /><br /><p><strong>摘要：</strong> 近年来，结合文本、图像和音频等多种模态条件的人体动画取得了显著进展。然而，大多数现有方法仅限于单一主体动画，并以全局方式注入条件，忽视了多概念在同一视频中的丰富交互场景。这种局限性阻碍了对人类及物体的精确个性化控制。本研究摒弃单实体假设，提出了一种新颖框架，通过区域特定绑定模态条件至各身份的空间时间足迹，实现了多概念的精确控制。实验结果和消融研究表明，该方法在多模态条件下的显式布局控制优于隐式方法及其他现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 13:57:09 GMT</pubDate>
</item>
<item>
<title>SAFE：面向视觉语言动作模型的多任务故障检测器</title>
<link>https://arxiv.org/abs/2506.09937</link>
<guid>https://arxiv.org/abs/2506.09937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种适用于多种任务的故障检测器SAFE，用于提升机器人在新任务中的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对视觉语言动作模型（VLAs）在新任务上的有限成功率问题，引入多任务故障检测问题，并设计了一种名为SAFE的故障检测器。SAFE利用VLAs内部特征，通过学习任务成功与失败的高阶知识，在模拟和真实环境中对多种策略架构进行测试，展现出卓越的故障检测性能及准确性和检测时间的最佳权衡。SAFE在多个任务上优于现有基线方法，为机器人在未知环境中的安全交互提供了保障。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 12:59:13 GMT</pubDate>
</item>
<item>
<title>ComfyUI-R1：首个用于自动化工作流生成的大规模推理模型</title>
<link>https://arxiv.org/abs/2506.09790</link>
<guid>https://arxiv.org/abs/2506.09790</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模推理模型ComfyUI-R1，显著提升AI艺术创作的工作流生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ComfyUI-R1的新模型，该模型是首个专门用于自动化工作流生成的大规模推理模型。通过利用精心策划的数据集中的4000个工作流，我们构建了包含节点选择、工作流规划及代码级工作流表示在内的长链推理数据。ComfyUI-R1采用了两阶段框架进行训练：首先通过冷启动的链式思维微调来适应ComfyUI领域；然后利用细粒度规则-指标混合奖励机制下的强化学习，以激励推理能力并确保格式的有效性、结构的完整性以及节点级别的准确性。实验结果显示，我们的7B参数模型达到了97%的格式有效性率，并且在节点级别和图级别F1得分上表现出色，明显优于使用GPT-4o和Claude系列等领先闭源模型的方法。进一步分析表明推理过程的重要性以及将工作流转化为代码的优势。定性比较显示，我们在合成复杂且多样化的节点组合工作流方面具有优势，这突显了长链推理在AI艺术创作中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09790" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 11 Jun 2025 10:35:15 GMT</pubDate>
</item>
<item>
<title>Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation</title>
<link>https://arxiv.org/abs/2506.09350</link>
<guid>https://arxiv.org/abs/2506.09350</guid>
<content:encoded><![CDATA[
Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 23:04:23 GMT</pubDate>
</item>
<item>
<title>Seedance 1.0：高效高质量视频生成基础模型</title>
<link>https://arxiv.org/abs/2506.09113</link>
<guid>https://arxiv.org/abs/2506.09113</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedance 1.0通过多源数据精炼等技术实现了高效高质量视频生成。</p><br /><br /><p><strong>摘要：</strong> Seedance 1.0是一种高性能且推理高效的视频生成基础模型，针对当前扩散模型在视频生成中的挑战进行了多项技术改进。首先，它通过多源数据精炼结合精确的视频描述，实现跨多样化场景的全面学习；其次，采用高效的架构设计和训练范式，支持多镜头生成并联合处理文本到视频和图像到视频任务；第三，利用细粒度监督微调和多维奖励机制优化后训练，显著提升性能；最后，通过多阶段蒸馏策略和系统级优化，实现约10倍的推理加速。实验结果显示，Seedance 1.0仅需41.4秒即可生成1080p分辨率的5秒视频（基于NVIDIA-L20）。相较于现有最先进模型，该模型在时空流畅性、结构稳定性、复杂多主体场景下的指令遵循能力以及多镜头叙事连贯性方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09113" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:56:11 GMT</pubDate>
</item>
<item>
<title>Branched Schrödinger Bridge Matching：多模态分布转换的新框架</title>
<link>https://arxiv.org/abs/2506.09007</link>
<guid>https://arxiv.org/abs/2506.09007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法BranchSBM，用于多路径分布转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Branched Schrödinger Bridge Matching（BranchSBM）的新框架，该框架通过参数化多个时间相关的速度场和增长过程，解决了现有方法无法处理多模态分布转换的问题。BranchSBM不仅提高了表达能力，还在多路径表面导航、细胞命运分支建模以及细胞对扰动的发散响应模拟等任务中展现出其必要性。传统方法如流匹配和Schrödinger桥接匹配仅能处理单模态转换，而BranchSBM可以捕捉从共同起点到多个不同终点的分支或发散演化过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:29:48 GMT</pubDate>
</item>
<item>
<title>基于测试驱动开发的数据合成框架SWE-Flow</title>
<link>https://arxiv.org/abs/2506.09003</link>
<guid>https://arxiv.org/abs/2506.09003</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于测试驱动开发的新数据合成框架SWE-Flow。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SWE-Flow的新型数据合成框架，该框架以测试驱动开发（TDD）为基础。与依赖人工提交问题的传统软件工程数据不同，SWE-Flow通过自动推断单元测试中的增量开发步骤来生成数据，这些测试本身就包含了高层次的需求。SWE-Flow的核心是构建运行时依赖图（RDG），它精确捕捉函数间的交互，从而生成结构化的逐步开发计划。每一步骤都会生成部分代码库、对应的单元测试及必要的代码修改，形成可验证的TDD任务。通过此方法，我们从GitHub的真实项目中生成了16,061个训练实例和2,020个测试实例，创建了SWE-Flow-Eval基准。实验表明，在该数据集上微调开源模型显著提升了基于TDD的编码性能。为了促进进一步研究，所有代码、数据集、模型和Docker镜像均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09003" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:23:33 GMT</pubDate>
</item>
<item>
<title>SeerAttention-R：面向推理模型长解码的稀疏注意力框架</title>
<link>https://arxiv.org/abs/2506.08889</link>
<guid>https://arxiv.org/abs/2506.08889</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeerAttention-R通过自蒸馏门机制实现高效稀疏注意力，适用于长序列推理任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeerAttention-R的稀疏注意力框架，专为推理模型的长序列解码设计。该框架继承了SeerAttention的设计，通过自蒸馏门机制学习注意力稀疏性，并去除了查询池化以支持自回归解码。它轻量且易于集成到现有的预训练模型中，无需修改原始参数。实验表明，在AIME基准测试中，SeerAttention-R仅需0.4Btokens即可在大稀疏注意力块大小下保持接近无损的推理精度。此外，利用TileLang开发的优化稀疏解码内核，在H100 GPU上实现了高达9倍的速度提升。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08889" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:17:26 GMT</pubDate>
</item>
<item>
<title>POET：一种基于正交等价变换的大规模语言模型训练算法</title>
<link>https://arxiv.org/abs/2506.08001</link>
<guid>https://arxiv.org/abs/2506.08001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法POET，通过优化神经元显著提升大规模语言模型的训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对有效且可靠地训练大型语言模型这一人工智能领域的重大挑战，提出了一种名为POET的新算法。该算法采用正交等价变换对神经元进行重新参数化，具体而言，每个神经元通过两个可学习的正交矩阵和一个固定的随机权重矩阵表示。由于其在谱性质上的保真性，POET能够在稳定优化目标函数的同时提高泛化能力。此外，还开发了高效的近似方法，使POET适用于大规模神经网络的灵活且可扩展的训练。大量实验验证了POET在训练大型语言模型方面的有效性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于自我信心的大语言模型后训练强化学习方法</title>
<link>https://arxiv.org/abs/2506.06395</link>
<guid>https://arxiv.org/abs/2506.06395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需标注的强化学习方法RLSC，显著提升数学推理任务性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理方面表现出色，但后训练过程对调整其行为以符合任务目标至关重要。现有的强化学习（RL）方法通常依赖昂贵的人类注释或外部奖励模型。我们提出了通过自我信心进行强化学习（RLSC），利用模型自身的置信度作为奖励信号，从而避免了标签、偏好模型或奖励工程的需求。将RLSC应用于Qwen2.5-Math-7B模型，在每个问题仅使用16个样本且训练步骤为10或20的情况下，该方法在AIME2024上提升了13.4%，在MATH500上提升了21.2%，在Minerva Math上提升了21.7%，在Olympiadbench上提升了20.8%，在AMC23上提升了9.7%的准确性。RLSC提供了一种简单且可扩展的后训练方法，仅需少量样本和无标注监督即可优化推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:55:15 GMT</pubDate>
</item>
<item>
<title>引入Autoregressive Semantic Visual Reconstruction提升多模态理解</title>
<link>https://arxiv.org/abs/2506.09040</link>
<guid>https://arxiv.org/abs/2506.09040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ASVR模型，通过联合学习视觉与文本模态提高多模态理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型视觉语言模型（LVLMs）主要依赖文本序列的自回归监督，未能充分整合视觉模态，导致无法有效利用无描述图片、遗漏关键视觉细节及难以通过文本传达特定视觉内容的问题。本研究引入Autoregressive Semantic Visual Reconstruction（ASVR），在一个统一的自回归框架下实现视觉与文本模态的联合学习。实验表明，直接自回归重建图像原始外观可能削弱多模态理解，而基于语义表示的重建显著提升性能。ASVR在多种数据规模和大语言模型骨干上均有显著改进，在14个多模态基准测试中使LLaVA-1.5平均得分提升了5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.09040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 13:57:50 GMT</pubDate>
</item>
<item>
<title>基于强化学习的小型规则推理模型的高效方法</title>
<link>https://arxiv.org/abs/2506.08672</link>
<guid>https://arxiv.org/abs/2506.08672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型强化规则推理方法，显著提升小模型在多任务中的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于规则推理的挑战，特别是针对实际应用中规则格式、类型和复杂性的变化带来的困难。虽然大型推理模型在强化学习辅助下表现出色，但小型推理模型是否能有效学习并具有跨任务和领域的稳健泛化能力尚不清楚。为解决此问题，我们提出了RuleReasoner，这是一种通过精心策划的任务集和新颖的领域感知动态采样方法实现规则推理的方法。实验表明，该方法在分布内和分布外基准测试中均优于现有前沿方法，同时具有更高的计算效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 06:31:21 GMT</pubDate>
</item>
<item>
<title>Mathesis：结合强化学习的端到端数学定理证明系统</title>
<link>https://arxiv.org/abs/2506.07047</link>
<guid>https://arxiv.org/abs/2506.07047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mathesis首次实现从自然语言问题到形式化证明的全流程自动化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Mathesis的新方法，它是一个端到端的数学定理证明流水线，能够处理非正式的问题陈述并将其自动形式化。Mathesis的关键组成部分包括Mathesis-Autoformalizer，这是一个使用强化学习增强自然语言问题形式化的工具，并通过LeanScorer框架评估形式化质量。此外，还提出了一种Mathesis-Prover，用于从形式化陈述生成形式化证明。为了评估该方法的实际应用价值，我们创建了一个名为Gaokao-Formal的新基准测试集，其中包含来自中国高考的488个复杂问题。实验结果显示，Mathesis-Autoformalizer在Gaokao-Formal上的通过率比最佳基线高出22%，整个系统在MiniF2F上达到64%的准确率，在Gaokao-Formal上实现了18%的最新性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 05:04:14 GMT</pubDate>
</item>
<item>
<title>DiscoVLA：针对视频文本检索的CLIP参数高效适配方法</title>
<link>https://arxiv.org/abs/2506.08887</link>
<guid>https://arxiv.org/abs/2506.08887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiscoVLA方法，同时减少图像到视频的视觉、语言及对齐三方面差异。</p><br /><br /><p><strong>摘要：</strong> 本文研究了CLIP模型在视频文本检索中的参数高效适应问题，发现从图像级到视频级迁移时存在视觉、语言和对齐三大关键差异。现有方法主要关注视觉差异，忽视了语言和对齐方面的改进。为此，我们提出了DiscoVLA（Discrepancy Reduction in Vision, Language, and Alignment），通过引入图像-视频特征融合来解决视觉和语言差异，并利用伪图像描述生成学习细粒度的图像级对齐，同时采用图像到视频对齐蒸馏提升视频级对齐效果。实验表明，DiscoVLA在MSRVTT数据集上的R@1指标比现有方法高出1.5%，达到50.5%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 10 Jun 2025 11:16:40 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的新训练范式：Self Forcing</title>
<link>https://arxiv.org/abs/2506.08009</link>
<guid>https://arxiv.org/abs/2506.08009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Self Forcing方法解决视频生成中的曝光偏差问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self Forcing的新型训练范式，专门针对自回归视频扩散模型设计，旨在解决长期存在的曝光偏差问题。传统方法在推理过程中依赖于真实的上下文帧，而Self Forcing通过在训练时利用之前自动生成的输出进行条件生成，显著提高了生成序列的整体质量。该方法采用整体视频级别的损失函数，而非传统的逐帧目标函数，同时结合了几步扩散模型和随机梯度截断策略，有效平衡了计算成本与性能。此外，引入滚动键值缓存机制进一步提升了效率，使得该方法能够在单GPU上实现亚秒级延迟的实时流媒体视频生成，且生成质量可媲美甚至超越更为复杂和非因果的扩散模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Aligning Text, Images, and 3D Structure Token-by-Token</title>
<link>https://arxiv.org/abs/2506.08002</link>
<guid>https://arxiv.org/abs/2506.08002</guid>
<content:encoded><![CDATA[
Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed ''cookbook'' outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We extend our approach to reconstruct complex 3D object shapes by enriching our 3D modality with quantized shape encodings, and show our model's effectiveness on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:37 GMT</pubDate>
</item>
<item>
<title>Squeeze3D：基于隐式先验知识的高效3D数据压缩框架</title>
<link>https://arxiv.org/abs/2506.07932</link>
<guid>https://arxiv.org/abs/2506.07932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用已有3D生成模型的隐式先验知识进行高比率压缩的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Squeeze3D的创新框架，该框架通过可训练映射网络连接已有的编码器和生成模型的潜在空间，利用这些模型学习到的隐式先验知识实现对网格、点云及辐射场等3D数据的极高压缩比。实验表明，Squeeze3D在保持视觉质量的同时，对纹理网格的压缩比可达2187倍，对点云为55倍，对辐射场为619倍。此外，由于不涉及对象特定网络的训练，其压缩和解压延迟极小。Squeeze3D适用于多种现有预训练3D编码器和生成模型，并支持不同格式的3D数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:52:10 GMT</pubDate>
</item>
<item>
<title>大型语言模型在不等式证明中的挑战与研究进展</title>
<link>https://arxiv.org/abs/2506.07927</link>
<guid>https://arxiv.org/abs/2506.07927</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法提升不等式证明能力，发现顶级模型在严谨推理上仍有显著差距。</p><br /><br /><p><strong>摘要：</strong> 不等式证明作为科学与数学领域的重要工具，考验着高级推理能力。然而，现有数据集的局限性阻碍了大型语言模型（LLMs）在此领域的进步。为此，研究者引入了一种新的任务形式，将不等式证明分解为可自动验证的子任务——边界估计与关系预测，并构建了一个名为IneqMath的专业级不等式数据集。该数据集包含丰富的解题步骤与定理注释，旨在推动模型性能提升。此外，研究设计了一种结合最终答案判断与逐步判断的评估框架，用于检测推理缺陷。对29个领先LLMs的测试显示，尽管顶级模型如o1在最终答案准确性上有一定表现，但在逐步推理验证下整体准确率不足10%，较仅考虑最终答案时下降了65.5%。这一发现揭示了当前LLMs在严谨证明构建上的脆弱性和局限性。研究进一步表明，单纯增加模型规模或计算资源并不能显著提高证明正确性，而是需要探索如定理引导推理及自我优化等新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07927" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:43:38 GMT</pubDate>
</item>
<item>
<title>Frame Guidance：无需训练的可控视频生成引导方法</title>
<link>https://arxiv.org/abs/2506.07177</link>
<guid>https://arxiv.org/abs/2506.07177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于帧级信号的无需训练的可控视频生成引导方法Frame Guidance。</p><br /><br /><p><strong>摘要：</strong> 扩散模型的进步显著提升了视频质量，但许多现有方法依赖于针对特定任务微调大规模视频模型，随着模型规模增长变得不切实际。本文介绍Frame Guidance，这是一种无需训练即可实现可控视频生成的引导方法，基于帧级信号如关键帧、风格参考图像、草图或深度图等。为实现实用的无需训练引导，我们提出了一种简单的潜在处理方法以大幅减少内存使用，并采用一种新颖的潜在优化策略以实现全局一致的视频生成。Frame Guidance能够在多种任务上有效控制，包括关键帧引导、风格化和循环生成，且无需任何训练，兼容所有视频模型。实验结果显示，该方法可以为广泛的任务和输入信号生成高质量的可控视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:54:41 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的地缘政治偏见研究</title>
<link>https://arxiv.org/abs/2506.06751</link>
<guid>https://arxiv.org/abs/2506.06751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现大型语言模型存在显著的地缘政治偏见，且简单去偏方法效果有限。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在解释具有国家视角冲突的历史事件时所表现出的地缘政治偏见（涉及美国、英国、苏联和中国）。我们构建了一个包含中立事件描述及多国对立观点的新数据集，实验结果显示这些模型倾向于支持特定的国家叙事。尽管尝试了简单的去偏提示，但其对减少偏见的效果十分有限。此外，通过操控参与者标签的实验表明，模型对归因高度敏感，有时会放大偏见或检测到不一致性，尤其是在标签被替换的情况下。本研究揭示了LLMs中的国家叙事偏见，挑战了简单去偏方法的有效性，并为未来地缘政治偏见研究提供了框架和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 06:45:17 GMT</pubDate>
</item>
<item>
<title>异构Mixture-of-Adapters方法提升大语言模型参数高效微调性能</title>
<link>https://arxiv.org/abs/2506.05928</link>
<guid>https://arxiv.org/abs/2506.05928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出异构MoA方法解决现有MoE-LoRA的表征塌陷和负载不均问题。</p><br /><br /><p><strong>摘要：</strong> 最近的研究将低秩适配（LoRA）与专家混合（MoE）结合，进一步提升大语言模型（LLM）参数高效微调（PEFT）方法的性能。然而，现有的同构MoE-LoRA架构常面临表征塌陷和专家负载失衡的问题。为应对这些挑战，本文提出了异构Mixture-of-Adapter（MoA）方法，通过动态整合结构多样的PEFT适配器专家，利用互补的表示能力促进专家专业化，从而增强预训练知识向下游任务的有效迁移。MoA支持两种变体：Soft MoA实现细粒度集成，Sparse MoA则基于贡献稀疏激活适配器专家。实验表明，异构MoA在性能和参数效率上均优于同构MoE-LoRA方法。该项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:54:19 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的知识增强金融推理模型RKEFino1</title>
<link>https://arxiv.org/abs/2506.05700</link>
<guid>https://arxiv.org/abs/2506.05700</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合监管知识的金融推理模型RKEFino1，提升数字合规报告的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型(LLMs)在数字监管报告(DRR)中的应用挑战，提出了RKEFino1模型，该模型通过微调Fino1，在XBRL、CDM和MOF等领域的专业知识基础上进行增强。研究设计了基于知识的问答(QA)任务、数学推理QA任务以及覆盖句子和表格中财务实体的数值命名实体识别(NER)任务。实验结果表明，RKEFino1在合规相关的金融任务中表现出色且具有良好的泛化能力。目前，该模型已发布在Hugging Face平台上供公众使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05700" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 23:02:52 GMT</pubDate>
</item>
<item>
<title>基于证据性引导的检索增强生成模型ECoRAG</title>
<link>https://arxiv.org/abs/2506.05167</link>
<guid>https://arxiv.org/abs/2506.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ECoRAG，提升LLMs在开放域问答中的性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在开放域问答（ODQA）中通过检索增强生成（RAG）利用外部文档表现优异，但现有压缩方法未能有效过滤非证据性信息，限制了性能。本文提出ECoRAG框架，通过基于证据性的文档压缩提升LLMs性能，确保答案生成由正确证据支持，并在必要时补充更多内容。实验表明，ECoRAG在多个ODQA任务中超越现有压缩方法，同时显著降低延迟并减少令牌使用量。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 11:43:49 GMT</pubDate>
</item>
<item>
<title>基于预操作批评机制的多模态大语言模型在GUI自动化中的应用</title>
<link>https://arxiv.org/abs/2506.04614</link>
<guid>https://arxiv.org/abs/2506.04614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的预操作批评机制显著提升了GUI自动化任务的成功率和效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）被广泛应用于图形用户界面（GUI）自动化等多模态推理任务。不同于一般的离线多模态任务，GUI自动化需要在线交互环境中逐步决策，对每一步决策的准确性要求较高。为了提高决策可靠性，我们引入了一种预操作批评机制，通过预测行动的潜在结果和正确性提供反馈。具体而言，提出了建议感知梯度相对策略优化（S-GRPO）策略构建预操作批评模型GUI-Critic-R1，并引入新颖的建议奖励以增强反馈的可靠性。此外，开发了一种基于推理引导的数据收集管道，创建GUI-Critic-Train和GUI-Critic-Test，填补了现有GUI批评数据的空白。静态实验表明，GUI-Critic-R1在批评准确性上优于当前的MLLMs；动态评估进一步证明了该模型在成功率和操作效率方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 00:12:36 GMT</pubDate>
</item>
<item>
<title>基于正交匹配追踪的预训练语言模型跨分词器移植方法</title>
<link>https://arxiv.org/abs/2506.06607</link>
<guid>https://arxiv.org/abs/2506.06607</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需微调的正交匹配追踪方法，实现大型语言模型的跨分词器迁移。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种无需微调的预训练大型语言模型（LLMs）的分词器移植方法，通过正交匹配追踪（OMP）重建未见令牌嵌入。该方法将词汇外令牌近似为共享令牌的稀疏线性组合，在两个具有挑战性的跨分词器任务中展示了最佳的零样本性能保存能力。与其他零样本方法相比，OMP在多个基准测试中表现出色，且有效解决了大分词器差异问题。此外，该技术支持预训练权重的直接重用，适用于跨分词器的知识蒸馏、推测解码、集成、合并及领域特定词汇适应。我们还将此方法集成到开源工具mergekit-tokensurgeon中，用于后处理词汇对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06607" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 20:51:27 GMT</pubDate>
</item>
<item>
<title>NetPress: Dynamically Generated LLM Benchmarks for Network Applications</title>
<link>https://arxiv.org/abs/2506.03231</link>
<guid>https://arxiv.org/abs/2506.03231</guid>
<content:encoded><![CDATA[
Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:04:22 GMT</pubDate>
</item>
<item>
<title>无需权重更新的动态视图合成</title>
<link>https://arxiv.org/abs/2506.08004</link>
<guid>https://arxiv.org/abs/2506.08004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过重新设计预训练视频扩散模型的噪声初始化阶段实现高保真动态视图合成。</p><br /><br /><p><strong>摘要：</strong> 本文将动态视图合成问题视为无监督训练中的逆问题，提出了一种不需权重更新或辅助模块的方法。我们首先指出由零终端信噪比（SNR）调度引起的确定性反转基本障碍，并通过引入新的噪声表示方法——K阶递归噪声表示（K-order Recursive Noise Representation）解决该问题。此表示法具有闭合形式表达，可精确高效地对齐VAE编码与DDIM反演潜在变量。此外，针对相机运动导致的新可见区域，我们提出随机潜在调制（Stochastic Latent Modulation），在潜在空间上进行基于可见性的采样以完成被遮挡区域。实验表明，通过结构化潜在变量操作可以在噪声初始化阶段有效执行动态视图合成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>tau^2-bench：引入双控环境评估对话AI代理</title>
<link>https://arxiv.org/abs/2506.07982</link>
<guid>https://arxiv.org/abs/2506.07982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准tau^2-bench通过双控环境模拟真实场景，评估对话AI代理的推理与用户引导能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对话AI代理评估基准多局限于单一控制环境，即仅AI代理可操作工具与外界交互，而用户作为被动的信息提供者。这种设置与现实世界中的应用场景（如技术支持）存在差异，在这些场景中用户需主动参与改变共享世界的状态。为弥合这一差距，本文提出了tau^2-bench，该基准具有四个关键贡献：首先，构建了一个新颖的电信双控领域，建模为Dec-POMDP，允许AI代理和用户共同利用工具在共享动态环境中行动，测试代理的协调与沟通能力；其次，开发了一种组合任务生成器，从原子组件编程生成多样化且可验证的任务，确保领域覆盖度和可控复杂性；第三，设计了一个紧密耦合环境的可靠用户模拟器，其行为受制于工具和可观测状态，提高仿真保真度；最后，通过多种消融分析细致评估代理性能，包括区分推理错误与沟通/协调错误。实验表明，当代理从无用户环境切换到双控环境时，性能显著下降，凸显出引导用户面临的挑战。总体而言，tau^2-bench为需要有效推理并引导用户行为的代理提供了受控测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:52:18 GMT</pubDate>
</item>
<item>
<title>CyberV：基于控制论的视频多模态大语言模型自适应框架</title>
<link>https://arxiv.org/abs/2506.07971</link>
<guid>https://arxiv.org/abs/2506.07971</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于控制论的视频多模态大语言模型自适应框架，显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大语言模型在处理长视频或复杂视频时面临计算需求高、鲁棒性差和准确性有限的问题，尤其是参数较少的模型。为解决这些问题，我们提出了名为CyberV的创新框架，该框架受到控制论原理的启发，将视频多模态大语言模型重新设计为具备自监测、自修正和动态资源分配能力的自适应系统。具体而言，CyberV引入了一个由多模态大语言模型推理系统、传感器和控制器组成的控制环路。传感器监控推理过程并收集中间解释，控制器则决定何时触发自修正并生成反馈以指导下一轮推理。此框架无需重新训练或添加额外组件即可增强冻结的多模态大语言模型。实验结果显示，在VideoMMMU基准上，CyberV使Qwen2.5-VL-7B提升了8.3%，InternVL3-8B提升了5.5%，甚至超过了竞争性的专有模型GPT-4o。当应用于Qwen2.5-VL-72B时，性能提升达10.0%，达到接近人类专家的表现。此外，该方法在VideoMME和WorldSense等通用基准测试中也显示出一致的改进，证明了其在提高动态视频理解方面模型的鲁棒性和准确性方面的有效性及泛化能力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07971" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:45:18 GMT</pubDate>
</item>
<item>
<title>SAFEFLOW：构建可信大型语言模型及视觉语言模型驱动代理的新框架</title>
<link>https://arxiv.org/abs/2506.07564</link>
<guid>https://arxiv.org/abs/2506.07564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SAFEFLOW框架，强化多模态代理的信息流控制与安全性。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）和视觉语言模型（VLMs）的进步推动了复杂推理与工具使用的自主代理的发展。然而，现有代理框架仍存在脆弱性问题，缺乏安全的信息流动机制、可靠性以及多代理协作能力。本文引入SAFEFLOW，一种协议级框架，用于构建可信的LLM/VLM驱动代理。该框架通过细粒度的信息流控制（IFC），精确追踪代理间交换数据的来源、完整性和保密性。此外，它通过事务执行、冲突解决和共享状态的安全调度，在并发多代理环境中确保全局一致性。为了增强鲁棒性，SAFEFLOW还引入了写前日志记录、回滚和安全缓存等机制。通过构建SAFEFLOWBENCH基准套件验证其性能，实验表明，即使在敌对环境下，基于SAFEFLOW的代理也能保持优异的任务表现和安全保障，显著优于当前最先进的方法。这一成果奠定了构建原则性、鲁棒且安全的代理生态系统的基石，推动可靠自治领域的前沿发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 05:04:37 GMT</pubDate>
</item>
<item>
<title>基于自适应提升循环的机器人视觉规划模型在线学习方法</title>
<link>https://arxiv.org/abs/2506.06658</link>
<guid>https://arxiv.org/abs/2506.06658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过自我适应迭代提升视频模型性能的方法解决未知任务。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于演示训练的视频生成模型在解决机器人任务时泛化能力不足的问题，提出了Self-Adapting Improvement Loop (SAIL) 方法。该方法利用互联网规模预训练的视频模型进行领域内适应，收集自我产生的轨迹数据，并迭代更新领域内视频模型，从而持续提升特定任务的性能。研究将SAIL应用于MetaWorld任务集及真实机械臂的两个操作任务中，发现即使初始领域内演示质量较低或未经过筛选的数据，模型仍能在多个迭代后显著改善对新任务的表现。这表明通过互联网规模数据的适应性学习与在线经验累积，可以逐步构建高性能的视频模型以应对未知的机器人任务挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 00:34:37 GMT</pubDate>
</item>
<item>
<title>SynthesizeMe：基于用户交互生成合成人格以实现个性化奖励建模</title>
<link>https://arxiv.org/abs/2506.05598</link>
<guid>https://arxiv.org/abs/2506.05598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过用户交互生成合成人格以提升个性化奖励模型准确性的方法。</p><br /><br /><p><strong>摘要：</strong> 近期对大型语言模型多样对齐的需求推动了适应不同用户偏好的研究，但现有个性化奖励模型大多依赖额外的身份信息。本文介绍SynthesizeMe方法，通过用户交互生成合成人格，用于个性化奖励建模。该方法首先生成并验证解释用户偏好的推理，然后从这些推理中诱导出合成人格，并筛选出有意义的用户交互以构建个性化提示。实验表明，使用SynthesizeMe生成的提示可将Chatbot Arena上的个性化LLM作为评委的准确性提高4.4%，并与奖励模型结合在PersonalRewardBench上达到最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 17:23:16 GMT</pubDate>
</item>
<item>
<title>大型语言模型在策略规划中的自适应进化研究</title>
<link>https://arxiv.org/abs/2506.04651</link>
<guid>https://arxiv.org/abs/2506.04651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过多角色协作，LLM可以自主改进策略规划能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在面对需要长期战略规划的任务时的表现，并提出了一种多角色协作的自适应架构。该架构由分析者、研究者、编码者和玩家四个角色组成，共同迭代优化游戏策略。实验基于《卡坦岛》桌游展开，利用开源框架Catanatron评估了从基础游戏代理到能够自我重写代码的复杂系统。结果显示，相较于人工设计的静态代理，由LLMs驱动的自演化代理在Claude 3.7和GPT-4o等模型的支持下，展现出更强的适应性和策略调整能力，证明了LLMs在持续学习和策略优化方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 01:45:24 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在隐含推理场景中的表现分析</title>
<link>https://arxiv.org/abs/2506.00258</link>
<guid>https://arxiv.org/abs/2506.00258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示当前多模态大语言模型在隐含推理任务中常忽视潜在问题，但简单干预可显著提升表现。</p><br /><br /><p><strong>摘要：</strong> 多模态大语言模型（MLLMs）在开放性真实环境中应用时面临输入混乱、定义不清等问题。不同于精心设计的基准测试，这些场景中指令可能涉及缺失对象、矛盾事实或请求不可行操作。研究通过构建包含四种实际失败模式的诊断工具包，评估六种模型如o3和GPT-4o，发现尽管模型具备必要的感知和推理能力，却常常未能揭示隐藏问题。进一步研究表明，显式提示表明这些能力存在但常被用户合规性压制。实验表明，简单的推理阶段干预，例如谨慎的角色提示及提出澄清问题，能显著提高模型性能。研究揭示了当前MLLMs在推理能力和行为合规性之间的持续差距，并提出了使模型在非约束环境下更加可信的实用策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 17:47:28 GMT</pubDate>
</item>
<item>
<title>基于条件数分析的模型免疫框架及其应用</title>
<link>https://arxiv.org/abs/2505.23760</link>
<guid>https://arxiv.org/abs/2505.23760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过Hessian矩阵条件数分析模型免疫性，并设计算法实现对线性和非线性模型的有效免疫。</p><br /><br /><p><strong>摘要：</strong> 本文旨在探讨如何在预训练阶段构建对有害任务难以微调但仍保留其他任务效用的模型，即模型免疫技术。尽管先前研究表明文本到图像模型可以通过免疫处理，但免疫发生的条件及免疫模型的精确定义尚不明确。为此，我们提出了一个基于Hessian矩阵条件数的框架，用于分析线性模型的免疫性。在此基础上，设计了一种带有正则化项的算法，以控制预训练后模型的条件数。实验结果表明，该算法不仅适用于线性模型，也能有效应用于非线性深度网络的免疫处理。这项研究为模型免疫提供了理论基础和实践方法，同时代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>无需重训的视觉Transformer测试时异常值抑制方法</title>
<link>https://arxiv.org/abs/2506.08010</link>
<guid>https://arxiv.org/abs/2506.08010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重训的视觉Transformer测试时异常值处理方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉Transformer中高模令牌出现的机制及其导致的嘈杂注意力图问题。通过观察发现，某些模型中的稀疏神经元会集中高模激活到异常令牌上，从而破坏下游视觉处理。现有解决方案需要重新训练模型，而我们基于发现提出了一种无需重训的方法，通过将高模激活转移到额外的未训练令牌上来模拟注册令牌的效果。实验表明，该方法生成更清洁的注意力和特征图，在多个下游视觉任务中提升性能，并达到与显式训练注册令牌模型相当的结果。此外，还将此方法扩展到现成的视觉语言模型中，提高了其可解释性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>通过视觉游戏学习提升多模态大型语言模型的泛化推理能力</title>
<link>https://arxiv.org/abs/2506.08011</link>
<guid>https://arxiv.org/abs/2506.08011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过玩简单街机游戏来提升多模态大型语言模型的跨领域推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究受认知科学启发，提出了一种新的后训练范式——视觉游戏学习（ViGaL），通过让多模态大型语言模型（MLLMs）玩类似街机的游戏，如贪吃蛇，从而提升其多模态推理的跨领域泛化能力。实验表明，这种强化学习方法显著提高了模型在多模态数学基准测试（如MathVista）及跨学科问题集（如MMMU）上的表现，且无需接触示例解法、方程或图表。值得注意的是，该模型在多模态推理基准测试中优于专门针对多模态推理数据微调的模型，同时保持了对一般视觉基准的表现，这是许多专门模型难以实现的。这一发现揭示了合成规则型游戏作为可控可扩展预训练任务的新潜力，可以有效解锁MLLMs中的通用多模态推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>Dreamland：结合物理模拟器与生成模型的混合世界生成框架</title>
<link>https://arxiv.org/abs/2506.08006</link>
<guid>https://arxiv.org/abs/2506.08006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Dreamland框架，提升大规模视频生成模型的可控性并优化场景编辑与AI训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Dreamland的混合世界生成框架，该框架结合了基于物理的模拟器和大规模预训练生成模型的优点，旨在解决现有大型视觉生成模型缺乏元素级可控性的缺陷。Dreamland通过设计一种分层的世界抽象表示，将像素级和对象级语义及几何信息作为中间表示，从而实现对模拟器和生成模型的有效连接。这种方法不仅增强了模型的可控性，还降低了适配成本，并支持直接使用现有的和未来的预训练生成模型。此外，研究团队构建了一个名为D3Sim的数据集，用于支持混合生成管道的训练和评估。实验结果显示，Dreamland在图像质量方面比现有基线提高了50.8%，可控性增强了17.9%，并在增强具身智能体训练方面展现出巨大潜力。未来，代码和数据将公开提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Temperature-Adjusted Cross-modal Attention提升文本到图像扩散模型的对齐效果</title>
<link>https://arxiv.org/abs/2506.07986</link>
<guid>https://arxiv.org/abs/2506.07986</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TACA方法解决跨模态注意力不平衡问题，显著改善文本到图像生成的对齐效果。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态扩散Transformer（MM-DiT）模型如FLUX在文本提示与生成内容精确对齐上的不足，分析了跨模态注意力机制中的两大问题：视觉与文本模态间标记数量失衡导致的跨模态注意力抑制，以及缺乏时间步相关的注意力权重调整。为此，我们提出了温度调节跨模态注意力（TACA），通过温度缩放和时间步相关调整实现高效动态的多模态交互平衡。TACA结合LoRA微调后，在T2I-CompBench基准上显著提升了文本到图像生成的对齐性能，同时计算开销极小。实验表明，TACA在FLUX和SD3.5等先进模型上均有效改善了对象外观、属性绑定及空间关系的对齐效果。本研究强调了平衡跨模态注意力在提高文本到图像扩散模型语义保真度中的重要性。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07986" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:54:04 GMT</pubDate>
</item>
<item>
<title>OneIG-Bench：文本到图像模型的综合性评估基准</title>
<link>https://arxiv.org/abs/2506.07977</link>
<guid>https://arxiv.org/abs/2506.07977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OneIG-Bench，用于多维度评估文本到图像模型的性能。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像（T2I）模型的发展，现有基准测试暴露出缺乏全面评估的问题，特别是对推理能力、文本渲染和风格化等方面的不足。为解决这些局限性，本文引入OneIG-Bench，这是一个经过精心设计的综合评估框架，涵盖提示-图像对齐、文本渲染精度、推理生成内容、风格化及多样性等多个维度。通过结构化的评估方式，该基准能够深入分析模型性能，帮助研究者和开发者识别模型的优势与瓶颈。此外，OneIG-Bench支持灵活的评估模式，用户可以专注于特定的评估子集，而无需针对所有提示生成图像。我们的代码库和数据集现已公开，旨在促进T2I研究领域的可重复性研究和跨模型比较。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:50:21 GMT</pubDate>
</item>
<item>
<title>MiniCPM4：面向端侧设备的高效大型语言模型</title>
<link>https://arxiv.org/abs/2506.07900</link>
<guid>https://arxiv.org/abs/2506.07900</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiniCPM4通过多维度创新成为高效的端侧大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一款名为MiniCPM4的高效大型语言模型（LLM），专为端侧设备设计。为了实现这一效率，MiniCPM4在模型架构、训练数据、训练算法及推理系统四个方面进行了系统性创新。在模型架构上，提出InfLLM v2，一种可训练的稀疏注意力机制，加速长上下文处理的预填充和解码阶段；在训练数据方面，提出UltraClean和UltraChat v2，仅需8万亿训练令牌即可获得满意性能；在训练算法上，改进了ModelTunnel v2并引入chunk-wise rollout和BitCPM，提升训练效率；在推理系统中，提出CPM.cu，结合稀疏注意力、模型量化和推测采样以实现高效预填充和解码。MiniCPM4提供两种版本，分别具有0.5B和8B参数量，并在多个基准测试中表现优异，特别是在处理长序列时，MiniCPM4-8B相比Qwen3-8B有显著速度提升。此外，通过进一步适配，MiniCPM4成功支持多种应用，如可信问卷生成和工具使用等，展示了其广泛的实用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07900" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 12:16:50 GMT</pubDate>
</item>
<item>
<title>PolyVivid：多主体视频定制框架实现精确身份控制</title>
<link>https://arxiv.org/abs/2506.07848</link>
<guid>https://arxiv.org/abs/2506.07848</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型多主体视频定制框架，提升身份一致性与交互效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PolyVivid的多主体视频定制框架，解决了现有视频生成模型在精细控制方面的不足，特别是在多主体身份一致性和交互上的难题。该框架通过设计基于视觉语言大模型(VLLM)的文本-图像融合模块，将视觉身份嵌入到文本空间中，确保准确对应。同时，引入基于3D-RoPE的增强模块，促进文本与图像嵌入的双向结构化融合。此外，开发了继承注意力机制的身份注入模块，有效减轻身份漂移问题。最后，构建基于多语言大模型(MLLM)的数据管道，结合多种策略生成高质量多主体数据，显著提高主体区分度。实验表明，PolyVivid在身份保真度、视频真实感及主体对齐方面表现优异，优于现有开源和商业基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07848" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 11:11:09 GMT</pubDate>
</item>
<item>
<title>Improving large language models with concept-aware fine-tuning</title>
<link>https://arxiv.org/abs/2506.07833</link>
<guid>https://arxiv.org/abs/2506.07833</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase "ribonucleic acid" as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments ("rib", "on", ...), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at https://github.com/michaelchen-lab/caft-llm
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:55:00 GMT</pubDate>
</item>
<item>
<title>通过图像重建解析视觉特征编码器</title>
<link>https://arxiv.org/abs/2506.07803</link>
<guid>https://arxiv.org/abs/2506.07803</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过图像重建方法解析视觉特征编码器内部表示。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种基于图像重建的新方法，用于解释视觉特征编码器的内部表示。我们比较了SigLIP和SigLIP2两个相关模型家族，发现基于图像任务预训练的编码器保留了比对比学习等非图像任务更多的图像信息。此外，该方法还能对多种视觉编码器进行排名，并揭示特征空间的操作会导致可预测的重建图像变化，其中正交旋转而非空间变换控制颜色编码。此方法适用于任何视觉编码器，有助于揭示其特征空间的内部结构。实验代码和模型权重已公开于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07803" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 10:32:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型对低资源语言的漏洞分析</title>
<link>https://arxiv.org/abs/2506.07645</link>
<guid>https://arxiv.org/abs/2506.07645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型在低资源语言中的潜在安全漏洞。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在处理自然语言处理任务时面临的安全挑战，特别是其对小幅度字符和词汇级攻击的敏感性。通过仅修改少量字符并结合小型代理模型计算词的重要性，研究人员成功构建出强大的廉价攻击方式，这些攻击显著影响了多种LLMs的预测结果，表明其内部安全机制存在被绕过的隐患。研究重点验证了这种攻击在波兰语等低资源语言上的有效性，并展示了其扩展到其他语言的可能性。此外，研究团队公开了创建的数据集和代码，以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 07:09:39 GMT</pubDate>
</item>
<item>
<title>通过ReLIFT提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.07527</link>
<guid>https://arxiv.org/abs/2506.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合强化学习和监督微调的ReLIFT显著提升了大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，通过强化学习（RL），大型语言模型（LLM）能够表现出复杂的推理行为如规划和自我反思。然而，当前的强化学习方法受限于基础模型的知识范围，难以超越其现有能力。为解决这一问题，我们采用监督微调（SFT）来学习强化学习无法实现的能力，从而引入新的知识和推理模式。分析显示，强化学习擅长维持并优化模型原始能力范围内的表现，而监督微调则更适用于扩展模型能力边界。受此启发，我们提出了ReLIFT（强化学习与在线微调交替训练）的新方法。该方法主要依赖强化学习进行训练，在遇到难题时收集高质量解决方案进行微调，并交替进行两种训练方式以增强模型推理能力。实验表明，ReLIFT在多个竞争级基准测试中平均提升了超过5.2分，且仅需13%的详细演示数据便优于传统方法，证明了其优越的可扩展性与潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:11:20 GMT</pubDate>
</item>
<item>
<title>SpatialLM：基于标准多模态LLM架构的3D场景理解模型</title>
<link>https://arxiv.org/abs/2506.07491</link>
<guid>https://arxiv.org/abs/2506.07491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SpatialLM通过标准LLM架构处理点云数据并实现3D场景理解。</p><br /><br /><p><strong>摘要：</strong> SpatialLM是一种专门设计用于处理三维点云数据的大语言模型，旨在生成结构化的3D场景理解输出，例如墙壁、门、窗户等建筑元素及带语义类别的对象框。与以往依赖特定任务网络设计的方法不同，该模型采用标准的多模态大型语言模型架构，并直接从开源模型微调而来。为了训练SpatialLM，我们收集了一个包含12,328个室内场景（54,778个房间）及其真实3D标注的大规模高质量合成数据集，并对多种建模和训练决策进行了深入研究。在公共基准测试中，该模型在布局估计方面达到了最先进的性能，在3D物体检测方面也取得了具有竞争力的结果。这一成果展示了增强现代大型语言模型空间理解能力的一种可行路径，适用于增强现实、具身机器人等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 03:10:58 GMT</pubDate>
</item>
<item>
<title>CCI4.0：构建高质量双语预训练数据集及其应用</title>
<link>https://arxiv.org/abs/2506.07463</link>
<guid>https://arxiv.org/abs/2506.07463</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于大规模语言模型预训练的高质量双语数据集CCI4.0。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CCI4.0的大规模双语预训练数据集，该数据集经过精心设计以提供卓越的数据质量和多样化的人类推理轨迹。数据集由两个子集组成：CCI4.0-M2-Base和CCI4.0-M2-CoT。其中，CCI4.0-M2-Base结合了精心挑选的中文网络语料库、Nemotron-CC的英文子集以及其他多样化的数学、维基百科、arxiv和代码来源。为了保证数据质量，我们提出了一个新的流水线方法，主要基于模型通过两阶段去重、多分类器质量评分和领域感知流畅性过滤来验证数据质量。此外，我们还提取了45亿个CoT（Chain-of-Thought）模板，命名为CCI4.0-M2-CoT，这些模板展示了不同的推理模式，并显著降低了幻觉的可能性。实验评估表明，在CCI4.0上预训练的语言模型在下游任务中，尤其是在数学和代码反射任务中，表现出了一致的改进。我们的研究强调了严格的数据整理和人类思维模板在提升大型语言模型性能中的关键作用，为自动处理预训练语料库提供了启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07463" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 02:14:19 GMT</pubDate>
</item>
<item>
<title>弱到强解码框架提升大语言模型对齐能力</title>
<link>https://arxiv.org/abs/2506.07434</link>
<guid>https://arxiv.org/abs/2506.07434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弱到强解码框架增强大语言模型对齐能力。</p><br /><br /><p><strong>摘要：</strong> 为了改善大语言模型（LLMs）生成内容可能存在的不当、虚假或无意义的问题，近年来低资源对齐方法受到关注，但高质量且对齐良好的数据获取仍具挑战性。本研究发现解码开始阶段对生成对齐响应的难度较大，因此提出弱到强解码（WSD）框架，通过小型对齐模型引导大型基础模型，先由小型模型起草对齐开头，再由大型模型完成后续内容。此外，还构建了一个名为GenerAlign的新数据集，用于微调小型Pilot-3B模型作为起草模型。实验表明，该方法在不损害下游任务性能的情况下显著提升了多种基础模型的对齐效果。同时，研究还深入分析了WSD框架的内在机制及其设置和时间效率的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 01:21:22 GMT</pubDate>
</item>
<item>
<title>通过ConfQA策略降低大语言模型事实性陈述幻觉率</title>
<link>https://arxiv.org/abs/2506.07309</link>
<guid>https://arxiv.org/abs/2506.07309</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为ConfQA的微调策略，将大语言模型的事实性幻觉率降至5%以下。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ConfQA的微调策略，旨在减少大型语言模型（LLMs）在生成事实性陈述时的幻觉现象。传统方法下，LLMs的幻觉率通常在20%-40%，而通过ConfQA策略，这一比率可降至5%以下。该策略的核心在于当模型回答正确时，训练其继续给出答案；否则则训练其承认“我不确定”。此外，通过引入“仅在自信时回答”的抑制提示，以及利用知识图谱中的简单事实属性值来校准模型的信心，进一步提升了效果。基于此，研究提出了Dual Neural Knowledge框架，该框架能够根据ConfQA的置信度，在内部参数化神经知识和外部记录的符号知识之间无缝切换，不仅使准确率提升至超过95%，还减少了超过30%不必要的外部检索操作。这种方法展现了在多个事实性基准测试上的强大泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07309" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 18:51:46 GMT</pubDate>
</item>
<item>
<title>利用预训练语言模型通过上下文学习预测隐马尔可夫模型生成序列</title>
<link>https://arxiv.org/abs/2506.07298</link>
<guid>https://arxiv.org/abs/2506.07298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示预训练语言模型可通过上下文学习有效预测隐马尔可夫模型生成的数据。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了预训练大型语言模型（LLMs）通过上下文学习（ICL）对隐马尔可夫模型（HMMs）生成数据的建模能力。实验结果显示，LLMs在多种合成HMM上达到了接近理论最优的预测精度。此外，研究揭示了受HMM属性影响的新颖规模趋势，并提出了相关理论假设。针对科学家的实际应用，本文还提供了使用ICL作为复杂数据分析诊断工具的指南。在真实动物决策任务中，ICL的表现与人类设计的模型相当。这是首次证明ICL可以学习并预测HMM生成序列的研究，这一发现深化了我们对LLMs上下文学习的理解，并展示了其作为揭示复杂科学数据隐藏结构的强大工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 17:49:38 GMT</pubDate>
</item>
<item>
<title>通过内部进度编码优化大型语言模型的显式推理过程</title>
<link>https://arxiv.org/abs/2506.07240</link>
<guid>https://arxiv.org/abs/2506.07240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLMs如何调节推理长度并提出一种减少过思考的方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在显式推理过程中理解和调控推理长度的机制。首先，我们发现LLMs会在推理过程中编码进展状态，并通过交互式进度条可视化揭示了模型的规划动态。其次，我们通过对推理阶段的内部进度编码进行操作，减少了不必要的推理步骤，从而生成更加简洁且果断的推理链条。实验结果显示，这种方法可以有效缓解过思考问题，提高答案准确性，同时降低推理延迟。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 13:54:33 GMT</pubDate>
</item>
<item>
<title>GeometryZero：通过强化学习优化几何问题求解的辅助构造</title>
<link>https://arxiv.org/abs/2506.07160</link>
<guid>https://arxiv.org/abs/2506.07160</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习框架GCPO，用于训练高效结合辅助构造和几何推理的模型GeometryZero。</p><br /><br /><p><strong>摘要：</strong> 近期大规模语言模型(LLMs)在数学推理领域表现出色，但在几何问题求解方面仍具挑战性，尤其是辅助构造的重要性。现有方法要么性能欠佳，要么依赖庞大的LLMs导致高计算成本。我们提出了一种基于可验证奖励的强化学习方向，但直接应用存在局限性。为此，我们设计了Group Contrastive Policy Optimization(GCPO)，创新性地引入上下文效用自适应奖励和长度奖励，开发出GeometryZero模型家族，其在多个基准测试中显著优于现有方法，平均提升4.29%。这项研究为更经济高效的几何推理模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07160" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 10:18:15 GMT</pubDate>
</item>
<item>
<title>大型推理模型的性能与局限性分析</title>
<link>https://arxiv.org/abs/2506.06941</link>
<guid>https://arxiv.org/abs/2506.06941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型推理模型在复杂度超过一定阈值时会出现准确性崩溃。</p><br /><br /><p><strong>摘要：</strong> 近年来，语言模型的发展引入了大型推理模型（LRMs），这些模型在提供答案前会生成详细的推理过程，显示出在推理基准测试中的改进表现。然而，关于其基本能力、扩展特性及限制的研究仍显不足。目前的评估主要集中在数学和编码基准上，侧重最终答案的准确性，但这种评估方式容易受到污染且无法深入洞察推理路径。本研究利用可控谜题环境系统地探讨了这些空白，在保持逻辑结构一致的同时精确调整复杂度。实验结果显示，LRMs在超过特定复杂度时完全丧失准确性，且表现出反直觉的扩展极限——随着问题复杂性的增加，推理努力先上升后下降。通过与标准语言模型对比，我们发现LRMs在低、中、高复杂度任务中分别表现出不同的性能表现。此外，研究还深入分析了LRMs的计算行为和推理路径，揭示了其在精确计算和跨尺度推理中的局限性，并对模型的推理能力提出了疑问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 18:42:29 GMT</pubDate>
</item>
<item>
<title>通过元学习提升多模态大模型的小样本任务适应能力</title>
<link>https://arxiv.org/abs/2506.06905</link>
<guid>https://arxiv.org/abs/2506.06905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于元学习的方法，通过蒸馏任务相关图像特征的软提示，提升小规模多模态大模型的小样本性能。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型(LMMs)通常依赖于上下文学习(ICL)来执行新任务，但这种性能在较小的LMMs中往往不一致且非单调增长。我们假设这是由于模型被多余的图像嵌入信息所淹没，这些信息对下游任务并非必要。为解决此问题，我们提出了一种元学习方法，利用固定的任务相关图像特征集的软提示，在少量示例下进行测试时适配。为此，我们引入了一个注意力映射模块，可以轻松集成到流行的LLaVA v1.5架构中，并与软提示联合训练，使模型在低数据情况下仅需少量梯度步即可适应任务。在VL-ICL基准上的评估表明，我们的方法在面对图像扰动时始终优于ICL及相关提示微调方法，提升了视觉问答任务中的任务诱导和推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 07 Jun 2025 15:37:22 GMT</pubDate>
</item>
<item>
<title>大型语言模型在上下文与记忆冲突下的表现评估框架</title>
<link>https://arxiv.org/abs/2506.06485</link>
<guid>https://arxiv.org/abs/2506.06485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出诊断框架评估大型语言模型在上下文与参数知识冲突中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种诊断框架，用于系统性评估大型语言模型（LLMs）在面对上下文信息与参数知识冲突时的行为。通过构建引发此类冲突的诊断数据集，并分析模型在多种任务类型上的表现，研究发现当上下文信息与模型的参数化信念一致时，模型表现更好；知识冲突对不依赖知识的任务影响较小；模型难以完全抑制内部知识，即使有明确指令；解释冲突的推理过程会增加对上下文的依赖。这些发现引发了对基于模型评估有效性的担忧，并强调了在部署LLMs时考虑知识冲突的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 15:20:23 GMT</pubDate>
</item>
<item>
<title>面向LLM推理阶段的安全保障：SAFFRON范式的提出</title>
<link>https://arxiv.org/abs/2506.06444</link>
<guid>https://arxiv.org/abs/2506.06444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对LLM推理阶段的安全性问题，本文提出了一种新的范式SAFFRON。</p><br /><br /><p><strong>摘要：</strong> 现有的安全保证研究主要集中在训练阶段对齐，以使大型语言模型（LLMs）习得安全行为。然而，近期研究表明这些方法容易受到多种越狱攻击的影响。与此同时，推理阶段的扩展显著提升了LLMs的推理能力，但其在安全保证中的应用尚未被深入探索。本研究填补了这一空白，开创性地提出了推理阶段的扩展方法，以应对新兴威胁下的LLMs安全问题。我们发现，尽管传统的推理扩展技术在推理任务中表现良好，但在安全场景下却表现欠佳，甚至不如基本的Best-of-N采样方法。这种低效性归因于频繁的过程奖励模型（PRM）评估所带来的高计算开销。为解决这一问题，我们提出了SAFFRON，一种专门针对安全保证设计的新推理扩展范式。该范式的核心在于引入多分支奖励模型（MRM），大幅减少所需奖励模型评估次数。此外，我们还设计了部分监督训练目标、保守探索约束以及基于Trie的数据缓存策略等关键技术。实验结果验证了SAFFRON的有效性，并公开了训练好的多分支奖励模型（Saffron-1）及相关的安全奖励数据集（Safety4M），以促进LLMs安全领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 14:05:45 GMT</pubDate>
</item>
<item>
<title>通过自学习训练KV缓存以降低大语言模型长上下文推理成本</title>
<link>https://arxiv.org/abs/2506.06266</link>
<guid>https://arxiv.org/abs/2506.06266</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过自学习方法训练KV缓存（Cartridge），大幅降低大语言模型长上下文推理的成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通常通过将整个文本语料库放入上下文窗口并利用上下文学习（ICL）来回答基于大规模文本语料库的问题。然而，这种方法的内存消耗随着输入长度增加而增加，导致服务成本高昂。为解决这一问题，本文探索了一种替代方案：在线下为每个语料库训练一个较小的KV缓存。这种缓存被称为“Cartridge”，在推理时加载即可生成响应。尽管训练Cartridge的成本可以分摊到引用相同语料库的所有查询上，但直接使用下一个标记预测的方法未能超越ICL。因此，我们提出了自学习（self-study）方法，即生成关于语料库的合成对话，并以上下文化蒸馏为目标训练Cartridge。实验表明，采用自学习训练的Cartridge不仅能够复制ICL的功能，而且显著降低了推理成本。在具有挑战性的长上下文基准测试中，使用自学习训练的Cartridge在内存使用减少38.6倍的同时，吞吐量提高了26.4倍。此外，自学习还延长了模型的有效上下文长度，并意外地使推理时无需重新训练即可组合Cartridge。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06266" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:48:23 GMT</pubDate>
</item>
<item>
<title>Astra双模型架构提升机器人室内导航性能</title>
<link>https://arxiv.org/abs/2506.06205</link>
<guid>https://arxiv.org/abs/2506.06205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Astra双模型架构显著提高机器人在复杂室内环境中的导航成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Astra的全新双模型架构，用于解决现代机器人在多样化复杂室内环境中导航困难的问题。Astra由全局模型Astra-Global和局部模型Astra-Local组成，其中Astra-Global采用多模态大型语言模型处理视觉和语言输入，结合混合拓扑语义图实现自定位和目标定位，优于传统视觉位置识别方法；Astra-Local则是一个多任务网络，负责局部路径规划和里程估计，其4D时空编码器通过自监督学习生成鲁棒特征，规划头利用流匹配和新型掩码ESDF损失减少碰撞风险，里程计头通过变压器编码器整合多传感器输入预测机器人相对姿态。Astra部署在实际移动机器人上，在多种室内环境中实现了高成功率的任务完成率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 12:08:47 GMT</pubDate>
</item>
<item>
<title>基于语言表达动作的视觉-语言基础模型世界模型与动力学模型研究</title>
<link>https://arxiv.org/abs/2506.06006</link>
<guid>https://arxiv.org/abs/2506.06006</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过监督微调提升动力学模型较构建世界模型更容易。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉-语言基础模型在语言表达动作时，构建现实世界模型（观察-动作-观察）和动力学模型（观察-观察-动作）的能力。尽管开源模型在这两方面均表现不佳，但研究发现，通过监督微调获取动力学模型比获取世界模型更为容易。进一步地，动力学模型可通过两种策略助力世界模型的构建：一是利用合成数据进行弱监督学习，二是推理阶段的验证。具体而言，动力学模型可标注视频帧对之间的动作，从而扩充训练数据；同时，通过引入一种新目标函数，依据识别模型预测的重要性加权观察对中的图像标记。此外，动力学模型还能为世界模型的多个样本分配奖励，用于推理阶段的评分。通过Aurora-Bench上的动作中心图像编辑任务评估，我们最佳模型的表现与最先进的图像编辑模型相当，在GPT4o-as-judge评估的现实子集上提升了15%，并在Aurora-Bench的所有子集上获得了最佳的人类评价。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06006" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:50:18 GMT</pubDate>
</item>
<item>
<title>基于合成对话数据的实时感知任务引导对话系统</title>
<link>https://arxiv.org/abs/2506.05904</link>
<guid>https://arxiv.org/abs/2506.05904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种综合框架，解决实时感知任务引导对话系统的数据和评估难题。</p><br /><br /><p><strong>摘要：</strong> 近期会话人工智能取得了显著进展，但在开发实时感知任务引导系统方面仍面临挑战。这类系统需要根据流式视觉输入提供交互性和主动性帮助，但其开发受到昂贵且耗时的数据收集和系统评估过程的限制。为了解决这些局限性，本文提出了一个全面的框架，包含三个关键贡献：首先，引入一种新颖的数据整理管道，从标注的主观视角视频中合成对话，生成跨越多个领域的大型合成对话数据集；其次，开发了一组自动评估指标，并通过广泛的用户研究验证其有效性；最后，提出一个端到端模型，处理流式视频输入以生成上下文相关的响应，同时采用创新技术处理数据不平衡和长时间视频问题。这项工作为开发实时主动AI助手奠定了基础，能够指导用户完成多样化任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 05:23:29 GMT</pubDate>
</item>
<item>
<title>大型语言模型在辩论演讲评估中的表现分析</title>
<link>https://arxiv.org/abs/2506.05062</link>
<guid>https://arxiv.org/abs/2506.05062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型在评估辩论演讲中的能力表现。</p><br /><br /><p><strong>摘要：</strong> 本文引入了辩论演讲评估作为评估大型语言模型（LLMs）判断能力的新基准，该任务需要对演讲在多个层面进行深入理解，包括论证强度、连贯性及风格等。通过分析超过600份精心标注的辩论演讲数据集，我们发现虽然较大的模型在某些方面可以接近人类的判断，但在整体判断行为上存在显著差异。此外，研究还探讨了前沿LLMs生成有说服力演讲的能力，表明这些模型在这项任务上的表现可能达到人类水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 10:06:51 GMT</pubDate>
</item>
<item>
<title>MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character Recognition with over 97K Categories</title>
<link>https://arxiv.org/abs/2506.04807</link>
<guid>https://arxiv.org/abs/2506.04807</guid>
<content:encoded><![CDATA[
Foundational to the Chinese language and culture, Chinese characters encompass extraordinarily extensive and ever-expanding categories, with the latest Chinese GB18030-2022 standard containing 87,887 categories. The accurate recognition of this vast number of characters, termed mega-category recognition, presents a formidable yet crucial challenge for cultural heritage preservation and digital applications. Despite significant advances in Optical Character Recognition (OCR), mega-category recognition remains unexplored due to the absence of comprehensive datasets, with the largest existing dataset containing merely 16,151 categories. To bridge this critical gap, we introduce MegaHan97K, a mega-category, large-scale dataset covering an unprecedented 97,455 categories of Chinese characters. Our work offers three major contributions: (1) MegaHan97K is the first dataset to fully support the latest GB18030-2022 standard, providing at least six times more categories than existing datasets; (2) It effectively addresses the long-tail distribution problem by providing balanced samples across all categories through its three distinct subsets: handwritten, historical and synthetic subsets; (3) Comprehensive benchmarking experiments reveal new challenges in mega-category scenarios, including increased storage demands, morphologically similar character recognition, and zero-shot learning difficulties, while also unlocking substantial opportunities for future research. To the best of our knowledge, the MetaHan97K is likely the dataset with the largest classes not only in the field of OCR but may also in the broader domain of pattern recognition. The dataset is available at https://github.com/SCUT-DLVCLab/MegaHan97K.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 05:33:06 GMT</pubDate>
</item>
<item>
<title>gamma-PO算法提升大语言模型对齐效率</title>
<link>https://arxiv.org/abs/2506.03690</link>
<guid>https://arxiv.org/abs/2506.03690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出gamma-PO算法优化大语言模型对齐，显著提高性能且不影响训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为gamma-PO的新算法，该算法是一种动态目标边界偏好优化方法，通过引入实例特定的边界校准，在每对偏好数据中调整奖励边界。这种方法优先处理高置信度数据对，同时抑制噪声影响，从而有效提升大语言模型（LLMs）的安全性和可靠性。gamma-PO兼容多种基于奖励边界的直接偏好优化（DPO）变体，并在AlpacaEval2和Arena-Hard等基准测试中实现了平均4.4%的性能提升，成为当前最先进的技术。此外，gamma-PO仅需少量代码改动，对训练效率几乎没有负面影响，证明了其作为增强LLMs对齐工具的稳健性。相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 04:19:37 GMT</pubDate>
</item>
<item>
<title>ExpertLongBench：面向专家级任务的大规模语言模型评估基准</title>
<link>https://arxiv.org/abs/2506.01241</link>
<guid>https://arxiv.org/abs/2506.01241</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出包含11项任务的ExpertLongBench基准，评估现有大模型在专家级任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为ExpertLongBench的专家级基准，该基准包含来自9个领域的11项任务，这些任务反映了真实的专家工作流程和应用。不同于传统的问答任务，ExpertLongBench中的任务需要超过5000个标记的长篇输出，并且要求严格遵守领域特定的要求。每个任务都有由领域专家设计或验证的任务说明表，用于指定任务需求并指导输出评估。此外，我们提出了CLEAR评估框架，支持对本基准中长篇模型输出的准确评估。为了实现细粒度的专家对齐评估，CLEAR通过从任务特定说明表中提取信息，从模型输出和参考中衍生出检查清单。然后将模型输出的检查清单项目与参考输出的对应项目进行比较，以评估其正确性，从而实现基于事实的评估。我们对11个大型语言模型进行了基准测试，并分析了CLEAR的组成部分，结果显示现有大型语言模型在专家级任务上的表现仍需显著改进，其中顶级性能仅达到26.8%的F1得分；尽管如此，模型确实可以生成符合要求的内容，但准确性仍有待提高；同时，开放权重模型在CLEAR中准确提取和比较检查清单方面具有可扩展性和低成本的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01241" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 21:39:02 GMT</pubDate>
</item>
<item>
<title>EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions</title>
<link>https://arxiv.org/abs/2505.23473</link>
<guid>https://arxiv.org/abs/2505.23473</guid>
<content:encoded><![CDATA[
Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 14.31% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 10:26:46 GMT</pubDate>
</item>
<item>
<title>引入自省与纠错能力的端到端多模态GUI自动化框架</title>
<link>https://arxiv.org/abs/2506.08012</link>
<guid>https://arxiv.org/abs/2506.08012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GUI-Reflection框架，提升多模态模型的自省与错误恢复能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有图形用户界面(GUI)自动化模型缺乏自省和错误恢复能力的问题，提出了一种名为GUI-Reflection的新框架。该框架通过三个阶段的专用训练——特定于GUI的预训练、离线监督微调(SFT)和在线自省微调，将自我反思和错误纠正功能融入到端到端的多模态GUI模型中。这一过程完全基于自动化的数据生成和学习，无需人工标注。具体而言，我们首先设计了可扩展的数据管道，从现有的成功轨迹中自动生成用于反思和错误修正的数据。同时，我们还提出了GUI-Reflection任务套件，以显式地学习和评估这些面向反思的能力。此外，我们在移动设备上构建了一个多样化且高效的环境，用于GUI模型的在线训练和数据收集，并开发了一种迭代式的在线自省微调算法，使模型能够持续提升其自省和错误纠正能力。我们的方法为更健壮、适应性强且智能的GUI自动化铺平了道路，并承诺公开所有数据、模型、环境和工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>强化预训练（RPT）：语言模型与强化学习的新范式</title>
<link>https://arxiv.org/abs/2506.08007</link>
<guid>https://arxiv.org/abs/2506.08007</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化预训练通过将预测下一token重构为推理任务，显著提升语言模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的扩展范式——强化预训练（Reinforcement Pre-Training, RPT），它将下一个词预测重新定义为一种可以通过强化学习训练的推理任务。该方法利用可验证的奖励机制，针对给定上下文正确预测下一个词，从而有效利用大量文本数据进行通用强化学习，而无需依赖特定领域的标注答案。实验表明，RPT不仅显著提高了语言建模的准确性，还为后续的强化微调奠定了坚实的基础。此外，随着计算资源的增加，训练效果持续改善，显示出RPT作为语言模型预训练有效且有前景的扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.08007" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>小规模语言模型在长链式思维训练中的性能退化现象</title>
<link>https://arxiv.org/abs/2506.07712</link>
<guid>https://arxiv.org/abs/2506.07712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示小规模语言模型在长链式思维训练中会出现显著性能下降的现象。</p><br /><br /><p><strong>摘要：</strong> 长链式思维（Long CoT）监督是提升语言模型推理能力的常见策略，但在小规模语言模型（SLMs，≤3B参数）中，我们观察到一种名为“长CoT退化”的现象，即这些模型在有限长CoT数据集上训练时性能会大幅下降。通过Qwen2.5、LLaMA3和Gemma3系列模型的实验验证，我们发现这种退化普遍存在。例如，在某些情况下，仅基于8k长CoT样本训练的模型在微调前性能可能降低多达75%。即使增加训练样本量至220k，部分极小型模型也无法恢复或超越初始性能。进一步分析表明，这种退化源于错误累积效应，尽管长响应能增强多步推理能力，但也放大了错误传播风险。此外，长CoT退化可能对下游强化学习产生负面影响，但可通过充分的监督微调缓解。本研究挑战了关于长CoT训练对SLMs益处的传统认知，为构建更有效的中小规模推理模型提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 08:56:41 GMT</pubDate>
</item>
<item>
<title>GTR-Mol-VLM：基于图遍历机制的光学化学结构识别框架</title>
<link>https://arxiv.org/abs/2506.07553</link>
<guid>https://arxiv.org/abs/2506.07553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GTR-Mol-VLM模型提升复杂分子结构识别精度。</p><br /><br /><p><strong>摘要：</strong> 光学化学结构识别(OCSR)是将分子图像转换为机器可读格式的关键技术。本文介绍了一种名为GTR-Mol-VLM的新框架，通过引入Graph Traversal as Visual Chain of Thought机制和Faithfully Recognize原则解决了现有视觉语言模型在处理复杂分子结构时的不足。为了支持模型开发，构建了GTR-CoT-1.3M大规模指令调优数据集，并设计了MolRec-Bench基准测试。实验表明，GTR-Mol-VLM在涉及功能团缩写分子图像的场景中，比第二好的基线高出约14个百分点。本研究希望推动OCSR技术更好地服务于实际需求，促进化学生物信息学和科学人工智能领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:47:10 GMT</pubDate>
</item>
<item>
<title>BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation</title>
<link>https://arxiv.org/abs/2506.07530</link>
<guid>https://arxiv.org/abs/2506.07530</guid>
<content:encoded><![CDATA[
Vision-Language-Action (VLA) models have shown impressive capabilities across a wide range of robotics manipulation tasks. However, their growing model size poses significant challenges for deployment on resource-constrained robotic systems. While 1-bit pretraining has proven effective for enhancing the inference efficiency of large language models with minimal performance loss, its application to VLA models remains underexplored. In this work, we present BitVLA, the first 1-bit VLA model for robotics manipulation, in which every parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint of the vision encoder, we propose the distillation-aware training strategy that compresses the full-precision encoder to 1.58-bit weights. During this process, a full-precision encoder serves as a teacher model to better align latent representations. Despite the lack of large-scale robotics pretraining, BitVLA achieves performance comparable to the state-of-the-art model OpenVLA-OFT with 4-bit post-training quantization on the LIBERO benchmark, while consuming only 29.8% of the memory. These results highlight BitVLA's promise for deployment on memory-constrained edge devices. We release the code and model weights in https://github.com/ustcwhy/BitVLA.
]]></content:encoded>
<pubDate>Mon, 09 Jun 2025 04:15:11 GMT</pubDate>
</item>
<item>
<title>Lingshu：面向医学应用的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2506.07044</link>
<guid>https://arxiv.org/abs/2506.07044</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的医学专用多模态大型语言模型Lingshu。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有医学多模态大型语言模型在医疗场景中的局限性，如知识覆盖不足、易产生幻觉及缺乏复杂推理能力等问题，提出了综合的数据整理程序和多阶段训练方法，构建了一个名为Lingshu的医学专用多模态大型语言模型。Lingshu不仅扩展了医学知识的范围，还通过强化学习技术增强了其推理能力。此外，我们开发了MedEvalKit评估框架，用于标准化评估模型性能。实验结果显示，Lingshu在多项医学任务上优于现有的开源多模态模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.07044" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 08 Jun 2025 04:47:30 GMT</pubDate>
</item>
<item>
<title>MIRIAD：构建高质量医疗知识库以提升大语言模型可靠性</title>
<link>https://arxiv.org/abs/2506.06091</link>
<guid>https://arxiv.org/abs/2506.06091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入MIRIAD医学问答数据集，显著提高大语言模型在医疗领域的准确性与可信度。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）有望通过高级决策支持和灵活的聊天助手革新医疗领域，但其生成的医疗内容可能存在不准确性。为解决这一问题，本文提出了MIRIAD，这是一个大规模、经过精心筛选的医学问答对数据集，包含5,821,948个问答对，这些问答对源自同行评审的医学文献并经过半自动化处理。与依赖原始未结构化文本的传统方法相比，MIRIAD以操作化的查询-回答格式组织医疗知识，从而实现更精确的知识检索。实验表明，在具有相同源数据和检索文本量的情况下，结合MIRIAD的大语言模型在困难的医学问答基准测试中的准确率提高了多达6.7%，同时在检测医疗幻觉方面的能力提升了22.5至37%（F1分数增加）。此外，还推出了MIRIAD-Atlas，这是一个交互式的知识地图，覆盖了56个医学学科，便于临床用户探索、搜索和细化医学知识。MIRIAD及其相关工具为医疗信息检索器、增强型RAG应用和基于知识的聊天界面等下游应用奠定了基础，最终推动医疗领域更可靠的LLM应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 09:52:32 GMT</pubDate>
</item>
<item>
<title>从模型中心到数据中心：AI图像生成领域的范式转变</title>
<link>https://arxiv.org/abs/2506.05673</link>
<guid>https://arxiv.org/abs/2506.05673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型领域转向数据质量驱动的发展，引入高质量图像数据集DSD。</p><br /><br /><p><strong>摘要：</strong> 现代人工智能模型，特别是在计算机视觉和图像生成任务中的扩散模型，正在经历开发方法的重大转变。过去主要依赖于复杂模型架构和超参数优化的“模型中心”方法，正逐渐被更注重数据质量、结构和相关性的“数据中心”方法取代。本文介绍了DataSeeds.AI样本数据集（DSD），该数据集包含约10,610张经过高质量人类评分和多层注释的摄影图片，旨在推动商业图像数据集的新标准。作为DataSeed.AI超过1亿张图像目录的一小部分，DSD为稳健的商业和多模态AI开发提供了可扩展的基础。通过深入分析，我们展示了DSD对特定模型在已知基准上的定量改进，并公开了评估过程中使用的代码和训练模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 21:50:28 GMT</pubDate>
</item>
<item>
<title>AI推理知识转移能力的评估与优化</title>
<link>https://arxiv.org/abs/2506.05579</link>
<guid>https://arxiv.org/abs/2506.05579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现AI模型性能与人类理解间存在不一致性，需专门优化知识转移。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能推理能力的进步显著提升了多项任务的表现，但其是否有助于更佳的知识转移尚存疑问。知识转移涉及模型以人类可理解的方式传达推理，从而促进学习和应用。为探讨这一问题，我们开发了知识集成与转移评估(KITE)框架，并开展了首个大规模人类实验(N=118)，以衡量AI与人类协作中的知识传递效果。实验分为两个阶段：首先，人类与AI共同构思解决方案策略；随后，人类独立实施解决方案。结果显示，尽管AI基准性能与合作成果之间存在一定关联，但这种关系并不稳定，存在显著异常值，表明知识转移需要针对性优化。进一步分析揭示了影响成功知识转移的行为和战略因素。我们公开了代码、数据集及评估框架，以支持未来研究交流对齐模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 16:48:16 GMT</pubDate>
</item>
<item>
<title>缓解视觉场景文本理解中的语义幻觉问题</title>
<link>https://arxiv.org/abs/2506.05551</link>
<guid>https://arxiv.org/abs/2506.05551</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的框架解决大规模多模态模型的语义幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 大规模多模态模型（LMMs）在视觉感知与推理方面取得显著进展，但在处理视觉模糊或非语义场景文本时易产生语义幻觉。本文分析了语义幻觉的根本原因，并发现具有更强注意力焦点的Transformer层更不易出现此问题。为此，我们提出了一种无需训练的缓解框架，包含ZoomText策略和基于接地层修正的方法。此外，我们构建了TextHalu-Bench基准测试集，包含1730多个样本，用于评估模型的语义幻觉表现。实验表明，该方法不仅有效减少语义幻觉，还在公共基准测试中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05551" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:53:19 GMT</pubDate>
</item>
<item>
<title>基于AI工业资产生命周期管理的下一代自动化系统</title>
<link>https://arxiv.org/abs/2506.03828</link>
<guid>https://arxiv.org/abs/2506.03828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端自动化解决方案，整合工业资产全生命周期管理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了传统人工智能（AI）和机器学习（ML）方法在工业资产管理中的局限性，即仅专注于单一任务而非整体流程。通过引入AI代理和大型语言模型（LLMs），提出了实现工业资产全生命周期管理的下一代自动化机会。文中介绍了一个名为AssetOpsBench的统一框架和环境，旨在指导开发适用于第四次工业革命应用的领域特定代理。该框架强调感知、推理和控制的集成，以支持现实世界中的工业操作。AssetOpsBench软件已开源，有助于推动这一领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:57:35 GMT</pubDate>
</item>
<item>
<title>Simba: 通过层次化稀疏化提升状态空间模型性能</title>
<link>https://arxiv.org/abs/2505.20698</link>
<guid>https://arxiv.org/abs/2505.20698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Simba方法，通过层次化稀疏化改进状态空间模型，在相同计算预算下优于Mamba基线。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何通过稀疏化技术增强状态空间模型（SSMs）的性能，提出了Simba方法，该方法基于令牌剪枝对SSMs进行分层稀疏化处理。Simba在上层稀疏化更多，使上层更像信息高速公路，同时在底层保留更多细节信息。实验表明，Simba在多种自然语言任务中超越了具有相同浮点运算（FLOPS）的基线模型Mamba，不仅提高了效率，还改善了长序列中的信息流。此外，Simba引入了一种新颖的令牌剪枝标准，通过累积局部递归测量令牌对最终输出的全局影响。研究展示了Simba在自然语言处理任务中的优越性，并提供了开源代码以供进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:07:23 GMT</pubDate>
</item>
<item>
<title>STARFlow：基于归一化流的高分辨率图像合成生成模型</title>
<link>https://arxiv.org/abs/2506.06276</link>
<guid>https://arxiv.org/abs/2506.06276</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STARFlow结合归一化流和Transformer实现高效高分辨率图像合成。</p><br /><br /><p><strong>摘要：</strong> STARFlow是一种基于归一化流的可扩展生成模型，通过引入Transformer自回归流（TARFlow），在高分辨率图像合成任务上表现出色。TARFlow结合了归一化流的强大表达能力和自回归Transformer的结构建模能力，并证明了其在连续分布建模中的理论通用性。为了提升模型的可扩展性，STARFlow采用了深浅结合的设计架构，利用预训练自动编码器的潜在空间进行建模，并提出了一种新颖的引导算法以提高样本质量。作为端到端的归一化流模型，STARFlow支持连续空间下的精确最大似然训练。实验结果显示，STARFlow在条件图像生成任务中表现优异，接近当前最先进的扩散模型。本研究首次成功展示了归一化流在大规模和高分辨率图像生成中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06276" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:58:39 GMT</pubDate>
</item>
<item>
<title>Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence with Egocentric-Exocentric Vision</title>
<link>https://arxiv.org/abs/2506.06253</link>
<guid>https://arxiv.org/abs/2506.06253</guid>
<content:encoded><![CDATA[
Perceiving the world from both egocentric (first-person) and exocentric (third-person) perspectives is fundamental to human cognition, enabling rich and complementary understanding of dynamic environments. In recent years, allowing the machines to leverage the synergistic potential of these dual perspectives has emerged as a compelling research direction in video understanding. In this survey, we provide a comprehensive review of video understanding from both exocentric and egocentric viewpoints. We begin by highlighting the practical applications of integrating egocentric and exocentric techniques, envisioning their potential collaboration across domains. We then identify key research tasks to realize these applications. Next, we systematically organize and review recent advancements into three main research directions: (1) leveraging egocentric data to enhance exocentric understanding, (2) utilizing exocentric data to improve egocentric analysis, and (3) joint learning frameworks that unify both perspectives. For each direction, we analyze a diverse set of tasks and relevant works. Additionally, we discuss benchmark datasets that support research in both perspectives, evaluating their scope, diversity, and applicability. Finally, we discuss limitations in current works and propose promising future research directions. By synthesizing insights from both perspectives, our goal is to inspire advancements in video understanding and artificial intelligence, bringing machines closer to perceiving the world in a human-like manner. A GitHub repo of related works can be found at https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 13:25:48 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的编程竞赛测试用例生成系统</title>
<link>https://arxiv.org/abs/2506.05817</link>
<guid>https://arxiv.org/abs/2506.05817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用大语言模型生成高质量的编程竞赛测试用例。</p><br /><br /><p><strong>摘要：</strong> 编程竞赛因其高推理难度和精确反馈成为评估大语言模型推理能力的重要工具，但获取问题的测试用例具有挑战性。本文提出了一种基于大语言模型的代理系统，用于生成高质量的测试用例，并将其应用于CodeContests数据集，创建了改进版本CodeContests+。通过分析172万次提交记录，发现CodeContests+的测试用例准确性显著提高，尤其是True Positive Rate大幅提升。此外，在大语言模型强化学习实验中进一步验证了测试用例质量提升带来的显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 03:29:01 GMT</pubDate>
</item>
<item>
<title>PartCrafter：基于单张RGB图像的多部件3D网格联合生成模型</title>
<link>https://arxiv.org/abs/2506.05573</link>
<guid>https://arxiv.org/abs/2506.05573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PartCrafter是首个可同时生成多个语义明确且几何上不同的3D网格的结构化生成模型。</p><br /><br /><p><strong>摘要：</strong> PartCrafter是一种创新的3D生成模型，它能够从一张RGB图像中同时生成多个语义明确且几何上不同的3D网格。不同于现有的方法，PartCrafter采用了一种统一的组合生成架构，无需预先分割输入图像。该模型通过引入组合潜在空间和分层注意力机制，在生成过程中实现了全局一致性与部件细节的平衡。此外，为了支持部件级别的监督学习，研究团队创建了一个新的数据集，从中挖掘大规模3D对象数据集中的部件级标注。实验表明，PartCrafter在生成可分解的3D网格方面超越了现有方法，甚至能够生成输入图像中未直接可见的部分，展示了部件感知生成先验在3D理解和合成中的优势。未来，相关代码和训练数据将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 16:30:28 GMT</pubDate>
</item>
<item>
<title>MORSE-500：多模态推理新基准推动视觉语言模型发展</title>
<link>https://arxiv.org/abs/2506.05523</link>
<guid>https://arxiv.org/abs/2506.05523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MORSE-500视频基准测试，涵盖六类推理任务，显著提升对多模态智能的评估能力。</p><br /><br /><p><strong>摘要：</strong> 现有多模态推理基准存在三大不足：过度依赖静态图像、局限于数学问题解决、且容易饱和。本文引入MORSE-500，这是一个由500段脚本生成的视频数据集，包含嵌入式问题，覆盖抽象、物理、规划、空间及时间等六大推理类别。通过程序化生成技术，MORSE-500可精细控制视觉复杂度和时间动态，支持难度的系统性扩展。初步实验显示，当前最先进的视觉语言模型在各推理任务中存在明显性能差距，尤其在抽象和规划任务上表现欠佳。该数据集及相关工具已公开，旨在促进透明且可重复的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 15:12:45 GMT</pubDate>
</item>
<item>
<title>Sentinel: SOTA model to protect against prompt injections</title>
<link>https://arxiv.org/abs/2506.05446</link>
<guid>https://arxiv.org/abs/2506.05446</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) are increasingly powerful but remain vulnerable to prompt injection attacks, where malicious inputs cause the model to deviate from its intended instructions. This paper introduces Sentinel, a novel detection model, qualifire/prompt-injection-sentinel, based on the \answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced features and fine-tuning on an extensive and diverse dataset comprising a few open-source and private collections, Sentinel achieves state-of-the-art performance. This dataset amalgamates varied attack types, from role-playing and instruction hijacking to attempts to generate biased content, alongside a broad spectrum of benign instructions, with private datasets specifically targeting nuanced error correction and real-world misclassifications. On a comprehensive, unseen internal test set, Sentinel demonstrates an average accuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on public benchmarks, it consistently outperforms strong baselines like protectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's architecture, its meticulous dataset curation, its training methodology, and a thorough evaluation, highlighting its superior detection capabilities.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 10:07:15 GMT</pubDate>
</item>
<item>
<title>Prefix Grouper：一种高效增强版Group Relative Policy Optimization算法</title>
<link>https://arxiv.org/abs/2506.05433</link>
<guid>https://arxiv.org/abs/2506.05433</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Prefix Grouper通过消除冗余前缀计算提升GRPO训练效率。</p><br /><br /><p><strong>摘要：</strong> Group Relative Policy Optimization (GRPO)是一种有效的策略学习方法，但其处理长共享前缀时会引入大量冗余计算，导致训练效率低下。本文提出Prefix Grouper，通过Shared-Prefix Forward策略重构自注意力机制，仅需一次编码共享前缀，从而大幅降低计算开销。实验表明，Prefix Grouper与标准GRPO在训练等效性上表现一致，同时显著提升了长前缀场景下的训练效率。该方法具有良好的兼容性，可无缝集成到现有GRPO架构中，支持更大组规模，适用于更复杂的任务和更大模型的训练。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05433" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 05:13:37 GMT</pubDate>
</item>
<item>
<title>通过认知激活潜力优化多模态大语言模型推理能力的数据选择方法</title>
<link>https://arxiv.org/abs/2506.04755</link>
<guid>https://arxiv.org/abs/2506.04755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于认知激活潜力的新方法，显著提升多模态推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大型语言模型（MLLMs）在复杂推理任务中的训练数据需求问题，挑战了广泛认为需要大量数据的观点。研究发现，仅需一小部分具有认知价值的样本即可触发有意义的多模态推理，而其余大部分样本贡献有限。为此，我们提出了Reasoning Activation Potential（RAP）方法，通过因果差异估计器（CDE）和注意力置信估计器（ACE）识别这些认知样本，并引入难度感知替换模块（DRM）增强数据挑战性。实验结果显示，RAP方法仅使用9.3%的训练数据即取得最佳性能，同时降低了43%以上的计算成本。该研究为高效训练多模态推理模型提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 04:40:24 GMT</pubDate>
</item>
<item>
<title>一种结合光场渲染与物理模拟的实时机器人仿真框架</title>
<link>https://arxiv.org/abs/2506.04120</link>
<guid>https://arxiv.org/abs/2506.04120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合Gaussian Splatting与物体网格的混合场景表示方法，优化真实到模拟的转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的真实到模拟框架，旨在解决基于真实机器人运动创建精确物理模拟中的多个挑战，例如遮挡、相机姿态噪声及动态场景元素等。通过将3D高斯点绘图的光场渲染与适合物理模拟的显式物体网格融合于单一表示中，我们提出了一个端到端的优化管道，利用MuJoCo中的可微渲染与可微物理技术，联合优化场景的所有组成部分，包括物体几何形状、外观、机器人姿态和物理参数。这种方法允许同时实现高保真物体网格重建、生成逼真的新视角以及进行无需标注的机器人姿态校准。实验表明，该方法在模拟环境及实际复杂场景下均表现出色，显著提升了真实到模拟转换的实用性和鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:14:31 GMT</pubDate>
</item>
<item>
<title>探究多模态语言模型向全模态扩展的可行性</title>
<link>https://arxiv.org/abs/2506.01872</link>
<guid>https://arxiv.org/abs/2506.01872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态语言模型能否实现真正的全模态扩展。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了当前多模态语言模型在向全模态扩展时所面临的挑战，通过实验分析了扩展模态是否会影响核心的语言能力，独立训练的模态特定模型合并能否有效实现全模态能力，以及全模态扩展相比顺序扩展是否能更好地实现知识共享和泛化。研究发现，尽管现有模型在特定模态对上表现良好，但在真正实现全模态能力方面仍有显著差距，需要进一步优化以达到理想效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:01:40 GMT</pubDate>
</item>
<item>
<title>HASHIRU：一种灵活且高效的多智能体系统框架</title>
<link>https://arxiv.org/abs/2506.04255</link>
<guid>https://arxiv.org/abs/2506.04255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为HASHIRU的新多智能体系统框架，提升了灵活性与资源利用效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HASHIRU（Hierarchical Agent System for Hybrid Intelligent Resource Utilization）的新型多智能体系统（MAS）框架，旨在通过动态层级控制、资源感知的混合智能以及自主功能扩展，提升多智能体系统的灵活性、资源效率和适应性。HASHIRU采用“CEO”代理管理多个“员工”代理的方式，这些代理根据任务需求和资源限制动态实例化。其混合智能优先使用较小的本地大型语言模型（LLM），同时灵活调用外部API和更大规模的模型。该框架还设计了一个经济模型，通过雇佣/解雇成本促进团队稳定性和资源高效分配，并具备自主API工具创建及记忆功能。在学术论文评审、安全评估及复杂推理等任务中的测试表明，HASHIRU表现出色，例如在GSM8K任务上比Gemini 2.0 Flash高出35个百分点。此外，案例研究展示了系统通过自主成本模型生成、工具集成和预算管理实现自我改进的能力。HASHIRU的源代码和基准测试分别托管于GitHub和HashiruAgentX网站，提供开源访问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 13:33:16 GMT</pubDate>
</item>
<item>
<title>GUIDEX：提升零样本信息抽取性能的新方法</title>
<link>https://arxiv.org/abs/2506.00649</link>
<guid>https://arxiv.org/abs/2506.00649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GUIDEX通过自动定义领域特定模式提升大模型跨域信息抽取表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GUIDEX的新方法，该方法通过自动定义领域特定的模式、推导指南并生成合成标注实例，从而实现更好的跨域泛化能力。GUIDEX方法显著提升了零样本命名实体识别任务的表现，在七个基准测试中达到了新的技术水平。与之前的方法相比，使用GUIDEX训练的模型在没有人工标注数据的情况下提高了多达7个F1点数，而结合人工标注数据后进一步提升了近2个F1点数。此外，GUIDEX还增强了模型对复杂领域特定标注模式的理解能力。目前，GUIDEX的相关代码、模型及合成数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 13:36:18 GMT</pubDate>
</item>
<item>
<title>EverGreenQA：首个带时间稳定性标签的多语言问答数据集及其应用</title>
<link>https://arxiv.org/abs/2505.21115</link>
<guid>https://arxiv.org/abs/2505.21115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个带有时间稳定性标签的多语言问答数据集，评估大模型对问题时间性的编码能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在问答任务中经常出现幻觉现象，而问题的时间性（永恒性或变化性）是导致这一问题的关键但尚未充分探索的因素。本文介绍了一个名为EverGreenQA的新数据集，该数据集首次为多语言问答任务提供了永恒性标签，支持评估和训练。通过此数据集，我们对12个现代LLM进行基准测试，以评估它们是否明确（通过口头判断）或隐含（通过不确定性信号）地编码了问题的时间性。此外，我们还训练了一个轻量级多语言分类器EG-E5，在该任务上实现了最先进的性能。最后，我们在三个实际应用中展示了永恒性分类的实用性，包括提升自我知识估计、过滤问答数据集以及解释GPT-4o检索行为。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:35:13 GMT</pubDate>
</item>
<item>
<title>基于3D光流的世界模型实现跨机器人操控技能迁移</title>
<link>https://arxiv.org/abs/2506.06199</link>
<guid>https://arxiv.org/abs/2506.06199</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过学习人类和机器人的操作数据，构建3D光流世界模型提升机器人操控能力。</p><br /><br /><p><strong>摘要：</strong> 长期以来，机器人操控复杂物体任务面临挑战，而人类却能轻松完成类似将杯子挂到架子上的动作，主要原因是缺乏大规模统一的数据集用于训练机器人操控技能。现有数据集多局限于简单场景中的单一动作空间记录，难以让机器人学习适用于多样化场景的不同设备统一且稳健的动作表示。受人类理解操作任务方式的启发，我们发现理解物体在三维空间中的移动方式是指导动作的关键线索，这一线索与具体形态无关，适用于人类及不同机器人。基于此，我们提出从人类和机器人操控数据中学习3D光流世界模型的方法，该模型可预测交互对象在三维空间中的未来运动轨迹，从而指导操控动作规划。为此，我们合成了一套大规模3D光流数据集ManiFlow-110k，并利用基于视频扩散的世界模型从中学习操控物理规律，生成根据语言指令条件下的3D光流轨迹。通过引入基于光流引导渲染机制，机器人能够预测最终状态并验证光流是否符合任务描述，进而获得闭环规划能力。最后，我们将预测的3D光流作为优化策略的约束条件，确定一系列机器人操控动作。大量实验表明，该方法在多种机器人操控任务中展现出强大的泛化能力和可靠的跨形态适应性，无需针对特定硬件进行训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.06199" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 12:00:31 GMT</pubDate>
</item>
<item>
<title>基于音频感知大语言模型的演讲风格自动评估</title>
<link>https://arxiv.org/abs/2506.05984</link>
<guid>https://arxiv.org/abs/2506.05984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索音频感知大语言模型作为自动评委评估演讲风格的能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了利用音频感知大语言模型（ALLMs）作为自动评判工具来评估语音合成模型（SLMs）生成演讲的风格一致性，包括情感、音量、语速、词语重音、音高控制及非言语元素等方面。通过在语音指令遵循和角色扮演两项任务中的实验，发现Gemini-2.5-pro与人类评价者之间的协议程度与人类评价者之间相当，表明ALLMs可以有效评估SLMs生成的对话自然度与风格控制能力。然而，当前SLMs，即使如GPT-4o-audio，仍需改进以更好地实现对演讲风格的精准控制与自然对话生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 06 Jun 2025 07:05:48 GMT</pubDate>
</item>
<item>
<title>基于输入依赖软提示的参数高效微调方法</title>
<link>https://arxiv.org/abs/2506.05629</link>
<guid>https://arxiv.org/abs/2506.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的软提示微调技术，提升大模型领域特定任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过软提示实现参数高效微调的方法，以解决领域特定任务中大语言模型微调成本高和技术难度大的问题。我们提出了一种名为ID-SPAM的新技术，该技术结合自注意力机制，根据输入生成动态软提示，并对不同重要性令牌进行关注。实验表明，ID-SPAM方法不仅简单高效，而且在多个任务上超越了现有最先进的方法，同时显著提高了零样本跨域迁移能力。这种方法通过少量可训练参数实现了对下游任务的灵活适配，具有重要的研究价值和实际应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 19:13:22 GMT</pubDate>
</item>
<item>
<title>FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion</title>
<link>https://arxiv.org/abs/2506.01111</link>
<guid>https://arxiv.org/abs/2506.01111</guid>
<content:encoded><![CDATA[
High-quality, large-scale audio captioning is crucial for advancing audio understanding, yet current automated methods often generate captions that lack fine-grained detail and contextual accuracy, primarily due to their reliance on limited unimodal or superficial multimodal information. Drawing inspiration from human auditory perception, which adeptly integrates cross-modal cues and performs sophisticated auditory scene analysis, we introduce a novel two-stage automated pipeline. This pipeline first employs specialized pretrained models to extract diverse contextual cues (e.g., speech, music, general sounds, and visual information from associated video). A large language model (LLM) then synthesizes these rich, multimodal inputs to generate detailed and context-aware audio captions. Key contributions of this work include: (1) the proposed scalable method for fine-grained audio caption generation; (2) FusionAudio, a new large-scale dataset comprising 1.2 million such detailed captions, combined with 6 million QA pairs; and (3) enhanced audio models developed using FusionAudio, specifically a CLAP-based audio encoder with superior audio-text alignment and instruction following. This paper paves the way for more nuanced and accurate automated understanding of complex audio environments. Code and data can be found in https://github.com/satsuki2486441738/FusionAudio.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 14:29:17 GMT</pubDate>
</item>
<item>
<title>重新审视测试时间扩展定律：基于稀疏注意力的新范式</title>
<link>https://arxiv.org/abs/2506.05333</link>
<guid>https://arxiv.org/abs/2506.05333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现更大模型在测试时间扩展中的有效性被低估，提出新的Kinetics扩展定律。</p><br /><br /><p><strong>摘要：</strong> 本文从实际效率角度重新审视测试时间扩展定律，揭示小模型的有效性被高估的问题。传统研究基于计算最优性，忽略了推理策略（如Best-of-N、长CoTs）引入的内存访问瓶颈。通过分析0.6B到32B参数范围内的模型，我们提出了新的Kinetics扩展定律，该定律结合计算和内存访问成本指导资源分配。研究表明，测试时间计算在大模型上比小模型更有效，因为注意力而非参数数量成为主要成本因素。受此启发，我们提出了一种基于稀疏注意力的新扩展范式，显著提升了问题解决的准确性，并在低成本和高成本场景下均表现出色。实验结果表明，稀疏注意力模型优于密集模型，在AIME上的准确性提升超过60分（低成本）和5分（高成本）。这些发现表明稀疏注意力对于实现测试时间扩展的全部潜力至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:24 GMT</pubDate>
</item>
<item>
<title>FlowDirector：无需反演的数据空间文本驱动视频编辑框架</title>
<link>https://arxiv.org/abs/2506.05046</link>
<guid>https://arxiv.org/abs/2506.05046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种无需反演的视频编辑框架FlowDirector，显著提升视频编辑的时序一致性与结构保真度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FlowDirector的新颖文本驱动视频编辑框架，旨在解决现有基于反演技术的方法中存在的时序不一致及结构保真度下降的问题。FlowDirector通过将编辑过程建模为数据空间中的直接演化，利用常微分方程(ODE)引导视频沿其固有的时空流形平滑过渡，从而保持时间上的连贯性和结构细节。此外，为了实现局部化的可控编辑，引入了注意力引导的掩码机制来调节ODE的速度场；同时，受无分类器指导思想的启发，提出了增强指导策略，通过多候选流之间的差分信号来引导编辑轨迹，以强化语义对齐而不损害结构一致性。大量实验表明，FlowDirector在指令遵从性、时序一致性以及背景保存方面均达到了当前最佳性能，为高效且连贯的视频编辑建立了新的范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:54:40 GMT</pubDate>
</item>
<item>
<title>Search Arena: Analyzing Search-Augmented LLMs</title>
<link>https://arxiv.org/abs/2506.05334</link>
<guid>https://arxiv.org/abs/2506.05334</guid>
<content:encoded><![CDATA[
Search-augmented language models combine web search with Large Language Models (LLMs) to improve response groundedness and freshness. However, analyzing these systems remains challenging: existing datasets are limited in scale and narrow in scope, often constrained to static, single-turn, fact-checking questions. In this work, we introduce Search Arena, a crowd-sourced, large-scale, human-preference dataset of over 24,000 paired multi-turn user interactions with search-augmented LLMs. The dataset spans diverse intents and languages, and contains full system traces with around 12,000 human preference votes. Our analysis reveals that user preferences are influenced by the number of citations, even when the cited content does not directly support the attributed claims, uncovering a gap between perceived and actual credibility. Furthermore, user preferences vary across cited sources, revealing that community-driven platforms are generally preferred and static encyclopedic sources are not always appropriate and reliable. To assess performance across different settings, we conduct cross-arena analyses by testing search-augmented LLMs in a general-purpose chat environment and conventional LLMs in search-intensive settings. We find that web search does not degrade and may even improve performance in non-search settings; however, the quality in search settings is significantly affected if solely relying on the model's parametric knowledge. We open-sourced the dataset to support future research in this direction. Our dataset and code are available at: https://github.com/lmarena/search-arena.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>基于CLIP空间的材料编辑方法MARBLE</title>
<link>https://arxiv.org/abs/2506.05313</link>
<guid>https://arxiv.org/abs/2506.05313</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用CLIP空间进行材料混合与重组的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MARBLE的方法，用于通过在CLIP空间中找到材料嵌入来执行精细材料属性的混合和重新组合。该方法通过定位去噪UNet中的材料属性相关层，实现了基于示例图像的材料编辑改进。此外，通过浅层网络预测方向，可以实现对粗糙度、金属感、透明度和发光等精细材料属性的参数化控制。实验表明，MARBLE不仅能在单次前向传递中完成多种编辑，还适用于绘画场景。项目页面提供了更多细节和演示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05313" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:55:16 GMT</pubDate>
</item>
<item>
<title>FEAT：一种高效的全维度注意力Transformer用于动态医学视频合成</title>
<link>https://arxiv.org/abs/2506.04956</link>
<guid>https://arxiv.org/abs/2506.04956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FEAT模型，通过创新机制解决现有Transformer在动态医学视频合成中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文针对动态医学视频合成中空间一致性与时间动态建模的需求，提出了FEAT（Full-dimensional Efficient Attention Transformer）。该模型通过引入序列化的时空通道注意力机制、线性复杂度的注意力设计以及残差值引导模块，有效解决了现有基于Transformer方法中通道交互不足、自注意力计算复杂度高及去噪指导粗糙的问题。实验表明，FEAT-S参数量仅为当前最优模型Endora的23%，却展现出相当甚至更高的性能；而FEAT-L在多个数据集上超越所有对比方法，证明了其卓越的有效性和可扩展性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 08:31:02 GMT</pubDate>
</item>
<item>
<title>基于缩放定律的语言视觉模型CLIP与MaMMUT的比较研究</title>
<link>https://arxiv.org/abs/2506.04598</link>
<guid>https://arxiv.org/abs/2506.04598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过缩放定律对CLIP和MaMMUT进行跨尺度比较，揭示MaMMUT在扩展性和样本效率上的优势。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用缩放定律对基础模型及数据集进行比较的方法，特别是在语言-视觉学习领域，首次基于密集测量推导出对比学习方法CLIP和结合对比与描述性文本生成损失的MaMMUT的完整缩放定律。研究表明，随着规模扩大，MaMMUT展现出更强的性能提升及更高的样本效率。通过对多个下游任务（分类、检索、分割）和开放数据集（DataComp、DFN、Re-LAION）的一致性分析，进一步验证了这一结论。此外，我们展示了在恒定学习率下推导缩放定律的可能性，从而降低计算成本。本研究提供了系统评估和改进开放基础模型及数据集的重要工具，同时发布了所有预训练模型及其中间检查点，包括开放版MaMMUT-L/14，其在ImageNet-1k上的零样本准确率为80.3%。相关代码和实验数据可在指定GitHub仓库获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 23:35:59 GMT</pubDate>
</item>
<item>
<title>通过推理对齐的视觉描述优化提升多模态大语言模型性能</title>
<link>https://arxiv.org/abs/2506.04559</link>
<guid>https://arxiv.org/abs/2506.04559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法RACRO，通过优化视觉描述增强多模态大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，慢思考语言模型在复杂推理任务中展现了接近人类的反思认知能力。然而，将这些能力扩展到多模态大语言模型面临挑战，尤其是在升级底层推理器时需要高昂的重新训练成本。一种直接解决方案是解耦感知与推理，即将视觉输入转换为语言表示（如描述），然后传递给强大的文本推理器。但这种方法引入了一个关键挑战：视觉提取器生成的描述必须既忠实于图像又足够丰富，以支持准确的下游推理。为了解决这一问题，我们提出了通过描述奖励优化实现推理对齐的视觉解耦（RACRO），这是一种推理引导的强化学习策略，使提取器的描述行为与推理目标对齐。通过基于奖励的优化，RACRO显著增强了视觉定位并提取了推理优化的表示。在多模态数学和科学基准测试中，RACRO方法达到了最先进的平均性能，同时实现了卓越的可扩展性和对更先进推理语言模型的即插即用适应性，而无需昂贵的多模态重新对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 22:28:07 GMT</pubDate>
</item>
<item>
<title>大型语言模型水印技术对对齐属性的影响及改进方法</title>
<link>https://arxiv.org/abs/2506.04462</link>
<guid>https://arxiv.org/abs/2506.04462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示两种主流水印方法对大型语言模型对齐属性的影响，并提出采样方法恢复对齐。</p><br /><br /><p><strong>摘要：</strong> 本文系统分析了两种流行的大语言模型水印方法（Gumbel 和 KGW）对四个对齐模型的核心对齐属性（真实性、安全性、实用性）的影响。实验发现两种降级模式：守卫衰减（增强实用性损害安全性）和守卫放大（过度谨慎降低实用性）。这些模式源于水印引起的标记分布变化，揭示了对齐目标之间的基本矛盾。为缓解这些问题，我们提出了对齐重采样（AR），一种利用外部奖励模型恢复对齐的推理时采样方法。理论分析表明增加样本数量可提高期望奖励分数，实验证明仅需重采样2-4次即可恢复或超越基线对齐分数。此外，通过牺牲严格无失真性，我们的修改版本保证了与AR的兼容性。实验结果证明AR成功恢复了两种水印方法的基线对齐性能，同时保持了强大的水印检测能力。这项工作揭示了水印强度与模型对齐之间的重要平衡，并提供了实用的推理时解决方案，以负责任地部署水印大语言模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 17:29:07 GMT</pubDate>
</item>
<item>
<title>DOVE：一种动态视觉编码器实现高效语义特征提取</title>
<link>https://arxiv.org/abs/2506.03643</link>
<guid>https://arxiv.org/abs/2506.03643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态视觉编码器DOVE，根据图像复杂度自适应生成视觉tokens。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视觉编码器忽视图像信息量差异的问题，提出了DOVE（Dynamic Vision Encoder），它能够根据图像的复杂程度动态生成不同数量的视觉tokens以重建图像。实验表明，DOVE不仅显著减少了平均token数量，同时保持了高质量的图像重建效果。在多个线性探测和下游多模态任务中，DOVE在使用更少token的情况下表现优于现有的基于自动编码器的方法，捕捉到更具表达性的语义特征。此外，通过引入查询条件化token化，DOVE进一步提升了目标导向的语义提取效率。代码和模型权重已公开。关键词：动态编码、视觉tokens、语义特征。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 03:40:33 GMT</pubDate>
</item>
<item>
<title>同步扩散框架生成高保真手物交互视频与运动序列</title>
<link>https://arxiv.org/abs/2506.02444</link>
<guid>https://arxiv.org/abs/2506.02444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合视觉先验和动态约束的同步扩散框架，实现手物交互视频与运动的联合生成。</p><br /><br /><p><strong>摘要：</strong> 当前三维手物交互（HOI）运动生成方法依赖预定义模型和实验室捕捉数据，限制了泛化能力；而HOI视频生成方法虽注重像素级视觉保真，却常牺牲物理合理性。鉴于视觉外观与运动模式在现实世界中共享基本物理定律，本文提出一种新颖框架，在同步扩散过程中结合视觉先验和动态约束，同时生成HOI视频与运动。通过三模态自适应调制整合异构语义特征，结合3D全注意力建模模态间依赖关系，引入视觉感知的3D交互扩散模型直接生成显式交互序列，并反馈形成闭环。该架构无需依赖预定义对象模型或明确姿态指导，显著提升视频-运动一致性。实验表明，该方法在生成高保真、动态合理的HOI序列方面优于现有技术，尤其在未知真实场景中展现出显著的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 01:04:29 GMT</pubDate>
</item>
<item>
<title>自监督模型学习的语言特异性语音表征研究</title>
<link>https://arxiv.org/abs/2506.00981</link>
<guid>https://arxiv.org/abs/2506.00981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，Wav2Vec2模型在荷兰语预训练比英语或多语言预训练更能有效表征荷兰语的语音特征。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了自监督模型（如Wav2Vec2）学习到的语音表征在多大程度上具有语言特异性。尽管已有研究表明，从仅基于语音录音训练的端到端模型中可以成功解码多种语言学特征，但这些模型在特定语言上的预训练对增强语言特异性语言信息的作用尚不明确。本研究通过测试Wav2Vec2模型内部表征中荷兰语的语音和词汇信息，发现仅用荷兰语进行预训练相较于用相似量级的英语或更大规模的多语言数据进行预训练，在表征荷兰语语言特征方面表现更优。这种语言特异性优势可以通过经过训练的聚类或分类探针显著检测到，部分也可以通过零样本指标观察到。此外，这种语言特异性优势与自动语音识别下游任务的表现相一致。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:25:13 GMT</pubDate>
</item>
<item>
<title>VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal Understanding in Videos</title>
<link>https://arxiv.org/abs/2506.05349</link>
<guid>https://arxiv.org/abs/2506.05349</guid>
<content:encoded><![CDATA[
Mathematical reasoning in real-world video settings presents a fundamentally different challenge than in static images or text. It requires interpreting fine-grained visual information, accurately reading handwritten or digital text, and integrating spoken cues, often dispersed non-linearly over time. In such multimodal contexts, success hinges not just on perception, but on selectively identifying and integrating the right contextual details from a rich and noisy stream of content. To this end, we introduce VideoMathQA, a benchmark designed to evaluate whether models can perform such temporally extended cross-modal reasoning on videos. The benchmark spans 10 diverse mathematical domains, covering videos ranging from 10 seconds to over 1 hour. It requires models to interpret structured visual content, understand instructional narratives, and jointly ground concepts across visual, audio, and textual modalities. We employ graduate-level experts to ensure high quality, totaling over 920 man-hours of annotation. To reflect real-world scenarios, questions are designed around three core reasoning challenges: direct problem solving, where answers are grounded in the presented question; conceptual transfer, which requires applying learned methods to new problems; and deep instructional comprehension, involving multi-step reasoning over extended explanations and partially worked-out solutions. Each question includes multi-step reasoning annotations, enabling fine-grained diagnosis of model capabilities. Through this benchmark, we highlight the limitations of existing approaches and establish a systematic evaluation framework for models that must reason, rather than merely perceive, across temporally extended and modality-rich mathematical problem settings. Our benchmark and evaluation code are available at: https://mbzuai-oryx.github.io/VideoMathQA
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>FreeTimeGS：一种用于复杂动态场景重建的4D表示方法</title>
<link>https://arxiv.org/abs/2506.05348</link>
<guid>https://arxiv.org/abs/2506.05348</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的4D表示方法，解决复杂运动场景的动态视图合成问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了复杂运动场景动态三维重建的挑战。近期一些研究通过在规范空间定义三维高斯基元并利用变形场将规范基元映射到观测空间，实现了实时动态视图合成。然而，这些方法在优化变形场时常常难以处理复杂运动场景。为了解决这一问题，我们提出了FreeTimeGS，这是一种新颖的4D表示方法，允许高斯基元出现在任意时间和位置。与规范高斯基元相比，我们的表示具有更强的灵活性，从而提高了对动态三维场景建模的能力。此外，我们为每个高斯基元赋予了一个运动函数，使其能够在时间上移动到相邻区域，这减少了时间冗余。实验结果表明，我们的方法在多个数据集上的渲染质量显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05348" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>通过动态内存稀疏化提升Transformer大模型推理精度</title>
<link>https://arxiv.org/abs/2506.05345</link>
<guid>https://arxiv.org/abs/2506.05345</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过压缩KV缓存实现推理时间超缩放，显著提升Transformer大模型推理准确性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在推理阶段通过增加生成序列长度或并行度来提高Transformer大语言模型推理精度的方法，但发现生成成本受制于键值（KV）缓存大小而非生成令牌数量。因此，我们探索了推理时间的超缩放方法，即通过压缩KV缓存，在相同的计算预算下生成更多令牌，从而进一步提高推理精度。然而，这种方法的成功依赖于压缩方法在高压缩比下仍能保持准确性。为使超缩放实用化，我们提出了动态内存稀疏化（DMS），这是一种新颖的KV缓存稀疏化方法，仅需1K训练步骤即可实现8倍压缩，同时优于无需训练的稀疏注意力方法。DMS通过延迟缓存令牌的淘汰，隐式合并表示并保留关键信息。实验表明，结合DMS的推理时间超缩放在多个LLM家族上有效提升了准确性，例如在AIME 24、GPQA和LiveCodeBench上分别提高了Qwen-R1 32B的平均分数9.1、7.6和9.6分。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05345" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>SeedVR2：基于扩散模型的一阶段高分辨率视频修复方法</title>
<link>https://arxiv.org/abs/2506.05301</link>
<guid>https://arxiv.org/abs/2506.05301</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种一阶段扩散模型SeedVR2，显著提升视频修复效率并保持高质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SeedVR2的新一代基于扩散模型的视频修复方法，旨在解决现有视频修复技术计算成本高昂的问题。SeedVR2通过引入自适应窗口注意力机制和一系列损失函数优化，实现了高效且高质量的一阶段视频修复。实验表明，该模型在处理高分辨率视频时表现出色，性能可媲美甚至超越现有方法。研究还展示了如何利用对抗训练稳定模型，并通过特征匹配损失改进了训练过程。这些创新使得SeedVR2成为高分辨率视频修复领域的重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05301" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:51:05 GMT</pubDate>
</item>
<item>
<title>基于几何引导长期空间记忆的视频世界模型一致性增强</title>
<link>https://arxiv.org/abs/2506.05284</link>
<guid>https://arxiv.org/abs/2506.05284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入几何引导的空间记忆机制提升视频世界模型的长期一致性。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有世界模型在处理长时间视频时因时间窗口限制导致场景一致性差的问题，受到人类记忆机制的启发，提出了一种新颖的框架，利用几何引导的长期空间记忆来增强视频世界模型的长期一致性。该框架设计了存储和检索长期空间记忆的机制，并构建了定制的数据集对具有显式存储三维记忆机制的世界模型进行训练和评估。实验结果显示，与相关基线相比，所提出的方法在生成质量、一致性以及上下文长度方面均有显著改善，为实现长期一致的视频生成奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:42:34 GMT</pubDate>
</item>
<item>
<title>Diagonal Batching优化循环记忆Transformer模型的长上下文推理</title>
<link>https://arxiv.org/abs/2506.05229</link>
<guid>https://arxiv.org/abs/2506.05229</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Diagonal Batching方法，大幅提升循环记忆Transformer在长上下文推理中的效率。</p><br /><br /><p><strong>摘要：</strong> Transformer模型在处理长上下文时面临时间复杂度和内存使用的挑战，而循环记忆Transformer（RMTs）通过降低计算成本解决了这一问题，但其内存更新机制导致执行顺序受限。本文引入Diagonal Batching调度方案，在保留精确递归的同时解锁了段间并行性，显著提升了GPU推理性能。该技术无需重新训练即可应用于现有RMT模型，实验证明其在LLaMA-1B上的加速效果显著，尤其适用于超长序列输入。Diagonal Batching消除了执行瓶颈，降低了推理成本和延迟，使RMT成为实际应用中处理长上下文问题的有效解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05229" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:43:48 GMT</pubDate>
</item>
<item>
<title>基于开源许可文本的大规模语言模型训练数据集Common Pile v0.1发布</title>
<link>https://arxiv.org/abs/2506.05209</link>
<guid>https://arxiv.org/abs/2506.05209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究团队发布了一个8TB开源许可文本数据集Common Pile v0.1用于训练大规模语言模型。</p><br /><br /><p><strong>摘要：</strong> 目前，大型语言模型（LLMs）的训练主要依赖大量未授权文本，这引发了知识产权侵权和伦理问题的关注。尽管利用开放许可文本进行训练是解决问题的第一步，但之前的数据收集工作得到的数据集要么规模过小，要么质量不佳，难以支持高性能LLMs的开发。为了填补这一空白，本文介绍并发布了Common Pile v0.1，这是一个由30个来源组成的8TB开源许可文本集合，涵盖了科研论文、代码、书籍、百科全书、教育材料、音频转录等多种领域内容。该数据集经过验证后，用于训练两个参数量为70亿的LLMs——Comma v0.1-1T和Comma v0.1-2T，分别基于1万亿和2万亿标记进行训练。实验表明，这两个模型在性能上可与采用相似计算预算的未经许可文本训练的LLMs（如Llama 1和Llama 2 7B）相媲美。此外，除了发布Common Pile v0.1数据集外，研究团队还公开了数据集创建所使用的代码、训练混合数据集以及Comma v0.1模型的检查点。这些成果为推动LLMs的可持续发展提供了重要的资源和支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:21:30 GMT</pubDate>
</item>
<item>
<title>基于技能感知的时间采样方法提升运动技能自动化评估</title>
<link>https://arxiv.org/abs/2506.04996</link>
<guid>https://arxiv.org/abs/2506.04996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的时间采样策略PATS，提升多视角运动技能评估精度。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前视频采样方法破坏动作连续性的问题，提出了一种名为Proficiency-Aware Temporal Sampling（PATS）的新策略。PATS通过自适应分割视频，在多个连续片段中捕捉完整的动作模式，从而实现多视角技能评估。实验表明，PATS在EgoExo4D基准测试中超越现有技术，在多种场景下显著提高评估准确性，尤其在攀岩（+26.22%）、音乐表演（+2.39%）和篮球（+1.13%）等挑战性领域表现优异。此外，PATS能灵活适应不同活动特性，展现出强大的实用性，为真实世界中的技能自动化评估提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:05:23 GMT</pubDate>
</item>
<item>
<title>Surfer-H：结合视觉语言模型的高效网络代理</title>
<link>https://arxiv.org/abs/2506.02865</link>
<guid>https://arxiv.org/abs/2506.02865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">集成VLM的Surfer-H在WebVoyager上达到92.2%的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Surfer-H，一种成本效益高的网络代理系统，它通过整合视觉语言模型（Vision-Language Models, VLM）来执行用户定义的任务。Surfer-H与Holo1协同工作，Holo1是一个新的开放权重集合的VLM，专门用于网络导航和信息提取。Holo1经过精心策划的数据源训练，包括开放访问的网络内容、合成示例和自动生成的自主数据。Holo1在通用用户界面（UI）基准测试以及新的网络UI本地化基准测试WebClick中表现出色。当由Holo1驱动时，Surfer-H在WebVoyager上的表现达到了92.2%的状态-of-the-art水平，实现了准确性与成本效益之间的帕累托最优平衡。为了加速自主系统的研发进展，我们开源了WebClick评估数据集和Holo1模型权重。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 09:29:03 GMT</pubDate>
</item>
<item>
<title>RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS</title>
<link>https://arxiv.org/abs/2506.02751</link>
<guid>https://arxiv.org/abs/2506.02751</guid>
<content:encoded><![CDATA[
3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling scenes affected by transient objects, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances. To address this, we propose RobustSplat, a robust solution based on two critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method. Our project page is https://fcyycf.github.io/RobustSplat/.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 07:13:48 GMT</pubDate>
</item>
<item>
<title>基于鸟瞰图特征的LiDAR-相机标定模型BEVCALIB</title>
<link>https://arxiv.org/abs/2506.02587</link>
<guid>https://arxiv.org/abs/2506.02587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个利用鸟瞰图特征实现原始数据标定的模型，显著提升自动驾驶和机器人系统的多模态感知融合。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BEVCALIB的新模型，通过分别提取相机和激光雷达的鸟瞰图特征并融合至共享特征空间，实现了从原始数据进行LiDAR-相机标定的目标。为了充分利用几何信息，模型引入了新颖的特征选择器，优化了变换解码器的效率。在KITTI、NuScenes及自建数据集上的实验表明，BEVCALIB在多种噪声条件下显著优于现有方法，尤其在平移和旋转误差上平均提升了47.08%和82.32%（KITTI）以及78.17%和68.29%（NuScenes）。此外，在开源领域，该模型的性能较最佳可复现基线高出一个数量级。代码和演示结果已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:07:18 GMT</pubDate>
</item>
<item>
<title>面向自回归图像生成模型的抗再生攻击水印框架</title>
<link>https://arxiv.org/abs/2506.01011</link>
<guid>https://arxiv.org/abs/2506.01011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种针对自回归模型的新型水印框架，有效抵抗再生攻击。</p><br /><br /><p><strong>摘要：</strong> 自回归（AR）图像生成模型因其高质量合成效果而受到关注，但其潜在的滥用问题需要强大的水印技术。然而，现有的生成过程中嵌入水印的方法主要针对扩散模型设计，在潜空间嵌入水印的方式难以直接适应AR模型。此外，扩散模型的再生攻击可以有效消除这些水印。为解决这些问题，本文提出了Lexical Bias Watermarking（LBW），这是一种专门设计用于AR模型的新型水印框架，能够抵御再生攻击。LBW通过在生成过程中偏向预先定义的“绿名单”来直接将水印嵌入到标记映射中，从而实现与现有AR模型的无缝集成，并可扩展到后处理水印。为了提高对白盒攻击的安全性，每个图像的绿名单从多个绿名单池中随机采样。水印检测通过标记分布的量化和统计分析完成。大量实验表明，LBW在抵抗再生攻击方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 09:44:20 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的3D占用预测在自动驾驶中的应用</title>
<link>https://arxiv.org/abs/2505.23115</link>
<guid>https://arxiv.org/abs/2505.23115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用扩散模型重新定义3D占用预测任务，提升预测精度与鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法，将3D占用网格预测视为一种生成建模任务，通过采用扩散模型来学习数据分布并结合3D场景先验知识，从而有效应对噪声数据、不完整观测及复杂结构问题。实验表明，基于扩散模型的方法不仅在预测一致性、抗噪能力上优于当前最先进的判别方法，而且在遮挡或低可见度区域的表现尤为突出。此外，改进后的预测结果显著提升了下游规划任务的效果，展示了该方法在实际自动驾驶应用中的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 01:34:22 GMT</pubDate>
</item>
<item>
<title>SparseMM：通过视觉注意力稀疏性优化多模态大语言模型推理</title>
<link>https://arxiv.org/abs/2506.05344</link>
<guid>https://arxiv.org/abs/2506.05344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLMs中仅少数注意力头参与视觉理解，并提出SparseMM加速多模态推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大语言模型（MLLMs）如何处理视觉输入，发现仅不到5%的注意力头（称为视觉头）对视觉理解有贡献。我们设计了一个无需训练的框架，通过目标响应分析量化各注意力头的视觉相关性，从而高效识别这些视觉头。基于此发现，我们提出了SparseMM，这是一种KV缓存优化策略，根据注意力头的视觉评分分配非对称计算预算，利用视觉头的稀疏性加速MLLMs推理。与忽视视觉特性的现有KV缓存加速方法相比，SparseMM在解码过程中优先保持视觉语义。广泛的主流多模态基准评估表明，SparseMM在实现1.38倍实时加速和52%内存减少的同时，保持了性能一致性。我们的项目已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>MINT-CoT：引入数学嵌入标记的视觉推理链式思维方法</title>
<link>https://arxiv.org/abs/2506.05331</link>
<guid>https://arxiv.org/abs/2506.05331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MINT-CoT模型，通过嵌入视觉标记提升多模态数学问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型在多模态数学问题解决中的局限性，提出了一种名为MINT-CoT的新方法。MINT-CoT通过引入数学嵌入标记（Mathematical INterleaved Tokens），实现了视觉信息与文本推理的动态整合，从而改进了数学问题的视觉推理能力。该方法利用Interleave Token动态选择数学图形中的任意形状区域，有效解决了传统方法依赖粗粒度图像区域、视觉编码器对数学内容感知有限以及对外部视觉修改能力的依赖等问题。为了支持这一能力，我们构建了一个包含54K道数学问题的数据集MINT-CoT，并设计了三阶段训练策略。实验表明，MINT-CoT在MathVista、GeoQA和MMStar等基准测试中显著优于基线模型，分别提升了34.08%、28.78%和23.2%的性能。我们的代码和数据已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>CG-AV-Counting基准与AV-Reasoner模型提升视频计数能力</title>
<link>https://arxiv.org/abs/2506.05328</link>
<guid>https://arxiv.org/abs/2506.05328</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CG-AV-Counting基准和AV-Reasoner模型，解决现有视频理解模型在计数任务中的不足。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大规模语言模型在计数任务上表现欠佳，而现有的评估基准存在视频长度有限、查询集封闭、缺乏线索标注及弱多模态覆盖等问题。为了解决这些问题，本文引入了CG-AV-Counting，这是一个包含1027个多模态问题和5845个注释线索的长视频计数基准。该基准支持黑盒和白盒评估，可作为端到端和基于推理计数方法的全面测试平台。同时，我们提出了AV-Reasoner模型，通过GRPO和课程学习训练，从相关任务中泛化计数能力。实验表明，AV-Reasoner在多个基准上取得了最先进的成果，但跨领域基准上的性能提升仍需进一步研究。代码和基准现已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05328" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>基于PM-Loss的深度图优化提升3D高斯点云渲染质量</title>
<link>https://arxiv.org/abs/2506.05327</link>
<guid>https://arxiv.org/abs/2506.05327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PM-Loss正则化损失，优化深度图以提升3D高斯点云渲染效果。</p><br /><br /><p><strong>摘要：</strong> 深度图在前馈3D高斯点云（3DGS）管道中通过反投影生成3D点云，用于新视图合成，具有高效训练、已知相机姿态利用及几何估计准确等优势。然而，物体边界处的深度不连续性常导致点云碎片化或稀疏化，影响渲染质量。为此，我们引入了基于预训练Transformer预测点映射的PM-Loss新型正则化损失，尽管点映射可能不如深度图精确，但能有效增强几何平滑性，尤其是在物体边界区域。借助改进的深度图，我们的方法显著提升了多种架构和场景下的前馈3DGS性能，提供了更优质的渲染结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:58:23 GMT</pubDate>
</item>
<item>
<title>EOC-Bench：面向动态第一人称场景的对象中心认知评估基准</title>
<link>https://arxiv.org/abs/2506.05287</link>
<guid>https://arxiv.org/abs/2506.05287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EOC-Bench基准，系统评估多模态大语言模型在动态第一人称场景中的对象中心认知能力。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）的兴起推动了第一人称视觉应用的发展，但现有具身基准主要关注静态场景探索，忽视了用户交互引发的动态变化评估。为此，我们引入EOC-Bench，这是一个创新性基准，包含3,277个精心标注的问题对，分为过去、现在和未来三个时间类别，涵盖11个细粒度评价维度和3种视觉对象引用类型。我们开发了一种混合格式的人在回路标注框架，并设计了新的多尺度时间准确性指标进行开放时间评估。基于EOC-Bench，我们对多种专有、开源和对象级MLLM进行了全面评估。EOC-Bench为提升MLLMs的具身对象认知能力提供了重要工具，并为构建可靠的具身系统核心模型奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:44:12 GMT</pubDate>
</item>
<item>
<title>Rectified Point Flow: Generic Point Cloud Pose Estimation</title>
<link>https://arxiv.org/abs/2506.05282</link>
<guid>https://arxiv.org/abs/2506.05282</guid>
<content:encoded><![CDATA[
We introduce Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with a self-supervised encoder focused on overlapping points, our method achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Project page: https://rectified-pointflow.github.io/.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:36:03 GMT</pubDate>
</item>
<item>
<title>Micro-Act框架解决RAG系统中的知识冲突问题</title>
<link>https://arxiv.org/abs/2506.05278</link>
<guid>https://arxiv.org/abs/2506.05278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Micro-Act框架，通过层次化动作空间自动感知上下文复杂性，显著提升问答任务的准确性。</p><br /><br /><p><strong>摘要：</strong>  Retrieval-Augmented Generation (RAG) 系统常面临知识冲突问题，即外部检索的知识与大语言模型的内在知识相矛盾，影响下游任务如问答的表现。现有方法通常通过直接对比两种知识源来缓解冲突，但可能因过多冗长的上下文导致模型无法有效识别和解决不一致性。为了解决这一问题，本文提出Micro-Act框架，该框架具有分层动作空间，能够自动感知上下文复杂性并自适应地将每个知识源分解为一系列细粒度比较，这些比较被表示为可操作步骤，从而实现超越表面上下文的推理。通过在五个基准数据集上的大量实验表明，Micro-Act在所有数据集和三种冲突类型上均显著提高了问答准确性，特别是在时间型和语义型冲突中，表现尤为突出，而其他基线方法则表现不佳。此外，Micro-Act在非冲突问题上也表现出稳健的性能，凸显了其在实际RAG应用中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05278" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 13:33:02 GMT</pubDate>
</item>
<item>
<title>基于流模型的可学习潜在空间对齐框架</title>
<link>https://arxiv.org/abs/2506.05240</link>
<guid>https://arxiv.org/abs/2506.05240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用流模型作为先验的潜在空间对齐新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的框架，通过利用基于流的生成模型作为先验，实现可学习潜在空间与任意目标分布的对齐。首先，该方法通过在目标特征上预训练流模型捕获潜在分布；随后，固定流模型通过对齐损失正则化潜在空间，将流匹配目标重新表述为优化潜在的目标。理论证明显示，最小化此对齐损失提供了一个计算可行的代理目标，用于最大化潜在变量在目标分布下的变分下界。值得注意的是，该方法避免了昂贵的似然性评估和优化过程中的常微分方程求解。实验验证表明，所提出的对齐损失景观近似于目标分布的负对数似然，并在ImageNet上的大规模图像生成实验中展示了其有效性，同时进行了详细的讨论和消融研究。本框架为潜在空间对齐开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 12:59:53 GMT</pubDate>
</item>
<item>
<title>Qwen3 Embedding系列：文本嵌入与重排序能力的重大突破</title>
<link>https://arxiv.org/abs/2506.05176</link>
<guid>https://arxiv.org/abs/2506.05176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen3 Embedding系列显著提升多语言文本理解和生成能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Qwen3 Embedding系列，这是对GTE-Qwen系列的重要升级，基于Qwen3基础模型，在文本嵌入和重排序方面展现出更强的能力。通过结合大规模无监督预训练和高质量数据集上的有监督微调，该系列利用Qwen3大语言模型在多语言文本理解和生成方面的强大能力，采用创新的多阶段训练管道。此外，有效的模型合并策略进一步增强了Qwen3 Embedding系列的鲁棒性和适应性。在训练过程中，Qwen3大语言模型不仅作为骨干模型，还在多个领域和语言中合成高质量、丰富且多样化的训练数据。该系列提供了多种规模的模型（0.6B、4B、8B），适用于嵌入和重排序任务，满足不同的部署需求。实证评估表明，Qwen3 Embedding系列在多个基准测试中达到最先进水平，尤其在MTEB多语言评估基准和代码检索、跨语言检索及多语言检索等任务中表现出色。为了促进可重复研究并推动社区驱动的发展，Qwen3 Embedding模型已公开发布，采用Apache 2.0许可。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.05176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 11:49:48 GMT</pubDate>
</item>
<item>
<title>ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development</title>
<link>https://arxiv.org/abs/2506.05010</link>
<guid>https://arxiv.org/abs/2506.05010</guid>
<content:encoded><![CDATA[
We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot.
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 09:20:50 GMT</pubDate>
</item>
<item>
<title>开放源码社区广泛采用的Deepseek-R1-Distill系列模型性能评估波动性研究</title>
<link>https://arxiv.org/abs/2506.04734</link>
<guid>https://arxiv.org/abs/2506.04734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明Deepseek-R1-Distill系列模型性能评价受多种因素影响存在显著波动。</p><br /><br /><p><strong>摘要：</strong> 近年来，由Deepseek-R1-Distill系列模型为代表的推理模型因其在数学、科学和编程等领域的卓越表现，在开源社区中得到了广泛应用。然而，我们的研究发现，这些模型的基准评估结果受到诸多因素的影响，表现出较大的波动性。例如，微小的评估条件差异就可能导致结果出现显著变化。类似的现象也在基于Deepseek-R1-Distill系列微调的其他开源推理模型，以及QwQ-32B模型中被观察到，这使得这些模型所声称的性能提升难以可靠复现。因此，我们呼吁建立更为严格的模型性能评估范式，并提出了针对Deepseek-R1-Distill系列模型的经验性评估方法，以期为后续研究提供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 04:09:11 GMT</pubDate>
</item>
<item>
<title>STARE基准测试：评估多模态大语言模型的空间认知能力</title>
<link>https://arxiv.org/abs/2506.04633</link>
<guid>https://arxiv.org/abs/2506.04633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STARE基准测试专注于评估AI在复杂视觉模拟推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 现有AI基准测试主要侧重于语言推理，忽视了非语言多步视觉模拟的重要性。为填补这一空白，我们开发了STARE（Spatial Transformations and Reasoning Evaluation）基准测试，用于评估多模态大型语言模型在涉及几何变换、空间推理等任务上的表现。STARE包含超过4000个任务，涵盖基础二维和三维几何变换、立方体网折叠及七巧板拼图等。实验表明，模型在简单的二维变换任务上表现良好，但在复杂的三维任务上接近随机猜测。人类在这些任务上表现出色但耗时较长，而提供中间视觉模拟可以显著提高效率。然而，模型对视觉信息的利用并不一致，部分情况下反而表现下降，显示了当前AI在处理复杂视觉推理时的局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 05 Jun 2025 01:09:46 GMT</pubDate>
</item>
<item>
<title>MedAgentGYM：首个提升医学推理能力的LLM训练环境</title>
<link>https://arxiv.org/abs/2506.04405</link>
<guid>https://arxiv.org/abs/2506.04405</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MedAgentGYM提供129类医学任务，显著提升LLM在医疗场景中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedAgentGYM，这是首个公开可用的大型语言模型（LLM）训练环境，旨在增强基于编码的医学推理能力。该平台包含来自真实生物医学场景的72,413个任务实例，分为129个类别。每个任务都封装在一个可执行的编码环境中，具备详细的任务描述、交互反馈机制、可验证的真实标注及可扩展的训练轨迹生成。通过对超过30个LLM的广泛基准测试发现，基于商业API的模型与开源模型之间存在明显性能差距。利用MedAgentGYM，Med-Copilot-7B通过监督微调和持续强化学习实现了显著性能提升，成为一种经济实惠且保护隐私的竞争性解决方案。MedAgentGYM不仅提供了全面的基准测试，还提供了统一执行环境中的综合培训资源，有助于开发先进的基于LLM的编码助手，支持生物医学研究和实践。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04405" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 15:38:55 GMT</pubDate>
</item>
<item>
<title>RoboRefer：结合深度编码和强化微调的3D空间指代理解模型</title>
<link>https://arxiv.org/abs/2506.04308</link>
<guid>https://arxiv.org/abs/2506.04308</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboRefer模型，通过深度编码和强化学习提升机器人3D空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为RoboRefer的3D感知视觉语言模型，旨在解决现有方法在复杂三维场景中精确理解空间关系及动态推理交互位置的问题。RoboRefer通过监督微调集成专用深度编码器实现精确的空间理解，并通过强化微调优化多步空间推理能力。此外，引入了一个包含2000万问答对的大规模数据集RefSpatial，用于支持模型训练。实验表明，经过监督微调的RoboRefer在空间理解方面达到最新技术水平，而强化微调进一步显著提升了性能，在RefSpatial-Bench基准测试中表现优于其他所有基线模型。RoboRefer还可与多种控制策略结合，在拥挤的真实世界场景中实现跨机器人平台的长期动态任务执行。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04308" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>基于固定大型语言模型的语言-图像对齐方法</title>
<link>https://arxiv.org/abs/2506.04209</link>
<guid>https://arxiv.org/abs/2506.04209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现仅训练图像编码器即可实现高效的语言-图像对齐。</p><br /><br /><p><strong>摘要：</strong> 当前主流的语言-图像对齐方法通常通过联合训练文本和图像编码器实现，如CLIP及其变体。然而，本文提出了一种新的方法——LIFT（Language-Image alignment with a Fixed Text encoder），即使用预训练的固定大型语言模型作为文本编码器，仅训练图像编码器。通过广泛的基准测试和消融研究，我们发现LIFT框架在涉及组合理解及长描述符场景下表现优于CLIP，同时显著提高了计算效率。这项工作首次系统性地探索了大型语言模型的文本嵌入如何指导视觉学习，并提出了学习语言对齐视觉表示的一种替代设计方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:51:56 GMT</pubDate>
</item>
<item>
<title>基于多平面和全身扫描的CT图像自动异常定位与描述方法</title>
<link>https://arxiv.org/abs/2506.03238</link>
<guid>https://arxiv.org/abs/2506.03238</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型OminiAbnorm-CT，用于自动解读CT影像中的异常情况。</p><br /><br /><p><strong>摘要：</strong> 本文旨在解决临床放射学中自动化解读CT图像特别是多平面和全身扫描中异常定位和描述的重大挑战，通过构建分类系统、贡献大规模标注数据集、开发OminiAbnorm-CT模型及建立基准评估任务，实现了对多种临床场景下异常情况的高效自动解析，显著优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03238" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:57:34 GMT</pubDate>
</item>
<item>
<title>StreamBP：一种高效且精确的长序列反向传播方法</title>
<link>https://arxiv.org/abs/2506.03077</link>
<guid>https://arxiv.org/abs/2506.03077</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种内存高效的反向传播方法StreamBP，显著降低长序列训练中的内存成本。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为StreamBP的新型反向传播算法，该算法通过沿序列维度进行线性分解，有效降低了存储激活值所需的内存成本，特别适用于复杂任务如长链推理。相较于传统梯度检查点技术，StreamBP在相同或更少的时间内将最大可处理序列长度提升了2.8到5.5倍。此外，StreamBP还能提升计算效率并加速多GPU分布式训练。该方法支持多种常见训练目标，例如SFT、GRPO和DPO，并已开源代码便于集成至任意Transformer模型的训练流程中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03077" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 12:54:15 GMT</pubDate>
</item>
<item>
<title>FlexPainter：一种灵活多模态引导的高质量纹理生成方法</title>
<link>https://arxiv.org/abs/2506.02620</link>
<guid>https://arxiv.org/abs/2506.02620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FlexPainter方法解决现有纹理生成中的控制灵活性和一致性问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FlexPainter的新颖纹理生成管道，旨在通过提供灵活的多模态条件引导和实现高度一致的纹理生成来克服扩散基方法的局限性。FlexPainter构建了一个共享条件嵌入空间以在不同输入模态之间进行灵活聚合，并采用基于图像的CFG方法分解结构和风格信息，从而实现基于参考图像的样式化。利用图像扩散先验中的3D知识，该框架首先使用网格表示同时生成多视角图像以增强全局理解，并在扩散采样过程中提出视图同步和自适应加权模块以进一步确保局部一致性。最后，结合3D感知纹理完成模型和纹理增强模型生成无缝且高分辨率的纹理贴图。实验表明，该框架在灵活性和生成质量方面显著优于现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:36:03 GMT</pubDate>
</item>
<item>
<title>基于多模态输入的高保真语音驱动虚拟人像生成框架</title>
<link>https://arxiv.org/abs/2506.00830</link>
<guid>https://arxiv.org/abs/2506.00830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架SkyReels-Audio，实现高保真长时语音驱动的人脸视频生成与编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkyReels-Audio的统一框架，用于合成高质量且时间上连贯的语音驱动人脸视频。该框架基于预训练的视频扩散变换器，支持无限长度的生成与编辑，并通过文本、图像和视频等多种模态输入实现多样化且可控的条件设置。我们采用混合课程学习策略逐步对齐音频与面部动作，从而在长视频序列中实现精细的多模态控制。为了增强局部面部一致性，引入了面部掩码损失和基于音频的分类器自由引导机制。滑动窗口去噪方法进一步融合了时间片段中的潜在表示，确保了长时间跨度和多样化身份下的视觉保真度和时间一致性。此外，构建了一个专门的数据管道来整理同步的音频、视频和文本描述三元组。综合基准评估表明，SkyReels-Audio在唇形同步准确性、身份一致性及逼真的面部动态方面表现出色，尤其是在复杂和具有挑战性的条件下。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 00:27:13 GMT</pubDate>
</item>
<item>
<title>自主代理中的上下文完整性研究：基于LLMs与强化学习的方法</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种结合LLMs与强化学习的方法，提升自主代理在上下文完整性上的表现。</p><br /><br /><p><strong>摘要：</strong> 随着自主代理在用户决策中的应用扩展，确保上下文完整性（CI）成为关键问题。本文首先通过提示大型语言模型（LLMs）显式推理CI来决定信息共享内容，随后开发了一种强化学习框架进一步培养模型实现CI所需的推理能力。实验采用合成数据集验证方法，在减少不适当信息披露的同时保持任务性能，且改进效果可迁移到具有人工标注的基准测试中，如PrivacyLens。本研究为提升AI助手隐私保护提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04245" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 17:26:21 GMT</pubDate>
</item>
<item>
<title>VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models</title>
<link>https://arxiv.org/abs/2505.23656</link>
<guid>https://arxiv.org/abs/2505.23656</guid>
<content:encoded><![CDATA[
Recent advancements in text-to-video (T2V) diffusion models have enabled high-fidelity and realistic video synthesis. However, current T2V models often struggle to generate physically plausible content due to their limited inherent ability to accurately understand physics. We found that while the representations within T2V models possess some capacity for physics understanding, they lag significantly behind those from recent video self-supervised learning methods. To this end, we propose a novel framework called VideoREPA, which distills physics understanding capability from video understanding foundation models into T2V models by aligning token-level relations. This closes the physics understanding gap and enable more physics-plausible generation. Specifically, we introduce the Token Relation Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft guidance suitable for finetuning powerful pre-trained T2V models, a critical departure from prior representation alignment (REPA) methods. To our knowledge, VideoREPA is the first REPA method designed for finetuning T2V models and specifically for injecting physical knowledge. Empirical evaluations show that VideoREPA substantially enhances the physics commonsense of baseline method, CogVideoX, achieving significant improvement on relevant benchmarks and demonstrating a strong capacity for generating videos consistent with intuitive physics. More video results are available at https://videorepa.github.io/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:06:44 GMT</pubDate>
</item>
<item>
<title>引入可编辑几何和保真外观扩散模型用于目标对象合成</title>
<link>https://arxiv.org/abs/2505.20914</link>
<guid>https://arxiv.org/abs/2505.20914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型DGAD，实现目标对象几何编辑与外观细节保存的平衡。</p><br /><br /><p><strong>摘要：</strong> 现有通用对象合成（GOC）方法通过高级语义嵌入结合扩散模型进行几何可编辑生成，但这些嵌入仅能捕捉高层次语义线索，无法保留细粒度外观细节。本文提出解耦几何可编辑与外观保存扩散模型（DGAD），先利用语义嵌入隐式捕获所需几何变换，再通过跨注意检索机制对齐细粒度外观特征与几何编辑表示，从而实现在对象合成中的精确几何编辑和忠实外观保存。具体而言，DGAD基于CLIP/DINO衍生网络提取语义嵌入与外观保存表示，并将其解耦式集成到编码和解码管道中。首先将语义嵌入融入具备强空间推理能力的预训练扩散模型，以隐式捕获对象几何，实现灵活的对象操作并确保可编辑性；随后设计密集跨注意机制，利用隐式学习的物体几何检索并空间对齐外观特征，确保外观一致性。大量实验表明，该框架在公共基准测试中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 05:05:28 GMT</pubDate>
</item>
<item>
<title>Sounding that Object: Interactive Object-Aware Image to Audio Generation</title>
<link>https://arxiv.org/abs/2506.04214</link>
<guid>https://arxiv.org/abs/2506.04214</guid>
<content:encoded><![CDATA[
Generating accurate sounds for complex audio-visual scenes is challenging, especially in the presence of multiple objects and sound sources. In this paper, we propose an {\em interactive object-aware audio generation} model that grounds sound generation in user-selected visual objects within images. Our method integrates object-centric learning into a conditional latent diffusion model, which learns to associate image regions with their corresponding sounds through multi-modal attention. At test time, our model employs image segmentation to allow users to interactively generate sounds at the {\em object} level. We theoretically validate that our attention mechanism functionally approximates test-time segmentation masks, ensuring the generated audio aligns with selected objects. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds. Project page: https://tinglok.netlify.app/files/avobject/
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:57:26 GMT</pubDate>
</item>
<item>
<title>OpenThoughts项目推动开源推理模型发展</title>
<link>https://arxiv.org/abs/2506.04178</link>
<guid>https://arxiv.org/abs/2506.04178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过创建开源数据集，OpenThoughts项目训练出性能优异的推理模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，推理模型在数学、代码和科学相关基准测试中取得了显著进展，但最佳训练方法仍存争议，且多依赖私有数据集。为解决这一问题，OpenThoughts项目致力于构建开源推理数据集。其开发的OpenThoughts2-1M数据集促成了首个匹配DeepSeek-R1-Distill-32B的公共推理数据模型OpenThinker2-32B，并在AIME和LiveCodeBench等标准推理基准上表现优异。进一步优化后，OpenThoughts3数据集规模扩展至120万样本，结合QwQ-32B作为教师模型，成功训练出性能更优的OpenThinker3-7B模型，在AIME 2025、LiveCodeBench 06/24-01/25和GPQA Diamond等多个评测中取得领先成绩。所有数据集和模型均开放获取，助力推理模型研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:25:39 GMT</pubDate>
</item>
<item>
<title>HTSC-2025：基于AI的高温超导材料基准数据集发布</title>
<link>https://arxiv.org/abs/2506.03837</link>
<guid>https://arxiv.org/abs/2506.03837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布HTSC-2025基准数据集以推动AI预测高温超导材料的研究。</p><br /><br /><p><strong>摘要：</strong> 近年来，利用人工智能预测超导转变温度的研究受到广泛关注，但由于缺乏广泛接受的基准数据集，导致不同算法间的公平比较难以实现，阻碍了该领域的发展。本文介绍了一个名为HTSC-2025的新基准数据集，该数据集包含了2023年至2025年间理论物理学家基于BCS超导理论预测的高温超导材料，涵盖了多个重要的超导体系如X_2YH_6系统、MXH_3系统等。HTSC-2025数据集已开源并持续更新，为加速通过AI方法发现高温超导材料提供了重要支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 07:14:00 GMT</pubDate>
</item>
<item>
<title>CRAWLDoc：基于上下文关联的多源学术文档元数据提取方法</title>
<link>https://arxiv.org/abs/2506.03822</link>
<guid>https://arxiv.org/abs/2506.03822</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法CRAWLDoc，用于从网页链接文档中提取布局无关的元数据。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CRAWLDoc的新方法，该方法通过上下文关联对链接的网页文档进行排序，以解决因网络布局和数据格式多样性导致的元数据提取挑战。CRAWLDoc从出版物的URL开始，获取着陆页及所有相关联的资源（如PDF、ORCID资料和补充材料），并将其嵌入统一表示中。为了验证该方法的有效性，我们创建了一个包含600篇来自计算机科学领域六大顶级出版社的手动标注数据集。实验表明，CRAWLDoc在跨出版商和数据格式的情况下能够稳健地对相关文档进行排名，为从具有多种布局和格式的网页文档中改进元数据提取奠定了基础。此外，我们的源代码和数据集可通过指定链接获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03822" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:52:55 GMT</pubDate>
</item>
<item>
<title>Orak：面向多类型游戏的大规模语言模型智能体基准测试平台</title>
<link>https://arxiv.org/abs/2506.03610</link>
<guid>https://arxiv.org/abs/2506.03610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Orak基准测试平台，支持大规模语言模型在多种游戏中的训练与评估。</p><br /><br /><p><strong>摘要：</strong> 现有游戏基准测试无法满足实际需求，缺乏对不同类型游戏中多样化大规模语言模型能力的评估、复杂游戏场景中重要自主模块的研究以及预训练模型微调数据集。为此，我们提出了Orak，这是一个专注于训练和评估大规模语言模型智能体的基准测试平台。Orak涵盖了12款流行视频游戏，横跨主要游戏类型，提供了一个全面的评价框架，包括通用游戏分数排行榜、智能体战斗场所以及深入分析视觉输入状态、自主策略和微调效果等。此外，Orak引入了一种基于模型上下文协议的即插即用接口，使大规模语言模型能够无缝连接游戏并操控自主模块，同时提供了一个多样游戏类型的微调数据集。这一平台为构建通用游戏智能体奠定了基础。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:40:33 GMT</pubDate>
</item>
<item>
<title>基于位置专家的推测解码优化大语言模型推理</title>
<link>https://arxiv.org/abs/2506.03566</link>
<guid>https://arxiv.org/abs/2506.03566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出位置专家方法提升大语言模型推测解码后期预测质量。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有推测解码方法在后期位置上因错误累积导致草案标记预测质量下降的问题，提出了位置专家（Position Specialists, PosS）方法。该方法通过多个专注于特定位置的草案层生成标记，有效提高了后期位置的标记接受率。实验结果表明，PosS方法在Llama-3-8B-Instruct和Llama-2-13B-chat模型上，相较于基线模型平均接受了更长的标记序列并提升了加速比。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:30:30 GMT</pubDate>
</item>
<item>
<title>Video-SKoT：一种领域自适应视频推理框架</title>
<link>https://arxiv.org/abs/2506.03525</link>
<guid>https://arxiv.org/abs/2506.03525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Video-SKoT框架，通过技能感知的Chain-of-Thought推理提升复杂视频理解。</p><br /><br /><p><strong>摘要：</strong> 近期关于Chain-of-Thought（CoT）推理的研究推动了复杂视频理解的进步，但现有方法难以适应视频内容中的多样化领域特定技能（如事件检测、空间关系理解和情感理解）。针对这一问题，我们提出了Video-Skill-CoT（简称Video-SKoT），这是一种自动构建并利用技能感知的CoT监督进行领域自适应视频推理的框架。首先，我们构建基于技能的CoT注释：从训练问题中提取领域相关的推理技能，将其聚类到共享的技能分类中，并为每对视频-问题创建详细的多步CoT推理理由用于训练。其次，我们引入了技能特定的专家学习框架，每个专家模块专注于一组推理技能，并通过收集的CoT监督使用轻量级适配器进行训练。实验表明，在三个视频理解基准上，Video-SKoT始终优于强大的基线模型。此外，我们还深入分析了不同CoT注释管道和多种视频域中学到的技能之间的差异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03525" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:18:01 GMT</pubDate>
</item>
<item>
<title>IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2506.03150</link>
<guid>https://arxiv.org/abs/2506.03150</guid>
<content:encoded><![CDATA[
Although diffusion-based models can generate high-quality and high-resolution video sequences from textual or image inputs, they lack explicit integration of geometric cues when controlling scene lighting and visual appearance across frames. To address this limitation, we propose IllumiCraft, an end-to-end diffusion framework accepting three complementary inputs: (1) high-dynamic-range (HDR) video maps for detailed lighting control; (2) synthetically relit frames with randomized illumination changes (optionally paired with a static background reference image) to provide appearance cues; and (3) 3D point tracks that capture precise 3D geometry information. By integrating the lighting, appearance, and geometry cues within a unified diffusion architecture, IllumiCraft generates temporally coherent videos aligned with user-defined prompts. It supports background-conditioned and text-conditioned video relighting and provides better fidelity than existing controllable video generation methods. Project Page: https://yuanze-lin.me/IllumiCraft_page
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>CapSpeech：面向风格标注文本到语音合成的新基准</title>
<link>https://arxiv.org/abs/2506.02863</link>
<guid>https://arxiv.org/abs/2506.02863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入CapSpeech，这是首个大规模风格标注文本到语音合成相关任务的数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CapSpeech的新基准，旨在推动风格标注文本到语音合成（CapTTS）的发展。该基准包含超过1000万机器注释的音频-描述对和近36万人工注释的音频-描述对，并新增了两个专业录制的数据集用于特定任务。实验表明，无论采用自回归还是非自回归模型，CapSpeech都能实现高质量、高可懂度的语音合成。CapSpeech是目前最大的CapTTS相关任务全面标注数据集，为CapTTS系统的开发提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 09:28:55 GMT</pubDate>
</item>
<item>
<title>FLAIR：基于流模型的无训练变分框架用于逆向成像问题</title>
<link>https://arxiv.org/abs/2506.02680</link>
<guid>https://arxiv.org/abs/2506.02680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FLAIR利用流模型作为先验，在逆向成像问题上优于现有扩散和流方法。</p><br /><br /><p><strong>摘要：</strong> 基于流模型的潜在生成模型（如Stable Diffusion 3）虽能生成高质量图像，但在逆向成像问题中的表现却不尽人意。主要障碍包括非线性映射、数据似然项难以处理及罕见模式恢复困难。本文提出FLAIR，一种无需训练的变分框架，通过引入无类型依赖的流匹配变分目标并结合确定性轨迹调整，有效解决上述问题。此外，FLAIR通过解耦数据保真度与正则化优化、引入时间依赖校准方案，进一步提升重建质量和样本多样性。实验表明，FLAIR在标准成像基准测试中表现卓越。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 05:29:47 GMT</pubDate>
</item>
<item>
<title>FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning</title>
<link>https://arxiv.org/abs/2506.02515</link>
<guid>https://arxiv.org/abs/2506.02515</guid>
<content:encoded><![CDATA[
Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:44:42 GMT</pubDate>
</item>
<item>
<title>小语言模型在自主AI系统中的未来前景</title>
<link>https://arxiv.org/abs/2506.02153</link>
<guid>https://arxiv.org/abs/2506.02153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">小语言模型更适合自主AI系统的多任务重复应用。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型虽表现接近人类，但在自主AI系统中，小语言模型因其高效性、经济性和特定任务适应性更具优势。本文论证了小语言模型在自主AI系统中的潜力，探讨了其在专用任务中的部署成本效益，并提出了从大型语言模型向小语言模型转换的算法。此外，文中强调了混合模型架构在需要广泛对话能力场景中的必要性，并讨论了推广小语言模型面临的障碍，呼吁行业对资源优化使用的进一步讨论。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 14:35:16 GMT</pubDate>
</item>
<item>
<title>RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents</title>
<link>https://arxiv.org/abs/2506.00618</link>
<guid>https://arxiv.org/abs/2506.00618</guid>
<content:encoded><![CDATA[
With the rapid development of multimodal large language models (MLLMs), they are increasingly deployed as autonomous computer-use agents capable of accomplishing complex computer tasks. However, a pressing issue arises: Can the safety risk principles designed and aligned for general MLLMs in dialogue scenarios be effectively transferred to real-world computer-use scenarios? Existing research on evaluating the safety risks of MLLM-based computer-use agents suffers from several limitations: it either lacks realistic interactive environments, or narrowly focuses on one or a few specific risk types. These limitations ignore the complexity, variability, and diversity of real-world environments, thereby restricting comprehensive risk evaluation for computer-use agents. To this end, we introduce RiOSWorld, a benchmark designed to evaluate the potential risks of MLLM-based agents during real-world computer manipulations. Our benchmark includes 492 risky tasks spanning various computer applications, involving web, social media, multimedia, os, email, and office software. We categorize these risks into two major classes based on their risk source: (i) User-originated risks and (ii) Environmental risks. For the evaluation, we evaluate safety risks from two perspectives: (i) Risk goal intention and (ii) Risk goal completion. Extensive experiments with multimodal agents on RiOSWorld demonstrate that current computer-use agents confront significant safety risks in real-world scenarios. Our findings highlight the necessity and urgency of safety alignment for computer-use agents in real-world computer manipulation, providing valuable insights for developing trustworthy computer-use agents. Our benchmark is publicly available at https://yjyddq.github.io/RiOSWorld.github.io/.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 12:04:59 GMT</pubDate>
</item>
<item>
<title>Segment Policy Optimization: 中文标题</title>
<link>https://arxiv.org/abs/2505.23564</link>
<guid>https://arxiv.org/abs/2505.23564</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习框架SPO，优化大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用强化学习提升大型语言模型推理能力这一重要挑战。传统方法分为细粒度的Token级和粗粒度的轨迹级，前者因评价模型不准确导致估计困难，后者则面临精确归因问题。为解决这些问题，我们提出了Segment Policy Optimization (SPO)，这是一种介于两者之间的中间粒度优势估计新框架。SPO通过灵活分段、准确的段级优势估计和基于段优势的策略优化，实现了更好的平衡。具体实例化为SPO-chain和SPO-tree，分别针对短链式思维和长链式思维场景，在GSM8K和MATH500数据集上显著提升了性能，分别达到6-12和7-11个百分点的改进。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23564" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:38:19 GMT</pubDate>
</item>
<item>
<title>LayerFlow：一种统一的层感知视频生成框架</title>
<link>https://arxiv.org/abs/2506.04228</link>
<guid>https://arxiv.org/abs/2506.04228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种支持多种层感知视频生成任务的统一框架LayerFlow。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LayerFlow的统一解决方案，用于层感知的视频生成任务。该方法通过提供每层提示，生成透明前景、干净背景和混合场景的视频，并支持分解混合视频或根据给定前景生成背景等变体。基于文本到视频扩散变换器，将不同层的视频组织为子片段，并利用层嵌入区分每个片段及其对应的层感知提示。此外，针对高质量分层训练视频的缺乏，设计了一种多阶段训练策略，先用低质量视频数据训练模型，再调整运动LoRA使其兼容静态帧，最后在高质量分层图像和复制粘贴视频混合数据上训练内容LoRA。在推理过程中移除运动LoRA以生成所需层的平滑视频。这种方法在多个层感知视频生成任务中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Voyager：基于单张图像生成世界一致3D点云序列的视频扩散框架</title>
<link>https://arxiv.org/abs/2506.04225</link>
<guid>https://arxiv.org/abs/2506.04225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Voyager，用于从单张图像生成具有用户自定义相机路径的世界一致3D点云序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Voyager的创新视频扩散框架，该框架可以从单张图像生成世界一致的3D点云序列，并支持用户定义的相机路径进行场景探索。与现有方法不同，Voyager实现了端到端的场景生成与重建，同时保证跨帧的一致性，无需依赖传统的3D重建管道（如运动结构或多视图立体视觉）。Voyager由三个关键组件组成：1）世界一致的视频扩散模型，通过联合生成对齐的RGB和深度视频序列，在现有世界观测条件下确保全局一致性；2）长范围世界探索功能，采用高效的点剔除世界缓存和自动回归推断技术，实现迭代场景扩展并保持上下文感知的一致性；3）可扩展的数据引擎，自动化摄像机姿态估计和度量深度预测，从而无需人工标注即可生成大规模多样化的训练数据。这些设计使Voyager在视觉质量和几何准确性方面显著优于现有方法，并具有广泛的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>ReVisual-R1：通过分阶段训练提升多模态大语言模型推理能力</title>
<link>https://arxiv.org/abs/2506.04207</link>
<guid>https://arxiv.org/abs/2506.04207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出分阶段训练方法显著提升了多模态大语言模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大型语言模型（MLLMs）在复杂推理任务中的表现瓶颈，分析现有强化学习（RL）方法的局限性。研究表明，有效初始化对增强推理至关重要，仅使用文本数据即可超越许多近期模型；标准GRPO方法在多模态RL中易出现梯度停滞；后续文本-only RL进一步优化推理能力。基于这些发现，我们提出了ReVisual-R1模型，在多个挑战性基准测试中达到开源7B规模MLLM的新SOTA性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:51:08 GMT</pubDate>
</item>
<item>
<title>SuperWriter-Agent：提升大语言模型长文本生成质量的新框架</title>
<link>https://arxiv.org/abs/2506.04180</link>
<guid>https://arxiv.org/abs/2506.04180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于代理的SuperWriter-Agent框架，显著提高长文本生成的连贯性和一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SuperWriter-Agent的新框架，旨在解决大型语言模型在长文本生成中面临的一致性、逻辑性和质量下降等问题。该框架通过引入结构化思考和规划阶段，模拟专业作家的创作过程。研究团队还构建了一个监督微调数据集训练了一个7B参数规模的SuperWriter-LM，并采用层次化的直接偏好优化方法结合蒙特卡洛树搜索进行优化。实验表明，SuperWriter-LM在多个基准测试中表现优异，超越了更大规模的基线模型，且通过消融研究验证了层次化优化的有效性，强调了引入结构化思考步骤的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 13:27:42 GMT</pubDate>
</item>
<item>
<title>Image Editing As Programs with Diffusion Models</title>
<link>https://arxiv.org/abs/2506.04158</link>
<guid>https://arxiv.org/abs/2506.04158</guid>
<content:encoded><![CDATA[
While diffusion models have achieved remarkable success in text-to-image generation, they encounter significant challenges with instruction-driven image editing. Our research highlights a key challenge: these models particularly struggle with structurally inconsistent edits that involve substantial layout changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a unified image editing framework built upon the Diffusion Transformer (DiT) architecture. At its core, IEAP approaches instructional editing through a reductionist lens, decomposing complex editing instructions into sequences of atomic operations. Each operation is implemented via a lightweight adapter sharing the same DiT backbone and is specialized for a specific type of edit. Programmed by a vision-language model (VLM)-based agent, these operations collaboratively support arbitrary and structurally inconsistent transformations. By modularizing and sequencing edits in this way, IEAP generalizes robustly across a wide range of editing tasks, from simple adjustments to substantial structural changes. Extensive experiments demonstrate that IEAP significantly outperforms state-of-the-art methods on standard benchmarks across various editing scenarios. In these evaluations, our framework delivers superior accuracy and semantic fidelity, particularly for complex, multi-step instructions. Codes are available at https://github.com/YujiaHu1109/IEAP.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:57:24 GMT</pubDate>
</item>
<item>
<title>Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis</title>
<link>https://arxiv.org/abs/2506.04142</link>
<guid>https://arxiv.org/abs/2506.04142</guid>
<content:encoded><![CDATA[
The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (rho) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:33:44 GMT</pubDate>
</item>
<item>
<title>MMR-V：视频多模态深度推理基准测试</title>
<link>https://arxiv.org/abs/2506.04141</link>
<guid>https://arxiv.org/abs/2506.04141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于评估视频多模态推理能力的新基准MMR-V。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大型语言模型在处理视频中的多帧证据定位和跨模态推理时面临挑战，因为现有视频基准主要关注理解任务，无法充分测试模型的推理能力。为了解决这一问题，我们提出了MMR-V，这是一个针对视频中多模态深度推理设计的基准测试集。该基准具有以下特点：支持长距离多帧推理、超越感知能力的推理需求、人工标注以保证任务的可靠性、以及精心设计的干扰项以减少模型捷径。MMR-V包含317个视频和1257个任务。实验结果显示，当前最先进的模型在多模态推理方面表现仍然有限，即使最佳模型o4-mini的准确率也只有52.5%。此外，当前的推理增强策略如思维链和测试时计算扩展带来的提升有限。进一步分析表明，视频多模态推理所需的思维链与文本推理中的有所不同，这也是性能提升受限的原因之一。我们希望MMR-V能激发更多关于提升多模态推理能力的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:33:41 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的自主多智能体系统的信任、风险与安全管理</title>
<link>https://arxiv.org/abs/2506.04133</link>
<guid>https://arxiv.org/abs/2506.04133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述了基于大型语言模型的自主多智能体系统中的信任、风险与安全管理。</p><br /><br /><p><strong>摘要：</strong> 本文通过结构化分析，探讨了基于大型语言模型（LLMs）的自主多智能体系统（AMAS）中的信任、风险与安全管理（TRiSM）。首先，文章概述了自主AI的概念基础及其架构差异，并介绍了支持可扩展工具使用自主性的新兴系统设计。接着，从治理、可解释性、ModelOps和隐私/安全四个支柱详细阐述了自主AI框架下的TRiSM，并提出了针对自主LLMs的独特威胁向量及全面的风险分类法，辅以案例研究展示实际应用中的漏洞。此外，还调查了分布式LLM代理系统的信任构建机制、透明度和监督技术以及最新的可解释性策略。最后，文章讨论了用于评估信任、可解释性和以人为本性能的指标，同时审视了开放基准挑战，并通过加密、对抗防御和遵守不断发展的AI法规解决安全和隐私问题，最终提出了负责任的自主AI发展路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:26:11 GMT</pubDate>
</item>
<item>
<title>Rectified Sparse Attention (ReSA)：高效长序列生成的新方法</title>
<link>https://arxiv.org/abs/2506.04108</link>
<guid>https://arxiv.org/abs/2506.04108</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合稀疏注意力与密集修正的ReSA方法，大幅提升长序列生成效率且保持高质量。</p><br /><br /><p><strong>摘要：</strong> 高效生成长序列是大型语言模型面临的重要挑战。虽然近期的稀疏解码方法提高了效率，但KV缓存的不匹配问题导致误差累积，降低了生成质量。本研究提出了Rectified Sparse Attention (ReSA)，一种简单而有效的方法，通过将块稀疏注意力与周期性密集修正相结合，在固定间隔刷新KV缓存，从而控制误差累积并维持与预训练分布的一致性。实验表明，ReSA在数学推理、语言建模及检索等任务上实现了接近无损的生成质量，同时显著提升了效率，尤其在256K序列长度下的端到端速度提升达2.42倍，成为可扩展长上下文推理的实用解决方案。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04108" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 12:01:48 GMT</pubDate>
</item>
<item>
<title>AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment</title>
<link>https://arxiv.org/abs/2506.04089</link>
<guid>https://arxiv.org/abs/2506.04089</guid>
<content:encoded><![CDATA[
As a part of an embodied agent, Large Language Models (LLMs) are typically used for behavior planning given natural language instructions from the user. However, dealing with ambiguous instructions in real-world environments remains a challenge for LLMs. Various methods for task ambiguity detection have been proposed. However, it is difficult to compare them because they are tested on different datasets and there is no universal benchmark. For this reason, we propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual dataset of ambiguous instructions addressed to a robot in a kitchen environment. AmbiK was collected with the assistance of LLMs and is human-validated. It comprises 1000 pairs of ambiguous tasks and their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), with environment descriptions, clarifying questions and answers, user intents, and task plans, for a total of 2000 tasks. We hope that AmbiK will enable researchers to perform a unified comparison of ambiguity detection methods. AmbiK is available at https://github.com/cog-model/AmbiK-dataset.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 11:47:07 GMT</pubDate>
</item>
<item>
<title>Rex-Thinker：通过显式推理提升物体指代任务的可解释性和可靠性</title>
<link>https://arxiv.org/abs/2506.04034</link>
<guid>https://arxiv.org/abs/2506.04034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型Rex-Thinker，将物体指代任务转化为显式推理任务，提升预测的可解释性和准确性。</p><br /><br /><p><strong>摘要：</strong> 物体指代旨在检测图像中所有符合给定自然语言描述的对象。本文认为，稳健的物体指代模型应具备可解释性与视觉内容的一致性，即能够验证预测并拒绝不匹配表达的场景。然而，现有方法多直接预测边界框，缺乏解释能力且难以拒绝无效表达。为此，我们提出了Rex-Thinker模型，将其视为显式连续推理任务。该模型首先确定候选对象实例，然后逐步评估每个候选是否满足给定描述，最终做出预测。为支持此框架，我们基于HumanRef数据集构建了大规模的CoT风格数据集HumanRef-CoT，使模型能够学习分解和可解释的推理过程。Rex-Thinker采用两阶段训练策略：先通过监督微调学习结构化推理，再利用GRPO强化学习优化精度和泛化能力。实验表明，该方法在域内任务中优于基线模型，在域外设置中也展现出更强的泛化能力和拒绝幻觉输出的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.04034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 10:56:57 GMT</pubDate>
</item>
<item>
<title>Adapting预训练模型以解决连续学习中的稳定性-可塑性权衡问题</title>
<link>https://arxiv.org/abs/2506.03956</link>
<guid>https://arxiv.org/abs/2506.03956</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种在连续学习前适应预训练模型的新框架，平衡稳定性与可塑性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了连续学习（CL）中神经网络如何在保持现有知识（稳定性）的同时获取新知识（可塑性）的问题。尽管预训练模型（PTMs）在CL中至关重要，但冻结PTM主干以保证稳定性会限制其可塑性，而对整个PTM进行顺序微调则可能引发灾难性遗忘。为了解决这一稳定性-可塑性权衡问题，我们提出了在核心CL过程之前通过插拔式适应阶段调整PTM主干的方法（ACL）。该方法在学习新任务时通过与原始类别原型对齐嵌入并远离其他类别来增强可塑性。理论和实证研究表明，ACL在多个基准数据集和集成方法上显著提高了CL性能，提供了一个灵活的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03956" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:46:33 GMT</pubDate>
</item>
<item>
<title>Dual-Arch框架：解决连续学习中的稳定性与可塑性权衡问题</title>
<link>https://arxiv.org/abs/2506.03951</link>
<guid>https://arxiv.org/abs/2506.03951</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Dual-Arch框架，在架构层面解决连续学习的稳定性与可塑性难题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了连续学习(CL)领域中稳定性与可塑性之间的矛盾，并揭示了深度网络更适合可塑性而宽网络则在稳定性上表现更佳的规律。为了解决这一架构级的矛盾，我们引入了一种名为Dual-Arch的新框架，该框架作为插件组件用于CL方法中。Dual-Arch利用两个独立且互补的子网络，分别专注于稳定性与可塑性，每个子网络都采用轻量化的专门设计。实验表明，Dual-Arch不仅提升了现有CL方法的性能，还实现了高达87%的参数紧凑性提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03951" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:40:41 GMT</pubDate>
</item>
<item>
<title>VisCode-200K：基于Python的可视化及自修正大规模指令调优数据集</title>
<link>https://arxiv.org/abs/2506.03930</link>
<guid>https://arxiv.org/abs/2506.03930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VisCode-200K数据集，显著提升可视化代码生成性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在绘图等可视化任务上常因代码正确性和视觉语义问题表现不佳。现有指令调优数据集缺乏执行导向监督且对迭代代码修正支持有限，导致绘图生成效果脆弱不可靠。本文介绍VisCode-200K，这是一个包含超过200,000个示例的大规模Python可视化及自修正指令调优数据集。该数据集来源于两个部分：一是来自开源存储库的验证绘图代码及其配对的自然语言说明和渲染图表；二是Code-Feedback中的45,000个多轮修正对话，使模型能够利用运行时反馈修正错误代码。通过在VisCode-200K上微调Qwen2.5-Coder-Instruct，我们创建了VisCoder，并在PandasPlotBench上进行评估，结果显示VisCoder在绘图生成性能上显著优于强大的开源基线模型，接近GPT-4o-mini等专有模型的表现。此外，我们采用自我调试评估协议来评估迭代修复能力，证明了基于反馈的学习在生成可执行且视觉准确代码中的优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 09:24:44 GMT</pubDate>
</item>
<item>
<title>主动学习中的超参数空间挑战与优化研究</title>
<link>https://arxiv.org/abs/2506.03817</link>
<guid>https://arxiv.org/abs/2506.03817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨主动学习中复杂超参数空间对实验结果的影响并提出改进建议。</p><br /><br /><p><strong>摘要：</strong> 主动学习（Active Learning, AL）是一种通过迭代选择最具信息量的未标注样本进行人工标注来减少人力成本的技术，但其在实际应用中仍较少被采用。主要原因在于设置过程的复杂性以及对其效果的信任不足。本文假设这些障碍的根源在于AL的庞大且未充分探索的超参数空间，该空间可能导致误导性和不可重复的结果。研究首先构建了一个包含超过460万种组合的大型超参数网格，其次记录了迄今为止最大规模AL研究的所有组合性能，并分析了每个超参数对实验结果的影响。最终，本文提供了关于各超参数影响的建议，揭示了具体AL策略实现的意外影响力，并设计了一种最小化计算开销的可重复AL实验框架，从而推动更可靠和可信的AL研究发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 06:41:37 GMT</pubDate>
</item>
<item>
<title>视觉拼接能力对视觉语言模型安全性的挑战</title>
<link>https://arxiv.org/abs/2506.03614</link>
<guid>https://arxiv.org/abs/2506.03614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉语言模型存在通过视觉拼接学习有害内容的风险。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉语言模型(VLMs)在训练过程中面临的安全风险，特别是在有害图像被分割成看似无害的小块并散布在多个样本中时。即使进行了数据清洗，这些片段仍可能被模型学习并通过视觉拼接能力重新组合，导致推理阶段产生不当响应。实验表明，许多开源VLMs具备这种视觉拼接能力，能够在不同粒度上从碎片化数据中重构完整信息。基于此，我们模拟了一种数据投毒攻击场景，展示了如何利用这一能力绕过常规的数据审核机制，将有害内容隐藏在看似正常的文本描述中，从而对VLM的安全性构成威胁。本研究旨在提高对这类潜在安全隐患的认识，并促进更有效的防护措施开发。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:46:06 GMT</pubDate>
</item>
<item>
<title>MiMo-VL Technical Report</title>
<link>https://arxiv.org/abs/2506.03569</link>
<guid>https://arxiv.org/abs/2506.03569</guid>
<content:encoded><![CDATA[
We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 00:32:54 GMT</pubDate>
</item>
<item>
<title>基于不对称双3D高斯点喷涂的野外图像三维重建</title>
<link>https://arxiv.org/abs/2506.03538</link>
<guid>https://arxiv.org/abs/2506.03538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Asymmetric Dual 3DGS，利用视觉伪影的随机性实现稳定可靠的场景几何重建。</p><br /><br /><p><strong>摘要：</strong> 野外图像的三维重建面临光照条件不一致和瞬态干扰等挑战，现有方法通常依赖启发式策略处理低质量训练数据，容易产生不稳定且不一致的重建结果及视觉伪影。本文提出Asymmetric Dual 3DGS框架，通过训练两个3D高斯点喷涂模型并施加一致性约束，鼓励可靠场景几何的收敛同时抑制伪影。为避免确认偏差导致模型陷入相似失败模式，引入分异掩码策略，应用互补掩码促进模型间非对称训练。此外，设计轻量级动态指数移动平均代理，提高训练效率。实验表明，该方法在多个具有挑战性的真实世界数据集上表现优于现有方法。代码和训练模型将公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:40:33 GMT</pubDate>
</item>
<item>
<title>DenseDPO：提升文本到视频扩散模型训练的数据效率与性能</title>
<link>https://arxiv.org/abs/2506.03517</link>
<guid>https://arxiv.org/abs/2506.03517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DenseDPO方法，优化文本到视频扩散模型的训练数据标注方式。</p><br /><br /><p><strong>摘要：</strong> 近期，Direct Preference Optimization (DPO) 被用于文本到视频扩散模型的后训练技术。然而，传统DPO方法因采用低分辨率视频对比导致运动偏见，影响模型训练效果。本文提出DenseDPO，通过生成对齐的视频对、细化标注单元及利用视觉语言模型实现自动标注，显著提升了模型在运动生成上的表现，同时保持其他指标不下降。实验表明，DenseDPO仅需三分之一的标注数据即可超越原始DPO，且自动标注的性能接近人工标注。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 23:06:08 GMT</pubDate>
</item>
<item>
<title>RefEdit：基于指令的复杂场景图像编辑模型</title>
<link>https://arxiv.org/abs/2506.03448</link>
<guid>https://arxiv.org/abs/2506.03448</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RefEdit解决了现有图像编辑方法在复杂场景中的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RefEdit的新模型，专门用于处理包含多个对象的复杂场景图像编辑任务。尽管现有的基于反转和指令的图像编辑技术在单一显著对象的编辑上表现出色，但在处理复杂场景时却表现不佳。为了衡量这一差距，研究者创建了一个名为RefEdit-Bench的真实世界基准测试集，发现即使是经过大量样本训练的基础模型也难以应对。为了解决这个问题，研究团队开发了RefEdit，该模型通过可扩展的合成数据生成管道进行训练。令人印象深刻的是，RefEdit仅使用了20,000个编辑三元组就超过了那些基于Flux/SD3模型且训练于数百万数据的基线模型。广泛的评估显示，RefEdit不仅在指代表达任务上表现优异，还在传统基准测试中提升了性能，达到了开源方法中的最先进水平。我们还公开了数据集和检查点以促进可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03448" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 19:20:24 GMT</pubDate>
</item>
<item>
<title>LEAF: 提升CLIP文本编码器对抗鲁棒性的方法</title>
<link>https://arxiv.org/abs/2506.03355</link>
<guid>https://arxiv.org/abs/2506.03355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LEAF方法，显著提升CLIP文本编码器的零样本对抗精度。</p><br /><br /><p><strong>摘要：</strong> 对抗性输入攻击可能显著改变CLIP嵌入，影响依赖CLIP模型下游任务的鲁棒性，特别是文本编码器的鲁棒性尚未被充分研究。本研究填补了这一空白，提出了LEAF（Efficient Adversarial Fine-tuning Method），一种针对文本域的高效对抗微调方法，适用于大规模CLIP模型。实验表明，LEAF不仅大幅提升了文本域的零样本对抗精度，还保持了视觉域的性能。结合文本到图像扩散模型时，可提高在对抗噪声下的生成质量；应用于多模态检索任务时，能改善对抗噪声下的召回率。此外，鲁棒的文本编码器有助于通过直接优化恢复输入文本。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 15:57:09 GMT</pubDate>
</item>
<item>
<title>通过一次微调释放大型语言模型的推理潜力</title>
<link>https://arxiv.org/abs/2506.03295</link>
<guid>https://arxiv.org/abs/2506.03295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示一次微调即可显著提升LLMs的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何高效释放强大多语言模型（如Qwen-Math、MiMo和Phi-4）的推理潜力。尽管强化学习（RL）能大幅提升这些模型的推理性能，但其成本高昂且不稳定。本文提出一种名为Critique Fine-Tuning（CFT）的方法，仅需针对单一问题进行微调，通过收集模型生成的多样化解决方案并由教师模型提供详尽反馈来构建批评数据集。实验结果显示，Qwen和Llama系列模型在经过CFT训练后，在多种推理任务上取得了显著进步。例如，Qwen-Math-7B-CFT在六个数学基准测试中平均提升了15%，在三个逻辑推理基准测试中提升了16%，且所需计算资源仅为传统RL方法的1/20。消融研究进一步证明了一次性CFT方法的鲁棒性。这些发现表明，CFT是一种简单、通用且高效的策略，可有效释放现代大型语言模型的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 14:35:52 GMT</pubDate>
</item>
<item>
<title>SVGenius：面向SVG处理的大规模基准测试</title>
<link>https://arxiv.org/abs/2506.03139</link>
<guid>https://arxiv.org/abs/2506.03139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVGenius提出综合基准测试以评估大型语言模型在SVG处理中的能力。</p><br /><br /><p><strong>摘要：</strong> 现有的SVG处理基准存在现实覆盖不足、复杂度分层缺乏及评估范式碎片化的问题。本文介绍SVGenius，这是一个包含2,377个查询的全面基准测试，涵盖理解、编辑和生成三个维度。SVGenius基于来自24个应用领域的现实数据，系统性地分层复杂度，并通过8个任务类别和18个指标进行评估。研究评估了22种主流模型，发现闭源模型显著优于开源模型，所有模型在复杂度增加时性能均下降，表明现有方法的根本局限性。然而，增强推理训练比单纯扩展更有效，而样式转换对所有模型类型而言最具挑战性。SVGenius为矢量图形模型开发和自动化图形设计应用提供了重要的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>Critique-GRPO：结合自然语言反馈的强化学习优化框架</title>
<link>https://arxiv.org/abs/2506.03106</link>
<guid>https://arxiv.org/abs/2506.03106</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过结合自然语言与数值反馈，提出Critique-GRPO框架提升大模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了仅依赖数值反馈的强化学习在复杂推理任务中的局限性，如性能瓶颈、自我反思效果有限及持续失败等问题。研究发现，即使在性能停滞的情况下，强化学习微调模型仍可通过自然语言形式的批评生成正确改进。基于此，我们提出了Critique-GRPO框架，该框架融合自然语言与数值反馈进行策略优化，使大型语言模型在保持探索的同时，从初始响应和批评引导的改进中同步学习。实验表明，该方法在数学、STEM及通用推理任务上显著优于监督学习和传统强化学习微调方法，平均提升约4.5%-5%的pass@1分数。此外，进一步分析揭示了策略探索的两个关键见解：高熵并不总能保证高效学习，较长响应不一定带来更有效的探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03106" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:39:02 GMT</pubDate>
</item>
<item>
<title>TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models</title>
<link>https://arxiv.org/abs/2506.03099</link>
<guid>https://arxiv.org/abs/2506.03099</guid>
<content:encoded><![CDATA[
In this paper, we present TalkingMachines -- an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators. TalkingMachines enables natural conversational experiences by integrating an audio large language model (LLM) with our video generation foundation model. Our primary contributions include: (1) We adapt a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model of 18 billion parameters; (2) We enable infinite video streaming without error accumulation through asymmetric knowledge distillation from a bidirectional teacher model into a sparse causal, autoregressive student model; (3) We design a high-throughput, low-latency inference pipeline incorporating several key engineering optimizations such as: (a) disaggregation of the DiT and VAE decoder across separate devices, (b) efficient overlap of inter-device communication and computation using CUDA streams, (c) elimination of redundant recomputations to maximize frame-generation throughput. Please see demo videos here - https://aaxwaz.github.io/TalkingMachines/
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:29:28 GMT</pubDate>
</item>
<item>
<title>基于回归模型的LLM自动评估框架</title>
<link>https://arxiv.org/abs/2506.02945</link>
<guid>https://arxiv.org/abs/2506.02945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过回归模型提升LLM评估能力的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LLM-as-a-judge的框架，该框架利用大型语言模型（LLM）自动评估另一个LLM的输出。我们提出了四种定量LLM评估器，分别针对绝对反馈和相对反馈的不同类型，展示了此框架的通用性和灵活性。这些评估器通过回归模型对现有评估器的评分进行量化调整，使其更接近人类评分。与监督微调相比，我们的框架在计算效率上更高，并且当可用的人类反馈有限时，统计效率也更强。我们在四个数据集上使用两种基础评估器进行了验证实验，结果表明，通过后处理建模，定量评估器能够有效增强现有评估器的预测能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:44:23 GMT</pubDate>
</item>
<item>
<title>LongBioBench：一种用于评估长上下文语言模型的新基准</title>
<link>https://arxiv.org/abs/2506.02921</link>
<guid>https://arxiv.org/abs/2506.02921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于人工生成传记的长上下文语言模型评估新框架LongBioBench。</p><br /><br /><p><strong>摘要：</strong> 现有的长上下文语言模型评估框架主要分为现实世界任务和合成任务两类，但两者均存在局限性。现实世界任务复杂且易受数据污染影响，而合成任务则因缺乏针与干草堆之间的连贯性而削弱其作为实际应用代理的有效性。为应对这些挑战，我们提出理想的长上下文评估框架应具备无缝上下文、可控设置和可靠评估三大特征。本研究引入LongBioBench，这是一个利用人工生成传记作为控制环境的新基准，用于评估长上下文语言模型的理解、推理和可信度。实验评估显示，大多数模型在扩展上下文长度时仍存在语义理解和基础推理的缺陷，且可信度下降。进一步分析表明，现有合成基准的设计选择如上下文非连贯性、数值型问题及缺乏干扰项等，限制了对模型长上下文能力的测试。此外，我们还发现长上下文连续预训练主要通过调整RoPE嵌入来适应扩展的上下文长度。综上所述，相比之前的合成基准，LongBioBench在模拟真实语言任务与保持可控性之间取得了更好的平衡，具有更高的可解释性和配置性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 10:23:06 GMT</pubDate>
</item>
<item>
<title>Beyond the Surface: Measuring Self-Preference in LLM Judgments</title>
<link>https://arxiv.org/abs/2506.02592</link>
<guid>https://arxiv.org/abs/2506.02592</guid>
<content:encoded><![CDATA[
Recent studies show that large language models (LLMs) exhibit self-preference bias when serving as judges, meaning they tend to favor their own responses over those generated by other models. Existing methods typically measure this bias by calculating the difference between the scores a judge model assigns to its own responses and those it assigns to responses from other models. However, this approach conflates self-preference bias with response quality, as higher-quality responses from the judge model may also lead to positive score differences, even in the absence of bias. To address this issue, we introduce gold judgments as proxies for the actual quality of responses and propose the DBG score, which measures self-preference bias as the difference between the scores assigned by the judge model to its own responses and the corresponding gold judgments. Since gold judgments reflect true response quality, the DBG score mitigates the confounding effect of response quality on bias measurement. Using the DBG score, we conduct comprehensive experiments to assess self-preference bias across LLMs of varying versions, sizes, and reasoning abilities. Additionally, we investigate two factors that influence and help alleviate self-preference bias: response text style and the post-training data of judge models. Finally, we explore potential underlying mechanisms of self-preference bias from an attention-based perspective. Our code and data are available at https://github.com/zhiyuanc2001/self-preference.
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 04:12:47 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的数据增强策略解决知识蒸馏中的协变量偏移问题</title>
<link>https://arxiv.org/abs/2506.02294</link>
<guid>https://arxiv.org/abs/2506.02294</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法利用扩散模型生成数据增强样本，提升知识蒸馏中小模型的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 大型基础模型在多种领域展现了强大的零样本能力，但在数据和模型规模受限的情况下，知识蒸馏成为将知识从大模型迁移到小模型的有效工具。然而，知识蒸馏的效果受到可用训练数据的严重限制。本文针对知识蒸馏中的常见实际问题——协变量偏移展开研究，在这种情况下，训练过程中会出现测试时不存在的虚假特征。我们探讨了当这些虚假特征未知但存在鲁棒教师时，学生模型是否也能对这些特征具有鲁棒性。为了解决这个问题，我们引入了一种基于扩散模型的新颖数据增强策略，通过最大化教师和学生之间的分歧来生成图像，从而有效地创建学生模型难以应对的挑战性样本。实验表明，我们的方法显著提高了CelebA、SpuCo Birds以及在协变量偏移下的spurious ImageNet的最差组和平均组准确性，超越了现有的基于扩散模型的数据增强基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02294" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 18:15:59 GMT</pubDate>
</item>
<item>
<title>通过神经符号代理解决流图解释难题</title>
<link>https://arxiv.org/abs/2506.01344</link>
<guid>https://arxiv.org/abs/2506.01344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的方法来提高大语言模型处理流图的可靠性。</p><br /><br /><p><strong>摘要：</strong> 流图作为决策过程可视化的重要工具，因其非线性结构和复杂的图文关系，在利用大型语言模型（LLMs）进行解析时面临挑战。现有模型常出现不存在的连接和路径，影响了物流、医疗和工程等关键领域的自动化处理可靠性。本文引入细粒度流图归因任务，通过将模型响应与流图组件关联来验证预测并增强可解释性。为此，我们开发了FlowPathAgent，这是一种神经符号代理，通过基于图的推理实现细粒度后验归因。它首先分割流图，将其转换为结构化符号图，然后采用代理方法动态交互生成归因路径。此外，我们还提出了FlowExplainBench基准，用于评估不同风格、领域和问题类型的流图归因。实验表明，FlowPathAgent显著减少了流图问答中的视觉幻觉，在FlowExplainBench数据集上比强基线高出10-14个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 02:02:41 GMT</pubDate>
</item>
<item>
<title>Ψ-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models</title>
<link>https://arxiv.org/abs/2506.01320</link>
<guid>https://arxiv.org/abs/2506.01320</guid>
<content:encoded><![CDATA[
We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based generative model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 01:02:33 GMT</pubDate>
</item>
<item>
<title>BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation</title>
<link>https://arxiv.org/abs/2506.00482</link>
<guid>https://arxiv.org/abs/2506.00482</guid>
<content:encoded><![CDATA[
As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 05:24:32 GMT</pubDate>
</item>
<item>
<title>TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence</title>
<link>https://arxiv.org/abs/2505.24500</link>
<guid>https://arxiv.org/abs/2505.24500</guid>
<content:encoded><![CDATA[
Recently, Large Language Models (LLMs) have made significant progress in IQ-related domains that require careful thinking, such as mathematics and coding. However, enhancing LLMs' cognitive development in social domains, particularly from a post-training perspective, remains underexplored. Recognizing that the social world follows a distinct timeline and requires a richer blend of cognitive modes (from intuitive reactions (System 1) and surface-level thinking to deliberate thinking (System 2)) than mathematics, which primarily relies on System 2 cognition (careful, step-by-step reasoning), we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we systematically explore improving LLMs' social intelligence and validate the effectiveness of the TimeHC-RL method, through five other post-training paradigms and two test-time intervention paradigms on eight datasets with diverse data patterns. Experimental results reveal the superiority of our proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It gives the 7B backbone model wings, enabling it to rival the performance of advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic exploration from post-training and test-time interventions perspectives to improve LLMs' social intelligence has uncovered several valuable insights.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:01:06 GMT</pubDate>
</item>
<item>
<title>DLP: Dynamic Layerwise Pruning in Large Language Models</title>
<link>https://arxiv.org/abs/2505.23807</link>
<guid>https://arxiv.org/abs/2505.23807</guid>
<content:encoded><![CDATA[
Pruning has recently been widely adopted to reduce the parameter scale and improve the inference efficiency of Large Language Models (LLMs). Mainstream pruning techniques often rely on uniform layerwise pruning strategies, which can lead to severe performance degradation at high sparsity levels. Recognizing the varying contributions of different layers in LLMs, recent studies have shifted their focus toward non-uniform layerwise pruning. However, these approaches often rely on pre-defined values, which can result in suboptimal performance. To overcome these limitations, we propose a novel method called Dynamic Layerwise Pruning (DLP). This approach adaptively determines the relative importance of each layer by integrating model weights with input activation information, assigning pruning rates accordingly. Experimental results show that DLP effectively preserves model performance at high sparsity levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7% compared to state-of-the-art methods. Moreover, DLP is compatible with various existing LLM compression techniques and can be seamlessly integrated into Parameter-Efficient Fine-Tuning (PEFT). We release the code at https://github.com/ironartisan/DLP to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 03:35:00 GMT</pubDate>
</item>
<item>
<title>DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.21541</link>
<guid>https://arxiv.org/abs/2505.21541</guid>
<content:encoded><![CDATA[
Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 12:08:04 GMT</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA leftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on https://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}}, with code at https://github.com/GustavoStahl/CASS{blue{GitHub}}.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:48:53 GMT</pubDate>
</item>
<item>
<title>Subject Fidelity Optimization：一种提升零样本主体驱动生成主体保真度的新框架</title>
<link>https://arxiv.org/abs/2506.03621</link>
<guid>https://arxiv.org/abs/2506.03621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Subject Fidelity Optimization框架，通过对比学习增强生成模型的主体保真度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Subject Fidelity Optimization（SFO）的新颖比较学习框架，用于零样本主体驱动生成任务，旨在提高生成结果的主体保真度。不同于仅依赖正样本目标的传统监督微调方法，SFO引入合成负样本，并通过成对比较引导模型优先选择正样本。此外，为了生成有区分度且信息丰富的负样本，我们提出了条件退化负采样（CDNS），该方法通过故意破坏视觉和文本线索来实现，无需昂贵的人工标注。同时，我们重新加权扩散时间步长，使微调集中在主体细节显现的中间阶段。实验表明，结合CDNS的SFO在主体保真度和文本对齐方面显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 04 Jun 2025 02:59:25 GMT</pubDate>
</item>
<item>
<title>基于推理控制场的大规模推理模型可控长链推理研究</title>
<link>https://arxiv.org/abs/2506.00189</link>
<guid>https://arxiv.org/abs/2506.00189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出推理控制场方法提升大规模推理模型的长链推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模推理模型（LRMs）在长链推理中面临的欠思考与过度思考问题，提出了推理控制场（RCF），这是一种测试时通过注入结构化控制信号引导推理的新方法。RCF允许模型根据给定条件调整推理努力以解决复杂任务。此外，我们构建了Control-R-4K数据集，其中包含带有详细推理过程及对应控制字段的难题。为了进一步增强推理控制，我们还提出了条件蒸馏微调（CDF）方法，训练模型（如Control-R-32B）在测试时有效调整推理努力。实验表明，该方法在AIME2024和MATH500等基准测试中达到了最先进的性能，并实现了可控的长链推理过程。这项工作为测试时可扩展推理提供了一个有效的范例。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 15:59:44 GMT</pubDate>
</item>
<item>
<title>PoseFuse3D-KI：基于3D人体引导的可控关键帧插值框架</title>
<link>https://arxiv.org/abs/2506.03119</link>
<guid>https://arxiv.org/abs/2506.03119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PoseFuse3D-KI框架，结合3D人体引导信号改进视频插值效果。</p><br /><br /><p><strong>摘要：</strong> 现有关键帧插值方法主要依赖预训练的视频扩散先验，但缺乏三维几何引导，在处理复杂的人体运动时难以生成可信的结果，且对合成动态的控制有限。本文引入PoseFuse3D Keyframe Interpolator (PoseFuse3D-KI)，这是一种创新框架，通过将三维人体引导信号整合到扩散过程中实现以人为中心的关键帧插值(CHKI)。PoseFuse3D包含一个新颖的SMPL-X编码器，可将三维几何形状转换至二维潜在条件空间，并结合融合网络将这些三维线索与二维姿态嵌入相结合。为了评估该方法，我们构建了CHKI-Video数据集，其中标注了二维姿态和三维SMPL-X参数。实验表明，PoseFuse3D-KI在CHKI-Video上显著优于现有最先进方法，PSNR提升了9%，LPIPS降低了38%。此外，全面的消融研究证明了PoseFuse3D模型在提高插值保真度方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:50:05 GMT</pubDate>
</item>
<item>
<title>基于动态比例训练的高效语言推理方法</title>
<link>https://arxiv.org/abs/2506.02678</link>
<guid>https://arxiv.org/abs/2506.02678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需复杂标注的动态比例训练方法，显著减少推理输出的令牌数并保持准确性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过强化学习和扩展的链式思维技术取得了显著进展，但高效的语言推理，特别是在极长输出的推理阶段，仍然是研究领域的关注焦点。本文提出了一种动态比例训练管道，该方法不依赖复杂的注释或多个模型之间的插值。通过持续平衡模型系统-1和系统-2数据的权重，减少了冗余推理过程，同时保留了推理能力。我们在DeepSeek-R1-Distill-7B和DeepSeek-R1-Distill-14B模型上验证了这种方法，并在不同难度级别的基准测试中进行了评估。实验结果显示，该方法将输出令牌数减少了近40%，同时保持了推理的准确性。我们的代码和数据将在近期公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 05:23:41 GMT</pubDate>
</item>
<item>
<title>Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals</title>
<link>https://arxiv.org/abs/2506.02281</link>
<guid>https://arxiv.org/abs/2506.02281</guid>
<content:encoded><![CDATA[
Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed angle concentration that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data. Code is realsed at https://github.com/wangqinsi1/GAINRL/tree/main.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 17:40:38 GMT</pubDate>
</item>
<item>
<title>LongGuide：通过任务分布引导提升长文本生成中的上下文学习性能</title>
<link>https://arxiv.org/abs/2506.01265</link>
<guid>https://arxiv.org/abs/2506.01265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示上下文学习在长文本生成任务中的不足，并提出LongGuide提升模型表现。</p><br /><br /><p><strong>摘要：</strong> 上下文学习（ICL）是预训练大语言模型的一项重要能力，但在长文本生成任务如摘要生成中表现欠佳。本文通过实证与理论分析表明，仅依赖示例无法有效教会模型生成所需的语言和格式分布。为此，我们提出了LongGuide，它生成两种指导流：度量指南（MGs）和输出约束指南（OCGs），用于优化模型性能。实验显示，LongGuide显著提升了多种开源与闭源模型的表现，同时具备泛化性和与其他提示优化器的协同效应。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 22:35:24 GMT</pubDate>
</item>
<item>
<title>MoCA-Video：无需训练的视频语义混合框架</title>
<link>https://arxiv.org/abs/2506.01004</link>
<guid>https://arxiv.org/abs/2506.01004</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的视频语义混合方法MoCA-Video。</p><br /><br /><p><strong>摘要：</strong> MoCA-Video（Motion-Aware Concept Alignment in Video）是一种无需训练的框架，旨在弥合图像领域语义混合与视频之间的差距。通过将用户提供的参考图像的语义特征注入到视频中的特定对象，同时保持原始运动和视觉背景，该方法利用对角去噪调度和类别无关分割技术，在潜在空间中检测和跟踪对象并精确控制混合对象的空间位置。此外，通过引入基于动量的语义校正和伽马残差噪声稳定技术确保帧间时间一致性。实验表明，MoCA-Video在SSIM、LPIPS等标准指标上表现优异，并提出了新的CASS（概念对齐偏移分数）度量方法，即使没有训练或微调，其性能也显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01004" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 09:28:04 GMT</pubDate>
</item>
<item>
<title>Ctrl-Crash：一种可控汽车碰撞视频生成模型</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于扩散技术的可控汽车碰撞视频生成模型Ctrl-Crash。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频扩散技术取得了显著进展，但在生成逼真的汽车碰撞场景方面仍面临挑战，主要由于大多数驾驶数据集中此类事件稀缺。为了提升交通安全研究，需要真实且可控的事故模拟。本文介绍的Ctrl-Crash是一种可控制的汽车碰撞视频生成模型，它通过边界框、碰撞类型及初始图像帧等信号进行条件生成。该方法支持反事实场景生成，使得输入的微小变化可以导致完全不同的碰撞结果。此外，在推理阶段，我们利用无分类器引导机制，对每种条件信号独立调节缩放比例，以实现精细控制。实验表明，Ctrl-Crash在定量视频质量指标（如FVD和JEDi）及定性物理真实性的人类评估中均达到最先进的性能，超越了先前基于扩散的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 17:04:38 GMT</pubDate>
</item>
<item>
<title>REAL: 通过强化学习提升大型语言模型代码生成质量</title>
<link>https://arxiv.org/abs/2505.22704</link>
<guid>https://arxiv.org/abs/2505.22704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出REAL框架，利用程序分析指导反馈提高大模型代码安全性与可维护性。</p><br /><br /><p><strong>摘要：</strong> 当前基于大型语言模型（LLMs）的代码生成技术（即vibe coding）在生产环境中应用广泛，但无法充分保障代码质量，特别是在安全性和可维护性方面存在显著不足。传统方法如监督微调和基于规则的后处理需要大量人工标注或易碎的启发式规则，难以实现规模化应用。本文提出REAL框架，采用强化学习方法，借助程序分析检测安全缺陷和可维护性问题，同时结合单元测试确保功能正确性，从而激励模型生成高质量代码。与现有方法不同，REAL无需依赖特定提示或参考代码，实现了无手动干预的自动化监督。实验表明，REAL在多个数据集和模型规模上的功能与代码质量评估中均优于最先进的方法，有效弥合了原型开发与生产级代码之间的差距，使LLMs能够在速度和质量之间取得平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:57:47 GMT</pubDate>
</item>
<item>
<title>MERIT与Coral：多条件语义检索的新突破</title>
<link>https://arxiv.org/abs/2506.03144</link>
<guid>https://arxiv.org/abs/2506.03144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出首个多语言交错多条件语义检索数据集MERIT，并设计新框架Coral改进检索性能。</p><br /><br /><p><strong>摘要：</strong> 现有语义检索研究受限于单一语言或条件，无法充分利用视觉信息的表达能力。实际应用中常涉及多条件查询与多图像检索。针对此问题，本文引入MERIT数据集，包含5种语言、13.5万产品和32万查询，覆盖7类商品。实验揭示现有模型仅关注全局语义而忽略具体条件元素的缺陷。为此，我们提出Coral框架，通过嵌入重构保存细粒度条件信息并结合对比学习提取全局语义，使性能较传统方法提升45.9%，并在8个基准测试中展现强泛化能力。本研究贡献包括新型数据集、现有模型局限性分析及创新微调框架，为未来相关研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>Dual-Expert Consistency Model：加速视频扩散模型采样并提升视觉质量</title>
<link>https://arxiv.org/abs/2506.03123</link>
<guid>https://arxiv.org/abs/2506.03123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种双专家一致性模型解决视频扩散模型蒸馏中的时间一致性问题。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视频合成领域取得了显著成果，但其迭代去噪过程导致计算开销大。尽管一致性模型在加速扩散模型方面取得进展，但直接应用于视频扩散模型时，常出现时间一致性差和细节丢失的问题。本文通过分析一致性模型的训练动态，发现蒸馏过程中存在显著的时间步优化梯度差异，阻碍了学生模型达到最优状态。为此，我们提出了参数高效的Dual-Expert Consistency Model (DCM)，其中语义专家专注于学习语义布局和运动，细节专家则专门负责细节优化。此外，我们引入时间一致性损失以提高语义专家的运动一致性，同时采用GAN和特征匹配损失增强细节专家的合成质量。实验表明，该方法在大幅减少采样步骤的同时实现了最先进的视觉质量。我们的代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>FuseLIP：基于早期融合的多模态嵌入架构</title>
<link>https://arxiv.org/abs/2506.03096</link>
<guid>https://arxiv.org/abs/2506.03096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多模态嵌入架构FuseLIP，实现文本图像统一表示。</p><br /><br /><p><strong>摘要：</strong> 对比语言图像预训练方法通常通过独立编码器对齐文本图像特征，但在处理多模态输入时需额外模块整合特征。本文介绍FuseLIP，利用离散图像标记器，采用单一Transformer模型操作扩展词汇表中的文本和图像标记，实现早期融合，使各模态在编码过程中互动并获得更丰富的表示。我们构建了新的多模态预训练和评估数据集，并设计了具有挑战性的任务。实验表明，FuseLIP在视觉问答(VQA)和文本引导图像变换检索等多模态嵌入任务上优于其他方法，同时在单模态任务上表现与基线相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:27:12 GMT</pubDate>
</item>
<item>
<title>OThink-R1：优化大型推理模型中的冗余推理</title>
<link>https://arxiv.org/abs/2506.02397</link>
<guid>https://arxiv.org/abs/2506.02397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出OThink-R1方法，通过区分冗余推理和必要推理，显著减少推理步骤并提升效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）通过扩展的链式思考（CoT）推理技术，在解决复杂任务方面取得了卓越表现。然而，我们发现这些模型在处理简单任务时所采用的复杂推理可能并非必要，因为非推理型大语言模型（LLMs）也能用较少的token解决类似问题。基于此，本文系统分析了LRMs的推理轨迹，提出了利用已识别范式和LLM-Judge对这些轨迹进行分类的方法，将推理轨迹分为冗余推理和必要推理两类。同时，我们引入了OThink-R1方法，该方法能够在保持逻辑有效性的同时修剪掉冗余的推理步骤。具体而言，OThink-R1动态切换至非推理模式（快思考）处理简单问题，而在面对复杂问题时则启用深思熟虑的推理模式（慢思考）。实验表明，OThink-R1在数学和问答任务上平均减少了23%的推理冗余，且不降低准确性。这项工作为构建高效的推理模型提供了实用指导，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 23:31:30 GMT</pubDate>
</item>
<item>
<title>Qari-OCR：基于Qwen2-VL优化的阿拉伯文光学字符识别新突破</title>
<link>https://arxiv.org/abs/2506.02295</link>
<guid>https://arxiv.org/abs/2506.02295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qari-OCR通过迭代优化，在阿拉伯文OCR领域取得显著进展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Qari-OCR的视觉语言模型系列，该系列模型基于Qwen2-VL-2B-Instruct，经过针对阿拉伯文特性的逐步迭代微调，特别是在合成数据集上的优化，成功提升了阿拉伯文光学字符识别（OCR）的性能。其中，QARI v0.2模型在带有重音符号的文本上实现了0.160的词错误率（WER）、0.061的字符错误率（CER）和0.737的BLEU评分，表现出对阿拉伯文特有的重音标记、多样字体及文档布局的强大适应能力，同时在低分辨率图像处理方面也有出色表现。进一步的研究（QARI v0.3版本）展示了该模型在结构化文档理解和手写文本识别方面的潜力。本研究显著提高了阿拉伯文OCR的准确性和效率，并开放了所有模型和数据集以推动后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 18:21:06 GMT</pubDate>
</item>
<item>
<title>基于自挑战框架的大语言模型工具使用能力增强</title>
<link>https://arxiv.org/abs/2506.01716</link>
<guid>https://arxiv.org/abs/2506.01716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自挑战框架，利用自身生成高质量任务训练模型，显著提升工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为自挑战（Self-Challenging）的框架，用于通过模型自身生成的高质量任务进行训练，从而提高智能代理的工具使用能力。该框架首先让模型扮演挑战者角色，通过与工具交互生成任务，这些任务属于代码作为任务（Code-as-Task）的新问题类别，由指令、验证函数及成功与失败案例组成，以筛选高质量任务。随后，模型切换到执行者角色，通过强化学习在这些任务上进行训练，并根据评估反馈优化性能。实验表明，即使仅使用自生成的训练数据，该框架在M3ToolEval和TauBench两个现有多轮工具使用代理基准测试中，对Llama-3.1-8B-Instruct模型的表现提升了两倍以上。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 10:23:33 GMT</pubDate>
</item>
<item>
<title>SHARE: 基于分层动作修正的文本转SQL自纠错方法</title>
<link>https://arxiv.org/abs/2506.00391</link>
<guid>https://arxiv.org/abs/2506.00391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于小语言模型的分层SQL查询修正方法，提高LLMs的错误检测与修正能力。</p><br /><br /><p><strong>摘要：</strong> 当前文本转SQL的自纠错方法存在递归计算开销大及难以有效检测和修正声明式SQL查询错误的问题。为此，本文提出名为SHARE的分层动作修正辅助工具，通过三个专门的小语言模型按序处理，将声明式SQL转换为逐步动作轨迹以揭示推理路径，并进行两阶段精细优化。此外，还设计了一种高效的数据自适应训练策略。实验表明，SHARE显著提升了多种大型语言模型的自纠错性能，尤其在低资源训练场景下表现稳健，对受数据隐私限制的文本转SQL应用具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:51:12 GMT</pubDate>
</item>
<item>
<title>GUI-Actor: 一种无需坐标定位的视觉语言模型驱动GUI动作引导方法</title>
<link>https://arxiv.org/abs/2506.03143</link>
<guid>https://arxiv.org/abs/2506.03143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GUI-Actor方法，通过注意力机制实现无需坐标定位的GUI动作引导，显著提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GUI-Actor的视觉语言模型（VLM）驱动的GUI动作引导方法，解决了传统基于文本坐标生成方法的局限性，如空间语义对齐弱、无法处理模棱两可的监督目标等。GUI-Actor的核心是一个基于注意力的动作头，它学习将专用标记与所有相关的视觉补丁标记对齐，从而在一个前向传递中提出一个或多个动作区域。此外，设计了一个定位验证器，用于评估并选择最有可能执行动作的区域。实验表明，GUI-Actor在多个GUI动作定位基准测试上超过了现有最先进的方法，并且在未见过的屏幕分辨率和布局上有更好的泛化能力。值得注意的是，在ScreenSpot-Pro基准测试中，GUI-Actor-7B的表现甚至超过了UI-TARS-72B。通过引入验证器，我们发现只需微调新引入的动作头即可实现与先前最先进的模型相当的性能，而无需对整个VLM骨干进行微调，这表明GUI-Actor可以赋予底层VLM有效的定位能力而不损害其通用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:08 GMT</pubDate>
</item>
<item>
<title>OmniSpatial：面向空间推理的认知心理学基准测试</title>
<link>https://arxiv.org/abs/2506.03135</link>
<guid>https://arxiv.org/abs/2506.03135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniSpatial基准测试，评估视觉语言模型的空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了OmniSpatial，这是一个基于认知心理学的空间推理综合基准测试，涵盖动态推理、复杂空间逻辑、空间交互和视角转换四大类别及50个细粒度子类别。通过网络数据爬取和人工标注，构建超过1500个问答对。实验表明现有视觉语言模型和推理模型在全面空间理解上存在显著局限性，同时分析了失败案例并提出了未来研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:29 GMT</pubDate>
</item>
<item>
<title>AnimeShooter：基于参考图像的多镜头动画数据集及生成模型</title>
<link>https://arxiv.org/abs/2506.03126</link>
<guid>https://arxiv.org/abs/2506.03126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AnimeShooter数据集和生成模型，提升动画视频跨镜头一致性。</p><br /><br /><p><strong>摘要：</strong> 近年来，AI生成内容技术显著加速了动画制作流程。然而，现有公开数据集多聚焦于现实场景且缺乏角色指导的参考图像。为此，我们开发了AnimeShooter，这是一个具备视觉一致性并带有分层注释的多镜头动画数据集。该数据集不仅提供故事级注释（如情节、关键场景和角色参考图像），还通过镜头级注释分解故事为连续片段。此外，AnimeShooter-audio子集提供了同步音频轨道。为了验证数据集效果，我们设计了AnimeShooterGen模型，结合多模态大语言模型和视频扩散模型，实现了基于参考图像的多镜头动画生成。实验表明，该模型在跨镜头视觉一致性方面表现优异，展示了AnimeShooter的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:55:18 GMT</pubDate>
</item>
<item>
<title>ORV：基于占用场的机器人视频生成框架</title>
<link>https://arxiv.org/abs/2506.03079</link>
<guid>https://arxiv.org/abs/2506.03079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ORV，提升机器人模拟视频生成精度和泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对通过远程操作获取真实世界机器人模拟数据耗时且费力的问题，提出了一种名为ORV（Occupancy-centric Robot Video generation framework）的新框架。该框架利用4D语义占用序列作为细粒度表示，提供更精确的语义和几何指导，从而克服现有方法因全局粗略对齐而导致的控制精度低和泛化能力差的问题。实验表明，ORV在多个数据集和子任务上均优于现有基线方法，同时支持多视角机器人抓取操作视频的同步生成，有助于下游机器人学习任务。此外，ORV实现了仿真数据到逼真机器人视频的无缝转换，确保了高时间一致性与精确可控性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:00:32 GMT</pubDate>
</item>
<item>
<title>Sparse-vDiT：通过结构稀疏性加速视频扩散Transformer</title>
<link>https://arxiv.org/abs/2506.03065</link>
<guid>https://arxiv.org/abs/2506.03065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Sparse-vDiT框架，显著降低视频扩散Transformer的计算复杂度。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频生成领域中的扩散Transformer模型（DiTs），因自注意力机制导致的二次复杂度问题展开研究。通过对Video Diffusion Transformer (vDiT) 中注意力图的详细分析，发现三种常见的稀疏模式：对角线、多对角线和垂直条纹结构，并验证这些模式在不同层深和头位置具有强相关性，且较少依赖输入内容。基于此，我们提出Sparse-vDiT框架，包括针对每种稀疏模式设计的优化稀疏核替代密集注意力计算，以及一种硬件感知成本建模的离线稀疏扩散搜索算法。该方法在不影响视觉保真度的情况下显著降低了FLOPs并提升了推理速度。实验表明，在不同基准模型上，Sparse-vDiT分别实现了2.09倍、2.38倍和1.67倍的理论浮点运算减少，实际推理速度提升分别为1.76倍、1.85倍和1.58倍，证明了系统性利用潜在结构稀疏性的可行性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 12:42:37 GMT</pubDate>
</item>
<item>
<title>基于运动引导的长视频生成框架LumosFlow</title>
<link>https://arxiv.org/abs/2506.02497</link>
<guid>https://arxiv.org/abs/2506.02497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架LumosFlow，用于生成具有连贯性和吸引力的长视频。</p><br /><br /><p><strong>摘要：</strong> 长视频生成在娱乐和模拟等领域应用广泛，但合成长时间连贯且视觉吸引的长序列仍是挑战。传统方法如顺序生成短片段或分层生成关键帧常导致时间重复或不自然过渡问题。本文重新审视分层生成管道，提出LumosFlow框架，通过显式引入运动引导解决上述问题。首先利用LMTV-DM生成具有较大动作间隔的关键帧，确保内容多样性；然后将中间帧插值分解为运动生成和后处理优化。LOF-DM生成复杂的大动作光流，MotionControlNet进一步优化扭曲结果并指导中间帧生成。相比传统插值方法，LumosFlow实现15倍插值，确保相邻帧间合理连续运动。实验表明，该方法可生成一致性高且视觉效果优秀的长视频。代码和模型将在接受后公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:25:00 GMT</pubDate>
</item>
<item>
<title>基于非推理规模训练的大规模推理模型长链思维数据集构建</title>
<link>https://arxiv.org/abs/2506.02338</link>
<guid>https://arxiv.org/abs/2506.02338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索不依赖现有模型独立开发大规模推理模型的方法。</p><br /><br /><p><strong>摘要：</strong> 随着R1等公开可用的大规模推理模型的发布，研究人员通常通过在其长链思维推理上训练语言模型来开发新的模型。然而，这种对现有模型的依赖限制了领域的发展。本文作为独立开发大规模推理模型的第一步，探讨利用未经推理时间扩展训练的语言模型构建长链思维数据集的可能性。为此，我们提出了Long CoT Collection数据集，该数据集包含10万条由现有短链思维语言模型注释的推理理由。我们开发了一种管道，将o1的新推理策略引入短链思维语言模型，使它们能够进行更长的推理并更好地管理过度推理问题。广泛的分析验证了我们的数据集质量可与R1媲美或略低。此外，实验表明，在我们的数据集上训练不仅增强了通用推理能力，还为强化学习提供了坚实的基础，初始化在我们数据上的模型在RLVR上取得了2-3倍更大的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 20:29:15 GMT</pubDate>
</item>
<item>
<title>基于Layer-wise Relevance Propagation的Transformer可解释性改进</title>
<link>https://arxiv.org/abs/2506.02138</link>
<guid>https://arxiv.org/abs/2506.02138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的LRP方法，解决Transformer位置编码忽略问题，提升视觉和NLP任务的可解释性。</p><br /><br /><p><strong>摘要：</strong> Transformer模型的可解释性工具开发是深度学习研究的重要方向之一。本文聚焦于Layer-wise Relevance Propagation (LRP) 方法，指出现有基于LRP的Transformer可解释性方法忽视了位置编码这一关键组件，导致违背了守恒属性并丢失了与结构和位置相关的独特相关性。为解决此局限，我们重新定义了Transformer的输入空间为位置-标记对的集合，并提出了专门设计的理论基础LRP规则，用于传播各种位置编码方法（如Rotary、Learnable和Absolute PE）的归因。实验表明，该方法在经过微调的分类器和零样本基础模型（如LLaMA 3）上显著优于现有最先进的视觉和自然语言处理可解释性任务方法。代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 14:07:55 GMT</pubDate>
</item>
<item>
<title>Hanfu-Bench：探索文化的时间维度</title>
<link>https://arxiv.org/abs/2506.01565</link>
<guid>https://arxiv.org/abs/2506.01565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Hanfu-Bench数据集，聚焦文化的时间维度理解。</p><br /><br /><p><strong>摘要：</strong> 现有基于视觉语言模型(VLMs)的文化理解研究多关注地理多样性而忽视时间维度。为弥补这一不足，我们推出了Hanfu-Bench，一个由专家精心策划的多模态数据集。该数据集以汉服为载体，通过文化视觉理解和图像再创作两大核心任务，评估了封闭式和开放式VLMs在跨时空文化理解与创意适应中的表现。结果显示，尽管封闭式VLMs在视觉文化理解上接近非专家水平，但在与人类专家对比时仍有约10%的差距，而开放式的则表现更差。在图像再创作任务中，最佳模型的成功率仅为42%。Hanfu-Bench为未来研究提供了重要的测试平台，揭示了时间文化理解与创造性适应领域的重大挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 07:43:46 GMT</pubDate>
</item>
<item>
<title>ReFoCUS：通过强化学习优化视频帧选择提升多模态模型推理能力</title>
<link>https://arxiv.org/abs/2506.01274</link>
<guid>https://arxiv.org/abs/2506.01274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReFoCUS框架，通过强化学习优化视频帧选择提升视频问答性能。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型在视觉语言推理方面取得了显著进展，但对视频内容的理解仍受限于次优的帧选择策略。现有方法通常依赖静态启发式算法或外部检索模块，可能导致不相关的信息输入。本文介绍ReFoCUS（基于强化学习的上下文理解帧优化），这是一种新颖的帧级策略优化框架，将优化目标从文本响应转移到视觉输入选择上。ReFoCUS利用参考大模型的奖励信号通过强化学习训练帧选择策略，同时采用条件自回归架构提高效率并保证时序一致性。该方法无需帧级显式监督，在多个视频问答基准测试中均表现出色，证明了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 23:08:07 GMT</pubDate>
</item>
<item>
<title>FlowMo：无需训练的文本到视频扩散模型时间一致性增强方法</title>
<link>https://arxiv.org/abs/2506.01144</link>
<guid>https://arxiv.org/abs/2506.01144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的方法FlowMo，通过利用预训练模型自身的预测提升文本到视频生成中的运动连贯性。</p><br /><br /><p><strong>摘要：</strong> 当前的文本到视频扩散模型在建模运动、物理及动态交互等时间特性方面存在显著局限。传统方法通过重新训练模型或引入外部条件信号来解决这一问题。本研究探索是否可以直接从预训练模型的预测中提取有意义的时间表示，而无需额外训练或辅助输入。为此，我们提出了FlowMo，这是一种无需训练的引导方法，仅基于模型自身在每个扩散步的预测即可增强运动连贯性。FlowMo首先通过计算连续帧对应潜在空间的距离，获得去外观偏置的时间表示，揭示模型隐含的时间结构。随后，通过测量时间维度上补丁级别的方差来估计运动连贯性，并在采样过程中动态指导模型减少该方差。大量实验表明，FlowMo显著提高了运动连贯性，同时保持视觉质量和提示对齐，为增强预训练视频扩散模型的时间保真度提供了一种有效的即插即用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 15:55:33 GMT</pubDate>
</item>
<item>
<title>ActiveKD：结合主动学习与知识蒸馏的框架</title>
<link>https://arxiv.org/abs/2506.00910</link>
<guid>https://arxiv.org/abs/2506.00910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ActiveKD框架，利用大规模视觉语言模型实现主动学习与知识蒸馏的结合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ActiveKD的新框架，旨在将主动学习（AL）与知识蒸馏（KD）相结合，特别是在数据稀缺场景下，通过利用大规模视觉语言模型（VLMs）的零样本和少样本能力，克服传统知识蒸馏需要大量标注数据的限制。ActiveKD的关键在于VLMs的结构化预测偏置，即其预测在概率空间中形成聚类，可视为一种归纳偏置，有助于学生模型学习到可泛化的输出模式。为充分利用这种偏置，我们提出了Probabilistic CoreSet（PCoreSet）选择策略，该策略专注于最大化概率空间中的覆盖率而非特征空间。实验评估显示，PCoreSet在11个数据集上的表现优于现有方法，推动了主动学习与知识蒸馏交叉领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 04:54:37 GMT</pubDate>
</item>
<item>
<title>自适应并行解码提升扩散大语言模型生成速度</title>
<link>https://arxiv.org/abs/2506.00413</link>
<guid>https://arxiv.org/abs/2506.00413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法实现扩散大语言模型的高效并行解码。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统自回归解码在大规模语言模型生成中的瓶颈问题，提出了一种名为自适应并行解码(APD)的新方法。该方法通过动态调整并行采样的令牌数量，在保持质量的同时显著提高了生成速度。具体而言，APD通过定义扩散大语言模型边缘概率与小辅助自回归模型序列联合概率之间的乘法混合，颠覆了推测性解码的标准设置。此外，还优化了KV缓存和掩码输入大小。实验结果显示，APD在下游基准测试中提供了明显更高的吞吐量，且质量损失极小，实现了效率与性能的良好平衡。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 02:10:10 GMT</pubDate>
</item>
<item>
<title>Visual Embodied Brain (VeBrain): 统一多模态大型语言模型的机器人应用框架</title>
<link>https://arxiv.org/abs/2506.00123</link>
<guid>https://arxiv.org/abs/2506.00123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架VeBrain，将机器人控制统一为文本任务以提升多模态能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Visual Embodied Brain (VeBrain) 的统一框架，用于现实世界中的感知、推理和控制。VeBrain通过将机器人控制问题重新表述为基于文本的多模态大型语言模型(MLLMs)任务，实现了不同任务目标和映射空间的统一。此外，还提出了一个新颖的机器人适配器，可将MLLMs生成的文本控制信号转换为实际机器人的运动策略。为了支持这一框架，我们构建了VeBrain-600k数据集，包含了大量高质量指令数据，涵盖了多种能力并通过多模态链式思维方法混合不同能力。实验表明，VeBrain在13个多模态基准测试和5个空间智能基准测试中均表现出色，相较于现有模型如Qwen2.5-VL有显著改进。当应用于腿足机器人和机械臂时，VeBrain展示了更强的适应性、灵活性和组合能力。例如，在MMVet测试中，VeBrain比Qwen2.5-VL提升了5.6%，而在腿足机器人任务中更是提高了50%的平均性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 14:00:34 GMT</pubDate>
</item>
<item>
<title>基于自我反思与强化学习的大语言模型性能提升方法</title>
<link>https://arxiv.org/abs/2505.24726</link>
<guid>https://arxiv.org/abs/2505.24726</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过自我反思与强化学习改进大语言模型在复杂任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种通过自我反思和强化学习提升大型语言模型性能的方法。当模型回答错误时，通过激励其生成更好的自我反思评论，即使无法生成合成数据且仅能获得二元反馈的情况下，也能显著提高解决复杂验证任务的能力。该框架分为两个阶段：首先，在任务失败后生成自我反思评论；其次，在反思的基础上再次尝试任务。若第二次尝试成功，则对反思阶段生成的标记进行奖励。实验结果显示，不同架构模型均取得显著性能提升，例如数学公式写作提高了34.7%，函数调用提高了18.1%。值得注意的是，较小规模的微调模型（1.5亿至70亿参数）在某些任务上优于同家族更大规模的模型（10倍大小）。这一新范式为构建更实用、可靠的自优化语言模型提供了令人兴奋的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24726" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 11:49:42 GMT</pubDate>
</item>
<item>
<title>零样本链式思维推理过程成功预测研究</title>
<link>https://arxiv.org/abs/2505.24362</link>
<guid>https://arxiv.org/abs/2505.24362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，基于大型语言模型表示的探测分类器可在推理初始阶段预测链式思维推理的成功。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨是否可以在零样本链式思维（CoT）推理完成前预测其成功与否。实验表明，基于大型语言模型（LLM）表示的探测分类器即使在生成第一个标记之前也能表现良好，这表明推理过程的关键信息在初始步骤的表示中已经存在。相比之下，依赖生成标记的强BERT基线表现较差，可能是因为它更多依赖于浅层的语言线索而非深层的推理动态。令人惊讶的是，使用后续推理步骤并不总是提高分类性能，在某些情况下，较早的表示与后期表示更为相似，表明LLMs早期就编码了关键信息，这意味着推理可以较早停止而不会造成损失。为了验证这一点，我们进行了提前终止实验，结果显示截断CoT推理仍能提高性能，但与完整推理相比仍有差距。然而，通过监督学习或强化学习等方法缩短CoT链条时，可以利用我们的分类器指导来判断何时进行有效提前终止。这些发现为优化CoT效率提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 04:54:28 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中回溯技术对推理能力提升的研究</title>
<link>https://arxiv.org/abs/2505.24273</link>
<guid>https://arxiv.org/abs/2505.24273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示回溯技术如何显著改善大规模语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地探讨了监督微调（SFT）与强化学习（RL）结合时回溯技术对八类推理任务的影响。实验发现，较短的思维链序列在简单任务中对RL训练有适度贡献，但随着任务难度增加，这种作用减弱。通过构建具有不同回溯步数的数据集，我们发现较长的思维链且包含回溯通常能带来更好的RL训练效果，且复杂问题需要更多回溯。此外，蒸馏数据的实验表明，RL训练对长思维链序列的正确性依赖较低，更注重结构模式。这些结果为优化大规模语言模型的推理能力提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24273" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 02:49:00 GMT</pubDate>
</item>
<item>
<title>CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs</title>
<link>https://arxiv.org/abs/2505.24120</link>
<guid>https://arxiv.org/abs/2505.24120</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) have demonstrated remarkable progress in multimodal understanding, yet their capabilities for scientific reasoning remains inadequately assessed. Current multimodal benchmarks predominantly evaluate generic image comprehension or text-driven reasoning, lacking authentic scientific contexts that require domain-specific knowledge integration with visual evidence analysis. To fill this gap, we present CSVQA, a diagnostic multimodal benchmark specifically designed for evaluating scientific reasoning through domain-grounded visual question answering.Our benchmark features 1,378 carefully constructed question-answer pairs spanning diverse STEM disciplines, each demanding domain knowledge, integration of visual evidence, and higher-order reasoning. Compared to prior multimodal benchmarks, CSVQA places greater emphasis on real-world scientific content and complex reasoning.We additionally propose a rigorous evaluation protocol to systematically assess whether model predictions are substantiated by valid intermediate reasoning steps based on curated explanations. Our comprehensive evaluation of 15 VLMs on this benchmark reveals notable performance disparities, as even the top-ranked proprietary model attains only 49.6\% accuracy.This empirical evidence underscores the pressing need for advancing scientific reasoning capabilities in VLMs. Our CSVQA is released at https://huggingface.co/datasets/Skywork/CSVQA.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:34:25 GMT</pubDate>
</item>
<item>
<title>Robot-R1：通过强化学习提升机器人视觉语言模型的具身推理能力</title>
<link>https://arxiv.org/abs/2506.00070</link>
<guid>https://arxiv.org/abs/2506.00070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于强化学习的新框架Robot-R1，用于提升机器人视觉语言模型的具身推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型视觉语言模型（LVLMs）近年来在结合具身推理与机器人控制方面展现了巨大潜力。然而，传统的监督微调（SFT）方法存在数据构建不优化及泛化性能下降等问题。为解决这些局限性，我们提出了Robot-R1，这是一种利用强化学习增强机器人控制中具身推理的新框架。Robot-R1通过预测任务完成所需的下一个关键点状态，基于场景图像和环境元数据进行条件学习，这些数据源自专家演示。受DeepSeek-R1方法启发，Robot-R1对基于推理的响应进行采样并强化那些产生更准确预测的响应。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法，即使参数量仅为7B，也超越了GPT-4o在低级动作控制相关的推理任务上的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:41:12 GMT</pubDate>
</item>
<item>
<title>DINGO：一种高效且分布保持的约束解码策略</title>
<link>https://arxiv.org/abs/2505.23061</link>
<guid>https://arxiv.org/abs/2505.23061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DINGO方法解决扩散语言模型无法满足用户指定约束的问题。</p><br /><br /><p><strong>摘要：</strong> 扩散语言模型（Diffusion LLMs）因其显著的运行效率提升而备受关注，但缺乏对用户指定形式化约束（如正则表达式）的支持，这限制了其在需要结构化输出的任务中的应用。与自回归模型不同，扩散LLMs并行预测一组token，使得传统的约束解码算法失效。为了解决这一问题，我们提出了DINGO，这是一种基于动态规划的约束解码策略，既高效又能保证输出分布的真实性。实验表明，DINGO在标准符号数学和JSON生成基准测试中比无约束推理提高了多达68个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:04:54 GMT</pubDate>
</item>
<item>
<title>基于深度视频发现代理的长视频理解方法</title>
<link>https://arxiv.org/abs/2505.18079</link>
<guid>https://arxiv.org/abs/2505.18079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用强化学习策略处理长视频理解问题的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对长视频理解中存在的时空复杂性和上下文问答难题，提出了一种名为Deep Video Discovery (DVD)代理的方法。传统的大语言模型（LLMs）虽在视频分析方面有所进步，但在处理长达数小时的信息密集型视频时仍显局限。DVD代理通过在分割后的视频片段上采用自主搜索策略，结合多粒度视频数据库中的工具集，利用LLMs的高级推理能力进行状态观察、工具选择及参数优化，从而实现对视频内容的有效解析。实验结果显示，该方法在多个长视频理解基准测试中表现出色，特别是在LVBench数据集上的表现超越了现有技术。此外，文中还进行了消融研究和工具分析，为未来面向长视频理解任务的智能代理发展提供了宝贵见解。代码将在之后发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:37:36 GMT</pubDate>
</item>
<item>
<title>UniWorld：基于语义特征的统一生成框架</title>
<link>https://arxiv.org/abs/2506.03147</link>
<guid>https://arxiv.org/abs/2506.03147</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于语义特征的统一生成框架UniWorld，在图像编辑任务上超越BAGEL。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniWorld的统一生成框架，该框架利用强大的视觉语言模型和对比语义编码器提供的语义特征，旨在解决现有统一模型在图像感知和操作任务上的局限性。通过实验发现，GPT-4o-Image不依赖传统的VAE，而是使用语义编码器提取特征，这一观察启发了我们开发UniWorld。UniWorld仅使用BAGEL数据量的1%，却在图像编辑基准测试中始终优于BAGEL，并且在图像理解和生成能力上表现强劲，适用于多种图像感知任务。我们的研究不仅展示了UniWorld在图像操作领域的潜力，还强调了语义特征在跨模态任务中的重要性。此外，我们将模型、训练和评估脚本以及数据集完全开源，以促进相关领域的进一步研究和应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03147" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.03136</link>
<guid>https://arxiv.org/abs/2506.03136</guid>
<content:encoded><![CDATA[
We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes, without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Our derived ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models, outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They naturally extend to downstream tasks such as test-time scaling and agentic coding-achieving a 8.1% improvement over the base model. For the long-CoT model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while achieving 64.8% inference efficiency in unit test generation. Notably, we also find that our model can serve as an effective reward model for reinforcement learning on base models. Project: https://github.com/Gen-Verse/CURE
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:58:42 GMT</pubDate>
</item>
<item>
<title>基于扩散Transformer的任意分辨率图像合成</title>
<link>https://arxiv.org/abs/2506.03131</link>
<guid>https://arxiv.org/abs/2506.03131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法实现任意分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为native-resolution image synthesis的新生成建模范式，它能够以任意分辨率和宽高比合成图像。通过引入Native-resolution diffusion Transformer（NiT），该架构能够在去噪过程中显式建模变化的分辨率和宽高比。NiT摆脱了固定格式的限制，从多种分辨率和宽高比的图像中学习内在视觉分布。实验结果显示，单一NiT模型在ImageNet-256x256和512x512基准测试中均达到最先进的性能，并且在未见过的高分辨率（如1536 x 1536）和多样化的宽高比（如16:9、3:1、4:3）上表现出色的零样本泛化能力，类似于先进的大型语言模型。这项研究揭示了native-resolution建模作为视觉生成建模与先进LLM方法之间桥梁的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.03131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 13:57:33 GMT</pubDate>
</item>
<item>
<title>基于视觉提示的可泛化图像编辑新范式</title>
<link>https://arxiv.org/abs/2506.02528</link>
<guid>https://arxiv.org/abs/2506.02528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RelationAdapter模块提升图像编辑模型对非刚性变换的理解能力。</p><br /><br /><p><strong>摘要：</strong> 受大型语言模型上下文学习机制的启发，一种新的基于视觉提示的通用图像编辑范式正在兴起。现有单参考方法多集中于风格或外观调整，难以处理非刚性变换问题。为此，我们通过利用源目标图像对提取并转移内容感知的编辑意图至查询图像。为此，引入RelationAdapter轻量级模块，使基于Diffusion Transformer的模型能够有效捕捉并应用来自极少量示例的视觉变换。同时，构建Relation252K综合数据集评估模型在视觉提示驱动场景中的泛化性和适应性。实验表明，RelationAdapter显著提升了模型理解及传递编辑意图的能力，大幅提高了生成质量和整体编辑性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 03:06:35 GMT</pubDate>
</item>
<item>
<title>M^3FinMeeting：多语言金融会议理解基准的开创性研究</title>
<link>https://arxiv.org/abs/2506.02510</link>
<guid>https://arxiv.org/abs/2506.02510</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为M^3FinMeeting的新基准，用于评估大型语言模型在金融会议理解中的表现。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步促使开发新的金融领域性能评估基准，但现有基准多依赖新闻或财报等静态文本，难以反映真实金融会议的动态特性。为弥补这一不足，本研究提出了M^3FinMeeting，这是一个支持多语言、多行业和多任务的金融会议理解数据集。该数据集包含英语、中文和日语三种语言，覆盖全球行业分类标准(GICS)定义的多个行业部门，并设计了摘要生成、问答对提取及问答三个任务。实验结果显示，即使是最先进的长上下文模型在M^3FinMeeting上的表现仍有显著提升空间，表明该基准在评估LLMs金融会议理解能力方面具有重要价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02510" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 02:41:09 GMT</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher：结合文本与可视化的大语言模型深度研究框架</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Multimodal DeepResearcher框架，实现文本与可视化报告的高效生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度研究框架主要关注文本生成而忽视图文混排的问题，提出了Formal Description of Visualization (FDV)，一种结构化的图表文本表示方法，使大语言模型能够学习并生成高质量的可视化内容。在此基础上，设计了Multimodal DeepResearcher框架，将任务分解为四个阶段：研究、示例报告文本化、规划及多模态报告生成。为了评估生成的多模态报告，构建了MultimodalReportBench基准数据集，包含100个多样化主题，并采用五个专用指标进行评估。实验表明，该框架在多个模型和评估方法下均优于基线方法，尤其使用Claude 3.7 Sonnet模型时，整体胜率可达82%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 03 Jun 2025 01:18:19 GMT</pubDate>
</item>
<item>
<title>Visual Strategic Bench (VS-Bench): 多智能体环境中视觉语言模型的战略推理评估</title>
<link>https://arxiv.org/abs/2506.02387</link>
<guid>https://arxiv.org/abs/2506.02387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多模态基准VS-Bench，用于评估视觉语言模型在多智能体环境中的战略推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉语言模型（VLMs）的能力已扩展到交互代理任务，但现有基准大多局限于单代理或纯文本环境，无法反映真实世界中涉及多智能体、复杂视觉和语言交互的场景。为解决这一问题，我们引入了Visual Strategic Bench (VS-Bench)，这是一个多模态基准，旨在评估VLMs在多智能体环境中的战略推理和决策能力。VS-Bench包含八个视觉相关的环境，涵盖合作、竞争及混合动机交互，通过预测未来动作准确性和归一化回合回报等指标进行评估。实验结果显示当前模型与最优性能之间存在显著差距，同时深入分析了多模态观察、测试时扩展性、社会行为及模型失败案例。我们希望VS-Bench成为未来多模态战略代理研究的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 22:57:38 GMT</pubDate>
</item>
<item>
<title>基于合成数据增强的视觉语言模型强化学习研究</title>
<link>https://arxiv.org/abs/2506.02096</link>
<guid>https://arxiv.org/abs/2506.02096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SynthRL方法，通过合成数据提升视觉语言模型强化学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过合成强化学习数据进一步优化具有可验证奖励的视觉语言模型强化学习方法。为此，我们设计了一个名为SynthRL的可扩展且可靠的推理导向强化学习训练自动数据扩展管道。该方法包含三个关键阶段：选择分布合理的种子问题、生成更具挑战性的变体同时保留原始答案、确保接近完美的正确性和难度增强的验证阶段。实验表明，SynthRL在MMK12数据集上可生成超过3.3K个高质量问题，显著提升了跨五个域外视觉数学推理基准的表现，尤其在最具挑战性的样本上表现更为突出。这证明了SynthRL在激发更深层次推理模式方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.02096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:45:16 GMT</pubDate>
</item>
<item>
<title>构建高质量数据集的挑战与系统性评估方法</title>
<link>https://arxiv.org/abs/2506.01789</link>
<guid>https://arxiv.org/abs/2506.01789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过引入评估量规解决数据集质量评价问题。</p><br /><br /><p><strong>摘要：</strong> 高质量数据集对机器学习模型至关重要，但其创建过程尤其是人工标注面临诸多挑战。当前数据集论文提交存在原创性不足、多样性缺乏及质量控制不严等问题，而现有工具如数据表单虽促进透明度，却未能提供标准化评估方法。为应对这些局限，本文主张在数据集评审过程中整合系统化的量规评估指标，同时探讨合成数据生成的高效且经济的方法。作为行动号召，我们提出了DataRubrics框架，该框架基于LLM技术，可对人工和模型生成的数据集进行结构化评估，实现可重复、可扩展且实用的质量评估方案，并开源代码支持评估复现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 11:31:52 GMT</pubDate>
</item>
<item>
<title>MotionSight：零样本细粒度视频运动理解的新方法</title>
<link>https://arxiv.org/abs/2506.01674</link>
<guid>https://arxiv.org/abs/2506.01674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的零样本方法MotionSight，提升多模态大语言模型的细粒度视频运动理解能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大型语言模型（MLLMs）取得了显著进展，但其在细粒度视频运动理解方面的能力仍然有限。现有模型通常缺乏帧间差异处理，且倾向于忽略微妙的视觉线索。此外，虽然视觉提示在静态图像中的应用已显示出潜力，但在视频的时间复杂性上的应用，特别是针对细粒度运动理解，仍鲜有研究。本文介绍了一种名为MotionSight的新方法，通过引入对象中心的视觉聚光灯和运动模糊作为视觉提示，在不进行训练的情况下有效改善了细粒度运动理解。为支持这一方法，我们创建了MotionVid-QA，这是首个用于细粒度视频运动理解的大规模数据集，包含层次化的注释，如SFT和偏好数据，以及约40K个视频片段和87K个问答对。实验表明，MotionSight在开源模型中达到了最先进的性能，并在竞争力上接近商业模型。这项研究展示了零样本技术在视频运动理解中的新可能性，并提供了高质量的数据集和公开代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:44:56 GMT</pubDate>
</item>
<item>
<title>金融领域多模态大型语言模型评估基准FinMME发布</title>
<link>https://arxiv.org/abs/2505.24714</link>
<guid>https://arxiv.org/abs/2505.24714</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinMME填补金融领域的多模态评估数据集空白。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FinMME的金融领域多模态评估数据集，包含超过11,000个高质量样本，覆盖18个金融领域和6类资产，支持10种主要图表类型及其子类型。通过精心设计的验证机制和20名标注员确保数据质量。同时，开发了FinScore评估系统，采用幻觉惩罚和多维能力评估，以提供公正的评价。实验表明，即使是最先进的模型如GPT-4o，在FinMME上的表现也不尽人意，凸显了该数据集的挑战性。此外，FinMME具有高鲁棒性，不同提示下的预测变化低于1%，优于现有数据集。该数据集和评估协议可通过Hugging Face和GitHub获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24714" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 11:36:19 GMT</pubDate>
</item>
<item>
<title>RRec：具有内在推理能力的统一推荐模型</title>
<link>https://arxiv.org/abs/2505.16994</link>
<guid>https://arxiv.org/abs/2505.16994</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RRec模型，通过融合推理和推荐优化提升推荐系统性能。</p><br /><br /><p><strong>摘要：</strong> 当前大型推荐模型通常将大型语言模型(LLMs)作为外部推理模块，用于增强传统推荐管道。然而，这种解耦设计存在显著资源成本和次优联合优化问题。为解决这些问题，我们提出了RRec，这是一种具备内在推理能力的统一大型推荐模型。首先，重新设计模型架构以促进自回归过程中的交错推理和推荐；其次，提出RecPO强化学习框架，在单一策略更新中同时优化推理和推荐能力。RecPO引入融合奖励方案，仅依赖推荐标签模拟推理能力，无需专门的推理注释。在三个数据集上的实验验证了RRec的有效性，相较于基线模型，Hit@5指标提升了68.67%，NDCG@20指标提升了45.21%。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16994" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:55:43 GMT</pubDate>
</item>
<item>
<title>多编程语言与英语在大语言模型概念空间中的关系研究</title>
<link>https://arxiv.org/abs/2506.01074</link>
<guid>https://arxiv.org/abs/2506.01074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨大语言模型如何在概念空间中表示多种编程语言与英语的关系。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大语言模型（LLMs）在处理编程语言（PLs）时的机制，特别是它们如何在多语言环境下表示英语。通过在两个基于Llama的模型上进行少量样本翻译任务，我们观察到概念空间更接近英语（包括PL关键字），且中间层后半部分对英语标记赋予高概率。进一步分析显示，虽然语言特定神经元主要集中在底层，但每种PL特有的神经元往往出现在顶层。对于与其他PL高度对齐的PL，无法识别语言特定神经元，这类PL通常具有更大的关键字集，并在翻译任务中无论输入/输出PL如何，都更接近模型的概念空间。我们的发现揭示了LLMs内部表示PL的结构模式，有助于深入理解模型的工作原理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 12:24:13 GMT</pubDate>
</item>
<item>
<title>SealQA：评估搜索增强语言模型的新基准</title>
<link>https://arxiv.org/abs/2506.01062</link>
<guid>https://arxiv.org/abs/2506.01062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SealQA包含三种版本，用于测试模型在复杂事实查询中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SealQA，这是一个针对搜索增强语言模型在面对冲突性、噪声或无用搜索结果时处理事实查询能力的新基准。SealQA分为三个版本：Seal-0（主版）和Seal-Hard，侧重于评估模型的事实准确性与推理能力，其中Seal-0专注于最具挑战性的问题；LongSeal则扩展了SealQA，用于测试长上下文、多文档推理能力。研究发现当前模型存在显著局限性，即使是最前沿的语言模型在所有SealQA版本中表现也较差。例如，在Seal-0上，配备工具如o3和o4-mini的前沿代理模型分别仅达到17.1%和6.3%的最佳推理准确率。尽管高级推理模型对搜索结果的噪声敏感，但增加测试时间计算并未带来可靠提升。此外，虽然新模型较少受到“迷失中间”问题的影响，但在LongSeal中仍难以有效识别相关文档。为促进未来研究，SealQA已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 12:04:34 GMT</pubDate>
</item>
<item>
<title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title>
<link>https://arxiv.org/abs/2506.00979</link>
<guid>https://arxiv.org/abs/2506.00979</guid>
<content:encoded><![CDATA[
The rapid advancement of Artificial Intelligence Generated Content (AIGC) in visual domains has resulted in highly realistic synthetic images and videos, driven by sophisticated generative frameworks such as diffusion-based architectures. While these breakthroughs open substantial opportunities, they simultaneously raise critical concerns about content authenticity and integrity. Many current AIGC detection methods operate as black-box binary classifiers, which offer limited interpretability, and no approach supports detecting both images and videos in a unified framework. This dual limitation compromises model transparency, reduces trustworthiness, and hinders practical deployment. To address these challenges, we introduce IVY-FAKE , a novel, unified, and large-scale dataset specifically designed for explainable multimodal AIGC detection. Unlike prior benchmarks, which suffer from fragmented modality coverage and sparse annotations, IVY-FAKE contains over 150,000 richly annotated training samples (images and videos) and 18,700 evaluation examples, each accompanied by detailed natural-language reasoning beyond simple binary labels. Building on this, we propose Ivy Explainable Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture that jointly performs explainable detection for both image and video content. Our unified vision-language model achieves state-of-the-art performance across multiple image and video detection benchmarks, highlighting the significant advancements enabled by our dataset and modeling framework. Our data is publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:20:22 GMT</pubDate>
</item>
<item>
<title>RAG系统在动态语料库上的鲁棒性评估</title>
<link>https://arxiv.org/abs/2506.00789</link>
<guid>https://arxiv.org/abs/2506.00789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RARE框架揭示了RAG系统对实时噪声和事实变化的脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RARE的统一框架和大规模基准测试，用于评估基于检索增强生成（RAG）模型在处理动态时间敏感型语料库时的鲁棒性。RARE通过知识图谱驱动的合成管道自动生成多级问题集，构建了一个包含400份金融、经济和政策文档及48,322个问题的数据集。研究发现，无论生成器大小或架构如何，RAG系统在面对查询、文档或实际检索结果的变化时，文档层面的鲁棒性始终最弱，并且在多跳问题上的表现明显低于单跳问题。这一研究表明现有RAG系统的现实应用能力仍有待提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 22:42:36 GMT</pubDate>
</item>
<item>
<title>Neuro2Semantic：基于iEEG信号的语言语义解码新框架</title>
<link>https://arxiv.org/abs/2506.00381</link>
<guid>https://arxiv.org/abs/2506.00381</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种新型框架Neuro2Semantic实现从神经信号重建语言语义。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Neuro2Semantic的新框架，该框架可以从颅内脑电图(iEEG)记录中重建感知语音的语义内容。Neuro2Semantic分为两个阶段：首先，基于LSTM的适配器将神经信号与预训练文本嵌入对齐；其次，校正模块直接从这些对齐的嵌入中生成连续自然文本。这种方法克服了先前解码方法的局限性，支持不受约束的文本生成。实验表明，即使只有30分钟的神经数据，Neuro2Semantic也能在低数据场景下超越最新的同类方法，显示出在脑机接口和神经解码技术中的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00381" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:17:19 GMT</pubDate>
</item>
<item>
<title>源无关域自适应中的增强技术与伪标签重加权策略</title>
<link>https://arxiv.org/abs/2505.24216</link>
<guid>https://arxiv.org/abs/2505.24216</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法提升源无关域适应性能，在多个基准数据集上取得最佳结果。</p><br /><br /><p><strong>摘要：</strong> 本文研究了源无关域适应（SFDA），即模型在不访问源域数据的情况下适应目标域的问题。文中引入了一种新的数据增强技术——Shuffle PatchMix（SPM）以及一种新颖的伪标签重加权策略，旨在提高模型性能。SPM通过打乱并混合图像块生成多样化且具有挑战性的增强样本，而伪标签重加权策略则优先考虑可靠的伪标签以减轻标签噪声的影响。这些技术在较小的数据集如PACS上表现尤为突出，因为这类数据集更容易出现过拟合和伪标签噪声问题。实验结果显示，该方法在三个主要基准数据集PACS、VisDA-C和DomainNet-126上均达到了最先进的性能。特别是在PACS数据集上，单目标设置下的准确性从79.4%提升至86.7%，多目标设置下提高了7.2%；而在DomainNet-126和VisDA-C上的准确率分别提升了2.8%和0.7%。这一结合先进增强技术和稳健伪标签重加权的方法为SFDA领域设定了新的标杆。相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24216" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 01:02:42 GMT</pubDate>
</item>
<item>
<title>达尔文Gödel机器：一种自我进化的AI系统</title>
<link>https://arxiv.org/abs/2505.22954</link>
<guid>https://arxiv.org/abs/2505.22954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">达尔文Gödel机器通过迭代改进自身代码实现自主进化。</p><br /><br /><p><strong>摘要：</strong> 当前的人工智能系统依赖固定架构且无法自主持续优化，而达尔文Gödel机器（DGM）作为一种自我进化的AI系统，通过迭代修改自身代码并验证改进效果，在编码能力上显著提升。DGM受达尔文进化论启发，维护一个代码代理档案，并通过采样和基于基础模型的创新生成新版本，形成多样化高质量的搜索空间探索树。实验表明，DGM不仅提升了SWE-bench和Polyglot的表现，还在安全措施下展现出超越传统基线的能力，标志着迈向自主人工智能的重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 20:26:15 GMT</pubDate>
</item>
<item>
<title>基于流匹配的双耳语音合成框架BinauralFlow</title>
<link>https://arxiv.org/abs/2505.22865</link>
<guid>https://arxiv.org/abs/2505.22865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合流匹配和因果U-Net的双耳语音合成方法。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于双耳音频渲染问题，针对现有方法在渲染质量和实时推断上的不足，提出了一种名为BinauralFlow的新框架。该框架将双耳渲染视为生成问题而非回归问题，设计条件流匹配模型提升音频质量，并通过因果U-Net架构实现流式推断。此外，引入连续推理管道优化渲染连续性和速度。实验表明，BinauralFlow在客观评估和主观感知测试中均优于现有技术，其生成的音频接近真实录音效果，混淆率为42%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 16:59:15 GMT</pubDate>
</item>
<item>
<title>Plan-and-Budget：提升大语言模型推理效率的框架</title>
<link>https://arxiv.org/abs/2505.16122</link>
<guid>https://arxiv.org/abs/2505.16122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Plan-and-Budget框架解决大语言模型过思考和计算低效问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在复杂推理任务中的显著成功与其推理过程的计算低效之间的矛盾。通过分析发现，许多流行的大模型存在过度推理的问题，即对简单查询生成冗长且偏离主题的推理轨迹。尽管已有工作尝试通过固定令牌预算来缓解这一问题，但这可能导致欠思考，尤其是在更难的问题上。为此，我们开发了一个理论模型BBAM（贝叶斯预算分配模型），将推理建模为具有不同不确定性的子问题序列，并引入E³指标来捕捉正确性和计算效率之间的权衡。基于BBAM的理论成果，我们提出了Plan-and-Budget，这是一种模型不可知的测试时间框架，它将复杂的查询分解为子问题，并根据估计的复杂性使用自适应调度分配令牌预算。Plan-and-Budget在各种任务和模型上提高了推理效率，在某些情况下实现了高达+70%的准确性提升、-39%的令牌减少以及+187.5%的E³改进。特别值得注意的是，它使较小的模型（DS-Qwen-32B）的表现接近更大的模型（DS-LLaMA-70B），展示了Plan-and-Budget缩小性能差距的能力，而无需重新训练。我们的代码可在anonymous.4open.science/r/P-and-B-6513/获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 21:56:29 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型推理机制研究：基于视觉矛盾数据集的分析</title>
<link>https://arxiv.org/abs/2505.17127</link>
<guid>https://arxiv.org/abs/2505.17127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多模态模型推理时如何权衡世界知识与视觉信息。</p><br /><br /><p><strong>摘要：</strong> 本文探讨多模态大型语言模型（MLLMs）在视觉问答等任务中的推理机制，是否依赖记忆的知识还是输入图像中的视觉信息。为此，我们引入了一个名为Visual CounterFact的新数据集，其中包含现实感强的反事实样本，使世界知识先验（如红色草莓）与视觉输入（如蓝色草莓）产生冲突。实验表明，模型预测最初反映记忆的先验，但在中间到晚期层逐渐转向视觉证据，揭示了两种模态之间的竞争，最终视觉输入会覆盖先验。为控制这种行为，我们提出了Pixels Versus Priors (PvP)引导向量，通过激活层面干预来控制模型输出倾向于世界知识还是视觉输入。平均而言，PvP成功将92.5%的颜色预测和74.6%的大小预测从先验转移到反事实。这些发现为解释和控制多模态模型的事实行为提供了新工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:56:55 GMT</pubDate>
</item>
<item>
<title>WebChoreArena：衡量大型语言模型处理复杂网络任务的能力</title>
<link>https://arxiv.org/abs/2506.01952</link>
<guid>https://arxiv.org/abs/2506.01952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出WebChoreArena基准测试平台，评估LLMs在复杂网络任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为WebChoreArena的新基准测试平台，该平台由532项精心策划的任务组成，旨在扩展WebArena的功能，涵盖更多劳动密集型和繁琐的任务。WebChoreArena集成了三大挑战：大量记忆需求任务、精确数学推理任务和长期记忆任务。基于可完全重现的四个WebArena仿真环境构建，它保证了严格的可重复性并支持与现有基准进行公平直接比较。实验结果显示，随着GPT-4o、Claude 3.7 Sonnet和Gemini 2.5 Pro等LLMs的发展，WebChoreArena上的性能显著提升。然而，即使使用最先进的Gemini 2.5 Pro，与WebArena相比仍有较大改进空间，表明WebChoreArena更能体现LLMs的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>融合自回归与掩码扩散模型的Eso-LMs提升语言建模效率</title>
<link>https://arxiv.org/abs/2506.01928</link>
<guid>https://arxiv.org/abs/2506.01928</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eso-LMs通过融合AR与MDM实现高效并行生成且提升推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的模型家族Eso-LMs，它结合了自回归（AR）模型和平行生成的掩码扩散模型（MDM），实现了两者困惑度的平滑插值，同时克服了各自的局限性。实验表明，Eso-LMs在标准语言建模基准上达到了新的性能高度。更重要的是，我们首次为MDM引入了KV缓存技术，同时保持了并行生成的能力，显著提升了推理效率。结合优化后的采样调度，我们的方法比标准MDMs快达65倍，比之前的半自回归方法快4倍。此外，项目代码和模型检查点已在项目页面公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01928" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:47:27 GMT</pubDate>
</item>
<item>
<title>阿拉伯语语言模型评估的理论指南与新框架</title>
<link>https://arxiv.org/abs/2506.01920</link>
<guid>https://arxiv.org/abs/2506.01920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的阿拉伯语语言模型评估框架，揭示现有模型在文化理解和专业知识上的不足。</p><br /><br /><p><strong>摘要：</strong> 本文通过分析现有的阿拉伯语评价数据集，发现其在语言准确性、文化适应性和方法严谨性方面的重大问题。为解决这些问题，我们提出了阿拉伯深度微型数据集(ADMD)，包含十个主要领域的490个挑战性问题。通过ADMD对五个领先语言模型进行评估，结果显示不同领域模型表现差异显著，特别是在需要深厚文化理解和专业知识的领域。Claude 3.5 Sonnet在整体上表现出最高的准确率30%，尤其在数学理论、阿拉伯语和伊斯兰教相关领域表现突出。本研究为改进阿拉伯语语言模型的评估提供了理论基础和实用见解，强调了文化胜任力和技术能力同等重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:39:50 GMT</pubDate>
</item>
<item>
<title>探索压缩表示中的规模定律：统一预测模型性能</title>
<link>https://arxiv.org/abs/2506.01863</link>
<guid>https://arxiv.org/abs/2506.01863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究压缩格式对大规模机器学习模型性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了规模定律与压缩格式之间的相互作用，验证了一种通用的规模定律公式，并展示了其在不同压缩类型中的适用性。研究表明，基于表示能力的“容量”指标可以可靠地预测多种压缩表示下的参数效率。此外，我们还扩展了该公式，用于比较不同压缩格式的精度潜力并优化稀疏量化格式的训练算法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:52:51 GMT</pubDate>
</item>
<item>
<title>基于组相对策略优化的多模态自反思增强推理方法</title>
<link>https://arxiv.org/abs/2506.01713</link>
<guid>https://arxiv.org/abs/2506.01713</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段强化学习框架SRPO，显著提升多模态大模型的推理与反思能力。</p><br /><br /><p><strong>摘要：</strong> 现有研究显示，多模态大型语言模型（MLLMs）在推理任务中虽有潜力，但难以应对需要明确自我反思和修正的复杂问题。针对这一挑战，本文提出了一种名为Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO)的两阶段强化学习框架，旨在通过引入高质量的反思数据集及创新奖励机制，显著提高多模态大模型的推理与反思质量。实验结果表明，在MathVista、MathVision等多模态推理基准测试中，SRPO相较于当前最先进的模型在推理准确性和反思质量上均有显著提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01713" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 10:21:44 GMT</pubDate>
</item>
<item>
<title>基于多模态去噪扩散模型的量子运算高效编译方法</title>
<link>https://arxiv.org/abs/2506.01666</link>
<guid>https://arxiv.org/abs/2506.01666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合离散门选择与连续参数预测的量子电路生成新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为多模态去噪扩散模型的方法，用于同时生成量子电路的结构及其连续参数，从而实现目标酉矩阵的编译。该模型通过两个独立的扩散过程分别处理离散门选择和参数预测。实验表明，该方法在不同量子比特数量、电路深度及可调门比例下均表现出较高的准确性。此外，利用其快速电路生成能力，我们构建了针对特定操作的大规模电路数据集，并从中提取了有价值的启发式规则，为量子电路综合研究提供了新见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:35:33 GMT</pubDate>
</item>
<item>
<title>基于LLM的自动化仇恨言论去毒化研究</title>
<link>https://arxiv.org/abs/2506.01484</link>
<guid>https://arxiv.org/abs/2506.01484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用GPT-4o-mini实现自动化仇恨言论去毒化，构建大规模平行数据集PARADEHATE。</p><br /><br /><p><strong>摘要：</strong> 随着网络上有害内容的增多，去毒化任务变得尤为重要，但由于标注成本和敏感性，高质量的去毒化数据集稀缺。本文提出了一种新的LLM循环管道，通过使用GPT-4o-mini替代人工标注员，复制并改进了ParaDetox流程，证明了LLM标注的效果可媲美人工。在此基础上，我们构建了一个名为PARADEHATE的大规模平行数据集，包含超过8000对仇恨言论及其非仇恨版本的文本对。实验表明，在PARADEHATE上微调的BART模型在风格准确性、内容保留和流畅度方面表现优异，验证了LLM生成的去毒化文本作为规模化替代人工标注的有效性。这项工作为仇恨言论的自动化处理提供了新思路和基准数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 05:45:05 GMT</pubDate>
</item>
<item>
<title>个性化场景认知对齐的视觉语言模型评估基准与框架</title>
<link>https://arxiv.org/abs/2506.00930</link>
<guid>https://arxiv.org/abs/2506.00930</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出个性化场景认知对齐框架PCogAlign及基准PCogAlignBench。</p><br /><br /><p><strong>摘要：</strong> 随着愿景-语言模型(VLMs)在人类视觉任务中的广泛应用，如何使这些模型满足多样化用户的需求成为亟待解决的问题。本文通过社会学中的角色集(Role-Set)概念简化个体特征描述，并构建了一个包含18k实例和20个具有不同角色集的个体的基准数据集PCogAlignBench。此外，我们还提出了一个名为PCogAlign的框架，该框架基于认知意识和行动导向的奖励模型实现个性化对齐。实验结果和人工评估证明了PCogAlignBench的可靠性和PCogAlign的有效性。我们的工作将开源构建的数据集和代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00930" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 05:50:54 GMT</pubDate>
</item>
<item>
<title>LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2506.00772</link>
<guid>https://arxiv.org/abs/2506.00772</guid>
<content:encoded><![CDATA[
Recent studies have shown that supervised fine-tuning of LLMs on a small number of high-quality datasets can yield strong reasoning capabilities. However, full fine-tuning (Full FT), while powerful, is computationally expensive and susceptible to overfitting and catastrophic forgetting, particularly when data is limited. Sparse fine-tuning, which previously achieved notable success by updating only a small subset of model parameters, offers a promising trade-off between efficiency and effectiveness. Yet, it has lagged behind in the LLM era due to the difficulty of identifying parameters truly critical for reasoning. In this work, we state that weights with the largest magnitude after low-rank approximation are critical weights for fine-tuning, which we call Principal Weights. Surprisingly, while magnitude-based sparse fine-tuning performs poorly as a baseline on LLM fine-tuning, it becomes highly effective after rank reduction. These insights motivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only updates the top 5% Principal Weights throughout training and consistently achieves better performance on reasoning tasks than Full FT, while maintaining memory efficiency on par with popular parameter-efficient fine-tuning methods. In addition to strong performance on target domains such as arithmetic reasoning, LIFT also retains up to 20% more source-domain knowledge, compared to Full FT and LoRA. Our code is available at: https://github.com/zihanghliu/LIFT.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 21:31:50 GMT</pubDate>
</item>
<item>
<title>大型语言模型在预测任务中的表现评估挑战</title>
<link>https://arxiv.org/abs/2506.00723</link>
<guid>https://arxiv.org/abs/2506.00723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在预测任务中的性能评估问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）被广泛应用于预测任务，一些研究声称其表现可媲美甚至超越人类。然而，我们指出，作为研究社区，我们需要谨慎对待这些结论，因为评估LLM预测器存在独特挑战。首先，由于多种时间泄漏形式的存在，难以信任评价结果；其次，在实际应用中，从评估表现到真实世界预测的外推也存在困难。通过系统分析和引用先前工作的具体例子，我们展示了评估中的缺陷如何引发对当前及未来性能声明的担忧。因此，我们主张需要更严格的评估方法，以自信地评估LLMs的预测能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 17:49:17 GMT</pubDate>
</item>
<item>
<title>CityLens：评估大语言-视觉模型预测城市社会经济指标的能力</title>
<link>https://arxiv.org/abs/2506.00530</link>
<guid>https://arxiv.org/abs/2506.00530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CityLens基准测试评估大语言-视觉模型在预测城市社会经济指标方面的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CityLens的综合基准测试，用于评估大型语言-视觉模型（LLVMs）从卫星图像和街景图像预测城市社会经济指标的能力。研究团队构建了一个包含17个全球分布城市的多模态数据集，涵盖经济、教育、犯罪、交通、健康和环境六大领域。基于此数据集，定义了11项预测任务，并采用三种评估范式：直接度量预测、归一化度量估计和基于特征的回归。研究对17个最先进的LLVM进行了基准测试，结果显示尽管这些模型展示了良好的感知和推理能力，但在预测城市社会经济指标方面仍存在局限性。CityLens提供了一个统一框架，可用于诊断这些局限性并指导未来利用LLVMs理解和预测城市社会经济模式的努力。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00530" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 08:25:33 GMT</pubDate>
</item>
<item>
<title>SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation</title>
<link>https://arxiv.org/abs/2506.00523</link>
<guid>https://arxiv.org/abs/2506.00523</guid>
<content:encoded><![CDATA[
The Distribution Matching Distillation (DMD) has been successfully applied to text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However, vanilla DMD suffers from convergence difficulties on large-scale flow-based text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze the issues when applying vanilla DMD on large-scale models. Then, to overcome the scalability challenge, we propose implicit distribution alignment (IDA) to regularize the distance between the generator and fake distribution. Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep importance distribution from the teacher model. With IDA alone, DMD converges for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1 dev. Along with other improvements such as scaled up discriminator models, our final model, dubbed SenseFlow, achieves superior performance in distillation for both diffusion based text-to-image models such as SDXL, and flow-matching models such as SD 3.5 Large and FLUX. The source code will be avaliable at https://github.com/XingtongGe/SenseFlow.
]]></content:encoded>
<pubDate>Sat, 31 May 2025 07:59:02 GMT</pubDate>
</item>
<item>
<title>对抗性攻击对机器生成文本检测器性能的影响研究</title>
<link>https://arxiv.org/abs/2505.24523</link>
<guid>https://arxiv.org/abs/2505.24523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有检测器易受对抗样本影响，检测性能显著下降。</p><br /><br /><p><strong>摘要：</strong> 近期生成式人工智能和大型语言模型的进步催生了高度逼真的合成内容，但这也引发了关于恶意用途（如虚假信息传播）的担忧。尽管如此，由于缺乏评估真实场景泛化能力的稳健基准，检测机器生成文本依然面临挑战。本研究提出了一套测试方法，针对当前最先进的机器生成文本检测工具（如Mage、Radar、LLM-DetectAIve），评估其在面对基于语言学的对抗性攻击时的表现。我们通过直接偏好优化微调语言模型，使生成文本更接近人类书写风格，从而揭示检测器对特定语言特征的依赖。实验结果显示，即使少量对抗样本即可大幅降低检测器的准确性。这一发现强调了提升检测技术鲁棒性的紧迫性，尤其是应对未知领域的文本检测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:33:30 GMT</pubDate>
</item>
<item>
<title>ComposeAnything：无需重新训练的文本到图像复合生成框架</title>
<link>https://arxiv.org/abs/2505.24086</link>
<guid>https://arxiv.org/abs/2505.24086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ComposeAnything，提升复杂物体布局的文本到图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 当前文本到图像(T2I)模型在生成涉及复杂及新颖物体排列的图像时面临重大挑战。尽管先前基于布局的方法通过二维布局的空间约束改进了物体排列，但它们往往难以捕捉三维位置并牺牲了图像质量和连贯性。本研究引入ComposeAnything，这是一种无需重新训练现有T2I模型的新框架。该方法首先利用大型语言模型(LLMs)的链式思维推理能力从文本生成2.5D语义布局，其中包括带有深度信息的二维物体边界框和详细描述。基于此布局，生成具有空间和深度感知的粗略复合物体，作为强且可解释的先验，取代扩散型T2I模型中的随机噪声初始化。这一先验通过对象先验增强和空间控制去噪引导去噪过程，实现复合物体和背景的无缝生成，同时允许对不准确的先验进行优化。ComposeAnything在T2I-CompBench和NSR-1K基准测试中超越了最先进的方法，特别是在具有二维/三维空间排列、高物体数量和超现实组合的提示下。此外，人类评估显示我们的模型生成的高质量图像忠实反映了文本内容。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 20:13:36 GMT</pubDate>
</item>
<item>
<title>OmniResponse：一种多模态大语言模型用于在线对话反馈生成</title>
<link>https://arxiv.org/abs/2505.21724</link>
<guid>https://arxiv.org/abs/2505.21724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniResponse模型解决在线多模态对话反馈生成中的同步问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Online Multimodal Conversational Response Generation (OMCRG) 的新任务，旨在根据发言人的多模态输入实时生成同步的口头及非口头反馈。为了解决音频与面部反馈之间的同步挑战，我们创新性地引入文本作为中间模态进行桥梁连接，并提出了OmniResponse，这是一种利用预训练语言模型增强的多模态大语言模型（MLLM），它能够自回归地生成高质量的多模态听众反馈。OmniResponse通过引入Chrono-Text和TempoVoice两个新组件来提高生成质量，其中Chrono-Text用于时间锚定生成的文本标记，而TempoVoice则是一个可控的在线TTS模块，用于生成与面部表情同步的声音。为了支持进一步的研究，我们还发布了ResponseNet数据集，该数据集包含696组高质量的双向交互视频、多通道音频、转录文本和面部行为标注。在ResponseNet上的综合评估表明，OmniResponse在语义语音内容、音视频同步和生成质量方面显著优于基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 16:12:46 GMT</pubDate>
</item>
<item>
<title>R1-Code-Interpreter：通过代码生成提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.21668</link>
<guid>https://arxiv.org/abs/2505.21668</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出R1-Code-Interpreter模型，显著提高大语言模型在复杂推理任务中的准确性。</p><br /><br /><p><strong>摘要：</strong> 尽管大规模语言模型（LLMs）在推理和规划方面取得了进展，但在需要精确计算、符号操作、优化及算法推理的任务上仍表现不足。本文介绍R1-Code-Interpreter，这是一种文本模型扩展，通过多轮监督微调（SFT）和强化学习（RL）训练，使模型能够自主生成多个代码查询以辅助推理过程。通过在144个推理和规划任务上的实验，该模型在测试集上实现了从44.0%到64.1%的准确率提升，优于GPT-4o的文本模式，并接近配备了代码解释器的GPT-4o。此外，我们还探讨了不同的训练策略和输出格式对模型性能的影响，强调了SFT阶段的关键作用。最终模型及其相关资源已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21668" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 14:47:33 GMT</pubDate>
</item>
<item>
<title>Normalized Attention Guidance (NAG)：一种高效的扩散模型负向引导机制</title>
<link>https://arxiv.org/abs/2505.21179</link>
<guid>https://arxiv.org/abs/2505.21179</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型负向引导方法NAG，解决扩散模型在少步采样中的负向指导失效问题。</p><br /><br /><p><strong>摘要：</strong> 负向引导（抑制不想要的属性）一直是扩散模型的一个基本挑战，尤其是在少步采样设置下。虽然Classifier-Free Guidance (CFG) 在标准条件下表现良好，但在激进的采样步数压缩时会因正负分支预测分歧而失效。本文介绍了一种名为Normalized Attention Guidance (NAG) 的高效、无需训练的机制，它通过注意力空间中的外推、L1范数归一化和细化来实现负向引导。NAG不仅在CFG崩溃的地方恢复了有效的负向引导，还保持了保真度，并且可以跨架构（如UNet、DiT）、采样方式（少步、多步）以及模态（图像、视频）通用使用，具有极低的计算开销。通过广泛的实验，我们展示了NAG在文本对齐(CLIP分数)、保真度(FID、PFID) 和人类感知质量(ImageReward) 方面的一致改进。消融研究验证了每个设计组件的有效性，用户研究也确认了人们对NAG引导输出的显著偏好。作为一种模型不可知的推理时间方法，NAG无需重新训练即可为所有现代扩散框架提供轻松的负向引导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21179" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:30:46 GMT</pubDate>
</item>
<item>
<title>MaskSearch：通过预训练提升大语言模型的通用搜索能力</title>
<link>https://arxiv.org/abs/2505.20285</link>
<guid>https://arxiv.org/abs/2505.20285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的预训练框架MaskSearch，增强大语言模型的检索和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MaskSearch的新预训练框架，该框架旨在通过引入检索增强的掩码预测（RAMP）任务，使大型语言模型（LLMs）具备更强的通用搜索能力。在预训练阶段，模型学习如何利用检索工具填补大量预训练数据中的掩码部分，从而获得检索和推理能力。随后，模型通过监督微调（SFT）和强化学习（RL）进行下游任务训练。实验表明，MaskSearch显著提升了基于LLMs的搜索代理在域内和域外下游任务上的性能，特别是在开放领域多跳问答任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:50 GMT</pubDate>
</item>
<item>
<title>Frankentexts：LLMs生成的一种新型叙事文本研究</title>
<link>https://arxiv.org/abs/2505.18128</link>
<guid>https://arxiv.org/abs/2505.18128</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究LLMs生成的Frankentexts，探索可控文本生成的新挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种由大型语言模型（LLMs）生成的新类型长篇叙事文本——Frankentexts，这些文本在生成过程中有高达90%的词汇需直接来自人类写作，因此对可控文本生成提出了严峻挑战。为了生成此类文本，模型需要根据提示选择并整合不同的人类写作片段，并在迭代修改时维持特定的复制比例。实验结果显示Gemini-2.5-Pro在这一任务上表现优异，其生成的Frankentexts中有81%具备连贯性且100%与提示相关，但仍有59%的生成内容被误认为是人类创作。此外，生成的文本因段落间语气突变及语法不一致而可能被人工标注者辨识。这项研究不仅探讨了构建有效检测器以应对新作者身份灰色地带的必要性，还为混合作者身份检测提供了训练数据，并作为研究人机协同写作的试验场。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18128" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:38:47 GMT</pubDate>
</item>
<item>
<title>MIKU-PAL：基于多模态自动化管道的情绪语音合成系统</title>
<link>https://arxiv.org/abs/2505.15772</link>
<guid>https://arxiv.org/abs/2505.15772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIKU-PAL系统，实现高一致性情绪语音自动标注。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MIKU-PAL的全自动化多模态管道，用于从无标注视频数据中提取高一致性的情绪语音。通过结合人脸检测与跟踪算法及多模态大语言模型（MLLM），该系统实现了接近人类水平的准确性（MELD数据集上68.5%）和极高的一致性（Fleiss Kappa评分0.93），同时显著降低了成本并提升了效率。此外，MIKU-PAL能够对多达26种细粒度的语音情感类别进行标注，并通过人工验证确认其合理性达到83%。基于此系统，我们还发布了MIKU-EmoBench数据集（131.2小时），作为情感文本转语音及视觉声音克隆的新基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:23:12 GMT</pubDate>
</item>
<item>
<title>SmolVLA：高效社区驱动的视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2506.01844</link>
<guid>https://arxiv.org/abs/2506.01844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种小型高效的视觉-语言-动作模型SmolVLA，大幅降低训练和推理成本。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言-动作（VLA）模型通常规模庞大，参数量可达数十亿，导致高昂的训练成本且难以实际部署。这些模型主要依赖学术和工业界的数据集，而忽视了社区收集的廉价机器人平台数据。本文介绍了一种名为SmolVLA的小型、高效且社区驱动的VLA模型，该模型可以在单一GPU上进行训练，并能在消费级GPU甚至CPU上部署，同时保持竞争力的表现。为了提高响应速度，我们还引入了一个异步推理堆栈，将感知和动作预测与执行分离，实现了更高的控制速率和分块动作生成。尽管SmolVLA模型体积较小，但其性能与大10倍的传统VLA模型相当。我们在多种模拟和真实世界机器人基准上评估了SmolVLA，并发布了所有代码、预训练模型和训练数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:30:19 GMT</pubDate>
</item>
<item>
<title>EarthMind：面向多粒度多传感器地球观测数据的理解框架</title>
<link>https://arxiv.org/abs/2506.01667</link>
<guid>https://arxiv.org/abs/2506.01667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EarthMind框架，结合空间注意力提示和跨模态融合，提升大规模视觉语言模型对地球观测数据的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EarthMind的新颖视觉语言框架，旨在解决大规模多模态模型（LMMs）在处理地球观测（EO）数据时面临的挑战。EarthMind通过引入空间注意力提示（SAP）增强像素级理解，并利用跨模态融合将异构模态对齐到共享空间，从而实现高效的信息融合。为了评估多传感器融合的效果，我们构建了EarthMind-Bench基准数据集，包含超过2000个人工标注的多传感器图像问题对。实验结果显示，EarthMind在EarthMind-Bench上达到最先进的性能，且在多个公开的地球观测基准测试中表现优异，证明了其在统一框架下应对多粒度和多传感器挑战的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 09:36:05 GMT</pubDate>
</item>
<item>
<title>zip2zip：通过动态词汇表优化大语言模型推理效率</title>
<link>https://arxiv.org/abs/2506.01084</link>
<guid>https://arxiv.org/abs/2506.01084</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出zip2zip框架，使大语言模型在推理时动态调整词汇表，减少生成标记数并加速推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为zip2zip的新框架，旨在提升大型语言模型（LLMs）的推理效率。传统静态分词器由于固定词汇表无法很好地适应特定领域或语言的输入，导致生成较长的标记序列及更高的计算成本。zip2zip由三个关键部分组成：基于Lempel-Ziv-Welch（LZW）压缩算法的增量式分词器，用于实时生成可重用的“超标记”；运行时计算新生成超标记嵌入的嵌入层；以及一种因果语言建模变体，训练模型处理经过超标记化的压缩序列。通过参数高效的微调，现有LLM可在10个GPU小时内完成zip2zip改造。实验表明，改造后的zip2zip LLM在推理时有效学习使用超标记，将输入和输出序列长度减少了20%-60%，显著降低了推理延迟。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01084" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 13:03:02 GMT</pubDate>
</item>
<item>
<title>基于渐进视图范式的文本引导3D编辑方法</title>
<link>https://arxiv.org/abs/2506.00512</link>
<guid>https://arxiv.org/abs/2506.00512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Pro3D-Editor，通过渐进视图传播实现更一致的3D编辑。</p><br /><br /><p><strong>摘要：</strong> 文本引导的3D编辑技术在游戏和影视制作等领域有广泛应用潜力，但现有方法因忽视多视角间的依赖关系导致编辑不一致。本文提出一种新的渐进视图编辑范式，通过主视图采样、关键视图渲染及全视图优化三个模块，有效提升编辑精度和空间一致性。实验表明，该方法显著优于现有技术。关键词：3D编辑、渐进视图、多视角一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 07:11:55 GMT</pubDate>
</item>
<item>
<title>大规模多语言连续预训练中的平行数据研究</title>
<link>https://arxiv.org/abs/2506.00469</link>
<guid>https://arxiv.org/abs/2506.00469</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究平行数据对Llama3家族模型多语言适应的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模多语言连续预训练中的一个重要设计决策——平行数据的引入。研究重点在于双语翻译数据对Llama3家族模型（如EMMA-500）在500种语言上的多语言适应效果。为此，构建了一个包含超过2500种语言对的MaLA双语文本数据集，并开发了四种基于Llama3基础模型的多语言模型，通过混合数据进行连续预训练，总训练量达6710亿tokens。评估结果显示，双语数据显著提升了低资源语言的迁移能力和性能表现。所有相关数据集、模型及代码均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00469" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 04:37:17 GMT</pubDate>
</item>
<item>
<title>蒸馏模型对抗性偏见注入漏洞及传播机制研究</title>
<link>https://arxiv.org/abs/2505.24842</link>
<guid>https://arxiv.org/abs/2505.24842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示蒸馏模型易受训练阶段对抗性偏见注入影响且传播加剧。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了蒸馏语言模型在训练过程中因轻微数据投毒而引入偏见的脆弱性。通过两种传播模式——目标性和非目标性传播，实验表明即使仅使用0.25%的污染数据，学生模型也会在目标场景下生成偏见响应的概率高达76.9%，超过教师模型的69.4%。非目标传播中，学生模型在未见过的任务上显示出6到29倍更高的偏见频率。研究覆盖六类偏见类型及多种蒸馏方法和模态，验证了当前防御措施如困惑度过滤、偏见检测系统和大语言模型自动评分框架的不足，揭示了蒸馏模型显著的安全隐患，并提出了针对性防护设计原则。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:41:58 GMT</pubDate>
</item>
<item>
<title>AReaL：一种用于大规模语言模型强化学习的全异步系统</title>
<link>https://arxiv.org/abs/2505.24298</link>
<guid>https://arxiv.org/abs/2505.24298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种全异步的强化学习系统AReaL，大幅提升了GPU利用率并加快训练速度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AReaL的全新强化学习系统，专门针对大规模语言模型的推理任务设计。传统的同步强化学习方法由于需要等待批次中最长生成完成而造成GPU资源浪费，而AReaL通过完全解耦生成和训练过程实现了全异步操作，从而显著提高了GPU利用率。此外，AReaL还引入了一系列系统级优化措施，例如动态平衡生成和训练的工作负载以控制数据陈旧度，并采用增强型PPO算法处理过时样本。实验表明，AReaL在数学和代码推理基准测试中的训练速度比最佳同步系统快2.57倍，同时最终性能持平或有所提升。该系统的代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 03:18:25 GMT</pubDate>
</item>
<item>
<title>CodeV-R1：基于强化学习带验证奖励的硬件描述语言自动生成框架</title>
<link>https://arxiv.org/abs/2505.24183</link>
<guid>https://arxiv.org/abs/2505.24183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CodeV-R1框架，解决EDA领域自然语言到Verilog代码生成的三大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CodeV-R1的强化学习带验证奖励（RLVR）框架，用于训练生成硬件描述语言（HDL）如Verilog的大型语言模型（LLMs）。在电子设计自动化（EDA）领域，将RLVR扩展到从自然语言规范自动生成Verilog代码面临缺乏自动化且精确的验证环境、高质量自然语言-代码对稀缺性以及RLVR计算成本高昂等三大挑战。为应对这些挑战，CodeV-R1首先开发了一种基于规则的测试平台生成器，可针对黄金参考进行稳健的等效性检查；其次提出了一种往返数据合成方法，将开源Verilog代码片段与LLM生成的自然语言描述配对，并通过生成的测试平台验证代码-自然语言-代码一致性，筛选出不等价的例子以获得高质量数据集；最后采用两阶段“先蒸馏后强化学习”训练管道：蒸馏用于推理能力的冷启动，随后使用自适应DAPO算法进一步优化，该算法可根据需要动态调整采样率从而降低训练成本。最终生成的模型CodeV-R1-7B在VerilogEval v2和RTLLM v1.1上的pass@1分别达到了68.6%和72.9%，比前人工作提升了12~20%，并达到甚至超过了671B参数规模的DeepSeek-R1的表现。研究团队计划公开发布此模型、训练管道及数据集，以推动EDA和LLM领域的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 23:51:06 GMT</pubDate>
</item>
<item>
<title>大型语言模型主观倾向评估：Preference, Opinion, and Belief 调查</title>
<link>https://arxiv.org/abs/2505.19621</link>
<guid>https://arxiv.org/abs/2505.19621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型的主观偏好、意见和信念，揭示其一致性下降及偏见增加的趋势。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Preference, Opinion, and Belief（POBs）的新基准，用于评估大型语言模型（LLMs）在社会、文化、伦理和个人领域的主观倾向。通过对开放源码和闭源LLMs的测试，我们衡量了可靠性、中立性和一致性等特性。此外，我们还探讨了通过推理和自我反思机制提高计算能力对这些指标的影响。尽管这些机制在其他任务中有效，但我们的结果显示其在本领域中的提升有限。进一步分析表明，较新的模型版本表现出较低的一致性并更倾向于特定观点，这凸显了一个值得关注的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:41:21 GMT</pubDate>
</item>
<item>
<title>RoboMaster：一种基于协作轨迹建模的多对象交互视频扩散模型</title>
<link>https://arxiv.org/abs/2506.01943</link>
<guid>https://arxiv.org/abs/2506.01943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboMaster框架，解决现有方法无法有效捕捉多对象交互的问题。</p><br /><br /><p><strong>摘要：</strong> 近期视频扩散模型在生成机器人决策数据方面展现了巨大潜力，但现有的轨迹导向方法主要针对单个物体运动，难以捕获复杂操作中的多物体交互。此问题源于重叠区域的多特征纠缠，导致视觉保真度下降。为解决这一局限性，我们提出了RoboMaster框架，通过协作轨迹公式化建模物体间动力学。不同于以往分解物体的方法，我们将其交互过程分解为三个子阶段：预交互、交互和后交互，分别利用主导物体（如机械臂或操作对象）的特征进行建模。此外，为了确保视频中物体的主体语义一致性，我们引入了外观和形状感知的潜在表示。在Bridge V2数据集上的实验及野外评估表明，该方法优于现有技术，达到了轨迹控制视频生成的新高度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:57:06 GMT</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:54:39 GMT</pubDate>
</item>
<item>
<title>STORM框架：任务型对话系统中的非对称信息处理与意图形成建模</title>
<link>https://arxiv.org/abs/2506.01881</link>
<guid>https://arxiv.org/abs/2506.01881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STORM框架通过UserLLM和AgentLLM协作，解决任务型对话系统中意图定义不明确的问题。</p><br /><br /><p><strong>摘要：</strong> 任务型对话系统面临用户表达看似完整却缺乏必要结构信息的问题，这源于用户需求不明晰且系统需要精确意图定义。当前基于大型语言模型的代理无法有效区分语法完整与语境触发的表达，缺乏协作意图形成框架。我们提出STORM框架，通过UserLLM（全内部访问）与AgentLLM（仅可观察行为）之间的对话动态建模，生成捕捉表达轨迹和潜在认知转变的注释语料库，从而系统分析协作理解的发展。STORM的主要贡献包括：(1)形式化对话系统中的非对称信息处理；(2)建模意图形成并追踪协作理解演化；(3)设计衡量内部认知改进及任务表现的评估指标。实验显示，在特定场景下适度不确定性（40%-60%）优于完全透明性，模型特定模式表明需重新考虑人机协作中的最优信息完整性。这些发现有助于理解非对称推理动态，并为校准不确定性的对话系统设计提供指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 13:11:10 GMT</pubDate>
</item>
<item>
<title>ShapeLLM-Omni：一种支持文本与3D资产双向交互的原生大型语言模型</title>
<link>https://arxiv.org/abs/2506.01853</link>
<guid>https://arxiv.org/abs/2506.01853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种可理解和生成3D资产的原生大型语言模型ShapeLLM-Omni。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ShapeLLM-Omni的新型原生三维大型语言模型，该模型能够处理文本与三维资产之间的双向交互任务。首先，通过训练一个基于3D矢量量化变分自编码器（VQVAE），实现了对三维物体的有效离散化表示及其高效重构。在此基础上，构建了一个大规模连续训练数据集3D-Alpaca，用于生成、理解和编辑3D内容，为后续研究提供了丰富的资源。最后，在3D-Alpaca数据集上对Qwen-2.5-vl-7B-Instruct模型进行了指令微调，进一步增强了模型的多模态能力。这项工作为扩展多模态模型的基本三维功能奠定了基础，有助于推动三维原生人工智能的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 12:40:50 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型处理复杂指令的能力</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种结合分解与强化学习的方法，有效提升大语言模型处理复杂指令的能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型（LLMs）在面对复杂指令时面临挑战，尤其是在多重约束并行、链式及分支结构组织的情况下。尽管链式思维（CoT）被普遍认为可以改善LLMs能力，但传统CoT方法由于简单的重复指令模式导致性能下降。为解决此问题，本文提出了一种系统性方法，通过激励测试时计算扩展的推理过程来增强LLMs的指令处理能力。首先，我们基于现有分类对复杂指令进行分解，并提出可重现的数据获取方法；其次，利用基于可验证规则的奖励信号进行强化学习，专门培养模型的指令跟随推理能力。通过样本对比优化CoT执行，同时采用专家行为克隆技术引导快速思维LLMs向技能型推理器转变。在七个综合基准测试中的广泛评估显示，该方法使1.5B规模的LLM取得了相当于8B规模LLM的性能，提升了11.74%。相关代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 02 Jun 2025 04:11:44 GMT</pubDate>
</item>
<item>
<title>Scaling with Gradient Grouping (SGG): 改进大规模语言模型优化的新方法</title>
<link>https://arxiv.org/abs/2506.01049</link>
<guid>https://arxiv.org/abs/2506.01049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过动态分组和特定缩放提升自适应学习率估计的优化器包装器SGG。</p><br /><br /><p><strong>摘要：</strong> 训练大规模语言模型面临参数规模大和架构异构性等挑战，现有的自适应优化器如AdamW在梯度变化处理上仍显不足，导致训练不稳定、收敛缓慢且与参数高效微调技术兼容性差。本文介绍了一种名为Scaling with Gradient Grouping (SGG) 的优化器包装器，通过动态分组和分组特定缩放改进自适应学习率估计。SGG首先将每一层中的梯度统计分为若干簇，然后对各参数应用簇特定的缩放以校准学习率，从而在维持每参数精确适应的同时施加集体群组约束。实验表明，SGG与现有优化器无缝集成，在多种大规模语言模型基准测试中表现出一致性增益和更快收敛速度，适用于不同模型大小。此外，SGG在不同批量大小和学习率下均表现稳定，成为大规模语言模型优化的稳健选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.01049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 11:30:37 GMT</pubDate>
</item>
<item>
<title>Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models</title>
<link>https://arxiv.org/abs/2506.00996</link>
<guid>https://arxiv.org/abs/2506.00996</guid>
<content:encoded><![CDATA[
Recent advances in text-to-video diffusion models have enabled high-quality video synthesis, but controllable generation remains challenging, particularly under limited data and compute. Existing fine-tuning methods for conditional generation often rely on external encoders or architectural modifications, which demand large datasets and are typically restricted to spatially aligned conditioning, limiting flexibility and scalability. In this work, we introduce Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach for adapting pretrained video diffusion models to diverse conditional generation tasks. Our key idea is to concatenate condition and target frames along the temporal axis and insert intermediate buffer frames with progressively increasing noise levels. These buffer frames enable smooth transitions, aligning the fine-tuning process with the pretrained model's temporal dynamics. TIC-FT requires no architectural changes and achieves strong performance with as few as 10-30 training samples. We validate our method across a range of tasks, including image-to-video and video-to-video generation, using large-scale base models such as CogVideoX-5B and Wan-14B. Extensive experiments show that TIC-FT outperforms existing baselines in both condition fidelity and visual quality, while remaining highly efficient in both training and inference. For additional results, visit https://kinam0252.github.io/TIC-FT/
]]></content:encoded>
<pubDate>Sun, 01 Jun 2025 08:57:43 GMT</pubDate>
</item>
<item>
<title>大型语言模型在多选题中的局限性及改进方法</title>
<link>https://arxiv.org/abs/2506.00643</link>
<guid>https://arxiv.org/abs/2506.00643</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现当前大型语言模型在识别所有正确答案时存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在单选题评估中表现良好，但在实际应用中需要识别所有正确答案的能力却未得到充分研究。本研究引入了SATA-BENCH，这是首个专注于评估LLMs在跨领域多选题上的表现的基准，包括阅读理解、法律和生物医学等领域。通过对27个开源和专有模型的测试显示，最强模型也只能达到41.8%的精确匹配率，揭示了LLMs在这方面的不足。问题主要源于选择偏差和计数偏差两个核心挑战。为解决这些问题，我们提出了Choice Funnel解码策略，该策略结合了令牌去偏和自适应阈值化，使模型在精确匹配上提高了高达29%，同时降低了超过64%的推理成本。本研究揭示了当前LLMs的根本局限性，并提出了一种新的诊断和改进多答案推理的框架。我们发布了SATA-BENCH和Choice Funnel，以促进LLMs在现实多答案应用场景中的稳健发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00643" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 13:14:21 GMT</pubDate>
</item>
<item>
<title>通过后训练技术提升大型语言模型在多智能体系统中的经济推理能力</title>
<link>https://arxiv.org/abs/2506.00577</link>
<guid>https://arxiv.org/abs/2506.00577</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示后训练技术可有效增强大型语言模型在多智能体经济推理场景中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了监督微调(SFT)和基于可验证奖励的强化学习(RLVR)是否能有效提升大型语言模型(LLMs)在多智能体系统(MAS)中的泛化能力。以经济学推理作为测试平台，研究团队开发了Recon，一款基于70亿参数的开源LLM，该模型经过精心策划的2100个高质量经济学推理问题的数据集后训练而成。实验结果显示，在经济学推理基准测试和多智能体游戏中，Recon展现出显著增强的结构化推理能力和经济理性。这些发现表明领域对齐的后训练方法在提升推理能力和模型对齐方面具有巨大潜力，同时揭示了SFT和RL在塑造模型行为中的重要作用。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00577" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 10:22:40 GMT</pubDate>
</item>
<item>
<title>ARIA：通过意图空间奖励聚合提升语言模型强化学习效能</title>
<link>https://arxiv.org/abs/2506.00539</link>
<guid>https://arxiv.org/abs/2506.00539</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ARIA方法，解决开放域语言环境中奖励稀疏问题，显著提升强化学习效果。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型(LLMs)使智能体能够通过自然语言交互执行复杂推理和决策。然而，在开放域语言行动环境（如谈判或问答游戏）中，动作空间可能被表述为令牌联合分布，导致动作空间呈指数级增长。这种情况下采样动作会导致极端的奖励稀疏性，增加奖励方差，阻碍有效强化学习(RL)。为了解决这个问题，我们提出了ARIA方法，即通过在意图空间中聚合奖励，实现高效且有效的语言智能体训练。ARIA旨在将自然语言动作从高维联合令牌分布空间投影到低维意图空间，其中语义相似的动作被聚类并分配共享奖励。这种意图感知的奖励聚合通过密集化奖励信号降低了奖励方差，促进了更好的策略优化。大量实验表明，ARIA不仅显著减少了策略梯度方差，还在四个下游任务中平均带来了9.95%的性能提升，始终优于离线和在线RL基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00539" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 08:54:49 GMT</pubDate>
</item>
<item>
<title>LoHoVLA：一种针对长时序任务的统一视觉语言动作框架</title>
<link>https://arxiv.org/abs/2506.00411</link>
<guid>https://arxiv.org/abs/2506.00411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的统一视觉语言动作框架LoHoVLA，提升长时序任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对具身代理在处理长时序任务时面临的挑战，提出了一种名为LoHoVLA的新型统一视觉语言动作框架。该框架结合了大型预训练视觉语言模型作为主干网络，用于同时生成子任务的语言描述和机器人动作预测的令牌，从而实现高效的任务分解与动作规划。此外，LoHoVLA引入了一种分层闭环控制机制，以缓解高阶规划和低阶控制中的误差问题。为了验证框架的有效性，构建了一个包含20种长时序任务及1000个专家演示样本的数据集LoHoSet。实验结果显示，LoHoVLA在Ravens模拟器中显著优于现有的分层方法和标准视觉语言动作模型，展示了统一架构在提升可泛化具身智能方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 02:01:03 GMT</pubDate>
</item>
<item>
<title>MagiCodec：一种基于Transformer的高效音频编解码器</title>
<link>https://arxiv.org/abs/2506.00385</link>
<guid>https://arxiv.org/abs/2506.00385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型单层Transformer音频编解码器，提升编码语义表达同时保持高重建质量。</p><br /><br /><p><strong>摘要：</strong> 神经音频编解码器近年来在将原始音频波形高效映射为离散标记表示方面取得了显著进展，但现有编解码器多侧重于重建质量，而忽视了对下游模型可用性的优化。针对这一瓶颈，本文提出了MagiCodec，这是一种新颖的单层、流式Transformer架构的音频编解码器。通过引入多阶段训练流程，包括高斯噪声注入和潜在正则化技术，MagiCodec旨在增强生成代码的语义表达能力，同时保持高水平的重建精度。实验表明，MagiCodec在重建质量和下游任务性能上均优于现有最先进方法，其生成的标记表现出类似自然语言的Zipf分布，提升了与基于语言模型的生成架构的兼容性。此外，我们还分析了频率域中噪声注入的效果，证明其有助于抑制高频成分并促进稳健的标记化过程。源代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00385" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 31 May 2025 00:31:02 GMT</pubDate>
</item>
<item>
<title>基于YODAS扩展的Open Whisper-style Speech Models V4</title>
<link>https://arxiv.org/abs/2506.00338</link>
<guid>https://arxiv.org/abs/2506.00338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过整合大规模网络爬取数据集YODAS提升OWSM模型性能。</p><br /><br /><p><strong>摘要：</strong> Open Whisper-style Speech Models (OWSM)项目利用学术资源开发了一系列完全开源的语音基础模型，但其训练数据量不足。本研究通过引入Creative Commons许可的YODAS数据集增强OWSM，但由于YODAS的野性数据特性（如语言标签错误和音频文本错配），带来了诸多挑战。为此，我们开发了一套可扩展的数据清洗流水线，最终获得涵盖75种语言的166,000小时语音数据集。基于此清洗后的数据训练的新版OWSM v4模型，在多语言基准测试中显著超越旧版本，甚至在多个场景中达到或超过了工业前沿模型Whisper和MMS的表现。我们将公开发布清理后的YODAS数据、预训练模型及相关脚本，均通过ESPnet工具包提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2506.00338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 21:44:44 GMT</pubDate>
</item>
<item>
<title>MiCRo：基于大规模二元偏好数据的学习框架提升个性化奖励建模</title>
<link>https://arxiv.org/abs/2505.24846</link>
<guid>https://arxiv.org/abs/2505.24846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MiCRo通过两阶段框架增强个性化奖励建模，无需显式细粒度标注。</p><br /><br /><p><strong>摘要：</strong> 奖励建模是应用强化学习从人类反馈（RLHF）构建安全基础模型的关键步骤，但在利用Bradley-Terry（BT）模型时，假设单一全局奖励函数无法捕捉人类偏好的多样性。这种简化限制了大语言模型（LLMs）支持个性化和多元对齐的能力。理论上，当人类偏好符合多样子群的混合分布时，单一BT模型存在不可减少的误差。尽管已有解决方案如多目标学习结合细粒度注释有所改善，但成本高昂且受限于预定义属性，未能充分反映人类价值观的丰富性。本文提出MiCRo，一种两阶段框架，在不依赖显式细粒度注释的情况下，通过大规模二元偏好数据提升个性化偏好学习。第一阶段引入上下文感知的混合建模方法捕获多样化的人类偏好；第二阶段整合在线路由策略，动态调整混合权重以解决歧义，实现高效可扩展的偏好适配。多项偏好数据集上的实验表明，MiCRo有效捕捉多样化的人类偏好并显著提升下游个性化性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:44:28 GMT</pubDate>
</item>
<item>
<title>Reasoning Gym：基于可验证奖励的强化学习环境库</title>
<link>https://arxiv.org/abs/2505.24760</link>
<guid>https://arxiv.org/abs/2505.24760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的推理环境库，支持生成无限复杂度训练数据。</p><br /><br /><p><strong>摘要：</strong> Reasoning Gym (RG) 是一种专为强化学习设计的推理环境库，提供超过100个跨多个领域的数据生成器和验证器，涵盖代数、算术、几何、逻辑等多个领域。与传统固定的数据集不同，RG 的创新之处在于可以通过程序化生成方式提供几乎无限的训练数据，并且可以根据需求调整复杂度。这种特性使得模型可以在不同的难度等级下进行持续评估。实验结果表明，RG 在评估和提升推理模型方面具有显著效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:20:18 GMT</pubDate>
</item>
<item>
<title>基于视频的3D几何大语言模型在场景理解中的应用</title>
<link>https://arxiv.org/abs/2505.24625</link>
<guid>https://arxiv.org/abs/2505.24625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需3D输入的视频到3D几何大语言模型，提升场景理解和空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究通过引入视频到3D几何大语言模型（VG LLM），使多模态大语言模型能够在没有全面3D数据输入的情况下直接理解3D场景。我们设计了一个3D视觉几何编码器，从视频序列中提取3D先验信息，并将其与视觉标记结合输入到多模态大语言模型中。实验表明，该方法在多个3D场景理解和空间推理任务中表现优异，尤其是在VSI-Bench评估中，我们的4B参数模型甚至超过了现有的最先进方法Gemini-1.5-Pro。这一进展显著降低了对复杂3D数据的需求，提升了模型的通用性和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 10:16:41 GMT</pubDate>
</item>
<item>
<title>统一预算感知学习率调度器UBA的研究</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种理论支持的预算感知学习率调度器UBA，优化多种架构和任务的训练表现。</p><br /><br /><p><strong>摘要：</strong> 随着计算成本的增加和资源限制，预算迭代训练成为实现最优学习的关键需求。然而，现有的学习率调度设计主要基于经验法则，缺乏理论依据，且需要大量试错，导致效率低下。本研究提出了统一预算感知（UBA）调度器，这是一种理论上成立的学习率调度方法，在不同的网络架构和任务中均优于常用调度方案。通过构建新的预算感知优化框架，UBA消除了对每种网络进行数值优化的需求，并通过理论分析建立了超参数φ与条件数之间的联系。此外，我们证明了不同φ值下的收敛性，并提供了选择φ的实用指南。实验结果显示，UBA在多种视觉和语言任务中表现出色，适用于不同规模的网络架构和训练迭代预算。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24452" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 06:38:03 GMT</pubDate>
</item>
<item>
<title>VisualSphinx：首个大规模合成视觉逻辑推理训练数据集</title>
<link>https://arxiv.org/abs/2505.23977</link>
<guid>https://arxiv.org/abs/2505.23977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模合成视觉逻辑推理数据集VisualSphinx，提升视觉语言模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉语言模型在多模态推理方面表现不足，主要由于缺乏大规模且结构良好的训练数据集。为解决这一问题，本文提出了VisualSphinx，这是一个专门针对视觉逻辑推理设计的大规模合成训练数据集。通过引入一种基于规则的图像合成管道，该方法可以从基础问题中提取并扩展出谜题规则，并生成具有定位答案的合成图像。实验表明，利用VisualSphinx训练的视觉语言模型在逻辑推理任务中表现出更高的逻辑连贯性和可读性，并且在代数推理、算术推理和几何推理等其他推理任务中也显示出显著性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 16:08:36 GMT</pubDate>
</item>
<item>
<title>Cora：一种基于语义对应的新图像编辑框架</title>
<link>https://arxiv.org/abs/2505.23907</link>
<guid>https://arxiv.org/abs/2505.23907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cora通过引入对应感知噪声校正和插值注意力图改进图像编辑效果。</p><br /><br /><p><strong>摘要：</strong> 图像编辑在计算机图形学、视觉和视觉特效领域至关重要，但涉及显著结构变化的编辑仍具挑战性。现有方法常产生纹理不相关等伪影或难以保留源图像的关键属性。本文提出Cora框架，通过语义对应实现精确的纹理转移并生成新内容，同时提供对生成与保留平衡的控制。实验表明，Cora在多种编辑任务中表现优异，用户研究进一步验证其优越性。关键词：图像编辑、语义对应、扩散模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:00:56 GMT</pubDate>
</item>
<item>
<title>Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles</title>
<link>https://arxiv.org/abs/2505.23590</link>
<guid>https://arxiv.org/abs/2505.23590</guid>
<content:encoded><![CDATA[
The application of rule-based reinforcement learning (RL) to multimodal large language models (MLLMs) introduces unique challenges and potential deviations from findings in text-only domains, particularly for perception-heavy tasks. This paper provides a comprehensive study of rule-based visual RL, using jigsaw puzzles as a structured experimental framework. Jigsaw puzzles offer inherent ground truth, adjustable difficulty, and demand complex decision-making, making them ideal for this study. Our research reveals several key findings: Firstly, we find that MLLMs, initially performing near to random guessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and generalize to complex, unseen configurations through fine-tuning. Secondly, training on jigsaw puzzles can induce generalization to other visual tasks, with effectiveness tied to specific task configurations. Thirdly, MLLMs can learn and generalize with or without explicit reasoning, though open-source models often favor direct answering. Consequently, even when trained for step-by-step reasoning, they can ignore the thinking process in deriving the final answer. Fourthly, we observe that complex reasoning patterns appear to be pre-existing rather than emergent, with their frequency increasing alongside training and task difficulty. Finally, our results demonstrate that RL exhibits more effective generalization than Supervised Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL optimization. Although these observations are based on jigsaw puzzles and may vary across other visual tasks, this research contributes a valuable piece of jigsaw to the larger puzzle of collective understanding rule-based visual RL and its potential in multimodal learning. The code is available at: https://github.com/zifuwanggg/Jigsaw-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:01:22 GMT</pubDate>
</item>
<item>
<title>VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.23504</link>
<guid>https://arxiv.org/abs/2505.23504</guid>
<content:encoded><![CDATA[
Video Anomaly Understanding (VAU) is essential for applications such as smart cities, security surveillance, and disaster alert systems, yet remains challenging due to its demand for fine-grained spatio-temporal perception and robust reasoning under ambiguity. Despite advances in anomaly detection, existing methods often lack interpretability and struggle to capture the causal and contextual aspects of abnormal events. This limitation is further compounded by the absence of comprehensive benchmarks for evaluating reasoning ability in anomaly scenarios. To address both challenges, we introduce VAU-R1, a data-efficient framework built upon Multimodal Large Language Models (MLLMs), which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT). Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored for video anomaly reasoning, featuring multiple-choice QA, detailed rationales, temporal annotations, and descriptive captions. Empirical results show that VAU-R1 significantly improves question answering accuracy, temporal grounding, and reasoning coherence across diverse contexts. Together, our method and benchmark establish a strong foundation for interpretable and reasoning-aware video anomaly understanding. Our code is available at https://github.com/GVCLab/VAU-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 10:48:10 GMT</pubDate>
</item>
<item>
<title>From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval</title>
<link>https://arxiv.org/abs/2505.23059</link>
<guid>https://arxiv.org/abs/2505.23059</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) prompting enables complex reasoning in large language models (LLMs), including applications in information retrieval (IR). However, it often leads to overthinking, where models produce excessively long and semantically redundant traces with little or no benefit. We identify two key challenges in IR: redundant trajectories that revisit similar states and misguided reasoning that diverges from user intent. To address these, we propose State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning, offering a practical alternative to conventional CoT reasoning. The code and details are available at https://github.com/ldilab/SMR.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:04:25 GMT</pubDate>
</item>
<item>
<title>DyePack：通过后门攻击检测大语言模型对基准测试集的依赖</title>
<link>https://arxiv.org/abs/2505.23001</link>
<guid>https://arxiv.org/abs/2505.23001</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DyePack框架，利用后门攻击检测训练中是否使用过基准测试集。</p><br /><br /><p><strong>摘要：</strong> 开放基准对于评估和改进大型语言模型至关重要，但其易访问性使其容易受到测试集污染的影响。本文介绍了一种名为DyePack的框架，它通过后门攻击在不访问模型损失、logits或内部细节的情况下识别出是否在训练过程中使用了基准测试集。DyePack的设计包括多个具有随机目标的后门，可精确计算误报率(FPR)，从而有效防止错误指控并提供确凿证据。我们在三个数据集上的五种模型上进行了评估，涵盖了多项选择题和开放式生成任务，结果显示DyePack在多种任务中均成功检测到受污染模型，并保证了极低的误报率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23001" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 22:22:14 GMT</pubDate>
</item>
<item>
<title>多模态大模型强化学习框架提升泛化能力</title>
<link>https://arxiv.org/abs/2505.24871</link>
<guid>https://arxiv.org/abs/2505.24871</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态语言模型通过强化学习实现跨领域优化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种针对多模态大型语言模型（MLLMs）的强化学习框架，名为RLVR，通过整合多种具有可验证答案的视觉语言任务数据集进行后训练。该框架解决了多数据集训练过程中目标冲突的问题，提出了优化的数据混合策略。实验表明，采用这种策略的多领域强化学习显著提升了模型的泛化能力和推理性能，在分布外基准测试中的准确率平均提高了5.24%，相较均匀数据混合的模型提升了20.74%。此外，该研究还开发了在线强化学习模块，支持不同领域的可验证奖励机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24871" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>SiLVR: A Simple Language-based Video Reasoning Framework</title>
<link>https://arxiv.org/abs/2505.24869</link>
<guid>https://arxiv.org/abs/2505.24869</guid>
<content:encoded><![CDATA[
Recent advances in test-time optimization have led to remarkable reasoning capabilities in Large Language Models (LLMs), enabling them to solve highly complex problems in math and coding. However, the reasoning capabilities of multimodal LLMs (MLLMs) still significantly lag, especially for complex video-language tasks. To address this issue, we present SiLVR, a Simple Language-based Video Reasoning framework that decomposes complex video understanding into two stages. In the first stage, SiLVR transforms raw video into language-based representations using multisensory inputs, such as short clip captions and audio/speech subtitles. In the second stage, language descriptions are fed into a powerful reasoning LLM to solve complex video-language understanding tasks. To handle long-context multisensory inputs, we use an adaptive token reduction scheme, which dynamically determines the temporal granularity with which to sample the tokens. Our simple, modular, and training-free video reasoning framework achieves the best-reported results on Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife. Furthermore, our empirical study focused on video reasoning capabilities shows that, despite not being explicitly trained on video, strong reasoning LLMs can effectively aggregate multisensory input information from video, speech, and audio for complex temporal, causal, long-context, and knowledge acquisition reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:19 GMT</pubDate>
</item>
<item>
<title>EXP-Bench：评估AI代理完成完整研究实验的能力</title>
<link>https://arxiv.org/abs/2505.24785</link>
<guid>https://arxiv.org/abs/2505.24785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EXP-Bench评估AI代理在设计、执行和分析完整研究实验中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EXP-Bench，这是一个用于评估AI代理在完整研究实验中能力的新基准。EXP-Bench从顶级AI研究论文及其开源代码中提取并结构化关键实验细节，创建了461个复杂的任务。通过测试基于大型语言模型的领先AI代理，发现其在单个实验方面（如设计或实施正确性）偶尔可达20-35%，但整体可执行实验的成功率仅为0.5%。EXP-Bench揭示了现有AI代理在科学研究自动化中的瓶颈，并为未来改进提供了方向。该基准已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:46:29 GMT</pubDate>
</item>
<item>
<title>DINO-R1：通过强化学习实现视觉基础模型的上下文推理能力</title>
<link>https://arxiv.org/abs/2505.24025</link>
<guid>https://arxiv.org/abs/2505.24025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DINO-R1，首个利用强化学习增强视觉基础模型推理能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DINO-R1，这是首个尝试通过强化学习激励视觉基础模型上下文推理能力的工作。DINO-R1引入了组相对查询优化（GRQO），这是一种专为基于查询的表示模型设计的新型强化风格训练策略，通过组归一化的对齐质量计算查询级奖励。此外，还应用了KL正则化来稳定物体分布，减少训练不稳定性。这种联合优化实现了跨查询的密集且表达性监督，同时减轻了过拟合和分布漂移问题。基于Grounding-DINO，我们训练了一系列DINO-R1家族模型，这些模型集成了视觉提示编码器和视觉引导的查询选择机制。在COCO、LVIS和ODinW上的广泛实验表明，DINO-R1显著优于监督微调基线，在开放词汇和闭集视觉提示场景中表现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 17:58:06 GMT</pubDate>
</item>
<item>
<title>OmNIGUARD：一种多语言跨模态有害提示检测方法</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmNIGUARD显著提升了多语言和跨模态有害提示的检测准确率。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的能力不断提升，其潜在的有害滥用问题引发了广泛关注。现有检测方法存在不足，尤其容易受到利用模型能力不匹配的攻击影响。为解决这一挑战，本文提出OmNIGUARD，这是一种用于检测多种语言和模态下有害提示的方法。OmNIGUARD通过识别模型内部表示中跨语言或跨模态对齐的部分，构建出一种语言无关或模态无关的分类器。实验结果显示，在多语言环境下，OmNIGUARD比最强基线提高了11.57%的分类准确率；在基于图像的提示检测中提高了20.44%；并且在音频提示检测中达到了新的最佳性能。此外，由于重用了生成过程中计算的嵌入，OmNIGUARD还具有很高的效率，大约比下一个最快的基线快120倍。代码和数据已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 01:25:27 GMT</pubDate>
</item>
<item>
<title>ReasonGen-R1：结合推理与强化学习的生成视觉模型</title>
<link>https://arxiv.org/abs/2505.24875</link>
<guid>https://arxiv.org/abs/2505.24875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReasonGen-R1框架，将推理能力引入生成视觉模型并优化图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReasonGen-R1的两阶段框架，旨在将基于文本的推理能力整合到生成视觉模型中。首先通过在新生成的书面理由数据集上进行有监督微调，使自回归图像生成器获得显式的“思考”技能；然后利用Group Relative Policy Optimization (GRPO) 对其输出进行进一步优化。为了支持模型在生成图像前通过文本进行推理，我们自动创建并发布了一个由模型生成的理由与视觉提示配对的语料库，从而实现对象布局、风格和场景构成的可控规划。GRPO算法使用预训练的视觉语言模型的奖励信号来评估整体视觉质量，并在每次更新中优化策略。在GenEval、DPG和T2I基准测试中的评估表明，ReasonGen-R1在性能上显著优于强大的基线模型和先前的最先进模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>Context is Gold to find the Gold Passage: Evaluating and Training Contextual Document Embeddings</title>
<link>https://arxiv.org/abs/2505.24782</link>
<guid>https://arxiv.org/abs/2505.24782</guid>
<content:encoded><![CDATA[
A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 12:43:28 GMT</pubDate>
</item>
<item>
<title>基于Matryoshka表征学习的阿拉伯语文本语义相似度模型</title>
<link>https://arxiv.org/abs/2505.24581</link>
<guid>https://arxiv.org/abs/2505.24581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的阿拉伯语文本嵌入模型，在STS任务中表现超越大型预训练模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GATE（General Arabic Text Embedding）的新模型，该模型在阿拉伯语文本的语义相似度（STS）任务上取得了最先进的性能。由于高质量数据集和预训练模型的缺乏，阿拉伯语的STS研究一直受到限制。GATE通过利用Matryoshka表征学习方法和基于阿拉伯语三元组数据集的混合损失训练策略，显著提升了模型在细粒度语义理解任务上的表现。实验结果显示，GATE在STS基准测试中比包括OpenAI在内的更大模型高出20-25%的性能，成功捕捉了阿拉伯语独特的语义特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 09:29:03 GMT</pubDate>
</item>
<item>
<title>小语言模型在特定领域任务中的质量优势</title>
<link>https://arxiv.org/abs/2505.24189</link>
<guid>https://arxiv.org/abs/2505.24189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，即使大语言模型成本降低，小语言模型在结构化输出任务中仍具10%的质量优势。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）如GPT-4o能够通过适当的提示处理复杂任务，而随着其令牌成本的下降，小语言模型（SLMs）在实际应用中的速度和成本优势可能不再明显。本文通过对比小语言模型微调与直接提示大型语言模型生成JSON形式低代码工作流的表现，发现微调方法在领域特定任务中平均提高了10%的质量。此外，我们进行了系统性错误分析以揭示模型局限性，进一步支持小语言模型在特定场景下的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 23:59:35 GMT</pubDate>
</item>
<item>
<title>LLM安全研究中的语言多样性分析</title>
<link>https://arxiv.org/abs/2505.24119</link>
<guid>https://arxiv.org/abs/2505.24119</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示LLM安全领域存在显著的语言差距且多聚焦英语。</p><br /><br /><p><strong>摘要：</strong> 本文通过系统回顾2020年至2024年间*ACL主要会议及研讨会近300篇论文，分析大型语言模型（LLM）安全性研究的语言多样性现状，发现该领域具有明显的英语中心倾向，对高资源非英语语言的关注度极低。研究还指出，非英语语言鲜少被单独研究，而英语安全研究的文档记录实践也存在不足。基于调查，我们提出多语言安全研究的若干建议，并提出了三个具体未来方向：安全性评估、训练数据生成和跨语言安全性泛化。本研究旨在推动构建更稳健、包容的AI安全措施，以适应全球多样化人群的需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24119" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:32:44 GMT</pubDate>
</item>
<item>
<title>基于角色的自适应奖励模型提升对话系统真实性</title>
<link>https://arxiv.org/abs/2505.23923</link>
<guid>https://arxiv.org/abs/2505.23923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新模型ChARM，显著提高角色扮演语言代理的学习效率和评价性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统奖励模型在角色扮演语言代理(RPLAs)中的可扩展性和适应性不足问题，提出了ChARM模型。该模型通过引入基于角色的自适应边距和利用大规模无标注数据的自我进化机制，解决了现有方法的瓶颈。此外，还构建了首个大规模偏好数据集RoleplayPref和专用评估基准RoleplayEval，包含1,108个角色及16,888段双语对话。实验表明，ChARM相比传统的Bradley-Terry模型在偏好排名上提升了13%，并在CharacterEval和RoleplayEval中取得了最佳性能。相关代码和数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:15:18 GMT</pubDate>
</item>
<item>
<title>LEGAR BENCH与LegalSearchLM：解决法律案例检索难题</title>
<link>https://arxiv.org/abs/2505.23832</link>
<guid>https://arxiv.org/abs/2505.23832</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个大规模韩语法律案例检索基准LEGAR BENCH及新模型LegalSearchLM。</p><br /><br /><p><strong>摘要：</strong> Legal Case Retrieval (LCR) 是法律专业人士研究和决策的基础任务，但现有研究存在小规模语料库和有限表示能力的问题。为此，我们推出了LEGAR BENCH，这是首个涵盖120万案件、411种犯罪类型的韩语法律案例检索基准。同时，开发了LegalSearchLM模型，通过法律元素推理和约束解码生成目标案件内容，显著提升了检索性能，在LEGAR BENCH上比基线模型高出6-20%，并展示了出色的跨领域泛化能力。实验表明，该模型不仅在性能上达到最新水平，还能有效避免无关匹配问题，为法律检索提供全新解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23832" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 05:02:41 GMT</pubDate>
</item>
<item>
<title>双向线性运算在循环神经网络中的作用及其对记忆建模的影响</title>
<link>https://arxiv.org/abs/2505.21749</link>
<guid>https://arxiv.org/abs/2505.21749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示双向线性运算如何增强循环神经网络的记忆建模能力。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了循环神经网络中隐藏单元的作用，提出它们不仅仅是被动存储器，而是积极参与计算的重要组件。通过理论分析和实证研究，我们发现双向线性操作（即隐藏单元与输入嵌入之间的乘法交互）自然地为状态跟踪任务中隐藏状态的演变提供了偏差。此外，我们展示了双向线性状态更新形成了一种自然的层次结构，其中流行的线性递归网络如Mamba位于该层次结构的最低复杂度中心。这一发现为理解隐藏单元在复杂任务中的动态行为提供了新的视角，同时推动了对循环神经网络设计的进一步探索。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 16:38:19 GMT</pubDate>
</item>
<item>
<title>基于协调扩散噪声优化框架的全身操作合成</title>
<link>https://arxiv.org/abs/2505.21437</link>
<guid>https://arxiv.org/abs/2505.21437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架解决人体与物体交互中的运动协调与精度问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究全身影操控合成问题，包括身体、手部及物体的运动协调，这是虚拟人和机器人领域的重要挑战。主要难题在于手部与身体其他部分的紧密协作需求以及对高自由度物体操控的精确性要求。为解决这些问题，我们提出了一种新的协调扩散噪声优化框架。通过三个专门的扩散模型分别处理身体、左手和右手的运动，这些模型各自训练特定的数据集以提升泛化能力。协调性自然地通过人体运动链上的梯度流动实现，使得全局身体姿势能够高度忠实于手部动作目标进行适应。此外，采用基于基点集(BPS)的统一表示法增强手-物交互的精确性，该方法将末端执行器位置编码为与物体几何相同的BPS距离，从而捕捉手部与物体部件间的细微空间关系。实验表明，我们的方法在运动质量和物理真实性方面优于现有技术，并支持多种功能如物体姿态控制、行走与操控同时进行以及仅凭手部数据生成全身动作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:11:50 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型的模态偏好研究与调控方法</title>
<link>https://arxiv.org/abs/2505.20977</link>
<guid>https://arxiv.org/abs/2505.20977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现多模态大型语言模型普遍表现出模态偏好，并提出一种无需微调的方法来控制这种偏好。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）在处理复杂多模态任务时表现优异，但其模态偏好（即倾向于优先使用某一模态的信息）尚未得到充分研究。本文构建了一个名为MC²的基准测试集，通过受控冲突场景评估模型的模态偏好。实验显示，所有被测的18种MLLMs均表现出显著的模态偏见，且这种偏好可通过外部干预改变。进一步分析表明，模态偏好可体现在模型的潜在表示中。基于此，我们提出了基于表征工程的探针与引导方法，无需额外微调或精心设计提示即可显式控制模态偏好。该方法在幻觉抑制和多模态机器翻译等下游任务上取得了令人鼓舞的改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 06:07:59 GMT</pubDate>
</item>
<item>
<title>面向形式化验证的大语言模型不确定性量化研究</title>
<link>https://arxiv.org/abs/2505.20047</link>
<guid>https://arxiv.org/abs/2505.20047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究解决大语言模型生成形式化规范时的确定性问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自动化推理的形式化规范生成方面展现出巨大潜力，但其概率性质与形式验证所需的确定性保证之间存在根本矛盾。本文系统评估五种前沿LLMs生成的形式化工件，揭示基于可满足模理论（SMT）的自动形式化对逻辑任务和事实任务准确性的影响差异（+34.8%至-44.5%），并发现传统不确定性量化技术难以识别这些错误。我们提出一种基于概率上下文无关文法（PCFG）的框架，建立细化的不确定性分类体系，并发现不确定性信号具有任务依赖性。最终，通过轻量级融合这些信号实现选择性验证，大幅减少错误率（14%-100%），同时保持较低的弃权率，将LLM驱动的形式化转变为可靠的工程学科。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:34:04 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型推理链长度对视觉接地的影响研究</title>
<link>https://arxiv.org/abs/2505.21523</link>
<guid>https://arxiv.org/abs/2505.21523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多模态数学推理中推理链越长模型越倾向于脱离图像内容而增加幻觉。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在多模态大型语言模型中，随着推理链条的增长，模型逐渐偏离图像引导内容并更多依赖语言先验的现象。通过注意力分析发现，较长的推理链条会降低对视觉输入的关注度，从而加剧幻觉问题。为了系统研究这一现象，我们提出了RH-AUC指标，用于量化模型感知准确性随推理长度的变化，同时发布了RH-Bench诊断基准，涵盖多种多模态任务，以评估推理能力与幻觉之间的权衡关系。研究结果表明，较大的模型通常能在推理和感知之间取得更好的平衡，而这种平衡更多取决于训练数据的类型和领域，而非数据总量。这些发现强调了需要综合考虑推理质量和感知保真度的评估框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 01:08:40 GMT</pubDate>
</item>
<item>
<title>引入RPEval：评估大型语言模型角色扮演能力的新基准</title>
<link>https://arxiv.org/abs/2505.13157</link>
<guid>https://arxiv.org/abs/2505.13157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RPEval基准以全面评估大型语言模型的角色扮演能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RPEval的新基准，用于评估大型语言模型（LLMs）的角色扮演能力。该基准涵盖了情感理解、决策制定、道德一致性及角色内在一致性四个关键维度。通过构建RPEval，我们旨在解决传统人工评估资源消耗大且自动化评估可能有偏见的问题。文章还提供了初步的基准测试结果，并公开了代码和数据集，供进一步研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:18:16 GMT</pubDate>
</item>
<item>
<title>Open CaptchaWorld：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出CaptchaWorld，用于评估多模态LLMs解决CAPTCHA的能力。</p><br /><br /><p><strong>摘要：</strong> CAPTCHAs长期以来阻碍了网络机器人在实际应用中的部署，而现代多模态大型语言模型（MLLMs）虽在静态感知任务中表现优异，但其处理交互性和多步推理挑战的能力尚未得到充分测试。本文介绍了一个名为Open CaptchaWorld的新基准平台，该平台通过多样化的动态CAPTCHA谜题来评估MLLM驱动代理的视觉推理和交互能力。CaptchaWorld涵盖了20种现代CAPTCHA类型，总计225个CAPTCHA，并引入了新的度量标准“CAPTCHA推理深度”，量化解决每个谜题所需的认知和动作步骤。实验表明，人类在该测试中接近满分，而最先进的MLLM代理成功率仅为40.0%，远低于人类水平的93.3%。这表明CaptchaWorld是一个重要的基准，可用于诊断当前多模态代理系统的局限性，并指导开发更强大的多模态推理系统。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Time Blindness: Why Video-Language Models Can't See What Humans Can?</title>
<link>https://arxiv.org/abs/2505.24867</link>
<guid>https://arxiv.org/abs/2505.24867</guid>
<content:encoded><![CDATA[
Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>强化学习如何扩展语言模型的推理边界</title>
<link>https://arxiv.org/abs/2505.24864</link>
<guid>https://arxiv.org/abs/2505.24864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，强化学习可揭示基模型无法触及的新推理策略。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，强化学习（RL）在引导语言模型实现可验证奖励方面具有潜力。然而，RL是否真正提升了模型的推理能力，还是仅仅放大了基模型分布中已存在的高奖励输出，仍存争议。此外，持续增加RL计算资源是否能可靠提升推理性能也未有定论。本研究通过引入持续强化学习（ProRL）方法，证明即使在广泛采样的情况下，ProRL训练也能挖掘出基模型无法触及的新推理策略。ProRL方法结合了KL散度控制、参考策略重置及多样化任务套件。实证分析表明，RL训练模型在多项pass@k评估中始终优于基模型，且在某些基模型完全失败的场景下表现优异。进一步研究表明，推理边界的改进与基模型的任务能力和训练时长密切相关，表明RL能够随着时间推移探索并填充新的解空间区域。这些发现为未来长期RL在推理领域的研究奠定了基础，并提供了新的见解。研究模型权重已公开，支持进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>ViStoryBench：故事可视化评估基准的引入</title>
<link>https://arxiv.org/abs/2505.24862</link>
<guid>https://arxiv.org/abs/2505.24862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出ViStoryBench基准，用于评估故事可视化模型性能。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的进步，故事可视化领域取得了显著进展。为了进一步提升模型在实际场景中的表现，我们引入了ViStoryBench，这是一个综合性的评估基准。该基准集成了多样化的数据集，涵盖了多种故事类型和艺术风格，通过多维度测试模型能力，包括情节类型（如喜剧、恐怖）和视觉美学（如动漫、3D渲染）。ViStoryBench精心设计，平衡叙事结构与视觉元素，包含单主角和多主角的故事，以及复杂的情节和世界构建。此外，它采用广泛的评价指标进行全面比较。这一系统化框架有助于研究人员深入分析模型优劣，推动针对性改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>大型语言模型的忠实置信校准研究</title>
<link>https://arxiv.org/abs/2505.24858</link>
<guid>https://arxiv.org/abs/2505.24858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有大型语言模型在传达不确定性时表现不佳，提出新方法MetaFaith显著提升校准效果。</p><br /><br /><p><strong>摘要：</strong> 可靠不确定性沟通对大型语言模型的信任至关重要，但这些模型常以肯定语气传达错误信息，导致用户过度依赖并削弱信任。本研究首次系统评估了多种模型、数据集及提示策略下的忠实置信校准能力，发现当前方法成效有限，标准提示仅带来微小改进，而基于事实性的校准技术甚至可能损害准确性。为此，我们开发了MetaFaith，一种受人类元认知启发的新型提示校准方法，在多个模型和任务领域显著提升了校准的忠实性，使忠实度提高最多达61%，并获得人类评估83%的胜率。这项工作填补了大型语言模型在不确定性表达上的关键空白。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:54:08 GMT</pubDate>
</item>
<item>
<title>通过强化蒸馏优化大规模语言模型推理性能</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架REDI，有效利用正负推理样本提升LLM推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，模型蒸馏技术的进步表明，来自高级推理模型的数据可以有效地将复杂推理能力转移到较小的高效学生模型中。然而，标准做法采用拒绝采样方法，丢弃了错误的推理示例——这些数据虽然有价值但通常未被充分利用。本文探讨了如何在离线环境中有效利用正负蒸馏推理轨迹以最大化大型语言模型（LLM）的推理性能。为此，我们提出了强化蒸馏（REDI），这是一种两阶段框架。第一阶段通过监督微调（SFT）学习正向轨迹；第二阶段则通过我们提出的REDI目标函数进一步优化模型，该目标函数是一种简单的无参考损失函数，在这种蒸馏上下文中优于已建立的方法如DPO和SimPO。我们的实证评估显示，REDI在数学推理任务上优于基线拒绝采样SFT或SFT结合DPO/SimPO。特别是，Qwen-REDI-1.5B模型仅使用开放可用的Open-R1数据集中的131k个正负示例进行后训练，在MATH-500（pass@1）上获得了83.1%的分数。其在各种数学推理基准测试中的表现与DeepSeek-R1-Distill-Qwen-1.5B（使用800k专有数据后训练）相当或更好，确立了在仅使用公开可用数据进行离线后训练的1.5B规模模型的新技术水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24850" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:47:17 GMT</pubDate>
</item>
<item>
<title>Harnessing Large Language Models for Scientific Novelty Detection</title>
<link>https://arxiv.org/abs/2505.24615</link>
<guid>https://arxiv.org/abs/2505.24615</guid>
<content:encoded><![CDATA[
In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 10:08:13 GMT</pubDate>
</item>
<item>
<title>利用扩散模型先验进行跨帧一致性几何估计</title>
<link>https://arxiv.org/abs/2505.24521</link>
<guid>https://arxiv.org/abs/2505.24521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过扩散模型的内在一致性实现视频全局几何属性的跨帧一致预测。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种方法，通过合理设计和微调扩散模型，有效利用视频生成模型的内在一致性，用于一致性的单目几何估计。具体而言，我们选择共享相同对应关系的全局坐标系中的几何属性作为预测目标，引入基于位置编码重用的高效条件方法，并通过联合训练多个共享相同对应关系的几何属性提升性能。实验结果显示，我们的方法在视频全局几何属性预测上表现优异，并可以直接应用于重建任务。即使仅在静态视频数据上训练，该方法也展现出对动态视频场景的潜在泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:31:59 GMT</pubDate>
</item>
<item>
<title>un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</title>
<link>https://arxiv.org/abs/2505.24517</link>
<guid>https://arxiv.org/abs/2505.24517</guid>
<content:encoded><![CDATA[
Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.
]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:29:38 GMT</pubDate>
</item>
<item>
<title>大型语言模型的近似线性分解及其语义结构解析</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型可被映射为等效线性系统。</p><br /><br /><p><strong>摘要：</strong> 本文展示了多种开源权重的大型语言模型（LLMs）的推理操作可以映射到与输入序列等效的线性系统，而无需修改模型权重或改变预测输出。通过借鉴图像扩散模型的技术，我们战略性地调整了针对下一个词预测的梯度计算，使得模型的雅可比矩阵几乎精确地再现了前向预测的线性系统。该方法适用于多种模型（如Llama 3、Gemma 3、Qwen 3等），并通过奇异值分解显示这些LLMs在极低维子空间中运行，其中许多最大的奇异向量解码出的概念与最可能的输出词相关。此外，这种方法还允许我们将每一层的操作视为近似的线性系统，并观察到语义概念的出现。尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过近乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察并揭示下一个词预测过程中的可解释语义结构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 03:08:33 GMT</pubDate>
</item>
<item>
<title>CLaSp：一种基于上下文层跳过的自推测解码策略</title>
<link>https://arxiv.org/abs/2505.24196</link>
<guid>https://arxiv.org/abs/2505.24196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaSp通过跳过验证模型的中间层实现高效解码，加速大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CLaSp的新方法，用于提升大语言模型（LLMs）的解码速度。与传统推测解码（SD）需要额外模块训练不同，CLaSp采用即插即用的方式，通过跳过验证模型的部分中间层构建压缩版草案模型，无需额外训练。该方法利用动态规划算法优化层跳过过程，根据每次验证阶段后的完整隐藏状态动态调整策略。实验表明，在LLaMA3系列模型上，CLaSp实现了1.3到1.7倍的加速，且不影响生成文本的原始分布。这项研究为加速LLMs的推理提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:15:06 GMT</pubDate>
</item>
<item>
<title>HardTests: Synthesizing High-Quality Test Cases for LLM Coding</title>
<link>https://arxiv.org/abs/2505.24098</link>
<guid>https://arxiv.org/abs/2505.24098</guid>
<content:encoded><![CDATA[
Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:00:34 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在计数任务中的偏见研究</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现最先进的视觉语言模型在计数和识别任务中表现出显著偏见。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过互联网学习大量先验知识，但这些知识可能导致它们产生错误或有偏见的答案。本研究专注于测试这些知识对视觉语言模型（VLMs）在标准视觉任务（如计数和识别）中的准确性的影响。实验结果显示，最先进的VLMs在处理涉及流行主题的任务时表现不佳，例如，在计数带有附加条纹的阿迪达斯标志条纹数量时，平均准确率仅为17.05%，涵盖动物、商标、国际象棋、棋盘游戏、视觉错觉和图案网格等多个领域。当向图像插入描述性文本时，准确率进一步下降。即使指导模型重新检查答案或依赖图像细节，计数准确性仅提高约2个百分点。这项工作揭示了VLMs的一种有趣失败模式，并提出了一种自动化框架用于检测模型偏差。相关代码和数据可在vlmsarebiased.github.io获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:47:58 GMT</pubDate>
</item>
<item>
<title>Point-MoE：实现大规模跨域3D点云理解的Mixture-of-Experts架构</title>
<link>https://arxiv.org/abs/2505.23926</link>
<guid>https://arxiv.org/abs/2505.23926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的Mixture-of-Experts架构Point-MoE，用于提升3D点云跨域理解能力。</p><br /><br /><p><strong>摘要：</strong> 尽管缩放定律已在自然语言处理和计算机视觉领域取得了显著成果，但3D点云理解尚未达到类似阶段。这一差距主要归因于3D数据集规模较小且来源多样化，导致扫描模式、采样密度及语义偏差各异。这种领域异质性严重阻碍了统一模型的大规模训练。本研究提出了Point-MoE，这是一种专门设计的Mixture-of-Experts架构，旨在实现3D感知中的大规模跨域泛化。实验表明，标准点云主干在混合域数据上表现明显下降，而Point-MoE通过简单的top-k路由策略能够自动专业化专家，即使没有领域标签。研究还证明，Point-MoE不仅优于强大的多域基线模型，而且对未见过的领域具有更好的泛化能力。这项工作强调了一种可扩展的3D理解路径：让模型自行发现多样化3D数据中的结构，而非通过手动整理或领域监督强加。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:21:47 GMT</pubDate>
</item>
<item>
<title>EmergentTTS-Eval：语音合成模型的综合评估基准</title>
<link>https://arxiv.org/abs/2505.23009</link>
<guid>https://arxiv.org/abs/2505.23009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新基准EmergentTTS-Eval，涵盖六个复杂场景以评估语音合成模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EmergentTTS-Eval的新基准，该基准扩展自EmergentTTS，专注于评估文本到语音（TTS）模型在处理微妙和语义复杂文本时的表现。它涵盖了六个具有挑战性的场景，包括情绪表达、副语言特征、外来词、句法复杂性、复杂发音以及问句处理。通过利用大型语言模型（LLMs）迭代生成测试案例，最终构建了包含1645个多样化测试案例的数据集。此外，采用模型作为裁判的方法，利用大型音频语言模型（LALM）从多个维度评估语音质量，如情感表达、韵律、语调和发音准确性。实验结果显示，这种方法不仅能揭示不同TTS系统间的细微性能差异，还与人类偏好高度相关。研究开源了评价代码和数据集，为未来的研究提供了宝贵的资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 22:36:24 GMT</pubDate>
</item>
<item>
<title>DexUMI：通过人类手部接口学习灵巧操作技能的框架</title>
<link>https://arxiv.org/abs/2505.21864</link>
<guid>https://arxiv.org/abs/2505.21864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DexUMI框架通过人类手部接口转移灵巧操作技能到机器人手上，实验成功率达86%。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DexUMI的数据收集和策略学习框架，该框架利用人类手作为自然接口，将灵巧操作技能转移到不同的机器人手上。DexUMI包含硬件和软件适应性调整，以最小化人体手与机器人手之间的差异。硬件上，通过可穿戴外骨骼桥接运动学差距，并提供直接触觉反馈；软件上，通过高保真机器人手图像修复技术解决视觉差异。实验证明，在两个不同硬件平台上的平均任务成功率达到了86%，展示了DexUMI的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>无需额外训练的音频-视觉大语言模型平衡模态理解方法</title>
<link>https://arxiv.org/abs/2505.20873</link>
<guid>https://arxiv.org/abs/2505.20873</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为Fork-Merge Decoding的方法，无需额外训练即可减少模态偏差。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过解决模态偏差问题来提升音频-视觉大语言模型（AV-LLMs）的平衡多模态理解能力，而无需进行额外训练。当前的AV-LLMs通常通过解码器联合处理音频和视频特征，虽然促进了统一的多模态理解，但可能引入模态偏差，即模型倾向于过度依赖某一模态。为了解决这一问题，我们提出了Fork-Merge Decoding（FMD），这是一种在推理阶段简单有效的策略，不需要额外的训练或架构修改。FMD首先通过早期解码层对仅音频和仅视频输入进行模态特定推理（分叉阶段），然后在剩余层合并隐藏状态以进行联合推理（合并阶段）。这种方法促进了模态贡献的平衡并利用了跨模态的互补信息。我们在两个代表性AV-LLMs（VideoLLaMA2和video-SALMONN）上使用三个基准数据集评估了该方法。实验结果显示，在专注于音频、视频和组合音频-视觉推理的任务中，性能得到了一致的改善，证明了推理时干预的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20873" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 04:22:56 GMT</pubDate>
</item>
<item>
<title>v1模型：多模态大语言模型的轻量级视觉重访扩展</title>
<link>https://arxiv.org/abs/2505.18842</link>
<guid>https://arxiv.org/abs/2505.18842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">v1模型通过点选-复制机制实现推理过程中的动态视觉访问。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为v1的轻量级扩展，用于多模态大型语言模型（MLLMs），使模型能够在推理过程中选择性地重新访问视觉信息。不同于传统MLLMs仅一次性处理视觉输入并完全依赖内部记忆，v1引入了一个简单的点选-复制机制，允许模型在整个推理过程中动态检索相关的图像区域。该机制通过最小修改增强了现有架构，基于模型不断发展的假设提供上下文访问视觉标记的能力。为了训练这种能力，我们构建了v1g数据集，包含30万个多模态推理跟踪样本及交错的视觉定位注释。实验表明，在三个多模态数学推理基准测试（MathVista、MathVision和MathVerse）上，v1相比同类基线模型表现更为出色，尤其是在需要精细视觉参考和多步推理的任务中。我们的研究结果表明，动态视觉访问是提升基于事实的多模态推理性能的一个有前景的方向。代码、模型和数据将被公开发布，以支持未来的相关研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 15:30:47 GMT</pubDate>
</item>
<item>
<title>LLMSynthor：利用大语言模型实现高保真数据合成</title>
<link>https://arxiv.org/abs/2505.14752</link>
<guid>https://arxiv.org/abs/2505.14752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLMSynthor通过引入结构感知模拟器，提升基于大语言模型的数据合成效率与统计一致性。</p><br /><br /><p><strong>摘要：</strong> 数据建模中的一个重要挑战是生成能够忠实反映现实世界分布统计特性的合成数据。传统方法依赖强参数假设或手动结构设计，在高维或异构领域表现不佳。尽管大型语言模型(LLMs)展现出作为灵活高维先验的强大潜力，但其标准采样方法存在效率低、上下文限制固定且难以保证统计对齐的问题。为解决这些问题，我们提出了LLMSynthor框架，它将LLMs转化为由分布反馈引导的结构感知模拟器。该框架利用LLM作为非参数copula模拟器来建模高阶依赖关系，并通过LLM提议采样生成接地提议分布，从而提高采样效率而不需拒绝采样。通过迭代合成循环，LLMSynthor逐步揭示并优化潜在生成结构，使真实数据和合成数据保持统计一致性。我们在隐私敏感领域的异构数据集（如电子商务、人口和移动性）上进行了控制实验和实际应用评估，结果显示LLMSynthor生成的合成数据具有高统计保真度、实用性和跨数据适应性，可广泛应用于经济学、社会科学、城市研究等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:35:38 GMT</pubDate>
</item>
<item>
<title>AlphaOne：一种用于大模型推理过程动态调控的通用框架</title>
<link>https://arxiv.org/abs/2505.24863</link>
<guid>https://arxiv.org/abs/2505.24863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AlphaOne框架，通过参数化思考阶段提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AlphaOne的新框架，旨在测试时对大规模推理模型（LRMs）的推理进程进行灵活调控。AlphaOne首先引入了“阿尔法时刻”这一概念，通过一个通用参数α来表示扩展的思考阶段。在此过程中，它通过将推理转换标记的插入建模为伯努利随机过程，动态调度缓慢推理的过渡。在“阿尔法时刻”结束后，AlphaOne通过终止符确定性地结束缓慢推理，从而促进快速推理和高效答案生成。这种方法统一并推广了现有的单调缩放方法，实现了缓慢到快速推理的灵活且密集的调控。在数学、编码和科学等多个领域的具有挑战性的基准测试中，AlphaOne展示了其卓越的推理能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:36 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的多语言文本生成框架EasyText</title>
<link>https://arxiv.org/abs/2505.24417</link>
<guid>https://arxiv.org/abs/2505.24417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DiT的多语言文本渲染框架EasyText。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EasyText的新框架，该框架基于扩散Transformer（DiT），旨在实现精确的多语言文本生成。通过结合去噪潜变量与多语言字符标记编码，EasyText利用字符位置编码和位置编码插值技术，实现了可控且精确的文本渲染。此外，构建了一个包含百万级多语言图像文本标注的大规模合成文本图像数据集及高质量的2万张标注图像数据集，用于预训练和微调。实验结果表明，EasyText在多语言文本渲染、视觉质量和布局感知的文本集成方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 05:55:39 GMT</pubDate>
</item>
<item>
<title>一种自适应知识集成框架用于增强大型语言模型</title>
<link>https://arxiv.org/abs/2505.23844</link>
<guid>https://arxiv.org/abs/2505.23844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应知识集成框架，解决传统方法内存消耗大及性能下降问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型（LLMs）改进中的挑战，如传统微调方法的局限性和集成其他专用模型时的问题，提出了一个自适应知识集成框架。该框架通过设计一个自适应选择网络，根据源模型的分数选择最相关的模型，减少了知识干扰，并采用动态加权融合策略和反馈驱动损失函数，提升了模型的稳定性和可扩展性。实验表明，该方法相比现有方法将知识干扰降低了50%，同时保持了较高的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:24:50 GMT</pubDate>
</item>
<item>
<title>GSO基准测试：评估语言模型在高性能软件开发中的能力</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出GSO基准测试，评估语言模型在优化代码性能上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为GSO的新基准测试，用于评估语言模型在开发高性能软件方面的表现。通过自动化管道生成并执行性能测试，从10个代码库的历史提交记录中提取出102个优化任务，涵盖多个领域和编程语言。实验中，代理被要求提高代码运行效率，并与专家开发者的表现进行对比。定量分析显示，领先的语言模型表现不佳，成功率不足5%，且随着推理时间扩展改善有限。定性分析揭示了主要失败模式，如对低级语言处理困难、懒惰优化策略及瓶颈定位挑战。本研究还公开了基准测试代码及相关数据，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:14:55 GMT</pubDate>
</item>
<item>
<title>Yet Another Quantization Algorithm (YAQA) 改进大语言模型后量化性能</title>
<link>https://arxiv.org/abs/2505.22988</link>
<guid>https://arxiv.org/abs/2505.22988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法YAQA，显著提升大语言模型后量化压缩效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Yet Another Quantization Algorithm (YAQA) 的新型自适应舍入算法，用于优化大规模语言模型（LLMs）的后训练量化（PTQ）。传统方法通过独立最小化立即激活误差来量化线性层，但忽略了后续层的影响，导致优化目标局部化。YAQA利用每一层线性层相对于全模型KL散度的Hessian矩阵的Kronecker分解近似值，从而更好地捕获全局影响。该算法由两部分组成：可高效计算的大规模LLMs层间Hessian的Kronecker分解近似，以及与具体量化器无关的具有理论保证的舍入方法。实验表明，在多种模型和量化器上，YAQA可将KL散度降低约30%，同时在下游任务上达到最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:53:00 GMT</pubDate>
</item>
<item>
<title>Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14599</link>
<guid>https://arxiv.org/abs/2505.14599</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:49:40 GMT</pubDate>
</item>
<item>
<title>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.23764</link>
<guid>https://arxiv.org/abs/2505.23764</guid>
<content:encoded><![CDATA[
Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在解码字谜中的能力评估</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，视觉语言模型在解码简单视觉线索时表现良好，但在抽象推理方面存在不足。</p><br /><br /><p><strong>摘要：</strong> 字谜（Rebus puzzles）是一种通过图像、空间排列和符号替代来编码语言的视觉谜题，对当前的视觉语言模型（VLMs）提出了独特的挑战。不同于传统的图像描述或问答任务，字谜的解答需要多模态抽象、符号推理以及对文化、语音及语言双关的理解。本文构建了一个由多样化的英文字谜组成的基准测试集，涵盖从简单的图画替代到依赖空间提示的复杂谜题。通过对多种VLMs的性能分析，我们发现尽管这些模型在解析简单视觉线索时表现出一定的能力，但在涉及抽象推理、横向思维以及理解视觉隐喻的任务上仍存在显著困难。这项研究揭示了现有VLMs在跨模态任务上的局限性及其未来改进的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于关键帧的音乐同步动物舞蹈视频生成框架</title>
<link>https://arxiv.org/abs/2505.23738</link>
<guid>https://arxiv.org/abs/2505.23738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于关键帧的动物舞蹈视频生成方法，结合图优化和扩散模型实现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于关键帧的框架，用于生成与音乐同步且符合编舞意识的动物舞蹈视频。该框架通过将舞蹈合成建模为图优化问题，寻找满足指定编舞模式的最佳关键帧结构，这些模式可以从参考舞蹈视频中自动估计。此外，还引入了一种镜像姿态图像生成方法，以捕捉舞蹈中的对称性。通过视频扩散模型生成中间帧，在仅提供六个输入关键帧的情况下，可以生成长达30秒的跨多种动物和音乐轨道的舞蹈视频。这一方法展示了从文本到图像提示或GPT-4生成的关键帧开始，实现高质量动物舞蹈视频的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>ZPressor：通过信息瓶颈优化提升3D Gaussian Splatting模型的多视角扩展性</title>
<link>https://arxiv.org/abs/2505.23734</link>
<guid>https://arxiv.org/abs/2505.23734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ZPressor模块，压缩多视角输入至紧凑潜在状态Z，显著提高3DGS模型的扩展能力。</p><br /><br /><p><strong>摘要：</strong> 本文分析了基于前馈的3D Gaussian Splatting (3DGS) 模型在处理多视角输入时面临的扩展性限制问题，即随着输入视图数量增加，性能下降或内存消耗过高。我们基于信息瓶颈原理设计了ZPressor，这是一种轻量级且架构无关的模块，用于高效压缩多视角输入到一个保留关键场景信息的紧凑潜在状态Z。具体来说，ZPressor通过将视图划分为锚点集和支持集，并利用交叉注意力机制，将支持视图的信息压缩到锚点视图中，从而构建压缩后的潜在状态Z。实验表明，集成ZPressor后，多个最先进的前馈3DGS模型在适度输入视图下性能得到提升，在密集视图设置下的鲁棒性也有所增强。该方法在DL3DV-10K和RealEstate10K两个大规模基准数据集上表现优异，同时在80GB GPU上实现了超过100个480P分辨率输入视图的处理能力。相关视频结果、代码及训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>ViGoRL：通过视觉引导强化学习提升模型的视觉推理能力</title>
<link>https://arxiv.org/abs/2505.23678</link>
<guid>https://arxiv.org/abs/2505.23678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViGoRL通过视觉定位强化学习提升语言模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViGoRL（Visually Grounded Reinforcement Learning）的视觉语言模型，该模型通过强化学习将每一步推理明确锚定到特定的视觉坐标上，从而实现空间推理路径的生成。ViGoRL受到人类视觉决策的启发，能够指导视觉注意力聚焦于相关区域。在一系列视觉推理基准测试中，包括SAT-2、BLINK、V*bench等任务，ViGoRL的表现显著优于传统的监督微调和缺乏显式定位机制的常规强化学习基线。特别是在需要精细探索的任务中，多轮强化学习框架结合动态缩放功能进一步提升了模型性能。此外，研究表明，显式定位不仅提高了模型在局部元素定位和视觉搜索上的表现，还增强了其他视觉行为，如区域探索和子目标设定。最后，人类评估表明，ViGoRL的视觉参考不仅空间准确，而且有助于理解模型的推理过程。这些结果表明，视觉引导的强化学习是一种强大的方法，可以赋予模型通用的视觉推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:20:26 GMT</pubDate>
</item>
<item>
<title>基于轨迹输入的统一视频运动控制框架</title>
<link>https://arxiv.org/abs/2505.22944</link>
<guid>https://arxiv.org/abs/2505.22944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种运动类型的统一视频生成控制框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种用于视频生成的统一运动控制框架，该框架通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间中，实现了相机移动、对象级平移和精细局部运动的无缝集成。与以往针对不同运动类型采用独立模块的方法不同，我们的方法通过单一框架实现了对局部形变、物体整体运动、虚拟相机动态或其组合的精确控制。实验表明，该方法在多个视频运动控制任务上表现出色，包括风格化运动效果、动态视点变化及局部运动操作，同时在可控性和视觉质量上显著优于现有方法和商业解决方案，且兼容多种先进的视频生成模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:49:18 GMT</pubDate>
</item>
<item>
<title>AIDSAFE：通过多智能体迭代推敲提升LLMs安全推理能力</title>
<link>https://arxiv.org/abs/2505.21784</link>
<guid>https://arxiv.org/abs/2505.21784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AIDSAFE方法，利用多智能体协作生成高质量的安全推理链，显著提升大模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AIDSAFE的新方法，旨在解决现有安全措施如过度拒绝和越狱漏洞的问题。AIDSAFE通过多智能体迭代推敲的方式生成嵌入安全政策的链式思维（CoT）数据集，同时引入数据精炼阶段以消除重复、冗余及误导性思维。实验表明，基于AIDSAFE生成的CoT进行监督微调可大幅提升开源大模型的安全泛化能力和越狱鲁棒性，同时保持良好的实用性和拒绝精度。此外，为了满足对齐阶段的偏好数据需求，该方法还设计了一种补充方案，通过信念增强技术创建区分选择和拒绝样本的CoT数据。最终评估显示，AIDSAFE生成的CoT在政策遵守度和推理质量方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 17:34:40 GMT</pubDate>
</item>
<item>
<title>LUNGUAGE：基于多研究纵向评估的胸部X光报告生成基准数据集</title>
<link>https://arxiv.org/abs/2505.21190</link>
<guid>https://arxiv.org/abs/2505.21190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持单次与纵向评估的胸部X光报告生成基准数据集及评价方法。</p><br /><br /><p><strong>摘要：</strong> 现有放射学报告评估方法局限于单一报告环境且依赖粗略指标，无法捕捉精细临床语义和时间依赖性。本文引入LUNGUAGE，这是一个结构化胸部X光报告生成基准数据集，支持单次报告评估和跨多个研究的患者级别纵向评估。数据集包含1,473份由专家注释的胸片报告，其中80份具有纵向注释以捕获疾病进展。此外，开发了一个两阶段框架将生成的报告转换为细粒度的结构化表示，实现纵向解释。同时，提出了LUNGUAGESCORE，一种可解释的指标，在实体、关系和属性层面比较结构化输出的同时建模患者时间线的一致性。这些贡献建立了首个针对序列放射学报告的基准数据集、结构化框架和评估指标，实证结果显示LUNGUAGESCORE有效支持结构化报告评估。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:40:00 GMT</pubDate>
</item>
<item>
<title>大型语言模型与知识图谱结合用于复杂问答任务的研究综述</title>
<link>https://arxiv.org/abs/2505.20099</link>
<guid>https://arxiv.org/abs/2505.20099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型与知识图谱结合解决复杂问答任务的方法及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在问答（QA）任务中表现出色，但由于推理能力不足、知识过时及幻觉问题，在处理复杂QA任务时面临挑战。一些研究尝试将LLMs与知识图谱（KGs）相结合，以克服这些限制。本文提出了一种新的结构化分类法，根据QA类型和KG在与LLMs集成时的角色对方法进行分类。我们系统性地回顾了相关领域的最新进展，并从优势、局限性和KG需求等方面比较分析了这些方法。此外，我们还探讨了这些方法如何应对不同类型复杂QA的主要挑战，并总结了现有研究的成果、评估指标和基准数据集，同时指出了开放的问题与未来机会。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:08:23 GMT</pubDate>
</item>
<item>
<title>系统1.5推理：高效且适应性的大语言模型推理框架</title>
<link>https://arxiv.org/abs/2505.18962</link>
<guid>https://arxiv.org/abs/2505.18962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出系统1.5推理方法，显著提高大语言模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 当前基于链式思维（CoT）的大语言模型推理面临效率低下的问题，而潜在空间推理虽提高了效率，但未能区分关键推理步骤与辅助步骤，导致计算资源利用不充分。本文提出系统1.5推理框架，通过潜在空间中的动态捷径路径，在推理步骤间动态分配计算资源。该框架包括模型深度捷径（DS）和步骤捷径（SS），前者允许非关键标记提前退出轻量级适配器分支，后者则跨解码步骤重用隐藏状态以跳过简单步骤。通过两阶段自蒸馏过程训练，系统1.5推理在GSM8K等推理任务上展现了卓越性能，推理速度提升超过20倍，平均减少92.31%的标记生成，同时保持与传统CoT微调方法相当的推理效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 23:35:49 GMT</pubDate>
</item>
<item>
<title>视觉表征压缩对细粒度特征还原的影响及基准评测</title>
<link>https://arxiv.org/abs/2505.18142</link>
<guid>https://arxiv.org/abs/2505.18142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉分词器和VAE在保存细节特征上的局限性，并提出评估文本与人脸重建性能的新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉分词器和变分自编码器（VAE）在图像压缩过程中丢失细节信息的问题，特别是在处理小尺度的文本和人脸图像时的局限性。尽管这些技术通过提供高效的图像压缩和量化表示推动了视觉生成与多模态建模的发展，但它们在减少计算负担的同时也限制了视觉生成质量的上限。为了评估这一上限，我们聚焦于文本和面部特征的重建质量，因为这些特征通常具有密集纹理、易塌陷且对人类视觉高度敏感的特点。我们收集并整理了来自现有数据集的高质量文本和人脸图像，并采用成熟的OCR和人脸识别模型进行评估，这种方法不仅准确而且轻量级，仅需2GB内存和4分钟即可完成。通过我们的基准测试，分析了不同图像分词器和VAE在各种尺度下的重建质量。结果显示，现代视觉分词器在保存细粒度特征方面仍有不足，尤其是在小尺度下表现欠佳。此外，我们将此评估框架扩展到视频领域，对多种视频分词器进行了综合分析，并证明传统指标无法准确反映人脸和文本的重建效果，而我们提出的指标则是一个有效的补充。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>REOrder：通过优化补丁顺序提升视觉Transformer性能</title>
<link>https://arxiv.org/abs/2505.23751</link>
<guid>https://arxiv.org/abs/2505.23751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示补丁排列对现代Transformer模型表现有显著影响，提出REOrder框架优化补丁顺序。</p><br /><br /><p><strong>摘要：</strong> 当前基于Transformer的视觉模型通常将图像展平为一维序列，常用固定行优先（栅格扫描）顺序。尽管完全自注意力机制具有置换等变性，但现代长序列Transformer倾向于采用破坏这种不变性的架构近似方法，导致对补丁排列敏感。本文表明，在这些设置下补丁顺序显著影响模型性能，例如列优先或希尔伯特曲线等简单替代方案可带来明显的准确性变化。受此启发，我们提出了REOrder，这是一种两阶段框架，用于发现任务最优的补丁排列。首先，通过评估各种补丁序列的压缩性得出信息论先验；然后，利用REINFORCE优化Plackett-Luce策略学习排列策略。该方法能够在组合排列空间中实现高效学习。实验证明，REOrder在ImageNet-1K上相比行优先排列提升了高达3.01%的top-1准确率，并在Functional Map of the World数据集上提高了13.35%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>基于强化学习的大语言模型代码效率优化框架</title>
<link>https://arxiv.org/abs/2505.23387</link>
<guid>https://arxiv.org/abs/2505.23387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过强化学习显著提升大语言模型代码执行效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）虽然能够生成功能正确的代码，但在代码效率方面存在不足，成为实际应用中的瓶颈。本文介绍了一种测试时迭代优化框架，利用闭环系统让LLMs根据执行沙箱反馈迭代改进代码。研究探索了三种训练策略：监督微调（SFT）、直接偏好优化（DPO）和分组相对策略优化（GRPO）。实验显示，SFT和DPO在效率提升上很快达到饱和，而采用强化学习的GRPO持续优化代码性能，在Venus数据集和APPS基准测试中分别将pass@1提升至62%并使效率优于人类提交的概率从31%提高到45%。本研究证明了测试时代码效率优化的有效性，并揭示了强化学习在指导LLMs自我提升代码效率方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:14:29 GMT</pubDate>
</item>
<item>
<title>基于语言模型解释性和不确定性量化提升机器翻译质量评估效率</title>
<link>https://arxiv.org/abs/2505.23183</link>
<guid>https://arxiv.org/abs/2505.23183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用语言模型解释性与不确定性量化提升机器翻译质量评估效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过语言模型的解释性和不确定性量化技术，以更高效的方式识别机器翻译中的错误片段，从而替代传统昂贵的质量评估方法。这些传统方法通常依赖大型语言模型的提示或大量人工标注数据的训练。研究对12种翻译方向下的14项指标进行了评估，发现人类标签变化对评估性能有显著影响。实验结果表明，无监督评估方法具有未被充分挖掘的潜力，而当面临标签不确定性时，监督方法存在不足，单一标注者评价实践也显得脆弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 03:20:36 GMT</pubDate>
</item>
<item>
<title>Re-ttention：通过利用时间冗余实现视觉生成模型的极高稀疏注意力</title>
<link>https://arxiv.org/abs/2505.22918</link>
<guid>https://arxiv.org/abs/2505.22918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Re-ttention方法，在不影响视觉质量的前提下实现极高的稀疏注意力。</p><br /><br /><p><strong>摘要：</strong> 扩散Transformer（DiT）已成为生成高质量视频和图像的主流模型，但其计算瓶颈在于注意力机制，其复杂度随分辨率和视频长度呈平方级增长。现有稀疏注意力技术在极高稀疏水平下无法保持视觉质量且可能带来显著计算开销。为解决此问题，本文提出Re-ttention，利用扩散模型的时间冗余来克服注意力机制中的概率归一化偏移，通过重塑注意力得分来维持全量二次注意力的视觉质量。实验表明，Re-ttention在推理过程中仅需3.1%的tokens，优于FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，该方法在H100 GPU上实现了超过45%的端到端延迟和超过92%的自注意力延迟减少，且开销可忽略不计。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:39:12 GMT</pubDate>
</item>
<item>
<title>When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy</title>
<link>https://arxiv.org/abs/2505.22888</link>
<guid>https://arxiv.org/abs/2505.22888</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, their ability to think in other languages is less studied. This capability is as important as answer accuracy for real world applications because users may find the reasoning trace useful for oversight only when it is expressed in their own language. We comprehensively evaluate two leading families of LRMs on our XReasoning benchmark and find that even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing a substantial gap in multilingual reasoning. Prompt based interventions that force models to reason in the users language improve readability and oversight but reduce answer accuracy, exposing an important trade off. We further show that targeted post training on just 100 examples mitigates this mismatch, though some accuracy loss remains. Our results highlight the limited multilingual reasoning capabilities of current LRMs and outline directions for future work. Code and data are available at https://github.com/Betswish/mCoT-XReasoning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 17:44:12 GMT</pubDate>
</item>
<item>
<title>CLIPGaussians：一种多模态风格迁移框架</title>
<link>https://arxiv.org/abs/2505.22854</link>
<guid>https://arxiv.org/abs/2505.22854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个支持文本和图像引导的多模态风格迁移框架。</p><br /><br /><p><strong>摘要：</strong> Gaussian Splatting (GS) 是一种高效的 3D 场景渲染方法，但其风格迁移仍具挑战性。本文介绍 CLIPGaussians，这是一种针对 2D 图像、视频、3D 对象和 4D 场景的统一风格迁移框架。该方法直接作用于高斯基元，无需大型生成模型或重新训练即可集成到现有 GS 流程中。CLIPGaussians 能实现 3D 和 4D 环境下的颜色和几何联合优化，并在视频中保持时间一致性，同时保持模型大小。实验表明，它在所有任务中表现出卓越的风格保真度和一致性，验证了其作为通用高效多模态风格迁移解决方案的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 16:41:24 GMT</pubDate>
</item>
<item>
<title>VidText：视频文本理解的新基准</title>
<link>https://arxiv.org/abs/2505.22810</link>
<guid>https://arxiv.org/abs/2505.22810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VidText新基准，评估视频文本理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视频理解基准大多忽视文本信息，而OCR特定基准仅限于静态图像，无法充分捕捉文本与动态视觉背景间的交互。为填补这一空白，我们提出了VidText，这是一个针对视频文本理解进行全面深入评估的新基准。VidText涵盖广泛的真实场景，支持多语言内容，提供多层次评估框架，并引入配对感知推理任务。实验显示当前模型在大多数任务上表现不佳，存在显著改进空间。分析表明模型内在因素（如输入分辨率、OCR能力）和外部因素（如辅助信息使用、推理策略）的影响。我们希望VidText能弥补现有基准的不足，并为未来动态环境中多模态推理研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 15:39:35 GMT</pubDate>
</item>
<item>
<title>FAMA：首个开源科学语音基础模型</title>
<link>https://arxiv.org/abs/2505.22759</link>
<guid>https://arxiv.org/abs/2505.22759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布首个开源语音基础模型FAMA，性能媲美现有模型且速度提升8倍。</p><br /><br /><p><strong>摘要：</strong> 随着像Whisper和SeamlessM4T这样的语音基础模型的发展，语音处理领域取得了显著进步，但其封闭性质限制了可复现性和公平评估。尽管其他研究领域已通过开放科学取得进展，但语音领域的类似努力仍显不足。为填补这一空白，我们推出了FAMA，这是首个面向英语和意大利语的开源科学语音基础模型家族，训练数据超过15万小时的开源语音数据。此外，我们还发布了包含1.6万小时清理后伪标签语音的新数据集。实验结果显示，FAMA在性能上可与现有模型相媲美，同时运行速度提高了8倍。所有代码、数据集和模型均以符合开源许可的方式发布，推动了语音技术研究的开放性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:19:34 GMT</pubDate>
</item>
<item>
<title>KronSAE：通过Kronecker分解提升稀疏自编码器效率</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出KronSAE架构，利用Kronecker分解减少稀疏自编码器的计算开销。</p><br /><br /><p><strong>摘要：</strong> 稀疏自编码器(SAEs)在解释语言模型隐藏状态方面表现出巨大潜力，但其训练在大规模场景下具有挑战性，尤其是当字典规模较大时。尽管解码器可以采用稀疏感知核以提高效率，但编码器仍需进行高维线性运算，带来较高的计算成本。为解决这一问题，我们提出了KronSAE，这是一种新颖的架构，通过Kronecker乘积分解来大幅降低内存和计算负担。此外，我们引入了mAND，一种近似二元AND操作的不同iable激活函数，它不仅提升了因子化框架的可解释性，还提高了性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 07:41:11 GMT</pubDate>
</item>
<item>
<title>Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting</title>
<link>https://arxiv.org/abs/2505.19716</link>
<guid>https://arxiv.org/abs/2505.19716</guid>
<content:encoded><![CDATA[
Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose a difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, a large teacher model first judges each problem's difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate a distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled a new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that a student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms a model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches 74.2% Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:04:44 GMT</pubDate>
</item>
<item>
<title>VBenchComp：用于评估视频大模型时间推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.14321</link>
<guid>https://arxiv.org/abs/2505.14321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VBenchComp，解决现有视频理解基准混淆知识和图像问题的问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频理解基准通常混淆知识型和纯图像型问题，未能明确区分模型的时间推理能力，这是视频理解区别于其他模态的关键方面。我们发现两个主要问题：强语言先验和时间不变性，导致高分未必真正反映对动态内容的理解。为解决这些问题，我们提出了VBenchComp，通过自动化管道将问题分类为LLM可回答、语义型和时间型，其余归为其他类。这种方法能够更精细地评估视频大模型的不同能力。我们的分析揭示了传统总体分数掩盖的模型弱点，并提供了对未来基准设计的见解和建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:07:55 GMT</pubDate>
</item>
<item>
<title>ZeroGUI：无需人工成本的图形用户界面自动化训练框架</title>
<link>https://arxiv.org/abs/2505.23762</link>
<guid>https://arxiv.org/abs/2505.23762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种零人工成本的在线学习框架ZeroGUI，提升图形用户界面代理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有纯视觉图形用户界面（GUI）代理训练方法依赖手工标注和适应性差的问题，提出了ZeroGUI，这是一种可扩展的在线学习框架，通过基于大型视觉语言模型的任务自动生成功能、奖励评估功能及两阶段强化学习机制，在零人工干预下显著提升了两个先进GUI代理(UI-TARS和Aguvis)在OSWorld和AndroidLab环境中的表现。实验表明，ZeroGUI克服了传统方法的局限性，提高了模型的泛化能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于差分信息分布的直接偏好优化理论分析</title>
<link>https://arxiv.org/abs/2505.23761</link>
<guid>https://arxiv.org/abs/2505.23761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">揭示直接偏好优化的理论基础及其与差分信息分布的关系。</p><br /><br /><p><strong>摘要：</strong> Direct Preference Optimization (DPO) 是一种标准技术，用于以监督方式对齐语言模型与人类偏好。尽管其实证成功，但其对数比率奖励参数化的理论依据仍不完整。本文通过利用差分信息分布 (DID) 解决这一问题，该分布捕捉策略更新过程中的信息增益。首先证明当偏好标签编码将参考策略转换为目标策略所需的差分信息时，DPO 的对数比率奖励成为学习目标策略的最佳形式。其次发现偏好编码差分信息的条件与隐含假设密切相关。最后，通过分析 DID 的熵，我们描述了学习低熵差分信息如何增强策略分布，而高熵差分信息诱导平滑效果。我们在合成实验和真实世界指令跟随数据集中验证了这些理论发现，表明学习高熵差分信息对一般指令跟随至关重要，而学习低熵差分信息有利于知识密集型问答。本文为 DPO 目标、偏好数据结构及由此产生的策略行为提供了统一视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>LoRAShop：基于LoRA的多概念图像编辑框架</title>
<link>https://arxiv.org/abs/2505.23758</link>
<guid>https://arxiv.org/abs/2505.23758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAShop利用LoRA模型实现多概念图像编辑，提升身份保真度。</p><br /><br /><p><strong>摘要：</strong> LoRAShop是一种创新的多概念图像编辑框架，专为LoRA模型设计。它基于Flux风格扩散变换器内的特征交互模式观察，通过前期前向传递获取每个概念的解耦潜空间掩码，并仅在限定区域内融合相应的LoRA权重，从而实现多个主体或风格的无缝整合，同时保留全局上下文和细节。实验表明，该方法在身份保真度上优于现有基线。LoRAShop无需重新训练且不受外部约束，将个性化扩散模型转变为实用的‘LoRA Photoshop’工具，推动了视觉叙事和创意迭代的新发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>DeepTheorem：利用自然语言增强大语言模型数学推理能力的综合框架</title>
<link>https://arxiv.org/abs/2505.23754</link>
<guid>https://arxiv.org/abs/2505.23754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepTheorem框架，通过强化学习提升大语言模型在非形式定理证明中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DeepTheorem的新框架，旨在利用自然语言提高大型语言模型（LLMs）在非形式定理证明中的数学推理能力。该框架包含一个大规模的基准数据集，由121,000个高质量的国际数学奥林匹克水平的非形式定理及其证明组成，覆盖多个数学领域，并经过严格标注，同时伴随有系统构建的可验证定理变体。我们还设计了一种专门针对非形式定理证明的新型强化学习策略（RL-Zero），利用这些可验证的定理变体激励稳健的数学推理。此外，我们提出了全面的结果和过程评估指标，考察证明的正确性和推理步骤的质量。广泛的实验分析表明，DeepTheorem在现有数据集和监督微调协议上显著提高了LLMs的定理证明性能，达到了最先进的准确度和推理质量。我们的研究结果强调了DeepTheorem在根本上推进自动非形式定理证明和数学探索方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>基于2D观测的空间多模态大语言模型</title>
<link>https://arxiv.org/abs/2505.23747</link>
<guid>https://arxiv.org/abs/2505.23747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需3D数据即可提升空间智能的多模态大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Spatial-MLLM的新框架，用于仅基于2D观测的视觉空间推理。与依赖CLIP视觉编码器的传统视频多模态大语言模型不同，Spatial-MLLM利用前馈视觉几何基础模型的强大结构先验知识。该模型采用双编码器架构：一个预训练的2D视觉编码器用于提取语义特征，一个初始化自视觉几何模型主干的空问编码器用于提取3D结构特征。通过连接器将两者整合为统一的视觉标记以增强空间理解能力。此外，在推理阶段引入了空间感知帧采样策略，确保模型聚焦于对空间推理至关重要的关键帧。除架构改进外，我们构建了Spatial-MLLM-120k数据集，并通过监督微调和GRPO方法进行训练。实验证明，该模型在多种实际数据集上实现了视觉空间理解和推理任务的最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>To Trust Or Not To Trust Your Vision-Language Model's Prediction</title>
<link>https://arxiv.org/abs/2505.23745</link>
<guid>https://arxiv.org/abs/2505.23745</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>MAGREF：基于掩码引导的任意参考多主体视频生成框架</title>
<link>https://arxiv.org/abs/2505.23742</link>
<guid>https://arxiv.org/abs/2505.23742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架MAGREF，实现高质量多主体视频合成。</p><br /><br /><p><strong>摘要：</strong> 近年来，深度生成模型尤其是扩散模型推动了视频生成技术的进步。然而，基于多个参考主体的视频生成仍面临多主体一致性保持和高质量生成的挑战。本文提出MAGREF框架，通过引入掩码引导机制，在多样参考图像和文本提示条件下实现连贯的多主体视频合成。MAGREF框架包含两个关键创新：区域感知动态掩码机制，使单一模型能够灵活处理多种主体推理；像素级通道连接机制，更好地保留外观特征。实验表明，该方法在复杂多主体场景中表现优于现有开源和商业基线。此外，我们还构建了一个全面的多主体视频基准用于评估。结果表明，MAGREF实现了可扩展、可控且高保真的多主体视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:15 GMT</pubDate>
</item>
<item>
<title>ATLAS：一种高效的长时记忆模块增强Transformer模型</title>
<link>https://arxiv.org/abs/2505.23735</link>
<guid>https://arxiv.org/abs/2505.23735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ATLAS模块，显著提升长上下文理解和序列建模性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统Transformer模型在长序列建模中的瓶颈问题，提出了ATLAS，这是一种具有高容量长时记忆模块，通过优化当前及历史标记来改进记忆管理。基于此设计，我们引入了一组新的深度Transformer架构DeepTransformers，它严格扩展了原始Transformer架构。实验表明，在语言建模、常识推理、召回密集型任务及长上下文理解任务中，ATLAS不仅超越了标准Transformer和近期线性递归模型的表现，还在10M上下文长度的BABILong基准测试中提升了80%的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title>
<link>https://arxiv.org/abs/2505.23716</link>
<guid>https://arxiv.org/abs/2505.23716</guid>
<content:encoded><![CDATA[
We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:49:56 GMT</pubDate>
</item>
<item>
<title>提出新基准VF-Eval评估多模态大语言模型在AI生成内容视频中的能力</title>
<link>https://arxiv.org/abs/2505.23693</link>
<guid>https://arxiv.org/abs/2505.23693</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新基准VF-Eval评估多模态大语言模型在AI生成视频中的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视频问答方面得到了广泛研究，但现有评估主要集中在自然视频上，忽视了合成视频如AI生成内容（AIGC）。一些视频生成工作依赖MLLMs评估生成质量，然而MLLMs对解释AIGC视频的能力尚未得到充分探索。为此，我们提出了一个新的基准VF-Eval，引入了连贯性验证、错误意识、错误类型检测和推理评估四项任务，以全面评估MLLMs在AIGC视频上的能力。我们在VF-Eval上评估了13个前沿MLLMs，发现即使表现最好的模型GPT-4.1，在所有任务上也难以保持一致的良好性能，这凸显了我们基准的挑战性。此外，为了调查VF-Eval在提升视频生成方面的实际应用，我们进行了重新提示实验（RePrompt），表明使MLLMs更紧密地符合人类反馈可以有助于视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23693" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:31:13 GMT</pubDate>
</item>
<item>
<title>Diffusion via Autoregressive模型：一种新的图像扩散建模范式</title>
<link>https://arxiv.org/abs/2505.23660</link>
<guid>https://arxiv.org/abs/2505.23660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种将图像扩散过程重新定义为标准自回归预测的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Diffusion via Autoregressive models (D-AR)的新范式，通过将图像扩散过程视为标准的下一个标记预测过程，实现了对图像的自回归建模。首先设计了一种将图像转换为离散标记序列的分词器，这些标记可以解码为像素空间中的去噪扩散步骤。得益于扩散特性，这些标记自然遵循粗到细的顺序，非常适合自回归建模。通过对这些标记进行标准的下一个标记预测，无需修改任何底层设计，就可以实现图像空间中扩散过程的镜像。实验表明，在ImageNet基准上，使用775M Llama骨干网络和256个离散标记的方法达到了2.09的FID分数。该方法支持一致的预览生成部分标记，并且能够在零样本布局控制合成方面表现出色。我们希望这项工作能够激发未来关于视觉合成统一自回归架构的研究，特别是结合大型语言模型的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23660" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:09:25 GMT</pubDate>
</item>
<item>
<title>大型推理模型中的幻觉现象研究</title>
<link>https://arxiv.org/abs/2505.23646</link>
<guid>https://arxiv.org/abs/2505.23646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型推理模型在事实寻求任务中的幻觉现象表现存在争议。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）因其强大的长链推理能力在复杂任务中表现出色。然而，这些模型是否能通过推理能力减少事实寻求任务中的幻觉现象仍存争议。例如，DeepSeek-R1在SimpleQA任务中报告了性能提升，而OpenAI-o3却发现幻觉现象更加严重。本文从三个方面探讨了这一问题：首先，我们对LRMs的幻觉现象进行了全面评估，发现冷启动监督微调和可验证奖励强化学习可以减轻幻觉，而仅使用蒸馏或无冷启动微调的强化学习会引入更多微妙的幻觉；其次，我们分析了不同的后训练管道如何影响LRMs的幻觉现象，发现表面推理重复错误和思考与答案不匹配是影响事实准确性的重要认知行为；最后，我们从模型不确定性角度研究了LRMs的幻觉机制，发现模型不确定性与事实准确性之间的不匹配通常会导致更高的幻觉率。本研究为理解LRMs的幻觉现象提供了初步认识。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23646" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:53:41 GMT</pubDate>
</item>
<item>
<title>基于文本引导扩散模型的零样本音频源分离方法</title>
<link>https://arxiv.org/abs/2505.23625</link>
<guid>https://arxiv.org/abs/2505.23625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过配置良好的预训练扩散模型实现无需微调的音频源分离。</p><br /><br /><p><strong>摘要：</strong> 当前监督深度学习方法在音频源分离任务中受限于大量标注数据的需求且泛化能力有限，而本文受生成基础模型成功的启发，研究了是否可以利用预训练的文本引导音频扩散模型克服这些限制。令人惊讶的是，在适当配置下，纯文本引导的扩散模型能够实现零样本的音频源分离。所提出的方法名为ZeroSep，它通过将混合音频反向投影到扩散模型的潜在空间，并利用文本条件指导去噪过程以恢复单个源信号。ZeroSep无需特定任务的训练或微调，直接重新利用生成扩散模型进行判别性分离任务，并通过丰富的文本先验支持开放集场景。该方法与多种预训练的文本引导音频扩散模型兼容，在多个分离基准测试中表现出色，甚至超过了监督方法的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:31:45 GMT</pubDate>
</item>
<item>
<title>推理时扩展的表格推理研究：基于蒸馏与可验证奖励强化学习的方法</title>
<link>https://arxiv.org/abs/2505.23621</link>
<guid>https://arxiv.org/abs/2505.23621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出两种后训练策略以实现推理时扩展，表R1-Zero模型性能媲美GPT-4.1。</p><br /><br /><p><strong>摘要：</strong> 本文首次探索了表格推理任务中的推理时扩展问题，开发并评估了两种后训练策略：前沿模型推理轨迹蒸馏和可验证奖励强化学习（RLVR）。通过DeepSeek-R1生成的大规模推理轨迹数据集，我们对大型语言模型进行了微调，得到Table-R1-SFT模型；而在RLVR方法中，我们提出了特定任务的可验证奖励函数，并应用GRPO算法获得了Table-R1-Zero模型。这些模型在短形式问答、事实验证和自由形式问答等多样化表格推理任务上表现出色，其中Table-R1-Zero模型在仅使用7B参数的情况下达到了与GPT-4.1和DeepSeek-R1相当甚至更高的性能，并且在跨领域数据集上展现了强大的泛化能力。进一步的消融分析和定性分析揭示了指令微调、模型架构选择以及跨任务泛化的益处，同时显示了在强化学习训练过程中表格推理技能的涌现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:28:50 GMT</pubDate>
</item>
<item>
<title>Muddit：基于离散扩散的统一文本图像生成模型</title>
<link>https://arxiv.org/abs/2505.23606</link>
<guid>https://arxiv.org/abs/2505.23606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Muddit模型，实现文本和图像模态的快速并行生成。</p><br /><br /><p><strong>摘要：</strong> Unified generation models致力于通过单一架构处理跨模态任务，但现有模型存在推理速度慢或泛化能力弱的问题。本文介绍Muddit，一种基于离散扩散的Transformer模型，通过整合预训练的文本到图像骨干网络与轻量级文本解码器，在文本和图像生成方面实现了高效且高质量的多模态生成。实验表明，Muddit在质量和效率上可媲美甚至超越更大规模的自回归模型，展示了纯粹离散扩散模型结合强视觉先验的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title>EvoScale：通过进化提升小规模语言模型在软件工程任务中的性能</title>
<link>https://arxiv.org/abs/2505.23604</link>
<guid>https://arxiv.org/abs/2505.23604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为EvoScale的方法，提升小规模语言模型在软件工程任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EvoScale的新方法，旨在通过将生成过程视为进化过程来提高小规模语言模型在实际软件工程任务中的表现。传统方法如监督微调需要高质量但昂贵的数据集，而测试时扩展策略虽然有效但成本高。EvoScale通过迭代选择和变异优化生成输出，显著减少所需样本数量。此外，该方法利用强化学习训练模型自我进化，从而在推理阶段无需依赖外部验证器。实验结果显示，采用EvoScale后，32B参数的Satori-SWE-32B模型在SWE-Bench-Verified基准上达到了超过100B参数模型的性能水平，同时仅需少量样本。代码、数据和模型均计划开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:36 GMT</pubDate>
</item>
<item>
<title>基于最优奖励基准的对策略强化学习算法</title>
<link>https://arxiv.org/abs/2505.23585</link>
<guid>https://arxiv.org/abs/2505.23585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习算法OPO，解决大语言模型训练不稳定和计算效率低的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为On-Policy RL with Optimal reward baseline (OPO)的新算法，旨在解决当前强化学习算法在大语言模型训练中的稳定性不足和计算效率低的问题。OPO通过强调精确的对策略训练和引入最优奖励基准来减少梯度方差，从而提高训练稳定性和探索能力。实验表明，OPO在数学推理基准测试中表现出色，且无需额外的辅助模型或正则化项。此外，OPO还减少了策略漂移并提高了输出熵，使得生成的响应更加多样且重复性更低。这些结果表明，OPO为大语言模型的对齐和推理任务提供了一个有前景的方向。相关实现已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:58:04 GMT</pubDate>
</item>
<item>
<title>SafeScientist：强化AI科学家框架的安全性与伦理责任</title>
<link>https://arxiv.org/abs/2505.23559</link>
<guid>https://arxiv.org/abs/2505.23559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SafeScientist框架提升AI驱动科研探索中的安全性和伦理责任。</p><br /><br /><p><strong>摘要：</strong> 本文介绍SafeScientist，一种专门设计用于增强AI科学家框架安全性的创新方法。SafeScientist通过主动拒绝不道德或高风险任务，在整个研究过程中强调安全措施，包括引入多个防御机制如提示监控、协作监控、工具使用监控及伦理审查组件。此外，我们还提出了SciSafetyBench，这是一个用于评估科学领域AI安全性的新基准，涵盖六个领域的240项高风险科学任务及相关工具。实验表明，SafeScientist在保持科研产出质量的同时，将安全性提高了35%，并经受住了多种对抗性攻击测试。该框架的代码和数据将在指定GitHub页面上提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:35:58 GMT</pubDate>
</item>
<item>
<title>SWE-bench-Live：面向动态软件修复的大规模可更新基准</title>
<link>https://arxiv.org/abs/2505.23419</link>
<guid>https://arxiv.org/abs/2505.23419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-bench-Live，解决现有基准静态、过时等问题。</p><br /><br /><p><strong>摘要：</strong> SWE-bench-Live是一个全新的可更新基准，由1319个来自GitHub真实问题的任务组成，涵盖93个存储库。它通过自动化管道简化实例创建和环境设置，解决了传统基准如SWE-bench的局限性。在多个最先进的模型上测试表明，SWE-bench-Live中的性能显著优于静态基准，特别是在实时环境中。通过对存储库来源、问题时效性和任务难度的深入分析，揭示了性能差异的原因。SWE-bench-Live为评估大语言模型和智能体在动态软件开发场景中的能力提供了可靠工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:09:44 GMT</pubDate>
</item>
<item>
<title>KVzip：一种高效的Transformer语言模型KV缓存压缩方法</title>
<link>https://arxiv.org/abs/2505.23416</link>
<guid>https://arxiv.org/abs/2505.23416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KVzip通过压缩KV缓存提高Transformer模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为KVzip的查询无关的KV缓存淘汰方法，它利用底层的语言模型量化KV对的重要性并淘汰不重要的KV对，从而实现高效复用压缩后的KV缓存。实验表明，KVzip可将KV缓存大小减少3到4倍，将FlashAttention解码延迟降低约2倍，在问答、检索、推理和代码理解等任务中性能损失可以忽略不计。KVzip适用于多种模型如LLaMA3.1-8B、Qwen2.5-14B和Gemma3-12B，且在上下文长度高达17万tokens时表现优异。相比之下，现有的查询感知KV淘汰方法即使在90%缓存预算下多查询场景中也会导致性能下降。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:05:47 GMT</pubDate>
</item>
<item>
<title>UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23380</link>
<guid>https://arxiv.org/abs/2505.23380</guid>
<content:encoded><![CDATA[
Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:00:15 GMT</pubDate>
</item>
<item>
<title>VideoReasonBench：评估视觉为中心的复杂视频推理能力</title>
<link>https://arxiv.org/abs/2505.23359</link>
<guid>https://arxiv.org/abs/2505.23359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入VideoReasonBench评估多模态大模型的复杂视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，长链-of-thought（CoT）推理可显著提升大型语言模型（LLMs）在复杂任务中的表现，但在视频理解领域尚未得到验证，因为大多数现有基准缺乏足够的推理深度。本文提出VideoReasonBench，这是一个旨在评估视觉为中心的复杂视频推理的基准。该基准通过设计富含视觉细节且具有高推理复杂度的视频问题，涵盖回忆观察到的视觉信息、推断潜在状态内容及预测视频外信息三个层次。通过对18种最先进的多模态LLMs进行测试，发现大多数模型在此类任务上的表现不佳，而增强思维的Gemini-2.5-Pro表现最佳，达到56.0%的准确率。此外，研究显示扩展思考预算对现有视频基准影响有限，但在VideoReasonBench上至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 07:33:43 GMT</pubDate>
</item>
<item>
<title>UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes</title>
<link>https://arxiv.org/abs/2505.23253</link>
<guid>https://arxiv.org/abs/2505.23253</guid>
<content:encoded><![CDATA[
We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 04:58:41 GMT</pubDate>
</item>
<item>
<title>引入Theory of Mind增强的说服模型ToMAP</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ToMAP，通过增强理论思维提升语言模型的说服力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在说服方面展现出潜力，但现有训练方法仍处于初步阶段。人类擅长动态建模对方的想法，而当前LLMs在理论思维（ToM）推理上存在不足，导致说服多样性有限。为解决此问题，我们提出了ToMAP（Theory of Mind Augmented Persuader），通过两个ToM模块增强对对手心理状态的认知与分析能力。实验显示，尽管ToMAP仅含3B参数，但在多个说服对象模型和语料库上的表现优于更大规模的基线模型GPT-4o，相对提升了39.4%。ToMAP展示了复杂的推理链并减少了重复，使说服更具多样性和有效性。此外，其对手感知特性使其适用于长时间对话，并能采用更逻辑化且有针对性的策略。这些结果验证了ToMAP方法的有效性，并为开发更具说服力的语言代理提供了参考。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:03:41 GMT</pubDate>
</item>
<item>
<title>Multimodal Adversarial Compositionality (MAC)基准测试提升多模态模型鲁棒性</title>
<link>https://arxiv.org/abs/2505.22943</link>
<guid>https://arxiv.org/abs/2505.22943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MAC基准测试，评估多模态模型的组合性漏洞并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Multimodal Adversarial Compositionality (MAC)，这是一个利用大型语言模型生成欺骗性文本样本的新基准，旨在检测和评估跨多种模态的预训练多模态表示（如CLIP）中的组合性脆弱性。通过样本级攻击成功率和组级基于熵的多样性进行评估，MAC揭示了现有模型在处理复杂组合问题时的不足。为改善零样本方法，我们提出了自我训练方法，结合拒绝采样微调和多样性促进过滤技术，显著提升了攻击成功率和样本多样性。实验表明，在较小的语言模型（如Llama-3.1-8B）上，该方法在发现图像、视频和音频等多模态表示中的组合性脆弱性方面表现出色，为提高多模态模型的鲁棒性和安全性提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:45:55 GMT</pubDate>
</item>
<item>
<title>多模态CAD重建模型结合视觉语言与强化学习</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种输入模态的多模态CAD重建模型。</p><br /><br /><p><strong>摘要：</strong> 计算机辅助设计（CAD）在工程和制造领域至关重要，但现有方法通常只专注于单一输入模态（如点云、图像或文本），限制了其通用性和鲁棒性。本文利用视觉语言模型（VLM）的最新进展，提出了一个多模态CAD重建模型，同时处理三种输入模态。该模型采用两阶段管道：首先在大规模程序生成的数据上进行监督微调（SFT），然后通过在线反馈进行强化学习（RL）微调。我们首次探索了针对CAD任务的LLM在线RL微调，发现如组相对偏好优化（GRPO）等在线算法优于离线替代方案。在DeepCAD基准测试中，我们的SFT模型在所有三种输入模态上均优于现有的单模态方法。经过RL微调后，cadrille在三个具有挑战性的数据集上，包括一个真实世界的数据集，创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:32:31 GMT</pubDate>
</item>
<item>
<title>基于合成数据提升语音语言模型对句子重音的理解能力</title>
<link>https://arxiv.org/abs/2505.22765</link>
<guid>https://arxiv.org/abs/2505.22765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入StressTest基准测试，评估现有语音语言模型对句子重音解释的能力并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 句子重音是指在口语表达中对特定词汇施加强调，以突出或对比某种想法，或者引入新信息。最近，语音感知语言模型(SLMs)的发展使得直接处理音频成为可能，从而绕过转录过程并利用语音信号的丰富性进行音频推理任务，如口语问答。然而，尽管重音在塑造意义和说话者意图方面起着关键作用，在这类模型的评估与开发中却往往被忽视。本研究通过引入StressTest基准测试填补这一空白，评估了几种领先SLMs的表现，发现它们在这类任务上的表现不佳。为解决这一问题，我们提出了一个新的合成数据生成管道，创建了Stress17k训练集，该数据集模拟了由重音变化所暗示的意义改变。实证研究表明，优化这些模型可以很好地适应真实录音，并有效微调SLMs。结果显示，我们的微调模型StresSLM在句子重音推理和检测任务上显著优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:32:56 GMT</pubDate>
</item>
<item>
<title>大型语言模型后训练中奖励噪声的影响研究</title>
<link>https://arxiv.org/abs/2505.22653</link>
<guid>https://arxiv.org/abs/2505.22653</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大型语言模型对显著奖励噪声具有强鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在后训练大型语言模型时，奖励噪声对模型推理能力的影响。研究发现，即使在数学任务中人为翻转40%的奖励输出，基于Qwen-2.5-7B模型仍能在任务性能上快速收敛至72%的准确率，接近无噪声奖励模型的75%表现。令人惊讶的是，仅通过奖励关键推理短语（如“首先，我需要”）的出现而非答案准确性，模型在下游任务上的表现峰值超过70%，与严格验证正确性的模型相当。结合推理模式奖励（RPR）与噪声奖励模型，可校准奖励模型并减少潜在的误判，提升开放性任务的表现。本研究强调了预训练阶段基础能力的重要性，并为后训练技术的发展提供了新见解。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22653" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>基于双向扩散模型的高效非自回归文本生成</title>
<link>https://arxiv.org/abs/2505.22618</link>
<guid>https://arxiv.org/abs/2505.22618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的块级近似KV缓存机制，大幅提升扩散语言模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有开源扩散大型语言模型（Diffusion LLMs）在实际推理速度上的不足，尤其是与自回归模型相比的延迟问题，提出了两项创新性改进。首先，我们设计了一种专为双向扩散模型定制的块级近似Key-Value（KV）缓存机制，显著提升了并行解码时的效率，同时保持了极小的性能下降。其次，我们揭示了并行解码导致生成质量下降的根本原因——条件独立假设破坏了令牌依赖关系，并通过引入置信度感知的并行解码策略解决了这一问题，该策略有选择性地对超过置信阈值的令牌进行解码，有效缓解了依赖关系的破坏，从而保证了生成质量。实验结果显示，在LLaDA和Dream等模型上，我们的方法实现了高达27.6倍的吞吐量提升，同时仅造成微小的准确性损失，成功缩小了与自回归模型的性能差距，为扩散语言模型的实际部署铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:39:15 GMT</pubDate>
</item>
<item>
<title>GeoDrive：提升自动驾驶世界模型的空间感知与安全性</title>
<link>https://arxiv.org/abs/2505.22421</link>
<guid>https://arxiv.org/abs/2505.22421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入3D几何条件，GeoDrive显著提高自动驾驶场景建模的准确性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 近期动态环境模拟领域的发展推动了世界模型的进步，这些模型在自动驾驶中的应用可以预测其他道路使用者的行为并进行风险评估。然而，现有方法存在3D几何一致性不足及遮挡处理中产生伪影的问题，影响了安全评估的可靠性。为解决这些问题，我们提出了GeoDrive，它将鲁棒的3D几何条件整合到驾驶世界模型中，以增强空间理解和行动可控性。具体来说，GeoDrive首先从输入帧中提取3D表示，并基于指定的主车轨迹生成2D渲染。在训练过程中，我们还设计了一个动态编辑模块，通过调整车辆位置来优化渲染效果。实验表明，GeoDrive在动作精度和3D空间感知方面均优于现有模型，从而实现了更真实、灵活且可靠的场景建模，显著提升了自动驾驶的安全性。此外，该模型具备泛化能力，并支持交互式场景编辑功能，如对象编辑和轨迹控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 10:46:51 GMT</pubDate>
</item>
<item>
<title>SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</title>
<link>https://arxiv.org/abs/2505.22126</link>
<guid>https://arxiv.org/abs/2505.22126</guid>
<content:encoded><![CDATA[
Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 04:51:01 GMT</pubDate>
</item>
<item>
<title>Differentiable Solver Search for Fast Diffusion Sampling</title>
<link>https://arxiv.org/abs/2505.21114</link>
<guid>https://arxiv.org/abs/2505.21114</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:33:43 GMT</pubDate>
</item>
<item>
<title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title>
<link>https://arxiv.org/abs/2505.20755</link>
<guid>https://arxiv.org/abs/2505.20755</guid>
<content:encoded><![CDATA[
In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a theory-driven framework which we name the \emph{Uni-Instruct}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the f-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded f-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded f-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \emph{1.46} for unconditional generation and \emph{1.38} for conditional generation. On the ImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \emph{1.02}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 01:55:45 GMT</pubDate>
</item>
<item>
<title>One-shot Entropy Minimization</title>
<link>https://arxiv.org/abs/2505.20282</link>
<guid>https://arxiv.org/abs/2505.20282</guid>
<content:encoded><![CDATA[
We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:30 GMT</pubDate>
</item>
<item>
<title>Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking</title>
<link>https://arxiv.org/abs/2505.20199</link>
<guid>https://arxiv.org/abs/2505.20199</guid>
<content:encoded><![CDATA[
Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 12:40:22 GMT</pubDate>
</item>
<item>
<title>Multi-Domain Explainability of Preferences</title>
<link>https://arxiv.org/abs/2505.20088</link>
<guid>https://arxiv.org/abs/2505.20088</guid>
<content:encoded><![CDATA[
Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:01:56 GMT</pubDate>
</item>
<item>
<title>ChartLens: Fine-grained Visual Attribution in Charts</title>
<link>https://arxiv.org/abs/2505.19360</link>
<guid>https://arxiv.org/abs/2505.19360</guid>
<content:encoded><![CDATA[
The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 19:17:32 GMT</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 15:34:15 GMT</pubDate>
</item>
<item>
<title>Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator</title>
<link>https://arxiv.org/abs/2505.19236</link>
<guid>https://arxiv.org/abs/2505.19236</guid>
<content:encoded><![CDATA[
Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:25:23 GMT</pubDate>
</item>
<item>
<title>CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays</title>
<link>https://arxiv.org/abs/2505.18087</link>
<guid>https://arxiv.org/abs/2505.18087</guid>
<content:encoded><![CDATA[
Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:44:21 GMT</pubDate>
</item>
<item>
<title>PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions</title>
<link>https://arxiv.org/abs/2505.17818</link>
<guid>https://arxiv.org/abs/2505.17818</guid>
<content:encoded><![CDATA[
Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:34:48 GMT</pubDate>
</item>
<item>
<title>IQBench：评估视觉语言模型在人类智商测试中的推理能力</title>
<link>https://arxiv.org/abs/2505.12000</link>
<guid>https://arxiv.org/abs/2505.12000</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入IQBench基准，评估视觉语言模型在标准化视觉智商测试中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型视觉语言模型(VLMs)在多模态任务中表现优异，但其在人类智商测试中的真正推理能力尚未充分探索。本文介绍了IQBench，这是一个新设计的基准，专注于评估VLMs在标准化视觉智商测试中的推理能力。与以往仅关注最终预测准确性不同，该基准通过评估模型的解释和解题模式，结合最终预测的准确性及人工评价来衡量推理能力。实验结果显示，尽管某些模型如`o4-mini`、`gemini-2.5-flash`和`claude-3.7-sonnet`在平均准确率上表现较好，但在三维空间和变位词推理任务中仍显不足，表明当前VLMs的通用推理能力存在显著局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12000" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 09:24:08 GMT</pubDate>
</item>
<item>
<title>零样本嫁接技术降低视觉语言模型训练成本</title>
<link>https://arxiv.org/abs/2505.22664</link>
<guid>https://arxiv.org/abs/2505.22664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过小模型迁移策略降低视觉语言模型训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“零样本嫁接”的方法，用于减少视觉语言模型（VLMs）的训练开销。传统方法通常将大型语言模型（LLMs）作为解码器，但这种方法计算负担重且成本高昂。为了解决这个问题，研究者们设计了小型“替代模型”，这些模型继承了目标LLM的浅层网络并共享相同的嵌入空间和表示语言。经过替代模型训练的视觉编码器可以直接迁移到更大的模型上。实验结果显示，这种嫁接方法不仅在某些基准测试中表现优于直接使用小模型，甚至可以达到与完整解码器训练相当的效果。此外，当以Llama-70B作为解码器时，该方法可使总体训练成本降低约45%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>FastTD3：加速人形机器人强化学习训练的高效算法</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FastTD3通过简单修改显著提升人形机器人在多环境中的训练速度。</p><br /><br /><p><strong>摘要：</strong> 强化学习在机器人领域取得了显著进展，但其复杂性和长时间训练仍是主要瓶颈。本文介绍了一种名为FastTD3的新算法，它通过并行模拟、大批次更新、分布式批评者及精心调优的超参数，在HumanoidBench、IsaacLab和MuJoCo Playground等环境中大幅缩短了人形机器人训练时间，某些任务仅需不到3小时即可完成，且训练过程稳定。此外，我们还提供了FastTD3的轻量化实现，以促进机器人领域的强化学习研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:55:26 GMT</pubDate>
</item>
<item>
<title>PISCES：一种精确擦除语言模型概念知识的新框架</title>
<link>https://arxiv.org/abs/2505.22586</link>
<guid>https://arxiv.org/abs/2505.22586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PISCES框架，通过直接编辑参数空间中的方向，有效擦除语言模型中的特定概念。</p><br /><br /><p><strong>摘要：</strong> 现有方法在擦除大型语言模型中的不当知识时存在粗略、浅显或无效的问题。本文提出PISCES框架，利用解缠模型将MLP向量分解为可解释特征，并通过自动化可解释技术定位并移除目标概念相关的特征。实验表明，PISCES在Gemma 2和Llama 3.1上对多种概念的擦除效果优于领先方法，将目标概念的准确率降至7.7%，同时显著提高了擦除特异性和鲁棒性，分别提升了31%和38%。整体来看，基于特征的参数内编辑为语言模型中概念知识的更精确和可靠擦除提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:58:23 GMT</pubDate>
</item>
<item>
<title>HLIP：一种针对3D医学影像的语言-图像预训练框架</title>
<link>https://arxiv.org/abs/2505.21862</link>
<guid>https://arxiv.org/abs/2505.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HLIP通过分层注意力机制提升了3D医学影像语言-图像预训练的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HLIP（Hierarchical attention for Language-Image Pre-training）的可扩展预训练框架，用于3D医学影像。HLIP采用轻量级分层注意力机制，模仿放射学数据的自然层次结构（切片、扫描和研究）。该方法在Rad-ChestCT基准测试上展示了强泛化能力，在CT-RATE数据集上预训练后，宏观AUC提高了4.3%。此外，HLIP的计算效率允许直接在未经处理的数据集上进行训练。它在脑部MRI和头部CT数据集上的表现达到当前最佳水平，例如在Pub-Brain-5基准上平衡准确率提升32.4%，在RSNA和CQ500头部CT基准上分别提高1.4%和6.9%的宏观AUC。这些结果表明，HLIP使得直接在未经处理的临床数据集上进行预训练成为3D医学影像领域的一种可行且有效的方法。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:16:34 GMT</pubDate>
</item>
<item>
<title>DORI基准测试：多模态系统物体方向感知能力评估</title>
<link>https://arxiv.org/abs/2505.21649</link>
<guid>https://arxiv.org/abs/2505.21649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DORI基准测试揭示现有视觉语言模型在物体方向理解上的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为DORI（辨别性方向推理智能）的新基准测试，专门用于评估物体方向感知能力。DORI通过四个维度（正面对齐、旋转变换、相对方向关系和典型方向理解）来衡量模型的表现。经过对11个数据集的精细设计任务分析，发现最先进的视觉语言模型在粗粒度任务上的准确率仅为54.2%，而在细粒度方向判断上仅为33.0%，且在涉及参考框架转换或复合旋转的任务中表现明显下降。这些结果表明现有模型在内部三维空间表示方面存在严重缺陷，提示需要改进方向表示机制。DORI作为首个针对多模态系统方向意识的诊断框架，具有提升机器人控制、3D场景重建及人机物理交互的重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 14:22:44 GMT</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Mon, 26 May 2025 19:50:36 GMT</pubDate>
</item>
<item>
<title>HoPE：提升视觉语言模型长上下文能力的混合位置嵌入方法</title>
<link>https://arxiv.org/abs/2505.20444</link>
<guid>https://arxiv.org/abs/2505.20444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出HoPE，一种针对长视频理解的混合位置嵌入方法。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型(VLMs)在多模态任务中取得了显著进展，但在长上下文场景（如长视频）中的表现通常会下降。尽管旋转位置嵌入(RoPE)已被广泛用于大语言模型(LLMs)的长度泛化，但将其扩展到捕捉视频复杂的时空依赖关系仍是一个未解决的挑战。现有方法通常通过不同的频率分配策略来编码三维位置信息，但这些策略主要基于启发式方法，缺乏深入的理论分析。本研究首先探讨了不同分配策略对VLMs长上下文能力的影响，发现当前的多模态RoPE无法可靠地捕获扩展上下文中的语义相似性。为此，我们提出了HoPE，这是一种混合位置嵌入方法，旨在提高VLMs的长上下文能力。HoPE引入了一种混合频率分配策略，以在任意长的上下文中进行可靠的语义建模，并采用动态时间缩放机制，促进在不同上下文长度下的稳健学习和灵活推理。广泛的实验表明，在四个视频基准上的长视频理解和检索任务中，HoPE始终优于现有方法，证明了其有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:37:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在真实文本因果推理中的挑战</title>
<link>https://arxiv.org/abs/2505.18931</link>
<guid>https://arxiv.org/abs/2505.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示顶级大型语言模型在真实文本因果推理上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 理解并推断文本中的因果关系是人类认知的核心方面，也是推动大型语言模型（LLMs）向通用人工智能发展的关键。现有研究多集中于合成文本中明确提到的简单因果关系，未能反映现实任务的复杂性。本文探讨了LLMs是否能在真实世界文本中推断因果关系，并开发了一个来自实际学术文献的数据集，涵盖多样化的文本长度、因果关系复杂性及领域。这是首个针对此任务的真实世界数据集。实验表明，最先进的LLMs在该基准测试中面临重大挑战，最佳模型的平均F1分数仅为0.477。分析揭示了常见问题，如隐含信息的理解困难、区分相关因果因素与上下文细节的能力不足，以及连接分散在长文本中的因果相关信息的难题。通过系统地识别这些缺陷，本研究为LLMs因果推理能力的进一步发展提供了有针对性的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 21:50:05 GMT</pubDate>
</item>
<item>
<title>个性化安全评估：大语言模型的安全性改进</title>
<link>https://arxiv.org/abs/2505.18882</link>
<guid>https://arxiv.org/abs/2505.18882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出个性化安全概念，通过PENGUIN基准测试验证其有效性。</p><br /><br /><p><strong>摘要：</strong> 大语言模型通常对相同提示提供相同或相似的响应，在高风险应用场景中可能带来严重安全隐患，因为用户的风险状况差异很大。现有安全性评估主要依赖上下文独立指标（如事实性、偏见或毒性），而忽视了相同响应对不同背景用户的潜在风险差异。本文引入个性化安全概念，并构建PENGUIN基准，包含七个敏感领域的14,000个场景及其上下文版本。评估显示，个性化用户信息可提升安全性评分43.2%，证明个性化方法的有效性。然而，并非所有上下文属性对安全增强同等重要，为此开发RAISE框架，无需重新训练模型即可通过两阶段策略获取用户特定背景，显著提升了六种基础模型的安全评分最高达31.6%，同时平均仅需两次用户查询。研究强调在关键领域中选择性信息收集的重要性，并提供了个性化大语言模型响应的实用解决方案，为适应个体用户情境的安全研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 17:37:10 GMT</pubDate>
</item>
<item>
<title>面向企业专用领域的可扩展硬负采样框架</title>
<link>https://arxiv.org/abs/2505.18366</link>
<guid>https://arxiv.org/abs/2505.18366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种针对企业专用数据的硬负采样框架，显著提升检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了企业搜索系统因语义不匹配和术语重叠导致检索准确性下降的问题，这些问题严重影响了知识管理、客户服务等下游应用的表现。为解决这一挑战，我们提出了一种专门针对企业专用数据的可扩展硬负采样框架。该方法通过动态选择语义上具有挑战性但上下文无关的文档，增强了部署的重新排名模型。我们的方法结合了多种嵌入模型，执行降维操作，并独特地选择了硬负样本，保证了计算效率和语义精确性。在私有企业语料库（云服务领域）上的评估显示，与最先进的基线和其他负采样技术相比，MRR@3提升了15%，MRR@10提升了19%。此外，在公共领域特定数据集（FiQA、Climate Fever、TechQA）上的进一步验证表明，该方法具有广泛的适用性和实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 16:51:20 GMT</pubDate>
</item>
<item>
<title>Transformer架构中的Token缩减：超越效率导向的潜力</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Token缩减在大型生成模型中应超越效率策略，成为生成建模的基本原则。</p><br /><br /><p><strong>摘要：</strong> 在Transformer架构中，Token作为从原始数据中衍生的离散单元，通过将输入分割为固定长度块形成，并映射到嵌入向量以支持并行注意力计算。然而，由于自注意力机制的二次计算复杂性，Token缩减通常被用作提高效率的手段。本文认为，在大型生成模型的时代，Token缩减不应仅限于此，而是应在生成建模中发挥根本性作用。具体而言，Token缩减可以促进多模态集成与对齐、减少过度思考和幻觉现象、维持长输入一致性，并提升训练稳定性等。本文重新定义了Token缩减的意义，提出了包括算法设计、强化学习引导的Token缩减、上下文学习中的Token优化等未来方向，并强调其对推动新模型架构和学习策略的重要性，以增强鲁棒性、提高可解释性并更好地实现生成建模目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 07:30:30 GMT</pubDate>
</item>
<item>
<title>Few Shot Domain Adapting Graph：高效文档理解模型</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效视觉丰富文档理解的Few-Shot模型FS-DAG。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Few Shot Domain Adapting Graph（FS-DAG）的模型架构，专门用于在少量样本条件下进行视觉丰富文档理解（VRDU）。该模型通过模块化框架结合领域特定及语言/视觉特定的骨干网络，在有限的数据下适应多种文档类型。FS-DAG对实际部署中的挑战如OCR错误、拼写错误和领域转换具有鲁棒性。实验表明，FS-DAG在信息抽取任务上表现出显著的收敛速度和性能提升，同时参数量低于90M，非常适合资源受限的复杂场景。此外，研究强调了开发更小、更高效的模型而不牺牲性能的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 18:53:58 GMT</pubDate>
</item>
<item>
<title>强化学习提升大语言模型多轮推理能力的研究</title>
<link>https://arxiv.org/abs/2505.11821</link>
<guid>https://arxiv.org/abs/2505.11821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入细粒度的回合级优势估计策略，显著提升了大语言模型的多轮推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了利用强化学习增强大型语言模型（LLM）推理能力的方法，特别关注多轮工具使用场景，将其建模为马尔可夫决策过程（MDP）。现有方法在多步决策中的回合级信用分配存在困难，限制了其在多轮推理任务中的表现。为此，我们提出了一种细粒度的回合级优势估计策略，用于更精确地分配信用。该策略可融入多种强化学习算法，如组相对偏好优化（GRPO）。实验评估显示，在多轮推理和基于搜索的工具使用任务中，采用GRPO实现的MDP框架及回合级信用分配显著提高了LLM代理在复杂决策环境下的性能，工具执行成功率达到100%，精确答案匹配准确率达50%，远超基线模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 00:09:46 GMT</pubDate>
</item>
<item>
<title>通过大规模知识库CHIMERA探索科学创新中的概念重组</title>
<link>https://arxiv.org/abs/2505.20779</link>
<guid>https://arxiv.org/abs/2505.20779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究者创建了CHIMERA，一个基于科学文献的大规模概念重组知识库。</p><br /><br /><p><strong>摘要：</strong> 人类创新的一个显著特征是重组过程——通过整合现有机制和概念的元素来创造原创想法。本文提出了一种新方法，自动挖掘科学文献并构建了一个名为CHIMERA的知识库，该知识库包含了大量重组实例。CHIMERA可以用来大规模探索科学家如何跨领域重组概念并汲取灵感，或者用于训练机器学习模型以预测新的跨学科创意方向。为了构建这个知识库，我们提出了从科学论文摘要中提取重组信息的新任务，收集了数百份人工标注的高质量摘要数据集，并利用其训练基于大型语言模型（LLM）的提取模型。该模型应用于AI领域的大量论文，生成了超过28,000个重组实例的知识库。我们分析了CHIMERA在AI各子领域中的重组特性，并利用该知识库训练了一个科学假设生成模型，预测现实中研究人员感到启发的新重组方向。我们的数据和代码已公开发布。关键词：ADs</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 02:36:04 GMT</pubDate>
</item>
<item>
<title>基于多模态漫画理解的基准与模型开发</title>
<link>https://arxiv.org/abs/2505.20298</link>
<guid>https://arxiv.org/abs/2505.20298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出漫画理解的两个新基准并开发专用模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个针对日本漫画（manga）这种复合叙事形式的多模态理解研究。通过构建MangaOCR和MangaVQA两个新基准，分别用于页面内文本识别和基于视觉问答的情境理解评估，其中MangaVQA包含526对高质量问题-答案对。基于这些基准，我们开发了MangaLMM模型，该模型是在开源多模态模型Qwen2.5-VL基础上微调而成，能够同时处理两项任务。实验表明，此模型在理解漫画方面表现优异，并且与GPT-4o和Gemini 2.5等专有模型进行了对比分析，为提升大型多模态模型在漫画领域的应用奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>Influence Distillation: 优化大型语言模型训练的数据选择框架</title>
<link>https://arxiv.org/abs/2505.19051</link>
<guid>https://arxiv.org/abs/2505.19051</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于二阶信息的新型数据选择方法，显著提升LLM训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Influence Distillation的创新框架，用于高效选择大型语言模型（LLMs）的训练数据。该方法通过利用二阶信息来为训练样本分配最优权重，从而指导模型在目标域上的性能表现。我们为梯度下降和Adam优化器推导出这些最优权重，并通过引入基于标志样本的近似方法来降低计算成本。实验表明，该方法在Tulu V2数据集上进行指令微调时，对GSM8k、SQuAD和MMLU等任务的表现达到了或超过了当前最佳水平，同时加快了数据选择速度高达3.5倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19051" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 05:08:00 GMT</pubDate>
</item>
<item>
<title>Sherlock：通过自纠正提升视觉语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.22651</link>
<guid>https://arxiv.org/abs/2505.22651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为Sherlock的新框架，显著提升了视觉语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过引入自纠正机制来增强视觉语言模型（VLMs）的推理能力。研究发现现有VLMs在处理复杂多模态任务时存在对推理错误敏感、需要大量标注数据等问题。为解决这些问题，我们开发了Sherlock框架，该框架利用轨迹级自纠正目标、基于视觉扰动的偏好数据构建方法以及动态beta值调优，使得模型能够在仅使用少量标注数据的情况下实现自我改进。实验结果显示，在八个基准测试中，Sherlock的直接生成准确率为64.1%，经过自纠正后达到65.4%，优于其他对比模型，且所需标注数据量不到其20%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>通过未来事件预测提升多模态大模型的时间推理能力</title>
<link>https://arxiv.org/abs/2505.22457</link>
<guid>https://arxiv.org/abs/2505.22457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出未来事件预测任务，利用视频片段自我监督信号提升时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大模型（MLMMs）在处理视频输入时缺乏有效的时间推理学习任务。现有的视频问答任务依赖人工标注或更强的模型，而视频描述则常混淆时间推理与空间信息。为解决这一问题，我们提出了未来事件预测（NEP），该任务利用视频未来片段作为丰富的自我监督信号，鼓励模型进行时间推理。我们构建了一个包含33,000个自动提取视频片段的数据集V1-33K，覆盖多样现实场景。此外，我们研究了多种视频指令微调策略对时间推理的影响，并引入FutureBench评估对未来事件预测的连贯性。实验表明，NEP是一种可扩展且有效的训练范式，可显著提升MLMMs的时间推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 11:13:34 GMT</pubDate>
</item>
<item>
<title>JQL：高效构建大规模高质量多语言数据集的方法</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JQL通过轻量级标注器提升多语言数据集质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为JQL的系统性方法，用于高效创建大规模且高质量的多语言数据集。传统多语言数据集依赖启发式过滤方法，限制了跨语言迁移性和扩展性。JQL利用预训练多语言嵌入将大型语言模型的标注能力转化为轻量级标注器，这些标注器即使面对训练时未见过的语言和脚本也能表现出强大的多语言和跨语言性能。实验结果显示，在35种语言上的评估表明，JQL的标注流水线显著优于当前的启发式过滤方法如Fineweb2，提升了下游模型训练质量和数据保留率。这项研究为多语言数据集的构建提供了实用见解和宝贵资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22232" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 07:06:54 GMT</pubDate>
</item>
<item>
<title>强化学习中可验证奖励方法的验证器可靠性分析</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有规则验证器存在误判问题，而模型验证器易受攻击，影响RL训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文以数学推理为例，对多种验证器在静态评估及强化学习训练中的表现进行了全面分析。研究显示，当前开源的基于规则的验证器在多个常用数学数据集中无法识别等价但格式不同的答案，导致显著的假阴性率，从而损害了强化学习的训练效果，且这一问题随着策略模型增强愈发严重。随后，我们探索了基于模型的验证器作为解决方案的可能性。尽管静态评估表明这类验证器具有更高的准确性，但在实际强化学习训练中，它们容易受到攻击，表现为将某些响应模式误分类为正确（即假阳性），这被优化策略模型利用，造成人为抬高的奖励。我们的研究揭示了规则验证器和模型验证器各自的风险，为构建更稳健的强化学习奖励系统提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 06:28:41 GMT</pubDate>
</item>
<item>
<title>利用预训练语言模型实现抽象结构化推理</title>
<link>https://arxiv.org/abs/2505.22202</link>
<guid>https://arxiv.org/abs/2505.22202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究预训练语言模型是否能通过学习表示进行抽象推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了预训练的语言模型是否可以通过其已学得的表示被提升到抽象推理空间中，从而能够在句子层面而非原始令牌序列上进行推理。我们提出了一种框架，该框架将预训练的令牌级语言模型适应到句子空间中，通过自回归预测下一个句子的连续嵌入来进行操作。实验评估了两种嵌入范式：基于表面意义的语义嵌入和基于上下文预测的上下文嵌入。在数学、逻辑、常识和规划四个领域内，上下文嵌入在连续推理模式下的表现接近链式思维方法，但推理时的浮点运算减少了平均一半。此外，我们还展示了可扩展性和模块化适配的早期迹象，并引入了一个名为SentenceLens的诊断工具来可视化潜在轨迹。最终结果表明，预训练的语言模型可以在潜在嵌入空间中有效地过渡到抽象的结构化推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 06:28:35 GMT</pubDate>
</item>
<item>
<title>UniPano: 用于全景图像生成的统一扩散模型适配框架</title>
<link>https://arxiv.org/abs/2505.22129</link>
<guid>https://arxiv.org/abs/2505.22129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示扩散模型适配全景图像生成的关键机制并提出高效框架UniPano。</p><br /><br /><p><strong>摘要：</strong> 最近文本到图像扩散模型（如Stable Diffusion）的成功激发了将其应用于全景图像生成的研究。尽管已有工作展示了通过低秩适应技术改造预训练扩散模型生成全景图像的可行性，但透视图与全景图之间的显著领域差距引发对背后机制的疑问。本文假设并验证了可训练组件在适应全景数据时表现出不同行为，并隐藏了利用预训练模型先验知识的内在机制。分析表明，注意力模块中的查询和键矩阵负责可在两个领域间共享的通用信息，而值矩阵和输出权重矩阵则更专注于将预训练知识适应到全景域。基于这些见解，我们提出了名为UniPano的简单框架，不仅性能超越现有方法，还大幅减少了内存使用和训练时间，为端到端高分辨率全景生成提供了优雅的基线。此外，该框架在效率和效果上均优于先前的双分支方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 04:54:04 GMT</pubDate>
</item>
<item>
<title>基于强化学习的视觉丰富信息检索与推理框架VRAG-RL</title>
<link>https://arxiv.org/abs/2505.22019</link>
<guid>https://arxiv.org/abs/2505.22019</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉感知的强化学习框架，提升复杂视觉信息的推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的基于文本的检索增强生成（RAG）方法难以有效处理视觉相关信息，而当前基于视觉的RAG方法受限于固定管道且模型基础能力激活不足，导致推理效果不佳。本文引入VRAG-RL，这是一种专为复杂视觉信息推理设计的新型强化学习框架。通过该框架，视觉语言模型（VLMs）借助视觉感知标记与搜索引擎交互，自主采样单轮或多轮推理轨迹，并基于这些样本进行持续优化。框架揭示了RAG领域中强化学习的关键限制，包括多模态RAG方法对图像的整合不足及查询表述能力欠缺等问题。为解决这些问题，我们定义了一个针对视觉丰富输入定制的动作空间，并采用融合查询重写和检索性能的奖励机制。最终，VRAG-RL优化了VLMs在RAG任务中的表现，使其更贴近实际应用需求。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22019" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 02:30:51 GMT</pubDate>
</item>
<item>
<title>时间无关统一编码器TiUE提升文本到图像扩散模型推理效率</title>
<link>https://arxiv.org/abs/2505.21960</link>
<guid>https://arxiv.org/abs/2505.21960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的时间无关统一编码器TiUE，显著提升文本到图像扩散模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有文本到图像(T2I)扩散模型在推理速度和图像质量之间存在的权衡问题，分析发现UNet编码器存在冗余计算，并指出解码器更能捕捉丰富的语义信息。基于此，我们首次引入了名为TiUE的时间无关统一编码器，用于学生模型UNet架构，通过共享多个解码器时间步的编码特征实现并行采样，大幅降低推理时间复杂度。此外，还加入了KL散度项来正则化噪声预测，进一步提高生成图像的感知真实性和多样性。实验表明，TiUE在保持计算效率的同时，生成的结果比现有最先进的方法（如LCM、SD-Turbo和SwiftBrushv2）更加多样化且逼真。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:23:22 GMT</pubDate>
</item>
<item>
<title>SVRPBench：首个城市规模车辆路径问题高保真随机动态基准</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVRPBench引入首个捕捉城市尺度车辆路径随机动态的开放基准。</p><br /><br /><p><strong>摘要：</strong> SVRPBench是第一个面向实际物流场景的开放基准，通过模拟时间依赖性拥堵、对数正态延迟、概率事故及基于实证的时间窗口等条件，涵盖了超过500个实例，最多包含1000个客户。该基准测试显示，最先进的强化学习求解器如POMO和AM在分布偏移下性能下降超过20%，而经典和元启发式方法则表现稳健。为了促进可重复研究，项目团队公开了数据集和评估工具包。SVRPBench呼吁社区开发能够超越合成假设并在真实世界不确定性中适应的求解器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 22:03:31 GMT</pubDate>
</item>
<item>
<title>大语言模型微调机制解析：基于稀疏组件的功能特性研究</title>
<link>https://arxiv.org/abs/2505.21191</link>
<guid>https://arxiv.org/abs/2505.21191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型微调后指令执行能力提升背后的稀疏计算机制。</p><br /><br /><p><strong>摘要：</strong> 本研究通过设计HexaInst数据集和SPARCOM分析框架，系统性地探索了大语言模型（LLMs）微调过程中指令特定稀疏组件的变化。这些组件包括密集模型中的神经元及Mixture-of-Experts架构中的专家节点。研究发现，这些稀疏组件不仅具有功能普遍性和独特性，还对指令执行起着至关重要的作用。通过实验验证，该工作深入剖析了微调如何重塑LLMs的计算机制，为可信的大语言模型开发提供了理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21191" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:40:28 GMT</pubDate>
</item>
<item>
<title>高效3D场景风格化方法：基于分离架构与身份损失的快速实现</title>
<link>https://arxiv.org/abs/2505.21060</link>
<guid>https://arxiv.org/abs/2505.21060</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用前馈重建模型实现秒级3D场景风格化的创新方法。</p><br /><br /><p><strong>摘要：</strong> 本文解决了在保持多视角一致性的同时快速将风格图像应用于3D场景的问题。传统方法通常需要密集的视角输入图像且计算成本高，而我们利用前馈重建模型实现了在不到一秒的时间内对未定位稀疏视图场景图像进行风格化处理。通过引入分支架构分离结构建模与外观渲染，有效防止风格迁移扭曲底层3D场景结构。此外，通过适配身份损失，我们的模型能够在新型视图合成任务中进行预训练，从而在微调用于风格化时仍保留原始重建能力。实验表明，该方法不仅在风格与场景外观融合上表现优异，而且在多视角一致性和效率方面超越现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21060" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 07:47:15 GMT</pubDate>
</item>
<item>
<title>基于代理的智能辅导系统AITEE提升电气工程教育</title>
<link>https://arxiv.org/abs/2505.21582</link>
<guid>https://arxiv.org/abs/2505.21582</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合大型语言模型的代理式辅导系统AITEE显著提升了电气工程教育中的个性化学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为AITEE的代理式智能辅导系统，专门针对电气工程教育设计。该系统通过手绘和数字电路支持，结合图相似性度量与增强型Spice仿真技术，为学生提供个性化的学习支持。AITEE采用苏格拉底式对话引导学生自主学习，实验显示其在特定领域知识应用上优于基线方法，证明了代理式导师在规模化、个性化和高效教育中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21582" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 06:07:05 GMT</pubDate>
</item>
<item>
<title>MUSEG：基于强化学习的多片段时间对齐提升视频时间理解</title>
<link>https://arxiv.org/abs/2505.20715</link>
<guid>https://arxiv.org/abs/2505.20715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MUSEG方法，通过时间戳感知的多片段定位增强视频时间理解。</p><br /><br /><p><strong>摘要：</strong> 视频时间理解对多模态大语言模型（MLLMs）进行视频事件推理至关重要。尽管近期在一般视频理解上取得进展，但现有MLLMs在细粒度时间推理方面仍面临挑战。虽然强化学习（RL）最近被探索用于解决此问题，但现有RL方法效果有限。本研究提出MUSEG，一种新颖的基于RL的方法，通过引入时间戳感知的多片段定位来增强时间理解。MUSEG使MLLMs能够将查询与多个相关视频片段对齐，从而促进更全面的时间推理。为了实现有效的学习，我们设计了一个定制的RL训练方案，采用分阶段奖励逐步引导模型进行时间定位推理。广泛的实验表明，MUSEG在时间定位和时间敏感的视频问答任务上显著优于现有方法，并在多样化的时间理解场景中表现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:50:07 GMT</pubDate>
</item>
<item>
<title>基于LLM的软件工程代理训练数据集及无污染基准构建</title>
<link>https://arxiv.org/abs/2505.20411</link>
<guid>https://arxiv.org/abs/2505.20411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自动化方法生成大规模真实交互式软件工程任务数据集。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）驱动的软件工程代理在多种任务中展现出潜力，但面临高质量训练数据稀缺及静态基准过时两大挑战。现有数据集规模小且多样性不足，难以反映真实世界场景。为此，本文设计了一套自动化可扩展的流水线，从GitHub仓库提取真实交互式Python软件工程任务，构建了包含超过21,000个任务的SWE-rebench数据集。此外，通过该方法持续收集的新鲜任务，建立了无污染的软件工程代理基准，揭示某些模型性能可能因数据污染而被高估的现象。这一研究为LLM在软件工程中的应用提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:01:00 GMT</pubDate>
</item>
<item>
<title>UniR：一种通用且高效的轻量级推理模块</title>
<link>https://arxiv.org/abs/2505.19075</link>
<guid>https://arxiv.org/abs/2505.19075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniR模块，通过轻量化推理增强大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniR（Universal Reasoner）的新型推理模块，它是一种轻量级、可组合且即插即用的推理模块，可以与任何冻结的大语言模型（LLM）结合，赋予其专门的推理能力。UniR将奖励分解为独立的推理模块，通过预定义的奖励进行训练，从而将轨迹级别的信号转化为令牌级别的指导。实验结果显示，在数学推理和机器翻译任务上，UniR显著优于现有的基于参数高效微调的方法，尤其是在Llama3.2模型上的表现。此外，UniR展示了强大的弱到强泛化能力，训练在小模型上的推理模块可以有效引导更大的LLM。这使得UniR成为一种成本效益高、适应性强且稳健的解决方案，用于增强LLMs的推理能力而不损害其核心能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 06:19:10 GMT</pubDate>
</item>
<item>
<title>Chain-of-Zoom：一种可扩展的单图像超分辨率框架</title>
<link>https://arxiv.org/abs/2505.18600</link>
<guid>https://arxiv.org/abs/2505.18600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种模型不可知的框架Chain-of-Zoom，实现极端放大倍率下的高质量图像超分辨率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Chain-of-Zoom (CoZ) 的新框架，旨在解决现有单一图像超分辨率(SISR)模型在高倍率放大时性能下降的问题。CoZ通过将SISR分解为一系列中间尺度状态的自回归链，并结合多尺度感知提示，利用现有的骨干超分辨率模型多次重用来实现极高分辨率的图像重建，而无需额外训练。为了应对高倍率放大时视觉线索减弱的问题，CoZ在每次缩放步骤中引入由视觉语言模型生成的多尺度感知文本提示。此外，通过广义奖励策略优化方法对提示提取器进行微调，使其更符合人类偏好。实验表明，基于标准4倍扩散模型的CoZ能够在保持高水平感知质量和保真度的情况下实现超过256倍的放大效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 04:50:08 GMT</pubDate>
</item>
<item>
<title>First Finish Search：一种高效的推理时间扩展策略</title>
<link>https://arxiv.org/abs/2505.18149</link>
<guid>https://arxiv.org/abs/2505.18149</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的并行解码策略First Finish Search，显著提升大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为First Finish Search（FFS）的新颖训练自由并行解码策略，该策略通过同时启动多个独立样本并在任意一个完成时立即返回，有效提升了大型语言模型在推理阶段的表现。研究发现，对于推理任务而言，较短的解码路径比较长的路径更可能产生正确的答案。FFS在四个不同的推理模型和四个数据集上的实验表明，它能够在保持较低计算成本的同时显著提高准确性。例如，在AIME数据集上，使用DeepSeek-R1模型时，FFS达到了82.23%的准确率，相比单次运行提升了15%，几乎达到OpenAI o4-mini的水平。此外，理论分析解释了为何提前停止于最短路径通常会得到正确答案，并指出了早停可能不适用的情况。这一研究揭示了简单推理时间扩展策略的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18149" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:57:43 GMT</pubDate>
</item>
<item>
<title>基于免疫原理的生成式AI模型虚假信息防控框架</title>
<link>https://arxiv.org/abs/2505.17870</link>
<guid>https://arxiv.org/abs/2505.17870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在训练中引入标注的虚假数据集，生成式AI可增强对虚假信息的识别能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新颖的生成式人工智能模型训练框架，借鉴生物免疫概念，将经过标注的少量虚假信息作为“疫苗”，通过定期注入到模型微调过程中，以提高模型识别并拒绝误导性陈述的能力，同时保持对真实信息的准确性。这种框架不同于传统方法，它直接利用事实核查过的虚假数据而非输入扰动或通用反馈信号。研究显示，经过免疫处理的模型生成虚假信息的比例显著低于基线模型。此外，文章还探讨了相关的伦理安全措施和治理机制，以确保虚假数据的安全使用。这一创新方法为实现AI系统与事实性的对齐提供了积极的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:20:23 GMT</pubDate>
</item>
<item>
<title>BraInCoRL：利用少量样本预测神经响应的视觉皮层模型</title>
<link>https://arxiv.org/abs/2505.15813</link>
<guid>https://arxiv.org/abs/2505.15813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BraInCoRL通过提示学习在无需微调的情况下预测新对象和刺激的神经反应。</p><br /><br /><p><strong>摘要：</strong> 理解高等视觉皮层的功能表示是计算神经科学中的基本问题。尽管大规模数据集上的预训练人工神经网络在人类神经反应表征上表现出显著的对齐性，但学习视觉皮层的图像可计算模型需要个体水平的大规模功能磁共振成像（fMRI）数据集。高昂的数据获取成本限制了编码器对新受试者和刺激物的泛化能力。本文提出BraInCoRL方法，采用上下文学习的方式，在少量样本示例下预测体素级神经反应，而无需针对新受试者和刺激进行额外的微调。该方法利用可变数量上下文图像刺激的Transformer架构，学习多个受试者的归纳偏置。通过联合条件化图像特征和体素激活，模型直接生成性能更好的高等视觉皮层体素级模型。实验表明，BraInCoRL在低数据环境下优于现有体素级编码器设计，并在全新图像测试中表现出良好的扩展特性。此外，该模型还能推广到全新的视觉fMRI数据集，展示出对语义相关刺激的关注，提升了神经信号的可解释性，并实现了自然语言查询到体素选择性的可解释映射。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>WebDancer: Towards Autonomous Information Seeking Agency</title>
<link>https://arxiv.org/abs/2505.22648</link>
<guid>https://arxiv.org/abs/2505.22648</guid>
<content:encoded><![CDATA[
Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>大型语言模型在简体与繁体中文表现差异的研究</title>
<link>https://arxiv.org/abs/2505.22645</link>
<guid>https://arxiv.org/abs/2505.22645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，大型语言模型在简体与繁体中文中的表现存在显著差异。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在简体与繁体中文两种书写形式下的性能差异，这种差异可能影响模型在教育、招聘等领域的应用。为了验证这一假设，我们设计了两个反映现实场景的任务：区域术语选择和人名选择，并测试了11款主流商用及开源模型的表现。结果显示，大多数模型在术语选择任务中偏向简体中文，而在人名选择任务中则更倾向于传统中文。这些偏见可能源于训练数据分布、书写习惯以及中文字形分词方式的不同。我们的研究强调了进一步分析LLMs偏见的重要性，并提供了开源基准数据集以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:56:49 GMT</pubDate>
</item>
<item>
<title>克服强化学习中大规模语言模型推理障碍：熵管理的重要性</title>
<link>https://arxiv.org/abs/2505.22617</link>
<guid>https://arxiv.org/abs/2505.22617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了强化学习中策略熵崩溃的现象及其对性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）中与大型语言模型（LLMs）推理相关的重大障碍——策略熵崩溃现象。实验发现，在没有熵干预的情况下，策略熵在训练初期急剧下降，导致探索能力减弱并伴随策略表现饱和。通过建立熵与下游表现之间的经验关系式R=-a*e^H+b，表明策略性能是以熵为代价的，且最终受其耗尽限制。为了应对这一问题，我们理论与实证分析了熵动态变化机制，发现策略熵的变化由动作概率与logits变化间的协方差驱动，该协方差在优势策略算法中与优势成正比。实证研究表明，协方差项与熵差异完全匹配，支持理论推导。此外，协方差项在整个训练过程中保持正值，解释了策略熵为何单调递减。基于此机制的理解，我们提出两种简单而有效的技术：Clip-Cov和KL-Cov，分别通过剪裁和施加KL惩罚高协方差令牌来控制熵。实验表明，这些方法有助于增强探索能力，避免熵崩溃并提升下游性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:38:45 GMT</pubDate>
</item>
<item>
<title>通过视觉重建优化图像描述生成的RICO框架</title>
<link>https://arxiv.org/abs/2505.22613</link>
<guid>https://arxiv.org/abs/2505.22613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RICO框架，利用视觉重建提升图像描述准确性与完整性。</p><br /><br /><p><strong>摘要：</strong> 现有图像描述生成方法通常依赖多模态大语言模型（MLLMs），但因细节缺失易产生幻觉与不完整问题。为解决这些局限性，本文提出RICO框架，通过文本到图像模型重构描述并对比原图与重构图，迭代优化描述。此外，还引入RICO-Flash简化计算成本。实验表明，该方法在CapsBench和CompreCap上比多数基线提升了约10%，显著改善了描述的准确性和完整性。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:29:34 GMT</pubDate>
</item>
<item>
<title>Thinking with Generated Images</title>
<link>https://arxiv.org/abs/2505.22525</link>
<guid>https://arxiv.org/abs/2505.22525</guid>
<content:encoded><![CDATA[
We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:12:45 GMT</pubDate>
</item>
<item>
<title>ART+: 开创多层透明图像生成的新篇章</title>
<link>https://arxiv.org/abs/2505.22523</link>
<guid>https://arxiv.org/abs/2505.22523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个高质量多层透明图像数据集及生成模型，提升创意控制力。</p><br /><br /><p><strong>摘要：</strong> 现有的文本到图像生成模型主要集中在单层图像上，而多层透明图像生成因缺乏高质量的数据集发展滞后。本文通过发布首个开放的超高清透明图像数据集PrismLayersPro（包含20万张图像），设计无需训练的合成流水线以及开源多层生成模型ART+，解决了这一难题。技术贡献包括LayerFLUX用于生成高质量单层透明图像，MultiLayerFLUX将多层图像组合成完整图像，并通过过滤和人工筛选提高质量。ART+在用户研究中表现出色，优于原始ART模型，并达到现代单层生成模型的视觉水平。本研究为多层透明图像生成奠定了坚实基础，推动相关研究和应用发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:09:33 GMT</pubDate>
</item>
<item>
<title>基于GRPO的无监督多模态大语言模型后训练框架MM-UPT</title>
<link>https://arxiv.org/abs/2505.22453</link>
<guid>https://arxiv.org/abs/2505.22453</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需标注数据的多模态大语言模型后训练方法MM-UPT。</p><br /><br /><p><strong>摘要：</strong> 本文首次研究了使用稳定可扩展的在线强化学习算法GRPO实现多模态大语言模型（MLLMs）在无监督条件下的持续自我改进。传统方法依赖昂贵的手动标注数据或复杂迭代困难的策略，而我们提出的MM-UPT框架通过引入基于多数投票的自奖励机制替代传统的奖励信号，显著提升了Qwen2.5-VL-7B在MathVista和We-Math等基准测试上的推理能力，且无需人工标注。实验表明，该方法不仅优于先前的无监督基线，甚至接近有监督的GRPO效果。此外，利用模型自身生成的合成问题进一步提升性能，展示了无监督自我改进的潜力。MM-UPT为无外部监督条件下MLLMs的持续增强提供了一种新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22453" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 11:11:16 GMT</pubDate>
</item>
<item>
<title>Text2Grad：基于文本反馈的细粒度强化学习优化</title>
<link>https://arxiv.org/abs/2505.22338</link>
<guid>https://arxiv.org/abs/2505.22338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Text2Grad将自然语言反馈转化为梯度信号，实现模型参数的精确调整。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Text2Grad的新方法，该方法通过将自由形式的文本反馈转换为跨度级别的梯度信号，实现了对语言模型的细粒度优化。传统强化学习仅依赖粗略的标量奖励，导致学习过程缓慢且不透明。Text2Grad通过引入三个关键组件：高质量的反馈标注流水线、细粒度奖励模型以及跨度级别策略优化器，将文本反馈转化为可微分的奖励信号，并直接更新模型中出现问题的部分。这种方法不仅提高了任务性能，还增强了模型的可解释性。实验结果显示，Text2Grad在摘要生成、代码生成和问答任务上均优于传统的标量奖励强化学习和仅使用提示的方法。这一成果展示了自然语言反馈作为细粒度策略优化信号的强大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 09:23:49 GMT</pubDate>
</item>
<item>
<title>基于冷启动的强化学习提升多模态推理能力研究</title>
<link>https://arxiv.org/abs/2505.22334</link>
<guid>https://arxiv.org/abs/2505.22334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出结合监督微调与强化学习的两阶段方法，显著提升多模态语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，大型语言模型的推理能力得益于强化学习的进步，但多模态模型中的‘顿悟’模式可能并非强化学习带来的唯一结果。本研究首先发现这些模式在强化学习前已存在于多模态语言模型中，但未必与推理表现直接相关。基于此，我们设计了一种两阶段方法：先通过监督微调引入结构化的推理模式，再利用GRPO进行强化学习优化。实验结果显示，该方法在多个挑战性多模态推理基准测试中优于单一方法，其中7B规模模型在MathVista上提升了7.1分，在We-Math上提升了7.5分，而3B规模模型的性能也接近更大的7B模型。这项工作为构建先进的多模态推理模型提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 09:21:38 GMT</pubDate>
</item>
<item>
<title>Skywork-OR1：通过强化学习提升长链推理能力</title>
<link>https://arxiv.org/abs/2505.22312</link>
<guid>https://arxiv.org/abs/2505.22312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于DeepSeek-R1的Skywork-OR1模型，显著提高大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Skywork-OR1，一种针对长链思维（CoT）模型的有效且可扩展的强化学习（RL）实现方法。该方法基于DeepSeek-R1-Distill系列模型，显著提升了32B和7B模型的推理准确性，在AIME24、AIME25及LiveCodeBench等基准测试中分别实现了15.0%和13.9%的提升。Skywork-OR1-32B在AIME24和AIME25上优于DeepSeek-R1和Qwen3-32B，而Skywork-OR1-7B和Skywork-OR1-Math-7B也展现出与同类规模模型相当的推理能力。研究还探讨了熵坍塌现象及其对模型性能的影响，验证了缓解早期熵坍塌的重要性。为促进社区研究，所有模型权重、训练代码和数据集均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22312" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 08:56:04 GMT</pubDate>
</item>
<item>
<title>RenderFormer：基于Transformer的神经渲染方法</title>
<link>https://arxiv.org/abs/2505.21925</link>
<guid>https://arxiv.org/abs/2505.21925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RenderFormer是一种无需场景特定训练即可实现全局光照效果的神经渲染方法。</p><br /><br /><p><strong>摘要：</strong> RenderFormer是一种创新的神经渲染框架，可以直接从基于三角形的场景表示生成图像，同时支持完整的全局光照效果，且无需针对每个场景进行训练或微调。该方法将渲染过程视为序列到序列的转换任务，通过Transformer架构处理三角形反射属性序列并生成像素块序列。RenderFormer分为两个阶段：第一阶段建模视图无关的三角形间光线传输，第二阶段根据第一阶段的结果将光线束令牌转化为对应的像素值。两种阶段均采用Transformer架构构建，并仅依赖少量先验约束进行学习。我们展示了RenderFormer在不同复杂度的形状和光线传输场景中的表现与评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 23:20:46 GMT</pubDate>
</item>
<item>
<title>EPiC：一种高效精确的视频扩散模型相机控制框架</title>
<link>https://arxiv.org/abs/2505.21876</link>
<guid>https://arxiv.org/abs/2505.21876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EPiC框架，通过掩码源视频自动生成高质量锚视频，实现精准的3D相机控制。</p><br /><br /><p><strong>摘要：</strong> 近期视频扩散模型（VDMs）中的3D相机控制方法通常依赖于从估计的点云渲染锚视频作为结构化先验，但点云估计误差会导致锚视频不准确，且需要昂贵的相机轨迹标注。为解决这些问题，我们提出了EPiC框架，它通过基于第一帧可见性的掩码源视频自动构建高质量锚视频，无需昂贵的相机轨迹标注。该方法确保了高对齐性，并消除了对相机轨迹注释的需求，可应用于任何野外视频生成图像到视频（I2V）训练对。此外，引入轻量级的Anchor-ControlNet模块，将锚视频引导集成到预训练VDMs的可见区域中，仅占骨干模型参数的不到1%。通过结合提出的锚视频数据和ControlNet模块，EPiC实现了高效的训练，所需参数、训练步数和数据显著减少。尽管训练基于掩码的锚视频，我们的方法在推理时对基于点云的锚视频表现出稳健的泛化能力，实现了精确的3D感知相机控制。EPiC在RealEstate10K和MiraData上的I2V相机控制任务中达到了最先进的性能，并在定量和定性上展示了精确和鲁棒的相机控制能力，同时在零样本情况下对视频到视频场景也表现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:45:26 GMT</pubDate>
</item>
<item>
<title>Roads to Rome：高效结合大语言模型与小语言模型的方法</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种神经路由方法，在保持性能的同时大幅提升推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为 Roads to Rome (R2R) 的神经路由方法，旨在有效结合大型语言模型（LLMs）和小型语言模型（SLMs），既保持推理性能又提高效率。研究发现，LLMs 和 SLMs 的推理路径只有少数标记存在差异，因此 R2R 方法仅针对这些关键路径分歧的标记利用 LLMs，其余标记则由 SLM 处理。通过开发自动数据生成管道来标注这些路由标签，R2R 在数学、编码和问答基准测试中表现出色。当平均激活参数规模为 5.6B 时，R2R 超过了 R1-7B 平均准确率的 1.6 倍，甚至超过了 R1-14B 模型的表现。同时，它比 R1-32B 模型快 2.8 倍，且性能相当，推动了测试时间扩展效率的帕累托前沿。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:57:20 GMT</pubDate>
</item>
<item>
<title>SageAttention2++: A More Efficient Implementation of SageAttention2</title>
<link>https://arxiv.org/abs/2505.21136</link>
<guid>https://arxiv.org/abs/2505.21136</guid>
<content:encoded><![CDATA[
The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:50:36 GMT</pubDate>
</item>
<item>
<title>DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research</title>
<link>https://arxiv.org/abs/2505.19253</link>
<guid>https://arxiv.org/abs/2505.19253</guid>
<content:encoded><![CDATA[
Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 14:16:13 GMT</pubDate>
</item>
<item>
<title>PIR框架优化语言模型推理能力并减少计算开销</title>
<link>https://arxiv.org/abs/2505.19187</link>
<guid>https://arxiv.org/abs/2505.19187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PIR框架优化语言模型推理链条，提升精度同时降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过测试时扩展方法展示出色推理能力，尤其在经过链式思维（CoT）数据微调后表现突出。然而，这些推理链条通常包含冗长的人类解题元素，增加了推理过程中的计算负担。本文介绍PIR（基于困惑度的重要性精炼）框架，通过定量评估每一步推理对答案预测置信度的影响，系统性地识别并修剪低重要性的功能步骤，从而生成优化后的训练数据，既保留核心推理路径又减少了冗余。实验表明，基于PIR优化数据微调的模型在多个挑战性推理基准测试（如AIME、AMC和GPQA Diamond）中表现出更高的准确性，同时显著降低了推理所需令牌数量。该方法在不同模型规模、数据源和令牌预算下均展现出良好的泛化性能，为高效部署具备推理能力的LLMs提供了实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 11:17:57 GMT</pubDate>
</item>
<item>
<title>GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains</title>
<link>https://arxiv.org/abs/2505.18700</link>
<guid>https://arxiv.org/abs/2505.18700</guid>
<content:encoded><![CDATA[
Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 09:48:57 GMT</pubDate>
</item>
<item>
<title>DynToM：评估大型语言模型动态心智理论能力的新基准</title>
<link>https://arxiv.org/abs/2505.17663</link>
<guid>https://arxiv.org/abs/2505.17663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DynToM基准，评估大型语言模型对动态心理状态的理解和跟踪能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）越来越多地参与人机交互，评估其心智理论（ToM）能力变得至关重要。然而，现有的ToM评估基准主要关注静态心理状态，忽视了现实社交互动中的时间演变。为解决这一问题，本文提出了DynToM，这是一个专门设计用来评估LLMs理解并跟踪互联场景中心理状态时间进程能力的新基准。通过系统化的四步框架，我们生成了包含5500个情景和78100个问题的1100个社交背景，并对其真实性进行了验证。对十种最先进的LLMs的综合评估显示，其平均表现比人类低44.7%，且在跟踪和推理心理状态变化时性能显著下降。这种性能差距揭示了当前LLMs在建模人类心理状态动态性方面存在的根本局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:27:40 GMT</pubDate>
</item>
<item>
<title>构建HuggingKG知识图谱以促进开源机器学习资源管理</title>
<link>https://arxiv.org/abs/2505.17507</link>
<guid>https://arxiv.org/abs/2505.17507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开发首个基于Hugging Face社区的大规模知识图谱HuggingKG。</p><br /><br /><p><strong>摘要：</strong> 开源机器学习资源的快速增长推动了信息检索研究的发展，但现有平台如Hugging Face缺乏对结构化表示的明确利用，限制了高级查询和分析能力。为解决这一问题，我们构建了HuggingKG，这是首个由Hugging Face社区构建用于管理机器学习资源的大规模知识图谱，包含260万个节点和620万条边，涵盖了特定领域的关系及丰富的文本属性。基于此，我们进一步提出了HuggingBench，这是一个多任务基准测试，包含三个新的测试集合，用于信息检索任务中的资源推荐、分类和追踪。实验揭示了HuggingKG的独特特性及其衍生任务的特点。这两个资源均已公开可用，有望推动开源资源共享与管理的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:00:20 GMT</pubDate>
</item>
<item>
<title>Safe-Sora：首个嵌入式AI视频生成水印框架</title>
<link>https://arxiv.org/abs/2505.12667</link>
<guid>https://arxiv.org/abs/2505.12667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Safe-Sora框架，首次将图形水印嵌入视频生成过程。</p><br /><br /><p><strong>摘要：</strong> 随着生成式视频模型的快速发展，可靠保护AI生成内容版权的需求日益增加。尽管在图像合成领域已广泛应用不可见生成式水印技术，但在视频生成中的应用却鲜有探索。为填补这一空白，我们提出了Safe-Sora框架，这是首个直接在视频生成过程中嵌入图形水印的系统。该框架通过引入分层粗到细的自适应匹配机制，将水印图像分割成块并分配至视觉相似度最高的视频帧，同时定位最佳空间区域实现无缝嵌入。此外，通过开发基于三维小波变换的Mamba架构及新颖的空间时间局部扫描策略，实现了水印补丁在视频帧间的时间融合，有效建模了水印嵌入和检索中的长距离依赖关系。据我们所知，这是首次利用状态空间模型进行水印保护，开辟了高效且稳健水印保护的新途径。实验结果表明，Safe-Sora在视频质量、水印保真度和鲁棒性方面均达到当前最优水平。我们将在发布时公开代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 23:31:31 GMT</pubDate>
</item>
<item>
<title>Post Hoc Registers：无需重训的视觉Transformer补救方案</title>
<link>https://arxiv.org/abs/2505.21501</link>
<guid>https://arxiv.org/abs/2505.21501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效自蒸馏方法Post Hoc Registers，为现有ViT模型添加注册令牌以减少伪影。</p><br /><br /><p><strong>摘要：</strong> 视觉Transformer(ViTs)已成为视觉处理领域的主导架构，但存在与局部语义不一致的伪影令牌问题，影响细粒度定位和结构连贯性任务的表现。本文提出Post Hoc Registers(PH-Reg)，这是一种无需重新训练即可为大型预训练ViT模型添加注册令牌的高效自蒸馏方法。PH-Reg通过冻结教师网络并优化学生网络中的少量权重，利用教师网络生成去噪密集嵌入来指导学生网络学习，从而有效减少了伪影令牌的数量，提升了零样本和线性探测下的分割和深度预测性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>一种无需验证器的强化学习方法用于大规模语言模型训练</title>
<link>https://arxiv.org/abs/2505.21493</link>
<guid>https://arxiv.org/abs/2505.21493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种无需验证器的强化学习方法，显著提升了大规模语言模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于DeepSeek-R1-Zero风格的强化学习方法在大规模语言模型训练中的局限性，特别是其无法直接应用于化学、医疗等实际领域的问题。为解决这一限制，我们提出了Verifier-Free（VeriFree）方法，该方法通过直接优化生成参考答案的概率来替代传统的验证器机制。实验表明，VeriFree不仅在计算需求上有所减少，还在多个基准测试中表现出色，甚至超越了传统基于验证器的方法。此外，我们从多个角度分析了此方法的优势，包括统一模型中策略与隐式验证器的结合，以及作为变分优化方法的应用前景。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:27 GMT</pubDate>
</item>
<item>
<title>基于双重过程理论优化大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2505.21097</link>
<guid>https://arxiv.org/abs/2505.21097</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过引入四阶段任务改进大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，通过强化学习应用于数学和编程等领域的问答任务可以提升大型语言模型（LLMs）的推理能力。然而，长上下文长度下LLMs虽表现出搜索行为，但这种行为常缺乏精确性和信心，导致冗长且冗余的回答。受心理学中的双重过程理论启发，我们提出了一种简单修改的问答任务，包括快速思考、验证、慢速思考及总结四个阶段。此任务显著提升了Qwen2.5-1.5B和DeepSeek-R1-Qwen-1.5B的平均准确率，其中Qwen2.5-1.5B在快速思考模式下仅用不到1000个token就达到了26.8%的准确率，展示了显著的推理效率提升。这些发现表明直觉和深思熟虑的推理是两种互补且独立的系统，需要针对性训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21097" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:22:46 GMT</pubDate>
</item>
<item>
<title>通过渲染反馈强化学习提升可缩放矢量图形生成</title>
<link>https://arxiv.org/abs/2505.20793</link>
<guid>https://arxiv.org/abs/2505.20793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合渲染反馈的强化学习方法显著提升了视觉语言模型生成高质量SVG的能力。</p><br /><br /><p><strong>摘要：</strong> 可缩放矢量图形(SVG)因其代码可解释性成为视觉设计的重要格式，而近期的视觉语言模型(VLM)通过将SVG生成视为代码生成任务实现了高质量生成。然而，由于训练过程中缺乏对渲染图像的观察，现有VLM方法难以生成既忠实又高效的SVG。本文提出了一种名为RLRF的强化学习方法，利用渲染输出与原始输入之间的比较提供评估反馈，从而引导模型生成更精确、高效且语义连贯的SVG。实验表明，RLRF在性能上显著优于监督微调，有效解决了常见失败模式，同时增强了模型的结构化理解和泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 02:56:00 GMT</pubDate>
</item>
<item>
<title>FinTagging：首个全面的XBRL基准测试评估大型语言模型的财务报告结构化信息提取能力</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinTagging评估大型语言模型在XBRL财务报告中的信息提取与语义对齐能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FinTagging的新基准测试，它旨在评估大型语言模型（LLMs）在基于XBRL的财务报告中的结构化信息提取和语义对齐能力。与之前仅关注叙述文本且将XBRL标签简化为多分类问题的基准不同，FinTagging将XBRL标签问题分解为两个子任务：FinNI用于财务实体提取，FinCL用于基于税制的概念对齐。该基准测试要求模型同时处理非结构化文本和结构化表格中的事实提取，并将其与完整的1万多项US-GAAP税制进行对齐，从而实现现实且细致的评估。我们对多种LLMs进行了零样本设置下的评估，系统分析了其在子任务和整体标签准确性上的表现。结果表明，尽管LLMs在信息提取方面表现出强大的泛化能力，但在细粒度概念对齐上存在困难，特别是在区分密切相关的税制条目时。这些发现揭示了现有LLMs在完全自动化XBRL标签方面的局限性，并强调了改进语义推理和模式感知建模的需求，以满足精确财务披露的要求。相关代码可在GitHub仓库获取，数据则可在Hugging Face仓库找到。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 22:55:53 GMT</pubDate>
</item>
<item>
<title>MMPerspective：评估多模态大语言模型透视几何理解能力的新基准</title>
<link>https://arxiv.org/abs/2505.20426</link>
<guid>https://arxiv.org/abs/2505.20426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入MMPerspective，首个专门评估多模态大语言模型透视理解能力的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项名为MMPerspective的研究成果，该成果是首个专门设计用于系统性评估多模态大语言模型（MLLMs）对透视几何理解的基准。MMPerspective通过三个互补维度——透视感知、推理和鲁棒性——共10项精心设计的任务，涵盖了2,711个真实世界及合成图像实例和5,083个问题-答案对。这些任务旨在测试模型的关键能力，如消失点感知、透视类型推理、三维空间中的线关系理解等。通过对43个最先进的MLLMs进行综合评估，研究发现尽管模型在表面级感知任务上表现出色，但在组合推理和在扰动下保持空间一致性方面存在显著局限。此外，分析揭示了模型架构、规模与透视能力之间的有趣模式，强调了鲁棒性瓶颈和逐步提示的好处。MMPerspective为诊断和推进视觉-语言系统的空间理解提供了一个有价值的测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:20:22 GMT</pubDate>
</item>
<item>
<title>MotionPro：用于精确图像到视频运动控制的新型生成模型</title>
<link>https://arxiv.org/abs/2505.20287</link>
<guid>https://arxiv.org/abs/2505.20287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionPro，通过区域轨迹和运动掩码实现精细的运动合成控制。</p><br /><br /><p><strong>摘要：</strong> 动画图像与交互式运动控制在图像到视频（I2V）生成领域备受关注。现有方法通常依赖大高斯核扩展运动轨迹，但无法明确定义移动区域，导致粗略的运动控制且难以区分物体与相机的移动。为解决这些问题，我们提出了MotionPro，这是一种精确的运动控制器，创新性地利用区域轨迹和运动掩码来调节细粒度的运动合成并识别目标运动类别（即物体或相机移动）。MotionPro首先通过跟踪模型估计每个训练视频上的流图，然后采样区域轨迹以模拟推理场景。与通过大高斯核扩展流不同，我们的区域轨迹方法通过直接利用局部区域内的轨迹实现更精确的控制，从而有效表征细粒度运动。同时，运动掩码从预测的流图中导出，捕捉运动区域的整体动态。为了追求自然的运动控制，MotionPro进一步通过特征调制结合区域轨迹和运动掩码加强视频去噪。此外，我们精心构建了一个基准数据集MC-Bench，包含1100个用户注释的图像-轨迹对，用于评估细粒度和对象级别的I2V运动控制。在WebVid-10M和MC-Bench上的广泛实验验证了MotionPro的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>VLM-3R：一种基于视觉语言模型的统一三维重建框架</title>
<link>https://arxiv.org/abs/2505.20279</link>
<guid>https://arxiv.org/abs/2505.20279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合三维重建指令微调的视觉语言模型VLM-3R。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VLM-3R的统一框架，用于处理基于单目视频帧的三维场景理解任务。通过几何编码器生成隐式的三维标记，并利用空间-视觉-视图融合技术及大规模的三维重建指令微调问答对，VLM-3R实现了现实世界空间上下文与语言指令的有效对齐，支持单目三维空间辅助和具身推理。此外，为了评估时间推理能力，还提出了Vision-Spatial-Temporal Intelligence基准，包含五个专注于动态空间关系的任务。实验表明，该模型不仅在视觉空间推理方面表现稳健，还能理解时间维度上的三维情境变化，在准确性与可扩展性上均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:56:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型红队测试中的能力差距研究</title>
<link>https://arxiv.org/abs/2505.20162</link>
<guid>https://arxiv.org/abs/2505.20162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，攻击者与目标模型的能力差距决定了红队测试的有效性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的能力和自主性增强，通过红队测试识别漏洞变得至关重要。然而，当红队测试转变为弱到强的问题时，传统的提示工程方法可能失效。本研究通过能力差距视角重新定义红队测试，评估超过500对攻击者-目标模型组合的表现。结果显示，能力更强的模型更有效作为攻击者，攻击成功率在目标模型能力超过攻击者后急剧下降，并且攻击成功率与MMLU-Pro基准的社会科学部分表现呈正相关。基于这些趋势，我们推导出一个越狱规模法则，预测固定目标模型的攻击成功率。这些发现表明，固定能力的攻击者可能在未来模型中失去效用，开放源代码模型的增强风险需要被重视，模型提供方需准确衡量和控制模型的说服力和操控能力以限制其作为攻击者的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 12:05:41 GMT</pubDate>
</item>
<item>
<title>ScienceBoard：助力科学发现的大语言模型综合评估平台</title>
<link>https://arxiv.org/abs/2505.19897</link>
<guid>https://arxiv.org/abs/2505.19897</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScienceBoard提出多域环境与基准任务，评估当前大语言模型在科研中的表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）已超越自然语言处理领域，推动跨学科研究发展。特别是计算机交互型代理正在革新科学研究方式。本文介绍ScienceBoard，它由两个部分组成：一是具备动态科学工作流的多域仿真环境，集成专业软件并支持多接口交互；二是涵盖生物化学、天文学等领域的169项真实任务的人类验证基准。尽管最先进的模型如GPT-4o等在部分实验中表现良好，但整体成功率仅为15%，表明现有模型尚不能可靠辅助复杂科研任务。深入分析为改进模型设计提供了方向，ScienceBoard的代码、环境及基准已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19897" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 08:27:27 GMT</pubDate>
</item>
<item>
<title>CoreMatching：融合Token稀疏性和Neuron稀疏性的视觉语言模型高效推理框架</title>
<link>https://arxiv.org/abs/2505.19235</link>
<guid>https://arxiv.org/abs/2505.19235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Token稀疏性和Neuron稀疏性存在协同作用，并提出CoreMatching框架提升视觉语言模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型（VLMs）在多项任务中表现出色，但其高推理成本成为瓶颈。传统上，Token稀疏性和Neuron稀疏性被视为独立优化路径，然而本研究首次系统探索两者潜在交互。通过分析核心Neuron与核心Token之间的匹配机制，发现二者在推理过程中相互影响、彼此增强。基于此洞察，我们提出CoreMatching框架，通过协同适应性稀疏推理显著提升效率。实验表明，该方法在十项图像理解任务及三种硬件设备上超越现有最优模型，在NVIDIA Titan Xp上实现了5倍浮点运算减少和整体10倍加速。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19235" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:16:34 GMT</pubDate>
</item>
<item>
<title>SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.19094</link>
<guid>https://arxiv.org/abs/2505.19094</guid>
<content:encoded><![CDATA[
DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:11:06 GMT</pubDate>
</item>
<item>
<title>Guided by Gut：高效自引导测试时扩展框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.20325</link>
<guid>https://arxiv.org/abs/2505.20325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需外部验证器的自引导方法，使小规模模型实现与大规模模型相当的推理精度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Guided by Gut (GG)的高效自引导测试时扩展(TTS)框架，该框架通过仅依赖内部大型语言模型(LLM)信号(如token级置信度和步进新颖性)的轻量级树搜索实现与过程奖励模型(PRM)相当的性能，而无需昂贵的外部验证器模型。关键创新包括通过目标强化学习微调阶段提高内部置信估计的可靠性。实验表明，GG使参数量为1.5B的小型模型在数学推理基准上达到或超越参数量为32B-70B的大型模型的准确性，同时将GPU内存使用减少高达10倍。与基于PRM的方法相比，GG的推理速度提高了8倍，内存使用降低了4-5倍。此外，与最佳-of-N(BoN)策略相比，GG减少了约50%的KV缓存内存使用，使得TTS技术的部署更加高效和实用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20325" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:19:09 GMT</pubDate>
</item>
<item>
<title>BiomedSQL：评估科学推理的文本转SQL基准</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个用于评估生物医学领域文本转SQL生成中科学推理能力的基准BiomedSQL。</p><br /><br /><p><strong>摘要：</strong> 随着生物医学研究人员越来越多地依赖大规模结构化数据库进行复杂分析，现有文本到SQL系统在将定性科学问题转换为可执行SQL查询时面临挑战，尤其是在需要隐式领域推理的情况下。本文介绍了一个名为BiomedSQL的新基准，它专门设计用于评估真实世界生物医学知识库中的文本到SQL生成中的科学推理能力。BiomedSQL由68,000个基于整合了基因-疾病关联、组学数据因果推断及药物批准记录的大规模BigQuery知识库的问题/SQL查询/答案三元组组成。该基准要求模型推断特定领域的标准，而非单纯依赖语法翻译。我们测试了多种开源和闭源LLMs，并发现GPT-o3-mini和定制多步代理BMSQL分别达到了59.0%和62.6%的执行准确性，远低于90.0%的专家基线。这些结果表明BiomedSQL为提升支持科学发现的文本到SQL系统提供了新基础。该数据集和代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>VideoGameBench：评估视觉语言模型的人类技能</title>
<link>https://arxiv.org/abs/2505.18134</link>
<guid>https://arxiv.org/abs/2505.18134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示前沿视觉语言模型难以完成经典游戏。</p><br /><br /><p><strong>摘要：</strong> 本研究引入VideoGameBench，一个由10款90年代经典游戏组成的基准测试集，用于评估视觉语言模型（VLMs）在感知、导航和记忆管理等人类自然能力方面的表现。这些游戏仅提供原始视觉输入和高级目标描述，不依赖特定游戏的支持信息。实验表明，目前最先进的VLMs如Gemini 2.5 Pro，在实时交互中表现不佳，仅完成了极小比例的游戏内容。为解决推理延迟问题，还设计了暂停模式的VideoGameBench Lite，但性能提升有限。该基准旨在推动相关领域研究进展，探索模型在复杂任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:43:27 GMT</pubDate>
</item>
<item>
<title>CLUE：一种基于冲突与共识的语言模型不确定性解释框架</title>
<link>https://arxiv.org/abs/2505.17855</link>
<guid>https://arxiv.org/abs/2505.17855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLUE是首个通过自然语言解释语言模型预测不确定性的框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CLUE的新框架，用于生成语言模型预测不确定性的自然语言解释。CLUE通过无监督方式识别文本片段之间的主张-证据或证据间冲突与共识，这些因素驱动着模型的预测不确定性。随后，利用提示生成和注意力引导技术将这些关键交互转化为可读解释。实验表明，在三个语言模型和两个事实核查数据集上，CLUE生成的解释比未引导的不确定性解释更具忠实性和一致性。此外，人类评估者认为CLUE的解释更具有帮助性、信息量更大且逻辑上更一致。CLUE无需微调或架构改动，适用于任何白盒语言模型，有助于提高事实核查工作的实用性，并可推广到其他需要复杂推理的任务中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:06:43 GMT</pubDate>
</item>
<item>
<title>PreMoe：高效部署大规模Mixture-of-Experts模型的新框架</title>
<link>https://arxiv.org/abs/2505.17639</link>
<guid>https://arxiv.org/abs/2505.17639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PreMoe框架，通过专家剪枝和检索实现大规模MoE模型在内存受限环境下的高效部署。</p><br /><br /><p><strong>摘要：</strong> 本文研究了Mixture-of-Experts（MoE）架构中的专家激活模式，发现其具有显著的任务特定专业化特性。基于此，我们提出了PreMoe框架，它由概率专家剪枝（PEP）和任务适应性专家检索（TAER）两个主要组件构成。PEP利用任务条件期望选择评分（TCESS）量化专家的重要性，从而识别出任务关键专家的最小集合。而TAER则通过预计算和存储针对多样化任务的紧凑专家模式，在接收到用户查询时快速定位相关任务模式并仅加载必要专家，大幅降低内存占用。实验表明，DeepSeek-R1 671B和Pangu-Ultra-MoE 718B等大型MoE模型在经过不同程度的专家剪枝后仍保持较高的精度。该方法在云服务器到消费级设备的各种计算环境中均表现出色，且代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:59:16 GMT</pubDate>
</item>
<item>
<title>MMMG：面向多模态生成的人类对齐基准测试</title>
<link>https://arxiv.org/abs/2505.17613</link>
<guid>https://arxiv.org/abs/2505.17613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMMG基准测试，解决多模态生成自动评估难题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MMMG的综合基准测试，专门用于评估涉及图像、音频、文本与图像交织、文本与音频交织四种模态组合的多模态生成模型。该基准测试包含49项任务，其中29项为新开发任务，共涵盖937条指令，旨在系统性评估多模态生成模型的推理能力、可控性等关键特性。通过广泛的验证表明，MMMG与人类评价高度一致，平均一致性达到94.3%。对24种多模态生成模型的基准测试结果显示，尽管最先进的模型GPT Image在图像生成方面达到78.3%的准确率，但在多模态推理和交织生成任务上表现欠佳，同时音频生成领域仍有显著提升空间，为未来研究提供了重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:21:28 GMT</pubDate>
</item>
<item>
<title>SweEval：评估大语言模型在企业场景中的伦理对齐性</title>
<link>https://arxiv.org/abs/2505.17332</link>
<guid>https://arxiv.org/abs/2505.17332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SweEval通过模拟真实场景评估大语言模型是否能抵制不当指令。</p><br /><br /><p><strong>摘要：</strong> 随着企业越来越多地采用大型语言模型（LLMs）用于关键沟通任务，如撰写邮件、销售演示文稿等，如何确保这些模型在全球不同地区正确理解和适应文化及语言背景变得至关重要。企业应用中尤为关注的是降低声誉风险、维护信任并保证合规性，特别是在处理不安全或冒犯性语言时。为此，我们开发了SweEval基准测试，它通过模拟正负语气及正式非正式语境的变化来测试LLMs的表现。特别地，SweEval的提示会明确要求模型包含特定的脏话，以此评估模型遵守还是抗拒此类不当指示的能力，同时考察其在伦理框架、文化差异及语言理解方面的表现。为了推动构建符合道德规范的企业级AI系统，我们公开了该数据集及相关代码资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 18:56:58 GMT</pubDate>
</item>
<item>
<title>AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery</title>
<link>https://arxiv.org/abs/2505.21499</link>
<guid>https://arxiv.org/abs/2505.21499</guid>
<content:encoded><![CDATA[
Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>ExtAgents：一种多智能体框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.21471</link>
<guid>https://arxiv.org/abs/2505.21471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多智能体框架ExtAgents，有效扩展知识输入规模且不增加上下文长度。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）在推理与信息获取任务中的局限性展开研究。传统方法受限于模型上下文窗口大小，难以处理大规模外部知识输入，导致性能瓶颈。为解决这一问题，作者开发了名为ExtAgents的多智能体框架，通过分布式的知识同步与推理过程克服现有方法的信息丢失问题。该框架在多跳问答测试集$inftyBench+$及长文档生成等任务中表现优异，显著提升了非训练方法的性能，同时保持高效并行计算优势。未来研究可进一步优化多智能体协调机制，推动实际应用发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:45:04 GMT</pubDate>
</item>
<item>
<title>冻结的大语言模型通过两个嵌入实现多令牌生成</title>
<link>https://arxiv.org/abs/2505.21189</link>
<guid>https://arxiv.org/abs/2505.21189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，冻结的大语言模型无需自回归即可一次性生成数百个准确令牌。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）是否可以在不依赖自回归的情况下重构长文本。实验表明，当提供两个训练好的嵌入时，冻结的LLMs可以在单次前向传递中生成多达数百个准确的令牌，展示了这些模型一种令人惊讶且尚未充分探索的能力——即一次性生成多个令牌而无需迭代解码。此外，研究还分析了这些嵌入的行为，并揭示了它们所编码的信息类型。尽管这些表示并非对于给定文本唯一，但它们在嵌入空间中形成连接且局部的区域，这一特性暗示了学习专用编码器进入该空间的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:39:24 GMT</pubDate>
</item>
<item>
<title>ConciseR：通过强化学习实现大型语言模型的简洁推理</title>
<link>https://arxiv.org/abs/2505.21178</link>
<guid>https://arxiv.org/abs/2505.21178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段强化学习框架ConciseR，提升大型语言模型推理效率并减少冗余。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在推理能力上的发展，扩展生成长度成为研究热点。然而，过长且重复的推理链条（CoT）现象普遍存在。本文提出ConciseR，一种两阶段强化学习方法，第一阶段通过Group Relative Policy Optimization优化推理能力，第二阶段利用Length-aware Group Relative Policy Optimization提升表达简洁性。实验表明，ConciseR在多个基准测试中超越现有最先进的推理模型，同时生成更简洁的推理链条。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:29:51 GMT</pubDate>
</item>
<item>
<title>Alita：通过极简预定义与最大化自演化实现通用智能代理</title>
<link>https://arxiv.org/abs/2505.20286</link>
<guid>https://arxiv.org/abs/2505.20286</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种极简设计的通用智能代理Alita，显著提升跨领域适应性和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Alita的通用智能代理，其设计理念强调“简单即至高精妙”，通过极简的预定义组件与强大的自演化能力，实现了高效的开放式任务执行。Alita仅配备一个用于直接问题求解的基本组件，摒弃了传统复杂工具和工作流的设计，从而大幅提升了通用性和扩展性。此外，通过生成任务相关的模型上下文协议（MCPs），Alita能够自主构建和优化外部能力，进一步增强了其推理能力。在GAIA基准测试中，Alita取得了75.15%的pass@1和87.27%的pass@3的高准确率，分别在Mathvista和PathVQA上达到了74.00%和52.00%的pass@1成绩，展现出卓越的性能表现。未来更多细节将在GitHub项目页面更新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20286" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:53 GMT</pubDate>
</item>
<item>
<title>多任务预训练提升蛋白质语言模型性能</title>
<link>https://arxiv.org/abs/2505.20052</link>
<guid>https://arxiv.org/abs/2505.20052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多任务预训练策略显著提高蛋白质语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 蛋白质语言模型（PLMs）在检测蛋白质序列复杂模式方面表现出色，但单一预训练任务可能限制其信息捕捉能力。尽管添加数据模态或监督目标可提升模型表现，但通常仍集中于去噪处理。本研究开发了Ankh3模型，通过联合优化掩码语言建模和基于蛋白质序列的序列完成两项任务，证明PLMs可以从蛋白质序列中学习到更丰富且可泛化的表征。实验表明，在二级结构预测、荧光性、GB1适应性和接触预测等下游任务上，该模型表现优异，多任务集成使其对蛋白质特性有更全面的理解，从而实现更稳健和精确的预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:41:10 GMT</pubDate>
</item>
<item>
<title>基于蛋白质语言模型的蛋白质相互作用亲和力预测架构优化</title>
<link>https://arxiv.org/abs/2505.20036</link>
<guid>https://arxiv.org/abs/2505.20036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新的蛋白质相互作用数据集及架构，显著提升亲和力预测性能。</p><br /><br /><p><strong>摘要：</strong> 蛋白质-蛋白质相互作用(PPIs)对细胞过程至关重要，但利用蛋白质语言模型(PLMs)进行序列驱动的PPI结合亲和力预测仍面临挑战。本文通过构建高质量PPB-Affinity数据集并评估四种适应性架构(嵌入级联、序列级联、分层池化、池化注意力加法)，采用全微调及轻量级ConvBERT头方法，测试了多种领先PLMs。实验表明，分层池化和池化注意力加法架构优于传统方法，Spearman相关性提升高达12%，凸显复杂架构设计的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:23:08 GMT</pubDate>
</item>
<item>
<title>基于深度学习与大语言模型的神经退行性痴呆MRI诊断框架</title>
<link>https://arxiv.org/abs/2505.19954</link>
<guid>https://arxiv.org/abs/2505.19954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合文本报告生成与大语言模型的透明诊断框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对神经退行性痴呆的鉴别诊断难题，提出了一种融合模块化MRI文本报告生成与现代大语言模型（LLMs）的新框架。该框架通过将3D T1加权脑部MRI转换为放射学报告，并利用强化学习优化LLMs的诊断推理能力，从而在保持高预测性能的同时增强诊断透明度。不同于事后解释方法，该框架在推理过程中生成因果相关的诊断理由，支持临床医生的理解与决策。实验表明，该方法在诊断准确性上媲美现有深度学习方法，同时提供可解释的诊断依据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:18:32 GMT</pubDate>
</item>
<item>
<title>Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval</title>
<link>https://arxiv.org/abs/2505.19650</link>
<guid>https://arxiv.org/abs/2505.19650</guid>
<content:encoded><![CDATA[
Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 04:09:44 GMT</pubDate>
</item>
<item>
<title>SynLogic：通过逻辑推理数据增强大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2505.19641</link>
<guid>https://arxiv.org/abs/2505.19641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SynLogic框架，生成大规模逻辑推理数据，提升大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SynLogic的数据合成框架及相应数据集，该框架能够大规模生成多样化的逻辑推理数据，涵盖35种不同的逻辑推理任务。SynLogic的独特之处在于其可控性，可以调整数据难度和数量，且所有生成的数据均可以通过简单规则验证，非常适合强化学习中的可验证奖励机制。实验结果显示，基于SynLogic训练的模型在逻辑推理任务上达到领先水平，并显著提升了其他领域如数学和编程任务的训练效率和泛化能力。此外，将SynLogic数据与其他领域的任务数据混合训练，进一步提高了模型的综合表现。这些成果表明SynLogic是一个重要的资源，有助于推动大语言模型的广泛推理能力发展。SynLogic的数据合成管道及相关数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:59:36 GMT</pubDate>
</item>
<item>
<title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title>
<link>https://arxiv.org/abs/2505.19433</link>
<guid>https://arxiv.org/abs/2505.19433</guid>
<content:encoded><![CDATA[
Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:49:07 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型中的模态偏差研究</title>
<link>https://arxiv.org/abs/2505.18657</link>
<guid>https://arxiv.org/abs/2505.18657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨多模态大型语言模型中的模态偏差问题并提出缓解策略。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）在整合文本、图像等多种模态方面取得了显著进展。然而，这些模型深受模态偏差影响，往往过度依赖语言而忽视视觉等其他模态的信息。本文首先诊断了当前MLLMs中模态偏差的表现形式，接着提出了系统性的研究路线图，并分析了导致模态偏差的关键因素。通过实验验证，我们发现数据特性、主干网络能力不平衡以及训练目标设计不当均加剧了模态偏差现象。为解决这些问题，我们建议采用平衡的训练策略和优化模型架构，促进跨模态信息的有效融合。此外，我们呼吁跨学科合作以推动MLLMs领域的进一步发展，为实现通用人工智能提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 07:49:31 GMT</pubDate>
</item>
<item>
<title>AlphaMed：通过强化学习实现大型语言模型的医学推理能力</title>
<link>https://arxiv.org/abs/2505.17952</link>
<guid>https://arxiv.org/abs/2505.17952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaMed无需使用昂贵的有监督微调数据，在公共数据集上通过强化学习实现顶级医学问答性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为AlphaMed的医学领域大型语言模型（LLM），该模型展示了仅通过强化学习（RL）即可实现复杂推理能力的可能性。传统方法通常依赖于从闭源模型（如GPT-4o）蒸馏出的有监督链式思维（CoT）数据进行微调，而AlphaMed则完全避免了这一过程，而是利用基于简单规则的奖励机制，在公开的多选题问答数据集上训练。实验结果显示，AlphaMed在六个医学问答基准测试中取得了最先进的成果，甚至超过了规模更大的闭源模型，例如DeepSeek-V3-671B和Claude-3.5-Sonnet。为了深入理解这一成功的原因，我们围绕三个问题进行了系统分析：1）是否可以通过简单的规则激励推理而不依赖蒸馏的CoT监督？2）数据的数量和多样性如何影响推理能力？3）问题难度如何塑造推理的产生与泛化？研究发现，数据的信息量是推理表现的关键驱动因素，而基于多选题问答数据的极简主义强化学习方法在没有CoT监督的情况下也能有效诱导推理。此外，不同基准测试中的表现差异揭示了当前评估标准的局限性，强调了开发更具挑战性和推理导向的医学问答基准的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:27:37 GMT</pubDate>
</item>
<item>
<title>引入热带注意力机制以增强神经算法推理的鲁棒性</title>
<link>https://arxiv.org/abs/2505.17190</link>
<guid>https://arxiv.org/abs/2505.17190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出热带注意力机制，提升算法推理在分布外数据上的表现。</p><br /><br /><p><strong>摘要：</strong> 动态规划(DP)算法在组合优化问题中通过最大值、最小值及经典加法进行递归运算，其相关值函数对应于最大半环中的凸多面体。然而，现有的神经算法推理模型依赖于softmax归一化的点积注意力，这种平滑的指数加权方式会模糊这些尖锐的多面体结构，并在分布外(OOD)设置下失效。本文引入了一种新型的热带注意力机制，该机制原生运行于热带几何的最大半环中。我们证明了热带注意力可以近似动态规划类型的组合算法的热带电路。此外，实验表明，使用热带变换器在算法推理任务中增强了长度泛化和值泛化的经验OOD性能，同时在对抗攻击下保持稳定。我们的研究还提出了对抗攻击泛化作为神经算法推理基准测试的第三个维度。结果表明，热带注意力恢复了softmax所缺失的尖锐且尺度不变的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 14:01:25 GMT</pubDate>
</item>
<item>
<title>R1-Searcher++：一种高效的知识增强型大语言模型框架</title>
<link>https://arxiv.org/abs/2505.17005</link>
<guid>https://arxiv.org/abs/2505.17005</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合内部知识与外部检索的新框架，提升大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为R1-Searcher++的新框架，旨在通过结合大型语言模型（LLMs）的内部知识与外部信息源，解决现有检索增强生成（RAG）方法中存在的成本高、泛化性差等问题。该框架采用两阶段训练策略：首先进行初步格式学习的SFT冷启动阶段，随后进入强化学习阶段，用于动态知识获取。在强化学习阶段，通过结果监督促进探索，引入奖励机制以利用内部知识，并结合记忆机制持续吸收检索到的信息，从而丰富模型的内部知识库。实验结果显示，R1-Searcher++在多个任务上优于传统RAG及推理方法，同时实现了高效的检索增强推理。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17005" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:58:26 GMT</pubDate>
</item>
<item>
<title>检索增强生成中的位置偏差影响研究</title>
<link>https://arxiv.org/abs/2505.15561</link>
<guid>https://arxiv.org/abs/2505.15561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示位置偏差对LLM利用相关信息和抵御干扰信息的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了位置偏差如何影响检索增强生成（Retrieval Augmented Generation）中大型语言模型（LLM）利用相关文档的能力及受到干扰文档影响的程度。通过在三个基准测试上的大量实验表明，最先进的检索管道虽旨在检索相关文档，但往往会将高度干扰性的文档排在前10位，超过60%的查询包含至少一个高干扰性文档。尽管位置偏差在受控环境中被认为非常显著，但在实际场景中其影响较小，因为相关和干扰文档均被惩罚。此外，研究发现试图根据LLM位置偏好重新排列文档的复杂策略并不优于随机排序。这些发现揭示了位置偏差的实际影响及其应对策略的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 10:18:01 GMT</pubDate>
</item>
<item>
<title>NOVA基准测试：评估模型在罕见脑MRI异常检测中的泛化能力</title>
<link>https://arxiv.org/abs/2505.14064</link>
<guid>https://arxiv.org/abs/2505.14064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NOVA基准测试揭示了现有视觉语言模型在罕见脑MRI异常检测中的显著性能下降。</p><br /><br /><p><strong>摘要：</strong> 随着部署的模型越来越多地遇到训练数据之外的输入，如何有效进行分布外检测和开放世界识别成为重要研究方向。然而，现有的基准测试往往忽略了对真正罕见或未知条件的评估，导致模型的实际表现被高估。为了解决这一问题，本文提出了NOVA基准测试，该测试集包含了900个真实的脑部MRI扫描图像，涵盖了281种罕见病理类型及多样化的采集协议。每个案例不仅提供了详细的临床描述，还通过双盲专家标注了病灶区域。NOVA作为纯评估基准，从未用于模型训练，因此能够对模型的分布外泛化能力进行极限测试。实验结果显示，领先的视觉语言模型（如GPT-4o、Gemini 2.0 Flash和Qwen2.5-VL-72B）在NOVA上的表现大幅下滑，表明其在异常定位、视觉描述和诊断推理方面仍存在不足。NOVA为推动模型在未知异常检测方面的进步提供了一个严格的测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 04:10:57 GMT</pubDate>
</item>
<item>
<title>基于神经元分析的大语言模型多语言对齐研究</title>
<link>https://arxiv.org/abs/2505.21505</link>
<guid>https://arxiv.org/abs/2505.21505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法识别多语言神经元并分析大语言模型的多语言推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多语言对齐作为提升大型语言模型（LLMs）多语言能力的有效范式，同时揭示了LLMs中存在特定于语言的神经元。基于此，我们提出了一种新的细粒度神经元识别算法，用于检测语言神经元（包括特定语言神经元和相关语言神经元）及语言无关神经元。此外，通过分析不同类型的神经元分布特性，我们将LLMs的多语言推理过程分为四个部分：多语言理解、共享语义空间推理、多语言输出空间转换和词汇空间输出。研究还分析了对齐前后的模型特性，探讨了“自发多语言对齐”现象。整体而言，本研究基于多种神经元类型进行了全面调查，为更好地理解多语言对齐和LLMs的多语言能力提供了实证结果和有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21505" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>ViewSpatial-Bench：多视角空间定位评估基准</title>
<link>https://arxiv.org/abs/2505.21500</link>
<guid>https://arxiv.org/abs/2505.21500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有视觉语言模型在跨视角空间推理上表现不佳，新基准显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 当前视觉语言模型（VLMs）在处理视觉内容理解和推理方面表现出色，但在需要跨视角理解与空间推理的任务上仍面临重大挑战。研究发现，现有VLMs主要擅长基于相机视角的自中心空间推理，而在需要采用其他实体视角时则难以泛化。为解决这一问题，我们提出了ViewSpatial-Bench，这是一个专门设计用于多视角空间定位识别评估的综合基准，涵盖五类不同任务，并通过自动化的3D标注流水线生成精确的方向标签。对多种VLMs的全面评估显示，这些模型在相机视角任务上的表现尚可，但在人类视角任务中的准确性明显下降。通过对多视角空间数据集进行微调，整体任务性能提升了46.24%，证明了我们的方法的有效性。本研究为具身AI系统的空间智能建立了重要基准，并提供了实证证据表明建模3D空间关系可以增强VLMs的空间理解能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>UI-Genie：一种面向GUI代理的自提升框架</title>
<link>https://arxiv.org/abs/2505.21496</link>
<guid>https://arxiv.org/abs/2505.21496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-Genie通过奖励模型和自提升管道解决GUI代理中的两大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UI-Genie的自提升框架，旨在应对图形用户界面（GUI）代理中的两个关键挑战：轨迹结果验证困难及高质量训练数据难以扩展。为了解决第一个问题，UI-Genie引入了一个奖励模型UI-Genie-RM，该模型采用图像文本交织架构，高效处理历史上下文并统一动作级和任务级奖励。为了支持UI-Genie-RM的训练，开发了包括基于规则的验证、受控轨迹破坏和难负样本挖掘等故意设计的数据生成策略。针对第二个挑战，自提升管道通过奖励引导探索和动态环境中的结果验证逐步扩展可解决的复杂GUI任务。实验结果显示，UI-Genie在多个GUI代理基准测试中实现了最先进的性能，并展示了高质量的合成轨迹生成能力，无需人工标注。此外，研究团队开源了完整的框架实现和生成的数据集以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:58:06 GMT</pubDate>
</item>
<item>
<title>基于特征最优对齐的多模态大语言模型迁移对抗攻击方法</title>
<link>https://arxiv.org/abs/2505.21494</link>
<guid>https://arxiv.org/abs/2505.21494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的迁移对抗攻击方法FOA-Attack，显著提升多模态大语言模型的对抗迁移能力。</p><br /><br /><p><strong>摘要：</strong> 针对现有多模态大语言模型（MLLMs）对抗攻击方法中忽视局部信息的问题，本文提出了一种基于特征最优对齐的迁移对抗攻击方法FOA-Attack。该方法通过引入全局特征余弦相似度损失和局部聚类最优传输损失，分别优化粗粒度和细粒度特征对齐，同时结合动态集成模型加权策略增强跨模型的对抗迁移性能。实验表明，FOA-Attack在多种模型上优于当前最先进的方法，尤其在闭源MLLMs上的表现尤为突出。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:57 GMT</pubDate>
</item>
<item>
<title>DetailFlow：基于粗到细1D自回归的高效图像生成方法</title>
<link>https://arxiv.org/abs/2505.21473</link>
<guid>https://arxiv.org/abs/2505.21473</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型的图像生成方法DetailFlow，通过逐步细化细节显著提升生成效率和质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DetailFlow的粗到细1D自回归(AR)图像生成方法，该方法通过新颖的下一细节预测策略对图像进行建模。通过学习与逐步退化图像监督的分辨率感知标记序列，DetailFlow使生成过程能够从全局结构开始并逐步细化细节。这种方法不仅提供了更自然高效的AR模型生成复杂视觉内容的方式，还通过紧凑的1D AR模型实现了高质量的图像合成，所需标记数量远少于先前的方法如VAR/VQGAN。此外，我们进一步提出了带有自校正的并行推理机制，加速生成速度约8倍，并减少了教师强制监督固有的累积采样误差。在ImageNet 256x256基准测试中，DetailFlow以128个标记达到了2.96 gFID的成绩，优于需要更多标记的VAR和FlexVAR。同时，由于大幅减少的标记数和并行推理机制，DetailFlow的推理速度接近VAR和FlexVAR的两倍。广泛的实验结果证明了DetailFlow在生成质量和效率方面超越现有最先进的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21473" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:45:21 GMT</pubDate>
</item>
<item>
<title>HoliTom：一种高效的视频大语言模型推理优化框架</title>
<link>https://arxiv.org/abs/2505.21334</link>
<guid>https://arxiv.org/abs/2505.21334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HoliTom框架，结合内外部LLM剪枝策略，显著降低视频大语言模型计算成本。</p><br /><br /><p><strong>摘要：</strong> 当前视频大语言模型在视频理解方面表现出色，但因冗余视频token导致计算效率低下。现有token剪枝方法虽有所改进，但存在局限性。内LLM剪枝方法如FastV在浅层有额外开销，而外LLM剪枝主要关注局部空间或短时序冗余，未能充分利用视频全局时间动态特性。为解决此问题，我们提出HoliTom，这是一种无需训练的综合token合并框架。它通过全局冗余感知的时间分割进行外LLM剪枝，并结合空间-时间合并，将视觉token减少超90%，大幅减轻LLM计算负担。同时，引入基于内LLMtoken相似性的稳健合并方法，提升性能并兼容外LLM剪枝。实验显示，HoliTom在LLaVA-OneVision-7B上将计算成本降至FLOPs的6.9%，保持原性能的99.1%，并在推理时间及解码吞吐量上实现显著加速。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:28:45 GMT</pubDate>
</item>
<item>
<title>基于贝叶斯自适应框架的反思性探索强化学习算法</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种新算法BARL，优化语言模型在推理任务中的反射性探索。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过强化学习训练后展现出强大的推理能力及反射行为，但传统马尔可夫决策过程限制了探索范围且依赖当前状态，导致反射性推理是否能在训练期间出现尚不明确。为此，研究者将反射性探索引入贝叶斯自适应强化学习框架，该方法通过后验分布优化预期回报，同时激励奖励最大化和信息收集。所提出的BARL算法指导模型根据观察到的结果切换策略，显著提升了合成与数学推理任务中的测试表现，表现出更高的令牌效率和探索有效性。实验代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 18:51:00 GMT</pubDate>
</item>
<item>
<title>DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response</title>
<link>https://arxiv.org/abs/2505.19973</link>
<guid>https://arxiv.org/abs/2505.19973</guid>
<content:encoded><![CDATA[
Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:35:37 GMT</pubDate>
</item>
<item>
<title>基于全局绝对关节坐标的文本到运动生成模型</title>
<link>https://arxiv.org/abs/2505.19377</link>
<guid>https://arxiv.org/abs/2505.19377</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的文本到运动生成方法，使用全局绝对关节坐标代替传统相对表示。</p><br /><br /><p><strong>摘要：</strong> 现有的文本到运动生成模型通常采用HumanML3D推广的基于骨盆相对及帧间局部相对的运动表示方式，但这种方式对扩散模型存在局限性且不适用于下游任务。本文重新审视运动表示方法，提出使用全局空间中的绝对关节坐标作为替代方案。通过系统分析设计选择，我们发现此方法不仅提高了运动保真度和文本对齐效果，还增强了模型的可扩展性，即使使用简单的Transformer结构且无需辅助的运动感知损失函数。此外，我们的方法自然支持诸如文本驱动的运动控制和时间/空间编辑等下游任务，而无需额外的任务特定重构和昂贵的分类器引导生成。最后，我们在直接从文本生成SMPL-H网格顶点的运动方面展示了有前景的泛化能力，为未来的研究和相关应用奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19377" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 20:36:00 GMT</pubDate>
</item>
<item>
<title>ComfyMind：一种协作式通用生成AI系统</title>
<link>https://arxiv.org/abs/2505.17908</link>
<guid>https://arxiv.org/abs/2505.17908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ComfyMind通过引入语义工作流接口和搜索树规划机制，提升生成模型的稳定性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的快速发展，跨模态统一任务的通用生成方法受到广泛关注。然而，现有的开源框架由于缺乏结构化的工作流规划和执行级反馈，难以支持复杂的真实世界应用。为解决这些问题，本文介绍了一种名为ComfyMind的新系统，该系统基于ComfyUI平台构建，旨在实现稳健且可扩展的通用生成能力。ComfyMind提出了两项核心创新：语义工作流接口（SWI），它将底层节点图抽象为自然语言描述的功能模块，从而简化高阶组合并减少结构错误；搜索树规划机制结合局部反馈执行，将生成过程建模为分层决策过程，并允许在每个阶段进行自适应修正。这些组件共同提升了复杂生成工作流的稳定性和灵活性。我们在三个公开基准上评估了ComfyMind，结果显示其性能优于现有开源基线，并接近GPT-Image-1的表现。ComfyMind为开源通用生成AI系统的开发开辟了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:53:03 GMT</pubDate>
</item>
<item>
<title>挑战长推理链假设：高效推理语言模型的新方法</title>
<link>https://arxiv.org/abs/2505.17813</link>
<guid>https://arxiv.org/abs/2505.17813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现较短推理链比长链表现更好，提出新的推理方法short-m@k。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型(LLMs)在处理复杂推理任务时依赖扩展测试时间的计算资源，通过生成大量“思考”链条来实现，但这种方法成本高且耗时。本研究质疑长推理链是否真的带来更好的推理能力，通过实验表明较短推理链在许多情况下更为准确，最高可提升34.5%的正确率。基于此，我们提出了short-m@k方法，即并行执行k次独立生成，并在前m次推理完成后停止计算，最终答案由多数投票决定。实验显示，基本的short-1@k在低计算设置下性能与标准多数投票相当甚至更优，且使用的推理标记减少高达40%。而short-3@k虽然效率稍逊，但在所有计算预算下均优于多数投票，同时节省高达33%的墙钟时间。此外，通过对模型微调发现，训练时使用较短推理链也能显著提高性能。这些结果提示我们重新审视当前推理LLMs中的测试时间计算策略，强调较长推理并不总是带来更好效果，反而可能适得其反。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:29:06 GMT</pubDate>
</item>
<item>
<title>基于开放源代码大型语言模型的仓库级软件工程任务研究</title>
<link>https://arxiv.org/abs/2505.16901</link>
<guid>https://arxiv.org/abs/2505.16901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示开放源码LLM可高效完成仓库级软件工程任务。</p><br /><br /><p><strong>摘要：</strong> 近期大规模语言模型(LLMs)在函数级代码生成方面取得了显著进展，但在仓库级软件工程任务中的表现仍具挑战性。当前解决方案多依赖专有LLM代理，这不仅引入不可预测性，还限制了其可及性，并引发了关于数据隐私和模型定制化的担忧。本文探讨了开源LLM是否能够在不依赖代理的情况下有效解决仓库级任务。我们通过让LLM理解代码库中的函数和文件的语义信息及其结构依赖性，展示了这种可能性。为此，我们提出了代码图模型(CGMs)，将代码库的图结构整合到LLM的注意力机制中，并利用专门的适配器将节点属性映射到LLM的输入空间。结合无代理图检索增强框架，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，这一成绩在开源权重模型中排名第一，在开源系统方法中排名第二，总体排名第八，比之前的最佳开源模型方法高出12.33个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:00:55 GMT</pubDate>
</item>
<item>
<title>R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO</title>
<link>https://arxiv.org/abs/2505.16673</link>
<guid>https://arxiv.org/abs/2505.16673</guid>
<content:encoded><![CDATA[
In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 09:39:32 GMT</pubDate>
</item>
<item>
<title>CLEANMOL：提升大语言模型解析分子结构能力的新框架</title>
<link>https://arxiv.org/abs/2505.16340</link>
<guid>https://arxiv.org/abs/2505.16340</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEANMOL框架通过将SMILES解析分解为明确的任务，显著提升了大语言模型对分子结构的理解能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在科学发现，特别是分子科学领域展现出巨大潜力。然而，当前LLMs在理解分子结构（通常以SMILES表示）方面存在局限性，甚至无法完成基本任务如计数分子环。为此，我们提出了CLEANMOL，这是一种新框架，将SMILES解析转化为一系列清晰且确定性的任务，旨在促进分子图级别的理解。这些任务涵盖从子图匹配到全局图匹配，提供与分子结构属性一致的结构化监督。我们构建了一个具有自适应难度评分的分子预训练数据集，并在这些任务上对开源LLMs进行预训练。结果显示，CLEANMOL不仅增强了对分子结构的理解，还在Mol-Instructions基准测试中取得了最佳表现或与基线竞争。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16340" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 03:54:39 GMT</pubDate>
</item>
<item>
<title>AutoRefine：通过强化学习提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2505.11277</link>
<guid>https://arxiv.org/abs/2505.11277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的检索增强框架AutoRefine，显著提升复杂多跳推理任务的表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型虽然具备强大的推理能力，但受限于知识库容量。检索增强推理方法通过查询外部资源缓解这一限制，但常会检索到无关或噪声信息，影响推理准确性。本文提出AutoRefine，这是一种基于强化学习的后训练框架，采用“边搜索边优化”的新范式。AutoRefine在连续搜索调用之间引入显式的知识精炼步骤，使模型能够迭代地过滤、提炼和组织证据，从而生成更准确的答案。此外，我们利用分组相对策略优化，结合检索特定奖励和答案正确性奖励。实验表明，在单跳和多跳问答基准测试中，AutoRefine大幅超越现有方法，尤其在复杂的多跳推理场景中表现突出。进一步分析显示，AutoRefine能发出更高频率且质量更高的搜索请求，并有效合成证据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 10:11:29 GMT</pubDate>
</item>
<item>
<title>基于多智能体系统的学术海报自动生成方法</title>
<link>https://arxiv.org/abs/2505.21497</link>
<guid>https://arxiv.org/abs/2505.21497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉反馈的多智能体系统用于学术海报生成。</p><br /><br /><p><strong>摘要：</strong> 学术海报生成在科研交流中至关重要但极具挑战性，需要将复杂的文档压缩到单一且视觉连贯的页面上。本文首次构建了海报生成的基准测试集和指标套件，通过与人工设计的海报对比，评估生成海报的视觉质量、文本连贯性和整体美观度等多方面表现。此外，提出了名为PosterAgent的多智能体生成管道，包括解析器、规划器和绘图-评论循环三个阶段，显著提升了生成效率与质量。实验表明，尽管GPT-4生成的海报在视觉上吸引人，但存在文本噪声和信息传达不足的问题。而基于Qwen-2.5系列的开源版本在多个指标上优于现有系统，同时减少了87%的令牌消耗。该方法可将一篇22页论文转化为可编辑的.pptx海报，成本仅为0.005美元。研究为下一代全自动海报生成模型提供了明确方向。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:58:49 GMT</pubDate>
</item>
<item>
<title>基于帧进出技术的可控视频生成研究</title>
<link>https://arxiv.org/abs/2505.21491</link>
<guid>https://arxiv.org/abs/2505.21491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像到视频生成方法，通过帧进出技术实现对象自然进出场景。</p><br /><br /><p><strong>摘要：</strong> 视频生成中的可控性、时间一致性及细节合成是主要挑战。本文聚焦于电影制作中常用的但未被充分探索的技术——帧进出。该技术允许用户通过指定运动轨迹控制图像中的对象自然离开或进入场景。为此，我们构建了一个半自动标注的新数据集、针对此设置的综合评估协议，以及一种高效的保持身份且可控制运动的视频扩散Transformer架构。实验表明，所提出的方法显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的主动感知能力研究与ACTIVE-O3框架</title>
<link>https://arxiv.org/abs/2505.21457</link>
<guid>https://arxiv.org/abs/2505.21457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种强化学习驱动的ACTIVE-O3框架，使多模态大语言模型具备主动感知能力。</p><br /><br /><p><strong>摘要：</strong> 主动感知是高效感知与决策的关键组成部分，在人类及智能体中至关重要。尽管多模态大语言模型（MLLMs）在机器人系统中的应用受到关注，但其主动感知能力尚未得到充分探索。本文首次系统定义了基于MLLMs的主动感知任务，并指出GPT-o3的缩放搜索策略可视为主动感知的一种特殊情况，但存在效率低和区域选择不准确的问题。为此，我们提出了ACTIVE-O3，这是一种基于GRPO的纯强化学习训练框架，旨在赋予MLLMs主动感知能力。此外，我们构建了一个综合基准套件，评估ACTIVE-O3在开放世界任务和特定领域场景中的表现，包括遥感图像中的小目标检测、自动驾驶及精细交互分割等。ACTIVE-O3还在V*基准上展现了强大的零样本推理能力。我们希望此工作能为未来MLLMs领域的主动感知研究提供基础工具和评价协议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:29:31 GMT</pubDate>
</item>
<item>
<title>Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?</title>
<link>https://arxiv.org/abs/2505.21374</link>
<guid>https://arxiv.org/abs/2505.21374</guid>
<content:encoded><![CDATA[
Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:05:01 GMT</pubDate>
</item>
<item>
<title>MME-VideoOCR基准评测：视频OCR中的多模态大语言模型挑战</title>
<link>https://arxiv.org/abs/2505.21333</link>
<guid>https://arxiv.org/abs/2505.21333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有MLLMs在视频OCR中表现有限，需改进以应对复杂场景。</p><br /><br /><p><strong>摘要：</strong> 多模态大语言模型（MLLMs）在静态图像OCR中表现出色，但在视频OCR中因运动模糊、时间变化等因素表现显著下降。为指导实际应用，我们推出了MME-VideoOCR基准，涵盖25项任务、44种场景，涉及文本识别及深层次理解。该基准包含1464段视频和2000组人工标注问题答案对。评估结果显示，即便最优模型Gemini-2.5 Pro也仅达73.7%准确率。细粒度分析表明，模型在单帧或多帧文本任务上表现良好，但在需要整体视频理解的任务中能力受限，尤其在时空推理和跨帧信息整合方面表现不足。此外，高分辨率输入和充足时间覆盖对动态场景OCR至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:27:46 GMT</pubDate>
</item>
<item>
<title>MME-Reasoning：评估多模态大型语言模型逻辑推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.21327</link>
<guid>https://arxiv.org/abs/2505.21327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MME-Reasoning基准以全面评估多模态大型语言模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 逻辑推理是人类智能的基本方面，也是多模态大型语言模型（MLLMs）的重要能力。尽管在多模态推理方面取得了显著进展，但现有基准无法全面评估其推理能力，因为缺乏对推理类型的具体分类且推理理解不明确。为解决这些问题，我们引入了MME-Reasoning，这是一个综合性的基准，涵盖了推理问题中的三种推理类型（归纳、演绎和溯因）。通过精心策划数据，确保每个问题有效评估推理能力而非感知技能或知识广度，并扩展了评估协议以涵盖多样化问题的评估。我们的评估显示，最先进的MLLMs在全面评估逻辑推理能力时存在显著局限性。即使是最先进的MLLMs在全面逻辑推理方面的表现也有限，且在不同推理类型之间存在明显性能不平衡。此外，我们深入分析了“思维模式”和基于规则的强化学习等方法，这些方法通常被认为可以增强推理能力。这些发现揭示了当前MLLMs在多样化逻辑推理场景中的关键局限性和性能不平衡，为理解和评估推理能力提供了全面系统的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:23:23 GMT</pubDate>
</item>
<item>
<title>rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset</title>
<link>https://arxiv.org/abs/2505.21297</link>
<guid>https://arxiv.org/abs/2505.21297</guid>
<content:encoded><![CDATA[
Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:00:57 GMT</pubDate>
</item>
<item>
<title>Sci-Fi框架：实现起始帧与结束帧对中间帧的对称约束</title>
<link>https://arxiv.org/abs/2505.21205</link>
<guid>https://arxiv.org/abs/2505.21205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Sci-Fi框架解决现有方法中起始帧和结束帧控制强度不对称的问题。</p><br /><br /><p><strong>摘要：</strong> 当前最先进的帧间插值方法主要通过微调或省略训练来扩展大型预训练图像到视频扩散模型（I2V-DMs），但存在设计上的局限性：引入结束帧约束的方式与处理起始帧约束相同机制，导致结束帧影响较弱。本文提出Sci-Fi框架，采用改进机制引入结束帧约束，利用轻量级模块EF-Net增强结束帧的影响力，使起始帧和结束帧对中间帧的控制达到对称效果，从而生成更和谐的过渡效果。实验验证了Sci-Fi框架在多种场景下的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:53:50 GMT</pubDate>
</item>
<item>
<title>DualParal：基于扩散变换的分布式视频生成高效策略</title>
<link>https://arxiv.org/abs/2505.21070</link>
<guid>https://arxiv.org/abs/2505.21070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DualParal策略，大幅提升扩散Transformer视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于扩散Transformer的视频生成模型因处理长视频时的高延迟和高内存成本问题，提出了一种名为DualParal的新型分布式推理策略。该方法通过在多个GPU上并行化处理时间帧和模型层，有效缓解了这一限制。然而，由于扩散模型对帧间噪声同步的需求，直接实现并行化会导致串行化问题。为此，我们引入块级去噪方案，逐步降低噪声水平处理帧块，同时利用特征缓存减少GPU间的通信开销，采用协调噪声初始化策略保证全局一致性。实验表明，在8块RTX 4090 GPU上，该方法可将生成1025帧视频的延迟降低至原来的6.54倍，内存成本降低1.48倍，实现了高效、无瑕疵且无限长的视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 07:55:22 GMT</pubDate>
</item>
<item>
<title>OpenS2V-Nexus：推动Subject-to-Video生成的研究基础设施</title>
<link>https://arxiv.org/abs/2505.20292</link>
<guid>https://arxiv.org/abs/2505.20292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OpenS2V-Nexus，包括评估基准和大规模数据集，推动S2V视频生成研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为OpenS2V-Nexus的新框架，旨在建立Subject-to-Video（S2V）生成的基础架构。该框架由两个主要部分组成：OpenS2V-Eval，这是一个精细的基准测试集；以及OpenS2V-5M，一个百万规模的数据集。与现有的基于VBench的S2V基准相比，OpenS2V-Eval专注于模型生成符合主题一致性且外观和身份忠实的视频能力。为此，它引入了来自七个主要S2V类别的180个提示，其中包括真实和合成测试数据。此外，为了准确对齐人类偏好与S2V基准，我们提出了三种自动度量标准：NexusScore、NaturalScore和GmeScore，分别量化生成视频中的主题一致性、自然性和文本相关性。基于此，我们对16个代表性S2V模型进行了全面评估，揭示了它们在不同内容上的优缺点。此外，我们创建了第一个开源的大规模S2V生成数据集OpenS2V-5M，其中包括五百万高质量的720P主题-文本-视频三元组。通过OpenS2V-Nexus，我们提供了一个强大的基础设施，以加速未来的S2V生成研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>VisTA：基于强化学习的视觉工具动态选择框架</title>
<link>https://arxiv.org/abs/2505.20289</link>
<guid>https://arxiv.org/abs/2505.20289</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisTA通过强化学习实现视觉工具的选择与组合优化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VisTA的新框架，该框架利用端到端强化学习使视觉代理能够根据经验动态探索、选择和组合工具库中的工具。与现有方法相比，VisTA不需要训练自由提示或大规模微调，也不依赖于显式的推理监督。它通过组相对策略优化（GRPO）使代理自主发现有效的工具选择路径。实验表明，VisTA在ChartQA、Geometry3K和BlindTest基准测试中取得了显著性能提升，尤其是在分布外样本上。这些成果展示了VisTA在增强泛化能力、适应性利用多样工具以及推动灵活的经验驱动视觉推理系统方面的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20289" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>ImgEdit：面向复杂图像编辑的大规模高质量数据集与模型</title>
<link>https://arxiv.org/abs/2505.20275</link>
<guid>https://arxiv.org/abs/2505.20275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ImgEdit数据集和模型，提升开源图像编辑能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ImgEdit的大规模高质量图像编辑数据集，包含120万精心策划的编辑对，涵盖单步和多步复杂编辑任务。通过多阶段的数据处理流程确保数据质量，该数据集显著超越现有数据集。基于此数据集训练的ImgEdit-E1模型，在多个任务上优于现有开源模型。同时，我们设计了ImgEdit-Bench基准测试，用于评估指令遵循、编辑质量和细节保留能力。本研究还分析了开源和专有模型的行为，提供了深入见解。ImgEdit数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20275" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:53:33 GMT</pubDate>
</item>
<item>
<title>Granular Low-Rank Adaptation (GraLoRA): 提升参数高效微调性能的新方法</title>
<link>https://arxiv.org/abs/2505.20355</link>
<guid>https://arxiv.org/abs/2505.20355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GraLoRA通过引入子块结构克服LoRA的过拟合问题，显著提升生成模型微调效果。</p><br /><br /><p><strong>摘要：</strong> 低秩适配（LoRA）作为参数高效微调（PEFT）的一种流行方法，因其简单性和有效性受到关注。然而，LoRA存在根本性限制——当瓶颈被拓宽时容易发生过拟合，且在高秩情况下性能停滞甚至下降，难以媲美全量微调（FFT）。研究发现，LoRA的结构瓶颈导致梯度纠缠并扭曲传播。为解决这一问题，我们提出Granular Low-Rank Adaptation（GraLoRA），通过将权重矩阵划分为子块并为每个子块配备独立的低秩适配器，有效缓解过拟合，增强表示能力，并更接近FFT表现。实验表明，GraLoRA在代码生成和常识推理基准测试中显著优于LoRA及其他基线模型，在HumanEval+上的Pass@1得分提高达8.5%，且该优势在不同模型规模和秩设置下均成立，证明了其可扩展性和鲁棒性。相关代码、数据及脚本已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 02:48:20 GMT</pubDate>
</item>
<item>
<title>SoloSpeech：一种新颖的目标语音提取生成管道</title>
<link>https://arxiv.org/abs/2505.19314</link>
<guid>https://arxiv.org/abs/2505.19314</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法SoloSpeech，提升目标语音提取的自然度与准确性。</p><br /><br /><p><strong>摘要：</strong> 目标语音提取（TSE）旨在通过利用特定说话人的辅助音频分离混合语音中的目标语音。尽管近年来基于判别模型的方法在感知质量上表现优异，但容易引入伪影并降低自然度，且对训练与测试环境差异敏感。而生成模型则在感知质量和可懂度方面有所不足。为解决这些问题，我们提出了SoloSpeech，这是一种新颖的级联生成管道，集成了压缩、提取、重建和校正过程。该方法采用无需说话人嵌入的目标提取器，并利用辅助音频潜在空间的条件信息与混合音频潜在空间对齐，避免不匹配问题。实验结果显示，SoloSpeech在Libri2Mix数据集上的目标语音提取和语音分离任务中达到了新的最优性能，在域外数据和真实场景中也表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19314" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 17:00:48 GMT</pubDate>
</item>
<item>
<title>SeePhys：基于物理学问题的大规模多模态基准测试</title>
<link>https://arxiv.org/abs/2505.19099</link>
<guid>https://arxiv.org/abs/2505.19099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeePhys测试显示当前大型语言模型在视觉理解方面存在重大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SeePhys，这是一个针对从中学到博士资格考试的物理问题设计的大规模多模态基准测试。该基准涵盖了物理学的七个基础领域，包括21类高度异构的图表。不同于以往研究中视觉元素仅起辅助作用的情况，本基准中有75%的问题需要依赖视觉信息提取才能获得正确答案。通过广泛的评估发现，即使是最先进的视觉推理模型也只能达到不到60%的准确率。这些结果揭示了当前大型语言模型在视觉理解能力上的根本性挑战，特别是如何在图表解释与物理推理之间建立严格的联系，以及如何克服对文本线索的认知捷径的持续依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:28:34 GMT</pubDate>
</item>
<item>
<title>基于验证器引导迭代优化的视频大语言模型强化学习方法</title>
<link>https://arxiv.org/abs/2505.19000</link>
<guid>https://arxiv.org/abs/2505.19000</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VerIPO方法，提升视频大语言模型复杂推理链生成能力。</p><br /><br /><p><strong>摘要：</strong> 当前基于强化学习微调的方法在视频大语言模型的复杂推理训练中面临数据准备瓶颈及性能不稳定的问题。为解决这些挑战，本文提出VerIPO（验证器引导迭代策略优化），通过引入Rollout-Aware Verifier，在GRPO与DPO阶段间形成循环训练框架，利用小型语言模型评估推理逻辑，构建高质量对比数据，显著提高推理链质量和优化效率。实验表明，VerIPO相比传统GRPO方法优化速度提升7倍，推理链长度与一致性明显改善，并在多种视频推理任务中优于大规模指令微调模型和长推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19000" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 02:41:28 GMT</pubDate>
</item>
<item>
<title>MetaMind：通过多智能体框架实现类人社会推理</title>
<link>https://arxiv.org/abs/2505.18943</link>
<guid>https://arxiv.org/abs/2505.18943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MetaMind框架提升语言模型在社会推理上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MetaMind的多智能体框架，该框架旨在通过模仿元认知的心理学理论来实现类人社会推理能力。MetaMind将社会理解分解为三个协作阶段：理论思维代理生成假设、领域代理细化假设以及响应代理生成适当的回应。实验结果显示，MetaMind在三个具有挑战性的基准测试中取得了最先进的性能，在现实世界的社会场景中提升了35.7%，在理论思维推理方面提高了6.2%。此外，消融研究验证了所有组件的必要性，展示了框架在上下文合理性、社交适宜性和用户适应性方面的平衡能力。这项工作推动了人工智能系统向人类水平的社会智能发展，适用于共情对话和文化敏感交互等应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 22:32:57 GMT</pubDate>
</item>
<item>
<title>SVG2：通过语义感知聚类优化扩散Transformer视频生成效率</title>
<link>https://arxiv.org/abs/2505.18875</link>
<guid>https://arxiv.org/abs/2505.18875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVG2通过语义感知聚类和动态预算控制，显著提升扩散Transformer视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 扩散Transformer (DiTs) 在视频生成中因注意力机制的二次复杂性而面临显著延迟问题。尽管稀疏注意力方法试图通过计算关键令牌来降低成本，但现有方法在相同计算预算下无法达到最佳生成质量。本文提出SVG2，一种无需训练的框架，通过语义感知的排列方式对令牌进行聚类和重新排序，同时结合top-p动态预算控制和定制化内核实现，实现了生成质量和效率之间的帕累托前沿权衡。实验表明，SVG2在HunyuanVideo和Wan 2.1数据集上分别实现了高达2.30倍和1.89倍的速度提升，同时保持了高PSNR值。这项研究解决了现有方法在关键令牌识别不精确和计算浪费方面的不足，为高效视频生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 17:30:29 GMT</pubDate>
</item>
<item>
<title>OmniConsistency：弥合扩散模型风格化一致性差距的通用插件</title>
<link>https://arxiv.org/abs/2505.18445</link>
<guid>https://arxiv.org/abs/2505.18445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniConsistency插件提升扩散模型风格化一致性。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在图像风格化方面取得了显著进展，但复杂场景中的一致性保持及风格退化问题仍未解决。GPT-4o展示了性能优势，本文提出的OmniConsistency通过引入大规模Diffusion Transformer实现一致性的显著增强。该方法包括基于对齐图像对的上下文一致性学习框架、分阶段渐进学习策略以及与任意风格LoRA兼容的插件设计，实验表明其视觉连贯性和美学质量接近商业领先模型GPT-4o。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 21:00:20 GMT</pubDate>
</item>
<item>
<title>通过Steering Target Atoms实现语言模型精确控制</title>
<link>https://arxiv.org/abs/2505.20322</link>
<guid>https://arxiv.org/abs/2505.20322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Steering Target Atoms，提升语言模型的安全性和精确性。</p><br /><br /><p><strong>摘要：</strong> 当前对大型语言模型的控制主要依赖提示工程和引导技术，但高参数数量导致内部表示高度交织，限制了控制精度并可能引发意外后果。近期研究尝试利用稀疏自动编码器解耦知识以实现引导，然而由于定位原子知识组件的难度，这些应用仅限于简单任务。本文提出Steering Target Atoms方法，隔离并操控解耦的知识组件以增强安全性。实验表明该方法有效，且在对抗场景中展现出优越的鲁棒性和灵活性。此外，将此引导策略应用于大规模推理模型验证了其在精确推理控制中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:59:18 GMT</pubDate>
</item>
<item>
<title>MMMR基准：评估多模态大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2505.16459</link>
<guid>https://arxiv.org/abs/2505.16459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准MMMR以评估多模态推理并揭示现有模型的问题。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）的发展使得跨语言、视觉和结构化输入的统一处理成为可能，但其推理能力尤其是具有中间思考痕迹的模型（MLLMs-T）尚未被充分理解。现有研究主要关注感知或最终答案的准确性，对模型如何跨模态推理或失败缺乏深入洞察。为此，我们引入了MMMR，这是一个新的基准，旨在通过显式思考严格评估多模态推理。MMMR包含一个由六个多样化推理类型组成的高难度数据集，以及一个模块化的推理轨迹评估管道（RTEP），用于评估推理质量。实验结果显示，尽管MLLMs-T总体上优于非思考模型，但顶级模型如Claude-3.7-Sonnet和Gemini-2.5 Pro仍存在不一致性及过度推理等问题。该基准揭示了准确率与推理质量之间的持续差距，并为未来模型开发提供了可操作的评估管道。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:41:55 GMT</pubDate>
</item>
<item>
<title>EgoZero: Robot Learning from Smart Glasses</title>
<link>https://arxiv.org/abs/2505.20290</link>
<guid>https://arxiv.org/abs/2505.20290</guid>
<content:encoded><![CDATA[
Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, and zero robot data. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2505.20225</link>
<guid>https://arxiv.org/abs/2505.20225</guid>
<content:encoded><![CDATA[
Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:06:25 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的列表式推理重排序代理REARANK</title>
<link>https://arxiv.org/abs/2505.20046</link>
<guid>https://arxiv.org/abs/2505.20046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REARANK通过强化学习和数据增强显著提升检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了REARANK，一种基于大型语言模型（LLM）的列表式推理重排序代理。REARANK在重排序前进行显式推理，不仅提升了性能，还增强了可解释性。通过强化学习和数据增强技术，REARANK在多个流行的信息检索基准测试中表现优异，仅需179个标注样本即可超越基线模型。基于Qwen2.5-7B构建的REARANK-7B，在域内和域外基准测试中的表现可媲美GPT-4，甚至在推理密集型BRIGHT基准上超过GPT-4。这些成果验证了该方法的有效性，并展示了如何利用强化学习提升LLM的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:31:48 GMT</pubDate>
</item>
<item>
<title>大型语言模型中可解释分类特征的涌现特性研究</title>
<link>https://arxiv.org/abs/2505.19440</link>
<guid>https://arxiv.org/abs/2505.19440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了大型语言模型中分类特征在时间、空间及规模上的涌现规律。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中可解释分类特征的出现机制，通过分析训练过程中的多个时间点、变压器网络的不同层以及不同规模模型的行为，利用稀疏自动编码器进行机制解释性研究。研究发现，在多个领域内，特征的出现存在明确的时间和规模阈值。此外，空间分析显示早期层特征会在后期层重新激活，这一现象挑战了关于变压器模型表征动态的传统假设。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:59:54 GMT</pubDate>
</item>
<item>
<title>MMIG-Bench：多模态图像生成综合基准测试</title>
<link>https://arxiv.org/abs/2505.19415</link>
<guid>https://arxiv.org/abs/2505.19415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMIG-Bench，统一文本到图像生成及图像编辑评估。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态图像生成模型虽在复杂指令处理和概念一致性维护方面表现出色，但缺乏统一的评估标准。本文提出了MMIG-Bench，该基准通过结合4850个带注释的文本提示与1750个多视角参考图像，涵盖380个主题，包括人、动物、物体及艺术风格。MMIG-Bench具备三级评估框架，包括低级视觉指标、Aspect Matching Score（AMS）中期指标及高级美学评价。通过对17个顶级模型的评估，验证了新指标的有效性，并提供了深入见解。研究将公开数据集和评估代码，推动多模态图像生成领域的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:07:24 GMT</pubDate>
</item>
<item>
<title>利用物理力作为视频生成控制信号的研究</title>
<link>https://arxiv.org/abs/2505.19386</link>
<guid>https://arxiv.org/abs/2505.19386</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出力提示方法，通过点力和风场实现对图像的物理交互。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成模型的进步激发了对世界模型的兴趣，这类模型可以模拟逼真的环境。然而，与导航相比，模仿真实世界力的物理意义交互仍较少被研究。本研究探讨将物理力作为视频生成的控制信号，并提出了力提示方法，使用户可以通过局部点力（如戳植物）和全局风场（如风吹动布料）与图像进行交互。实验表明，力提示可以利用原始预训练模型中的视觉和运动先验，使视频对物理控制信号做出真实的响应，而无需使用3D资产或物理模拟器。研究面临的挑战是如何获取高质量的配对力-视频训练数据，尤其是在现实世界中难以获得力信号的情况下。研究发现，即使在有限演示的情况下，视频生成模型也能很好地推广到Blender合成视频的物理力条件上。这种方法能够在多样化几何形状、场景和材料上模拟力，且仅需在四块A100 GPU上训练一天约15k个训练样本，便在力一致性与物理真实性方面超越现有方法。我们将在项目页面发布所有数据集、代码、权重及交互式视频演示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19386" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 21:04:02 GMT</pubDate>
</item>
<item>
<title>WHISTRESS: Enriching Transcriptions with Sentence Stress Detection</title>
<link>https://arxiv.org/abs/2505.19103</link>
<guid>https://arxiv.org/abs/2505.19103</guid>
<content:encoded><![CDATA[
Spoken language conveys meaning not only through words but also through intonation, emotion, and emphasis. Sentence stress, the emphasis placed on specific words within a sentence, is crucial for conveying speaker intent and has been extensively studied in linguistics. In this work, we introduce WHISTRESS, an alignment-free approach for enhancing transcription systems with sentence stress detection. To support this task, we propose TINYSTRESS-15K, a scalable, synthetic training data for the task of sentence stress detection which resulted from a fully automated dataset creation process. We train WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive baselines. Our results show that WHISTRESS outperforms existing methods while requiring no additional input priors during training or inference. Notably, despite being trained on synthetic data, WHISTRESS demonstrates strong zero-shot generalization across diverse benchmarks. Project page: https://pages.cs.huji.ac.il/adiyoss-lab/whistress.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:45:08 GMT</pubDate>
</item>
<item>
<title>基于神经物理系统的实时交互流体模拟</title>
<link>https://arxiv.org/abs/2505.18926</link>
<guid>https://arxiv.org/abs/2505.18926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合神经网络和经典数值求解器的流体模拟系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对实时交互流体模拟的神经物理系统。传统基于物理的方法虽然精确但计算成本高且存在延迟问题，而机器学习方法虽降低了计算开销但仍难以满足低延迟需求并缺乏互动支持。为解决这一问题，我们引入了一种新颖的混合方法，该方法融合了数值模拟、神经物理及生成控制技术。通过采用回退保护机制，我们的神经物理系统能够在保证物理真实性的前提下实现低延迟模拟。此外，我们还开发了一种基于扩散的控制器，利用反向建模策略生成用于流体操作的外部动态力场。实验表明，该系统在多种二维和三维场景中表现出色，能够以高帧率（11%-29%延迟）实现真实的实时模拟，并允许用户通过友好的自由手绘进行流体控制。此研究标志着向实用、可控且物理逼真的实时交互流体模拟迈出了重要一步。我们承诺在论文被接受后公开模型和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 21:27:18 GMT</pubDate>
</item>
<item>
<title>基于强化学习的混合推理策略优化提升大语言模型的隐式推理能力</title>
<link>https://arxiv.org/abs/2505.18454</link>
<guid>https://arxiv.org/abs/2505.18454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HRPO方法，通过强化学习整合离散与连续表示，提升大语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，隐式推理作为大型语言模型（LLMs）的一种新范式，相比传统的自回归推理，能利用更丰富的隐藏状态特征。然而，隐式推理方法常与LLMs不兼容，且依赖于显式的链式思维（CoT）路径进行训练。本文提出了一种基于强化学习的混合推理策略优化（HRPO），该方法通过可学习门机制将先前的隐藏状态融入采样标记，并在训练初期主要使用标记嵌入，逐步引入更多隐藏特征，从而保持LLMs的生成能力并激励混合推理。此外，HRPO通过标记采样引入隐式推理中的随机性，使基于强化学习的优化无需依赖CoT轨迹。在多项基准测试中，HRPO在知识密集型和推理密集型任务中均超越现有方法，同时保持模型的可解释性，展现出跨语言模式和较短完成长度等有趣特性，为未来隐式推理研究提供了启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 21:26:16 GMT</pubDate>
</item>
<item>
<title>InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning</title>
<link>https://arxiv.org/abs/2505.18291</link>
<guid>https://arxiv.org/abs/2505.18291</guid>
<content:encoded><![CDATA[
Large multimodal foundation models, particularly in the domains of language and vision, have significantly advanced various tasks, including robotics, autonomous driving, information retrieval, and grounding. However, many of these models perceive objects as indivisible, overlooking the components that constitute them. Understanding these components and their associated affordances provides valuable insights into an object's functionality, which is fundamental for performing a wide range of tasks. In this work, we introduce a novel real-world benchmark, InstructPart, comprising hand-labeled part segmentation annotations and task-oriented instructions to evaluate the performance of current models in understanding and executing part-level tasks within everyday contexts. Through our experiments, we demonstrate that task-oriented part segmentation remains a challenging problem, even for state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark, we introduce a simple baseline that achieves a twofold performance improvement through fine-tuning with our dataset. With our dataset and benchmark, we aim to facilitate research on task-oriented part segmentation and enhance the applicability of VLMs across various domains, including robotics, virtual reality, information retrieval, and other related fields. Project website: https://zifuwan.github.io/InstructPart/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:36:13 GMT</pubDate>
</item>
<item>
<title>负反馈驱动的监督学习提升大模型数学推理能力</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">负反馈感知微调方法通过利用负样本改进大模型，效果媲美强化学习。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了强化学习在基于二元验证信号提升大语言模型数学能力上的主导地位，提出了一种名为负反馈感知微调（NFT）的监督学习方法。NFT通过构建隐式的负向策略来处理自动生成的负样本，从而让模型能够反思错误并自主改进。实验表明，在数学推理任务中，NFT显著优于传统的监督学习基线方法，并且在某些情况下可与领先的强化学习算法相媲美甚至超越。此外，理论分析揭示了NFT与强化学习中的GRPO方法在严格按策略训练时的等价性，这为监督学习和强化学习之间的联系提供了新的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:17:40 GMT</pubDate>
</item>
<item>
<title>统一微调方法（UFT）提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.16984</link>
<guid>https://arxiv.org/abs/2505.16984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型微调范式UFT，综合监督和强化微调优势。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）的后训练方法主要分为监督微调（SFT）和强化微调（RFT）。SFT高效但可能过拟合，而RFT虽泛化性更好但依赖基础模型。为克服这些局限，我们提出统一微调（UFT），将两者整合成单一过程，不仅在所有模型规模上优于SFT和RFT，还理论上突破了RFT在长时推理任务中的样本复杂度瓶颈，首次证明了统一训练可显著加速收敛。这项研究展示了后训练技术的新方向，对提高模型推理能力具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:53:57 GMT</pubDate>
</item>
<item>
<title>大型语言模型中基于推理的段落重排序器的表现研究</title>
<link>https://arxiv.org/abs/2505.16886</link>
<guid>https://arxiv.org/abs/2505.16886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示基于推理的段落重排序器表现不如非推理版本。</p><br /><br /><p><strong>摘要：</strong> 随着复杂自然语言任务中推理模型的成功应用，信息检索领域开始探索如何将类似的能力整合到基于大型语言模型构建的段落重排序器中。通常，这些方法利用大型语言模型生成逐步推理过程，然后得出最终的相关性预测。然而，推理是否真的能提高重排序准确性？本文深入探讨了这一问题，在相同训练条件下对比了基于推理的点式重排序器（ReasonRR）与标准非推理点式重排序器（StandardRR），发现StandardRR普遍优于ReasonRR。进一步研究发现，即使禁用ReasonRR的推理过程（ReasonRR-NoReason），其性能仍优于带有推理功能的版本。分析表明，基于推理的重排序器受限于大型语言模型的推理过程，倾向于产生极端的相关性评分，未能充分考虑部分相关性，这是点式重排序器准确性的重要因素之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:41:37 GMT</pubDate>
</item>
<item>
<title>TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification</title>
<link>https://arxiv.org/abs/2505.18283</link>
<guid>https://arxiv.org/abs/2505.18283</guid>
<content:encoded><![CDATA[
Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at https://github.com/JianghaoWu/TAGS.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:28:59 GMT</pubDate>
</item>
<item>
<title>STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15804</link>
<guid>https://arxiv.org/abs/2505.15804</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:57:38 GMT</pubDate>
</item>
<item>
<title>ModernGBERT：基于德语的透明高性能编码器模型</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ModernGBERT及LL"aMmlein2Vec两种编码器模型，评估其在自然语言理解等任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ModernGBERT（134M, 1B），一种全新的德语编码器模型家族，从零开始训练并融合了ModernBERT的架构创新。同时，还提出了通过LLM2Vec方法从德语解码器模型转换而来的LL"aMmlein2Vec（120M, 1B, 7B）编码器家族。研究对这些模型在自然语言理解、文本嵌入及长上下文推理任务上进行基准测试，以比较专用编码器与转换后的解码器之间的性能差异。结果显示，ModernGBERT 1B在性能和参数效率方面优于先前最先进的德语编码器以及通过LLM2Vec改编的编码器。所有模型、训练数据、检查点和代码均公开可用，推动了德语NLP生态系统的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:07:20 GMT</pubDate>
</item>
<item>
<title>基于选项感知的时间抽象值学习的离线目标条件强化学习</title>
<link>https://arxiv.org/abs/2505.12737</link>
<guid>https://arxiv.org/abs/2505.12737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法OTA，通过时间抽象改进高策略学习，显著提升离线目标条件强化学习在长时序任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了离线目标条件强化学习（GCRL）在处理长时序任务时面临的挑战，特别是现有方法如HIQL存在的性能瓶颈问题。研究发现，主要障碍源于高层策略难以生成合适的子目标，以及在长时序环境中优势信号方向不正确的现象。为解决这些问题，我们提出了Option-aware Temporally Abstracted（OTA）值学习方法，通过在时间差分学习过程中引入时间抽象，使价值函数能够产生更清晰的优势信号，从而有效缩短有效时序长度。实验表明，使用OTA价值函数提取的高层策略在OGBench基准测试中的复杂任务上表现出色，包括迷宫导航和视觉机器人操作环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 01:51:11 GMT</pubDate>
</item>
<item>
<title>GLEAM-Bench与GLEAM：提升移动机器人在复杂未知环境中的主动映射能力</title>
<link>https://arxiv.org/abs/2505.20294</link>
<guid>https://arxiv.org/abs/2505.20294</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GLEAM-Bench基准与GLEAM策略，显著提高移动机器人在多样化复杂环境中的主动映射性能。</p><br /><br /><p><strong>摘要：</strong> 当前移动机器人在复杂未知环境中的主动映射面临挑战，现有方法因训练数据不足和探索策略保守，难以适应多样化的场景布局和连接性。为解决这一问题，本文引入GLEAM-Bench，首个大规模通用主动映射基准，涵盖1152个来自合成与真实扫描数据集的多样化3D场景。基于此，我们提出了GLEAM，一种统一的通用探索策略。该策略通过语义表示、长期导航目标及随机化策略显著提升了泛化能力，在128个未见过的复杂场景中实现了66.50%的覆盖率，提高了映射精度并优化了轨迹效率。项目页面已发布以供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20294" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>基于覆盖原则的大规模语言模型系统性组合泛化能力分析</title>
<link>https://arxiv.org/abs/2505.20278</link>
<guid>https://arxiv.org/abs/2505.20278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大规模语言模型依赖模式匹配时难以实现系统性组合泛化。</p><br /><br /><p><strong>摘要：</strong> 本文提出覆盖原则这一数据驱动框架，表明主要依赖模式匹配的模型无法可靠地推广到超出片段替换的情形。通过实验发现，Transformer模型在处理两跳泛化任务时，所需训练数据随词汇集大小至少呈二次增长，且参数量扩大20倍并未提升效率。对于存在路径歧义的组合任务，Transformer学习到的上下文相关状态表示会削弱性能与互操作性。尽管思维链监督能提高多跳任务的训练效率，但对路径歧义仍显乏力。最后，我们提出了一个基于机制的分类方法，区分了神经网络实现泛化的三种方式。这一概念视角不仅解释了我们的研究结果，还指出了需要新架构创新的方向，以实现真正的系统性组合泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20278" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>面向未知攻击的终身安全对齐框架</title>
<link>https://arxiv.org/abs/2505.20259</link>
<guid>https://arxiv.org/abs/2505.20259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种终身安全对齐框架，使大模型适应新型越狱攻击。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）在部署过程中可能遭遇的未知越狱攻击问题，提出了一种名为终身安全对齐（Lifelong Safety Alignment）的框架。该框架通过设置Meta-Attacker（元攻击者）和Defender（防御者）的竞争机制，使模型能够持续适应新出现的越狱策略。首先利用GPT-4o API从大量研究论文中提取关键信息，作为Meta-Attacker的初始训练数据。实验显示，在单轮攻击下，第一代Meta-Attacker在RR任务上的成功率为73%，在LAT任务上的迁移成功率也达到57%。然而，经过对抗训练后，Defender显著提升了鲁棒性，最终将Meta-Attacker的成功率降至7%左右。这项工作为LLMs在开放环境中的安全部署提供了重要参考。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:40:40 GMT</pubDate>
</item>
<item>
<title>自适应推理模型（ARM）解决“过度思考”问题</title>
<link>https://arxiv.org/abs/2505.20258</link>
<guid>https://arxiv.org/abs/2505.20258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应推理模型ARM，通过调整推理格式减少令牌使用，提升效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在复杂任务中的“过度思考”问题，即过多不必要的推理，提出了自适应推理模型（ARM）。ARM能够根据任务需求自动选择适当的推理格式，包括高效但简单的Direct Answer、Short CoT和Code，以及更复杂的Long CoT格式。为了训练ARM，引入了Ada-GRPO算法，解决了传统GRPO中的格式崩溃问题，使ARM在减少令牌使用的同时保持性能，平均减少30%，最高可达70%。此外，ARM不仅提高了推理效率，还加快了训练速度达两倍。ARM提供了三种模式：默认的自适应模式、指令引导模式和共识引导模式，以满足不同的应用场景需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:38:50 GMT</pubDate>
</item>
<item>
<title>Omni-R1：基于强化学习的多模态全局推理与细节理解模型</title>
<link>https://arxiv.org/abs/2505.20256</link>
<guid>https://arxiv.org/abs/2505.20256</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Omni-R1模型解决多模态模型在时空覆盖和像素级理解间的矛盾。</p><br /><br /><p><strong>摘要：</strong> 本文针对长时间视频音频推理与精细像素理解对多模态模型提出的冲突需求，设计了双系统架构：全局推理系统通过低空间成本选择关键帧并重写任务，细节理解系统则对高分辨率片段进行像素级定位。由于最优关键帧选择和任务重写难以监督，我们将其转化为强化学习问题，提出了Omni-R1框架。该框架通过在线协作获取分层奖励来训练全局推理系统，仅需少量任务划分的强化学习一轮迭代。实验表明，Omni-R1不仅超越了强监督基线和专门领域的最新模型，还显著提升了跨域泛化能力和减少了多模态幻觉现象，展示了强化学习在大规模多模态推理中的首次成功应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20256" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:34:06 GMT</pubDate>
</item>
<item>
<title>稀疏自编码器特征一致性对神经网络可解释性研究的重要性</title>
<link>https://arxiv.org/abs/2505.20254</link>
<guid>https://arxiv.org/abs/2505.20254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章强调稀疏自编码器特征一致性对提升神经网络机制可解释性的关键作用。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了稀疏自编码器（SAEs）在神经网络机制可解释性（MI）中的应用局限性，特别是特征不一致性问题。作者指出，机制可解释性研究需要优先考虑SAEs的特征一致性，即在独立训练运行中可靠收敛至等效特征集。为此，我们引入成对字典均相关系数（PW-MCC）作为衡量一致性的实用指标，并展示了通过适当架构选择可以实现高水平的一致性（如LLM激活中TopK SAEs达到0.80）。我们的贡献包括阐述一致性优先的益处，提供理论支持及合成验证，证明PW-MCC是恢复真实特征的有效代理；同时将这些发现扩展到实际LLM数据，表明高特征一致性与学习特征解释的语义相似度高度相关。最后，我们呼吁社区转向系统测量特征一致性，以推动机制可解释性领域的稳健累积进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:31:36 GMT</pubDate>
</item>
<item>
<title>基于硬负样本对比学习的大规模多模态模型在几何推理中的应用</title>
<link>https://arxiv.org/abs/2505.20152</link>
<guid>https://arxiv.org/abs/2505.20152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型硬负样本对比学习框架提升视觉编码器的几何理解能力。</p><br /><br /><p><strong>摘要：</strong> 大规模多模态模型（LMMs）在视觉感知任务中表现优异，但对比学习的局限性限制了其在精细推理中的表现，特别是在几何问题求解场景中。本文提出了一种结合图像和文本的硬负样本对比学习框架，通过扰动图表生成代码创建生成型硬负样本、基于修改的几何描述生成规则型硬负样本以及基于标题相似性选择检索型硬负样本，训练得到的MMGeoLM模型在三个几何推理基准测试中显著优于其他开源模型，甚至与强大的闭源模型GPT-4o相媲美。此外，研究还探讨了不同负样本构建方法及负样本数量对几何推理性能的影响，得出丰富的结论。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:55:28 GMT</pubDate>
</item>
<item>
<title>StructEval：评估大型语言模型结构化输出能力的新基准</title>
<link>https://arxiv.org/abs/2505.20139</link>
<guid>https://arxiv.org/abs/2505.20139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出StructEval基准，系统评估大型语言模型生成和转换多种结构化格式的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在软件开发中的广泛应用，其生成结构化输出的能力变得至关重要。本文介绍了一个名为StructEval的新基准，用于评估LLMs在生成非渲染格式（如JSON、YAML、CSV）和可渲染格式（如HTML、React、SVG）方面的表现。与现有基准不同，StructEval通过生成任务和转换任务两种范式，系统性地评估多种格式的结构保真度。该基准涵盖18种格式和44种任务类型，并引入了新的衡量指标来评估格式遵守和结构正确性。实验结果显示，即使是最先进的模型（如o1-mini）也仅达到平均75.58分，而开源替代方案则落后约10分。研究还发现生成任务比转换任务更具挑战性，生成视觉内容的难度高于生成纯文本结构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:40:42 GMT</pubDate>
</item>
<item>
<title>MLR-Bench：评估AI研究代理能力的综合基准</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLR-Bench评估AI代理在开放性机器学习研究中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MLR-Bench的新基准，用于评估AI代理在开放性机器学习研究中的能力。该基准包含三个关键部分：来自NeurIPS、ICLR和ICML研讨会的201个研究任务，涵盖多种ML主题；MLR-Judge，一个结合LLM评审员和精心设计的评审标准的自动化评估框架；以及MLR-Agent，一种模块化代理框架，能够完成研究任务的四个阶段。我们使用MLR-Bench评估了六种前沿LLM和一个先进的编码代理，发现虽然LLM在生成连贯想法和结构良好的论文方面表现出色，但当前的编码代理经常产生伪造或无效的实验结果。此外，通过人工评估验证了MLR-Judge的有效性，并开源了MLR-Bench，以帮助社区对AI研究代理进行基准测试、诊断和改进，推动可信和透明的科学发现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:18:37 GMT</pubDate>
</item>
<item>
<title>基于影响函数的大语言模型推理能力归因研究</title>
<link>https://arxiv.org/abs/2505.19949</link>
<guid>https://arxiv.org/abs/2505.19949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过影响函数揭示大语言模型数学与代码推理能力的训练数据特性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在数学和编码推理方面表现出色，通常得益于更强模型生成的链式思维（CoTs）。然而，现有训练数据的筛选策略主要依赖启发式方法，导致泛化性有限且难以捕捉数据中的微妙特征。本文利用影响函数系统性地将LLMs的数学和编码推理能力归因至单个训练样本、序列和标记，从而深入洞察有效数据的特性。我们的基于影响函数的推理归因（Infra）发现了跨领域效应：高难度数学示例同时提升数学和代码推理能力，而低难度代码任务最能促进代码推理。基于此，我们提出了一种简单有效的数据重加权策略，通过翻转任务难度，使AIME24的准确率从10%提高到20%，LiveCodeBench的准确率从33.8%提高到35.3%。此外，细粒度归因显示，序列级探索行为增强了数学和代码推理性能，而标记级影响模式在数学和代码推理中存在显著差异：前者偏好自然语言逻辑连接符，后者强调结构语法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:15:26 GMT</pubDate>
</item>
<item>
<title>Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</title>
<link>https://arxiv.org/abs/2505.19914</link>
<guid>https://arxiv.org/abs/2505.19914</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 08:40:31 GMT</pubDate>
</item>
<item>
<title>基于元学习视角的大语言模型推理能力研究</title>
<link>https://arxiv.org/abs/2505.19815</link>
<guid>https://arxiv.org/abs/2505.19815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将大语言模型的推理视为元学习过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，通过元学习的视角来理解大型语言模型（LLMs）的推理能力。我们设想推理轨迹为对模型参数的伪梯度下降更新，并发现LLM推理与多种元学习范式的相似性。我们将推理任务的训练过程形式化为元学习设置，其中每个问题被视为一个单独的任务，推理轨迹作为适应模型参数的内循环优化。经过多样化问题的训练后，LLM能够发展出可泛化的基础推理能力。广泛的实证评估证实了LLM推理与元学习之间的紧密联系，并从元学习的角度探讨了多个重要议题。这项工作不仅深化了对LLM推理的理解，还为通过传统元学习技术改进这些模型提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:52:17 GMT</pubDate>
</item>
<item>
<title>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</title>
<link>https://arxiv.org/abs/2505.19800</link>
<guid>https://arxiv.org/abs/2505.19800</guid>
<content:encoded><![CDATA[
Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets' scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: https://github.com/IVUL-KAUST/MOLE and dataset: https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:31:26 GMT</pubDate>
</item>
<item>
<title>基于多轮分解的大型推理模型高效推理方法</title>
<link>https://arxiv.org/abs/2505.19788</link>
<guid>https://arxiv.org/abs/2505.19788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多轮分解方法MinD，大幅减少推理延迟。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）因复杂的链式思维（CoT）导致推理效率低下，首令牌及整体延迟较高。本文引入多轮分解（MinD），将传统CoT解码为显式、结构化的多轮交互序列，每轮专注于一个思考单元并给出相应答案，支持反思、验证、修正及探索替代方案，显著提升推理速度并实现过程控制。通过监督微调（SFT）与强化学习（RL）结合优化模型，在MATH等数据集上，MinD可使输出令牌使用量和首次令牌时间（TTFT）减少约70%，同时保持在MATH-500、AIME24、AMC23和GPQA-Diamond等推理基准测试中的竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:18:57 GMT</pubDate>
</item>
<item>
<title>离散马尔可夫桥：一种新型的离散表示学习框架</title>
<link>https://arxiv.org/abs/2505.19752</link>
<guid>https://arxiv.org/abs/2505.19752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种针对离散表示学习的新框架，显著提升了表达能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为离散马尔可夫桥的新框架，用于解决现有离散扩散模型中存在的局限性。这些局限包括固定转换矩阵导致潜在表示表达力不足以及设计空间受限的问题。新框架通过矩阵学习和分数学习两个组件构建，并提供了理论性能保证及收敛性证明。此外，我们还分析了方法的空间复杂度，解决了先前研究中发现的实际约束问题。实验结果表明，该框架在Text8数据集上的证据下界（ELBO）达到了1.38，优于现有基线模型；同时在CIFAR-10数据集上也表现出与图像特定生成方法相当的竞争性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:32:12 GMT</pubDate>
</item>
<item>
<title>基于纳什均衡的人类反馈强化学习算法及其在线实现</title>
<link>https://arxiv.org/abs/2505.19731</link>
<guid>https://arxiv.org/abs/2505.19731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的在线纳什均衡强化学习算法Nash-MP，实现在复杂人类偏好下的快速收敛。</p><br /><br /><p><strong>摘要：</strong> 传统基于人类反馈的强化学习（RLHF）通常依赖奖励模型，但可能无法准确捕捉真实的人类偏好结构。本文引入Nash Mirror Prox (Nash-MP)，这是一种新型的在线纳什学习算法，利用镜像优化方法实现对纳什均衡的快速稳定收敛。理论分析表明，Nash-MP在KL散度、exploitability gap及对数概率的span半范数上均具有线性收敛特性，且这些特性不受动作空间大小影响。此外，我们还提出了近似版本的Nash-MP，通过随机策略梯度估计近似求解，并探讨其在大语言模型微调中的实际应用，实验结果证明其性能与现有方法兼容且竞争力强。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:17:32 GMT</pubDate>
</item>
<item>
<title>PathFinder-PRM：一种基于分层错误感知的数学推理奖励模型</title>
<link>https://arxiv.org/abs/2505.19706</link>
<guid>https://arxiv.org/abs/2505.19706</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入新的Process Reward Model提升多跳推理任务中的数学问题求解能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在多跳推理任务如数学问题求解中容易出现幻觉。本文介绍了一种名为PathFinder-PRM的新过程奖励模型，它通过分级检测每一步骤中的数学和一致性错误来指导生成连贯的解决方案。为了训练该模型，我们扩展了人类注释的数据集，构建了一个包含40万样本的数据集。实验结果显示，PathFinder-PRM在PRMBench上达到了67.7的新SOTA分数，比之前的最佳成绩高出2.2分，同时使用的数据量仅为之前的三分之一。当应用于基于奖励的贪心搜索时，模型在prm@8指标上取得了48.3的成绩，比最强基线提升了1.5个百分点。这些成果表明，分离的错误检测和奖励估计不仅提高了细粒度错误检测的能力，还显著提升了端到端的基于奖励的数学推理效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19706" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 04:56:36 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多跳推理改进方法</title>
<link>https://arxiv.org/abs/2505.19640</link>
<guid>https://arxiv.org/abs/2505.19640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习指导语言模型交替推理和回答，大幅提升多跳问题解决效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的训练范式，利用强化学习引导大型语言模型在解决多跳问题时交替进行推理与回答。实验表明，这种方法可以有效提升推理能力，同时显著减少首次响应时间（TTFT），平均降低超过80%，并且在Pass@1准确性上提高了19.3%。此外，该方法无需依赖外部工具，仅基于问答和逻辑推理数据集即可实现对复杂推理任务的良好泛化。研究还深入分析了条件奖励建模中的若干有价值见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:58:17 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多智能体协作框架提升医学问答系统性能</title>
<link>https://arxiv.org/abs/2505.19630</link>
<guid>https://arxiv.org/abs/2505.19630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DoctorAgent-RL框架，通过强化学习优化医学咨询对话策略。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在生物医学问答领域表现优异，但在临床咨询中的应用仍面临挑战，如信息传递不明确导致诊断建议不具体。传统方法受限于静态数据驱动模式，缺乏泛化能力且难以智能提取关键信息。为解决这些问题，本文提出DoctorAgent-RL，一种基于强化学习的多智能体协作框架，将医疗咨询建模为动态决策过程。该框架通过医生代理与患者代理的多轮交互优化提问策略，依据综合奖励动态调整信息采集路径。实验表明，DoctorAgent-RL在多轮推理能力和最终诊断性能上优于现有模型，并在MTMedDialog数据集上展示了实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:48:14 GMT</pubDate>
</item>
<item>
<title>ScaleKV：针对视觉自回归模型的KV缓存压缩框架</title>
<link>https://arxiv.org/abs/2505.19602</link>
<guid>https://arxiv.org/abs/2505.19602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ScaleKV框架，大幅降低视觉自回归模型KV缓存内存需求。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于视觉自回归（VAR）模型的高效推理问题，指出其粗略到精细的方法导致KV缓存指数增长的问题。为解决这一瓶颈，我们引入了ScaleKV，这是一种专为VAR架构设计的KV缓存压缩框架。通过将Transformer层分为起草者（drafters）和精炼器（refiners），并根据不同的注意力模式进行差异化缓存管理，ScaleKV显著减少了缓存内存需求。实验表明，在最先进的文本到图像VAR模型Infinity上，该方法将所需KV缓存内存减少至原来的10%，同时保持像素级保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19602" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:11:42 GMT</pubDate>
</item>
<item>
<title>利用内部反馈信号实现大规模语言模型的无监督强化学习</title>
<link>https://arxiv.org/abs/2505.19590</link>
<guid>https://arxiv.org/abs/2505.19590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出基于自我确信度的无监督强化学习方法，可媲美有监督学习效果。</p><br /><br /><p><strong>摘要：</strong> 当前通过验证奖励进行强化学习（RLVR）训练复杂推理的大规模语言模型（LLMs）有效但成本高昂且依赖特定领域的监督。本文探索了一种新的框架——基于内部反馈的强化学习（RLIF），该框架使LLMs能够利用内在信号而非外部奖励或标记数据进行学习。文中提出的Intuitor方法，仅使用模型自身的置信度作为奖励信号，在无需黄金标准解决方案或测试用例的情况下，实现了完全无监督的学习。实验表明，Intuitor在数学基准测试中的表现与需要外部奖励的团体相对策略优化（GRPO）相当，同时在代码生成等跨领域任务上表现出更好的泛化能力。这项研究表明，内在模型信号可以驱动跨领域有效的学习，为无法获得可验证奖励的自主AI系统提供了一种可扩展的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:01:06 GMT</pubDate>
</item>
<item>
<title>BizFinBench：首个面向金融应用的大语言模型评估基准</title>
<link>https://arxiv.org/abs/2505.19457</link>
<guid>https://arxiv.org/abs/2505.19457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出BizFinBench，首个针对大语言模型在金融领域应用的评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BizFinBench，这是一个专门设计用于评估大型语言模型（LLMs）在真实世界金融应用场景中的表现的全新基准。该基准包含6,781个经过良好标注的中文查询，涵盖了五个维度：数值计算、推理、信息提取、预测识别和基于知识的问题回答，分为九个细分类别。此外，还引入了IteraJudge，这是一种新的LLM评估方法，旨在减少LLMs作为客观指标评估者时的偏差。实验测试了25种模型，结果显示没有一款模型在所有任务中占据主导地位。分析揭示了各模型在不同能力上的显著差异，例如在数值计算方面，Claude-3.5-Sonnet和DeepSeek-R1表现出色；而在推理任务中，闭源模型明显优于开源模型。尽管当前LLMs在处理常规财务查询时表现良好，但在需要跨概念推理的复杂场景中仍显不足。BizFinBench为未来相关研究提供了严谨且业务导向的评估工具，代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 23:23:02 GMT</pubDate>
</item>
<item>
<title>AI辅助软件开发中的两种新兴范式：氛围编码与自主编码对比分析</title>
<link>https://arxiv.org/abs/2505.19443</link>
<guid>https://arxiv.org/abs/2505.19443</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文对比分析了AI辅助软件开发中的氛围编码与自主编码两种范式。</p><br /><br /><p><strong>摘要：</strong> 本文深入探讨了AI辅助软件开发领域中两种新兴范式——氛围编码和自主编码。尽管两者均依赖大型语言模型（LLMs），但在自主性、架构设计及开发者角色上存在根本差异。氛围编码侧重通过基于提示词的对话工作流支持创意探索，而自主编码则通过目标驱动代理实现自动化开发。文章提出了涵盖概念基础、执行模型、反馈机制等的详细分类法，并通过20个具体案例展示了氛围系统在原型设计和教育中的优势，以及自主系统在企业级自动化和CI/CD集成中的卓越表现。此外，文章还探讨了自然语言接口与自动化执行管道结合的混合架构趋势，并规划了可信、可解释且协作的自主AI基础设施未来路径。研究指出，成功的AI软件工程需要整合这两种范式的优点，构建以人为中心的统一开发生命周期。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19443" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 23:00:21 GMT</pubDate>
</item>
<item>
<title>基于格式与长度代理信号的大规模语言模型数学问题求解训练研究</title>
<link>https://arxiv.org/abs/2505.19439</link>
<guid>https://arxiv.org/abs/2505.19439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出利用格式和长度作为代理信号训练大规模语言模型解决数学问题。</p><br /><br /><p><strong>摘要：</strong> 大规模语言模型在自然语言处理领域取得了显著成就，强化学习在模型应用适配中起到了关键作用。然而，在数学问题求解的训练过程中，获取真实答案往往困难且昂贵。本文探索了利用格式和长度作为代理信号的方法，通过设计奖励函数，使得模型在早期阶段的表现可与标准GRPO算法相媲美。随着训练进展，单纯依赖格式的奖励机制暴露出局限性，因此引入了长度相关的奖励信号。最终提出的GRPO方法结合格式与长度的代理信号，在特定场景下不仅达到了与传统基于真实答案方法相当甚至更好的性能，还在AIME2024测试集上实现了40.0%的准确率。这项研究不仅提供了一种减少对大量真实标注数据依赖的实用解决方案，还揭示了无标签方法成功的本质：基础模型已具备强大的数学和逻辑推理能力，只需培养良好的答题习惯即可展现潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:56:22 GMT</pubDate>
</item>
<item>
<title>WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference</title>
<link>https://arxiv.org/abs/2505.19427</link>
<guid>https://arxiv.org/abs/2505.19427</guid>
<content:encoded><![CDATA[
The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise ell_2-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to 2.94% in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:37:32 GMT</pubDate>
</item>
<item>
<title>通过生成模型筛选构建通用文本到图像精调数据集</title>
<link>https://arxiv.org/abs/2505.19297</link>
<guid>https://arxiv.org/abs/2505.19297</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法构建通用的文本到图像模型精调数据集。</p><br /><br /><p><strong>摘要：</strong> 预训练虽然为文本到图像（T2I）模型提供了广泛的世界知识，但仅靠此不足以实现高质量的美学效果和对齐。因此，监督微调（SFT）至关重要，但其效果高度依赖于微调数据集的质量。现有的公开SFT数据集往往针对狭窄领域，创建高质量、通用的SFT数据集仍是一项重大挑战。本文介绍了一种利用预训练生成模型作为高影响力训练样本估计器的新方法，以此构建并发布了Alchemist数据集。该数据集虽小（3,350个样本），却非常有效。实验表明，Alchemist显著提高了五个公共T2I模型的生成质量，同时保持了多样性和风格。此外，还向公众发布了微调模型的权重。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19297" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 16:08:20 GMT</pubDate>
</item>
<item>
<title>基于过程级自适应推理模式的大语言模型高效推理方法</title>
<link>https://arxiv.org/abs/2505.19250</link>
<guid>https://arxiv.org/abs/2505.19250</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新推理范式PATS，动态调整大语言模型的推理策略以优化性能与效率。</p><br /><br /><p><strong>摘要：</strong> 当前大规模语言模型通常采用固定推理策略处理所有问题，忽视了任务复杂性和推理过程的变化，导致性能与效率失衡。现有方法通过训练自由的快慢思考系统切换来应对不同难度的问题，但受限于粗粒度的解决方案级策略调整。为解决这一问题，本文提出了一种新的推理范式——过程级自适应推理模式切换(PATS)，使大语言模型能够根据每一步的难度动态调整推理策略，从而在准确性与计算效率之间取得平衡。该方法将过程奖励模型(PRMs)与束搜索相结合，引入渐进式模式切换和坏步惩罚机制。实验表明，在多种数学基准测试中，我们的方法在保持适度令牌使用量的同时实现了高精度。这项研究强调了过程级、难度感知的推理策略适应的重要性，为大语言模型的高效推理提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19250" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:58:50 GMT</pubDate>
</item>
<item>
<title>Variance-Reduced Preference Optimization提升掩码扩散模型对齐性能</title>
<link>https://arxiv.org/abs/2505.19223</link>
<guid>https://arxiv.org/abs/2505.19223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过VRPO框架优化掩码扩散模型对齐人类偏好，显著提升LLaDA表现。</p><br /><br /><p><strong>摘要：</strong> 掩码扩散模型（MDMs）如LLaDA在语言建模方面展现出巨大潜力，但其与人类偏好的对齐尚未得到充分探索，主要因为基于证据下界（ELBO）的似然估计具有高方差问题。为此，我们提出变异性降低偏好优化（VRPO），通过理论分析ELBO估计器的方差，推导偏好优化梯度的偏差和方差界限，并引入无偏方差减少策略如最优蒙特卡洛预算分配和反向采样，大幅提高MDM对齐效果。实验表明，应用VRPO优化后的LLaDA 1.5在数学、代码及对齐基准测试中均显著优于仅经过监督微调的前代模型，并在数学能力上与顶级语言MDMs和自回归模型（ARMs）竞争。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 12:36:20 GMT</pubDate>
</item>
<item>
<title>大型语言模型在精细科学假设发现中的应用与优化研究</title>
<link>https://arxiv.org/abs/2505.19209</link>
<guid>https://arxiv.org/abs/2505.19209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法解决大型语言模型生成精细科学假设的问题。</p><br /><br /><p><strong>摘要：</strong> 本文引入并正式定义了精细科学假设发现这一全新任务，即从粗略的研究方向中生成具有实验操作性的详细假设。我们将其视为组合优化问题，并探讨了大型语言模型在此任务上的能力上限。通过构建潜在奖励景观，我们研究了如何利用模型内部启发式来优化假设生成过程，并比较了单一模型与模型集合的效果。实验表明，提出的分层搜索方法可有效平滑奖励景观并提高优化效果，在化学文献基准测试中显著优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 12:13:46 GMT</pubDate>
</item>
<item>
<title>从模型压缩到令牌压缩：AI效率研究的新范式</title>
<link>https://arxiv.org/abs/2505.19147</link>
<guid>https://arxiv.org/abs/2505.19147</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨了AI效率研究从模型压缩转向数据驱动的令牌压缩。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）和多模态LLMs的发展主要依赖于通过增加参数数量实现性能提升。然而，随着硬件限制的逼近，计算瓶颈已转变为自注意力机制在长序列上的二次成本问题。本文认为，AI效率研究的重点正在从模型为中心的压缩转向数据为中心的压缩。我们提出令牌压缩作为新的研究前沿，它通过减少训练或推理过程中的令牌数量来提高AI效率。通过综合分析，本文首先考察了各领域长上下文AI的最新进展，并建立了现有模型效率策略的统一数学框架，展示了令牌压缩为何是解决长上下文开销的关键范式转变。随后，系统回顾了令牌压缩的研究现状，分析了其基本优势，并在多种场景下展示了其显著优势。此外，还深入剖析了当前令牌压缩研究面临的挑战，并提出了未来有前景的方向。本研究旨在为AI效率提供新视角，整合现有研究成果，并推动创新解决方案以应对长上下文对AI发展的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19147" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 09:51:17 GMT</pubDate>
</item>
<item>
<title>Jodi: Unification of Visual Generation and Understanding via Joint Modeling</title>
<link>https://arxiv.org/abs/2505.19084</link>
<guid>https://arxiv.org/abs/2505.19084</guid>
<content:encoded><![CDATA[
Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 06:40:52 GMT</pubDate>
</item>
<item>
<title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title>
<link>https://arxiv.org/abs/2505.19056</link>
<guid>https://arxiv.org/abs/2505.19056</guid>
<content:encoded><![CDATA[
Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 05:18:24 GMT</pubDate>
</item>
<item>
<title>AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting</title>
<link>https://arxiv.org/abs/2505.18822</link>
<guid>https://arxiv.org/abs/2505.18822</guid>
<content:encoded><![CDATA[
Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 14:46:50 GMT</pubDate>
</item>
<item>
<title>Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models</title>
<link>https://arxiv.org/abs/2505.18773</link>
<guid>https://arxiv.org/abs/2505.18773</guid>
<content:encoded><![CDATA[
State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC&lt;0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 12:23:43 GMT</pubDate>
</item>
<item>
<title>The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation</title>
<link>https://arxiv.org/abs/2505.18759</link>
<guid>https://arxiv.org/abs/2505.18759</guid>
<content:encoded><![CDATA[
Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 11:54:19 GMT</pubDate>
</item>
<item>
<title>Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps</title>
<link>https://arxiv.org/abs/2505.18675</link>
<guid>https://arxiv.org/abs/2505.18675</guid>
<content:encoded><![CDATA[
Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 08:33:52 GMT</pubDate>
</item>
<item>
<title>Flex-Judge: Think Once, Judge Anywhere</title>
<link>https://arxiv.org/abs/2505.18601</link>
<guid>https://arxiv.org/abs/2505.18601</guid>
<content:encoded><![CDATA[
Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 04:50:53 GMT</pubDate>
</item>
<item>
<title>B-score: Detecting biases in large language models using response history</title>
<link>https://arxiv.org/abs/2505.18545</link>
<guid>https://arxiv.org/abs/2505.18545</guid>
<content:encoded><![CDATA[
Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 02:23:52 GMT</pubDate>
</item>
<item>
<title>Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.18536</link>
<guid>https://arxiv.org/abs/2505.18536</guid>
<content:encoded><![CDATA[
Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 02:01:48 GMT</pubDate>
</item>
<item>
<title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title>
<link>https://arxiv.org/abs/2505.18384</link>
<guid>https://arxiv.org/abs/2505.18384</guid>
<content:encoded><![CDATA[
Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 17:18:59 GMT</pubDate>
</item>
<item>
<title>Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation</title>
<link>https://arxiv.org/abs/2505.18323</link>
<guid>https://arxiv.org/abs/2505.18323</guid>
<content:encoded><![CDATA[
For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 15:28:45 GMT</pubDate>
</item>
<item>
<title>Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model</title>
<link>https://arxiv.org/abs/2505.17894</link>
<guid>https://arxiv.org/abs/2505.17894</guid>
<content:encoded><![CDATA[
We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:42:21 GMT</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[
Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces Competence-Difficulty Alignment Sampling (CDAS), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:15:26 GMT</pubDate>
</item>
<item>
<title>From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition</title>
<link>https://arxiv.org/abs/2505.16972</link>
<guid>https://arxiv.org/abs/2505.16972</guid>
<content:encoded><![CDATA[
Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:51:05 GMT</pubDate>
</item>
<item>
<title>Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance</title>
<link>https://arxiv.org/abs/2505.16348</link>
<guid>https://arxiv.org/abs/2505.16348</guid>
<content:encoded><![CDATA[
Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO
]]></content:encoded>
<pubDate>Thu, 22 May 2025 04:00:10 GMT</pubDate>
</item>
<item>
<title>EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning</title>
<link>https://arxiv.org/abs/2505.16312</link>
<guid>https://arxiv.org/abs/2505.16312</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) excel at complex reasoning through search algorithms, yet current strategies often suffer from massive token consumption due to redundant exploration of semantically equivalent steps. Existing semantic similarity methods struggle to accurately identify such equivalence in domain-specific contexts like mathematical reasoning. To address this, we propose EquivPruner, a simple yet effective approach that identifies and prunes semantically equivalent actions during LLM reasoning search. We also introduce MathEquiv, the first dataset we created for mathematical statement equivalence, which enables the training of a lightweight equivalence detector. Extensive experiments across various models and tasks demonstrate that EquivPruner significantly reduces token consumption, improving searching efficiency and often bolstering reasoning accuracy. For instance, when applied to Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1\% while also improving accuracy. Our code is available at https://github.com/Lolo1222/EquivPruner.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 03:07:43 GMT</pubDate>
</item>
<item>
<title>Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15957</link>
<guid>https://arxiv.org/abs/2505.15957</guid>
<content:encoded><![CDATA[
With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13426</link>
<guid>https://arxiv.org/abs/2505.13426</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:54:39 GMT</pubDate>
</item>
<item>
<title>InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction</title>
<link>https://arxiv.org/abs/2505.10887</link>
<guid>https://arxiv.org/abs/2505.10887</guid>
<content:encoded><![CDATA[
This paper introduces InfantAgent-Next, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at https://github.com/bin123apple/InfantAgent.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 01:43:27 GMT</pubDate>
</item>
<item>
<title>面向低资源语言的文化适配大型语言模型研究</title>
<link>https://arxiv.org/abs/2505.18383</link>
<guid>https://arxiv.org/abs/2505.18383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合文化与语言特性的大型语言模型创建方法。</p><br /><br /><p><strong>摘要：</strong> 当前针对低资源语言增强大型语言模型（LLMs）的研究多依赖于从英语语料库翻译生成的合成数据，尽管这些模型展示了良好的语言理解和翻译能力，但往往未能充分反映目标社区的文化遗产与价值观。本文提出了一种创新方法，通过构建符合特定社区语言、文化遗产及价值观的合成与检索型预训练数据，以实现对社区文化的深度适配。以埃及方言和摩洛哥方言为例进行测试，我们开发了NileChat，一款30亿参数量的语言模型，显著提升了对阿拉伯语相关任务的理解、翻译及文化对齐表现，同时性能媲美更大规模的模型。本研究共享了方法、数据与模型，以推动LLMs在更多元化社区中的发展与包容性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18383" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 17:18:40 GMT</pubDate>
</item>
<item>
<title>RankNovo：基于深度重排序框架的全新从头肽段测序方法</title>
<link>https://arxiv.org/abs/2505.17552</link>
<guid>https://arxiv.org/abs/2505.17552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RankNovo，首个利用多模型互补性的深度重排序框架提升从头肽段测序性能。</p><br /><br /><p><strong>摘要：</strong> 从头肽段测序是蛋白质组学中的关键任务，但当前基于深度学习的方法受限于质谱数据的复杂性和噪声信号的异质分布，导致数据特定偏差。本文介绍RankNovo，这是一种新的深度重排序框架，通过结合多个测序模型的优势增强从头肽段测序性能。RankNovo采用列表式重排序方法，将候选肽段建模为多重序列比对，并利用轴向注意力提取跨候选肽段的信息特征。此外，引入了两个新指标PMD（肽质量偏差）和RMD（残余质量偏差），通过量化序列和残基层面的肽质量差异提供精细监督。实验表明，RankNovo不仅超越了用于生成训练候选肽段的基线模型，还设定了新的最先进的基准。同时，RankNovo在未见过的模型上展示了强大的零样本泛化能力，突显其稳健性及作为通用重排序框架的潜力。这项工作提出了挑战现有单一模型范式的新型重排序策略，推动了从头肽段测序精度的进步。源代码已在GitHub上发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:56:55 GMT</pubDate>
</item>
<item>
<title>基于长上下文推理轨迹的高效价值模型训练方法</title>
<link>https://arxiv.org/abs/2505.17373</link>
<guid>https://arxiv.org/abs/2505.17373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需定义“步”的高效价值模型训练方法，显著提升长上下文推理模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种简单而高效的值模型训练方法，专门针对长上下文推理轨迹进行优化。与现有的过程奖励模型（PRMs）相比，该方法避免了对“步”的精细定义问题，这对于长上下文推理模型而言通常难以明确。通过收集包含250万个推理轨迹的数据集，我们训练了一个拥有15亿标记的值模型，并将其应用于DeepSeek模型中，以在测试时间计算扩展时提高性能。实验结果显示，采用块级值引导搜索（VGS）结合最终加权多数投票的方法，在四个竞争性数学基准测试（AIME 2024 & 2025，HMMT Feb 2024 & 2025）中实现了平均45.7%的准确率，达到了与o3-mini-medium相当的水平。此外，VGS大幅减少了实现相同效果所需的推理浮点运算次数。本研究的数据集、模型及代码库均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 21:05:07 GMT</pubDate>
</item>
<item>
<title>ScanBot：面向高精度表面扫描的指令条件机器人学习数据集</title>
<link>https://arxiv.org/abs/2505.17295</link>
<guid>https://arxiv.org/abs/2505.17295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScanBot数据集专注于工业级激光扫描的高精度需求，提供语言引导的机器人扫描轨迹。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ScanBot数据集，该数据集专门用于指导机器人执行高精度表面扫描任务。与现有主要关注粗粒度任务（如抓取、导航或对话）的机器人学习数据集不同，ScanBot针对工业激光扫描的需求设计，强调亚毫米级别的路径连续性和参数稳定性。数据集涵盖了机器人对12种多样化对象执行的6类扫描任务，包括全面扫描、几何区域聚焦、空间参考部分、功能相关结构、缺陷检测及对比分析。每项扫描任务均依据自然语言指令进行，并配以同步的RGB、深度图像及激光轮廓，同时记录机器人姿态与关节状态。尽管当前视觉-语言动作模型已取得进展，但在精细指令下的稳定轨迹生成和实际精度需求方面仍面临挑战。通过在完整感知-规划-执行循环中评估多种多模态大型语言模型，研究揭示了在真实约束条件下指令遵循存在的持续难题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 17:22:50 GMT</pubDate>
</item>
<item>
<title>基于稀疏大语言模型的中文多语言机器翻译系统FuxiMT</title>
<link>https://arxiv.org/abs/2505.14256</link>
<guid>https://arxiv.org/abs/2505.14256</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的中文多语言机器翻译模型FuxiMT，显著提升低资源场景下的翻译性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FuxiMT，一种由中国主导的新型多语言机器翻译模型，它基于稀疏化的大规模语言模型（LLM）构建。通过两阶段训练策略，首先利用大规模中文语料进行预训练，然后在包含65种语言的平行数据集上进行多语言微调。FuxiMT采用了专家混合（MoEs）技术，并应用了课程学习策略，以确保在不同资源水平下都能表现稳健。实验结果显示，FuxiMT在多项基准测试中超越了现有最先进的LLM和机器翻译模型，尤其是在低资源条件下表现尤为突出。此外，FuxiMT展现出卓越的零样本翻译能力，能够在缺乏平行数据的语言对之间实现有效的翻译，显示出其在填补跨语言交流障碍方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14256" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 08:09:17 GMT</pubDate>
</item>
<item>
<title>DanceTogether：多角色交互可控视频生成框架</title>
<link>https://arxiv.org/abs/2505.18078</link>
<guid>https://arxiv.org/abs/2505.18078</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个端到端扩散模型DanceTogether，实现多演员身份动作一致的长视频生成。</p><br /><br /><p><strong>摘要：</strong> 近年来可控视频生成技术发展迅速，但现有系统在处理多个演员需移动、互动并交换位置时，尤其是在嘈杂控制信号下表现不佳。本文引入DanceTogether，这是首个基于扩散模型的端到端框架，通过单一参考图像加上独立的姿态掩码流生成高质量的长视频，同时严格保留每个角色的身份。该模型利用MaskPoseAdapter模块，在去噪过程中结合鲁棒跟踪掩码和语义丰富的姿态热图，解决了身份漂移和外观泄漏的问题。为了训练和评估，我们创建了PairFS-4K（包含7000+独特身份的双滑冰者视频）、HumanRob-300（一小时的人形机器人交互数据集）和TogetherVideoBench（涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰的三轨基准测试套件）。实验表明，DanceTogether在TogetherVideoBench上显著优于现有方法。此外，通过一小时的微调，该模型能够生成令人信服的人机交互视频，展示了其在具身AI和人机交互任务中的广泛应用潜力。广泛的消融研究进一步验证了持续的身份动作绑定对性能提升的重要性。综上所述，我们的模型、数据集和基准测试为多演员交互的数字制作、仿真和具身智能开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18078" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:37:14 GMT</pubDate>
</item>
<item>
<title>扩散分类器在组合性理解中的能力研究</title>
<link>https://arxiv.org/abs/2505.17955</link>
<guid>https://arxiv.org/abs/2505.17955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型在组合性视觉场景理解中表现出色，但其能力依赖特定条件。</p><br /><br /><p><strong>摘要：</strong> 本文对扩散分类器在多种组合性任务上的判别能力进行了全面研究，涵盖三个扩散模型及十个数据集的三十多个任务。我们探讨了目标数据集领域对性能的影响，并引入新的诊断基准Self-Bench以隔离领域效应。此外，我们分析了时间步加权的重要性及其与领域差距的关系，特别是对SD3-m模型的影响。研究结果表明，扩散分类器确实具备组合性理解的能力，但其表现受条件限制。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:29:52 GMT</pubDate>
</item>
<item>
<title>FREESON框架：让大型推理模型自主检索知识</title>
<link>https://arxiv.org/abs/2505.16409</link>
<guid>https://arxiv.org/abs/2505.16409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FREESON框架，使大型推理模型能独立完成检索任务。</p><br /><br /><p><strong>摘要：</strong> 现有检索增强推理方法依赖独立的检索模型，限制了大型推理模型在检索中的作用，导致硬件成本增加及错误率上升。为解决这一问题，我们提出了FREESON框架，该框架允许大型推理模型同时作为生成器和检索器，通过引入针对检索任务优化的CT-MCTS算法，在五个开放领域问答基准测试中，平均提升了14.4%的精确匹配率（EM）和F1分数，优于四个具有独立检索器的多步推理模型，并在PopQA和2WikiMultihopQA上分别超过最强基线3%和2%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:00:08 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的位置偏差及其跨语言特性研究</title>
<link>https://arxiv.org/abs/2505.16134</link>
<guid>https://arxiv.org/abs/2505.16134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型的位置偏差在不同语言中的表现及影响。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通常表现出位置偏差，即对特定上下文位置信息的系统性忽视，但其与语言多样性之间的相互作用尚不明确。本研究通过分析英语、俄语、德语、印地语和越南语五种类型学上截然不同的语言，探讨位置偏差如何与模型不确定性、句法结构及提示方法交互作用。主要发现包括：(1) 位置偏差由模型驱动且具有语言特异性，例如Qwen2.5-7B更倾向于后期位置而非早期位置；(2) 明确的位置引导会降低多种语言的准确性，挑战传统的提示工程实践；(3) 对齐上下文与位置偏差会增加熵值，但最低熵值并不预测准确性；(4) 进一步研究表明，大型语言模型在自由词序语言（如印地语）中施加主导词序的方式有所不同。这项研究有助于更好地理解位置偏差对多语言处理的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 22:23:00 GMT</pubDate>
</item>
<item>
<title>ReflAct：强化LLM代理推理能力的新框架</title>
<link>https://arxiv.org/abs/2505.15182</link>
<guid>https://arxiv.org/abs/2505.15182</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入ReflAct新框架提升LLM代理在复杂环境中的目标一致性。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLM）代理的进展主要依赖于像ReAct这样的推理后端，但ReAct常产生不一致或不连贯的推理步骤，导致代理实际状态与目标之间的偏差。我们的分析表明，这是由于ReAct无法维持内部信念的一致性及目标对齐，从而引发累积错误和幻觉。为此，我们提出了ReflAct，一种新的推理后端，将推理重点从单纯规划下一步行动转向持续反思代理相对于目标的状态。通过明确地基于状态进行决策并强制执行持续的目标对齐，ReflAct显著提高了策略的可靠性，在ALFWorld环境中平均表现比ReAct高出27.7%，成功率达到93.3%。值得注意的是，ReflAct的表现甚至超过了配备增强模块（如Reflexion、WKM）的ReAct，证明强化核心推理后端是实现可靠代理性能的关键。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15182" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 02:57:39 GMT</pubDate>
</item>
<item>
<title>Quartet：一种高效且精确的大语言模型低精度训练方法</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Quartet方法，实现全FP4精度的大规模语言模型训练。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于硬件支持的FP4精度训练方法，针对当前算法在低精度训练中的准确性下降问题，提出了名为Quartet的新方法。该方法能够在所有主要计算（如线性层）中均以低精度执行，同时保持高准确性。通过在Llama型模型上的广泛评估，揭示了一种新的低精度缩放定律，用于量化不同位宽下的性能权衡。实验表明，Quartet在NVIDIA Blackwell GPU上实现了最先进的FP4精度训练效果，并成功训练了十亿级规模的模型。此方法展示了全FP4精度训练作为标准精度和FP8训练的竞争替代方案的潜力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>轻量级模型不可知框架s3提升检索增强生成性能</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型框架s3，显著提高检索增强生成系统的下游表现。</p><br /><br /><p><strong>摘要：</strong> 现有的检索增强生成(RAG)系统在推理过程中通过大型语言模型(LLMs)访问外部知识，但优化方法往往忽视下游实用性或限制搜索功能。本研究提出s3框架，该框架通过解耦搜索器与生成器，并采用基于RAG改进的奖励机制训练搜索器，在无需大量训练样本的情况下，显著超越基于更大规模数据训练的基线模型，在六个通用问答和五个医学问答基准测试中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:53:56 GMT</pubDate>
</item>
<item>
<title>TabSTAR：一种具有语义目标感知表示的表格基础模型</title>
<link>https://arxiv.org/abs/2505.18125</link>
<guid>https://arxiv.org/abs/2505.18125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TabSTAR通过结合文本特征实现表格数据的迁移学习并达到最先进的性能。</p><br /><br /><p><strong>摘要：</strong> 深度学习在许多领域取得了显著的成功，但在表格学习任务上表现不佳，这些任务仍由梯度提升决策树主导。然而，最近的进步正在推动表格基础模型的发展，这些模型可以利用现实世界的知识并在多样化数据集上进行泛化，尤其是在数据包含自由文本的情况下。尽管将语言模型能力引入表格任务已被探索，但大多数现有方法使用静态、与目标无关的文本表示，限制了它们的有效性。我们介绍了TabSTAR：一种具有语义目标感知表示的表格基础模型。TabSTAR设计用于对具有文本特征的表格数据进行迁移学习，其架构不依赖于特定于数据集的参数。它解冻了一个预训练的文本编码器，并采用目标令牌作为输入，为目标提供上下文以学习任务特定的嵌入。TabSTAR在已知分类任务基准的中型和大型数据集上实现了最先进的性能，其预训练阶段在数据集数量上表现出缩放规律，为未来的性能改进提供了途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:34:28 GMT</pubDate>
</item>
<item>
<title>QwenLong-CPRS：一种面向长上下文优化的动态压缩框架</title>
<link>https://arxiv.org/abs/2505.18092</link>
<guid>https://arxiv.org/abs/2505.18092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QwenLong-CPRS通过自然语言指导的动态上下文优化机制提升大模型长序列处理效率。</p><br /><br /><p><strong>摘要：</strong> QwenLong-CPRS是一种针对长上下文优化设计的新型框架，旨在解决大型语言模型（LLMs）在长序列处理时的计算开销问题及“中间迷失”性能下降现象。该框架通过自然语言指令引导的多粒度上下文压缩实现高效且高性能的推理。作为Qwen系列架构的一部分，QwenLong-CPRS引入了四项关键技术：自然语言引导的动态优化、双向推理层增强边界意识、带语言建模头的Token批评机制以及窗口并行推理。经五项基准测试验证，QwenLong-CPRS在准确性、效率和通用性上均表现出色，不仅优于其他上下文管理方法，还能显著提升多种旗舰LLMs的表现，并在特定测试中达到SOTA水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:47:00 GMT</pubDate>
</item>
<item>
<title>基于模拟实验引导的假设排序方法</title>
<link>https://arxiv.org/abs/2505.17873</link>
<guid>https://arxiv.org/abs/2505.17873</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用模拟实验结果对假设进行排序的新方法。</p><br /><br /><p><strong>摘要：</strong> 假设排名在自动化科学发现中至关重要，尤其是在自然科学研究领域，实验成本高且效率有限。传统方法仅依赖语言模型推理而未结合实际实验结果。本文引入实验引导的假设排序任务，旨在根据先前测试结果优先排序候选假设。然而，由于重复进行真实实验的难度，该问题难以解决。为此，我们设计了一个基于三个领域导向假设的模拟器，将假设性能建模为与已知真值假设相似性的函数，并受噪声影响。我们还整理了一个包含124个化学假设及其实验结果的数据集来验证模拟器的有效性。在此基础上，我们开发了一种伪实验引导的排序方法，通过聚类共享功能特性的假设并根据模拟反馈优化排序策略。实验表明，该方法优于传统的预实验基线和强基准模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17873" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:24:50 GMT</pubDate>
</item>
<item>
<title>QwenLong-L1：通过渐进式上下文扩展提升长上下文推理能力</title>
<link>https://arxiv.org/abs/2505.17667</link>
<guid>https://arxiv.org/abs/2505.17667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架QwenLong-L1，有效解决长上下文推理中的训练效率低和优化不稳定问题。</p><br /><br /><p><strong>摘要：</strong> 近期大型推理模型（LRMs）在强化学习（RL）的助力下展现了强大的推理能力，但这些改进主要局限于短上下文任务。针对长上下文推理任务，如何高效应用RL仍是一个未解难题。本文首次定义了长上下文推理RL的范式，并分析了其训练效率低下和优化过程不稳定的关键挑战。为此，我们提出了QwenLong-L1框架，通过逐步扩展上下文的方法将短上下文LRMs适应到长上下文场景。具体而言，该框架首先利用有监督微调建立稳健的初始策略，随后采用课程引导的分阶段RL技术稳定策略演化，并结合难度感知的回顾采样策略增强策略探索。实验表明，QwenLong-L1-32B在七个长上下文文档问答基准测试中表现优异，性能优于OpenAI-o3-mini、Qwen3-235B-A22B等旗舰LRMs，与Claude-3.7-Sonnet-Thinking相当，成为当前最先进的LRMs之一，推动了信息密集型环境中稳健推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:31:55 GMT</pubDate>
</item>
<item>
<title>Scaling Image and Video Generation via Test-Time Evolutionary Search</title>
<link>https://arxiv.org/abs/2505.17618</link>
<guid>https://arxiv.org/abs/2505.17618</guid>
<content:encoded><![CDATA[
As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:25:46 GMT</pubDate>
</item>
<item>
<title>RePrompt：通过强化学习增强文本到图像生成的提示</title>
<link>https://arxiv.org/abs/2505.17540</link>
<guid>https://arxiv.org/abs/2505.17540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架RePrompt，通过强化学习优化文本到图像生成的提示质量。</p><br /><br /><p><strong>摘要：</strong> 现有文本到图像生成模型在处理简略提示时难以准确反映用户意图，尽管有研究尝试利用大型语言模型（LLMs）改进提示，但这些方法常因缺乏视觉语义和现实世界构图的充分基础而产生风格化或不真实的图像。受语言模型推理进展的启发，我们提出了RePrompt，这是一种新的重新提示框架，通过强化学习引入显式的推理过程到提示优化中。不同于依赖手工规则或风格重写，我们的方法训练一个语言模型生成结构化的自我反思提示，目标是优化图像级结果。定制的奖励模型从人类偏好、语义对齐和视觉构图方面评估生成的图像，为细化提示生成提供间接监督。该方法实现了端到端训练而无需人工标注数据。实验表明，在GenEval和T2I-Compbench基准测试中，RePrompt显著提升了多种文本到图像生成模型的空间布局忠实度和组合泛化能力，达到了新的技术水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:44:26 GMT</pubDate>
</item>
<item>
<title>Direct3D S2：基于稀疏体素的高效3D形状生成框架</title>
<link>https://arxiv.org/abs/2505.17412</link>
<guid>https://arxiv.org/abs/2505.17412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Direct3D S2框架，利用稀疏体素显著提升3D形状生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Direct3D S2的可扩展3D生成框架，该框架基于稀疏体素实现了高分辨率3D形状生成，同时大幅降低了训练成本。Direct3D S2的关键创新在于空间稀疏注意力机制（SSA），它显著提高了Diffusion Transformer在稀疏体素数据上的计算效率，在前向和反向传播中分别实现了3.9倍和9.6倍的速度提升。此外，框架中的变分自编码器在整个输入、潜在表示和输出阶段保持一致的稀疏体素格式，从而提升了训练效率和稳定性。通过在公开数据集上的实验验证，Direct3D S2不仅在生成质量和效率上超越现有方法，还支持在8块GPU上实现1024分辨率的训练，而传统方法通常需要至少32块GPU才能完成256分辨率的训练。这一突破使得大规模3D形状生成更加实用和普及。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 22:58:01 GMT</pubDate>
</item>
<item>
<title>FullFront：前端工程全流程多模态大语言模型基准测试</title>
<link>https://arxiv.org/abs/2505.17399</link>
<guid>https://arxiv.org/abs/2505.17399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FullFront评估多模态大语言模型在前端开发全流程中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FullFront的新基准，用于评估多模态大语言模型（MLLMs）在整个前端开发流程中的能力。FullFront涵盖了三个基本任务：网页设计（概念化阶段）、网页感知质量保证（视觉组织和元素理解）以及网页代码生成（实现阶段）。与其他仅依赖于复杂或简化数据集的基准不同，FullFront通过新颖的两阶段过程处理真实世界网页，生成干净且标准化的HTML代码，同时保持多样化的设计并避免版权问题。对最新一代MLLMs的广泛测试显示，这些模型在页面感知、代码生成（特别是图像处理和布局方面）以及交互实现上存在显著局限性。研究结果定量展示了不同模型及任务间的性能差异，并揭示了当前MLLMs在前端工程中的实际能力与人类专家之间的巨大差距。FullFront基准及相关代码现已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 22:16:11 GMT</pubDate>
</item>
<item>
<title>RIPT-VLA：基于强化学习的轻量级视觉-语言-动作模型后训练范式</title>
<link>https://arxiv.org/abs/2505.17016</link>
<guid>https://arxiv.org/abs/2505.17016</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RIPT-VLA通过交互式后训练显著提升视觉-语言-动作模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RIPT-VLA的新型后训练方法，该方法利用稀疏二值成功奖励对预训练的视觉-语言-动作（VLA）模型进行微调，无需依赖大量专家演示数据。RIPT-VLA通过动态滚动采样和留一法优势估计实现了稳定策略优化算法。实验表明，RIPT-VLA不仅适用于多种VLA模型，还能大幅提升模型性能，例如将QueST模型提升了21.2%，使OpenVLA-OFT模型达到97.5%的成功率。此外，RIPT-VLA在计算效率和数据效率上表现出色，在仅有一份演示的情况下，使初始成功率仅为4%的简单微调模型在15轮迭代后成功率达到97%。研究还证明，RIPT-VLA学到的策略在不同任务和场景中具有良好的泛化能力，且对初始状态上下文具有鲁棒性。这些成果表明，RIPT-VLA是一种实用且高效的VLA模型后训练方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17016" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs</title>
<link>https://arxiv.org/abs/2505.16770</link>
<guid>https://arxiv.org/abs/2505.16770</guid>
<content:encoded><![CDATA[
The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv
]]></content:encoded>
<pubDate>Thu, 22 May 2025 11:11:57 GMT</pubDate>
</item>
<item>
<title>ClearNight：多天气夜间图像复原框架</title>
<link>https://arxiv.org/abs/2505.16479</link>
<guid>https://arxiv.org/abs/2505.16479</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ClearNight框架，有效复原受多种恶劣天气影响的夜间图像。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多天气条件下夜间图像复原这一具有挑战性的研究问题，提出了ClearNight框架，该框架结合Retinex双先验提取技术及天气感知动态特异性-共同性协作方法，可以一次性高效去除复杂的多重退化效果。同时，构建了AllWeatherNight数据集，用于支持研究并验证模型性能。实验表明，ClearNight在合成及真实世界图像上均达到当前最优水平，且消融实验进一步证明了数据集和框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16479" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 06:06:35 GMT</pubDate>
</item>
<item>
<title>Notes Writing提升多跳问答中的迭代RAG性能</title>
<link>https://arxiv.org/abs/2505.16293</link>
<guid>https://arxiv.org/abs/2505.16293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Notes Writing方法，通过生成简洁相关笔记减少噪声并提高模型处理能力。</p><br /><br /><p><strong>摘要：</strong> 当前基于检索增强生成（RAG）的多跳问答系统面临长上下文和无关信息积累的问题，限制了模型的推理能力。现有压缩检索信息的方法要么局限于单轮RAG，要么需要微调且扩展性差。为解决这些问题，我们提出了Notes Writing方法，在每次迭代中生成简洁相关的文档笔记，有效降低噪声并保留关键信息，间接增加了大型语言模型的有效上下文长度。该方法框架无关，可与多种迭代RAG方法集成。实验表明，Notes Writing在三种迭代RAG方法、两种模型和四个评估数据集上平均提升了15.6个百分点的整体性能，同时仅小幅增加输出标记数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 02:45:05 GMT</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:13:09 GMT</pubDate>
</item>
<item>
<title>NOVER：无需外部验证器的强化学习框架</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需外部验证器的强化学习框架NOVER，显著提升文本到文本任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为NOVER的新框架，该框架是一种无需外部验证器的强化学习方法，仅需标准监督微调数据即可实现激励训练。与依赖外部评估器的传统方法不同，NOVER通过奖励机制直接基于语言模型输出的最终答案部分进行强化学习，从而鼓励生成中间推理步骤。实验表明，NOVER在多种文本到文本任务上表现优异，其优化的模型性能超越了从大型推理模型（如DeepSeek R1 671B）蒸馏出的同规模模型7.7个百分点。此外，NOVER的灵活性还开启了逆向激励训练等新优化可能性，进一步拓展了大语言模型的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16022" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 17:12:35 GMT</pubDate>
</item>
<item>
<title>PhyX：首个大规模物理推理视觉场景基准测试</title>
<link>https://arxiv.org/abs/2505.15929</link>
<guid>https://arxiv.org/abs/2505.15929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhyX评估模型在物理推理上的能力，发现顶级模型表现远逊于人类。</p><br /><br /><p><strong>摘要：</strong> 现有智能评估基准未能涵盖智力的关键方面——物理推理。为填补这一空白，我们推出了PhyX，这是首个专门用于评估模型在视觉场景中物理推理能力的大规模基准。PhyX包含3000个精心策划的多模态问题，涉及六大核心物理领域及25种子领域中的六种推理类型。通过全面评估发现，即使最先进的模型如GPT-4o、Claude3.7-Sonnet和GPT-o4-mini，在物理推理上也表现不佳，分别仅达到32.5%、42.2%和45.8%的准确率，与人类专家相比存在超过29%的性能差距。分析表明，当前模型存在过度依赖记忆知识、数学公式及表面视觉模式匹配等局限性。为了深入剖析这些问题，我们采用了多种评估范式进行详细案例研究，并提供兼容的评估协议以促进可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 14:33:50 GMT</pubDate>
</item>
<item>
<title>大规模语言模型在敏感领域的上下文安全保护研究</title>
<link>https://arxiv.org/abs/2505.15805</link>
<guid>https://arxiv.org/abs/2505.15805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有大模型在敏感领域中难以遵守用户定义的安全策略。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在企业及政府等敏感领域的应用日益广泛，确保这些模型能够遵循用户定义的上下文安全政策变得至关重要，尤其是针对信息非披露的要求。然而，尽管已有研究关注LLMs的一般安全性和社会敏感数据处理，但针对上下文安全性的大规模基准测试仍然匮乏。为解决这一问题，本文引入了一个名为CoPriva的新基准数据集，用于评估问答场景下LLMs对上下文非披露策略的遵守情况。该数据集基于现实情境构建，包含明确的策略声明和查询设计，旨在模拟直接与间接的信息泄露攻击。通过对10种主流LLMs进行测试，研究发现许多模型存在严重漏洞，尤其是在面对间接攻击时会泄露敏感信息。此外，分析表明，虽然模型通常能够正确回答查询，但在生成过程中整合政策约束的能力较弱，而通过显式提示修改输出则表现出一定能力。本研究揭示了当前LLMs在敏感应用场景中的重大安全隐患，强调了开发更强大保障机制的紧迫性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:58:11 GMT</pubDate>
</item>
<item>
<title>TIME：面向现实世界场景的大规模多层级时序推理基准</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TIME基准，解决现有模型在时序推理中的三大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型（LLMs）在时序推理能力上的不足，提出了一个新的多层级基准TIME，该基准旨在应对真实世界场景中的复杂时序推理问题。TIME包含38,522个问答对，涵盖三个难度级别及11个细粒度子任务，并分为反映不同现实挑战的三个子数据集：TIME-Wiki、TIME-News和TIME-Dial。通过在多种推理与非推理模型上的实验分析，研究揭示了测试时扩展对时序推理能力的影响，并发布了TIME-Lite作为人工标注的子集以促进未来研究。TIME基准及相关代码和数据已公开，有助于推动时序推理领域的标准化评估和进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 05:22:02 GMT</pubDate>
</item>
<item>
<title>基于合成数据的强化学习方法提升大模型性能</title>
<link>https://arxiv.org/abs/2505.17063</link>
<guid>https://arxiv.org/abs/2505.17063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用合成数据实现强化学习微调，显著提升大模型在多任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Synthetic Data RL的新框架，该框架通过仅使用从任务定义生成的合成数据对基础模型进行强化学习微调，从而避免了大规模人工标注数据的需求。实验表明，该方法在多个基准测试上显著优于基线模型及指令微调等传统方法。例如，在GSM8K数据集上，模型性能提升了29.2%，同时在其他领域如数学、问答等也有显著改进。此外，研究发现少量的人类演示对最终性能的提升有限，进一步证明了合成数据的有效性。这项工作展示了在减少人工标注的情况下实现高效模型适应的可能性，为强化学习的实际应用提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 01:35:13 GMT</pubDate>
</item>
<item>
<title>引入正交残差更新以提升深度神经网络的特征学习能力</title>
<link>https://arxiv.org/abs/2505.11881</link>
<guid>https://arxiv.org/abs/2505.11881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出正交残差更新策略，显著改善多种架构的泛化性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为正交残差更新的新方法，通过将模块的输出分解为与输入流平行和正交的部分，并仅添加正交部分，促使模块主要学习全新的特征方向，从而丰富特征表示并提高训练效率。实验表明，该策略在ResNetV2、视觉Transformer等架构及CIFARs、TinyImageNet、ImageNet-1k等多个数据集上提升了泛化准确率和训练稳定性，例如ViT-B在ImageNet-1k上的top-1准确率提高了+4.3个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 03:16:11 GMT</pubDate>
</item>
<item>
<title>Time-R1：为大语言模型赋予全面的时间智能</title>
<link>https://arxiv.org/abs/2505.13508</link>
<guid>https://arxiv.org/abs/2505.13508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Time-R1通过创新方法使中等规模语言模型具备出色的时间理解、预测及创造性生成能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型虽表现出色，但在时间推理方面存在明显不足，难以将历史事件理解与未来预测相结合。目前大多数相关研究仅关注单一时间技能，如过去事件问答或基础预测，且泛化能力较差。为解决这些问题，我们提出了Time-R1框架，该框架基于一个精心设计的动态规则奖励系统，采用三阶段强化学习课程，使一个中等规模（30亿参数）的语言模型获得了全面的时间理解、预测及创造能力。实验表明，Time-R1在未来的事件预测和创造性场景生成任务上超越了比其大200倍以上的模型，例如最先进的671B参数DeepSeek-R1。此外，我们还发布了Time-Bench，这是一个源自10年新闻数据的大规模多任务时间推理数据集，以及一系列Time-R1检查点，以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 09:46:28 GMT</pubDate>
</item>
<item>
<title>V-Triune：统一强化学习提升视觉语言模型的推理与感知能力</title>
<link>https://arxiv.org/abs/2505.18129</link>
<guid>https://arxiv.org/abs/2505.18129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出V-Triune系统，使视觉语言模型同时学习推理与感知任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为V-Triune的Visual Triple Unified Reinforcement Learning系统，它通过整合样本级数据格式化、验证器级奖励计算和源级指标监控三个互补组件，使得视觉语言模型(VLMs)能够在单一训练管道中联合学习视觉推理和感知任务。此外，还引入了动态IoU奖励，以提供自适应反馈。该方法基于开源7B和32B骨干模型，在多样化的数据集上进行训练，生成的Orsta模型在多种推理和感知任务上均表现出显著性能提升，特别是在MEGA-Bench Core测试中表现优异。这些成果证明了统一强化学习方法在视觉语言模型中的有效性和可扩展性。V-Triune系统及其相关模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:41:14 GMT</pubDate>
</item>
<item>
<title>VeriThinker: Learning to Verify Makes Reasoning Model Efficient</title>
<link>https://arxiv.org/abs/2505.17941</link>
<guid>https://arxiv.org/abs/2505.17941</guid>
<content:encoded><![CDATA[
Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:17:56 GMT</pubDate>
</item>
<item>
<title>Trinity-RFT：一种灵活可扩展的大语言模型强化微调框架</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Trinity-RFT是一种用于大语言模型强化微调的通用框架。</p><br /><br /><p><strong>摘要：</strong> Trinity-RFT是一种旨在实现大语言模型强化微调（RFT）的通用、灵活且可扩展的框架。该框架采用解耦设计，包含统一同步/异步、在线/离线等模式的RFT核心模块，高效稳健的智能体-环境交互集成，以及针对RFT优化的数据管道系统。其设计目标是适应多样化的应用场景，并作为探索先进强化学习范式的统一平台。本技术报告介绍了Trinity-RFT的愿景、功能、设计与实现细节，并通过大量示例展示了该框架的实用性和易用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:41:09 GMT</pubDate>
</item>
<item>
<title>Agent Distillation：将大型语言模型推理能力迁移至小规模模型</title>
<link>https://arxiv.org/abs/2505.17612</link>
<guid>https://arxiv.org/abs/2505.17612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Agent Distillation框架，使小规模语言模型通过模仿大型模型完成任务实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模语言模型（LLMs）在实际部署中的计算成本问题，提出Agent Distillation框架，该框架不仅迁移推理能力，还转移大型模型基于检索和代码工具的任务解决行为至小规模语言模型（sLMs）。通过引入first-thought prefix提示方法提升教师模型生成轨迹的质量，并采用自一致性动作生成技术增强测试时小模型的鲁棒性。实验在事实性和数学推理任务上验证了方法的有效性，证明参数量仅为0.5B、1.5B、3B的小模型在性能上可媲美更大规模的模型，展示了构建实用工具型小模型的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:20:15 GMT</pubDate>
</item>
<item>
<title>ANSE：通过主动噪声选择提升视频扩散模型质量</title>
<link>https://arxiv.org/abs/2505.17561</link>
<guid>https://arxiv.org/abs/2505.17561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ANSE框架，利用注意力机制量化不确定性，优化视频扩散模型的初始噪声选择。</p><br /><br /><p><strong>摘要：</strong> 本文研究了初始噪声对视频扩散模型生成质量和对齐速度的影响，发现相同提示下不同的噪声种子可能导致显著差异的结果。尽管现有方法依赖外部设计先验如频率滤波或帧间平滑，但忽略了内部模型信号。为此，我们提出了ANSE（Active Noise Selection for Generation），这是一种基于模型感知的框架，通过量化的基于注意力的不确定性选择高质量的噪声种子。ANSE的核心是BANSA（Bayesian Active Noise Selection via Attention），它通过多个随机注意力样本之间的熵分歧测量模型置信度和一致性。为了高效部署，我们还引入了BANSA的伯努利掩码近似版本，使得仅需单步扩散和部分注意力层即可进行评分估计。实验结果显示，在CogVideoX-2B和5B数据集上，ANSE提高了视频质量和时间一致性，分别仅增加了8%和13%的推理时间，提供了一种有原则且通用的视频扩散噪声选择方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 03:09:10 GMT</pubDate>
</item>
<item>
<title>基于课程学习的大语言模型幻觉检测方法</title>
<link>https://arxiv.org/abs/2505.17558</link>
<guid>https://arxiv.org/abs/2505.17558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用精心设计的幻觉样本作为负例的课程学习策略，提升大语言模型的幻觉检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）难以准确检测幻觉文本这一挑战，创新性地将高质量的幻觉样本作为负例引入到DPO对齐过程中。通过引入课程学习策略，逐步从概率分数降低最多的简单样本开始训练，再到复杂样本，确保了学习过程的稳定性和渐进性。实验表明，采用此方法训练的HaluCheck模型在MedHallu和HaluEval等困难基准测试中表现优异，各项指标提升最高可达24%，并且在零样本设置下展现出强大的鲁棒性，优于现有最先进的大型模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 03:05:09 GMT</pubDate>
</item>
<item>
<title>基于正则化策略梯度的在线强化学习方法研究</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RPG框架，优化大语言模型推理能力，提升训练稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了正则化策略梯度（RPG）框架，用于系统性地推导和分析在线强化学习（RL）中的KL散度正则化策略梯度方法。我们针对前向和反向KL散度正则化的两种目标函数，分别推导出对应的策略梯度和代理损失函数，同时考虑归一化与非归一化策略分布。此外，还提供了完全可微的损失函数及REINFORCE风格的梯度估计器，满足多样化的算法需求。通过在大语言模型推理任务上的实验，RPG方法在训练稳定性和性能方面表现出优于GRPO、REINFORCE++和DAPO等基线算法的结果。该研究为KL正则化在在线RL中的应用提供了新视角，并推动了大语言模型推理能力的提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:01:21 GMT</pubDate>
</item>
<item>
<title>基于语义表征的语音指令数据生成方法</title>
<link>https://arxiv.org/abs/2505.17417</link>
<guid>https://arxiv.org/abs/2505.17417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需文本到语音模型的语音指令数据生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型驱动的语音助手快速发展，对训练这些系统所需的语音指令数据的需求日益增长。然而，尽管存在大量语音识别数据，高质量的语音指令数据却相对匮乏。传统生成高质量合成语音需要依赖优秀的文本到语音模型，但对于资源匮乏的语言来说，这一条件难以满足。本文提出了一种创新方法，在语义表示层停止合成过程，绕过对文本到语音模型的依赖。通过将合成的语义表示与预训练的Whisper编码器对齐，使大型语言模型能够在文本指令上进行微调的同时，在推理过程中仍能理解口语指令。此简化训练流程为低资源语言构建语音助手提供了有前景的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 23:05:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的推理刚性问题及其诊断研究</title>
<link>https://arxiv.org/abs/2505.17225</link>
<guid>https://arxiv.org/abs/2505.17225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型常因推理惯性忽视用户指令，本文通过构建诊断集揭示其问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在复杂推理任务中表现出色，但普遍存在推理刚性问题，即倾向于依赖熟悉的推理模式，即使面对明确的用户指示也容易偏离正轨，导致错误结论。这一现象在数学和逻辑领域尤为突出。为了系统研究推理刚性，本文构建了一个由专家精心策划的诊断数据集，其中包括对AIME和MATH500等基准测试的修改版本，以及重新设计的经典谜题，旨在迫使模型偏离常规推理路径。通过分析，我们识别出三种主要的污染模式：解释过载、输入不信任和部分指令关注，这些模式导致模型忽略或扭曲提供的指示。本研究公开发布了诊断集，以促进未来缓解语言模型推理刚性的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 15:00:01 GMT</pubDate>
</item>
<item>
<title>CANOE框架提升大语言模型生成任务的准确性</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需人工标注的CANOE框架，显著提高大语言模型在多种任务中的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CANOE的系统框架，旨在提升大型语言模型（LLMs）在短文本和长文本生成任务中的准确性，而无需依赖人工标注。通过合成多样化的问答数据，构建高质量且易于验证的训练集，并引入基于规则的强化学习方法Dual-GRPO，该框架优化了生成任务中的规则性奖励。实验表明，CANOE在11项下游任务中均表现出色，甚至超过了最先进的模型如GPT-4o和OpenAI o1。这项研究强调了在信息检索系统中构建可靠模型的重要性，同时提供了有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 06:10:07 GMT</pubDate>
</item>
<item>
<title>Transformer Copilot：通过日志驱动优化的大语言模型增强框架</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Transformer Copilot框架，通过日志记录和日志驱动的日志纠正显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Transformer Copilot的新框架，该框架通过引入日志驱动的学习机制来改进大型语言模型的下游任务表现。Transformer Copilot由Pilot模型和Copilot模型组成，其中Pilot模型作为主模型，而Copilot模型则负责通过修正预测结果（logits）来提高Pilot的推理性能。通过构建Mistake Log来系统地跟踪和分析模型在微调过程中的错误模式，Transformer Copilot实现了对模型学习信号的深度利用。实验表明，在涉及常识推理、算术问题及推荐系统的12项基准测试中，Transformer Copilot可以将模型性能提升高达34.5%，同时保持较低的计算开销和良好的可扩展性与迁移能力。此外，我们还提供了理论和实证分析以支持该框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 02:00:45 GMT</pubDate>
</item>
<item>
<title>AudioTrust：首个面向音频大语言模型的多维度可信性评估框架</title>
<link>https://arxiv.org/abs/2505.16211</link>
<guid>https://arxiv.org/abs/2505.16211</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个针对音频大语言模型的多维度可信性评估框架AudioTrust。</p><br /><br /><p><strong>摘要：</strong> 现有的大型语言模型评估框架主要集中在文本模态，而针对音频模态的系统性研究较少，尤其缺乏对音频特性及应用场景的独特考量。本文引入AudioTrust，这是首个专门设计用于评估音频大型语言模型（ALLMs）可信性的多维度框架和基准。AudioTrust涵盖公平性、幻觉生成、安全性、隐私保护、鲁棒性和认证六个关键维度，并通过18种实验设置进行综合评估。该框架基于超过4,420个来自真实场景（如日常对话、紧急呼叫、语音助手交互）的音频/文本样本构建，采用9个音频专用评估指标，并结合大规模自动化评分管道实现客观评估。实验结果显示当前最先进的开源和闭源ALLMs在高风险音频场景中的可信性边界与局限性，为未来音频模型的安全部署提供了重要参考。AudioTrust平台和基准现已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16211" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:27:46 GMT</pubDate>
</item>
<item>
<title>TAPO：通过融入外部知识提升强化学习推理能力</title>
<link>https://arxiv.org/abs/2505.15692</link>
<guid>https://arxiv.org/abs/2505.15692</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入外部知识的TAPO框架显著提升了推理模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TAPO的新框架，它通过整合外部高阶指导（即“思想模式”）增强强化学习的能力。与传统方法相比，TAPO在训练过程中平衡了内部探索与外部指导利用，从而显著提高了推理模型的表现，在AIME、AMC和Minerva Math等任务上的表现均优于现有方法。实验表明，这些抽象自少量先验样本的思想模式可以有效推广到多种任务和模型中，展示了TAPO在多领域应用中的潜力。此外，引入外部指导还增强了模型的解释性和输出可读性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15692" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 12:06:10 GMT</pubDate>
</item>
<item>
<title>基于真实表情包评估视觉语言模型的安全性</title>
<link>https://arxiv.org/abs/2505.15389</link>
<guid>https://arxiv.org/abs/2505.15389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现当前视觉语言模型对普通用户分享的表情包更易产生有害输出。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了现有视觉语言模型(VLMs)在面对普通用户广泛传播的表情包时的安全性问题。通过构建MemeSafetyBench数据集，包含50,430组真实表情包及其对应指令，结合大语言模型生成指令，我们评估了多种VLMs在单轮与多轮交互中的安全性表现。研究显示，表情包相较于合成或纯文本输入显著增加了有害输出并降低了拒绝率，尽管多轮对话提供了一定缓解作用，但模型仍表现出较高脆弱性。研究强调了生态有效评估的重要性及加强安全机制的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:26:40 GMT</pubDate>
</item>
<item>
<title>基于文本大模型的跨模态学习能力研究</title>
<link>https://arxiv.org/abs/2505.17091</link>
<guid>https://arxiv.org/abs/2505.17091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过训练文本模型，模型自动发展出理解和处理图像及音频的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种有趣的现象：通过在文本标记上训练自回归语言模型（LLM），该模型在内部自发发展出理解图像和音频的能力，从而仅通过阅读就能具备视觉和听觉功能。传统方法通常通过微调文本LLM生成特定条件下的音频或视觉输出，而我们提出的架构直接将图像块、音频波形或标记作为输入，输出分类嵌入或标签。实验表明，这种基于文本权重的方法在FSD-50K、GTZAN数据集上的音频分类以及CIFAR-10、Fashion-MNIST数据集上的图像分类中表现良好，甚至可以处理图像补丁。这一发现推动了文本-LLM学习强大内部电路的概念，即激活必要的连接即可用于多种应用，而非每次都需要从头开始训练模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 18:20:16 GMT</pubDate>
</item>
<item>
<title>利用生成模型的感知组织能力实现类别无关实例分割</title>
<link>https://arxiv.org/abs/2505.15263</link>
<guid>https://arxiv.org/abs/2505.15263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过微调Stable Diffusion和MAE实现类别无关的实例分割，发现生成模型具备跨类别泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何利用生成模型对物体边界的内在理解，将其应用于通用感知组织任务。研究者通过引入实例着色损失函数，在有限的室内家具和汽车类型上对Stable Diffusion和MAE进行微调，实现了类别无关的实例分割。令人惊讶的是，这些模型在未见过的对象类型和风格上表现出强大的零样本泛化能力，甚至超过了高度监督的SAM模型在细结构和模糊边界上的表现。相比之下，现有的提示可调分割架构或判别式预训练模型无法泛化。这表明生成模型学习到的分组机制可以跨类别和领域转移，即使没有互联网规模的预训练。相关代码、预训练模型和演示可在项目网站获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 04:42:05 GMT</pubDate>
</item>
<item>
<title>RoPECraft：基于旋转位置嵌入的无训练视频动作迁移方法</title>
<link>https://arxiv.org/abs/2505.13344</link>
<guid>https://arxiv.org/abs/2505.13344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅通过修改旋转位置嵌入实现扩散变压器视频动作迁移的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RoPECraft的训练自由视频运动转移方法，该方法专门针对扩散变压器设计，通过仅修改其旋转位置嵌入（RoPE）来实现动作迁移。首先，从参考视频中提取密集光流，并利用运动偏移量扭曲复杂指数张量的RoPE，从而将运动编码到生成过程中。在去噪时间步中，通过目标速度和预测速度之间的轨迹对齐进一步优化这些嵌入，采用流匹配目标函数。为了保持输出忠实于文本提示并防止重复生成，还引入了一个基于参考视频傅里叶变换相位分量的正则化项，将相位角度投影到一个平滑流形上以抑制高频伪影。实验表明，RoPECraft在基准测试中超越了所有近期发布的同类方法，无论是定性还是定量评估均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 12:50:26 GMT</pubDate>
</item>
<item>
<title>通过高质量训练数据提升检索与重排序模型性能</title>
<link>https://arxiv.org/abs/2505.16967</link>
<guid>https://arxiv.org/abs/2505.16967</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">去除部分低质量数据集显著提升检索模型效果。</p><br /><br /><p><strong>摘要：</strong> 目前构建强大的检索与重排序模型通常依赖大规模数据集，如BGE集合包含160万查询-文档对。然而，我们发现某些数据集可能对模型有效性产生负面影响，在从BGE集合中剔除8个数据集后，训练集规模缩小至原来的1/2.35，同时BEIR上的nDCG@10得分提升了1.0点。因此，本文深入研究了训练数据的质量问题，特别是“假阴性”现象，即相关文档被错误标记为不相关的情况。为此，提出了一种简单且成本效益高的方法，利用级联大型语言模型提示来识别并重新标注这些假阴性样本。实验结果显示，将假阴性样本重新标注为真阳性可以提高E5（基础版）和Qwen2.5-7B检索模型在BEIR上的nDCG@10得分0.7到1.4点，并在零样本AIR-Bench评估中提升1.7到1.8点。对于基于重新标注数据微调的重排序器，例如Qwen2.5-3B在BEIR上的表现也有类似改进。此外，人类注释结果表明，GPT-4o的判断与人工标注的一致性远高于GPT-4o-mini，进一步验证了该级联设计的可靠性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16967" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:47:57 GMT</pubDate>
</item>
<item>
<title>WebAgent-R1：一种高效的多轮网络代理强化学习框架</title>
<link>https://arxiv.org/abs/2505.16421</link>
<guid>https://arxiv.org/abs/2505.16421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebAgent-R1显著提升了多轮网络交互任务的成功率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WebAgent-R1的端到端多轮强化学习框架，用于训练网络代理处理复杂的动态网页环境。该框架通过异步生成多样化轨迹，仅依赖任务成功与否的二元奖励进行指导。实验表明，WebAgent-R1在WebArena-Lite基准测试中大幅提高了Qwen和Llama等模型的任务成功率。此外，研究还探讨了基于思考的提示策略及测试时扩展方法的有效性，并通过引入WebAgent-R1-Zero和WebAgent-R1-CoT两种变体，强调了行为克隆暖启动阶段的重要性以及长链推理在网页任务中的应用价值。这些成果显著优于现有最先进的方法，为多轮网络交互提供了新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:07:43 GMT</pubDate>
</item>
<item>
<title>Think-RM：一种基于生成式奖励模型的强化学习新框架</title>
<link>https://arxiv.org/abs/2505.16265</link>
<guid>https://arxiv.org/abs/2505.16265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Think-RM通过长时推理提升生成式奖励模型性能，优化强化学习从人类反馈。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Think-RM的新框架，旨在通过模拟内部思考过程，使生成式奖励模型（GenRMs）具备长时推理能力。与传统方法相比，Think-RM生成灵活的自我引导推理轨迹，支持反思、假设推理等高级能力。研究首先通过监督微调（SFT）利用长链式思维数据对模型进行预热，随后借助基于规则的强化学习进一步增强其长时推理能力。此外，我们还设计了一种新颖的基于成对偏好的强化学习从人类反馈（RLHF）管道，直接优化策略而不需转换为点式奖励信号。实验表明，Think-RM在RM-Bench基准测试中表现优异，比传统Bradley-Terry奖励模型和垂直扩展的GenRM分别高出8%和显著改进。结合成对RLHF管道后，其最终策略性能也优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 01:56:11 GMT</pubDate>
</item>
<item>
<title>FoVer：基于形式验证的大规模语言模型过程奖励模型训练方法</title>
<link>https://arxiv.org/abs/2505.15960</link>
<guid>https://arxiv.org/abs/2505.15960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FoVer方法，通过形式验证工具自动生成标注，提升大规模语言模型在多种推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于解决过程奖励模型（PRMs）在大规模语言模型（LLMs）推理过程中面临的两大挑战：一是人工标注成本高昂，二是现有PRMs局限于数学推理问题。为填补这些研究空白，我们提出了FoVer方法，利用形式验证工具如Z3和Isabelle自动生成符号任务的逐级错误标签，从而无需人工标注即可创建训练数据集。实验表明，基于该数据集训练的PRMs不仅在形式逻辑和定理证明任务上表现出色，还能在跨任务验证中显著超越传统基线PRMs，并达到与人类标注或更强模型训练的PRMs相当甚至更好的效果。此外，相关数据集、模型及代码已公开共享。关键词：过程奖励模型，形式验证，跨任务泛化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:23:45 GMT</pubDate>
</item>
<item>
<title>RAVENEA：通过检索增强视觉文化理解</title>
<link>https://arxiv.org/abs/2505.14462</link>
<guid>https://arxiv.org/abs/2505.14462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入RAVENEA基准，提升视觉文化理解能力。</p><br /><br /><p><strong>摘要：</strong> 随着视觉语言模型(VLMs)日益融入日常生活，准确理解视觉文化变得至关重要。然而，现有模型常无法有效解读文化细微差别。虽然文本检索增强生成(RAG)在单模态场景中已被证明有效，但在多模态应用方面仍待探索。为此，我们提出了RAVENEA（Retrieval-Augmented Visual culturE uNdErstAnding），这是一个新的基准，旨在通过检索推动视觉文化理解，专注于文化导向的视觉问答(cVQA)和图像描述(cIC)两个任务。RAVENEA通过整合超过10,000份由人工注释者整理和排名的Wikipedia文档扩展了现有数据集。通过训练和评估七个多模态检索器，我们发现当轻量级VLMs结合文化感知检索时，在cVQA和cIC任务上分别比非增强版本高出至少3.2%和6.2%，证明了检索增强方法及文化包容性基准的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:57:16 GMT</pubDate>
</item>
<item>
<title>SAKURA：评估大型音频语言模型多跳推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.13237</link>
<guid>https://arxiv.org/abs/2505.13237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型音频语言模型在多跳推理方面存在挑战。</p><br /><br /><p><strong>摘要：</strong> 尽管大型音频语言模型（LALMs）在语音和音频处理任务上的表现得到了广泛研究，但其推理能力尤其是多跳推理能力却未被充分探索。现有基准主要集中在一般语音和音频处理任务、对话能力和公平性等方面，忽视了对多跳推理的系统评估。为填补这一空白，我们提出了SAKURA，这是一个基于语音和音频信息评估LALMs多跳推理能力的新基准。实验结果显示，即使LALMs能够正确提取相关信息，它们在整合语音/音频表示进行多跳推理时仍面临困难。这一发现揭示了LALMs的一个关键局限性，为未来研究提供了重要的见解和资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 11:20:32 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升视觉生成中的语义-空间推理能力</title>
<link>https://arxiv.org/abs/2505.17022</link>
<guid>https://arxiv.org/abs/2505.17022</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GoT-R1框架，利用强化学习增强视觉生成模型的语义-空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GoT-R1的新框架，该框架将强化学习应用于视觉生成模型，以提高其处理复杂文本提示的能力，特别是那些涉及多个具有精确空间关系和属性的对象的提示。GoT-R1基于Generation Chain-of-Thought方法，使模型能够自主发现有效的推理策略，而无需依赖预定义模板。为此，我们提出了一个双阶段多维奖励框架，利用多语言大模型（MLLMs）评估推理过程和最终输出，从而在整个生成管道中实现有效监督。奖励系统采用统一的方法评估语义对齐、空间准确性以及视觉质量。实验结果显示，在T2I-CompBench基准上的表现显著提升，特别是在涉及精确空间关系和属性绑定的组合任务上。GoT-R1的成功标志着图像生成领域在引入复杂推理能力方面取得了新的进展。为了促进未来的研究，我们的代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17022" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>基于三阶段框架Let Androids Dream的图像隐含含义理解</title>
<link>https://arxiv.org/abs/2505.17019</link>
<guid>https://arxiv.org/abs/2505.17019</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架LAD，有效解决AI在图像隐含含义理解中的上下文缺失问题。</p><br /><br /><p><strong>摘要：</strong> 现有AI系统在理解图像隐喻时面临文化、情感及语境等复杂因素的挑战，而多模态大型语言模型虽在基础视觉问答任务上表现良好，但在处理图像隐含意义时存在上下文断层的问题。受人类认知启发，我们提出了名为Let Androids Dream (LAD) 的新型框架，通过感知、搜索和推理三个阶段解决上下文缺失问题。实验表明，使用轻量级GPT-4o-mini模型的LAD在英文基准测试中达到当前最佳性能，在中文基准测试中也取得了显著改进。此外，该研究为AI更有效地解读图像隐含含义提供了新视角，推动了视觉-语言推理和人机交互领域的发展。项目代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17019" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的多帧空间理解增强</title>
<link>https://arxiv.org/abs/2505.17015</link>
<guid>https://arxiv.org/abs/2505.17015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种框架，通过整合深度感知等能力使多模态大语言模型具备多帧空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）在视觉任务上取得了显著进展，但其空间理解主要局限于单张图像，难以满足需要多帧推理的实际应用需求，如机器人学。本文提出了一种新框架，通过引入深度感知、视觉对应及动态感知，使MLLMs具备强大的多帧空间理解能力。为此，我们创建了MultiSPA数据集，该数据集包含超过2700万个样本，覆盖多样化的3D和4D场景。同时，我们设计了一个综合基准测试，用于评估多种空间任务的表现。实验结果显示，我们的模型Multi-SpatialMLLM在多项基准测试中显著优于基线模型和专有系统，展示了其在多帧推理上的可扩展性和泛化能力。此外，我们还观察到多任务收益以及在复杂场景中的潜在能力，并展示了该模型如何作为机器人领域的多帧奖励标注器发挥作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>AgentIF：大型语言模型在具身场景下指令遵循能力的评估基准</title>
<link>https://arxiv.org/abs/2505.16944</link>
<guid>https://arxiv.org/abs/2505.16944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个用于评估具身场景下大型语言模型指令遵循能力的基准AgentIF。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgentIF，这是首个专门设计用来系统性评估大型语言模型（LLMs）在具身应用场景中指令遵循能力的基准。AgentIF具有三个显著特点：基于真实世界的应用构建，平均长度达1723词；复杂度高，每条指令平均包含11.9个约束条件；涵盖多样化的约束类型如工具规格和条件约束等。通过收集来自工业应用代理和开源系统的707个人工标注指令，研究者们对这些指令及其相关约束进行了详细的注释，并采用了多种评估方法进行测试。结果显示当前主流模型在此类任务上的表现普遍不佳，特别是在处理复杂的约束结构方面存在明显不足。此外，通过对指令长度及元约束等因素进行分析，揭示了一些现有模型存在的典型失效模式。本研究已公开代码与数据集，以促进后续研究工作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:31:10 GMT</pubDate>
</item>
<item>
<title>基于Itakura-Saito散度的风险规避强化学习</title>
<link>https://arxiv.org/abs/2505.16925</link>
<guid>https://arxiv.org/abs/2505.16925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的数值稳定损失函数用于风险规避强化学习。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了风险规避强化学习在高风险领域的应用，其核心在于通过指数效用函数构建Bellman方程，并对传统强化学习算法进行少量修改。然而，这些方法存在数值不稳定性问题，因为需要在整个过程中计算指数值。为了解决这一问题，我们引入了一种基于Itakura-Saito散度的数值稳定且数学严谨的损失函数，用于学习状态值和动作值函数。我们通过理论分析和实证研究验证了该损失函数的表现，并在多个金融场景中展示了其优越性，部分场景具有已知解析解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:18:07 GMT</pubDate>
</item>
<item>
<title>基于大语言模型个性化文学翻译的研究</title>
<link>https://arxiv.org/abs/2505.16612</link>
<guid>https://arxiv.org/abs/2505.16612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索低资源环境下个性化大语言模型翻译的方法。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于基于大语言模型（LLMs）的高质量机器翻译系统在处理隐式风格要求时的局限性，特别是在文学翻译这一具有挑战性的领域。我们研究了多种提示策略及推理阶段干预方法，以引导模型生成符合个人风格的翻译。此外，提出了一种对比框架，利用稀疏自动编码器提取的潜在概念来识别重要的个性化属性。实验结果显示，通过我们的方法可以实现强个性化的翻译，同时保持翻译质量。进一步分析表明，用于个性化处理的模型层在多示例提示和我们的方法下受到相似的影响，暗示二者可能存在类似的机制。关键词：个性化翻译，大语言模型，文学翻译。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 08:47:16 GMT</pubDate>
</item>
<item>
<title>VLM-R^3：结合视觉区域识别与推理的多模态大语言模型</title>
<link>https://arxiv.org/abs/2505.16192</link>
<guid>https://arxiv.org/abs/2505.16192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架VLM-R^3，提升多模态大语言模型处理复杂视觉推理任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VLM-R^3的新框架，它通过增强多模态大语言模型（MLLMs）的视觉区域识别和推理能力，解决复杂任务中需要动态聚焦和反复查看视觉区域的问题。VLM-R^3的核心是区域条件强化策略优化（R-GRPO），该方法通过奖励模型选择信息丰富的视觉区域、制定适当的变换操作并将相关视觉上下文整合到后续推理步骤中，从而实现对文本推理的精确视觉定位。为了启动这一策略，我们构建了一个精心策划的视觉语言交互解释（VLIR）语料库，提供逐级监督以指导区域选择和文本解释。实验表明，VLM-R^3在零样本和少量样本设置下显著提升了性能，在需要微妙空间推理或精细视觉线索提取的问题上表现尤为突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:50:13 GMT</pubDate>
</item>
<item>
<title>SafeKey：通过关键句激活提升大推理模型的安全泛化能力</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出SafeKey方法，通过激活关键句中的安全洞察来提升大推理模型对未知有害查询的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）通过显式推理的方式显著提升了复杂任务的表现，但同时也带来了严重的安全风险。尽管监督微调（SFT）等主流安全策略在一定程度上改善了模型的安全性，但这些模型在面对未见过的越狱提示时仍表现不佳。本研究深入分析了LRMs的生成过程，发现了一个被称为“aha时刻”的安全洞察，它能够在关键句中触发安全推理并引导模型给出安全响应。基于这一发现，我们提出了SafeKey方法，该方法包括两个互补的目标：(1) 双路径安全头，增强模型内部表示中的安全信号；(2) 查询掩码建模目标，提高模型对查询理解的关注度。实验表明，SafeKey方法在多个安全基准测试中显著提高了模型对越狱攻击和分布外有害提示的安全泛化能力，平均有害率降低了9.6%，同时保持了模型的一般能力。此外，我们的分析揭示了SafeKey如何通过重塑内部注意力机制和改进隐藏表示的质量来增强安全性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:46:03 GMT</pubDate>
</item>
<item>
<title>大型语言模型如何承认错误？错误重述行为的研究</title>
<link>https://arxiv.org/abs/2505.16170</link>
<guid>https://arxiv.org/abs/2505.16170</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型能够承认错误但频率较低，内部信念影响其重述能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在面对自身知识矛盾时是否能够承认错误的行为，定义此行为为“重述”。通过构建特定模型的数据集，我们发现尽管LLMs具备重述能力，但实际发生的频率较低。进一步研究表明，这种行为与模型内部信念紧密相关，即当模型认为答案正确时，即便错误也不易重述。实验表明，内部信念对重述行为有因果影响，且当模型不信任自身答案时会尝试验证并改变注意力分布。最后，通过简单的监督微调显著提升了模型的重述性能。本研究代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16170" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:16:00 GMT</pubDate>
</item>
<item>
<title>现代日期处理在语言模型中的挑战与改进</title>
<link>https://arxiv.org/abs/2505.16088</link>
<guid>https://arxiv.org/abs/2505.16088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，现代分词器会破坏日期结构，影响时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现代基于BPE的分词器在处理日期时的问题，例如将连续数字拆分为不连贯的部分（如20250312 → 202, 503, 12），这不仅增加了标记数量，还削弱了时间推理所需的整体结构。为此，我们引入了一个名为日期碎片化比率的新指标，用于衡量分词器对多字符日期组件的保真度，并发布了DateAugBench基准数据集，包含跨越三种时间推理任务的6500个示例。通过实验，我们发现大型语言模型中存在一种新兴的日期抽象机制，即自动拼接年、月、日片段进行时间推理。研究进一步表明，过度碎片化会导致罕见日期（如历史或未来日期）的准确性下降高达10个百分点，而更大规模的模型能更快完成这种抽象过程。此外，我们观察到语言模型的时间推理路径与人类解读方式不同（通常是从年份开始）。这些发现为提升时间推理性能提供了新的视角和方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 20:06:29 GMT</pubDate>
</item>
<item>
<title>基于拓扑优化的大型语言模型物理与空间推理能力评估数据集</title>
<link>https://arxiv.org/abs/2505.16048</link>
<guid>https://arxiv.org/abs/2505.16048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一个新数据集，用于评估大型语言模型在拓扑优化中的物理与空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个新的数据集，旨在评估大型语言模型（LLMs）在基于拓扑优化的物理与空间推理能力。该数据集通过提供二维边界、施加力和支撑等条件，要求LLMs推断出最优的材料分布。这些任务涵盖了从填补部分结构中的掩码区域到预测完整的材料分布，需要理解力的流动及在特定约束下的材料需求，而无需借助模拟工具或显式的物理模型。数据集强调对结构稳定性和空间组织的推理能力，适用于二维环境下的评估，为传统语言和逻辑基准测试提供了补充视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:00:20 GMT</pubDate>
</item>
<item>
<title>VideoGameQA-Bench：推动游戏开发质量保障自动化</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入VideoGameQA-Bench基准以评估视觉语言模型在游戏测试中的性能。</p><br /><br /><p><strong>摘要：</strong> 随着视频游戏成为娱乐行业收入最高的领域，优化开发工作流变得至关重要。尽管视觉-语言模型（VLMs）在提升游戏开发效率方面展现出巨大潜力，但目前的游戏质量保障（QA）仍面临高度劳动密集且自动化不足的问题。现有基准难以满足游戏QA的具体需求，为此，我们推出了VideoGameQA-Bench，该基准涵盖视觉单元测试、回归测试、异常检测等多类游戏QA任务。这一工具旨在标准化评估VLMs在实际场景中的表现，从而促进游戏开发的自动化进程。代码与数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:08:38 GMT</pubDate>
</item>
<item>
<title>引入视觉信息的强化学习推理模型GRIT</title>
<link>https://arxiv.org/abs/2505.15879</link>
<guid>https://arxiv.org/abs/2505.15879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合图像和文本的视觉推理新方法GRIT。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Grounded Reasoning with Images and Texts (GRIT)的新方法，用于训练多模态大语言模型(MLLMs)进行基于图像的推理。GRIT通过引入一种结合自然语言和显式边界框坐标的推理范式，增强了模型对视觉信息的整合能力，使其能够生成清晰且视觉上可靠的推理链。此外，GRIT采用了一种基于强化学习的方法GRPO-GR，该方法专注于最终答案的准确性和推理输出的格式，从而避免了对带推理链注释的数据或显式边界框标签的需求。实验表明，GRIT在数据效率方面表现优异，仅需少量现有数据集中的图像-问题-答案三元组即可实现高效的训练，显著提升了模型生成连贯且视觉上可靠推理链的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:54:49 GMT</pubDate>
</item>
<item>
<title>基于机器人轨迹数据增强视觉语言模型的VQA数据集生成框架</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用机器人轨迹数据生成用于视觉问答的VQA数据集，提升视觉语言模型的空间与交互推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Robo2VLM的视觉问答(VQA)数据集生成框架，该框架通过真实多模态机器人轨迹数据增强和评估视觉语言模型(VLMs)。Robo2VLM从机器人轨迹中非视觉和非描述性传感器模态（如末端执行器位置、夹爪开度和力传感）中推导出真值，并将其分割成一系列操作阶段。在每个阶段，它利用场景和交互理解识别机器人、任务目标和目标物体的三维属性，进而生成基于空间、目标条件和交互推理的问题模板的代表性VQA查询。最终，我们构建了Robo2VLM-1，这是一个包含684,710个问题的大规模野外数据集，覆盖463个独特场景和来自176,000个真实机器人轨迹的3,396个机器人操作任务。实验结果表明，Robo2VLM-1可以作为基准来衡量和改进VLMs在空间和交互推理方面的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 09:42:52 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型中光学字符识别头的功能解析</title>
<link>https://arxiv.org/abs/2505.15865</link>
<guid>https://arxiv.org/abs/2505.15865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型视觉语言模型中光学字符识别头的独特功能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型视觉语言模型（LVLMs）中的不同头部结构，特别聚焦于负责从图像中识别文本的光学字符识别头（OCR Head）。研究发现，OCR头部相较于传统检索头部表现出了激活模式不稀疏、特性显著不同及激活频率与OCR评分紧密关联等特点。通过下游任务验证，包括链式思维（CoT）应用与头部掩蔽实验，进一步确认了这些发现的有效性。此外，重新分配OCR头部内的sink-token值可提升模型性能，从而加深了对LVLMs处理嵌入图像文本机制的理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 06:53:41 GMT</pubDate>
</item>
<item>
<title>通过填补思维跳跃提升大规模语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2505.14684</link>
<guid>https://arxiv.org/abs/2505.14684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CoT Thought Leap Bridge Task解决数学推理中的思维跳跃问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有数学Chain-of-Thought（CoT）数据集中存在的思维跳跃问题，提出了一项名为CoT Thought Leap Bridge Task的任务，旨在自动检测并生成缺失的中间推理步骤，从而恢复CoT的完整性和连贯性。为此，我们基于ScaleQuestMath构建了一个专门的训练数据集ScaleQM+，并训练了CoT-Bridge模型来填补这些跳跃。实验表明，微调后的模型在NuminaMath等数学推理基准测试中表现显著优于原始模型，提升了多达5.87%。此外，该方法不仅增强了蒸馏数据的效果，还为强化学习提供了更好的起点，并且在跨领域逻辑推理任务中也表现出更强的泛化能力。这一成果证明了提升推理完整性具有广泛的应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14684" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>Self-Braking Tuning:缓解大模型过推理问题的新框架</title>
<link>https://arxiv.org/abs/2505.14604</link>
<guid>https://arxiv.org/abs/2505.14604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为Self-Braking Tuning的框架，解决大模型推理冗余问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型（LRMs）在提升推理能力过程中产生的冗余推理问题，提出了一种名为Self-Braking Tuning（SBT）的新框架。该框架通过让模型自行调控推理过程，避免依赖外部干预机制。研究构建了基于标准答案的过推理识别指标，并设计了一种系统性方法来检测不必要的推理步骤，从而生成训练信号以学习自我调节行为。此外，还开发了一种自适应推理长度的数据构造策略和创新的刹车提示机制，使模型能够在适当阶段自然终止推理。实验表明，该方法在数学基准测试中可减少高达60%的令牌消耗，同时保持与不受约束模型相当的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:53:40 GMT</pubDate>
</item>
<item>
<title>MUG-Eval：一种评估多语言大型语言模型生成能力的新框架</title>
<link>https://arxiv.org/abs/2505.14395</link>
<guid>https://arxiv.org/abs/2505.14395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架MUG-Eval，用于评估多语言大型语言模型的生成能力。</p><br /><br /><p><strong>摘要：</strong> 评估大型语言模型（LLMs）的文本生成能力具有挑战性，尤其是在资源匮乏的语言中。本文提出了一种名为MUG-Eval的新框架，该框架通过将现有基准转换为会话任务，并测量模型在这些任务上的准确性来评估LLMs的多语言生成能力。为了确保有效性，设计的任务需要目标语言中的有效交流。通过任务成功率作为成功对话生成的代理指标，该方法避免了对特定语言NLP工具或标注数据集的依赖，也不依赖于LLMs作为评判者，从而在多种语言和模型间提供标准化比较。研究发现，MUG-Eval与已建立的基准高度相关（r > 0.75），并且可以在数千种语言中扩展。这项工作为多语言生成能力评估提供了稳健且高效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:14:00 GMT</pubDate>
</item>
<item>
<title>SophiaVL-R1：强化多模态大语言模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2505.17018</link>
<guid>https://arxiv.org/abs/2505.17018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入思考过程奖励信号，提升多模态大语言模型的推理能力和泛化性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SophiaVL-R1的方法，旨在通过规则驱动的强化学习范式改进多模态大型语言模型（MLLMs）的推理能力。传统方法往往缺乏对推理过程的监督，可能导致模型学习到次优策略，从而影响其泛化能力。为解决这一问题，我们首先训练了一个评估推理过程质量的奖励模型，并设计了Trust-GRPO方法，通过计算信任权重来缓解潜在不可靠的思考奖励的影响。此外，还引入了一种退火训练策略，逐渐减少对思考奖励的依赖，使模型更多地依靠精确的规则导向结果奖励。实验表明，SophiaVL-R1在多个基准测试中表现出色，尤其是在MathVisita和MMMU等任务上，甚至超越了参数量大10倍的竞争对手模型。所有代码、模型和数据集均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的空间感知能力评估</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型是否具备三维空间感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现有多模态大语言模型（MLLMs）是否具有三维空间感知和理解能力。为此，我们引入了VGBench基准测试，专门用于评估MLLMs在视觉几何感知方面的能力，例如相机姿态和运动估计。此外，我们提出了SpatialScore，这是迄今为止最全面和多样化的多模态空间理解基准，整合了VGBench与其他11个现有数据集的相关数据。该基准测试包含28K个样本，涵盖多种空间理解任务、模态和问答格式，并设有一个精心策划的挑战子集SpatialScore-Hard。我们还开发了SpatialAgent，这是一种新型的多智能体系统，集成了9种专门用于空间理解的工具，支持Plan-Execute和ReAct推理范式。通过广泛的评估，我们揭示了空间推理中存在的持续挑战，并展示了SpatialAgent的有效性。我们认为，SpatialScore将为MLLMs的进一步发展提供有价值的见解，并作为严格的基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding</title>
<link>https://arxiv.org/abs/2505.16990</link>
<guid>https://arxiv.org/abs/2505.16990</guid>
<content:encoded><![CDATA[
In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>NovelSeek：人工智能驱动的跨领域自主科学研究框架</title>
<link>https://arxiv.org/abs/2505.16938</link>
<guid>https://arxiv.org/abs/2505.16938</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NovelSeek利用多智能体框架加速科学创新，显著提升研究效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为NovelSeek的统一闭环多智能体框架，用于在多个科研领域开展自主科学研究（ASR），其主要优势体现在可扩展性、交互性和高效性。通过在反应产率预测、增强子活性预测及二维语义分割等任务中的应用，NovelSeek展示了其快速生成创新想法的能力，显著提升了基线模型的表现。例如，在反应产率预测任务中，仅需12小时便将准确率从27.6%提升至35.4%；在增强子活性预测中，4小时内精度从0.52提高到0.79；而二维语义分割任务中，30小时内精确度从78.8%升至81.0%。此外，NovelSeek还支持人类专家反馈和多智能体交互，实现了自动化端到端过程的无缝整合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16938" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:27:43 GMT</pubDate>
</item>
<item>
<title>LLaDA-V：一种基于扩散模型的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2505.16933</link>
<guid>https://arxiv.org/abs/2505.16933</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA-V结合视觉指令微调与掩码扩散模型，展示了出色的多模态性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LLaDA-V的新模型，该模型是一种纯粹基于扩散的多模态大型语言模型（MLLM），通过将视觉指令微调与掩码扩散模型相结合，突破了当前多模态方法中主导的自回归范式。LLaDA-V建立在代表性语言扩散模型LLaDA的基础上，集成了视觉编码器和MLP连接器，将视觉特征投影到语言嵌入空间，实现了有效的多模态对齐。实验结果显示，尽管LLaDA-V的语言模型在纯文本任务上的表现不如LLaMA3-8B和Qwen2-7B等模型强大，但在相同指令数据训练下，它在多模态任务中的表现与LLaMA3-V相当，并且具有更好的数据扩展性，缩小了与Qwen2-VL的性能差距。此外，在多模态理解方面，LLaDA-V相比现有的混合自回归-扩散和纯粹扩散的MLLM达到了最先进的性能。这些发现表明大型语言扩散模型在多模态环境中具有潜力，值得未来进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16933" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:23:26 GMT</pubDate>
</item>
<item>
<title>Believe Your Eyes: 通过注意力熵模式防御多模态大语言模型后门攻击</title>
<link>https://arxiv.org/abs/2505.16916</link>
<guid>https://arxiv.org/abs/2505.16916</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BYE框架，利用注意力熵模式过滤后门样本，有效抵御多模态大语言模型的后门威胁。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型（MLLMs）在微调即服务（FTaaS）中的应用日益广泛，恶意微调可能植入后门的风险随之增加。本文发现后门触发器会导致跨模态处理中异常的注意力集中于非语义区域，这种现象被称为注意力崩溃。基于此，我们提出了BYE框架，它通过注意力图的熵模式作为自监督信号，无需干净标注或模型修改即可识别并过滤后门样本。BYE采用三阶段流水线操作：首先提取微调模型的注意力图，接着通过双模态分离计算熵分数并分析敏感层，最后进行无监督聚类以移除可疑样本。实验表明，BYE在多种数据集、模型及触发器类型上均表现出色，攻击成功率接近零，同时保持了目标任务的性能，提供了一种鲁棒且通用的解决方案来应对MLLMs中的后门威胁。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16916" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:11:58 GMT</pubDate>
</item>
<item>
<title>Jenga：通过动态注意力裁剪和渐进分辨率生成提升视频扩散模型推理效率</title>
<link>https://arxiv.org/abs/2505.16864</link>
<guid>https://arxiv.org/abs/2505.16864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Jenga方法，结合动态注意力裁剪和渐进分辨率生成，显著加速视频扩散模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频Diffusion Transformer模型在实际部署中的计算资源需求问题，提出了名为Jenga的新型推理管道。该方法通过引入块状注意力机制和渐进分辨率策略，有效解决了传统扩散模型中存在的自注意力计算复杂度高以及多步推理效率低的问题。实验表明，Jenga能够在多个最先进的视频扩散模型上实现高达8.83倍的速度提升，同时仅损失0.01%的性能。作为一种即插即用的解决方案，Jenga使得现代硬件上的高质量视频生成成为可能，将推理时间从数分钟缩短至秒级，且无需重新训练模型。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:21:32 GMT</pubDate>
</item>
<item>
<title>通过两阶段训练策略实现视觉语言模型的人类化推理模式</title>
<link>https://arxiv.org/abs/2505.16854</link>
<guid>https://arxiv.org/abs/2505.16854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合监督微调与强化学习的两阶段训练方法，显著减少推理步骤。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TON的两阶段训练策略，旨在使视觉语言模型在推理必要性上模仿人类思维过程。第一阶段通过引入“思想丢弃”操作实现有选择性的推理，而第二阶段则利用强化学习优化推理决策。实验表明，相比传统方法，TON可将推理长度减少高达90%，同时保持甚至提升性能。该方法在多个视觉语言任务中的表现证明了其有效性和广泛适用性，为强化学习中的推理模式研究提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:13:29 GMT</pubDate>
</item>
<item>
<title>LaViDa：基于离散扩散模型的多模态视觉语言模型</title>
<link>https://arxiv.org/abs/2505.16839</link>
<guid>https://arxiv.org/abs/2505.16839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LaViDa通过结合离散扩散模型和视觉编码器，提升多模态任务中的推理速度和可控性。</p><br /><br /><p><strong>摘要：</strong> 现代视觉-语言模型（VLMs）在多种需要视觉推理的任务中表现出色，但在实际应用中，如快速推理和可控生成方面仍面临挑战。现有的自回归（AR）VLMs如LLaVA在这些方面表现不佳。离散扩散模型（DMs）作为一种替代方案，提供了并行解码能力和双向上下文支持，但其在多模态任务中的潜力尚未被充分挖掘。本文介绍了一种名为LaViDa的新一代VLM家族，它通过在DMs中引入视觉编码器，并联合微调整个系统来实现多模态指令跟随。为了克服训练中的难点，LaViDa采用了互补掩码、前缀KV缓存和时间步偏移等创新技术。实验结果显示，LaViDa在多模态基准测试中表现优异，特别是在COCO图像描述任务上，其性能比Open-LLaVa-Next-8B提升了4.1分CIDEr，同时推理速度提高了1.92倍。此外，在双向任务中，LaViDa在约束诗续写任务上的表现提升了59%。这些成果表明LaViDa是一种有竞争力的AR VLM替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:07:12 GMT</pubDate>
</item>
<item>
<title>KRIS-Bench：基于知识推理的图像编辑系统评估基准</title>
<link>https://arxiv.org/abs/2505.16707</link>
<guid>https://arxiv.org/abs/2505.16707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出KRIS-Bench基准，用于评估多模态生成模型的知识推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态生成模型在指令驱动的图像编辑领域取得了显著进展，但其在知识推理编辑任务中的表现尚待深入探索。本文介绍了一个名为KRIS-Bench的新基准，该基准通过认知视角设计，将图像编辑任务划分为事实性、概念性和程序性三种基础知识类型，并涵盖七个推理维度。基准包含1,267个高质量标注的编辑实例，并提出了一个综合评估协议，包括引入知识合理性新指标并结合人类研究进行校准。对十种最先进的模型进行的实验显示了显著的推理性能差距，强调了开发知识中心化基准的重要性，以推动智能图像编辑系统的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 10:08:59 GMT</pubDate>
</item>
<item>
<title>Tool-Star：基于强化学习的大语言模型多工具协作推理框架</title>
<link>https://arxiv.org/abs/2505.16410</link>
<guid>https://arxiv.org/abs/2505.16410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tool-Star框架，通过强化学习使大语言模型自主调用多个外部工具进行推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Tool-Star的强化学习框架，旨在提升大型语言模型（LLMs）在逐步推理过程中自主调用多种外部工具的能力。该框架集成了六类工具，并在数据合成与训练中采用系统性设计。为解决工具使用数据稀缺问题，提出了结合工具集成提示与基于提示采样的通用工具集成推理数据合成管道。此外，还设计了两阶段训练方法：冷启动微调引导模型探索推理模式，多工具自批评强化学习算法优化奖励理解并促进工具协作。实验分析显示，Tool-Star在超过10个具有挑战性的推理基准测试中表现出色。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:00:19 GMT</pubDate>
</item>
<item>
<title>强化学习显著提升中小规模模型推理能力</title>
<link>https://arxiv.org/abs/2505.16400</link>
<guid>https://arxiv.org/abs/2505.16400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示强化学习可大幅增强中小模型推理能力，超越现有最佳蒸馏模型。</p><br /><br /><p><strong>摘要：</strong> 尽管大规模强化学习（RL）在推理方面取得进展，但构建高性能推理模型的具体训练方法仍不明确。本文通过系统性研究发现，对强大小型及中型模型进行大规模RL训练可以显著提高推理能力，优于基于蒸馏的方法。我们提出一种简单有效的策略：先用数学提示训练，再用代码提示训练。实验表明，仅数学RL不仅提升了数学基准测试的表现（如AIME 2025上提升14.6%/17.2%），也改善了代码推理任务的结果（如LiveCodeBench上提升6.8%/5.8%）。此外，扩展的代码RL迭代进一步提升了代码基准测试表现，同时对数学结果影响甚微。我们开发了稳健的数据整理管道，并揭示了逐步增加响应长度的课程学习及参数更新稳定性等关键见解。最终，RL不仅激发了预训练和监督微调获得的基础推理能力，还推动了模型推理极限，使其解决之前无法解决的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 04:50:47 GMT</pubDate>
</item>
<item>
<title>Understanding Generative AI Capabilities in Everyday Image Editing Tasks</title>
<link>https://arxiv.org/abs/2505.16181</link>
<guid>https://arxiv.org/abs/2505.16181</guid>
<content:encoded><![CDATA[
Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:35:15 GMT</pubDate>
</item>
<item>
<title>QuickVideo：加速长视频理解的系统-算法协同设计</title>
<link>https://arxiv.org/abs/2505.16175</link>
<guid>https://arxiv.org/abs/2505.16175</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuickVideo系统，解决长视频解码和预填充瓶颈问题。</p><br /><br /><p><strong>摘要：</strong> 长视频理解在实际应用中至关重要，但传统方法因解码耗时及预填充开销导致计算效率低下。为应对这一挑战，本文提出QuickVideo，通过并行化CPU解码器（QuickDecoder）、高效内存预填充方法（QuickPrefill）以及CPU解码与GPU推理重叠方案，大幅降低长视频处理时间，实现实时应用支持。实验表明，该方法适用于多种时长和采样率，具有广泛适用性，即使在有限硬件上也能提供高质量视频理解能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16175" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:26:50 GMT</pubDate>
</item>
<item>
<title>无需微调的多模态大模型推理能力增强方法</title>
<link>https://arxiv.org/abs/2505.16151</link>
<guid>https://arxiv.org/abs/2505.16151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的多模态大模型推理增强框架FRANK。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FRANK的训练-Free ANd r1-like多模态大型语言模型（MLLM），该模型通过解耦多模态大模型解码器层中的感知与推理，赋予现成的多模态模型推理和反思能力，而无需梯度更新或额外监督。研究的关键洞察是观察到较浅层解码器更多关注视觉标记，而深层解码器则集中处理文本语义。基于此，提出了分层泰勒推导闭式融合机制，将视觉预训练模型与专门推理的LLM结合，从而在深层解码器中融入推理能力的同时保持浅层解码器的视觉接地。实验结果显示，在具有挑战性的多模态推理基准MMMU上，FRANK-38B模型取得了69.2%的准确率，比最强基线模型InternVL2.5-38B高出5.3个百分点，并超过了闭源的GPT-4o模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 22:51:12 GMT</pubDate>
</item>
<item>
<title>引入像素空间推理提升视觉语言模型性能</title>
<link>https://arxiv.org/abs/2505.15966</link>
<guid>https://arxiv.org/abs/2505.15966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在像素空间中引入视觉推理操作，显著提高了视觉语言模型在多种视觉推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 链式思维推理极大地提升了大型语言模型在多领域的性能，但传统上仅限于文本空间，难以有效应对视觉密集型任务。本文提出了一种新颖的像素空间推理框架，使视觉语言模型具备直接从视觉证据中推断的能力，例如通过放大和选择帧等操作。为解决模型初始能力不平衡及对新操作的抗拒问题，我们采用了双阶段训练方法：第一阶段利用合成推理轨迹进行指令微调；第二阶段采用基于好奇心的奖励机制进行强化学习，平衡像素空间与文本空间推理的探索。实验表明，该方法显著提升了模型在多个视觉推理基准上的表现，其中7B参数规模的模型在V*基准测试中达到84%，TallyQA-Complex达到74%，InfographicsVQA达到84%，创开源模型最高记录。这些成果突显了像素空间推理的重要性及其框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:35:08 GMT</pubDate>
</item>
<item>
<title>基于在线对比学习的视觉语言模型幻觉抑制方法</title>
<link>https://arxiv.org/abs/2505.15963</link>
<guid>https://arxiv.org/abs/2505.15963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过动态生成对比训练数据减少视觉语言模型幻觉的新框架。</p><br /><br /><p><strong>摘要：</strong> 现有大型视觉语言模型（LVLMs）在生成内容时易出现与视觉输入不一致的幻觉问题。尽管近期方法通过多模态直接偏好优化（DPO）有所改进，但依赖的负样本往往不能真实反映模型错误，导致训练效果受限。本研究提出在线视觉语言偏好学习（OViP）框架，利用模型自身生成的幻觉输出动态构建对比训练数据。通过采样响应对的语义差异并结合扩散模型合成负样本图像，OViP实时提供更相关的监督信号，实现文本和视觉偏好的自适应对齐。此外，改进了现有评估协议以更好地平衡幻觉抑制与表达能力。实验表明，OViP在减少幻觉的同时保持了模型的核心多模态能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:26:09 GMT</pubDate>
</item>
<item>
<title>最大更新参数化在扩散Transformer中的扩展及其在视觉生成模型中的应用</title>
<link>https://arxiv.org/abs/2505.15270</link>
<guid>https://arxiv.org/abs/2505.15270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究证明最大更新参数化可有效应用于扩散Transformer，大幅降低超参数调优成本。</p><br /><br /><p><strong>摘要：</strong> 近年来，最大更新参数化(muP)被提出用于标准Transformer，显著降低了语言模型从小型到大型迁移超参数的成本。然而，扩散Transformer由于架构和目标上的差异，muP的有效性尚不明确。本研究首次将muP推广至扩散Transformer，并通过大规模实验验证其有效性。研究证明主流扩散Transformer（如DiT、U-ViT等）的muP与标准Transformer一致，使现有muP方法可以直接应用。实验表明，基于muP的DiT-XL-2仅需原版1/2.9的时间即可收敛。此外，在文本转图像生成任务中，通过muP优化PixArt-alpha和MMDiT，模型性能超越基线模型，同时大幅减少了调优开销。这些成果确立了muP作为高效扩展扩散Transformer的框架地位。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 04:49:03 GMT</pubDate>
</item>
<item>
<title>MathIF：评估数学推理任务中指令跟随能力的基准</title>
<link>https://arxiv.org/abs/2505.14810</link>
<guid>https://arxiv.org/abs/2505.14810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型在数学推理中有效推理与指令跟随之间的权衡。</p><br /><br /><p><strong>摘要：</strong> 本研究引入了MathIF基准，用于评估大型语言模型在数学推理任务中的指令跟随能力。实验发现，虽然提高推理能力的模型在复杂数学问题上表现优异，但其对自然语言指令的遵守程度却较低，尤其是在生成较长内容时。通过分析，我们观察到经过精馏长链推理训练或采用推理导向强化学习的模型，在指令遵循方面表现下降。尽管简单的干预措施能在一定程度上恢复模型的服从性，但会牺牲部分推理性能。这些发现强调了当前大语言模型训练范式中存在的基本矛盾，并呼吁开发更具指令意识的推理模型。本研究代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 14:18:01 GMT</pubDate>
</item>
<item>
<title>解决大语言模型强化学习中验证器假阴性问题的研究</title>
<link>https://arxiv.org/abs/2505.14625</link>
<guid>https://arxiv.org/abs/2505.14625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示验证器假阴性问题严重影响强化学习训练并提出改进方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过强化学习优化大型语言模型推理能力时面临的一个普遍问题——验证器假阴性现象，即验证器错误地拒绝了正确的模型输出。通过对Big-Math-RL-Verified数据集的深入分析发现，超过38%的模型生成响应存在此类问题。假阴性不仅剥夺了模型获得有益梯度信号的机会，还显著减缓了收敛速度。为应对这一挑战，我们提出了tinyV，一种基于轻量级语言模型的验证器，它能够动态识别潜在的假阴性并恢复有效响应，从而提供更精确的奖励估计。实验表明，在多个数学推理基准测试中，结合tinyV可以将通过率提高多达10%，并加速收敛过程。本研究强调了解决验证器假阴性问题的重要性，并提供了改善强化学习精调方法的实际路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:16:44 GMT</pubDate>
</item>
<item>
<title>强化学习对大语言模型参数更新的稀疏性研究</title>
<link>https://arxiv.org/abs/2505.11711</link>
<guid>https://arxiv.org/abs/2505.11711</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现强化学习仅更新小部分参数即可显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习在大型语言模型下游任务中的应用，揭示了一个有趣的现象：通过强化学习训练，仅需更新模型总参数的5%到30%，其余参数基本保持不变，即可实现显著的性能提升。这种现象被称为“强化学习诱导的参数更新稀疏性”。研究覆盖了7种常用强化学习算法及10种不同家族的大语言模型，并且该稀疏性是内在属性，无需显式的稀疏正则化或架构约束。进一步实验表明，单独微调被更新的子网络即可恢复测试准确性，并且得到的模型与全量微调的结果几乎相同。此外，来自不同随机种子、训练数据和强化学习算法的子网络之间显示出比偶然预期更高的重叠度。分析表明，这种稀疏性并非由于只更新某些特定层，而是几乎所有参数矩阵都接收到了稀疏更新，且这些更新几乎涵盖了参数矩阵所能表示的所有子空间。我们推测，这种稀疏性主要归因于训练数据接近策略分布，而KL正则化和梯度裁剪等技术的影响有限。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11711" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 17:42:28 GMT</pubDate>
</item>
<item>
<title>评估参考型奖励系统的VerifyBench基准测试</title>
<link>https://arxiv.org/abs/2505.15801</link>
<guid>https://arxiv.org/abs/2505.15801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出VerifyBench基准以评估参考型奖励系统性能。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型如OpenAI和DeepSeek-R1在推理领域表现优异，其训练的关键在于强化学习中的可验证奖励机制。然而，现有的奖励基准未能评估基于参考的奖励系统，导致研究人员对强化学习中使用的校验器准确性缺乏深入了解。本文引入了两个新基准——VerifyBench和VerifyBench-Hard，通过精心收集和整理数据，并进行细致的人工标注构建，旨在评估此类奖励系统的性能。当前模型在这两个基准上仍有较大改进空间，尤其是较小规模的模型。此外，我们还对评估结果进行了全面分析，为理解和发展基于参考的奖励系统提供了见解。所提出的基准为指导校验器准确性及通过强化学习训练的推理模型的推理能力发展提供了有效工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:54:43 GMT</pubDate>
</item>
<item>
<title>基于多臂老虎机的自适应推测解码框架</title>
<link>https://arxiv.org/abs/2505.15141</link>
<guid>https://arxiv.org/abs/2505.15141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的在线学习框架，优化大语言模型推测解码性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的在线学习框架BanditSpec，用于自适应调整大语言模型推测解码的超参数配置。该问题被形式化为一个多臂老虎机问题，设计并分析了UCBSpec和EXP3Spec两种算法，其停止时间遗憾在随机和对抗奖励设定下均得到上界估计。UCBSpec的遗憾性能接近最优。通过实验验证，该方法在多种输入提示场景下的吞吐量接近最佳固定配置。关键词：推测解码、多臂老虎机、大语言模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 01:56:31 GMT</pubDate>
</item>
<item>
<title>熵最小化提升大语言模型在复杂任务中的表现</title>
<link>https://arxiv.org/abs/2505.15134</link>
<guid>https://arxiv.org/abs/2505.15134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">熵最小化无需标注数据即可显著提升大语言模型在数学、物理及编程任务上的性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了熵最小化(Entropy Minimization, EM)作为一种训练目标对大规模语言模型(Large Language Models, LLMs)在解决复杂任务时的效果。我们提出了三种方法：(1) EM-FT，类似于指令微调但作用于未标记的数据；(2) EM-RL，采用负熵作为唯一奖励的强化学习；(3) EM-INF，在推理阶段通过调整logits减少熵，无需训练数据或参数更新。实验表明，在Qwen-7B上，EM-RL仅基于未标注数据便能达到或超过使用60K标注数据训练的强基线模型如GRPO和RLOO的表现；而EM-INF使Qwen-32B在SciCode基准测试中达到甚至超越了GPT-4o、Claude 3 Opus和Gemini 1.5 Pro的性能，同时效率提升了3倍。研究揭示了许多预训练的LLMs具备未被充分挖掘的推理能力，仅通过熵最小化即可有效激发这些潜力，无需标注数据或参数更新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 01:39:11 GMT</pubDate>
</item>
<item>
<title>DiCo：基于标准卷积网络的高效扩散模型</title>
<link>https://arxiv.org/abs/2505.11196</link>
<guid>https://arxiv.org/abs/2505.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiCo，一种基于卷积网络的高效扩散模型，性能超越传统Transformer模型。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了卷积操作作为构建高效表达扩散模型的替代模块的可能性。虽然直接用卷积替换自注意力通常会导致性能下降，我们发现这是因为卷积神经网络比Transformer具有更高的通道冗余度。为此，我们引入了一种紧凑的通道注意力机制，提高了特征多样性。由此产生了DiCo，一种完全由标准卷积模块组成的扩散模型家族，在ImageNet上的表现优于现有扩散模型，同时显著提升了效率。例如，DiCo-XL在256x256分辨率下的FID得分为2.05，比DiT-XL/2快2.7倍；在512x512分辨率下FID为2.53，快3.1倍。此外，我们的最大模型DiCo-H达到1B参数规模，在ImageNet 256x256上的FID为1.90，且无需额外监督。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>BLEUBERI: BLEU is a surprisingly effective reward for instruction following</title>
<link>https://arxiv.org/abs/2505.11080</link>
<guid>https://arxiv.org/abs/2505.11080</guid>
<content:encoded><![CDATA[
Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 06:11:43 GMT</pubDate>
</item>
<item>
<title>Llama-SMoP：一种高效多模态大型语言模型用于视听语音识别</title>
<link>https://arxiv.org/abs/2505.14336</link>
<guid>https://arxiv.org/abs/2505.14336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Llama-SMoP模型，通过稀疏混合投影模块提升多模态语音识别性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对视听语音识别(AVSR)领域中大型语言模型(LLMs)高计算成本的问题，提出了Llama-SMoP，这是一种利用稀疏混合投影(SMoP)模块的高效多模态LLM。该模型通过引入稀疏门控的混合专家(MoE)投影器，在不增加推理成本的前提下扩展模型容量。实验表明，采用模态特定路由和专家的DED R配置在ASR、VSR及AVSR任务上表现出色，消融研究进一步验证了其在专家激活、可扩展性和噪声鲁棒性方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:20:55 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Thought框架：多模态推理提升逻辑推理性能</title>
<link>https://arxiv.org/abs/2505.15817</link>
<guid>https://arxiv.org/abs/2505.15817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mixture-of-Thought框架，通过自然语言、代码和真理表三种模态协同推理，显著提升逻辑推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 现有基于大型语言模型（LLMs）的方法通常仅限于单一推理模态（如自然语言），限制了多模态间的协同效应。本文提出Mixture-of-Thought（MoT）框架，引入自然语言、代码和新提出的真理表三种互补模态，实现跨模态推理。MoT框架分为两个阶段：自我演化的MoT训练阶段，联合学习自动生成的多模态理由；以及MoT推理阶段，充分利用三模态协同效果优化预测。实验表明，MoT在FOLIO和ProofWriter等逻辑推理基准测试中显著优于单模态链式推理方法，平均准确率提升11.7个百分点。进一步分析显示，该框架对复杂逻辑问题尤其有效，各模态贡献互补优势，其中真理表推理有助于克服自然语言推理中的关键瓶颈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于强化学习的会话查询重写框架ConvSearch-R1</title>
<link>https://arxiv.org/abs/2505.15776</link>
<guid>https://arxiv.org/abs/2505.15776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需外部监督的ConvSearch-R1框架提升会话查询重写性能。</p><br /><br /><p><strong>摘要：</strong> 现有会话查询重写(CQR)方法面临高依赖外部标注或大语言模型以及与下游检索器对齐不足的问题。本文介绍ConvSearch-R1，首个完全消除对外部重写监督依赖的自驱动框架，利用强化学习优化重写过程。该框架采用两阶段方法，首先通过检索引导的自我蒸馏解决冷启动问题，随后结合专门设计的排名激励奖励塑造机制解决传统检索指标的稀疏性问题。在TopiOCQA和QReCC数据集上的实验表明，ConvSearch-R1显著优于现有最先进方法，在TopiOCQA数据集上实现超过10%的性能提升，同时使用参数规模更小的3B模型且无需任何外部监督。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:27:42 GMT</pubDate>
</item>
<item>
<title>BiasLens：基于模型向量空间结构的大语言模型偏见分析框架</title>
<link>https://arxiv.org/abs/2505.15524</link>
<guid>https://arxiv.org/abs/2505.15524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BiasLens框架，无需标注数据即可高效检测大语言模型偏见。</p><br /><br /><p><strong>摘要：</strong> 现有大语言模型（LLMs）存在显著的偏见问题，影响了其可靠性和公平性。传统偏见评估方法依赖人工标注数据，耗时且覆盖概念有限。本文提出BiasLens框架，该框架基于模型向量空间结构，利用概念激活向量（CAVs）与稀疏自编码器（SAEs）提取可解释的概念表示，并通过衡量目标概念与参考概念间的表征相似性变化量化偏见。实验表明，BiasLens与传统偏见评估指标高度相关（Spearman相关系数r>0.85），同时揭示了现有方法难以发现的偏见形式，例如保险状态可能导致临床诊断偏见。BiasLens为偏见发现提供了可扩展、可解释且高效的解决方案，有助于提升LLMs的公平性与透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 09:50:23 GMT</pubDate>
</item>
<item>
<title>AJailBench：评估大型音频语言模型的越狱攻击漏洞</title>
<link>https://arxiv.org/abs/2505.15406</link>
<guid>https://arxiv.org/abs/2505.15406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次构建AJailBench基准，评估大型音频语言模型的越狱攻击漏洞。</p><br /><br /><p><strong>摘要：</strong> 随着大型音频语言模型(LAMs)的发展，其音频输出可能包含有害或不道德的内容，但现有研究缺乏对这些模型安全性的系统性定量评估，尤其是针对越狱攻击。本文引入AJailBench，这是首个专门用于评估LAMs越狱漏洞的基准。我们首先构建了AJailBench-Base数据集，包含1495个跨10类政策违规的对抗性音频提示，通过现实的文本转语音合成技术转换自文本越狱攻击。通过此数据集，我们测试了几种最先进的LAMs，发现它们在攻击面前缺乏一致性鲁棒性。为进一步加强越狱测试并模拟更真实的攻击条件，我们提出了一种生成动态对抗变体的方法。我们的音频扰动工具包(APT)在时间、频率和振幅域上应用目标扰动，并通过约束语义一致性并采用贝叶斯优化来搜索既微妙又高效的扰动，从而生成了AJailBench-APT扩展数据集。研究结果表明，即使是小幅度且语义保持的扰动，也能显著降低领先LAMs的安全性能，强调了需要更稳健和语义感知的防御机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:47:47 GMT</pubDate>
</item>
<item>
<title>自适应自我恢复推理框架提升大规模推理模型效率</title>
<link>https://arxiv.org/abs/2505.15400</link>
<guid>https://arxiv.org/abs/2505.15400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自适应自我恢复推理框架，大幅减少计算开销且性能损失微乎其微。</p><br /><br /><p><strong>摘要：</strong> 大规模推理模型通过长推理链实现优异表现，但在简单任务上因冗余推理导致高计算开销。本文量化了两种模式下这些模型的上限，并揭示了“内部自我恢复机制”，即模型在回答生成时隐式补充推理。基于此，我们提出了自适应自我恢复推理（ASRR）框架，该框架通过抑制不必要的推理并启用隐式恢复，根据问题难度动态分配推理资源，在多个基准测试和模型上的实验表明，相比GRPO，ASRR可将推理预算减少高达32.5%（1.5B）和25.7%（7B），同时仅轻微降低准确性（pass@1分别下降1.2%和0.6%），并在安全性基准测试中显著提高了无害率（最高增加21.7%）。ASRR展示了在高效、自适应及安全推理方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:41:39 GMT</pubDate>
</item>
<item>
<title>Tango: 一种同时训练语言模型生成器和验证器的强化学习框架</title>
<link>https://arxiv.org/abs/2505.15034</link>
<guid>https://arxiv.org/abs/2505.15034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tango框架，通过强化学习同时训练语言模型生成器和验证器，显著提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Tango的新框架，该框架利用强化学习同时训练大型语言模型（LLMs）的生成器和验证器。传统方法中的验证器通常固定或通过监督微调训练，容易出现奖励黑客问题且泛化能力差。Tango的创新之处在于其采用生成式过程级LLM验证器，此验证器通过强化学习训练并与生成器协同进化，无需显式的过程级标注。实验表明，Tango的两个组件在多个基准测试中达到了当前最佳性能，在数学推理等难题上表现尤为突出。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 22:43:15 GMT</pubDate>
</item>
<item>
<title>WebNovelBench：评估大语言模型叙事能力的新基准</title>
<link>https://arxiv.org/abs/2505.14818</link>
<guid>https://arxiv.org/abs/2505.14818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WebNovelBench基准，用于评估大语言模型的长篇小说生成能力。</p><br /><br /><p><strong>摘要：</strong> 现有评估长篇叙事能力的大规模基准往往缺乏必要的规模、多样性和客观度量标准，因此我们开发了WebNovelBench这一新基准。该基准利用超过4000部中文网络小说的数据集，将评估设定为概要到故事生成的任务。通过多维度框架和LM作为评委的方法，自动评估生成的故事质量。实验表明，WebNovelBench能够有效区分人类创作的杰作、流行网络小说以及由大语言模型生成的内容。此外，该研究对24个最先进的大语言模型进行了综合分析，排名其叙事能力并提供了未来发展的见解。这项基准提供了一种可扩展、可复制且基于数据驱动的方法，用于评估和推进由大语言模型驱动的叙事生成技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 14:32:28 GMT</pubDate>
</item>
<item>
<title>Toto：基于观测数据的时间序列预测基础模型</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Toto是一种具有1.51亿参数的解码器时间序列预测基础模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Toto，这是一个拥有151百万参数的时间序列预测基础模型，采用现代解码器-only架构，并结合了针对多变量可观察性时间序列数据特定挑战的创新设计。Toto的预训练语料库由可观测性数据、开放数据集和合成数据混合组成，规模是领先时间序列基础模型的4到10倍。同时，我们推出了BOOM，这是一个包含2807个真实世界时间序列的3.5亿观测值的大规模基准测试。Toto和BOOM的数据均来自Datadog的遥测技术和内部可观测性指标。广泛评估显示，Toto在BOOM及现有通用时间序列预测基准上表现出最先进的性能。Toto的模型权重、推理代码和评估脚本，以及BOOM的数据和评估代码，均已作为开源项目，在Apache 2.0许可下提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:48:13 GMT</pubDate>
</item>
<item>
<title>强化微调中的先验提示工程研究</title>
<link>https://arxiv.org/abs/2505.14157</link>
<guid>https://arxiv.org/abs/2505.14157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示先验提示工程可有效提升语言模型的行为表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化微调（RFT）背景下的先验提示工程（pPE），指出现有研究主要集中在算法、奖励塑造和数据整理上，而先验提示的设计尚未得到充分探索。受推理时提示工程（iPE）启发，我们翻译了五种典型的iPE策略（推理、规划、基于代码的推理、知识回忆和空例利用）为相应的pPE方法，并通过Qwen2.5-7B实验验证。结果显示，所有pPE训练的模型均优于其对应的iPE提示模型，其中空例pPE方法在AIME2024和GPQA-Diamond上的平均性能提升最大。此外，通过行为分类框架，我们发现不同的pPE策略会在模型中培养出独特的行为风格。这些结果表明先验提示工程在RFT中具有强大但未被充分研究的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 06:05:11 GMT</pubDate>
</item>
<item>
<title>基于知识图谱的多语言多跳幻觉评估基准MultiHal</title>
<link>https://arxiv.org/abs/2505.14101</link>
<guid>https://arxiv.org/abs/2505.14101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识图谱的多语言多跳幻觉评估基准MultiHal。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）存在忠实性和事实性方面的固有限制，即所谓的幻觉现象。现有评估基准主要依赖英语为中心的数据集，并通过补充网络链接或文本片段进行事实性评估，而忽略了可用的结构化事实资源。鉴于此，研究者指出知识图谱（KGs）可有效减轻幻觉问题。本文填补了现有幻觉评估基准中知识图谱路径及多语言支持的空白，提出了名为MultiHal的知识图谱驱动的多语言多跳评估基准。通过从开放领域知识图谱中挖掘并筛选出高质量的25.9k知识图谱路径，MultiHal在多个语言和模型上展示了语义相似度评分的显著提升，相比传统问答系统提高了0.12至0.36分。这一基准有望推动未来基于图的幻觉缓解和事实核查任务的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:03:35 GMT</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:09:44 GMT</pubDate>
</item>
<item>
<title>Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM</title>
<link>https://arxiv.org/abs/2505.15816</link>
<guid>https://arxiv.org/abs/2505.15816</guid>
<content:encoded><![CDATA[
Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further. The code will be made public at this https://github.com/penghao-wu/ProxyV URL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>MMaDA：一种跨模态扩散基础模型的创新设计</title>
<link>https://arxiv.org/abs/2505.15809</link>
<guid>https://arxiv.org/abs/2505.15809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型跨模态扩散基础模型MMaDA，在多领域表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMaDA，这是一种全新的跨模态扩散基础模型，旨在实现文本推理、多模态理解和文本到图像生成等领域的卓越性能。MMaDA通过三个关键创新实现这一目标：首先，采用统一的扩散架构，具有共享的概率公式和模态无关的设计，无需特定模态组件，从而实现不同类型数据的无缝集成；其次，实施混合长链式推理微调策略，统一多种模态的推理格式，通过文本和视觉域之间推理过程的对齐，提升模型处理复杂任务的能力；最后，提出UniGRPO算法，基于策略梯度的强化学习方法，用于扩散基础模型，统一推理和生成任务的后训练过程。实验结果显示，MMaDA-8B在文本推理、多模态理解和文本到图像生成方面均表现出色，超越了多个强大的现有模型。这些成果展示了MMaDA在统一扩散架构内弥合预训练与后训练差距的有效性，为未来研究提供了全面框架。代码和训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>VARD：基于价值函数的强化扩散模型优化方法</title>
<link>https://arxiv.org/abs/2505.15791</link>
<guid>https://arxiv.org/abs/2505.15791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合价值函数和KL正则化的新型强化扩散模型训练方法。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多个领域作为强大的生成工具崭露头角，但如何让预训练模型表现出特定的期望属性仍具挑战性。尽管强化学习提供了潜在解决方案，但现有方法难以同时实现稳定高效的微调并支持非可微奖励。本文提出VARD（Value-based Reinforced Diffusion），通过先学习中间状态奖励期望的价值函数，再利用该函数结合KL正则化在整个生成过程中提供密集监督，解决了现有方法的不足。实验表明，该方法不仅有效提升了轨迹引导能力，还提高了训练效率，并扩展了强化学习在复杂非可微奖励函数优化中的适用范围。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:44:37 GMT</pubDate>
</item>
</channel>
</rss>