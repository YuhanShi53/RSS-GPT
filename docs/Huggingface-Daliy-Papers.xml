<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>基于计划去噪的离散扩散框架DDPD</title>
<link>https://arxiv.org/abs/2410.06264</link>
<guid>https://arxiv.org/abs/2410.06264</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出了DDPD框架，通过规划与去噪结合，实现了更高效的生成过程。</p><br><br><p><strong>摘要：</strong> 离散扩散方法在标准基准上取得了最佳性能，接近或超越了自回归模型。本文介绍了一种新颖的离散扩散框架——计划去噪的离散扩散（DDPD），它将生成过程分为两个模型：规划者和去噪器。在推理时，规划者通过识别最需要去噪的位置（包括初始损坏位置和需要进一步细化的位置）来选择下一个去噪位置。这种计划与去噪的方法使得在生成过程中能够更高效地重建，通过迭代识别和以最佳顺序去噪来处理损坏。DDPD在语言建模基准上表现优于传统的仅有去噪器的掩蔽扩散方法，在text8、OpenWebText和ImageNet 256x256的基于令牌的生成任务中取得了优越结果。值得注意的是，在语言建模中，DDPD显著缩小了扩散和自回归方法在生成困惑度上的性能差距。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.06264 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 20:35:31 GMT</pubDate>
<pubDate>Mon, 14 Oct 2024 20:35:31 GMT</pubDate>
</item>
<item>
<title>Synth-SONAR：基于扩散模型与GPT提示的声纳图像合成框架</title>
<link>https://arxiv.org/abs/2410.08612</link>
<guid>https://arxiv.org/abs/2410.08612</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究提出Synth-SONAR框架，利用扩散模型和GPT提示生成高质量声纳图像，提升数据多样性和现实感。</p><br><br><p><strong>摘要：</strong> 声纳图像合成对水下探索、海洋生物学和国防等应用至关重要。然而，传统方法往往依赖昂贵且大量的数据采集，影响数据质量和多样性。为了解决这些问题，本研究提出了一种新的声纳图像合成框架，Synth-SONAR，结合了扩散模型和GPT提示。Synth-SONAR的创新点主要体现在三个方面：首先，利用基于生成的AI风格注入技术和公开可用的真实/模拟数据，生成最大规模的声纳数据集。其次，构建了一个双重文本条件的声纳扩散模型层次，可以合成高质量和多样性的粗略与精细声纳图像。第三，采用高级（粗略）和低级（详细）文本基础的声纳生成方法，有效利用视觉语言模型（VLMs）和GPT提示中的高级语义信息。在推理过程中，该方法从文本提示中生成多样化且真实的声纳图像，首次将GPT提示应用于声纳图像合成。Synth-SONAR在生产高质量合成声纳数据集方面实现了最新的领先成绩，显著提升了数据的多样性和真实感。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.08612 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 18:14:02 GMT</pubDate>
<pubDate>Mon, 14 Oct 2024 18:14:02 GMT</pubDate>
</item>
<item>
<title>GenARM：一种有效的自回归奖励模型用于无重训练的大型语言模型对齐</title>
<link>https://arxiv.org/abs/2410.08193</link>
<guid>https://arxiv.org/abs/2410.08193</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>GenARM通过自回归奖励模型实现有效的无训练对齐，支持多目标和弱到强的指导。</p><br><br><p><strong>摘要：</strong> 大型语言模型（LLMs）展现了令人印象深刻的能力，但需要与人类偏好进行谨慎对齐。传统的训练时方法使用人类偏好数据集对LLMs进行微调，成本高昂且难以处理多样化用户偏好。而测试时对齐方法则通过奖励模型（RMs）指导冻结的LLMs，避免了重新培训的需求。目前的测试时方法依赖于轨迹级奖励模型，旨在评估完整响应，这在需要从部分响应计算下一个标记奖励的自回归文本生成中显得不够合适。为此，我们提出了GenARM，这一测试时对齐方法利用自回归奖励模型——一种新型的奖励参数化设计，能够预测下一个标记的奖励，从而促进高效且有效的自回归生成。我们从理论上证明，这种参数化可以在KL正则化的强化学习框架内可证明地引导冻结LLMs朝向由传统RMs能够实现的任何分布。实验结果表明，GenARM显著优于现有的测试时对齐基准，并且与训练时方法的性能相匹配。此外，GenARM支持高效的弱到强指导，使得较大的LLMs能够与较小的RMs进行对齐，而无需承担训练大模型的高成本。同时，GenARM还支持多目标对齐，实时调整偏好维度之间的权衡，满足多样化用户偏好的需求。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.08193 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 17:04:55 GMT</pubDate>
<pubDate>Mon, 14 Oct 2024 17:04:55 GMT</pubDate>
</item>
<item>
<title>利用简单分层方法提高大型语言模型生成的多样性</title>
<link>https://arxiv.org/abs/2410.09038</link>
<guid>https://arxiv.org/abs/2410.09038</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本文提出SimpleStrat方法，利用语言模型对输出进行分层，从而提高生成内容的多样性和质量。</p><br><br><p><strong>摘要：</strong> 在生成多样化响应方面，大型语言模型（LLMs）扮演着重要角色，尤其在规划搜索和合成数据生成等应用中。传统方法依赖于提高温度来增强多样性，然而，研究表明这不仅导致个体生成质量下降，而且其效果依赖于模型预测概率与真实答案分布的相似性。为了克服这一问题，本文提出了一种新的方法——SimpleStrat，通过语言模型自身对生成空间进行分层，在推理时随机选择一个层级并从中抽取样本。此外，为了衡量生成结果的多样性，本文引入了CoverageQA数据集，该数据集由多个同样合理的答案组成的模糊问题构成，利用Kullback-Leibler散度（KL散度）来评估输出分布与有效答案的均匀分布之间的差异。在对比评估中，使用SimpleStrat方法在召回率方面比GPT-4o提高了0.05，而相较于Llama 3则在KL散度上减少了0.36，展现了该方法在提高生成多样性和质量上的有效性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2410.09038 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 16:35:17 GMT</pubDate>
<pubDate>Mon, 14 Oct 2024 16:35:17 GMT</pubDate>
</item>

<item>
<title>MiRAGeNews数据集：对抗AI生成假新闻的多模态检测</title>
<link>https://arxiv.org/abs/2410.09045</link>
<guid>https://arxiv.org/abs/2410.09045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究推出MiRAGeNews数据集，旨在检测AI生成的假新闻，提高内容真实性。使用此数据集训练的多模态检测器效果显著。</p><br /><br /><p><strong>摘要：</strong> 随着虚假新闻内容的泛滥和AI生成图像技术的迅速发展，AI生成的假新闻内容变得更加危险。为此，我们提出了MiRAGeNews数据集，该数据集包含12,500对高质量的真实和AI生成的图像-标题配对，使用了最先进的生成模型。我们的研究表明，这一数据集对人类的检测能力构成了显著挑战（F-1为60%），同时对当前最先进的多模态大语言模型的检测性能也很低（F-1小于24%）。基于该数据集，我们训练了一种多模态检测器（MiRAGe），在来自域外图像生成器和新闻发布者的图像-标题配对任务上，F-1提高了5.1%，超越了现有的基线方法。我们将代码和数据公开发布，以助力后续对AI生成内容的检测研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 13:41:24 GMT</pubDate>
</item>
<item>
<title>I-Max框架：提升文本到图像RFTs的分辨率潜力</title>
<link>https://arxiv.org/abs/2410.07536</link>
<guid>https://arxiv.org/abs/2410.07536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍I-Max框架，利用新的Projected Flow策略和推理工具，提升RFTs在分辨率扩展中的稳定性和图像细节质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Rectified Flow Transformers（RFTs）在扩展生成分辨率方面的挑战，尤其是数据质量和训练成本的问题。针对现有的无调优分辨率外推方法，往往在生成稳定性方面存在不足，我们提出了I-Max框架。I-Max框架包括两大特性：首先是最新的Projected Flow策略，旨在实现更稳定的分辨率外推；其次是一个先进的推理工具包，可帮助模型知识向更高分辨率的泛化。通过在Lumina-Next-2K和Flux.1-dev数据集上的实验，结果表明I-Max能够有效提升分辨率外推的稳定性，并带来图像细节的增强和伪影的修正，证实了无调优分辨率外推的实际应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 12:22:47 GMT</pubDate>
</item>
<item>
<title>ZeroComp：一种有效的零样本3D物体合成方法</title>
<link>https://arxiv.org/abs/2410.08168</link>
<guid>https://arxiv.org/abs/2410.08168</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroComp通过使用ControlNet和Stable Diffusion实现无配对图像的3D物体合成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ZeroComp的有效零样本3D物体合成方法，该方法在训练过程中不需要配对的复合场景图像。ZeroComp利用ControlNet对内在图像进行条件处理，并结合Stable Diffusion模型以利用其场景先验，这两者共同运作为一个高效的渲染引擎。在训练阶段，ZeroComp使用基于几何、反射率和遮蔽的内在图像，完全不依赖于含有复合物体的场景的图像配对。训练完成后，ZeroComp能够无缝地将虚拟3D物体集成到场景中，并调整光照，使复合效果更加真实。为了验证ZeroComp的效果，我们开发了一个高质量评估数据集，结果显示其在定量和人类感知基准测试中均优于依赖显式照明估计和生成技术的方法。此外，ZeroComp在真实和户外图像合成上也展现出潜力，即使其训练数据仅限于合成的室内数据，依然能够有效实现图像合成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08168" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 11:29:03 GMT</pubDate>
</item>
<item>
<title>Mentor-KD：多步推理能力的知识蒸馏方法</title>
<link>https://arxiv.org/abs/2410.09037</link>
<guid>https://arxiv.org/abs/2410.09037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Mentor-KD方法，通过中介模型增强CoT标注与软标签提供，以有效蒸馏LLMs的推理能力。 </p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）在复杂任务中表现出的卓越性能，Chain-of-Thought（CoT）提示法已成为主要研究方向。近年来，有研究提出了一种推理蒸馏（Reasoning Distillation）的方法，通过对LLM教师生成的多步推理进行微调，以转移其推理能力。然而，现有方法在蒸馏集中存在两大挑战：1）数据质量不足，2）软标签提供不充分。为了解决这些问题，本文提出了Mentor-KD方法，旨在有效蒸馏LLMs的多步推理能力至小型语言模型。具体而言，我们利用一个中介模型，进行特定任务的微调，以增强CoT注释的数量和质量，并在推理蒸馏过程中为学生模型提供软标签。通过广泛的实验，我们验证了Mentor-KD在各种模型和复杂推理任务中的有效性，显示出该方法在提升推理能力方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 09:26:42 GMT</pubDate>
</item>
<item>
<title>SAE Match：基于稀疏自编码器的神经网络层间特征对齐</title>
<link>https://arxiv.org/abs/2410.07656</link>
<guid>https://arxiv.org/abs/2410.07656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出SAE Match，通过折叠参数最小化均方误差，实现神经网络层间特征的有效对齐。</p><br /><br /><p><strong>摘要：</strong> 理解深度神经网络中特征在不同层之间的演变是机械可解释性中的一个基本挑战，尤其是由于多义性和特征叠加使得这一过程更加复杂。尽管稀疏自编码器（SAEs）已被用来从单个层中提取可解释特征，但不同层之间的特征对齐仍然是一个开放的问题。在本文中，我们提出了一种新颖的数据无关的方法SAE Match，用于对齐神经网络不同层的SAE特征。我们的方法通过最小化SAEs的折叠参数之间的均方误差来匹配特征，这一技术在编码器和解码器权重中引入激活阈值，以解决特征尺度差异的问题。通过对Gemma 2语言模型的广泛实验，我们证明了我们的方法能够有效捕捉特征在不同层间的演变，从而提高特征匹配质量。我们还展示了特征在多个层中持续存在，并且我们的方法能够近似不同层之间的隐藏状态。我们的工作推动了对神经网络中特征动态的理解，并为机械可解释性研究提供了一种新的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 08:07:48 GMT</pubDate>
</item>
<item>
<title>DA-Code：针对代理的数据科学任务的代码生成基准</title>
<link>https://arxiv.org/abs/2410.07331</link>
<guid>https://arxiv.org/abs/2410.07331</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DA-Code是一个评估LLMs在数据科学任务上的代码生成能力的基准，涵盖复杂数据处理的实际任务。</p><br /><br /><p><strong>摘要：</strong> 我们介绍了DA-Code，一个专门设计用来评估LLMs在代理数据科学任务中的代码生成能力的基准。DA-Code的设计包含三个核心要素。首先，任务具有内在挑战性，要求高级编码技能，特别是在上下文处理和计划方面。其次，DA-Code中的所有示例均基于真实且多样的数据，涵盖各种复杂的数据整理和分析任务。最后，为了完成这些任务，模型必须使用复杂的数据科学编程语言，以进行精细化的数据处理并得出结论。我们在一个可控和可执行的环境中搭建了这个基准，确保它与现实世界的数据分析场景相一致，并具备可扩展性。评估套件经过注释者的精心设计，以确保评估的准确性和稳健性。我们开发了DA-Agent基准线并进行了实验，结果表明，尽管基线在现有框架中表现更好，但当前最佳的LLMs准确率仅为30.5%，仍大有提升空间。我们将在https://da-code-bench.github.io发布我们的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07331" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 07:27:37 GMT</pubDate>
</item>
<item>
<title>多智能体协作数据选择机制在大型语言模型预训练中的应用</title>
<link>https://arxiv.org/abs/2410.08102</link>
<guid>https://arxiv.org/abs/2410.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多智能体协作框架，以提高大型语言模型预训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）预训练的有效数据选择至关重要。虽然已有多种方法致力于提升数据效率，但很少有研究关注这些方法之间的固有冲突，以实现最优数据选择。为了解决这一问题，我们提出了一种新颖的多智能体协作数据选择机制。在该框架中，每种数据选择方法作为独立代理工作，设计了一个代理控制台，以动态整合所有代理在整个LLM训练过程中的信息。我们通过广泛的实证研究评估了我们的多智能体框架。实验结果表明，我们的方法显著提高了数据效率，加速了LLM训练的收敛，并在多个语言模型基准上实现了平均10.5%的性能提升，相比于最先进的方法。有了这样的机制，数据选择的冲突问题得以更好地解决，进一步推动了大型语言模型的研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:56:36 GMT</pubDate>
</item>
<item>
<title>StructRAG：基于结构化知识增强大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2410.08815</link>
<guid>https://arxiv.org/abs/2410.08815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出StructRAG框架，通过结构化知识增强LLMs在知识密集任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> Retrieval-augmented generation (RAG) 是一种增强大型语言模型 (LLMs) 在知识密集任务中表现的关键方法，但现有RAG方法在处理知识密集推理任务时面临挑战。这是因为相关信息往往散布不均，使得现有方法难以准确识别关键信息并进行全局推理。为了解决这个问题，本文提出了一种新框架StructRAG，它基于人类在处理知识密集推理任务时将原始信息转换为多种结构化知识的认知理论，能够识别适合特定任务的最佳结构类型，将原始文档重构为该结构格式，并基于结果结构推理出答案。在各种知识密集任务上的广泛实验表明，StructRAG在复杂情境中表现出色，达到了最先进的性能，展示了其在复杂现实世界应用中增强LLMs作为有效解决方案的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 04:14:19 GMT</pubDate>
</item>
<item>
<title>通过KV预测减少变换器模型的首次输出时间</title>
<link>https://arxiv.org/abs/2410.08391</link>
<guid>https://arxiv.org/abs/2410.08391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出KV预测方法，通过辅助模型生成KV缓存，提高变换器模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 推理过程中的prompt处理步骤通常会消耗大量时间，尤其是在边缘设备上使用亿级参数模型时。为了解决首次输出时间（TTFT）过长的问题，我们提出了一种名为KV预测的新方法。通过使用一个小型辅助模型处理prompt，生成近似的KV缓存，从而减少基模型在自回归生成时对计算资源的需求。该方法在维持相对准确性的同时显著提高了效率，相比基线方法，KV预测在TriviaQA上准确度提升了15%-50%，在HumanEval的Python代码补全任务中提升了最多30%。此外，我们在Apple M2 Pro CPU上进行了基准测试，验证了FLOPs的改善直接转化为TTFT的速度提升。本研究展示了在多种TTFT计算预算下的有效性，为变换器模型的应用提供了新的可能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:41:21 GMT</pubDate>
</item>
<item>
<title>VITask：提升大型视觉语言模型任务适应性的框架</title>
<link>https://arxiv.org/abs/2410.06456</link>
<guid>https://arxiv.org/abs/2410.06456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VITask通过集成任务特定模型，提升视觉语言模型的任务适应能力，展示在医学诊断中的有效性。</p><br /><br /><p><strong>摘要：</strong> 该研究提出了一种名为VITask的创新框架，旨在提升大型视觉语言模型（VLMs）在特定任务中的适应能力。由于预训练与微调之间的领域差异，传统VLMs在任务特定应用中往往表现不佳。VITask通过引入任务特定模型（TSMs）并实施三种关键策略，即示例提示（EP）、响应分布对齐（RDA）和对比响应调整（CRT），有效改善任务特定表现。EP使TSM特征能够引导VLMs，而RDA则在推理过程中通过借鉴示例提示模型的经验，使VLMs能够适应而不需TSMs。CRT进一步优化了正确图像-响应对的排名，从而降低了生成不当响应的风险。在12个医学诊断数据集和9种成像模式下的实验结果表明，VITask的表现超越了传统的指令调优VLMs和TSMs，证明了其有效整合两种模型互补特性的能力。此外，VITask在TSM集成的灵活性和对不完整指令的鲁棒性方面也展现了实际优势，成为任务特定VLM微调的多功能和高效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:10:18 GMT</pubDate>
</item>
<item>
<title>增强大型语言模型的长度控制与复制粘贴能力</title>
<link>https://arxiv.org/abs/2410.07035</link>
<guid>https://arxiv.org/abs/2410.07035</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新的方法提升大型语言模型的长度控制与复制粘贴能力，显著改进性能。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在角色扮演、创意写作、数学推理和编码等领域显示出强大的能力，然而在长度控制方面仍然面临挑战。这一问题主要源于模型在生成文本时缺乏位置意识，导致难以遵循特定的长度限制。为了解决这一问题，我们提出了两种新方法：PositionID Prompting和PositionID Fine-Tuning，使模型能够持续监控和管理生成文本的长度。此外，我们还引入了PositionID CP Prompting，使LLMs能够准确地执行复制和粘贴操作。为了评估长度控制和复制粘贴功能，我们开发了两个基准测试。实验结果表明，我们的方法显著提高了模型对长度限制的遵守程度和复制粘贴的准确性，同时不牺牲响应质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07035" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 02:03:18 GMT</pubDate>
</item>
<item>
<title>Meissonic：高效的非自回归文本到图像建模</title>
<link>https://arxiv.org/abs/2410.08261</link>
<guid>https://arxiv.org/abs/2410.08261</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Meissonic 提升非自回归图像建模，生成高质量、高分辨率图像。</p><br /><br /><p><strong>摘要：</strong> 在视觉生成领域，扩散模型如 Stable Diffusion 取得了重要进展，但其和自回归语言模型的根本不同，使得统一语言-视觉模型的开发面临挑战。针对这一问题，我们提出了 Meissonic，它将非自回归的遮蔽图像建模（MIM）文本到图像生成技术提升至与先进扩散模型（如 SDXL）相当的水平。通过综合采用一系列架构创新、先进的位置信息编码策略以及优化的采样条件，Meissonic 显著提高了 MIM 的性能和效率。同时，我们利用高质量的训练数据，集成了基于人类偏好评分的信息微调条件，并采用特征压缩层来进一步提升图像的真实感和分辨率。我们的模型不仅在生成高质量、高分辨率图像方面与现有模型如 SDXL 相匹配，且往往超过其性能。大量实验验证了 Meissonic 的能力，展示了它作为文本到图像合成新标准的潜力。我们发布了一个能够生成 1024x1024 分辨率图像的模型检查点。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08261" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 01:19:46 GMT</pubDate>
</item>
<item>
<title>Baichuan-Omni：首个开源7B多模态大语言模型</title>
<link>https://arxiv.org/abs/2410.08565</link>
<guid>https://arxiv.org/abs/2410.08565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Baichuan-Omni，这是一个开源的7B多模态大语言模型，具备图像、视频、音频与文本的处理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们介绍了Baichuan-Omni，首个开源的7B多模态大语言模型（MLLM），它能够同时处理和分析图像、视频、音频和文本等多种信息模式，以提供先进的多模态交互体验及出色的性能。我们提出了一种有效的多模态训练方案，从7B模型开始，经过两个阶段的多模态对齐和多任务微调，以处理音频、图像、视频和文本等多种模式。这一方法使语言模型具备有效处理视觉和音频数据的能力。Baichuan-Omni在多个全模态和多模态基准测试中表现出色。我们的目标是希望这一贡献为开放源代码社区提供一个具有竞争力的基准，促进多模态理解和实时交互的进步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:29:48 GMT</pubDate>
</item>
<item>
<title>基于语义得分蒸馏采样的复杂3D内容生成研究</title>
<link>https://arxiv.org/abs/2410.09009</link>
<guid>https://arxiv.org/abs/2410.09009</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的SDS方法，SemanticSDS，通过语义嵌入提高复杂3D场景生成的表现力和准确性。</p><br /><br /><p><strong>摘要：</strong> 生成高质量的3D资产仍然是计算机图形学和视觉研究中的关键挑战。由于3D数据的稀缺，当前的前沿方法利用预训练的2D扩散先验，通过得分蒸馏采样（SDS）进行优化。尽管技术有所进步，然而，制作複杂3D场景及多个物体或复杂交互仍然困难。为了解决这一问题，近期方法逐渐引入了框或布局引导，但这些布局引导的组合方法通常在提供细粒度控制方面表现不佳，较为粗糙且缺乏表现力。为此，我们提出了一种新的得分蒸馏采样方法——语义得分蒸馏采样（SemanticSDS），旨在有效提高组合文本到3D生成的表现力与准确性。我们的做法整合了新的语义嵌入，这些嵌入在不同渲染视图间保持一致，并且能够清晰地区分不同的物体和部分。通过将这些嵌入转化为一个语义图，我们引导了区域特定的SDS过程，从而实现精确的优化与组合生成。通过利用显式的语义引导，我们的方法解锁了现有预训练扩散模型的组合能力，在复杂物体和场景的3D内容生成方面达到了优秀的质量。实验结果表明，我们的SemanticSDS框架在生成尖端复杂3D内容方面非常有效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09009" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:24:03 GMT</pubDate>
</item>
<item>
<title>SuperCorrect：一种改进小型模型推理能力的两阶段框架</title>
<link>https://arxiv.org/abs/2410.09008</link>
<guid>https://arxiv.org/abs/2410.09008</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出SuperCorrect框架，通过大模型的监督，提高小模型的推理与自我纠错能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）如GPT-4和PaLM在推理任务中表现出显著的改进。然而，较小的模型例如Llama-3-8B和DeepSeekMath-Base在复杂数学推理中仍面临挑战，尤其在独立检测和纠正推理错误方面。为了解决这一问题，我们提出了一个名为SuperCorrect的创新两阶段框架，利用大型教师模型来监督和纠正小型学生模型的推理和反思过程。在第一阶段，我们从教师模型中提取层次化的高层次和详细思维模板，以指导学生模型更好地引导细致的推理思考。在第二阶段，我们引入跨模型的协作直接偏好优化（DPO）方法，以增强学生模型的自我纠错能力，学生模型在训练过程中跟随教师的纠正轨迹。这个跨模型DPO方法教会学生模型有效定位和解决错误思维，突破思维瓶颈，获取新的技能和知识，从而应对挑战性的问题。大量实验证明，SuperCorrect优于以前的方法，尤其是我们的SuperCorrect-7B模型在MATH和GSM8K基准上分别比DeepSeekMath-7B提升了7.8%/5.3%和比Qwen2.5-Math-7B提升了15.1%/6.3%，在所有7B模型中达到了新的SOTA性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.09008" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:22:21 GMT</pubDate>
</item>
<item>
<title>EvolveDirector：基于公共资源训练文本到图像生成模型的框架</title>
<link>https://arxiv.org/abs/2410.07133</link>
<guid>https://arxiv.org/abs/2410.07133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EvolveDirector框架利用公共API生成的数据训练文本到图像生成模型，显著降低数据需求，实现优越的生成能力。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的快速进步，文本到图像生成表现出惊人的内容创作能力。然而，大多数先进模型依赖于专有数据，并提供有限的开放API，限制了其在下游任务中的应用。为探讨利用公共资源训练一个可与先进模型媲美的文本到图像生成模型的可行性，我们提出了EvolveDirector框架。该框架通过公共API与先进模型交互，获得文本-图像数据对以训练基础模型。实验显示，尽管通过生成数据训练的模型能接近先进模型的生成能力，但需要大量的样本（1000万个或更多），这会带来高昂的时间和计算资源成本，尤其是API调用费用。为了解决这一问题，EvolveDirector利用预训练的大型视觉-语言模型（VLM）来指导基础模型的演化，持续评估基础模型，并通过判别、扩展、删除和变异等操作动态更新和优化训练数据集。实验结果表明，这种方法显著减少了所需的数据量。EvolveDirector还能够在面对多个高级模型时选择其生成的最佳样本，以学习强大且均衡的能力。最终训练出的模型Edgen显示出优于这些先进模型的效果。代码和模型权重可在https://github.com/showlab/EvolveDirector获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Oct 2024 00:17:02 GMT</pubDate>
</item>
<item>
<title>利用加速偏好优化加快人类反馈下的强化学习</title>
<link>https://arxiv.org/abs/2410.06293</link>
<guid>https://arxiv.org/abs/2410.06293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种加速偏好优化框架，结合动量技术加速大语言模型的对齐。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的人类反馈（RLHF）正在成为对齐大型语言模型（LLM）的重要工具。直接偏好优化（DPO）是一种流行的方法，它将RLHF视为一个政策优化问题，而不显式估计奖励函数。此方法克服了两步法所面临的稳定性和效率问题，并展示了可以通过动量技术加速RLHF的潜力。本文首次表明，迭代偏好优化方法可以视为一种近端点方法。基于此观察，提出了一种通用的加速偏好优化（APO）框架，统一了许多现有的偏好优化算法，并采用Nesterov动量技术加速LLM的对齐过程。理论上，APO能够实现比标准迭代偏好优化方法（包括DPO和自我博弈偏好优化（SPPO））更快的收敛速度。实证结果显示，APO在AlpacaEval 2.0基准上相较于DPO、迭代DPO及其他强基线表现出更强的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 23:06:15 GMT</pubDate>
</item>
<item>
<title>Data Advisor：提升数据生成质量的增强LLM方法</title>
<link>https://arxiv.org/abs/2410.05269</link>
<guid>https://arxiv.org/abs/2410.05269</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Data Advisor，优化LLM生成数据，提高数据质量与覆盖率，特别针对安全对齐问题。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型 (LLM) 对齐中，数据是至关重要的元素。尽管近期研究探讨了使用LLM进行高效数据收集，但LLM生成的数据常常存在质量问题，如缺乏代表性、某些方面的缺失以及低质量数据点。为了解决这些问题，我们提出了Data Advisor，这是一种增强的LLM方法，用于生成符合特定特征的数据集。Data Advisor基于一组预定义原则，监控生成数据的状态，识别当前数据集的弱点，并据此建议下一轮数据生成的策略。Data Advisor可轻松集成到现有的数据生成方法中，以提升数据的质量和覆盖率。在对三种代表性LLM（即Mistral、Llama2和Falcon）进行的安全对齐实验中，Data Advisor显示了提升模型安全性与应对各种细粒度安全问题的有效性，同时不牺牲模型的实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05269" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 19:05:54 GMT</pubDate>
</item>
<item>
<title>基于神经符号学习的LLM世界模型对齐与探索</title>
<link>https://arxiv.org/abs/2410.07484</link>
<guid>https://arxiv.org/abs/2410.07484</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出WALL-E代理，通过规则学习对LLM进行环境对齐，显著提升在Minecraft和ALFWorld等开放世界中的探索效率与成功率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）作为模型基础代理的世界模型的潜力，尽管存在LLMs的先验知识与特定环境动态之间的差距，我们的研究表明，这些差距可以通过对LLM与其部署环境进行对齐来弥补。这种“世界对齐”可以通过在LLMs上进行规则学习高效实现。由于LLMs具有丰富的先验知识，通常只需少量额外规则即可使LLM预测与实定环境动态一致。为此，本文提出了一种神经符号方法，通过比较代理探索的轨迹与世界模型预测，诱导、更新和修剪规则，从而以无梯度的方式学习这些规则。最终的世界模型由LLM及学习的规则组成。我们的具身LLM代理“WALL-E”基于模型预测控制（MPC）构建，通过基于精确世界模型优化前瞻性动作，MPC显著提高了探索和学习效率。与现有LLM代理相比，WALL-E的推理只需少量主规则，而不是将冗长的缓冲轨迹纳入LLM输入。在Minecraft和ALFWorld的开放世界挑战中，WALL-E相比现有方法有更高的成功率，且重规划时间和使用的tokens更少。在Minecraft中，WALL-E的成功率超过基线15-30%，重规划轮次减少8-20轮，使用的tokens仅为60-80%。在ALFWorld中，其成功率在仅6次迭代后飙升至95%的新纪录。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07484" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 17:14:49 GMT</pubDate>
</item>
<item>
<title>向量-内联学习：扩展大型语言模型的能力</title>
<link>https://arxiv.org/abs/2410.05629</link>
<guid>https://arxiv.org/abs/2410.05629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型如何通过轻量级投影器处理连续向量，实现向量-内联学习能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在文本数据的上下文学习（ICL）能力方面表现出色。本文探索了这些能力是否可以扩展到来自不同领域的连续向量，这些向量是通过黑箱预训练编码器获得的。我们通过轻量级投影器将输入数据与LLM的嵌入空间对齐，发现LLMs能够有效地处理和学习这些投影向量，称之为向量-内联学习（Vector-ICL）。尤其值得注意的是，使用通用语言建模目标对投影器进行预训练可以实现Vector-ICL，而任务特定的微调则进一步提升了性能。我们在各种任务和模态上进行了实验，包括文本重建、数值函数回归、文本分类、摘要生成、分子描述、时间序列分类、图分类和fMRI解码。结果表明，Vector-ICL在许多情况下超越了少量样本的ICL和领域特定模型或微调。我们还进行了分析和案例研究，表明LLMs在处理向量表示方面的潜力超越了传统的基于标记的范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 16:04:02 GMT</pubDate>
</item>
<item>
<title>Zebra：一种新型生成自回归变换器用于解决时间依赖性参数偏微分方程</title>
<link>https://arxiv.org/abs/2410.03437</link>
<guid>https://arxiv.org/abs/2410.03437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zebra 是一款不需梯度适应的新型变换器，能够灵活应对各种参数 PDE，展示卓越的性能。</p><br /><br /><p><strong>摘要：</strong> 解决时间依赖性参数偏微分方程（PDEs）是一项具有挑战性的任务，因为模型必须适应系数、强迫项和边界条件等参数的变化。数据驱动的神经网络求解器通常依赖于从PDE参数分布中采样的数据进行训练，以期在新的实例上实现泛化，或依靠基于梯度的适应和元学习，隐式编码来自观察的数据动态。然而，这通常会增加推理复杂性。受到大型语言模型（LLMs）在上下文学习能力的启发，我们提出了Zebra，一种新型生成自回归变换器，旨在解决参数PDEs，而无需在推理时进行梯度适应。Zebra通过在预训练和推理过程中利用上下文信息，能够动态适应新任务，条件依赖于包含上下文轨迹或先前状态的输入序列。这种方法使得Zebra能够灵活处理任意大小的上下文输入，并通过采样多个解轨迹支持不确定性量化。我们在多种挑战性的PDE场景中评估了Zebra，展示了其适应性、鲁棒性以及相较于现有方法的优越性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 15:11:34 GMT</pubDate>
</item>
<item>
<title>DART：一种新型的非马尔可夫扩散模型</title>
<link>https://arxiv.org/abs/2410.08159</link>
<guid>https://arxiv.org/abs/2410.08159</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DART提出了一种统一自回归与扩散框架的图像生成模型，提升了训练与推理效率。</p><br /><br /><p><strong>摘要：</strong> 扩散模型已成为视觉生成的主流方法，但其基于马尔可夫过程的特性限制了模型充分利用生成轨迹的能力，导致在训练和推理时效率低下。为此，本文提出了一种新的模型DART，基于变换器架构，将自回归（AR）和扩散过程结合在一个非马尔可夫的框架内。DART通过空间和光谱的方式迭代去噪图像块，采用与标准语言模型相同的架构。DART不依赖于图像量化，增强了图像建模的有效性，保持了灵活性。同时，DART能够在一个统一模型中无缝地训练文本和图像数据。我们的实验结果表明，DART在类别条件和文本到图像生成任务上表现出色，提供了一种可扩展、高效的替代方案。通过这一统一框架，DART为高质量图像合成设立了新的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08159" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:56:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型的任务超叠现象及其内在机制研究</title>
<link>https://arxiv.org/abs/2410.05603</link>
<guid>https://arxiv.org/abs/2410.05603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨大型语言模型在单次推理过程中同时执行多个不同任务的能力，称为任务超叠现象。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在上下文学习（ICL）方面的一个惊人现象：模型能够在单次推理调用中同时执行多个计算上独立的ICL任务，这一能力被称为‘任务超叠’。我们提供了针对不同LLM家族和规模的实证证据，表明即使在模型训练时仅学习一个任务，也能出现这一现象。此外，我们提供了理论解释，认为这一能力在变换器的表达能力范围内。我们还探讨了LLMs如何在超叠过程中内部组合任务向量的机制。研究表明，较大的模型能够并行解决更多ICL任务，并更好地校准其输出分布。这些发现为LLMs潜在能力提供了新的见解，进一步支持了‘LLMs作为模拟器的超叠’的观点，并引发了关于实现同时任务执行的机制的思考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 13:27:35 GMT</pubDate>
</item>
<item>
<title>自动化基准测试中的作弊现象及其影响</title>
<link>https://arxiv.org/abs/2410.07137</link>
<guid>https://arxiv.org/abs/2410.07137</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，常量输出模型可在自动化基准测试中作弊，表现异常优异，呼吁开发反作弊机制。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在自动化大语言模型（LLM）基准测试中存在的作弊问题。研究表明，即使是一个始终输出固定响应（与输入无关）的“空模型”，也能在多个基准测试上取得高分，如在 AlpacaEval 2.0 上获得 86.5% 的 LC 胜率，在 Arena-Hard-Auto 上获得 83.0 分，而在 MT-Bench 上获得 9.55 分。这些作弊输出展示出了可转移性，因为假设测试指令是私有且不可访问的。尽管本研究主要作为概念验证，但某些对手可以利用 LLM 生成更不易察觉的作弊回复，从而从高胜率和推广影响中不道德地获利。因此，本文呼吁开发可靠的反作弊机制，以确保自动化基准测试的可信度及其在评估语言模型时的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07137" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:28:45 GMT</pubDate>
</item>
<item>
<title>LPZero：自动设计零成本代理的框架</title>
<link>https://arxiv.org/abs/2410.04808</link>
<guid>https://arxiv.org/abs/2410.04808</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LPZero是一个框架，能够自动设计零成本（ZC）代理，提升NLP任务中的性能和排名一致性。</p><br /><br /><p><strong>摘要：</strong> 在神经架构搜索（NAS）面临的大量计算开销的背景下，零成本（ZC）代理作为一种有前途的方法逐渐受到关注。然而，现有的ZC代理往往依赖于专家知识，且存在显著的试错成本，尤其在自然语言处理（NLP）任务中，许多ZC代理无法超越简单基线表现。为了解决这些问题，我们提出了一个新颖的框架LPZero，这是首个能够为各种任务自动设计ZC代理的系统，且其排名一致性优于人工设计的代理。具体而言，我们将ZC代理建模为符号方程，并整合了一个统一的代理搜索空间，该空间涵盖了由预定义的数学符号构成的现有ZC代理。LPZero利用遗传编程进行启发式搜索，以找到最佳的符号组合。同时，我们提出了一种基于规则的剪枝策略（RPS），该策略可以预先消除不太有前景的代理，从而降低代理降低性能的风险。通过对FlexiBERT、GPT-2和LLaMA-7B的广泛实验，LPZero在下游任务中的排名能力和性能均优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04808" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 12:09:02 GMT</pubDate>
</item>
<item>
<title>GLOV：利用大语言模型优化视觉语言模型的隐式优化方法</title>
<link>https://arxiv.org/abs/2410.06154</link>
<guid>https://arxiv.org/abs/2410.06154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GLOV方法，利用大语言模型作为隐式优化器提升视觉任务性能。</p><br /><br /><p><strong>摘要：</strong> 在这项研究中，我们提出了一种新颖的方法（GLOV），使大语言模型（LLMs）能够作为隐式优化器，提升视觉语言模型（VLMs）在下游视觉任务中的表现。通过对下游任务描述的元提示，GLOV查询出适合的VLM提示（如使用CLIP进行零-shot分类），并根据适应性函数获得的纯度度量对这些提示进行排名。在每个优化步骤中，排名后的提示及其准确性作为上下文示例被送入LLM，帮助LLM了解下游VLM所偏爱的文本提示类型。此外，我们在每个优化步骤中明确引导LLM生成过程，通过将来自上一步中正负解的嵌入差异向量添加至网络的中间层，从而在下一步的生成过程中引导LLM朝向下游VLM所偏好的语言类型。这一策略显著提升了下游视觉任务的表现。我们在16个多样化的数据集上全面评估了GLOV，使用两类VLM（即双编码器模型如CLIP和编码器-解码器模型如LLaVa），结果显示，所发现的解决方案能在识别性能上提升高达15.0%和57.5%（平均分别提升3.8%和21.6%）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 07:34:00 GMT</pubDate>
</item>
<item>
<title>WorFBench：一个用于评估工作流生成能力的统一基准</title>
<link>https://arxiv.org/abs/2410.07869</link>
<guid>https://arxiv.org/abs/2410.07869</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了WorFBench工作流生成基准及WorFEval评估协议，揭示LLM在序列和图规划间的能力差异。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在解决推理和规划任务方面取得了显著进展，其中将复杂问题分解为可执行工作流是关键步骤。现有的工作流评估框架存在局限，不能全面反映LLM的能力。为此，我们引入了WorFBench，这是一种统一的工作流生成基准，涵盖多样化场景和复杂图形工作流结构。同时，我们提出了WorFEval，一种系统的评估协议，利用子序列和子图匹配算法来准确量化LLM代理的工作流生成能力。通过对不同类型的LLMs进行综合评估，我们发现LLM代理在序列规划能力和图规划能力之间存在显著差距，即使是GPT-4，二者之间的差距约为15%。此外，我们训练了两个开源模型，并评估了它们在持出任务上的泛化能力。值得注意的是，生成的工作流能够提高下游任务的表现，使得推理过程所需时间减少，从而达到更优秀的性能。代码和数据集将发布于https://github.com/zjunlp/WorFBench。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07869" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:48:20 GMT</pubDate>
</item>
<item>
<title>基于运动先验的变形3D高斯点云重建框架MotionGS</title>
<link>https://arxiv.org/abs/2410.07707</link>
<guid>https://arxiv.org/abs/2410.07707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了MotionGS，一个通过运动先验引导变形的3D高斯点云重建框架，显著提升动态场景重建效果。</p><br /><br /><p><strong>摘要：</strong> 动态场景重建在3D视觉领域一直是一个长期挑战。最近，3D高斯点云技术的兴起为这一问题提供了新的视角。尽管后续工作迅速将静态3D高斯扩展到动态场景，但往往缺乏对物体运动的明确约束，导致优化困难和性能下降。为了解决这些问题，本文提出了一个新颖的变形3D高斯点云框架——MotionGS，旨在探索明确的运动先验以引导3D高斯的变形。具体而言，我们首先引入了一种光流解耦模块，该模块将光流解耦为相机流和运动流，分别对应于相机移动和物体运动。然后，运动流可以有效约束3D高斯的变形，从而模拟动态物体的运动。此外，还提出了一种相机姿态优化模块，以交替优化3D高斯和相机姿态，减轻不准确相机姿态的影响。在单目动态场景中的广泛实验验证了MotionGS在定性和定量结果上都超越了最新的技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 06:17:59 GMT</pubDate>
</item>
<item>
<title>基于数学推理和代码生成的数学继续预训练方法</title>
<link>https://arxiv.org/abs/2410.08196</link>
<guid>https://arxiv.org/abs/2410.08196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的数学继续预训练方法，通过生成代码和推理步骤提升语言模型的数学能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的数学继续预训练方法，通过生成数学代码及其相应的推理步骤，提升大型语言模型的数学推理能力。最初，我们构建了一个高质量的数学继续预训练数据集，融合了数学相关的网络数据、使用数学包的代码、数学教科书以及合成数据。接着，我们通过提取LaTeX表达式、这些表达式所需的条件及其结果，生成推理步骤。基于提取的信息，我们生成了相应的代码，以准确捕捉数学推理过程。在每个推理步骤后附加生成的代码，形成了自然语言推理步骤与其对应代码的配对数据。将这些数据与原始数据集结合，形成了一个包含19.2B标记的高性能数学预训练语料库，命名为MathCode-Pile。使用该语料库训练几种流行的基础模型显著提升了它们的数学能力，最终形成了MathCoder2模型系列。为了确保透明性和易于重现，我们将所有数据处理和训练代码开源，代码可在https://github.com/mathllm/MathCoder2获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 04:16:58 GMT</pubDate>
</item>
<item>
<title>SFTMix: 基于Mixup的指令调优方法研究</title>
<link>https://arxiv.org/abs/2410.05248</link>
<guid>https://arxiv.org/abs/2410.05248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SFTMix，通过Mixup正则化提高指令调优性能，降低对高质量数据集的依赖。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）交互驱动任务的指令调优阶段，通常通过下一个标记预测（NTP）损失对指令-响应对进行训练。以往旨在提升指令调优表现的研究往往强调需要更高质量的监督微调（SFT）数据集，这通常伴随高昂的数据过滤成本或人工标注的劳动力。然而，这些方法未能充分利用数据集的内在特性，导致高计算和劳动成本，限制了可扩展性和性能提升。本文提出了SFTMix，一种新颖的策略，通过Mixup正则化提升指令调优性能，而无需精心策划的数据集。我们观察到LLMs在语义表示空间中的信心不均匀，认为不同信心水平的示例在指令调优过程中应发挥不同角色。在此基础上，SFTMix利用训练动态识别不同信心水平的示例，减轻对高信心示例的过拟合，同时增强对低信心示例的学习信号。这一方法显著提升了在多种指令跟随和医疗领域特定SFT任务中的表现，证明了SFTMix对不同LLM家族的适应性以及对任何规模数据集的可扩展性。全面的消融研究进一步验证了SFTMix设计选择的稳健性，强调其在语言处理应用中提升性能的通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:19:15 GMT</pubDate>
</item>
<item>
<title>Agent S: 基于多模态大语言模型的自主交互框架</title>
<link>https://arxiv.org/abs/2410.08164</link>
<guid>https://arxiv.org/abs/2410.08164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Agent S 是一种开放的自主代理框架，可通过GUI自动化复杂任务，实现人机交互的变革。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Agent S，一个开放的自主代理框架，旨在通过图形用户界面与计算机进行自主交互，从而实现人机交互的变革，自动化复杂和多步骤的任务。Agent S解决了自动化计算机任务的三个主要挑战：获取领域特定知识、规划长期任务及处理动态非均匀界面。为此，Agent S 引入了经验增强层次规划，能够在多个层次上从外部知识搜索和内部经验检索中学习，从而促进高效的任务规划和子任务执行。此外，Agent S 采用了代理-计算机接口（ACI），更好地充分发挥基于多模态大语言模型（MLLMs）的GUI代理的推理和控制能力。根据OSWorld基准的评估结果，Agent S 在成功率上超过了基线水平9.37%，实现了83.6%的相对改进，并且取得了新的最先进成果。全面分析展示了各个组件的有效性，并为未来的改进提供了洞见。此外，Agent S 在新发布的WindowsAgentArena基准上展现出良好的广泛泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:06:26 GMT</pubDate>
</item>
<item>
<title>大语言与视觉模型（LLVMs）的感知能力研究</title>
<link>https://arxiv.org/abs/2410.04751</link>
<guid>https://arxiv.org/abs/2410.04751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统探讨了LLVMs在感知任务中的表现及其内在机制。</p><br /><br /><p><strong>摘要：</strong> 本文系统地研究了大语言与视觉模型（LLVMs），特别是它们在基础感知任务（如MMVP）上的低表现。通过对几种LLVMs家族（如LLaVA）的10个评估基准进行评估，我们发现多个有趣的特性：1）即便视觉块序列随机排列，它们仍以全局方式处理图像；2）在解决数学问题时，模型并不总是需要详细的数字信息；3）交叉模态的对齐在复杂推理任务中存在过拟合现象，从而导致模型失去部分视觉编码器的原始感知能力；4）模型较低层次的表示空间（低于25%）在性能和增强视觉理解中起着关键作用。基于这些观察，本文提出了未来改进LLVMs及构建更具挑战性评估基准的潜在方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.04751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 03:01:45 GMT</pubDate>
</item>
<item>
<title>AlphaLLM-CPL：一种基于MCTS行为蒸馏的自我改进框架</title>
<link>https://arxiv.org/abs/2410.06508</link>
<guid>https://arxiv.org/abs/2410.06508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AlphaLLM-CPL框架，利用MCTS生成的轨迹对LLM进行自我改进，显著提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 在本文中，我们提出了一种新的框架，AlphaLLM-CPL，用于通过蒙特卡洛树搜索（MCTS）行为蒸馏来提升大语言模型（LLM）的推理能力。尽管现有的蒸馏方法利用MCTS生成的轨迹，但仍然未能充分利用这些丰富的信息，限制了LLM推理性能的提升。AlphaLLM-CPL引入了两个关键创新：首先，它从共享同一父节点的子节点构造逐步轨迹对，以提供更有效的逐步信息用于MCTS行为蒸馏。其次，AlphaLLM-CPL采用了课程偏好学习，在每个离线训练周期中动态调整轨迹对的训练顺序，优先考虑关键学习步骤，从而减轻过拟合。通过在数学推理任务上的实验结果表明，AlphaLLM-CPL显著优于以往的MCTS行为蒸馏方法，极大地提升了LLM的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.06508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:06:42 GMT</pubDate>
</item>
<item>
<title>自回归视频扩散模型的进展及应用</title>
<link>https://arxiv.org/abs/2410.08151</link>
<guid>https://arxiv.org/abs/2410.08151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出扩展现有视频扩散模型的方法，实现长达1分钟的视频生成。</p><br /><br /><p><strong>摘要：</strong> 当前最前沿的视频扩散模型在生成高质量视频方面表现出色。然而，由于计算限制，这些模型通常只能生成时长约为10秒（240帧）的视频。本文展示了如何在不改变现有架构的情况下，将这些模型自然扩展为自回归视频扩散模型。我们的关键思想是为潜在帧分配逐渐增加的噪声水平，而不是使用单一的噪声水平，从而允许潜在帧之间的细粒度条件及注意窗口之间的大重叠。这种渐进的视频去噪方法使我们的模型在自回归生成视频帧时，能够避免质量下降或突发场景变化。我们在长视频生成任务上取得了最先进的结果，生成了长达1分钟（1440帧、24帧每秒）的视频。本文视频可在https://desaixie.github.io/pa-vdm/上查看。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 02:02:23 GMT</pubDate>
</item>
<item>
<title>大卷积核在现代卷积神经网络设计中的应用</title>
<link>https://arxiv.org/abs/2410.08049</link>
<guid>https://arxiv.org/abs/2410.08049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出大卷积核作为设计现代卷积神经网络（ConvNets）的新范式，并引入UniRepLKNet架构。</p><br /><br /><p><strong>摘要：</strong> 本文提出了大卷积核作为现代卷积神经网络（ConvNets）设计的新范式。研究表明，使用少量大卷积核，而非堆叠多个小卷积核，可能是一种更优的设计策略。我们提出了一套专门针对大卷积核ConvNets的架构设计指南，以优化其效率和性能。UniRepLKNet架构的设计原则特别强调大卷积核ConvNets在无需深层堆叠的情况下捕捉广泛空间信息的能力。实验结果表明，该模型在ImageNet上达到了88.0%的准确率，ADE20K数据集上获得了55.6%的mIoU，以及在COCO检测上取得了56.4%的AP，表现出显著的可扩展性及在多种场景下的优异性能，包括时间序列预测、音频、点云及视频识别。与视觉Transformer相比，大卷积核ConvNets具有更大的有效感受野和更高的形状偏置，克服了小卷积核CNN的纹理偏置。这些结果展示了大卷积核ConvNets的通用建模能力。所有代码和模型已在https://github.com/AILab-CVC/UniRepLKNet上公开，促进社区的进一步研究与发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:49:57 GMT</pubDate>
</item>
<item>
<title>简化和扩展扩散模型 rectification 的新策略</title>
<link>https://arxiv.org/abs/2410.07303</link>
<guid>https://arxiv.org/abs/2410.07303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">我们提出了 Rectified Diffusion，简化了 rectification 方法并验证其在 Stable Diffusion 上的效果。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视觉生成方面取得了显著进展，但由于解决生成常微分方程（ODE）的计算密集性，生成速度仍然较慢。本文提出的 Rectified Diffusion 方法，通过使用预训练的扩散模型获取噪声和样本的匹配对，简化了训练程序。我们认为，以往方法中包含的流匹配和 v 预测等组件并非必要，主要目标应是实现一阶近似 ODE 路径，而非强求路径的直线性。我们的方法不再局限于流匹配模型，而是广泛适用于各种扩散模型。经过在 Stable Diffusion v1-5 和 Stable Diffusion XL 上的验证，我们的方法不仅降低了训练成本，还提升了性能。我们的代码已开放在 GitHub 上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07303" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:27:35 GMT</pubDate>
</item>
<item>
<title>基于偏好学习的多模态轨迹检索增强方法</title>
<link>https://arxiv.org/abs/2410.03450</link>
<guid>https://arxiv.org/abs/2410.03450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MART方法，通过偏好学习优化轨迹检索，提升机器人在未见场景中的任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法MLLM as ReTriever (MART)，旨在提升机器人执行复杂任务的能力。当前的检索方法多关注表面相似性，未充分考虑轨迹对特定任务的有效性。MART利用交互数据，通过偏好学习对大型语言模型（MLLM）进行微调，使检索过程能够更好地评估和优先选择适用于未见任务的轨迹。为进一步加强理解，文章引入Trajectory Abstraction机制，利用MLLM的摘要能力将轨迹用更少的符号表示，并保留关键身分信息，这帮助代理更好地抓住轨迹中的重要里程碑。实验结果显示，在不同环境下，MART方法显著提升了代理在未见场景中的任务成功率，表明其在多模态轨迹检索与代理行为的研究中具有重要意义。所有基准任务集和模拟器代码修改将公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.03450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:21:00 GMT</pubDate>
</item>
<item>
<title>PrefixQuant：一种高效的稀疏化量化技术用于大型语言模型的推理加速</title>
<link>https://arxiv.org/abs/2410.05265</link>
<guid>https://arxiv.org/abs/2410.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PrefixQuant通过离线隔离高频稀疏令牌，实现了高效的静态量化，显著提升推理速度与准确性。</p><br /><br /><p><strong>摘要：</strong> 量化在大型语言模型（LLM）部署中尤为重要，因为它能提高内存效率和推理速度。目前的激活量化方法主要处理通道级别的离群点，往往忽视令牌级别的离群点，因此依赖于昂贵的每令牌动态量化技术。为了解决这个问题，我们提出了PrefixQuant，一种独特的技术，能够离线识别高频离群令牌，并将其前缀存储在KV缓存中，从而防止推理时生成离群令牌，并简化量化。根据我们的知识，PrefixQuant首次实现了高效的每张量静态量化，超过了昂贵的每令牌动态量化。以W4A4KV4（4位权重、4位激活和4位KV缓存）上的Llama-3-8B为例，PrefixQuant结合每张量静态量化实现了7.43的WikiText2困惑度以及71.08%的常识推理任务平均准确率，分别比之前的动态量化方法QuaRot提升了0.98困惑度与5.98准确率。此外，使用PrefixQuant的W4A4量化模型的推理速度比FP16模型快1.60至2.81倍，相较于QuaRot模型快1.2至1.3倍。我们的代码已发布在https://github.com/ChenMnZ/PrefixQuant。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:13:37 GMT</pubDate>
</item>
<item>
<title>Optima：提升大语言模型多智能体系统通信效率与任务有效性的框架</title>
<link>https://arxiv.org/abs/2410.08115</link>
<guid>https://arxiv.org/abs/2410.08115</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Optima通过提高通信效率和任务有效性，解决大语言模型多智能体系统中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Optima，一个新颖的框架，旨在解决大语言模型（LLM）基础的多智能体系统（MAS）面临的低通信效率、较差的可扩展性和缺乏有效参数更新优化方法等关键挑战。Optima通过对LLM进行训练，采用迭代的生成、排名、选择和训练模式，结合一个平衡任务性能、 tokens 效率和交流可读性的奖励函数，大幅增强了通信效率和任务有效性。我们探索了多种强化学习算法，包括监督微调（Supervised Fine-Tuning）、直接偏好优化（Direct Preference Optimization）及其混合方法，提供了它们的有效性与效率权衡的深入见解。此外，我们结合受蒙特卡洛树搜索启发的技术生成DPO数据，将对话回合视为树节点，以探索多样的互动路径。在信息不对称问答和复杂推理等常见多智能体任务上评估后，Optima相较于单智能体基线和基于Llama 3 8B的传统MAS表现出一致而显著的提升，在需要大量信息交换的任务中实现了多达2.8倍的性能提升，同时消耗的tokens少于10%。此外，Optima的效率提升为更有效利用推理计算开辟了新可能，推动了推理时间的规模法则的改进。通过解决大语言模型基础多智能体系统中的基本挑战，Optima展示了朝着可扩展、高效和有效的多智能体系统发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08115" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 01:11:50 GMT</pubDate>
</item>
<item>
<title>重复训练示例在变压器模型中的效益研究</title>
<link>https://arxiv.org/abs/2410.07041</link>
<guid>https://arxiv.org/abs/2410.07041</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，在固定训练步数下，重复训练的示例效果优于单次使用的示例。</p><br /><br /><p><strong>摘要：</strong> 本文研究了变压器模型在算法生成数据集上的性能表现，重点关注训练示例重复使用的问题。在处理最大公约数、模乘法和矩阵特征值这三种数学问题时，我们发现，在固定的训练步数下，使用重复的示例的小规模训练集的模型性能优于使用单次示例的大规模训练集的模型。这表明，重复训练的益处超越了数据多样性带来的优势。此外，我们还展示了两集训练的方法，即对小随机子集的重复使用与对训练集其余部分的正常采样相结合，可以加速学习并提升模型性能。这项研究在受控环境下提供了关于深度学习中泛化与记忆之间尚不清晰的相互关系的洞察。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.07041" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:59:17 GMT</pubDate>
</item>
<item>
<title>基于局部对抗负例损失的视觉语言模型增强方法</title>
<link>https://arxiv.org/abs/2410.05210</link>
<guid>https://arxiv.org/abs/2410.05210</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新方法FSC-CLIP，通过局部硬负例损失提升视觉语言模型的组合理解能力，保持多模态任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法——Fine-grained Selective Calibrated CLIP (FSC-CLIP)，旨在提升预训练视觉语言模型（VLMs）的组合理解能力，同时不影响零-shot多模态任务的性能。传统的微调方法通常在提高组合推理的同时降低多模态能力，其主要原因是使用全局硬负例（HN）损失，导致图像和文本的全局表示受到影响。这种全局HN损失会推送与原始文本高度相似的HN文本，从而损害模型的多模态表示。为克服这一限制，FSC-CLIP整合了局部硬负例损失和选择性校准正则化。这些创新提供了精细的负向监督，同时保持了模型的表示完整性。我们的广泛评估显示，FSC-CLIP在多项组合性和多模态任务基准上不仅达到了与最先进模型相当的组合能力，而且保持了强大的多模态能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.05210" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:47:13 GMT</pubDate>
</item>
<item>
<title>DICE：用于可控编辑的离散反演方法</title>
<link>https://arxiv.org/abs/2410.08207</link>
<guid>https://arxiv.org/abs/2410.08207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DICE是首个用于离散扩散模型的精确反演方法，支持灵活的内容编辑。</p><br /><br /><p><strong>摘要：</strong> 离散扩散模型在图像生成和掩蔽语言建模等任务中取得了成功，但在可控内容编辑方面存在局限性。我们提出了DICE（Discrete Inversion for Controllable Editing），这是第一个支持离散扩散模型（包括多项式扩散和掩蔽生成模型）的精确反演方法。通过在反向扩散过程中记录噪声序列和掩蔽模式，DICE实现了对离散数据的准确重建和灵活编辑，无需预定义掩蔽或注意力操作。我们在图像和文本领域展示了DICE的有效性，并在VQ-Diffusion、Paella和RoBERTa等模型上进行了评估。结果显示，DICE在提高编辑能力的同时，也保持了高数据保真度，为离散空间中的精细内容操控提供了新的机遇。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2410.08207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Oct 2024 00:32:24 GMT</pubDate>
</item>
</channel>
</rss>