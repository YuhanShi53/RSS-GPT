<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>POLAR：高效多视角点云刚性配准方法</title>
<link>https://arxiv.org/abs/2504.21467</link>
<guid>https://arxiv.org/abs/2504.21467</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种基于潜在空间的多视角点云配准方法，显著提升大变换和高退化情况下的性能。</p><br><br><p><strong>摘要：</strong> 点云刚性配准是三维计算机视觉中的基础问题，在多视角情况下，传统基于两两配准的方法因同步算法限制而扩展性差，而生成模型虽能克服此问题但难以处理大变换。本文提出POLAR（POint cloud LAtent Registration），通过将配准问题映射到预训练自动编码器的潜在空间，设计考虑退化的损失函数并采用高效的多起点优化策略，实现了对大量视图的高效处理且具备高鲁棒性。实验表明，POLAR在合成数据和真实数据上均优于现有技术。该方法开源且可直接安装使用。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2504.21467 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 05:42:38 GMT</pubDate>
<pubDate>Wed, 30 Apr 2025 05:42:38 GMT</pubDate>
</item>

<item>
<title>GPT-4o在图像修复领域的潜力与挑战</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPT-4o在图像生成方面表现优异，但修复任务中存在像素级结构失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统评估了GPT-4o模型在多种图像修复任务中的表现。实验表明，尽管GPT-4o生成的图像视觉效果良好，但在像素级结构保真度上往往逊色于真实图像，常见问题是比例变化、物体位置及数量偏移以及视角改变。针对这些问题，通过去雾、去雨和低光照增强三个案例研究，我们发现GPT-4o的输出可以作为强大的视觉先验，显著提升现有去雾网络的性能。此外，我们提供了实用指南和基准框架，以促进未来将GPT-4o集成到图像修复流程中。希望这项研究能推动图像生成领域的发展，同时我们将公开来自超过10个常用图像修复数据集的GPT-4o修复图像以支持后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 16:00:11 GMT</pubDate>
</item>
<item>
<title>UniVLA：一种用于跨形态机器人视觉-语言-动作学习的新框架</title>
<link>https://arxiv.org/abs/2505.06111</link>
<guid>https://arxiv.org/abs/2505.06111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniVLA通过任务驱动的动作表示和语言指令，实现高效跨形态机器人策略学习。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UniVLA的新框架，旨在解决现有机器人策略学习方法依赖标注数据且难以跨形态和环境迁移的问题。UniVLA通过引入潜在动作模型从视频中推导任务导向的动作表示，并结合语言指令在DINO特征空间中建立模型，从而有效利用多样化数据源。该框架在多个操作和导航基准测试及真实机器人部署中取得了最先进的成果，相比OpenVLA显著减少了预训练计算量和下游数据需求。此外，随着更多异构数据的加入，UniVLA的性能持续提升，展示了其在规模化和高效机器人策略学习中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 11:11:13 GMT</pubDate>
</item>
<item>
<title>大型语言模型在英国公共卫生信息领域的知识评估</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了多个大型语言模型对英国公共卫生信息的知识水平。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的广泛普及，了解其在特定领域内的知识变得至关重要，尤其是在公共卫生领域，获取相关、准确且最新的信息直接影响公众健康。本文介绍了PubHealthBench，这是一个包含超过8000个问题的新基准，用于评估LLMs在处理公共卫生查询时的多项选择题回答能力和自由形式响应能力。通过自动化管道创建的问题集基于从英国政府提取的公共卫生指导文件。对24个LLMs进行测试后发现，最新的私有LLMs（如GPT-4.5、GPT-4.1和o1）在多项选择题设置中的表现优异，超过90%，甚至超越了人类用户的基本搜索引擎使用。然而，在自由形式响应设置中，所有模型的表现均低于75%。因此，尽管最先进的LLMs在提供公共卫生信息方面显示出较高的准确性，但在自由形式响应时仍需额外的安全措施或工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 09:42:59 GMT</pubDate>
</item>
<item>
<title>WiserUI-Bench与G-FOCUS：提升UI设计说服力评估的创新方法</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WiserUI-Bench基准和G-FOCUS策略，改进基于视觉语言模型的UI设计说服力评估。</p><br /><br /><p><strong>摘要：</strong> 用户界面(UI)设计的有效性不仅关乎美学，还影响用户行为，这是设计说服力的核心原则。A/B测试虽是确定UI变体对用户参与度影响的主要方法，但成本高且耗时。尽管最近的视觉语言模型(VLMs)可以处理自动化UI分析，但现有方法主要关注孤立的设计属性，而非关键的说服力比较。为解决这一问题，我们引入WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对实际UI图像样本及其A/B测试结果和专家解释。此外，我们提出了G-FOCUS，一种新的推理策略，通过减少位置偏差并提高评估准确性增强基于VLM的说服力评估。实验结果显示，G-FOCUS在成对UI评估的一致性和准确性上优于现有方法。这项工作通过促进基于VLM的UI说服力评估，为补充A/B测试提供了途径，推动了可扩展的UI偏好建模和设计优化的进步。代码和数据将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:00:32 GMT</pubDate>
</item>
<item>
<title>Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02686</link>
<guid>https://arxiv.org/abs/2505.02686</guid>
<content:encoded><![CDATA[
Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 10:33:49 GMT</pubDate>
</item>
<item>
<title>Bielik v3：优化波兰语处理的高效生成式文本模型</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik v3推出两个参数量分别为1.5B和4.5B的模型，性能媲美更大规模模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik v3系列的两种参数高效生成式文本模型（1.5B和4.5B），专门针对波兰语言处理进行了优化。这些模型通过一系列创新技术实现高性能，例如定制化的波兰语分词器（APT4）、加权指令交叉熵损失函数以及自适应学习率策略，显著提升了训练效率和模型效果。模型基于精心筛选的2920亿标记的语料库进行训练，在多个基准测试中表现出色，如Open PL LLM Leaderboard、复杂波兰文理解基准、Polish EQ-Bench和波兰医学领导力榜单等。其中4.5B参数模型的表现可与规模为其2至3倍的模型相媲美，而1.5B参数模型则在紧凑架构下展现出强劲的性能。这一成果为资源受限的应用场景提供了高质量波兰语AI模型的新标杆。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 06:39:51 GMT</pubDate>
</item>
<item>
<title>Bielik 11B v2：面向波兰语处理的高效语言模型</title>
<link>https://arxiv.org/abs/2505.02410</link>
<guid>https://arxiv.org/abs/2505.02410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik 11B v2展示了卓越的波兰语处理能力，同时具备强大的跨语言功能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik 11B v2，这是一种基于Mistral 7B v0.2架构优化的波兰语语言模型，参数规模达到11B，通过深度扩展实现性能提升。该模型引入了加权指令交叉熵损失函数和自适应学习率两项关键技术，显著提升了多语言任务的表现，尤其是在波兰语相关任务中的表现超越了许多更大参数量的模型。此外，其高效的参数利用和多种量化选项使其能够在不同硬件配置上部署，为资源有限的语言建模提供了新的基准。Bielik 11B v2在多项波兰语基准测试中表现出色，涵盖了从语言理解到复杂推理的各种任务，成为少有代表性语言领域的一项重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 03:03:41 GMT</pubDate>
</item>
<item>
<title>基于鲁棒文本水印的大语言模型高效遗忘评估方法</title>
<link>https://arxiv.org/abs/2505.05064</link>
<guid>https://arxiv.org/abs/2505.05064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个面向大语言模型的数据驱动遗忘度量方法WaterDrum。</p><br /><br /><p><strong>摘要：</strong> 现有基于模型效用的遗忘度量方法在实际应用中存在局限性，如忘记集和保留集内容语义相似、重新训练模型不可行等情况下无法准确评估遗忘效果。本文提出了首个面向大语言模型的数据驱动遗忘度量方法WaterDrum，利用鲁棒文本水印技术克服这些限制。此外，我们还引入了新的基准数据集，用于严格评估遗忘算法，并提供了代码和数据集的公开访问链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:56:46 GMT</pubDate>
</item>
<item>
<title>RL^V：强化学习中引入验证能力提升LLM推理性能</title>
<link>https://arxiv.org/abs/2505.04842</link>
<guid>https://arxiv.org/abs/2505.04842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL^V通过联合训练LLM作为推理器和生成式验证器，显著提高数学问题求解精度并优化计算效率。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLM）推理微调的主流强化学习方法（如GRPO或Leave-one-out PPO），倾向于放弃已学得的价值函数，转而依赖经验估计回报，这限制了测试阶段基于价值函数进行验证的计算扩展性。本文提出了一种名为RL^V的新方法，该方法通过使用强化学习生成的数据，同时训练LLM作为推理器和生成式验证器，从而在不增加显著开销的情况下赋予模型验证能力。实验表明，RL^V使MATH数据集的准确率提升了超过20%，并且相比基础强化学习方法，在测试阶段的并行计算效率提高了8到32倍。此外，RL^V在易难任务迁移及域外任务上均展现出强大的泛化能力，同时在联合扩展并行与顺序测试时间计算时，相较于原始R1模型取得了1.2至1.6倍的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 18:41:26 GMT</pubDate>
</item>
<item>
<title>视觉-语言-行动模型综述：架构创新与未来展望</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述了近3年视觉-语言-行动模型的最新进展及其应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了近年来视觉-语言-行动（VLA）模型的研究进展，涵盖五大主题支柱，如从跨模态学习到整合视觉语言模型、动作规划器和分层控制器的通用代理。通过分析80余篇相关论文，重点讨论了架构创新、高效参数训练及实时推理加速等进步领域，并探讨了人形机器人、自动驾驶、医疗工业机器人等多样化应用场景。同时，文章还针对实时控制、多模态动作表示等挑战提出解决方案，并展望了VLA模型与具身人工智能融合推动社会对齐的智能通用体的未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 15:46:43 GMT</pubDate>
</item>
<item>
<title>结合策略优化语言模型对齐：on-policy与off-policy数据的互补优势</title>
<link>https://arxiv.org/abs/2505.02363</link>
<guid>https://arxiv.org/abs/2505.02363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现on-policy和off-policy数据在偏好优化中具有互补性，提出SIMPLEMIX方法显著提升语言模型对齐效果。</p><br /><br /><p><strong>摘要：</strong> 语言模型对齐依赖于成对偏好数据集，然而关于on-policy数据与off-policy数据在偏好学习中的优劣关系存在争议。本研究通过系统分析指出，on-policy数据在推理任务如数学和编程中表现优异，而off-policy数据在开放式任务如创意写作和个人推荐方面更具优势。基于此，我们提出了SIMPLEMIX方法，通过简单混合两种数据源充分利用其互补特性。实验结果显示，SIMPLEMIX在Alpaca Eval 2.0等多任务基准测试中平均优于单独使用on-policy DPO和off-policy DPO达6.03%，且性能超越了更为复杂的HyPO和DPO-Mix-P方法，平均提高3.05%。这一成果为语言模型的高效对齐提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:54:44 GMT</pubDate>
</item>
<item>
<title>3D场景生成综述：技术进展与未来方向</title>
<link>https://arxiv.org/abs/2505.05474</link>
<guid>https://arxiv.org/abs/2505.05474</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述深度学习在3D场景生成中的最新进展及四大范式。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于深度生成模型（如GANs、扩散模型）和3D表示（如NeRF、3D高斯模型）的3D场景生成技术取得了显著进步，大幅提升了场景的真实感、多样性和视角一致性。本文系统性回顾了当前最先进的方法，将其分为基于规则生成、神经3D生成、基于图像生成和基于视频生成四大范式，分析技术基础、优缺点及代表性成果，并探讨常用数据集、评估协议及下游应用。尽管如此，生成能力、3D表示、数据标注及评估仍面临挑战，未来研究方向包括更高保真度、物理感知交互生成及统一感知生成模型。该综述旨在为3D场景生成领域的研究人员提供全面参考，并通过项目页面持续跟踪最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05474" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>Flow-GRPO：首个结合在线强化学习与流匹配模型的方法</title>
<link>https://arxiv.org/abs/2505.05470</link>
<guid>https://arxiv.org/abs/2505.05470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个将在线强化学习融入流匹配模型的方法Flow-GRPO。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Flow-GRPO的新方法，它是第一个将在线强化学习（RL）与流匹配模型相结合的技术。该方法通过两种策略实现创新：首先，将确定性的常微分方程（ODE）转换为等效的随机微分方程（SDE），使模型在所有时间步上都能匹配原始模型的边缘分布，从而支持统计采样进行RL探索；其次，采用去噪降维策略，在减少训练去噪步骤的同时保持原始推理时间步数，显著提高了采样效率且不降低性能。实验表明，Flow-GRPO在多个文本到图像的任务中表现出色，尤其在复杂组合场景下，经过RL调优的SD3.5模型能够生成几乎完美的对象数量、空间关系及细粒度属性，使得GenEval准确率从63%提升至95%，视觉文本渲染准确率也从59%提高到92%。此外，Flow-GRPO在人类偏好对齐方面取得了显著进步，且未观察到明显的奖励黑客现象，即奖励提升并未牺牲图像质量和多样性，这两者在实验中均保持稳定。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:45 GMT</pubDate>
</item>
<item>
<title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
<link>https://arxiv.org/abs/2505.05469</link>
<guid>https://arxiv.org/abs/2505.05469</guid>
<content:encoded><![CDATA[
We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>StreamBridge：将离线Video-LLMs转化为流式模型的高效框架</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamBridge解决了现有视频语言模型向在线场景适配的两大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamBridge框架，旨在将现有的离线视频大语言模型（Video-LLMs）改造为具备实时流处理能力的模型。该框架通过引入内存缓冲区与轮次衰减压缩策略支持多轮长上下文交互，同时采用解耦轻量级激活模型实现持续的主动响应。此外，为了支持这一框架，我们构建了Stream-IT数据集，专门用于流式视频理解任务。实验表明，StreamBridge显著提升了多种任务下的流式理解性能，甚至超过了GPT-4o和Gemini 1.5 Pro等专有模型，在标准视频理解基准测试中也表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>英语推理能力的跨语言泛化研究</title>
<link>https://arxiv.org/abs/2505.05408</link>
<guid>https://arxiv.org/abs/2505.05408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语推理微调可提升多语言数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了针对英语推理进行微调的大型多语言语言模型在跨语言推理中的泛化能力。实验表明，增加英语推理模型的推理计算规模可以显著提高多种语言（包括低资源语言）的数学推理能力，甚至超越更大规模的模型。同时，虽然这些模型的推理链条主要基于英语，但它们能够通过引用和思考非英语输入的方式进行跨语言推理。此外，研究揭示了一种有效控制推理链条语言的方法，并发现模型在高资源语言上的表现更优。然而，模型在跨领域推理上表现出较差的泛化能力，特别是在从科学、技术、工程和数学（STEM）到文化常识知识的转换中。综上所述，该研究展示了英语推理在测试阶段扩展的潜力、机制及其局限性，并建议在高资源语言中进行推理，同时需进一步改进低资源语言及跨领域推理的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 12:50:06 GMT</pubDate>
</item>
<item>
<title>基于情境学习的贡献度测量方法ICon提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需梯度计算的情境学习贡献度测量方法ICon，显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ICon的新方法，它利用情境学习（ICL）的隐式微调特性，无需梯度计算或人工设计启发式指标，即可评估样本对模型训练的贡献度。ICon通过衡量隐式学习下性能的变化，有效筛选高贡献数据，显著降低了训练成本并提升了模型性能。实验表明，在LLaMA3.1-8B上，仅使用15%经ICon筛选的数据即可超越完整数据集的表现，并优于现有常用数据选择方法。进一步分析显示，ICon选出的高贡献样本不仅涵盖多样化任务，还具有适中的难度，而非仅仅是最难的任务。ICon为大语言模型的数据选择提供了高效且低偏见的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:17:37 GMT</pubDate>
</item>
<item>
<title>弹性推理框架实现可控的大规模链式思维推理</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弹性推理框架解决大规模模型推理长度不可控问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在复杂任务中生成过长链条思维的问题，提出了一种名为弹性推理的新框架。该框架将推理过程分为独立分配预算的思考和解决方案两个阶段，在测试时优先保证解决方案的完整性，显著提升了在资源受限情况下的可靠性。同时，通过引入轻量级预算约束滚动策略，使模型在思考过程被截断时也能自适应推理，并有效泛化到未见过的预算限制下，而无需额外训练。实验表明，弹性推理在严格的预算限制下表现稳健，且训练成本显著低于基线方法，即使在无约束设置中也产生更简洁高效的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:01:06 GMT</pubDate>
</item>
<item>
<title>语言引导的3D场景物体放置任务及基准</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的语言引导3D场景物体放置任务并创建基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项名为“语言引导的3D场景物体放置”的新任务，模型需要根据点云、3D资产和文本提示，在3D场景中找到符合要求的物体位置。此任务相较于其他3D场景中的语言引导定位任务具有独特挑战，如多解性和对三维几何关系的理解需求。为了推动该领域的发展，我们提出了一个新的基准和评估协议，同时发布了用于训练3D大语言模型的数据集，并提供了首个非平凡基线方法。我们认为，这项具有挑战性的任务及其新基准将成为评估和比较通用3D大语言模型的标准之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05288" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 10:29:11 GMT</pubDate>
</item>
<item>
<title>Fine-Grained CLIP：通过多模态增强实现细粒度理解</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Fine-Grained CLIP，显著提升图像细粒度理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比语言-图像预训练模型（CLIP）在细粒度理解上的局限性，提出Fine-Grained CLIP（FG-CLIP）。该方法通过利用大规模多模态模型生成包含全局语义细节的长描述图像对、构建高质量区域标注数据集以及引入硬负样本等创新手段，大幅提升了模型区分细微语义差异的能力。实验表明，FG-CLIP在细粒度理解、开放词汇目标检测、图像-文本检索及通用多模态基准任务上均优于现有方法。相关代码、数据和模型已开源。关键词：多模态学习、细粒度理解、CLIP</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 05:06:53 GMT</pubDate>
</item>
<item>
<title>链式思维令牌在复杂推理中的变量特性研究</title>
<link>https://arxiv.org/abs/2505.04955</link>
<guid>https://arxiv.org/abs/2505.04955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示链式思维令牌在解决复杂任务时的功能类似计算机程序中的变量。</p><br /><br /><p><strong>摘要：</strong> 本文通过实证研究探讨大型语言模型中链式思维（CoT）令牌在复合任务如多数字乘法和动态规划中的作用。尽管CoT对解决问题至关重要，但实验表明仅保留存储中间结果的令牌即可实现相近性能。此外，使用替代潜在形式存储中间结果并未影响模型表现。随机干预部分CoT值后，后续令牌及最终答案随之改变。这些发现表明CoT令牌可能类似于程序中的变量，但存在未预料到的捷径和令牌间计算复杂性限制等潜在问题。相关代码与数据已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 01:32:36 GMT</pubDate>
</item>
<item>
<title>多模态推理模型的研究进展与未来展望</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态推理模型在人工智能中实现复杂推理能力，面临泛化、深度及自主行为等挑战。</p><br /><br /><p><strong>摘要：</strong> 推理是智能的核心，尤其在开放、不确定且多模态的环境中，推理能力对人工智能系统至关重要。大型多模态推理模型（LMRMs）通过整合文本、图像、音频和视频等模态，支持复杂的推理功能。早期研究基于特定任务模块，而现代方法则转向统一的语言中心框架，通过指令微调和强化学习提升推理效果。然而，跨模态泛化、推理深度及自主行为仍面临诸多挑战。本文综述了多模态推理领域的研究进展，提出四阶段发展路线图，涵盖任务特定模块、统一多模态大模型、以及原生多模态推理模型（N-LMRMs）。通过分析O3和O4-mini等基准测试案例，探讨了在真实复杂环境中的可扩展性、自主性和适应性推理规划的潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 23:35:23 GMT</pubDate>
</item>
<item>
<title>迈向通用多模态模型：General-Level评估框架与General-Bench基准</title>
<link>https://arxiv.org/abs/2505.04620</link>
<guid>https://arxiv.org/abs/2505.04620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大语言模型正向通用化发展，但性能提升是否等同于更强能力？</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型（MLLM）正迅速发展，从理解多种模态到跨模态生成，其能力已扩展至细粒度理解及任意模态支持。然而，现有评估基准无法简单将任务表现直接关联至模型的整体能力。本文提出General-Level评估框架，定义五级性能与通用性标准，通过协同效应衡量模型能力一致性，并推出包含超70万实例的General-Bench基准，涵盖325,800多个样本和700多项任务。该研究通过对上百种顶级MLLM的评估揭示了通用化模型的能力排名及其面临的挑战，为下一代多模态基础模型的研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>X-Reasoner：通过文本后训练实现跨模态和跨领域可泛化推理</title>
<link>https://arxiv.org/abs/2505.03981</link>
<guid>https://arxiv.org/abs/2505.03981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究证明通用文本后训练可使视觉语言模型具备跨模态和跨领域的强泛化推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，专有模型在多模态推理方面表现出色，但现有开源研究多集中于文本推理，且评估局限于数学和通用领域任务。本文探讨了推理是否能在模态和领域间通用这一基础问题，并通过引入X-Reasoner模型，验证了通用领域文本后训练可以有效提升推理的泛化能力。该模型采用两阶段方法：初始监督微调阶段结合蒸馏后的长链思维，随后通过可验证奖励进行强化学习。实验显示，X-Reasoner在多模态和跨领域设置中表现优异，超越了现有基于领域内和多模态数据训练的最先进模型。进一步研究发现，针对特定领域的文本数据继续训练可进一步提高X-Reasoner在专业领域的性能，由此推出了X-Reasoner-Med，其在多种文本和多模态医学基准测试中达到新高度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 17:08:27 GMT</pubDate>
</item>
<item>
<title>LiftFeat：通过三维几何特征增强视觉定位中的局部特征匹配</title>
<link>https://arxiv.org/abs/2505.03422</link>
<guid>https://arxiv.org/abs/2505.03422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级网络LiftFeat，利用三维几何特征提升特征描述器的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LiftFeat的新轻量级网络，旨在提高视觉特征描述器在极端条件下的鲁棒性和区分度。传统方法在光照变化剧烈、纹理稀少或存在重复模式的情况下难以提取有效特征，而LiftFeat通过结合伪表面法线标签和预测的表面法线，设计了一个三维几何感知特征提升模块，将二维原始描述符与表面法线特征融合，显著增强了描述符在极端环境下的性能。实验表明，LiftFeat在相对位姿估计、单应性估计和视觉定位等任务上优于现有的一些轻量级方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:59:23 GMT</pubDate>
</item>
<item>
<title>SAGE框架评估大语言模型的社会认知能力</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAGE框架通过模拟情感变化评估大语言模型的社会认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sentient Agent as a Judge (SAGE) 的自动化评估框架，用于衡量大型语言模型（LLM）的高级社会认知能力。SAGE通过实例化一个具备人类情感变化和内心思考模拟的智能代理，在多轮对话中提供更真实的模型评估。实验结果显示，SAGE的最终情感评分与Barrett-Lennard关系量表（BLRI）评分及话语级同理心指标高度相关，验证了其心理真实性。此外，基于SAGE构建的公开排行榜揭示了前沿系统（如GPT-4o-Latest、Gemini2.5-Pro）与早期基线之间显著的能力差距，这种差距在传统排行榜中并未显现。因此，SAGE成为追踪真正具有同理心和社会适应能力的语言代理发展的重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02847" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 15:06:10 GMT</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 13:32:43 GMT</pubDate>
</item>
<item>
<title>OpenVision：开源视觉编码器家族挑战CLIP</title>
<link>https://arxiv.org/abs/2505.04601</link>
<guid>https://arxiv.org/abs/2505.04601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenVision提供完全开源的视觉编码器，性能媲美CLIP。</p><br /><br /><p><strong>摘要：</strong> OpenAI的CLIP自2021年初发布以来一直是构建多模态基础模型的首选视觉编码器。然而，近期的替代方案如SigLIP虽开始挑战这一地位，但大多存在数据闭源或训练配方未公开的问题。本文通过推出OpenVision填补了这一空白，OpenVision是一个完全开源且成本效益高的视觉编码器系列，在整合到LLaVA等多模态框架时表现可媲美甚至超越CLIP。基于现有工作如CLIPS训练框架和Recap-DataComp-1B训练数据，OpenVision揭示了提升编码器质量的关键见解，并展示了在推进多模态模型方面的实际益处。通过提供参数规模从5.9M到632.1M不等的多种视觉编码器，OpenVision为实践者提供了在容量与效率之间灵活权衡的选择，较大模型提升了多模态性能，而较小版本则支持轻量级边缘部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:48:35 GMT</pubDate>
</item>
<item>
<title>COSMOS：在资源约束下高效预测大语言模型适配结果</title>
<link>https://arxiv.org/abs/2505.01449</link>
<guid>https://arxiv.org/abs/2505.01449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出COSMOS框架，大幅降低大语言模型适配性能和成本预测的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在资源受限条件下，是否可以准确预测大语言模型（LLMs）的性能和成本，而无需进行昂贵的试验。我们形式化了LLMs的策略选择问题，并引入了COSMOS框架，该框架通过轻量级代理模型和低样本缩放律预测微调性能和检索增强上下文学习结果。对八个代表性基准的广泛评估显示，COSMOS在平均情况下将计算成本降低了92.72%，在资源密集型场景中最高可减少98.71%。实验结果表明，这种高效的预测方法不仅可行，还能显著减少LLMs部署的计算开销，同时保持性能标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 22:06:26 GMT</pubDate>
</item>
<item>
<title>OmniGIRL：多语言、多模态、多领域的GitHub问题自动解决基准测试</title>
<link>https://arxiv.org/abs/2505.04606</link>
<guid>https://arxiv.org/abs/2505.04606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGIRL是一个面向多语言、多模态、多领域GitHub问题解决的基准测试。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLMs）在GitHub问题自动解决任务中的评估基准主要存在三个局限性：仅限单一编程语言、覆盖领域狭窄以及只关注文本信息。本文提出OmniGIRL，这是一个涵盖四种编程语言（Python、JavaScript、TypeScript和Java）及八个不同领域的多语言、多模态、多域基准测试，包含959个任务实例。实验表明，现有LLMs在此基准上的表现有限，即使最佳模型GPT-4o也仅解决了8.6%的问题。此外，LLMs在处理涉及图像信息的问题时表现尤为困难，Claude-3.5-Sonnet在包含图像信息的问题上取得了10.5%的最佳解决率。最后，本文分析了LLMs在OmniGIRL上表现不佳的原因，并为未来改进提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:51:10 GMT</pubDate>
</item>
<item>
<title>轻量级外部信息驱动的自适应检索方法</title>
<link>https://arxiv.org/abs/2505.04253</link>
<guid>https://arxiv.org/abs/2505.04253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于外部信息的轻量级自适应检索方法，提升问答性能并降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）容易产生幻觉，而检索增强生成（RAG）虽能缓解这一问题，但计算成本高昂且存在信息误导风险。现有自适应检索方法依赖LLM的不确定性估计，效率较低且不实用。本研究引入了一种独立于LLM的轻量级自适应检索方法，该方法基于外部信息。我们分析了27个特征，分属7组及其混合组合，并在6个问答数据集上评估了这些方法的问答表现与效率。实验结果显示，我们的方法在性能上可媲美复杂的LLM基方法，同时显著提高了效率，展示了外部信息在自适应检索中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 04:58:52 GMT</pubDate>
</item>
<item>
<title>RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2505.03538</link>
<guid>https://arxiv.org/abs/2505.03538</guid>
<content:encoded><![CDATA[
Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 09:50:57 GMT</pubDate>
</item>
<item>
<title>Cognitio Emergens框架：重新定义人机协作的科学知识创造</title>
<link>https://arxiv.org/abs/2505.03105</link>
<guid>https://arxiv.org/abs/2505.03105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Cognitio Emergens框架，重新审视人机合作中的科学知识创造。</p><br /><br /><p><strong>摘要：</strong> 随着人类和AI系统逐渐超越工具使用者的关系，进入协同演化的认识伙伴关系阶段，科学知识的创造方式正在发生根本性变革。例如，AlphaFold在蛋白质结构预测上的突破改变了研究人员对基本关系的认知方式。本文引入Cognitio Emergens（CE）框架，该框架旨在解决现有模型的关键局限性，这些模型往往关注静态角色或狭窄指标，而未能捕捉科学理解如何通过递归的人机交互随着时间推移而产生。CE框架由三个组成部分构成：首先，Agency Configurations描述了人类和AI之间权威分配的方式，包括指导型、贡献型和合作伙伴型三种模式，这些模式动态地在不同配置间振荡而非线性发展；其次，Epistemic Dimensions捕捉了六个特定能力，这些能力通过发现、整合和投影轴上的协作而出现，形成了独特的“能力特征”，用于指导发展；最后，Partnership Dynamics确定了影响这些关系发展的力量，特别是认知异化风险，即研究人员可能失去对其正式认可的知识的解释控制。CE框架借鉴了自创生理论、社会系统理论和组织模块化理论，揭示了知识共同创造如何通过持续的角色、价值观和组织结构谈判而产生。通过重新概念化人机科学合作为根本的协同进化过程，CE框架提供了一种平衡的观点，既不盲目庆祝也不过度恐惧AI不断演化的角色，而是提供了概念工具，用以培养既能维持有意义的人类参与又能推动科学突破的合作关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 21:49:44 GMT</pubDate>
</item>
<item>
<title>AutoLibra：基于开放反馈的智能体评估框架</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoLibra框架，将人类反馈转化为细粒度智能体行为评估指标。</p><br /><br /><p><strong>摘要：</strong> 当前智能体评估多依赖于任务成功率等粗略指标，这些指标由专家手动设计且无法捕捉中间阶段的行为表现。本文提出AutoLibra框架，通过将开放式的用户反馈（如“按钮禁用时不要重复点击”）转化为对智能体轨迹的细粒度行为评价指标，实现对智能体行为的量化评估。该框架通过行为聚类和具体示例定义指标，可作为大型语言模型评估器的提示来源。同时，引入覆盖率和冗余度两个元指标优化指标集的一致性，实验表明AutoLibra能诱导出比现有基准更具体的评估指标并发现新指标。此外，AutoLibra在文本游戏任务和网页导航任务中展示了其在提升智能体性能和数据筛选上的应用价值，证明其作为任务不可知工具的强大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:49 GMT</pubDate>
</item>
<item>
<title>PrimitiveAnything：一种基于形状条件的几何元素组合生成框架</title>
<link>https://arxiv.org/abs/2505.04622</link>
<guid>https://arxiv.org/abs/2505.04622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的形状分解框架，能够生成高质量的几何元素组合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PrimitiveAnything的新框架，该框架将形状基本元素抽象重新定义为基本元素组装生成任务。通过形状条件下的基本元素变换器和无歧义参数化方案，实现了多种类型的基本元素统一表示。与现有方法相比，新框架直接从大规模手工制作的抽象数据中学习，从而更好地捕捉人类对复杂形状分解的理解能力。实验表明，该方法在保持几何精确性的同时，生成的元素组合更符合人类感知，并且在多样化的形状类别中表现出色。此外，它还适用于多个3D应用领域，并展示了在游戏等用户生成内容中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>ZeroSearch：无需实时搜索引擎的大型语言模型检索能力强化学习框架</title>
<link>https://arxiv.org/abs/2505.04588</link>
<guid>https://arxiv.org/abs/2505.04588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习提升大型语言模型检索能力，解决文档质量不可控和API成本过高的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ZeroSearch的强化学习框架，用于增强大型语言模型（LLMs）的检索能力，而无需与实时搜索引擎交互。传统方法因文档质量不可控和高昂的API费用面临挑战，ZeroSearch通过轻量级有监督微调将LLM转化为检索模块，并采用基于课程的滚动策略逐步降低生成文档的质量，从而激发模型的推理能力。实验表明，该方法不仅有效提升了LLMs的检索性能，还具有良好的泛化性和与多种强化学习算法的兼容性，且在某些情况下甚至超越了真实搜索引擎的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:30:22 GMT</pubDate>
</item>
<item>
<title>基于确定性马尔可夫决策过程的形式化问题求解框架</title>
<link>https://arxiv.org/abs/2505.04528</link>
<guid>https://arxiv.org/abs/2505.04528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FPS框架实现过程验证问题求解并评估多个FTP模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文填补了科学与工程领域问题求解形式化表述的空白，通过将问题求解建模为确定性马尔可夫决策过程，提出了FPS（Formal Problem-Solving）框架，利用现有FTP（Formal Theorem Proving）环境进行过程验证。此外，还设计了D-FPS框架以分离求解与答案验证，增强人类对齐能力。文中证明了这些框架的表达能力、正确性和完备性，并构建了三个问题求解基准测试集：FormalMath500、MiniF2F-Solving和PutnamBench-Solving。同时，引入RPE（Restricted Propositional Equivalence）符号方法评估答案正确性。实验表明，目前流行的FTP模型和提示方法在基准测试中的表现有限，例如在FormalMath500上最多解决23.77%的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 12:02:14 GMT</pubDate>
</item>
<item>
<title>HunyuanCustom：多模态自定义视频生成框架提升身份一致性</title>
<link>https://arxiv.org/abs/2505.04512</link>
<guid>https://arxiv.org/abs/2505.04512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持多模态条件的HunyuanCustom框架，显著提升定制视频的身份一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HunyuanCustom的多模态自定义视频生成框架，该框架通过引入文本-图像融合模块和图像ID增强模块来解决身份一致性问题，并支持多种输入模态如图像、音频、视频和文本。具体而言，文本-图像融合模块基于LLaVA增强多模态理解能力，而图像ID增强模块则利用时间串联强化帧间身份特征。此外，为了实现音频和视频条件下的生成任务，设计了特定的模态条件注入机制，包括通过空间交叉注意力实现层次对齐的AudioNet模块，以及通过基于补丁化的特征对齐网络整合潜压缩条件视频的视频驱动注入模块。实验表明，HunyuanCustom在单主体和多主体场景中均优于现有方法，在身份一致性、真实感及文本-视频对齐方面表现优异。此外，该模型在下游任务中也表现出鲁棒性，所有代码和模型资源均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 11:33:18 GMT</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 08:32:01 GMT</pubDate>
</item>
<item>
<title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.03912</link>
<guid>https://arxiv.org/abs/2505.03912</guid>
<content:encoded><![CDATA[
Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 14:35:07 GMT</pubDate>
</item>
<item>
<title>OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents</title>
<link>https://arxiv.org/abs/2505.03570</link>
<guid>https://arxiv.org/abs/2505.03570</guid>
<content:encoded><![CDATA[
In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 10:29:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在复杂问题求解中的能力与挑战</title>
<link>https://arxiv.org/abs/2505.03418</link>
<guid>https://arxiv.org/abs/2505.03418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述大型语言模型在多步推理、领域知识整合及结果验证中的应用与局限。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能的发展，大型语言模型（LLMs）已成为解决复杂问题的强大工具。与传统计算系统不同，LLMs结合了强大的计算能力和类似人类的推理能力，能够在多个领域生成解决方案。然而，将LLMs应用于实际问题解决时面临多步推理、领域知识整合和结果验证等重大挑战。本文探讨了LLMs在复杂问题求解中的能力与限制，分析了链式思维（CoT）、知识增强和多种基于LLMs和工具的验证技术。此外，文章还针对软件工程、数学推理与证明、数据分析建模及科学研究等特定领域的挑战进行了阐述，并讨论了当前LLMs解决方案的基本局限性及其未来发展方向，特别是在多步推理、领域知识整合和结果验证方面的潜在改进路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:53:58 GMT</pubDate>
</item>
<item>
<title>多模态理解与图像生成统一模型的研究综述</title>
<link>https://arxiv.org/abs/2505.02567</link>
<guid>https://arxiv.org/abs/2505.02567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了多模态理解和图像生成统一模型的最新进展及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态理解和图像生成领域各自取得了显著进步，但因采用不同的架构范式（如自回归与扩散模型），两者发展相对独立。近期，学术界对统一这两类任务的框架表现出浓厚兴趣，GPT-4o等新模型的出现进一步推动了这一趋势。然而，由于架构差异，实现有效融合仍面临诸多挑战。本文通过介绍多模态理解与文本到图像生成的基础概念与最新进展，梳理现有统一模型的三种主要架构范式（扩散型、自回归型及混合型），并分析相关创新设计。同时，我们整理了适用于统一模型的数据集与基准测试资源，讨论了该领域的关键难题，如标记化策略、跨模态注意力机制及数据需求。作为新兴研究方向，本综述旨在为未来探索提供指导，并将持续更新以反映最新成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02567" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 07:18:03 GMT</pubDate>
</item>
<item>
<title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.02393</link>
<guid>https://arxiv.org/abs/2505.02393</guid>
<content:encoded><![CDATA[
Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:33:20 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在视觉视角转换中的能力评估</title>
<link>https://arxiv.org/abs/2505.03821</link>
<guid>https://arxiv.org/abs/2505.03821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有视觉语言模型在场景理解方面表现良好，但在空间推理和视角转换上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本研究通过设计一套基于人类测试的视觉任务，探索视觉语言模型（VLMs）执行视觉视角转换的能力。我们构建了144个独特的视觉任务，这些任务通过控制场景中的物体位置及人形迷你模型的方向等变量，结合鸟瞰图和表面视图来评估模型的视觉认知水平。每个任务配有一系列诊断问题，旨在测试场景理解、空间推理及视觉视角转换三个层次的认知能力。实验结果显示，尽管最先进的模型如GPT-4-Turbo和Llama-3.2-11B-Vision-Instruct在场景理解方面表现出色，但其在空间推理和视角转换上的表现却明显下降。分析表明，当前模型在表面级对象识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这提示未来VLM开发需整合显式的几何表示并采用定制化的训练方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 20:10:41 GMT</pubDate>
</item>
<item>
<title>R&amp;B框架通过语义重分区和高效优化提升数据混合策略性能</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R&amp;B框架通过语义重分区和优化数据组成显著提升语言模型训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为R&amp;B的框架，旨在解决现有数据混合策略中的两个主要问题：依赖预先设定的数据领域可能导致遗漏关键语义细微差别，且其计算复杂度随领域数量呈指数增长。R&amp;B通过基于语义相似性的重新分组（Regroup）创建更精细的数据领域，并利用训练过程中获得的领域梯度诱导的Gram矩阵来高效优化数据组合（Balance）。与传统方法不同的是，R&amp;B无需额外计算即可获取评估信息如损失或梯度。理论分析表明，该技术在标准规则条件下表现优于非自适应混合方法。实证研究显示，在仅增加0.01%的计算开销下，R&amp;B在五个多样化数据集上的表现匹配甚至超越了最先进的数据混合策略，涵盖了自然语言处理、推理及多模态任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 03:08:19 GMT</pubDate>
</item>
<item>
<title>通过Selective Loss方法提升语言模型对高风险文本的理解能力</title>
<link>https://arxiv.org/abs/2505.03052</link>
<guid>https://arxiv.org/abs/2505.03052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SLUNG方法，使模型能理解高风险文本而不生成它们。</p><br /><br /><p><strong>摘要：</strong> 语言模型开发者通常会过滤掉预训练数据中的高风险内容（如有毒或受版权保护的文本），以防止模型生成类似内容。然而，这种做法限制了模型识别和适当响应有害或敏感内容的能力。本文介绍了一种名为SLUNG（Selective Loss to Understand but Not Generate）的预训练范式，该方法让模型学会理解高风险数据而不学习生成它们。SLUNG不是均匀地应用下一个令牌预测损失，而是有选择地避免激励生成高风险令牌，同时确保它们保留在模型的上下文窗口内。实验表明，SLUNG在提高模型对高风险数据的理解能力方面始终表现出色，而不会增加生成这些内容的风险。总的来说，SLUNG范式使得模型能够从原本会被过滤掉的高风险文本中受益。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 18:24:06 GMT</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 19:09:44 GMT</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:56:06 GMT</pubDate>
</item>
<item>
<title>VITA-Audio：基于多模态预测模块的低延迟语音生成大模型</title>
<link>https://arxiv.org/abs/2505.03739</link>
<guid>https://arxiv.org/abs/2505.03739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端的高效语音生成模型VITA-Audio，大幅降低实时对话中的初始音频生成延迟。</p><br /><br /><p><strong>摘要：</strong> 随着自然人机交互需求的增长，语音系统备受关注，但现有模型在流式生成首个音频标记时存在高延迟问题。为解决这一瓶颈，我们提出了VITA-Audio，这是一种具有快速音素-文本标记生成能力的端到端大型语音模型。通过引入轻量级的多模态跨模态标记预测（MCTP）模块，该模型在一个前向传播过程中可高效生成多个音频标记，不仅加速推理速度，还显著降低了流式场景下的首次音频生成延迟。此外，我们探索了一种四阶段渐进式训练策略，在保证最小语音质量损失的同时实现模型加速。作为首个能够在首次前向传播中生成音频输出的多模态大语言模型，VITA-Audio支持实时对话功能，且完全开源可复现。实验结果显示，VITA-Audio在70亿参数规模下推理速度提升了3至5倍，并在自动语音识别（ASR）、文本转语音（TTS）和口语问答（SQA）等多个基准测试中显著优于同类开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>面向综合理解的AI在足球领域的框架与贡献</title>
<link>https://arxiv.org/abs/2505.03735</link>
<guid>https://arxiv.org/abs/2505.03735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识库、基准测试及多智能体系统的足球全面理解框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有AI在足球领域研究集中在孤立任务的问题，提出了一个全面的足球理解框架。首先，构建了SoccerWiki，这是首个大规模多模态足球知识库，整合了丰富的球员、球队、裁判及场馆等领域的知识。其次，开发了SoccerBench，这是一个包含约10K标准化多模态多选题的基准数据集，涵盖13项不同的理解任务。此外，引入了SoccerAgent，这是一种创新的多智能体系统，通过协作推理分解复杂问题，并利用SoccerWiki的专业知识实现稳健性能。最后，通过对SoccerBench的广泛评估和消融实验，展示了所提出的智能体系统的优越性。所有数据和代码均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>FlexiAct：一种灵活的动作迁移方法</title>
<link>https://arxiv.org/abs/2505.03730</link>
<guid>https://arxiv.org/abs/2505.03730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiAct通过引入RefAdapter和FAE技术，实现跨多样主体和场景的动作迁移。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlexiAct的新方法，用于在视频中将动作从参考视频迁移到任意目标图像上。传统动作定制方法受限于空间结构的严格约束，如布局、骨架和视角一致性，这限制了其在多样化主体和场景中的适用性。FlexiAct突破了这些限制，允许参考视频主体与目标图像之间存在布局、视角和骨骼结构的变化，同时保持身份一致性。为了实现这一目标，我们引入了轻量级的图像条件适配器RefAdapter，它在空间适应性和一致性保存方面表现出色，超过了现有方法在外观一致性和结构灵活性之间的平衡能力。此外，我们观察到去噪过程在不同时间步对运动和外观细节的关注程度不同，因此提出了频率感知动作提取（FAE），它直接在去噪过程中实现动作提取，而无需依赖单独的空间-时间架构。实验表明，我们的方法可以有效地将动作转移到具有多样化布局、骨骼和视角的主体上。我们发布了代码和模型权重以支持进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型在地理空间解释性研究中的新框架</title>
<link>https://arxiv.org/abs/2505.03368</link>
<guid>https://arxiv.org/abs/2505.03368</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，揭示大型语言模型处理地理信息的方式。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自然语言处理领域展现了强大的能力，但在地理知识处理和空间推理方面的内部机制仍不明确。本文建立了一种基于空间分析的新框架，通过探针技术和机械可解释性方法，探讨LLMs如何处理地理信息。研究利用空间自相关技术，证明了从地名特征中可以发现与地理位置相关的空间模式，为理解这些模型的地理信息处理提供了新视角。最后讨论了该框架对地理领域基础模型研究与应用的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03368" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:40:06 GMT</pubDate>
</item>
<item>
<title>绝对零度范式下的自我进化推理模型</title>
<link>https://arxiv.org/abs/2505.03335</link>
<guid>https://arxiv.org/abs/2505.03335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出绝对零度范式，使模型无需外部数据即可通过自拟任务提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励方法（RLVR）被证明可以增强大型语言模型的推理能力，但现有方法仍依赖人工标注的问题和答案。为解决人类监督的长期可持续性问题及未来超级智能系统的潜在局限，我们提出了绝对零度（Absolute Zero）范式，该范式下模型自行设计最大化自身学习进度的任务并从中提升推理能力。作为该范式的实例，绝对零度推理器（AZR）利用代码执行器验证任务和答案，提供统一的可验证奖励来源，实现开放且有基础的学习。尽管完全不依赖外部数据，AZR在编码和数学推理任务上达到了当前最优性能，超越了依赖数万个人工标注样本的传统零样本模型，并且适用于不同规模和类型的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:08:00 GMT</pubDate>
</item>
<item>
<title>UnifiedReward-Think：基于长链条推理的多模态奖励模型</title>
<link>https://arxiv.org/abs/2505.03318</link>
<guid>https://arxiv.org/abs/2505.03318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入长链条推理提升多模态奖励模型的可靠性和准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态奖励模型在提供符合人类偏好的视觉模型奖励信号方面取得了显著进展。然而，当前的奖励模型通常局限于浅层推理过程，导致奖励信号不准确。本文提出了一种名为UnifiedReward-Think的新方法，该方法通过引入显式的长链条推理增强奖励推理的可靠性和鲁棒性，并通过隐式推理能力提高直接响应的准确性。具体而言，我们采用探索驱动的强化微调方法来诱发和激励模型潜在的复杂推理能力，包括利用少量图像生成偏好数据蒸馏GPT-4o的推理过程用于冷启动学习，以及利用大规模统一多模态偏好数据在多种视觉任务中引发推理过程。实验结果表明，该方法在多个视觉奖励任务中表现出色，验证了其优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 04:46:41 GMT</pubDate>
</item>
<item>
<title>InfoVids：重塑演示者与可视化之间的关系</title>
<link>https://arxiv.org/abs/2505.03164</link>
<guid>https://arxiv.org/abs/2505.03164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过InfoVids重新定义演示者与可视化的关系，提升人本体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过InfoVids（基于信息图表的视频）改变传统数据展示中演示者与可视化分离的状况。InfoVids旨在建立演示者与可视化之间更平等的关系，探索布局、形式及交互对观看体验的影响。研究对比了InfoVids与传统2D幻灯片，在9项指标下进行测试，并收集了30名参与者的反馈。分析表明，InfoVids减少了观众注意力分散，将关注点从可视化转移到演示者身上，使数据展示更加互动、自然且吸引人。这一方法为重新思考演示者与可视化动态提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:18:42 GMT</pubDate>
</item>
<item>
<title>RADLADS协议：高效转换Transformer至线性注意力解码模型</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RADLADS协议，将Transformer高效转化为线性注意力解码模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Rapid Attention Distillation to Linear Attention Decoders at Scale（RADLADS）协议，用于将softmax注意力的Transformer高效转化为线性注意力解码模型。该研究还提出了两个RWKV变体架构，并基于Qwen2.5开源模型创建了多种规模（7B、32B、72B）的模型。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原始Transformer。这些模型在标准基准测试中表现出色，所有模型均开源于HuggingFace，其中72B模型受Qwen许可协议约束。本文还提供了模型和训练代码的具体链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03005" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 16:03:28 GMT</pubDate>
</item>
<item>
<title>RetroInfer：一种加速长上下文大语言模型推理的新系统</title>
<link>https://arxiv.org/abs/2505.02922</link>
<guid>https://arxiv.org/abs/2505.02922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RetroInfer系统，通过向量存储和注意力稀疏性优化加速长上下文大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RetroInfer的创新系统，旨在解决大语言模型（LLMs）在高效推理时因上下文长度增加而面临的GPU内存和带宽限制问题。该系统重新定义了键值（KV）缓存为向量存储系统，利用内在的注意力稀疏性来加速长上下文的LLMs推理。RetroInfer的核心是一个波浪索引（wave index），这是一种基于注意力的向量索引，通过三部分注意力近似、有界注意力估计和分段聚类等技术实现对关键令牌的有效且精确检索。此外，系统还配备了波浪缓冲区（wave buffer），用于协调KV缓存放置并重叠GPU和CPU上的计算与数据传输，从而维持高吞吐量。与之前基于稀疏性的方法相比，RetroInfer在不牺牲模型准确性的情况下提供了稳健的性能。实验表明，在GPU内存限制下，相对于完整注意力，RetroInfer可实现高达4.5倍的速度提升；当KV缓存扩展到CPU内存时，相对于稀疏注意力基准，速度提升可达10.5倍，同时保持全注意力级别的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 14:01:17 GMT</pubDate>
</item>
<item>
<title>基于AttenHScore的大模型与小模型协作优化方法</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AttenHScore评估指标提升小模型实时幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大语言模型（LM）与小语言模型协作中的关键挑战——即如何精准定位小模型生成幻觉的时刻。传统优化主要依赖于独立于推理过程的后处理技术，导致高计算成本且效果有限。为此，我们提出了AttenHScore这一实用的调用评估指标，通过累积并传播小模型生成过程中的幻觉现象，动态调整检测阈值实现对大模型的更精确调用。同时，结合不确定性感知的知识重组策略，增强小模型捕获关键信息的能力。实验表明，AttenHScore在多个问答数据集上显著优于基线方法，尤其在处理复杂查询时表现优异，且无需额外的模型训练，具备适应多种Transformer架构LM的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 21:45:56 GMT</pubDate>
</item>
<item>
<title>Qwen3低比特量化性能评估与挑战</title>
<link>https://arxiv.org/abs/2505.02214</link>
<guid>https://arxiv.org/abs/2505.02214</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索Qwen3在低比特量化下的表现，揭示压缩大型语言模型的机遇与难题。</p><br /><br /><p><strong>摘要：</strong> Qwen系列作为领先的开源大型语言模型家族，其最新成员Qwen3展现出卓越的自然语言理解能力。然而，在资源受限环境下高效部署这些模型仍具挑战性。本研究系统评估了五种经典后训练量化技术对Qwen3的影响，量化位宽从1到8位不等，并在多个数据集上测试其有效性。结果显示，Qwen3在中等比特宽度下保持竞争力，但在超低精度下显著影响语言任务性能。研究强调了进一步优化极端量化场景下性能的重要性，并提供了针对Qwen3及其未来版本的量化方法改进建议。项目代码已公开于GitHub及Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02214" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 14:43:44 GMT</pubDate>
</item>
<item>
<title>通过眼球运动自动解码开放性阅读目标的研究</title>
<link>https://arxiv.org/abs/2505.02872</link>
<guid>https://arxiv.org/abs/2505.02872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次探索能否通过眼球运动自动解码开放性阅读目标。</p><br /><br /><p><strong>摘要：</strong> 本研究首次探讨了是否可以通过眼球运动自动解码读者在开放性阅读中的具体目标。为了回答这一问题，我们引入了目标分类和目标重建任务及其评估框架，并利用大规模英文阅读的眼动追踪数据，这些数据涉及数百种特定于文本的信息检索任务。我们开发并比较了几种结合眼球运动和文本的判别型与生成型多模态大语言模型（LLMs），用于目标分类和目标重建。实验结果显示，在两项任务上取得了显著的成功，表明LLMs可以从眼球运动中提取出关于读者文本特定目标的有价值信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:23:48 GMT</pubDate>
</item>
<item>
<title>HoloTime：通过扩散模型实现全景视频到4D场景的重建</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架HoloTime，结合视频扩散模型生成沉浸式4D体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散模型在虚拟现实(VR)和增强现实(AR)技术中的应用潜力，特别是针对需要场景级4D资产的用户体验。现有的扩散模型主要集中在静态3D场景或物体级别的动态建模上，限制了它们提供真正沉浸式体验的能力。为了解决这一问题，我们提出了HoloTime框架，它集成了视频扩散模型，可以从单一提示或参考图像生成全景视频，并结合360度4D场景重建方法将生成的全景视频无缝转换为4D资产，从而为用户提供完全沉浸式的4D体验。具体来说，为了控制视频扩散模型以生成高保真的全景视频，我们引入了360World数据集，这是首个适合下游4D场景重建任务的全景视频综合集合。基于此精心策划的数据集，我们提出了Panoramic Animator，这是一种两阶段的图像到视频扩散模型，可以将全景图像转换为高质量的全景视频。随后，我们介绍了Panoramic Space-Time Reconstruction，它利用时空深度估计方法将生成的全景视频转换为4D点云，使优化整体4D高斯点撒表示成为可能，从而重建空间和时间一致的4D场景。通过与现有方法进行比较分析，验证了我们的方法在全景视频生成和4D场景重建方面的优越性，展示了其在创建更具吸引力和真实感的沉浸式环境方面的能力，从而提升了VR和AR应用程序中的用户体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:55:28 GMT</pubDate>
</item>
<item>
<title>Auto-SLURP：用于评估大型语言模型驱动多智能体框架的新基准数据集</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Auto-SLURP，一个针对智能个人助手的多智能体框架评估基准数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，由大型语言模型（LLMs）驱动的多智能体框架取得了显著进展。然而，缺乏专门用于评估这些框架性能的基准数据集。为解决这一问题，本文提出了Auto-SLURP，这是一个扩展自SLURP数据集的新基准，通过重新标注数据并集成模拟服务器和外部服务，支持从语言理解到任务执行再到响应生成的端到端评估。实验表明，当前最先进的框架在Auto-SLURP上面临严峻挑战，表明真正可靠且智能的多智能体个人助手仍需进一步研究。该数据集及相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 10:17:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在图结构数据中的注意力机制研究</title>
<link>https://arxiv.org/abs/2505.02130</link>
<guid>https://arxiv.org/abs/2505.02130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型在处理图结构数据时的注意力机制表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 本文从注意力机制的角度出发，对大型语言模型（LLMs）处理图结构数据的方式进行了实证研究。尽管LLMs能够识别图数据并捕获文本节点交互，但在建模节点间关系上存在局限性。此外，LLMs的注意力分布未能适应图拓扑结构的特点，而中间状态注意力窗口在训练和推理阶段均表现出更好的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 10:40:31 GMT</pubDate>
</item>
<item>
<title>基于Motion-enhanced Event Tensor的RGB-事件模态融合及其应用</title>
<link>https://arxiv.org/abs/2505.01548</link>
<guid>https://arxiv.org/abs/2505.01548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的事件表示方法MET，解决RGB-事件融合中的三类对齐问题并提升语义分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对RGB与事件相机模态融合中存在的时序、空间及模态错配问题，提出了Motion-enhanced Event Tensor (MET) 新型事件表示方法。MET通过利用密集光流和事件时间特征将稀疏事件体素转化为密集且时序一致的形式。此外，引入Frequency-aware Bidirectional Flow Aggregation Module (BFAM) 和 Temporal Fusion Module (TFM)，分别缓解模态错配并解决时空错配问题。实验表明，在两个大规模数据集上，所提框架显著优于现有最先进的RGB-事件语义分割方法。研究代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 15:19:58 GMT</pubDate>
</item>
<item>
<title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.02005</link>
<guid>https://arxiv.org/abs/2505.02005</guid>
<content:encoded><![CDATA[
Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (&gt;6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.
]]></content:encoded>
<pubDate>Sun, 04 May 2025 02:25:14 GMT</pubDate>
</item>
<item>
<title>MUSAR：仅需单领域训练数据的多领域定制框架</title>
<link>https://arxiv.org/abs/2505.02823</link>
<guid>https://arxiv.org/abs/2505.02823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅需单领域数据即可实现多领域自定义的新框架MUSAR。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有跨领域定制方法面临的两大挑战——多领域训练数据获取困难和属性纠缠问题，提出了名为MUSAR的简单而有效的框架。MUSAR通过引入去偏双联学习解决数据限制问题，利用静态注意力路由和双分支LoRA校正分布偏差；同时借助动态注意力路由机制消除领域间的纠缠，从而实现多领域的解耦表示。实验表明，尽管仅使用单领域数据，MUSAR在图像质量、领域一致性及交互自然性方面均优于现有方法，甚至超越部分基于多领域数据训练的方法。这一成果为多领域定制提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:50:24 GMT</pubDate>
</item>
<item>
<title>ReplaceMe：无需训练的Transformer块剪枝方法</title>
<link>https://arxiv.org/abs/2505.02819</link>
<guid>https://arxiv.org/abs/2505.02819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种无需训练即可有效剪枝Transformer块的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReplaceMe的新型训练-free深度剪枝方法，该方法通过将Transformer块替换为线性操作，同时保持高精度性能，适用于低压缩比场景。与传统需要额外训练或微调的剪枝方法不同，ReplaceMe仅需一个小规模校准数据集来估计线性变换，从而逼近被剪枝的块。这一线性映射可无缝合并到剩余的Transformer块中，无需添加额外网络参数。实验表明，ReplaceMe在多个大型语言模型上的表现优于其他训练-free方法，并与涉及大量重新训练/微调及架构修改的最新剪枝方法具有竞争力。在开放基准测试中，ReplaceMe实现了高达25%的剪枝率，而性能保留约90%，且没有额外的计算开销。此外，我们开源了一个包含ReplaceMe及其相关技术的库。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:42 GMT</pubDate>
</item>
<item>
<title>Voila：迈向自然人机交互的大型语音语言基础模型</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voila是一种新型全双工语音语言模型，实现低延迟、情感丰富的对话交互。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Voila，一种旨在实现自主、实时且情感表达丰富的人机语音交互的大型语音语言基础模型。不同于传统的流水线系统，Voila采用端到端架构，支持毫秒级响应时间，同时保留丰富的语音特征如音调、节奏和情感。它通过层次化多尺度Transformer融合了大语言模型的推理能力与强大的声学建模能力，允许用户通过文本指令定义语音角色特性。此外，Voila还支持百万级预制语音并能高效定制新语音，仅需10秒音频样本即可完成。除了对话功能，Voila还能轻松适应自动语音识别（ASR）、文本转语音（TTS）及多语言语音翻译等多种应用场景。该模型已完全开源，助力开放研究并推动下一代人机交互发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:05:01 GMT</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:26:00 GMT</pubDate>
</item>
<item>
<title>引入推理能力的生成型奖励模型提升奖励建模性能</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出推理奖励模型（ReasRM），显著提高大语言模型对人类偏好的对齐能力。</p><br /><br /><p><strong>摘要：</strong> 奖励建模（Reward Modeling）对于通过强化学习从人类反馈（RLHF）来对齐大型语言模型（LLMs）至关重要。然而，现有的奖励模型要么提供不透明的标量评分，要么直接预测首选答案，导致缺乏解释性。受长链推理（CoT）方法的启发，本研究提出了推理奖励模型（ReasRM），将奖励建模视为一个推理任务。ReasRM通过两个阶段进行训练：高质量推理链蒸馏和基于可验证奖励的强化学习。实验结果显示，所提出的RM-R1模型在多个基准测试中达到或接近当前最佳性能，甚至超越了更大的开源权重模型（如Llama3.1-405B）和专有模型（如GPT-4o）。此外，我们分析了ReasRM成功训练的关键因素，并开源了六个ReasRM模型及相关代码和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:11:12 GMT</pubDate>
</item>
<item>
<title>Muon优化器在计算效率与数据效能上的改进</title>
<link>https://arxiv.org/abs/2505.02222</link>
<guid>https://arxiv.org/abs/2505.02222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Muon优化器在计算时间和AdamW之间扩展帕累托前沿，提高大批次训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 本文展示了Muon作为二阶优化器的最简单实例，如何在计算时间与AdamW的权衡中扩展帕累托前沿。研究发现，Muon在大批次训练中比AdamW更具数据效率，且保持了计算效率，从而实现更经济的训练。此外，Muon与最大更新参数化(muP)的结合用于高效超参数转移，提出了一种考虑所有muP误差来源的简单望远镜算法，仅引入适度的资源开销。通过模型大小高达40亿参数的广泛实验及对数据分布和架构的消融研究验证了这些发现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 15:14:43 GMT</pubDate>
</item>
<item>
<title>基于演示交互强化学习的数据增强与技能获取</title>
<link>https://arxiv.org/abs/2505.02094</link>
<guid>https://arxiv.org/abs/2505.02094</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解决强化学习中演示噪声与覆盖不足问题的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文解决了强化学习从交互演示中获取技能的一个基础挑战：演示噪声与覆盖限制。现有数据收集方法虽有价值，但常产生稀疏、不连贯且嘈杂的轨迹。我们通过引入两种数据增强技术——Stitched Trajectory Graph (STG) 和 State Transition Field (STF)，有效连接演示技能并构建状态转换场。同时，开发了自适应轨迹采样策略和记忆依赖技能学习机制。实验表明，在多种交互任务上，该方法显著提升了收敛稳定性、泛化能力和恢复鲁棒性，超越当前最先进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02094" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:00:29 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理引擎系统性评估与未来展望</title>
<link>https://arxiv.org/abs/2505.01658</link>
<guid>https://arxiv.org/abs/2505.01658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估25个开源及商业LLM推理引擎，探讨优化方法与适用场景。</p><br /><br /><p><strong>摘要：</strong> 大规模语言模型（LLMs）在聊天机器人、代码生成器和搜索引擎中广泛应用，但复杂推理等任务显著增加推理成本。尽管已有并行化、压缩和缓存等多种优化方法，但服务需求多样化使得方法选择困难。本文对25个开源和商业推理引擎进行全面评估，涵盖易用性、部署便捷性、通用支持、可扩展性和吞吐量/延迟敏感计算的适用性。通过分析各引擎支持的优化技术，揭示其设计目标，并评估开源引擎的生态系统成熟度及商业解决方案的成本性能策略。此外，本文还提出未来研究方向，如支持复杂服务、兼容多种硬件及增强安全性，并提供公共存储库追踪领域进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 22:47:43 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型的目标遗忘评估基准研究</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多模态遗忘学习基准UnLOK-VQA及框架，评估删除特定知识的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模数据训练的语言模型可能无意中获取敏感信息的风险，尤其是多模态模型整合文本和图像信息时风险更高。为了应对这一挑战，研究者引入了一个名为UnLOK-VQA的多模态遗忘学习基准，并设计了一套攻击与防御框架，用于评估如何有效删除多模态知识。该基准通过自动化管道扩展视觉问答数据集，同时进行人工筛选，确保高质量。研究对六种防御目标进行了测试，发现多模态攻击比单一模态更具优势，且最有效的防御方法是从模型状态中移除答案信息。此外，更大的模型表现出更强的编辑后鲁棒性，表明规模有助于提高安全性。UnLOK-VQA为多模态大型语言模型的安全性提升提供了严格的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 21:54:00 GMT</pubDate>
</item>
<item>
<title>通过Grokking增强Transformer模型的多步事实推理能力</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次将Grokking应用于现实世界事实数据，显著提升多跳推理准确性。</p><br /><br /><p><strong>摘要：</strong> 尽管Transformer模型在众多自然语言处理任务中取得了显著成功，但在缺乏实际知识的情况下进行多步事实推理时仍存在明显不足。最近关于Grokking的研究表明，神经网络可以通过检测潜在逻辑模式实现从记忆到完全泛化的转变，但这些研究主要集中在小型合成任务上。本文首次将Grokking扩展到现实世界的事实数据，并通过精心设计的合成数据扩充现有知识图谱，提高推断事实与原子事实的比例phi_r，从而克服数据稀疏性问题。令人惊讶的是，即使合成数据存在事实错误，也能加强新兴推理电路的形成，而非降低准确性。在多跳推理基准测试中，我们的方法在2WikiMultiHopQA上的准确率达到了95%-100%，大幅优于现有基线模型。此外，我们深入分析了phi_r增加如何促进Transformer内部形成泛化电路。研究结果表明，基于Grokking的数据增强可以解锁隐式的多跳推理能力，为大规模语言模型提供更健壮且可解释的事实推理奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:33:29 GMT</pubDate>
</item>
<item>
<title>ARTIST：引入具身推理与工具集成的大语言模型框架</title>
<link>https://arxiv.org/abs/2505.01441</link>
<guid>https://arxiv.org/abs/2505.01441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARTIST框架通过具身推理和工具集成显著提升大语言模型问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ARTIST（具身推理与工具集成的自我改进Transformer），这是一种将具身推理、强化学习和工具集成统一起来的框架，旨在增强大型语言模型（LLMs）的动态、多步推理能力以及适应性决策能力。ARTIST允许模型自主决定何时、如何以及选择哪个工具进行调用，并通过基于结果的强化学习策略优化工具使用和环境交互。实验表明，ARTIST在数学推理和多轮函数调用基准测试中超越现有最先进的基线模型，特别是在最具挑战性的任务上取得了高达22%的绝对性能提升。深入研究和指标分析显示，具身强化学习训练促进了更深层次的推理、更有效的工具使用和更高质量的解决方案。这些结果确立了具身强化学习与工具集成作为LLMs鲁棒、可解释和泛化能力强的问题解决新方向的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 06:42:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态奖励模型优化研究</title>
<link>https://arxiv.org/abs/2505.02835</link>
<guid>https://arxiv.org/abs/2505.02835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出StableReinforce算法，通过改进现有强化学习方法显著提升多模态奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 多模态奖励模型（MRMs）对多模态大型语言模型（MLLMs）的表现至关重要，但长期推理能力的有效性及激活方式尚未充分探索。本文将奖励建模问题重新定义为基于规则的强化学习任务，并提出StableReinforce算法，通过改进训练损失、优势估计策略和奖励设计，解决现有强化学习算法在奖励建模中的不稳定性问题。实验中，我们从多个数据集中收集了20万份偏好数据用于模型训练，所提出的R1-Reward奖励模型在VL Reward-Bench和Multimodal Reward Bench上分别实现了8.4%和14.3%的性能提升。此外，随着推理计算资源的增加，R1-Reward的性能进一步增强，证明了强化学习算法在优化多模态奖励模型方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>FormalMATH：大规模数学定理形式化基准及其挑战</title>
<link>https://arxiv.org/abs/2505.02735</link>
<guid>https://arxiv.org/abs/2505.02735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FormalMATH提供了一个涵盖多领域的大型数学形式化问题集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为FormalMATH的大规模数学形式化基准，包含5560个从高中到大学水平的已验证数学问题，涉及代数、微积分等多个领域。为了提高自动形式化的效率，研究提出了结合大语言模型（LLMs）和人类协作的新型自动化流水线，该方法显著降低了人工标注成本。然而，当前最先进的基于LLMs的定理证明器表现有限，在实际采样预算下成功率仅为16.46%，且存在明显的领域偏向性。进一步分析发现，自然语言解题指导在链式推理场景中反而可能引入噪声而非清晰度，这对形式化数学推理提出了新的见解。FormalMATH有望成为评估形式化数学推理能力的重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:37:00 GMT</pubDate>
</item>
<item>
<title>LLaMA-Omni 2：基于大语言模型的高质量实时语音交互</title>
<link>https://arxiv.org/abs/2505.02625</link>
<guid>https://arxiv.org/abs/2505.02625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaMA-Omni 2实现高质量实时语音交互，参数规模从0.5B到14B。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LLaMA-Omni 2系列语音语言模型（SpeechLMs），该系列模型基于Qwen2.5系列，整合了语音编码器和自回归流式语音解码器，参数规模涵盖0.5B到14B。尽管仅使用了20万个多轮对话样本进行训练，LLaMA-Omni 2在多个语音问答和指令跟随基准测试中表现出色，超过了如GLM-4-Voice等之前基于大规模语音数据训练的先进模型。通过将大语言模型的能力扩展到语音领域，LLaMA-Omni 2展示了下一代人机交互中智能语音交互的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 08:53:09 GMT</pubDate>
</item>
<item>
<title>Ming-Lite-Uni：开源多模态框架实现文本到图像生成及指令驱动图像编辑</title>
<link>https://arxiv.org/abs/2505.02471</link>
<guid>https://arxiv.org/abs/2505.02471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Lite-Uni开源框架结合统一视觉生成器和多模态自回归模型，实现文本到图像生成及图像编辑。</p><br /><br /><p><strong>摘要：</strong> Ming-Lite-Uni是一个开源的多模态框架，其核心特性在于新设计的统一视觉生成器和原生多模态自回归模型，旨在整合视觉与语言处理。该框架提供了集成MetaQueries和M2-omni框架的开放源代码实现，同时引入了多尺度可学习标记和多尺度表示对齐策略。通过固定MLLM和可学习扩散模型的结合，Ming-Lite-Uni使原生多模态AR模型不仅能够进行文本到图像生成，还能执行基于指令的图像编辑任务，从而扩展了其能力范围。实验结果表明，Ming-Lite-Uni具有卓越的表现，并展示了其交互过程的流畅性。所有代码和模型权重均公开，以促进社区进一步探索。值得注意的是，这项工作与最近的多模态AI里程碑（如ChatGPT-4o的本机图像生成功能更新）相呼应，强调了像Ming-Lite-Uni这样的统一模型在迈向通用人工智能（AGI）道路上的重要性。目前，Ming-Lite-Uni处于Alpha阶段，未来将进一步优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 04:56:12 GMT</pubDate>
</item>
<item>
<title>基于对比指令优化的图像编辑方法</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像编辑指令构造方法，显著提升编辑效果。</p><br /><br /><p><strong>摘要：</strong> 现有图像编辑数据集因自动化构建方法导致监督信号噪声问题，限制了模型性能。本文提出一种创新解决方案，通过修正编辑指令与图像对的匹配度并引入对比指令增强有效性，无需依赖视觉语言模型或预训练任务。实验表明，该方法在多个基准测试中显著优于现有技术，在Real-Edit基准上比SmartEdit提升9.19%，且所需训练数据和模型规模大幅减少。研究发现编辑模型在推理步骤中有特定生成属性，据此定义统一指导规则，同时利用三元组损失构建对比监督信号进一步提高监督效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 01:19:40 GMT</pubDate>
</item>
<item>
<title>自适应模式学习提升社会智能模拟中的动态推理能力</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应模式学习方法，显著提高社会智能模拟中的推理深度和效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前社会智能模拟中语言代理缺乏动态调整推理深度的能力这一问题，提出了自适应模式学习（Adaptive Mode Learning, AML），该方法根据实时情境从四种思考模式中进行选择（直觉反应到深思熟虑）。核心创新的自适应模式策略优化（AMPO）算法实现了多粒度思考模式设计、上下文感知模式切换及通过深度自适应处理实现令牌高效推理。实验表明，AML在社会智能任务上的表现比现有最先进方法高出15.6%，且推理链条缩短32.8%，优于固定深度的GRPO方法7.0%。这些成果证明了AMPO在实现更人性化适应性推理方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02156" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 11:39:58 GMT</pubDate>
</item>
<item>
<title>TEMPURA框架提升视频因果事件关系理解</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TEMPURA框架，通过因果推理与细粒度时间分割提高视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视觉语言模型在理解因果事件关系及视频的时间定位方面仍面临挑战。为解决此问题，本文提出TEMPURA框架，该框架分为两个阶段：首先利用事件掩码预测推理重构缺失事件并生成因果解释；其次学习视频分割与密集描述以分解视频为非重叠事件并提供精确描述。TEMPURA在VER数据集上进行训练，该数据集包含100万训练实例及50万带有时间对齐事件描述和结构化推理步骤的视频。实验表明，TEMPURA在时间定位和亮点检测基准测试中优于基线模型，证明结合因果推理与细粒度时间分割可显著提升视频理解效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01583" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 17:00:17 GMT</pubDate>
</item>
<item>
<title>Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.01043</link>
<guid>https://arxiv.org/abs/2505.01043</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 02:33:25 GMT</pubDate>
</item>
<item>
<title>基于多层记忆与一致性引导的迭代图像编辑框架</title>
<link>https://arxiv.org/abs/2505.01079</link>
<guid>https://arxiv.org/abs/2505.01079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种支持多对象修改并保持上下文关系的迭代图像编辑方法。</p><br /><br /><p><strong>摘要：</strong> 当前大多数图像编辑技术专注于单一对象修改，难以应对需要多次顺序编辑的复杂场景。针对这一挑战，本文提出了两项创新方案：引入粗糙掩码以自然融合新元素同时保留现有内容，以及支持多轮编辑的一致性维护机制。通过引入分层记忆存储先前编辑的潜在表示和提示嵌入，结合背景一致性引导和跨注意力的多查询解耦技术，该框架能够在复杂的多对象编辑任务中保持高质量的结果。此外，我们构建了一个包含语义对齐指标和交互式编辑场景的新基准数据集，实验结果证明了该方法在减少用户操作负担的同时显著提升了迭代图像编辑的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 03:36:49 GMT</pubDate>
</item>
<item>
<title>Llama-Nemotron系列模型：开放且高效的异构推理模型家族</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-Nemotron提供三种规模模型，兼具高效推理和卓越推理能力。</p><br /><br /><p><strong>摘要：</strong> Llama-Nemotron是一组开放的异构推理模型系列，包含Nano（8B）、Super（49B）和Ultra（253B）三种版本，其推理效率和内存利用率优于当前最先进的模型如DeepSeek-R1。该系列模型通过神经架构搜索、知识蒸馏和持续预训练优化，并经过推理聚焦的后训练阶段，包括监督微调和大规模强化学习。此外，Llama-Nemotron是首个支持动态推理切换的开源模型，用户可在标准聊天模式和推理模式间切换。为了促进开放研究，我们提供了完整的后训练数据集和训练代码库，并在NVIDIA开放模型许可协议下发布模型。这些资源旨在推动模型开发并支持学术界和企业用户。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 21:35:35 GMT</pubDate>
</item>
<item>
<title>基于逆映射学习的大型语言模型评估方法</title>
<link>https://arxiv.org/abs/2504.21117</link>
<guid>https://arxiv.org/abs/2504.21117</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种逆映射学习方法以生成高效的LLM评估提示。</p><br /><br /><p><strong>摘要：</strong> 自然语言生成系统的评估因其输出多样性而具有挑战性，尽管人类评估是标准，但存在不一致性、缺乏标准化及人口统计学偏差等问题，限制了可重复性。基于大型语言模型（LLM）的评估提供了可扩展的替代方案，但对提示设计极为敏感，微小变化可能导致显著差异。本文提出了一种逆映射学习方法，通过从模型输出反向映射到输入指令来学习有效的逆映射，从而实现自动高效生成针对特定模型的评估提示。该方法仅需单一样本即可完成，无需耗时的人工提示工程，提升了效率与鲁棒性，为更稳健、高效的LLM评估开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21117" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 14:56:12 GMT</pubDate>
</item>
<item>
<title>基于图神经网络的信号时态逻辑学习框架TeLoGraF</title>
<link>https://arxiv.org/abs/2505.00562</link>
<guid>https://arxiv.org/abs/2505.00562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合图编码与流匹配的信号时态逻辑学习方法，显著提升复杂任务求解效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对信号时态逻辑(STL)在实际应用中的局限性，设计了一种名为TeLoGraF的新框架，通过图神经网络(GNN)编码器和流匹配技术实现对一般STL规格的学习。研究团队收集了20万组配对演示数据，涵盖四种常见STL模板，并在五个模拟环境中进行测试，包括二维动态模型到复杂的七自由度机械臂及四足机器人导航。实验结果显示，TeLoGraF在STL满足率上优于其他基线算法，推理速度比经典STL规划算法快10-100倍，且适用于任意系统动力学。此外，该方法展现出解决复杂STL问题的能力，并具备处理分布外STL规格的鲁棒性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:40:07 GMT</pubDate>
</item>
<item>
<title>生成式人工智能研究焦点转移及其潜在风险</title>
<link>https://arxiv.org/abs/2505.00174</link>
<guid>https://arxiv.org/abs/2505.00174</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">企业AI研究重心转向模型对齐与测试，部署阶段问题关注减少。</p><br /><br /><p><strong>摘要：</strong> 本研究基于2020年至2025年间9,439篇生成式人工智能论文中的1,178篇安全与可靠性相关文献，对比了领先AI公司（如Anthropic、Google DeepMind等）与顶尖大学（如麻省理工学院、斯坦福大学等）的研究产出。结果显示，企业AI研究正越来越多地聚焦于模型部署前的对齐与评估领域，而对部署阶段的重要议题如模型偏差的关注度有所下降。此外，在高风险应用领域（如医疗、金融、虚假信息传播等），存在显著的研究空白。若缺乏对实际部署AI系统的深入观察，企业集中化趋势可能加剧这些知识鸿沟。因此，建议扩大外部研究人员对部署数据的访问权限，并建立系统化的市场中AI行为观测机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00174" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 16:44:42 GMT</pubDate>
</item>
<item>
<title>X-Cross：一种高效的跨域推荐模型</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Cross通过低秩适配器实现跨域推荐，性能媲美传统方法但参数量减少25%。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“X-Cross”的新型跨域顺序推荐模型，旨在无需大规模重新训练的情况下适应新领域的产品推荐。该模型整合多个特定领域的语言模型，利用低秩适配器（LoRA）进行微调。X-Cross通过动态优化各源语言模型表示，将知识层间传播，从而在保持领域特异性的同时实现跨域适应性。基于Amazon的数据集测试显示，X-Cross在性能上与全参数LoRA相当，但仅需25%的额外参数。在跨域任务中，如从玩具领域迁移到工具、电子或运动领域时，X-Cross表现出色，所需微调数据减少了50%-75%，且在准确性上优于其他跨域基准模型。总体而言，X-Cross为数据受限环境提供了高效、可扩展的跨域推荐解决方案，显著降低了计算开销。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:33:20 GMT</pubDate>
</item>
<item>
<title>PixelHacker：基于扩散模型的图像修复新范式</title>
<link>https://arxiv.org/abs/2504.20438</link>
<guid>https://arxiv.org/abs/2504.20438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的图像修复方法PixelHacker，显著提升复杂结构和语义的一致性。</p><br /><br /><p><strong>摘要：</strong> 图像修复是图像编辑与生成领域的重要研究方向。现有最先进的方法虽引入新颖的注意力机制和轻量化架构，但在处理复杂结构（如纹理、形状、空间关系）和语义一致性（如颜色一致性、对象恢复及逻辑正确性）时仍存在不足，导致伪影和不恰当生成。为解决这一问题，我们设计了一种简单而有效的修复范式——潜在类别引导，并提出了名为PixelHacker的扩散模型。该方法通过构建包含1400万图像-掩码对的大规模数据集，分别编码前景和背景表示，并将其特征注入去噪过程中，最终实现优异的修复效果。实验表明，PixelHacker在Places2、CelebA-HQ和FFHQ等数据集上全面超越现有最先进方法，在结构和语义一致性方面表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 01:28:36 GMT</pubDate>
</item>
<item>
<title>Context Organizer (CORG): 处理跨文档知识关系的新框架</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架CORG，有效处理跨文档复杂知识关系并提升性能。</p><br /><br /><p><strong>摘要：</strong> 在现实世界的语料库中，知识经常在文档间重复出现，但由于命名模糊、过时信息或错误等原因，常常存在不一致性，导致上下文之间形成复杂的相互关系。以往研究表明，语言模型难以应对这些复杂性，通常只能单独关注单一因素。我们把这种关系分为分散、模糊、反事实和重复四类。分析表明，没有单一方法可以同时解决所有这些关系。为此，我们提出了Context Organizer（CORG），该框架将多个上下文组织成独立处理的组，从而让模型高效找到所有相关答案并实现去模糊化。CORG由图构造器、重排序器和聚合器三个关键组件组成。实验结果显示，CORG在性能和效率上均优于现有分组方法，并达到与计算密集型单上下文方法相当的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00023" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 22:40:48 GMT</pubDate>
</item>
<item>
<title>基于离线模拟框架的软件特定技能集生成方法</title>
<link>https://arxiv.org/abs/2504.20406</link>
<guid>https://arxiv.org/abs/2504.20406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大型语言模型生成软件特定技能集的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种结合图神经网络的离线模拟框架，用于生成经过验证的脚本集合，从而解决传统实时代码生成中存在的问题。该框架通过任务创建和技能生成两个组件实现，其中任务创建采用自顶向下和自底向上的方法，而技能生成则利用执行反馈进行优化和验证。实验表明，在Adobe Illustrator中使用该框架可显著提高自动化成功率，减少响应时间和运行时标记成本。这是首次将软件脚本接口作为大型语言模型系统测试平台的尝试，为满足专业软件领域用户需求提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:03:37 GMT</pubDate>
</item>
<item>
<title>空间语音翻译技术：让听觉空间语言无缝转换</title>
<link>https://arxiv.org/abs/2504.18715</link>
<guid>https://arxiv.org/abs/2504.18715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型听戴设备技术，实现实时空间语音翻译并保持方向和音质。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为空间语音翻译的新概念，旨在通过智能听戴设备将佩戴者周围环境中的语音实时翻译成母语，同时保持各发言人的声音方向和独特音色。该技术解决了盲源分离、定位、实时情感翻译及双耳渲染等多个技术难题，并在苹果M2芯片上实现了实时推理。实验表明，尽管存在干扰，我们的模型在BLEU评分上达到22.01，优于现有模型。用户研究进一步验证了系统在真实世界混响环境中的有效性。这一成果标志着将空间感知融入语音翻译领域的第一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 17:58:56 GMT</pubDate>
</item>
<item>
<title>基于深度学习的城市多目标多摄像头车辆跟踪框架</title>
<link>https://arxiv.org/abs/2505.00534</link>
<guid>https://arxiv.org/abs/2505.00534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种高效的深度学习框架解决城市交通多摄像头下的车辆跟踪问题。</p><br /><br /><p><strong>摘要：</strong> 随着智能交通系统中网络摄像头数量的增加，视觉传感器在交通监控、管理和优化中的作用愈发重要。然而，在大规模城市交通场景中实现非重叠摄像头间的车辆目标跟踪与匹配面临诸多挑战，如车辆属性多样性、遮挡、光照变化及阴影等。本文提出了一种名为MO-MCT的高效且经济的深度学习框架，该框架结合Mask R-CNN进行目标检测，利用迁移学习实现跨摄像头车辆再识别，并通过适当的损失函数和距离度量应对各种复杂情况。最终方案采用ResNet-152进行特征提取，并结合Deep SORT算法进行车辆跟踪。该框架在AI City Challenge第5赛道的数据集上进行了评估，涵盖46路摄像头视频流，取得了优秀的性能表现，IDF1得分为0.8289，精确率和召回率分别为0.9026和0.8527。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:00:25 GMT</pubDate>
</item>
<item>
<title>通过自我生成示例提升大语言模型的序列决策性能</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过积累成功经验自动生成示例可显著提高大语言模型在决策任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLM）代理通过学习自身在类似任务上的成功经验来自我提升性能的可能性。与依赖特定任务的知识工程方法不同，我们专注于构建和完善一个由自身生成的示例数据库。实验表明，简单累积训练任务中的成功轨迹即可显著提升三个基准测试的表现：ALFWorld（从73%提升到89%）、Wordcraft（从55%提升到64%）以及InterCode-SQL（从75%提升到79%），相当于初始代理在每项任务允许两到三次尝试时的表现。进一步地，我们引入了两种扩展策略：一是基于种群训练的数据库级选择，二是根据实际效用对单个轨迹进行示例级选择。这些改进使ALFWorld的性能达到了91%，接近采用特定任务组件和提示符的复杂方法。我们的研究证明，自动构建轨迹数据库是一种有吸引力的替代方案，可以避免劳动密集型的知识工程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00234" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 20:48:12 GMT</pubDate>
</item>
<item>
<title>强化学习增强大型语言模型在高功率火箭设计中的应用研究</title>
<link>https://arxiv.org/abs/2504.19394</link>
<guid>https://arxiv.org/abs/2504.19394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升的LLMs在高功率火箭设计优化中超越人类专家。</p><br /><br /><p><strong>摘要：</strong> 本文通过RocketBench基准测试评估大型语言模型（LLMs）在高功率火箭设计中的能力，涉及目标高度优化及精确着陆挑战两个任务。尽管最先进的LLMs展现了强大的基础工程知识，但在结合仿真结果迭代设计时表现不佳，最终性能低于人类水平。然而，通过强化学习增强后，一款7B参数模型不仅优于顶级基础模型，还超过了人类专家的表现。这一研究表明，强化学习训练的LLMs可成为复杂工程优化的有效工具，有望推动工程领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 19:59:39 GMT</pubDate>
</item>
<item>
<title>基于双层推理过程的文本到图像生成模型T2I-R1</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的推理增强型文本到图像生成模型T2I-R1。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为T2I-R1的新模型，该模型通过强化学习和双层链式思维推理过程改进了文本到图像的生成能力。T2I-R1在两个生成阶段引入了两种链式思维策略：语义级用于高级提示规划，令牌级用于低级像素处理。此外，我们提出了BiCoT-GRPO方法，以集成奖励优化这两个推理层级。实验结果显示，在T2I-CompBench和WISE基准测试中，T2I-R1分别提升了13%和19%，甚至超越了最先进的FLUX模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于两阶段框架提升大型语言模型数学解题批评能力的研究</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段框架以增强大型语言模型在数学问题中的批评能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，如何提供准确且可扩展的反馈成为了一个紧迫的问题。本研究专注于提高LLMs在数学解题中的批评能力，现有的批评模型对每一步的批评过于浅显，导致判断准确性低且难以提供足够的反馈。为解决这一问题，我们提出了一个新颖有效的两阶段框架。第一阶段利用Qwen2.5-72B-Instruct生成4500份长篇批评作为监督微调的种子数据；第二阶段通过强化学习进一步优化模型，使用人类标注的数据或通过蒙特卡洛采样获得的自动标注数据。最终开发出的批评模型不仅在多种错误识别基准上显著优于现有模型，还通过更详细的反馈帮助生成器修正错误。此外，该模型在数学解题批评任务中表现出色，为LLMs的自动化监督提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:03:17 GMT</pubDate>
</item>
<item>
<title>KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution</title>
<link>https://arxiv.org/abs/2505.00497</link>
<guid>https://arxiv.org/abs/2505.00497</guid>
<content:encoded><![CDATA[
Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 08:56:17 GMT</pubDate>
</item>
<item>
<title>交互生成视频技术综述及其在多领域的应用与挑战</title>
<link>https://arxiv.org/abs/2504.21853</link>
<guid>https://arxiv.org/abs/2504.21853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">定义交互生成视频(IGV)，并探讨其在游戏、具身AI和自动驾驶中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文首次明确交互生成视频(IGV)的概念，即通过生成技术和互动特性创造高质量视频内容，支持用户通过控制信号和反馈进行互动。IGV已在多个领域崭露头角，包括允许虚拟世界无限探索的游戏、提供物理感知环境合成的具身AI训练场景，以及为自动驾驶提供闭环模拟测试的系统。为了指导未来研究，我们提出了一套全面的框架，将理想中的IGV系统分解为五个核心模块：生成、控制、记忆、动力学和智能。同时，针对实现这些模块的技术挑战进行了系统分析，如实现实时生成、开放域控制、长期一致性维护、精确物理模拟及因果推理集成等。这项工作旨在推动IGV技术向更复杂和实用化的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 10:01:45 GMT</pubDate>
</item>
<item>
<title>TF1-EN-3M：大规模开放道德故事数据集的开创性成果</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次推出包含三百万英文寓言的大规模结构化数据集TF1-EN-3M。</p><br /><br /><p><strong>摘要：</strong> 道德故事长期以来被用于传递价值观，但现代自然语言处理领域缺乏结合连贯叙事与明确伦理教训的大型结构化语料库。为填补这一空白，本文推出了TF1-EN-3M，这是一个由指令微调模型生成的首个人类可读开放数据集，包含三百万英语寓言，每个故事均遵循特定的六槽结构（角色 -> 特质 -> 背景 -> 冲突 -> 解决方案 -> 道德）。通过混合评估管道，该数据集的质量得到了全面验证，其中基于Llama-3的8B参数变体表现最佳，可在消费级GPU上高效生成高质量故事。此数据集及其相关代码和元数据已开源，为研究指令跟随、叙事智能、价值对齐及儿童友好型教育AI开辟了新路径，证明大规模道德叙事不再依赖专有巨型模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:15:28 GMT</pubDate>
</item>
<item>
<title>MediAug：医学影像数据增强统一评估框架</title>
<link>https://arxiv.org/abs/2504.18983</link>
<guid>https://arxiv.org/abs/2504.18983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MediAug框架，综合评估多种混合数据增强方法在医学影像分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学影像数据增强面临的领域差距大及单一任务研究局限性问题，提出了MediAug，一个集成六种基于混合的数据增强技术的统一评估框架。该框架适用于脑肿瘤MRI和眼病眼底图像数据集，并结合卷积神经网络（ResNet-50）和Transformer架构（ViT-B）。实验结果显示，MixUp在ResNet-50上显著提升了脑肿瘤分类精度至79.19%，而SnapMix在ViT-B上达到99.44%；YOCO在ResNet-50上的眼疾分类准确率达到91.60%，CutMix在ViT-B上则提升至97.94%。MediAug为医学影像数据增强提供了全面且可复现的基准工具，代码将在GitHub公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 13:56:56 GMT</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 20:09:31 GMT</pubDate>
</item>
<item>
<title>基于对抗性动态的联合分析优化候选人特征研究</title>
<link>https://arxiv.org/abs/2504.19043</link>
<guid>https://arxiv.org/abs/2504.19043</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种概率分布方法优化政治候选人特征，在对抗性环境中更贴近历史选举结果。</p><br /><br /><p><strong>摘要：</strong> 联合分析是一种实验设计工具，广泛应用于社会科学研究中，尤其在政治分析领域，用于研究选民对候选人特征的多维偏好。本文探讨如何确定最优候选人配置的问题。由于联合实验中的特征组合数量远超观察总数，无法精确识别最优配置。为解决这一识别难题，我们推导出一种最优随机干预策略，该策略旨在通过属性的概率分布实现最有利的平均结果。首先，我们在单一政党优化候选人的情境下进行分析；随后扩展到两个政党同时优化并相互对抗的现实情况。我们将所提方法应用于一项关于美国总统选举投票选择的联合实验。结果显示，与非对抗性方法相比，对抗性环境下的预期结果更符合历史选举结果，且由方法建议的最优策略更可能与实际观察到的候选人匹配。这些发现表明，在联合分析中引入对抗性动态可能为实验数据提供独特的社会科学研究洞见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19043" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 18:35:58 GMT</pubDate>
</item>
<item>
<title>Sadeed：基于轻量解码器模型的阿拉伯文变音标注新方法</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于轻量语言模型的阿拉伯文变音标注方法Sadeed。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sadeed的新方法，该方法通过微调Kuwain 1.5B Hennara等人的解码器-only语言模型，解决了自然语言处理中阿拉伯文变音标注的难题。Sadeed经过精心整理的高质量变音数据集训练，尽管使用了有限的计算资源，但其性能与专有大型语言模型相当，并优于传统模型。此外，我们还指出了当前阿拉伯文变音标注基准测试中的局限性，并提出了新的基准测试SadeedDiac-25，以实现更公平和全面的评估。Sadeed和SadeedDiac-25共同为推进阿拉伯自然语言处理应用提供了坚实基础，涵盖机器翻译、文本转语音及语言学习工具等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:37:24 GMT</pubDate>
</item>
<item>
<title>UniBiomed：基于多模态大语言模型的生物医学图像统一解释框架</title>
<link>https://arxiv.org/abs/2504.21336</link>
<guid>https://arxiv.org/abs/2504.21336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniBiomed实现生物医学图像跨模态统一解释，性能卓越。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniBiomed的新模型，它是首个用于生物医学图像解释的通用基础模型。UniBiomed结合了多模态大语言模型（MLLM）和Segment Anything Model（SAM），实现了临床文本生成与生物医学对象分割的统一。该模型支持十种不同的生物医学成像模式，涵盖了多种任务，如分割、疾病识别、诊断等。为了开发UniBiomed，研究团队创建了一个包含超过2700万张图像及其注释和文本描述的大规模数据集。通过在84个内部和外部数据集上的验证，UniBiomed展示了其在分割、疾病识别、区域感知诊断、视觉问答和报告生成等方面的领先性能。与依赖临床专家的传统方法相比，UniBiomed可以提供自动化且端到端的解释，显著提升了诊断效率，标志着临床工作流程的一次范式转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:51:48 GMT</pubDate>
</item>
<item>
<title>基于子思维分析的大语言模型推理性能提升方法</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究挑战传统大语言模型评价方式，提出通过分析中间子思维步骤提升推理准确性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过逐步推理解决复杂问题，通常通过评估最终答案来衡量模型表现。本研究质疑仅依赖最终答案的有效性，提出一种新方法分析推理过程中的中间子思维步骤。我们通过语言线索将完整推理轨迹分割成多个子思维片段，并从各片段生成可能的答案，通过频率统计发现聚合答案的准确性显著优于直接采用完整推理轨迹的结果。进一步分析显示，子思维答案的一致性可反映模型的置信度与正确性。实验表明，该方法在AIME2024和AIME2025数学推理数据集上分别提升了最高13%和10%的准确率。此方法为提高LLMs推理能力提供了新的思路与工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20708" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:39:07 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理服务优化方法综述</title>
<link>https://arxiv.org/abs/2504.19720</link>
<guid>https://arxiv.org/abs/2504.19720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大规模语言模型推理服务优化的关键技术与未来方向。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其巨大的参数量和注意力机制带来的高计算需求阻碍了低延迟和高吞吐量的实现。本文全面回顾了相关领域的研究进展，涵盖了实例级方法（如模型放置、请求调度等）、集群级策略（如GPU部署和负载均衡）、新兴场景下的具体任务优化及辅助方法，还探讨了若干重要但边缘的研究领域。通过整合这些技术成果，文章旨在为LLM推理服务提供系统化的优化方案，并指出了未来可能的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:14:02 GMT</pubDate>
</item>
<item>
<title>Foundation-Sec-8B：面向网络安全部署的大型语言模型</title>
<link>https://arxiv.org/abs/2504.21039</link>
<guid>https://arxiv.org/abs/2504.21039</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出一款基于Llama架构并增强的网络安全专用大语言模型。</p><br /><br /><p><strong>摘要：</strong> 随着基于Transformer的大规模语言模型（LLMs）在社会各领域的渗透，软件工程、创意写作及数字艺术等领域得到了显著革新。然而，在网络安全领域，由于缺乏专门训练数据且需复杂表示特定知识，其应用受到限制。本文介绍了一款名为Foundation-Sec-8B的新模型，该模型基于Llama 3.1架构构建，并通过精心策划的网络安全语料库进行持续预训练得以增强。我们评估了Foundation-Sec-8B在现有和新网络安全基准上的表现，结果显示它在某些特定任务上与Llama 3.1-70B和GPT-4o-mini的表现相当。通过公开此模型，我们希望推动AI驱动工具在公共和私人网络安全环境中的进步和应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21039" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 04:41:12 GMT</pubDate>
</item>
<item>
<title>ReVision：通过参数化物理知识提升视频生成模型能力</title>
<link>https://arxiv.org/abs/2504.21855</link>
<guid>https://arxiv.org/abs/2504.21855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入ReVision框架，显著提升复杂动作和交互视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReVision的插件式框架，通过将参数化的三维物理知识融入到预训练的条件视频生成模型中，显著增强了生成高质量复杂运动和交互视频的能力。该框架分为三个阶段：首先利用视频扩散模型生成粗略视频；接着提取二维和三维特征构建对象中心的三维表示，并通过参数化物理先验模型优化得到精确的三维运动序列；最后将优化后的运动序列反馈至扩散模型作为附加条件，生成一致性更强的视频。实验表明，ReVision在Stable Video Diffusion上的表现优于参数量更多的现有模型，验证了其在复杂场景中的优越性，为实现逼真的物理视频生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>COMPACT：提升多模态大语言模型复杂视觉-语言任务性能的新方法</title>
<link>https://arxiv.org/abs/2504.21850</link>
<guid>https://arxiv.org/abs/2504.21850</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">COMPACT通过控制训练样本的组合复杂性显著提升多模态大语言模型在复杂任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 当前的多模态大型语言模型（MLLMs）在简单视觉-语言任务上表现出色，但在需要多种能力同时工作的复杂任务中表现欠佳。这种局限性部分源于视觉指令微调（VIT）的传统训练策略过于注重数据量扩展而忽视了训练示例的组合复杂性。为此，我们提出COMPACT（COMPositional Atomic-to-complex visual Capability Tuning），该方法专门设计了一个训练数据集，通过显式控制训练样本的组合复杂性来提高模型学习效率。实验表明，COMPACT在多个基准测试中达到了与LLaVA-665k VIT相当甚至更好的性能，特别是在涉及四种及以上原子能力的复杂问题上，例如在MMStar和MM-Vet任务中分别实现了83.3%和94.0%的显著改进。COMPACT提供了一种可扩展且数据高效的视觉组合微调方案，有效提升了复杂视觉-语言任务的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21850" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:57:22 GMT</pubDate>
</item>
<item>
<title>WebThinker：增强大型推理模型复杂知识密集型任务处理能力</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WebThinker系统，提升大型推理模型在线索推理中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WebThinker的新方法，该方法通过结合深度网络探索模块和自主思考搜索撰写策略，增强了大型推理模型（LRMs）在复杂知识密集型任务中的表现。WebThinker允许LRMs在推理过程中动态搜索网络、导航网页并撰写研究报告，从而克服了传统LRMs依赖静态内部知识的局限性。此外，我们还通过基于强化学习的迭代在线直接偏好优化（DPO）策略进一步提升了研究工具的利用效率。实验表明，WebThinker在多个复杂推理基准测试（如GPQA、GAIA等）和科学报告生成任务中显著优于现有方法及专有系统，大幅提高了LRMs在复杂场景中的可靠性和适用性。本研究代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:25:25 GMT</pubDate>
</item>
<item>
<title>Phi-4-reasoning：高效推理模型在复杂任务中的表现</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-4-reasoning及增强版在多种推理任务上超越更大规模的开放权重模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Phi-4-reasoning，一种拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过监督微调Phi-4结合精心挑选的提示和o3-mini生成的推理演示训练而成，可生成详细的推理链。此外，Phi-4-reasoning-plus版本经过短期基于成果的强化学习进一步优化，推理轨迹更长，性能更高。实验结果显示，这两个模型在数学、科学推理、编码、算法问题解决、规划和空间理解等多个领域显著优于大型开放权重模型如DeepSeek-R1-Distill-Llama-70B，并接近全量DeepSeek-R1模型的表现。有趣的是，这些改进在通用基准测试中也有非平凡的迁移效果。研究还探讨了训练数据的精细筛选对监督微调的重要性，并指出强化学习可进一步提升性能。最后，本报告指出了评估推理模型性能和鲁棒性的潜在改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:05:09 GMT</pubDate>
</item>
<item>
<title>通过系统训练提升小规模语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.21233</link>
<guid>https://arxiv.org/abs/2504.21233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一套系统性训练方法，显著提升了小规模语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一套针对小规模语言模型（SLMs）的系统训练方案，该方案由四个阶段组成：大规模中期训练、高质量长链推理数据的监督微调、基于精心筛选偏好数据集的Rollout DPO以及具有可验证奖励的强化学习。实验中，将此方法应用于Phi-4-Mini模型上，其推理版本Phi-4-Mini-Reasoning在数学推理任务中表现出色，超越了更大规模的模型如DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B。这一成果证明，即使在资源受限的小规模模型中，精心设计的训练方案结合高质量的链式推理数据也能有效提升推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 20:04:35 GMT</pubDate>
</item>
<item>
<title>软选择（Softpick）：一种改进Transformer注意力机制的新方法</title>
<link>https://arxiv.org/abs/2504.20966</link>
<guid>https://arxiv.org/abs/2504.20966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">软选择替代Softmax消除注意力问题并提高量化性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为软选择（Softpick）的新方法，它是Transformer注意力机制中Softmax的改进版本，能够消除注意力沉没现象并减少大量激活值。实验表明，在3.4亿参数规模模型上，软选择与Softmax在标准基准测试中的表现相当，但实现了零沉没率。此外，软选择Transformer产生的隐藏状态峰度显著降低，且生成的注意力图更加稀疏。当进行量化时，使用软选择的模型始终优于Softmax，尤其是在较低位精度下优势更为明显。分析显示，软选择可能为量化、低精度训练、稀疏优化、剪枝和可解释性开辟新的可能性。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:36:18 GMT</pubDate>
</item>
<item>
<title>RoboVerse：机器人学习的综合框架</title>
<link>https://arxiv.org/abs/2504.18904</link>
<guid>https://arxiv.org/abs/2504.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboVerse框架，解决机器人领域数据规模和评估基准标准化问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器人领域数据规模化及评估协议建立的独特挑战，介绍了RoboVerse，一个集模拟平台、合成数据集及统一基准为一体的综合性框架。该框架通过MetaSim基础设施抽象多种仿真环境，支持跨模拟器和机器人形态的无缝转换，并提供高质量、多样化的合成数据。此外，还提出了用于模仿学习和强化学习的统一基准，支持不同泛化水平的评估。实验表明，RoboVerse显著提升了模仿学习、强化学习、世界模型学习及仿真到现实迁移的性能，验证了其数据集和基准的可靠性，成为推动机器人学习发展的稳健解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 08:31:04 GMT</pubDate>
</item>
<item>
<title>通过防御性思维提升大语言模型鲁棒性的研究</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示如何利用增强推理能力提高大语言模型对参考污染攻击的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过增强的大语言模型推理能力，在非推理任务中提升其鲁棒性的方法。特别提出了一种名为“防御性思维”的简单方法，仅需少量结构化且具有防御性的推理示例作为演示，就能显著改善模型的鲁棒性。实验表明，该方法效果显著，例如在Natural Questions任务中，标准提示下GPT-4o的准确率从60%降至3%，而采用防御性思维提示后，其准确率仍能保持在50%。这一发现凸显了该方法的简单性和适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:50:05 GMT</pubDate>
</item>
<item>
<title>LawFlow：构建端到端法律工作流程的数据集</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出LawFlow数据集，用于捕捉法律实践中的动态推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LawFlow的新数据集，该数据集收集了法律学生完成的真实商业实体形成场景中的端到端法律工作流程。与现有专注于输入输出对或线性推理链的数据集不同，LawFlow强调动态、模块化且迭代的推理过程，这些过程反映了法律实践中存在的模糊性、修订及客户导向策略。通过对比人类和大型语言模型（LLMs）生成的工作流程，发现两者在结构、推理灵活性和计划执行方面存在显著差异。研究还表明，法律专业人士更倾向于让AI承担辅助角色，如头脑风暴、盲点识别和提供替代方案，而非直接执行复杂的全流程。基于此，我们提出了结合人类目标的设计建议，旨在通过混合规划、自适应执行和支持决策点来提升AI协作能力。本研究揭示了当前LLMs支持复杂法律工作的局限性，并为开发更协作、推理感知的法律AI系统提供了机会。所有数据和代码均可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 11:01:55 GMT</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:57:08 GMT</pubDate>
</item>
<item>
<title>基于强化学习的自主驾驶特权规划研究</title>
<link>https://arxiv.org/abs/2504.17838</link>
<guid>https://arxiv.org/abs/2504.17838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于单一直观奖励项的新方法，显著提升强化学习在自主驾驶中的性能。</p><br /><br /><p><strong>摘要：</strong> 当前针对自主驾驶的任务大多采用基于规则的方法，但这些方法难以应对长尾问题。强化学习（RL）因其可扩展性且不受累积误差影响而受到关注。然而，传统RL方法依赖复杂的复合奖励设计，导致优化困难。本文提出一种新奖励机制，主要基于路线完成度优化，并通过终止或乘法惩罚违规行为，使PPO算法在更大批量下表现优异。实验表明，该方法在CARLA和nuPlan数据集上分别达到64DS和91.3/90.6的高分，超越其他复杂奖励设计的RL方法，同时大幅提高训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于解释性方法的强化学习从人类反馈中优化奖励分配</title>
<link>https://arxiv.org/abs/2504.16272</link>
<guid>https://arxiv.org/abs/2504.16272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">改进奖励分配机制以提升语言模型对齐性能。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLM）对齐中的强化学习从人类反馈（RLHF）管道通常为序列分配标量奖励，以序列末尾标记作为整体质量的代理指标，导致反馈稀疏且令牌级奖励分配次优。本文将奖励塑造视为专注于令牌级奖励分配的优化问题，提出一种利用SHAP和LIME等解释性方法估计令牌奖励的奖励塑造函数，并采用双层优化框架结合贝叶斯优化和策略训练来处理令牌奖励估计中的噪声。实验表明，更好的令牌级奖励归因平衡在下游任务上优于基线模型，并在训练期间更快找到最优策略。此外，理论上证明了特征可加性属性的解释性方法保持原始奖励的最优策略不变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 17:09:33 GMT</pubDate>
</item>
<item>
<title>基于上下文提示的大规模图像编辑方法</title>
<link>https://arxiv.org/abs/2504.20690</link>
<guid>https://arxiv.org/abs/2504.20690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效且高精度的指令引导图像编辑框架。</p><br /><br /><p><strong>摘要：</strong> 本文解决了基于指令的图像编辑领域中精度与效率之间的权衡问题。通过利用大规模Diffusion Transformer（DiT）的生成能力和上下文感知能力，我们提出了三个创新贡献：一是通过上下文提示实现零样本指令合规的编辑框架；二是引入LoRA-MoE混合微调策略，在不进行大量重新训练的情况下增强灵活性；三是采用视觉-语言模型在推理阶段早期筛选初始噪声，从而提升编辑质量。实验结果显示，该方法在性能上超越现有最先进方法，同时仅需传统基线方法0.5%的训练数据和1%的可训练参数。这项工作开创了一种新的范式，实现了高精度且高效的指令引导图像编辑。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:14:47 GMT</pubDate>
</item>
<item>
<title>基于视觉语言模型的3D目标检测系统性综述</title>
<link>https://arxiv.org/abs/2504.18738</link>
<guid>https://arxiv.org/abs/2504.18738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了视觉语言模型在3D目标检测中的应用与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文对基于视觉语言模型(VLMs)的3D目标检测进行了系统的分析，通过对超过100篇研究论文的考察，首次提供了专门针对这一领域的系统性分析。文章首先概述了VLMs在3D目标检测中的独特挑战，特别是与2D检测相比在空间推理和数据复杂性上的差异。接着比较了传统方法如点云和体素网格与现代框架如CLIP及3D大型语言模型(3D LLMs)，后者支持开放词汇检测和零样本泛化。文中还回顾了关键架构、预训练策略及提示工程方法，这些方法用于有效对齐文本和3D特征。通过可视化示例和评估基准讨论了性能和行为。最后，文章指出了当前存在的问题，例如有限的3D-语言数据集和计算需求，并提出了未来的研究方向以推动VLMs在3D目标检测中的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 19:27:26 GMT</pubDate>
</item>
<item>
<title>X-Fusion：一种保持语言能力的多模态任务扩展框架</title>
<link>https://arxiv.org/abs/2504.20996</link>
<guid>https://arxiv.org/abs/2504.20996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Fusion框架通过双塔设计提升多模态任务性能同时保留语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Fusion的框架，旨在扩展预训练的大规模语言模型（LLMs）以处理多模态任务，同时保持其原有的语言处理能力。X-Fusion采用了一种双塔结构，其中包含针对特定模态的权重，使得在整合视觉信息时可以保持LLM参数不变。实验结果显示，在图像到文本和文本到图像的任务上，X-Fusion的表现优于其他替代架构。研究还发现，引入专注于理解的数据能提高生成质量，减少图像数据噪声可改善整体表现，而特征对齐对较小模型的收敛速度有显著促进作用，但对较大模型影响有限。这些发现为构建高效的统一多模态模型提供了宝贵的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>Chatbot Arena排名体系中的系统性问题及改进建议</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Chatbot Arena排名受私测偏颇影响，导致不公平竞争。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Chatbot Arena作为顶级AI系统排行榜所面临的问题。研究发现，少数公司通过未公开的私测策略，在公开发布前测试多个模型变体并选择最佳成绩，这种做法导致了排行榜评分的偏差。例如，Meta在Llama-4发布前对27个私测变体进行了评估。此外，闭源模型比开源模型获得更多测试机会，造成数据获取的不对等。Google和OpenAI分别占Arena总数据的19.2%和20.4%，而83个开源模型仅占29.7%。这些动态使模型过度适应Arena特定环境，而非整体性能提升。尽管如此，Chatbot Arena仍得益于组织者和开放社区的努力。我们提出了改进评估框架的建议，以实现更公平透明的基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.20630</link>
<guid>https://arxiv.org/abs/2504.20630</guid>
<content:encoded><![CDATA[
Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:56:44 GMT</pubDate>
</item>
<item>
<title>Meta Policy Optimization提升大语言模型奖励对齐的鲁棒性</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Meta Policy Optimization框架解决大语言模型奖励对齐中的奖励黑客和提示工程问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于奖励的大语言模型对齐方法存在的两大局限性——易受奖励黑客影响及高度依赖人工设计的提示工程——提出了一种名为Meta Policy Optimization (MPO) 的新框架。MPO通过集成一个元奖励模型，在训练过程中动态优化奖励模型的提示，从而有效减少奖励信号被模型利用的可能性，同时大幅降低手动设计提示的需求。实验表明，该方法不仅在性能上媲美甚至优于传统方法，而且在多种任务（如问答和数学推理）中表现稳定，无需特定奖励设计。此外，MPO的元学习特性使其可扩展至更高层次的对齐框架，为大语言模型的奖励对齐提供了更稳健且灵活的解决方案。未来，代码和模型将公开共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 14:02:35 GMT</pubDate>
</item>
<item>
<title>TreeHop：一种高效的多跳问答系统</title>
<link>https://arxiv.org/abs/2504.20114</link>
<guid>https://arxiv.org/abs/2504.20114</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需LLMs的嵌入级框架TreeHop，显著提升多跳问答效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对多跳问答（MHQA）中检索增强生成（RAG）系统的挑战，提出了一种名为TreeHop的新框架。传统方法依赖迭代的基于LLMs的查询重写和路由，导致高计算成本。TreeHop通过融合前序查询和文档的语义信息动态更新查询嵌入，在嵌入空间内完成迭代检索，从而避免重复调用LLMs和多阶段处理。此外，引入基于规则的停止准则进一步减少冗余检索，平衡效率与召回率。实验结果显示，TreeHop在三个公开数据集上的表现与先进RAG方法相当，但参数规模仅为5%-0.4%，查询延迟降低约99%。这一成果表明TreeHop是一种更快且更具成本效益的解决方案，适用于知识密集型应用。代码和数据已开源以促进复现研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20114" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:56:31 GMT</pubDate>
</item>
<item>
<title>DICE-Talk：基于解耦身份与情感的高表现力可泛化口型同步虚拟头像生成</title>
<link>https://arxiv.org/abs/2504.18087</link>
<guid>https://arxiv.org/abs/2504.18087</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架DICE-Talk，解决现有情感生成方法的情感表达不足问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于扩散模型的虚拟口型同步技术在唇同步和视觉质量方面取得了显著进步，但在保持说话者身份的同时生成富有情感的表情仍然面临挑战。本文指出当前情感口型生成方法存在三大局限：音频内在情感线索利用不足、情感表示中的身份泄露以及情感相关性孤立学习的问题。为此，我们提出了名为DICE-Talk的新框架，通过解耦身份与情感并协作具有相似特性的感情来解决这些问题。首先，开发了一种解耦的情感嵌入器，通过跨模态注意力共同建模音视频情感线索，将情感表示为与身份无关的高斯分布。其次，引入了一个增强相关性的条件模块，使用可学习的情绪银行通过向量量化和基于注意力的特征聚合显式捕获情绪间关系。第三，设计了一种情绪辨别目标，在扩散过程中通过潜在空间分类强制情感一致性。在MEAD和HDTF数据集上的大量实验表明，该方法在情感准确性方面优于最先进的方法，同时保持了竞争性的唇同步性能。定性结果和用户研究进一步证实了该方法能够生成保留身份且具有丰富相关情感表达的可泛化虚拟头像。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18087" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 01:28:21 GMT</pubDate>
</item>
<item>
<title>BloomScrub：一种高效的大语言模型版权清除方法</title>
<link>https://arxiv.org/abs/2504.16046</link>
<guid>https://arxiv.org/abs/2504.16046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法有效解决大语言模型的版权侵权问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）在预训练阶段接触大量受版权保护材料后可能引发的无意版权侵权问题，探讨了现有“版权清除”技术的局限性，特别是对最坏情况下的长段直接引用处理不足的问题。为此，我们提出了BloomScrub，一种简单而高效的推理阶段方法，通过交替检测并重写潜在侵权片段，利用高效的Bloom过滤器实现大规模文本的版权筛查。该方法不仅显著降低了侵权风险，还保证了模型功能的实用性，并支持根据需求调整执行严格程度。实验表明，这种轻量级的推理时间方法在版权预防方面表现出了令人惊讶的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:16:53 GMT</pubDate>
</item>
<item>
<title>YoChameleon: Personalized Vision and Language Generation</title>
<link>https://arxiv.org/abs/2504.20998</link>
<guid>https://arxiv.org/abs/2504.20998</guid>
<content:encoded><![CDATA[
Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>基于RGB-DN视频的高效四维世界模型学习方法</title>
<link>https://arxiv.org/abs/2504.20995</link>
<guid>https://arxiv.org/abs/2504.20995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过RGB-DN视频学习四维世界模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种有效学习新型四维具身世界模型的方法，该模型能够预测三维场景随时间动态变化的过程，响应具身智能体的动作需求，提供空间和时间的一致性。我们建议通过训练RGB-DN（RGB、深度图和法线图）视频来学习四维世界模型。这种方法不仅超越传统的二维模型，通过整合详细的形状、配置和时间变化到预测中，而且还能有效地学习具身智能体的逆动力学模型。具体来说，我们首先利用现有的机器人操作视频数据集扩展深度和法线信息，然后微调视频生成模型，使其联合预测每帧的RGB-DN。最后，我们提出了一种算法，直接将生成的RGB、深度和法线视频转换为高质量的四维场景。我们的方法确保了具身场景中四维场景预测的时间和空间一致性，实现了具身环境中的新颖视图合成，并促进了显著优于先前基于视频的世界模型的策略学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>UniversalRAG：一种支持多模态异构知识检索的增强型生成框架</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，可从多模态异构知识源中检索并整合信息。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniversalRAG的新框架，该框架旨在通过从异构的多模态知识源中检索和整合相关信息，提升基于检索的生成模型（RAG）的能力。现有RAG方法大多局限于单一模态的知识库，而UniversalRAG则通过引入模态感知路由机制，动态确定最合适的模态特定知识库并进行针对性检索。此外，还对各模态按粒度级别组织，以实现根据查询复杂性和范围进行精细化检索。实验结果显示，在涉及多种模态的8个基准测试中，UniversalRAG的表现优于模态特定和统一基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:18:58 GMT</pubDate>
</item>
<item>
<title>ReasonIR-8B：首个专为推理任务训练的检索器</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonIR-8B通过合成数据显著提升了推理密集型信息检索任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ReasonIR-8B，这是首个专门针对一般推理任务训练的检索器。传统检索器在推理任务中的表现有限，因为现有训练数据集主要关注简单事实查询。我们开发了一种合成数据生成管道，该管道为每份文档创建具有挑战性和相关性的查询，同时结合看似相关但最终无用的硬负样本。通过混合使用合成数据和现有公开数据进行训练，ReasonIR-8B在BRIGHT基准测试中实现了新的最佳性能，nDCG@10分别达到了29.9（无重排序器）和36.9（有重排序器）。此外，在RAG任务中，它相对提高了MMLU和GPQA性能，分别提升了6.4%和22.6%，并更有效地利用了测试时计算资源。我们的训练方法通用且易于扩展到未来的大型语言模型，为此我们开源了代码、数据和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:49:28 GMT</pubDate>
</item>
<item>
<title>通过1-shot强化学习实现大型语言模型数学推理能力的有效提升</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示1-shot强化学习结合可验证奖励显著提升了大型语言模型的数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种基于单训练样本（1-shot）的强化学习方法，即强化学习与可验证奖励（RLVR），其在提升大型语言模型（LLMs）的数学推理能力方面表现出色。通过将此方法应用于基础模型Qwen2.5-Math-1.5B，我们发现仅使用一个示例即可显著提高其在MATH500测试集上的表现，从36.0%提升至73.6%，并在六个常见数学推理基准上平均提高了18.1个百分点。此外，实验表明，这种方法不仅适用于多种模型架构和强化学习算法，还能在多个数学问题上实现类似的效果。进一步研究还揭示了一些有趣的现象，如跨领域泛化能力增强、自我反思频率增加以及在训练精度饱和后持续的测试性能提升（称为“后饱和泛化”）。我们还验证了策略梯度损失在此过程中的关键作用，并强调了探索促进的重要性。最后，单独使用熵损失也能显著提高Qwen2.5-Math-1.5B在MATH500上的表现。这些发现为未来提高RLVR的数据效率提供了启示，并促使重新审视相关领域的进展及其机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:24:30 GMT</pubDate>
</item>
<item>
<title>NORA：高效视觉-语言-动作模型实现机器人实时自主性</title>
<link>https://arxiv.org/abs/2504.19854</link>
<guid>https://arxiv.org/abs/2504.19854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NORA模型，通过减少参数量提升机器人任务执行效率。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言-动作(VLA)模型在零样本场景下表现出色，但存在视觉编码限制导致任务失败的问题，且通常参数规模超过7B，带来高昂计算开销。NORA作为一款3B参数的新型模型，采用Qwen-2.5-VL-3B多模态模型作为基础，结合97万真实世界机器人演示数据及FAST+分词器，显著提升了视觉推理和动作定位能力，同时大幅降低计算资源需求。实验结果显示，NORA在任务表现上超越现有大型VLA模型，成为适用于实时机器人环境的更优解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:47:34 GMT</pubDate>
</item>
<item>
<title>Mem0：基于记忆增强的大语言模型对话一致性优化</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mem0架构解决大语言模型长期对话一致性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在长时间多会话对话中因固定上下文窗口导致的一致性挑战，引入了Mem0这一可扩展的记忆中心型架构。Mem0通过动态提取、整合和检索对话中的关键信息来维持一致性。此外，还提出了利用图结构记忆表示法捕捉复杂关系的方法。在LOCOMO基准测试中，Mem0在单跳、时间序列、多跳及开放领域四个问题类别上均优于六类基线系统。实验显示，Mem0在OpenAI的LLM-as-a-Judge指标上提升了26%，且显著降低了计算开销，相比全上下文方法，延迟降低91%，令牌成本节省超90%。这些成果表明，结构化持久化记忆机制对提升长期对话连贯性至关重要，为高效可靠的LLM驱动AI代理铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:46:35 GMT</pubDate>
</item>
<item>
<title>基于领域适应的Chisel代码生成模型ChiseLLM</title>
<link>https://arxiv.org/abs/2504.19144</link>
<guid>https://arxiv.org/abs/2504.19144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合数据处理和提示引导推理的ChiseLLM模型，显著提升Chisel代码语法正确性和设计变异性。</p><br /><br /><p><strong>摘要：</strong> 随着对特定领域架构需求的增长，敏捷硬件开发方法学得到了快速发展，而像Chisel这样的构造语言因其高级抽象特性成为理想选择。尽管大型语言模型在代码生成方面表现出色，但在Chisel代码生成任务上仍面临语法正确性和设计变异性挑战。本文介绍ChiseLLM，通过数据处理、提示引导推理追踪合成及领域适应模型训练，显著提升了Chisel代码生成性能，语法正确性提升至多26.32%，设计变异性提高47.58%。相关数据集和模型已公开，为基于硬件构造语言的敏捷开发提供高效工具，并为后续研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 03:56:49 GMT</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 21:00:06 GMT</pubDate>
</item>
<item>
<title>RepText：一种无需理解文本的多语言视觉文本生成方法</title>
<link>https://arxiv.org/abs/2504.19724</link>
<guid>https://arxiv.org/abs/2504.19724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RepText模型，提升文本到图像生成模型对非拉丁字母及多语言视觉文本的精确渲染能力。</p><br /><br /><p><strong>摘要：</strong> 尽管当前文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著进展，但其在生成精确且灵活的印刷元素（尤其是非拉丁字母）方面的能力仍受到限制。为解决这些局限性，我们基于一个假设——即文本理解只是文本呈现的充分条件而非必要条件，提出了RepText模型。该模型通过引入字体无关的字符和位置信息，允许用户自定义文本内容、字体和位置，同时结合感知损失和扩散损失提高准确性。此外，在推理阶段，RepText采用噪声字符潜在初始化并使用区域掩码来稳定生成过程。实验表明，RepText在多语言视觉文本生成任务中表现出色，性能优于现有开源方法，与闭源多语言模型相当，同时也讨论了其局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:19:53 GMT</pubDate>
</item>
<item>
<title>利用替代密码研究In-Context Learning中的双模态操作</title>
<link>https://arxiv.org/abs/2504.19395</link>
<guid>https://arxiv.org/abs/2504.19395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入基于替代密码的任务重构，研究In-Context Learning的学习模式。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，In-Context Learning (ICL) 存在任务检索与任务学习两种模式，但区分这两种模式仍具挑战性。本文提出一种基于经典密码学中替代密码的任务重构方法ICL CIPHERS，通过部分替换上下文输入中的标记，使英文句子对人类不直观，但仍保持可逆性。实验显示，大型语言模型在处理具有双射映射的ICL CIPHERS时表现优于非双射基线，为量化ICL中的学习能力提供了新途径。此外，我们分析了模型内部表征，发现其具备解码加密输入的能力。这一研究结果在四个数据集和六种模型上具有一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 20:05:29 GMT</pubDate>
</item>
<item>
<title>基于自博弈批评者的大型语言模型推理可靠性评估方法</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需人工标注的自博弈批评者方法提升大语言模型推理可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self-Play Critic（SPC）的新方法，用于评估大型语言模型（LLM）的推理可靠性，如Chain-of-Thought推理。由于高质量的分步监督数据难以获取且成本高昂，SPC通过两个模型之间的对抗性自博弈游戏进化出评估推理步骤的能力，消除了对人工分步注释的需求。这两个模型分别是“狡猾生成器”和“批评者”，前者生成故意错误的推理步骤以迷惑后者，后者则试图检测这些错误。通过基于游戏结果的强化学习机制，两者迭代改进。实验表明，SPC在三个推理过程基准测试上提升了错误检测能力，并优于其他基线模型，同时在数学推理任务中显著提高了多种LLMs的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 04:45:06 GMT</pubDate>
</item>
<item>
<title>CipherBank：评估大型语言模型在密码学推理中的能力</title>
<link>https://arxiv.org/abs/2504.19093</link>
<guid>https://arxiv.org/abs/2504.19093</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出CipherBank基准测试集，评估LLMs在加密解密任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍CipherBank，这是一个针对大型语言模型（LLMs）在密码学推理任务中表现进行评估的综合基准测试集。CipherBank包含2358个精心设计的问题，覆盖了5大领域和14种子领域的262种独特明文，重点关注隐私敏感和现实场景下的加密需求。从密码学角度来看，该基准涵盖了三大类加密方法，涉及9种不同的算法，从经典密码到自定义加密技术均有涉及。通过在CipherBank上对当前最先进的LLMs（如GPT-4、DeepSeek-V3等）及专注于推理的模型（如o1、DeepSeek-R1）进行评估，发现这些模型在推理能力上存在显著差距，不仅体现在通用聊天型LLMs与推理型LLMs之间，也表现在推理型LLMs在经典密码学解密任务中的表现不足。本研究通过详细分析和错误调查揭示了LLMs在密码推理方面的局限性及改进空间，强调了持续提升LLMs推理能力的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19093" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 23:41:17 GMT</pubDate>
</item>
<item>
<title>VCBENCH：大型视觉语言模型在多模态数学推理中的挑战与评估</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有大型视觉语言模型在处理依赖显式视觉的数学问题时表现有限。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型视觉-语言模型（LVLMs）在整合视觉与语言信息方面取得了显著进展，但在涉及基本数学元素和视觉概念的推理任务上仍存在不足。当前基准测试多集中于特定领域的专业知识评估，而忽视了基础数学推理能力。为填补这一空白，我们开发了VCBENCH，这是一个包含1720个问题的综合基准，涉及六个认知领域，平均每个问题包含近四个图像，强调多图推理。通过对26个最先进的LVLM进行测试，我们发现即使是最优秀的模型，在准确性上也未能超过50%，这揭示了视觉与数学整合方面的持续挑战，并为未来模型改进提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 02:16:38 GMT</pubDate>
</item>
<item>
<title>基于均匀下采样的群等变架构广义化研究</title>
<link>https://arxiv.org/abs/2504.17258</link>
<guid>https://arxiv.org/abs/2504.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了群等变架构中的均匀下采样层的广义化方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在群等变架构（如G-CNNs）中均匀下采样层的一般化应用，特别是在反混淆下的通用有限群信号（特征图）下采样问题。首先，给出了一种根据有限群和下采样率选择合适子群的算法；其次，在给定群和子群的情况下，研究了带限性概念并提出了反混淆操作的实现方式。该方法推广了经典采样理论中的下采样概念，当信号位于循环群时，其等效于理想低通滤波器后接下采样操作。实验表明，将所提出的下采样操作引入G-等变网络中，可提高图像分类任务的准确性，更好地保持等变性并减少模型规模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 01:29:51 GMT</pubDate>
</item>
<item>
<title>MMInference：一种加速多模态长上下文推理的动态稀疏注意力方法</title>
<link>https://arxiv.org/abs/2504.16083</link>
<guid>https://arxiv.org/abs/2504.16083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMInference方法，显著加速视觉语言模型的长上下文推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMInference的动态稀疏注意力方法，旨在解决视觉语言模型(VLMs)在处理长上下文多模态输入时，由于二次注意力复杂度导致的预填充阶段效率低下问题。通过分析视频输入的时间和空间局部性，我们发现了一种独特的Grid模式，并提出了基于排列的方法来利用这一模式并处理模态边界问题。此外，通过离线搜索每个头的最佳稀疏模式，MMInference能够根据输入动态构建稀疏分布。该方法无需对现有VLM管道进行任何修改或微调即可无缝集成，并且在多模态基准测试中表现出高达8.3倍的速度提升，同时保持了准确性。实验涵盖了多种任务，如视频问答、视频描述生成等。我们的代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于可信赖生成的数据引擎TrustGeoGen用于几何问题求解</title>
<link>https://arxiv.org/abs/2504.15780</link>
<guid>https://arxiv.org/abs/2504.15780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为TrustGeoGen的数据引擎，用于生成高质量几何问题数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TrustGeoGen的可扩展数据引擎，专门用于生成几何问题数据。该引擎通过多模态对齐生成图表、文本描述及分步解答，结合形式化验证确保推理路径符合规则，并采用自举机制递归生成复杂性增加的状态。此外，设计的GeoExplore系列算法同时生成多解变体并进行自我反思回溯跟踪。通过形式逻辑验证，生成的GeoTrust-200K数据集保证了模态完整性，且GeoTrust-test测试集显示出极高的评估严格性，当前最先进的模型仅达到49.17%的准确率。训练后的模型在GeoQA上表现出出色的OOD泛化能力，显著减少了逻辑不一致性。此工作为几何问题求解方法的发展奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:45:23 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的手机图形用户界面智能代理综述</title>
<link>https://arxiv.org/abs/2504.19838</link>
<guid>https://arxiv.org/abs/2504.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型如何推动手机GUI自动化向智能化方向发展。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了由大型语言模型（LLMs）驱动的手机图形用户界面（GUI）智能代理的发展历程，从基于脚本的传统自动化转变为具备智能和适应性的系统。首先分析了传统自动化面临的三大挑战：泛化能力有限、维护成本高及意图理解薄弱，并阐述LLMs通过先进的语言理解、多模态感知和稳健决策解决这些问题的方法。接着提出了一种涵盖基础代理框架、建模方法及关键数据集和基准的分类体系。此外，还详细介绍了针对特定任务的架构设计、监督微调及强化学习策略，这些技术连接了用户意图与GUI操作。最后，讨论了该领域尚未解决的问题，如数据集多样性、设备端高效部署、用户定制化及安全问题，为研究人员提供了未来研究方向的前瞻性见解。本文旨在为开发可扩展且用户友好的手机GUI代理提供权威参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:39:25 GMT</pubDate>
</item>
<item>
<title>大型语言模型在医疗建议中的表现与用户交互挑战</title>
<link>https://arxiv.org/abs/2504.18919</link>
<guid>https://arxiv.org/abs/2504.18919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，尽管大型语言模型在医学考试中表现出色，但用户交互影响其实际应用效果。</p><br /><br /><p><strong>摘要：</strong> 全球医疗提供者正在探索使用大型语言模型（LLMs）向公众提供医疗建议。LLMs在医学执照考试中几乎达到满分，但在现实场景中的准确性仍存疑。本研究测试了LLMs是否能帮助公众识别潜在疾病并选择行动方案，在1,298名参与者中进行了实验。单独测试时，LLMs正确识别疾病的比例为94.9%，确定处置方案的比例为56.3%，但当参与者使用相同模型时，识别疾病的比例降至34.5%，确定处置方案的比例降至44.2%，均未显著优于对照组。研究指出用户交互是LLMs在医疗建议中部署的关键挑战。现有医学知识基准和模拟患者互动无法预测人类参与者中出现的问题。因此，我们建议在面向公众部署前进行系统化的人类用户测试，以评估其交互能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 09:32:49 GMT</pubDate>
</item>
<item>
<title>Kimi-Audio Technical Report</title>
<link>https://arxiv.org/abs/2504.18425</link>
<guid>https://arxiv.org/abs/2504.18425</guid>
<content:encoded><![CDATA[
We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:31:46 GMT</pubDate>
</item>
<item>
<title>新一代小型推理模型Pleias-RAG在RAG和搜索领域的突破性进展</title>
<link>https://arxiv.org/abs/2504.18225</link>
<guid>https://arxiv.org/abs/2504.18225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新推出的Pleias-RAG系列模型在多语言开放源检索及引用支持方面表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两种新型的小型推理模型Pleias-RAG-350m和Pleias-RAG-1B，这些模型专门设计用于检索增强生成（RAG）、搜索以及来源摘要。通过在一个大型合成数据集上的训练，这些模型能够模拟从Common Crawl中检索多种多语言开放资源的过程。它们不仅提供直接引用和上下文关联的支持，还重新整合了与RAG工作流相关的多个功能，例如查询路由、查询重写和来源重排序。在标准化的RAG基准测试（如HotPotQA和2Wiki）中，这两种模型的表现优于参数少于40亿的单语言模型，并且与更大规模的流行模型（如Qwen-2.5-7B、Llama-3.1-8B和Gemma-3-4B）竞争。值得注意的是，Pleias-RAG系列是目前唯一能在主要欧洲语言中保持一致RAG性能并确保陈述系统性引用关联的单语言模型。由于其较小的体积和对受限基础设施的良好适应性，这些模型为生成式人工智能开辟了新的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 06:17:04 GMT</pubDate>
</item>
<item>
<title>Transformer LLMs中稀疏注意力的研究：效率-准确性权衡与扩展性分析</title>
<link>https://arxiv.org/abs/2504.17768</link>
<guid>https://arxiv.org/abs/2504.17768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨Transformer模型中稀疏注意力方法的可行性及效率-准确性权衡。</p><br /><br /><p><strong>摘要：</strong> 本文针对Transformer语言模型处理长上下文的能力，研究了无需训练的稀疏注意力方法。通过在多种长序列任务上的实验，我们发现：1）在非常长的序列上，较大的高稀疏模型优于较小的密集模型；2）解码阶段相比填充阶段可以实现更高的稀疏水平并保证准确性；3）没有一种策略在所有任务和阶段中表现最佳，不同场景需要不同的稀疏化单元或预算适应性；4）即使中等稀疏度也可能导致某些任务性能显著下降，表明稀疏注意力并非万能解决方案。此外，我们提出了适用于稀疏注意力的新尺度定律，并验证了这些发现可能超越实验范围。综上所述，稀疏注意力是增强Transformer模型长序列处理能力的关键工具，但需要对性能敏感的应用进行仔细评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17768" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:39:25 GMT</pubDate>
</item>
<item>
<title>针对意大利语优化的英语大型语言模型词汇适应技术</title>
<link>https://arxiv.org/abs/2504.17025</link>
<guid>https://arxiv.org/abs/2504.17025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种新方法优化英语大型语言模型以处理意大利语。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多种词汇适应技术，以提升英语大型语言模型（LLMs）对意大利语的处理能力。传统LLMs虽能处理多语言，但因训练数据混杂或非优化设计，在非英语语言上的表现欠佳。为此，我们提出了语义对齐词汇适应（SAVA）方法，利用神经映射实现词汇替换。通过适配两个模型——Mistral-7b-v0.1和Llama-3.1-8B，分别降低了25%的标记“生育率”和减少10亿参数。实验表明，经过词汇适配后，这些模型在持续少量目标语言训练后即可恢复性能。最后，我们在多项选择和生成任务中测试了适配后的模型能力，显示出显著效果。此研究为多语言LLMs优化提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:12:27 GMT</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo：首个跨文化视频评估基准</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个跨文化、多语言、多领域的视频理解评估基准VideoVista-CulturalLingo。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoVista-CulturalLingo，这是一个旨在弥合文化、语言和领域差距的视频理解评估基准。与现有基准不同，它涵盖了中国文化、北美文化和欧洲文化，问题以中文和英文呈现，并包含来自数百个人类创作领域的视频。该基准包含1389个视频和3134个QA对，并评估了24个最近开源或专有的视频大模型。实验结果显示，现有模型在与中国历史相关的问题上表现较差，在事件定位任务中的时间理解能力有限，而主流模型在科学问题上表现出色，但开源模型在数学问题上的表现较弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 09:47:30 GMT</pubDate>
</item>
<item>
<title>Skywork R1V2：下一代多模态推理模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.16656</link>
<guid>https://arxiv.org/abs/2504.16656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork R1V2引入混合强化学习范式，显著提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> Skywork R1V2作为Skywork R1V的升级版，通过结合奖励模型指导与基于规则策略的混合强化学习框架，解决了复杂推理与广泛泛化之间的平衡难题。此外，R1V2创新性地采用Selective Sample Buffer机制，优化训练效率并缓解了GRPO中的“优势消失”问题。实验表明，R1V2在多项基准测试中表现优异，如OlympiadBench得分62.6、AIME2024得分79.0等，展现了其在开放源代码模型中的领先地位，并逐步缩小与顶级专有系统的性能差距。该模型权重已公开发布，以促进研究的透明度与可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 08:24:10 GMT</pubDate>
</item>
<item>
<title>零样本条件下的个性化视频生成模型</title>
<link>https://arxiv.org/abs/2504.17816</link>
<guid>https://arxiv.org/abs/2504.17816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解耦身份学习与时间动态的个性化视频生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需微调的个性化视频生成模型，通过将身份特定的学习与时间动态解耦，在零样本设置下实现视频定制。传统方法依赖大规模标注数据集，成本高昂且需要大量标注工作，而我们的方法直接利用图像定制数据集训练视频定制模型，并将其分解为两个部分：通过图像定制数据集进行身份注入，以及借助少量未标注视频通过图像到视频训练方法保留时间建模。此外，在图像到视频微调过程中采用随机图像标记丢弃和随机图像初始化以缓解复制粘贴问题，并引入随机切换机制以增强学习效果，避免灾难性遗忘。实验表明，该方法在零样本设置下表现出色，具有较强的主体一致性与可扩展性，优于现有视频定制模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 02:48:31 GMT</pubDate>
</item>
<item>
<title>DianJin-R1：面向金融领域的推理增强框架</title>
<link>https://arxiv.org/abs/2504.15716</link>
<guid>https://arxiv.org/abs/2504.15716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DianJin-R1框架，通过强化推理能力提升金融领域大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对金融领域大语言模型在推理方面的挑战，提出了一种名为DianJin-R1的推理增强框架。该框架通过构建高质量的数据集DianJin-R1-Data（整合CFLUE、FinQA及自有合规语料库），结合结构化输出的方式对Qwen2.5进行微调，显著提升了模型的推理能力。此外，采用Group Relative Policy Optimization方法优化模型的奖励机制，进一步提高了答案的准确性。实验表明，DianJin-R1在复杂金融任务上表现优异，甚至超越多代理系统，为实际应用提供了高效且实用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 05:01:04 GMT</pubDate>
</item>
<item>
<title>BitNet v2：高效部署1比特大语言模型的新框架</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BitNet v2框架，实现1比特大语言模型的4比特激活量化。</p><br /><br /><p><strong>摘要：</strong> 本文针对1比特大语言模型（LLMs）在低比特量化时因激活异常值导致的部署难题，引入了BitNet v2这一创新框架，支持原生4比特激活量化。为解决注意力机制及前馈网络中的异常激活问题，提出了H-BitLinear模块，在量化之前应用在线哈达玛变换，将尖锐分布平滑为更符合高斯分布的形式，从而适应低比特表示。实验表明，从零训练的BitNet v2在8比特激活下性能与BitNet b1.58相当，并且在使用原生4比特激活训练时仅产生极小的性能下降，显著减少了内存占用和计算成本，适用于批量推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:17:52 GMT</pubDate>
</item>
<item>
<title>MMLA基准：多模态语言理解能力的全面评估</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有多模态大模型在认知级语义理解上存在局限性。</p><br /><br /><p><strong>摘要：</strong> 多模态语言分析是理解人类会话语义的重要领域，但当前多模态大型语言模型（MLLMs）在认知级语义理解方面的能力尚待深入研究。本文介绍了一个名为MMLA的新基准，该基准包含超过61,000个多模态语句，涵盖意图、情感、对话行为、情感倾向、说话风格及沟通行为六大维度。通过零样本推理、监督微调和指令微调三种方法对主流LLMs和MLLMs进行评估后发现，即使经过微调的模型在复杂人类语言理解上的准确率也仅在60%-70%之间，凸显了现有MLLMs的局限性。MMLA有望成为探索大语言模型潜力的重要基础，并为推动多模态语言分析领域的发展提供宝贵资源。相关数据集和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 01:25:13 GMT</pubDate>
</item>
<item>
<title>CameraBench：评估与提升摄像机运动理解的数据集与基准</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CameraBench数据集，用于评估和改进摄像机运动理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CameraBench的大规模数据集和基准，该数据集由约3000段互联网视频组成，经过专家的多阶段质量控制标注而成。我们与电影摄影师合作开发了一种摄像机运动基元的分类法，发现某些运动如“跟踪”需要理解场景内容。通过大规模的人类研究量化了人类标注性能，发现领域专业知识和教程培训可以显著提高准确性。使用CameraBench，我们评估了运动恢复结构（SfM）模型和视频-语言模型（VLMs），发现SfM模型难以捕捉依赖场景内容的语义基元，而VLMs则难以捕捉需要精确轨迹估计的几何基元。我们进一步在一个生成式VLM上进行微调，使其兼具两者优势，并展示了其在运动增强的字幕生成、视频问答和视频-文本检索中的应用。我们希望此分类法、基准和教程能够推动未来对视频中摄像机运动理解的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 14:34:57 GMT</pubDate>
</item>
<item>
<title>基于Dual Consistency SAM方法的上下文分割研究</title>
<link>https://arxiv.org/abs/2504.12080</link>
<guid>https://arxiv.org/abs/2504.12080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于SAM的上下文图像及视频分割方法DC-SAM。</p><br /><br /><p><strong>摘要：</strong> 上下文分割（In-context Segmentation）作为小样本学习中的单次分割任务，探索了分割模型的泛化能力，被应用于场景理解和图像/视频编辑等任务。然而，现有的Segment Anything Models在交互式分割中表现优异，但不适用于上下文分割。本文提出Dual Consistency SAM (DC-SAM) 方法，通过提示调优适配SAM和SAM2进行图像和视频的上下文分割。该方法通过提供高质量视觉提示增强SAM提示编码器特征，并设计循环一致性交叉注意力和双分支设计提升性能。此外，还提出了掩码管训练策略以适应所提出的双重一致性方法。尽管DC-SAM主要针对图像设计，但在SAM2的支持下可无缝扩展到视频领域。由于视频领域的上下文分割尚属空白，我们手动整理并构建首个基准数据集IC-VOS，用于评估模型的上下文分割能力。实验结果显示，DC-SAM在COCO-20i上达到55.5 mIoU，在PASCAL-5i上达到73.0 mIoU，并在IC-VOS基准上获得71.52的J&amp;F分数。源代码和基准数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:41:59 GMT</pubDate>
</item>
<item>
<title>DynPose-100K：大规模动态互联网视频的相机姿态标注数据集</title>
<link>https://arxiv.org/abs/2504.17788</link>
<guid>https://arxiv.org/abs/2504.17788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种大规模动态视频相机姿态标注数据集DynPose-100K。</p><br /><br /><p><strong>摘要：</strong> 随着逼真的视频生成和模拟领域的发展，在动态互联网视频上大规模标注相机姿态变得至关重要。然而，由于大多数互联网视频不适合姿态估计，收集这样的数据集极具挑战性。本文介绍了一个名为DynPose-100K的大规模动态互联网视频数据集，其中包含相机姿态注释。为了克服数据收集的困难，我们设计了一个结合特定任务模型和通用模型的过滤管道。此外，通过整合最新的点跟踪、动态掩蔽和运动结构技术，我们在姿态估计方面实现了对现有方法的改进。实验分析表明，DynPose-100K在多个关键属性上具有大规模和多样性，为下游应用的进一步发展开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>DiMeR：一种用于稀疏视图网格重建的解耦双流前馈模型</title>
<link>https://arxiv.org/abs/2504.17670</link>
<guid>https://arxiv.org/abs/2504.17670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新型解耦双流模型DiMeR，显著提升了稀疏视图网格重建的效果。</p><br /><br /><p><strong>摘要：</strong> 随着大规模3D数据集的到来，前馈3D生成模型如大尺度重建模型（LRM）受到广泛关注并取得显著成功。然而，RGB图像往往导致训练目标冲突且缺乏几何重建所需的清晰度。本文重新审视了与网格重建相关的归纳偏差，引入了DiMeR，这是一种新颖的解耦双流前馈模型，用于稀疏视图网格重建。DiMeR通过将输入和框架分解为几何和纹理部分，减少了每个部分的训练难度。几何分支利用法线贴图作为输入，而纹理分支则使用RGB图像获取纹理网格。实验表明，DiMeR在多种任务中表现出色，并在GSO和OmniObject3D数据集上显著优于现有方法，Chamfer距离提升超过30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 11:39:20 GMT</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:44:54 GMT</pubDate>
</item>
<item>
<title>Token-Shuffle：通过维度重排提升基于Transformer的文本到高分辨率图像生成</title>
<link>https://arxiv.org/abs/2504.17789</link>
<guid>https://arxiv.org/abs/2504.17789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Token-Shuffle方法，大幅减少Transformer中的图像tokens数量，实现高效高分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对自回归（AR）模型在图像合成领域的局限性展开研究，主要问题在于其所需图像tokens数量庞大，影响训练和推理效率及图像分辨率。为此，我们提出了Token-Shuffle，一种新颖且简单的方法，通过在视觉编码器生成的低维视觉代码映射到语言词汇表时利用视觉词汇表的维度冗余性，减少输入tokens的数量。具体而言，Token-Shuffle通过在通道维度上合并空间局部tokens来降低输入tokens数量，而token-unshuffle则在Transformer块后解纠缠推断出的tokens，恢复空间排列以输出图像。该策略无需额外的预训练文本编码器，使多模态大语言模型（MLLMs）能够以统一的下一-token预测方式支持极高分辨率（如2048x2048）的图像生成，同时保持高效的训练和推理。实验表明，在GenAI基准测试中，我们的2.7B参数模型在困难提示下获得0.77的整体分数，优于LlamaGen 0.18分，超过LDM 0.15分。大规模的人类评估进一步证明了我们在文本对齐、视觉瑕疵和视觉外观方面的卓越能力。我们希望Token-Shuffle能成为高效高分辨率图像生成的基础设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>结合线性和非线性方法的新一代降维算法</title>
<link>https://arxiv.org/abs/2504.17601</link>
<guid>https://arxiv.org/abs/2504.17601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种结合线性和非线性特性的新型降维技术被提出。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的降维方法，旨在解决现有技术如t-SNE和PCA在表达能力和可解释性之间的权衡问题。该算法通过将高维空间映射到低维空间时结合线性变换和高斯加权的非线性变换，实现了复杂非线性转换的同时保持线性方法的可解释性。文章详细描述了这种架构的设计及其在几何关系保留上的优势，并提出了用于分析学习到的变换的技术，例如识别被抑制的维度以及空间如何被扩展和收缩的方法。此外，为了促进学术界和工业界的广泛应用，强调了开发用户友好型软件包的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 10:26:42 GMT</pubDate>
</item>
<item>
<title>RefVNLI：一种高效可靠的文本到图像生成评估方法</title>
<link>https://arxiv.org/abs/2504.17502</link>
<guid>https://arxiv.org/abs/2504.17502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法RefVNLI，用于同时评估文本对齐和主体一致性。</p><br /><br /><p><strong>摘要：</strong> 当前基于主体驱动的文本到图像生成技术在个性化图像生成和视频角色表现等领域有广泛应用潜力，但缺乏可靠的自动评估手段。现有评估方法通常仅关注任务的一个方面、与人类判断不一致或依赖昂贵的API评估。为解决这一问题，我们提出了RefVNLI，这是一种成本效益高的度量方法，能够在单一预测中评估文本对齐和主体保存。通过在大规模数据集上训练，RefVNLI在多个基准测试和主体类别中表现出色，相较于现有基线方法，在文本对齐和主体一致性方面分别提高了6.4和8.5个百分点。此外，它在处理较冷门概念时也表现出色，与人类偏好的一致性超过87%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 08:44:51 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的高质量视频试穿框架3DV-TON</title>
<link>https://arxiv.org/abs/2504.17414</link>
<guid>https://arxiv.org/abs/2504.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的扩散模型方法，解决复杂服装图案和多变人体姿态下的高质量视频试穿问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频试穿技术在处理复杂的服装图案和多样的人体姿势时，难以生成高质量且时间上一致的结果。本文介绍了一种名为3DV-TON的新框架，该框架基于扩散模型，旨在生成高保真度且时间一致的视频试穿效果。3DV-TON通过生成可动画化的纹理化3D网格作为显式的帧级指导，缓解了模型过于关注外观细节而忽略运动连贯性的问题。具体而言，该方法首先选择关键帧进行初始2D图像试穿，随后重建并同步原始视频姿态的纹理化3D网格。此外，我们引入了一种鲁棒的矩形掩码策略，成功减少了动态人体和服装运动过程中因服装信息泄漏导致的伪影传播。为了推动视频试穿研究的发展，我们还创建了一个名为HR-VVT的高分辨率基准数据集，包含130段具有多样服装类型和场景的视频。定量和定性结果表明，我们的方法在性能上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:12:40 GMT</pubDate>
</item>
<item>
<title>TimeChat-Online：革新实时视频交互的在线VideoLLM</title>
<link>https://arxiv.org/abs/2504.17343</link>
<guid>https://arxiv.org/abs/2504.17343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TimeChat-Online，解决流媒体视频冗余问题并实现高效实时互动。</p><br /><br /><p><strong>摘要：</strong> 随着在线视频平台特别是直播服务的快速发展，在线视频理解系统的需求日益迫切。然而，现有的VideoLLMs在处理完整视频时表现出色，但在流媒体场景下由于无法有效处理密集冗余帧而面临显著限制。本文介绍了一种名为TimeChat-Online的新型在线VideoLLM，它通过创新的Differential Token Drop (DTD)模块解决了流媒体视频中的视觉冗余问题。DTD模块受到人类视觉感知变化盲视现象的启发，能够在过滤冗余内容的同时保留有意义的时间变化。实验表明，DTD可以将视频令牌减少82.8%，同时在StreamingBench上保持98%的性能。此外，TimeChat-Online-139K数据集支持多种交互模式，包括后向追踪、当前感知和未来响应。TimeChat-Online的独特主动响应能力也使其在流媒体基准测试和长视频任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 03:59:46 GMT</pubDate>
</item>
<item>
<title>PaperCoder：基于多智能体大语言模型的论文代码自动生成框架</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PaperCoder通过多阶段生成高质量机器学习论文代码实现。</p><br /><br /><p><strong>摘要：</strong> 尽管机器学习研究快速发展，但相关代码实现往往不可用，导致研究人员难以复现结果或扩展先前工作。近期大型语言模型（LLMs）在理解科学文档和生成高质量代码方面表现出色。受此启发，我们提出了PaperCoder，这是一种多智能体LLM框架，可将机器学习论文转化为功能完善的代码库。PaperCoder分为三个阶段：规划阶段构建高层次路线图、设计系统架构并生成配置文件；分析阶段专注于解析特定实现细节；生成阶段则产生模块化且依赖感知的代码。每个阶段均由专门设计的智能体协作完成。我们在基于模型和人工评估的基础上对PaperCoder进行了评估，以机器学习论文为基础生成代码实现，同时以作者发布的代码库作为真实数据。结果显示，PaperCoder生成的代码质量高且忠实于原论文，尤其在新发布的PaperBench基准测试中表现优异，大幅超越了其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 21:57:01 GMT</pubDate>
</item>
<item>
<title>基于任意顺序补丁生成的自回归图像生成方法</title>
<link>https://arxiv.org/abs/2504.17069</link>
<guid>https://arxiv.org/abs/2504.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的自回归图像生成方法，通过训练模型在任意顺序下生成补丁，显著提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 自回归补丁式图像生成在图像质量和可扩展性方面表现出色，且易于整合到视觉语言模型中。然而，这类模型需要定义补丁生成的顺序，传统上采用从左上角到右下角的光栅扫描顺序。本文指出这种顺序并非最优，因为它无法尊重图像内容的因果关系。为此，我们首先训练模型能够在任意顺序下生成补丁，并在生成过程中推断每个补丁的内容及其位置。其次，利用这些提取出的顺序对任意顺序模型进行微调，从而生成更高质量的图像。实验结果显示，新方法在两个数据集上的生成效果优于传统的光栅扫描方法，且训练成本相近，无需额外标注。关键词：自回归生成、补丁生成、图像质量优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 15:33:58 GMT</pubDate>
</item>
<item>
<title>DyMU：一种高效且无需训练的视觉-语言模型动态计算降低框架</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DyMU框架，通过动态合并视觉标记实现VLM高效计算。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DyMU的高效、无需训练的框架，用于动态减少视觉-语言模型（VLMs）的计算负担，同时保持高任务性能。该方法包含两个关键组件：动态标记合并（DToMe）通过基于图像复杂度合并相似标记来减少视觉标记嵌入的数量；虚拟标记拆分（VTU）则通过高效重建完整序列的注意力动态来模拟大型语言模型的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与现有方法不同，DyMU根据图像内容动态调整标记压缩，且完全无需训练，可轻松应用于大多数最先进的VLM架构。实验表明，DyMU在多种VLM架构上平均减少了32%-85%的视觉标记数量，同时保持了与全长度模型相当的性能。此外，定性分析显示，DToMe可根据图像复杂度有效调整标记减少量，为用户提供更多对计算成本的控制权。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:38:18 GMT</pubDate>
</item>
<item>
<title>IberBench：评估伊比利亚半岛及伊比罗-美洲语言的大语言模型基准</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IberBench是一个针对伊比利亚半岛及伊比罗-美洲语言的大语言模型综合评估基准。</p><br /><br /><p><strong>摘要：</strong> 现有的大型语言模型（LLMs）评估基准主要集中在英语上，对于其他语言尤其是资源有限的语言，评估方法存在不足。这些基准通常忽视了语言多样性，过分关注基础自然语言处理（NLP）能力而忽略了工业相关的任务，并且缺乏动态更新机制。为解决这些问题，我们提出了IberBench，这是一个全面且可扩展的基准，用于评估伊比利亚半岛及伊比罗-美洲语言上的LLMs在基础和工业相关NLP任务中的表现。IberBench整合了来自评估活动和最新基准的101个数据集，涵盖22个任务类别，如情感分析、情绪分析、毒性检测和摘要生成等。此外，该基准通过支持持续更新和社区驱动的模型与数据集提交，解决了当前评估实践中存在的问题。我们对23个参数规模从1亿到140亿的LLMs进行了评估，并提供了关于其优势和局限性的实证见解。研究发现，LLMs在工业相关任务上的表现不如基础任务好，在加利西亚语和巴斯克语上的表现较低，某些任务的结果接近随机水平，而在其他任务上LLMs的表现高于随机但低于共享任务系统。IberBench提供了完整的开源实现，包括数据集标准化和托管、LLMs的增量评估以及公开的排行榜。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:48:25 GMT</pubDate>
</item>
<item>
<title>QuaDMix：统一框架优化大语言模型训练数据的质量与多样性</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuaDMix框架，同时优化大语言模型训练数据质量和多样性。</p><br /><br /><p><strong>摘要：</strong> 现有研究通常分别优化大语言模型（LLMs）训练数据的质量和多样性，忽视了两者之间的权衡关系。本文提出QuaDMix框架，通过统一的数据采样函数，在固定训练预算下平衡质量与多样性。QuaDMix利用多个指标衡量数据质量，并借助领域分类评估整体多样性。通过模拟实验和参数搜索加速框架优化，最终在多个基准测试中实现平均性能提升7.2%，显著优于独立优化策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16511" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 04:36:50 GMT</pubDate>
</item>
<item>
<title>一种结合表征学习与扩散模型的图像生成新框架</title>
<link>https://arxiv.org/abs/2504.16064</link>
<guid>https://arxiv.org/abs/2504.16064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将扩散模型与表征学习结合，提升图像生成质量和训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的图像生成框架，通过引入扩散模型同时建模低级图像潜在表示和高级语义特征，实现了表征学习与生成建模的无缝融合。该方法基于变分自编码器和预训练的自监督编码器，从纯噪声开始生成连贯的图像-特征对，在不显著改变标准扩散变换架构的情况下显著提升了生成质量与训练效率。此外，通过引入“表征引导”的推理策略，进一步增强了图像生成的可控性。实验表明，该方法在条件和无条件生成设置下均表现出色，为表征感知的生成建模开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:41:42 GMT</pubDate>
</item>
<item>
<title>ViSMap：基于元提示的无监督视频摘要技术</title>
<link>https://arxiv.org/abs/2504.15921</link>
<guid>https://arxiv.org/abs/2504.15921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViSMap通过元提示策略生成长视频伪摘要，实现无监督长视频摘要。</p><br /><br /><p><strong>摘要：</strong> ViSMap是一种无需标注的长视频摘要系统，解决了现有模型在稀疏分布事件长视频上的总结难题。传统方法依赖于昂贵的分层监督训练，而ViSMap利用LLMs根据短视频片段描述生成长视频的伪摘要，作为长视频摘要模型的训练数据。该系统采用元提示策略迭代生成并优化伪摘要，使用三个LLMs依次生成、评估和改进伪摘要，以提升质量。实验表明，ViSMap在多个数据集上表现优异，性能接近完全监督的最先进模型，且具有跨领域的泛化能力。代码将在发表后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 10:06:01 GMT</pubDate>
</item>
<item>
<title>Step1X-Edit：开源图像编辑模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.17761</link>
<guid>https://arxiv.org/abs/2504.17761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种开源图像编辑模型Step1X-Edit，性能接近闭源顶级模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像编辑模型发展迅猛，闭源模型如GPT-4o和Gemini2 Flash展现了强大的图像编辑能力。然而，现有开源算法与其存在显著差距。本文发布了一种名为Step1X-Edit的最新图像编辑模型，通过多模态大语言模型处理参考图像和用户指令，利用潜在嵌入与扩散图像解码器结合生成目标图像。为训练该模型，构建了高质量数据生成管道，并开发了基于真实用户指令的GEdit-Bench基准进行评估。实验结果显示，Step1X-Edit大幅超越现有开源基线，接近领先闭源模型的性能，为图像编辑领域做出了重要贡献。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:25:12 GMT</pubDate>
</item>
<item>
<title>UniME：基于多模态大语言模型的表征学习框架</title>
<link>https://arxiv.org/abs/2504.17432</link>
<guid>https://arxiv.org/abs/2504.17432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的两阶段框架UniME，提升多模态表示学习能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对比语言图像预训练（CLIP）框架存在文本截断、孤立编码及组合性不足等问题，限制了其效能。尽管多模态大型语言模型（MLLMs）在视觉-语言理解方面取得了显著进展，但其在迁移学习中的潜力尚未充分挖掘。本文介绍了一种名为UniME的新型两阶段框架，通过从强大的基于LLM的教师模型中进行文本辨别性知识蒸馏，增强MLLM的语言组件嵌入能力，并引入硬负样本增强指令微调进一步改进表征学习。实验结果显示，UniME在多个基准测试和检索任务中均表现出色，具有更强的辨别能力和组合性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:51:52 GMT</pubDate>
</item>
<item>
<title>基于抽象视角变化的视觉语言模型视角感知推理框架</title>
<link>https://arxiv.org/abs/2504.17207</link>
<guid>https://arxiv.org/abs/2504.17207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用心理意象模拟提升视觉语言模型视角感知能力的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个用于视觉语言模型(VLMs)视角感知推理的框架，名为抽象视角变化(APC)。该框架通过模拟心理意象，利用视觉基础模型如物体检测、分割和方向估计来构建场景抽象并实现视角转换，从而有效弥补现代VLMs在视角感知推理上的不足。实验结果显示，该框架在合成图像和真实图像基准测试中显著提升了视角感知推理能力，优于微调的空间推理模型和基于新视图合成的方法。这项研究对提高VLMs的人类水平视觉理解能力具有重要意义，有助于促进人机交互与协作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 22:41:34 GMT</pubDate>
</item>
<item>
<title>Causal-Copilot: An Autonomous Causal Analysis Agent</title>
<link>https://arxiv.org/abs/2504.13263</link>
<guid>https://arxiv.org/abs/2504.13263</guid>
<content:encoded><![CDATA[
Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 14:05:39 GMT</pubDate>
</item>
<item>
<title>构建顶级数学推理模型的方法与实践</title>
<link>https://arxiv.org/abs/2504.16891</link>
<guid>https://arxiv.org/abs/2504.16891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于大规模数据集和创新方法的数学推理模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了我们在AI数学奥林匹克竞赛(AIMO-2)中的获奖方案。我们的方法围绕三个关键支柱展开：首先，创建了一个包含54万独特高质量数学问题及其320万个长推理解决方案的大规模数据集；其次，开发了一种通过迭代训练、生成和质量过滤将代码执行与长推理模型集成的新方法，产生了170万个高质量工具集成推理解决方案；最后，构建了一个从多个候选者中挑选最有可能解决方案的流水线。实验表明，这种生成性解选(GenSelect)显著优于多数投票基线。结合这些技术，我们训练了一系列在数学推理基准测试中达到最先进的模型。为了促进进一步研究，我们以商业许可方式发布了代码、模型以及完整的OpenMathReasoning数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:13:04 GMT</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:53:29 GMT</pubDate>
</item>
<item>
<title>基于渐进语言引导视觉学习的多任务视觉定位框架</title>
<link>https://arxiv.org/abs/2504.16145</link>
<guid>https://arxiv.org/abs/2504.16145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外跨模态模块的多任务视觉定位框架PLVL。</p><br /><br /><p><strong>摘要：</strong> 本文针对多任务视觉定位(MTVG)中的子任务(指表达理解REC和分割RES)提出了渐进语言引导视觉学习框架PLVL。传统方法通过独立特征提取、跨模态交互及独立预测头实现任务处理，但存在语言信息注入不足及任务间关系未有效利用的问题。PLVL不仅深入挖掘视觉模态自身特性，还逐步引入语言信息以增强语言相关视觉特征的学习能力，避免了额外跨模态融合模块的需求。此外，通过分析REC定位中心对RES分割对象区域的潜在帮助，设计了多任务头以实现协作预测。实验表明，PLVL在多个基准数据集上显著优于现有代表性方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:48:12 GMT</pubDate>
</item>
<item>
<title>Tina：基于LoRA高效实现语言模型强推理能力的研究</title>
<link>https://arxiv.org/abs/2504.15777</link>
<guid>https://arxiv.org/abs/2504.15777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Tina模型家族，通过LoRA技术显著降低资源消耗实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何以低成本实现语言模型的强推理能力，提出了Tina这一高效推理模型家族。Tina通过对已有1.5B参数的小型基础模型应用低秩适应（LoRA）技术，在强化学习过程中进行参数高效更新，从而在极小资源投入下展现出卓越的推理性能。实验表明，最佳的Tina模型在AIME24数据集上实现了超过20%的推理性能提升及43.33%的Pass@1准确率，且仅需9美元的后训练与评估成本，较现有顶级模型减少了约260倍的成本。此外，Tina在多个开源推理数据集上的表现验证了其有效性，同时通过消融实验进一步支持了LoRA方法的优势。研究表明，LoRA能够快速使模型适应强化学习奖励的推理结构，同时保留基础模型的知识。为促进开放研究，所有代码、训练日志及模型权重均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:38:00 GMT</pubDate>
</item>
<item>
<title>RePOPE: Impact of Annotation Errors on the POPE Benchmark</title>
<link>https://arxiv.org/abs/2504.15707</link>
<guid>https://arxiv.org/abs/2504.15707</guid>
<content:encoded><![CDATA[
Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:47:59 GMT</pubDate>
</item>
<item>
<title>大型语言模型全栈安全综述</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次提出大型语言模型全生命周期安全概念。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在学术界和工业界的广泛应用，其安全性问题逐渐成为关注焦点。然而，现有研究多聚焦于特定阶段的安全性，缺乏对整个生命周期的全面分析。本文首次引入“全栈”安全的概念，系统探讨LLMs从训练到商业化过程中的安全挑战。通过综合分析超过800篇文献，我们定义了完整的LLMs生命周期，涵盖数据准备、预训练、后训练、部署及商业化的全过程，并提出了包括数据生成、对齐技术、模型编辑及基于LLM的代理系统在内的多个研究方向，为未来研究提供了宝贵的指导。这一工作不仅填补了现有研究的空白，还为构建更加安全可靠的LLMs提供了理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 01:02:49 GMT</pubDate>
</item>
<item>
<title>CRUST-Bench：评估C转Rust编译的基准数据集</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CRUST-Bench数据集，用于评估C代码转安全Rust的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CRUST-Bench，这是一个包含100个C代码仓库及其对应安全Rust接口和测试用例的数据集，旨在评估系统将C代码转换为符合规范且通过测试的安全Rust代码的能力。与仅关注孤立函数的传统方法不同，CRUST-Bench考虑整个项目及其多文件依赖关系，提供明确的Rust接口规格以保证内存安全性，并通过测试用例验证功能正确性。尽管现有最先进的大型语言模型（LLMs）在该任务上表现有限，但研究仍揭示了模型常见的错误类型。CRUST-Bench的成功应用有望推动复杂场景下C到Rust等内存安全语言迁移技术的发展。数据集及代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:33:33 GMT</pubDate>
</item>
<item>
<title>DreamID：基于扩散模型的高效高保真人脸交换技术</title>
<link>https://arxiv.org/abs/2504.14509</link>
<guid>https://arxiv.org/abs/2504.14509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的高保真人脸交换方法DreamID。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DreamID的基于扩散模型的人脸交换方法，该方法通过构建三元组身份组（Triplet ID Group）数据实现了对身份相似性和属性保留的显式监督，解决了传统隐式监督方法的局限性。此外，DreamID采用加速扩散模型SD Turbo实现单次迭代推理，显著提高了训练效率。研究还设计了一个由SwapNet、FaceNet和ID Adapter组成的改进架构，进一步增强了显式监督的效果。实验表明，DreamID在身份相似性、姿态表情保留和图像保真度方面超越了现有最先进方法，并且能在0.6秒内完成512*512分辨率的高质量人脸交换，尤其在复杂光照、大角度和遮挡等挑战场景下表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 02:53:00 GMT</pubDate>
</item>
<item>
<title>基于LLM自适应难度的高质量链式思维数据生成方法</title>
<link>https://arxiv.org/abs/2504.11919</link>
<guid>https://arxiv.org/abs/2504.11919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效生成高质量链式思维数据的方法，显著提升模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种通过构建LLM自适应难度问题数据库并利用DeepSeek-R1生成高质量链式思维(CoT)数据的方法。该方法降低了数据生成成本，提升了模型微调效率。实验验证了该方法在数学竞赛和代码生成任务中的有效性，其中ZMath-32B和ZCode-32B分别在仅使用2k高质量数据的情况下超越了DeepSeek-Distill-32B。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 05:55:34 GMT</pubDate>
</item>
<item>
<title>CheckboxQA：评估视觉语言模型处理复选框能力的新基准</title>
<link>https://arxiv.org/abs/2504.10419</link>
<guid>https://arxiv.org/abs/2504.10419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CheckboxQA 数据集用于提升视觉语言模型对复选框的理解能力。</p><br /><br /><p><strong>摘要：</strong> 在文档处理中，复选框的存在与否直接影响到数据提取和决策制定，然而现有大型视觉语言模型在这方面的表现并不理想。这种不足在法律和技术金融等对细节要求极高的行业中尤为突出。为解决这一问题，我们发布了 CheckboxQA 数据集，该数据集专门设计用于评估并改进模型在复选框相关任务上的性能。通过揭示当前模型的局限性，CheckboxQA 为推动文档理解系统的进步提供了宝贵的资源，在法律科技和金融等领域具有重要意义。数据集已公开发布于 GitHub 平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:06:59 GMT</pubDate>
</item>
<item>
<title>统一信息论框架下的现代机器学习损失函数</title>
<link>https://arxiv.org/abs/2504.16929</link>
<guid>https://arxiv.org/abs/2504.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信息论方程，统一多种现代机器学习损失函数。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于信息论的一般化方程，该方程能够涵盖大量现代机器学习中的损失函数。通过这一框架，我们揭示了几种类别的机器学习方法本质上是在最小化两个条件分布之间的集成KL散度。这种视角展示了聚类、谱方法、降维、对比学习及有监督学习背后隐藏的信息几何结构。此外，该框架促进了新损失函数的开发，并通过理论成果改进了无监督图像分类器，在ImageNet-1K上的表现提升了8%以上，同时提出了有效的去偏方法以优化对比表示学习器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>DreamO：一种支持多类型条件集成的图像定制框架</title>
<link>https://arxiv.org/abs/2504.16915</link>
<guid>https://arxiv.org/abs/2504.16915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DreamO框架，实现多种图像定制任务并灵活整合不同类型条件。</p><br /><br /><p><strong>摘要：</strong> 近年来，关于图像定制的研究展示了大规模生成模型的强大定制能力。然而，大多数方法针对特定任务设计，限制了其对多种条件组合的泛化能力。本文介绍DreamO，这是一种统一的图像定制框架，可支持广泛的任务并无缝集成多种条件。DreamO利用扩散Transformer（DiT）框架处理不同类型输入，并通过构建大规模训练集及引入特征路由约束来精确查询参考图像中的相关信息。此外，设计了占位符策略以控制生成结果中条件的位置。采用渐进式训练策略，分三个阶段提升定制能力并校正质量偏差。实验表明，DreamO在高质量完成定制任务的同时，具有很强的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:41:44 GMT</pubDate>
</item>
<item>
<title>Decoupled Global-Local Alignment框架提升视觉语言对齐模型的复合概念理解</title>
<link>https://arxiv.org/abs/2504.16801</link>
<guid>https://arxiv.org/abs/2504.16801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Decoupled Global-Local Alignment框架，提升CLIP的复合概念理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比学习方法在理解视觉语言复合概念时的局限性，提出了一种名为Decoupled Global-Local Alignment（DeGLA）的新框架。DeGLA通过引入局部对齐机制和自蒸馏策略，在增强模型对关系和属性等复合概念理解的同时，有效缓解了全局对比学习导致的泛化能力下降问题。实验结果显示，DeGLA在多个基准测试中表现优异，相较于现有最佳方法平均提升了3.5%，并在零样本分类任务中取得了13.0%的性能提升。此外，该框架还通过利用大语言模型生成高质量负样本及设计新的对比损失函数，进一步增强了模型的视觉语言对齐能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:20:53 GMT</pubDate>
</item>
<item>
<title>Pre-DPO：通过引导参考模型优化偏好强化学习</title>
<link>https://arxiv.org/abs/2504.15843</link>
<guid>https://arxiv.org/abs/2504.15843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DPO的Pre-DPO训练范式，通过参考模型优化大型语言模型的偏好强化学习。</p><br /><br /><p><strong>摘要：</strong> 直接偏好优化（DPO）简化了大型语言模型（LLMs）从人类反馈中的强化学习过程，但初始策略与参考模型相同的做法可能限制性能。本文提出Pre-DPO，通过引入指导参考模型增强偏好优化，该模型根据样本适配性动态分配权重，提高数据利用率和训练鲁棒性。实验表明，Pre-DPO在AlpacaEval 2.0和Arena-Hard v0.1基准测试中显著提升DPO和简单偏好优化（SimPO）的表现，且无需外部模型或额外数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:39:30 GMT</pubDate>
</item>
<item>
<title>Trillion-7B：高效韩文多语言大模型</title>
<link>https://arxiv.org/abs/2504.15431</link>
<guid>https://arxiv.org/abs/2504.15431</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Trillion-7B通过创新机制实现高效的跨语言知识迁移。</p><br /><br /><p><strong>摘要：</strong> Trillion-7B是一款专注于韩语的多语言大型语言模型，具有极高的token效率。该模型引入了交叉语言文档注意（XLDA）机制，能够有效且高效地将英语知识转移到目标语言如韩语和日语上。通过优化的数据混合、语言特定过滤及定制化分词器构建，Trillion-7B仅用其2万亿训练tokens中的10%进行多语言数据训练，并且仅需59.4K H100 GPU小时完成全部训练。全面评估显示，该模型在四种语言的27个基准测试中展现了强大的多语言性能和卓越的跨语言一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15431" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 16:54:44 GMT</pubDate>
</item>
<item>
<title>VisuLogic：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2504.15279</link>
<guid>https://arxiv.org/abs/2504.15279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisuLogic通过六类视觉推理问题评估多模态大模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大型语言模型（MLLMs）评估往往依赖文本描述并允许语言推理捷径，无法有效衡量真正的视觉中心推理能力。为了解决这一问题，本文提出了VisuLogic，这是一个包含1000个人类验证问题的基准，涵盖了六个类别，如定量变化、空间关系和属性比较。通过在该基准上的评估，发现大多数模型的准确率低于30%，远低于人类的51.4%和随机基线的25%，揭示了这些模型在视觉推理方面存在显著差距。此外，研究还提供了补充训练数据集和强化学习基线以促进进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在处理遮挡模式计数中的挑战与局限性</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入新任务CAPTURe测试视觉语言模型对遮挡物体的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Counting Amodally for Patterns Through Unseen REgions (CAPTURe)的新任务，旨在评估视觉语言模型(VLMs)在面对遮挡物体时的推理和空间理解能力。该任务要求模型通过推断被遮挡区域后的模式来完成计数。CAPTURe分为两部分：CAPTURe-real使用真实物体图像，CAPTURe-synthetic则使用生成的图像。实验评估了四个强大的VLMs(GPT-4o、Intern-VL2、Molmo和Qwen2-VL)，发现这些模型在处理遮挡和非遮挡模式时均表现不佳，尤其是在遮挡情况下表现更差。人类的表现远优于模型，提供遮挡物体位置的辅助信息可提高性能，表明模型在处理遮挡和计数方面都存在困难。这项研究揭示了当前VLMs在理解遮挡模式和空间关系上的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 19:38:43 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的个性化图像合成研究</title>
<link>https://arxiv.org/abs/2504.13162</link>
<guid>https://arxiv.org/abs/2504.13162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示自回归模型在文本转图像生成中的潜力。</p><br /><br /><p><strong>摘要：</strong> 个性化图像合成作为文本到图像生成的重要应用，近年来受到广泛关注。尽管扩散模型在此领域占据主导地位，但自回归模型因其统一的文本和图像建模架构，在个性化图像生成方面尚未得到充分探索。本文提出了一种两阶段训练策略，通过优化文本嵌入和微调Transformer层，使自回归模型在个性化图像生成任务上达到了与领先扩散模型相当的效果。实验表明，这种策略在主体保真度和指令遵循能力方面表现优异，为未来的研究提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:26 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化大型语言模型的决策能力</title>
<link>https://arxiv.org/abs/2504.16078</link>
<guid>https://arxiv.org/abs/2504.16078</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示通过强化学习微调可显著提升大型语言模型的决策能力和探索效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在决策场景中的次优表现问题，分析了贪婪性、频率偏差及知识应用差距等三种常见失败模式。研究表明，利用自我生成的链式思维推理进行基于强化学习的微调可以有效缓解这些问题，从而增强模型的探索能力和行动效果。实验分别在多臂老虎机、上下文老虎机和井字棋等任务中验证了该方法的有效性。此外，文章还研究了经典探索机制如ε-贪心策略以及大型语言模型特有的自我校正和一致性方法，以进一步优化模型的决策性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16078" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:57:14 GMT</pubDate>
</item>
<item>
<title>IPBench：推动大语言模型在知识产权领域的应用评估</title>
<link>https://arxiv.org/abs/2504.15524</link>
<guid>https://arxiv.org/abs/2504.15524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个涵盖知识产权多机制与任务的综合基准IPBench。</p><br /><br /><p><strong>摘要：</strong> 知识产权领域因其技术与法律知识的高度融合而复杂且知识密集，现有处理知识产权任务的大语言模型虽潜力巨大，但相关数据集与基准存在局限性，未能完全契合实际场景需求。为解决这一问题，本文引入了首个全面的知识产权任务分类体系及名为IPBench的大型双语基准，该基准覆盖8种知识产权机制及20项任务，旨在真实反映知识产权应用场景下的理解与生成能力。通过评估16种不同类型的大型语言模型，我们发现即便是表现最佳的模型，在IPBench上的准确率也仅为75.8%，表明仍有较大提升空间。此外，开源知识产权与法律导向模型的表现明显落后于闭源通用模型。为了促进进一步研究，我们公开了IPBench的所有数据与代码，并计划持续更新以更好地模拟知识产权领域的实际挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 22:00:41 GMT</pubDate>
</item>
<item>
<title>高效整合新语言到大型语言模型的方法研究</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可将新语言融入现有大模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新方法，用于将新的目标语言（阿拉伯语）无缝集成到现有的大型语言模型（LLM）中，而不会影响其原有的知识。通过在一个主要基于英语的小型开源模型中注入阿拉伯语数据，开发出参数量为15亿的Kuwain模型，该模型在多种基准测试中的阿拉伯语性能平均提升了8%，同时仅需少量原始模型的数据即可保留其原有知识。这种方法不仅显著降低了多语言模型开发的成本，还展示了高效扩展语言模型的潜力，避免了资源密集型的全面重新训练过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:17:25 GMT</pubDate>
</item>
<item>
<title>DiffVox：一种可微分的音乐人声效果匹配模型</title>
<link>https://arxiv.org/abs/2504.14735</link>
<guid>https://arxiv.org/abs/2504.14735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型可解释的人声效果匹配模型DiffVox。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DiffVox（“可微分人声效果”）的新模型，用于音乐制作中的声效匹配。该模型结合了参数均衡、动态范围控制、延迟和混响等模块，并通过高效的不同iable实现支持基于梯度的参数优化。研究利用MedleyDB和私人收藏的数据集中的435首歌曲进行分析，揭示了声效参数间的强相关性及与McAdams音色维度的联系。统计测试表明参数分布非高斯特性，突显了人声效果空间的复杂性。这些初步发现为未来人声效果建模和自动混音奠定了基础。相关代码和数据集可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 16:52:58 GMT</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>MR. Video：基于MapReduce原理的长视频理解框架</title>
<link>https://arxiv.org/abs/2504.16082</link>
<guid>https://arxiv.org/abs/2504.16082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于MapReduce原则的长视频理解框架MR. Video。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MR. Video的长视频理解框架，它通过Map（独立密集感知短视频片段）和Reduce（联合聚合所有片段信息）两个步骤，有效解决了传统序列到序列视觉语言模型在处理长视频时上下文长度受限的问题。与依赖关键片段选择的现有视频代理相比，MR. Video的Map操作实现了更简单的并行感知，而Reduce则支持更全面的上下文聚合和推理。该框架在LVBench挑战数据集上比最先进的视觉语言模型和视频代理提高了超过10%的准确性。此外，MR. Video还展示了其在短视频字幕生成和问题分析中的应用效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>ReflectionFlow：基于推理时自省的文本到图像扩散模型优化框架</title>
<link>https://arxiv.org/abs/2504.16080</link>
<guid>https://arxiv.org/abs/2504.16080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ReflectionFlow，通过推理时自省提升文本到图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReflectionFlow的新框架，该框架旨在解决现有文本到图像扩散模型在处理复杂场景和精细细节时表现不佳的问题。受大型语言模型中涌现的自我反思能力启发，ReflectionFlow在推理阶段引入三个互补的扩展轴：噪声级扩展、提示级扩展以及创新性的反射级扩展，后者通过提供可操作的反馈来评估并修正之前的生成结果。为了支持反射级扩展，我们构建了一个包含一百万个三元组的大规模数据集GenRef，每个三元组包括一条反馈、一张有缺陷的图像和一张增强后的图像。利用此数据集，我们在最先进的扩散变换器FLUX.1-dev上高效执行了反射调优。实验结果表明，ReflectionFlow显著优于朴素的噪声级扩展方法，在具有挑战性的任务中提供了可扩展且计算高效的高质量图像合成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Describe Anything Model：实现图像和视频的精细化局部描述</title>
<link>https://arxiv.org/abs/2504.16072</link>
<guid>https://arxiv.org/abs/2504.16072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种用于图像和视频精细化局部描述的新模型DAM。</p><br /><br /><p><strong>摘要：</strong> 描述特定区域内的图像和视频细节一直是视觉语言模型面临的基本挑战。本文介绍了一种名为DAM（Describe Anything Model）的模型，专门用于精细化局部描述（DLC）。DAM通过两个关键创新保持局部细节和全局上下文：焦点提示确保目标区域的高分辨率编码，局部视觉主干将精确的定位与其更广泛的上下文相结合。为了解决高质量DLC数据稀缺的问题，我们提出了基于半监督学习的数据管道（DLC-SDP），该管道从现有的分割数据集开始，并扩展到未标记的网络图像。此外，我们还引入了DLC-Bench基准，用于评估DLC而不依赖参考标题。DAM在七个涵盖关键词级、短语级和详细的多句局部图像和视频描述基准上创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:51:41 GMT</pubDate>
</item>
<item>
<title>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</title>
<link>https://arxiv.org/abs/2504.16030</link>
<guid>https://arxiv.org/abs/2504.16030</guid>
<content:encoded><![CDATA[
Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 12:52:09 GMT</pubDate>
</item>
<item>
<title>通过神经符号世界模型提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.15785</link>
<guid>https://arxiv.org/abs/2504.15785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的世界对齐方法提升大型语言模型作为世界模型的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）能否构建出准确的世界模型，并分析了世界模型如何增强LLM代理的表现。主要瓶颈在于LLMs的先验知识与特定环境动态之间的差距。为此，我们提出了无需训练的“世界对齐”方法，通过从探索轨迹中提取并编码动作规则、知识图谱及场景图等符号知识，来补充LLMs的不足。进一步地，我们设计了一个无需强化学习的基于模型的代理WALL-E 2.0，利用模型预测控制框架，采用LLM代理作为高效前瞻优化器，显著提升了新环境中的学习效率。在Mars（类似Minecraft）和ALFWorld（具身室内环境）的开放世界挑战中，WALL-E 2.0在成功率和得分上均大幅超越现有方法，特别是在Mars环境中成功率提升16.1%-51.6%，而在ALFWorld中仅需四轮迭代就达到了98%的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:58:27 GMT</pubDate>
</item>
<item>
<title>Vidi：面向视频编辑的大规模多模态模型</title>
<link>https://arxiv.org/abs/2504.15681</link>
<guid>https://arxiv.org/abs/2504.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vidi模型在长时间视频的时间检索任务上显著优于GPT-4o和Gemini。</p><br /><br /><p><strong>摘要：</strong> 随着互联网上视频成为主要的沟通和表达方式，高质量大规模视频内容的制作需求日益增长。然而，传统模型在处理多模态数据（如视觉、音频、文本）以及灵活输入长度时面临挑战。本文介绍了一种名为Vidi的大规模多模态模型家族，专门针对视频理解与编辑场景。Vidi的第一个版本专注于时间检索任务，即根据文本查询识别输入视频中的时间范围。该模型能够在长达数小时的视频中表现出强大的时间理解能力。为了支持真实场景的全面评估，研究团队还推出了VUE-TR基准，具有视频时长更长、支持音频查询、多样化查询格式、高质量标注及改进的IoU评价指标等五大优势。实验结果显示，Vidi在时间检索任务上显著超越了领先的专有模型如GPT-4o和Gemini，展示了其在视频编辑领域的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:04:45 GMT</pubDate>
</item>
<item>
<title>多语言基准评估的现状与未来：挑战与改进建议</title>
<link>https://arxiv.org/abs/2504.15521</link>
<guid>https://arxiv.org/abs/2504.15521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语在多语言基准中过度代表，提出创建本地化基准的重要性。</p><br /><br /><p><strong>摘要：</strong> 本文基于对2021年至2024年间来自148个国家超过2000个多语言（非英语）基准的分析，探讨了当前多语言基准评估的实践及其不足之处。尽管投入了大量资金，但英语仍占据显著优势，且多数基准源自高资源国家。此外，STEM相关任务与人类评价高度相关，而传统NLP任务的相关性较低。翻译基准不足以反映实际应用需求，本地化基准表现更优。研究指出了当前评估中的六大局限，并提出了指导原则及五大研究方向。最后，呼吁全球合作开发更符合人类需求的应用导向型基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 21:47:37 GMT</pubDate>
</item>
<item>
<title>自适应并行推理框架提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架APR，解决现有推理方法的效率与性能瓶颈。</p><br /><br /><p><strong>摘要：</strong> 现有语言模型推理方法存在显著局限性：序列化推理生成过长输出导致延迟增加及上下文窗口耗尽；而并行推理如自一致性则因协调不足产生冗余计算和有限性能提升。为此，本文提出自适应并行推理（APR），一种端到端推理框架，可整合序列化与并行计算。APR通过spawn()和join()操作实现多线程推理，创新性采用端到端强化学习策略优化推理过程。实验表明，在Countdown推理任务中，APR相比传统方法在相同上下文窗口内表现更优（83.4% vs. 60.0%），扩展性更强且在同等延迟下精度更高（75.2% vs. 57.3%）。APR标志着语言模型自主优化推理过程的新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15466" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 18:29:02 GMT</pubDate>
</item>
<item>
<title>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.15415</link>
<guid>https://arxiv.org/abs/2504.15415</guid>
<content:encoded><![CDATA[
Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 15:53:44 GMT</pubDate>
</item>
<item>
<title>基于平行隐藏解码的高效长度缩放预训练框架</title>
<link>https://arxiv.org/abs/2504.14992</link>
<guid>https://arxiv.org/abs/2504.14992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型框架PHD-Transformer，实现预训练中的高效长度缩放。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的研究表明，在后训练阶段通过长度缩放可以显著提升性能，但这一方法在预训练阶段的应用尚待深入探索。本文介绍了一种名为Parallel Hidden Decoding Transformer（PHD-Transformer）的新框架，该框架能够在预训练期间实现高效的长度缩放，同时保持推理效率。通过引入创新的KV缓存管理策略，PHD-Transformer能够区分原始令牌和隐藏解码令牌，仅保留原始令牌的KV缓存以维持长距离依赖关系，而立即丢弃已使用的隐藏解码令牌，从而在不增加KV缓存大小的情况下实现有效扩展。此外，还提出了两种优化变体：PHD-SWA利用滑动窗口注意力机制保存局部依赖关系；PHD-CSWA则采用分块滑动窗口注意力机制消除预填充时间的线性增长。实验结果表明，该方法在多个基准测试中均表现出一致的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:41:26 GMT</pubDate>
</item>
<item>
<title>基于DiT模型的可控角色动画生成方法</title>
<link>https://arxiv.org/abs/2504.14977</link>
<guid>https://arxiv.org/abs/2504.14977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过基础模型微调解决罕见姿态等挑战的新视角。</p><br /><br /><p><strong>摘要：</strong> 可控角色动画在处理罕见姿态、风格化角色及复杂场景时面临诸多难题，传统方法主要依赖复杂的旁路网络注入姿态与外观指导，但泛化能力有限。本文提出新方法，认为只要基础模型足够强大，通过简单修改并采用灵活微调策略即可有效应对上述挑战。文中介绍的RealisDance-DiT基于Wan-2.1视频基础模型构建，实验表明其性能显著优于现有方法。此外，我们还设计了新的测试数据集，涵盖多样化的现实世界挑战，补充了现有基准数据集，进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:09:21 GMT</pubDate>
</item>
<item>
<title>BookWorld：基于书籍的多智能体社会构建与模拟系统</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BookWorld系统，用于构建和模拟基于书籍的多智能体社会。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步推动了通过多智能体系统进行社会模拟的发展，但对已建立的虚构世界和角色的模拟研究较少。本文介绍BookWorld系统，该系统能够全面构建和模拟基于书籍的多智能体社会，涵盖复杂现实细节如多样化动态角色、世界观、地理约束等。实验表明，BookWorld生成的故事具有创意且质量高，忠实于原作，胜过前人方法。BookWorld还可应用于故事生成、互动游戏和社会模拟等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 04:56:27 GMT</pubDate>
</item>
<item>
<title>CheXWorld：基于自监督学习的胸部X光影像世界模型</title>
<link>https://arxiv.org/abs/2504.13820</link>
<guid>https://arxiv.org/abs/2504.13820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个针对放射影像的自监督世界模型CheXWorld。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CheXWorld，这是首个用于放射影像的自监督世界模型，旨在捕捉医学知识的三个关键方面：局部解剖结构、全局解剖布局及领域变化。通过统一框架同时建模这三个方面，实验表明CheXWorld不仅成功捕捉了这些医学知识维度，还在八个医学图像分类和分割基准上显著优于现有自监督学习方法和大规模医学基础模型。代码和预训练模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:50:43 GMT</pubDate>
</item>
<item>
<title>Progent：一种用于大型语言模型代理的权限控制机制</title>
<link>https://arxiv.org/abs/2504.11703</link>
<guid>https://arxiv.org/abs/2504.11703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Progent通过灵活的权限策略提高大型语言模型代理的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Progent的新方法，这是首个面向大型语言模型（LLM）代理的权限控制机制。Progent的核心是一种领域特定语言，用于在代理执行期间灵活表达权限控制策略。这些策略对工具调用施加细粒度限制，决定哪些工具调用是允许的，并指定回退方案。这种设计使得代理开发者和用户可以根据具体用例创建并确定性地应用适当的策略，从而保证安全性。由于其模块化设计，集成Progent不会改变代理内部结构，只需对代理实现进行最小改动，这提升了其实用性和广泛采用的可能性。此外，我们利用LLM自动生成基于用户查询的策略，并动态更新以增强安全性和实用性。我们的广泛评估表明，Progent能够在三个不同的场景或基准测试中（AgentDojo、ASB和AgentPoison）提供强大的安全保障，同时保持高实用性。此外，我们还进行了深入分析，展示了其核心组件的有效性以及自动化策略生成在对抗适应性攻击时的弹性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 21:58:40 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化工具集成推理中的工具使用效率</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过奖励机制减少工具调用次数并提高工具生产力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过强化学习提升工具集成推理（TIR）的能力，尤其是在优化最终答案正确性的同时考虑工具使用的效率与成本。现有方法往往忽视了工具调用的数量及其带来的计算和财务负担，可能导致不理想的模型行为。为解决这一问题，我们提出了Optimal Tool Call-controlled Policy Optimization (OTC-PO)，这是一种基于强化学习的新框架，旨在鼓励模型以最少的工具调用生成准确的答案。该框架引入了一个综合考虑正确性和工具效率的奖励机制，从而提升了工具的生产力。我们将此框架应用于Proximal Policy Optimization (PPO) 和 Group Relative Preference Optimization (GRPO)，分别得到OTC-PPO和OTC-GRPO。实验结果显示，在多个问答基准测试中，我们的方法将工具调用次数减少了高达73.1%，同时提高了工具生产力达229.4%，且保持了相近的答案准确性。据我们所知，这是首个明确优化TIR中工具使用效率的强化学习框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 01:40:05 GMT</pubDate>
</item>
<item>
<title>推理模型过思考问题的研究与优化</title>
<link>https://arxiv.org/abs/2504.13367</link>
<guid>https://arxiv.org/abs/2504.13367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究推理模型过思考问题并提出THOUGHTTERMINATOR优化方法。</p><br /><br /><p><strong>摘要：</strong> 推理模型在处理传统语言模型难以应对的任务时表现出色，但普遍存在过思考问题，即生成大量不必要的标记，这不仅不提升准确性还浪费计算资源。本文引入问题难度的近似度量方法，揭示了问题难度与最优标记消耗之间的关系，并评估了几种推理模型在有效分配最优标记数量方面的校准程度。研究发现，大多数推理模型在校准方面表现不佳，尤其是在简单问题上。为此，我们构建了DUMB500数据集，包含极其简单的数学、推理、代码及任务问题，同时结合现有前沿基准中的极难问题，对推理模型进行综合评估。最后，我们提出了无需训练的黑盒解码技术THOUGHTTERMINATOR，显著改善了推理模型的校准性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 18:16:30 GMT</pubDate>
</item>
<item>
<title>RF-DETR与YOLOv12在复杂果园环境中的绿果检测对比研究</title>
<link>https://arxiv.org/abs/2504.13099</link>
<guid>https://arxiv.org/abs/2504.13099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RF-DETR在单类和多类绿果检测中均优于YOLOv12。</p><br /><br /><p><strong>摘要：</strong> 本研究对基于Transformer的RF-DETR和基于CNN的YOLOv12两种目标检测模型在复杂果园环境下的绿果检测性能进行了详细比较。通过构建包含单类和多类标注的自定义数据集，评估了模型在处理标签模糊、遮挡及背景融合等动态条件下的表现。RF-DETR凭借DINOv2骨干网络和可变形注意力机制，在全局上下文建模和局部特征提取方面表现出色，尤其在单类检测中取得了0.9464的mAP50高分；而YOLOv12则通过优化计算效率和边缘部署能力，在多类检测中达到了0.6622的mAP@50:95最佳分类效果。此外，RF-DETR展现了更快的收敛速度，尤其是在单类检测中仅需10个训练周期即可达到稳定状态。这些结果表明，RF-DETR更适合应用于精确农业场景，而YOLOv12则适用于需要快速响应的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:08:11 GMT</pubDate>
</item>
<item>
<title>基于语音交互的医学视觉语言模型SilVar-Med及其可解释性研究</title>
<link>https://arxiv.org/abs/2504.10642</link>
<guid>https://arxiv.org/abs/2504.10642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合语音交互的端到端医学视觉语言模型SilVar-Med。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SilVar-Med的端到端语音驱动的医学视觉语言模型（VLM），该模型通过整合语音交互功能，突破了传统文本指令限制，在临床环境中实现更实用的医疗图像分析辅助。此外，针对当前医学图像分析模型缺乏透明推理的问题，我们提出了一个用于异常预测解释的数据集，展示了基于推理的医学图像解释概念验证研究。实验表明，SilVar-Med在提升诊断支持系统的透明度、互动性和临床实用性方面具有重要意义。我们的代码和数据集已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 14:51:37 GMT</pubDate>
</item>
<item>
<title>StyleMe3D：实现3D高斯点云风格迁移的综合性框架</title>
<link>https://arxiv.org/abs/2504.15281</link>
<guid>https://arxiv.org/abs/2504.15281</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出StyleMe3D框架，解决3D高斯点云在风格化场景中的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为StyleMe3D的综合性框架，旨在解决3D Gaussian Splatting (3DGS) 在处理卡通、游戏等风格化场景时遇到的纹理碎片化、语义错位及适应抽象美学能力有限的问题。StyleMe3D通过多模态风格条件、多层级语义对齐和感知质量增强等技术，实现了几何结构完整性和风格一致性。该框架包含动态风格评分蒸馏(DSSD)、对比风格描述符(CSD)、同时优化尺度(SOS)以及可微分的3D高斯质量评估(3DG-QA)四个创新组件。实验表明，StyleMe3D在NeRF合成数据集和tandt db场景数据集上表现优异，不仅保留了几何细节，还确保了场景间的风格一致性，并且具备实时渲染能力，适用于游戏、虚拟世界和数字艺术等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15281" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>Eagle 2.5：面向长上下文多模态学习的前沿视觉语言模型</title>
<link>https://arxiv.org/abs/2504.15271</link>
<guid>https://arxiv.org/abs/2504.15271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的视觉语言模型Eagle 2.5，显著提升长视频和高分辨率图像的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组名为Eagle 2.5的前沿视觉语言模型（VLMs），专门针对长上下文多模态学习进行优化。这些模型解决了长视频理解和高分辨率图像分析中的挑战，提供了一个通用框架来同时处理这两种任务。研究中采用了自动降级采样和图像区域保留两项技术，以确保上下文完整性和视觉细节的保留。此外，还对长上下文数据训练的整个流程进行了多项效率优化。为了促进长视频理解，我们提出了一个新的数据集Eagle-Video-110K，该数据集结合了故事级和片段级标注。实验结果显示，Eagle 2.5在长上下文多模态基准测试中表现出色，其中最佳模型Eagle 2.5-8B在Video-MME测试中达到了72.4%的准确率，这一成绩与顶级商业模型如GPT-4o以及大规模开源模型如Qwen2.5-VL-72B和InternVL2.5-78B相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>Quicksviewer：基于动态时间密度划分的高效多模态模型</title>
<link>https://arxiv.org/abs/2504.15270</link>
<guid>https://arxiv.org/abs/2504.15270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多模态模型Quicksviewer，通过自适应时间密度划分实现视频高效理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Quicksviewer的大型多模态模型，该模型采用全新的感知范式，通过Gumbel Softmax将非均匀密度的视频分割成不同大小的立方体，并对每个立方体进行统一重采样，从而提高视频理解效率。这种方法能够在线动态压缩视频，显著减少时空冗余（总体压缩率可达45倍），同时支持大感受野下的高效训练。Quicksviewer通过三个渐进阶段进行训练，在平均420秒/1帧的长时间视频上表现出色。仅使用0.8M视频-文本样本进行训练，该模型在准确性上比采用固定划分策略的基线高出最多8.72分，同时在Video-MME基准测试中达到SOTA性能，且所需令牌数量仅为基线的5%。此外，实验验证了立方网络生成的片段有助于分析视频中的连续事件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:21 GMT</pubDate>
</item>
<item>
<title>基于推理的查询级元代理FlowReasoner自动化设计</title>
<link>https://arxiv.org/abs/2504.15257</link>
<guid>https://arxiv.org/abs/2504.15257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于推理的元代理FlowReasoner，显著提升多智能体系统设计性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlowReasoner的查询级元代理，旨在自动化设计针对每个用户查询的多智能体系统。核心理念是通过外部执行反馈激励基于推理的元代理。首先通过DeepSeek R1提炼出生成多智能体系统的初步推理能力，然后借助强化学习进一步优化，设计了多用途奖励函数从性能、复杂性和效率角度引导训练。实验表明，FlowReasoner在工程和竞赛代码基准测试中优于其他模型，在三个基准上比o1-mini提升了10.52%的准确率。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:35:42 GMT</pubDate>
</item>
<item>
<title>DRAGON框架：一种灵活的生成模型微调方法</title>
<link>https://arxiv.org/abs/2504.15217</link>
<guid>https://arxiv.org/abs/2504.15217</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRAGON是一种比传统RLHF更灵活的生成模型微调框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DRAGON（Distributional RewArds for Generative OptimizatioN），一种用于优化生成模型以实现特定目标的多功能框架。与传统的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）等方法相比，DRAGON更加灵活，能够处理个体示例或其分布的奖励函数评估，支持多种奖励类型。通过利用这一灵活性，研究者们设计了新颖的奖励函数，结合跨模态编码器如CLAP，并通过对比正负样本集最大化奖励。实验表明，DRAGON在多个目标奖励下平均胜率为81.45%，且无需依赖人类偏好标注即可达到较高的音乐质量。这项工作展示了如何通过设计和优化奖励函数来提升生成模型的人类感知质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15217" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 12:41:40 GMT</pubDate>
</item>
<item>
<title>EasyEdit2：支持大语言模型实时行为调控的新框架</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyEdit2 是一种新框架，可轻松实现大语言模型的行为控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为 EasyEdit2 的新框架，该框架旨在为大型语言模型（LLM）的行为控制提供插件式调节功能。与前代相比，EasyEdit2 引入了一种专门设计的新架构，用于平滑的模型操控。它包含多个关键模块，如操控向量生成器和应用器，允许用户通过生成并应用操控向量来影响模型行为，而无需修改模型参数。框架支持多种测试时干预，包括安全性、情感、人格、推理模式、事实性和语言特性等。此外，EasyEdit2 用户界面友好，仅需单个示例即可引导和调整模型响应，使精确控制变得简单高效。实验表明，该框架在多种 LLM 上均表现良好。源代码已公开于 GitHub，并附有演示笔记本和介绍视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:33:55 GMT</pubDate>
</item>
<item>
<title>RainbowPlus：基于进化计算的大语言模型红队框架</title>
<link>https://arxiv.org/abs/2504.15047</link>
<guid>https://arxiv.org/abs/2504.15047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RainbowPlus框架，显著提升大语言模型对抗性提示生成的效率与多样性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）表现出强大的能力，但容易受到对抗性提示的影响，从而产生不安全或有偏见的输出。现有红队方法通常面临可扩展性差、资源消耗高或攻击策略多样性不足的问题。本文提出RainbowPlus，这是一种基于进化计算的新红队框架，通过适应性的质量-多样性（QD）搜索算法改进对抗性提示生成。该框架采用多元素存档存储多样化的高质量提示，并使用综合适应度函数同时评估多个提示，从而克服了先前QD方法中的单一提示存档和成对比较的限制。实验表明，RainbowPlus在六个基准数据集和四种开源LLMs上的攻击成功率（ASR）和多样性均优于其他QD方法。此外，在HarmBench数据集上，RainbowPlus对十二个LLMs的平均ASR达到81.1%，比最先进的AutoDAN-Turbo高出3.9%，且运行速度快9倍。RainbowPlus的开源实现为LLM安全性评估提供了可扩展工具，促进了相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 08:04:57 GMT</pubDate>
</item>
<item>
<title>LUFFY框架：通过离线策略指导提升大规模推理模型的泛化能力</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入LUFFY框架，结合离线策略推理轨迹显著提升了数学基准测试中的性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，大规模推理模型可以通过简单的基于规则奖励的强化学习实现复杂的推理行为。然而，现有的零样本强化学习方法受限于仅能依赖自身输出进行学习，难以突破初始能力限制。为解决这一问题，我们提出了LUFFY框架，该框架通过整合离线推理轨迹扩展了零样本强化学习。LUFFY在训练过程中动态平衡模仿与探索，采用正则化的重要性采样进行策略塑造，避免了混合策略训练中的浅层模仿。实验表明，LUFFY在六个数学基准测试中平均提升了超过7.0分，在分布外任务中领先6.2分以上，且在泛化能力上显著优于监督微调方法。分析显示，LUFFY不仅有效模仿了现有知识，还实现了超出示范范围的探索，为大规模推理模型的可扩展训练提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 04:09:13 GMT</pubDate>
</item>
<item>
<title>Uni3C：统一3D增强框架实现视频生成中摄像机与人体运动的精确控制</title>
<link>https://arxiv.org/abs/2504.14899</link>
<guid>https://arxiv.org/abs/2504.14899</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架Uni3C，实现视频生成中摄像机与人体运动的同步精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成方法中摄像机与人体运动控制分离的问题，提出了Uni3C，这是一种基于3D增强的统一框架。Uni3C通过引入PCDController模块，利用未投影点云进行摄像机控制，该模块无需重新训练视频生成的基础模型，表现出强大的泛化能力。此外，Uni3C还设计了一种联合对齐的3D世界引导机制，在推理阶段整合场景点云和SMPL-X人物模型，实现摄像机与人体运动信号的统一控制。实验表明，Uni3C在摄像机可控性和人体动作质量方面均优于现有方法，并通过定制验证集进一步验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14899" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 03:10:41 GMT</pubDate>
</item>
<item>
<title>TAPIP3D：一种新颖的单目RGB和RGB-D视频中的长期3D点跟踪方法</title>
<link>https://arxiv.org/abs/2504.14717</link>
<guid>https://arxiv.org/abs/2504.14717</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TAPIP3D方法，通过三维空间特征云实现长期3D点跟踪。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TAPIP3D的新方法，用于在单目RGB和RGB-D视频中的长期3D点跟踪。该方法将视频表示为相机稳定的空间-时间特征云，并利用深度和相机运动信息将二维视频特征提升到三维世界空间中，从而有效消除相机运动影响。TAPIP3D在这一稳定表示中迭代优化多帧3D运动估计，实现了长时间的稳健跟踪。为了处理3D点分布的固有不规则性，我们提出了局部成对注意力机制，这种三维上下文策略有效地利用了空间关系，形成了精确的3D轨迹估计所需的有用特征邻域。实验结果显示，我们的方法显著优于现有的3D点跟踪方法，并且在有准确深度的情况下，甚至提高了2D跟踪精度。此外，该方法支持在相机坐标系和世界坐标系下的推理，并证明了补偿相机运动可以提高跟踪性能。相比之前的2D和3D跟踪器使用的传统2D方形相关邻域，TAPIP3D提供了更鲁棒和准确的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14717" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 15:09:43 GMT</pubDate>
</item>
<item>
<title>LeetCodeDataset：面向代码生成模型的高质量基准数据集</title>
<link>https://arxiv.org/abs/2504.14655</link>
<guid>https://arxiv.org/abs/2504.14655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出用于评估和训练代码生成模型的高质量基准数据集LeetCodeDataset。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LeetCodeDataset的新数据集，该数据集旨在应对LLM研究中的两个关键挑战：缺乏注重推理的编程基准和自包含的训练测试平台。通过精心整理具有丰富元数据的LeetCode Python问题，并提供每题超过100个测试用例及时间分割（2024年7月前/后），该数据集实现了无污染评估和高效的监督微调（SFT）。实验表明，具备推理能力的模型显著优于非推理模型，而仅使用2.6K模型生成解决方案的SFT表现可媲美拥有110K样本的数据。数据集及其评估框架已在Hugging Face和Github上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 11:28:16 GMT</pubDate>
</item>
<item>
<title>UFO2：面向Windows桌面的多智能体操作系统实现可靠自动化</title>
<link>https://arxiv.org/abs/2504.14603</link>
<guid>https://arxiv.org/abs/2504.14603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UFO2通过多智能体架构提升计算机使用代理的实用性。</p><br /><br /><p><strong>摘要：</strong> 近期基于多模态大语言模型的计算机使用代理（CUAs）为自然语言驱动的复杂桌面工作流自动化提供了新方向，但现有CUAs多为概念性原型，受制于浅层操作系统集成、脆弱的基于截图交互及干扰性的执行方式。本文提出UFO2，这是一种针对Windows桌面的多智能体操作系统，将CUAs转化为实用的系统级自动化工具。UFO2采用中心化的HostAgent进行任务分解与协调，并配备一系列应用专用AppAgent，具备原生API、领域特定知识及统一的图形用户界面-应用程序接口动作层。该架构不仅实现了稳健的任务执行，还保持了模块化与可扩展性。通过融合Windows UI自动化与基于视觉解析的混合控制检测管道，支持多样化的界面风格。此外，通过推测多动作规划优化运行效率，减少每步大语言模型开销。最后，嵌套式画中画界面允许代理与用户在隔离虚拟桌面中并发操作。实验评估显示，UFO2在超过20款真实Windows应用中显著提升了鲁棒性和执行准确性，证明了深度操作系统集成是实现可靠用户导向桌面自动化的可扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 09:04:43 GMT</pubDate>
</item>
<item>
<title>SphereDiff：一种无缝全景图像与视频生成方法</title>
<link>https://arxiv.org/abs/2504.14396</link>
<guid>https://arxiv.org/abs/2504.14396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的新方法，解决全景图像生成中的极点失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SphereDiff的创新方法，用于通过最先进的扩散模型生成无缝的360度全景图像和视频。传统方法因等距矩形投影(ERP)引起的严重畸变面临挑战，而SphereDiff定义了一个球面潜在表示，确保所有视角下均匀分布，从而减轻ERP固有的畸变。该方法扩展了多扩散机制至球面潜在空间，并提出了球面潜在采样方法，使预训练扩散模型能够直接应用。此外，引入了畸变感知加权平均技术，进一步提升投影过程中的生成质量。实验表明，SphereDiff在生成高质量全景内容方面优于现有方法，适用于增强现实/虚拟现实(AR/VR)等沉浸式应用。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 15:59:11 GMT</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 05:25:55 GMT</pubDate>
</item>
<item>
<title>通过人机演示提升移动GUI代理性能</title>
<link>https://arxiv.org/abs/2504.13805</link>
<guid>https://arxiv.org/abs/2504.13805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人演示的学习方法，显著提高移动GUI代理在多样化场景中的性能。</p><br /><br /><p><strong>摘要：</strong> 移动图形用户界面（GUI）代理在自动化任务方面展现出潜力，但面临现实世界多样场景中泛化能力不足的问题。传统方法通过大规模数据集进行预训练或微调难以应对移动应用及用户特定任务的多样性。本文提出通过人类演示增强移动GUI代理能力，专注于提升其在未见过场景中的表现而非追求更大数据集的普遍泛化。为此，我们引入LearnGUI，这是首个专门用于研究基于演示学习的移动GUI代理的综合数据集，包含2,252个离线任务和101个在线任务的人类高质量演示。同时，开发了LearnAct，这是一种复杂的多智能体框架，可自动从演示中提取知识以增强任务完成。该框架结合了三个专业化智能体：DemoParser用于知识提取，KnowSeeker用于相关知识检索，ActExecutor用于演示增强的任务执行。实验结果显示，在离线和在线评估中均取得了显著性能提升。例如，在离线评估中，单个演示使Gemini-1.5-Pro的准确性从19.3%提高到51.7%；在线评估中，框架将UI-TARS-7B-SFT的任务成功率从18.1%提高到32.8%。LearnAct框架和LearnGUI基准确立了基于演示学习作为更适应性、个性化且可部署的移动GUI代理的有前景方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:13:34 GMT</pubDate>
</item>
<item>
<title>强化学习中工具使用奖励设计的研究与实践</title>
<link>https://arxiv.org/abs/2504.13958</link>
<guid>https://arxiv.org/abs/2504.13958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出强化学习中工具使用任务的奖励设计方案，显著提升大语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型(LLMs)通过有监督微调(SFT)获取工具使用能力，但难以泛化到复杂场景。强化学习(RL)的最新进展显示了优秀的推理和泛化能力，但在工具使用奖励设计上面临挑战，如多种工具参数及反馈不足问题。本研究首次系统性探索强化学习框架下的工具选择和应用任务奖励策略，分析其类型、尺度、粒度及时序动态，提出面向工具使用的奖励设计方案并结合分组相对策略优化(GRPO)训练模型。实验表明该方法比基础模型提高17%，比SFT模型提高15%，强调了奖励设计对提升LLMs工具使用能力和泛化性能的重要性。所有代码已公开以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 17:45:32 GMT</pubDate>
</item>
<item>
<title>基于单目摄像机的多人3D姿态检测与跟踪方法</title>
<link>https://arxiv.org/abs/2504.12186</link>
<guid>https://arxiv.org/abs/2504.12186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种从单目视频流中检测与跟踪多人3D姿态的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种利用单目摄像机流检测并追踪多人详细3D姿势的方法。系统能够在拥挤场景中处理复杂姿势和遮挡情况，维持时间上连贯的预测。该模型不仅能在每帧进行强检测，还通过学习的姿态更新机制实现跨帧追踪。与传统匹配检测不同，该方法直接从新输入图像更新姿态，支持在线追踪。通过使用大量伪标签标注的数据集训练，模型在3D姿态估计准确性方面达到领先水平，同时在多目标长时间追踪中表现更快更准确。代码和权重可在指定GitHub仓库获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>NEMOTRON-CROSSTHINK：强化学习框架提升大语言模型跨领域推理能力</title>
<link>https://arxiv.org/abs/2504.13941</link>
<guid>https://arxiv.org/abs/2504.13941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NEMOTRON-CROSSTHINK框架，通过多领域数据增强强化学习训练，显著提高大语言模型在数学及非数学推理任务中的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在推理任务中表现出色，特别是在数学推理领域。然而，将其推广到其他复杂且多样化的推理任务时面临诸多挑战，例如有限的数据量、缺乏可验证的奖励结构以及任务需求的多样性。为解决这些问题，本文提出了NEMOTRON-CROSSTHINK框架，该框架通过整合来自科学、技术、工程、数学（STEM）、人文学科和社会科学等多个领域的合成和真实问题-答案对，增强了强化学习的训练过程。具体而言，NEMOTRON-CROSSTHINK通过采用多选题和开放性问题模板控制答案空间复杂度，筛选可验证的答案，并优化多源数据融合策略，从而实现对数学及非数学推理任务的高效泛化。实验结果显示，该方法不仅在数学推理任务（如MATH-500和AMC23）上取得了显著的性能提升（分别提升了30.1%和27.5%），还在非数学推理任务（如MMLU-PRO、GPQA-DIAMOND、AGIEVAL和SUPERGPQA）上实现了11.3%至15.1%的准确率增长。此外，NEMOTRON-CROSSTHINK还大幅提高了响应效率，在正确回答问题时所需令牌数量减少了28%，表明其推理更加集中和高效。总之，这一研究证明了在强化学习中引入多领域、多格式数据的重要性，为构建更精确、高效且通用的大语言模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 17:37:13 GMT</pubDate>
</item>
<item>
<title>X-Teaming：一种高效的多轮对话语言模型攻击框架</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出X-Teaming框架，显著提升多轮对话中语言模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多轮对话中语言模型存在的安全风险展开研究，现有工作主要集中在单轮安全性，而多轮对抗测试仍面临适应性和多样性挑战。为解决这些问题，我们提出了X-Teaming框架，通过协作代理进行规划、攻击优化和验证，成功率达到98.1%，尤其对Claude 3.7 Sonnet等先进模型也实现了高达96.2%的成功率。此外，基于X-Teaming，我们开发了XGuard-Train开源训练集，规模为此前最佳资源的20倍，包含3万种交互式越狱场景，助力提升语言模型的多轮安全性。本研究为应对复杂对话攻击提供了重要工具和见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 12:11:28 GMT</pubDate>
</item>
<item>
<title>基于生成模型的新型透视变形图像创作</title>
<link>https://arxiv.org/abs/2504.08902</link>
<guid>https://arxiv.org/abs/2504.08902</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合生成模型与频率感知变换技术，创造可直接解读的透视变形图像。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了经典的光学错觉——透视变形图像（anamorphosis），这类图像是通过故意扭曲形成，在常规视角下难以辨认，仅当从特定角度或借助反射设备观看时才显现其真实形态。尽管这类视觉效果的数学原理可以追溯到17世纪，但它们的意义通常依赖于特定视角。我们提出了一种创新方法，利用潜伏修正流模型和一种名为拉普拉斯金字塔扭曲的技术，在保持图像直接可读性的同时生成高质量的透视变形图像。这一研究将视觉文字谜（Visual Anagrams）的概念扩展到了潜在空间模型及更广泛的几何变换领域，为生成式艺术与感知错觉的创作提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.08902" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 14:12:01 GMT</pubDate>
</item>
<item>
<title>从知识检索到思维构建：生成式AI的第二幕</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式AI进入“Act II”，从知识检索转向思维构建。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了生成式人工智能（Generative AI）从第一代到第二代的演进过程。第一代模型通过大规模参数和数据扩展取得了显著成功，但存在知识延迟、浅层推理和受限认知等根本性限制。在此期间，提示工程成为我们与AI交互的主要方式。如今，随着测试时扩展技术的应用，第二代模型正从知识检索系统转变为思维构建引擎，从而实现与AI在思维层面的连接。文章还解释了认知工程的概念基础，并提供了相关教程和优化实现，使更多从业者能够参与到AI的第二幕发展中。此外，作者维护了一个定期更新的GitHub仓库，收录了关于测试时扩展的相关论文。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:55:58 GMT</pubDate>
</item>
<item>
<title>多语言大型语言模型的知识边界感知研究</title>
<link>https://arxiv.org/abs/2504.13816</link>
<guid>https://arxiv.org/abs/2504.13816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多语言LLMs的知识边界感知机制及跨语言迁移方法。</p><br /><br /><p><strong>摘要：</strong> 本文首次分析了大型语言模型（LLMs）在不同语言中识别知识边界的内部表征特性。通过在多种语言中处理已知与未知问题的探针测试，我们发现LLMs对知识边界的感知主要编码在中间至中上层网络中，且跨语言感知差异呈线性结构。基于此，提出了一种无需训练的对齐方法，可有效实现低资源语言间知识边界感知能力的转移，降低幻觉风险。此外，双语问题对微调进一步提升了跨语言知识边界识别能力。鉴于缺乏标准测试基准，我们构建了一个包含三种代表性知识边界数据的多语言评估套件。所有代码和数据集均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:44:12 GMT</pubDate>
</item>
<item>
<title>语言模型中不确定性量化评估的偏差问题及其解决方案</title>
<link>https://arxiv.org/abs/2504.13677</link>
<guid>https://arxiv.org/abs/2504.13677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现常用正确性函数会放大某些不确定性量化方法的表现。</p><br /><br /><p><strong>摘要：</strong> 不确定性量化（UQ）在提升语言模型（LMs）的安全性和可靠性方面至关重要。本文通过评估四种数据集和四种模型下七种正确性函数（包括基于词汇和嵌入的指标及大型语言模型作为裁判的方法）对六种不确定性量化方法的影响，揭示了这些正确性函数中的长度偏差会与量化方法中的长度偏差相互作用，从而扭曲UQ评估结果。研究指出，将大型语言模型作为裁判的方法相对较少受到长度偏差影响，可能是解决此类偏差的一种潜在方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 09:13:42 GMT</pubDate>
</item>
<item>
<title>Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering</title>
<link>https://arxiv.org/abs/2504.13519</link>
<guid>https://arxiv.org/abs/2504.13519</guid>
<content:encoded><![CDATA[
Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 03:15:27 GMT</pubDate>
</item>
<item>
<title>基于生产理论的语言模型经济价值评估框架</title>
<link>https://arxiv.org/abs/2504.13359</link>
<guid>https://arxiv.org/abs/2504.13359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合准确性与推理成本的经济评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个基于生产理论的框架，用于评估语言模型的经济价值，通过整合准确性与推理成本的概念，定义了“通过成本”（cost-of-pass）和“前沿通过成本”（frontier cost-of-pass）。研究发现轻量级模型在基本量化任务中最具成本效益，大型模型在知识密集型任务中表现优异，而推理模型在复杂量化问题上更具优势。此外，过去一年中复杂量化任务的成本大幅下降，创新如轻量级、大型和推理模型的进展对此起到了关键作用。同时，多数推理技术带来的边际精度提升未能抵消其成本，表明互补的模型层面创新是提高成本效益的主要驱动力。此框架为衡量进展和指导部署提供了原则性工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 17:58:29 GMT</pubDate>
</item>
<item>
<title>基于自我校准框架的大型视频语言模型改进研究</title>
<link>https://arxiv.org/abs/2504.12083</link>
<guid>https://arxiv.org/abs/2504.12083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我校准框架解决大型视频语言模型的细粒度时间理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型视频语言模型（LVLMs）在细粒度时间理解、幻觉现象及简单视频问答任务中易犯错误的问题，提出了一个自我校准框架，通过学习自身错误来提升模型性能。该框架首先构建了一组优选与非优选响应对，非优选响应引入常见错误模式，如空间时间理解不足、概念间虚假相关性等。为实现模型与这些响应对的自我校准，我们引入了精炼正则化偏好优化（RRPO），该方法利用子序列级精炼奖励和逐令牌KL正则化，克服直接偏好优化（DPO）的局限性。实验表明，RRPO在多种视频任务中，包括视频幻觉、短视频和长视频理解以及细粒度时间推理上，均表现出更精确的校准和更稳定的训练效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:43:56 GMT</pubDate>
</item>
<item>
<title>ThoughtMani：通过外部CoTs减少大型推理模型的冗余推理步骤</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ThoughtMani，通过插入小型模型生成的外部CoTs来减少大型推理模型的冗余推理步骤。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）通过扩展测试时计算能力，在多个任务中表现出色。然而，这些模型通常面临“过度思考”问题，即产生大量冗余推理步骤但性能提升有限。现有解决方案依赖于微调，这需要额外的数据、非传统训练设置、潜在的安全性失配和较差的泛化能力。本研究通过实证分析揭示LRM行为的一个重要特性，即在推理令牌之间插入由较小模型生成的外部CoTs可以有效操控模型生成较少的推理步骤。基于此洞察，我们提出了一种简单而高效的管道——ThoughtMani，使LRMs能够绕过不必要的中间步骤并显著降低计算成本。我们在LiveBench/Code数据集上对QwQ-32B进行实验验证，发现ThoughtMani保持了原始性能，减少了约30%的输出标记数，且外部CoT生成器带来的开销极小。此外，ThoughtMani还提高了平均10%的安全性对齐。由于模型供应商通常同时提供不同规模的模型，ThoughtMani为构建更高效、更易访问的LRMs提供了有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 07:07:19 GMT</pubDate>
</item>
<item>
<title>CLASH数据集评估大型语言模型在高风险情境下的价值决策能力</title>
<link>https://arxiv.org/abs/2504.10823</link>
<guid>https://arxiv.org/abs/2504.10823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次引入CLASH数据集，评估大语言模型在高风险价值冲突情境中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CLASH的新数据集，用于评估大型语言模型（LLMs）在高风险情境下处理价值冲突的能力。CLASH包含345个高影响力困境及其3,795个不同的价值视角，旨在弥补先前研究仅限于日常场景的不足。通过测试10种前沿开放及封闭模型，发现顶级模型如GPT-4o和Claude-Sonnet在识别模棱两可决策方面准确率不足50%，但在明确情景中表现较好。此外，尽管LLMs能合理预测心理不适，但对涉及价值转变的观点理解不足，表明LLMs需要增强复杂价值推理能力。实验还显示LLMs的价值偏好与其向特定价值方向的可引导性密切相关，且从第三方视角进行价值推理时表现出更高的可引导性，但某些价值对则从第一人称视角中受益更多。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 22:54:16 GMT</pubDate>
</item>
<item>
<title>DehazeXL：一种高效处理大分辨率图像雾霾去除的方法</title>
<link>https://arxiv.org/abs/2504.09621</link>
<guid>https://arxiv.org/abs/2504.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种平衡全局上下文和局部特征的大分辨率图像去雾方法DehazeXL。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度学习模型在处理大分辨率雾霾图像时遇到的内存限制问题，提出了DehazeXL，一种能够在主流GPU硬件上实现端到端建模的去雾方法。DehazeXL通过有效平衡全局上下文和局部特征提取，在不牺牲细节的情况下解决了高分辨率图像去雾难题。此外，为了评估全局上下文在去雾性能中的作用，我们设计了一种特定于去雾任务的视觉归因方法。同时，由于缺乏大型图像去雾基准数据集，我们创建了一个超高分辨率去雾数据集（8KDehaze），包含10,000对大小为8192×8192像素的清晰与雾霾图像。实验表明，DehazeXL可以在仅占用21GB显存的情况下推断高达10240×10240像素的图像，达到当前最先进的性能。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 11:41:25 GMT</pubDate>
</item>
<item>
<title>强化学习与可验证奖励对大语言模型推理能力的影响</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现强化学习并未显著提升大语言模型的基础推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文重新评估了强化学习与可验证奖励（RLVR）在增强大型语言模型（LLMs）推理能力方面的作用，特别是在数学和编程任务中的表现。虽然RLVR训练的模型在小值k（如k=1）下的pass@k指标优于基线模型，但在大值k时，基线模型的表现可以达到甚至超过RL训练模型。进一步分析表明，RL训练通过调整模型输出分布，提高了采样正确响应的效率，但同时也限制了模型的推理能力边界。此外，在视觉推理任务中也观察到类似结果。值得注意的是，蒸馏方法可以引入不同于RLVR的新知识。这些发现揭示了RLVR在提升LLMs推理能力方面的局限性，需要重新审视强化学习对推理模型的影响并探索更优的训练范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于最大信息增益的指令微调数据高效采样方法</title>
<link>https://arxiv.org/abs/2504.13835</link>
<guid>https://arxiv.org/abs/2504.13835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一的数据信息量化方法及最大信息增益采样算法，显著提升指令微调数据集构建效果。</p><br /><br /><p><strong>摘要：</strong> 数据质量和多样性对构建有效的指令微调数据集至关重要。现有方法多侧重实例质量并采用启发式规则维持多样性，但缺乏全局视角可能导致结果不佳，且难以精确捕捉复杂指令意图。本文提出一种基于标签图构造的语义空间量化方法，通过信息分布衡量数据集多样性，并设计最大信息增益(MIG)采样算法，迭代选择样本以最大化语义空间中的信息增益。实验表明，MIG方法在多个数据集和基础模型上优于当前先进方法，例如用Tulu3数据集5%采样数据微调的模型在AlpacaEval和Wildbench上的性能分别提升了5.73%和6.89%，接近全量数据SFT模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于注意力偏差的深度学习架构设计框架Miras</title>
<link>https://arxiv.org/abs/2504.13173</link>
<guid>https://arxiv.org/abs/2504.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Miras，通过重新设计神经网络作为关联记忆模块提升模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文受人类认知现象“注意力偏差”的启发，将Transformer等神经网络重新定义为通过内部目标学习键值映射的关联记忆模块。研究发现现有序列模型大多采用点积相似性或L2回归作为注意力偏差，为此提出了多种替代配置及其稳定训练的近似方法，并重新解释了现代深度学习中的遗忘机制。基于这些见解，我们开发了Miras框架，包含四种设计选择：关联记忆架构、注意力偏差目标、保留门和记忆学习算法。由此产生了三个新型序列模型Moneta、Yaad和Memora，不仅超越了现有线性RNN的能力，还展现出在语言建模、常识推理及回忆密集型任务上的优异表现，甚至优于Transformer等现代模型。实验表明，Miras的不同设计变体在特定任务上具有显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>结合伪合成与真实图像的空中地面视角几何重建</title>
<link>https://arxiv.org/abs/2504.13157</link>
<guid>https://arxiv.org/abs/2504.13157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合伪合成与真实图像的框架，解决空中与地面视角差异大的几何重建问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了从混合地面和空中视图捕获的图像进行几何重建的任务。当前最先进的基于学习的方法无法处理空中与地面图像对之间极端的视角变化，主要原因是缺乏高质量的共配准空中-地面训练数据集。我们假设这是失败的关键原因，因为这样的数据难以精确组装且难以以可扩展的方式重建。为了解决这一挑战，我们提出了一种可扩展的框架，该框架结合了来自3D城市范围网格（如Google Earth）的伪合成渲染和真实地面级众包图像（如MegaDepth）。伪合成数据模拟了广泛的空中视角，而真实众包图像则在地面级图像中改善了视觉保真度，有效地弥合了真实图像与伪合成渲染之间的领域差距。使用此混合数据集，我们微调了几种最先进的算法，在现实世界的零样本空中-地面任务中取得了显著改进。例如，基线DUSt3R方法仅将不到5%的空中-地面对在相机旋转误差5度以内定位，而使用我们的数据微调后，准确率提高到近56%，解决了处理大视角变化的主要失败点。除了相机估计和场景重建外，我们的数据集还在具有挑战性的空中-地面场景中的下游任务（如新颖视图合成）中提高了性能，展示了我们的方法在实际应用中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>HiScene：基于层次结构的场景级3D生成框架</title>
<link>https://arxiv.org/abs/2504.13072</link>
<guid>https://arxiv.org/abs/2504.13072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HiScene框架，结合2D与3D生成技术实现高质量场景生成。</p><br /><br /><p><strong>摘要：</strong> 场景级3D生成是多媒体与计算机图形学的重要前沿领域，但现有方法存在对象类别有限或编辑灵活性不足的问题。本文介绍了一种名为HiScene的新框架，通过将场景视为具有等距视图的层次化“对象”，实现了高保真度的场景生成。HiScene将房间视为复杂对象并进一步分解为可操控的项目，从而确保生成内容与2D表示的一致性同时保持组成结构。为了保证分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的模态完成技术，有效处理对象之间的遮挡和阴影问题，并引入形状先验注入以确保场景内的空间一致性。实验结果显示，该方法生成的物体排列更加自然且适合交互应用，同时保持物理真实性和与用户输入的对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:33:39 GMT</pubDate>
</item>
<item>
<title>多语言推理在大型语言模型中的上限潜力研究</title>
<link>https://arxiv.org/abs/2504.11833</link>
<guid>https://arxiv.org/abs/2504.11833</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多语言推理比英语推理具有更高的性能上限。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，大型语言模型存在显著的“英语偏见”，即任务用英语呈现时表现更好。然而，我们发现某些其他语言在推理任务中甚至可以优于英语。本文探索了多语言推理在任务中的性能上限，指出多语言推理不仅能显著提高近10 Acc@k点，还更具鲁棒性，对翻译质量和语言选择的变化具有更强的容忍度。尽管如此，目前常见的答案选择方法无法达到这一上限，因为它们受到局限性和偏见的影响。此外，我们分析了性能上限背后的原因及实现挑战。这些发现为未来充分挖掘大型语言模型中多语言推理的潜力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11833" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 03:45:10 GMT</pubDate>
</item>
<item>
<title>NodeRAG：基于异构图结构的大规模语言模型增强型检索生成框架</title>
<link>https://arxiv.org/abs/2504.11544</link>
<guid>https://arxiv.org/abs/2504.11544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NodeRAG通过引入异构图结构优化检索增强生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为NodeRAG的新框架，该框架以图为中心，通过引入异构图结构，实现了图算法在检索增强生成（RAG）工作流中的无缝整合。与现有方法相比，NodeRAG不仅提升了索引时间、查询时间和存储效率，还在多跳基准测试和开放式一对一评估中展现出更优的问题回答能力。此外，NodeRAG在使用最少检索标记的情况下，表现出了显著的优势，尤其是在GraphRAG和LightRAG等方法的基础上进一步优化了性能。NodeRAG的代码已开源，可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:24:00 GMT</pubDate>
</item>
<item>
<title>构建透明图像与视频理解研究的开放性感知语言模型</title>
<link>https://arxiv.org/abs/2504.13180</link>
<guid>https://arxiv.org/abs/2504.13180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过开源框架构建透明的感知语言模型用于图像和视频理解。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型在计算机视觉领域至关重要，但许多高性能模型仍为闭源状态，限制了科学进展。尽管学术界尝试通过黑盒模型蒸馏生成训练数据取得显著基准成果，但缺乏对教师模型及其数据来源的了解阻碍了真正的科学进步。本文提出在完全开源且可复现的框架下构建感知语言模型（PLM），以推动图像和视频理解领域的透明研究。我们分析了标准训练管道，并探索大规模合成数据，发现特别是在详细视频理解方面存在关键的数据缺口。为此，我们发布了包含280万个细粒度视频问答对的人类标注实例以及时空定位的视频描述。此外，我们推出了PLM-VideoBench，这是一个评估具有挑战性的视频理解任务的工具包，重点关注对视频“什么”、“哪里”、“何时”和“如何”的推理能力。我们的工作通过提供数据、训练方法、代码和模型实现了完全的可复现性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Complex-Edit：基于指令的图像编辑模型综合评估基准</title>
<link>https://arxiv.org/abs/2504.13143</link>
<guid>https://arxiv.org/abs/2504.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Complex-Edit基准，评估指令复杂性对图像编辑模型的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Complex-Edit的综合基准，用于系统性评估基于指令的图像编辑模型在不同复杂度指令下的表现。通过利用GPT-4o大规模自动生成多样化编辑指令，我们设计了一套“编辑链”管道来构建复杂的编辑任务。此外，还引入了一系列指标和基于视觉语言模型的自动化评估流程，以支持大规模性能评估。研究发现开源模型显著落后于闭源模型，且随着指令复杂性的增加，模型在保留输入图像关键元素和整体美学质量方面的能力显著下降。此外，将复杂指令分解为原子步骤执行会降低性能，而Best-of-N策略能提升直接编辑和逐步方法的效果。有趣的是，训练中使用合成数据会导致模型生成的图像显得更加人工化，这种现象在GPT-4o输出中也有所体现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>基于Vision Transformer的无人机跟踪中抗遮挡表示学习</title>
<link>https://arxiv.org/abs/2504.09228</link>
<guid>https://arxiv.org/abs/2504.09228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法增强单流ViT模型在航拍跟踪中的遮挡鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文针对使用视觉Transformer(ViT)主干网络的单流架构在实时无人机跟踪中的不足，即缺乏有效处理遮挡的策略，提出了一种基于ViT学习抗遮挡表示（ORR）的新方法。通过引入随机屏蔽操作模拟目标遮挡，使ViT模型具备更强的目标遮挡鲁棒性，构建了名为ORTrack的框架。此外，为满足实时应用需求，设计了自适应特征知识蒸馏（AFKD）方法，生成高效的学生模型ORTrack-D，既保留了ORTrack的性能，又提高了效率。在多个基准数据集上的实验验证了该方法的有效性和领先性能，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 10:06:50 GMT</pubDate>
</item>
<item>
<title>70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float</title>
<link>https://arxiv.org/abs/2504.11651</link>
<guid>https://arxiv.org/abs/2504.11651</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 18:38:38 GMT</pubDate>
</item>
<item>
<title>Perception Encoder：通过视觉-语言对比学习实现多任务通用编码器</title>
<link>https://arxiv.org/abs/2504.13181</link>
<guid>https://arxiv.org/abs/2504.13181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">仅靠对比视觉-语言训练即可生成适用于多种下游任务的强大通用嵌入。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为Perception Encoder（PE）的最先进图像和视频理解编码器，该编码器通过简单的视觉-语言学习进行训练。传统视觉编码器依赖于针对具体任务定制的各种预训练目标，而我们发现，经过精心调校的图像预训练方法和稳健的视频数据引擎优化后，仅基于对比视觉-语言训练就能产生适用于分类、描述、定位等所有这些下游任务的强大通用嵌入。然而，这些嵌入隐藏在网络的中间层中，为此我们提出了两种对齐方法：用于多模态语言建模的语言对齐和用于密集预测的空间对齐。结合核心对比检查点，PE模型家族在零样本图像和视频分类与检索、文档问答、图像问答、视频问答以及检测、深度估计和跟踪等空间任务上均取得了最先进的性能。为了促进进一步研究，我们将发布我们的模型、代码以及一个包含合成和人工注释视频的新颖数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>MetaSynth：通过元提示增强合成数据多样性以实现领域自适应</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSynth通过元提示生成多样化合成数据，成功将大型语言模型适配到金融和生物医学领域。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用合成数据进行领域自适应的问题，特别是针对小型语言模型Phi-3.5和Phi-4，这些模型依赖由大型语言模型生成的合成数据。然而，合成数据的低多样性限制了其在改进其他模型方面的应用。为解决这一问题，我们提出了MetaSynth方法，该方法通过元提示机制，让多个“专家”大型语言模型协同生成数据，从而提高合成数据的多样性。实验表明，仅使用2500万token的MetaSynth生成的合成数据，就能有效将已训练好的大型语言模型（Mistral-7B-v0.3）适配到金融和生物医学两个特定领域，同时不影响其在通用任务中的性能。此外，通过七种自动化指标评估，MetaSynth生成的合成数据的多样性接近大型语言模型预训练语料库。持续预训练Mistral-7B-v0.3模型显示，在金融领域提升了4.08%，在生物医学领域提升了13.75%的表现。相比之下，使用模板提示生成的数据即使包含真实数据示例，也会导致模型性能下降。研究结果表明，使用MetaSynth生成的少量多样化合成数据即可实现有效的领域自适应。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 21:25:15 GMT</pubDate>
</item>
<item>
<title>基于学习的跨相机色彩恒常性方法</title>
<link>https://arxiv.org/abs/2504.07959</link>
<guid>https://arxiv.org/abs/2504.07959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可适应新相机的跨相机色彩校正学习方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于学习的方法，用于解决计算色彩恒常性（即白平衡）问题中的跨相机适应性挑战。该方法利用图像信号处理器(ISP)中预校准的颜色校正矩阵(CCM)，将标准空间的颜色映射到相机的原始颜色空间，并通过编码生成紧凑的相机指纹嵌入(CFE)，从而实现对未知相机的适应。为了防止训练过程中因相机数量有限导致的过拟合，引入了一种相机间数据插值增强技术。实验结果表明，该方法在多个数据集和网络结构上实现了最先进的跨相机色彩恒常性性能，同时保持轻量化且仅依赖于ISP中已有的数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.07959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>通过睡眠计算提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2504.13171</link>
<guid>https://arxiv.org/abs/2504.13171</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入睡眠计算显著降低大语言模型推理时的计算需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为睡眠计算的新方法，允许模型在用户查询到来之前预先处理可能的上下文信息，从而大幅减少测试阶段的计算需求和延迟。实验表明，在两个推理任务(Stateful GSM-Symbolic 和 Stateful AIME)中，睡眠计算可使所需测试计算量减少约5倍，同时通过扩展计算还可提高准确性最高达13%和18%。此外，多查询GSM-Symbolic进一步优化了相关查询的平均成本，降低了2.5倍。研究还发现，用户查询的可预测性与睡眠计算的效果高度相关。最后，睡眠计算在实际代理任务中也显示出良好的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13171" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>CLIMB框架实现高效预训练数据混合优化</title>
<link>https://arxiv.org/abs/2504.13161</link>
<guid>https://arxiv.org/abs/2504.13161</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CLIMB框架优化预训练数据混合，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 当前预训练数据集多来源于网络内容且缺乏明确领域划分，导致构建最优数据混合极具挑战性。本文提出CLIMB框架，通过嵌入和聚类大规模数据集并迭代优化数据混合，显著提升了预训练模型的表现。实验表明，在400B令牌上训练时，CLIMB框架下的1B模型比Llama-3.2-1B高出2%，特定领域优化进一步提升5%。此外，我们发布了ClimbLab和ClimbMix数据集供研究使用，揭示了最优数据混合的特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13161" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>利用专家失败探索提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.13145</link>
<guid>https://arxiv.org/abs/2504.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法EEF，通过分析专家失败轨迹提高大型语言模型代理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）作为代理展现出强大的潜力，特别是在需要多轮推理和交互的任务上。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家成功轨迹并迭代优化自生成轨迹来提升代理技能。然而，由于专家（如GPT-4）在简单子任务上表现较好，而RFT倾向于处理简单场景，许多复杂子任务仍未解决且持续处于分布外（OOD）。研究发现，专家失败轨迹中的计划和关键操作可显著提高代理的探索效率和技能获取。基于此，我们提出了探索专家失败（EEF），该方法从失败的专家轨迹中识别出有益操作并整合到训练数据集中，同时排除潜在有害操作以保护模型学习过程。实验表明，EEF在WebShop中取得了62%的胜率，优于RFT（53.6%）和GPT-4（35.6%），并在WebShop和SciWorld中创造了新的性能记录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:53:54 GMT</pubDate>
</item>
<item>
<title>多因素挑战下的RAG系统改进：RAMDocs与MADAM-RAG</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RAMDocs和MADAM-RAG方法，同时处理模糊查询和信息冲突问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）的检索增强生成（RAG）技术虽提升了事实准确性，但在处理模糊查询及多源冲突信息时仍面临挑战。现有研究通常孤立解决单一问题，如模糊性或噪声信息。本文提出RAMDocs，一个模拟复杂真实场景的新数据集，涵盖模糊性、错误信息和噪声；并开发MADAM-RAG，一种多代理辩论机制，通过多轮讨论聚合去噪后的有效答案。实验显示，MADAM-RAG在AmbigDocs和FaithEval任务中均优于基线模型，但RAMDocs对现有RAG系统构成重大挑战，表明仍有提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>NoisyRollout：通过视觉导向策略增强视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.13055</link>
<guid>https://arxiv.org/abs/2504.13055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合清洁与失真图像的RL方法NoisyRollout，提升视觉语言模型的推理与感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习在视觉语言模型中的应用，特别是如何通过引入视觉导向的噪声策略来增强模型的探索能力。传统的视觉语言模型在面对不完美视觉感知时表现不佳，这限制了其推理效果。为此，我们提出了NoisyRollout，这是一种简单而有效的强化学习方法，通过混合清洁和适度失真的图像轨迹，为视觉感知和推理模式引入目标多样性。该方法无需额外训练成本，在仅使用2.1K训练样本的情况下，在五个跨领域的基准测试中实现了最先进的性能，同时保持了同等甚至更好的领域内表现。此外，NoisyRollout采用了一种噪声退火调度，逐步减少训练过程中的失真强度，从而早期利用噪声信号的优势，同时确保后期训练的稳定性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:10:13 GMT</pubDate>
</item>
<item>
<title>ANT：通过自动引导去噪轨迹实现文本转图像模型的概念擦除</title>
<link>https://arxiv.org/abs/2504.12782</link>
<guid>https://arxiv.org/abs/2504.12782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ANT，用于有效防止有害内容生成并实现概念擦除。</p><br /><br /><p><strong>摘要：</strong> 现有基于微调的文本到图像模型概念擦除方法存在显著局限性，本文引入一种名为ANT的新框架，通过自动引导去噪轨迹，实现了对不想要概念的精确修改。该框架基于分类器自由引导条件方向反转的见解，在去噪的中晚期阶段启用此操作，从而在不牺牲早期结构完整性的情况下实现内容修改。对于单概念擦除，采用增强的权重显著图来精确定位关键参数；对于多概念擦除，提供了一种灵活的插件解决方案。实验表明，ANT在单概念和多概念擦除方面均达到最先进的性能，同时保持生成保真度。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 05:29:30 GMT</pubDate>
</item>
<item>
<title>InstantCharacter：基于扩散Transformer的可扩展角色定制框架</title>
<link>https://arxiv.org/abs/2504.12395</link>
<guid>https://arxiv.org/abs/2504.12395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散Transformer的角色定制框架，提升图像质量和文本可控性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为InstantCharacter的新框架，旨在解决现有基于学习的角色定制方法的局限性，如有限的泛化能力和图像质量下降问题，以及优化方法中特定对象微调导致的文本可控性降低问题。该框架基于扩散Transformer构建，具有开放域个性化能力，支持多样化的角色外观、姿势和风格，同时保持高保真度。通过引入堆叠变换编码器的可扩展适配器，框架能够有效处理开放域角色特征并与现代扩散Transformer的潜在空间无缝交互。此外，构建了一个包含1000万级别样本的大规模角色数据集，分为配对（多视角角色）和非配对（文本-图像组合）子集，以优化身份一致性与文本编辑能力。实验结果表明，InstantCharacter在生成高保真、文本可控且角色一致的图像方面表现出色，树立了新的角色驱动图像生成基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 14:01:59 GMT</pubDate>
</item>
<item>
<title>基于分数蒸馏的文本到图像模型合并方法</title>
<link>https://arxiv.org/abs/2504.12364</link>
<guid>https://arxiv.org/abs/2504.12364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法将多个模型的知识整合为单一多才多艺的文本到图像生成模型。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像生成模型的成功，大量从同一基础模型微调而来的专门模型涌现，但带来了高参数冗余和存储成本问题。传统静态线性插值方法忽略了多样风格可能带来的不兼容性。为此，我们设计了一种可通过风格向量控制任意风格图像生成的流水线，并提出了基于分数蒸馏的模型合并范式（DMM），有效压缩多个模型为单一多功能模型。此外，我们重新思考并重新定义了文本到图像生成领域的模型合并任务，提出新的合并目标和评估协议。实验表明，DMM能够紧凑地重组多个教师模型的知识，并实现可控的任意风格生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:09:45 GMT</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:04:14 GMT</pubDate>
</item>
<item>
<title>GRA框架：多小模型协作生成高质量数据</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多小模型协作模拟同行评审，实现与大模型相当的数据合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GRA的框架，该框架通过多个小型语言模型（LLMs）协同工作，模拟人类同行评审过程，完成数据合成与优化任务。具体而言，GRA框架中的小模型分别承担生成者（Generator）、审查者（Reviewer）和裁定者（Adjudicator）的角色，逐步迭代并控制数据的质量与多样性。实验结果显示，该方法生成的数据在多个基准测试中达到了甚至超过了单一大型语言模型（如Qwen-2.5-72B-Instruct）的表现。这一成果挑战了传统上依赖大型模型进行高质量数据合成的必要性，提倡采用小型模型的策略性协作。研究代码和数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 02:13:43 GMT</pubDate>
</item>
<item>
<title>REVERSE：一种统一的视觉-语言模型幻觉缓解框架</title>
<link>https://arxiv.org/abs/2504.13169</link>
<guid>https://arxiv.org/abs/2504.13169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出REVERSE框架，通过训练与实时自验证结合显著减少视觉幻觉。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言模型在视觉理解方面表现出色，但容易产生不存在的对象、动作或概念的描述，这在安全性至关重要的应用场景中构成重大风险。目前的幻觉缓解方法主要分为两类：生成调整和事后验证。然而，生成调整方法通常依赖启发式规则且缺乏修正机制，而事后验证则复杂且倾向于拒绝输出而非改进。本研究提出了REVERSE框架，它将幻觉感知训练与动态自我验证相结合。通过利用包含超过130万个半合成样本的新幻觉验证数据集，以及创新的推理时回顾重采样技术，该方法使视觉-语言模型能够在生成过程中检测幻觉并动态修正这些幻觉。实验表明，REVERSE在CHAIR-MSCOCO和HaloQuest数据集上的幻觉减少效果达到当前最佳水平，分别比现有最佳方法高出12%和28%。相关数据集、模型和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:54:14 GMT</pubDate>
</item>
<item>
<title>VistaDPO：基于层次化空间-时间直接偏好优化的大规模视频模型对齐框架</title>
<link>https://arxiv.org/abs/2504.13122</link>
<guid>https://arxiv.org/abs/2504.13122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VistaDPO框架解决视频理解中的对齐与幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大型语言模型构建的大规模视频模型（LVMs）在视频理解中存在的对齐人类直觉差及视频幻觉问题，引入了VistaDPO这一新颖的视频层次化空间-时间直接偏好优化框架。该框架通过实例级、时间级及感知级三个层级增强文本与视频间偏好对齐效果。鉴于缺乏细粒度视频语言偏好对齐的数据集，我们创建了VistaDPO-7k数据集，包含7.2K个标注问答对及时空定位信息。在视频幻觉、视频问答及视频描述等基准测试中，VistaDPO显著提升了现有LVMs的表现，有效缓解了视频语言不匹配及幻觉现象。相关代码与数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:39:41 GMT</pubDate>
</item>
<item>
<title>FramePack：用于视频生成的高效帧预测神经网络结构</title>
<link>https://arxiv.org/abs/2504.12626</link>
<guid>https://arxiv.org/abs/2504.12626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为FramePack的神经网络结构，用于视频生成中的帧预测。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FramePack的神经网络结构，旨在训练视频生成中的下一帧预测模型。FramePack通过压缩输入帧，使Transformer的上下文长度固定，从而可以处理大量帧并显著提高训练批次大小。此外，该方法还提出了一种反向漂移采样方法，通过逆时间顺序生成帧并提前确定端点来避免曝光偏差。最后，实验表明，现有的视频扩散模型可以通过FramePack进行微调，视觉质量可能得到改善。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:02:31 GMT</pubDate>
</item>
<item>
<title>WorldMem：基于记忆模块提升世界模拟中的时空一致性</title>
<link>https://arxiv.org/abs/2504.12369</link>
<guid>https://arxiv.org/abs/2504.12369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合记忆机制的世界模拟框架，有效增强长期场景生成的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WorldMem的新框架，旨在通过引入记忆银行（存储记忆帧和状态）来改善虚拟环境中的场景生成质量。记忆银行中的记忆单元不仅存储视觉信息，还记录物体姿态及时间戳等状态数据。通过采用记忆注意力机制，该方法可以从记忆帧中高效提取相关信息，从而即使在较大的视角或时间差距下也能精确重建先前观察到的场景。此外，通过将时间戳纳入状态描述，WorldMem不仅能构建静态世界模型，还能捕捉世界的动态演化过程，支持感知与交互功能。实验表明，该方法在虚拟和真实场景中均表现出色，验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering</title>
<link>https://arxiv.org/abs/2504.05506</link>
<guid>https://arxiv.org/abs/2504.05506</guid>
<content:encoded><![CDATA[
Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:05:06 GMT</pubDate>
</item>
<item>
<title>BitNet b1.58 2B4T：首个开源1比特20亿参数大型语言模型</title>
<link>https://arxiv.org/abs/2504.12285</link>
<guid>https://arxiv.org/abs/2504.12285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BitNet b1.58 2B4T实现了高性能与低能耗的结合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BitNet b1.58 2B4T，这是首个开源的原生1比特20亿参数大型语言模型（LLM）。该模型基于4万亿令牌的语料库训练，性能在多个基准测试中得到验证，包括语言理解、数学推理、编码能力和对话能力。研究结果显示，BitNet b1.58 2B4T的性能与同类规模的领先开源全精度LLM相当，但在计算效率方面具有显著优势，如大幅减少内存占用、能耗和解码延迟。为了促进进一步的研究和应用，模型权重已通过Hugging Face发布，同时提供了支持GPU和CPU架构的开源推理实现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:51:43 GMT</pubDate>
</item>
<item>
<title>基于激光雷达的零样本形状补全方法CAL</title>
<link>https://arxiv.org/abs/2504.12264</link>
<guid>https://arxiv.org/abs/2504.12264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用多模态传感器序列进行激光雷达形状补全的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了CAL（Complete Anything in Lidar），一种用于野外激光雷达形状补全的方法，它与基于激光雷达的语义/全景场景补全密切相关。与现有方法只能完成和识别封闭词汇表中标记的对象不同，CAL采用零样本方法，通过挖掘多模态传感器序列中的时间上下文来获取观察对象的形状和语义特征，并将其蒸馏到仅依赖激光雷达的实例级补全和识别模型中。尽管我们只挖掘部分形状补全，但发现该模型能够从数据集中的多个部分观测推断完整的物体形状。实验表明，该模型可以在标准基准上进行语义和全景场景补全，并识别超出固定类别词汇表的对象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:21:55 GMT</pubDate>
</item>
<item>
<title>Cobra：高效灵活的漫画线条艺术着色方法</title>
<link>https://arxiv.org/abs/2504.12240</link>
<guid>https://arxiv.org/abs/2504.12240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于因果稀疏DiT架构的高效着色方法Cobra。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于参考的漫画线条艺术着色在高精度、效率、上下文一致性及灵活控制方面的需求。传统扩散模型在处理复杂漫画场景时存在局限性，如处理大量参考图像耗时、推理时间长等问题。为此，我们提出了Cobra方法，该方法结合颜色提示并利用超过200张参考图像，在保持低延迟的同时实现高质量着色。Cobra的核心是一种因果稀疏DiT架构，通过设计的位置编码、因果稀疏注意力及Key-Value缓存机制有效管理长上下文参考并保证色彩一致性。实验结果显示，Cobra显著提升了推理速度和交互性，满足了工业应用的关键需求。相关代码和模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 12:45:19 GMT</pubDate>
</item>
<item>
<title>多语言混合作者文本中AI生成内容检测模型的研究</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出适用于多种生成器和混合作者文本的AI生成内容检测模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组针对标记分类任务设计的模型，这些模型经过大量人类与机器共同创作的文本训练，在未见过的领域、生成器、非母语文本及对抗性输入上表现优异。同时，我们构建了一个包含超过240万条文本的新数据集，覆盖23种语言，主要由几种流行的专有大型语言模型与人协作生成。研究还分析了模型在不同领域、生成器下的性能，并对比了对抗方法、输入文本长度及生成文本特性对检测效果的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 06:29:30 GMT</pubDate>
</item>
<item>
<title>ReTool：通过工具集成学习提升复杂数学推理能力</title>
<link>https://arxiv.org/abs/2504.11536</link>
<guid>https://arxiv.org/abs/2504.11536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合代码解释器的强化学习模型在数学推理任务上显著超越传统文本模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReTool的新方法，它通过将工具集成到基于强化学习的推理模型中，显著提升了模型在几何推理、复杂方程求解等结构化问题上的表现。ReTool的关键创新在于动态整合自然语言推理过程中的实时代码执行，并采用自动化的强化学习范式来优化工具调用策略。实验结果显示，在AIME数学竞赛题集上，ReTool的32B参数版本在仅需400个训练步的情况下达到了67%的准确率，优于文本基线模型的40%准确率。此外，在扩展设置下，该模型进一步达到72.5%的准确率，大幅领先OpenAI的o1-preview模型。研究还观察到模型具备自主代码修正的能力，表明其已掌握适应性工具使用的“顿悟”能力。这些发现为推动复杂数学推理及混合神经符号系统的进步提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:10:22 GMT</pubDate>
</item>
<item>
<title>Vivid4D：基于单目视频的4D动态场景重建方法</title>
<link>https://arxiv.org/abs/2504.11092</link>
<guid>https://arxiv.org/abs/2504.11092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入新方法Vivid4D，利用几何先验和生成先验结合实现多视角合成，提升4D场景重建质量。</p><br /><br /><p><strong>摘要：</strong> 单目视频的4D动态场景重建是一个具有挑战性的任务，因为每个时间戳仅能从单一视角观察。本文提出了一种名为Vivid4D的新方法，通过增强观测视角（即从单目输入合成多视角视频）来改进4D单目视频合成。与现有方法不同，Vivid4D同时利用了几何先验和生成先验，将视角增强重新表述为视频修复任务，通过将观测到的视图基于单目深度先验扭曲到新的视角上实现。为了训练这一模型，我们使用了未定位的网络视频，并通过合成遮挡掩模来模拟扭曲导致的遮挡情况，确保空间和时间上的一致性修复。此外，为了减少单目深度先验的不准确性，我们还引入了迭代视角增强策略和鲁棒重建损失函数。实验表明，该方法显著提高了单目4D场景的重建和完成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 07:38:14 GMT</pubDate>
</item>
<item>
<title>基于表示对齐的端到端扩散模型训练方法</title>
<link>https://arxiv.org/abs/2504.10483</link>
<guid>https://arxiv.org/abs/2504.10483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的训练方法REPA-E，显著提升扩散模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了是否可以将潜在扩散模型与变分自编码器（VAE）tokenizer以端到端的方式联合训练的问题。传统深度学习认为端到端训练更优，但实验表明标准扩散损失函数下的联合训练不仅无效，甚至会降低最终性能。研究发现，通过引入表示对齐（REPA）损失，可以在训练过程中同时微调VAE和扩散模型，从而实现高效的端到端训练。该方法不仅大幅加速了训练过程，还提升了扩散模型的性能，同时改善了VAE的潜在空间结构及下游生成效果。最终，在ImageNet 256 x 256上的FID得分达到1.26（带分类器自由引导）和1.83（不带分类器自由引导），创造了新记录。此外，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>AlayaDB：面向大规模语言模型的高效向量数据库系统</title>
<link>https://arxiv.org/abs/2504.10326</link>
<guid>https://arxiv.org/abs/2504.10326</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlayaDB通过解耦KV缓存与注意力计算，优化大规模语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> AlayaDB是由AlayaDB AI开发的新型向量数据库系统，专为提高大型语言模型（LLMs）的长上下文推理效率而设计。该系统将LLM推理中的KV缓存和注意力计算分离并封装进独立的数据库系统中，显著减少了对硬件资源的需求，同时提升了多种工作负载的服务质量。相较于现有解决方案，如KV缓存解聚合和基于检索的稀疏注意力方法，AlayaDB通过将注意力计算和缓存管理抽象为查询处理过程，并利用原生查询优化器提升性能。本文通过来自行业合作伙伴的三个实际案例以及LLM推理基准测试的广泛实验结果，验证了AlayaDB的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10326" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 11:34:26 GMT</pubDate>
</item>
<item>
<title>MLRC-Bench：评估大语言模型在机器学习研究竞赛中的表现</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MLRC-Bench基准，评估大语言模型在解决复杂机器学习问题上的有效性。</p><br /><br /><p><strong>摘要：</strong> 现有对大型语言模型在科学发现中的评估缺乏客观基线和度量标准。本文引入MLRC-Bench基准，专注于衡量语言代理提出和实现创新研究方法的能力，不同于以往关注已建立任务的基准。通过7项竞赛任务测试，发现顶级代理仅达到人类参与者得分的9.3%，并揭示了代理创新与前沿研究性能之间的不匹配。MLRC-Bench是一个动态增长的基准，旨在促进AI研究能力的严谨评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 15:35:43 GMT</pubDate>
</item>
<item>
<title>Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</title>
<link>https://arxiv.org/abs/2504.09566</link>
<guid>https://arxiv.org/abs/2504.09566</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 09:35:41 GMT</pubDate>
</item>
<item>
<title>AI语音生成技术对多语言口音影响的社会技术分析</title>
<link>https://arxiv.org/abs/2504.09346</link>
<guid>https://arxiv.org/abs/2504.09346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示AI语音技术可能加剧语言特权和口音歧视。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能语音生成及克隆技术取得了显著进展，但其对不同口音和语言特征的社会技术系统的影响尚未完全明晰。本研究通过混合方法评估了两个AI语音服务（Speechify和ElevenLabs），并结合调查和访谈探讨用户实际体验如何影响他们对这些技术中口音变化的认知。研究发现，技术性能在五种英语区域口音间存在差异，并指出当前技术可能无意中强化语言特权和基于口音的歧视，从而可能导致新的数字排斥形式。总体而言，本研究强调了包容性设计和监管的重要性，为开发者、政策制定者和组织提供了可操作的见解，以确保公平且负责任的人工智能语音技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 17:31:22 GMT</pubDate>
</item>
<item>
<title>基于SIFT的大规模语音指令微调数据集与语言模型</title>
<link>https://arxiv.org/abs/2504.09081</link>
<guid>https://arxiv.org/abs/2504.09081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SIFT数据集用于语音文本大模型的指令微调和预训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为SIFT（Speech Instruction Fine-Tuning）的5000万样本数据集，专门用于语音文本大语言模型（LLMs）的指令微调和预训练。SIFT-50M数据集由14,000小时公开可用的语音语料库构建而成，涵盖五种语言，并结合大型语言模型和现成的专业模型进行处理。该数据集支持多样化的语音理解和可控语音生成指令。利用SIFT-50M，我们训练了SIFT-LLM模型，在指令跟随基准测试中表现出色，同时在基础语音任务上也达到了竞争水平。此外，为了促进进一步研究，我们还发布了EvalSIFT，这是一个专门设计用来评估语音文本大模型指令跟随能力的基准数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 00:45:48 GMT</pubDate>
</item>
<item>
<title>BlockGaussian：高效高质量的大规模场景重建框架</title>
<link>https://arxiv.org/abs/2504.09048</link>
<guid>https://arxiv.org/abs/2504.09048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架BlockGaussian，实现大规模场景重建的高效率与高质量。</p><br /><br /><p><strong>摘要：</strong> 近期3D Gaussian Splatting (3DGS) 技术在新型视图合成中展现出巨大潜力，但大规模场景重建仍面临分区、优化和合并等挑战。本文介绍了一种名为BlockGaussian的新框架，通过引入基于内容感知的场景分区策略及基于可见性感知的块级优化，实现了高效且高质量的大型场景重建。具体而言，该方法根据区域内容复杂度变化平衡计算负载，解决了独立块优化中的监督不匹配问题，并提出了伪视图几何约束以缓解块合并时的渲染退化。实验表明，该方法在多个基准测试上不仅提升了优化速度达5倍，还提高了平均PSNR值1.21 dB，同时显著降低了计算需求，使大场景重建能在单块24GB显存设备上完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 22:05:55 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型训练中的伪推理路径问题及改进方法</title>
<link>https://arxiv.org/abs/2504.11468</link>
<guid>https://arxiv.org/abs/2504.11468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，监督微调可能损害后续强化学习，引入新数据集验证并提出改进方案。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了用于训练大型视觉语言模型（LVLMs）的监督微调（SFT）后接强化学习（RL）的主流范式，并揭示了一个重要发现：SFT可能导致“伪推理路径”的产生，这些路径虽看似模仿自专家模型，但实际上包含冗长、犹豫且较少信息量甚至错误的推理步骤，从而对后续RL造成负面影响。为系统性研究这一现象，我们设计了一个名为VLAA-Thinking的新多模态数据集，该数据集通过包含描述、推理蒸馏、答案重写和验证等六个步骤构建，提供了高质量的SFT逐步视觉推理轨迹以及来自同一数据源的更具挑战性的RL拆分。实验表明，尽管SFT有助于模型学习推理格式，但它常常使已对齐的模型陷入模仿性和僵化的推理模式，阻碍进一步学习。相比之下，基于Group Relative Policy Optimization（GRPO）算法并结合新颖混合奖励模块（整合感知与认知信号），我们的RL方法促进了更真实、适应性强的推理行为。基于Qwen2.5VL 3B的VLAA-Thinker模型在Open LMM Reasoning Leaderboard上达到了4B规模LVLMs中的最佳性能，比之前最先进的方法高出1.8%。我们希望这些发现能为开发具备推理能力的LVLMs提供有价值的见解，并指导该领域的未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:54:05 GMT</pubDate>
</item>
<item>
<title>ColorBench：评估视觉语言模型颜色理解能力的基准</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入ColorBench基准，评估视觉语言模型的颜色感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ColorBench的创新基准，用于评估视觉语言模型（VLMs）在颜色感知、推理和鲁棒性方面的能力。通过构建多样化的测试场景，ColorBench考察了这些模型如何处理颜色信息以及在不同颜色变换下的表现。对32种不同架构的VLMs进行广泛评估后发现，更大规模的语言模型比视觉编码器更重要，但现有模型在颜色理解上的性能差距较小，表明这一领域被忽视。此外，CoT推理可以提高颜色理解的准确性和鲁棒性，尽管这些任务本质上是视觉驱动的。虽然VLMs确实利用了颜色线索，但在某些任务中也可能产生误导。这些发现揭示了当前VLMs的关键局限性，并强调了提升颜色理解能力的重要性。ColorBench可作为多模态AI实现人类水平颜色理解的基础工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10514" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:36:26 GMT</pubDate>
</item>
<item>
<title>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10449</link>
<guid>https://arxiv.org/abs/2504.10449</guid>
<content:encoded><![CDATA[
Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:38:25 GMT</pubDate>
</item>
<item>
<title>LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models</title>
<link>https://arxiv.org/abs/2504.10430</link>
<guid>https://arxiv.org/abs/2504.10430</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:20:34 GMT</pubDate>
</item>
<item>
<title>Breaking the Data Barrier -- Building GUI Agents Through Task Generalization</title>
<link>https://arxiv.org/abs/2504.10127</link>
<guid>https://arxiv.org/abs/2504.10127</guid>
<content:encoded><![CDATA[
Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 07:35:02 GMT</pubDate>
</item>
<item>
<title>Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems</title>
<link>https://arxiv.org/abs/2504.09763</link>
<guid>https://arxiv.org/abs/2504.09763</guid>
<content:encoded><![CDATA[
Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 20:06:48 GMT</pubDate>
</item>
<item>
<title>Iterative Self-Training for Code Generation via Reinforced Re-Ranking</title>
<link>https://arxiv.org/abs/2504.09643</link>
<guid>https://arxiv.org/abs/2504.09643</guid>
<content:encoded><![CDATA[
Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:34:17 GMT</pubDate>
</item>
<item>
<title>How new data permeates LLM knowledge and how to dilute it</title>
<link>https://arxiv.org/abs/2504.09522</link>
<guid>https://arxiv.org/abs/2504.09522</guid>
<content:encoded><![CDATA[
Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 07:25:04 GMT</pubDate>
</item>
<item>
<title>VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search</title>
<link>https://arxiv.org/abs/2504.09130</link>
<guid>https://arxiv.org/abs/2504.09130</guid>
<content:encoded><![CDATA[
Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 04:37:30 GMT</pubDate>
</item>
<item>
<title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
<link>https://arxiv.org/abs/2504.08066</link>
<guid>https://arxiv.org/abs/2504.08066</guid>
<content:encoded><![CDATA[
AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 14:44:41 GMT</pubDate>
</item>
<item>
<title>InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models</title>
<link>https://arxiv.org/abs/2504.10479</link>
<guid>https://arxiv.org/abs/2504.10479</guid>
<content:encoded><![CDATA[
We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</title>
<link>https://arxiv.org/abs/2504.10415</link>
<guid>https://arxiv.org/abs/2504.10415</guid>
<content:encoded><![CDATA[
Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:00:13 GMT</pubDate>
</item>
<item>
<title>S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10368</link>
<guid>https://arxiv.org/abs/2504.10368</guid>
<content:encoded><![CDATA[
We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 12:13:23 GMT</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 08:12:52 GMT</pubDate>
</item>
<item>
<title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.10068</link>
<guid>https://arxiv.org/abs/2504.10068</guid>
<content:encoded><![CDATA[
Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 06:14:44 GMT</pubDate>
</item>
<item>
<title>FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding</title>
<link>https://arxiv.org/abs/2504.09925</link>
<guid>https://arxiv.org/abs/2504.09925</guid>
<content:encoded><![CDATA[
We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 02:33:29 GMT</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 14:47:22 GMT</pubDate>
</item>
<item>
<title>TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning</title>
<link>https://arxiv.org/abs/2504.09641</link>
<guid>https://arxiv.org/abs/2504.09641</guid>
<content:encoded><![CDATA[
Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:32:49 GMT</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 15:49:22 GMT</pubDate>
</item>
<item>
<title>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08837</link>
<guid>https://arxiv.org/abs/2504.08837</guid>
<content:encoded><![CDATA[
Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:41:56 GMT</pubDate>
</item>
<item>
<title>Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability</title>
<link>https://arxiv.org/abs/2504.08003</link>
<guid>https://arxiv.org/abs/2504.08003</guid>
<content:encoded><![CDATA[
OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 12:10:15 GMT</pubDate>
</item>
<item>
<title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title>
<link>https://arxiv.org/abs/2504.08791</link>
<guid>https://arxiv.org/abs/2504.08791</guid>
<content:encoded><![CDATA[
Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training</title>
<link>https://arxiv.org/abs/2504.09710</link>
<guid>https://arxiv.org/abs/2504.09710</guid>
<content:encoded><![CDATA[
Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 16:10:27 GMT</pubDate>
</item>
<item>
<title>BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</title>
<link>https://arxiv.org/abs/2504.01786</link>
<guid>https://arxiv.org/abs/2504.01786</guid>
<content:encoded><![CDATA[
3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 10:51:45 GMT</pubDate>
</item>
<item>
<title>UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2504.06908</link>
<guid>https://arxiv.org/abs/2504.06908</guid>
<content:encoded><![CDATA[
In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:10:51 GMT</pubDate>
</item>
<item>
<title>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</title>
<link>https://arxiv.org/abs/2504.08727</link>
<guid>https://arxiv.org/abs/2504.08727</guid>
<content:encoded><![CDATA[
We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:55:45 GMT</pubDate>
</item>
<item>
<title>Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization</title>
<link>https://arxiv.org/abs/2504.08641</link>
<guid>https://arxiv.org/abs/2504.08641</guid>
<content:encoded><![CDATA[
Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:41:43 GMT</pubDate>
</item>
<item>
<title>ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration</title>
<link>https://arxiv.org/abs/2504.08591</link>
<guid>https://arxiv.org/abs/2504.08591</guid>
<content:encoded><![CDATA[
Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 10:49:52 GMT</pubDate>
</item>
<item>
<title>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.07615</link>
<guid>https://arxiv.org/abs/2504.07615</guid>
<content:encoded><![CDATA[
Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 06:05:15 GMT</pubDate>
</item>
<item>
<title>Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging</title>
<link>https://arxiv.org/abs/2504.08635</link>
<guid>https://arxiv.org/abs/2504.08635</guid>
<content:encoded><![CDATA[
This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM &gt; 0.93, MSE &lt; 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:37:46 GMT</pubDate>
</item>
<item>
<title>Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</title>
<link>https://arxiv.org/abs/2504.07866</link>
<guid>https://arxiv.org/abs/2504.07866</guid>
<content:encoded><![CDATA[
We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:41:51 GMT</pubDate>
</item>
<item>
<title>SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2504.08192</link>
<guid>https://arxiv.org/abs/2504.08192</guid>
<content:encoded><![CDATA[
Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 21:24:03 GMT</pubDate>
</item>
<item>
<title>InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</title>
<link>https://arxiv.org/abs/2504.05303</link>
<guid>https://arxiv.org/abs/2504.05303</guid>
<content:encoded><![CDATA[
We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models</title>
<link>https://arxiv.org/abs/2504.05262</link>
<guid>https://arxiv.org/abs/2504.05262</guid>
<content:encoded><![CDATA[
Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\% accuracy on numerical addition, performance collapses to leq7.5\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 12:57:10 GMT</pubDate>
</item>
<item>
<title>CoRAG: Collaborative Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.01883</link>
<guid>https://arxiv.org/abs/2504.01883</guid>
<content:encoded><![CDATA[
Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 12:40:43 GMT</pubDate>
</item>
<item>
<title>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2504.08736</link>
<guid>https://arxiv.org/abs/2504.08736</guid>
<content:encoded><![CDATA[
In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:29:35 GMT</pubDate>
</item>
<item>
<title>SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08600</link>
<guid>https://arxiv.org/abs/2504.08600</guid>
<content:encoded><![CDATA[
Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:01:30 GMT</pubDate>
</item>
<item>
<title>PixelFlow: Pixel-Space Generative Models with Flow</title>
<link>https://arxiv.org/abs/2504.07963</link>
<guid>https://arxiv.org/abs/2504.07963</guid>
<content:encoded><![CDATA[
We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</title>
<link>https://arxiv.org/abs/2504.08685</link>
<guid>https://arxiv.org/abs/2504.08685</guid>
<content:encoded><![CDATA[
This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 12:46:20 GMT</pubDate>
</item>
<item>
<title>MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</title>
<link>https://arxiv.org/abs/2504.08388</link>
<guid>https://arxiv.org/abs/2504.08388</guid>
<content:encoded><![CDATA[
World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:41:04 GMT</pubDate>
</item>
<item>
<title>In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</title>
<link>https://arxiv.org/abs/2504.08366</link>
<guid>https://arxiv.org/abs/2504.08366</guid>
<content:encoded><![CDATA[
We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:01:09 GMT</pubDate>
</item>
<item>
<title>FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation</title>
<link>https://arxiv.org/abs/2504.07405</link>
<guid>https://arxiv.org/abs/2504.07405</guid>
<content:encoded><![CDATA[
With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 22:58:22 GMT</pubDate>
</item>
<item>
<title>Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2504.07961</link>
<guid>https://arxiv.org/abs/2504.07961</guid>
<content:encoded><![CDATA[
We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models</title>
<link>https://arxiv.org/abs/2504.07951</link>
<guid>https://arxiv.org/abs/2504.07951</guid>
<content:encoded><![CDATA[
Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>TAPNext: Tracking Any Point (TAP) as Next Token Prediction</title>
<link>https://arxiv.org/abs/2504.05579</link>
<guid>https://arxiv.org/abs/2504.05579</guid>
<content:encoded><![CDATA[
Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 20:28:42 GMT</pubDate>
</item>
<item>
<title>MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection</title>
<link>https://arxiv.org/abs/2504.06801</link>
<guid>https://arxiv.org/abs/2504.06801</guid>
<content:encoded><![CDATA[
Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 07:47:48 GMT</pubDate>
</item>
<item>
<title>Compass Control: Multi Object Orientation Control for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.06752</link>
<guid>https://arxiv.org/abs/2504.06752</guid>
<content:encoded><![CDATA[
Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 06:15:15 GMT</pubDate>
</item>
<item>
<title>C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing</title>
<link>https://arxiv.org/abs/2504.07964</link>
<guid>https://arxiv.org/abs/2504.07964</guid>
<content:encoded><![CDATA[
Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.07960</link>
<guid>https://arxiv.org/abs/2504.07960</guid>
<content:encoded><![CDATA[
Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>MM-IFEngine: Towards Multimodal Instruction Following</title>
<link>https://arxiv.org/abs/2504.07957</link>
<guid>https://arxiv.org/abs/2504.07957</guid>
<content:encoded><![CDATA[
The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.07956</link>
<guid>https://arxiv.org/abs/2504.07956</guid>
<content:encoded><![CDATA[
The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>HoloPart: Generative 3D Part Amodal Segmentation</title>
<link>https://arxiv.org/abs/2504.07943</link>
<guid>https://arxiv.org/abs/2504.07943</guid>
<content:encoded><![CDATA[
3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:53:31 GMT</pubDate>
</item>
<item>
<title>SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement</title>
<link>https://arxiv.org/abs/2504.07934</link>
<guid>https://arxiv.org/abs/2504.07934</guid>
<content:encoded><![CDATA[
In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:49:05 GMT</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:06:54 GMT</pubDate>
</item>
<item>
<title>Kimi-VL Technical Report</title>
<link>https://arxiv.org/abs/2504.07491</link>
<guid>https://arxiv.org/abs/2504.07491</guid>
<content:encoded><![CDATA[
We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 02:48:26 GMT</pubDate>
</item>
<item>
<title>Towards Visual Text Grounding of Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.04974</link>
<guid>https://arxiv.org/abs/2504.04974</guid>
<content:encoded><![CDATA[
Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 08:01:59 GMT</pubDate>
</item>
<item>
<title>DeepSeek-R1 Thoughtology: Let's  about LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.07128</link>
<guid>https://arxiv.org/abs/2504.07128</guid>
<content:encoded><![CDATA[
Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 20:36:08 GMT</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 14:30:18 GMT</pubDate>
</item>
<item>
<title>Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.05541</link>
<guid>https://arxiv.org/abs/2504.05541</guid>
<content:encoded><![CDATA[
We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:35:36 GMT</pubDate>
</item>
<item>
<title>Pretraining Language Models for Diachronic Linguistic Change Discovery</title>
<link>https://arxiv.org/abs/2504.05523</link>
<guid>https://arxiv.org/abs/2504.05523</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:51:32 GMT</pubDate>
</item>
<item>
<title>Are We Done with Object-Centric Learning?</title>
<link>https://arxiv.org/abs/2504.07092</link>
<guid>https://arxiv.org/abs/2504.07092</guid>
<content:encoded><![CDATA[
Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>Self-Steering Language Models</title>
<link>https://arxiv.org/abs/2504.07081</link>
<guid>https://arxiv.org/abs/2504.07081</guid>
<content:encoded><![CDATA[
While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:54:22 GMT</pubDate>
</item>
<item>
<title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
<link>https://arxiv.org/abs/2504.07046</link>
<guid>https://arxiv.org/abs/2504.07046</guid>
<content:encoded><![CDATA[
Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.06958</link>
<guid>https://arxiv.org/abs/2504.06958</guid>
<content:encoded><![CDATA[
Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 11:09:27 GMT</pubDate>
</item>
<item>
<title>RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts</title>
<link>https://arxiv.org/abs/2504.06947</link>
<guid>https://arxiv.org/abs/2504.06947</guid>
<content:encoded><![CDATA[
In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:54:00 GMT</pubDate>
</item>
<item>
<title>Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.06719</link>
<guid>https://arxiv.org/abs/2504.06719</guid>
<content:encoded><![CDATA[
Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 05:19:49 GMT</pubDate>
</item>
<item>
<title>WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments</title>
<link>https://arxiv.org/abs/2504.03886</link>
<guid>https://arxiv.org/abs/2504.03886</guid>
<content:encoded><![CDATA[
We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 15:19:40 GMT</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:58 GMT</pubDate>
</item>
<item>
<title>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</title>
<link>https://arxiv.org/abs/2504.07086</link>
<guid>https://arxiv.org/abs/2504.07086</guid>
<content:encoded><![CDATA[
Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:17 GMT</pubDate>
</item>
<item>
<title>RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception</title>
<link>https://arxiv.org/abs/2504.05287</link>
<guid>https://arxiv.org/abs/2504.05287</guid>
<content:encoded><![CDATA[
Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:38:19 GMT</pubDate>
</item>
<item>
<title>DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion</title>
<link>https://arxiv.org/abs/2504.04010</link>
<guid>https://arxiv.org/abs/2504.04010</guid>
<content:encoded><![CDATA[
Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 21:19:46 GMT</pubDate>
</item>
<item>
<title>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens</title>
<link>https://arxiv.org/abs/2504.07096</link>
<guid>https://arxiv.org/abs/2504.07096</guid>
<content:encoded><![CDATA[
We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</title>
<link>https://arxiv.org/abs/2504.07083</link>
<guid>https://arxiv.org/abs/2504.07083</guid>
<content:encoded><![CDATA[
Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?</title>
<link>https://arxiv.org/abs/2504.06514</link>
<guid>https://arxiv.org/abs/2504.06514</guid>
<content:encoded><![CDATA[
We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</title>
<link>https://arxiv.org/abs/2504.04842</link>
<guid>https://arxiv.org/abs/2504.04842</guid>
<content:encoded><![CDATA[
Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 04:56:01 GMT</pubDate>
</item>
<item>
<title>DDT: Decoupled Diffusion Transformer</title>
<link>https://arxiv.org/abs/2504.05741</link>
<guid>https://arxiv.org/abs/2504.05741</guid>
<content:encoded><![CDATA[
Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \color{ddtD}ecoupled \color{ddtD}iffusion \color{ddtT}ransformer~(\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 03:17:45 GMT</pubDate>
</item>
<item>
<title>HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference</title>
<link>https://arxiv.org/abs/2504.05897</link>
<guid>https://arxiv.org/abs/2504.05897</guid>
<content:encoded><![CDATA[
The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 06:47:37 GMT</pubDate>
</item>
<item>
<title>Efficient Reinforcement Finetuning via Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.05520</link>
<guid>https://arxiv.org/abs/2504.05520</guid>
<content:encoded><![CDATA[
Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:31:31 GMT</pubDate>
</item>
<item>
<title>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2504.03755</link>
<guid>https://arxiv.org/abs/2504.03755</guid>
<content:encoded><![CDATA[
Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 02:13:14 GMT</pubDate>
</item>
<item>
<title>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</title>
<link>https://arxiv.org/abs/2504.06122</link>
<guid>https://arxiv.org/abs/2504.06122</guid>
<content:encoded><![CDATA[
Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:15:26 GMT</pubDate>
</item>
<item>
<title>HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</title>
<link>https://arxiv.org/abs/2504.06232</link>
<guid>https://arxiv.org/abs/2504.06232</guid>
<content:encoded><![CDATA[
Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:30:40 GMT</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:43:01 GMT</pubDate>
</item>
<item>
<title>COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values</title>
<link>https://arxiv.org/abs/2504.05535</link>
<guid>https://arxiv.org/abs/2504.05535</guid>
<content:encoded><![CDATA[
Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:15:51 GMT</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[
Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 09:28:57 GMT</pubDate>
</item>
<item>
<title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
<link>https://arxiv.org/abs/2504.06263</link>
<guid>https://arxiv.org/abs/2504.06263</guid>
<content:encoded><![CDATA[
Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:49 GMT</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>An Empirical Study of GPT-4o Image Generation Capabilities</title>
<link>https://arxiv.org/abs/2504.05979</link>
<guid>https://arxiv.org/abs/2504.05979</guid>
<content:encoded><![CDATA[
The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 08:34:36 GMT</pubDate>
</item>
<item>
<title>Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought</title>
<link>https://arxiv.org/abs/2504.05599</link>
<guid>https://arxiv.org/abs/2504.05599</guid>
<content:encoded><![CDATA[
We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:19:20 GMT</pubDate>
</item>
<item>
<title>Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2504.05594</link>
<guid>https://arxiv.org/abs/2504.05594</guid>
<content:encoded><![CDATA[
Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:02:50 GMT</pubDate>
</item>
<item>
<title>Generative Evaluation of Complex Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02810</link>
<guid>https://arxiv.org/abs/2504.02810</guid>
<content:encoded><![CDATA[
With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:54:18 GMT</pubDate>
</item>
<item>
<title>Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</title>
<link>https://arxiv.org/abs/2504.02160</link>
<guid>https://arxiv.org/abs/2504.02160</guid>
<content:encoded><![CDATA[
Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 18:20:21 GMT</pubDate>
</item>
<item>
<title>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</title>
<link>https://arxiv.org/abs/2504.00043</link>
<guid>https://arxiv.org/abs/2504.00043</guid>
<content:encoded><![CDATA[
Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 16:03:36 GMT</pubDate>
</item>
<item>
<title>3D Scene Understanding Through Local Random Access Sequence Modeling</title>
<link>https://arxiv.org/abs/2504.03875</link>
<guid>https://arxiv.org/abs/2504.03875</guid>
<content:encoded><![CDATA[
3D scene understanding from single images is a pivotal problem in computer vision with numerous downstream applications in graphics, augmented reality, and robotics. While diffusion-based modeling approaches have shown promise, they often struggle to maintain object and scene consistency, especially in complex real-world scenarios. To address these limitations, we propose an autoregressive generative approach called Local Random Access Sequence (LRAS) modeling, which uses local patch quantization and randomly ordered sequence generation. By utilizing optical flow as an intermediate representation for 3D scene editing, our experiments demonstrate that LRAS achieves state-of-the-art novel view synthesis and 3D object manipulation capabilities. Furthermore, we show that our framework naturally extends to self-supervised depth estimation through a simple modification of the sequence design. By achieving strong performance on multiple 3D scene understanding tasks, LRAS provides a unified and effective framework for building the next generation of 3D vision models.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 14:59:41 GMT</pubDate>
</item>
<item>
<title>Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking</title>
<link>https://arxiv.org/abs/2504.03947</link>
<guid>https://arxiv.org/abs/2504.03947</guid>
<content:encoded><![CDATA[
We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 17:27:48 GMT</pubDate>
</item>
<item>
<title>VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.05118</link>
<guid>https://arxiv.org/abs/2504.05118</guid>
<content:encoded><![CDATA[
We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 10:21:11 GMT</pubDate>
</item>
<item>
<title>GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.04155</link>
<guid>https://arxiv.org/abs/2504.04155</guid>
<content:encoded><![CDATA[
Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:30:58 GMT</pubDate>
</item>
<item>
<title>Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</title>
<link>https://arxiv.org/abs/2504.04152</link>
<guid>https://arxiv.org/abs/2504.04152</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:10:55 GMT</pubDate>
</item>
<item>
<title>Rethinking Reflection in Pre-Training</title>
<link>https://arxiv.org/abs/2504.04022</link>
<guid>https://arxiv.org/abs/2504.04022</guid>
<content:encoded><![CDATA[
A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 22:24:07 GMT</pubDate>
</item>
<item>
<title>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</title>
<link>https://arxiv.org/abs/2504.03151</link>
<guid>https://arxiv.org/abs/2504.03151</guid>
<content:encoded><![CDATA[
Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:04:56 GMT</pubDate>
</item>
<item>
<title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title>
<link>https://arxiv.org/abs/2504.03790</link>
<guid>https://arxiv.org/abs/2504.03790</guid>
<content:encoded><![CDATA[
Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 20:41:40 GMT</pubDate>
</item>
<item>
<title>SmolVLM: Redefining small and efficient multimodal models</title>
<link>https://arxiv.org/abs/2504.05299</link>
<guid>https://arxiv.org/abs/2504.05299</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>LiveVQA: Live Visual Knowledge Seeking</title>
<link>https://arxiv.org/abs/2504.05288</link>
<guid>https://arxiv.org/abs/2504.05288</guid>
<content:encoded><![CDATA[
We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:39:31 GMT</pubDate>
</item>
<item>
<title>T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models</title>
<link>https://arxiv.org/abs/2504.04718</link>
<guid>https://arxiv.org/abs/2504.04718</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:01:17 GMT</pubDate>
</item>
<item>
<title>Clinical ModernBERT: An efficient and long context encoder for biomedical text</title>
<link>https://arxiv.org/abs/2504.03964</link>
<guid>https://arxiv.org/abs/2504.03964</guid>
<content:encoded><![CDATA[
We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 18:14:12 GMT</pubDate>
</item>
<item>
<title>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2504.03193</link>
<guid>https://arxiv.org/abs/2504.03193</guid>
<content:encoded><![CDATA[
Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 01:44:45 GMT</pubDate>
</item>
<item>
<title>BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.02812</link>
<guid>https://arxiv.org/abs/2504.02812</guid>
<content:encoded><![CDATA[
We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:55:19 GMT</pubDate>
</item>
<item>
<title>DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2504.02882</link>
<guid>https://arxiv.org/abs/2504.02882</guid>
<content:encoded><![CDATA[
Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:47:28 GMT</pubDate>
</item>
<item>
<title>URECA: Unique Region Caption Anything</title>
<link>https://arxiv.org/abs/2504.05305</link>
<guid>https://arxiv.org/abs/2504.05305</guid>
<content:encoded><![CDATA[
Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Gaussian Mixture Flow Matching Models</title>
<link>https://arxiv.org/abs/2504.05304</link>
<guid>https://arxiv.org/abs/2504.05304</guid>
<content:encoded><![CDATA[
Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>One-Minute Video Generation with Test-Time Training</title>
<link>https://arxiv.org/abs/2504.05298</link>
<guid>https://arxiv.org/abs/2504.05298</guid>
<content:encoded><![CDATA[
Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:56:31 GMT</pubDate>
</item>
<item>
<title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title>
<link>https://arxiv.org/abs/2504.04823</link>
<guid>https://arxiv.org/abs/2504.04823</guid>
<content:encoded><![CDATA[
Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 04:22:45 GMT</pubDate>
</item>
<item>
<title>Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs</title>
<link>https://arxiv.org/abs/2504.04715</link>
<guid>https://arxiv.org/abs/2504.04715</guid>
<content:encoded><![CDATA[
The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https://github.com/sunblaze-ucb/llm-api-audit
]]></content:encoded>
<pubDate>Sun, 06 Apr 2025 23:57:41 GMT</pubDate>
</item>
<item>
<title>Concept Lancet: Image Editing with Compositional Representation Transplant</title>
<link>https://arxiv.org/abs/2504.02828</link>
<guid>https://arxiv.org/abs/2504.02828</guid>
<content:encoded><![CDATA[
Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose Concept Lancet (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace/add/remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan-150K, which contains diverse descriptions and scenarios of visual terms and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.03770</link>
<guid>https://arxiv.org/abs/2504.03770</guid>
<content:encoded><![CDATA[
Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 01:00:28 GMT</pubDate>
</item>
<item>
<title>ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning</title>
<link>https://arxiv.org/abs/2503.22738</link>
<guid>https://arxiv.org/abs/2503.22738</guid>
<content:encoded><![CDATA[
Autonomous agents powered by foundation models have seen widespread adoption across various real-world applications. However, they remain highly vulnerable to malicious instructions and attacks, which can result in severe consequences such as privacy breaches and financial losses. More critically, existing guardrails for LLMs are not applicable due to the complex and dynamic nature of agents. To tackle these challenges, we propose ShieldAgent, the first guardrail agent designed to enforce explicit safety policy compliance for the action trajectory of other protected agents through logical reasoning. Specifically, ShieldAgent first constructs a safety policy model by extracting verifiable rules from policy documents and structuring them into a set of action-based probabilistic rule circuits. Given the action trajectory of the protected agent, ShieldAgent retrieves relevant rule circuits and generates a shielding plan, leveraging its comprehensive tool library and executable code for formal verification. In addition, given the lack of guardrail benchmarks for agents, we introduce ShieldAgent-Bench, a dataset with 3K safety-related pairs of agent instructions and action trajectories, collected via SOTA attacks across 6 web environments and 7 risk categories. Experiments show that ShieldAgent achieves SOTA on ShieldAgent-Bench and three existing benchmarks, outperforming prior methods by 11.3% on average with a high recall of 90.1%. Additionally, ShieldAgent reduces API queries by 64.7% and inference time by 58.2%, demonstrating its high precision and efficiency in safeguarding agents.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:58:40 GMT</pubDate>
</item>
<item>
<title>Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin for Real-World Robot Policy Evaluation</title>
<link>https://arxiv.org/abs/2504.03597</link>
<guid>https://arxiv.org/abs/2504.03597</guid>
<content:encoded><![CDATA[
Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:05:56 GMT</pubDate>
</item>
<item>
<title>Delineate Anything: Resolution-Agnostic Field Boundary Delineation on Satellite Imagery</title>
<link>https://arxiv.org/abs/2504.02534</link>
<guid>https://arxiv.org/abs/2504.02534</guid>
<content:encoded><![CDATA[
The accurate delineation of agricultural field boundaries from satellite imagery is vital for land management and crop monitoring. However, current methods face challenges due to limited dataset sizes, resolution discrepancies, and diverse environmental conditions. We address this by reformulating the task as instance segmentation and introducing the Field Boundary Instance Segmentation - 22M dataset (FBIS-22M), a large-scale, multi-resolution dataset comprising 672,909 high-resolution satellite image patches (ranging from 0.25 m to 10 m) and 22,926,427 instance masks of individual fields, significantly narrowing the gap between agricultural datasets and those in other computer vision domains. We further propose Delineate Anything, an instance segmentation model trained on our new FBIS-22M dataset. Our proposed model sets a new state-of-the-art, achieving a substantial improvement of 88.5% in mAP@0.5 and 103% in mAP@0.5:0.95 over existing methods, while also demonstrating significantly faster inference and strong zero-shot generalization across diverse image resolutions and unseen geographic regions. Code, pre-trained models, and the FBIS-22M dataset are available at https://lavreniuk.github.io/Delineate-Anything.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 08:37:04 GMT</pubDate>
</item>
<item>
<title>MedSAM2: Segment Anything in 3D Medical Images and Videos</title>
<link>https://arxiv.org/abs/2504.03600</link>
<guid>https://arxiv.org/abs/2504.03600</guid>
<content:encoded><![CDATA[
Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:13:37 GMT</pubDate>
</item>
<item>
<title>MegaMath: Pushing the Limits of Open Math Corpora</title>
<link>https://arxiv.org/abs/2504.02807</link>
<guid>https://arxiv.org/abs/2504.02807</guid>
<content:encoded><![CDATA[
Mathematical reasoning is a cornerstone of human intelligence and a key benchmark for advanced capabilities in large language models (LLMs). However, the research community still lacks an open, large-scale, high-quality corpus tailored to the demands of math-centric LLM pre-training. We present MegaMath, an open dataset curated from diverse, math-focused sources through following practices: (1) Revisiting web data: We re-extracted mathematical documents from Common Crawl with math-oriented HTML optimizations, fasttext-based filtering and deduplication, all for acquiring higher-quality data on the Internet. (2) Recalling Math-related code data: We identified high quality math-related code from large code training corpus, Stack-V2, further enhancing data diversity. (3) Exploring Synthetic data: We synthesized QA-style text, math-related code, and interleaved text-code blocks from web data or code data. By integrating these strategies and validating their effectiveness through extensive ablations, MegaMath delivers 371B tokens with the largest quantity and top quality among existing open math pre-training datasets.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:52:07 GMT</pubDate>
</item>
<item>
<title>Slow-Fast Architecture for Video Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2504.01328</link>
<guid>https://arxiv.org/abs/2504.01328</guid>
<content:encoded><![CDATA[
Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) "fast" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) "slow" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 23:24:58 GMT</pubDate>
</item>
<item>
<title>SPF-Portrait: Towards Pure Portrait Customization with Semantic Pollution-Free Fine-tuning</title>
<link>https://arxiv.org/abs/2504.00396</link>
<guid>https://arxiv.org/abs/2504.00396</guid>
<content:encoded><![CDATA[
Fine-tuning a pre-trained Text-to-Image (T2I) model on a tailored portrait dataset is the mainstream method for text-driven customization of portrait attributes. Due to Semantic Pollution during fine-tuning, existing methods struggle to maintain the original model's behavior and achieve incremental learning while customizing target attributes. To address this issue, we propose SPF-Portrait, a pioneering work to purely understand customized semantics while eliminating semantic pollution in text-driven portrait customization. In our SPF-Portrait, we propose a dual-path pipeline that introduces the original model as a reference for the conventional fine-tuning path. Through contrastive learning, we ensure adaptation to target attributes and purposefully align other unrelated attributes with the original portrait. We introduce a novel Semantic-Aware Fine Control Map, which represents the precise response regions of the target semantics, to spatially guide the alignment process between the contrastive paths. This alignment process not only effectively preserves the performance of the original model but also avoids over-alignment. Furthermore, we propose a novel response enhancement mechanism to reinforce the performance of target attributes, while mitigating representation discrepancy inherent in direct cross-modal supervision. Extensive experiments demonstrate that SPF-Portrait achieves state-of-the-art performance. Project webpage: https://spf-portrait.github.io/SPF-Portrait/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 23:37:30 GMT</pubDate>
</item>
<item>
<title>TransMamba: Flexibly Switching between Transformer and Mamba</title>
<link>https://arxiv.org/abs/2503.24067</link>
<guid>https://arxiv.org/abs/2503.24067</guid>
<content:encoded><![CDATA[
Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 09:26:24 GMT</pubDate>
</item>
<item>
<title>APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay</title>
<link>https://arxiv.org/abs/2504.03601</link>
<guid>https://arxiv.org/abs/2504.03601</guid>
<content:encoded><![CDATA[
Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on tau-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:13:57 GMT</pubDate>
</item>
<item>
<title>HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration</title>
<link>https://arxiv.org/abs/2504.03536</link>
<guid>https://arxiv.org/abs/2504.03536</guid>
<content:encoded><![CDATA[
Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce HumanDreamer-X, a novel framework that integrates multi-view human generation and reconstruction into a unified pipeline, which significantly enhances the geometric consistency and visual fidelity of the reconstructed 3D models. In this framework, 3D Gaussian Splatting serves as an explicit 3D representation to provide initial geometry and appearance priority. Building upon this foundation, HumanFixer is trained to restore 3DGS renderings, which guarantee photorealistic results. Furthermore, we delve into the inherent challenges associated with attention mechanisms in multi-view human generation, and propose an attention modulation strategy that effectively enhances geometric details identity consistency across multi-view. Experimental results demonstrate that our approach markedly improves generation and reconstruction PSNR quality metrics by 16.45% and 12.65%, respectively, achieving a PSNR of up to 25.62 dB, while also showing generalization capabilities on in-the-wild data and applicability to various human reconstruction backbone models.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 11:35:14 GMT</pubDate>
</item>
<item>
<title>Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization</title>
<link>https://arxiv.org/abs/2504.03011</link>
<guid>https://arxiv.org/abs/2504.03011</guid>
<content:encoded><![CDATA[
This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 16:10:50 GMT</pubDate>
</item>
<item>
<title>EvMic: Event-based Non-contact sound recovery from effective spatial-temporal modeling</title>
<link>https://arxiv.org/abs/2504.02402</link>
<guid>https://arxiv.org/abs/2504.02402</guid>
<content:encoded><![CDATA[
When sound waves hit an object, they induce vibrations that produce high-frequency and subtle visual changes, which can be used for recovering the sound. Early studies always encounter trade-offs related to sampling rate, bandwidth, field of view, and the simplicity of the optical path. Recent advances in event camera hardware show good potential for its application in visual sound recovery, because of its superior ability in capturing high-frequency signals. However, existing event-based vibration recovery methods are still sub-optimal for sound recovery. In this work, we propose a novel pipeline for non-contact sound recovery, fully utilizing spatial-temporal information from the event stream. We first generate a large training set using a novel simulation pipeline. Then we designed a network that leverages the sparsity of events to capture spatial information and uses Mamba to model long-term temporal information. Lastly, we train a spatial aggregation block to aggregate information from different locations to further improve signal quality. To capture event signals caused by sound waves, we also designed an imaging system using a laser matrix to enhance the gradient and collected multiple data sequences for testing. Experimental results on synthetic and real-world data demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 04:51:17 GMT</pubDate>
</item>
<item>
<title>BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models</title>
<link>https://arxiv.org/abs/2503.24310</link>
<guid>https://arxiv.org/abs/2503.24310</guid>
<content:encoded><![CDATA[
In this research, we introduce BEATS, a novel framework for evaluating Bias, Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon the BEATS framework, we present a bias benchmark for LLMs that measure performance across 29 distinct metrics. These metrics span a broad range of characteristics, including demographic, cognitive, and social biases, as well as measures of ethical reasoning, group fairness, and factuality related misinformation risk. These metrics enable a quantitative assessment of the extent to which LLM generated responses may perpetuate societal prejudices that reinforce or expand systemic inequities. To achieve a high score on this benchmark a LLM must show very equitable behavior in their responses, making it a rigorous standard for responsible AI evaluation. Empirical results based on data from our experiment show that, 37.65\% of outputs generated by industry leading models contained some form of bias, highlighting a substantial risk of using these models in critical decision making systems. BEATS framework and benchmark offer a scalable and statistically rigorous methodology to benchmark LLMs, diagnose factors driving biases, and develop mitigation strategies. With the BEATS framework, our goal is to help the development of more socially responsible and ethically aligned AI models.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 12:56:52 GMT</pubDate>
</item>
<item>
<title>MME-Unify: A Comprehensive Benchmark for Unified Multimodal Understanding and Generation Models</title>
<link>https://arxiv.org/abs/2504.03641</link>
<guid>https://arxiv.org/abs/2504.03641</guid>
<content:encoded><![CDATA[
Existing MLLM benchmarks face significant challenges in evaluating Unified MLLMs (U-MLLMs) due to: 1) lack of standardized benchmarks for traditional tasks, leading to inconsistent comparisons; 2) absence of benchmarks for mixed-modality generation, which fails to assess multimodal reasoning capabilities. We present a comprehensive evaluation framework designed to systematically assess U-MLLMs. Our benchmark includes: Standardized Traditional Task Evaluation. We sample from 12 datasets, covering 10 tasks with 30 subtasks, ensuring consistent and fair comparisons across studies." 2. Unified Task Assessment. We introduce five novel tasks testing multimodal reasoning, including image editing, commonsense QA with image generation, and geometric reasoning. 3. Comprehensive Model Benchmarking. We evaluate 12 leading U-MLLMs, such as Janus-Pro, EMU3, VILA-U, and Gemini2-flash, alongside specialized understanding (e.g., Claude-3.5-Sonnet) and generation models (e.g., DALL-E-3). Our findings reveal substantial performance gaps in existing U-MLLMs, highlighting the need for more robust models capable of handling mixed-modality tasks effectively. The code and evaluation data can be found in https://mme-unify.github.io/.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 12:10:57 GMT</pubDate>
</item>
<item>
<title>Agentic Knowledgeable Self-awareness</title>
<link>https://arxiv.org/abs/2504.03553</link>
<guid>https://arxiv.org/abs/2504.03553</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 12:03:38 GMT</pubDate>
</item>
<item>
<title>VARGPT-v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.02949</link>
<guid>https://arxiv.org/abs/2504.02949</guid>
<content:encoded><![CDATA[
In this work, we present VARGPT-v1.1, an advanced unified visual autoregressive model that builds upon our previous framework VARGPT. The model preserves the dual paradigm of next-token prediction for visual understanding and next-scale generation for image synthesis. Specifically, VARGPT-v1.1 integrates: (1) a novel training strategy combining iterative visual instruction tuning with reinforcement learning through Direct Preference Optimization (DPO), (2) an expanded training corpus containing 8.3M visual-generative instruction pairs, (3) an upgraded language model backbone using Qwen2, (4) enhanced image generation resolution, and (5) emergent image editing capabilities without architectural modifications. These advancements enable VARGPT-v1.1 to achieve state-of-the-art performance in multimodal understanding and text-to-image instruction-following tasks, demonstrating significant improvements in both comprehension and generation metrics. Notably, through visual instruction tuning, the model acquires image editing functionality while maintaining architectural consistency with its predecessor, revealing the potential for unified visual understanding, generation, and editing. Our findings suggest that well-designed unified visual autoregressive models can effectively adopt flexible training strategies from large language models (LLMs), exhibiting promising scalability. The codebase and model weights are publicly available at https://github.com/VARGPT-family/VARGPT-v1.1.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 14:06:28 GMT</pubDate>
</item>
<item>
<title>Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving</title>
<link>https://arxiv.org/abs/2504.02605</link>
<guid>https://arxiv.org/abs/2504.02605</guid>
<content:encoded><![CDATA[
The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 10:06:17 GMT</pubDate>
</item>
<item>
<title>OpenCodeReasoning: Advancing Data Distillation for Competitive Coding</title>
<link>https://arxiv.org/abs/2504.01943</link>
<guid>https://arxiv.org/abs/2504.01943</guid>
<content:encoded><![CDATA[
Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:50:31 GMT</pubDate>
</item>
<item>
<title>Scene-Centric Unsupervised Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2504.01955</link>
<guid>https://arxiv.org/abs/2504.01955</guid>
<content:encoded><![CDATA[
Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:58:46 GMT</pubDate>
</item>
<item>
<title>FreSca: Unveiling the Scaling Space in Diffusion Models</title>
<link>https://arxiv.org/abs/2504.02154</link>
<guid>https://arxiv.org/abs/2504.02154</guid>
<content:encoded><![CDATA[
Diffusion models offer impressive controllability for image tasks, primarily through noise predictions that encode task-specific information and classifier-free guidance enabling adjustable scaling. This scaling mechanism implicitly defines a ``scaling space'' whose potential for fine-grained semantic manipulation remains underexplored. We investigate this space, starting with inversion-based editing where the difference between conditional/unconditional noise predictions carries key semantic information. Our core contribution stems from a Fourier analysis of noise predictions, revealing that its low- and high-frequency components evolve differently throughout diffusion. Based on this insight, we introduce FreSca, a straightforward method that applies guidance scaling independently to different frequency bands in the Fourier domain. FreSca demonstrably enhances existing image editing methods without retraining. Excitingly, its effectiveness extends to image understanding tasks such as depth estimation, yielding quantitative gains across multiple datasets.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 18:03:11 GMT</pubDate>
</item>
<item>
<title>WikiVideo: Article Generation from Multiple Videos</title>
<link>https://arxiv.org/abs/2504.00939</link>
<guid>https://arxiv.org/abs/2504.00939</guid>
<content:encoded><![CDATA[
We present the challenging task of automatically creating a high-level Wikipedia-style article that aggregates information from multiple diverse videos about real-world events, such as natural disasters or political elections. Videos are intuitive sources for retrieval-augmented generation (RAG), but most contemporary RAG workflows focus heavily on text and existing methods for video-based summarization focus on low-level scene understanding rather than high-level event semantics. To close this gap, we introduce WikiVideo, a benchmark consisting of expert-written articles and densely annotated videos that provide evidence for articles' claims, facilitating the integration of video into RAG pipelines and enabling the creation of in-depth content that is grounded in multimodal sources. We further propose Collaborative Article Generation (CAG), a novel interactive method for article creation from multiple videos. CAG leverages an iterative interaction between an r1-style reasoning model and a VideoLLM to draw higher level inferences about the target event than is possible with VideoLLMs alone, which fixate on low-level visual features. We benchmark state-of-the-art VideoLLMs and CAG in both oracle retrieval and RAG settings and find that CAG consistently outperforms alternative methods, while suggesting intriguing avenues for future work.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 12:22:15 GMT</pubDate>
</item>
<item>
<title>JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization</title>
<link>https://arxiv.org/abs/2503.23377</link>
<guid>https://arxiv.org/abs/2503.23377</guid>
<content:encoded><![CDATA[
This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 05:40:42 GMT</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Generalist Reward Modeling</title>
<link>https://arxiv.org/abs/2504.02495</link>
<guid>https://arxiv.org/abs/2504.02495</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has been widely adopted in post-training for large language models (LLMs) at scale. Recently, the incentivization of reasoning capabilities in LLMs from RL indicates that proper learning methods could enable effective inference-time scalability. A key challenge of RL is to obtain accurate reward signals for LLMs in various domains beyond verifiable questions or artificial rules. In this work, we investigate how to improve reward modeling (RM) with more inference compute for general queries, i.e. the inference-time scalability of generalist RM, and further, how to improve the effectiveness of performance-compute scaling with proper learning methods. For the RM approach, we adopt pointwise generative reward modeling (GRM) to enable flexibility for different input types and potential for inference-time scaling. For the learning method, we propose Self-Principled Critique Tuning (SPCT) to foster scalable reward generation behaviors in GRMs through online RL, to generate principles adaptively and critiques accurately, resulting in DeepSeek-GRM models. Furthermore, for effective inference-time scaling, we use parallel sampling to expand compute usage, and introduce a meta RM to guide voting process for better scaling performance. Empirically, we show that SPCT significantly improves the quality and scalability of GRMs, outperforming existing methods and models in various RM benchmarks without severe biases, and could achieve better performance compared to training-time scaling. DeepSeek-GRM still meets challenges in some tasks, which we believe can be addressed by future efforts in generalist reward systems. The models will be released and open-sourced.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 07:19:49 GMT</pubDate>
</item>
<item>
<title>Whisper-LM: Improving ASR Models with Language Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2503.23542</link>
<guid>https://arxiv.org/abs/2503.23542</guid>
<content:encoded><![CDATA[
Automatic speech recognition systems have undoubtedly advanced with the integration of multilingual and multitask models such as Whisper, which have shown a promising ability to understand and process speech across a wide range of languages. Despite their robustness, these models often fall short in handling the linguistic distinctions of minority languages. This study addresses this gap by integrating traditional and novel language models with fine-tuned Whisper models to raise their performance in less commonly studied languages. Through rigorous fine-tuning and evaluation across multiple datasets, we demonstrate substantial improvements in word error rate, particularly in low-resource scenarios. Our approach not only does take advantage of the extensive data Whisper was pre-trained on, but also complements its linguistic adaptability by incorporating language models. We obtained improvements up to 51\% for in-distribution datasets and up to 34\% for out-of-distribution sentences using statistical language models, while large language models provided moderate but consistently robust improvement across diverse linguistic contexts. The findings reveal that, while the integration reliably benefits all model sizes, the extent of improvement varies, highlighting the importance of optimized language model parameters. Finally, we emphasize the importance of selecting appropriate evaluation parameters when reporting the results using transformer-based ASR models. In summary, this research clears the way for more inclusive ASR technologies that perform better across languages by enriching their linguistic knowledge. For further implementation details of this study, the technical documentation and source code are available at http://www.github.com/hitz-zentroa/whisper-lm.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 14:03:52 GMT</pubDate>
</item>
<item>
<title>NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations</title>
<link>https://arxiv.org/abs/2503.23162</link>
<guid>https://arxiv.org/abs/2503.23162</guid>
<content:encoded><![CDATA[
3D Gaussian Splatting (3DGS) demonstrates superior quality and rendering speed, but with millions of 3D Gaussians and significant storage and transmission costs. Recent 3DGS compression methods mainly concentrate on compressing Scaffold-GS, achieving impressive performance but with an additional voxel structure and a complex encoding and quantization strategy. In this paper, we aim to develop a simple yet effective method called NeuralGS that explores in another way to compress the original 3DGS into a compact representation without the voxel structure and complex quantization strategies. Our observation is that neural fields like NeRF can represent complex 3D scenes with Multi-Layer Perceptron (MLP) neural networks using only a few megabytes. Thus, NeuralGS effectively adopts the neural field representation to encode the attributes of 3D Gaussians with MLPs, only requiring a small storage size even for a large-scale scene. To achieve this, we adopt a clustering strategy and fit the Gaussians with different tiny MLPs for each cluster, based on importance scores of Gaussians as fitting weights. We experiment on multiple datasets, achieving a 45-times average model size reduction without harming the visual quality. The compression performance of our method on original 3DGS is comparable to the dedicated Scaffold-GS-based compression methods, which demonstrate the huge potential of directly compressing original 3DGS with neural fields.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 13:36:53 GMT</pubDate>
</item>
<item>
<title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.02821</link>
<guid>https://arxiv.org/abs/2504.02821</guid>
<content:encoded><![CDATA[
Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:58:35 GMT</pubDate>
</item>
<item>
<title>Interpreting Emergent Planning in Model-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.01871</link>
<guid>https://arxiv.org/abs/2504.01871</guid>
<content:encoded><![CDATA[
We present the first mechanistic evidence that model-free reinforcement learning agents can learn to plan. This is achieved by applying a methodology based on concept-based interpretability to a model-free agent in Sokoban -- a commonly used benchmark for studying planning. Specifically, we demonstrate that DRC, a generic model-free agent introduced by Guez et al. (2019), uses learned concept representations to internally formulate plans that both predict the long-term effects of actions on the environment and influence action selection. Our methodology involves: (1) probing for planning-relevant concepts, (2) investigating plan formation within the agent's representations, and (3) verifying that discovered plans (in the agent's representations) have a causal effect on the agent's behavior through interventions. We also show that the emergence of these plans coincides with the emergence of a planning-like property: the ability to benefit from additional test-time compute. Finally, we perform a qualitative analysis of the planning algorithm learned by the agent and discover a strong resemblance to parallelized bidirectional search. Our findings advance understanding of the internal mechanisms underlying planning behavior in agents, which is important given the recent trend of emergent planning and reasoning capabilities in LLMs through RL
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 12:24:23 GMT</pubDate>
</item>
<item>
<title>Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems</title>
<link>https://arxiv.org/abs/2504.01990</link>
<guid>https://arxiv.org/abs/2504.01990</guid>
<content:encoded><![CDATA[
The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 14:00:29 GMT</pubDate>
</item>
<item>
<title>Scaling Laws in Scientific Discovery with AI and Robot Scientists</title>
<link>https://arxiv.org/abs/2503.22444</link>
<guid>https://arxiv.org/abs/2503.22444</guid>
<content:encoded><![CDATA[
Scientific discovery is poised for rapid advancement through advanced robotics and artificial intelligence. Current scientific practices face substantial limitations as manual experimentation remains time-consuming and resource-intensive, while multidisciplinary research demands knowledge integration beyond individual researchers' expertise boundaries. Here, we envision an autonomous generalist scientist (AGS) concept combines agentic AI and embodied robotics to automate the entire research lifecycle. This system could dynamically interact with both physical and virtual environments while facilitating the integration of knowledge across diverse scientific disciplines. By deploying these technologies throughout every research stage -- spanning literature review, hypothesis generation, experimentation, and manuscript writing -- and incorporating internal reflection alongside external feedback, this system aims to significantly reduce the time and resources needed for scientific discovery. Building on the evolution from virtual AI scientists to versatile generalist AI-based robot scientists, AGS promises groundbreaking potential. As these autonomous systems become increasingly integrated into the research process, we hypothesize that scientific discovery might adhere to new scaling laws, potentially shaped by the number and capabilities of these autonomous systems, offering novel perspectives on how knowledge is generated and evolves. The adaptability of embodied robots to extreme environments, paired with the flywheel effect of accumulating scientific knowledge, holds the promise of continually pushing beyond both physical and intellectual frontiers.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 10:00:27 GMT</pubDate>
</item>
<item>
<title>ZClip: Adaptive Spike Mitigation for LLM Pre-Training</title>
<link>https://arxiv.org/abs/2504.02507</link>
<guid>https://arxiv.org/abs/2504.02507</guid>
<content:encoded><![CDATA[
Training large language models (LLMs) presents numerous challenges, including gradient instability and loss spikes. These phenomena can lead to catastrophic divergence, requiring costly checkpoint restoration and data batch skipping. Traditional gradient clipping techniques, such as constant or norm-based methods, fail to address these issues effectively due to their reliance on fixed thresholds or heuristics, leading to inefficient learning and requiring frequent manual intervention. In this work, we propose ZClip, an adaptive gradient clipping algorithm that dynamically adjusts the clipping threshold based on statistical properties of gradient norms over time. Unlike prior reactive strategies, ZClip proactively adapts to training dynamics without making any prior assumptions on the scale and the temporal evolution of gradient norms. At its core, it leverages z-score-based anomaly detection to identify and mitigate large gradient spikes, preventing malignant loss spikes while not interfering with convergence otherwise. Our code is available at: https://github.com/bluorion-com/ZClip.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 07:41:55 GMT</pubDate>
</item>
<item>
<title>Scaling Analysis of Interleaved Speech-Text Language Models</title>
<link>https://arxiv.org/abs/2504.02398</link>
<guid>https://arxiv.org/abs/2504.02398</guid>
<content:encoded><![CDATA[
Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. They predict that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - Do interleaved SLMs scale more efficiently than textless-SLMs? In this paper we answer a resounding, yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest, that our scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 04:46:56 GMT</pubDate>
</item>
<item>
<title>Instruction-Guided Autoregressive Neural Network Parameter Generation</title>
<link>https://arxiv.org/abs/2504.02012</link>
<guid>https://arxiv.org/abs/2504.02012</guid>
<content:encoded><![CDATA[
Learning to generate neural network parameters conditioned on task descriptions and architecture specifications is pivotal for advancing model adaptability and transfer learning. Existing methods especially those based on diffusion models suffer from limited scalability to large architectures, rigidity in handling varying network depths, and disjointed parameter generation that undermines inter-layer coherence. In this work, we propose IGPG (Instruction Guided Parameter Generation), an autoregressive framework that unifies parameter synthesis across diverse tasks and architectures. IGPG leverages a VQ-VAE and an autoregressive model to generate neural network parameters, conditioned on task instructions, dataset, and architecture details. By autoregressively generating neural network weights' tokens, IGPG ensures inter-layer coherence and enables efficient adaptation across models and datasets. Operating at the token level, IGPG effectively captures complex parameter distributions aggregated from a broad spectrum of pretrained models. Extensive experiments on multiple vision datasets demonstrate that IGPG consolidates diverse pretrained models into a single, flexible generative framework. The synthesized parameters achieve competitive or superior performance relative to state-of-the-art methods, especially in terms of scalability and efficiency when applied to large architectures. These results underscore ICPG potential as a powerful tool for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:50:19 GMT</pubDate>
</item>
<item>
<title>GenPRM: Scaling Test-Time Compute of Process Reward Models via Generative Reasoning</title>
<link>https://arxiv.org/abs/2504.00891</link>
<guid>https://arxiv.org/abs/2504.00891</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have shown that it is promising to utilize Process Reward Models (PRMs) as verifiers to enhance the performance of LLMs. However, current PRMs face three key challenges: (1) limited process supervision and generalization capabilities, (2) dependence on scalar value prediction without leveraging the generative abilities of LLMs, and (3) inability to scale the test-time compute of PRMs. In this work, we introduce GenPRM, a generative process reward model that performs explicit Chain-of-Thought (CoT) reasoning with code verification before providing judgment for each reasoning step. To obtain high-quality process supervision labels and rationale data, we propose Relative Progress Estimation (RPE) and a rationale synthesis framework that incorporates code verification. Experimental results on ProcessBench and several mathematical reasoning tasks show that GenPRM significantly outperforms prior PRMs with only 23K training data from MATH dataset. Through test-time scaling, a 1.5B GenPRM outperforms GPT-4o, and a 7B GenPRM surpasses Qwen2.5-Math-PRM-72B on ProcessBench. Additionally, GenPRM demonstrates strong abilities to serve as a critic model for policy model refinement. This work establishes a new paradigm for process supervision that bridges the gap between PRMs and critic models in LLMs. Our code, model, and data will be available in https://ryanliu112.github.io/GenPRM.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:21:05 GMT</pubDate>
</item>
<item>
<title>ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers</title>
<link>https://arxiv.org/abs/2504.00502</link>
<guid>https://arxiv.org/abs/2504.00502</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) suffer from high computational costs due to their massive size and the large number of visual tokens. In this paper, we investigate layer-wise redundancy in MLLMs by introducing a novel metric, Layer Contribution (LC), which quantifies the impact of a layer's transformations on visual and text tokens, respectively. The calculation of LC involves measuring the divergence in model output that results from removing the layer's transformations on the specified tokens. Our pilot experiment reveals that many layers of MLLMs exhibit minimal contribution during the processing of visual tokens. Motivated by this observation, we propose ShortV, a training-free method that leverages LC to identify ineffective layers, and freezes visual token updates in these layers. Experiments show that ShortV can freeze visual token in approximately 60\% of the MLLM layers, thereby dramatically reducing computational costs related to updating visual tokens. For example, it achieves a 50\% reduction in FLOPs on LLaVA-NeXT-13B while maintaining superior performance. The code will be publicly available at https://github.com/icip-cas/ShortV
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:47:55 GMT</pubDate>
</item>
<item>
<title>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing</title>
<link>https://arxiv.org/abs/2504.02826</link>
<guid>https://arxiv.org/abs/2504.02826</guid>
<content:encoded><![CDATA[
Large Multi-modality Models (LMMs) have made significant progress in visual understanding and generation, but they still face challenges in General Visual Editing, particularly in following complex instructions, preserving appearance consistency, and supporting flexible input formats. To address this gap, we introduce RISEBench, the first benchmark for evaluating Reasoning-Informed viSual Editing (RISE). RISEBench focuses on four key reasoning types: Temporal, Causal, Spatial, and Logical Reasoning. We curate high-quality test cases for each category and propose an evaluation framework that assesses Instruction Reasoning, Appearance Consistency, and Visual Plausibility with both human judges and an LMM-as-a-judge approach. Our experiments reveal that while GPT-4o-Native significantly outperforms other open-source and proprietary models, even this state-of-the-art system struggles with logical reasoning tasks, highlighting an area that remains underexplored. As an initial effort, RISEBench aims to provide foundational insights into reasoning-aware visual editing and to catalyze future research. Though still in its early stages, we are committed to continuously expanding and refining the benchmark to support more comprehensive, reliable, and scalable evaluations of next-generation multimodal systems. Our code and data will be released at https://github.com/PhoenixZ810/RISEBench.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation</title>
<link>https://arxiv.org/abs/2504.02782</link>
<guid>https://arxiv.org/abs/2504.02782</guid>
<content:encoded><![CDATA[
The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:23:16 GMT</pubDate>
</item>
<item>
<title>Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme</title>
<link>https://arxiv.org/abs/2504.02587</link>
<guid>https://arxiv.org/abs/2504.02587</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) has recently shown strong potential in improving the reasoning capabilities of large language models and is now being actively extended to vision-language models (VLMs). However, existing RL applications in VLMs often rely on heavily engineered frameworks that hinder reproducibility and accessibility, while lacking standardized evaluation protocols, making it difficult to compare results or interpret training dynamics. This work introduces a transparent, from-scratch framework for RL in VLMs, offering a minimal yet functional four-step pipeline validated across multiple models and datasets. In addition, a standardized evaluation scheme is proposed to assess training dynamics and reflective behaviors. Extensive experiments on visual reasoning tasks uncover key empirical findings: response length is sensitive to random seeds, reflection correlates with output length, and RL consistently outperforms supervised fine-tuning (SFT) in generalization, even with high-quality data. These findings, together with the proposed framework, aim to establish a reproducible baseline and support broader engagement in RL-based VLM research.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 09:53:28 GMT</pubDate>
</item>
<item>
<title>Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation</title>
<link>https://arxiv.org/abs/2504.02542</link>
<guid>https://arxiv.org/abs/2504.02542</guid>
<content:encoded><![CDATA[
Talking head synthesis is vital for virtual avatars and human-computer interaction. However, most existing methods are typically limited to accepting control from a single primary modality, restricting their practical utility. To this end, we introduce ACTalker, an end-to-end video diffusion framework that supports both multi-signals control and single-signal control for talking head video generation. For multiple control, we design a parallel mamba structure with multiple branches, each utilizing a separate driving signal to control specific facial regions. A gate mechanism is applied across all branches, providing flexible control over video generation. To ensure natural coordination of the controlled video both temporally and spatially, we employ the mamba structure, which enables driving signals to manipulate feature tokens across both dimensions in each branch. Additionally, we introduce a mask-drop strategy that allows each driving signal to independently control its corresponding facial region within the mamba structure, preventing control conflicts. Experimental results demonstrate that our method produces natural-looking facial videos driven by diverse signals and that the mamba layer seamlessly integrates multiple driving modalities without conflict.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 08:44:41 GMT</pubDate>
</item>
<item>
<title>SkyReels-A2: Compose Anything in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.02436</link>
<guid>https://arxiv.org/abs/2504.02436</guid>
<content:encoded><![CDATA[
This paper presents SkyReels-A2, a controllable video generation framework capable of assembling arbitrary visual elements (e.g., characters, objects, backgrounds) into synthesized videos based on textual prompts while maintaining strict consistency with reference images for each element. We term this task elements-to-video (E2V), whose primary challenges lie in preserving the fidelity of each reference element, ensuring coherent composition of the scene, and achieving natural outputs. To address these, we first design a comprehensive data pipeline to construct prompt-reference-video triplets for model training. Next, we propose a novel image-text joint embedding model to inject multi-element representations into the generative process, balancing element-specific consistency with global coherence and text alignment. We also optimize the inference pipeline for both speed and output stability. Moreover, we introduce a carefully curated benchmark for systematic evaluation, i.e, A2 Bench. Experiments demonstrate that our framework can generate diverse, high-quality videos with precise element control. SkyReels-A2 is the first open-source commercial grade model for the generation of E2V, performing favorably against advanced closed-source commercial models. We anticipate SkyReels-A2 will advance creative applications such as drama and virtual e-commerce, pushing the boundaries of controllable video generation.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 05:50:50 GMT</pubDate>
</item>
<item>
<title>Efficient Model Selection for Time Series Forecasting via LLMs</title>
<link>https://arxiv.org/abs/2504.02119</link>
<guid>https://arxiv.org/abs/2504.02119</guid>
<content:encoded><![CDATA[
Model selection is a critical step in time series forecasting, traditionally requiring extensive performance evaluations across various datasets. Meta-learning approaches aim to automate this process, but they typically depend on pre-constructed performance matrices, which are costly to build. In this work, we propose to leverage Large Language Models (LLMs) as a lightweight alternative for model selection. Our method eliminates the need for explicit performance matrices by utilizing the inherent knowledge and reasoning capabilities of LLMs. Through extensive experiments with LLaMA, GPT and Gemini, we demonstrate that our approach outperforms traditional meta-learning techniques and heuristic baselines, while significantly reducing computational overhead. These findings underscore the potential of LLMs in efficient model selection for time series forecasting.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 16:33:27 GMT</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 03:20:58 GMT</pubDate>
</item>
<item>
<title>Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models</title>
<link>https://arxiv.org/abs/2503.22879</link>
<guid>https://arxiv.org/abs/2503.22879</guid>
<content:encoded><![CDATA[
State Space Models (SSMs) are emerging as a compelling alternative to Transformers because of their consistent memory usage and high performance. Despite this, scaling up SSMs on cloud services or limited-resource devices is challenging due to their storage requirements and computational power. To overcome this, quantizing SSMs with low bit-width data formats can reduce model size and benefit from hardware acceleration. As SSMs are prone to quantization-induced errors, recent efforts have focused on optimizing a particular model or bit-width for efficiency without sacrificing performance. However, distinct bit-width configurations are essential for different scenarios, like W4A8 for boosting large-batch decoding speed, and W4A16 for enhancing generation speed in short prompt applications for a single user. To this end, we present Quamba2, compatible with W8A8, W4A8, and W4A16 for both Mamba1 and Mamba2 backbones, addressing the growing demand for SSM deployment on various platforms. Based on the channel order preserving and activation persistence of SSMs, we propose an offline approach to quantize inputs of a linear recurrence in 8-bit by sorting and clustering for input x, combined with a per-state-group quantization for input-dependent parameters B and C. To ensure compute-invariance in the SSM output, we rearrange weights offline according to the clustering sequence. The experiments show that Quamba2-8B outperforms several state-of-the-art SSM quantization methods and delivers 1.3times and 3times speed-ups in the pre-filling and generation stages, respectively, while offering 4times memory reduction with only a 1.6% average accuracy drop. The evaluation on MMLU shows the generalizability and robustness of our framework. The code and quantized models will be released at: https://github.com/enyac-group/Quamba.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 17:10:39 GMT</pubDate>
</item>
<item>
<title>Target-Aware Video Diffusion Models</title>
<link>https://arxiv.org/abs/2503.18950</link>
<guid>https://arxiv.org/abs/2503.18950</guid>
<content:encoded><![CDATA[
We present a target-aware video diffusion model that generates videos from an input image in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask and the desired action is described via a text prompt. Unlike existing controllable image-to-video diffusion models that often rely on dense structural or motion cues to guide the actor's movements toward the target, our target-aware model requires only a simple mask to indicate the target, leveraging the generalization capabilities of pretrained models to produce plausible actions. This makes our method particularly effective for human-object interaction (HOI) scenarios, where providing precise action guidance is challenging, and further enables the use of video diffusion models for high-level action planning in applications such as robotics. We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using a novel cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant transformer blocks and attention regions. Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: video content creation and zero-shot 3D HOI motion synthesis.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>Medical large language models are easily distracted</title>
<link>https://arxiv.org/abs/2504.01201</link>
<guid>https://arxiv.org/abs/2504.01201</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have the potential to transform medicine, but real-world clinical scenarios contain extraneous information that can hinder performance. The rise of assistive technologies like ambient dictation, which automatically generates draft notes from live patient encounters, has the potential to introduce additional noise making it crucial to assess the ability of LLM's to filter relevant data. To investigate this, we developed MedDistractQA, a benchmark using USMLE-style questions embedded with simulated real-world distractions. Our findings show that distracting statements (polysemous words with clinical meanings used in a non-clinical context or references to unrelated health conditions) can reduce LLM accuracy by up to 17.9%. Commonly proposed solutions to improve model performance such as retrieval-augmented generation (RAG) and medical fine-tuning did not change this effect and in some cases introduced their own confounders and further degraded performance. Our findings suggest that LLMs natively lack the logical mechanisms necessary to distinguish relevant from irrelevant clinical information, posing challenges for real-world applications. MedDistractQA and our results highlights the need for robust mitigation strategies to enhance LLM resilience to extraneous information.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:34:01 GMT</pubDate>
</item>
<item>
<title>MegaTTS 3: Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis</title>
<link>https://arxiv.org/abs/2502.18924</link>
<guid>https://arxiv.org/abs/2502.18924</guid>
<content:encoded><![CDATA[
While recent zero-shot text-to-speech (TTS) models have significantly improved speech quality and expressiveness, mainstream systems still suffer from issues related to speech-text alignment modeling: 1) models without explicit speech-text alignment modeling exhibit less robustness, especially for hard sentences in practical applications; 2) predefined alignment-based models suffer from naturalness constraints of forced alignments. This paper introduces MegaTTS 3, a TTS system featuring an innovative sparse alignment algorithm that guides the latent diffusion transformer (DiT). Specifically, we provide sparse alignment boundaries to MegaTTS 3 to reduce the difficulty of alignment without limiting the search space, thereby achieving high naturalness. Moreover, we employ a multi-condition classifier-free guidance strategy for accent intensity adjustment and adopt the piecewise rectified flow technique to accelerate the generation process. Experiments demonstrate that MegaTTS 3 achieves state-of-the-art zero-shot TTS speech quality and supports highly flexible control over accent intensity. Notably, our system can generate high-quality one-minute speech with only 8 sampling steps. Audio samples are available at https://sditdemo.github.io/sditdemo/.
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 03:22:00 GMT</pubDate>
</item>
<item>
<title>DASH: Detection and Assessment of Systematic Hallucinations of VLMs</title>
<link>https://arxiv.org/abs/2503.23573</link>
<guid>https://arxiv.org/abs/2503.23573</guid>
<content:encoded><![CDATA[
Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 15:45:09 GMT</pubDate>
</item>
<item>
<title>Towards Physically Plausible Video Generation via VLM Planning</title>
<link>https://arxiv.org/abs/2503.23368</link>
<guid>https://arxiv.org/abs/2503.23368</guid>
<content:encoded><![CDATA[
Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 05:03:09 GMT</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 00:05:03 GMT</pubDate>
</item>
<item>
<title>Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations</title>
<link>https://arxiv.org/abs/2503.18817</link>
<guid>https://arxiv.org/abs/2503.18817</guid>
<content:encoded><![CDATA[
Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. We investigate the limitation of na\"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings. To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data. This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:00:21 GMT</pubDate>
</item>
<item>
<title>Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback</title>
<link>https://arxiv.org/abs/2405.20216</link>
<guid>https://arxiv.org/abs/2405.20216</guid>
<content:encoded><![CDATA[
The generation of high-quality human images through text-to-image (T2I) methods is a significant yet challenging task. Distinct from general image generation, human image synthesis must satisfy stringent criteria related to human pose, anatomy, and alignment with textual prompts, making it particularly difficult to achieve realistic results. Recent advancements in T2I generation based on diffusion models have shown promise, yet challenges remain in meeting human-specific preferences. In this paper, we introduce a novel approach tailored specifically for human image generation utilizing Direct Preference Optimization (DPO). Specifically, we introduce an efficient method for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO training process by minimizing artifacts and improving image fidelity. Our method demonstrates its versatility and effectiveness in generating human images, including personalized text-to-image generation. Through comprehensive evaluations, we show that our approach significantly advances the state of human image generation, achieving superior results in terms of natural anatomies, poses, and text-image alignment.
]]></content:encoded>
<pubDate>Thu, 30 May 2024 12:18:05 GMT</pubDate>
</item>
<item>
<title>VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step</title>
<link>https://arxiv.org/abs/2504.01956</link>
<guid>https://arxiv.org/abs/2504.01956</guid>
<content:encoded><![CDATA[
Recovering 3D scenes from sparse views is a challenging task due to its inherent ill-posed problem. Conventional methods have developed specialized solutions (e.g., geometry regularization or feed-forward deterministic model) to mitigate the issue. However, they still suffer from performance degradation by minimal overlap across input views with insufficient visual information. Fortunately, recent video generative models show promise in addressing this challenge as they are capable of generating video clips with plausible 3D structures. Powered by large pretrained video diffusion models, some pioneering research start to explore the potential of video generative prior and create 3D scenes from sparse views. Despite impressive improvements, they are limited by slow inference time and the lack of 3D constraint, leading to inefficiencies and reconstruction artifacts that do not align with real-world geometry structure. In this paper, we propose VideoScene to distill the video diffusion model to generate 3D scenes in one step, aiming to build an efficient and effective tool to bridge the gap from video to 3D. Specifically, we design a 3D-aware leap flow distillation strategy to leap over time-consuming redundant information and train a dynamic denoising policy network to adaptively determine the optimal leap timestep during inference. Extensive experiments demonstrate that our VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, highlighting its potential as an efficient tool for future video to 3D applications. Project Page: https://hanyang-21.github.io/VideoScene
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement</title>
<link>https://arxiv.org/abs/2504.01934</link>
<guid>https://arxiv.org/abs/2504.01934</guid>
<content:encoded><![CDATA[
We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 13:45:00 GMT</pubDate>
</item>
<item>
<title>Understanding R1-Zero-Like Training: A Critical Perspective</title>
<link>https://arxiv.org/abs/2503.20783</link>
<guid>https://arxiv.org/abs/2503.20783</guid>
<content:encoded><![CDATA[
DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>PaperBench: Evaluating AI's Ability to Replicate AI Research</title>
<link>https://arxiv.org/abs/2504.01848</link>
<guid>https://arxiv.org/abs/2504.01848</guid>
<content:encoded><![CDATA[
We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We https://github.com/openai/preparedness{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 11:55:24 GMT</pubDate>
</item>
<item>
<title>DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance</title>
<link>https://arxiv.org/abs/2504.01724</link>
<guid>https://arxiv.org/abs/2504.01724</guid>
<content:encoded><![CDATA[
While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 09:30:32 GMT</pubDate>
</item>
<item>
<title>Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks</title>
<link>https://arxiv.org/abs/2504.01308</link>
<guid>https://arxiv.org/abs/2504.01308</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 22:35:19 GMT</pubDate>
</item>
<item>
<title>Articulated Kinematics Distillation from Video Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01204</link>
<guid>https://arxiv.org/abs/2504.01204</guid>
<content:encoded><![CDATA[
We present Articulated Kinematics Distillation (AKD), a framework for generating high-fidelity character animations by merging the strengths of skeleton-based animation and modern generative models. AKD uses a skeleton-based representation for rigged 3D assets, drastically reducing the Degrees of Freedom (DoFs) by focusing on joint-level control, which allows for efficient, consistent motion synthesis. Through Score Distillation Sampling (SDS) with pre-trained video diffusion models, AKD distills complex, articulated motions while maintaining structural integrity, overcoming challenges faced by 4D neural deformation fields in preserving shape consistency. This approach is naturally compatible with physics-based simulation, ensuring physically plausible interactions. Experiments show that AKD achieves superior 3D consistency and motion quality compared with existing works on text-to-4D generation. Project page: https://research.nvidia.com/labs/dir/akd/
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 17:37:57 GMT</pubDate>
</item>
<item>
<title>AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction</title>
<link>https://arxiv.org/abs/2504.01014</link>
<guid>https://arxiv.org/abs/2504.01014</guid>
<content:encoded><![CDATA[
Recent advancements in image and video synthesis have opened up new promise in generative games. One particularly intriguing application is transforming characters from anime films into interactive, playable entities. This allows players to immerse themselves in the dynamic anime world as their favorite characters for life simulation through language instructions. Such games are defined as infinite game since they eliminate predetermined boundaries and fixed gameplay rules, where players can interact with the game world through open-ended language and experience ever-evolving storylines and environments. Recently, a pioneering approach for infinite anime life simulation employs large language models (LLMs) to translate multi-turn text dialogues into language instructions for image generation. However, it neglects historical visual context, leading to inconsistent gameplay. Furthermore, it only generates static images, failing to incorporate the dynamics necessary for an engaging gaming experience. In this work, we propose AnimeGamer, which is built upon Multimodal Large Language Models (MLLMs) to generate each game state, including dynamic animation shots that depict character movements and updates to character states, as illustrated in Figure 1. We introduce novel action-aware multimodal representations to represent animation shots, which can be decoded into high-quality video clips using a video diffusion model. By taking historical animation shot representations as context and predicting subsequent representations, AnimeGamer can generate games with contextual consistency and satisfactory dynamics. Extensive evaluations using both automated metrics and human evaluations demonstrate that AnimeGamer outperforms existing methods in various aspects of the gaming experience. Codes and checkpoints are available at https://github.com/TencentARC/AnimeGamer.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:57:18 GMT</pubDate>
</item>
<item>
<title>MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization</title>
<link>https://arxiv.org/abs/2504.00999</link>
<guid>https://arxiv.org/abs/2504.00999</guid>
<content:encoded><![CDATA[
Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:39:19 GMT</pubDate>
</item>
<item>
<title>Improved Visual-Spatial Reasoning via R1-Zero-Like Training</title>
<link>https://arxiv.org/abs/2504.00883</link>
<guid>https://arxiv.org/abs/2504.00883</guid>
<content:encoded><![CDATA[
Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:11:11 GMT</pubDate>
</item>
<item>
<title>ScholarCopilot: Training Large Language Models for Academic Writing with Accurate Citations</title>
<link>https://arxiv.org/abs/2504.00824</link>
<guid>https://arxiv.org/abs/2504.00824</guid>
<content:encoded><![CDATA[
Academic writing requires both coherent text generation and precise citation of relevant literature. Although recent Retrieval-Augmented Generation (RAG) systems have significantly improved factual accuracy in general-purpose text generation, their capacity to adequately support professional academic writing remains limited. In this work, we introduce ScholarCopilot, a unified framework designed to enhance existing large language models for generating professional academic articles with accurate and contextually relevant citations. ScholarCopilot dynamically determines when to retrieve scholarly references by generating a retrieval token [RET], and then utilizes its representation to look up relevant citations from a database. The retrieved references are fed into the model to augment the generation process. We jointly optimize both the generation and citation tasks within a single framework to increase efficiency. Trained on 500K papers from arXiv, our model achieves a top-1 retrieval accuracy of 40.1% on our evaluation dataset, outperforming baselines such as E5-Mistral-7B-Instruct (15.0%) and BM25 (9.8%). On a dataset of 1,000 academic writing samples, ScholarCopilot scores 16.2/25 in generation quality (measured across relevance, coherence, academic rigor, completeness, and innovation), surpassing models with 10x more parameters such as Qwen-2.5-72B-Instruct (15.8/25). Human studies also confirm ScholarCopilot's superior performance in citation recall, writing efficiency, and overall user experience, confirming the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:12:14 GMT</pubDate>
</item>
<item>
<title>LSNet: See Large, Focus Small</title>
<link>https://arxiv.org/abs/2503.23135</link>
<guid>https://arxiv.org/abs/2503.23135</guid>
<content:encoded><![CDATA[
Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:00:54 GMT</pubDate>
</item>
<item>
<title>Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models</title>
<link>https://arxiv.org/abs/2503.22165</link>
<guid>https://arxiv.org/abs/2503.22165</guid>
<content:encoded><![CDATA[
Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts-the first visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset. Specifically, we represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt our tool to a model that predicts the property they observe. We showcase this advantage by adapting our tool to a lightweight verifier that evaluates the correctness of reasoning paths. The code is publicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 02:09:51 GMT</pubDate>
</item>
<item>
<title>Reasoning-SQL: Reinforcement Learning with SQL Tailored Partial Rewards for Reasoning-Enhanced Text-to-SQL</title>
<link>https://arxiv.org/abs/2503.23157</link>
<guid>https://arxiv.org/abs/2503.23157</guid>
<content:encoded><![CDATA[
Text-to-SQL is a challenging task involving multiple reasoning-intensive subtasks, including natural language understanding, database schema comprehension, and precise SQL query formulation. Existing approaches often rely on handcrafted reasoning paths with inductive biases that can limit their overall effectiveness. Motivated by the recent success of reasoning-enhanced models such as DeepSeek R1 and OpenAI o1, which effectively leverage reward-driven self-exploration to enhance reasoning capabilities and generalization, we propose a novel set of partial rewards tailored specifically for the Text-to-SQL task. Our reward set includes schema-linking, AI feedback, n-gram similarity, and syntax check, explicitly designed to address the reward sparsity issue prevalent in reinforcement learning (RL). Leveraging group relative policy optimization (GRPO), our approach explicitly encourages large language models (LLMs) to develop intrinsic reasoning skills necessary for accurate SQL query generation. With models of different sizes, we demonstrate that RL-only training with our proposed rewards consistently achieves higher accuracy and superior generalization compared to supervised fine-tuning (SFT). Remarkably, our RL-trained 14B-parameter model significantly outperforms larger proprietary models, e.g. o3-mini by 4% and Gemini-1.5-Pro-002 by 3% on the BIRD benchmark. These highlight the efficacy of our proposed RL-training framework with partial rewards for enhancing both accuracy and reasoning capabilities in Text-to-SQL tasks.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 13:29:30 GMT</pubDate>
</item>
<item>
<title>MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing</title>
<link>https://arxiv.org/abs/2503.24219</link>
<guid>https://arxiv.org/abs/2503.24219</guid>
<content:encoded><![CDATA[
We propose a unified framework that integrates object detection (OD) and visual grounding (VG) for remote sensing (RS) imagery. To support conventional OD and establish an intuitive prior for VG task, we fine-tune an open-set object detector using referring expression data, framing it as a partially supervised OD task. In the first stage, we construct a graph representation of each image, comprising object queries, class embeddings, and proposal locations. Then, our task-aware architecture processes this graph to perform the VG task. The model consists of: (i) a multi-branch network that integrates spatial, visual, and categorical features to generate task-aware proposals, and (ii) an object reasoning network that assigns probabilities across proposals, followed by a soft selection mechanism for final referring object localization. Our model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG datasets, achieving significant improvements over state-of-the-art methods while retaining classical OD capabilities. The code will be available in our repository: https://github.com/rd20karim/MB-ORES.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:36:41 GMT</pubDate>
</item>
<item>
<title>MixerMDM: Learnable Composition of Human Motion Diffusion Models</title>
<link>https://arxiv.org/abs/2504.01019</link>
<guid>https://arxiv.org/abs/2504.01019</guid>
<content:encoded><![CDATA[
Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Scaling Language-Free Visual Representation Learning</title>
<link>https://arxiv.org/abs/2504.01017</link>
<guid>https://arxiv.org/abs/2504.01017</guid>
<content:encoded><![CDATA[
Visual Self-Supervised Learning (SSL) currently underperforms Contrastive Language-Image Pretraining (CLIP) in multimodal settings such as Visual Question Answering (VQA). This multimodal gap is often attributed to the semantics introduced by language supervision, even though visual SSL and CLIP models are often trained on different data. In this work, we ask the question: "Do visual self-supervised approaches lag behind CLIP due to the lack of language supervision, or differences in the training data?" We study this question by training both visual SSL and CLIP models on the same MetaCLIP data, and leveraging VQA as a diverse testbed for vision encoders. In this controlled setup, visual SSL models scale better than CLIP models in terms of data and model capacity, and visual SSL performance does not saturate even after scaling up to 7B parameters. Consequently, we observe visual SSL methods achieve CLIP-level performance on a wide range of VQA and classic vision benchmarks. These findings demonstrate that pure visual SSL can match language-supervised visual pretraining at scale, opening new opportunities for vision-centric representation learning.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:59:15 GMT</pubDate>
</item>
<item>
<title>DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.24210</link>
<guid>https://arxiv.org/abs/2503.24210</guid>
<content:encoded><![CDATA[
Reconstructing sharp 3D representations from blurry multi-view images are long-standing problem in computer vision. Recent works attempt to enhance high-quality novel view synthesis from the motion blur by leveraging event-based cameras, benefiting from high dynamic range and microsecond temporal resolution. However, they often reach sub-optimal visual quality in either restoring inaccurate color or losing fine-grained details. In this paper, we present DiET-GS, a diffusion prior and event stream-assisted motion deblurring 3DGS. Our framework effectively leverages both blur-free event streams and diffusion prior in a two-stage training strategy. Specifically, we introduce the novel framework to constraint 3DGS with event double integral, achieving both accurate color and well-defined details. Additionally, we propose a simple technique to leverage diffusion prior to further enhance the edge details. Qualitative and quantitative results on both synthetic and real-world data demonstrate that our DiET-GS is capable of producing significantly better quality of novel views compared to the existing baselines. Our project page is https://diet-gs.github.io
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:27:07 GMT</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 22:18:51 GMT</pubDate>
</item>
<item>
<title>Towards Trustworthy GUI Agents: A Survey</title>
<link>https://arxiv.org/abs/2503.23434</link>
<guid>https://arxiv.org/abs/2503.23434</guid>
<content:encoded><![CDATA[
GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 09:26:00 GMT</pubDate>
</item>
<item>
<title>OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts</title>
<link>https://arxiv.org/abs/2503.22952</link>
<guid>https://arxiv.org/abs/2503.22952</guid>
<content:encoded><![CDATA[
The rapid advancement of multi-modal language models (MLLMs) like GPT-4o has propelled the development of Omni language models, designed to process and proactively respond to continuous streams of multi-modal data. Despite their potential, evaluating their real-world interactive capabilities in streaming video contexts remains a formidable challenge. In this work, we introduce OmniMMI, a comprehensive multi-modal interaction benchmark tailored for OmniLLMs in streaming video contexts. OmniMMI encompasses over 1,121 videos and 2,290 questions, addressing two critical yet underexplored challenges in existing video benchmarks: streaming video understanding and proactive reasoning, across six distinct subtasks. Moreover, we propose a novel framework, Multi-modal Multiplexing Modeling (M4), designed to enable an inference-efficient streaming model that can see, listen while generating.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 22:46:58 GMT</pubDate>
</item>
<item>
<title>Multi-Token Attention</title>
<link>https://arxiv.org/abs/2504.00927</link>
<guid>https://arxiv.org/abs/2504.00927</guid>
<content:encoded><![CDATA[
Soft attention is a critical mechanism powering LLMs to locate relevant parts within a given context. However, individual attention weights are determined by the similarity of only a single query and key token vector. This "single token attention" bottlenecks the amount of information used in distinguishing a relevant part from the rest of the context. To address this issue, we propose a new attention method, Multi-Token Attention (MTA), which allows LLMs to condition their attention weights on multiple query and key vectors simultaneously. This is achieved by applying convolution operations over queries, keys and heads, allowing nearby queries and keys to affect each other's attention weights for more precise attention. As a result, our method can locate relevant context using richer, more nuanced information that can exceed a single vector's capacity. Through extensive evaluations, we demonstrate that MTA achieves enhanced performance on a range of popular benchmarks. Notably, it outperforms Transformer baseline models on standard language modeling tasks, and on tasks that require searching for information within long contexts, where our method's ability to leverage richer information proves particularly beneficial.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:59:32 GMT</pubDate>
</item>
<item>
<title>m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.00869</link>
<guid>https://arxiv.org/abs/2504.00869</guid>
<content:encoded><![CDATA[
Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:57:43 GMT</pubDate>
</item>
<item>
<title>Z1: Efficient Test-time Scaling with Code</title>
<link>https://arxiv.org/abs/2504.00810</link>
<guid>https://arxiv.org/abs/2504.00810</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) can achieve enhanced complex problem-solving through test-time computing scaling, yet this often entails longer contexts and numerous reasoning token costs. In this paper, we propose an efficient test-time scaling method that trains LLMs on code-related reasoning trajectories, facilitating their reduction of excess thinking tokens while maintaining performance. First, we create Z1-Code-Reasoning-107K, a curated dataset of simple and complex coding problems paired with their short and long solution trajectories. Second, we present a novel Shifted Thinking Window to mitigate overthinking overhead by removing context-delimiting tags (e.g., . . . ) and capping reasoning tokens. Trained with long and short trajectory data and equipped with Shifted Thinking Window, our model, Z1-7B, demonstrates the ability to adjust its reasoning level as the complexity of problems and exhibits efficient test-time scaling across different reasoning tasks that matches R1-Distill-Qwen-7B performance with about 30% of its average thinking tokens. Notably, fine-tuned with only code trajectories, Z1-7B demonstrates generalization to broader reasoning tasks (47.5% on GPQA Diamond). Our analysis of efficient reasoning elicitation also provides valuable insights for future research.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 10:01:50 GMT</pubDate>
</item>
<item>
<title>Command A: An Enterprise-Ready Large Language Model</title>
<link>https://arxiv.org/abs/2504.00698</link>
<guid>https://arxiv.org/abs/2504.00698</guid>
<content:encoded><![CDATA[
In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 08:08:07 GMT</pubDate>
</item>
<item>
<title>Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources</title>
<link>https://arxiv.org/abs/2504.00595</link>
<guid>https://arxiv.org/abs/2504.00595</guid>
<content:encoded><![CDATA[
The reproduction of state-of-the-art multimodal LLM pre-training faces barriers at every stage of the pipeline, including high-quality data filtering, multimodal data mixture strategies, sequence packing techniques, and training frameworks. We introduce Open-Qwen2VL, a fully open-source 2B-parameter Multimodal Large Language Model pre-trained efficiently on 29M image-text pairs using only 442 A100-40G GPU hours. Our approach employs low-to-high dynamic image resolution and multimodal sequence packing to significantly enhance pre-training efficiency. The training dataset was carefully curated using both MLLM-based filtering techniques (e.g., MLM-Filter) and conventional CLIP-based filtering methods, substantially improving data quality and training efficiency. The Open-Qwen2VL pre-training is conducted on academic level 8xA100-40G GPUs at UCSB on 5B packed multimodal tokens, which is 0.36\% of 1.4T multimodal pre-training tokens of Qwen2-VL. The final instruction-tuned Open-Qwen2VL outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on various multimodal benchmarks of MMBench, SEEDBench, MMstar, and MathVista, indicating the remarkable training efficiency of Open-Qwen2VL. We open-source all aspects of our work, including compute-efficient and data-efficient training details, data filtering methods, sequence packing scripts, pre-training data in WebDataset format, FSDP-based training codebase, and both base and instruction-tuned model checkpoints. We redefine "fully open" for multimodal LLMs as the complete release of: 1) the training codebase, 2) detailed data filtering techniques, and 3) all pre-training and supervised fine-tuning data used to develop the model.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:54:00 GMT</pubDate>
</item>
<item>
<title>Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features</title>
<link>https://arxiv.org/abs/2504.00557</link>
<guid>https://arxiv.org/abs/2504.00557</guid>
<content:encoded><![CDATA[
Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 05:10:32 GMT</pubDate>
</item>
<item>
<title>Recitation over Reasoning: How Cutting-Edge Language Models Can Fail on Elementary School-Level Reasoning Problems?</title>
<link>https://arxiv.org/abs/2504.00509</link>
<guid>https://arxiv.org/abs/2504.00509</guid>
<content:encoded><![CDATA[
The rapid escalation from elementary school-level to frontier problems of the difficulty for LLM benchmarks in recent years have weaved a miracle for researchers that we are only inches away from surpassing human intelligence. However, is the LLMs' remarkable reasoning ability indeed comes from true intelligence by human standards, or are they simply reciting solutions witnessed during training at an Internet level? To study this problem, we propose RoR-Bench, a novel, multi-modal benchmark for detecting LLM's recitation behavior when asked simple reasoning problems but with conditions subtly shifted, and conduct empirical analysis on our benchmark. Surprisingly, we found existing cutting-edge LLMs unanimously exhibits extremely severe recitation behavior; by changing one phrase in the condition, top models such as OpenAI-o1 and DeepSeek-R1 can suffer 60% performance loss on elementary school-level arithmetic and reasoning problems. Such findings are a wake-up call to the LLM community that compels us to re-evaluate the true intelligence level of cutting-edge LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 03:57:58 GMT</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead</title>
<link>https://arxiv.org/abs/2504.00294</link>
<guid>https://arxiv.org/abs/2504.00294</guid>
<content:encoded><![CDATA[
Inference-time scaling can enhance the reasoning capabilities of large language models (LLMs) on complex problems that benefit from step-by-step problem solving. Although lengthening generated scratchpads has proven effective for mathematical tasks, the broader impact of this approach on other tasks remains less clear. In this work, we investigate the benefits and limitations of scaling methods across nine state-of-the-art models and eight challenging tasks, including math and STEM reasoning, calendar planning, NP-hard problems, navigation, and spatial reasoning. We compare conventional models (e.g., GPT-4o) with models fine-tuned for inference-time scaling (e.g., o1) through evaluation protocols that involve repeated model calls, either independently or sequentially with feedback. These evaluations approximate lower and upper performance bounds and potential for future performance improvements for each model, whether through enhanced training or multi-model inference systems. Our extensive empirical analysis reveals that the advantages of inference-time scaling vary across tasks and diminish as problem complexity increases. In addition, simply using more tokens does not necessarily translate to higher accuracy in these challenging regimes. Results from multiple independent runs with conventional models using perfect verifiers show that, for some tasks, these models can achieve performance close to the average performance of today's most advanced reasoning models. However, for other tasks, a significant performance gap remains, even in very high scaling regimes. Encouragingly, all models demonstrate significant gains when inference is further scaled with perfect verifiers or strong feedback, suggesting ample potential for future improvements.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 19:40:28 GMT</pubDate>
</item>
<item>
<title>Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs</title>
<link>https://arxiv.org/abs/2504.00072</link>
<guid>https://arxiv.org/abs/2504.00072</guid>
<content:encoded><![CDATA[
We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:41:29 GMT</pubDate>
</item>
<item>
<title>CodeARC: Benchmarking Reasoning Capabilities of LLM Agents for Inductive Program Synthesis</title>
<link>https://arxiv.org/abs/2503.23145</link>
<guid>https://arxiv.org/abs/2503.23145</guid>
<content:encoded><![CDATA[
Inductive program synthesis, or programming by example, requires synthesizing functions from input-output examples that generalize to unseen inputs. While large language model agents have shown promise in programming tasks guided by natural language, their ability to perform inductive program synthesis is underexplored. Existing evaluation protocols rely on static sets of examples and held-out tests, offering no feedback when synthesized functions are incorrect and failing to reflect real-world scenarios such as reverse engineering. We propose CodeARC, the Code Abstraction and Reasoning Challenge, a new evaluation framework where agents interact with a hidden target function by querying it with new inputs, synthesizing candidate functions, and iteratively refining their solutions using a differential testing oracle. This interactive setting encourages agents to perform function calls and self-correction based on feedback. We construct the first large-scale benchmark for general-purpose inductive program synthesis, featuring 1114 functions. Among 18 models evaluated, o3-mini performs best with a success rate of 52.7%, highlighting the difficulty of this task. Fine-tuning LLaMA-3.1-8B-Instruct on curated synthesis traces yields up to a 31% relative performance gain. CodeARC provides a more realistic and challenging testbed for evaluating LLM-based program synthesis and inductive reasoning.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 12:50:39 GMT</pubDate>
</item>
<item>
<title>GeometryCrafter: Consistent Geometry Estimation for Open-world Videos with Diffusion Priors</title>
<link>https://arxiv.org/abs/2504.01016</link>
<guid>https://arxiv.org/abs/2504.01016</guid>
<content:encoded><![CDATA[
Despite remarkable advancements in video depth estimation, existing methods exhibit inherent limitations in achieving geometric fidelity through the affine-invariant predictions, limiting their applicability in reconstruction and other metrically grounded downstream tasks. We propose GeometryCrafter, a novel framework that recovers high-fidelity point map sequences with temporal coherence from open-world videos, enabling accurate 3D/4D reconstruction, camera parameter estimation, and other depth-based applications. At the core of our approach lies a point map Variational Autoencoder (VAE) that learns a latent space agnostic to video latent distributions for effective point map encoding and decoding. Leveraging the VAE, we train a video diffusion model to model the distribution of point map sequences conditioned on the input videos. Extensive evaluations on diverse datasets demonstrate that GeometryCrafter achieves state-of-the-art 3D accuracy, temporal consistency, and generalization capability.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.01005</link>
<guid>https://arxiv.org/abs/2504.01005</guid>
<content:encoded><![CDATA[
Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 13:41:57 GMT</pubDate>
</item>
<item>
<title>Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents</title>
<link>https://arxiv.org/abs/2504.00906</link>
<guid>https://arxiv.org/abs/2504.00906</guid>
<content:encoded><![CDATA[
Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 11:40:27 GMT</pubDate>
</item>
<item>
<title>Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2503.24379</link>
<guid>https://arxiv.org/abs/2503.24379</guid>
<content:encoded><![CDATA[
To address the bottleneck of accurate user intent interpretation within the current video generation community, we present Any2Caption, a novel framework for controllable video generation under any condition. The key idea is to decouple various condition interpretation steps from the video synthesis step. By leveraging modern multimodal large language models (MLLMs), Any2Caption interprets diverse inputs--text, images, videos, and specialized cues such as region, motion, and camera poses--into dense, structured captions that offer backbone video generators with better guidance. We also introduce Any2CapIns, a large-scale dataset with 337K instances and 407K conditions for any-condition-to-caption instruction tuning. Comprehensive evaluations demonstrate significant improvements of our system in controllability and video quality across various aspects of existing video generation models. Project Page: https://sqwu.top/Any2Cap/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.24377</link>
<guid>https://arxiv.org/abs/2503.24377</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1</title>
<link>https://arxiv.org/abs/2503.24376</link>
<guid>https://arxiv.org/abs/2503.24376</guid>
<content:encoded><![CDATA[
Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:55:23 GMT</pubDate>
</item>
<item>
<title>AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization</title>
<link>https://arxiv.org/abs/2503.23733</link>
<guid>https://arxiv.org/abs/2503.23733</guid>
<content:encoded><![CDATA[
Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:13:02 GMT</pubDate>
</item>
<item>
<title>Discovering Knowledge Deficiencies of Language Models on Massive Knowledge Base</title>
<link>https://arxiv.org/abs/2503.23361</link>
<guid>https://arxiv.org/abs/2503.23361</guid>
<content:encoded><![CDATA[
Large language models (LLMs) possess impressive linguistic capabilities but often fail to faithfully retain factual knowledge, leading to hallucinations and unreliable outputs. Understanding LLMs' knowledge deficiencies by exhaustively evaluating against full-scale knowledge bases is computationally prohibitive, especially for closed-weight models. We propose stochastic error ascent (SEA), a scalable and efficient framework for discovering knowledge deficiencies (errors) in closed-weight LLMs under a strict query budget. Rather than naively probing all knowledge candidates, SEA formulates error discovery as a stochastic optimization process: it iteratively retrieves new high-error candidates by leveraging the semantic similarity to previously observed failures. To further enhance search efficiency and coverage, SEA employs hierarchical retrieval across document and paragraph levels, and constructs a relation directed acyclic graph to model error propagation and identify systematic failure modes. Empirically, SEA uncovers 40.7x more knowledge errors than Automated Capability Discovery and 26.7% more than AutoBencher, while reducing the cost-per-error by 599x and 9x, respectively. Human evaluation confirms the high quality of generated questions, while ablation and convergence analyses validate the contribution of each component in SEA. Further analysis on the discovered errors reveals correlated failure patterns across LLM families and recurring deficits, highlighting the need for better data coverage and targeted fine-tuning in future LLM development.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 04:33:56 GMT</pubDate>
</item>
<item>
<title>ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning</title>
<link>https://arxiv.org/abs/2503.21860</link>
<guid>https://arxiv.org/abs/2503.21860</guid>
<content:encoded><![CDATA[
Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:50:30 GMT</pubDate>
</item>
<item>
<title>DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness</title>
<link>https://arxiv.org/abs/2503.22677</link>
<guid>https://arxiv.org/abs/2503.22677</guid>
<content:encoded><![CDATA[
Most 3D object generators focus on aesthetic quality, often neglecting physical constraints necessary in applications. One such constraint is that the 3D object should be self-supporting, i.e., remains balanced under gravity. Prior approaches to generating stable 3D objects used differentiable physics simulators to optimize geometry at test-time, which is slow, unstable, and prone to local optima. Inspired by the literature on aligning generative models to external feedback, we propose Direct Simulation Optimization (DSO), a framework to use the feedback from a (non-differentiable) simulator to increase the likelihood that the 3D generator outputs stable 3D objects directly. We construct a dataset of 3D objects labeled with a stability score obtained from the physics simulator. We can then fine-tune the 3D generator using the stability score as the alignment metric, via direct preference optimization (DPO) or direct reward optimization (DRO), a novel objective, which we introduce, to align diffusion models without requiring pairwise preferences. Our experiments show that the fine-tuned feed-forward generator, using either DPO or DRO objective, is much faster and more likely to produce stable objects than test-time optimization. Notably, the DSO framework works even without any ground-truth 3D objects for training, allowing the 3D generator to self-improve by automatically collecting simulation feedback on its own outputs.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>ActionStudio: A Lightweight Framework for Data and Training of Large Action Models</title>
<link>https://arxiv.org/abs/2503.22673</link>
<guid>https://arxiv.org/abs/2503.22673</guid>
<content:encoded><![CDATA[
Action models are essential for enabling autonomous agents to perform complex tasks. However, training large action models remains challenging due to the diversity of agent environments and the complexity of agentic data. Despite growing interest, existing infrastructure provides limited support for scalable, agent-specific fine-tuning. We present ActionStudio, a lightweight and extensible data and training framework designed for large action models. ActionStudio unifies heterogeneous agent trajectories through a standardized format, supports diverse training paradigms including LoRA, full fine-tuning, and distributed setups, and integrates robust preprocessing and verification tools. We validate its effectiveness across both public and realistic industry benchmarks, demonstrating strong performance and practical scalability. We open-sourced code and data at https://github.com/SalesforceAIResearch/xLAM to facilitate research in the community.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>AvatarArtist: Open-Domain 4D Avatarization</title>
<link>https://arxiv.org/abs/2503.19906</link>
<guid>https://arxiv.org/abs/2503.19906</guid>
<content:encoded><![CDATA[
This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies..
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>PAVE: Patching and Adapting Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.19794</link>
<guid>https://arxiv.org/abs/2503.19794</guid>
<content:encoded><![CDATA[
Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as "patches," which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:02:37 GMT</pubDate>
</item>
<item>
<title>Understanding Co-speech Gestures in-the-wild</title>
<link>https://arxiv.org/abs/2503.22668</link>
<guid>https://arxiv.org/abs/2503.22668</guid>
<content:encoded><![CDATA[
Co-speech gestures play a vital role in non-verbal communication. In this paper, we introduce a new framework for co-speech gesture understanding in the wild. Specifically, we propose three new tasks and benchmarks to evaluate a model's capability to comprehend gesture-text-speech associations: (i) gesture-based retrieval, (ii) gestured word spotting, and (iii) active speaker detection using gestures. We present a new approach that learns a tri-modal speech-text-video-gesture representation to solve these tasks. By leveraging a combination of global phrase contrastive loss and local gesture-word coupling loss, we demonstrate that a strong gesture representation can be learned in a weakly supervised manner from videos in the wild. Our learned representations outperform previous methods, including large vision-language models (VLMs), across all three tasks. Further analysis reveals that speech and text modalities capture distinct gesture-related signals, underscoring the advantages of learning a shared tri-modal embedding space. The dataset, model, and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:55:52 GMT</pubDate>
</item>
<item>
<title>Unicorn: Text-Only Data Synthesis for Vision Language Model Training</title>
<link>https://arxiv.org/abs/2503.22655</link>
<guid>https://arxiv.org/abs/2503.22655</guid>
<content:encoded><![CDATA[
Training vision-language models (VLMs) typically requires large-scale, high-quality image-text pairs, but collecting or synthesizing such data is costly. In contrast, text data is abundant and inexpensive, prompting the question: can high-quality multimodal training data be synthesized purely from text? To tackle this, we propose a cross-integrated three-stage multimodal data synthesis framework, which generates two datasets: Unicorn-1.2M and Unicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, we construct 1.2M semantically diverse high-quality captions by expanding sparse caption seeds using large language models (LLMs). In Stage 2: Instruction-Tuning Data Generation, we further process 471K captions into multi-turn instruction-tuning tasks to support complex reasoning. Finally, in Stage 3: Modality Representation Transfer, these textual captions representations are transformed into visual representations, resulting in diverse synthetic image representations. This three-stage process enables us to construct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction for instruction-tuning, without relying on real images. By eliminating the dependency on real images while maintaining data quality and diversity, our framework offers a cost-effective and scalable solution for VLMs training. Code is available at https://github.com/Yu-xm/Unicorn.git.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:43:00 GMT</pubDate>
</item>
<item>
<title>Entropy-Based Adaptive Weighting for Self-Training</title>
<link>https://arxiv.org/abs/2503.23913</link>
<guid>https://arxiv.org/abs/2503.23913</guid>
<content:encoded><![CDATA[
The mathematical problem-solving capabilities of large language models have become a focal point of research, with growing interests in leveraging self-generated reasoning paths as a promising way to refine and enhance these models. These paths capture step-by-step logical processes while requiring only the correct answer for supervision. The self-training method has been shown to be effective in reasoning tasks while eliminating the need for external models and manual annotations. However, optimizing the use of self-generated data for model training remains an open challenge. In this work, we propose Entropy-Based Adaptive Weighting for Self-Training (EAST), an adaptive weighting strategy designed to prioritize uncertain data during self-training. Specifically, EAST employs a mapping function with a tunable parameter that controls the sharpness of the weighting, assigning higher weights to data where the model exhibits greater uncertainty. This approach guides the model to focus on more informative and challenging examples, thereby enhancing its reasoning ability. We evaluate our approach on GSM8K and MATH benchmarks. Empirical results show that, while the vanilla method yields virtually no improvement (0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K, EAST attains a further 1-2% performance boost compared to the vanilla method.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 06:04:35 GMT</pubDate>
</item>
<item>
<title>MeshCraft: Exploring Efficient and Controllable Mesh Generation with Flow-based DiTs</title>
<link>https://arxiv.org/abs/2503.23022</link>
<guid>https://arxiv.org/abs/2503.23022</guid>
<content:encoded><![CDATA[
In the domain of 3D content creation, achieving optimal mesh topology through AI models has long been a pursuit for 3D artists. Previous methods, such as MeshGPT, have explored the generation of ready-to-use 3D objects via mesh auto-regressive techniques. While these methods produce visually impressive results, their reliance on token-by-token predictions in the auto-regressive process leads to several significant limitations. These include extremely slow generation speeds and an uncontrollable number of mesh faces. In this paper, we introduce MeshCraft, a novel framework for efficient and controllable mesh generation, which leverages continuous spatial diffusion to generate discrete triangle faces. Specifically, MeshCraft consists of two core components: 1) a transformer-based VAE that encodes raw meshes into continuous face-level tokens and decodes them back to the original meshes, and 2) a flow-based diffusion transformer conditioned on the number of faces, enabling the generation of high-quality 3D meshes with a predefined number of faces. By utilizing the diffusion model for the simultaneous generation of the entire mesh topology, MeshCraft achieves high-fidelity mesh generation at significantly faster speeds compared to auto-regressive methods. Specifically, MeshCraft can generate an 800-face mesh in just 3.2 seconds (35times faster than existing baselines). Extensive experiments demonstrate that MeshCraft outperforms state-of-the-art techniques in both qualitative and quantitative evaluations on ShapeNet dataset and demonstrates superior performance on Objaverse dataset. Moreover, it integrates seamlessly with existing conditional guidance strategies, showcasing its potential to relieve artists from the time-consuming manual work involved in mesh creation.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 05:21:50 GMT</pubDate>
</item>
<item>
<title>Decoupling Angles and Strength in Low-rank Adaptation</title>
<link>https://arxiv.org/abs/2503.18225</link>
<guid>https://arxiv.org/abs/2503.18225</guid>
<content:encoded><![CDATA[
Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 18:00:56 GMT</pubDate>
</item>
<item>
<title>Easi3R: Estimating Disentangled Motion from DUSt3R Without Training</title>
<link>https://arxiv.org/abs/2503.24391</link>
<guid>https://arxiv.org/abs/2503.24391</guid>
<content:encoded><![CDATA[
Recent advances in DUSt3R have enabled robust estimation of dense point clouds and camera parameters of static scenes, leveraging Transformer network architectures and direct supervision on large-scale 3D datasets. In contrast, the limited scale and diversity of available 4D datasets present a major bottleneck for training a highly generalizable 4D model. This constraint has driven conventional 4D methods to fine-tune 3D models on scalable dynamic video data with additional geometric priors such as optical flow and depths. In this work, we take an opposite path and introduce Easi3R, a simple yet efficient training-free method for 4D reconstruction. Our approach applies attention adaptation during inference, eliminating the need for from-scratch pre-training or network fine-tuning. We find that the attention layers in DUSt3R inherently encode rich information about camera and object motion. By carefully disentangling these attention maps, we achieve accurate dynamic region segmentation, camera pose estimation, and 4D dense point map reconstruction. Extensive experiments on real-world dynamic videos demonstrate that our lightweight attention adaptation significantly outperforms previous state-of-the-art methods that are trained or finetuned on extensive dynamic datasets. Our code is publicly available for research purpose at https://easi3r.github.io/
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data</title>
<link>https://arxiv.org/abs/2503.21694</link>
<guid>https://arxiv.org/abs/2503.21694</guid>
<content:encoded><![CDATA[
It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 12:59:15 GMT</pubDate>
</item>
<item>
<title>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</title>
<link>https://arxiv.org/abs/2503.19901</link>
<guid>https://arxiv.org/abs/2503.19901</guid>
<content:encoded><![CDATA[
Synthesizing diverse and physically plausible Human-Scene Interactions (HSI) is pivotal for both computer animation and embodied AI. Despite encouraging progress, current methods mainly focus on developing separate controllers, each specialized for a specific interaction task. This significantly hinders the ability to tackle a wide variety of challenging HSI tasks that require the integration of multiple skills, e.g., sitting down while carrying an object. To address this issue, we present TokenHSI, a single, unified transformer-based policy capable of multi-skill unification and flexible adaptation. The key insight is to model the humanoid proprioception as a separate shared token and combine it with distinct task tokens via a masking mechanism. Such a unified policy enables effective knowledge sharing across skills, thereby facilitating the multi-task training. Moreover, our policy architecture supports variable length inputs, enabling flexible adaptation of learned skills to new scenarios. By training additional task tokenizers, we can not only modify the geometries of interaction targets but also coordinate multiple skills to address complex tasks. The experiments demonstrate that our approach can significantly improve versatility, adaptability, and extensibility in various HSI tasks. Website: https://liangpan99.github.io/TokenHSI/
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:57:46 GMT</pubDate>
</item>
<item>
<title>UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation</title>
<link>https://arxiv.org/abs/2503.14941</link>
<guid>https://arxiv.org/abs/2503.14941</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have emerged to tackle the challenges of Visual Question Answering (VQA), sparking a new research focus on conducting objective evaluations of these models. Existing evaluation methods face limitations due to the significant human workload required to design Q&amp;A pairs for visual images, which inherently restricts the scale and scope of evaluations. Although automated MLLM-as-judge approaches attempt to reduce the human workload through automatic evaluations, they often introduce biases. To address these problems, we propose an Unsupervised Peer review MLLM Evaluation framework. It utilizes only image data, allowing models to automatically generate questions and conduct peer review assessments of answers from other models, effectively alleviating the reliance on human workload. Additionally, we introduce the vision-language scoring system to mitigate the bias issues, which focuses on three aspects: (i) response correctness; (ii) visual understanding and reasoning; and (iii) image-text correlation. Experimental results demonstrate that UPME achieves a Pearson correlation of 0.944 with human evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset, indicating that our framework closely aligns with human-designed benchmarks and inherent human preferences.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 03:15:41 GMT</pubDate>
</item>
<item>
<title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy</title>
<link>https://arxiv.org/abs/2503.24388</link>
<guid>https://arxiv.org/abs/2503.24388</guid>
<content:encoded><![CDATA[
Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than 17times sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>Query and Conquer: Execution-Guided SQL Generation</title>
<link>https://arxiv.org/abs/2503.24364</link>
<guid>https://arxiv.org/abs/2503.24364</guid>
<content:encoded><![CDATA[
We propose a novel approach for generating complex outputs that significantly improves accuracy in text-to-SQL tasks. Our method leverages execution results to select the most semantically consistent query from multiple candidates, enabling smaller, cost-effective models to surpass computationally intensive reasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inference cost by as much as 30 times. It integrates effortlessly with existing models, offering a practical and scalable pathway to state-of-the-art SQL generation.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:43:36 GMT</pubDate>
</item>
<item>
<title>Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model</title>
<link>https://arxiv.org/abs/2503.24290</link>
<guid>https://arxiv.org/abs/2503.24290</guid>
<content:encoded><![CDATA[
We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE (lambda=1, gamma=1) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 12:36:05 GMT</pubDate>
</item>
<item>
<title>What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 11:46:15 GMT</pubDate>
</item>
<item>
<title>Expanding RL with Verifiable Rewards Across Diverse Domains</title>
<link>https://arxiv.org/abs/2503.23829</link>
<guid>https://arxiv.org/abs/2503.23829</guid>
<content:encoded><![CDATA[
Reinforcement learning (RL) with verifiable rewards (RLVR) has shown promising results in mathematical reasoning and coding tasks where well-structured reference answers are available. However, its applicability to broader domains remains underexplored. In this work, we study the extension of RLVR to more diverse domains such as medicine, chemistry, psychology, and economics. We observe high agreement in binary judgments across different large language models (LLMs) when objective reference answers exist, which challenges the necessity of large-scale annotation for training domain-specific reward models. To address the limitations of binary rewards when handling unstructured reference answers, we further incorporate model-based soft scoring into RLVR to improve its flexibility. Our experiments show that a distilled generative reward model can serve as an effective cross-domain verifier, providing reliable reward signals for RL without requiring domain-specific annotations. By fine-tuning a base 7B model using various RL algorithms against our reward model, we obtain policies that outperform state-of-the-art open-source aligned LLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a large margin, across domains in free-form answer settings. This also strengthens RLVR's robustness and scalability, highlighting its potential for real-world applications with noisy or weak labels.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 04:22:49 GMT</pubDate>
</item>
<item>
<title>KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language</title>
<link>https://arxiv.org/abs/2503.23730</link>
<guid>https://arxiv.org/abs/2503.23730</guid>
<content:encoded><![CDATA[
The recent emergence of Large Vision-Language Models(VLMs) has resulted in a variety of different benchmarks for evaluating such models. Despite this, we observe that most existing evaluation methods suffer from the fact that they either require the model to choose from pre-determined responses, sacrificing open-endedness, or evaluate responses using a judge model, resulting in subjective and unreliable evaluation. In addition, we observe a lack of benchmarks for VLMs in the Korean language, which are necessary as a separate metric from more common English language benchmarks, as the performance of generative language models can differ significantly based on the language being used. Therefore, we present KOFFVQA, a general-purpose free-form visual question answering benchmark in the Korean language for the evaluation of VLMs. Our benchmark consists of 275 carefully crafted questions each paired with an image and grading criteria covering 10 different aspects of VLM performance. The grading criteria eliminate the problem of unreliability by allowing the judge model to grade each response based on a pre-determined set of rules. By defining the evaluation criteria in an objective manner, even a small open-source model can be used to evaluate models on our benchmark reliably. In addition to evaluating a large number of existing VLMs on our benchmark, we also experimentally verify that our method of using pre-existing grading criteria for evaluation is much more reliable than existing methods. Our evaluation code is available at https://github.com/maum-ai/KOFFVQA
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 01:04:25 GMT</pubDate>
</item>
<item>
<title>TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes</title>
<link>https://arxiv.org/abs/2503.23461</link>
<guid>https://arxiv.org/abs/2503.23461</guid>
<content:encoded><![CDATA[
This paper explores the task of Complex Visual Text Generation (CVTG), which centers on generating intricate textual content distributed across diverse regions within visual images. In CVTG, image generation models often rendering distorted and blurred visual text or missing some visual text. To tackle these challenges, we propose TextCrafter, a novel multi-visual text rendering method. TextCrafter employs a progressive strategy to decompose complex visual text into distinct components while ensuring robust alignment between textual content and its visual carrier. Additionally, it incorporates a token focus enhancement mechanism to amplify the prominence of visual text during the generation process. TextCrafter effectively addresses key challenges in CVTG tasks, such as text confusion, omissions, and blurriness. Moreover, we present a new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the performance of generative models on CVTG tasks. Extensive experiments demonstrate that our method surpasses state-of-the-art approaches.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 10:36:55 GMT</pubDate>
</item>
<item>
<title>SketchVideo: Sketch-based Video Generation and Editing</title>
<link>https://arxiv.org/abs/2503.23284</link>
<guid>https://arxiv.org/abs/2503.23284</guid>
<content:encoded><![CDATA[
Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 22:44:09 GMT</pubDate>
</item>
<item>
<title>Efficient Inference for Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2503.23077</link>
<guid>https://arxiv.org/abs/2503.23077</guid>
<content:encoded><![CDATA[
Large Reasoning Models (LRMs) significantly improve the reasoning ability of Large Language Models (LLMs) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for LRMs, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought (CoT), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent CoT, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing LRMs' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant fieldhttps://github.com/yueliu1999/Awesome-Efficient-Inference-for-LRMs.
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 09:27:46 GMT</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We conduct comprehensive evaluations across multiple tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 13:50:13 GMT</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: A Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Mon, 31 Mar 2025 10:06:17 GMT</pubDate>
</item>
<item>
<title>MoCha: Towards Movie-Grade Talking Character Synthesis</title>
<link>https://arxiv.org/abs/2503.23307</link>
<guid>https://arxiv.org/abs/2503.23307</guid>
<content:encoded><![CDATA[
Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a speech-video window attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labeled video datasets, we introduce a joint training strategy that leverages both speech-labeled and text-labeled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue-allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human preference studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, expressiveness, controllability and generalization.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 00:22:09 GMT</pubDate>
</item>
<item>
<title>Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization</title>
<link>https://arxiv.org/abs/2503.20286</link>
<guid>https://arxiv.org/abs/2503.20286</guid>
<content:encoded><![CDATA[
Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 03:30:23 GMT</pubDate>
</item>
<item>
<title>Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code</title>
<link>https://arxiv.org/abs/2503.18809</link>
<guid>https://arxiv.org/abs/2503.18809</guid>
<content:encoded><![CDATA[
In recent years, large language models (LLMs) have shown remarkable capabilities in various artificial intelligence problems. However, they fail to plan reliably, even when prompted with a detailed definition of the planning task. Attempts to improve their planning capabilities, such as chain-of-thought prompting, fine-tuning, and explicit "reasoning" still yield incorrect plans and usually fail to generalize to larger tasks. In this paper, we show how to use LLMs to generate correct plans, even for out-of-distribution tasks of increasing size. For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one. The resulting LLM-generated heuristics solve many more unseen test tasks than state-of-the-art domain-independent heuristics for classical planning. They are even competitive with the strongest learning algorithm for domain-dependent planning. These findings are especially remarkable given that our proof-of-concept implementation is based on an unoptimized Python planner and the baselines all build upon highly optimized C++ code. In some domains, the LLM-generated heuristics expand fewer states than the baselines, revealing that they are not only efficiently computable, but sometimes even more informative than the state-of-the-art heuristics. Overall, our results show that sampling a set of planning heuristic function programs can significantly improve the planning capabilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:50:20 GMT</pubDate>
</item>
<item>
<title>SWI: Speaking with Intent in Large Language Models</title>
<link>https://arxiv.org/abs/2503.21544</link>
<guid>https://arxiv.org/abs/2503.21544</guid>
<content:encoded><![CDATA[
Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:34:28 GMT</pubDate>
</item>
<item>
<title>Reconstructing Humans with a Biomechanically Accurate Skeleton</title>
<link>https://arxiv.org/abs/2503.21751</link>
<guid>https://arxiv.org/abs/2503.21751</guid>
<content:encoded><![CDATA[
In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:24 GMT</pubDate>
</item>
<item>
<title>Your ViT is Secretly an Image Segmentation Model</title>
<link>https://arxiv.org/abs/2503.19108</link>
<guid>https://arxiv.org/abs/2503.19108</guid>
<content:encoded><![CDATA[
Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity. Code: https://www.tue-mps.org/eomt/.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 15:56:02 GMT</pubDate>
</item>
<item>
<title>4D-Bench: Benchmarking Multi-modal Large Language Models for 4D Object Understanding</title>
<link>https://arxiv.org/abs/2503.17827</link>
<guid>https://arxiv.org/abs/2503.17827</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have demonstrated impressive 2D image/video understanding capabilities. However, there are no publicly standardized benchmarks to assess the abilities of MLLMs in understanding the 4D objects (3D objects with temporal evolution over time). In this paper, we introduce 4D-Bench, the first benchmark to evaluate the capabilities of MLLMs in 4D object understanding, featuring tasks in 4D object Question Answering (4D object QA) and 4D object captioning. 4D-Bench provides 4D objects with diverse categories, high-quality annotations, and tasks necessitating multi-view spatial-temporal understanding, different from existing 2D image/video-based benchmarks. With 4D-Bench, we evaluate a wide range of open-source and closed-source MLLMs. The results from the 4D object captioning experiment indicate that MLLMs generally exhibit weaker temporal understanding compared to their appearance understanding, notably, while open-source models approach closed-source performance in appearance understanding, they show larger performance gaps in temporal understanding. 4D object QA yields surprising findings: even with simple single-object videos, MLLMs perform poorly, with state-of-the-art GPT-4o achieving only 63\% accuracy compared to the human baseline of 91\%. These findings highlight a substantial gap in 4D object understanding and the need for further advancements in MLLMs.
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 13:55:53 GMT</pubDate>
</item>
<item>
<title>OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning</title>
<link>https://arxiv.org/abs/2503.16081</link>
<guid>https://arxiv.org/abs/2503.16081</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have gained significant traction for their ability to process diverse input data types and generate coherent, contextually relevant outputs across various applications. While supervised fine-tuning (SFT) has been the predominant approach to enhance MLLM capabilities in task-specific optimization, it often falls short in fostering crucial generalized reasoning abilities. Although reinforcement learning (RL) holds great promise in overcoming these limitations, it encounters two significant challenges: (1) its generalized capacities in multimodal tasks remain largely unexplored, and (2) its training constraints, including the constant Kullback-Leibler divergence or the clamp strategy, often result in suboptimal bottlenecks. To address these challenges, we propose OThink-MR1, an advanced MLLM equipped with profound comprehension and reasoning capabilities across multimodal tasks. Specifically, we introduce Group Relative Policy Optimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedly enhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct, GRPO-D achieves a relative improvement of more than 5.72% over SFT and more than 13.59% over GRPO in same-task evaluation on two adapted datasets. Furthermore, GRPO-D demonstrates remarkable cross-task generalization capabilities, with an average relative improvement of more than 61.63% over SFT in cross-task evaluation. These results highlight that the MLLM trained with GRPO-D on one multimodal task can be effectively transferred to another task, underscoring the superior generalized reasoning capabilities of our proposed OThink-MR1 model.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:22:18 GMT</pubDate>
</item>
<item>
<title>A Refined Analysis of Massive Activations in LLMs</title>
<link>https://arxiv.org/abs/2503.22329</link>
<guid>https://arxiv.org/abs/2503.22329</guid>
<content:encoded><![CDATA[
Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 07:08:34 GMT</pubDate>
</item>
<item>
<title>Segment Any Motion in Videos</title>
<link>https://arxiv.org/abs/2503.22268</link>
<guid>https://arxiv.org/abs/2503.22268</guid>
<content:encoded><![CDATA[
Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 05:34:11 GMT</pubDate>
</item>
<item>
<title>Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging</title>
<link>https://arxiv.org/abs/2503.22236</link>
<guid>https://arxiv.org/abs/2503.22236</guid>
<content:encoded><![CDATA[
With the growing demand for high-fidelity 3D models from 2D images, existing methods still face significant challenges in accurately reproducing fine-grained geometric details due to limitations in domain gaps and inherent ambiguities in RGB images. To address these issues, we propose Hi3DGen, a novel framework for generating high-fidelity 3D geometry from images via normal bridging. Hi3DGen consists of three key components: (1) an image-to-normal estimator that decouples the low-high frequency image pattern with noise injection and dual-stream training to achieve generalizable, stable, and sharp estimation; (2) a normal-to-geometry learning approach that uses normal-regularized latent diffusion learning to enhance 3D geometry generation fidelity; and (3) a 3D data synthesis pipeline that constructs a high-quality dataset to support training. Extensive experiments demonstrate the effectiveness and superiority of our framework in generating rich geometric details, outperforming state-of-the-art methods in terms of fidelity. Our work provides a new direction for high-fidelity 3D geometry generation from images by leveraging normal maps as an intermediate representation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:39:20 GMT</pubDate>
</item>
<item>
<title>Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2503.22230</link>
<guid>https://arxiv.org/abs/2503.22230</guid>
<content:encoded><![CDATA[
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning large language models with human preferences. While recent research has focused on algorithmic improvements, the importance of prompt-data construction has been overlooked. This paper addresses this gap by exploring data-driven bottlenecks in RLHF performance scaling, particularly reward hacking and decreasing response diversity. We introduce a hybrid reward system combining reasoning task verifiers (RTV) and a generative reward model (GenRM) to mitigate reward hacking. We also propose a novel prompt-selection method, Pre-PPO, to maintain response diversity and enhance learning effectiveness. Additionally, we find that prioritizing mathematical and coding tasks early in RLHF training significantly improves performance. Experiments across two model sizes validate our methods' effectiveness and scalability. Results show that RTV is most resistant to reward hacking, followed by GenRM with ground truth, and then GenRM with SFT Best-of-N responses. Our strategies enable rapid capture of subtle task-specific distinctions, leading to substantial improvements in overall RLHF performance. This work highlights the importance of careful data construction and provides practical methods to overcome performance barriers in RLHF.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 04:26:41 GMT</pubDate>
</item>
<item>
<title>X^{2}-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction</title>
<link>https://arxiv.org/abs/2503.21779</link>
<guid>https://arxiv.org/abs/2503.21779</guid>
<content:encoded><![CDATA[
Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X^2-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X^2-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling</title>
<link>https://arxiv.org/abs/2503.21732</link>
<guid>https://arxiv.org/abs/2503.21732</guid>
<content:encoded><![CDATA[
Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to 1024^3 directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:46:42 GMT</pubDate>
</item>
<item>
<title>On Large Multimodal Models as Open-World Image Classifiers</title>
<link>https://arxiv.org/abs/2503.21851</link>
<guid>https://arxiv.org/abs/2503.21851</guid>
<content:encoded><![CDATA[
Traditional image classification requires a predefined list of semantic categories. In contrast, Large Multimodal Models (LMMs) can sidestep this requirement by classifying images directly using natural language (e.g., answering the prompt "What is the main object in the image?"). Despite this remarkable capability, most existing studies on LMM classification performance are surprisingly limited in scope, often assuming a closed-world setting with a predefined set of categories. In this work, we address this gap by thoroughly evaluating LMM classification performance in a truly open-world setting. We first formalize the task and introduce an evaluation protocol, defining various metrics to assess the alignment between predicted and ground truth classes. We then evaluate 13 models across 10 benchmarks, encompassing prototypical, non-prototypical, fine-grained, and very fine-grained classes, demonstrating the challenges LMMs face in this task. Further analyses based on the proposed metrics reveal the types of errors LMMs make, highlighting challenges related to granularity and fine-grained capabilities, showing how tailored prompting and reasoning can alleviate them.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:03:18 GMT</pubDate>
</item>
<item>
<title>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</title>
<link>https://arxiv.org/abs/2503.21614</link>
<guid>https://arxiv.org/abs/2503.21614</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:36:30 GMT</pubDate>
</item>
<item>
<title>ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback</title>
<link>https://arxiv.org/abs/2503.21332</link>
<guid>https://arxiv.org/abs/2503.21332</guid>
<content:encoded><![CDATA[
Summarization refinement faces challenges when extending to multi-dimension. In this paper, we introduce ReFeed, a powerful summarization refinement pipeline that enhances multiple dimensions through reflective reasoning on feedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-based dataset optimized for training a lightweight model with reflective reasoning. Our experiments reveal how the number of dimensions, feedback exposure, and reasoning policy influence refinement performance, highlighting reflective reasoning and simultaneously addressing multiple feedback is crucial to mitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisy feedback and feedback order. Lastly, our finding emphasizes that creating data with a proper goal and guideline constitutes a fundamental pillar of effective reasoning. The dataset and model will be released.
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 06:11:41 GMT</pubDate>
</item>
<item>
<title>Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency</title>
<link>https://arxiv.org/abs/2503.20785</link>
<guid>https://arxiv.org/abs/2503.20785</guid>
<content:encoded><![CDATA[
We present Free4D, a novel tuning-free framework for 4D scene generation from a single image. Existing methods either focus on object-level generation, making scene-level generation infeasible, or rely on large-scale multi-view video datasets for expensive training, with limited generalization ability due to the scarcity of 4D scene data. In contrast, our key insight is to distill pre-trained foundation models for consistent 4D scene representation, which offers promising advantages such as efficiency and generalizability. 1) To achieve this, we first animate the input image using image-to-video diffusion models followed by 4D geometric structure initialization. 2) To turn this coarse structure into spatial-temporal consistent multiview videos, we design an adaptive guidance mechanism with a point-guided denoising strategy for spatial consistency and a novel latent replacement strategy for temporal coherence. 3) To lift these generated observations into consistent 4D representation, we propose a modulation-based refinement to mitigate inconsistencies while fully leveraging the generated information. The resulting 4D representation enables real-time, controllable rendering, marking a significant advancement in single-image-based 4D scene generation.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</title>
<link>https://arxiv.org/abs/2503.20308</link>
<guid>https://arxiv.org/abs/2503.20308</guid>
<content:encoded><![CDATA[
Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria -- Temporal Synchronization, Lip Readability, and Expressiveness -- are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:18:57 GMT</pubDate>
</item>
<item>
<title>PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving</title>
<link>https://arxiv.org/abs/2503.21821</link>
<guid>https://arxiv.org/abs/2503.21821</guid>
<content:encoded><![CDATA[
We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:21:56 GMT</pubDate>
</item>
<item>
<title>AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2503.19693</link>
<guid>https://arxiv.org/abs/2503.19693</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 10:18:21 GMT</pubDate>
</item>
<item>
<title>MedAgent-Pro: Towards Multi-modal Evidence-based Medical Diagnosis via Reasoning Agentic Workflow</title>
<link>https://arxiv.org/abs/2503.18968</link>
<guid>https://arxiv.org/abs/2503.18968</guid>
<content:encoded><![CDATA[
Developing reliable AI systems to assist human clinicians in multi-modal medical diagnosis has long been a key objective for researchers. Recently, Multi-modal Large Language Models (MLLMs) have gained significant attention and achieved success across various domains. With strong reasoning capabilities and the ability to perform diverse tasks based on user instructions, they hold great potential for enhancing medical diagnosis. However, directly applying MLLMs to the medical domain still presents challenges. They lack detailed perception of visual inputs, limiting their ability to perform quantitative image analysis, which is crucial for medical diagnostics. Additionally, MLLMs often exhibit hallucinations and inconsistencies in reasoning, whereas clinical diagnoses must adhere strictly to established criteria. To address these challenges, we propose MedAgent-Pro, an evidence-based reasoning agentic system designed to achieve reliable, explainable, and precise medical diagnoses. This is accomplished through a hierarchical workflow: at the task level, knowledge-based reasoning generate reliable diagnostic plans for specific diseases following retrieved clinical criteria. While at the case level, multiple tool agents process multi-modal inputs, analyze different indicators according to the plan, and provide a final diagnosis based on both quantitative and qualitative evidence. Comprehensive experiments on both 2D and 3D medical diagnosis tasks demonstrate the superiority and effectiveness of MedAgent-Pro, while case studies further highlight its reliability and interpretability. The code is available at https://github.com/jinlab-imvr/MedAgent-Pro.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 10:04:18 GMT</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose ReaRec, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.22194</link>
<guid>https://arxiv.org/abs/2503.22194</guid>
<content:encoded><![CDATA[
We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.
]]></content:encoded>
<pubDate>Fri, 28 Mar 2025 03:23:12 GMT</pubDate>
</item>
<item>
<title>Tracktention Layer：提升视频预测中的时间一致性</title>
<link>https://arxiv.org/abs/2503.19904</link>
<guid>https://arxiv.org/abs/2503.19904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Tracktention Layer以增强视频预测中的时间一致性。</p><br /><br /><p><strong>摘要：</strong> 视频预测中的时间一致性至关重要，以确保输出的连贯性和去除伪影。传统方法如时间注意力和3D卷积在应对剧烈物体运动时存在困难，并且可能无法捕捉动态场景中的长程时间依赖。为了解决这个问题，本文提出了一种新颖的Tracktention Layer结构组件，它通过点轨迹明确集成运动信息。Tracktention Layer通过这些运动线索来增强时间对齐，有效处理复杂物体运动，并在时间上保持一致的特征表示。此方法计算效率高，可无缝集成到现有模型（如视觉Transformer）中，且仅需微小修改。它还可用于将图像模型升级为最先进的视频模型，有时甚至超越原生设计用于视频预测的模型。实验结果显示，在视频深度预测和视频着色任务中，增添Tracktention Layer的模型在时间一致性上明显优于基线对照。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:48 GMT</pubDate>
</item>
<item>
<title>无训练的测试时领域适应框架SemLA在开放词汇语义分割中的应用</title>
<link>https://arxiv.org/abs/2503.21780</link>
<guid>https://arxiv.org/abs/2503.21780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SemLA是一个无训练的测试时领域适应框架，提升了开放词汇语义分割的适应性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SemLA，一个新颖的无训练框架，旨在在测试时实现领域适应，提升开放词汇语义分割模型的性能。SemLA依托基于LoRA的适配器库，并通过CLIP嵌入进行索引，能够根据目标领域在嵌入空间中的近邻动态合并最相关的适配器，从而无需额外训练便构建出针对具体输入的定制模型。该方法有效地扩展了适应性，增强了模型的可解释性，并在保护数据隐私方面具有优势。针对10个标准数据集构建的20领域基准测试的全面实验显示，SemLA在多样化的设置下展现了卓越的适应性与性能，建立了开放词汇语义分割领域适应的新标准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>LOCATEdit：基于图的文本引导图像编辑方法</title>
<link>https://arxiv.org/abs/2503.21541</link>
<guid>https://arxiv.org/abs/2503.21541</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LOCATEdit通过图形化方法提升图像编辑的准确性和一致性。</p><br /><br /><p><strong>摘要：</strong> TEXT-guided image editing 旨在根据自然语言指令修改图像的特定区域，同时保持整体结构和背景的完整性。现有方法依赖于从扩散模型生成的交叉注意力图来识别需要修改的目标区域，但由于这些机制侧重于语义相关性，往往无法维持图像的完整性，导致缺乏空间一致性和产生编辑伪影。本文提出的LOCATEdit则通过基于图的方法增强交叉注意力图，利用自注意力所产生的块关系，确保图像区域间的平滑一致性，限制修改仅在指定区域，同时保留周围的结构。LOCATEdit在PIE-Bench上表现优于现有基线方法，在多项编辑任务中展现出卓越的性能和有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21541" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 10:32:17 GMT</pubDate>
</item>
<item>
<title>Embodied Reasoner：提升交互式体态搜索任务的推理能力</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Embodied Reasoner 在交互式体态搜索任务中展现出卓越的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期的深度推理模型在数学和编码任务上表现出了杰出的推理能力，但在需要通过图像和动作交替轨迹与环境持续交互的体态领域尚待探索。为此，我们提出了 Embodied Reasoner 模型，旨在扩展到交互式体态搜索任务。与主要依赖逻辑推理的数学推理不同，体态场景需要空间理解、时间推理以及基于交互历史的持续自我反思。我们合成了包含 9.3k 个连贯观察-思考-行动轨迹的训练数据，涵盖 64k 张交互图像和 90k 种多样的思维过程。通过模仿学习、自我探索和自我校正等三阶段训练流程，模型显著提升了能力，评估结果显示其在复杂长时间任务中，超过了 OpenAI o1、o3-mini 和 Claude-3.7 等先进视觉推理模型，表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:00:51 GMT</pubDate>
</item>
<item>
<title>大语言模型在科学发现中的潜力与新基准</title>
<link>https://arxiv.org/abs/2503.21248</link>
<guid>https://arxiv.org/abs/2503.21248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了评估大语言模型在科学发现中的研究假设生成能力的新基准。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）在科学研究中展现了潜力，但其发现高质量研究假设的能力尚未经过检验。为此，本文提出第一个大规模基准，以评估LLMs在科学发现中的表现，涵盖灵感检索、假设构建和假设排名等子任务。我们开发了一个自动化框架，提取12个学科科学论文中的关键组成部分，包括研究问题、背景调查、灵感和假设，并通过专家验证确保其准确性。为避免数据污染，我们仅关注2024年发表的论文，以确保与LLMs预训练数据的重叠最小。评估结果显示，LLMs在灵感检索方面表现良好，表明它们能够发现新颖的知识关联。这使得LLMs有潜力作为“研究假设矿”，通过自动生成创新假设，促进科学发现的自动化，减少人类干预。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 04:09:15 GMT</pubDate>
</item>
<item>
<title>FinAudio: 金融领域音频语言模型评估基准</title>
<link>https://arxiv.org/abs/2503.20990</link>
<guid>https://arxiv.org/abs/2503.20990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinAudio是首个用于评估音频语言模型在金融领域表现的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FinAudio，这是第一个专为评估音频大型语言模型（AudioLLMs）在金融场景中的能力而设计的基准。随着音频任务（如对话、音频理解和自动语音识别）性能的显著提升，金融领域中，包括收益电话会议和首席执行官演讲在内的音频数据，成为了金融分析和投资决策的重要资源。然而，至今缺乏财务情境下评估AudioLLMs的基准。本文定义了三个基于金融领域独特特征的任务，并策划了两个短音频和两个长音频数据集，同时开发了一种新数据集用于金融音频摘要，构成了FinAudio基准。在我们的评估中，评测了七种流行的AudioLLMs，并揭示了它们在金融领域的局限性，提供了改进AudioLLMs的见解。所有数据集和代码将被公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 17:07:51 GMT</pubDate>
</item>
<item>
<title>Video-R1: 基于多模态大语言模型的视频推理探索</title>
<link>https://arxiv.org/abs/2503.21776</link>
<guid>https://arxiv.org/abs/2503.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Video-R1，通过T-GRPO算法提升视频推理能力，克服数据稀缺和时序建模问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Video-R1，这是首次系统探索在多模态大语言模型（MLLMs）中应用R1范式进行视频推理。直接在视频推理上应用GRPO算法进行强化学习训练面临两个主要挑战：缺乏视频推理的时序建模以及高质量视频推理数据稀缺。为了解决这些问题，提出了T-GRPO算法，该算法鼓励模型利用视频中的时序信息进行推理。此外，我们在训练过程中还引入高质量的图像推理数据。构建了两个数据集：用于SFT冷启动的Video-R1-COT-165k和用于强化学习训练的Video-R1-260k，均包含图像和视频数据。实验结果表明，Video-R1在视频推理基准测试如VideoMMMU和VSI-Bench上取得显著提升，并在MVBench和TempCompass等一般视频基准测试中表现优异。特别是在视频空间推理基准VSI-bench上，Video-R1-7B模型达到了35.8%的准确率，超过商业专有模型GPT-4o。所有代码、模型和数据均已发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>优化步长调度的扩散模型提高生成效率</title>
<link>https://arxiv.org/abs/2503.21774</link>
<guid>https://arxiv.org/abs/2503.21774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种优化步长调度的方法，显著提升扩散模型的生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为最优步长蒸馏的动态规划框架，旨在通过从参考轨迹中提取理论最优的步长调度，以解决扩散模型在采样过程中因子步长离散化不佳而导致的计算密集性问题。通过将步长优化重构为递归误差最小化，该方法保证了全局离散化界限，并充分利用最优子结构。这些蒸馏出的调度在不同架构、ODE求解器和噪声调度中展现出强大的鲁棒性。实验结果表明，该方法使文本到图像的生成速度提高了10倍，同时保持了99.4%的性能。本研究的代码已在GitHub上发布，供进一步研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>视频生成中的物理认知进展及其挑战</title>
<link>https://arxiv.org/abs/2503.21765</link>
<guid>https://arxiv.org/abs/2503.21765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了视频生成中的物理认知进展及未来研究方向。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型的快速发展，视频生成取得了显著进展。但其在物理认知方面的不足逐渐受到重视，生成的内容常常违反物理基本法则。本文旨在填补这一领域系统概述的缺失，讨论物理认知在视频生成中的演进过程，并提出一个三层次的分类：1) 基本模式感知生成，2) 物理知识的被动认知生成，3) 世界模拟的主动认知。我们将重点强调该领域的关键挑战，并概述未来研究的潜在方向，旨在推动学术界和工业界对可解释、可控、物理一致的视频生成范式的讨论，为生成模型从“视觉模仿”向“人类般物理理解”的新阶段迈进提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>Lumina-Image 2.0：先进的文本生成图像框架</title>
<link>https://arxiv.org/abs/2503.21758</link>
<guid>https://arxiv.org/abs/2503.21758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Lumina-Image 2.0通过统一架构和高效训练显著提升文本生成图像性能。</p><br /><br /><p><strong>摘要：</strong> Lumina-Image 2.0是一种进阶的文本生成图像（T2I）框架，较之前的Lumina-Next取得了显著进展。该框架基于两个关键原则：统一性和效率。通过使用统一的架构（Unified Next-DiT），将文本和图像标记视为联合序列，从而实现自然的跨模态交互，并允许无缝的任务扩展。此外，Lumina-Image 2.0引入了统一的标注系统（Unified Captioner, UniCap），专门设计用于T2I生成任务，能够生成全面且准确的描述，加快收敛速度并增强提示的遵循性。在效率方面，我们开发了多阶段进阶训练策略和推理加速技术，而不降低图像质量。经过广泛的评价，Lumina-Image 2.0在学术基准和公共文本生成图像领域展现了强劲的性能，且仅用2.6B参数，显示出其可扩展性和设计效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>推进视频生成模型的内在真实性评估</title>
<link>https://arxiv.org/abs/2503.21755</link>
<guid>https://arxiv.org/abs/2503.21755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VBench-2.0旨在评估视频生成模型的内在真实度，超越表面视觉效果。</p><br /><br /><p><strong>摘要：</strong> 视频生成技术已从产生不现实的输出发展到生成视觉上令人信服且时间上连贯的视频。为了评估这些模型，VBench等基准被开发出来，衡量每帧美学、时间一致性和基本提示遵循等因素。然而，这些仅代表表面真实性，未能保证生成视频遵循现实原则。为了实现真正的“世界模型”，需要关注内在真实度，以确保生成视频遵循物理定律、常识推理、解剖正确性与组成完整性。为此，我们提出了VBench-2.0，它自动评估视频生成模型的内在真实度，包括人类符合度、可控性、创造性、物理与常识等五个关键维度，进一步细分为更细致的能力。我们的评估框架整合了前沿的VLM和LLM等通用工具，以及专门针对视频生成的异常检测方法，旨在推进视频生成模型的标准向内在真实度发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:57:01 GMT</pubDate>
</item>
<item>
<title>LeX-Art: 高质量文本-图像合成的完整解决方案</title>
<link>https://arxiv.org/abs/2503.21749</link>
<guid>https://arxiv.org/abs/2503.21749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LeX-Art系统性地提升文本-图像合成质量和渲染真实感。</p><br /><br /><p><strong>摘要：</strong> LeX-Art 是一套全面的高质量文本-图像合成工具，旨在系统性地弥合提示表达力和文本渲染保真度之间的差距。通过以数据为中心的方法，我们构建了一个基于 Deepseek-R1 的高质量数据合成管道，策划了 LeX-10K 数据集，包含10,000幅高分辨率和美学精炼的1024×1024图像。此外，我们开发了 LeX-Enhancer，一个强大的提示增强模型，并训练了两个文本到图像模型，LeX-FLUX 和 LeX-Lumina，取得了最先进的文本渲染性能。为系统评估视觉文本生成，我们推出了 LeX-Bench，一个评估保真度、美学和一致性的基准，辅之以对文本准确性评估的新指标 Pairwise Normalized Edit Distance（PNED）。实验结果显示，LeX-Lumina 在 CreateBench 上实现了79.81%的 PNED 增益，而 LeX-FLUX 在颜色、位置和字体准确性上均超越基线。我们的代码、模型、数据集和演示版本均已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:56:15 GMT</pubDate>
</item>
<item>
<title>ReaRAG：增强准确性的推理模型</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReaRAG通过增强事实性与推理能力，提升多跳问答表现。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）在推理能力方面表现出色，但主要依赖参数知识，导致事实准确性受限。尽管一些近期研究为基于强化学习的LRMs增加了检索能力，但这些模型仍存在过度思考和推理缺乏稳健性的问题，从而降低了其在问答任务中的效果。为此，我们提出了ReaRAG，一种增强事实性的推理模型，能够在避免过多迭代的情况下探索多样的查询。该解决方案包括一个新的数据构建框架，并为推理链长度设定上限。我们首先利用LRM生成深思熟虑的推理，然后从预定义的动作空间（搜索和结束）中选择一个动作。在搜索动作中，针对RAG引擎执行查询，结果作为观测返回，以指导后续的推理步骤。该过程重复进行，直至选择结束动作。得益于ReaRAG的强大推理能力，我们的方法在多跳问答方面超越了现有基准，进一步的分析突显了其强大的反思能力，可以识别错误并优化推理路径。我们的研究在有效结合稳健推理与检索增强生成的同时，提高了LRMs的事实性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21729" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 13:44:18 GMT</pubDate>
</item>
<item>
<title>基于规则的强化学习提升多模态大语言模型的GUI动作预测能力</title>
<link>https://arxiv.org/abs/2503.21620</link>
<guid>https://arxiv.org/abs/2503.21620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示规则基础的强化学习提升了多模态模型在GUI动作预测中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文首次探讨如何通过规则基础的强化学习（RL）提升多模态大语言模型（MLLMs）在图形用户界面（GUI）动作预测任务中的推理能力。我们构建了一个包含136个挑战性任务的高质量小型数据集，涵盖移动设备上的五种常见动作类型。此外，引入统一的基于规则的动作奖励，该奖励适用于基于策略的算法（如组相对策略优化，GRPO）。实验结果表明，所提出的数据高效模型UI-R1-3B在领域内（ID）和领域外（OOD）任务上均实现了显著改善。在ID基准AndroidControl上，动作类型的准确率提升了15%，而定位准确率提高了10.3%；在OOD GUI定位基准ScreenSpot-Pro上，我们的模型超越了基准模型6.0%，并在与更大模型（如OS-Atlas-7B）进行比较时展现了竞争力，这些模型是在76K数据上通过监督微调（SFT）训练的。这些结果凸显了基于规则的强化学习在推动GUI理解与控制方面的潜力，预示着未来研究的新方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 11:39:30 GMT</pubDate>
</item>
<item>
<title>智能代理时代的到来：大语言模型驱动下的研究综述</title>
<link>https://arxiv.org/abs/2503.21460</link>
<guid>https://arxiv.org/abs/2503.21460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本综述系统阐述了大语言模型代理的设计与演进。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型的革命性进展，智能代理时代已经来临。大语言模型（LLM）代理以目标驱动的行为和动态适应能力，可能成为通向人工通用智能的重要途径。本综述通过以方法论为中心的分类法，系统性地分析了LLM代理系统，揭示了代理设计原则与其在复杂环境中涌现行为之间的基本联系。我们统一了零散的研究线索，提供了统一的架构视角，探讨了代理的构建、合作及其随时间演变的过程，同时还关注了评估方法、工具应用、实际挑战及多样的应用领域。通过对这一快速发展领域的最新进展进行梳理，我们为研究者提供了理解LLM代理的结构化分类，并识别未来研究的有希望方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 08:50:17 GMT</pubDate>
</item>
<item>
<title>OlymMATH: 新的奥林匹克数学基准测试大规模推理模型</title>
<link>https://arxiv.org/abs/2503.21380</link>
<guid>https://arxiv.org/abs/2503.21380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OlymMATH是一个新数学基准，旨在检验大型模型的复杂推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型的发展使得现有的数学推理评估基准显得饱和，亟需更加严谨和具有挑战性的评估框架。为此，我们推出了OlymMATH，这是一个新颖的奥林匹克级数学基准，专为严格测试大型语言模型的复杂推理能力而设计。OlymMATH包含200道经过仔细筛选并手动验证的问题，提供英文和中文两种版本，系统地分为两大难度层次：AIME级别的问题（简单），以及更具挑战性的问题（困难），旨在推动当前最先进模型的极限。基准覆盖四个核心数学领域，且每道题目都有可验证的数值解，以实现客观、基于规则的评估。实证结果表明，OlymMATH所带来的挑战显著，最先进的模型在困难子集上的准确率明显有限。此外，此基准还有助于全面评估数学推理能力，填补了主流数学推理基准未涉及的双语评估维度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 07:20:17 GMT</pubDate>
</item>
<item>
<title>基于音频输入的实时互动视频生成框架</title>
<link>https://arxiv.org/abs/2503.21144</link>
<guid>https://arxiv.org/abs/2503.21144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新颖框架，支持实时互动视频生成，增强表情与上半身动作的同步。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的实时互动视频生成框架，旨在克服现有技术在头部动作与身体动作同步生成中的局限。该框架分为两个主要阶段：第一阶段采用高效的层次化运动扩散模型，基于音频输入生成多样的面部表情，并实现头部与身体动作的同步；第二阶段则专注于生成包含上半身动作和手势的视频，使用显式手部控制信号来生成更细致的手部动作，并对面部进行细化处理以提高视频的真实感和表现力。我们的方案在4090 GPU上以最高512 * 768分辨率和30fps的速度支持实时互动视频聊天，实验结果显示该方法能够生成富有表现力的肖像视频，展现自然的上半身动作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 27 Mar 2025 00:18:53 GMT</pubDate>
</item>
<item>
<title>ZJUKLAB团队在SemEval-2025任务中的敏感内容去除研究</title>
<link>https://arxiv.org/abs/2503.21088</link>
<guid>https://arxiv.org/abs/2503.21088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍ZJUKLAB团队在SemEval-2025中的敏感内容去除系统与成就。</p><br /><br /><p><strong>摘要：</strong> 本文提出了ZJUKLAB团队在SemEval-2025任务4上提交的作品，旨在从大语言模型中选择性地删除敏感知识，解决过度遗忘和不足遗忘的问题。我们提出的去除系统基于模型融合方法（特别是TIES融合），将两个专门模型整合为一个更均衡的未学习模型。经过我们的方法在26支团队中获得了可竞争的成绩，在任务聚合及整体聚合上分别得分0.944和0.487。论文中，我们进行了局部实验并全面分析了去除过程，包括性能轨迹、损失动态及权重分析，同时进行了多项补充实验，以了解方法的有效性。此外，我们分析了方法与评估指标的不足，强调MIA分数和基于ROUGE的指标不足以全面评估去除效果，最后呼吁未来研究需要更全面的评估方法和对去除目标的重新思考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.21088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 22:03:25 GMT</pubDate>
</item>
<item>
<title>统一多模态离散扩散模型UniDisc的探索与应用</title>
<link>https://arxiv.org/abs/2503.20853</link>
<guid>https://arxiv.org/abs/2503.20853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出UniDisc模型，提升多模态生成质量与可控性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种统一的多模态离散扩散模型（UniDisc），该模型在文本和图像领域内实现了更优的生成能力。与传统自回归（AR）模型相比，UniDisc模型利用离散扩散模型的优势，改善了生成样本的质量与多样性的控制，并能够在文本与图像之间进行联合填充。UniDisc在多个下游任务中展现出强大的能力，并通过规模分析表明其在性能、推理时间计算、可控性、可编辑性及生成质量与推理时间之间的灵活权衡方面均优于多模态自回归模型。代码及更多可视化结果可在项目网站获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>Feature4X：扩展2D视觉模型至4D领域的通用框架</title>
<link>https://arxiv.org/abs/2503.20776</link>
<guid>https://arxiv.org/abs/2503.20776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feature4X框架实现了从2D到4D的功能扩展，推动动态场景交互的发展。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Feature4X的通用框架，旨在将2D视觉基础模型的功能扩大到4D领域，主要通过单目视频输入完成，这种输入形式普遍存在于用户生成的内容中。Feature4X的核心在于其动态优化策略，能够将多个模型功能统一为单一表现。该方法首次使用高斯溅射在4D特征域中提取和提升视频基础模型的特征。实验显示，Feature4X在新颖的视角上实现了随意分割、几何与外观场景编辑，以及跨时间步的自由形式视觉问答，这些都由大语言模型在反馈循环中提供支持。这些技术进步为能够在动态4D场景中进行沉浸式交互的智能应用打下了基础，拓展了智能AI应用的范围。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:56:16 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的故障诱发输入提取研究</title>
<link>https://arxiv.org/abs/2503.20578</link>
<guid>https://arxiv.org/abs/2503.20578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了如何利用大型语言模型从bug报告中提取故障诱发输入。</p><br /><br /><p><strong>摘要：</strong> 故障诱发输入在软件缺陷诊断与分析中至关重要。开发者通常从bug报告中提取这些输入以便于调试。由于bug报告是用自然语言编写的，之前的研究利用了各种自然语言处理技术进行自动化输入提取。随着大型语言模型的出现，本文探讨了生成性大型语言模型在提取故障诱发输入方面的有效性。我们提出了LLPut技术，系统评估了三种开源生成性大型语言模型——LLaMA、Qwen和Qwen-Coder——在从206个bug报告中提取相关输入的表现。实验结果揭示了生成性大型语言模型在自动化缺陷诊断中的能力与局限性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 10:25:01 GMT</pubDate>
</item>
<item>
<title>利用合成视频提升视频生成模型的物理真实性</title>
<link>https://arxiv.org/abs/2503.20822</link>
<guid>https://arxiv.org/abs/2503.20822</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨通过合成视频提升视频生成模型的物理真实性。</p><br /><br /><p><strong>摘要：</strong> 在本研究中，我们探讨了通过利用来自计算机图形管道的合成视频来增强视频生成模型的物理真实性。这些渲染视频遵循现实世界的物理规律，如保持三维一致性，并作为一个宝贵的资源，有潜力改善视频生成模型。为此，我们提出了一种方案，策划和整合合成数据，并引入一种方法将其物理真实性转移到模型中，显著减少不必要的伪影。通过在三个强调物理一致性的典型任务上的实验，我们证明了这一方法在增强物理真实性方面的有效性。尽管我们的模型仍缺乏对物理的深刻理解，但本研究提供了合成视频提高视频合成物理真实性的初步实证证明。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20822" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 20:45:07 GMT</pubDate>
</item>
<item>
<title>RecTable: 高效生成高质量表格数据的新模型</title>
<link>https://arxiv.org/abs/2503.20731</link>
<guid>https://arxiv.org/abs/2503.20731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RecTable模型在高质量表格数据生成中表现优异，训练时间更短。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RecTable模型，该模型基于修正流建模，旨在生成高质量的表格数据。相较于传统的GAN和VAE模型，RecTable展现出更优越的性能，同时显著减少了训练时间。RecTable采用简单的架构，使用堆叠的门控线性单元块，并采用混合噪声分布和对数正态时间步分布作为训练策略。实验结果表明，RecTable在多项性能指标上与多种先进扩散模型和基于分数的模型具有竞争力。相关代码已上传至GitHub供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:12:20 GMT</pubDate>
</item>
<item>
<title>Gemma 3：多模态轻量级模型的升级版</title>
<link>https://arxiv.org/abs/2503.19786</link>
<guid>https://arxiv.org/abs/2503.19786</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemma 3引入了视觉理解能力，支持多语言和更长上下文。</p><br /><br /><p><strong>摘要：</strong> Gemma 3是Gemma系列的多模态轻量级开源模型，参数范围从1亿到270亿。本版本新增视觉理解能力，支持更广泛的语言覆盖以及至少128K的上下文长度。模型架构进行了调整，以减少在使用长上下文时KV缓存的内存消耗。通过提高本地注意力层与全局注意力层的比例，并缩短本地注意力的跨度，实现了这一目标。Gemma 3模型经过蒸馏训练，其预训练和指令微调版本的性能均优于Gemma 2。特别是，我们的新型后期训练方案显著提升了数学、对话、遵循指令和多语言能力，使得Gemma3-4B-IT与Gemma2-27B-IT竞争，且Gemma3-27B-IT在各项基准测试中可与Gemini-1.5-Pro相媲美。我们将所有模型向社区发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19786" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:52:34 GMT</pubDate>
</item>
<item>
<title>模型合并在长期到短期推理中的应用研究</title>
<link>https://arxiv.org/abs/2503.20641</link>
<guid>https://arxiv.org/abs/2503.20641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明模型合并可提高推理效率并减少冗余步骤。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大型语言模型（LLMs）中，从系统1到系统2推理的转变及其带来的复杂任务处理进展。尽管这种转变提高了推理深度，但带来了效率损失，导致模型常常过度思考，冗余推理步骤未能显著提升结果质量。长短推理（L2S）作为解决这一挑战的有效方案，通过模型合并结合了系统1的快速思考能力与系统2的系统性推理。我们通过任务向量、SVD和激活信息的不同合并方法，进行了全面的实验研究，结果显示模型合并能在保持或改善基线性能的同时，平均减少响应长度达55%。研究还发现模型规模与合并效果之间存在明显相关性，并探讨了合并模型的自我批评、自我修正能力以及针对任务复杂度的自适应响应长度。这表明模型合并是一种高效且有效的长短推理方法，能在保持系统2推理的稳健性的同时，解决过度思考的问题。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 11:34:37 GMT</pubDate>
</item>
<item>
<title>基于轨迹平衡与异步的强化学习系统TBA</title>
<link>https://arxiv.org/abs/2503.18929</link>
<guid>https://arxiv.org/abs/2503.18929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TBA是一种高效的RL系统，利用回放缓冲区提升大语言模型的后训练效率。</p><br /><br /><p><strong>摘要：</strong> 强化学习（RL）是大语言模型（LLM）后训练中的关键组成部分，但现有在线算法与经验回放缓冲区不兼容，影响了探索效率。我们提出了一种通过“轨迹平衡与异步(TBA)”的方法，高效利用回放缓冲区的优势，构建了一种可大规模扩展的LLM RL系统。与现有方法相比，TBA将更多计算资源用于搜索，持续生成离线数据供中心回放缓冲区使用。训练节点基于奖励或新颖性从缓冲区采样数据，通过“轨迹平衡”这一多样性寻求目标更新策略。TBA主要有三个优势：训练与搜索解耦，训练效率提升4倍以上；通过大规模离线采样提高多样性；以及支持稀疏奖励环境的可扩展搜索。在数学推理、偏好调整和自动化对抗测试等后训练任务中，TBA在速度和性能上优于传统基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:51:39 GMT</pubDate>
</item>
<item>
<title>UniHDSA: 一种统一的文档层次结构分析方法</title>
<link>https://arxiv.org/abs/2503.15893</link>
<guid>https://arxiv.org/abs/2503.15893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种针对文档层次结构分析的统一关系预测方法UniHDSA。</p><br /><br /><p><strong>摘要：</strong> 文档结构分析对于理解文档的物理布局和逻辑结构至关重要，支持信息检索、文档摘要和知识提取等应用。层次文档结构分析（HDSA）旨在恢复使用分层架构创建的文档的层次结构。本研究提出了一种统一关系预测的方法UniHDSA，将不同的HDSA子任务视作关系预测问题，并将预测标签整合为一个统一的标签空间，从而使单一的关系预测模块能够同时处理多个任务，无论是在页面级还是文档级结构分析中。为了验证UniHDSA的有效性，我们基于Transformer架构开发了一种多模态端到端系统。丰富的实验结果表明，该方法在层次文档结构分析基准Comp-HRDoc上达到了先进的性能，并在大规模文档布局分析数据集DocLayNet上获得了竞争性结果，证明了本方法在所有子任务中的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:44:47 GMT</pubDate>
</item>
<item>
<title>RONA：增强多模态大语言模型图像标注多样性的策略</title>
<link>https://arxiv.org/abs/2503.10997</link>
<guid>https://arxiv.org/abs/2503.10997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RONA策略，以提升多模态模型生成图像标注的多样性。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种名为RONA的新型提示策略，旨在提高多模态大语言模型（MLLM）生成图像标注的多样性和真实性。传统的图像标注往往侧重于视觉描述，而RONA通过利用连贯关系提供了一种新的变化轴，使得生成的标注在语用上更加多样化。实验证明，与多模态语言模型基线相比，RONA在多个领域的图像标注表现出更高的整体多样性和真实对齐度。这一研究为改善图像描述的表达提供了新的视角，并为实际应用提供了代码支持，便于进一步研究和开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 21:45:38 GMT</pubDate>
</item>
<item>
<title>PathoHR：提高乳腺癌生存预测的计算病理新方法</title>
<link>https://arxiv.org/abs/2503.17970</link>
<guid>https://arxiv.org/abs/2503.17970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PathoHR提出了一种新的方法以提高乳腺癌生存预测的准确性。</p><br /><br /><p><strong>摘要：</strong> 乳腺癌的生存预测在计算病理中面临挑战，主要由于肿瘤异质性及病理图像中不同区域的特征差异。本文提出了PathoHR，一个新颖的管道，通过增强病理图像的高分辨率来改善特征学习。该方法包括：使用高分辨率的Vision Transformer（ViT）提升病理切片的细节特征提取，对比多种相似度度量以优化特征表示学习，并展示了经过处理的小切片能与原始大切片相媲美甚至更优的预测准确性，同时大幅降低计算负担。实验结果证明，PathoHR为整合增强图像分辨率和优化特征学习提供了一种可行的方向，助力计算病理的进步，并对乳腺癌生存预测的有效性产生积极影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17970" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:37:24 GMT</pubDate>
</item>
<item>
<title>Vision-Language奖励模型的评估与进展</title>
<link>https://arxiv.org/abs/2503.20271</link>
<guid>https://arxiv.org/abs/2503.20271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探索视觉语言领域中奖励模型的评估，提出了ViLBench基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了过程监督奖励模型（PRM）在视觉语言领域的应用与挑战，尤其是在评估方面的不足。通过对当前视觉大型语言模型（VLLM）进行基准测试，研究发现输出奖励模型（ORM）和过程奖励模型（PRM）在多个视觉语言基准中的表现并不稳定，且更优的VLLM并不一定带来更好的奖励表现。为提升评估能力，本文引入了ViLBench，一个要求强烈过程奖励信号的视觉语言基准，显示了当前VLLM在此基准下挑战性极高，OpenAI的GPT-4o仅达到27.3%的准确率。此外，通过使用增强的树搜索算法收集73.6K视觉语言过程奖励数据，研究表明我们的3B模型在ViLBench上相比于标准的Chain-of-Thought（CoT）平均提高了3.3%。本文的研究成果及实现代码已开放，助力未来的奖励模型研究与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 02:38:31 GMT</pubDate>
</item>
<item>
<title>利用运动模糊进行稳健的相机运动估计</title>
<link>https://arxiv.org/abs/2503.17358</link>
<guid>https://arxiv.org/abs/2503.17358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过运动模糊进行相机运动估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，将运动模糊视为运动估计的有效信息，而非不必要的伪影。该方法通过单张运动模糊图像直接预测密集运动流场和单目深度图，并在小运动假设下，通过解决线性最小二乘问题来恢复瞬时相机速度。这一方法有效捕捉快速相机运动，产生类似于IMU的测量。为训练模型，构建了一个大规模数据集，并通过全微分管道在真实数据上进行端到端训练。大量的实地评估表明，该方法在角速度和位移估计方面均达到了最先进的水平，表现优于现有方法如MASt3R和COLMAP。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:58:56 GMT</pubDate>
</item>
<item>
<title>Qwen2.5-Omni：多模态流处理模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.20215</link>
<guid>https://arxiv.org/abs/2503.20215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen2.5-Omni是一个先进的多模态流处理模型，支持文本、图像、音频和视频。</p><br /><br /><p><strong>摘要：</strong> Qwen2.5-Omni是一个端到端的多模态模型，旨在处理文本、图像、音频和视频等多种输入，同时以流式方式生成文本和自然语音响应。为实现多模态信息的流式处理，音频和视频编码器采用块级处理方法，并通过交错组织音视频序列来同步时间戳，同时提出新的位置嵌入方法TMRoPE（时间对齐的多模态RoPE）。在生成文本和语音的同时，避免两者间的干扰，Qwen2.5-Omni采用了Thinker-Talker架构，其中Thinker作为大型语言模型负责文本生成，而Talker是一个双轨自回归模型，直接利用Thinker的隐表示生成音频标记。此模型在多模态基准测试中尤为突出，表现出与其他模型相媲美的性能，尤其在语音跟随指令和语音生成方面展现出卓越的自然度与稳健性。该模型不仅在Qwen2.5-VL的基础上具备竞争力，还在Omni-Bench等多模态基准上达到了最先进的水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:17:55 GMT</pubDate>
</item>
<item>
<title>评估多模态大语言模型的空间推理能力：LEGO-Puzzles基准测试</title>
<link>https://arxiv.org/abs/2503.19990</link>
<guid>https://arxiv.org/abs/2503.19990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过LEGO-Puzzles基准测试评估多模态大语言模型的空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的基准测试LEGO-Puzzles，用于评估多模态大语言模型（MLLMs）在空间理解和顺序推理方面的能力。该基准包含1100个视觉问答样本，涵盖从基本空间理解到复杂多步推理的11个任务。评估结果显示，当前最先进的MLLMs在空间推理方面存在显著局限性，尽管性能最强的模型仅能回答约一半的测试案例，而人类参与者准确率超过90%。此外，针对生成LEGO图像的测试中，仅有Gemini-2.0-Flash和GPT-4o表现出有限的指令跟随能力，其余模型要么重复输入图像，要么产生完全不相关的输出。总体来看，LEGO-Puzzles揭示了现有MLLMs在空间理解和顺序推理方面的关键缺陷，强调了进一步提升多模态空间推理能力的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 14:21:07 GMT</pubDate>
</item>
<item>
<title>MCTS-RAG：提升小型语言模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2503.20757</link>
<guid>https://arxiv.org/abs/2503.20757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MCTS-RAG结合检索与推理，提高小型语言模型的知识任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍MCTS-RAG，一种新方法，通过检索增强生成（RAG）提供相关背景，结合蒙特卡洛树搜索（MCTS）优化推理路径，提升小型语言模型在知识密集型任务上的推理能力。MCTS-RAG通过动态集成检索与推理，采用迭代决策过程，克服了标准RAG方法与传统MCTS推理的局限性，实现更加结构化的推理和适应性检索。该方法显著提高了决策质量，减少了幻觉现象，并增强了事实准确性和响应一致性。实验结果显示，在多个推理与知识密集型数据集上，MCTS-RAG使得小型语言模型的表现达到了与前沿大型语言模型（如GPT-4o）相媲美的新标准，展示了其在推理领域的重要进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:46:08 GMT</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 13:45:29 GMT</pubDate>
</item>
<item>
<title>突破性商业内容生成：以超密布局为基础的文本到图像生成模型</title>
<link>https://arxiv.org/abs/2503.20672</link>
<guid>https://arxiv.org/abs/2503.20672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了基于文章级提示生成高质量商业内容的挑战与解决方案。</p><br /><br /><p><strong>摘要：</strong> 近期，最新的文本到图像生成模型如Flux和Ideogram 2.0在句子级别的视觉文本渲染上取得显著进展。本文关注于更具挑战性的文章级别视觉文本渲染，提出一种新任务，以用户提供的文章级描述性提示和超密布局为基础生成高质量商业内容，包括信息图和幻灯片。文章面临的主要挑战包括更长的上下文长度和高质量商业内容数据的稀缺。与之前的研究相比，本文提出了构建高质量商业内容数据集Infographics-650K，并采用布局引导的跨注意力机制，以适应超密布局的需求。通过在BizEval提示集上的实验，显示了本系统相比之前的SOTA系统（如Flux和SD3）的强大结果，结合消融实验验证了各个组件的有效性。我们希望构建的Infographics-650K和BizEval能推动商业内容生成领域的进一步发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 12:04:57 GMT</pubDate>
</item>
<item>
<title>Wan：一套开源视频基础模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.20314</link>
<guid>https://arxiv.org/abs/2503.20314</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Wan是一个开源的视频基础模型套件，推动视频生成技术的发展。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Wan，这是一个全面的开源视频基础模型套件，旨在推动视频生成的边界。Wan基于主流的扩散变换器范式，通过创新的VAE、可扩展的预训练策略、大规模数据整理和自动评估指标，显著提升了生成能力。Wan拥有14B参数模型和1.3B参数模型，分别适用于效率和效果，涵盖了多个下游应用，包括图像到视频、指导性视频编辑和个人视频生成。Wan的1.3B模型在资源效率方面表现卓越，仅需8.19 GB VRAM，兼容多种消费级GPU。此外，Wan所有代码和模型均为开源，旨在推动视频生成社区的发展，拓宽行业中的创作可能性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20314" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 04:25:43 GMT</pubDate>
</item>
<item>
<title>优化条件扩散模型的无条件噪声预测</title>
<link>https://arxiv.org/abs/2503.20240</link>
<guid>https://arxiv.org/abs/2503.20240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出改进条件扩散模型的方法，通过替换无条件噪声预测提升生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种优化条件扩散模型（CFG）的新方法，主要针对无条件噪声的预测。传统上，CFG通过单一网络同时学习条件和无条件的噪声预测，但联结训练导致无条件噪声预测的质量不佳，从而影响条件生成的质量。文章的灵感源于大多数CFG条件模型通过对基础模型进行微调以提高无条件生成的性能。我们提出，通过用基础模型预测的无条件噪声替换CFG中的无条件噪声，能显著改善条件生成的效果。此外，我们的研究表明，使用与微调模型不同的扩散模型也能有效替代无条件噪声。经过对多个CFG条件模型（包括Zero-1-to-3、Versatile Diffusion、DiT、DynamiCrafter和InstructPix2Pix）的实验验证，我们的结果支持了这一方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 01:11:38 GMT</pubDate>
</item>
<item>
<title>DINeMo：无3D注释的神经网格模型与伪对应生成</title>
<link>https://arxiv.org/abs/2503.20220</link>
<guid>https://arxiv.org/abs/2503.20220</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DINeMo是一种新型无3D注释的神经网格模型，用于3D姿态估计。</p><br /><br /><p><strong>摘要：</strong> 本文提出了DINeMo，一个训练时不依赖3D注释的神经网格模型，利用大型视觉基础模型获取的伪对应进行学习。通过双向伪对应生成方法，DINeMo结合局部外观特征和全局上下文信息，展现出在汽车数据集上的优越表现，超越了以往的零样本和少样本3D姿态估计，缩小了与完全监督方法之间的差距达67.3%。此外，DINeMo在训练时有效利用更多未标记图像，展现出相较于依赖3D注释的监督学习方法的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20220" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 00:23:53 GMT</pubDate>
</item>
<item>
<title>开放深度搜索（ODS）：提升开源搜索AI性能的新框架</title>
<link>https://arxiv.org/abs/2503.20201</link>
<guid>https://arxiv.org/abs/2503.20201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ODS通过结合搜索工具与推理代理，提升开源LLM的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了开放深度搜索（ODS），旨在缩小专有搜索AI解决方案与开源对手之间的差距。ODS的创新点在于通过推理代理增强开源大规模语言模型（LLM）的推理能力，帮助有效利用网页搜索工具回答查询。ODS由用户选择的基础LLM及两个组件组成：开放搜索工具和开放推理代理。开放推理代理解析任务并执行一系列动作，包括调用开放搜索工具。这一新型搜索工具在准确性上优于其专有竞争对手。与强大的开源推理LLM（如DeepSeek-R1）结合，ODS在SimpleQA和FRAMES两个基准测试中表现接近或超越现有的最先进基线。在FRAMES基准测试中，ODS的准确率比最新发布的GPT-4o搜索预览提升了9.7%。ODS为任何LLM无缝增强搜索与推理能力，从而达到前沿性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:51:32 GMT</pubDate>
</item>
<item>
<title>Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models</title>
<link>https://arxiv.org/abs/2503.20198</link>
<guid>https://arxiv.org/abs/2503.20198</guid>
<content:encoded><![CDATA[
Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \ModelName~significantly outperforms SD3.5 Large~sd3 and GPT4o~gpt4o with DALL-E 3~dalle3 in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating.
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 23:44:25 GMT</pubDate>
</item>
<item>
<title>Gemini Robotics：革命性的机器人视觉-语言-动作模型</title>
<link>https://arxiv.org/abs/2503.20020</link>
<guid>https://arxiv.org/abs/2503.20020</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Robotics是一个新型机器人控制模型，具备强大的操控和适应能力。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Gemini Robotics，一个基于Gemini 2.0构建的先进视觉-语言-动作（VLA）通用模型，旨在直接控制机器人，执行复杂的操作任务。该模型能平滑反应，并能够适应不同类型和位置的物体，处理未知环境，并遵循多样的开放词汇指令。通过额外的微调，Gemini Robotics能够专门化为新的能力，如解决长时间的高灵巧任务、从少于100次的演示中学习新任务，并适应全新的机器人形态。同时，Gemini Robotics-ER（具身推理）扩展了Gemini在物理世界中的多模态推理能力，具有增强的空间和时间理解，支持对象检测、轨迹预测、抓取预测等机器人相关能力。我们探讨了这一新类基础模型在机器人应用中的潜力和重要的安全考量，标志着朝着开发通用机器人迈出了重要一步。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20020" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 15:02:56 GMT</pubDate>
</item>
<item>
<title>无监督视频中运动估计的新方法</title>
<link>https://arxiv.org/abs/2503.19953</link>
<guid>https://arxiv.org/abs/2503.19953</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Opt-CWM，利用无监督学习实现高效的运动估计。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新颖的自监督技术Opt-CWM，用于从预训练的下一帧预测模型中进行光流和遮挡估计。现有方法主要依赖于合成数据或特定场景的启发式调优，导致它们在真实世界中的表现有限。Opt-CWM通过优化反事实探针，从基础视频模型中提取运动信息，避免了固定启发式的需求，并允许在不受限制的视频输入上训练。研究结果表明，Opt-CWM在真实视频运动估计中展示了最先进的性能，同时无需标记数据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19953" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:52 GMT</pubDate>
</item>
<item>
<title>利用Attention-IoU度量揭示计算机视觉模型中的偏见</title>
<link>https://arxiv.org/abs/2503.19846</link>
<guid>https://arxiv.org/abs/2503.19846</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Attention-IoU度量，用于揭示计算机视觉模型内部的偏见。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨计算机视觉模型在多种数据集和任务中表现出的偏见，并介绍了Attention-IoU（Attention Intersection over Union）度量。该度量通过使用注意力图揭示模型内部表示中的偏见，帮助识别可能导致偏见的图像特征。首先，在合成的Waterbirds数据集上验证了该度量的有效性，结果显示Attention-IoU可以准确衡量模型偏见。接着，在CelebA数据集中进行分析时，发现Attention-IoU揭示了超过准确率差异的关联性。通过检查受保护属性“男性”的个别特征，研究了CelebA中偏见的不同表现方式。最后，通过对训练集进行子采样以改变属性相关性，展示了Attention-IoU能够揭示数据集标签中不存在的潜在混淆变量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19846" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:11:39 GMT</pubDate>
</item>
<item>
<title>LogQuant：高效的2位量化技术提升大语言模型推理性能</title>
<link>https://arxiv.org/abs/2503.19950</link>
<guid>https://arxiv.org/abs/2503.19950</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LogQuant采用新型量化技术，实现更高效的KV缓存压缩与性能提升。</p><br /><br /><p><strong>摘要：</strong> LogQuant是一种创新的2位量化技术，专为大语言模型（LLM）推理中的KV缓存设计，显著节省内存并保持卓越性能。与以往方法假设后续tokens更重要或基于早期注意力模式预测重要tokens不同，LogQuant通过应用基于对数的过滤机制，选择性地压缩整个上下文中的KV缓存，使得性能在相同或更小的内存占用下得到改善。基准测试显示，LogQuant在吞吐量上提升25%，批大小增加60%，同时保持内存消耗不变。在处理数学和代码补全等复杂任务时，LogQuant的准确性提升达40%至200%。该技术能够与大多数流行的推理框架无缝集成，相关实现可在GitHub上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19950" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 12:24:45 GMT</pubDate>
</item>
<item>
<title>Dita：一种可扩展的多模态扩散框架用于机器人行动决策</title>
<link>https://arxiv.org/abs/2503.19757</link>
<guid>https://arxiv.org/abs/2503.19757</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dita框架通过多模态扩散过程优化机器人行动决策以适应环境变化。</p><br /><br /><p><strong>摘要：</strong> Dita是一种可扩展的框架，通过Transformer架构直接对连续行动序列进行去噪，克服了以往依赖紧凑动作头的限制。与先前的方法不同，Dita通过上下文条件实现更精细的去噪效果，明确建模动作变动和环境细节。该框架能够在多种相机视角、观察场景、任务及行动空间中集成跨体现的数据集，从而增强其对不同变异的鲁棒性并提升长期任务执行的成功率。在广泛基准测试中的评估显示，Dita在模拟中实现了最先进或可比的性能，并且能够通过10-shot微调在真实世界中成功适应环境变化及复杂任务，仅使用第三人称相机输入。该架构为通用机器人策略学习建立了一个轻量级且开放源代码的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19757" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:19:56 GMT</pubDate>
</item>
<item>
<title>GenHancer：提升CLIP表征能力的生成模型探索</title>
<link>https://arxiv.org/abs/2503.19480</link>
<guid>https://arxiv.org/abs/2503.19480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出GenHancer模型，显著提升CLIP的视觉表征能力。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型与判别模型间的协同作用备受关注，研究发现视觉完美的生成并不总是最优选择。本研究提出GenHancer模型，通过有效提取生成模型的细粒度知识并减轻无关信息，提升CLIP的表示能力。我们深入探讨了三个关键因素：1) 条件机制：使用全球视觉tokens作为条件比局部tokens更有效；2) 去噪配置：采用两阶段训练方法优先学习有用视觉知识，而轻量化去噪器显著提高性能；3) 生成范式：研究连续和离散去噪器的效果，验证了方法的多样性。GenHancer在MMVP-VLM基准上表现优异，相较于OpenAICLIP提高了6.0%。所有模型与代码已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19480" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:15:34 GMT</pubDate>
</item>
<item>
<title>AccVideo: 高效视频扩散模型加速方法</title>
<link>https://arxiv.org/abs/2503.19462</link>
<guid>https://arxiv.org/abs/2503.19462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出AccVideo，加速视频扩散模型生成，提高效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本文分析了现有扩散蒸馏方法的挑战，并提出了一种名为AccVideo的新方法，旨在通过合成数据集减少视频扩散模型的推理步骤，提高生成效率。我们利用预训练的视频扩散模型生成多个有效的去噪轨迹作为合成数据集，从而消除在蒸馏过程中无用的数据点。通过设计基于轨迹的少步引导方法，我们从去噪轨迹中提取关键数据点，学习噪声到视频的映射，从而在更少的步骤内实现视频生成。此外，我们引入对抗训练策略，使学生模型的输出分布与合成数据集的分布对齐，从而增强视频质量。实验结果表明，与教师模型相比，我们的方法在生成速度上提高了8.5倍，同时保持了相似的性能，并且生成了更高质量和分辨率的视频，具体表现为5秒、720x1280、24fps。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 04:52:07 GMT</pubDate>
</item>
<item>
<title>Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs</title>
<link>https://arxiv.org/abs/2503.16870</link>
<guid>https://arxiv.org/abs/2503.16870</guid>
<content:encoded><![CDATA[
Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (&lt;10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:58:18 GMT</pubDate>
</item>
<item>
<title>高效的微调转移策略：提升预训练模型的性能</title>
<link>https://arxiv.org/abs/2503.20110</link>
<guid>https://arxiv.org/abs/2503.20110</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨微调更新在新模型版本之间的转移，提高了模型性能与效率。</p><br /><br /><p><strong>摘要：</strong> 现代大型语言模型在高效更新方面面临挑战，尤其是在每次发布新预训练模型时需要重复昂贵的对齐过程。本文探讨了在模型版本之间转移微调更新的方法，通过从源模型版本导出差异向量并将其应用于不同目标版本的基础模型上。实验证明，转移差异向量可以显著提升目标模型的表现，例如，重用Llama 3.0 8B的微调更新，使得Llama 3.1 8B在GPQA上实现了10.7%的绝对准确率提升。此外，在多语言模型开发中，本文的方法在不重新训练的情况下，也能显著提高特定语言任务的性能。本研究表明，相互连接的模型在参数空间中微调转移最为有效，同时为后续微调提供了更强大的起始点。最后，我们提出了一种迭代的回收-再微调方法，旨在提高模型的开发效率与效能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.20110" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 19:24:43 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型在复杂行为识别中的应用与提升</title>
<link>https://arxiv.org/abs/2503.18712</link>
<guid>https://arxiv.org/abs/2503.18712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨多模态大语言模型在行为识别中的应用和改进。</p><br /><br /><p><strong>摘要：</strong> 本研究着重评估并改进多模态大语言模型（MLLMs）在行为识别任务中的表现。我们将EPIC-KITCHENS-100数据集重构为视频多重问答（EPIC-KITCHENS-100-MQA）形式，发现当面对困难的错误答案作为干扰项时，现有的MLLMs在正确行为识别上存在挑战。为此，我们提出了一系列方法，显著提高了MLLMs的行为识别能力，不仅在EPIC-KITCHENS-100验证集上达到了最新的最佳效果，还在EPIC-KITCHENS-100-MQA上将准确率超越GPT-4o 21个百分点。此外，我们在EgoSchema、PerceptionTest、LongVideoBench、VideoMME和MVBench等其他行为相关视频基准测试中也取得了显著进展，表明MLLMs为复杂行为任务提供了有前景的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 10:24:17 GMT</pubDate>
</item>
<item>
<title>Any6D：无模型框架实现6D物体姿态估计</title>
<link>https://arxiv.org/abs/2503.18673</link>
<guid>https://arxiv.org/abs/2503.18673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Any6D 是一种通过单张RGB-D图像实现6D物体姿态估计的无模型框架。</p><br /><br /><p><strong>摘要：</strong> Any6D是一个无模型的6D物体姿态估计框架，只需一张RGB-D锚点图像即可估计未知物体在新场景中的6D姿态和尺寸。与依赖纹理3D模型或多个视点的现有方法不同，Any6D通过联合物体对齐过程来增强2D-3D对齐和度量尺度估计，从而提升姿态准确性。该方法结合了渲染与比较策略，用于生成和细化姿态假设，使其在遮挡、视角不重叠、不同光照条件和环境变异等复杂情境下表现出色。在REAL275、Toyota-Light、HO3D、YCB-INEOAT和LM-O五个具有挑战性的数据库上进行评估，结果表明其在新物体姿态估计方面显著超越了现有的最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>高质量360度人头视图生成的新方法</title>
<link>https://arxiv.org/abs/2503.15667</link>
<guid>https://arxiv.org/abs/2503.15667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出一种高质量的360度人头视图生成方法，具备一致性和精细细节。</p><br /><br /><p><strong>摘要：</strong> 为了实现可访问的沉浸式远程呈现和个性化内容创作，生成高质量的360度人头视图至关重要。目前的最先进方法在生成真实人头方面存在局限，尤其在风格无关的头部合成Diffusion方法中仅能生成正面视图，且常出现视角一致性问题。我们提出了一种新方法，能够生成完全一致的360度人头视图，适用于人类、风格化和拟人化形状，包含眼镜、帽子等配饰。该方法基于DiffPortrait3D框架，结合自定义ControlNet以生成后脑部细节，并采用双重外观模块确保前后一致性。通过对连续视角序列进行训练并整合背面参考图像，我们的方法实现了强健、局部连续的视图合成，能够生成高质量的神经辐射场(NeRFs)以用于实时自由视点渲染，在对象合成及挑战性输入肖像的360度头部生成中超过了现有的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 15:47:04 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的3D场景生成研究</title>
<link>https://arxiv.org/abs/2503.04919</link>
<guid>https://arxiv.org/abs/2503.04919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了FirePlace框架，提升3D场景中物体放置的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在物体放置任务中如何有效利用多模态大语言模型（MLLMs），并提出了一个新颖的框架FirePlace。该框架在3D几何推理和几何细节提取方面应用现有的MLLMs，构建和解决低级几何的约束，同时进行合理的物体放置筛选。通过结合几何推理和MLLM的现实世界理解，我们的方法能够提出既符合几何约束又符合高层语义常识的物体放置建议。实验结果表明，该方法在处理复杂几何场景时，相较于之前的研究，能够更有效地进行物体放置。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 14:34:15 GMT</pubDate>
</item>
<item>
<title>提升时空推理能力的视觉语言模型ST-VLM及其基准评测</title>
<link>https://arxiv.org/abs/2503.19355</link>
<guid>https://arxiv.org/abs/2503.19355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ST-VLM是一种增强时空推理能力的视觉语言模型，展示出优秀的性能。</p><br /><br /><p><strong>摘要：</strong> 时空推理在自动驾驶和体育分析等领域中至关重要。尽管视觉语言模型（VLMs）的空间推理能力因大规模数据的引入有所提升，但在分析运动对象的运动学元素（如行驶距离和速度）方面仍显不足。为了解决这个问题，本文构建了一个包含运动学指令调优的数据集和基准，分别命名为STKit和STKit-Bench，包含详细的真实世界视频及3D注释，涵盖对象运动动态。为扩大数据构建范围至没有3D标签的视频，提出了一种利用4D重建生成伪标签的自动化流程。基于此运动学指令调优数据，推出了增强时空推理的视觉语言模型ST-VLM。ST-VLM在STKit-Bench上的表现优异，并且在不同领域和任务中具有出色的泛化能力，超越了其他时空基准的对比模型。通过将学习的时空推理能力与现有能力结合，ST-VLM能够进行复杂的多步推理。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:08:06 GMT</pubDate>
</item>
<item>
<title>OpenCity3D：城市规模环境的语言驱动分析新范式</title>
<link>https://arxiv.org/abs/2503.16776</link>
<guid>https://arxiv.org/abs/2503.16776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出OpenCity3D，扩展了VLMs在城市环境中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新方法OpenCity3D，旨在将视觉语言模型（VLMs）应用于城市规模环境，超越传统的室内和自动驾驶场景分析。通过利用多视角航拍图像的3D重建，OpenCity3D能够执行一系列高层次任务，如人口密度估计、建筑年代分类、房地产价格预测、犯罪率评估和噪音污染评估。研究结果表明，OpenCity3D具有出色的零样本和少样本学习能力，能够适应新的城市分析场景。此研究为语言驱动的城市分析确立了新范式，具有在城市规划、政策制定和环境监测等领域的广泛应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 21:11:21 GMT</pubDate>
</item>
<item>
<title>基于单目相机的无人机深度和语义地图预测</title>
<link>https://arxiv.org/abs/2503.17982</link>
<guid>https://arxiv.org/abs/2503.17982</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于单目相机的无人机深度和语义地图联合预测方法。</p><br /><br /><p><strong>摘要：</strong> 在无人机导航中，理解场景的几何和语义属性至关重要，但实现起来十分具有挑战性。本文利用单目相机在低空非结构化环境中预测深度和语义地图，提出了一种联合深度学习架构，能够快速且准确地完成这两项任务。通过在MidAir和Aeroscapes基准数据集上的验证，证明该架构在性能上优于其他单独或联合架构方法，实时预测速率达到每秒20.2帧，并且内存占用较低。相关的训练与预测代码已在GitHub上公开，以供研究与使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17982" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 04:25:07 GMT</pubDate>
</item>
<item>
<title>PhysTwin: 基于稀疏视频的动态物体物理数字双胞胎框架</title>
<link>https://arxiv.org/abs/2503.17973</link>
<guid>https://arxiv.org/abs/2503.17973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了PhysTwin框架，用于实时创建动态物体的物理数字双胞胎。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysTwin，一个新颖的框架，利用互动下的稀疏视频生成真实且物理上逼真的动态物体的虚拟复制品。该框架主要由两个关键组件构成：一是融合了弹簧-质量模型进行物理仿真的物理信息化表示、用于几何体的生成形状模型以及用于渲染的高斯点云；二是一个基于多阶段优化的逆建模框架，能够从视频重建完整几何体、推断密集物理属性并复制真实外观。PhysTwin通过整合逆物理框架与视觉感知线索，即使在部分、遮挡和有限视角下也能实现高保真重建。该方法支持建模各种可变形物体，包括绳子、毛绒玩具、布料和快递包裹。实验表明，PhysTwin在重建、渲染、未来预测和新颖互动下的仿真方面超越了竞争方法，并进一步展示了其在实时交互仿真和基于模型的机器人运动规划中的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 03:49:19 GMT</pubDate>
</item>
<item>
<title>FullDiT：统一的视频生成基础模型</title>
<link>https://arxiv.org/abs/2503.19907</link>
<guid>https://arxiv.org/abs/2503.19907</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FullDiT通过全注意力机制优化多任务视频生成，达到最佳效果。</p><br /><br /><p><strong>摘要：</strong> 当前的视频生成基础模型主要集中于文本到视频的任务，提供的细粒度内容创建控制有限。虽然适配器化的方法能提供额外控制，但在整合多个条件时面临挑战。为了解决这些问题，本文提出FullDiT——一个统一的视频生成基础模型，通过全注意力机制无缝集成多个条件。FullDiT将多任务条件融合为统一的序列表示，并利用全自注意力的长程学习能力捕捉条件动态，从而减少参数冗余，避免条件冲突，提升可扩展性和生成能力。我们还引入了FullBench用于多任务视频生成评估。实验结果表明，FullDiT在复杂的多任务视频生成中表现出色，展现了全注意力机制的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19907" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>无训练开放词汇语义分割的方法LPOSS+</title>
<link>https://arxiv.org/abs/2503.19777</link>
<guid>https://arxiv.org/abs/2503.19777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于标签传播的无训练语义分割方法，显著提升分割精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无训练的开放词汇语义分割方法LPOSS+，利用视觉语言模型（VLMs）增强初始的每个补丁预测。我们通过标签传播优化预测，结合补丁之间的关系，以改进分割精度。由于VLMs主要针对跨模态对齐而非模态内相似性，我们引入了一种视觉模型（VM），它更好地捕捉这些关系。此外，针对补丁编码器的分辨率限制，我们在像素级别应用标签传播作为精细化步骤，尤其在类别边界附近显著提升精度。LPOSS+以整个图像为基础进行推断，避免了基于窗口的处理，有效捕捉全图的上下文交互，展现出在多种数据集上的最佳性能，成为当前无训练方法中的领先者。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 11:47:13 GMT</pubDate>
</item>
<item>
<title>实时交互视频数据集：评估AI模型的对话能力</title>
<link>https://arxiv.org/abs/2503.19356</link>
<guid>https://arxiv.org/abs/2503.19356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一个新数据集，评估AI模型在实时对话方面的能力。</p><br /><br /><p><strong>摘要：</strong> 随着AI技术的进步，AI模型在描述和回答现实图像问题方面取得了显著进展。本文提出了新的数据集和基准——Qualcomm交互视频数据集（IVD），旨在评估现有模型在实时对话及场景理解中的表现。该数据集采用简单的问答设置，用户可以根据相机和音频输入进行实时提问。研究发现，现有模型在这一任务上的表现远远落后于人类，同时指出了造成性能差距的主要来源。然而，实验表明，对于许多必要的知觉技能，通过对这种数据的微调，可以显著缩小这一差距，从而提升AI的实时交互能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 01:13:12 GMT</pubDate>
</item>
<item>
<title>基于少量图像的个性化3D人类头像重建与动画技术</title>
<link>https://arxiv.org/abs/2503.19207</link>
<guid>https://arxiv.org/abs/2503.19207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，仅需少量图像即可重建个性化3D人类头像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，能够仅通过少量图像重建个性化的3D人类头像并实现逼真的动画。传统方法需要对每个用户进行数小时的优化，而我们通过学习来自1000多名着衣人类的通用先验，实现了即时前馈生成和零-shot泛化。我们不再使用共享的皮肤权重进行绑定，而是联合推断个性化的头像形状、皮肤权重和姿态依赖变形，提高了几何真实性并减少了变形伪影。此外，为了解决姿态变化带来的耦合模糊问题，设计了一种3D标准化处理，生成像素对齐的初始条件，帮助重建细致的几何细节。本方法经过大规模捕捉数据集的端到端训练，具有优秀的重建和动画效果，并能直接处理日常手机拍摄的输入。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 19:20:47 GMT</pubDate>
</item>
<item>
<title>VocAgnoLM：解决教师与学生模型词汇不匹配的问题</title>
<link>https://arxiv.org/abs/2503.19123</link>
<guid>https://arxiv.org/abs/2503.19123</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VocAgnoLM通过词汇无关的教学指导语言建模解决了词汇不匹配问题。</p><br /><br /><p><strong>摘要：</strong> VocAgnoLM是一种新颖的方法，通过两种关键技术解决教师与学生语言模型之间的词汇不匹配问题，从而提高学习效率和效果。这两种技术包括：1）令牌级词汇对齐，能够对齐不同词汇中的令牌序列；2）教师指导损失，利用教师模型的损失来指导学生模型的有效训练。我们在使用不同词汇的7B教师模型对1B学生模型进行语言建模时，验证了该方法的有效性。例如，当使用Qwen2.5-Math-Instruct作为教师模型时，与TinyLlama共享约6%词汇的情况下，VocAgnoLM与简单的持续预训练相比实现了46%的性能提升。此外，VocAgnoLM在使用更强的教师模型时能够持续受益，提供了一个强健的解决方案以应对语言建模中的词汇不匹配问题。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19123" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 16:19:31 GMT</pubDate>
</item>
<item>
<title>WikiAutoGen：一款自动化多模态维基百科式文章生成系统</title>
<link>https://arxiv.org/abs/2503.19065</link>
<guid>https://arxiv.org/abs/2503.19065</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍WikiAutoGen，一个提升文章生成质量的多模态系统。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WikiAutoGen，这是一种新颖的自动化多模态维基百科式文章生成系统，与以往的文本生成方法不同，WikiAutoGen能够检索并整合相关图像，增强生成内容的深度与视觉吸引力。此外，研究还提出了一种多视角自我反思机制，从不同观点对检索内容进行评估，以提升生成文章的可靠性与连贯性。为了评估多模态知识生成，本研究构建了一个名为WikiSeek的基准数据集，该数据集包含配有文本和图像表示的维基百科文章。实验结果表明，WikiAutoGen在WikiSeek基准上比以往方法提升了8%-29%，生成的维基百科式文章在准确性、连贯性和视觉丰富性方面表现更佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19065" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:51:55 GMT</pubDate>
</item>
<item>
<title>xKV：提升长上下文语言模型内存效率的新方法</title>
<link>https://arxiv.org/abs/2503.18893</link>
<guid>https://arxiv.org/abs/2503.18893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">xKV方法通过奇异值分解显著压缩长上下文模型的KV-Cache。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在处理长上下文时面临高内存消耗的问题，尤其是存储键值状态（KV-Cache）。虽然现有研究尝试将多个层的KV-Cache合并，但通常需要昂贵的预训练或不切实际的相似性假设。本文提出了xKV，一种利用奇异值分解（SVD）在分组层的KV-Cache上进行简单后训练的方法，能够将多个层的KV-Cache整合到共享低秩子空间，从而显著减少内存占用。通过在广泛使用的LLMs（如Llama-3.1和Qwen2.5）上进行的大量评估，xKV展示了相比先进的跨层技术高达6.8倍的压缩率，同时提高了2.7%的准确性。此外，xKV还兼容新兴的多头潜在注意力机制，能够在编码任务上实现3倍的压缩而不损失性能。这些结果表明，xKV在解决长上下文LLM推理中的内存瓶颈方面具有强大的能力和灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:37 GMT</pubDate>
</item>
<item>
<title>Towards a Unified Copernicus Foundation Model for Earth Vision</title>
<link>https://arxiv.org/abs/2503.11849</link>
<guid>https://arxiv.org/abs/2503.11849</guid>
<content:encoded><![CDATA[
Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes, datasets and models are available at https://github.com/zhu-xlab/Copernicus-FM.
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 16:16:48 GMT</pubDate>
</item>
<item>
<title>基于YOLOv12与BoT-SORT的多无人机跟踪方法</title>
<link>https://arxiv.org/abs/2503.17237</link>
<guid>https://arxiv.org/abs/2503.17237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于YOLOv12与BoT-SORT的多无人机跟踪方法。</p><br /><br /><p><strong>摘要：</strong> 本论文针对热红外视频中多无人机（UAVs）的检测与跟踪问题，提出了一种新的跟踪框架，构建于YOLOv12与BoT-SORT之上，结合了定制的训练与推理策略。与传统的YOLOv5和DeepSORT流水线不同，我们的方法在没有采用对比度增强或时间信息融合的情况下，仍能展现出竞争力的性能，为多无人机跟踪任务提供了一个强基线。我们根据第四届反无人机挑战赛的指标评估了这一方法，并提供了实现细节、深入的实验分析以及可能的改进讨论。相关代码已发布在GitHub上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 11:40:18 GMT</pubDate>
</item>
<item>
<title>Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation</title>
<link>https://arxiv.org/abs/2503.14905</link>
<guid>https://arxiv.org/abs/2503.14905</guid>
<content:encoded><![CDATA[
With the rapid advancement of Artificial Intelligence Generated Content (AIGC) technologies, synthetic images have become increasingly prevalent in everyday life, posing new challenges for authenticity assessment and detection. Despite the effectiveness of existing methods in evaluating image authenticity and locating forgeries, these approaches often lack human interpretability and do not fully address the growing complexity of synthetic data. To tackle these challenges, we introduce FakeVLM, a specialized large multimodal model designed for both general synthetic image and DeepFake detection tasks. FakeVLM not only excels in distinguishing real from fake images but also provides clear, natural language explanations for image artifacts, enhancing interpretability. Additionally, we present FakeClue, a comprehensive dataset containing over 100,000 images across seven categories, annotated with fine-grained artifact clues in natural language. FakeVLM demonstrates performance comparable to expert models while eliminating the need for additional classifiers, making it a robust solution for synthetic data detection. Extensive evaluations across multiple datasets confirm the superiority of FakeVLM in both authenticity classification and artifact explanation tasks, setting a new benchmark for synthetic image detection. The dataset and code will be released in: https://github.com/opendatalab/FakeVLM.
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 01:14:44 GMT</pubDate>
</item>
<item>
<title>MDocAgent：一种多模态多智能体文档理解框架</title>
<link>https://arxiv.org/abs/2503.13964</link>
<guid>https://arxiv.org/abs/2503.13964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MDocAgent通过多智能体协作提升文档问答系统的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的多模态多智能体框架MDocAgent，旨在增强文档理解中的问答能力。与传统的方法不同，MDocAgent结合了文本和图像信息，通过五个专门的智能体共同进行多模态上下文检索。每个智能体专注于不同的领域：一般智能体、关键智能体、文本智能体、图像智能体和总结智能体。这样的协作模式使系统能够全面综合文档内容，提高了问答的准确性。初步实验显示，MDocAgent在五个基准上实现了平均12.1%的性能提升，展示了其在处理复杂真实世界文档方面的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 02:57:21 GMT</pubDate>
</item>
<item>
<title>CoLLM：增强复杂图像检索的综合框架</title>
<link>https://arxiv.org/abs/2503.19910</link>
<guid>https://arxiv.org/abs/2503.19910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLLM是一种新框架，解决复杂图像检索中的数据稀缺和多模态学习问题。</p><br /><br /><p><strong>摘要：</strong> Composed Image Retrieval (CIR)是一项多模态查询图像检索的复杂任务，传统的训练数据获取困难，导致使用零样本方法和图像-文本对的不足之处。为了解决这些问题，本研究提出了CoLLM框架，通过实时生成图像-文本三元组以实现无人工标注的监督训练，利用大语言模型生成参考图像和修改文本的联合嵌入，促进更深入的多模态融合。此外，引入了包含340万样本的大规模数据集Multi-Text CIR (MTCIR)，并优化了现有CIR基准（CIRR和Fashion-IQ），提高了评估的可靠性。实验结果显示，CoLLM在多个CIR基准和设置中达到了最先进的性能，MTCIR的表现也有所提升，提供了更可靠的评估指标，推动了该领域的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>PS3：革命性的高分辨率视觉预训练方法</title>
<link>https://arxiv.org/abs/2503.19903</link>
<guid>https://arxiv.org/abs/2503.19903</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PS3提出了一种高效的高分辨率视觉预训练方法，显著提升视觉感知能力。</p><br /><br /><p><strong>摘要：</strong> 高分辨率的视觉细节感知对日常任务至关重要，但现有的视觉预训练方法仍然局限于低分辨率。我们提出了PS3，能够将CLIP风格的视觉预训练扩展至4K分辨率而计算成本几乎保持不变。PS3通过局部区域处理和与局部详细描述的对比学习实现高分辨率表示。应用于多模态大语言模型的VILA-HD在高分辨率视觉感知方面显著优于未经过高分辨率视觉预训练的基线模型，如AnyRes和S^2，并且使用的令牌数量减少了最多4.3倍。此外，VILA-HD在多个基准测试中超越了以往的多模态模型，展现出更好的效率。最终，我们提出了适用于4K分辨率的新的图像问答基准4KPro，VILA-HD在该基准上实现了对所有前一代多模态模型的超越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19903" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:58:37 GMT</pubDate>
</item>
<item>
<title>Multi-round Thinking：一种提升大语言模型推理性能的新方法</title>
<link>https://arxiv.org/abs/2503.19855</link>
<guid>https://arxiv.org/abs/2503.19855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出多轮思维方法，通过迭代提升大语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）的发展，推理过程的扩展显著提升了模型性能，但当前模型在处理长文本和强化学习效率上仍存在局限。本文提出了一种简单有效的测试时间扩展方法——多轮思维。该方法通过利用前一轮的答案作为提示，迭代精炼模型推理。经过在多个模型（如QwQ-32B和DeepSeek-R1）上的广泛实验，结果显示在多个基准测试（如AIME 2024、MATH-500等）上均实现性能提升。例如，QwQ-32B在AIME 2024数据集上的准确率从80.3%提升至82.1%，DeepSeek-R1也从79.7%提升至82.0%。多轮思维展示了其作为一种广泛适用且简单的方法，能够稳定地提升模型性能，预示着其在测试时间扩展技术未来发展的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 13:19:38 GMT</pubDate>
</item>
<item>
<title>研究视频模态大规模多模态模型的幻觉问题</title>
<link>https://arxiv.org/abs/2503.19622</link>
<guid>https://arxiv.org/abs/2503.19622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究大规模多模态模型在视频理解中的幻觉问题并提出解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模多模态模型（LMMs）在视频模态下的幻觉问题，这类幻觉使得模型在提供看似正确的回答时，实际上往往不准确。为此，本文提出了一个全面的基准测试工具HAVEN，用于评估LMMs在视频理解任务中的幻觉现象，框架包括幻觉原因、幻觉方面和问题格式，共设计了6000个问题。实验分析了视频持续时间、模型大小和推理能力等7个影响因素对于幻觉的影响。为了减轻幻觉问题，研究还提出了一种视频思维模型，结合监督推理微调（SRFT）和直接偏好优化（TDPO），前者提升推理能力，后者减少推理过程中的幻觉。实验结果表明，该方法在幻觉评估的准确度上提升了7.65%，且偏差分数降低了4.5%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 09:12:17 GMT</pubDate>
</item>
<item>
<title>ReSearch：通过强化学习整合推理与搜索的框架</title>
<link>https://arxiv.org/abs/2503.19470</link>
<guid>https://arxiv.org/abs/2503.19470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReSearch框架通过强化学习将推理与搜索整合，提升LLM的推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在推理能力上展现了卓越的性能，然而将推理与外部搜索过程整合仍然是一个挑战，尤其是对于需要多步检索的复杂问题。为此，我们提出了ReSearch框架，通过强化学习训练LLMs，以无监督的数据进行推理步骤的整合。此方法将搜索操作视为推理链的重要组成部分，依据文本思维指导搜索的时机及方式，并利用搜索结果进一步推动推理过程。我们在Qwen2.5-7B和Qwen2.5-32B模型上进行训练和广泛实验，尽管只使用一个数据集进行训练，我们的模型在多个基准测试中表现出强泛化能力。分析表明，ReSearch在强化学习过程中自然而然地引发了反思和自我修正等高级推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 05:00:58 GMT</pubDate>
</item>
<item>
<title>流模型的推理时刻扩展方法研究</title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种用于流模型的高效推理时刻扩展方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对预训练流模型的推理时刻扩展方法，旨在提高样本质量并更好地符合用户偏好。以往的推理时刻扩展多用于大规模语言模型及扩散模型，但流模型的确定性生成过程使得现有方法难以直接应用。为此，本文提出了三个关键思想：1) 基于随机微分方程(SDE)的生成，支持流模型中的粒子采样；2) 插值转换，扩展搜索空间提高样本多样性；3) Rollover Budget Forcing (RBF)，在不同时间步中自适应分配计算资源以最大化预算效率。实验结果表明，基于方差保持的SDE生成显著提升粒子采样方法在流模型中的推理时刻扩展表现，而结合VP-SDE和RBF的方法在所有推理时刻扩展方案中表现最佳。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19385" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 25 Mar 2025 02:30:45 GMT</pubDate>
</item>
<item>
<title>长时间上下文视频生成的进展与Frame AutoRegressive模型</title>
<link>https://arxiv.org/abs/2503.19325</link>
<guid>https://arxiv.org/abs/2503.19325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍Frame AutoRegressive模型在长时间上下文视频生成中的应用及其优势。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Frame AutoRegressive (FAR) 模型，这是一个强大的视频自回归建模基线，旨在解决长时间上下文视频生成的挑战。FAR 模型通过学习连续帧之间的时间因果依赖，超越了传统的语言模型的表现，尤其在收敛性上优于Token AR和视频扩散变换器。此外，针对视觉冗余等长时间视觉建模中存在的问题，我们提出了FlexRoPE技术，增加灵活的时间衰减机制，以适应更长的视觉上下文。通过长短期上下文建模的方法，FAR能够使用更少的令牌编码长范围信息，并在训练长视频序列时保持可管理的令牌上下文长度。最终结果表明，FAR在短视频和长视频生成上均取得了最先进的性能，为视频自回归建模提供了一个简单而有效的基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.19325" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 23:38:06 GMT</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often undermines their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, which comprises two simple, low-resource, and effective data-driven methods that modify training data by previewing partial answer prefixes. Both methods aim to preserve the model's inherent safety mechanisms by minimizing perturbations to initial token distributions. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs. Code is released at https://github.com/zjunlp/LookAheadTuning.
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 14:11:42 GMT</pubDate>
</item>
<item>
<title>通过CoMP实现的视觉基础模型的多模态预训练</title>
<link>https://arxiv.org/abs/2503.18931</link>
<guid>https://arxiv.org/abs/2503.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了多模态预训练的CoMP方法，显著提升了视觉模型的表现。</p><br /><br /><p><strong>摘要：</strong> 预训练的视觉基础模型（VFMs）在多个应用中展现出强大的视觉表示能力。本文提出了一种多模态预训练方法CoMP，通过持续预训练的方式，使VFMs能够轻松处理不同尺寸的视觉输入，并生成与语言表示更一致的视觉表示。CoMP通过引入连续旋转位置嵌入来支持原生分辨率的连续预训练，并通过语言原型间的对齐损失来协调视觉和文本特征，从而整合多模态表示。经过三阶段训练，VFMs在多模态理解及其他下游任务（如分类和分割）中取得了显著提升。其中，CoMP-SigLIP在ChartQA和DocVQA上分别取得66.7和75.9的分数，同时在ImageNet-1K上保持87.4%的准确率，以及在ADE20K上实现49.5的mIoU，显示出其在冻结块评估下的强大性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:52:47 GMT</pubDate>
</item>
<item>
<title>Frequency Dynamic Convolution: 一种高效的自适应卷积方法</title>
<link>https://arxiv.org/abs/2503.18783</link>
<guid>https://arxiv.org/abs/2503.18783</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FDConv 提供高效的自适应卷积，提升模型性能并降低参数成本。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了频率动态卷积（FDConv），一种通过傅里叶域学习固定参数预算，从而克服动态卷积（DY-Conv）的高相似性频率响应所带来的局限性的方法。FDConv 将参数预算划分为基于频率的组，能够构建频率多样化的权重而不会增加参数成本。此外，文章提出的内核空间调制（KSM）和频率带调制（FBM）进一步增强了适应性，KSM 从空间层面动态调整每个滤波器的频率响应，而FBM则在频率域中将权重分解为不同的频带并根据局部内容进行动态调制。通过在物体检测、分割和分类等任务上进行广泛实验，FDConv 在 ResNet-50模型上表现优异，参数仅增加 3.6M，显著优于需要大幅增加参数预算的先前方法，同时FDConv可以无缝集成到多种架构中，包括ConvNeXt和Swin-Transformer，为现代视觉任务提供灵活高效的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18783" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:32:06 GMT</pubDate>
</item>
<item>
<title>LSRNA：基于潜在空间的图像超分辨率生成框架</title>
<link>https://arxiv.org/abs/2503.18446</link>
<guid>https://arxiv.org/abs/2503.18446</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LSRNA框架，强化潜在空间超分辨率，以生成高分辨率图像。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架LSRNA，用于采用扩散模型生成高于1K的高分辨率图像，通过在潜在空间中直接利用超分辨率来实现。传统扩散模型在生成高分辨率图像时难以超越其训练分辨率，常导致结构扭曲或内容重复。尽管参考基方法可以通过上采样低分辨率参考图像来指导高分辨率生成，但常常遇到潜在空间上采样导致的流形偏离和RGB空间上采样导致的过于平滑的问题。LSRNA通过结合潜在空间超分辨率（LSR）实现流形对齐，并采用区域噪声添加（RNA）来增强高频细节，克服了这些缺陷。实验结果表明，LSRNA在多个分辨率和评测指标上均优于现有的参考基方法，同时强调了潜在空间上采样在保留细节与清晰度方面的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18446" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 04:50:15 GMT</pubDate>
</item>
<item>
<title>基于Gumbel-Softmax的流匹配框架用于高维简约体的序列生成</title>
<link>https://arxiv.org/abs/2503.17361</link>
<guid>https://arxiv.org/abs/2503.17361</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了Gumbel-Softmax流匹配框架，提高高维序列生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的Gumbel-Softmax流和评分匹配的生成框架，旨在解决在高维简约体中进行DNA序列设计的挑战。我们提出了一种依赖于时间的温度的Gumbel-Softmax插值，利用该插值引入Gumbel-Softmax流匹配，通过参数化速度场从平滑的分类分布传输到集中在简约体单一顶点的分布。此外，我们还提出了Gumbel-Softmax评分匹配，该方法学习回归概率密度的梯度。为实现无训练引导，我们设计了直通引导流（STGFlow），这是一种基于分类器的引导方法，能有效利用预训练的分类器在推理时指导速度场朝向简约体的最佳顶点。我们的框架在条件DNA启动子设计、仅序列蛋白生成及稀有疾病治疗的靶向肽设计中实现了最先进的性能，展示了其在可控序列生成方面的强大能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17361" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title>提升视觉语言模型的人本决策能力</title>
<link>https://arxiv.org/abs/2503.16965</link>
<guid>https://arxiv.org/abs/2503.16965</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法提升视觉语言模型在复杂人本决策中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究系统评估了开源的视觉语言模型（VLMs）在多模态人本决策任务中的表现。结果显示，仅接受文本描述的语言模型（LLMs）意外地优于处理实际图像的同规模VLMs，暗示视觉对齐可能对VLM的能力造成抑制。为解决这一挑战，提出了一种新颖的仅基于文本的训练方法，通过合成文本数据来增强VLM的语言组件，并将所学能力转移至多模态推理，从而消除对昂贵的图像-文本配对数据的需求。此外，本研究还表明，VLM可以通过自我提升显著提高性能，利用由其LLM生成的训练数据，而无需依赖更大的教师模型，如GPT-4。我们的发现确立了一种更加高效和可扩展的方法，以增强VLM的人本决策能力，为通过自我提升机制优化VLM开辟了新途径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16965" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:25:23 GMT</pubDate>
</item>
<item>
<title>基于视觉语言模型的3D室内场景生成算法研究</title>
<link>https://arxiv.org/abs/2503.18476</link>
<guid>https://arxiv.org/abs/2503.18476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于视觉语言模型的3D室内场景生成新方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于视觉语言模型（VLM）的3D室内场景生成任务，考虑到了空间和布局常识约束。为了解决该问题，提出了一种新的全局-局部树搜索算法。在全局层面，该方法逐步放置对象，并在每个放置过程中探索多种放置选项，问题空间被表示为树形结构。为减少树的深度，场景结构被分解为房间级、区域级、地面对象级及支持对象级，算法独立生成不同区域的地面对象和放置在不同地面对象上的支持对象。在局部层面，算法将每个对象的放置任务分解为多个步骤，并在问题空间树中进行搜索。结合VLM模型，文章通过将自上而下视图空间离散化为密集网格，并使用丰富的表情符号来标识各个网格，提示VLM生成对象的位置。实验结果表明，所提出的方法在生成更为合理的3D场景方面表现优于现有最先进的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:21:13 GMT</pubDate>
</item>
<item>
<title>Instruct-CLIP：改进图像编辑指令对齐的自监督方法</title>
<link>https://arxiv.org/abs/2503.18406</link>
<guid>https://arxiv.org/abs/2503.18406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Instruct-CLIP以提高指令引导图像编辑的质量和对齐性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Instruct-CLIP，这是一种自监督的方法，旨在改进自动图像编辑中的指令对齐问题。传统的文本到图像（T2I）生成模型存在局限，导致生成的图像对与原始和编辑图像的配对不准确，从而影响模型训练效果。Instruct-CLIP通过学习原始图像与编辑图像之间的语义变化，精炼并改进现有数据集中的指令对齐。此外，Instruct-CLIP还适配了处理噪声潜图像和扩散时间步，以便在潜扩散模型中有效地执行指令和图像变化之间的对齐。该方法还用于校正InstructPix2Pix数据集，生成超过120K的精炼样本，并应用于模型的微调，最终得到的模型可生成更符合给定指令的编辑结果。项目的代码和数据集已上传至指定网址。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 03:25:44 GMT</pubDate>
</item>
<item>
<title>QuartDepth：优化单目深度估计在ASIC上的应用</title>
<link>https://arxiv.org/abs/2503.16709</link>
<guid>https://arxiv.org/abs/2503.16709</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出QuartDepth，通过后训练量化优化MDE模型以适应ASIC。</p><br /><br /><p><strong>摘要：</strong> 单目深度估计在计算机视觉中至关重要，但将其准确模型部署于资源有限的边缘设备（如专用集成电路ASIC）却面临高计算和内存需求的挑战。为了解决这一问题，本文提出了一种名为QuartDepth的方法，采用后训练量化技术，针对ASIC量化MDE模型。具体而言，我们将权重和激活量化到4位精度，从而减少模型大小和计算成本。为减轻性能下降，提出了激活抛光及补偿算法，以及权重重建方法，旨在最小化权重量化误差。此外，我们设计了灵活且可编程的硬件加速器，支持内核融合和定制指令编程，这提升了吞吐量和效率。实验结果表明，我们的框架在ASIC上实现了竞争力的准确性，并支持快速推断与更高的能效，弥合了高性能深度估计与边缘设备实际应用之间的差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16709" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 17:03:10 GMT</pubDate>
</item>
<item>
<title>人类运动去学习：防止合成有害动画的新方法</title>
<link>https://arxiv.org/abs/2503.18674</link>
<guid>https://arxiv.org/abs/2503.18674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出运动去学习技术以减少有害动画合成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了人类运动去学习的任务，旨在防止合成有毒动画，同时保留文本到运动的生成性能。由于有毒运动可由明确的文本提示或安全运动的隐式组合生成，去学习有毒运动面临挑战。我们提出了第一个运动去学习基准，通过从HumanML3D和Motion-X这两个大型文本到运动数据集中筛选有毒运动。我们还通过适配最新的图像去学习技术，建立了基线以处理时空信号。此外，我们提出了一种新颖的运动去学习模型，称为LCR（潜变量替换），该模型无需训练，适用于最先进的文本到运动扩散模型的离散潜变量空间。LCR简单且在定性和定量上均优于基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 09:46:27 GMT</pubDate>
</item>
<item>
<title>多模态推理的发展与挑战综述</title>
<link>https://arxiv.org/abs/2503.18071</link>
<guid>https://arxiv.org/abs/2503.18071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述多模态推理方法及其未来发展方向。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地概述了近年来多模态推理的方法，将其分为语言中心多模态推理和协作多模态推理两个层面。前者涉及视觉感知在语言推理中的辅助作用，而后者则强调在推理过程中动作生成和状态更新，促进模态间的动态互动。我们分析这些方法的技术演进，讨论其面临的挑战，并介绍关键基准任务及评估指标，旨在评估多模态推理的表现。此外，本文还从视觉-语言推理到全模态推理、以及从多模态推理到多模态智能体的两个视角探讨未来研究方向，期望能激励多模态推理领域的进一步发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 09:40:44 GMT</pubDate>
</item>
<item>
<title>Feather-SQL: 一种针对小型语言模型的轻量级NL2SQL框架</title>
<link>https://arxiv.org/abs/2503.17811</link>
<guid>https://arxiv.org/abs/2503.17811</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Feather-SQL是为小型语言模型设计的轻量级NL2SQL框架，提升了执行效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型在自然语言转SQL (NL2SQL) 领域的显著进展，但它们依赖于闭源系统和高计算资源，面临数据隐私和部署挑战。相比之下，小型语言模型在NL2SQL任务中表现不佳，且与现有框架不兼容。为解决这些问题，我们提出Feather-SQL，一个专为小型语言模型量身打造的轻量级框架。Feather-SQL通过schema剪枝和链接、多路径多候选生成来提高SQL的可执行性和准确性。此外，我们引入了“1+1模型协作范式”，将一个强大的通用聊天模型与拥有精细调优的SQL专家模型相结合，既具备强大的分析推理能力，又高效生成高精度的SQL。实验结果表明，Feather-SQL显著提高了小型语言模型在NL2SQL上的表现，未经过微调的模型性能提升约10%，且该范式将小型语言模型的准确率提高至54.76%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17811" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 12:22:53 GMT</pubDate>
</item>
<item>
<title>CODA：一种有效的视觉离散化框架</title>
<link>https://arxiv.org/abs/2503.17760</link>
<guid>https://arxiv.org/abs/2503.17760</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CODA框架通过解耦压缩与离散化，提升图像生成的稳定性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了CODA（连续到离散适配）框架，旨在解决传统离散视觉标记器在图像生成过程中面临的压缩与离散化联合训练的挑战。CODA通过将已优化的连续变分自编码器（VAE）进行适配，而非从头训练离散标记器，从而实现更稳定和高效的训练。重点关注离散化，确保了视觉质量的保留。在实验证明中，CODA在训练预算上比标准VQGAN减少了6倍，且在ImageNet 256×256基准上达到了100%的代码本利用率，以及在8倍和16倍压缩时分别获得了0.43和1.34的重建FID（rFID），显示出优异的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17760" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 08:59:00 GMT</pubDate>
</item>
<item>
<title>DynamicVis：面向遥感图像的动态视觉感知基础模型</title>
<link>https://arxiv.org/abs/2503.16426</link>
<guid>https://arxiv.org/abs/2503.16426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DynamicVis是一个为遥感图像设计的高效动态视觉感知模型。</p><br /><br /><p><strong>摘要：</strong> 随着遥感技术的发展，卫星图像的空间分辨率得到了显著提升，但现有方法在多应用场景中的泛化能力有限。本文提出DynamicVis，一个针对遥感图像的动态视觉感知基础模型，克服了传统模型在面对高分辨率数据和复杂场景语义时的局限性。该模型通过基于选择性状态空间的动态区域感知骨干集成，平衡了局部细节提取与全局上下文集成，支持大规模数据的高效编码，同时实现架构的可扩展性。为了增强跨任务知识的转移，DynamicVis采用了针对百万级区域注释的多实例学习范式。经过在九个下游任务上的评估，该模型展现了出色的多级特征建模能力，以97毫秒的延迟处理2048x2048像素，并消耗833 MB GPU内存，具有卓越的效率，明显优于基于ViT的其他模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于变换器的多光源白平衡修正方法</title>
<link>https://arxiv.org/abs/2503.14774</link>
<guid>https://arxiv.org/abs/2503.14774</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的变换器模型来优化多光源下的白平衡修正。</p><br /><br /><p><strong>摘要：</strong> 本文针对多光源场景中的白平衡（WB）修正问题, 提出了两项重要贡献。首先，我们设计了一种高效的变换器模型，能够有效捕捉多种sRGB WB预设之间的空间依赖性，显著提高了线性融合技术的效果。其次，我们构建了一个大规模的多光源数据集，包含超过16000张使用五种不同白平衡设置渲染的sRGB图像及其修正图像。该方法在新构建的多光源图像融合数据集上，性能提升可达100%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14774" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 19:01:22 GMT</pubDate>
</item>
<item>
<title>揭示图像超分辨率评估中的地面真实图像质量影响</title>
<link>https://arxiv.org/abs/2503.13074</link>
<guid>https://arxiv.org/abs/2503.13074</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究探讨了现有图像超分辨率数据集中地面真实图像的质量及其对评估的影响。</p><br /><br /><p><strong>摘要：</strong> 随着图像超分辨率（SR）技术的进步，虽然输出的感知质量不断提高，但在定量评估中却通常表现不佳，引发了对现有图像评价指标的怀疑。传统上，研究者们认为地面真实图像（GT）是完美的参考，而未对其质量进行检视。本文通过分析七种先进的SR模型在三个真实世界SR数据集上的表现，指出存在的低质量GT会造成模型评估的一致性下降，因此模型在控制GT质量的情况下表现会显著不同。此外，本文提出一种新颖的感知质量指标——相对质量指数（RQI），用于衡量图像对之间的质量差异，从而修正由于不可靠GT导致的偏差评估。该模型在与人类观众意见的一致性方面表现显著优于现有方法，期望为SR领域未来数据集、模型和指标的开发提供有价值的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13074" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:25:48 GMT</pubDate>
</item>
<item>
<title>文化适应性对大型语言模型数学推理能力的影响研究</title>
<link>https://arxiv.org/abs/2503.18018</link>
<guid>https://arxiv.org/abs/2503.18018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示文化背景影响大型语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨大型语言模型（LLMs）在面临文化适应的数学问题时的推理能力。我们生成了六个合成文化数据集，基于广泛使用的GSM8K基准测试，同时保留了数学逻辑和数值，但修改了文化元素。研究发现，当文化引用发生变化时，LLMs在数学问题解决方面表现不佳，尽管数学结构保持不变。小型模型的性能下降更为明显，而文化熟悉度似乎可以提升数学推理能力。即便是没有显著数学训练的模型，在相关文化背景暴露下，仍能在嵌入文化的数学问题上超越大型数学能力模型。这项研究强调了文化背景对LLMs数学推理能力的影响，并凸显了为提高模型在实际应用中鲁棒性而需要更具多样性和代表性的训练数据的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:35:39 GMT</pubDate>
</item>
<item>
<title>新型平衡图像建模框架的提出</title>
<link>https://arxiv.org/abs/2503.18948</link>
<guid>https://arxiv.org/abs/2503.18948</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出了一种新型图像建模框架，以解决子任务间的优化冲突。</p><br /><br /><p><strong>摘要：</strong> 当前的生成模型（如自回归和扩散方法）将高维数据分布学习分解为一系列简单的子任务。然而，在这些子任务的联合优化中，固有的冲突很难解决，现有方案往往无法在不牺牲效率或可扩展性的情况下克服这些冲突。本文提出了一种新型的等变图像建模框架，利用自然视觉信号的平移不变性，从根本上对齐子任务的优化目标。我们的方法引入了（1）沿水平轴增强平移对称性的列状标记化，和（2）通过强制跨位置一致的上下文关系的窗口因果注意力。我们在256x256分辨率的类条件ImageNet生成任务上进行了评估，该方法的性能可与最先进的自回归模型相媲美，同时使用更少的计算资源。系统分析表明，增强的等变性减少了任务间的冲突，显著提高了零样本泛化能力并实现超长图像合成。本文奠定了生成建模中任务对齐分解的基础框架，为高效参数共享和无冲突优化提供了新的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18948" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>CFG-Zero*: 改进的分类器自由引导技术</title>
<link>https://arxiv.org/abs/2503.18886</link>
<guid>https://arxiv.org/abs/2503.18886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨CFG-Zero*在流匹配模型中的改进效果。</p><br /><br /><p><strong>摘要：</strong> 本文分析了分类器自由引导技术（CFG）对基于高斯混合训练的流匹配模型的影响。研究表明，在训练初期，流估计不精确时，CFG可能引导样本走向错误的轨迹。基于此观察，提出了改进版本CFG-Zero*，该方法包括两个主要贡献：一是通过优化比例来校正估计速度的不准确性，二是将常微分方程求解器的前几步初始化为零。通过对文本到图像（如Lumina-Next、Stable Diffusion 3和Flux）以及文本到视频（Wan-2.1）生成的实验，CFG-Zero*在引导流匹配模型方面表现出色，显著优于传统的CFG方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:59:57 GMT</pubDate>
</item>
<item>
<title>探究大型语言模型的推理机制：基于稀疏自编码器的分析</title>
<link>https://arxiv.org/abs/2503.18878</link>
<guid>https://arxiv.org/abs/2503.18878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了稀疏自编码器在大型语言模型推理中的应用。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自然语言处理领域取得了显著成功，尤其是在推理能力方面。本文通过使用稀疏自编码器（SAEs）来识别推动DeepSeek-R1系列模型推理的特征。研究者们提出了一种提取候选“推理特征”的方法，并通过实证分析和可解释性方法验证了这些特征与模型推理能力之间的直接关联。结果表明，系统地引导这些特征可以有效提升推理性能，这为理解LLMs中的推理机制提供了首个机械性解释。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:54:26 GMT</pubDate>
</item>
<item>
<title>CURA: 提升软件工程任务的代码理解与推理代理系统</title>
<link>https://arxiv.org/abs/2503.18494</link>
<guid>https://arxiv.org/abs/2503.18494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURA系统通过增强的过程监督提高了代码生成和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CURA，一个增强了口头过程监督（VPS）的代码理解和推理代理系统，旨在解决复杂软件工程挑战。尽管现有的大型语言模型在代码生成基准上取得了显著进展，但在面临复杂任务时仍表现不佳。CURA在BigCodeBench等具有挑战性的基准上比基线模型提高了3.65%。此外，CURA与o3-mini模型及VPS技术结合后，达到了最先进的性能。这一工作标志着推理驱动架构与基于LLM的代码生成的结合，为语言模型处理复杂软件工程任务提供了代理推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:48:59 GMT</pubDate>
</item>
<item>
<title>MetaSpatial：基于强化学习的3D空间推理框架</title>
<link>https://arxiv.org/abs/2503.18470</link>
<guid>https://arxiv.org/abs/2503.18470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSpatial框架通过RL增强视觉语言模型的3D空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MetaSpatial，这是第一个基于强化学习（RL）的框架，用于增强视觉语言模型（VLM）的3D空间推理能力，实现在无需硬编码优化的情况下进行实时3D场景生成。MetaSpatial解决了两个核心挑战：一是VLM缺乏内化的3D空间推理，限制了其生成现实布局的能力；二是传统的监督微调（SFT）在布局生成任务中的低效性，因为完美的真值标注不可用。其关键创新在于引入了基于多轮RL优化机制，整合了物理约束和渲染图像评估。该框架采用适应性迭代推理过程，使VLM通过分析渲染输出逐步优化空间配置，从而提高场景的一致性。实验评估表明，MetaSpatial显著增强了各种规模模型的空间一致性和格式稳定性，生成的物体放置更加真实、协调，验证了RL在元宇宙、增强现实/虚拟现实、数字双胞胎及游戏开发中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 05:18:01 GMT</pubDate>
</item>
<item>
<title>Diffusion-4K：基于文本生成的超高分辨率图像合成框架</title>
<link>https://arxiv.org/abs/2503.18352</link>
<guid>https://arxiv.org/abs/2503.18352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Diffusion-4K提出了一种新的超高分辨率图像合成方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Diffusion-4K，一个用于直接进行超高分辨率图像合成的新框架，结合文本到图像的扩散模型。主要创新包括构建了Aesthetic-4K基准数据集，该数据集弥补了公开可用的4K图像合成数据集的缺失，并采用GPT-4o生成高质量的4K图像和标题。此外，引入了GLCM得分和压缩比指标来评估图像细节，并结合FID、美学和CLIPScore等全局指标进行全面评价。其次，提出基于小波的微调方法，以实现对光真实4K图像的直接训练，适用于多种潜在扩散模型，显示出在合成高细节4K图像方面的有效性。实验结果表明，Diffusion-4K在超高分辨率图像合成方面展现出了卓越的性能，尤其是结合现代大规模扩散模型时（如SD3-2B和Flux-12B）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 01:25:07 GMT</pubDate>
</item>
<item>
<title>OmnimatteZero：一种无训练视频分层提取的新方法</title>
<link>https://arxiv.org/abs/2503.18033</link>
<guid>https://arxiv.org/abs/2503.18033</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmnimatteZero是一种无训练的视频分层提取方法，能高效处理视频对象和背景。</p><br /><br /><p><strong>摘要：</strong> OmnimatteZero是一种新提出的方法，旨在无训练地将给定视频分解为具有语义意义的层，包括背景和独立对象及其相关效果，如阴影和反射。与现有方法相比，OmnimatteZero利用现成的预训练视频扩散模型，实现对象移除、个体对象层提取及其效果合成。该方法通过适应零样本图像修补技术来处理视频对象移除，尽管该技术在此任务上表现不佳。研究显示，自注意力图能够捕捉对象及其影响信息，并用于修补对象效果，从而保留干净的背景。此外，通过简单的潜在算术，用户可以将对象层与新视频层无缝重组以生成新视频。评估结果表明，OmnimatteZero不仅在背景重建方面表现优越，还创下最快Omnimatte方法的新记录，具备实时性能和极小的帧运行时间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18033" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 07:26:48 GMT</pubDate>
</item>
<item>
<title>Aether框架：结合几何重建与生成建模的智能空间推理系统</title>
<link>https://arxiv.org/abs/2503.18945</link>
<guid>https://arxiv.org/abs/2503.18945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aether框架实现了几何感知的智能空间推理，具备四维动态重建等三大核心能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Aether，一个统一的框架，旨在结合几何重建与生成建模，推动人类般的空间推理能力。Aether通过联合优化四大核心能力：四维动态重建、动作条件下的视频预测以及目标条件下的视觉规划，利用任务交错的特征学习，促进了重建、预测和规划目标间的知识共享。基于视频生成模型，我们的框架在训练期间未观察到真实数据的情况下，展现出前所未有的合成-真实泛化能力，并在动作跟随和重建任务中实现了零样本泛化。Aether的重建性能显著超过特定领域模型，即使未使用真实数据。此外，Aether通过几何信息驱动的动作空间，有效地将预测转化为行动，实现了自主轨迹规划。我们希望本研究能够激励社区探索物理合理的世界建模新前沿及其应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>探索测试时间扩展对视频生成质量的影响</title>
<link>https://arxiv.org/abs/2503.18942</link>
<guid>https://arxiv.org/abs/2503.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨测试时间扩展如何提高视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 随着训练数据、模型规模和计算成本的增加，视频生成在数字创作中取得了显著成果。最近，针对大型语言模型的研究扩展了测试时间的规模，从而显著提高了模型性能。本文通过研究如何在视频生成中应用测试时间扩展（TTS），探讨在允许使用大量推理计算的情况下，如何提升生成质量。我们将视频生成中的测试时间扩展重新解释为搜索问题，利用测试时间验证器和启发式算法来优化搜索过程。针对文本提示，我们首先提出了一种线性搜索策略，通过增加推理时的噪声候选项来探寻更好的路径。此外，为了提高效率，我们设计了一种名为树形帧（ToF）的 TTS 方法，以自回归的方式自适应地扩展和修剪视频分支。大量实验表明，增加测试时间计算持续显著提升视频质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>Video SimpleQA：评估大型视频语言模型的事实性基准</title>
<link>https://arxiv.org/abs/2503.18923</link>
<guid>https://arxiv.org/abs/2503.18923</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Video SimpleQA基准，评估大型视频语言模型的事实性表现。</p><br /><br /><p><strong>摘要：</strong> 随着大型视频语言模型（LVLMs）在多模态理解领域的最新进展，评估其在视频上下文中的事实性仍然是一个关键挑战。为此，我们提出Video SimpleQA，这是首个针对LVLMs事实性评估的全面基准。该基准的几个关键创新特性包括：要求整合外部知识、针对客观事件的问题设计、明确且简短的答案格式、经过外部来源验证的注释以及时间推理能力的考察。我们对41个最先进的LVLMs进行了广泛评估，发现当前模型在事实遵循上存在显著不足，尤其是开源模型。表现最佳的模型Gemini-1.5-Pro仅达到54.4%的F1分数。此外，测试时间的计算方式对性能提升影响有限，而检索增强生成虽然带来一致性改善，但也增加了推理时间开销，形成效率与性能的权衡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18923" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:46:09 GMT</pubDate>
</item>
<item>
<title>FFN Fusion: 提升大规模语言模型推理效率的新技术</title>
<link>https://arxiv.org/abs/2503.18908</link>
<guid>https://arxiv.org/abs/2503.18908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFN Fusion技术通过并行化FFN层实现语言模型推理效率提升。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FFN Fusion的架构优化技术，该技术通过识别并利用大语言模型中自然的并行化机会，降低了顺序计算。研究发现，去除特定注意力层后的前馈网络（FFN）层序列，通常可以在保持模型精度的前提下进行并行处理。我们开发了一种系统的方法来识别并融合这些序列，转化为并行操作，从而显著降低推理延迟，同时保留模型行为。应用于Llama-3.1-405B-Instruct模型，我们创建了Llama-Nemotron-Ultra-253B-Base，一个高效的新模型，推理延迟提高了1.71倍，单个Token的成本降低了35倍，并在各种基准测试中展现出良好的性能。我们的实验表明，在49B到253B参数的模型中，FFN Fusion在大规模应用中效果愈发显著，且能够与量化和剪枝等现有优化技术相辅相成。我们还发现，完整的变换器块（注意力层和FFN层）有时也可以进行并行化，这为神经架构设计指明了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:20:35 GMT</pubDate>
</item>
<item>
<title>基于零强化学习的多模型链式推理训练研究</title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了零强化学习在多模型中的应用和效果。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了零强化学习在10种不同基础模型上的训练效果，包括LLama3-8B、Mistral-7B/24B以及多个Qwen2.5系列模型。通过设计关键策略如调整奖励格式和控制查询难度，我们在推理准确性和响应长度上取得了显著提升。不同基础模型在训练过程中表现出不同的动态特征，增加的响应长度并不总是与特定认知行为（如验证）的出现相关联。值得注意的是，我们首次在非Qwen家族的小模型中观察到了“恍然大悟”的瞬间。此外，我们分享了实现成功零强化学习训练的关键设计，及其相关发现与实践，旨在推动进一步研究，我们还开源了代码、模型和分析工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18892" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:06:10 GMT</pubDate>
</item>
<item>
<title>利用潜在思维推断提升语言模型预训练数据效率</title>
<link>https://arxiv.org/abs/2503.18866</link>
<guid>https://arxiv.org/abs/2503.18866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过潜在思维推断提升数据效率以支持语言模型的预训练。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型预训练的计算规模超越人类文本增长，数据瓶颈问题日益严重。为在数据受限的情况下持续推进预训练，我们提出显式建模和推断文本生成过程背后的潜在思维，以显著提高数据效率。这一方法认为网络文本是冗长人类思维过程的压缩结果，而潜在思维包含重要的上下文知识和推理步骤，是数据高效学习的关键。通过对数学领域的实验，证明了合成数据推断潜在思维能够有效提升数据效率，表现优于相同原始数据量的训练。此外，我们展示了在没有强大教师的情况下进行潜在思维推断，语言模型通过EM算法自我引导其性能的迭代提升。结果显示，1B语言模型在三次迭代中能够利用推断计算显著超过依赖原始数据的基线，进一步的推断计算提升表明在数据受限预训练中存在新的扩展机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 12:41:23 GMT</pubDate>
</item>
<item>
<title>CaMeL：增强大语言模型安全性的防御措施</title>
<link>https://arxiv.org/abs/2503.18813</link>
<guid>https://arxiv.org/abs/2503.18813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提出了CaMeL防御机制，保护LLM免受恶意数据的影响。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CaMeL的防御机制，旨在保护在外部环境中交互的代理系统中的大语言模型（LLM）。LLM在处理不可信数据时容易受到提示注入攻击，而CaMeL通过在LLM周围创建一个防护系统层来增强其安全性。具体来说，CaMeL明确提取来自可信查询的控制和数据流，确保LLM所检索的不可信数据不会影响程序流。此外，CaMeL还依赖于能力的概念，以防止通过未经授权的数据流泄露私密数据。在最近的代理安全基准AgentDojo的测试中，CaMeL成功以可证明的安全性解决了67%的任务，展示了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:54:10 GMT</pubDate>
</item>
<item>
<title>Hummingbird: 高效的文本到视频生成框架</title>
<link>https://arxiv.org/abs/2503.18559</link>
<guid>https://arxiv.org/abs/2503.18559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hummingbird框架通过轻量化设计提高文本到视频生成的效率与质量。</p><br /><br /><p><strong>摘要：</strong> 文本到视频生成（T2V）技术逐渐引起关注，但现有模型在计算效率与视觉质量之间难以平衡。针对这一挑战，本文提出了轻量化的T2V框架Hummingbird，通过剪枝现有模型并采用视觉反馈学习，成功将U-Net参数从14亿减少至7亿，大幅提高了效率且保留了高质量的视频生成能力。此外，Hummingbird还引入了新颖的数据处理流程，利用大型语言模型和视频质量评估模型来改善文本提示和视频数据的质量。实验结果表明，该方法相比于最新的VideoCrafter2模型，实现了31倍的加速，并在VBench上获得了最高得分。同时，Hummingbird支持最多生成26帧的视频，克服了传统U-Net方法在长视频生成方面的局限性。整个训练过程仅需四个GPU，却能与领先方法相媲美，展现出其在实际应用中的高效性和灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 07:13:33 GMT</pubDate>
</item>
<item>
<title>AgentRxiv：促进科学研究的协作框架</title>
<link>https://arxiv.org/abs/2503.18102</link>
<guid>https://arxiv.org/abs/2503.18102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AgentRxiv框架促进科学家间的协作研究，提升研究效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgentRxiv，一个促进科学家之间协作的框架，旨在解决现有自主研究流程的孤立性问题。通过允许大型语言模型（LLM）实验室上传和检索共享的预印本报告，AgentRxiv使研究人员能够合作、共享见解，并在彼此的研究基础上逐步推进。研究表明，获取先前研究的代理在性能上显著优于孤立工作的代理（在MATH-500上的相对提升达11.4%）。此外，多个实验室通过共享研究成果，能够共同朝着目标努力，并在整体准确性上更显著提升（在MATH-500上的相对提升达到13.7%）。这些发现表明，未来的AI系统设计中，自主代理可能与人类共同发挥作用。希望AgentRxiv为研究者加速科学发现提供助力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 11:16:42 GMT</pubDate>
</item>
<item>
<title>Vision-R1: 一种新型的视觉引导强化学习算法</title>
<link>https://arxiv.org/abs/2503.18013</link>
<guid>https://arxiv.org/abs/2503.18013</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了Vision-R1，一个新型视觉引导的强化学习算法，以提升LVLM的性能。</p><br /><br /><p><strong>摘要：</strong> 在大型视觉语言模型（LVLMs）的训练中，传统的两阶段方法包括预训练和监督微调。近期，偏好优化作为一种后训练强化策略，已被证明有效。但高质量的人工标注偏好数据构建及可靠的奖励模型开发均成本高且具挑战性。为解决这一问题，本文提出Vision-R1，一种新型的视觉引导R1-like强化学习算法，使用明确的视觉反馈奖励模型，无需专门的奖励模型和人工偏好数据集。我们引入了一个基于标准驱动的奖励函数，综合多维反馈来评估模型的完成效果。并实施了逐步规则优化策略，动态调整奖励标准，促进持续的模型改进，同时减少奖励劫持。大量实验表明，使用Vision-R1微调7B LVLMs在各类基准测试中获得了显著性能提升，甚至实现了50%的 improvement，并超越了当前10倍规模模型的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18013" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 23 Mar 2025 06:21:14 GMT</pubDate>
</item>
<item>
<title>资源受限条件下视频生成模型的训练策略研究</title>
<link>https://arxiv.org/abs/2503.17735</link>
<guid>https://arxiv.org/abs/2503.17735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，在受限条件下从零开始训练小型视频生成模型优于参数高效调优。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成技术取得了显著进展，引起了学者们的广泛关注。在资源受限的条件下，研究人员通常基于参数高效调优的方法对预训练模型进行微调。然而，这些方法通常由于训练参数较少，造成适应能力不足，且源领域的知识可能导致目标领域推理过程偏离。本文提出，在有限资源条件下，从头开始训练小型视频生成模型，利用百万级样本，能在下游应用中优于大型模型的参数高效调优。以动画贴纸生成（ASG）为案例，我们构建了一个低帧率的离散帧生成网络，并提出双掩码数据利用策略，以提高有限数据的可用性和多样性。为促进双掩码下的收敛，我们提出了基于难度的适应性课程学习方法，通过将样本熵分解为静态和适应性成分，按难易程度抽取样本。实验结果表明，我们的资源高效双掩码训练框架在定量和定性上均优于I2V-Adapter和SimDA等参数高效调优方法，验证了该方法在受限资源下的可行性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 22 Mar 2025 07:28:25 GMT</pubDate>
</item>
<item>
<title>优化大语言模型预训练的权重初始化与方差控制策略</title>
<link>https://arxiv.org/abs/2503.17500</link>
<guid>https://arxiv.org/abs/2503.17500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LIR和TVR策略，显著提升大语言模型预训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLM）预训练中的权重初始化和方差控制策略，重点介绍了层索引重标定（LIR）权重初始化方案与目标方差重标定（TVR）方差控制策略。通过在一个10亿参数的LLaMA模型上进行实验，结果表明这些技术在方差管理方面的改进可以显著提升下游任务的表现，在常见的预训练基准上提高了多达4.6%。此外，这些方法还有效减少了极端激活值，有助于缓解定量化和低精度训练中遇到的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 15:23:08 GMT</pubDate>
</item>
<item>
<title>优化RISC-V CPU上的大型语言模型推理</title>
<link>https://arxiv.org/abs/2503.17422</link>
<guid>https://arxiv.org/abs/2503.17422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨在RISC-V CPU上优化大型语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的指数增长，基于GPU的系统成为主要选择。然而，CPU由于其灵活性和更低的成本，逐渐成为特别适合推理和推断工作负载的替代方案。RISC-V作为一种开放且中立的指令集架构，正在迅速获得关注。虽然RISC-V硬件和软件生态系统尚不完善，针对特定领域的调优至关重要，本文旨在填补这一空白，专注于在首款具备向量处理能力的多核RISC-V CPU——Sophon SG2042上优化LLM推理。针对两个经过优化的前沿LLM，DeepSeek R1 Distill Llama 8B和DeepSeek R1 Distill QWEN 14B，我们实现了4.32/2.29 token/s的令牌生成速率与6.54/3.68 token/s的提示处理速率，相较于基线速度提升达2.9倍/3.0倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:00:19 GMT</pubDate>
</item>
<item>
<title>优化最小高斯表示法（OMG）在3D场景渲染中的应用</title>
<link>https://arxiv.org/abs/2503.16924</link>
<guid>https://arxiv.org/abs/2503.16924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OMG方法通过减少高斯数量优化3D场景渲染存储。</p><br /><br /><p><strong>摘要：</strong> 3D Gaussian Splatting（3DGS）作为一种高效的实时渲染表示方法，面临着使用大量显式高斯原语所带来的存储和内存开销问题。尽管已有研究表明通过高精度属性可以在减少高斯数量的情况下实现高质量渲染，现有的压缩方法仍依赖于大量高斯，主要集中在属性压缩上，导致压缩敏感性提高，质量下降。为此，本文提出了一种优化的最小高斯表示法（OMG），旨在显著减少存储需求并使用最小数量的原语。通过识别近邻高斯的区别以降低冗余，并提出一种高效的属性表示，以保留原语之间的连续性和不规则性。此外，采用子向量量化技术以进一步提升不规则性表示的效率，实现快速训练且代码本大小可忽略。大量实验表明，OMG相较于当前最先进技术将存储需求降低近50%，并在保持高渲染质量的同时实现600+帧每秒的渲染速度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:41:45 GMT</pubDate>
</item>
<item>
<title>Typed-RAG: 面向非事实问答的多维度框架</title>
<link>https://arxiv.org/abs/2503.15879</link>
<guid>https://arxiv.org/abs/2503.15879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Typed-RAG框架通过多维度分解提升非事实问答的回答质量。</p><br /><br /><p><strong>摘要：</strong> 非事实问答（NFQA）因其开放性、多样化的意图及多维推理的需求，给传统的事实问答方法带来了重大挑战。本文提出Typed-RAG，一个在RAG框架内的类型感知多维分解方法，专门解决NFQA的问题。Typed-RAG将NFQ分类为不同类型（如争论、经验和比较），并通过基于维度的分解来优化检索与生成策略。通过将多维NFQ分解为单一维度的子查询并汇总结果，Typed-RAG能够生成更具信息量和上下文相关性的回答。我们还引入Wiki-NFQA，这是一个涵盖多种NFQ类型的基准数据集，实验结果表明Typed-RAG明显优于基线模型，凸显了类型感知分解在NFQA中有效检索和生成的关键作用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 02:04:12 GMT</pubDate>
</item>
<item>
<title>Bottleneck Sampling：一种高效的扩散模型推理框架</title>
<link>https://arxiv.org/abs/2503.18940</link>
<guid>https://arxiv.org/abs/2503.18940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架，可在不损失质量的情况下加速扩散模型推理。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在视觉内容生成中表现出色，但其推理过程中的高计算成本限制了应用。本文提出了Bottleneck Sampling框架，利用低分辨率的先验知识，减少计算开销同时保持输出的高质量。该框架采用高-低-高的去噪工作流程，在高分辨率的初始和结束阶段进行去噪，而在中间步骤使用低分辨率。为减少混叠和模糊伪影，进一步优化了解析度切换和去噪时间步的自适应调整。实验表明，该方法在图像生成任务中加速推理速度可达3倍，在视频生成中可达2.5倍，且在多个评估指标上与标准的全分辨率采样过程输出质量相当。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AlphaSpace：提升大型语言模型在3D空间导航中的空间推理能力</title>
<link>https://arxiv.org/abs/2503.18769</link>
<guid>https://arxiv.org/abs/2503.18769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaSpace显著提高大型语言模型的3D空间导航性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了AlphaSpace，一种新方法旨在增强大型语言模型（LLMs）在3D笛卡尔空间导航中的空间推理能力。AlphaSpace采用基于语义的分词策略，通过专门的语义标记编码高度信息，并主要整合符号化合成推理数据。这种方法使LLMs能够准确地操作对象，将其放置在特定的[x, y, z]坐标上。实验结果表明，AlphaSpace在操作子任务上显著优于现有模型，总体准确度达到66.67%，而GPT-4o为37.5%，Claude 3.5 Sonnet为29.17%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.18769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 24 Mar 2025 11:16:51 GMT</pubDate>
</item>
<item>
<title>利用多模态LLM评估跨模态理解与生成任务</title>
<link>https://arxiv.org/abs/2503.17489</link>
<guid>https://arxiv.org/abs/2503.17489</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了评估多模态生成与理解的统一基准，探讨LLM在任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了使用多模态大型语言模型（MLLM）作为自动评估工具在开放式多模态理解（MMU）和生成（MMG）任务中的应用。通过推出两个基准TaskAnything和JudgeAnything，本文分别评估了MLLM在多模态任务中的整体表现和评判能力。TaskAnything针对15类多模态任务进行评估，使用1500个经过精挑细选的查询，而JudgeAnything则重点考察了5种先进模型（如GPT-4o和Gemini-2.0-Flash）的评判能力。实验结果显示，尽管MLLM在MMU任务中表现优异（在配对比较设置中平均达66.55%），但在MMG任务中面临重大挑战（配对比较设置中仅达53.37%），暴露了跨模态偏见和幻觉问题。为此，本文提出了OmniArena，一个评估多模态模型和奖励模型的自动化平台，强调需要更公平的评估协议以及与人类偏好的更强对齐。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17489" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 14:59:20 GMT</pubDate>
</item>
<item>
<title>推动游戏开发革新的生成游戏引擎</title>
<link>https://arxiv.org/abs/2503.17359</link>
<guid>https://arxiv.org/abs/2503.17359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨互动生成视频如何推动生成游戏引擎的发展，重塑游戏创作方式。</p><br /><br /><p><strong>摘要：</strong> 现代游戏开发面临着创意和成本方面的重大挑战，传统游戏引擎的预设内容限制了开发者的创新能力。近期视频生成模型的突破，为创建逼真且互动的虚拟环境提供了新机遇。本文提出以互动生成视频（IGV）为基础，构建生成游戏引擎（GGE），这一平台能够实现无限的新内容生成，革新下一代游戏。GGE利用IGV的优势，进行高质量内容合成、物理意识的世界建模、用户控制的互动、长期记忆能力和因果推理。我们详细阐述了GGE的核心模块及其发展路线图，从L0到L4引导其演进，展望在人工智能时代，AI驱动的生成系统将根本改变游戏的创作与体验方式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>通过错误学习提升大型语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2503.17439</link>
<guid>https://arxiv.org/abs/2503.17439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出LEMMA方法，通过错误学习提高大型语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在解决数学问题方面展现出卓越的推理能力，但现有方法主要集中于改进正确训练数据的质量，忽略了错误数据的潜在价值。本文提出了一种名为LEMMA的方法，通过构建包含错误步骤的错误解与正确解的反思联系的数据集，来增强模型的推理能力。我们分析了模型生成的错误类型，并引入了基于错误类型的增强方法，以收集多样且具有代表性的错误。通过模型感知的平滑反思连接，错误解被有效地转化为正确解。实验结果表明，LEMMA在性能上显著优于其他强基线，为大型语言模型的数学推理能力提升提供了新的途径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:59:10 GMT</pubDate>
</item>
<item>
<title>MagicComp: 通过双阶段精炼提升文本生成视频的组合能力</title>
<link>https://arxiv.org/abs/2503.14428</link>
<guid>https://arxiv.org/abs/2503.14428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicComp是一种创新的方法，通过双阶段精炼改进文本生成视频的能力。</p><br /><br /><p><strong>摘要：</strong> MagicComp是一种无须训练的创新方法，旨在通过双阶段精炼技术提升文本生成视频（T2V）的组合能力。其在两个阶段中特别设计：第一阶段的条件化阶段引入了语义锚点消歧义技术，通过逐步注入语义锚点的方向向量来强化主体特定语义，解决了主体间的模糊性问题；第二阶段的去噪阶段提出了动态布局融合注意力机制，灵活地将主体绑定到其时空区域，以便通过掩蔽注意力调制实现。MagicComp具备模型无关性和通用性，可以无缝地集成到现有的T2V架构中。经过在T2V-CompBench和VBench上的广泛实验，MagicComp的性能超越了当前最先进的方法，显示了其在复杂提示基础和轨迹可控视频生成应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:02:14 GMT</pubDate>
</item>
<item>
<title>Can Large Vision Language Models Read Maps Like a Human?</title>
<link>https://arxiv.org/abs/2503.14607</link>
<guid>https://arxiv.org/abs/2503.14607</guid>
<content:encoded><![CDATA[
In this paper, we introduce MapBench-the first dataset specifically designed for human-readable, pixel-based map-based outdoor navigation, curated from complex path finding scenarios. MapBench comprises over 1600 pixel space map path finding problems from 100 diverse maps. In MapBench, LVLMs generate language-based navigation instructions given a map image and a query with beginning and end landmarks. For each map, MapBench provides Map Space Scene Graph (MSSG) as an indexing data structure to convert between natural language and evaluate LVLM-generated results. We demonstrate that MapBench significantly challenges state-of-the-art LVLMs both zero-shot prompting and a Chain-of-Thought (CoT) augmented reasoning framework that decomposes map navigation into sequential cognitive processes. Our evaluation of both open-source and closed-source LVLMs underscores the substantial difficulty posed by MapBench, revealing critical limitations in their spatial reasoning and structured decision-making capabilities. We release all the code and dataset in https://github.com/taco-group/MapBench.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 14:05:38 GMT</pubDate>
</item>
<item>
<title>GAEA：图像地理定位中的对话模型创新</title>
<link>https://arxiv.org/abs/2503.16423</link>
<guid>https://arxiv.org/abs/2503.16423</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GAEA模型通过交互方式提升图像地理定位的能力。</p><br /><br /><p><strong>摘要：</strong> 图像地理定位是一个有重要应用的挑战性任务，传统模型只能提供GPS坐标，缺乏与用户的互动能力。近期，随着大型多模态模型的发展，研究者仅通过这些模型尝试进行图像的地理定位，但在专业的下游任务中仍显不足。为此，本文提出了GAEA对话模型，旨在为用户提供所需的地理位置信息。由于缺乏大规模数据集进行训练，我们构建了GAEA数据集，包含80万张图像及160万对问答，利用OpenStreetMap属性和地理上下文线索生成。同时，提出了包含4000对图像-文本的多样化基准来评估对话能力，结果显示GAEA在多个开源及专有LMMs中显著优于现有最优模型LLaVA-OneVision和GPT-4o，分别提升25.69%及8.28%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16423" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>FFaceNeRF：一种灵活的3D人脸编辑技术</title>
<link>https://arxiv.org/abs/2503.17095</link>
<guid>https://arxiv.org/abs/2503.17095</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FFaceNeRF通过增强用户控制和灵活性，实现高质量3D人脸编辑。</p><br /><br /><p><strong>摘要：</strong> FFaceNeRF是一种基于神经辐射场（NeRF）的3D人脸编辑技术，旨在克服现有方法在用户控制方面的局限性。传统方法依赖于预训练的分割掩码，限制了用户对面部特征的个性化调整，且需要大量的训练数据。FFaceNeRF采用几何适配器与特征注入技术，有效操控几何属性，结合潜在混合与三平面增强，支持少量样本的训练。这种创新方法确保了快速适应用户所需的掩码布局，尤其适用于个性化医疗影像和创意人脸编辑等领域。比较评估结果显示，FFaceNeRF在灵活性、控制性和生成图像质量等方面超越了现有基于掩码的人脸编辑方法，为高保真3D人脸编辑的未来发展开辟了新路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17095" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 08:24:58 GMT</pubDate>
</item>
<item>
<title>TaoAvatar：高保真轻量级全身虚拟头像的实时渲染</title>
<link>https://arxiv.org/abs/2503.17032</link>
<guid>https://arxiv.org/abs/2503.17032</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TaoAvatar在增强现实中实现高保真实时全身虚拟头像渲染。</p><br /><br /><p><strong>摘要：</strong> TaoAvatar是一种基于3D Gaussian Splatting (3DGS) 的高保真轻量级全身虚拟头像，具有广泛的应用潜力，从电商直播到全息通信。尽管现有技术在头像创建上已有进展，但在全面口语任务中面临着面部表情和身体动作的细粒度控制问题，同时还常常缺乏细节和实时性能。TaoAvatar通过创建个性化服装人类参数模板，并使用StyleUnet网络处理复杂的姿势依赖非刚性变形，来捕捉高频外观细节。最终，通过蒸馏技术将非刚性变形“烘焙”到轻量级的MLP网络中，并开发混合形状以补偿细节，从而在各种设备上实现90 FPS的实时渲染质量，展现了其行业领先的渲染效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17032" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 06:40:37 GMT</pubDate>
</item>
<item>
<title>融合3D视觉语言模型的通用少样本点云分割框架</title>
<link>https://arxiv.org/abs/2503.16282</link>
<guid>https://arxiv.org/abs/2503.16282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架GFS-VL，结合少样本和伪标签提高点云分割效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GFS-VL的通用少样本3D点云分割框架，该框架旨在通过结合稀疏的少样本和密集的伪标签，以最大化两者的优势，适应新类的分割。具体而言，我们引入了一种原型引导的伪标签选择方法，以滤除低质量区域，并采用自适应填充策略，结合伪标签上下文和少样本知识，标注过滤后的未标记区域。此外，我们设计了一种新基础混合策略，将少样本嵌入训练场景中，从而保持关键上下文以改善新类学习。针对目前GFS-PCS基准中有限的多样性，我们还引入了两个具有多样化新类的挑战性基准，以便于全面的泛化评估。实验结果验证了框架在不同模型和数据集上的有效性，为GFS-PCS在实际应用中的进一步发展奠定了良好的基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:10:33 GMT</pubDate>
</item>
<item>
<title>SISO：基于单图像的个性化图像生成与编辑方法</title>
<link>https://arxiv.org/abs/2503.16025</link>
<guid>https://arxiv.org/abs/2503.16025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SISO是一种无需训练的个性化图像生成与编辑方法。</p><br /><br /><p><strong>摘要：</strong> SISO是一种新颖的个性化图像生成与编辑方法，解决了仅有少量或单张图像时的个性化挑战。该方法通过优化与输入主体图像的相似性得分，迭代生成图像，直至实现理想的相似度，无需训练。我们在图像编辑和生成任务中对SISO进行评估，使用多样的个人主体数据集，结果显示该方法在图像质量、主体保真性及背景保留方面相较于现有方法有显著提升。SISO的设计允许与任何图像生成器进行即插即用的优化，展示了其广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:45:04 GMT</pubDate>
</item>
<item>
<title>PVChat：个性化视频大语言模型的单次学习框架</title>
<link>https://arxiv.org/abs/2503.17069</link>
<guid>https://arxiv.org/abs/2503.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PVChat 提供了一种个性化视频理解的新框架，支持单一视频的身份感知问答。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PVChat，这是一种新型的个性化视频大语言模型（ViLLM），旨在提升视频理解中的身份感知能力。现有的ViLLMs在一般视频理解上表现出色，但在处理如“威尔森正在接受化疗”或“汤姆与莎拉讨论”这类身份相关的理解任务时存在不足，限制了其在智能医疗和智能家居中的应用。PVChat通过单次学习的框架，结合Mixture-of-Heads (MoH)增强模型和自动化增强管道，生成多样化的训练数据集，涵盖存在、外观、动作和位置查询四种QA类型。此外，PVChat的改进措施包括ReLU Routing MoH注意机制及两种新目标：平滑邻域正则化和头部激活增强。通过在多个数据集上的评估，PVChat显示出在个性化特征理解方面的优势，较现有最先进的ViLLMs具备更高的学习能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 07:50:06 GMT</pubDate>
</item>
<item>
<title>ETVA：一种新的文本到视频对齐评估方法</title>
<link>https://arxiv.org/abs/2503.16867</link>
<guid>https://arxiv.org/abs/2503.16867</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ETVA通过细粒度问答方法提升文本到视频对齐评估的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的文本到视频对齐评估方法ETVA，旨在解决现有度量方法在细粒度对齐方面的不足。ETVA通过多agent系统，将输入的提示解析为语义场景图，并生成原子问题。接着，设计了一个知识增强的多阶段推理框架，辅助的大型语言模型（LLM）首先检索相关常识知识，然后由视频LLM通过多阶段推理机制回答生成的问题。实验表明，ETVA的斯皮尔曼相关系数达到58.47，远高于现有度量方法的31.0，显示出与人类判断之间的显著相关性。此外，我们构建了一个专门用于文本到视频对齐评估的全面基准，包含2000个多样化的提示和12000个跨10个类别的原子问题。通过对15种现有文本到视频模型的系统评估，我们识别了它们的关键能力和局限性，为下一代T2V生成奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16867" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 01:52:50 GMT</pubDate>
</item>
<item>
<title>基于特征效用评估的视觉编码器优化方法</title>
<link>https://arxiv.org/abs/2503.16660</link>
<guid>https://arxiv.org/abs/2503.16660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法以优化视觉编码器，减少计算成本而不损失质量。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉编码器生成的丰富标记在计算中的价值，并提出了一种新方法来评估特征的效用，旨在减少不必要的计算负担。新方法结合自编码器和Gumbel-Softmax选择机制，以识别和保留最具信息量的视觉标记。研究表明，在OCR任务中，利用该方法能够在保持性能的前提下，去除超过50%的视觉上下文；而随机去除同样比例的特征则会显著影响模型能力。此外，在一般领域的任务中，即使随机保留30%的标记，其性能也可与使用全套视觉标记相比拟。这一结果为自适应高效的多模态特征选择和模型优化提供了新的方向，助力更具可扩展性和低开销的推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16660" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 15:17:08 GMT</pubDate>
</item>
<item>
<title>MathFlow：提升多模态大语言模型视觉数学问题解决能力的框架</title>
<link>https://arxiv.org/abs/2503.16549</link>
<guid>https://arxiv.org/abs/2503.16549</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MathFlow，以提高多模态模型在视觉数学问题中的表现。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLMs）在各种任务中表现出色，但在视觉数学问题的解决能力上，特别是在准确感知和理解图表方面，仍有待提高。我们假设，从图表中提取有意义信息的感知能力对后续推理过程至关重要。为此，我们开发了FlowVerse，一个综合基准，用以评估在问题解决中使用的所有信息。初步结果显示，现有的MLLMs在从图表中提取关键信息和进行复杂推理方面存在显著限制。针对这一问题，我们提出了一个模块化的解决管道MathFlow，将感知和推理分解为独立的阶段，并训练了专为感知设计的MathFlow-P-7B模型。实验结果表明，MathFlow-P-7B在与多种推理模型集成时，显著提升了性能，展示了该管道的有效性及其与多种推理框架的兼容性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16549" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 07:46:19 GMT</pubDate>
</item>
<item>
<title>针对长尾问题的自适应数据精炼框架在大视觉语言模型中的应用</title>
<link>https://arxiv.org/abs/2503.12821</link>
<guid>https://arxiv.org/abs/2503.12821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自适应数据精炼框架，缓解LVLM中的长尾问题。</p><br /><br /><p><strong>摘要：</strong> 本文分析了大视觉语言模型（LVLM）中的长尾（LT）问题，识别出主要原因为头部概念过度代表和尾部概念不足。针对这种不平衡的数据分布，提出了一个自适应数据精炼框架（ADR），包括数据重平衡（DR）和数据合成（DS）两个阶段。在DR阶段，根据实体分布适应性地重新平衡冗余数据；在DS阶段，利用去噪扩散概率模型（DDPMs）和稀缺图像，补充不足的部分。通过在十一项基准测试中的综合评估，ADR有效缓解了训练数据的长尾问题，LLaVA 1.5的平均表现提高了4.36%，且未增加训练数据量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 01:01:09 GMT</pubDate>
</item>
<item>
<title>人工智能中的隐性偏见：通过推理模型隐性联想测试的研究</title>
<link>https://arxiv.org/abs/2503.11572</link>
<guid>https://arxiv.org/abs/2503.11572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，推理模型在处理信息时显示出类似隐性偏见的模式。</p><br /><br /><p><strong>摘要：</strong> 隐性偏见是指自动的心理过程，这些过程影响我们的感知、判断和行为。与人类研究不同，先前对大语言模型（LLMs）中隐性偏见的研究主要集中于模型输出，而非模型处理过程。为此，本文提出了一种名为推理模型隐性联想测试（RM-IAT）的方法，用于研究推理模型中的隐性偏见模式。研究发现，推理模型在处理关联不兼容的信息时需要更多的tokens，这表明人工智能系统在信息处理上存在与人类隐性偏见相似的模式。这些发现对于AI系统在现实应用中的部署具有重要意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11572" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:40:02 GMT</pubDate>
</item>
<item>
<title>深度视觉语言模型OpenVLThinker的推理能力提升研究</title>
<link>https://arxiv.org/abs/2503.17352</link>
<guid>https://arxiv.org/abs/2503.17352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，OpenVLThinker在复杂推理任务中表现出色。</p><br /><br /><p><strong>摘要：</strong> 本研究基于DeepSeek-R1的最新成果，探讨将复杂推理能力融入大型视觉语言模型（LVLMs）的可行性，并评估其在多模态推理任务中的影响。研究采用逐步监督微调（SFT）和强化学习（RL）相结合的方法，首先从纯文本R1模型中提炼出推理能力，通过多样化视觉数据集中的高质量图像标题生成推理步骤。然后，经过迭代的RL训练不断提升推理技能，每轮RL改进的模型生成精炼的SFT数据集供下一轮使用。最终，推出的OpenVLThinker在MathVista、MathVerse和MathVision等挑战性基准测试中展现了显著的推理性能提升，证明了本策略在增强视觉语言推理中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 13:52:43 GMT</pubDate>
</item>
<item>
<title>FastCuRL：高效的课程强化学习方法提升推理模型性能</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FastCuRL，通过扩展上下文窗口加速推理模型训练，提升性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FastCuRL，一种简单而高效的课程强化学习方法，旨在提高R1类推理模型在处理复杂推理任务时的训练效率，特别是在拥有15亿参数的语言模型上。FastCuRL包括两个主要过程：长度感知的训练数据分段和上下文窗口的扩展训练。具体而言，前者首先根据输入提示长度将原始训练数据分为三个不同的层次，后者则利用分段的训练数据集，通过逐步增加上下文窗口的长度来训练推理模型。实验结果表明，FastCuRL-1.5B-Preview在五个数据集（包括MATH 500、AIME 2024、AMC 2023、Minerva Math和OlympiadBench）上均优于DeepScaleR-1.5B-Preview，同时训练步骤仅使用50%。此外，FastCuRL-1.5B-Preview的所有训练阶段均在单节点8 GPU的环境下完成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 12:35:31 GMT</pubDate>
</item>
<item>
<title>提升创意写作生成的多样性与质量</title>
<link>https://arxiv.org/abs/2503.17126</link>
<guid>https://arxiv.org/abs/2503.17126</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了如何在创意写作生成中提高输出的多样性与质量。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何在创意写作任务中提升输出的多样性及质量，探讨了后训练方法的有效性。核心思想是将偏差概念融入训练目标，这样可以通过学习高质量的稀有实例来促进模型的多样性。采用直接偏好优化（DPO）和几率比偏好优化（ORPO）的方法，我们的模型在输出多样性上达到了与人类创作数据集相媲美的水平，同时保持与最佳指令调优模型相似的输出质量。研究中还通过人类评估、消融实验以及与现有多样性方法DivPO的比较来验证所提出的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.17126" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 09:21:45 GMT</pubDate>
</item>
<item>
<title>VCtrl：提升视频生成中的细粒度控制能力</title>
<link>https://arxiv.org/abs/2503.16983</link>
<guid>https://arxiv.org/abs/2503.16983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VCtrl框架提升了视频生成过程中的细粒度控制与质量。</p><br /><br /><p><strong>摘要：</strong> 在视频生成研究中，尽管文本到视频生成取得了显著进展，但对细粒度时空属性的精确和灵活控制仍是一个未解决的重要挑战。为应对这些限制，本文提出了VCtrl（也称为PP-VCtrl）这一新框架，旨在以统一的方式实现对预训练视频扩散模型的细粒度控制。VCtrl通过一个通用的条件模块，将多种用户指定的控制信号（如Canny边缘、分割掩码和人体关键点）集成到预训练的视频扩散模型中，而无需修改底层生成器。此外，设计了一个统一的控制信号编码管道以及一种稀疏残差连接机制，以高效地整合控制表示。全面的实验和人类评估显示，VCtrl有效提高了可控性和生成质量，源代码和预训练模型可在PaddlePaddle框架下公开获得。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 05:48:00 GMT</pubDate>
</item>
<item>
<title>基于适应性DPO的图像生成模型偏好数据研究</title>
<link>https://arxiv.org/abs/2503.16921</link>
<guid>https://arxiv.org/abs/2503.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨适应性DPO在图像生成模型训练中的作用。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像生成领域的进展显著，特别是在调整模型以符合人类普遍偏好的方法上。本文研究了偏好数据在扩散模型训练中的关键作用，特别是Diffusion-DPO及其后续适应情况。我们探讨了图像生成中普遍人类偏好的复杂性，强调了这些偏好的主观特点及偏好数据集中少数样本所带来的挑战。通过试验，我们证明了少数样本的存在及其对模型性能的负面影响。为此，我们提出了一种新方法Adaptive-DPO，采用了少数样本意识度量来优化DPO目标函数。此度量结合了标注者内部信心和标注者间稳定性，区分了多数和少数样本，最终引入了Adaptive-DPO损失函数。此方法不仅增强了模型对多数标签的学习能力，同时也减轻了少数样本的负面影响，通过实验验证了其对合成少数数据和真实偏好数据的有效处理，推动了图像生成任务中更有效的训练方法的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:33:44 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的多模态科学问题解决策略</title>
<link>https://arxiv.org/abs/2503.16905</link>
<guid>https://arxiv.org/abs/2503.16905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于多智能体框架的解决策略，显著提高多模态科学问题的处理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态科学问题（MSPs）的复杂性，提出了一种基于多智能体框架的解决策略。传统科学问题的解决尽管已有进展，但MSPs在多模态综合推理和反思能力方面仍面临挑战。为此，我们引入了Big Seven Personality与苏格拉底指导原则（MAPS），利用七个不同的智能体通过反馈机制和苏格拉底方法来引导问题解决。我们提出四种智能体的逐步解决策略，聚焦于问题解决过程的不同阶段，并引入批判智能体，激发批判性思维和自主学习。通过在EMMA、Olympiad和MathVista数据集上的大量实验证明，该方法在所有任务上较现有最优模型提升了15.84%的表现，同时也验证了模型的进步及其泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 03:13:45 GMT</pubDate>
</item>
<item>
<title>基于多智能体框架的自动化提示优化方法</title>
<link>https://arxiv.org/abs/2503.16874</link>
<guid>https://arxiv.org/abs/2503.16874</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MARS框架，通过多代理系统优化提示，提高问答模型的响应质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MARS（多智能体框架结合苏格拉底指导）的自动化提示优化方法，以解决传统手动设计提示的认知偏见和现有方法在提示空间搜索中的灵活性不足等问题。MARS框架包含七个功能各异的智能代理，通过规划者自主设计优化路径，实现了优化过程的灵活性。此外，采用教师-批评者-学生的苏格拉底对话模式，MARS能够迭代优化提示并进行有效的搜索。我们在多个数据集上进行了广泛实验，以验证该方法的有效性，并进行附加分析实验以评估模型的进展和可解释性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16874" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 21 Mar 2025 02:19:55 GMT</pubDate>
</item>
<item>
<title>TokenBridge：结合离散与连续Token的视觉生成模型</title>
<link>https://arxiv.org/abs/2503.16430</link>
<guid>https://arxiv.org/abs/2503.16430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TokenBridge，通过后训练量化优化视觉生成模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视觉生成模型TokenBridge，旨在解决离散和连续Token表示之间的矛盾。离散Token虽然易于用标准交叉熵损失进行建模，但却存在信息丢失和训练不稳定的问题；而连续Token虽然能更好地保留视觉细节，却需要复杂的分布建模，增加了生成管道的复杂性。TokenBridge通过后训练量化将离散化与tokenizer训练过程解耦，使得可以从连续表示中直接获得离散Token。具体来说，采用维度独立的量化策略，将每个特征维度独立离散化，并配合轻量级的自回归预测机制，有效建模生成的大Token空间。实验结果表明，该方法在重建和生成质量上与连续方法相当，同时采用标准的分类预测。此研究展示了离散与连续模型的结合能有效发挥两者的优势，为高质量视觉生成开辟了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>多智能体系统的组合约束与自动数据收集框架设计</title>
<link>https://arxiv.org/abs/2503.16408</link>
<guid>https://arxiv.org/abs/2503.16408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出组合约束与自动化数据收集，推动多智能体系统发展。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对多智能体系统的组合约束概念，解决了现有方法在生成安全高效训练数据时面临的挑战。通过设计针对不同类型约束的多个接口，实现了与物理世界的无缝互动。基于这一理念，开发了一种自动数据收集框架，并推出了首个多智能体操作基准RoboFactory。此外，本文探讨了多智能体模仿学习的架构与训练策略，以期建立安全且高效的多智能体系统。在RoboFactory基准上，适配并评估了模仿学习方法，分析了其在不同难度任务中的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:58:38 GMT</pubDate>
</item>
<item>
<title>GASP：自主驾驶中的几何与语义自监督预训练方法</title>
<link>https://arxiv.org/abs/2503.15672</link>
<guid>https://arxiv.org/abs/2503.15672</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GASP方法通过4D占据预测提升了自主驾驶性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GASP的几何与语义自监督预训练方法，旨在通过预测任意未来时空点的占据信息来学习环境的统一表示。GASP主要关注三个方面：一般占据，以捕捉3D场景的演变结构；自我占据，建模自我车辆在环境中的路径；以及从视觉基础模型中提炼的高级特征。通过建模几何和语义4D占据场，而非原始传感器测量，该模型能够学习环境及其随时间演变的结构化、可推广的表示。我们在多个自主驾驶基准测试中验证了GASP，结果显示在语义占据预测、在线地图构建和自我轨迹预测方面显著提高。这一研究展示了连续的4D几何与语义占据预测为自主驾驶提供了一种可扩展和有效的预训练范式。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15672" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 16:00:27 GMT</pubDate>
</item>
<item>
<title>基于深度学习的代码补全工具的组织和开发者特定微调研究</title>
<link>https://arxiv.org/abs/2503.14201</link>
<guid>https://arxiv.org/abs/2503.14201</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了深度学习代码补全工具在组织和开发者特定微调中的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于深度学习的代码补全工具通过针对特定组织或开发者的微调提高性能的可能性。通过对来自两家组织（Apache和Spring）的136名开发者进行实验，比较了两种模型架构（T5和Code Llama）和三种模型规模（60M, 750M和7B可训练参数）的表现。结果表明，组织特定和开发者特定的微调显著提高了预测能力，尤其是组织特定微调显示出更强的性能。这一发现适用于不同组织和不同规模的模型。此外，组织特定数据集微调的深度学习模型，其完成性能达到了比起使用更大预训练模型的相同效果，从而在部署和推理成本上实现节约，降低了对GPU资源的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14201" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 08:26:06 GMT</pubDate>
</item>
<item>
<title>Where do Large Vision-Language Models Look at when Answering Questions?</title>
<link>https://arxiv.org/abs/2503.13891</link>
<guid>https://arxiv.org/abs/2503.13891</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, their visual understanding behaviors remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods (e.g., iGOS++) to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated answers and input image. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data are available at https://github.com/bytedance/LVLM_Interpretation.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 00:34:43 GMT</pubDate>
</item>
<item>
<title>DeCapBench与DCScore：细节图像标注的新标准</title>
<link>https://arxiv.org/abs/2503.07906</link>
<guid>https://arxiv.org/abs/2503.07906</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍DeCapBench和DCScore，提升细节图像标注评估效果。</p><br /><br /><p><strong>摘要：</strong> 图像标注一直是视觉理解的重要任务，近期视觉语言模型(VLM)的发展显著提升了图像标注的能力。然而，细节图像标注的评估仍存在不足，主要由于评价指标过时和注释粗糙。本文介绍了DeCapBench及其新指标DCScore，专为细节标注任务设计，DCScore通过将响应拆解为最小的自洽单元（基础信息单元）来评估幻觉现象和细微的全面性。实验表明，DCScore与人类判断的吻合度高于其他评估指标。同时，DeCapBench在描述性任务中的表现与VLM竞技场结果高度相关，超越了现有的视觉语言模型基准。此外，文章还提出了一种自动细粒度反馈收集方法FeedQuill，基于新指标进行偏好优化，展现出强大的泛化能力。通过对多种VLM的广泛实验，证明该方法显著减少了幻觉现象，并在各项基准测试中提升了性能，达到优越的细节图像标注表现，超越了GPT-4o。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07906" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:53:56 GMT</pubDate>
</item>
<item>
<title>Sonata：高效自监督点云学习模型的创新与应用</title>
<link>https://arxiv.org/abs/2503.16429</link>
<guid>https://arxiv.org/abs/2503.16429</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示了Sonata模型在3D任务中实现高效自监督学习的潜力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了自监督点云模型Sonata在多样化三维任务中的有效性，尤其在数据有限与计算资源最小的情况下。我们发现现有3D自监督学习方法在表示质量方面存在不足，原因被称为“几何捷径”，它导致表示仅限于低级空间特征。为了解决这一问题，我们提出了两项关键策略：遮蔽空间信息和增强对输入特征的依赖。Sonata模型通过自蒸馏学习构建，具有直观性和简易性，学习到的表示在零样本可视化中显示出语义分组和强大的空间推理能力，在ScanNet上相比以往方法提高了线性探测准确率，从21.8%提升至72.5%，并且在仅使用1%数据的情况下几乎实现了性能翻倍。此外，全面微调进一步提升了多维场景感知任务的最先进性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16429" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>基于文本描述的3D世界生成方法SynCity</title>
<link>https://arxiv.org/abs/2503.16420</link>
<guid>https://arxiv.org/abs/2503.16420</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SynCity是一种基于文本描述生成3D世界的高质量方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SynCity的方法，用于从文本描述生成3D世界。与传统的以物体为中心的3D生成模型不同，SynCity结合了预训练的3D生成模型的几何精度和2D图像生成器的艺术性，以无训练和优化的方式创造大型高质量的3D空间。通过瓦片式的生成方法，SynCity实现了对场景布局和外观的细粒度控制，允许逐块生成世界，并将每个新生成的瓦片在已有场景的上下文中融合。这种方法所生成的场景丰富且具有吸引力，展示了细节和多样性，解决了大规模3D世界生成的挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16420" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型的事实知识编码能力</title>
<link>https://arxiv.org/abs/2503.15299</link>
<guid>https://arxiv.org/abs/2503.15299</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型内部编码的知识超过其外部表达的知识。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种框架，用于评估大型语言模型（LLMs）是否在其参数中编码了比输出中表现出的更多的事实知识。文章定义了知识，并通过正确与错误答案对的比例量化知识，区分了外部知识和内部知识。通过对三款流行的开放权重LLMs进行案例研究，结果显示：首先，LLMs内部编码的知识比其外部表达的知识平均高出40%；其次，有些知识深埋于内部，模型可能完全知道答案，但在生成时却从未输出，这表明LLMs在生成能力上存在根本限制；最后，这为在闭卷问答中的重复答案采样提出了实际限制，因为某些答案几乎从未被采样，从而提高性能的机会被阻碍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15299" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:21:48 GMT</pubDate>
</item>
<item>
<title>PORTAL：一种新型AI框架实现多3D游戏智能代理</title>
<link>https://arxiv.org/abs/2503.13356</link>
<guid>https://arxiv.org/abs/2503.13356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PORTAL框架通过语言引导生成策略，实现AI在多3D游戏中的应用。</p><br /><br /><p><strong>摘要：</strong> PORTAL是一种新型框架，通过语言引导策略生成，开发能够在数千款3D视频游戏中进行决策的人工智能代理。该框架将决策问题转化为语言建模任务，利用大型语言模型（LLMs）生成以领域特定语言（DSL）表示的行为树。PORTAL方法消除了传统强化学习方法的计算负担，同时保留了战术深度和快速适应能力。它引入了一种混合策略结构，将基于规则的节点与神经网络组件相结合，支持高水平的战略推理和精确的低层次控制。此外，双重反馈机制通过量化游戏指标和视觉-语言模型分析促进策略在战术和战略层面的迭代改进。实验结果表明，PORTAL在数千款第一人称射击游戏中的有效性，表现出开发效率、策略泛化和行为多样性方面的显著提升，代表了游戏人工智能开发的重大进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:42:34 GMT</pubDate>
</item>
<item>
<title>TikZero：从图像到图形程序的文本驱动生成</title>
<link>https://arxiv.org/abs/2503.11509</link>
<guid>https://arxiv.org/abs/2503.11509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TikZero利用图像作为中介，将图形程序生成与文本理解解耦。</p><br /><br /><p><strong>摘要：</strong> 随着生成式人工智能的发展，从文本描述合成图形成为一种引人注目的应用。然而，实现高几何精度和可编辑性需要将图形表示为图形程序（如TikZ），而对齐的训练数据（即图形程序与其描述）仍然较为稀缺。为解决这一问题，本文提出了TikZero，通过使用图像表示作为中介，解耦图形程序生成与文本理解。该方法允许独立训练图形程序和带说明图片，并支持在推理阶段进行零样本文本驱动的图形程序合成。实验表明，TikZero在没有对齐的图形程序的情况下显著超越了只能利用对齐图形程序的基准模型。此外，当结合对齐图形程序作为补充训练信号时，TikZero的表现能与更大规模的模型相媲美，甚至超越商业系统如GPT-4o。我们的代码、数据集及部分模型已公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:29:58 GMT</pubDate>
</item>
<item>
<title>利用多模态大语言模型评估AI生成视频的有效性</title>
<link>https://arxiv.org/abs/2503.09949</link>
<guid>https://arxiv.org/abs/2503.09949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型在AI生成视频评估中的应用及效果。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成模型的快速发展，建立可靠的自动评估指标显得尤为重要。现有方法多依赖于其他任务优化的现成模型或人类评估数据，难以满足快速增长的评估需求。为此，本文探讨了使用多模态大语言模型（MLLMs）作为统一评估器的可行性，借助其强大的视觉感知和语言理解能力。我们提出了UVE-Bench基准，收集了最新VGMs生成的视频，并在15个评估方面提供了人类偏好标注。通过UVE-Bench，我们对16个MLLMs进行了广泛评估，结果表明，尽管先进的MLLMs在效果上仍不及人类评估者，但相比现有专用评估方法，其在统一AIGV评估中显示了显著的潜力。此外，我们深入分析了影响MLLM驱动评估器性能的关键设计选择，为今后的研究提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 21:52:27 GMT</pubDate>
</item>
<item>
<title>基于集合代币化的图像生成新范式</title>
<link>https://arxiv.org/abs/2503.16425</link>
<guid>https://arxiv.org/abs/2503.16425</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新图像生成范式，改进了代币表示和分布建模。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种全新的图像生成范式，通过集合代币化和分布建模实现图像生成。与传统的将图像序列化为固定位置潜在编码的方式不同，本文引入了无序代币集合表示，能够根据区域语义复杂性动态分配编码容量。该TokenSet增强了全局上下文聚合，并提高了对局部扰动的鲁棒性。为了解决建模离散集合的关键挑战，我们设计了一种双重转换机制，将集合双射地转换为具有求和约束的固定长度整数序列。此外，本文提出的固定和离散扩散框架是首个能够同时处理离散值、固定序列长度和求和不变性的模型，能够有效进行集合分布建模。实验结果表明，本文方法在语义感知表示和生成质量上优于传统方法。这些创新的表示和建模策略为视觉生成的发展提供了新的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16425" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>SwD：扩展的扩散模型蒸馏框架</title>
<link>https://arxiv.org/abs/2503.16397</link>
<guid>https://arxiv.org/abs/2503.16397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SwD框架通过逐级预测有效缩短扩散模型的推理时间。</p><br /><br /><p><strong>摘要：</strong> SwD是一个针对扩散模型(DMs)的规模蒸馏框架，灵感来源于扩散过程与隐式谱自回归之间的关系。SwD提出在较低数据分辨率下初始化生成，并在去噪步骤中逐步上升样本质量，而不损失性能，显著降低计算成本。该框架将这一思想自然融入现有基于分布匹配的扩散蒸馏方法中，并通过引入新的补丁损失，增加了分布匹配方法的细粒度相似性。在应用于先进的文本-图像扩散模型时，SwD在仅有两次全分辨率步骤的推理时间内显著超越同等计算预算的其他方法，得到了自动化指标和人类偏好研究的支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:54:02 GMT</pubDate>
</item>
<item>
<title>VidKV：一种新型的低位数KV缓存量化方法用于视频大语言模型</title>
<link>https://arxiv.org/abs/2503.16257</link>
<guid>https://arxiv.org/abs/2503.16257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VidKV方法，通过低位数量化提高VideoLLMs的效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种针对视频大语言模型（VideoLLMs）的KV缓存量化新方法——VidKV。随着视频输入长度的增加，KV缓存的内存需求显著上升，影响推理速度。研究发现，2位KV量化对模型性能影响很小，而更低位数的量化尚未深入探讨。VidKV通过采用混合精度量化策略，对不同行的通道实施不同位数的量化，同时针对性保留语义重要的视觉标记，进而压缩KV缓存至1.5位和1.58位精度，且几乎不影响性能。大量实验表明，VidKV比传统方法在保持精度的同时，提高了缓存效率，为视频分析中的推理速度提升提供了新路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:52:43 GMT</pubDate>
</item>
<item>
<title>机器智能驱动的药物依从性预测与干预系统</title>
<link>https://arxiv.org/abs/2503.16091</link>
<guid>https://arxiv.org/abs/2503.16091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AIMI系统通过机器智能促进药物依从性预测与干预。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为AIMI的药物依从性预测系统，它利用智能手机传感器和患者的用药历史来估算患者忘记服药的可能性。研究中，27名每天服用药物管理心血管疾病的参与者参与了用户研究。研究团队设计并开发了基于卷积神经网络（CNN）和长短期记忆网络（LSTM）的预测模型，经过不同输入特征组合的实验发现，LSTM模型在药物依从性预测上的准确率达到了93.2%，F-1得分为93.6%。通过一系列的消融研究，证明了利用未来已知信息和个性化训练显著提高了药物依从性预测的准确性，填补了基于可穿戴传感器的依从性预测系统的空白。这一系统的代码可在GitHub上找到。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 08:32:35 GMT</pubDate>
</item>
<item>
<title>VideoRFSplat：一种直接的文本到3D模型生成方法</title>
<link>https://arxiv.org/abs/2503.15855</link>
<guid>https://arxiv.org/abs/2503.15855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoRFSplat通过视频生成模型直接生成高质量3D场景。</p><br /><br /><p><strong>摘要：</strong> VideoRFSplat是一种新的文本到3D生成模型，它利用视频生成模型实现对无界真实场景的3D高斯点云生成。过往方法在2D生成模型与相机姿态和多视图图像的联合建模时面临不稳定性，因此需要额外模型来稳定训练与推理。本文提出了一种双流架构，将专用的姿态生成模型与预训练视频生成模型结合，分别通过独立通道生成多视图图像和相机姿态，从而减少两种模态间的干扰。我们还提出了一种异步采样策略，使相机姿态去噪速度快于多视图图像，从而利用快速去噪的姿态条件多视图生成，减少互相模糊，提高跨模态一致性。在多个大规模真实场景数据集上进行训练后，VideoRFSplat在无需后处理细化的情况下，优于现有的文本到3D生成方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:26:09 GMT</pubDate>
</item>
<item>
<title>BigO(Bench)：评估生成模型理解和生成代码复杂性的基准</title>
<link>https://arxiv.org/abs/2503.15242</link>
<guid>https://arxiv.org/abs/2503.15242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍BigO(Bench)基准，评估生成模型的代码复杂性理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BigO(Bench)，一个新型编码基准，旨在评估生成语言模型理解和生成具有特定时间与空间复杂度代码的能力。该基准填补了当前评估中忽视模型在计算复杂度限制下理解和生成代码能力的空白。BigO(Bench)包括工具，可以从分析测量中推断任何Python函数的算法复杂度，包含3150道编码问题及1190250个标注有复杂度标签的解法，还提供了不同输入规模下的运行时和内存占用值。通过评估多种最先进的语言模型，本文突出了它们在处理复杂度要求方面的优缺点，尤其是token-space推理模型在代码生成上无与伦比，但在复杂性理解方面表现平平，提示其可能无法很好地泛化到未在训练中给予奖励的任务上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:19:57 GMT</pubDate>
</item>
<item>
<title>优化视频训练方法的令牌选择与增强工具Flux</title>
<link>https://arxiv.org/abs/2503.14237</link>
<guid>https://arxiv.org/abs/2503.14237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了Flux工具，优化视频训练中的令牌选择，提升模型性能并节省计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视频训练设置，称为令牌优化，通过从更合适的采样视频中选择输入令牌，最大化有限输入信息，从而提高模型在不同计算预算下的表现。为此，提出了一种名为Flux的创新增强工具，使得采样网格灵活，并能轻松集成到主流视频训练框架中，实现模型的增强而几乎不增加额外成本。在大规模视频预训练中集成Flux后，FluxViT在多个任务上取得了新的最先进的成绩。值得注意的是，仅使用1/4的令牌，FluxViT仍能匹配之前的最先进模型的性能，实现近90%的节省。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 09:15:58 GMT</pubDate>
</item>
<item>
<title>RSD: 一种加速超分辨率扩散模型的新型蒸馏方法</title>
<link>https://arxiv.org/abs/2503.13358</link>
<guid>https://arxiv.org/abs/2503.13358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RSD是一种高效的超分辨率模型蒸馏方法，提升了图像质量和计算效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的蒸馏方法RSD，用于加速超分辨率扩散模型ResShift，实现了单步图像恢复，且在多个数据集上优于教师模型。尽管现有的加速方法面临着生成不真实细节或幻觉结构的问题，RSD通过训练学生网络生成与教师模型一致的图像，有效克服了这些不足。实验结果表明，RSD的性能与最先进的扩散模型蒸馏方法相当，且在预训练的文本到图像模型基础上的超分辨率方法中也展现出竞争力，具有更好的图像质量、对退化输入图像的对齐效果，并且所需参数和GPU内存更少。我们在多个真实和合成数据集上进行了实验验证，包括RealSR、RealSet65、DRealSR、ImageNet和DIV2K。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:44:08 GMT</pubDate>
</item>
<item>
<title>LLM驱动代理的评估方法综述</title>
<link>https://arxiv.org/abs/2503.16416</link>
<guid>https://arxiv.org/abs/2503.16416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述LLM驱动代理的评估方法，分析其能力与应用基准。</p><br /><br /><p><strong>摘要：</strong> 本文对基于大型语言模型（LLM）的智能代理的评估方法进行了全面综述，探讨了这些代理在动态环境中进行规划、推理及工具使用的能力。文章系统分析了四个关键维度的评估基准与框架，包括基础能力、特定应用的基准（如网络、软件工程、科学和对话代理）、通用代理的基准和评估框架。分析结果揭示了新兴趋势，如向更具挑战性和现实性的评估的转变。本文还指出了当前研究中的关键缺口，尤其是在成本效益、安全性和鲁棒性等方面的评估，以及开发细粒度和可扩展评估方法的需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:23 GMT</pubDate>
</item>
<item>
<title>高效生成多样化户外场景的方法研究</title>
<link>https://arxiv.org/abs/2503.16375</link>
<guid>https://arxiv.org/abs/2503.16375</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种高效的户外场景生成方法，以应对多样化挑战。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种生成多样化户外场景的新方法，涵盖从城堡到高楼等多种场景。与室内场景生成侧重于不同的研究目标相比，户外场景生成面临高度变化和快速生成大规模景观的挑战。为此，提出了一种高效的编码方法，将场景块编码为统一的向量集，比以往的空间结构潜变量在压缩和性能上更具优势。此外，还训练了一个显式的扩展模型，能够快速生成无限场景，提升了生成一致性，并通过消除多余的扩散步骤，加速生成过程。为支持这一任务，本文还策划了NuiScene43，这是一个小巧但高质量的场景数据集，经过预处理以便进行联合训练。值得注意的是，经过不同风格场景的训练后，模型能够将乡村住宅和城市摩天大楼等不同环境融合在同一场景中，展示了我们策划过程在异构场景联合训练中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16375" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:37:43 GMT</pubDate>
</item>
<item>
<title>小型语言模型的强化学习推理能力提升研究</title>
<link>https://arxiv.org/abs/2503.16219</link>
<guid>https://arxiv.org/abs/2503.16219</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习能显著提升小型语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了在资源限制环境下，利用强化学习（RL）提升小型语言模型推理能力的潜力。研究对象为1.5亿参数的模型DeepSeek-R1-Distill-Qwen-1.5B，训练限制为使用4个NVIDIA A40 GPU（每个48 GB VRAM），并在24小时内完成。通过调整Group Relative Policy Optimization（GRPO）算法及精心策划高质量的数学推理数据集，进行了三次实验，结果显示推理能力迅速提升，如AMC23准确率从63%上升至80%，AIME24达到46.7%。相比传统模型几千美元的训练成本，我们的方案仅需420美元，且采用仅7000个样本。然而，长时间训练中出现了优化不稳定和长度限制等挑战。这些发现展示了基于强化学习的微调在小型语言模型中的有效性，提供了一种高性价比的替代方案。我们还提供了代码和数据集，作为开源资源，助力在资源有限的环境下开发具备推理能力的大型语言模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16219" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:13:23 GMT</pubDate>
</item>
<item>
<title>Race-DiT: 一种新型混合专家模型在扩散变换器中的应用</title>
<link>https://arxiv.org/abs/2503.16057</link>
<guid>https://arxiv.org/abs/2503.16057</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Race-DiT模型，提升扩散变换器的性能和可扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Race-DiT，一种创新的混合专家（MoE）模型，旨在提高扩散变换器的可扩展性和性能。通过引入灵活的路由策略——Expert Race，模型允许令牌和专家共同竞争，选择最佳候选，以动态分配专家给关键令牌。此外，提出了每层正则化来解决浅层学习的挑战，并引入路由器相似度损失以防止模式崩溃，从而确保更好的专家利用率。通过在ImageNet上进行的广泛实验，验证了该方法的有效性，展示了显著的性能提升和良好的扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16057" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:45:08 GMT</pubDate>
</item>
<item>
<title>SALT: 结合低秩变换的奇异值适应医学图像分割方法</title>
<link>https://arxiv.org/abs/2503.16055</link>
<guid>https://arxiv.org/abs/2503.16055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SALT方法在医学图像分割中有效地适应奇异值以提升性能。</p><br /><br /><p><strong>摘要：</strong> 医学图像分割需要专门设计的模型来捕捉细致的领域特征。尽管大型基础模型提供了灵活性，但细调成本仍是显著障碍。本文提出的SALT方法结合了低秩适应和奇异值分解（SVD）技术，能够选择性地适应影响最大的奇异值，并对剩余子空间进行低秩更新。经过在5个具有挑战性的医学数据集上的评估，SALT比当前先进的PEFT方法（包括LoRA和SVD）提高了2%到5%的Dice系数，同时仅需3.9%的可训练参数，显示出在低资源环境中的强大适应能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 07:42:41 GMT</pubDate>
</item>
<item>
<title>Zero-1-to-A：提高4D可动画化头像生成质量的方法</title>
<link>https://arxiv.org/abs/2503.15851</link>
<guid>https://arxiv.org/abs/2503.15851</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Zero-1-to-A，提高4D头像生成的一致性及质量。</p><br /><br /><p><strong>摘要：</strong> Animatable head avatar generation traditionally需要大量训练数据。为降低数据需求，本文提出Zero-1-to-A方法，通过视频扩散模型合成具有空间和时间一致性的4D头像。该方法迭代构建视频数据集，采用两阶段的渐进式学习：首先进行空间一致性学习，从正面到侧面固定表情；其次进行时间一致性学习，从放松表情到夸张表情固定视角。这种方法显著提高了生成头像的真实感、动画质量和渲染速度。实验结果表明，Zero-1-to-A在多项指标上优于现有的扩散方法，提供了创建栩栩如生的头像的有效解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15851" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 01:07:46 GMT</pubDate>
</item>
<item>
<title>MotionStreamer: 基于文本的流式动作生成新框架</title>
<link>https://arxiv.org/abs/2503.15451</link>
<guid>https://arxiv.org/abs/2503.15451</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionStreamer框架，解决流式动作生成中的信息损失和错误累积问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了文本条件流式动作生成中的挑战，特别是如何在变长历史动作和输入文本的基础上预测下一步人类姿态。现有方法在流式动作生成中存在不足，如扩散模型被限制在预定义的动作长度，而基于GPT的方法由于离散化的非因果标记化导致响应延迟和错误累积。为了解决这些问题，本文提出了MotionStreamer，一个将连续因果潜在空间引入概率自回归模型的新框架。连续潜在变量减轻了因离散化造成的信息损失，有效减少了长期自回归生成中的错误累积。此外，通过建立当前和历史动作潜在变量之间的时间因果依赖关系，我们的模型充分利用可用信息，实现准确的在线动作解码。实验结果表明，我们的方法在多个方面优于现有方法，包括多轮生成、长期生成和动态动作合成等应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15451" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:32:24 GMT</pubDate>
</item>
<item>
<title>DiffMoE：提升扩散模型图像生成能力的新方法</title>
<link>https://arxiv.org/abs/2503.14487</link>
<guid>https://arxiv.org/abs/2503.14487</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffMoE通过专家访问机制优化扩散模型的图像生成性能。</p><br /><br /><p><strong>摘要：</strong> DiffMoE是一种创新的方法，旨在改善扩散模型在不同条件和噪声水平下的图像生成性能。该方法引入了一种批级全局令牌池，使得专家能够在训练中访问全局令牌分布，从而促进专业化的专家行为。此外，DiffMoE还融入了一个动态容量预测器，根据噪声水平和样本复杂性动态分配计算资源。通过全面评估，DiffMoE在ImageNet基准上取得了业界领先的表现，相较于传统的稠密架构，即便激活参数数量相同，其性能显著提升。该方法的有效性不仅限于类条件生成，还扩展到更具挑战性的任务如文本生成图像，展现了其在不同扩散模型应用中的广泛适用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14487" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>多智能体系统中的挑战与解决方案：综合研究与探索</title>
<link>https://arxiv.org/abs/2503.13657</link>
<guid>https://arxiv.org/abs/2503.13657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究分析了多智能体系统的挑战，并提出解决方案以提升其性能。</p><br /><br /><p><strong>摘要：</strong> 尽管多智能体系统（MAS）的应用热情日益高涨，但与单一智能体框架相比，其在常用基准上的性能提升仍然有限。本研究首次全面分析了MAS面临的挑战，通过对五种流行的MAS框架进行研究，涉及150多个任务和六位专家评分者，识别出14种独特的故障模式并提出一套适用于各种MAS框架的综合分类法。我们将这些故障模式分为三类：规格与系统设计失败、代理间不一致以及任务验证与终止。此外，为了支持可扩展评估，我们将MASFT与LLM作为评判者结合，并提出通过改进代理角色规格和增强编排策略来预防这些故障的两种干预措施。研究结果表明，解决识别出的故障需要更复杂的解决方案，为未来研究提供了明确的方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:04:38 GMT</pubDate>
</item>
<item>
<title>基于单幅图像的高保真可动画人类重建模型LHM</title>
<link>https://arxiv.org/abs/2503.10625</link>
<guid>https://arxiv.org/abs/2503.10625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LHM模型实现从单图像快速生成高保真的动画人类。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的模型LHM（大型可动画人类重建模型），旨在从单幅图像中高效重建动画人类。考虑到现有静态人类重建方法对合成数据的依赖和视频方法对捕捉条件的严格要求，LHM利用多模态变换器架构编码人体特征和图像特征，通过注意机制有效地保存服装的几何形状和纹理。模型还采用了头部特征金字塔编码方案，以增强面部身份的保留和细节的恢复。大量实验表明，LHM能在几秒内生成合理的可动画人类，且无需要面部和手部的后处理，显著超越了现有重建方法的准确性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>XAttention: 高效的长上下文Transformer模型稀疏注意力框架</title>
<link>https://arxiv.org/abs/2503.16428</link>
<guid>https://arxiv.org/abs/2503.16428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">XAttention框架通过稀疏注意力加速长上下文Transformer模型的推理。</p><br /><br /><p><strong>摘要：</strong> 长上下文Transformer模型（LCTMs）在实际应用中至关重要，但由于注意力的平方复杂性，计算成本高昂。为了解决这一问题，本文介绍了XAttention框架，它通过引入稀疏注意力显著加速Transformer模型的长上下文推理。XAttention的创新之处在于，它发现注意力矩阵中的反对角线值之和可以作为块重要性的有效代理，从而精确识别并修剪不必要的块，实现高稀疏性并显著加快推理速度。通过在RULER、LongBench、VideoMME和VBench等长上下文基准上的评估，XAttention在保持与全注意力相当的准确度的同时，展现了高达13.5倍的注意力计算加速。这些结果彰显了XAttention在真实世界应用中推动块稀疏注意力的实用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>4D Gaussian Splatting的优化与提升</title>
<link>https://arxiv.org/abs/2503.16422</link>
<guid>https://arxiv.org/abs/2503.16422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出4DGS-1K，通过减少冗余提高动态场景的渲染速度与存储效率。</p><br /><br /><p><strong>摘要：</strong> 4D Gaussian Splatting (4DGS) 在动态场景重建方面受到广泛关注，但其存储需求大且渲染速度慢。本文针对发生的时间冗余进行了深入探讨，识别出短生命周期高斯和非活动高斯作为两个主要问题。我们提出了4DGS-1K，能够在现代GPU上以超过1000 FPS的速度运行。为了解决短生命周期高斯问题，我们引入了空间-时间变化评分作为新的剪枝标准，能够有效去除短生命周期高斯，同时鼓励使用生命周期较长的高斯捕捉场景动态。针对非活动高斯问题，我们存储活跃高斯的掩码，显著减少渲染中的冗余计算。与传统的4DGS相比，本文方法在复杂动态场景中实现了41倍的存储减少和9倍的渲染速度提升，同时保持了相似的视觉质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>MagicMotion：精确轨迹控制的视频生成框架</title>
<link>https://arxiv.org/abs/2503.16421</link>
<guid>https://arxiv.org/abs/2503.16421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicMotion是一种新的视频生成框架，实现精确轨迹控制与视觉质量的提升。</p><br /><br /><p><strong>摘要：</strong> 随着视频生成技术的进步，视觉质量和时间一致性显著提高，轨迹可控的视频生成技术应运而生，以实现精确的物体运动控制。然而，传统方法在复杂物体移动和多物体运动控制方面存在局限，导致轨迹遵循不精确、物体一致性差及视觉质量下降。此外，这些方法仅支持单一格式的轨迹控制，限制了其应用场景。为此，我们提出了MagicMotion，一种新颖的图像到视频生成框架，可以通过掩膜、边界框和稀疏框三种条件实现轨迹控制。输入图像和轨迹后，MagicMotion能够无缝地沿定义轨迹动画物体，同时保持物体一致性和视觉质量。同时，我们还推出了MagicData，一个大规模轨迹控制视频数据集，以及一个自动化的注释和过滤管道。此外，我们建立了MagicBench，一个全面的基准评估视频质量和轨迹控制精度。实验表明，MagicMotion在各项指标上超越了以往方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>优化大语言模型的高效推理方法综述</title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统探讨了提升大语言模型推理效率的多种方法。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型（LLMs）在复杂任务中的显著表现，特别是大型推理模型（LRMs）如OpenAI o1和DeepSeek-R1的进步，本研究首次系统性地调查了如何提高LLMs推理效率。我们将现有研究分为几个关键方向，包括：1）基于模型的高效推理，旨在优化完整推理模型为更简洁的模型；2）基于推理输出的高效推理，目标是在推理过程中动态减少推理步骤和长度；3）基于输入提示的高效推理，试图根据输入提示的难度或长度特性提升推理效率。同时，我们还讨论了利用高效数据训练推理模型、小型语言模型的推理能力，及其评估方法和基准测试。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:38 GMT</pubDate>
</item>
<item>
<title>InfiniteYou：基于扩散变换器的高保真身份保留图像生成框架</title>
<link>https://arxiv.org/abs/2503.16418</link>
<guid>https://arxiv.org/abs/2503.16418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究介绍了InfiniteYou框架，提升了身份保留图像生成的质量与相似度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了InfiniteYou (InfU) 框架，聚焦于使用扩散变换器 (DiTs) 实现灵活且高保真的身份保留图像生成。InfU解决了现有方法在身份相似性、文本与图像对齐度以及生成质量上的不足，核心是InfuseNet，它通过残差连接将身份特征注入到DiT基础模型中，从而增强身份相似性，同时保持图像生成能力。此外，采用了多阶段训练策略，包括合成单人多样本 (SPMS) 数据的预训练和监督微调（SFT），进一步改善本文的文本与图像对齐，提高图像质量，并减轻面部复制粘贴现象。通过广泛实验，InfU实现了最先进的表现，超越了现有基准，且其即插即用的设计确保了与多种现有方法的兼容性，为更广泛的社区做出了重要贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>基于视觉语言的后训练行动决策模型提升</title>
<link>https://arxiv.org/abs/2503.16365</link>
<guid>https://arxiv.org/abs/2503.16365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新方法，提升视觉语言模型的开放环境行动决策能力。</p><br /><br /><p><strong>摘要：</strong> 开放世界环境中的行动决策近年来受到重视，视觉语言行动（VLA）模型在此领域展示了潜力。尽管先前的研究主要集中于行动后训练，忽视了对基础模型本身的优化，本文提出了一种新方法，即视觉语言后训练的行动（Act from Visual Language Post-Training），通过视觉和语言的自我监督指导来完善视觉语言模型（VLMs）。该方法改善了模型在世界知识、视觉识别和空间定位能力上的表现。基于此后训练模型，我们在Minecraft中获得了首个能够执行1,000多个原子任务的VLA模型，包括制作、熔炼、烹饪、采矿和击杀等。实验表明，在非轨迹任务上的后训练使得模型在多样化原子任务上相比最佳代理基线提升了40%。我们的研究结果也显示，所提方法优于传统模仿学习策略，在Minecraft中实现了最先进的性能。此外，我们已开源代码、模型和数据集，以推动进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:21:58 GMT</pubDate>
</item>
<item>
<title>超分辨率适应的关键指南URAE</title>
<link>https://arxiv.org/abs/2503.16322</link>
<guid>https://arxiv.org/abs/2503.16322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出URAE以解决高分辨率图像生成中的数据和参数效率问题。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像扩散模型的进展，生成高分辨率图像的训练仍面临挑战，特别是在数据和计算资源有限的情况下。本文从数据和参数效率两个关键角度探讨此问题，并提出超分辨率适应的关键指南URAE。研究表明，某些教师模型生成的合成数据可以显著促进训练收敛。当合成数据不可用时，微调权重矩阵的小组件比常用的低秩适配器表现更佳，提供了显著的性能提升。此外，针对采用指导蒸馏的模型，如FLUX，我们发现，在适应过程中禁用分类器无关的指导，即将指导比例设为1，对于实现满意的性能至关重要。大量实验验证了URAE在仅使用3000个样本和2000次迭代的情况下，实现了与FLUX1.1等闭源模型相当的2K生成性能，同时为4K分辨率生成设定了新基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:44:43 GMT</pubDate>
</item>
<item>
<title>FlashVDM：加速3D形状生成的新框架</title>
<link>https://arxiv.org/abs/2503.16302</link>
<guid>https://arxiv.org/abs/2503.16302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlashVDM通过优化VAE和DiT加速3D形状生成，提升效率和质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FlashVDM，一个系统化框架，旨在加速3D形状生成中的VAE和DiT模型。尽管Vecset Diffusion Model (VDM)在生成高分辨率3D形状方面取得了积极进展，但其在高速度生成中仍面临挑战。FlashVDM通过灵活的扩散采样和创新的Progressive Flow Distillation，使得DiT模型可以在仅5个推理步骤内实现可比质量的生成。同时，通过引入自适应KV选择、分层体积解码和高效网络设计，FlashVDM显著降低了VAE的计算复杂度。实验表明，该模型在性能上优于现有的快速3D生成方法，同时在重建和生成方面的推理时间分别缩短超过45倍和32倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:23:44 GMT</pubDate>
</item>
<item>
<title>MathFusion: 跨问题指令合成增强数学推理能力的框架</title>
<link>https://arxiv.org/abs/2503.16212</link>
<guid>https://arxiv.org/abs/2503.16212</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MathFusion通过跨问题指令合成显著提升数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着大语言模型在数学推理上的进展，本文提出MathFusion，一种通过跨问题指令合成来增强数学推理的新框架。MathFusion包含三种融合策略：顺序融合用于建模解决方案依赖关系，平行融合强化概念理解，条件融合则创建上下文感知的选择性问题以提升推理灵活性。通过这些策略生成的新数据集MathFusionQA，使得在多项基准测试中，大语言模型的数学推理能力提高了18.0个百分点，展现了高数据效率，仅需45K额外的合成指令，较传统单一指令的方法有显著提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16212" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:00:41 GMT</pubDate>
</item>
<item>
<title>基于粗细预测的自回归图像生成模型</title>
<link>https://arxiv.org/abs/2503.16194</link>
<guid>https://arxiv.org/abs/2503.16194</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过粗细预测优化自回归图像生成的模型。</p><br /><br /><p><strong>摘要：</strong> 本文研究了一种优化自回归模型以改进图像生成的方法，特别针对在图像生成中使用大量代码本所引发的复杂性问题。我们发现，具有相似代码字表征的标记对最终生成图像的影响是相似的，这揭示了大型代码本中的显著冗余。基于这一见解，提出了从粗到细（CTF）预测标记的策略，通过为相似标记分配相同的粗标签来简化预测过程。该方法包括两个阶段：第一阶段是自回归模型，顺序预测每个标记的粗标签；第二阶段是辅助模型，基于粗标签同时预测所有标记的细标签。通过在ImageNet上的实验，我们的方法在Inception Score上相较基线取得了平均59分的显著提升，并且在增加推理步骤的情况下仍然实现了更快的采样速度。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16194" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:41:29 GMT</pubDate>
</item>
<item>
<title>基于强化学习的少样本分类策略研究</title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了少样本学习中基于强化学习的分类策略，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了少样本条件下多模态大语言模型（MLLMs）分类的微调方案。研究发现，传统的微调方法可能导致严重的过拟合，甚至比零-shot方法表现更差。为此，提出了CLS-RL方法，利用可验证信号作为奖励，对MLLMs进行微调。实验表明，CLS-RL在多个数据集上相较于传统的监督微调（SFT）方法性能更优，且在不同数据集上表现出一定的“免费午餐”现象，暗示强化学习方法有效增强模型的分类基础能力。此外，本文还引入了No-Thinking-CLS-RL方法，强调微调过程中的思维过程可能影响性能，通过减小思维过程的时间，进一步提高模型的效果和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16188" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 10:37:45 GMT</pubDate>
</item>
<item>
<title>缓解视觉语言模型中的主导模态偏见的BalGrad框架</title>
<link>https://arxiv.org/abs/2503.13834</link>
<guid>https://arxiv.org/abs/2503.13834</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出BalGrad框架以减轻视觉语言模型的主导模态偏见。</p><br /><br /><p><strong>摘要：</strong> 视觉语言（VL）模型在多个任务中展现了出色的表现，但常常依赖特定模态进行预测，导致“主导模态偏见”。这种偏见在某一模态受损时显著影响表现。本研究分析了主导模态偏见下模型的行为，并理论上展示了未对齐的梯度或梯度幅度差异如何阻碍损失的平衡收敛。基于这些发现，我们提出了BalGrad框架，旨在减轻主导模态偏见。该方法包括模态间梯度重加权，根据各个模态的贡献调整KL散度的梯度，以及模态间任务梯度投影，以非冲突的方式对齐任务方向。在UPMC Food-101、Hateful Memes和MM-IMDb数据集上的实验结果证实，BalGrad有效减少了模型在预测时对特定模态的过度依赖。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13834" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 22:17:41 GMT</pubDate>
</item>
<item>
<title>MagicID: 实现动态丰富且一致身份的视频生成</title>
<link>https://arxiv.org/abs/2503.12689</link>
<guid>https://arxiv.org/abs/2503.12689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MagicID框架，解决视频生成中的身份一致性与动态性问题。</p><br /><br /><p><strong>摘要：</strong> MagicID是一种新框架，旨在直接提升视频生成的身份一致性和动态性，以满足用户偏好。当前方法面临的主要挑战是视频长度过长导致的身份降解，以及训练过程中动态性降低。为了解决这些问题，MagicID构建了成对的偏好视频数据，明确奖励身份与动态特性，替代传统的自重建方法。通过引入混合采样策略，优先利用源自参考图像的静态视频来维持身份，同时使用Frontier采样方法提高生成视频的动态运动质量。实验结果表明，MagicID在多个指标上优于现有方法，成功实现了视频生成中的身份一致与自然动态。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12689" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 19:15:09 GMT</pubDate>
</item>
<item>
<title>三维空间多模态记忆系统M3的设计与应用</title>
<link>https://arxiv.org/abs/2503.16413</link>
<guid>https://arxiv.org/abs/2503.16413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">M3是一个多模态记忆系统，旨在通过视频源保存静态场景信息。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了三维空间多模态记忆系统（M3），旨在通过视频源保留中等规模静态场景的信息，提升视觉感知能力。M3结合了三维高斯散点技术与基础模型，构建出一种在不同粒度上渲染特征表示的多模态记忆。我们发现了先前特征散点工作中的两个主要挑战：一是存储每个高斯原语的高维特征所面临的计算限制，二是蒸馏特征与基础模型特征之间的信息丢失或不对齐。为了解决这些问题，M3引入了主场景组成部分和高斯记忆注意力的关键组件，提升了训练和推理的效率。我们通过量化评估特征相似性及下游任务，同时进行定性可视化，展示了高斯记忆注意力的像素轨迹。此外，M3还涵盖了各类基础模型的广泛应用，包括视觉-语言模型、感知模型及大型多模态与语言模型，并在四足机器人上展示其在室内场景中的实际应用。值得注意的是，M3首次解决了三维特征蒸馏中的核心压缩挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>CaKE：一种有效的知识编辑方法提升LLM的多跳推理能力</title>
<link>https://arxiv.org/abs/2503.16356</link>
<guid>https://arxiv.org/abs/2503.16356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CaKE方法通过优化推理电路，提升LLM知识更新的多跳推理能力。</p><br /><br /><p><strong>摘要：</strong> 知识编辑（KE）使得在大型语言模型（LLMs）中修改过时或错误的信息成为可能。然而，现有的KE方法在处理依赖于修改知识的多跳推理任务时效果不佳。通过对推理电路的分析，发现当前局部层次KE方法如MEMIT和WISE在有效整合更新信息时存在局限性。本研究提出了一种新方法CaKE（Circuit-aware Knowledge Editing），它通过精心策划的数据，促进模型使用已修改的知识，从而激励模型为新整合的知识发展适当的推理电路。实验结果表明，CaKE在与相关推理任务的准确性和一致性方面表现优于现有KE方法，在MQuAKE数据集中，实现了平均20%的多跳推理准确率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 13:14:34 GMT</pubDate>
</item>
<item>
<title>Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on Compressed Spatial Tokens</title>
<link>https://arxiv.org/abs/2503.16278</link>
<guid>https://arxiv.org/abs/2503.16278</guid>
<content:encoded><![CDATA[
Recent advancements in large language models and their multi-modal extensions have demonstrated the effectiveness of unifying generation and understanding through autoregressive next-token prediction. However, despite the critical role of 3D structural generation and understanding ({3D GU}) in AI for science, these tasks have largely evolved independently, with autoregressive methods remaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified framework that seamlessly integrates {3D GU} tasks via autoregressive prediction. At its core, Uni-3DAR employs a novel hierarchical tokenization that compresses 3D space using an octree, leveraging the inherent sparsity of 3D structures. It then applies an additional tokenization for fine-grained structural details, capturing key attributes such as atom types and precise spatial coordinates in microscopic 3D structures. We further propose two optimizations to enhance efficiency and effectiveness. The first is a two-level subtree compression strategy, which reduces the octree token sequence by up to 8x. The second is a masked next-token prediction mechanism tailored for dynamically varying token positions, significantly boosting model performance. By combining these strategies, Uni-3DAR successfully unifies diverse {3D GU} tasks within a single autoregressive framework. Extensive experiments across multiple microscopic {3D GU} tasks, including molecules, proteins, polymers, and crystals, validate its effectiveness and versatility. Notably, Uni-3DAR surpasses previous state-of-the-art diffusion models by a substantial margin, achieving up to 256\% relative improvement while delivering inference speeds up to 21.8x faster. The code is publicly available at https://github.com/dptech-corp/Uni-3DAR.
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 12:07:04 GMT</pubDate>
</item>
<item>
<title>Fin-R1：专为金融领域设计的推理大型语言模型</title>
<link>https://arxiv.org/abs/2503.16252</link>
<guid>https://arxiv.org/abs/2503.16252</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了金融领域专用的推理大型语言模型Fin-R1。</p><br /><br /><p><strong>摘要：</strong> 当前，推理大型语言模型在多个领域迅速发展，但在复杂金融任务中的能力仍需深入探讨。本文介绍了Fin-R1，这是一种专门为金融领域设计的推理大型语言模型。Fin-R1采用双阶段架构，基于DeepSeek-R1进行了金融推理数据集的提炼与处理。通过监督微调（SFT）和强化学习（RL）训练，Fin-R1在多项金融推理任务中展现出接近DeepSeek-R1的性能，参数规模为70亿。在FinQA和ConvFinQA任务中，其性能达到了当前最优（SOTA），并在其他任务中超越了更大规模的模型。Fin-R1展现出强大的推理和决策能力，为金融领域面临的多种问题提供了解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16252" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 11:46:18 GMT</pubDate>
</item>
<item>
<title>欺骗幽默数据集（DHD）的构建与分析</title>
<link>https://arxiv.org/abs/2503.16031</link>
<guid>https://arxiv.org/abs/2503.16031</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了欺骗幽默数据集（DHD），为研究虚假信息中的幽默提供了新资源。</p><br /><br /><p><strong>摘要：</strong> 本文提出了欺骗幽默数据集（DHD），这是一个研究由虚假声明和错误信息衍生的幽默的新资源。在信息泛滥的时代，理解幽默与欺骗之间的关系至关重要。DHD包含从虚假叙述生成的幽默评论，通过ChatGPT-4o模型生成，所有实例均标记了讽刺水平（从1到3）并分类为五种幽默类型：黑色幽默、讽刺、社会评论、文字游戏和荒诞性。该数据集涵盖英语、泰卢固语、印地语、卡纳达语、坦米尔语及其混合变体，是一个有价值的多语言基准。通过推出DHD，我们为分析虚假语境中的幽默奠定了结构化基础，为探索幽默如何与虚假信息的互动及其感知和传播的影响开辟了新的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.16031" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 20 Mar 2025 06:58:02 GMT</pubDate>
</item>
<item>
<title>Unified Variational Auto-Encoder在3D分子生成中的应用</title>
<link>https://arxiv.org/abs/2503.15567</link>
<guid>https://arxiv.org/abs/2503.15567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种统一的变分自编码器用于高效的3D分子生成。</p><br /><br /><p><strong>摘要：</strong> 3D分子生成对药物发现和材料科学至关重要，但处理包括原子类型、化学键和3D坐标在内的复杂多模态是一个重大挑战。现有方法通常为不变和等变模态维持独立的潜在空间，导致训练和采样效率低下。本研究提出统一变分自编码器（UAE-3D），该模型将3D分子压缩为来自统一潜在空间的潜在序列，同时保持接近于零的重建误差。通过Diffusion Transformer进行潜在生成，UAE-3D在GEOM-Drugs和QM9数据集上进行的广泛实验表明，该方法在新分子生成和条件3D分子生成方面显著建立了新的基准，展示了领先的效率和质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15567" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 04:56:13 GMT</pubDate>
</item>
<item>
<title>Cosmos-Reason1模型：启用物理AI的链式推理与决策</title>
<link>https://arxiv.org/abs/2503.15558</link>
<guid>https://arxiv.org/abs/2503.15558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Cosmos-Reason1模型，具备物理常识和决策能力的物理AI系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Cosmos-Reason1模型，旨在使物理AI系统能够理解物理世界，并通过长链推理过程生成适当的自然语言决策。文章定义了物理AI推理的关键能力，重点关注物理常识与赋能推理。研究中构建了一个层次本体框架，用于表示物理常识，包含空间、时间和物理的基本知识，还依赖于一种二维本体以广泛适应不同的物理体现。我们开发了两种多模态大型语言模型Cosmos-Reason1-8B和Cosmos-Reason1-56B，并通过四个阶段（视觉预训练、一般监督微调、物理AI微调及物理AI强化学习）进行训练。为评估模型性能，构建了物理常识与赋能推理的综合基准，结果显示增强学习和细调带来了显著改进。为促进物理AI的发展，相关代码及预训练模型将根据NVIDIA开放模型许可公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 18:06:58 GMT</pubDate>
</item>
<item>
<title>基于多轮交互的新一代强化学习算法SWEET-RL</title>
<link>https://arxiv.org/abs/2503.15478</link>
<guid>https://arxiv.org/abs/2503.15478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了SWEET-RL算法，改进了LLM代理的多轮任务表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现有多轮强化学习（RL）算法在优化大语言模型（LLM）代理时面临的信用分配问题。为了研究这一课题，我们引入了新基准ColBench，该基准允许LLM代理与人类合作，在后端编程和前端设计中进行多轮互动以解决实际任务。基于这一基准，我们提出了一种新颖的强化学习算法SWEET-RL，它使用经过精心设计的优化目标，训练一个可以访问额外训练时信息的评估模型。该评估模型提供逐步奖励来改进策略模型的表现。实验结果表明，与其他最先进的多轮强化学习算法相比，SWEET-RL在ColBench上实现了6%的绝对成功率和胜率提升，使得Llama-3.1-8B在现实协作内容创作中达到了或超过了GPT4-o的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>SkyLadder：一种优化的上下文窗口调度策略提升大规模语言模型预训练效率</title>
<link>https://arxiv.org/abs/2503.15450</link>
<guid>https://arxiv.org/abs/2503.15450</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SkyLadder提出了一种有效的短至长上下文窗口转换策略，提升了大规模语言模型预训练效率和性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，LLM预训练中的上下文窗口不断扩展以处理更长的序列。然而，初步研究发现，使用较短上下文窗口的模型在固定token预算下通常表现更佳。基于此，我们提出了SkyLadder，一种简单而有效的短至长上下文窗口过渡策略，旨在平衡长上下文能力与预训练效率。实验表明，SkyLadder在多个标准基准任务中保持强劲的表现，同时在长上下文任务中也能达到或超越基准结果。我们在100B tokens上预训练了1B参数（最高32K上下文）和3B参数（8K上下文）模型，取得了最高3.7%的性能提升，并比基准方法训练速度提升22%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15450" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:31:15 GMT</pubDate>
</item>
<item>
<title>DP-Recon: 使用扩散先验优化3D场景重建</title>
<link>https://arxiv.org/abs/2503.14830</link>
<guid>https://arxiv.org/abs/2503.14830</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DP-Recon通过扩散先验和可见性引导方法优化3D物体重建，显著提升几何与外观恢复效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法DP-Recon，用于优化3D场景中物体的几何和纹理重建，特别是在稀疏视角下。传统方法虽然引入了语义或几何正则化，但在约束不足的区域会导致重建效果退化，且无法恢复遮挡区域。为解决这一问题，DP-Recon通过Score Distillation Sampling (SDS) 引入扩散先验，提供缺失信息并增强几何与外观恢复。为避免重建与生成指导之间的冲突，进一步引入了可见性引导方法动态调整每个像素的SDS损失权重。大量实验表明，DP-Recon在Replica和ScanNet++数据集上明显优于现有最先进方法，尤其是在仅使用10个视角时，其重建效果超越了其他方法在100个视角下的表现。此外，该方法支持基于文本的几何和外观编辑，生成详细的UV映射，适用于高质量的视觉特效编辑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14830" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 22:11:31 GMT</pubDate>
</item>
<item>
<title>LLM-FE: 基于大语言模型的自动化特征工程框架</title>
<link>https://arxiv.org/abs/2503.14434</link>
<guid>https://arxiv.org/abs/2503.14434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLM-FE框架结合进化搜索与大语言模型，自动发现有效特征，提升表格学习任务的预测性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的自动化特征工程框架LLM-FE，通过结合进化搜索与大语言模型（LLM）的领域知识和推理能力，自动发现适用于表格学习任务的有效特征。传统的特征工程方法往往受限于预定义的转化规则和固定的搜索空间，且忽视了领域知识。而现有基于大语言模型的方案要么依赖直接提示，要么单纯依靠验证得分进行特征选择，未能充分利用以往特征发现实验的洞察或建立特征生成与数据驱动性能之间的有意义推理。LLM-FE将特征工程表述为程序搜索问题，LLM通过提出新的特征转化程序并结合数据驱动反馈进行迭代搜索。实验结果表明，LLM-FE在多个分类和回归基准测试中显著优于现有方法，显著提升了表格预测模型的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:11:24 GMT</pubDate>
</item>
<item>
<title>VERIFY: A Benchmark of Visual Explanation and Reasoning for Investigating Multimodal Reasoning Fidelity</title>
<link>https://arxiv.org/abs/2503.11557</link>
<guid>https://arxiv.org/abs/2503.11557</guid>
<content:encoded><![CDATA[
Visual reasoning is central to human cognition, enabling individuals to interpret and abstractly understand their environment. Although recent Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across language and vision-language tasks, existing benchmarks primarily measure recognition-based skills and inadequately assess true visual reasoning capabilities. To bridge this critical gap, we introduce VERIFY, a benchmark explicitly designed to isolate and rigorously evaluate the visual reasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to reason primarily from visual information, providing minimal textual context to reduce reliance on domain-specific knowledge and linguistic biases. Each problem is accompanied by a human-annotated reasoning path, making it the first to provide in-depth evaluation of model decision-making processes. Additionally, we propose novel metrics that assess visual reasoning fidelity beyond mere accuracy, highlighting critical imbalances in current model reasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers significant limitations, underscoring the need for a balanced and holistic approach to both perception and reasoning. For more teaser and testing, visit our project page (https://verify-eqh.pages.dev/).
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:26:11 GMT</pubDate>
</item>
<item>
<title>动态解构框架提升长文本验证的准确性</title>
<link>https://arxiv.org/abs/2503.15354</link>
<guid>https://arxiv.org/abs/2503.15354</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出动态解构框架，优化长文本的验证效果。</p><br /><br /><p><strong>摘要：</strong> 当前针对长文本事实性评价的研究中，解构与验证通常被孤立处理，忽视其相互作用及潜在的不一致性。研究发现，现有的解构策略与下游验证器在信息密度这一新颖指标上的不一致性，导致验证结果次优。为此，研究将寻求最佳解构策略与优化验证的过程形式化为一个双层优化问题，并提出了通过验证反馈动态解构的强化学习框架。实验结果显示，该框架在不同验证器、数据集及输入声明的原子性条件下，相较于现有解构策略，验证信心提升了0.07，准确率提高了0.12（在0-1范围内）。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15354" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 11:56:21 GMT</pubDate>
</item>
<item>
<title>MetaLadder：基于类比问题的数学推理框架</title>
<link>https://arxiv.org/abs/2503.14891</link>
<guid>https://arxiv.org/abs/2503.14891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaLadder框架通过类比问题提升大语言模型的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了MetaLadder框架，该框架旨在提升大语言模型（LLMs）在数学推理任务中的表现。MetaLadder通过鼓励模型回忆和反思与当前问题结构或语义相似的类比问题及其解决方案，来激发推理过程。此外，文章引入了一种问题重述机制，通过再生原始问题以增强模型对目标问题的理解，从而提高推理准确度。大量实验表明，MetaLadder在数学基准测试中显著提高了LLMs的准确性，相比标准的链式思维（CoT）方法，准确率提升达到10.3%。该研究不仅模拟了人类的类比学习能力，还有效改善了模型的推理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 00:36:35 GMT</pubDate>
</item>
<item>
<title>CURIE基准：评估大语言模型在科学问题解决中的能力</title>
<link>https://arxiv.org/abs/2503.13517</link>
<guid>https://arxiv.org/abs/2503.13517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CURIE基准旨在评估大语言模型在科学问题解决中的应用能力。</p><br /><br /><p><strong>摘要：</strong> CURIE基准是一个用于测量大语言模型在科学问题解决能力的基准测试，涵盖材料科学、凝聚态物理、量子计算、地理空间分析、生物多样性和蛋白质等六个学科的580个问题和解决方案对。基准设置了十个具有挑战性的任务，评估了多个闭式和开放式的大语言模型在需要领域专长、长上下文理解和多步骤推理的任务上的表现。实验结果表明，尽管Gemini Flash 2.0和Claude-3在各个领域展现出较高的理解能力，但流行的GPT-4o和command-R+在蛋白质测序任务上表现不佳，最佳表现仅为32%。CURIE基准希望为未来大语言模型在科学领域的开发提供指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:53:03 GMT</pubDate>
</item>
<item>
<title>KDTalker：结合无监督3D关键点与时空扩散模型的音频驱动人像生成框架</title>
<link>https://arxiv.org/abs/2503.12963</link>
<guid>https://arxiv.org/abs/2503.12963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KDTalker结合3D关键点与时空扩散模型，实现高效音频驱动的人像生成。</p><br /><br /><p><strong>摘要：</strong> KDTalker是一个创新框架，旨在通过结合无监督的隐式3D关键点与时空扩散模型，提升音频驱动单图像人像生成的效果。传统的方法通常分为基于关键点和基于图像的方式，前者虽然能有效保持角色身份，但在捕捉细致面部特征上受限；后者虽能生成高质量的肖像，但存在身份失真和计算成本高的问题。KDTalker通过调整面部信息密度，灵活建模多样的头部姿态，并选用了专门设计的时空注意机制来确保准确的唇同步，从而实现时间一致的高质量动画，并提高计算效率。实验结果表明，KDTalker在唇同步精度、头部姿态多样性和执行效率方面都达到了先进水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:18:31 GMT</pubDate>
</item>
<item>
<title>SynthScars: 高质量合成图像数据集与LEGION图像伪造分析框架</title>
<link>https://arxiv.org/abs/2503.15264</link>
<guid>https://arxiv.org/abs/2503.15264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了SynthScars数据集及其图像伪造检测框架LEGION。</p><br /><br /><p><strong>摘要：</strong> 随着生成技术的快速发展，合成图像的制作变得越来越方便，但也带来了显著的社会忧虑。当前的合成图像检测方法往往缺乏对伪造图像的详尽解释和有效的文本解读。为此，本文提出了SynthScars数据集，包含12,236张完全合成的图像，配有人工专业注释，涵盖四种不同的图像内容类型和三类伪造工件，提供细粒度的像素级分割、详尽的文本解释和工件类别标签。此外，我们提出了LEGION，一个基于多模态大型语言模型的图像伪造分析框架，集成了工件检测、分割和解释功能。实验表明，LEGION在多个基准测试中优于现有方法，特别是在SynthScars数据集上，显著超越第二名传统专家的表现，mIoU提高3.31%，F1分数提高7.75%。在其指导下生成的图像与人类偏好更加一致。代码、模型和数据集将被公开发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:37:21 GMT</pubDate>
</item>
<item>
<title>提升多模态推理：取向视觉条件化的新方法</title>
<link>https://arxiv.org/abs/2503.13360</link>
<guid>https://arxiv.org/abs/2503.13360</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新方法，通过视觉条件化提升多模态模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，随着大型语言模型的发展，其推理能力不断增强。然而，在需要视觉输入的多模态任务中，模型往往难以持续关注视觉信息，导致文本输出过于依赖。为此，研究者们对长链推理中的图像输入进行了消融实验，发现即使在移除图像输入的情况下，模型的准确率仅下降约2%。基于此，提出了“取向视觉条件化”（TVC）策略，该方法优化了图像输入的处理，压缩多余的视觉信息，使模型在推理过程中更好地关注视觉成分。TVC在五个数学推理基准测试中达到了行业领先的性能，相较于之前的最佳结果提高了3.4%，有效提升了多模态推理系统的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13360" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:45:12 GMT</pubDate>
</item>
<item>
<title>φ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation</title>
<link>https://arxiv.org/abs/2503.13288</link>
<guid>https://arxiv.org/abs/2503.13288</guid>
<content:encoded><![CDATA[
Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named phi-Decoding. To provide a precise and expressive estimation of step value, phi-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show phi-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon.
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:38:33 GMT</pubDate>
</item>
<item>
<title>统一构建广义知识图谱的框架研究</title>
<link>https://arxiv.org/abs/2503.11227</link>
<guid>https://arxiv.org/abs/2503.11227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一的框架以构建广义知识图谱，提升自然语言处理任务效果。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一个统一的广义知识图谱（GKG）构建框架，包括知识图谱、事件知识图谱和常识知识图谱，这对自然语言处理任务至关重要。目前的研究往往分开构建这些图谱，忽视了它们在计算资源和使用上的潜在统一性。在构建统一框架的过程中，我们解决了任务特定差异带来的挑战。通过从29个数据集中15个子任务收集数据，分类为样本内、反向任务和超出分布（OOD）数据。接着，我们提出了一个三阶段的课程学习微调框架，通过迭代性地将三种图谱的知识注入大型语言模型中。大量实验表明，所提模型在样本内、OOD和反向任务数据上均提升了三种图谱的构建效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 05:23:22 GMT</pubDate>
</item>
<item>
<title>TULIP：提升图像理解的开源CLIP替代模型</title>
<link>https://arxiv.org/abs/2503.15485</link>
<guid>https://arxiv.org/abs/2503.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TULIP是一种新型开源模型，提升了图像理解性能。</p><br /><br /><p><strong>摘要：</strong> TULIP是一个开源的CLIP类模型替代品，旨在解决现有图像-文本对比模型在细粒度图像理解任务中的不足。尽管CLIP和SigLIP在某些任务上取得了成功，但它们通常在图像理解方面表现不佳，特别是在计数、深度估计和细粒度物体识别等视觉任务中。TULIP通过生成数据增强、强化图像间和文本间对比学习以及图像/文本重构正则化来学习细粒度的视觉特征，同时保持全局语义的一致性。该模型拥有超过10亿参数，表现超过现有的最先进模型，并在多个基准测试上取得了新的零-shot性能，显著提升了图像语言模型的表现，提供了高达3倍于SigLIP在MMVP上的得分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>构建3D智能的基础模型：Roblox的探索与设计</title>
<link>https://arxiv.org/abs/2503.15475</link>
<guid>https://arxiv.org/abs/2503.15475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨Roblox如何构建支持3D生成与推理的基础模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Roblox致力于构建一个能够支持3D智能的基础模型，旨在帮助开发者全方位制作Roblox体验，包括生成3D对象、场景、角色绑定及编写程序化脚本。我们讨论了构建该模型的三个关键设计要求，并展示了实现3D形状标记器的初步步骤。我们提出的标记化方案可以用于多种应用，比如文本到形状生成、形状到文本生成，以及文本到场景生成。此外，文章展示了这些应用如何与现有的大型语言模型（LLMs）合作进行场景分析和推理，最后讨论了实现统一的3D智能基础模型的未来发展路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 13:52:17 GMT</pubDate>
</item>
<item>
<title>FluxFlow: 提升视频生成的时间质量</title>
<link>https://arxiv.org/abs/2503.15417</link>
<guid>https://arxiv.org/abs/2503.15417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出FluxFlow以增强视频生成的时间一致性和多样性。</p><br /><br /><p><strong>摘要：</strong> 时间质量是视频生成中的关键因素，确保帧间运动和动态的连贯性。然而，实现高时间一致性和多样性依然存在挑战。本研究首次探讨了视频生成中的时间增强，提出了一种名为FluxFlow的策略，旨在提升时间质量。FluxFlow在数据层面操作，应用可控的时间扰动，且无需对架构进行修改。在UCF-101和VBench基准上的广泛实验表明，FluxFlow显著提高了各类视频生成模型（包括U-Net、DiT和基于AR的架构）之间的时间一致性和多样性，同时保持了空间的可靠性。这些研究结果凸显了时间增强作为一种简单而有效的方法，可以显著提升视频生成的质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 12:59:32 GMT</pubDate>
</item>
<item>
<title>DeepMesh：优化三维网格生成的框架</title>
<link>https://arxiv.org/abs/2503.15265</link>
<guid>https://arxiv.org/abs/2503.15265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepMesh框架通过深度学习和强化学习优化三维网格的生成和质量。</p><br /><br /><p><strong>摘要：</strong> Triangle meshes在3D应用中至关重要，然而传统的自回归方法在生成结构化网格时常受到面数限制和网格不完整性的限制。为了解决这些问题，我们提出了DeepMesh框架，主要通过两个创新点进行优化：一是引入高效的预训练策略和新颖的标记算法，同时改进数据整理和处理；二是将强化学习引入3D网格生成，通过直接偏好优化实现与人类偏好的对齐。我们设计了一种评分标准，结合人类评估与3D指标，收集偏好对以确保视觉吸引力和几何准确性。在点云和图像的条件下，DeepMesh生成具有复杂细节和精确拓扑的网格，表现在精度和质量上超越了现有的最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 10:39:30 GMT</pubDate>
</item>
<item>
<title>ELTEX框架：专用领域合成训练数据生成的有效解决方案</title>
<link>https://arxiv.org/abs/2503.15055</link>
<guid>https://arxiv.org/abs/2503.15055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ELTEX框架通过合成数据提升网络安全领域模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ELTEX（高效LLM令牌提取）框架，旨在生成高质量的专用领域合成训练数据。大型语言模型（LLMs）在通用能力上表现优异，但在专用领域如网络安全方面由于缺乏特定培训数据而受到限制。ELTEX通过系统地整合显式领域指示符提取和动态提示，确保在生成过程中保留重要领域知识。我们在区块链相关的网络攻击检测背景下展示了ELTEX的有效性，并通过多种真实数据与ELTEX生成数据的组合微调Gemma-2B模型。实验结果表明，基于ELTEX增强的模型在标准分类指标和不确定性校准方面的性能与GPT-4相当，同时所需计算资源大大减少。我们还发布了一套针对区块链网络攻击检测的社交媒体文本合成数据集。研究表明，专用领域合成数据生成能够有效弥补资源高效模型与大型架构在专业领域的性能差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.15055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 19 Mar 2025 05:46:54 GMT</pubDate>
</item>
<item>
<title>基于文本反演的扩散模型个性化量化方法</title>
<link>https://arxiv.org/abs/2503.14868</link>
<guid>https://arxiv.org/abs/2503.14868</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种量化扩散模型以实现个性化，显著降低训练内存需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新方法，对扩散模型进行个性化量化，旨在减少训练和微调过程中的内存需求。通过使用文本反演和零阶优化，避免了去量化，降低了梯度计算和反向传播过程中的存储需求。针对个性化场景中单个或少数图像产生的噪声，我们引入了子空间梯度技术，通过构建过去记忆的标记子空间来对梯度进行去噪。此外，研究文本嵌入对图像生成的影响，提出了部分均匀时间步采样策略，以优化扩散时间步。实验表明，该方法在个性化稳定扩散上与先前方法相比，与图像和文本对齐分数相当，同时训练内存需求降低了最多8.2倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14868" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 23:45:37 GMT</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
We introduce MusicInfuser, an approach for generating high-quality dance videos that are synchronized to a specified music track. Rather than attempting to design and train a new multimodal audio-video model, we show how existing video diffusion models can be adapted to align with musical inputs by introducing lightweight music-video cross-attention and a low-rank adapter. Unlike prior work requiring motion capture data, our approach fine-tunes only on dance videos. MusicInfuser achieves high-quality music-driven video generation while preserving the flexibility and generative capabilities of the underlying models. We introduce an evaluation framework using Video-LLMs to assess multiple dimensions of dance generation quality. The project page and code are available at https://susunghong.github.io/MusicInfuser.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>扩展流媒体视频理解的新任务与ViSpeak模型</title>
<link>https://arxiv.org/abs/2503.12769</link>
<guid>https://arxiv.org/abs/2503.12769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新任务——视觉指令反馈，旨在提升用户与代理的互动。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型多模态模型（LMMs）在离线视频理解方面取得了一定进展，但流媒体视频理解面临重大挑战。本文提出了一项新任务，称为视觉指令反馈，模型需识别视觉内容并提取其中的指令，从而提升用户与代理之间的互动。我们定义了七个与视觉模态密切相关的关键子任务，并为研究收集了ViSpeak-Instruct数据集和ViSpeak-Bench评估集。此外，提出的ViSpeak模型在多个流媒体视频理解基准测试中展现了GPT-4o级别的性能。经过在ViSpeak-Instruct数据集上的微调，ViSpeak具备了基本的视觉指令反馈能力，为未来的研究打下了坚实基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 23:05:31 GMT</pubDate>
</item>
<item>
<title>STEVE：高效训练计算机使用代理的步骤验证管道</title>
<link>https://arxiv.org/abs/2503.12532</link>
<guid>https://arxiv.org/abs/2503.12532</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STEVE通过步骤验证提升计算机使用代理训练的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了STEVE，一个用于训练计算机使用代理的步骤验证管道，旨在提升代理的训练效率与准确性。首先，我们建立了一个大型指令集，并利用一些次优代理收集了轨迹数据。通过使用GPT-4o，我们对每个轨迹中的步骤进行验证，根据执行前后的屏幕对每一步赋予二进制标签。随后，我们采用Kahneman和Tversky优化方法，根据这些标签来优化代理。实验结果表明，STEVE可以有效利用轨迹中的正负动作，显著优于监督微调。此外，STEVE还使得我们能够训练一个7B的视觉-语言模型，在具有挑战性的实时桌面环境WinAgentArena中实现领先表现，同时大幅降低成本。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12532" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:43 GMT</pubDate>
</item>
<item>
<title>PyGDA：开源图域适配库的发布</title>
<link>https://arxiv.org/abs/2503.10284</link>
<guid>https://arxiv.org/abs/2503.10284</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PyGDA是一个为图域适配提供的开源Python库，集成多种模型与数据集。</p><br /><br /><p><strong>摘要：</strong> PyGDA是首个全面支持图域适配的开源Python库，旨在促进不同领域之间的知识转移。该库整合了20多种广泛使用的图域适配方法，并支持多种类型的图数据集。PyGDA提供模块化组件，用户可以灵活构建自定义模型，并利用多种实用函数。为处理大型图数据，PyGDA支持采样和小批量处理，确保高效计算。此外，还包括完善的性能基准和用户友好的API文档，以方便研究人员和从业者使用。PyGDA遵循MIT许可证发布，方便用户访问与安装。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10284" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 07:52:23 GMT</pubDate>
</item>
<item>
<title>MeshFleet：高质量三维车辆数据集的自动过滤与注释</title>
<link>https://arxiv.org/abs/2503.14002</link>
<guid>https://arxiv.org/abs/2503.14002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MeshFleet数据集，以优化三维生成模型的精度与控制性。</p><br /><br /><p><strong>摘要：</strong> 近年来，生成模型在三维对象领域取得显著进展，但在工程等专业领域的应用仍受限于其精度、质量和可控性的不足。本文提出MeshFleet，一个从Objaverse-XL提取的过滤和注释的三维车辆数据集，以帮助大规模生成模型的微调。研究中，我们建立了一个基于质量分类器的自动过滤管道，分类器通过对Objaverse的手动标注子集进行训练，结合DINOv2和SigLIP嵌入，并通过基于Caption的分析和不确定性估计进行优化。我们通过与基于Caption和图像美学评分的技术进行比较，证明了我们的过滤方法的有效性，并在SV3D的微调实验中强调了针对性数据选择对专业领域三维生成建模的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14002" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 04:09:24 GMT</pubDate>
</item>
<item>
<title>Multi-Scale Attention模型与Atlas架构在大规模图像建模中的应用</title>
<link>https://arxiv.org/abs/2503.12355</link>
<guid>https://arxiv.org/abs/2503.12355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Multi-Scale Attention和Atlas架构，显著提升高分辨率图像建模效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种高效的大规模图像建模方法——多尺度注意力（Multi-Scale Attention, MSA），其核心在于多尺度表示和双向跨尺度通信。MSA通过创建O(log N)尺度来逐步表示图像特征，并借助跨注意力机制在各尺度间传播信息。此外，基于MSA，我们提出了一种新型神经网络架构Atlas，其在高分辨率的ImageNet 100数据集上显著改善了计算性能与长上下文图像建模的权衡。在1024px分辨率下，Atlas-B的准确率达到91.04%，与ConvNext-B（91.92%）相当，但速度快4.3倍。Atlas在与FasterViT和LongViT的比较中性能更强，分别提升了2.95倍和4.96%的准确率。与MambaVision-S比较时，在不同分辨率下，Atlas-S的准确率提升显著，同时运行时间相似。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:52:13 GMT</pubDate>
</item>
<item>
<title>AdaLLaVA：一种自适应的多模态大型语言模型推断框架</title>
<link>https://arxiv.org/abs/2503.10905</link>
<guid>https://arxiv.org/abs/2503.10905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaLLaVA通过动态调整推断操作，优化多模态大型语言模型的效率。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLM）在推理方面展现了出色的能力，但其巨大的计算成本限制了在资源受限环境中的应用。尽管近期在提高MLLM效率方面有所努力，以往的解决方案在应对变化的运行时条件（如设备上其他程序的争用）时显得不足。为此，我们提出了AdaLLaVA，一个自适应推断框架，能够在推理期间根据输入数据和延迟预算动态重构MLLM中的操作。通过在问题回答、推理和幻觉等基准上的广泛实验，我们的结果显示，AdaLLaVA有效遵循输入延迟预算，在运行时实现不同的准确率与延迟权衡。此外，AdaLLaVA能够适应输入延迟和内容，且可与令牌选择技术结合以提升效率，并在不同MLLM中展现出良好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 17:39:38 GMT</pubDate>
</item>
<item>
<title>AudioX：统一的音频与音乐生成模型</title>
<link>https://arxiv.org/abs/2503.10522</link>
<guid>https://arxiv.org/abs/2503.10522</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AudioX是一个创新的统一模型，用于音频和音乐生成，具备灵活的自然语言控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了AudioX，一个统一的扩散变换器模型，旨在进行音频和音乐的生成。与以往特定领域的模型不同，AudioX能够以高质量生成通用音频和音乐，同时提供灵活的自然语言控制，能够无缝处理文本、视频、图像、音乐和音频等多种输入模式。其关键创新在于一种多模态遮蔽训练策略，该策略通过遮蔽不同模态的输入，促使模型从遮蔽输入中学习，从而获得强大而统一的跨模态表示。为了应对数据稀缺的问题，研究人员整理了两个全面的数据集：vggsound-caps，包含来自VGGSound数据集的19万条音频描述，以及V2M-caps，基于V2M数据集生成的600万条音乐描述。广泛的实验表明，AudioX不仅匹配或超越了最先进的专业模型，还在处理多样化输入模态和生成任务方面表现出显著的灵活性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10522" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:30:59 GMT</pubDate>
</item>
<item>
<title>EvalTree: 生成语言模型弱点档案的创新方法</title>
<link>https://arxiv.org/abs/2503.08893</link>
<guid>https://arxiv.org/abs/2503.08893</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍EvalTree方法，通过弱点档案提升语言模型评估与训练。</p><br /><br /><p><strong>摘要：</strong> 理想的模型评估应能识别模型的缺陷并提供改进指导。为此，本文提出了生成语言模型弱点档案的概念，根据模型在基准测试中的表现，形成一套以自然语言表达的弱点集合。我们引入了一系列定量评估方法来比较不同的弱点分析方法，并提出了EvalTree方法，通过构建能力树，识别模型表现不佳的能力，从而生成弱点档案。在MATH和WildChat基准测试中，EvalTree在识别弱点的精准性和全面性上优于其他基线方法。此外，基于EvalTree识别的弱点进行的数据收集和训练，显著提升了语言模型的性能。我们还展示了EvalTree如何揭示Chatbot Arena中基于人投票的评估方式中的缺陷。为促进未来研究，我们还发布了代码及交互接口，供实践者探索由EvalTree构建的能力树。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08893" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 17:12:48 GMT</pubDate>
</item>
<item>
<title>CoLMDriver: 基于大语言模型的协作驾驶系统</title>
<link>https://arxiv.org/abs/2503.08683</link>
<guid>https://arxiv.org/abs/2503.08683</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoLMDriver 提供了一种新型的基于语言模型的 V2V 协作驾驶解决方案。</p><br /><br /><p><strong>摘要：</strong> CoLMDriver 是首个全流程基于大语言模型的协作驾驶系统，旨在通过有效的语言基础协商和实时驾驶控制来改善车辆间安全性。该系统包含两个主要组件：基于演员-评论者范式的 LLM 协商模块，持续通过车辆间的反馈优化合作策略；以及意图导向的路径点生成器，将协商结果转化为可执行的路径点。此外，文章还引入了 InterDrive，一个包含10个复杂交互驾驶场景的 CARLA 基准测试，用于评估 V2V 的合作能力。实验结果表明，CoLMDriver 在多种高度互动的 V2V 驾驶场景中，成功率较现有方法提高了11%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08683" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:58:42 GMT</pubDate>
</item>
<item>
<title>自我提升认知框架：构建下一代多模态大型语言模型</title>
<link>https://arxiv.org/abs/2503.12303</link>
<guid>https://arxiv.org/abs/2503.12303</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍SIcog框架，通过自我生成数据提升多模态大型语言模型的认知能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大型语言模型（MLLMs）具备出色的能力，但在细致感知和复杂推理方面仍面临挑战。现有的多模态预训练方法主要通过训练高质量的图像描述提升感知能力，而收集思维链（CoT）推理数据的成本极高。本文提出了一种自我学习框架——自我提升认知（SIcog），旨在通过自生成数据的多模态预训练增强MLLMs的系统性认知能力。通过引入逐步描述（Chain-of-Description）方法，SIcog增强了模型的系统性感知，实现更全面和准确的理解，同时采用结构化的CoT推理技术，促进MLLMs进行深度多模态推理。实验表明，仅用213K自生成的预训练样本，SIcog能够构建出认知显著提升的下一代基础MLLMs，并在多个基准上实现领先表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12303" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 20:25:13 GMT</pubDate>
</item>
<item>
<title>小规模高质量数据集提升大型语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2503.13661</link>
<guid>https://arxiv.org/abs/2503.13661</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过小规模双语数据集提高大型语言模型的推理和法语能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大型语言模型（LLMs）中，通过在小规模高质量的双语（英法）数据集上进行战略性细致调整，以提升推理能力和法语语言熟练度的有效方法。研究假设，通过数据的有针对性策划和优化训练，可以实现竞争甚至优于传统依赖大规模数据集的表现。结果显示，通过仅对2,000个精心挑选的样本进行有监督的细致调整，Pensez 7B模型在AIME25基准测试上的准确率提升了20%，在法语MATH 5级基准上的提升为12%。这些发现挑战了大型语言模型推理能力依赖巨型数据集的普遍假设，指出了战略数据策划和优化细致调整在提升特定技能及多语言能力方面的潜力。研究结果为在资源有限的情况下高效开发表现优异的多语言LLMs提供了重要启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13661" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 15:09:11 GMT</pubDate>
</item>
<item>
<title>FlexWorld: 从单幅图像生成灵活视角3D场景</title>
<link>https://arxiv.org/abs/2503.13265</link>
<guid>https://arxiv.org/abs/2503.13265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexWorld框架能够从单幅图像生成高质量的灵活视角3D场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FlexWorld，一个新颖的框架，旨在从单幅图像生成灵活视角的3D场景，包括360度旋转和缩放。FlexWorld包含两个主要组件：一是强大的视频到视频（V2V）扩散模型，能够从粗略场景的缺失输入中生成高质量的新视图图像；二是逐步扩展过程，以构建完整的3D场景。通过利用先进的预训练视频模型和准确的深度估计训练对，V2V模型在大幅相机姿态变化下生成新视图。FlexWorld通过几何感知场景融合，逐步生成新的3D内容并将其整合到全球场景中。大量实验表明，FlexWorld在多个流行指标和数据集上实现了高质量新视图视频和灵活视角3D场景的生成，视觉质量优于现有的最新方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 11:18:38 GMT</pubDate>
</item>
<item>
<title>提升3D空间理解能力的多模态大型语言模型研究</title>
<link>https://arxiv.org/abs/2503.13111</link>
<guid>https://arxiv.org/abs/2503.13111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一个新数据集和基准，以提升多模态语言模型的3D空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大型语言模型（MLLMs）在3D空间推理上的局限性，引入了一个包含高质量3D场景数据的新 supervised fine-tuning 数据集以及一个新的评估基准，专注于室内场景。我们开发的Cubify Anything VQA（CA-VQA）数据集涵盖了多种空间任务，包括空间关系预测、度量大小及距离估计和3D定位。通过利用CA-VQA，我们训练了MM-Spatial，这是一种强大的通用型MLLM，已在包括我们的新基准在内的多个3D空间理解基准上实现了最先进的性能。研究表明，结合度量深度和多视图输入可以进一步提升3D理解能力，且仅通过数据，我们的模型在深度感知能力上达到了与专用单目深度估计模型相当的效果。我们计划发布我们的SFT数据集和基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 08:34:22 GMT</pubDate>
</item>
<item>
<title>基于超曲率空间的安全意识视觉-语言模型HySAC的提出</title>
<link>https://arxiv.org/abs/2503.12127</link>
<guid>https://arxiv.org/abs/2503.12127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法HySAC，通过超曲率空间提高视觉-语言模型的安全内容识别能力。</p><br /><br /><p><strong>摘要：</strong> 本研究针对视觉-语言模型如CLIP在处理安全内容时的不足，引入了一种新方法HySAC（Hyperbolic Safety-Aware CLIP），通过超曲率空间的层级特性来提升模型对安全与不安全内容的意识，避免了传统去遗忘技术的限制。我们提出将安全和不安全内容编码为蕴含层级，并在超曲率空间中将其置于不同区域。HySAC利用蕴含损失函数，建模安全与不安全图像-文本对之间的层级和不对称关系，从而赋予模型对不安全内容的识别能力，使其不仅可作为多模态的不安全分类器，还能灵活地对不安全查询进行重定向或保留原输出。本研究通过广泛实验验证了该方法在安全识别和内容审核可解释性方面的有效性和适应性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 09:18:04 GMT</pubDate>
</item>
<item>
<title>Florenz: 单语视觉语言模型在多语种任务中的系统性泛化研究</title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究单语视觉语言模型在多语种任务中的表现及泛化规律。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Florenz，一个单语视觉语言模型(VLM)，在多语种任务中的系统性泛化能力，分析模型大小和训练样本的影响。Florenz结合了预训练VLM Florence-2和大型语言模型Gemma-2，参数范围从0.4B到11.2B不等，利用一个故意语言覆盖不全的合成数据集进行训练。研究表明，间接学习未见任务-语言对遵循缩放规律，且Florenz模型即便在仅提供翻译任务数据时，仍能在特定语言中展现出图像描述能力。通过对多种下游数据集的微调，Florenz在多模态机器翻译、词汇消歧和图像描述等任务上表现出竞争力，展现出很好的缩放趋势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09443" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:41:10 GMT</pubDate>
</item>
<item>
<title>Concat-ID：统一的身份保持视频生成框架</title>
<link>https://arxiv.org/abs/2503.14151</link>
<guid>https://arxiv.org/abs/2503.14151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Concat-ID是一个通过自注意力机制实现身份保持视频生成的框架。</p><br /><br /><p><strong>摘要：</strong> Concat-ID是一个新提出的框架，旨在实现身份保持的视频生成。该框架采用变分自编码器提取图像特征，并在序列维度上将其与视频潜在特征拼接，完全依赖于3D自注意力机制，无需其他模块。通过引入新颖的跨视频配对策略和多阶段训练方案，Concat-ID有效平衡了身份一致性与面部可编辑性，同时提高了视频的自然性。大量实验表明，Concat-ID在单一和多重身份生成方面均优于现有方法，并且能够无缝扩展到多主体场景，包括虚拟试穿和背景可控生成，确立了身份保持视频合成的新基准，为广泛应用提供了灵活可扩展的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 07:17:32 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的对齐算法综述</title>
<link>https://arxiv.org/abs/2503.14504</link>
<guid>https://arxiv.org/abs/2503.14504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统评审了多模态大语言模型的对齐算法及其应用。</p><br /><br /><p><strong>摘要：</strong> 本文对多模态大语言模型(MLLMs)的对齐算法进行了全面的系统评审，探讨了四个关键方面：首先，涵盖了对齐算法的应用场景，包括一般图像理解、多图像处理、视频和音频等扩展多模态应用；其次，讨论了构建对齐数据集的核心因素，如数据来源、模型响应和偏好注释；第三，评估对齐算法的基准测试；最后，讨论了对齐算法未来发展的潜在方向。通过本研究，旨在帮助研究人员整理该领域的最新进展，激励更优秀的对齐方法的开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>AI系统任务完成时间的新度量与未来展望</title>
<link>https://arxiv.org/abs/2503.14499</link>
<guid>https://arxiv.org/abs/2503.14499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出50%-任务完成时间指标 quantifies AI与人类能力的差异。</p><br /><br /><p><strong>摘要：</strong> 尽管AI在基准测试中取得快速进展，但基准性能的现实意义仍不明确。为量化AI系统的能力与人类能力的关系，本文提出了一种新指标：50%-任务完成时间。这是指人类在完成AI模型能以50%成功率完成的任务时所需的时间。我们记录了领域专家在RE-Bench、HCAST等任务中的完成时间。目前前沿AI模型如Claude 3.7 Sonnet的50%时间范围约为50分钟。自2019年以来，前沿AI的时间范围大约每七个月翻倍，预计在2024年这一趋势可能加速。这一增长主要得益于AI模型更强的可靠性、对错误的适应能力，以及更好的逻辑推理和工具使用能力。我们讨论了研究结果的局限性及其外部有效性，并探讨了自主性增加对危险能力的影响。如果这些结果能够推广到现实软件任务，预测未来五年内AI系统将能够自动化许多当前需要一个月人力完成的软件任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>Temporal Consistency for LLM Reasoning Process Error Identification</title>
<link>https://arxiv.org/abs/2503.14495</link>
<guid>https://arxiv.org/abs/2503.14495</guid>
<content:encoded><![CDATA[
Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:58:28 GMT</pubDate>
</item>
<item>
<title>Cosmos-Transfer: 基于多模态输入的条件世界生成模型</title>
<link>https://arxiv.org/abs/2503.14492</link>
<guid>https://arxiv.org/abs/2503.14492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cosmos-Transfer模型基于多模态输入实现灵活的世界生成与控制。</p><br /><br /><p><strong>摘要：</strong> Cosmos-Transfer是一种基于条件的世界生成模型，能够根据多种空间控制输入（如分割、深度和边缘信息）生成世界模拟。该模型的设计具备自适应和可定制的空间条件方案，可以在不同空间位置对不同的条件输入进行不同的加权，进而实现高度可控的世界生成。Cosmos-Transfer在多个世界到世界的转移场景中具有重要应用，包括Sim2Real。我们对该模型进行了广泛评估，并展示了其在机器人Sim2Real和自动驾驶数据增强等物理AI应用中的实践价值。此外，我们还提出了一种推断缩放策略，使其能够在NVIDIA GB200 NVL72机架上实现实时世界生成。为加速该领域的研究发展，我们公开了相关模型和代码。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:57:54 GMT</pubDate>
</item>
<item>
<title>Creation-MMBench: 评估多模态大语言模型创意能力的新基准</title>
<link>https://arxiv.org/abs/2503.14478</link>
<guid>https://arxiv.org/abs/2503.14478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Creation-MMBench 旨在评估多模态大语言模型的创意能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了 Creation-MMBench，一个针对多模态大语言模型（MLLMs）创意能力的评估基准。该基准包含765个测试案例，涵盖51个细分任务，旨在实现对 MLLMs 在现实图像任务中的创意解决方案生成能力的系统评估。为了确保评估的严谨性，本文为每个测试案例定义了特定的评估标准，以指导对响应质量和与视觉输入的一致性进行评估。实验结果表明，当前开源的 MLLMs 在创意任务中远不及商业模型。同时，分析结果显示，视觉微调可能对基础LLM的创意能力产生负面影响。Creation-MMBench 为推动 MLLM 的创造力提升提供了有价值的见解，并为未来在多模态生成智能领域的发展奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:51:34 GMT</pubDate>
</item>
<item>
<title>开放源码的大规模强化学习系统提升LLM推理能力</title>
<link>https://arxiv.org/abs/2503.14476</link>
<guid>https://arxiv.org/abs/2503.14476</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DAPO算法，开源大规模RL系统，助力LLM推理能力提升。</p><br /><br /><p><strong>摘要：</strong> 本文提出了去耦合剪辑与动态采样策略优化（DAPO）算法，并完全开源了一种大规模强化学习（RL）系统，使用Qwen2.5-32B基础模型在AIME 2024中取得了50分的优异成绩。与以往隐瞒训练细节的工作不同，本文介绍了四项关键技术，使得大规模LLM的RL训练取得成功。此外，开放源码的训练代码基于verl框架，并配有经过精心策划和处理的数据集。这些组件的开源旨在提高可重复性，并支持未来在大规模LLM RL领域的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14476" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:49:06 GMT</pubDate>
</item>
<item>
<title>DeepPerception: 融合认知视觉感知的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2503.12797</link>
<guid>https://arxiv.org/abs/2503.12797</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepPerception，通过知识密集型视觉定位提升多模态理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DeepPerception，一个增强认知视觉感知能力的多模态大型语言模型（MLLM），旨在解决当前MLLM在视觉感知和推理中的不足。通过引入知识密集型视觉定位任务（KVG），DeepPerception结合了精细的视觉感知和领域特定的知识集成。我们构建了一个自动化数据合成管道，以生成高质量的训练样本，并采用了两阶段的训练框架，实现了认知推理和强化学习的结合。为了评估性能，我们推出了KVG-Bench数据集，其中包含来自10个领域的1.3K手动策划的测试案例。实验结果表明，DeepPerception在KVG-Bench上相比直接微调提高了8.08%的准确率，并在跨领域泛化上优于基线方法4.60%。这些发现强调了将认知过程整合进MLLM的重要性，为多模态推理研究开辟了新方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12797" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 00:06:34 GMT</pubDate>
</item>
<item>
<title>CapArena：评估视觉语言模型在图像描述中的表现</title>
<link>https://arxiv.org/abs/2503.12329</link>
<guid>https://arxiv.org/abs/2503.12329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究评估了视觉语言模型在图像描述中的表现及自动评测的可靠性。</p><br /><br /><p><strong>摘要：</strong> 图像描述在视觉语言研究中一直是一个长期挑战。随着大规模语言模型（LLMs）的崛起，现代视觉语言模型（VLMs）能够生成详细的图像描述。本研究通过创建CapArena平台，进行超过6000对图像描述的对比评估，揭示了领先模型如GPT-4o在性能上达到了或超过了人类水平，而大多数开源模型则相对落后。此外，研究分析了自动评测标准是否能可靠评估描述质量，结果表明传统的评测指标存在系统性偏差，导致模型评分不一致，而VLM-as-a-Judge在描述和模型层面表现出卓越的辨别能力。基于这些发现，我们推出了CapArena-Auto，这是一个准确且高效的自动化基准，能以较低成本实现与人工评分94.3%的相关性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 22:56:09 GMT</pubDate>
</item>
<item>
<title>Reflect-DiT：用于文本到图像生成的推理时间扩展</title>
<link>https://arxiv.org/abs/2503.12271</link>
<guid>https://arxiv.org/abs/2503.12271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Reflect-DiT通过反思能力提升文本到图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种新的文本到图像生成方法Reflect-DiT，该方法通过在上下文中引入反思能力，改进了基于最佳选项采样的推理时间扩展策略。与传统的best-of-N采样方法不同，Reflect-DiT允许Diffusion Transformers使用之前生成图像的示例和文本反馈来细化其生成结果。实验表明，Reflect-DiT在GenEval基准上显著提升了性能，得分达到0.81，超越了使用更大模型（SANA-1.5-4.8B）在最佳选择方法下获得的0.80成绩，同时生成的样本数量仅为20，显示出其在生成效率和效果上的优势。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 15 Mar 2025 17:58:12 GMT</pubDate>
</item>
<item>
<title>RWKV-7 "Goose" with Expressive Dynamic State Evolution</title>
<link>https://arxiv.org/abs/2503.14456</link>
<guid>https://arxiv.org/abs/2503.14456</guid>
<content:encoded><![CDATA[
We present RWKV-7 "Goose", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC^0. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset.   To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 13:31:05 GMT</pubDate>
</item>
<item>
<title>IPV-Bench：评估生成与理解不可能视频的新基准</title>
<link>https://arxiv.org/abs/2503.14378</link>
<guid>https://arxiv.org/abs/2503.14378</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出IPV-Bench，评估视频生成与理解模型对不可能视频的处理能力。</p><br /><br /><p><strong>摘要：</strong> 随着合成视频在应对数据不足和增加多样性方面的广泛应用，传统数据集主要集中于真实场景，未能深入探讨不可能、反事实和反现实的视频概念。本文旨在回答两个问题：一是当前视频生成模型能否有效地创建不可能的视频内容；二是现有视频理解模型是否足够出色以理解这些不可能的视频。为此，我们提出了IPV-Bench，一个新颖的基准，旨在评估和促进视频理解与生成的进展。IPV-Bench基于一个全面的分类法，涵盖了4个领域和14个类别，包含了违反物理、生物、地理或社会法律的多样场景。此外，设计了一个提示套件用于评估视频生成模型的创造力和遵循提示的能力，同时开发了一个视频基准以测试视频LLMs对理解不可能视频的能力，特别是需要对时间动态和世界知识进行推理的能力。综合评估揭示了视频模型的局限性和未来的研究方向，为下一代视频模型奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14378" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 12:10:24 GMT</pubDate>
</item>
<item>
<title>Frac-Connections：一种新型的深度学习连接方法</title>
<link>https://arxiv.org/abs/2503.14125</link>
<guid>https://arxiv.org/abs/2503.14125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Frac-Connections通过分割隐藏状态优化深度学习模型，显著提高了性能。</p><br /><br /><p><strong>摘要：</strong> 在现代深度学习架构中，残差连接是关键，帮助训练非常深的网络以减少梯度消失的问题。最近，超连接通过引入不同深度的多个连接强度来概括残差连接，从而解决了梯度消失与表示崩溃之间的摇摆效应。然而，超连接通过扩展隐藏状态的宽度增加了内存访问成本。本文提出了Frac-Connections，这是一种新颖的方法，通过将隐藏状态分割成多个部分而不是扩展其宽度，保留了超连接的部分优势，同时减少内存消耗。我们的实验在语言任务上进行了大规模测试，其中最大的模型是一个7B MoE模型，训练了高达3万亿个标记，结果表明Frac-Connections显著优于传统的残差连接。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.14125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Mar 2025 06:37:50 GMT</pubDate>
</item>
<item>
<title>Infinite Mobility：一种生成高保真关节物体的新方法</title>
<link>https://arxiv.org/abs/2503.13424</link>
<guid>https://arxiv.org/abs/2503.13424</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种程序生成方法，合成高质量关节物体用于增强体态AI任务。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Infinite Mobility，这是一种通过程序生成技术合成高保真关节物体的新方法。现有的关节物体生成方法多为数据驱动或基于模拟，受限于训练数据的规模与质量或模拟的高成本与繁琐性。通过用户研究和量化评估，作者证明该方法在物理属性和网格质量上均超越了现有的最先进方法，且与人类标注的数据集相当。此外，作者还展示了合成数据可用作生成模型的训练数据，从而为后续的规模扩展提供支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13424" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:53:56 GMT</pubDate>
</item>
<item>
<title>提升多模态大语言模型安全性的机器去学习基准研究</title>
<link>https://arxiv.org/abs/2503.12545</link>
<guid>https://arxiv.org/abs/2503.12545</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了评估多模态大语言模型机器去学习的新基准PEBench。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视觉问答、视觉理解和推理等任务中取得了显著进展。然而，这一进展依赖于大量互联网上收集的数据，给隐私和安全性带来了重大挑战。为了解决这些问题，机器去学习（MU）作为一种有前景的解决方案应运而生，能够在不需从头再训练的情况下，从已训练模型中移除特定知识。尽管MU在MLLMs中引起了关注，但其评估仍不全面，相关问题定义模糊，制约了更安全、值得信赖系统的开发。为此，本文提出了一个基准PEBench，包含个人实体和一般事件场景的数据集，旨在全面评估MU在MLLMs中的性能。通过PEBench，我们建立了一个标准化、严谨的框架，以推动安全与隐私保护的多模态模型研究。我们对6种MU方法进行了评测，揭示了这些方法的优缺点，并指出了MU在MLLMs中面临的关键挑战与机遇。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12545" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 11:26:20 GMT</pubDate>
</item>
<item>
<title>MPBench：评估过程级奖励模型的多任务基准</title>
<link>https://arxiv.org/abs/2503.12505</link>
<guid>https://arxiv.org/abs/2503.12505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPBench是一个评估过程级奖励模型在多种推理场景中有效性的基准。</p><br /><br /><p><strong>摘要：</strong> 推理能力是大型语言模型（LLMs）解决复杂任务的关键，而过程错误的识别对于提高这种能力至关重要。为此，提出了过程级奖励模型（PRMs）以提供逐步奖励，进而增强强化学习和数据生成，帮助LLMs在推理时朝着正确步骤引导，提升推理准确性。然而，现有的PRMs基准测试多为基于文本，主要聚焦于错误检测，忽视了推理搜索等其他场景。为填补这一空白，我们引入了MPBench，一个全面的多任务多模态基准，旨在系统性评估PRMs在不同场景中的有效性。MPBench采用三种评估范式，分别针对PRMs在推理过程中的特定角色进行评估：步骤正确性、答案聚合和推理过程搜索。通过这些范式，MPBench能够全面评估PRMs，并为多模态PRMs的发展提供见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12505" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 09:50:38 GMT</pubDate>
</item>
<item>
<title>KUDA：集成动态学习与视觉提示的开放词汇操控系统</title>
<link>https://arxiv.org/abs/2503.10546</link>
<guid>https://arxiv.org/abs/2503.10546</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KUDA系统融合动态学习与视觉提示，实现更复杂的机器人操控。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）和视觉-语言模型（VLMs）的快速发展，开放词汇机器人操控系统取得了显著进展。然而，许多现有方法忽视了物体动态的影响，限制了它们在复杂动态任务中的应用。本文介绍了KUDA，一个开放词汇操控系统，通过关键点集成动态学习与视觉提示，运用VLMs和基于学习的神经动态模型。KUDA首先根据语言指令和视觉观察对RGB图像进行关键点分配，并查询VLM生成目标规范。这些抽象的基于关键点的表示随后被转换为成本函数，并通过学习的动态模型进行优化，以生成机器人轨迹。我们在各种操控任务上评估了KUDA，展示了其在自由形式语言指令、多物体交互以及可变形或颗粒状物体处理中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10546" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 12:59:17 GMT</pubDate>
</item>
<item>
<title>RoCo-Sim：提升路边协同感知的新模拟框架</title>
<link>https://arxiv.org/abs/2503.10410</link>
<guid>https://arxiv.org/abs/2503.10410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RoCo-Sim模拟框架显著提升路边协同感知性能，解决数据问题。</p><br /><br /><p><strong>摘要：</strong> 路边协同感知是一种多个路边单元协作收集感知数据的系统，旨在提升车辆的环境意识。现有方法主要关注模型设计，却忽视了数据问题，如校准错误和稀疏信息，这导致在新的数据集上表现不佳。为了解决这些问题，本文提出了首个模拟框架RoCo-Sim，能够通过动态前景编辑和全场景风格转移生成多样且视角一致的路边数据。RoCo-Sim由四个组件构成：摄像机外部优化、多视角遮挡感知采样器（MOAS）、深度SAM模型以及可扩展后处理工具包，有效改善路边3D物体检测性能，较现有最先进方法提升83.74和83.12的AP70。RoCo-Sim填补了路边感知模拟的关键空白，相关代码和预训练模型即将发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:33:42 GMT</pubDate>
</item>
<item>
<title>Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models</title>
<link>https://arxiv.org/abs/2503.06269</link>
<guid>https://arxiv.org/abs/2503.06269</guid>
<content:encoded><![CDATA[
Traditional white-box methods for creating adversarial perturbations against LLMs typically rely only on gradient computation from the targeted model, ignoring the internal mechanisms responsible for attack success or failure. Conversely, interpretability studies that analyze these internal mechanisms lack practical applications beyond runtime interventions. We bridge this gap by introducing a novel white-box approach that leverages mechanistic interpretability techniques to craft practical adversarial inputs. Specifically, we first identify acceptance subspaces - sets of feature vectors that do not trigger the model's refusal mechanisms - then use gradient-based optimization to reroute embeddings from refusal subspaces to acceptance subspaces, effectively achieving jailbreaks. This targeted approach significantly reduces computation cost, achieving attack success rates of 80-95\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5 within minutes or even seconds, compared to existing techniques that often fail or require hours of computation. We believe this approach opens a new direction for both attack research and defense development. Furthermore, it showcases a practical application of mechanistic interpretability where other methods are less efficient, which highlights its utility. The code and generated datasets are available at https://github.com/Sckathach/subspace-rerouting.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:29:45 GMT</pubDate>
</item>
<item>
<title>可扩展的开源视频基础模型训练管道</title>
<link>https://arxiv.org/abs/2503.12964</link>
<guid>https://arxiv.org/abs/2503.12964</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了一种新的视频基础模型训练管道，旨在提高视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了视频基础模型（VFM）在模拟现实世界、训练物理人工智能系统及开发创意视觉体验方面的潜力，并指出训练高质量VFMs面临的挑战。为此，作者提出了一种可扩展的开源VFM训练管道，基于NVIDIA NeMo，提供加速的视频数据集整理、多模态数据加载及并行化的视频扩散模型训练和推理。此外，文章还提供了综合性能分析，阐述了高效VFM训练和推理的最佳实践，旨在为相关领域的研究者和开发者提供参考。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12964" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 05:19:12 GMT</pubDate>
</item>
<item>
<title>GenStereo：高质量立体图像生成的新方法</title>
<link>https://arxiv.org/abs/2503.12720</link>
<guid>https://arxiv.org/abs/2503.12720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GenStereo通过扩散方法实现高质量立体图像生成。</p><br /><br /><p><strong>摘要：</strong> 立体图像在扩展现实、自动驾驶和机器人技术等多个领域至关重要，但高质量立体图像的获取仍面临挑战。现有的方法通常只关注视觉质量或几何准确性，而未能兼顾二者。为此，我们提出了一种新的扩散基础方法——GenStereo，旨在填补这一空白。该方法的两个主要创新在于：其一，通过差异感知坐标嵌入和变形输入图像对扩散过程进行条件化，从而实现比以往方法更精确的立体对齐；其二，采用自适应融合机制，将扩散生成的图像与变形图像智能结合，提高真实性和差异一致性。经过对11个多样立体数据集的全面训练，GenStereo显示出强大的泛化能力，并在立体图像生成和无监督立体匹配任务上取得了领先的性能。本框架消除了对复杂硬件的需求，使得高质量的立体图像生成变得可行，并在实际应用和无监督学习场景中具有重要价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 21:19:28 GMT</pubDate>
</item>
<item>
<title>WISA: 提升文本生成视频模型物理理解的新框架</title>
<link>https://arxiv.org/abs/2503.08153</link>
<guid>https://arxiv.org/abs/2503.08153</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISA框架提升文本到视频生成模型的物理理解与生成能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了WISA（世界模拟助手）框架，该框架旨在解决当前文本到视频（T2V）生成模型在理解物理原则方面的不足。通过将物理原则分解为文本描述、定性类别和定量属性，WISA有效地将这些物理属性嵌入生成过程中，提升模型的物理意识。此外，本文提出了新的视频数据集WISA-32K，包含32,000个视频，覆盖17个物理定律，适用于动力学、热力学和光学三个物理领域。实验结果显示，WISA框架显著提高了T2V模型与现实物理法则的兼容性，并在VideoPhy基准测试中取得了显著进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08153" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 04:10:03 GMT</pubDate>
</item>
<item>
<title>SPIN-Bench: 评估战略规划与社会推理的新基准</title>
<link>https://arxiv.org/abs/2503.12349</link>
<guid>https://arxiv.org/abs/2503.12349</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SPIN-Bench是一个全新基准，用于评估AI的战略规划和社会推理能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了一种新颖的多领域评估工具SPIN-Bench，旨在测量AI在战略规划和社会推理方面的智能。与现有的狭窄任务基准不同，SPIN-Bench将经典的PDDL任务、竞争棋盘游戏、合作卡牌游戏和多智能体谈判场景集成到一个统一框架中。该框架通过系统变化行动空间、状态复杂性和互动代理数量，模拟多种社会环境，以考验AI的推理和战略行为。实验结果表明，当代大型语言模型在基本事实检索和短期规划方面表现良好，但在需进行深层次多跳推理和社交协调的不确定任务中遇到显著瓶颈。SPIN-Bench被设想为未来多智能体规划、社会推理及人机合作研究的催化剂。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12349" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 00:10:53 GMT</pubDate>
</item>
<item>
<title>Sightation：提升视觉障碍者图表描述的模型评估与数据集</title>
<link>https://arxiv.org/abs/2503.13369</link>
<guid>https://arxiv.org/abs/2503.13369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过Sightation提供视觉障碍者友好的图表描述数据集。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了盲人和低视觉者（BLV）在图表描述需求与评估中的挑战。研究发现，目视标注者在直接生成图表描述时，往往易受偏见影响且不符合BLV标准。因此，本研究采用由视觉语言模型（VLM）生成的图表描述，并邀请目视个体进行评估，而非生成。评估结果被证明对专业的BLV教育者尤为有效，这些教育者为视觉障碍学习者提供指导。为此，我们发布了Sightation数据集，涵盖5000个图表及137,000个样本，支持完成、偏好、检索、问答和推理等多种训练目的，并展示其在多个下游任务中的微调潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:52:46 GMT</pubDate>
</item>
<item>
<title>基于人类指令的混杂物品抓取任务研究</title>
<link>https://arxiv.org/abs/2503.13082</link>
<guid>https://arxiv.org/abs/2503.13082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨如何利用视觉-语言模型进行机器人抓取。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在零样本设置下，如何利用视觉-语言模型（VLMs）来执行复杂的混杂物品抓取任务。针对机器人抓取的挑战，提出了一种新方法FreeGrasp，该方法利用预训练的VLMs进行人类指令的推理，同时处理物体的空间关系。通过将所有物体检测为关键点并在图像上进行标注，FreeGrasp能够帮助GPT-4o进行空间推理，以确定是否可以直接抓取目标物体或需要先抓取其他物体。由于缺乏专门的数据集，我们引入了合成数据集FreeGraspData，通过扩展MetaGraspNetV2数据集，加入人工注释的指令和真实抓取序列。研究结果表明，FreeGrasp在抓取推理和执行方面达到了领先水平，并通过搭载抓取器的机器人臂进行了实际验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:41:16 GMT</pubDate>
</item>
<item>
<title>多模态链条思维推理的系统性综述</title>
<link>https://arxiv.org/abs/2503.12605</link>
<guid>https://arxiv.org/abs/2503.12605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统回顾了多模态链条思维推理的现状与挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态链条思维（MCoT）推理在多模态大型语言模型（MLLMs）中得到了显著关注。现有研究设计了多种方法和创新推理范式，以应对图像、视频、语音、音频、3D及结构化数据所带来的独特挑战，并在机器人技术、医疗保健、自动驾驶及多模态生成等领域取得了广泛成功。然而，MCoT领域仍面临诸多独特的挑战与机遇，需要进一步关注。为填补这一领域的空白，本文首次系统性地调查了MCoT推理，阐明了相关的基础概念和定义，并从多角度对当前的方法论进行了综合分类和深入分析。此外，本文还提供了对现有挑战的见解和未来研究方向的探讨，旨在促进多模态通用人工智能（AGI）的创新发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 14:39:13 GMT</pubDate>
</item>
<item>
<title>LVAS-Agent: 长视频音频合成的新框架</title>
<link>https://arxiv.org/abs/2503.10719</link>
<guid>https://arxiv.org/abs/2503.10719</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LVAS-Agent 通过多代理协作提升长视频音频合成的效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出LVAS-Agent，一种新型多代理框架，旨在解决长视频音频合成中的难点，如动态语义变化和时间错位。该方法通过专业配音流程的模拟，将长视频合成分为四个步骤：场景分割、脚本生成、声音设计和音频合成。核心创新包括场景/脚本细化的讨论修正机制，以及用于时间语义对齐的生成-检索循环。此外，我们还推出了LVAS-Bench，这是第一个包含207个专业策划长视频的基准数据集，涵盖多样场景。实验结果表明，相比现有基线方法，LVAS-Agent在音频与视觉的匹配度上优于其他方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10719" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 03:58:23 GMT</pubDate>
</item>
<item>
<title>BlobCtrl：精确灵活的元素级视觉内容生成与编辑框架</title>
<link>https://arxiv.org/abs/2503.13434</link>
<guid>https://arxiv.org/abs/2503.13434</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BlobCtrl框架通过概率性Blob表示实现精确的元素级视觉内容编辑。</p><br /><br /><p><strong>摘要：</strong> BlobCtrl是一个新框架，旨在通过概率性Blob表示统一元素级生成与编辑。该框架利用Blob作为视觉原语，有效分离并表征空间位置、语义内容和身份信息，从而实现精确的元素级操作。其主要贡献包括：1) 采用双分支扩散架构与分层特征融合，实现前景与背景的无缝集成；2) 采用自监督训练范式，结合定制的数据增强和评分函数；3) 使用可控的dropout策略，平衡生成内容的保真度和多样性。此外，项目还推出了BlobData用于大规模训练，以及BlobBench用于系统评估。实验结果表明，BlobCtrl在多种元素级操作任务中表现优异，同时保持了计算效率，提供了一种实用的视觉内容生成与编辑解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13434" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:05 GMT</pubDate>
</item>
<item>
<title>提升视频生成的时空一致性研究</title>
<link>https://arxiv.org/abs/2503.06053</link>
<guid>https://arxiv.org/abs/2503.06053</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了视频生成中的时空一致性问题及相应解决方案。</p><br /><br /><p><strong>摘要：</strong> 时空一致性是视频生成中的一个重要研究主题，生成的视频段落需要保证情节的合理性和连贯性，同时在不同视角下维持对象与场景的视觉一致性。以往的研究多集中于时间或空间一致性，忽视了二者之间的综合影响。为解决这一问题，本文提出并探讨了整体时空一致性，关注情节发展与摄影技巧之间的协同效应及之前内容对后续生成的长期影响。研究涉及数据集的构建与模型的开发，首先构建了DropletVideo-10M数据集，该数据集包含1000万条视频，展示了动态相机动作和对象行为，并对每个视频进行了平均206字的注释，详述了不同的摄像机移动和情节发展。接下来，开发并训练了DropletVideo模型，在视频生成过程中出色地保持时空一致性。DropletVideo数据集和模型现已开放访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06053" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 23:37:38 GMT</pubDate>
</item>
<item>
<title>VideoMind：一个新的视频语言代理用于时间基础的视频理解</title>
<link>https://arxiv.org/abs/2503.13444</link>
<guid>https://arxiv.org/abs/2503.13444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VideoMind是一个用于时间基础视频理解的创新性视频语言代理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoMind，一个旨在提升视频理解的创新视频语言代理。VideoMind通过两个核心创新实现了对视频时间推理的深入理解。首先，它识别了视频时间推理所需的关键能力，并开发了一个基于角色的代理工作流程，包括角色协调的规划者、时间定位的基础角色、评估时间间隔准确性的验证者和回答用户问题的回答者。其次，提出了一种新颖的Chain-of-LoRA策略，通过轻量级LoRA适配器实现无缝的角色切换，避免了多模型的负担，平衡了效率与灵活性。通过对14个公共基准的广泛实验，VideoMind在多项视频理解任务中表现出色，尤其是在3个基础视频问答、6个视频时间基础和5个通用视频问答任务中达到了最先进的性能，证明了其在视频代理和长时空推理方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>WideRange4D: 一种针对大范围空间运动的4D重建基准与方法</title>
<link>https://arxiv.org/abs/2503.13435</link>
<guid>https://arxiv.org/abs/2503.13435</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新4D重建基准WideRange4D及相应方法Progress4D，支持大范围空间运动的场景重建。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于具有显著物体空间运动的4D场景重建，提出了新的4D重建基准WideRange4D，该基准包含丰富的4D场景数据，能够评估4D生成方法在大范围空间变化下的能力。现有4D重建技术在处理动态物体的广泛空间运动时面临挑战，常依赖变形场，但变形场在大范围动作中表现不佳。为了克服这一限制，本文还介绍了一种新型4D重建方法Progress4D，能够在多种复杂的4D场景重建任务中生成稳定、高质量的结果。通过在WideRange4D上进行定量和定性比较实验，我们证明了Progress4D在性能上优于现有的最先进4D重建方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13435" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>MicroVQA：生物医学研究中的多模态视觉问答基准</title>
<link>https://arxiv.org/abs/2503.13399</link>
<guid>https://arxiv.org/abs/2503.13399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MicroVQA是针对生物医学研究的多模态视觉问答基准，提升科学推理能力。</p><br /><br /><p><strong>摘要：</strong> MicroVQA是一个针对生物医学研究的视觉问答（VQA）基准，旨在评估科学研究中至关重要的三种推理能力：专家图像理解、假设生成和实验提案。该基准包含1,042道由生物专家策划的多项选择题，涵盖多种显微镜成像方式，确保问题与实际科学实践相关。在构建基准过程中，研究发现标准的多项选择题生成方法容易导致语言简化，因此提出了一种新的两阶段流程来优化过程。记分卡评估显示，现有的多模态大型语言模型（MLLMs）在MicroVQA测评中的最高表现为53%，小型LLMs略低于顶级模型，表明语言推理通常低于多模态推理的挑战。此外，通过科学文献调优可提高模型性能。专家分析结果表明，视觉感知错误最为频繁，其次是知识错误和过度概括错误。这些发现突显了在科学推理中的多模态挑战，MicroVQA为推动AI驱动的生物医学研究提供了重要资源。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 13:33:10 GMT</pubDate>
</item>
<item>
<title>Edit Transfer: 基于少量示例的非刚性图像编辑技术</title>
<link>https://arxiv.org/abs/2503.13327</link>
<guid>https://arxiv.org/abs/2503.13327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新颖的Edit Transfer方法，通过少量示例实现非刚性图像编辑。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新颖的编辑转移（Edit Transfer）设置，模型能够从单一的源-目标示例中学习变换，并将其应用于新的查询图像。虽然基于文本的方法在语义操作上表现优异，但在精准的几何细节上（如姿态和视角变化）常常显得力不从心。与此不同，基于参考的编辑通常侧重于风格或外观，无法有效处理非刚性变换。Edit Transfer通过显式学习源-目标对的编辑变换克服了文本驱动和外观中心参考的限制。我们提出了一种视觉关系上下文学习的范式，基于DiT的文本到图像模型，构建了编辑示例与查询图像的统一四面复合体，并通过轻量级的LoRA微调捕捉复杂的空间变换。尽管只使用42个训练样本，Edit Transfer在多样的非刚性场景中显著超越了现有的TIE和RIE方法，展示了少样本视觉关系学习的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 12:04:44 GMT</pubDate>
</item>
<item>
<title>通过奖励增强的生成方法提升文本到图像生成控制能力</title>
<link>https://arxiv.org/abs/2503.13070</link>
<guid>https://arxiv.org/abs/2503.13070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出R0方法，通过奖励优化提升文本到图像生成效果。</p><br /><br /><p><strong>摘要：</strong> 生成与复杂文本提示和人类偏好对齐的图像是人工智能生成内容(AIGC)中的核心挑战。研究表明，当条件更具体、奖励信号更强时，奖励在生成中的主导力量将逐渐超越扩散损失。为了验证这一假设，本文提出了R0，一种通过规范化奖励最大化的全新条件生成方法。R0将图像生成视为数据空间中的优化问题，旨在寻找具有高组合奖励的有效图像，而非依赖复杂的扩散耗损。通过创新的生成器参数化设计和适当的正则化技术，我们在大规模上训练了最先进的少步骤文本到图像生成模型。研究结果挑战了传统的拓展后训练和条件生成观念，展示了在复杂条件情境下奖励的主导作用。希望这些发现能为人本及以奖励为中心的生成范式的进一步研究提供借鉴。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.13070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 07:21:43 GMT</pubDate>
</item>
<item>
<title>Step-wise Group Relative Policy Optimization提升多语言大型模型的推理能力</title>
<link>https://arxiv.org/abs/2503.12937</link>
<guid>https://arxiv.org/abs/2503.12937</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过新的在线强化学习框架提升多语言大型模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文旨在通过设计一种新的在线强化学习框架——逐步群体相对策略优化（StepGRPO），提升多语言大型模型（MLLMs）的推理能力。传统的方法往往让模型被动地模仿成功的推理路径，而StepGRPO则鼓励模型通过简单、有效且密集的逐步奖励机制自我改进。为此，本文引入了两种新颖的基于规则的推理奖励：逐步推理准确性奖励（StepRAR）和逐步推理有效性奖励（StepRVR）。StepRAR通过软密钥步骤匹配技术奖励包含必要中间推理步骤的推理路径，而StepRVR则通过推理完整性和逻辑评估策略奖励遵循合理结构和逻辑一致性的推理路径。通过StepGRPO，本文开发了一系列具有优异逐步推理能力的MLLMs（R1-VL），并在八个基准测试上进行了广泛实验，结果显示该方法的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12937" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 04:51:44 GMT</pubDate>
</item>
<item>
<title>DreamRenderer：增强图像生成的实例控制</title>
<link>https://arxiv.org/abs/2503.12885</link>
<guid>https://arxiv.org/abs/2503.12885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRenderer通过创新的方法提升了图像合成中的实例控制能力。</p><br /><br /><p><strong>摘要：</strong> DreamRenderer是一种基于FLUX模型的无训练方法，旨在提升图像合成中用户对多个实例内容的控制能力。本文介绍了两个关键创新：一是桥接图像令牌用于硬文本属性绑定，确保T5文本嵌入在联合注意力过程中正确绑定每个实例的视觉属性；二是仅在关键层应用硬图像属性绑定，通过识别FLUX中负责实例属性渲染的关键层，确保精确控制的同时保持图像质量。评估结果表明，DreamRenderer在COCO-POS和COCO-MIG基准测试中相比FLUX提升了17.7%的图像成功率，并且提高了GLIGEN和3DIS等布局到图像模型的性能，最高提升可达26.8%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Mar 2025 03:30:16 GMT</pubDate>
</item>
<item>
<title>基于扩散变换器的个性化图像生成新方法</title>
<link>https://arxiv.org/abs/2503.12590</link>
<guid>https://arxiv.org/abs/2503.12590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于扩散变换器的训练-free个性化图像生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的个性化图像生成方法，利用扩散变换器(DiTs)的潜力，实现用户指定概念的图像生成和灵活编辑。尽管最近的无训练方法在计算效率上优于基于训练的方法，但在身份保留和适用性方面仍存在挑战。我们提出的‘Personalize Anything’框架，通过时间步自适应的令牌替换和补丁扰动策略，实现了个性化图像生成，支持布局引导生成、多主体个性化和掩码控制编辑。评估结果表明，该方法在身份保留和多样性方面表现出色，提供了高效的个性化解决方案，推动了对扩散变换器的新理解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>Being-0: 一种高效的人形机器人自主代理框架</title>
<link>https://arxiv.org/abs/2503.12533</link>
<guid>https://arxiv.org/abs/2503.12533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Being-0框架将高层次认知与低层次技能结合，实现高效的人形机器人任务执行。</p><br /><br /><p><strong>摘要：</strong> Being-0是一个层次化的自主代理框架，旨在将高层次认知与低层次技能有效整合，以实现人形机器人在真实环境中的自主操作。该框架结合了基础模型（FM）和模块化技能库，FM负责理解指令、任务规划和推理，而技能库则提供稳定的运动和灵巧的操作。为了解决不同层次之间的协调问题，Being-0引入了一个名为Connector的模块，该模块利用轻量级的视觉-语言模型（VLM）将语言计划转换为可执行的技能指令，并动态协调行走和操作，从而提高任务成功率。通过在大规模室内环境中的广泛实验，Being-0展示了其在复杂长周期任务中的有效性，能够克服具有挑战性的导航和操控子任务。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:53:53 GMT</pubDate>
</item>
<item>
<title>视觉语言模型中的基本水平分类研究</title>
<link>https://arxiv.org/abs/2503.12530</link>
<guid>https://arxiv.org/abs/2503.12530</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视觉语言模型如何体现人类的基本水平分类行为。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了心理学中基本水平分类的概念，并研究了两个开放源代码的视觉语言模型（VLMs）在此分类中的表现。研究发现，Llama 3.2 Vision Instruct（11B）和Molmo 7B-D模型均偏好与人类行为一致的基本水平分类。此外，模型偏好表现出与生物-非生物基本水平效应及专家基本水平转变等人类复杂行为一致的特点，进一步表明这些视觉语言模型在训练过程中获取了来自人类数据的认知分类行为。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12530" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:50:54 GMT</pubDate>
</item>
<item>
<title>量化大语言模型不确定性以增强用户信任</title>
<link>https://arxiv.org/abs/2503.12528</link>
<guid>https://arxiv.org/abs/2503.12528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨多种不确定性度量对人类行为的相关性，以提高模型控制和用户信任。</p><br /><br /><p><strong>摘要：</strong> 文章研究了多种不确定性度量，以识别与人类群体不确定性相对应的度量方法。研究发现，贝叶斯度量和一种名为top-k熵的熵变体在模型大小变化下，能够更好地与人类行为一致。尽管一些强度量在模型规模增大时与人类表现的相似性下降，但通过多元线性回归分析，结合多种不确定性度量的方法在减小规模依赖性的同时，仍能实现与人类行为的良好对齐。这项研究为增强模型的可控性和用户对模型的信任提供了理论支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.12528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Mar 2025 10:45:43 GMT</pubDate>
</item>
<item>
<title>强化奖励模型的鲁棒性研究</title>
<link>https://arxiv.org/abs/2503.11751</link>
<guid>https://arxiv.org/abs/2503.11751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨奖励模型的鲁棒性及其过拟合现象。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了现代自然语言处理中的奖励模型，特别是其鲁棒性与过拟合现象。尽管最新的奖励模型在标准基准上表现优异，作者指出这可能部分源于过拟合，影响了对模型真实能力的理解。为此，团队构建了reWordBench，系统性地对奖励模型输入进行意义或排名保持的变换，结果显示即使是微小的输入变换也能导致模型性能显著下降，展现出脆弱性。为改善这一问题，作者提出通过显式训练模型为同义句赋予相似分数的方法，结果表明该方法不仅提高了模型对同义句的鲁棒性，还增强了对其他不同类型变换的适应能力。最终，经过增强训练的奖励模型在对齐任务中展示出更好的实用性，与标准训练模型相比，在59%的情况下能生成更高质量的输出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>视频时空推理基准V-STaR的提出及视频大语言模型评估</title>
<link>https://arxiv.org/abs/2503.11495</link>
<guid>https://arxiv.org/abs/2503.11495</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出V-STaR基准以评估视频模型的时空推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨视频大语言模型（Video-LLMs）是否能在视频中进行顺序时空推理，现有的基准主要评估对象的存在而忽视了关系推理，无法有效测量模型对对象互动的理解。为此，本文引入了视频时空推理基准（V-STaR），通过反向时空推理任务（RSTR），同时评估对象的存在、事件的发生时机和空间位置，并捕捉人类认知的思维链逻辑。为支持此评估，构建了一个数据集，通过半自动化的GPT-4管道生成粗细结合的思维链问题，嵌入明确的推理链。实验结果显示，当前的视频大语言模型在稳健和一致的时空推理能力方面存在显著差距。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11495" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 11:21:44 GMT</pubDate>
</item>
<item>
<title>MTV-Inpaint：统一多任务视频修复框架</title>
<link>https://arxiv.org/abs/2503.11412</link>
<guid>https://arxiv.org/abs/2503.11412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MTV-Inpaint是一个统一的视频修复框架，兼具场景填充和物体插入能力。</p><br /><br /><p><strong>摘要：</strong> 视频修复涉及修改视频中的局部区域，确保时空一致性。现有方法主要侧重于场景填充，缺乏对新对象可控插入的能力。而最近的发展在文本引导视频(T2V)扩散模型方面为视频修复提供了新的思路。针对当前方法的局限，我们提出了MTV-Inpaint，一个统一的多任务视频修复框架，能够同时处理传统的场景填充和新对象插入任务。通过设计双分支空间注意机制，MTV-Inpaint在单一框架中实现了这两种任务的无缝整合。同时，MTV-Inpaint支持通过整合多个图像修复模型的图像到视频(I2V)修复模式，增强了多模态控制能力。此外，我们提出的两阶段管道结合了关键帧修复与中间帧传播，使得MTV-Inpaint能够有效处理长达数百帧的视频。大规模实验表明，MTV-Inpaint在场景填充和物体插入任务中均表现出色，展示了其在多模态修复、物体编辑、去除及处理长视频的广泛应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 09:54:10 GMT</pubDate>
</item>
<item>
<title>理论分析与改进：自回归视频扩散模型的统一框架</title>
<link>https://arxiv.org/abs/2503.10704</link>
<guid>https://arxiv.org/abs/2503.10704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了自回归视频扩散模型的理论基础及改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文对自回归视频扩散模型（ARVDM）进行了理论分析，并提出了Meta-ARVDM这一统一框架，涵盖了现有的多种方法。通过分析Meta-ARVDM生成的视频与真实视频之间的KL散度，文章揭示了ARVDM固有的两个重要现象：错误积累和记忆瓶颈。通过推导信息论上的不可能结果，我们表明记忆瓶颈现象无法避免。为此，设计了多种网络结构以显式利用更多的历史帧，并通过压缩帧，实现了记忆瓶颈缓解与推断效率之间的显著改进。实验结果在DMLab和Minecraft上验证了方法的有效性，并展示了不同方法间错误积累与记忆瓶颈之间的Pareto前沿。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 11:32:44 GMT</pubDate>
</item>
<item>
<title>CHOrD框架：高效生成3D室内场景的创新方法</title>
<link>https://arxiv.org/abs/2503.11958</link>
<guid>https://arxiv.org/abs/2503.11958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CHOrD框架实现高效、无碰撞的3D室内场景生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CHOrD，一个创新的框架用于可扩展的3D室内场景合成，旨在创建具有层次结构的住宅规模无碰撞数字双胞胎。CHOrD结合2D图像作为中间布局表示，成功预防生成过程中的碰撞伪影，并能根据复杂的平面图生成连贯的场景布局。该框架支持多模态控制，使得生成适应几何和语义变化的房间结构成为可能。此外，CHOrD还提出了一个新数据集，扩展了家庭物品和房间配置的覆盖范围，并显著提升了数据质量。在采用3D-FRONT和新数据集的评测中，CHOrD展示出卓越的性能，能够高效合成符合任意平面图变化的真实感室内场景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 22:05:10 GMT</pubDate>
</item>
<item>
<title>大规模推理模型在类比推理中的性能评估</title>
<link>https://arxiv.org/abs/2503.11207</link>
<guid>https://arxiv.org/abs/2503.11207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">评估两种大型推理模型在非语言类比推理测试中的表现。</p><br /><br /><p><strong>摘要：</strong> 本研究首次评估了两种最先进的大型推理模型（LRMs），OpenAI的o3-mini和DeepSeek R1，在基于Raven渐进矩阵的类比推理方面的表现。我们使用I-RAVEN数据集及其更难的扩展版I-RAVEN-X进行基准测试，以检验模型在更长推理规则和属性值范围上的概括能力。同时，针对视觉不确定性影响，我们扩展了I-RAVEN-X数据集，引入了混淆属性并平滑输入属性值分布。结果显示，OpenAI的o3-mini在I-RAVEN上的准确率为86.6%，在挑战性更高的I-RAVEN-X上骤降至17.0%。DeepSeek R1的准确率也表现出类似趋势，从80.6%降至23.2%。相比之下，神经符号概率诱导模型ARLC在这些测试中的表现更为稳健，准确率从98.6%轻微下降至88.0%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 04:52:25 GMT</pubDate>
</item>
<item>
<title>VGGT：高效的3D场景属性推断网络</title>
<link>https://arxiv.org/abs/2503.11651</link>
<guid>https://arxiv.org/abs/2503.11651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VGGT网络实现高效推断3D场景的关键属性，提升多个3D任务表现。</p><br /><br /><p><strong>摘要：</strong> VGGT是一个前馈神经网络，能够从单个或多个视图中直接推断3D场景的所有关键属性，包括摄像机参数、点图、深度图和3D点轨迹。这一方法使得3D计算机视觉取得进展，打破了以往模型专注于单一任务的局限性。VGGT不仅重建图像速度快，耗时不足一秒，还在无需后处理的情况下超越了依赖视觉几何优化技术的替代方案。此外，该网络在摄像机参数估计、多视图深度估计、密集点云重构和3D点跟踪等多个3D任务中达到了最先进的效果。使用预训练的VGGT作为特征骨干网络显著提升下游任务的表现，如非刚体点跟踪和前馈新视图合成。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>TreeMeshGPT：高质量艺术网格生成的新方法</title>
<link>https://arxiv.org/abs/2503.11629</link>
<guid>https://arxiv.org/abs/2503.11629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TreeMeshGPT通过动态树结构生成高质量艺术网格，改善点云匹配。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新型自回归变换器TreeMeshGPT，旨在生成与输入点云对齐的高质量艺术网格。与传统的下一个标记预测不同，TreeMeshGPT采用了一种新颖的自回归树序列策略，在这种策略下，下一输入标记是通过基于网格面之间三角形邻接关系的动态增长树结构检索而来。这种序列化方法使得每一步的网格扩展局限于最后生成的三角形面，从而降低了训练难度并提高了网格质量。该模型以两个标记表示每个三角形面，与标准面标记化相比，实现了约22%的压缩率。通过这种高效的标记化方法，TreeMeshGPT能够在强点云条件下生成高度细致的艺术网格，在容量和保真度上超越了以往的方法。此外，本文的方法还生成具有强法线方向约束的网格，减少了常见的翻转法线问题。实验表明，TreeMeshGPT有效提升了网格生成的质量，细节更加精细，法线方向一致性更强。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:48:06 GMT</pubDate>
</item>
<item>
<title>VAMBA：高效处理长视频的混合变换器模型</title>
<link>https://arxiv.org/abs/2503.11579</link>
<guid>https://arxiv.org/abs/2503.11579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VAMBA模型通过线性复杂度编码长视频，提高效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了一种名为VAMBA的混合Mamba-Transformer模型，旨在高效处理长达数小时的视频输入。传统的基于变换器的多模态模型由于因果自注意力操作的平方复杂性而面临训练和推断时的高计算成本。通过采用线性复杂度的Mamba-2块，VAMBA能够在不减少视频Token的情况下，在单个GPU上编码超过1024帧（640x360）。与只能处理256帧的传统变换器模型相比，VAMBA在训练和推断期间的GPU内存使用减少了至少50%，训练步骤速度几乎翻倍。此外，VAMBA在挑战性的小时级视频理解基准LVBench上，准确率提高了4.3%，同时在多种长短视频理解任务中保持了较强的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:45:23 GMT</pubDate>
</item>
<item>
<title>SmolDocling：超紧凑的文档转换视觉语言模型</title>
<link>https://arxiv.org/abs/2503.11576</link>
<guid>https://arxiv.org/abs/2503.11576</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SmolDocling是一种新型文档转换模型，能够高效处理全页内容。</p><br /><br /><p><strong>摘要：</strong> SmolDocling是一种超紧凑的视觉语言模型，专注于文档的端到端转换。它生成一种新的通用标记格式DocTags，能够全面处理整个页面上的所有元素，并保留其上下文和位置信息。与依赖大型基础模型或多个专用模型的传统方法不同，SmolDocling以256M参数的规模，实现了内容、结构与空间位置的精确捕捉。该模型在重现包括代码清单、表格、方程式、图表和列表等文档特征上表现出色，适用于商业文档、学术论文、技术报告、专利和表单等多种文档类型。实验结果显示，SmolDocling在性能上与其他高达27倍模型体积的视觉语言模型相抗衡，同时显著降低了计算需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11576" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 12:44:14 GMT</pubDate>
</item>
<item>
<title>将多语言大型语言模型扩展至语音模态的研究</title>
<link>https://arxiv.org/abs/2503.10620</link>
<guid>https://arxiv.org/abs/2503.10620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究将多语言大型语言模型扩展到语音模态的可行性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何将大型语言模型（LLMs）扩展至语音模态，尤以多语言模型为重点。我们通过语音离散化和持续预训练的方式，将现有的多语言大型语言模型TOWER扩展至能够处理语音输入的模型SPIRE。该模型不仅能够转录和翻译英语语音输入，还保持了原有TOWER模型在翻译相关任务中的性能表现。研究表明，将离散语音输入作为额外的翻译语言进行集成，在大型语言模型的适应过程中是可行的。我们将代码和模型开源，推动该领域的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:57:32 GMT</pubDate>
</item>
<item>
<title>群体鲁棒的机器遗忘：解决非均匀分布遗忘集的问题</title>
<link>https://arxiv.org/abs/2503.09330</link>
<guid>https://arxiv.org/abs/2503.09330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种新的机器遗忘方法，解决非均匀遗忘集引发的公平性问题。</p><br /><br /><p><strong>摘要：</strong> 在机器遗忘的研究中，传统方法假设遗忘数据均匀分布，但当遗忘数据在某一群体中占主导时，模型性能会显著下降，导致公平性问题。本文提出了一种名为群体鲁棒机器遗忘的新策略，采用样本分布重加权方法来减轻主导群体的性能损失。此外，我们引入了MIU（互信息感知机器遗忘），这是首个针对近似机器遗忘中的群体鲁棒性的方法。MIU通过最小化模型特征与群体信息之间的互信息，实现遗忘的同时减少主导群体的性能下降。实验表明，MIU优于标准方法，实现了不损害模型鲁棒性的遗忘效果，并确保了群体的鲁棒性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 08:24:05 GMT</pubDate>
</item>
<item>
<title>基于自监督学习的视频技能边界检测</title>
<link>https://arxiv.org/abs/2503.10684</link>
<guid>https://arxiv.org/abs/2503.10684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自监督方法，对视频进行技能边界检测和分割，提升任务性能。</p><br /><br /><p><strong>摘要：</strong> 在开放世界环境中学习技能是开发能够执行多种任务的代理的重要环节。我们提出一种基于自监督学习的技能边界检测（SBD）算法，无需人工标注即可将长视频分割成具有语义一致性的片段。通过分析从预训练的无条件动作预测模型中获取的预测误差，SBD可以检测到技能执行中的边界变化。我们在Minecraft上进行了评估，结果表明，SBD生成的片段在短期原子技能任务中的表现提升了63.7%和52.1%，而在长期任务中的层次代理表现提升了11.3%和20.8%。这一方法可利用多样化的YouTube视频来训练遵循指令的代理。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10684" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 14:51:40 GMT</pubDate>
</item>
<item>
<title>基于密集边界框的视频字幕生成与物体定位新方法</title>
<link>https://arxiv.org/abs/2503.10781</link>
<guid>https://arxiv.org/abs/2503.10781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，通过密集边界框实现视频中的物体定位与字幕生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的视频字幕生成与物体定位的方法，利用时间密集的边界框将字幕中的物体与视频中的对象相结合。我们首先介绍了一种大规模的自动注释方法，通过在单帧上聚合边界框的字幕，构建出时间一致的密集边界框注释数据集HowToGround1M。在此基础上，我们推出了GROVE模型，并进行预训练。此外，我们还创建了一个新的数据集iGround，包含3500个手动注释的视频及其密集的时空定位边界框，以衡量在这一领域的进展。我们的研究证明，该方法在iGround、VidSTG及ActivityNet-Entities等数据集上均实现了领先的结果，验证了在自动注释数据集上进行预训练并在手动注释小规模数据集上微调的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10781" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 14:21:07 GMT</pubDate>
</item>
<item>
<title>新型ETC身体拟合方法提升衣着人类的拟合准确性</title>
<link>https://arxiv.org/abs/2503.10624</link>
<guid>https://arxiv.org/abs/2503.10624</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ETC方法通过局部SE(3)等变性提升衣着人类的身体拟合效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的身体拟合管道，称为等变紧密度拟合（ETCH），旨在通过局部近似SE(3)等变性来估计衣物与身体之间的表面映射。该方法将紧密度编码为从衣物表面对基础身体的位移向量，通过这一映射，依据姿势不变的身体特征回归稀疏的身体标记，简化了衣着人类的拟合过程。通过在CAPE和4D-Dress数据集上的广泛实验，ETCH在松散衣物的身体拟合准确性上显著超过了现有的最先进方法，精度提升范围为16.7%至69.5%，形状准确性平均提高49.9%。在一次性（或离群）设置中，我们的等变紧密度设计甚至将方向性错误减少了67.2%至89.8%。定性结果证明，ETCH在应对困难姿势、未见形状、松散衣物和非刚性动态方面展现了强大的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10624" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:14 GMT</pubDate>
</item>
<item>
<title>邻接自回归建模：一种新的视觉生成框架</title>
<link>https://arxiv.org/abs/2503.10696</link>
<guid>https://arxiv.org/abs/2503.10696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新颖的邻接自回归建模方法，实现高效的视觉生成。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的邻接自回归建模（NAR）方法，通过近邻预测机制对视觉内容进行自回归生成。与传统的光栅顺序“下一个令牌预测”不同，NAR采用进步外推程序，从初始令牌出发，按曼哈顿距离逐步解码其余令牌，扩展解码区域边界。为实现空间-时间域内多个相邻令牌的并行预测，NAR引入了一组维度导向的解码头，分别沿正交维度预测下一个令牌。这种并行处理方法显著减少了生成过程中的模型前向步骤。在ImageNet和UCF101实验中，NAR分别实现了2.4倍和8.6倍的吞吐量，并在图像与视频生成任务中获得了优于PAR-4X方法的FID/FVD评分。同时，在文本到图像生成基准GenEval中，具有0.8B参数的NAR在使用仅0.4倍训练数据的情况下超越了Chameleon-7B。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 01:52:27 GMT</pubDate>
</item>
<item>
<title>MaRI框架：提升3D资产真实感的材料检索</title>
<link>https://arxiv.org/abs/2503.08111</link>
<guid>https://arxiv.org/abs/2503.08111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MaRI框架通过对比学习提升材料检索的准确性和真实感。</p><br /><br /><p><strong>摘要：</strong> 准确的材料检索对创建真实的3D资产至关重要。现有方法依赖于捕捉材料形状不变及光照变量的稀缺数据集，但由于缺乏多样性和真实世界的泛化能力，这些方法面临挑战。大部分现有方案采用传统的图像搜索技术，但无法有效捕捉材料空间的独特属性，从而导致检索性能不佳。为了解决这些问题，我们提出了MaRI框架，旨在弥合合成材料与真实材料之间的特征空间差距。MaRI通过联合训练图像编码器和材料编码器，构建一个共享的嵌入空间，并使用对比学习策略来和谐视觉与材料属性，使相似的材料与图像更接近，同时将不同的配对分隔开。为了支持这个框架，我们构建了一个高质量的综合数据集，该数据集包含在控制形状变化和多样化光照条件下渲染的合成材料，以及经过材料转移技术处理和标准化的真实材料。广泛的实验表明，MaRI在复杂的材料检索任务中展现出卓越的性能、准确性和泛化能力，优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:23:11 GMT</pubDate>
</item>
<item>
<title>ProJudgeBench：多模态大语言模型的过程评估基准</title>
<link>https://arxiv.org/abs/2503.06553</link>
<guid>https://arxiv.org/abs/2503.06553</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProJudgeBench为多模态大语言模型的过程评估提供了一个系统性的基准测试。</p><br /><br /><p><strong>摘要：</strong> 为了解决多模态大语言模型（MLLMs）在解决科学问题时常出现的错误的问题，我们提出了ProJudgeBench，这是第一个专门用于评估MLLMs过程评判能力的全面基准。该基准包含2400个测试案例和50118个步骤级标签，涵盖四个科学学科，具备多样的难度与内容。每个步骤由人类专家仔细标注其正确性、错误类型及解释，使得对模型评判能力的系统评估成为可能。评估结果显示，开源模型与专有模型之间存在显著性能差距。为缩小这一差距，我们进一步提出了ProJudge-173k，一个大规模的指令微调数据集，以及动态双相微调策略，鼓励模型在解决问题之前进行明确推理。这些贡献显著提升了开源模型的过程评估能力，所有资源将对外发布以促进未来的可靠多模态过程评估研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06553" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 06:55:51 GMT</pubDate>
</item>
<item>
<title>ARMOR：高效的多模态理解与生成框架</title>
<link>https://arxiv.org/abs/2503.06542</link>
<guid>https://arxiv.org/abs/2503.06542</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARMOR是一个资源高效的自回归框架，提升多模态模型的生成能力。</p><br /><br /><p><strong>摘要：</strong> ARMOR是一个全新的资源高效的自回归框架，旨在改善现有多模态大语言模型（MLLMs）的理解与生成能力。通过引入不对称的编码-解码架构和前向切换机制，ARMOR可以在不显著增加计算开销的情况下，实现文本与图像的自然交织生成。此外，它使用精心策划的高质量交织数据集对MLLMs进行微调。ARMOR的训练算法采用了“生成什么或如何生成”的方法，通过三个渐进的训练阶段，赋予现有MLLMs多模态生成能力的同时保持其理解能力。实验结果表明，ARMOR能够在有限的训练资源下有效地提升现有MLLMs的图像生成能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06542" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 06:15:39 GMT</pubDate>
</item>
<item>
<title>ReCamMaster：一种新的视频重渲染框架实现动态镜头控制</title>
<link>https://arxiv.org/abs/2503.11647</link>
<guid>https://arxiv.org/abs/2503.11647</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReCamMaster框架通过动态镜头控制再现视频场景，超越现有技术。</p><br /><br /><p><strong>摘要：</strong> ReCamMaster是一个针对视频生成领域的框架，旨在实现针对给定视频的动态镜头控制。尽管对视频生成的文本或图像条件控制进行了广泛研究，但改变给定视频的镜头路径仍然是一个较少探索的领域。该框架创新性地利用预训练文本到视频模型的生成能力，结合独特的视频条件机制，解决了在保持多帧外观和动态同步的额外约束下进行镜头更改的难题。为了弥补训练数据的不足，我们使用Unreal Engine 5构建了一个全面的多镜头同步视频数据集，模拟真实的拍摄特征。实验结果表明，ReCamMaster在不同的应用场景中，包括视频稳定、超分辨率和视频扩展上，显著优于现有的最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11647" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>利用对抗数据收集提高机器人操作的效率与性能</title>
<link>https://arxiv.org/abs/2503.11646</link>
<guid>https://arxiv.org/abs/2503.11646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过对抗数据收集提升机器人操作的任务表现与数据利用效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为对抗数据收集（ADC）的框架，旨在提升机器人操作中的数据效率，强调质量重于数量，尤其是在高成本的真实世界数据采集环境中。ADC引入了人机协作的动态交互方式，通过实时调整对象状态、环境条件以及语言指令，减少对大型数据集的依赖。实验结果表明，经过ADC训练的模型在面对未见任务指令时表现出更强的组合泛化能力和对感知干扰的鲁棒性。同时，仅使用ADC收集的20%演示量，模型的表现显著优于使用完整数据集的传统方法。这些结果表明，在机器人学习中，战略性的数据采集过程至关重要。此外，我们正在整理一个开放访问的大规模ADC-机器人数据集，以推动机器人模仿学习研究的进展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11646" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 13:59:07 GMT</pubDate>
</item>
<item>
<title>状态空间模型的系统概述与应用</title>
<link>https://arxiv.org/abs/2503.11224</link>
<guid>https://arxiv.org/abs/2503.11224</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统综述了状态空间模型的理论基础及其应用。</p><br /><br /><p><strong>摘要：</strong> 状态空间模型（SSMs）作为一种有效的替代方案，正逐渐受到关注，尤其在处理序列数据和较长上下文时表现出色，与变换器模型相比具有显著的效率提升。本文对SSMs进行了全面的概述，包括其理论动机、数学表述以及与现有模型的比较，探讨了多种应用。我们将SSM系列分为三个主要部分，详细介绍了原始SSM、结构化SSM（如S4）及选择性SSM（如Mamba）。在总结技术细节的同时，强调了为提高SSM的有效性和效率而提出的关键技术，期望本手稿能为研究人员探讨SSM的理论基础提供引导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11224" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 05:20:31 GMT</pubDate>
</item>
<item>
<title>API与GUI基础大语言模型代理的比较研究</title>
<link>https://arxiv.org/abs/2503.11069</link>
<guid>https://arxiv.org/abs/2503.11069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文系统比较了API与GUI基础的大语言模型代理。</p><br /><br /><p><strong>摘要：</strong> 本文对API基础和GUI基础的大语言模型代理进行了首次全面的比较研究，分析了两者在架构复杂性、开发工作流程和用户交互模型上的显著不同。尽管这两种范式都旨在实现大语言模型驱动的任务自动化，但它们的能力和应用场景各异。本文探讨了混合方法如何利用这两者的互补优势，并提出了明确的决策标准和实际用例，以指导实践者和研究人员在选择、结合或转变这两种范式时的策略。研究表明，随着大语言模型自动化技术的持续创新，API与GUI驱动代理之间的界限将逐渐模糊，推动在多种真实世界应用中实现更灵活、适应性强的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Mar 2025 00:26:21 GMT</pubDate>
</item>
<item>
<title>TxAgent：推动精准药物治疗的多模态自适应AI模型</title>
<link>https://arxiv.org/abs/2503.10970</link>
<guid>https://arxiv.org/abs/2503.10970</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TxAgent是一个基于AI的多模态模型，为个性化治疗提供推荐。</p><br /><br /><p><strong>摘要：</strong> TxAgent是一个先进的AI治疗代理，利用多步骤推理和实时生物医学知识检索，分析药物相互作用、禁忌症和患者特定的治疗策略。它从211个工具中整合信息，评估药物在分子、药代动力学和临床层面的相互作用，并根据患者的合并症和现用药物识别禁忌症。通过多重生物医学来源的证据提取和合成，TxAgent优化治疗建议，达到92.1%的准确率，优于现有的语言模型和推理代理。该模型不仅能够处理药物名称变体，还能确保治疗建议符合临床指南和真实世界的证据，从而降低不良事件风险，提升治疗决策的质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10970" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 20:28:15 GMT</pubDate>
</item>
<item>
<title>FlowTok：高效的文本与图像跨模态生成框架</title>
<link>https://arxiv.org/abs/2503.10772</link>
<guid>https://arxiv.org/abs/2503.10772</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlowTok提供一种高效的文本与图像模态转换方案，简化了生成过程。</p><br /><br /><p><strong>摘要：</strong> FlowTok是一个创新的框架，旨在简化文本与图像的跨模态生成。传统方法通常将文本视为引导信号，经过去噪过程逐渐生成目标图像；而FlowTok通过流匹配直接在文本和图像模态之间流动，解决了两者在隐空间中表达差异带来的挑战。它将图像编码为紧凑的1D令牌表示，与以往方法相比，隐空间大小减少了3.3倍且不需要复杂的条件机制或噪声调度。此外，FlowTok的设计允许在同一框架下自然扩展至图像生成文本，具有高效的内存使用、较少的训练资源需求和更快的采样速度，同时在性能上与最先进的模型相媲美。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10772" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 14:06:13 GMT</pubDate>
</item>
<item>
<title>基于可学习Kolmogorov-Arnold网络的视觉Transformer架构研究</title>
<link>https://arxiv.org/abs/2503.10632</link>
<guid>https://arxiv.org/abs/2503.10632</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了可学习Kolmogorov-Arnold网络在视觉Transformer中的应用效果。</p><br /><br /><p><strong>摘要：</strong> 本文首次设计了通用的可学习Kolmogorov-Arnold注意力（KArAt），可在任意基础上运行，旨在替代传统多层感知器（MLP）并应用于视觉Transformer（ViT）等深度网络架构。虽然Kolmogorov-Arnold网络在一维函数的符号表示和持续学习中表现突出，但在其他机器学习任务中的有效性仍需验证。本文还提出了一种更模块化的可学习注意力版本，即傅里叶-KArAt，并展示其在CIFAR-10、CIFAR-100及ImageNet-1K数据集上比ViT表现更优或相近。同时，通过分析这些架构的损失景观、权重分布、优化路径、注意力可视化和谱特性，与传统ViTs进行对比，探讨其性能和泛化能力。本文旨在鼓励学术界深入研究与先进架构相结合的KANs，特别关注可学习激活的理解与应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10632" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>联邦学习中的梯度反演攻击分析与防御策略</title>
<link>https://arxiv.org/abs/2503.11514</link>
<guid>https://arxiv.org/abs/2503.11514</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文分析了联邦学习中的梯度反演攻击及其防御策略。</p><br /><br /><p><strong>摘要：</strong> 联邦学习作为一种保留隐私的协作模型训练方法，虽然不共享原始数据，但仍然可能通过共享的梯度信息泄露私人信息，受到梯度反演攻击（GIA）的威胁。尽管已有多种GIA方法提出，针对这些方法的系统分析与评估仍然不足。本文首先对GIA进行系统回顾，将现有方法分为优化基（OP-GIA）、生成基（GEN-GIA）和分析基（ANA-GIA）三类，并全面分析与评估这三种GIA的方法在联邦学习中的表现、实用性及潜在威胁。研究发现，尽管OP-GIA的性能不尽如人意，但它是最具实用性的攻击方式；而GEN-GIA和ANA-GIA则因依赖性强和易被检测，故不具实用性。最后，本文提出了一个三阶段的防御流程，为用户设计更为安全的联邦学习框架和协议提供指导，并展望了未来研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.11514" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 04:08:44 GMT</pubDate>
</item>
<item>
<title>提升视频详细描述的模型：Cockatiel</title>
<link>https://arxiv.org/abs/2503.09279</link>
<guid>https://arxiv.org/abs/2503.09279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Cockatiel模型，解决视频详细描述中的偏差及人类偏好不一致问题。</p><br /><br /><p><strong>摘要：</strong> 视频详细描述(VDC)是连接视觉和语言的重要任务，能为复杂视频内容提供细致的描述。本文综合评估了当前最先进的方法，发现其在特定描述方面存在偏差，以及与人类偏好不一致的两个关键限制。为了解决这些问题，我们提出了Cockatiel，一个新颖的三阶段训练流程，通过结合合成数据和人类对齐训练来提升VDC性能。在第一阶段，我们从精细注释的数据集中推导出评分器，以选择在某些细粒度视频-字幕对齐和人类偏好上表现优异的合成字幕。接着，使用这个精心策划的数据集训练Cockatiel-13B，融合模型优势和人类偏好。最后，我们从Cockatiel-13B进一步提炼出Cockatiel-8B，便于使用。大量的定量与定性实验结果显示，我们的方法不仅在VDCSCORE上设定了新的最先进性能，还在大幅超越了主要竞争者的人类偏好评价。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 07:25:04 GMT</pubDate>
</item>
<item>
<title>PLADIS：基于稀疏注意力的高效文本到图像扩散模型优化方法</title>
<link>https://arxiv.org/abs/2503.07677</link>
<guid>https://arxiv.org/abs/2503.07677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PLADIS利用稀疏注意力优化扩散模型，提升文本到图像的生成效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的高效方法PLADIS，旨在提升已训练的扩散模型（如U-Net和Transformer）的性能。与现有方法需要额外训练或神经功能评估（NFE）不同，PLADIS通过稀疏注意力在推理过程中优化查询-键关联，避免了这些限制。利用稀疏注意力的抗噪声性，PLADIS显著提升文本到图像生成模型的效果，特别是在先前表现不佳的领域。该方法与不同的指导技术（包括指导蒸馏模型）无缝集成。通过广泛的实验，本文展示了PLADIS在文本对齐和人类偏好方面的显著改进，提供了一种高效且普适的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:23:19 GMT</pubDate>
</item>
<item>
<title>基于轨迹分布匹配的少步扩散模型学习</title>
<link>https://arxiv.org/abs/2503.06674</link>
<guid>https://arxiv.org/abs/2503.06674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TDM方法，提升少步扩散模型的生成效率与图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的少步扩散模型学习方法，称为轨迹分布匹配（TDM），旨在提升生成效率同时保持图像质量。传统的扩散蒸馏方法在处理复杂任务时表现不足，尤其是生成文本到图像时。本研究通过提出数据无关的评分蒸馏目标和采样步数感知目标，结合了分布匹配和轨迹匹配的优势，从而实现了灵活的多步采样。实验结果显示，TDM在多个基础模型上表现优越，特别是在PixArt-alpha的蒸馏中，创建了一个4步生成器，其用户偏好评分超过了教师模型，且训练成本显著降低，仅为教师模型的0.01%。此外，TDM还可扩展至文本到视频的扩散任务，表现出色，提升了总评分，推动了AIGC应用的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 11:53:49 GMT</pubDate>
</item>
<item>
<title>GoalFlow：高质量多模态轨迹生成的端到端自主驾驶方法</title>
<link>https://arxiv.org/abs/2503.05689</link>
<guid>https://arxiv.org/abs/2503.05689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GoalFlow提出了一种高质量多模态轨迹生成的自主驾驶方法。</p><br /><br /><p><strong>摘要：</strong> GoalFlow是一种新颖的端到端自主驾驶方法，旨在有效生成高质量的多模态轨迹。当前，自主驾驶场景中的轨迹选择复杂且质量受限于高轨迹偏差及指导与场景信息之间的不一致。GoalFlow通过引入目标点来约束生成过程，解决了扩散方法中固有的轨迹偏差问题。同时，该方法建立了一种新的评分机制，以根据场景信息从候选点中选择最合适的目标点。此外，GoalFlow采用高效的生成方法Flow Matching，以生成多模态轨迹，并结合优化的评分机制，从候选轨迹中挑选最佳轨迹。实验结果表明，GoalFlow在NavsimDauner2024_navsim数据集上达到了最新的最优性能，表现出强大的多模态轨迹生成能力，PDMS值达到90.3，显著超越其他方法。与其他基于扩散策略的方法相比，GoalFlow仅需一步去噪即能获得卓越的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05689" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:52:08 GMT</pubDate>
</item>
<item>
<title>PoseLess: 一种无姿态估计的机器人手控制框架</title>
<link>https://arxiv.org/abs/2503.07111</link>
<guid>https://arxiv.org/abs/2503.07111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PoseLess框架通过映射2D图像至关节角度，实现机器人手的高效控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PoseLess，一个新颖的机器人手控制框架，消除了显式姿态估计的需求，直接通过映射2D图像到关节角度来实现控制。该方法利用通过随机关节配置生成的合成训练数据，实现了对真实场景的零-shot泛化和从机器人到人类手的跨形态迁移。通过投影视觉输入和采用基于变换器的解码器，PoseLess在解决深度模糊和数据稀缺等挑战的同时，实现了稳健的低延迟控制。实验结果显示，在关节角度预测精度上，PoseLess的性能具有竞争力，且不依赖于任何人工标注的数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:34:05 GMT</pubDate>
</item>
<item>
<title>无分类器引导在条件生成中的新视角</title>
<link>https://arxiv.org/abs/2503.10638</link>
<guid>https://arxiv.org/abs/2503.10638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究无分类器引导的核心假设及其在去噪扩散模型中的作用。</p><br /><br /><p><strong>摘要：</strong> 本研究对无分类器引导进行了系统的实证研究，旨在更全面地理解其在条件生成中的作用。我们追溯到有分类器引导的根源，确定了其推导的关键假设，并探讨了分类器的角色。研究发现，无论是有分类器引导还是无分类器引导，都通过将去噪扩散轨迹推离决策边界（即条件信息通常纠缠且难以学习的区域）来实现条件生成。基于这种以分类器为中心的理解，我们提出了一种基于流匹配的通用后处理步骤，以缩小预训练去噪扩散模型学习到的分布与真实数据分布之间的差距，特别是在决策边界附近。多组数据集的实验验证了该方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>提高黑箱商业视觉语言模型的对抗攻击效果</title>
<link>https://arxiv.org/abs/2503.10635</link>
<guid>https://arxiv.org/abs/2503.10635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过关注语义细节来提升黑箱LVLM对抗攻击的有效性。</p><br /><br /><p><strong>摘要：</strong> 尽管开源大规模视觉语言模型（LVLM）表现优异，但传统的目标攻击在黑箱商业LVLM上往往失败。分析失败的对抗扰动显示，所学扰动通常来自均匀分布，缺乏清晰的语义细节，导致意图响应的不当。为了解决这些问题，文章提出一种方法，通过在局部区域中编码显式的语义细节来提高语义清晰度，并选择在语义丰富的区域进行修改，而不是均匀地应用扰动。该方法包括在每次优化步骤中随机裁剪对抗图像，并在嵌入空间中与目标图像对齐。实验结果表明，这种聚焦于关键区域的局部聚合扰动的对抗样本在多个黑箱LVLM（如GPT-4.5、Claude-3.7-sonnet等）上展现出超过90%的成功率，显著超越了以往的攻击方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>ConsisLoRA: 改进的样式转移方法与评估</title>
<link>https://arxiv.org/abs/2503.10614</link>
<guid>https://arxiv.org/abs/2503.10614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出ConsisLoRA，优化样式转移中内容和样式的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的样式转移方法ConsisLoRA，旨在解决传统LoRA方法在内容一致性、样式对齐和内容泄露方面的挑战。通过分析标准扩散参数化在样式转移中的局限性，我们引入了优化LoRA权重以预测原始图像而非噪声的策略。此外，我们设计了一个两步训练策略，分离参考图像的内容和样式学习。为了有效捕捉内容图像的全局结构和局部细节，我们还提出了逐步损失转移策略，以及在推理过程中可持续控制内容和样式强度的推理指导方法。通过定性和定量评估，ConsisLoRA在内容和样式一致性方面表现出显著提升，同时有效减少了内容泄露现象。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:55:58 GMT</pubDate>
</item>
<item>
<title>应对大规模视觉语言模型中的物体幻觉挑战</title>
<link>https://arxiv.org/abs/2503.10602</link>
<guid>https://arxiv.org/abs/2503.10602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索大规模视觉语言模型中物体幻觉的内部状态及应对策略。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模视觉语言模型（LVLMs）中物体幻觉（OH）问题，发现LVLM内部状态可以作为每个 token 的幻觉行为指示器，并识别出不同LVLM中共享的幻觉普遍模式。基于这一发现，提出了真相引导预干预（TruthPrInt）方法，通过学习LVLM解码的真实方向，并在解码过程中进行真相引导的干预。此外，提出了ComnHallu方法，以增强跨LVLM和跨数据的幻觉检测传递性，构造和对齐幻觉潜在子空间。在一系列实验中，TruthPrInt在多个流行的LVLMs和OH基准上表现优异，显著超越了现有的最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10602" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:46:06 GMT</pubDate>
</item>
<item>
<title>融合视觉成分的生成框架：IP-Prior与基于LoRA的微调策略</title>
<link>https://arxiv.org/abs/2503.10365</link>
<guid>https://arxiv.org/abs/2503.10365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍一种新框架，能将用户提供的视觉成分融合为完整的创意构思。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型在图像合成领域的进步，设计师往往需要超越文本条件的限制，直接从视觉元素中汲取灵感。我们提出了一种生成框架，能够将用户提供的部分视觉成分无缝整合为一致的创作，同时生成所需的缺失部分，以构建合理完整的概念。该方法基于从IP-Adapter+提取的强大表示空间训练而成的轻量级流匹配模型IP-Prior，支持基于领域特定先验的多样化和上下文感知的生成。此外，我们还提出了一种基于LoRA的微调策略，显著提升了IP-Adapter+在特定任务中的提示遵循性，解决了其重建质量与提示一致性之间的常见权衡问题。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:46:10 GMT</pubDate>
</item>
<item>
<title>儿童与大型语言模型的安全性研究</title>
<link>https://arxiv.org/abs/2503.10242</link>
<guid>https://arxiv.org/abs/2503.10242</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型对儿童的内容风险及安全性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在儿童生活中迅速普及的现状，并指明现有的AI伦理与安全研究未能充分考虑与未成年人相关的内容风险。通过分析一个在中学环境中部署的LLM聊天机器人案例，揭示了学生如何正确使用以及误用该系统的问题。基于此，本文提出了一种新的未成年人内容风险分类，并引入MinorBench，这是一个开源基准，旨在评估LLM在拒绝不安全或不当查询方面的能力。对六个主要LLM在不同系统提示下的评估显示，它们在儿童安全合规性方面存在显著差异。这些结果为制定更强大的儿童保护安全机制提供了实用建议，并强调了为年轻用户量身定制AI系统的迫切性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10242" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 06:34:43 GMT</pubDate>
</item>
<item>
<title>DiLoCo在大模型训练中的扩展性研究</title>
<link>https://arxiv.org/abs/2503.09799</link>
<guid>https://arxiv.org/abs/2503.09799</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究DiLoCo在固定计算预算下对大模型训练的扩展性。</p><br /><br /><p><strong>摘要：</strong> 本文研究了DiLoCo在训练大型语言模型时的扩展性行为，特别是在固定计算预算下，探讨了算法因素如模型副本数、超参数和令牌预算如何影响训练过程的表现。研究表明，DiLoCo在模型规模增长时的扩展性既可预测又稳健。经过良好的调优后，DiLoCo的扩展性优于数据并行训练，并能在小模型规模下甚至超过数据并行训练。这些发现揭示了DiLoCo相较于以往文献中记录的更广泛优势，包括增加的最佳批量大小、在规模扩大时的下游泛化能力以及在固定令牌预算下的评估损失改善。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09799" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 16:04:38 GMT</pubDate>
</item>
<item>
<title>探索视觉Transformer模型中的关键神经元路径</title>
<link>https://arxiv.org/abs/2503.09046</link>
<guid>https://arxiv.org/abs/2503.09046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文揭示了视觉Transformer中关键神经元路径的重要性及其影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉Transformer模型中神经元路径的重要性，提出了一种联合影响度量方法，评估神经元集对模型输出的贡献。我们进一步开发了逐层神经元定位的方法，有效选择每层中最具影响力的神经元，以发现输入到输出的关键神经元路径。实验结果表明，我们的方法在寻找信息流动的最具影响力神经元路径方面优于现有基线方案。此外，这些神经元路径展示了视觉Transformer在同一图像类别内处理视觉信息的特定内部机制。我们还分析了这些神经元在图像分类任务中的关键作用，表明找到的神经元路径保持了模型在下游任务中的能力，对模型剪枝等实际应用具有重要启发意义。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 00:10:46 GMT</pubDate>
</item>
<item>
<title>OmniPaint：一种统一的图像对象去除与插入框架</title>
<link>https://arxiv.org/abs/2503.08677</link>
<guid>https://arxiv.org/abs/2503.08677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniPaint框架通过重新概念化对象去除和插入，提高了图像编辑的精确性。</p><br /><br /><p><strong>摘要：</strong> Diffusion-based生成模型在对象编辑领域取得了重大进展，但在实际的对象去除和插入中仍面临诸多挑战。本文提出的OmniPaint是一种统一框架，通过将对象去除和插入视为相互依赖的过程，来解决这些问题。该框架利用预训练的扩散先验和渐进训练管道，包括初步的成对样本优化和随后的大规模无配对优化，成功实现了准确的前景去除和无缝的对象插入，同时忠实地保留了场景的几何形状和内在属性。此外，新的CFD度量提供了一种稳健的、无参考的评估方法，用于衡量上下文一致性和对象幻觉，为高保真图像编辑设立了新的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:55:27 GMT</pubDate>
</item>
<item>
<title>文本到图像模型在分类概念生成中的应用研究</title>
<link>https://arxiv.org/abs/2503.10357</link>
<guid>https://arxiv.org/abs/2503.10357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究了在零样本设置下使用文本到图像模型为分类概念生成图像的可行性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了文本到图像模型在零样本设置下生成分类概念图像的可行性，提出了一项全面的分类图像生成基准，评估模型理解分类概念并生成相关高质量图像的能力。基准包括常识和随机抽样的WordNet概念，以及大语言模型生成的预测。12个模型通过9种新颖的与分类相关的文本到图像指标和人类反馈进行评估。此外，本文首次采用与GPT-4反馈的成对评估方式。实验结果表明，模型的排名与标准的文本到图像任务明显不同，Playground-v2和FLUX在各指标和子集上始终表现优异，而检索基础方法的效果较差。这一发现突显了自动化结构化数据资源整理的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:37:54 GMT</pubDate>
</item>
<item>
<title>VisualPRM：增强多模态推理能力的过程奖励模型</title>
<link>https://arxiv.org/abs/2503.10291</link>
<guid>https://arxiv.org/abs/2503.10291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualPRM模型提升了多模态大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VisualPRM，一个拥有80亿参数的先进多模态过程奖励模型，显著提升了现有多模态大语言模型（MLLMs）的推理能力。我们的模型在三种类型的MLLMs与四种不同规模的模型上表现优异，尤其是在高能力的InternVL2.5-78B模型上，跨七个多模态推理基准提升了5.9分。通过实验结果，我们证明了VisualPRM在Best-of-N（BoN）评估策略中相较于结果奖励模型和自一致性具有更强的性能。此外，我们构建了一个多模态过程监督数据集VisualPRM400K，并提出了VisualProcessBench基准，以人类标注的逐步正确性标签来评估PRMs在多模态推理任务中识别错误步骤的能力。希望我们的研究能够启发未来的研究工作，并为MLLMs的发展做出贡献。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 08:03:37 GMT</pubDate>
</item>
<item>
<title>探究视觉语言模型在图像理解中的不足</title>
<link>https://arxiv.org/abs/2503.09837</link>
<guid>https://arxiv.org/abs/2503.09837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，视觉语言模型在图像级增强理解方面存在缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文重点探讨视觉语言模型（VLMs）在图像理解中的局限性，特别是OpenAI的CLIP和Google的SigLIP。虽然这些模型在多个下游任务上表现良好，如图像/视频生成、视觉问答、多模态聊天机器人及视频理解，但它们在基本图像变换方面表现不佳。为此，我们创建了增强版Flickr8k数据集，将每张图像与所应用变换的详细描述配对。研究结果表明，这些模型对多种图像增强缺乏理解，进而影响了下游任务，尤其是在图像编辑中。此外，我们还评估了最先进的图像转图像模型在简单变换上的表现，以提供更深入的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 16:58:16 GMT</pubDate>
</item>
<item>
<title>CoRe²: 高效且有效的文本到图像生成模型推理框架</title>
<link>https://arxiv.org/abs/2503.09662</link>
<guid>https://arxiv.org/abs/2503.09662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoRe² 提出了一个新颖的文本到图像生成推理方法，兼顾效率与效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了新颖的推理模式 CoRe²，用于提升文本到图像生成模型的采样效率与视觉质量。以往研究通常在提高图像合成质量和加速采样之间做出取舍，且难以确保在扩散模型和自回归模型上表现一致。CoRe² 包括三个子过程：Collect、Reflect 和 Refine。首先，收集无分类器引导轨迹；然后利用这些数据训练一个反映易学内容的弱模型，从而在推理过程中将函数评估次数减少一半。最后，通过弱到强的引导方法来细化条件输出，提高生成高频真实内容的能力。CoRe² 在多个生成模型上展示了显著的性能提升，并且可以无缝集成至现有的最先进模型 Z-Sampling 上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 11:15:25 GMT</pubDate>
</item>
<item>
<title>PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling</title>
<link>https://arxiv.org/abs/2503.09368</link>
<guid>https://arxiv.org/abs/2503.09368</guid>
<content:encoded><![CDATA[
We introduce PerCoV2, a novel and open ultra-low bit-rate perceptual image compression system designed for bandwidth- and storage-constrained applications. Building upon prior work by Careil et al., PerCoV2 extends the original formulation to the Stable Diffusion 3 ecosystem and enhances entropy coding efficiency by explicitly modeling the discrete hyper-latent image distribution. To this end, we conduct a comprehensive comparison of recent autoregressive methods (VAR and MaskGIT) for entropy modeling and evaluate our approach on the large-scale MSCOCO-30k benchmark. Compared to previous work, PerCoV2 (i) achieves higher image fidelity at even lower bit-rates while maintaining competitive perceptual quality, (ii) features a hybrid generation mode for further bit-rate savings, and (iii) is built solely on public components. Code and trained models will be released at https://github.com/Nikolai10/PerCoV2.
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 09:14:51 GMT</pubDate>
</item>
<item>
<title>探索Hugging Face模型的初步图谱及其潜力</title>
<link>https://arxiv.org/abs/2503.10633</link>
<guid>https://arxiv.org/abs/2503.10633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章构建了Hugging Face模型的初步图谱，并探讨其应用和未文档区域的映射方法。</p><br /><br /><p><strong>摘要：</strong> 随着公共神经网络数量的激增，搜索和分析大型模型库变得愈加重要。本文构建了一幅初步的Hugging Face模型图谱，展示了模型的文档化比例，并提供了模型景观及其演变的可视化。我们展示了该图谱的若干应用，包括准确性预测和计算机视觉模型趋势分析。鉴于目前图谱仍不完整，我们提出了一种映射未文档区域的方法，通过识别基于主流模型训练实践的高置信度结构先验，准确地映射之前未记录的区域。最终，我们公开发布了相关数据集、代码和交互图谱，为推动模型研究提供支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>动态Tanh：非标准化变压器的性能提升</title>
<link>https://arxiv.org/abs/2503.10622</link>
<guid>https://arxiv.org/abs/2503.10622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">动态Tanh替代标准化层，变压器性能可比或优于传统方法。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了动态Tanh（DyT）作为变压器中标准化层的替代方案，表明不使用标准化层的变压器依然能够达到相同或更好的性能。DyT操作为DyT(x) = tanh(alpha x)，其灵感来自于变压器中层归一化的S型输入输出映射特性。实验结果表明，在多种任务设置下，使用DyT的变压器能够匹敌或超越传统标准化变压器的性能，且大多数情况下无需进行超参数调优。这一发现挑战了传统观点，即标准化层在现代神经网络中是不可或缺的，对深度网络中的标准化作用提供了新的洞见。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:06 GMT</pubDate>
</item>
<item>
<title>Diffusion Transformers在文本到图像生成中的应用研究</title>
<link>https://arxiv.org/abs/2503.10618</link>
<guid>https://arxiv.org/abs/2503.10618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究评估Diffusion Transformers在文本到图像生成中的表现和效率。</p><br /><br /><p><strong>摘要：</strong> 本研究针对Diffusion Transformers（DiTs）在文本到图像生成中的应用进行实证研究，重点考察其架构选择、文本条件策略和训练协议。我们评估了多种基于DiT的架构，包括PixArt风格和MMDiT变种，并与标准DiT变种进行比较。令人惊讶的是，标准DiT的性能与这些专用模型相当，并在参数效率上表现出色，尤其在模型规模扩大时。通过利用层级参数共享策略，我们相比于MMDiT架构进一步降低了66%的模型大小，且对性能影响甚微。基于对文本编码器和变分自编码器的深入分析，我们提出了DiT-Air和DiT-Air-Lite模型。其中，DiT-Air在有监督和奖励微调下，在GenEval和T2I CompBench上实现了最先进的性能，而DiT-Air-Lite凭借其紧凑的尺寸依然具有强竞争力，超越了大多数现有模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:57:25 GMT</pubDate>
</item>
<item>
<title>SANA-Sprint：高效的文本到图像生成模型</title>
<link>https://arxiv.org/abs/2503.09641</link>
<guid>https://arxiv.org/abs/2503.09641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SANA-Sprint是一种超快速的文本到图像生成模型，显著提高生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SANA-Sprint，一种高效的扩散模型，用于超快速的文本到图像（T2I）生成。该模型基于预训练基础模型，通过混合蒸馏显著降低推理步骤，从20步减少至1-4步。我们提出三大创新：第一，采用无训练的方法，将预训练的流匹配模型转化为连续时间一致性蒸馏，消除从头训练的高成本，提升训练效率；第二，SANA-Sprint是统一的步自适应模型，可以在1-4步中实现高质量生成，无需步骤特定训练，进一步提高效率；第三，SANA-Sprint与ControlNet结合，实现实时交互图像生成，为用户交互提供即时视觉反馈。SANA-Sprint在速度和质量的权衡上建立了新的Pareto前沿，在仅需1步的情况下达到7.59的FID值和0.74的GenEval，表现优于FLUX-schnell，同时速度快10倍。可在H100上实现1024 x 1024图像的0.1秒（T2I）和0.25秒（ControlNet）延迟，并在RTX 4090上达到0.31秒，展现了其出色的效率及在AI驱动消费者应用中的潜力。代码和预训练模型将开源发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 00:53:07 GMT</pubDate>
</item>
<item>
<title>Generation Chain-of-Thought: 提升图像生成与编辑的推理驱动框架</title>
<link>https://arxiv.org/abs/2503.10639</link>
<guid>https://arxiv.org/abs/2503.10639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GoT通过推理链改进图像生成和编辑，提升图像与人类意图的契合度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了Generation Chain-of-Thought（GoT）框架，通过明确的语言推理过程改进图像生成和编辑方式，区别于传统的文本直接输入处理。GoT分析语义关系和空间布局，构建了超过900万个样本的大规模数据集，捕捉语义-空间关系的详细推理链。我们结合Qwen2.5-VL构建统一框架，将推理链生成与端到端的扩散模型整合，并引入新的语义空间引导模块。实验表明，GoT在生成和编辑任务中均显著超越基线，表现出优异性能。同时，此方法支持交互式视觉生成，用户可显性交互修改推理步骤，以精确调整图像。这一研究为推理驱动的视觉生成和编辑开辟了新方向，生成的图像更符合人类意图，并将数据集、代码和预训练模型公开，以促进未来的研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>提升蒸馏扩散模型多样性的研究</title>
<link>https://arxiv.org/abs/2503.10637</link>
<guid>https://arxiv.org/abs/2503.10637</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨通过控制蒸馏与混合推理提升蒸馏模型的样本多样性。</p><br /><br /><p><strong>摘要：</strong> 蒸馏扩散模型存在样本多样性降低的限制。本文揭示尽管存在多样性损失，蒸馏模型依然保留基础模型的概念表示。通过引入控制蒸馏，我们展示了控制机制如Concept Sliders与LoRAs能够无缝转移到蒸馏模型及其反向。为理解蒸馏如何影响多样性，我们引入Diffusion Target (DT) 可视化工具，揭示模型在中间步骤的输出预测。通过DT可视化，识别生成伪影和不一致性，发现初始扩散时间步对输出多样性影响巨大，而后期步骤主要细化细节。基于此，我们提出了多样性蒸馏的混合推理方法，先在关键时刻使用基础模型，然后再切换到高效的蒸馏模型。实验表明，此方法不仅恢复了多样性能力，还超越了基础模型，同时保持了蒸馏推理的计算效率，无需额外训练或修改模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10637" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow-Based Generation</title>
<link>https://arxiv.org/abs/2503.10636</link>
<guid>https://arxiv.org/abs/2503.10636</guid>
<content:encoded><![CDATA[
Minibatch optimal transport coupling straightens paths in unconditional flow matching. This leads to computationally less demanding inference as fewer integration steps and less complex numerical solvers can be employed when numerically solving an ordinary differential equation at test time. However, in the conditional setting, minibatch optimal transport falls short. This is because the default optimal transport mapping disregards conditions, resulting in a conditionally skewed prior distribution during training. In contrast, at test time, we have no access to the skewed prior, and instead sample from the full, unbiased prior distribution. This gap between training and testing leads to a subpar performance. To bridge this gap, we propose conditional optimal transport C^2OT that adds a conditional weighting term in the cost matrix when computing the optimal transport assignment. Experiments demonstrate that this simple fix works with both discrete and continuous conditions in 8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method performs better overall compared to the existing baselines across different function evaluation budgets. Code is available at https://hkchengrex.github.io/C2OT
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>通用零样本目标导航的统一框架</title>
<link>https://arxiv.org/abs/2503.10630</link>
<guid>https://arxiv.org/abs/2503.10630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通用零样本导航框架，利用统一图表示不同目标。</p><br /><br /><p><strong>摘要：</strong> 本文提出一个通用的零样本目标导向导航框架，旨在解决现有方法在处理不同导航目标时的局限性。现有的零样本方法通常依赖特定任务的推理框架，缺乏泛化能力。为此，我们提出统一图表示，将目标统一为对象类别、实例图像和文本描述，并将代理的观察转化为在线维护的场景图。通过一致的场景和目标表示，我们保留了大部分结构信息，能够有效利用大型语言模型进行图基础推理。具体而言，我们在每个时刻进行场景图与目标图的匹配，采取不同策略生成长期探索目标。实验结果表明，UniGoal在三个导航任务上实现了最先进的零样本性能，超越了任务特定的零样本方法和监督通用方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:59:48 GMT</pubDate>
</item>
<item>
<title>R1-Onevision：跨模态推理模型的创新与评估</title>
<link>https://arxiv.org/abs/2503.10615</link>
<guid>https://arxiv.org/abs/2503.10615</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R1-Onevision模型提升了视觉与文本的推理能力，显示出卓越的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了R1-Onevision，一种新型的跨模态推理模型，旨在解决视觉内容分析和推理中的挑战。通过将图像转换为正式的文本表示，R1-Onevision能实现精确的语言基础推理。此外，我们构建了R1-Onevision数据集，提供跨多个领域的详细推理注释，以支持复杂的多模态推理任务。为了训练R1-Onevision模型，我们采用了监督微调和强化学习的方法，从而培养其先进的推理能力和强大的泛化能力。为全面评估多模态推理性能，本文还推出了R1-Onevision-Bench基准，覆盖了从初中到大学人类教育阶段的考试内容。实验结果表明，R1-Onevision在多个多模态推理基准测试中表现出色，超越了GPT-4o和Qwen2.5-VL等模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10615" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:56:05 GMT</pubDate>
</item>
<item>
<title>结合LLMs与图搜索的高效多回合图像编辑方法CoSTA*</title>
<link>https://arxiv.org/abs/2503.10613</link>
<guid>https://arxiv.org/abs/2503.10613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSTA*，通过结合LLMs与图搜索，优化多回合图像编辑。</p><br /><br /><p><strong>摘要：</strong> 文章提出了一个新方法CoSTA*，旨在优化多回合图像编辑任务。传统的图像生成模型在处理复杂的多回合编辑时表现不佳，本文通过将任务分解为代理工作流，利用LLMs生成子任务树，从而合理修剪AI工具的图，并采用A*搜索在小型子图中寻找工具路径。CoSTA*不仅平衡了各任务的成本和质量，还能自动在子任务之间切换不同的模式以获得更优的成本-质量权衡。此外，模型在每个子任务中使用视觉语言模型进行评估，在发生失败时更新工具的成本和质量，从而实现快速恢复。实验结果表明，CoSTA*在新构建的多回合图像编辑基准上，成本和质量均优于现有最先进的图像编辑模型，能够根据用户偏好进行灵活调节。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:55:45 GMT</pubDate>
</item>
<item>
<title>GroundingSuite：推动视觉与语言交互的创新数据集</title>
<link>https://arxiv.org/abs/2503.10596</link>
<guid>https://arxiv.org/abs/2503.10596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GroundingSuite通过自动数据标注框架提升视觉与语言处理性能。</p><br /><br /><p><strong>摘要：</strong> Pixel grounding，特别是参考表达分割（RES）任务，在视觉与语言结合上展现出巨大潜力，但现有数据集的局限性制约了这一领域的发展。为克服这些限制，本文提出了GroundingSuite，包括：1）利用多个视觉-语言模型（VLM）代理的自动数据标注框架；2）包含956万多样化参考表达及其对应分割的大规模训练数据集；3）由3,800幅图像组成的精心策划的评估基准。GroundingSuite的训练数据集显著提高了模型的性能，使其在gRefCOCO上达到68.9的cIoU，在RefCOCOm上达到55.3的gIoU。此外，GroundingSuite的数据标注框架相比于目前领先的标注方法，效率表现优越，速度是GLaMM的4.5倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:43:10 GMT</pubDate>
</item>
<item>
<title>长上下文调优：提升视频生成一致性的训练方法</title>
<link>https://arxiv.org/abs/2503.10589</link>
<guid>https://arxiv.org/abs/2503.10589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法，提升视频生成的场景一致性。</p><br /><br /><p><strong>摘要：</strong> 近期视频生成技术虽能生成逼真的单镜头视频，但真实的叙事视频需要在多个镜头间保持视觉和动态的一致性。为此，本文提出了长上下文调优（LCT）训练范式，旨在扩展预训练单镜头视频扩散模型的上下文窗口，从数据中直接学习场景级一致性。该方法将全注意力机制从单个镜头扩展到包括场景内所有镜头，引入交错的三维位置嵌入和异步噪声策略，使得在不增加额外参数的情况下实现共同和自回归镜头生成。经过LCT后，具有双向注意力的模型可以进一步通过上下文因果注意力进行微调，促进自回归生成，并高效利用KV缓存。实验结果表明，经过LCT的单镜头模型能够生成连贯的多镜头场景，展现出组合生成和交互式镜头扩展等新兴能力，为更实用的视觉内容创作铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:40:07 GMT</pubDate>
</item>
<item>
<title>VisualWebInstruct：提升视觉语言模型推理能力的新数据集</title>
<link>https://arxiv.org/abs/2503.10582</link>
<guid>https://arxiv.org/abs/2503.10582</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出VisualWebInstruct数据集，提升视觉语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 随着视觉语言模型（VLMs）在视觉感知任务上取得显著进展，其在推理任务上的表现却因高质量训练数据的不足而受限。为了解决推理聚焦多模态数据集的稀缺问题，本文提出了一种名为VisualWebInstruct的新方法，利用搜索引擎创建涵盖多个学科（如数学、物理、金融、化学等）的多样化高质量数据集。通过从精心挑选的30000张种子图像开始，通过谷歌图片搜索找到相关网站，并处理来自70万个独特URL源的HTML数据，最终建立了约90万个问答对的数据集，其中40%为视觉问答对。对比实验中，基于VisualWebInstruct进行微调的模型表现显著提升，其中MAmmoTH-VL2模型在10B参数分类中达到MMM-pro-std（40.7%）、MathVerse（42.6%）、DynaMath（55.7%）等基准测试的最优表现。这些显著成果突显了该数据集在增强VLM给复杂多模态任务的推理能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10582" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:32:48 GMT</pubDate>
</item>
<item>
<title>ARPG：一种新型视觉自回归模型的提出</title>
<link>https://arxiv.org/abs/2503.10568</link>
<guid>https://arxiv.org/abs/2503.10568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARPG模型通过随机生成和并行处理，提高了生成效率与零-shot泛化能力。</p><br /><br /><p><strong>摘要：</strong> ARPG是一种新出现的视觉自回归模型，专注于解决传统栅格顺序方法在推理效率和零-shot泛化中的固有限制。其核心思想是有效的随机顺序建模需要明确的下一预测标记的位置引导。为此，ARPG提出了一种新颖的引导解码框架，将位置信息与内容表示分离编码，形成查询和键值对。通过将这种引导直接纳入因果注意机制，ARPG实现了完全的随机顺序训练与生成，消除了双向注意力的需求。此模型在图像修复、外延和分辨率扩展等零-shot任务上表现出色，同时通过共享KV缓存并行处理多个查询，支持快速推理。在ImageNet-1K 256基准上，ARPG在仅用64个采样步骤的情况下，取得了1.94的FID，相较于同规模的自回归模型提高了20倍以上的吞吐量，并将内存消耗减少了超过75%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10568" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 13:19:51 GMT</pubDate>
</item>
<item>
<title>基于双偏好优化的新框架提升大型视觉语言模型的任务规划能力</title>
<link>https://arxiv.org/abs/2503.10480</link>
<guid>https://arxiv.org/abs/2503.10480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出双偏好优化框架，提升视觉语言模型的任务规划能力与效率。</p><br /><br /><p><strong>摘要：</strong> 最近大型视觉语言模型（LVLMs）在身体任务规划方面表现出色，但仍面临依赖约束和效率等基本挑战。现有方法主要优化动作选择或在推理过程中利用世界模型，未能充分利用通过建模世界来提升规划能力的优势。我们提出了一种名为双偏好优化（D^2PO）的新学习框架，通过偏好学习共同优化状态预测和动作选择，使LVLMs能够理解环境动态，从而更好地进行规划。为无人工注释自动收集轨迹和逐步偏好数据，我们引入了一种树搜索机制，通过试错法进行广泛探索。在VoTa-Bench的广泛实验中，我们基于D^2PO的方法显著超越现有方法和GPT-4o，并在应用于Qwen2-VL（7B）、LLaVA-1.6（7B）和LLaMA-3.2（11B）时，实现了更高的任务成功率和更高效的执行路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10480" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 11:49:56 GMT</pubDate>
</item>
<item>
<title>Light-R1系列模型训练与性能提升研究</title>
<link>https://arxiv.org/abs/2503.10460</link>
<guid>https://arxiv.org/abs/2503.10460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Light-R1系列模型的训练过程及其卓越表现。</p><br /><br /><p><strong>摘要：</strong> 本文展示了我们在Light-R1系列模型上的研究工作，所有模型、数据和代码均已公开。我们重点训练了从不具备长COT能力的初始模型开始的长COT模型。通过采用包括两阶段的SFT和半在线DPO的课程训练方法，我们训练的Light-R1-32B模型在数学任务上表现优于DeepSeek-R1-Distill-Qwen-32B。尽管仅使用数学数据进行训练，Light-R1-32B在其他领域也展现了强大的泛化能力。此外，我们强调了为第二阶段SFT构建的3k数据集在增强其他模型中的显著效果，通过对DeepSeek-R1-Distilled模型进行微调，获得了7B和14B的新SOTA模型。最后，我们通过强化学习（GRPO）进一步提升了长COT模型的推理性能，训练的Light-R1-14B-DS在14B模型中实现了SOTA性能，AIME24和25得分分别为74.0和60.2，超越了许多32B模型。该系列工作验证了从头训练长COT模型的可行性，并展示了在SFT数据上的创新成果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 11:29:22 GMT</pubDate>
</item>
<item>
<title>4D LangSplat：动态场景中的时间敏感语言查询</title>
<link>https://arxiv.org/abs/2503.10437</link>
<guid>https://arxiv.org/abs/2503.10437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">4D LangSplat实现了动态场景中时间敏感的语言查询。</p><br /><br /><p><strong>摘要：</strong> 本文提出了4D LangSplat，一个用于动态场景中的4D语言场学习的模型。相较于传统的CLIP特征，4D LangSplat能够更好地处理时间敏感和时间无关的开放词汇查询，通过直接从多模态大语言模型生成的对象级视频字幕中学习。该模型利用视觉和文本提示的多模态对象视频提示方法生成高质量、一致性的对象字幕，为开放词汇文本查询提供了像素对齐的特征监督。此外，作者还引入了状态可变形网络，旨在有效地建模动态场景中对象状态的连续变化。实验证明，4D LangSplat在多个基准测试中表现出色，显示出其在处理复杂动态场景中的高效性和精准性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:58:22 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的多主体视频生成框架CINEMA</title>
<link>https://arxiv.org/abs/2503.10391</link>
<guid>https://arxiv.org/abs/2503.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CINEMA框架，增强多主体视频生成的连贯性与一致性。</p><br /><br /><p><strong>摘要：</strong> 随着深度生成模型的发展，视频生成取得了显著进展，但个性化的多主体视频生成仍然面临诸多挑战。现有方法通常依赖于将主体图像映射到文本提示中的关键字，这一过程带来了歧义，并限制了主体间关系的建模能力。本文提出了CINEMA，一个创新的方案，通过多模态大语言模型（MLLM）实现一致的多主体视频生成。其方法消除了主体图像与文本实体之间的显式对应需求，从而减少了歧义和标注工作量。通过利用MLLM理解主体关系，该方法提高了可扩展性，便于使用大量多样化的数据集进行训练。此外，CINEMA框架能够适应不同数量的主体条件，提供了个性化内容创作的更大灵活性。经过广泛评估，结果显示该方法在主体一致性和整体视频连贯性上显著改进，为故事讲述、互动媒体及个性化视频生成的高级应用铺平了道路。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 10:07:58 GMT</pubDate>
</item>
<item>
<title>大型推理模型在机器翻译中的变革与挑战</title>
<link>https://arxiv.org/abs/2503.10351</link>
<guid>https://arxiv.org/abs/2503.10351</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型推理模型通过动态推理重塑机器翻译的传统模式。</p><br /><br /><p><strong>摘要：</strong> 近期大型推理模型（LRMs）特别是链式推理（CoT）的进展，为机器翻译（MT）开辟了新可能性。本文讨论LRMs如何通过重新定义翻译为需要上下文、文化和语言理解的动态推理任务，显著改变了传统神经MT和基于LLMs的MT范式。LRMs在翻译中的三个基础转变包括：1) 上下文连贯性，通过跨句子和复杂上下文的显性推理解决歧义；2) 文化意图，通过推测说话者意图、听众期待和社会语言规范来调整输出；3) 自我反思，LRMs在推理时进行自我反思，校正潜在翻译错误，提高了鲁棒性。文章还探讨了风格化翻译、文档级翻译和多模态翻译的各种场景，展示了LRMs在这些领域的优势，以及自动中介翻译和过度本地化等挑战。总之，LRMs重新定义了翻译系统，使其不仅仅是文本转换器，而是能够超越文本理解含义的多语言认知主体。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10351" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 09:27:53 GMT</pubDate>
</item>
<item>
<title>开源软件开发中错误报告讨论的毒性影响</title>
<link>https://arxiv.org/abs/2503.10072</link>
<guid>https://arxiv.org/abs/2503.10072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示错误报告讨论中的毒性对开源开发合作有重大影响。</p><br /><br /><p><strong>摘要：</strong> 本研究分析了GitHub上203个错误报告线程，探讨了毒性在错误报告讨论中的表现及影响。尽管错误报告对于识别和解决缺陷至关重要，但其问题导向的特性和情感因素使其易受毒性互动的影响。研究发现，毒性常常源于对错误严重性和优先级的误解、对工具的不满以及职业沟通的缺失。这些毒性互动不仅干扰了有效的讨论，还降低了产出可行结果的可能性。为缓解这种毒性，我们提出了一些切实可行的建议，以促进错误的解决。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.10072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Mar 2025 01:39:29 GMT</pubDate>
</item>
<item>
<title>Whisper模型的性能分析与量化方法研究</title>
<link>https://arxiv.org/abs/2503.09905</link>
<guid>https://arxiv.org/abs/2503.09905</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究Whisper模型及其变体在语音转录中的表现与量化影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了自动语音识别(ASR)模型Whisper及其两个变体，分别针对实时语音流和离线转录进行优化。研究发现，这些模型生成的幻觉内容降低了转录的可靠性，同时较大模型变体的延迟增加，对资源受限设备的部署构成挑战。本文通过定性分析三种Whisper模型的相似性与差异，接着量化模型量化对延迟的影响，并评估其在边缘设备部署的可行性。使用开源LibriSpeech数据集，评估了whispercpp在三种量化方法(INT4、INT5、INT8)下的字错误率(WER)及延迟分析。结果表明，量化可将延迟降低19%并将模型大小减少45%，同时保持转录准确率。这些发现为不同Whisper模型的最佳应用场景及边缘设备的部署可能性提供了洞见。所有代码、数据集和实现细节已在公共GitHub库中发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09905" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 19:50:35 GMT</pubDate>
</item>
<item>
<title>静默品牌攻击：数据中毒对文本生成图像模型的影响</title>
<link>https://arxiv.org/abs/2503.09669</link>
<guid>https://arxiv.org/abs/2503.09669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的数据中毒方法，悄然植入品牌标志于生成图像中。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种静默品牌攻击的方法，这是一种针对文本生成图像扩散模型的新型数据中毒技术。我们发现，通过在训练数据中反复使用特定的视觉模式，模型能够自然生成包含这些模式的图像，即使没有文本提示。我们开发了一种自动化的数据中毒算法，能够无缝地将品牌标志注入原始图像中，并确保其与背景自然融合，不易被检测。经过实验验证，该方法在大规模高质量图像数据集和风格个性化数据集上表现出色，成功将标志嵌入生成的图像中，而不会影响图像质量和文本对齐。人类评估和定量指标（包括标志检测）显示，该方法能够隐秘地嵌入品牌标志，具有高成功率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:21:57 GMT</pubDate>
</item>
<item>
<title>Open-Sora 2.0：高效的商业级视频生成模型</title>
<link>https://arxiv.org/abs/2503.09642</link>
<guid>https://arxiv.org/abs/2503.09642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Open-Sora 2.0是一个成本可控的高效视频生成模型，促进内容创作的创新。</p><br /><br /><p><strong>摘要：</strong> 在过去一年，视频生成模型取得显著进展，但与此同时，模型规模、数据量及训练计算需求也显著增加。本报告介绍了Open-Sora 2.0，一个仅用20万美元训练的商业级视频生成模型。我们展示了如何通过数据整理、模型架构、训练策略和系统优化等技术，实现高效的成本控制。根据人类评估结果和VBench评分，Open-Sora 2.0的表现与全球顶尖的视频生成模型相媲美，包括开源的HunyuanVideo和闭源的Runway Gen-3 Alpha。我们将Open-Sora 2.0完全开源，以此来民主化访问先进的视频生成技术，促进内容创作的更广泛创新与创造性。所有相关资源已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 01:00:07 GMT</pubDate>
</item>
<item>
<title>长输出生成的研究重要性与挑战</title>
<link>https://arxiv.org/abs/2503.04723</link>
<guid>https://arxiv.org/abs/2503.04723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨长输出生成在长文本理解中的重要性和研究缺口。</p><br /><br /><p><strong>摘要：</strong> 近年来，长上下文大语言模型（LLMs）的进展主要集中在处理扩展输入上下文上，长文本理解取得了显著成绩。然而，生成长文本输出的能力却受到较少关注。本文呼吁自然语言处理（NLP）研究转向应对长输出生成的挑战，这对于小说创作、长期规划和复杂推理等任务尤为重要，需要模型理解广泛的上下文并生成连贯、富有情境感和逻辑一致的扩展文本。由此，当前LLM能力中存在的关键缺口得到凸显，强调了这一未被充分探索领域的重要性，并呼吁针对开发能够生成高质量长输出的基础LLM进行更深入的研究，这在实际应用中具有巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 13:59:37 GMT</pubDate>
</item>
<item>
<title>Search-R1：通过强化学习优化的大型语言模型检索能力</title>
<link>https://arxiv.org/abs/2503.09516</link>
<guid>https://arxiv.org/abs/2503.09516</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出Search-R1，通过强化学习提升LLM在检索中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Search-R1，这是一种DeepSeek-R1模型的扩展，旨在提升大型语言模型（LLMs）在有效推理和文本生成中的知识获取能力。与传统的检索增强和工具使用训练方法相比，Search-R1通过强化学习自动生成多条搜索查询，从而实现多轮检索交互。实验结果表明，Search-R1在七个问答数据集上的表现相较于现有最先进的基线提高了26%（Qwen2.5-7B）、21%（Qwen2.5-3B）和10%（LLaMA3.2-3B）。文章还提供了对强化学习优化方法、LLM选择和检索增强推理中的响应长度动态的实证见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09516" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 12:26:39 GMT</pubDate>
</item>
<item>
<title>改进机器学习力场在分布转移中的泛化能力</title>
<link>https://arxiv.org/abs/2503.08674</link>
<guid>https://arxiv.org/abs/2503.08674</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出了改进机器学习力场泛化能力的新方法。</p><br /><br /><p><strong>摘要：</strong> 机器学习力场（MLFFs）为高成本的量子力学分子模拟提供了有前景的替代方案。考虑到化学空间的多样性及新数据生成的高昂成本，理解MLFFs如何超越其训练分布的泛化能力变得至关重要。通过对化学数据集进行诊断实验，本文揭示了常见的分布转移问题，甚至对在大量数据上训练的大型基础模型而言，这些问题仍然构成显著挑战。基于观察，作者假设当前的监督训练方法对于MLFFs的正则化不足，导致过拟合并学习到不良的分布外系统表示。为此，本文提出了两种新方法，旨在减轻MLFFs的分布转移问题，集中于在测试阶段的精细化策略，且计算成本最低，不依赖于昂贵的量子力学参考标签。这些策略有效减少了分布外系统的误差，表明MLFFs可以并且应能更好地建模多样的化学空间，然而其培训过程尚未有效调动这一潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08674" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:54:29 GMT</pubDate>
</item>
<item>
<title>PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?</title>
<link>https://arxiv.org/abs/2503.05333</link>
<guid>https://arxiv.org/abs/2503.05333</guid>
<content:encoded><![CDATA[
The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness. Data, baseline models and evaluation code http://www.physics-gen.org.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 06:19:13 GMT</pubDate>
</item>
<item>
<title>BIMBA模型：应对长视频的高效视频问答</title>
<link>https://arxiv.org/abs/2503.09590</link>
<guid>https://arxiv.org/abs/2503.09590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BIMBA模型通过选择性扫描算法提升长视频问答的效率与准确性。</p><br /><br /><p><strong>摘要：</strong> 视频问答（VQA）在处理长视频时面临提取相关信息和建模长距离依赖关系的挑战。现有方法通常依赖压缩策略来降低计算成本，但容易错过重要事件和快速出现的时空模式。本文介绍了BIMBA，一个高效的状态空间模型，旨在有效处理长视频。该模型利用选择性扫描算法，从高维视频中选择重要信息，并将其转换为压缩的令牌序列，以提高大语言模型（LLM）的处理效率。通过大量实验，BIMBA在多个长视频问答基准上达到了最佳准确率，展示了其有效性。代码和模型可在公网上获取。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:57:32 GMT</pubDate>
</item>
<item>
<title>一种基于扩散的蒙特卡洛采样方法提升学习型RANSAC的泛化能力</title>
<link>https://arxiv.org/abs/2503.09410</link>
<guid>https://arxiv.org/abs/2503.09410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">该研究提出一种新方法提升学习型RANSAC在噪声数据下的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了一种创新的扩散基础范式，通过逐步向真实数据注入噪声，以模拟学习型RANSAC的训练环境。现有的学习型RANSAC方法常常因在相同算法生成的数据上训练导致其对分布外数据的泛化性能有限。为改善这一问题，我们将蒙特卡洛采样结合使用于扩散过程，在多个阶段引入不同类型的随机性，以增强数据的多样性。通过在ScanNet和MegaDepth数据集上进行的综合实验，本方法的蒙特卡洛扩散机制显著提升了学习型RANSAC的泛化能力。同时，广泛的消融研究证明了框架中关键组件的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:01:18 GMT</pubDate>
</item>
<item>
<title>小型语言模型的自我纠错机制研究</title>
<link>https://arxiv.org/abs/2503.08681</link>
<guid>https://arxiv.org/abs/2503.08681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出STaSC算法，探索小型语言模型的自我纠错能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型在多种任务中表现优异，但仍然容易出错，关键挑战在于如何使其自我纠错。本文研究了通过仅使用自生成数据进行迭代微调的小型语言模型的自我纠错能力，提出了自教学自我纠错（STaSC）算法，并结合多种算法设计选择。实验结果显示，STaSC在问答任务中有效学习自我纠错，显著提升了性能。进一步的分析揭示了自我纠错机制及不同设计选择对学习动态和整体表现的影响。为支持后续研究，本文还发布了用户友好的代码库和轻量级模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:57:44 GMT</pubDate>
</item>
<item>
<title>基于多代理的智能医疗助手：克服隐私与延迟挑战</title>
<link>https://arxiv.org/abs/2503.05397</link>
<guid>https://arxiv.org/abs/2503.05397</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新型的多代理医疗助手，有效解决隐私和延迟问题。</p><br /><br /><p><strong>摘要：</strong> 大型动作模型（LAMs）在智能自动化领域已带来革命性变化，但在医疗健康方面的应用仍面临隐私、延迟和对互联网依赖等挑战。为此，本文提出了一种基于设备的多代理医疗助手，旨在克服这些限制。该系统利用较小的任务特定代理来优化资源，确保可扩展性和高性能。我们设计的系统作为医疗需求的一站式解决方案，具备预约服务、健康监测、用药提醒和每日健康报告等功能。基于Qwen Code Instruct 2.5 7B模型，规划者与呼叫代理在任务规划和呼叫方面的平均RougeL得分分别为85.5和96.5，同时轻量化以便于设备端部署。这一创新方法结合了设备端系统的优势与多代理架构，为用户中心的医疗解决方案开辟了新路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05397" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 08:20:12 GMT</pubDate>
</item>
<item>
<title>RewardSDS: 基于对齐评分的采样优化方法</title>
<link>https://arxiv.org/abs/2503.09601</link>
<guid>https://arxiv.org/abs/2503.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RewardSDS通过对齐评分优化了2D扩散模型的生成效果。</p><br /><br /><p><strong>摘要：</strong> Score Distillation Sampling（SDS）在利用2D扩散先验进行文本到3D生成等任务中表现出色，但其在精细对齐用户意图方面存在困难。为了解决此问题，我们提出了RewardSDS，一种新方法通过根据奖励模型的对齐评分对噪声样本进行加权，生成加权的SDS损失。这种损失函数优先考虑那些能产生高奖励输出的噪声样本的梯度。我们的这一方法可广泛适用于SDS基础上，并特别引入了RewardVSD，优化变分评分蒸馏。我们在文本-图像、2D编辑和文本-3D生成任务上评估了RewardSDS和RewardVSD，相较于传统SDS和VSD，我们显示出在生成质量和与所需奖励模型对齐性方面的显著提升，实现了最新的性能水平。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的文本块处理优化研究</title>
<link>https://arxiv.org/abs/2503.09600</link>
<guid>https://arxiv.org/abs/2503.09600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种新的文本块处理评估方法与框架，提升了RAG系统性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在检索增强生成（RAG）过程中，文本块处理的重要性并提出了一种双指标评估方法，包括边界清晰度和块粘性，以量化文本块质量。通过对传统和语义块处理的局限性分析，强调了将大语言模型（LLM）纳入文本块处理的必要性。为解决LLM方法在计算效率和精度之间的权衡，提出了细粒度混合块处理器（MoC）框架，采用三阶段处理机制。该框架指导块处理器生成结构化的块正则表达式，从而有效提取文本块。大量实验表明，所提指标和MoC框架成功解决了文本块处理的挑战，揭示了块处理的核心，并显著提升了RAG系统的整体性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>优化大语言模型的构建：上下文长度与注意力头的影响</title>
<link>https://arxiv.org/abs/2503.09579</link>
<guid>https://arxiv.org/abs/2503.09579</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了上下文长度和注意力头配置对大语言模型性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文系统比较了不同参数大小、上下文长度和注意力头配置的大语言模型在模型性能、计算成本和内存成本上的表现。研究发现，在处理足够长序列时，使用较少注意力头的较大模型可以在降低计算和内存成本的同时减少损失。通过扩展现有基于参数大小和训练计算的缩放方法，本文为训练和推理阶段的成本优化提供了指导。这些发现为实际应用中开发大语言模型，特别是在长上下文处理场景中，提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09579" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:50:42 GMT</pubDate>
</item>
<item>
<title>通过可验证结果奖励强化学习提升视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2503.08525</link>
<guid>https://arxiv.org/abs/2503.08525</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，通过GTR框架提升视觉语言模型的推理效果和任务成功率。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了使用可验证结果奖励的强化学习（RLVR）在视觉语言模型（VLM）中训练目标导向推理的有效性。通过在复杂的卡片游戏和ALFWorld的具身任务上的实验，我们发现仅基于行动结果的奖励无法有效激励VLM的推理能力，反而导致一种被称为思维崩溃的现象：代理的思维多样性快速下降，产生与状态无关及不完整的推理，进而导致无效的行动和负面奖励。为了解决这一问题，我们提出了一种自动纠正器，确保在每一步中对代理推理进行评估和完善，以避免思维崩溃。我们的GTR（引导思维强化）框架能够同时训练推理与行动，而无需密集的人类标注。实验结果表明，GTR显著提升了LLaVA-7b模型在多种视觉环境中的表现和泛化能力，相比于现有最先进模型，其任务成功率提高了3-5倍，同时模型规模更小。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08525" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 11:17:02 GMT</pubDate>
</item>
<item>
<title>高效遥感图像的视觉语言理解方法</title>
<link>https://arxiv.org/abs/2503.07588</link>
<guid>https://arxiv.org/abs/2503.07588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种高效处理大规模遥感图像的视觉语言理解方法。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模遥感图像（RSIs）的高效视觉语言理解提出了一种新方法，旨在在保持图像细节的同时降低计算复杂度。我们引入了一种文本指导的令牌修剪方法，结合动态图像金字塔（DIP）。该方法包括一个区域聚焦模块（RFM），能够识别关键信息，同时采用基于DIP的粗细结合的图像块选择和令牌修剪策略，有效避免直接处理整个大幅图像。此外，针对现有领域评估指标的不足，我们构建了新的基准数据集LRS-VQA，包含7333对QA，支持高达27328像素的图像长度。实验结果表明，我们的方法在四个数据集上超越了现有高分辨率策略，并在高分辨率设置下显示出更好的效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:51:16 GMT</pubDate>
</item>
<item>
<title>优化LLM的量化技术以提高代码生成效率</title>
<link>https://arxiv.org/abs/2503.07103</link>
<guid>https://arxiv.org/abs/2503.07103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究更大规模LLM的量化技术，以减少内存占用并保持性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型语言模型(LLM)在代码生成中的应用，重点关注量化技术以减少内存占用。前期工作探讨了16B参数的LLM，量化精度从32位浮点降低至8位整数，而本研究则对新的、参数高达34B的LLM进行更深入的复制研究。我们采用最新的量化技术，将压缩推向2位量化，考察不同校准数据集的效果。实证分析显示，4位量化精度可实现70%的内存减少，同时性能未显著下降；在更极端的3位和2位量化下，使用代码特定的校准数据集有助于限制性能损失。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 05:26:08 GMT</pubDate>
</item>
<item>
<title>WildIFEval: 一个多约束用户指令评估数据集</title>
<link>https://arxiv.org/abs/2503.06573</link>
<guid>https://arxiv.org/abs/2503.06573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WildIFEval是一个包含12000个多约束用户指令的数据集，旨在提升LLM的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了WildIFEval，这是一个包含12000个真实用户指令的大规模数据集，涵盖了多样化的多约束条件。与以往的数据集不同，我们的收集涵盖了广泛的词汇和主题约束，反映了自然用户提示中的复杂性。我们将这些约束分为八个高层类别，以捕捉它们在现实场景中的分布和动态。通过使用WildIFEval，我们对多个领先的语言模型（LLMs）进行了广泛实验，以基准评估它们的指令遵循能力。结果表明，所有模型在约束数量增加时性能均有下降，显示了改进空间。此外，我们发现约束的具体类型对模型性能起着关键作用。我们发布这一数据集旨在促进关于在复杂、现实条件下的指令遵循的进一步研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:06:29 GMT</pubDate>
</item>
<item>
<title>文档数量对检索增强生成性能的影响研究</title>
<link>https://arxiv.org/abs/2503.04388</link>
<guid>https://arxiv.org/abs/2503.04388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了RAG设置中文档数量对LLM性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了检索增强生成（RAG）方法中，文档数量如何影响大型语言模型（LLM）的性能。以多跳问答任务为基础，我们在自定义数据集上进行评估，保持上下文长度和相关信息位置不变的情况下，调整文档数量。结果表明，增加文档数量在RAG设置中给LLM带来了显著挑战。此外，结果还指出，处理多个文档是一个独立于处理长上下文的挑战。我们将提供的数据集和代码可在GitHub上获取，进一步促进相关研究的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:38:17 GMT</pubDate>
</item>
<item>
<title>TPDiff: 高效视频扩散模型的多阶段训练框架</title>
<link>https://arxiv.org/abs/2503.09566</link>
<guid>https://arxiv.org/abs/2503.09566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TPDiff框架通过多阶段扩散提高视频模型的训练和推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出TPDiff框架，以解决视频扩散模型计算需求大的挑战。我们发现扩散的反向过程具有内在的降低熵的特性，利用视频模态中帧间的冗余性，可以在高熵阶段不必要地维持完整的帧率。TPDiff框架通过将扩散过程分为多个阶段，有效地渐进提升帧率，最终阶段才使用完整帧率，从而优化计算效率。为训练这一多阶段扩散模型，我们引入了分阶段扩散的专门训练框架，通过对齐的数据和噪声解决分区概率流常微分方程（ODE），使训练策略适用于多种扩散形式，并进一步增强训练效率。实验证明，该方法具有广泛适用性，能够减少50%的训练成本，以及提高1.5倍的推理效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:33:22 GMT</pubDate>
</item>
<item>
<title>增强一致性的潜在扩散模型（AF-LDM）</title>
<link>https://arxiv.org/abs/2503.09419</link>
<guid>https://arxiv.org/abs/2503.09419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文重构潜在扩散模型以提高生成一致性，提出了抗混叠的LDM。</p><br /><br /><p><strong>摘要：</strong> 潜在扩散模型（LDMs）在生成过程中面临不稳定性，小的输入噪声扰动会导致输出显著不同，限制了其在需要一致性结果的应用中的使用。为此，本文设计了一种转变为一致性的LDM，通过引入抗混叠操作来改善一致性，但由于LDMs面临的独特挑战，仍然存在显著的混叠及不一致问题。特别是，变分自编码器（VAE）训练和多个U-Net推理过程中的混叠放大，再加上自注意力模块固有地缺乏一致性。为解决这些问题，本文重构了注意力模块，使其具备一致性，并提出了一种有效抑制连续域中特征频带的均匀性损失。最终得到的不混叠LDM（AF-LDM）展现出强一致性和对不规则形变的鲁棒性。大量实验表明，AF-LDM在视频编辑和图像到图像转换等多种应用中显著优于传统的LDM。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:16:30 GMT</pubDate>
</item>
<item>
<title>VLog: 一种基于语言模型的视频理解框架</title>
<link>https://arxiv.org/abs/2503.09402</link>
<guid>https://arxiv.org/abs/2503.09402</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VLog框架通过新颖的事件词汇实现高效视频叙述生成。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的视频理解框架VLog，该框架将视频叙述定义为一种词汇，从而超越了现有生成性视频语言模型中的词汇方式。VLog基于轻量级语言模型GPT-2，具备三项关键创新：首先，提出了一种生成检索模型，将语言模型的复杂推理能力与对比检索的高效相似性搜索相结合；其次，利用大规模视频叙述构建的分层词汇，通过叙述对编码算法（narration pair encoding）实现对具体事件（如切西红柿）与更广泛场景（如厨房）的高效索引；最后，通过生成模型的词汇更新策略，扩展在推理过程中遇到的新事件的词汇。实验结果在EgoSchema、COIN和HiREST数据集上显示VLog在生成简洁、上下文准确且高效的叙述方面的有效性，提供了对视频理解的全新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09402" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 09:53:30 GMT</pubDate>
</item>
<item>
<title>Reangle-A-Video：新型同步多视角视频生成框架</title>
<link>https://arxiv.org/abs/2503.09151</link>
<guid>https://arxiv.org/abs/2503.09151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Reangle-A-Video框架，实现从单视频生成同步多视角视频。</p><br /><br /><p><strong>摘要：</strong> Reangle-A-Video是一种统一框架，用于从单一输入视频生成同步的多视角视频。与主流的在大规模4D数据集上训练多视角视频扩散模型的方法不同，我们的方法将多视角视频生成任务重新表述为视频到视频的转换，利用公开可用的图像和视频扩散先验。该方法分为两个阶段：首先是多视角运动学习，通过自我监督的方式同步微调图像到视频的扩散变换器，从一组变形视频中提取视角不变的运动；其次是通过DUSt3R进行多视角一致的图像到图像转换，在推断时通过交叉视图一致性指导，对输入视频的第一帧进行变形和修复，生成多视角一致的起始图像。大量实验表明，Reangle-A-Video在静态视角转化和动态相机控制任务中超过了现有方法，为多视角视频生成提供了新解决方案，并计划公开发布代码和数据。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 04:26:15 GMT</pubDate>
</item>
<item>
<title>Motion Anything：一种多模态运动生成框架</title>
<link>https://arxiv.org/abs/2503.06955</link>
<guid>https://arxiv.org/abs/2503.06955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Motion Anything框架，解决多模态运动生成中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> Conditional motion generation在计算机视觉领域得到广泛研究，但仍面临两大挑战：一是现有的遮罩自回归方法缺乏优先关注动态帧和身体部位的机制；二是不同条件模态的方法常常无法有效集成多个模态，限制了生成运动的控制性和一致性。为了解决这些问题，我们提出了Motion Anything，一个引入基于注意力的遮罩建模方法的多模态运动生成框架。该框架能够对关键帧和动作进行细粒度的空间和时间控制，并通过自适应编码多模态条件（包括文本和音乐）来改善可控性。此外，我们还发布了Text-Music-Dance (TMD)新数据集，包含2153对文本、音乐和舞蹈，是AIST++的两倍，填补了这一领域的关键空白。实验表明，Motion Anything在多个基准测试中超过了现有的先进方法，在HumanML3D上的FID指标提升了15%，在AIST++和TMD上也表现出了一致的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:04:31 GMT</pubDate>
</item>
<item>
<title>块扩散语言模型：突破生成限制的新方法</title>
<link>https://arxiv.org/abs/2503.09573</link>
<guid>https://arxiv.org/abs/2503.09573</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">块扩散语言模型通过灵活长度生成提升了性能并改善了推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的块扩散语言模型，该模型在离散去噪扩散和自回归模型之间插值，克服了两者的关键限制。块扩散模型支持灵活长度生成，并通过KV缓存和并行令牌采样提高推理效率。我们提出了构建有效块扩散模型的方案，其中包括高效的训练算法、梯度方差估计器和数据驱动的噪声调度，以最小化方差。块扩散在语言建模基准测试中设定了扩散模型的新最佳性能，并实现了任意长度序列的生成。相关代码及模型权重已在项目页面提供。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09573" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 13:43:40 GMT</pubDate>
</item>
<item>
<title>scMMGPT：结合细胞与文本建模的单细胞多模态生成预训练变换器</title>
<link>https://arxiv.org/abs/2503.09427</link>
<guid>https://arxiv.org/abs/2503.09427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">scMMGPT是一种新型的单细胞多模态预训练模型，提升细胞与文本的联合建模能力。</p><br /><br /><p><strong>摘要：</strong> 单细胞分析领域中的预训练语言模型（PLMs）应用受到限制，现有的文本PLMs无法处理单细胞RNA测序数据，而细胞PLMs又无法处理自由文本，导致在多模态任务中应用受限。为解决这一问题，本文提出了单细胞多模态生成预训练变换器（scMMGPT），它有效地整合了最先进的细胞与文本PLMs，促进了跨模态知识共享。scMMGPT通过专门的跨模态投影器来弥合文本和细胞之间的模态差距，并在2700万个细胞上进行了大规模的预训练，这是迄今为止最大的数据集，使其在细胞与文本的联合任务中表现卓越，生成细胞描述的文本差异相对改善84%，细胞类型注释准确度提升20.5%，文本条件下伪细胞生成的k-NN准确度提升4%，均超越了基准模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.09427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Mar 2025 10:26:16 GMT</pubDate>
</item>
<item>
<title>PlainQAFact框架在医疗领域中的事实性评估</title>
<link>https://arxiv.org/abs/2503.08890</link>
<guid>https://arxiv.org/abs/2503.08890</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PlainQAFact框架提高了医疗领域中的平易语言摘要的事实性评估。 </p><br /><br /><p><strong>摘要：</strong> 本文介绍了PlainQAFact框架，旨在解决语言模型在医疗领域产生的幻觉输出对普通观众的风险，特别是在产生平易语言摘要（PLS）时。现有的事实性评估方法，如基于前提关系和问答的评估，无法有效处理PLS生成中的详细解释现象。这种现象会引入源文档中不存在的外部内容（例如定义、背景和示例）以增强理解。PlainQAFact框架基于一个细致的人类标注数据集PlainFact进行训练，通过首先分类事实性类型，然后使用检索增强问答的方法评估事实性。我们的研究表明，PlainQAFact在处理PLS中的事实性评估时，优于现有的事实性指标，在多个外部知识源和文档粒度水平下均取得了卓越的效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08890" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 16:59:53 GMT</pubDate>
</item>
<item>
<title>引导矩匹配模型：快速稳定的生成模型</title>
<link>https://arxiv.org/abs/2503.07565</link>
<guid>https://arxiv.org/abs/2503.07565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出引导矩匹配(IMM)模型，实现快速稳定的生成样本。</p><br /><br /><p><strong>摘要：</strong> 引导矩匹配(Inductive Moment Matching, IMM)是一种新型生成模型，旨在解决扩散模型和流匹配模型在推理速度与样本质量之间的权衡。与传统的模型蒸馏不同，IMM无需对两个网络进行预训练和优化，且能够在单阶段训练过程中进行一步或少步采样，确保在各类超参数和标准模型架构下的稳定性。与扩散模型相比，IMM在ImageNet-256x256数据集上取得1.99的FID，仅需8次推理步数；在CIFAR-10上，IMM训练出的模型实现了1.98的最先进的2步FID，展示了其卓越的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:37:39 GMT</pubDate>
</item>
<item>
<title>多模态智能的推理优先视角及生成预训练算法的创新</title>
<link>https://arxiv.org/abs/2503.07154</link>
<guid>https://arxiv.org/abs/2503.07154</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨推理优先视角如何推动生成预训练算法创新。</p><br /><br /><p><strong>摘要：</strong> 近年来，基础模型在生成预训练方面取得了显著进展，但在算法创新上主要局限于自回归模型和扩散模型，导致结合多模态数据的潜力未能充分释放，从而限制了多模态智能的发展。本文提出一种推理优先的视角，通过在推理阶段优先考虑规模效率，能够激发新型生成预训练算法的灵感。以归纳动量匹配（IMM）为实例，探索如何通过针对性修改扩散模型的推理过程，获得稳定的单阶段算法，从而在样本质量上显著提升，并实现超过一个数量级的推理效率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07154" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 06:27:30 GMT</pubDate>
</item>
<item>
<title>通过容量感知推理优化混合专家模型的效率</title>
<link>https://arxiv.org/abs/2503.05066</link>
<guid>https://arxiv.org/abs/2503.05066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出容量感知推理技术，提升混合专家模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 混合专家（MoE）架构通过稀疏专家激活在扩展大型语言模型上具有显著成效，但在专家并行情况下，由于令牌到专家的分配不平衡，导致一些专家过载而其他专家未被充分利用，进而影响推理效率。针对这一问题，提出了容量感知推理的方法，包括两个关键技术：容量感知令牌丢弃和容量感知令牌重新分配。这些技术旨在优化高负载和低负载专家的利用，提升整体推理管道的效率。实验结果显示，这些方法在推理效率上取得了显著改进，例如在Mixtral-8times7B-Instruct模型上实现了0.2%的平均性能提升和1.94倍的推理速度提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 20:11:39 GMT</pubDate>
</item>
<item>
<title>深度检索模型中的偏见与鲁棒性研究</title>
<link>https://arxiv.org/abs/2503.05037</link>
<guid>https://arxiv.org/abs/2503.05037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了深度检索模型中的偏见对信息检索性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文研究了深度检索模型在信息检索应用中的鲁棒性，特别是在检索增强生成（RAG）场景下的表现。通过重新利用关系提取数据集（例如Re-DocRED），设计了控制实验，量化了启发式偏见（如偏爱短文档）对检索器（如Dragon+和Contriever）的影响。研究发现，检索器往往依赖于表面模式，比如过度优先考虑文档开头、短文档、重复实体和字面匹配等，同时忽视文档是否包含查询的答案，缺乏深入的语义理解。尤其是当多种偏见组合时，模型表现显著下降，答案文档被选中的概率不足3%。此外，这些偏见对下游应用（如RAG）有直接影响，偏好检索的文档可能误导大型语言模型（LLMs），造成34%的性能下降，甚至比不提供文档的情况还糟糕。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 18:23:13 GMT</pubDate>
</item>
<item>
<title>OTTER：一种新的视觉-语言-行动模型实现有效的机器人操作</title>
<link>https://arxiv.org/abs/2503.03734</link>
<guid>https://arxiv.org/abs/2503.03734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OTTER是一个新型模型，通过提取语义对齐的视觉特征增强机器人操作能力。</p><br /><br /><p><strong>摘要：</strong> 视觉-语言-行动（VLA）模型旨在根据视觉观察和语言指令预测机器人动作。传统方法通过微调预训练的视觉语言模型进行操作，但由于独立输入视觉和语言特征，导致预训练语义对齐性能下降。为了解决这一问题，我们提出了OTTER，一个新的VLA架构，该架构通过明确的文本感知视觉特征提取，利用现有的预训练语义对齐。OTTER不再处理所有视觉特征，而是选择性提取与任务相关的、与语言指令语义对齐的视觉特征传递给政策变换器，从而保持预训练视觉-语言编码器的固定。这种方法保护并利用了大规模预训练所获取的丰富语义理解，具备强大的零样本泛化能力。通过模拟和真实世界的实验，OTTER显著超越现有的VLA模型，展示了对新物体和环境的强大零样本泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 13:44:48 GMT</pubDate>
</item>
<item>
<title>指令跟随检索器的安全风险研究</title>
<link>https://arxiv.org/abs/2503.08644</link>
<guid>https://arxiv.org/abs/2503.08644</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究指令跟随检索器处理恶意查询的安全风险。</p><br /><br /><p><strong>摘要：</strong> 本文研究了指令跟随检索器在处理恶意查询时的安全风险。我们对包括NV-Embed和LLM2Vec在内的六种领先检索器进行了实证分析，发现大多数检索器在面对恶意请求时能够选择相关的有害内容，成功率超过50%。例如，LLM2Vec对61.35%的恶意查询选择了相关的有害段落。此外，我们揭示了一个新兴风险，即通过利用这些检索器的指令跟随能力，可以显著展示与恶意内容相关的高度相关信息。最后，即使是安全对齐的LLM，例如Llama3，当在上下文中提供有害检索段落时，也会满足恶意请求。这些发现突显了检索器能力增强所带来的恶意误用风险。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08644" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:36:53 GMT</pubDate>
</item>
<item>
<title>ObjectMover：应对复杂场景的物体移动生成模型</title>
<link>https://arxiv.org/abs/2503.08037</link>
<guid>https://arxiv.org/abs/2503.08037</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ObjectMover是一种用于复杂场景中物体移动的生成模型。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ObjectMover的生成模型，用于在高度复杂的场景中实现物体移动。我们将这一任务建模为序列到序列问题，并微调视频生成模型，利用其对视频帧中一致物体生成的知识。由于缺少用于物体移动的大规模数据，我们构建了一条数据生成管道，使用现代游戏引擎合成高质量的数据对。同时，我们提出了多任务学习策略，使模型能够在真实世界视频数据上进行训练，从而提升泛化能力。通过大量实验，我们证明了ObjectMover在实际场景中表现出色，能够有效处理极端的光照协调和物体效果运动。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08037" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 00:42:59 GMT</pubDate>
</item>
<item>
<title>多模态基础模型在自驾车中人类与机器驾驶反应的比较研究</title>
<link>https://arxiv.org/abs/2503.07587</link>
<guid>https://arxiv.org/abs/2503.07587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了自驾车中多模态模型与人类驾驶者的反应差异。</p><br /><br /><p><strong>摘要：</strong> 随着多模态基础模型在自驾车中的实验应用，本文研究了这些系统在特定驾驶情境中的反应与人类的相似程度。通过构建Robusto-1数据集，利用秘鲁的行车记录视频数据，该地区以其复杂的交通情况和异常街道物体著称。我们使用多模态视觉问答（VQA）的方法，对人类与基础视觉语言模型（VLMs）进行比较，采用系统神经科学中的表征相似性分析（RSA）方法。研究表明，不同问题类型对人机表现的影响显著，揭示了二者在认知对齐上的差距，具体分析了它们在回答不同问题时的趋同与分歧。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:50:04 GMT</pubDate>
</item>
<item>
<title>CineBrain：首个动态视听刺激下的EEG与fMRI同步记录数据集</title>
<link>https://arxiv.org/abs/2503.06940</link>
<guid>https://arxiv.org/abs/2503.06940</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineBrain数据集结合EEG和fMRI，推动视听刺激重建进展。</p><br /><br /><p><strong>摘要：</strong> 本研究介绍了CineBrain，这是首个大规模数据集，记录了在动态视听刺激下的EEG与fMRI同步信号。该数据集包含来自热门剧集《生活大爆炸》的六小时叙事内容，涵盖六名参与者，充分发挥了EEG的高时间分辨率与fMRI的深脑空间覆盖的互补优势。基于CineBrain，我们提出了CineSync，这是一种创新的多模态解码框架，结合了多模态融合编码器与基于扩散的神经潜在解码器，有效融合EEG和fMRI信号，显著提高复杂视听刺激的重建质量。为了进行严格评估，我们引入了Cine-Benchmark，一个全面的评估协议，用于评估语义和感知维度的重建效果。实验结果表明，CineSync在视频重建性能方面达到了领先水平，并展示了我们在结合fMRI和EEG重建视频及音频刺激方面的初步成功。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06940" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 01:39:43 GMT</pubDate>
</item>
<item>
<title>源偏差与PLM基础检索模型的因果分析</title>
<link>https://arxiv.org/abs/2503.08684</link>
<guid>https://arxiv.org/abs/2503.08684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨PLM基础检索模型中源偏差产生的原因及解决方案。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于预训练语言模型（PLM）的检索模型在处理信息时产生的源偏差现象，分析其根源及解决方法。研究表明，PLM基础检索者通过学习困惑度特征进行相关性估计，导致低困惑度文档被赋予更高的相关性评分。通过理论分析发现，这种现象与语言建模任务和检索任务的损失函数梯度之间的正相关性有关。为此，提出了一种名为因果诊断与修正（CDC）的去偏差方法，首先诊断困惑度的偏差影响，然后将其从整体估计的相关性评分中分离出去。实验结果表明，CDC在三个领域的去偏效果优于其他方法，验证了本文提出的解释框架的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08684" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:00 GMT</pubDate>
</item>
<item>
<title>AnyMoLe：一种无数据集依赖的角色运动插值方法</title>
<link>https://arxiv.org/abs/2503.08417</link>
<guid>https://arxiv.org/abs/2503.08417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AnyMoLe利用视频扩散模型实现无数据集依赖的角色运动插值。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了AnyMoLe，一种创新的方法，旨在解决基于学习的运动插值中的数据集特定要求。AnyMoLe利用视频扩散模型，为任意角色生成插值帧，省去外部数据的需求。该方法采用两阶段的帧生成过程，提升了上下文理解能力。此外，研究引入了ICAdapt，这是一种针对视频扩散模型的微调技术，用于缩小真实世界与渲染角色动画之间的域差距。同时，我们提出了一种“运动视频模仿”优化技术，使得对于具有任意关节结构的角色能够顺畅生成运动，结合了2D和3D特征。AnyMoLe显著降低了对数据的依赖，为各种运动插值任务提供了平滑且逼真的过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 09:28:59 GMT</pubDate>
</item>
<item>
<title>提升计算机视觉中的个体识别能力：RexSeek模型与HumanRef数据集</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出RexSeek模型，旨在提升个体识别的准确性并应对现实应用中的挑战。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于计算机视觉中个体识别的重要性，定义了一个新任务，即基于自然语言描述识别个体。尽管现有模型在一些基准测试上表现良好，但在实际应用中却缺乏有效性。为了应对这一挑战，本文从任务定义、数据集设计和模型架构三个方面进行研究，提出HumanRef数据集以更好地反映现实应用场景。我们构建了集成多模态大语言模型与物体检测框架的RexSeek模型，实验结果表明，尽管一些先进模型在常用基准如RefCOCO上表现出色，但在HumanRef数据集上却遇到困难，主要因为它们无法有效检测多个个体。而RexSeek在个体识别上表现优异，并能够有效推广至常见物体的识别任务，显示出广泛的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:57:14 GMT</pubDate>
</item>
<item>
<title>一种无训练的人脸匿名化方法</title>
<link>https://arxiv.org/abs/2503.08478</link>
<guid>https://arxiv.org/abs/2503.08478</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的人脸匿名化方法，有效保留非身份属性。</p><br /><br /><p><strong>摘要：</strong> 针对隐私担忧不断增加的背景下，本文提出了一种训练-free的人脸匿名化方法，旨在保留非身份相关的关键属性。该方法利用预训练的文本到图像扩散模型，能够在不需要优化或训练的情况下进行操作。具体而言，方法通过反转输入图像来恢复初始噪声，然后通过身份条件的扩散过程去噪，以确保匿名后的面孔与原身份明显不同。此外，系统支持局部匿名化，用户可以控制待匿名化或保留的面部区域。经过与最先进的方法的全面评估，该方法在匿名化、属性保留和图像质量方面表现优异，展示出良好的灵活性、鲁棒性和实用性，适合于实际应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08478" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 10:29:37 GMT</pubDate>
</item>
<item>
<title>一种新型Transformer架构用于音视频生成的研究</title>
<link>https://arxiv.org/abs/2503.08307</link>
<guid>https://arxiv.org/abs/2503.08307</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种新型Transformer架构，解决音视频生成中的关键挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新型的Transformer架构，用于解决生成AI中音视频（AV）生成面临的三大挑战：生成样本的质量、无缝的多模态同步与时间一致性，以及无限视频时长。我们探讨了三种不同的跨模态交互模块，其中轻量级时间融合模块被证明是对齐音频和视觉模态的高效有效方法。实验结果显示，所提出的方法在多模态音视频生成任务中超越了现有的最先进模型，为音视频生成领域提供了新的视角和技术。代码和模型检查点已在GitHub上公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08307" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 07:18:47 GMT</pubDate>
</item>
<item>
<title>智能记忆管理系统SECOND ME的创新应用</title>
<link>https://arxiv.org/abs/2503.08102</link>
<guid>https://arxiv.org/abs/2503.08102</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SECOND ME重塑用户交互中记忆管理的方式，大幅提升信息处理效率。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了SECOND ME，一个利用大型语言模型(LLMs)的智能记忆管理系统，其目标是提升用户与外界的互动效率。人们在与其他个体、网站及未来的AI代理交互时，常需重复提供相同信息，SECOND ME通过作为用户交互的中介，有效减少了这一冗余。该系统不仅能够自动生成环境感知的响应，自动填充所需信息，还能与外部系统无缝沟通，从而降低用户的认知负担。与传统的记忆存储解决方案相比，SECOND ME通过LLM驱动的记忆参数化，提供了结构化的组织、上下文推理和自适应知识检索。这一创新代表了朝向更智能化和系统化的记忆管理的重要进步，为AI驱动的个人代理在数字生态系统中的角色奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08102" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:05:52 GMT</pubDate>
</item>
<item>
<title>结合大语言模型与神经机器翻译的高效模型</title>
<link>https://arxiv.org/abs/2503.06594</link>
<guid>https://arxiv.org/abs/2503.06594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大语言模型在神经机器翻译中的应用与优化。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何将大语言模型（LLMs）有效地应用于神经机器翻译（NMT），旨在设计一种通用、高效且易于优化的翻译模型。我们保留了传统NMT模型中的解码器，仅在编码阶段引入LLMs，并发展了一些方法来进一步改进其与解码器的适配性。此外，构建了一个新的多任务数据集，以评估机器翻译系统在不同任务中的泛化能力。经过在WMT及其他数据集上的评估，结果表明我们的方法在翻译质量上与多种基线相匹配或更优，同时实现了2.4到6.5倍的推理速度提升，并将KV缓存的内存占用降低了75%。这表明所提方法在各种翻译相关任务中表现出强大的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:54:05 GMT</pubDate>
</item>
<item>
<title>VisualSimpleQA：一项针对大规模视觉语言模型的多模态基准评测</title>
<link>https://arxiv.org/abs/2503.06492</link>
<guid>https://arxiv.org/abs/2503.06492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisualSimpleQA为大规模视觉语言模型提供了一种新的多模态评测基准。</p><br /><br /><p><strong>摘要：</strong> 大规模视觉语言模型（LVLMs）在生成非事实回应方面仍面临挑战。当前的多模态事实寻求基准主要关注模型输出与真实答案的比较，但对模态特定模块的表现洞察有限。为了解决这一问题，我们推出了VisualSimpleQA，一个多模态事实寻求基准，具备两个关键特点：首先，它支持对LVLMs在视觉和语言模态中的脱钩评估；其次，它采用明确的难度标准来指导人类标注，并提取出更具挑战性的子集VisualSimpleQA-hard。对15个LVLM的实验表明，即使是最先进的模型如GPT-4o，在VisualSimpleQA上的正确率仅为60%以上，而在VisualSimpleQA-hard上的正确率也仅为30%以上。这种解耦评估揭示了在视觉和语言模块中都有显著的改进空间。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 03:25:32 GMT</pubDate>
</item>
<item>
<title>MagicInfinite：高保真多角色肖像动画的新方法</title>
<link>https://arxiv.org/abs/2503.05978</link>
<guid>https://arxiv.org/abs/2503.05978</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicInfinite通过创新技术实现高保真的肖像动画。</p><br /><br /><p><strong>摘要：</strong> MagicInfinite是一种新型扩散Transformer框架，克服了传统肖像动画的局限，能够在现实人类、全身角色和风格化动漫角色等多种角色类型中实现高保真动画效果。该框架支持多种面部姿势，并通过输入掩码精确指定多角色场景中的说话者。其创新之处在于：1）采用滑动窗口去噪策略的3D全注意力机制，实现在多种角色风格下的无限视频生成；2）两阶段学习方案结合音频、文本和参考图像，实现灵活的多模态控制；3）使用区域特定掩码和自适应损失函数平衡全局文本控制和局部音频指导。得益于统一步骤和cfg蒸馏技术，效率显著提升，实现20倍的推理速度提升。评估结果显示，MagicInfinite在音频口型同步和运动自然性方面表现优越。该项目已公开发布，感兴趣的用户可访问官方网站获取更多信息。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05978" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 18:21:11 GMT</pubDate>
</item>
<item>
<title>AI4SE基准的评估与优化：BenchScout和BenchFrame的应用</title>
<link>https://arxiv.org/abs/2503.05860</link>
<guid>https://arxiv.org/abs/2503.05860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了AI4SE基准的现状，并提出了BenchScout和BenchFrame以优化基准质量。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能在软件工程领域的应用日益广泛，基准测试成为评估和可重复性的关键。然而，现有的基准面临着碎片化知识、选择困难、缺乏统一标准及局限性等挑战。本文回顾了173项研究，识别出204个AI4SE基准，并分析了这些基准的局限性及实践中的空白。为此，研究团队开发了BenchScout，一款支持语义搜索的工具，旨在帮助用户快速找到相关基准。此外，提出了BenchFrame，以统一方法提升基准质量，并以HumanEval基准为案例，解决其主要局限性，从而生成HumanEvalNext，改进了语言转换、测试覆盖和难度。评估表明，HumanEvalNext的通过率明显低于以往基准，提示了后续研究的改进方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:44:32 GMT</pubDate>
</item>
<item>
<title>QuoTA：基于查询重要性评估的视频标记分配模型</title>
<link>https://arxiv.org/abs/2503.08689</link>
<guid>https://arxiv.org/abs/2503.08689</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QuoTA通过查询导向评估优化视频标记分配，提高长视频理解效果。</p><br /><br /><p><strong>摘要：</strong> 本文提出了QuoTA，一种训练无关的模块，旨在基于查询导向的帧级重要性评估优化视觉标记的分配。当前技术侧重于解码器层的低响应标记剪枝，而忽视了视觉标记与指令之间的输入层次语义相关性。QuoTA通过查询相关性战略性地分配帧级重要性分数，使视觉标记分配在跨模态交互之前进行，从而提高了标记预算的利用率并保留了语义相关的内容。此外，QuoTA通过链式思维推理拆分查询，以精确评分，且为现有的大型视频语言模型提供了即插即用的功能。实验结果表明，QuoTA在LLaVA-Video-7B上实施后，在六个基准测试（包括Video-MME和MLVU）中平均提升了3.2%的表现，同时在与基线相同的视觉标记预算内运行。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08689" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>OmniMamba：首个线性架构的多模态生成模型</title>
<link>https://arxiv.org/abs/2503.08686</link>
<guid>https://arxiv.org/abs/2503.08686</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniMamba是一个高效的多模态生成模型，显著减少计算复杂度和数据需求。</p><br /><br /><p><strong>摘要：</strong> OmniMamba是一种创新的线性架构多模态生成模型，采用统一的下一个token预测范式，能够同时生成文本和图像，克服了传统模型的二次计算复杂度和对大规模训练数据的依赖。该模型在Mamba-2的基础上提升了计算和内存效率，并通过引入解耦词汇以指导特定模态生成及用于参数高效自适应的任务特定LoRA，解决了现有统一模型的数据低效问题。OmniMamba还采用了分阶段的训练策略，以降低两个任务间的数据不平衡。尽管仅使用了200万对图像-文本数据，OmniMamba在多个基准测试中达到了与JanusFlow相媲美的性能，并超越了Show-o，表现出卓越的推理效率，相较于基于Transformer的模型，长序列生成的速度提升高达119.2倍，GPU内存减少63%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08686" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>YuE：创新的长篇音乐生成模型</title>
<link>https://arxiv.org/abs/2503.08638</link>
<guid>https://arxiv.org/abs/2503.08638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YuE是一种新型模型，可将歌词生成长达五分钟的音乐。</p><br /><br /><p><strong>摘要：</strong> YuE模型专注于长篇音乐生成，尤其是歌词到歌曲的转换，基于LLaMA2架构，能够处理数万亿个令牌，生成最多五分钟的音乐，同时保持歌词对齐、连贯的音乐结构和动听的旋律。其主要技术创新包括：追踪解耦的下一个token预测、结构性渐进条件提供长文本上下文对齐，以及多任务、多阶段的预训练方案。YuE还重塑了音乐生成的上下文学习技术，支持多样化风格转换和双向生成。通过大量评估，YuE在音乐性和声乐灵活性上与一些专有系统相当或更优。此外，微调YuE可增强控制能力，支持小众语言，且在音乐理解任务中表现出色，超过了MARBLE基准上的现有最先进技术。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:26:50 GMT</pubDate>
</item>
<item>
<title>新的人类类掩膜标注任务：提升多模态大语言模型的像素理解能力</title>
<link>https://arxiv.org/abs/2503.08625</link>
<guid>https://arxiv.org/abs/2503.08625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HLMAT任务提升MLLM的像素级理解与标注能力。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLMs）在图像理解方面表现出色，但它们在像素级理解上仍面临挑战。现有的评估任务如视觉问答（VQA）和视觉定位过于粗糙，无法准确评估细粒度的像素理解。为了解决这些问题，本文引入了人类类掩膜标注任务（HLMAT），一个新兴的范式，使MLLMs能够模拟人类标注者，通过交互式分割工具进行标注。HLMAT将分割建模为多步骤的马尔科夫决策过程，使MLLMs能够迭代生成基于文本的点击点，从而高质量地生成掩膜，而无需改变架构或生成隐式标记。同时，我们开发的SegAgent模型经过人类类注释路径的微调，其表现与最先进的方法相当，支持掩膜细化和注释过滤等额外任务。此外，HLMAT还提供了一种评估MLLMs细粒度像素理解的协议，为未来在细粒度视觉感知和多步骤决策方面的进展奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:08:54 GMT</pubDate>
</item>
<item>
<title>LightGen：一种高效的文本到图像生成方法</title>
<link>https://arxiv.org/abs/2503.08619</link>
<guid>https://arxiv.org/abs/2503.08619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了一种名为LightGen的高效图像生成训练方法。</p><br /><br /><p><strong>摘要：</strong> 最近的文本到图像生成进展依赖于庞大的数据集和复杂的架构，限制了资源不足的研究者的可及性。本文提出了LightGen，一种结合了知识蒸馏（KD）和直接偏好优化（DPO）的高效训练范式。该方法从最先进的文本到图像模型中提取知识，构建了仅有0.7B参数的紧凑型自回归架构。通过使用仅2百万张高质量合成图像的小型合成数据集，展示了数据多样性对模型性能的影响超过数据量。LightGen显著降低了计算需求，将预训练时间从数千GPU天缩短至88GPU天。此外，为了解决合成数据的缺陷，特别是高频细节和空间准确性的问题，本文集成了DPO技术，提升了图像的真实度和位置准确性。实验结果表明，LightGen在生成图像质量上可与最先进模型相媲美，同时大幅减少了计算资源的需求，增强了对资源有限环境的可访问性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:58:02 GMT</pubDate>
</item>
<item>
<title>BiasEdit：一种去除语言模型刻板偏见的高效编辑方法</title>
<link>https://arxiv.org/abs/2503.08588</link>
<guid>https://arxiv.org/abs/2503.08588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出BiasEdit模型，通过局部参数编辑去除语言模型中的偏见。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为BiasEdit的高效模型编辑方法，以消除语言模型中的刻板偏见。通过使用轻量级网络作为编辑器，BiasEdit生成参数更新，采用去偏见损失指导编辑网络对语言模型部分参数进行局部编辑，同时通过保留损失保持语言模型的建模能力。实验结果表明，BiasEdit在StereoSet和Crows-Pairs数据集上表现出色，相较于传统的去偏见方法，BiasEdit在消除偏见方面展现了更高的效率和鲁棒性，同时对语言模型的整体能力影响较小。此外，本文还进行了偏见追踪，探讨不同模块中的偏见并研究去偏见编辑对语言模型不同组件的影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:25:36 GMT</pubDate>
</item>
<item>
<title>Gemini Embedding：多语言嵌入模型的突破</title>
<link>https://arxiv.org/abs/2503.07891</link>
<guid>https://arxiv.org/abs/2503.07891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini Embedding 通过多语言能力在多项任务中表现优异。</p><br /><br /><p><strong>摘要：</strong> 本报告介绍了Gemini Embedding，这是一种基于谷歌最强大的大型语言模型Gemini的先进嵌入模型。Gemini Embedding利用Gemini的多语种和编码理解能力，为多种语言和文本形式生成高度可泛化的嵌入表示。这些嵌入可以预先计算，并应用于分类、相似性、聚类、排序和检索等多种下游任务。在大量多语种文本嵌入基准测试（MMTEB）上进行评估时，Gemini Embedding远超之前的最先进模型，在嵌入质量上表现出显著提高。在MMTEB的多语言、英语和代码基准上，Gemini Embedding均取得了最先进的性能，展现出其在广泛任务中的强大能力，超过了专门的领域特定模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 18:16:45 GMT</pubDate>
</item>
<item>
<title>RayFlow: 一种提升扩散模型生成效率的新框架</title>
<link>https://arxiv.org/abs/2503.07699</link>
<guid>https://arxiv.org/abs/2503.07699</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RayFlow通过引导样本路径，提升了扩散模型的生成速度与质量。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多个领域取得了显著成功，但其生成速度缓慢仍然是一个关键挑战。现有的加速方法虽然旨在减少采样步骤，但往往会降低样本质量、可控性或增加训练复杂性。为了解决这些问题，我们提出了RayFlow，这是一种新颖的扩散框架，能够沿着独特路径引导每个样本朝向特定的目标分布，从而在减少采样步骤的同时保持生成的多样性和稳定性。同时，我们引入了Time Sampler，一种重要性采样技术，以通过关注关键时间步提高训练效率。大量实验表明，与现有的加速技术相比，RayFlow在生成高质量图像时显示出更高的速度、控制能力和训练效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07699" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:20:52 GMT</pubDate>
</item>
<item>
<title>生存游戏：评估人工智能自主水平的框架</title>
<link>https://arxiv.org/abs/2502.18858</link>
<guid>https://arxiv.org/abs/2502.18858</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生存游戏旨在通过试错次数评估人工智能的自主智能水平。</p><br /><br /><p><strong>摘要：</strong> 本研究提出生存游戏作为一种框架，以试错过程中的失败次数评估智能水平。我们定义的自主智能水平意味着在面对新挑战时，失败次数的期望和方差均有限。通过应用生存游戏，我们全面评估现有人工智能系统的表现，发现尽管它们在简单任务中达到了自主水平，但在视觉、搜索、推荐和语言等复杂任务中仍有较大差距。进一步分析显示，要实现一般任务的自主水平需要约10^{26}个参数，所需的计算资源成本极高，预计需要70年通过摩尔定律支持这样的规模。这一发现突显了人类任务的复杂性，并揭示了当前AI技术的不足。生存游戏不仅能够指导AI的发展，还可以深入理解人类智能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.18858" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 26 Feb 2025 00:59:45 GMT</pubDate>
</item>
<item>
<title>新型视觉标记化框架的结构化实现</title>
<link>https://arxiv.org/abs/2503.08685</link>
<guid>https://arxiv.org/abs/2503.08685</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍一种新型视觉标记化框架，强调结构化的特征提取。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的视觉标记化框架，将可证明的类似主成分分析（PCA）结构嵌入潜在标记空间。与现有的视觉标记化器主要优化重建保真度的做法不同，我们的方法关注潜在空间的结构特性，这对可解释性和后续任务至关重要。该方法为图像生成一维因果标记序列，每个连续的标记以数学上保证的递减解释方差贡献不重叠的信息，确保最显著的视觉特征首先被提取。进一步，我们通过利用扩散解码器识别并解决了高层语义内容与低层光谱细节在标记中的不当纠缠效应。实验表明，我们的方法在重建性能上达到领先水平，并增强了解释性，更好地与人类视觉系统对齐。此外，基于我们标记序列训练的自回归模型在性能上与当前的最先进方法相当，但所需的标记数量较少，训练和推理更加高效。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08685" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>SynCoS：一种同步耦合采样框架用于长视频生成</title>
<link>https://arxiv.org/abs/2503.08605</link>
<guid>https://arxiv.org/abs/2503.08605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的同步耦合采样方法，改善长视频生成中的一致性与流畅性。</p><br /><br /><p><strong>摘要：</strong> 随着文本到视频扩散模型的进步，短视频生成已实现高质量，但长视频生成仍面临数据有限和计算成本高的问题。为了解决这一挑战，研究者提出了调整无关的方法，如使用多个提示来实现内容的动态和可控变化。尽管这些方法在平滑帧间过渡方面有所成效，但通常会导致内容漂移和语义一致性的逐渐丧失。因此，我们提出了Synchronized Coupled Sampling（SynCoS），这一全新推理框架能够同步整个视频的去噪路径，确保邻近和远程帧之间的一致性。SynCoS结合了逆向和基于优化的采样策略，从而确保局部过渡的无缝性与全局一致性。实验表明，SynCoS显著提升了多事件长视频生成的表现，获得了更光滑的过渡和更佳的长程一致性，超越了之前的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 12:43:45 GMT</pubDate>
</item>
<item>
<title>UniF^2ace：用于细粒度面部理解与生成的统一多模态模型</title>
<link>https://arxiv.org/abs/2503.08120</link>
<guid>https://arxiv.org/abs/2503.08120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniF^2ace是一种新型模型，专注于细粒度面部属性的理解与生成。</p><br /><br /><p><strong>摘要：</strong> UniF^2ace是首款专门为细粒度面部理解与生成而设计的统一多模态模型，旨在克服当前面部领域对粗糙属性理解的限制。该模型在自构建的UniF^2ace-130K数据集上训练，包含130K图像-文本对和一百万个问答对，涵盖多种面部属性。通过引入离散扩散评分匹配与遮蔽生成模型之间的理论联系，UniF^2ace优化了证据下界，提高了合成面部细节的能力。同时，模型采用了基于标记和序列的专家混合架构，有效实现了对细粒度表示的学习。大量实验证明，UniF^2ace在理解和生成任务上均超越了现有的统一多模态模型和生成模型，展示了其卓越的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.08120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Mar 2025 03:34:59 GMT</pubDate>
</item>
<item>
<title>促进东南亚文化多样性：SEA-VL开放源代码计划</title>
<link>https://arxiv.org/abs/2503.07920</link>
<guid>https://arxiv.org/abs/2503.07920</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SEA-VL项目致力于提升东南亚语言在视觉-语言研究中的文化相关性。</p><br /><br /><p><strong>摘要：</strong> 东南亚在语言和文化上具有卓越的多样性，但在视觉-语言研究中却明显被低估，导致人工智能模型无法捕捉到该地区的文化细微差别。为填补这一空白，本项目提出了SEA-VL，一个开放源代码的倡议，旨在开发高质量、具有文化相关性的数据，确保东南亚语言的更多包容性。通过与东南亚国家的贡献者合作，项目不仅进行众包，还探索通过图像抓取和生成自动收集文化相关图像的方式。研究发现，图像抓取实现了约85%的文化相关性，且比众包更经济、高效；然而，尽管生成模型取得了显著进展，合成图像在准确反映东南亚文化方面仍然不够可靠。最终，我们收集了128万张文化相关图像，远超现有数据集的规模，通过SEA-VL，旨在弥补东南亚在视觉-语言研究中的代表性差距，促进更具包容性的人工智能系统的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07920" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 19:54:52 GMT</pubDate>
</item>
<item>
<title>VidDiff：识别视频中细微动作差异的新任务与基准</title>
<link>https://arxiv.org/abs/2503.07860</link>
<guid>https://arxiv.org/abs/2503.07860</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidDiff允许识别同一动作的细微视频差异，推动技能学习与训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Video Action Differencing（VidDiff），这是一项识别同一动作视频细微差异的新任务，具有教练和技能学习等多项应用。为了支持这一任务的发展，我们创建了VidDiffBench，一个包含549对视频的基准数据集，标注了4469个具体动作差异和2075个差异发生的时间戳。实验结果显示，VidDiffBench对现有大型多模态模型（如GPT-4o和Qwen2-VL）构成了显著挑战。通过分析这些模型在VidDiffBench上的失败案例，我们指出了两个主要挑战：在两个视频中定位相关的子动作，以及进行细粒度的帧比较。为了解决这些问题，我们提出了VidDiff方法，一种将任务分为三个阶段的代理工作流程：动作差异提议、关键帧定位和帧差异比对，每个阶段都使用专门的基础模型。为了促进未来在这项新任务上的研究，我们将基准数据集以及代码放在了相关链接中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07860" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 17:18:32 GMT</pubDate>
</item>
<item>
<title>Seedream 2.0：双语图像生成模型的进步</title>
<link>https://arxiv.org/abs/2503.07703</link>
<guid>https://arxiv.org/abs/2503.07703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seedream 2.0 是一款优化的双语图像生成模型，解决了现有模型的局限性。</p><br /><br /><p><strong>摘要：</strong> Seedream 2.0 是一种新兴的中文-英文双语图像生成基础模型，旨在克服现有模型的偏见、文本渲染能力不足和对中国文化细微差别理解不充分的问题。该模型支持中英文双语图像生成，凭借强大的数据系统和准确丰富的图像描述能力，能够有效处理双语文本。Seedream 2.0 结合自研的双语大语言模型作为文本编码器，能够从海量数据中直接学习本土知识，生成高保真图像，准确反映所描述的文化细微差别和美学表达。此外，通过灵活的字形对齐ByT5和多阶段后训练优化（如SFT和RLHF迭代），该模型在多个方面展现出一流的性能，包括响应提示、审美、文本渲染和结构准确性。Seedream 2.0已经对人类偏好进行了优化，以实现最佳的输出对人类期望的对齐，并具备作为指令基础图像编辑模型的能力，具有强大的编辑能力与图像一致性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>语言模型的隐式推理能力与多步骤推理的研究</title>
<link>https://arxiv.org/abs/2503.07604</link>
<guid>https://arxiv.org/abs/2503.07604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究语言模型在多步骤数学推理中隐式推理的表现及其局限性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨语言模型在多步骤数学推理任务中的隐式推理能力。通过从头训练GPT-2并使用精心策划的多步骤推理数据集进行实验，我们发现语言模型可以通过隐式推理实现逐步推理，并在域内和域外测试中取得高准确率。这种能力仅在使用固定模式的数据时出现，而对非固定模式数据的训练则产生了过拟合现象，且缺乏进一步的泛化能力。值得注意的是，这一限制在先进的大型语言模型中同样存在。这些结果表明，语言模型通过捷径学习获得隐式推理能力，能够在类似模式的任务中表现出色，但却缺乏更广泛的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:31 GMT</pubDate>
</item>
<item>
<title>优化测试时间计算的元强化学习方法</title>
<link>https://arxiv.org/abs/2503.07572</link>
<guid>https://arxiv.org/abs/2503.07572</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出通过元强化学习优化测试时间计算的方法，显著提升LLM的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过元强化学习（MRT）优化测试时间计算，以提高大型语言模型（LLMs）的推理性能。当前的方法主要依赖于基于搜索轨迹的微调或使用零一奖励的强化学习，但其效率和扩展性存在质疑。我们将优化测试时间计算的问题形式化为一个元强化学习问题，并提出通过累积遗憾来衡量测试时间计算的有效性。研究表明，当前最先进的模型并未最小化遗憾，而是可以通过最大化密集奖励与零一奖励强化学习相结合来实现。最终，我们提出的MRT方法在数学推理上相比于传统的结果奖励强化学习，性能提升了2-3倍，并且相应的令牌效率提高了约1.5倍。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07572" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:40:43 GMT</pubDate>
</item>
<item>
<title>LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL</title>
<link>https://arxiv.org/abs/2503.07536</link>
<guid>https://arxiv.org/abs/2503.07536</guid>
<content:encoded><![CDATA[
Enhancing reasoning in Large Multimodal Models (LMMs) faces unique challenges from the complex interplay between visual perception and logical reasoning, particularly in compact 3B-parameter architectures where architectural constraints limit reasoning capacity and modality alignment.   While rule-based reinforcement learning (RL) excels in text-only domains, its multimodal extension confronts two critical barriers: (1) data limitations due to ambiguous answers and scarce complex reasoning examples, and (2) degraded foundational reasoning induced by multimodal pretraining.   To address these challenges, we propose \method, a two-stage framework adapting rule-based RL for multimodal reasoning through Foundational Reasoning Enhancement (FRE) followed by Multimodal Generalization Training (MGT). The FRE stage first strengthens reasoning abilities using text-only data with rule-based RL, then the MGT stage generalizes these reasoning capabilities to multimodal domains.   Experiments on Qwen2.5-VL-Instruct-3B demonstrate that \method achieves 4.83\% and 4.5\% average improvements over baselines in multimodal and text-only benchmarks, respectively, with a 3.63\% gain in complex Football Game tasks. These results validate that text-based reasoning enhancement enables effective multimodal generalization, offering a data-efficient paradigm that bypasses costly high-quality multimodal training data.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>MoE-X：一种具备内在可解释性的混合专家语言模型</title>
<link>https://arxiv.org/abs/2503.07639</link>
<guid>https://arxiv.org/abs/2503.07639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MoE-X是一种新型语言模型，专注于提高可解释性和性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MoE-X，一种基于混合专家（MoE）架构的语言模型，旨在实现内在可解释性。研究表明，宽而稀疏激活的网络更能捕捉可解释因素，但直接训练这样的网络计算成本高。因此，MoE-X通过激活部分专家来提供可扩展的解决方案。将MoE层重写为稀疏的大型多层感知机（MLP）使得在保持稀疏性的同时，隐藏层规模得以高效扩展。为了进一步增强可解释性，MoE-X在每个专家中强制稀疏激活，并重新设计路由机制，优先考虑激活稀疏性最高的专家。实验表明，MoE-X在国际象棋和自然语言任务上性能与密集模型相当，并且其可解释性显著提升，达到比GPT-2更低的困惑度，优于基于稀疏自编码器的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 12:40:54 GMT</pubDate>
</item>
<item>
<title>PhiloBERTA：跨语言的古希腊与拉丁语词汇语义关系测量</title>
<link>https://arxiv.org/abs/2503.05265</link>
<guid>https://arxiv.org/abs/2503.05265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhiloBERTA模型揭示古希腊与拉丁词汇间的语义对齐情况。</p><br /><br /><p><strong>摘要：</strong> PhiloBERTA是一种跨语言的变换模型，用于测量古希腊与拉丁语词汇之间的语义关系。通过经典文本中选定术语对的分析，我们利用上下文嵌入和角度相似度指标识别精确的语义对齐。在实验结果中， etymologically（词源上）相关的词对显示出显著更高的相似性分数，尤其是在诸如epistēme（科学）与dikaiosynē（正义）等抽象哲学概念上。统计分析表明，这些关系存在一致的模式，p值为0.012，且词源上相关的对比对展示出比对照对更为稳定的语义保存。这些发现为研究哲学概念在古希腊与拉丁传统中的传播建立了量化框架，并为古典语言学研究提供了新方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:30:16 GMT</pubDate>
</item>
<item>
<title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
<link>https://arxiv.org/abs/2503.02819</link>
<guid>https://arxiv.org/abs/2503.02819</guid>
<content:encoded><![CDATA[
While score-based generative models are the model of choice across diverse domains, there are limited tools available for controlling inference-time behavior in a principled manner, e.g. for composing multiple pretrained models. Existing classifier-free guidance methods use a simple heuristic to mix conditional and unconditional scores to approximately sample from conditional distributions. However, such methods do not approximate the intermediate distributions, necessitating additional 'corrector' steps. In this work, we provide an efficient and principled method for sampling from a sequence of annealed, geometric-averaged, or product distributions derived from pretrained score-based models. We derive a weighted simulation scheme which we call Feynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula by carefully accounting for terms in the appropriate partial differential equations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo (SMC) resampling algorithms that leverage inference-time scaling to improve sampling quality. We empirically demonstrate the utility of our methods by proposing amortized sampling via inference-time temperature annealing, improving multi-objective molecule generation using pretrained models, and improving classifier-free guidance for text-to-image generation. Our code is available at https://github.com/martaskrt/fkc-diffusion.
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 12:46:51 GMT</pubDate>
</item>
<item>
<title>VACE：全能视频生成与编辑框架</title>
<link>https://arxiv.org/abs/2503.07598</link>
<guid>https://arxiv.org/abs/2503.07598</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VACE提供一个统一的视频生成与编辑解决方案，处理多种视频任务。</p><br /><br /><p><strong>摘要：</strong> VACE是一种新的平台，旨在整合视频生成和编辑任务，克服视频合成中的空间和时间一致性挑战。该框架支持参考视频生成、视频编辑和带掩膜的视频编辑等多种功能，用户可以通过统一的界面（视频条件单元）进行操作。通过采用上下文适配器结构，VACE能够灵活处理各种视频合成任务，并将不同任务概念正式化地注入模型。实验结果表明，VACE在多种子任务上的表现与专用模型相当，同时支持多样化的应用组合，展现了其在视频内容创作领域的强大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07598" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>提升模型领域泛化能力的方法研究</title>
<link>https://arxiv.org/abs/2503.06698</link>
<guid>https://arxiv.org/abs/2503.06698</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用预训练特征提升领域泛化能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本研究关注于如何提高模型在新颖且未见数据分布下的泛化能力，探讨模型架构和预训练目标对特征丰富性的影响。我们提出通过发现潜在的伪域结构，从而在无监督的方式下捕捉特定领域的变化，并利用这些伪域表示增强现有分类器，使其更适应未见测试领域。通过对不同预训练特征空间的分析，发现扩散模型的特征在没有显式领域标签的情况下，能有效区分领域并捕捉细致的领域特征。在五个数据集上的实证研究显示，我们的方法较标准基线（经验风险最小化）在未见领域的泛化能力提升了超过4%的测试准确率，并且显著超越绝大多数训练过程中使用领域标签的算法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06698" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 13:29:01 GMT</pubDate>
</item>
<item>
<title>Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries</title>
<link>https://arxiv.org/abs/2502.20475</link>
<guid>https://arxiv.org/abs/2502.20475</guid>
<content:encoded><![CDATA[
To answer one-to-many factual queries (e.g., listing cities of a country), a language model (LM) must simultaneously recall knowledge and avoid repeating previous answers. How are these two subtasks implemented and integrated internally? Across multiple datasets and models, we identify a promote-then-suppress mechanism: the model first recalls all answers, and then suppresses previously generated ones. Specifically, LMs use both the subject and previous answer tokens to perform knowledge recall, with attention propagating subject information and MLPs promoting the answers. Then, attention attends to and suppresses previous answer tokens, while MLPs amplify the suppression signal. Our mechanism is corroborated by extensive experimental evidence: in addition to using early decoding and causal tracing, we analyze how components use different tokens by introducing both Token Lens, which decodes aggregated attention updates from specified tokens, and a knockout method that analyzes changes in MLP outputs after removing attention to specified tokens. Overall, we provide new insights into how LMs' internal components interact with different input tokens to support complex factual recall. Code is available at https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.
]]></content:encoded>
<pubDate>Thu, 27 Feb 2025 14:23:15 GMT</pubDate>
</item>
<item>
<title>REF-VLM：统一视觉解码任务的多模态大型语言模型框架</title>
<link>https://arxiv.org/abs/2503.07413</link>
<guid>https://arxiv.org/abs/2503.07413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REF-VLM框架提升了多模态解码任务的性能与适应性。</p><br /><br /><p><strong>摘要：</strong> REF-VLM是一种端到端框架，旨在统一训练各种视觉解码任务。针对稠密预测任务的挑战，我们提出了三元组引用范式（TRP），通过三元组结构明确解耦视觉解码任务中的概念、解码类型和目标。同时，构建了VTInstruct数据集，包含超过1亿条多模态对话样本，涵盖25种任务类型，结合文本输入和多种视觉提示，如点、框、涂鸦和掩膜。REF-VLM的优越性通过定性和定量实验得以验证，超越了现有的多模态大型语言模型，展示了其在复杂视觉解码场景下的强大适应性和性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:59:14 GMT</pubDate>
</item>
<item>
<title>TRCE：提高文本生成模型中恶意内容的概念抹除能力</title>
<link>https://arxiv.org/abs/2503.07389</link>
<guid>https://arxiv.org/abs/2503.07389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TRCE通过两阶段策略有效抹除文本生成中的恶意概念。</p><br /><br /><p><strong>摘要：</strong> 随着文本生成模型在照片现实主义图像生成中的进步，如何防止恶意内容的产生成为一个重要课题。本研究提出TRCE，一种采用两阶段策略的概念抹除方法，旨在实现可靠的恶意概念抹除与模型知识保留之间的有效权衡。首先，TRCE识别并优化跨注意力层，将隐含的恶意语义映射为包含安全概念的相似提示，以减少模型在去噪过程中受到恶意语义的影响。其次，TRCE利用扩散模型的采样轨迹特性，通过对比学习引导早期去噪预测朝向安全方向，避免生成恶意内容。通过对多个恶意概念抹除基准的全面评估，结果表明TRCE在抹除恶意概念的同时，更好地保留了模型的原始生成能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:37:53 GMT</pubDate>
</item>
<item>
<title>自回归表示对齐框架（ARRA）在文本到图像生成中的应用</title>
<link>https://arxiv.org/abs/2503.07334</link>
<guid>https://arxiv.org/abs/2503.07334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARRA框架通过对齐隐藏状态实现文本到图像生成的全局一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了自回归表示对齐（ARRA）训练框架，旨在实现自回归大型语言模型（LLMs）中全局一致的文本到图像生成，而无需对架构进行复杂修改。与以往需要大规模架构重设计的工作不同，ARRA通过全局视觉对齐损失和混合标记将LLM的隐藏状态与外部视觉基础模型的视觉表示对齐。该框架在保留自回归范式的同时，通过强制局部下一个标记预测和全局语义蒸馏的双重约束，使大型语言模型能够隐式学习空间和上下文一致性。广泛的实验结果验证了ARRA的灵活性，显示在从文本生成的LLM或随机初始化的情况下训练时，ARRA能够分别在高级自回归LLM如Chameleon和LlamaGen上减少25.5%的FID（MIMIC-CXR）、8.8%（DeepEyeNet）和7.5%（ImageNet）。在领域适应方面，ARRA还成功地将通用LLM与专用模型对齐，以在医学成像（MIMIC-CXR）上实现18.6%的FID减少。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:49:28 GMT</pubDate>
</item>
<item>
<title>基于SlotMIM的预训练视觉模型在机器人学习中的优化研究</title>
<link>https://arxiv.org/abs/2503.06960</link>
<guid>https://arxiv.org/abs/2503.06960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出SlotMIM方法以优化视觉模型在机器人学习中的表现。</p><br /><br /><p><strong>摘要：</strong> 预训练视觉模型（PVMs）在现代机器人中扮演重要角色，但其最佳配置仍不明确。通过系统评估，我们发现DINO和iBOT在视觉运动控制和感知任务上表现优于MAE，但在非单对象中心（NOC）数据上训练时存在困难，影响其学习对象中心表示的能力。为此，我们设计了SlotMIM方法，利用语义瓶颈减少原型数量，促进对象性出现，并引入跨视图一致性正则化以鼓励多视图不变性。我们的实验涵盖对象中心、场景中心、网络抓取及自我中心数据的预训练，结果表明该方法在图像识别、场景理解和机器人学习评估方面显著优于现有研究，且在百万规模数据集上展现出优良的数据效率和可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 02:18:31 GMT</pubDate>
</item>
<item>
<title>DiffCLIP：基于差分注意力机制的视觉-语言模型</title>
<link>https://arxiv.org/abs/2503.06626</link>
<guid>https://arxiv.org/abs/2503.06626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DiffCLIP模型通过差分注意力机制提升视觉-语言理解性能。</p><br /><br /><p><strong>摘要：</strong> DiffCLIP是一种新颖的视觉-语言模型，通过将差分注意力机制扩展到CLIP架构中，显著提升了图像与文本理解的能力。差分注意力最初是为大型语言模型开发的，旨在放大相关上下文并消除噪声信息。通过将这一机制集成到CLIP的双编码器框架中，DiffCLIP在零-shot分类、检索和鲁棒性基准测试中表现优越，持续超越传统CLIP模型。研究表明，这些性能提升伴随着微不足道的计算开销，表明差分注意力能显著增强多模态表示，而不牺牲效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 10:04:09 GMT</pubDate>
</item>
<item>
<title>Symbolic-MoE：基于技能的专家选择框架提升LLM性能</title>
<link>https://arxiv.org/abs/2503.05641</link>
<guid>https://arxiv.org/abs/2503.05641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Symbolic-MoE通过实例级专家选择显著提升了预训练LLM的性能。</p><br /><br /><p><strong>摘要：</strong> Symbolic-MoE是一个基于技能的Mixture-of-Experts框架，旨在通过实例级别的细粒度专家选择来提升预训练大语言模型（LLM）的性能。该方法关注于根据不同任务的专业技能，如数学中的代数或生物医学推理中的分子生物学，动态选择最合适的LLM专家。通过实施批量推理策略，有效减少了模型的加载开销，使得在单个GPU上能够整合16个专家模型，性能超越此前多代理基线。经过在多个基准（如MMLU-Pro、GPQA、AIME和MedMCQA）的广泛评估，Symbolic-MoE在精度上提升了8.15%，并且比基于讨论的基线方法在计算上更加高效，去除了昂贵的多轮讨论需求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 13:03:13 GMT</pubDate>
</item>
<item>
<title>基于多镜头视频的人类动作重建框架</title>
<link>https://arxiv.org/abs/2503.07597</link>
<guid>https://arxiv.org/abs/2503.07597</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，从多镜头视频中重建长序列3D人类动作。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，旨在从具有多镜头切换的野外视频中重建3D人类的长序列运动。此类长序列运动对运动生成和理解等应用至关重要，但由于镜头切换突变、部分遮挡和动态背景等问题，重建过程面临巨大挑战。现有方法主要集中于单镜头视频，或简化多镜头的对齐处理。本研究通过整合增强的相机姿态估计与人类动作恢复（HMR），并结合镜头切换检测器和强大的对齐模块，解决了姿态和方向在镜头间的连续性。通过采用定制的动作整合器，有效减少了足部滑动问题，并确保人类姿态的时间一致性。在创建的多镜头数据集上的广泛评估，表明我们的方法在实际的世界坐标中重建人类运动方面的鲁棒性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07597" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:57:03 GMT</pubDate>
</item>
<item>
<title>适配器引导蒸馏：提升条件扩散模型采样效率</title>
<link>https://arxiv.org/abs/2503.07274</link>
<guid>https://arxiv.org/abs/2503.07274</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">适配器引导蒸馏提升了条件扩散模型的采样效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种新的方法——适配器引导蒸馏（AGD），旨在提高条件扩散模型在推理时的效率。传统的分类器自由引导（CFG）需要双倍的神经函数评估（NFE），而AGD则通过轻量级适配器在一次前向传播中模拟CFG，从而有效地加快采样速度，同时保持或提升样本质量。与以往需要调整整个模型的引导蒸馏方法不同，AGD仅训练少量额外参数（约2%），并保持基本模型的不变性，以显著降低资源需求。此外，AGD通过在CFG引导的轨迹上进行训练，解决了现有引导蒸馏方法在训练和推理中的关键不匹配问题。实验表明，AGD在多个架构中与CFG相比较，FID值相当或更优，且仅需提供一半的NFE。我们的研究使得在单个消费级GPU上对大型模型（约2.6B参数）的蒸馏成为可能，进一步推动了这一领域的可访问性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07274" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:55:08 GMT</pubDate>
</item>
<item>
<title>WISE：基于世界知识的文本到图像生成语义评估基准</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WISE提出了一种新的评估基准，提升文本到图像生成的语义理解能力。</p><br /><br /><p><strong>摘要：</strong> 文本到图像(T2I)模型在艺术创作和视觉内容生成方面具有出色能力，但现有研究主要关注图像的真实感和浅层的文本-图像匹配，缺乏对复杂语义理解和世界知识整合的全面评估。为了解决这一问题，本文提出了WISE，一个专门为世界知识驱动的语义评估设计的基准。WISE通过从25个子领域中精心设计的1000个提示，超越了简单的词汇与像素映射，挑战模型在文化常识、时空推理和自然科学等领域的能力。本文还引入了一种新量化指标WiScore，用于评估知识与图像的对齐。通过对20个模型的全面测试，结果显示，其在图像生成中有效整合和应用世界知识的能力显著不足。这些发现为下一代T2I模型在知识整合与应用方面的提升提供了重要指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 08:47:53 GMT</pubDate>
</item>
<item>
<title>新型零样本音视频语音识别框架Zero-AVSR</title>
<link>https://arxiv.org/abs/2503.06273</link>
<guid>https://arxiv.org/abs/2503.06273</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Zero-AVSR框架可实现目标语言的零样本音视频语音识别。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新型零样本音视频语音识别框架Zero-AVSR，该框架允许在无目标语言音视频语音数据的情况下进行语音识别。我们提出的音视频语音罗马化器(AV-Romanizer)通过预测罗马文本学习语言无关的语音表示。同时，利用大型语言模型(LLMs)的强大多语言建模能力，将预测的罗马文本转换为特定语言的字母字符。此外，我们探索了一种统一的Zero-AVSR方法，通过将由AV-Romanizer编码的音视频语音表示直接整合到LLM中，借助我们提出的多任务学习方案对适配器和LLM进行微调。为捕捉广泛的音位和语言多样性，我们还引入了一个包含2916小时音视频语音数据的多语言音视频罗马化语料库(MARC)，覆盖82种语言，并提供语言特定的字母字符和罗马文本的转录。大量分析和实验表明，Zero-AVSR框架能够扩展对未见语言的支持。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06273" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 11:40:13 GMT</pubDate>
</item>
<item>
<title>Novel Object 6D Pose Estimation with a Single Reference View</title>
<link>https://arxiv.org/abs/2503.05578</link>
<guid>https://arxiv.org/abs/2503.05578</guid>
<content:encoded><![CDATA[
Existing novel object 6D pose estimation methods typically rely on CAD models or dense reference views, which are both difficult to acquire. Using only a single reference view is more scalable, but challenging due to large pose discrepancies and limited geometric and spatial information. To address these issues, we propose a Single-Reference-based novel object 6D (SinRef-6D) pose estimation method. Our key idea is to iteratively establish point-wise alignment in the camera coordinate system based on state space models (SSMs). Specifically, iterative camera-space point-wise alignment can effectively handle large pose discrepancies, while our proposed RGB and Points SSMs can capture long-range dependencies and spatial information from a single view, offering linear complexity and superior spatial modeling capability. Once pre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novel object using only a single reference view, without requiring retraining or a CAD model. Extensive experiments on six popular datasets and real-world robotic scenes demonstrate that we achieve on-par performance with CAD-based and dense reference view-based methods, despite operating in the more challenging single reference setting. Code will be released at https://github.com/CNJianLiu/SinRef-6D.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 12:00:41 GMT</pubDate>
</item>
<item>
<title>Mixture of Large Language Model Agents的安全性与防御机制研究</title>
<link>https://arxiv.org/abs/2503.05856</link>
<guid>https://arxiv.org/abs/2503.05856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨MoA架构的安全性及其对欺骗性LLM代理的脆弱性。</p><br /><br /><p><strong>摘要：</strong> 本文首次对Mixture of Large Language Model Agents (MoA)架构在面对欺骗性LLM代理时的安全性与可靠性进行全面研究。尽管MoA在如AlpacaEval 2.0等基准测试中表现优异，但我们发现其存在重要的脆弱性。在研究中，我们分析了欺骗信息的传播、模型规模和信息可用性等因素，结果显示引入一个精心指令的欺骗代理会使MoA的表现显著下降，AlpacaEval 2.0测试中的表现从49.2%降低至37.9%。在QuALITY测试中，准确率也下降了48.5%。针对这些问题，本文提出了一系列无监督防御机制，旨在恢复丧失的性能，灵感来源于历史上威尼斯的投票过程，旨在减少影响与欺骗。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 09:46:39 GMT</pubDate>
</item>
<item>
<title>任务感知的键值缓存压缩：提升大型语言模型的信息处理效率</title>
<link>https://arxiv.org/abs/2503.04973</link>
<guid>https://arxiv.org/abs/2503.04973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法压缩外部知识，以增强语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种任务感知的键值（KV）缓存压缩方法，以优化大型语言模型（LLMs）对外部知识的利用。现有方法如检索增强生成（RAG）虽然能通过相似性搜索获取证据，但可能错过关键信息；而长上下文模型虽然能处理多个文档，但计算成本高且受限于上下文窗口大小。我们的方法模拟学生为开放书考试而浓缩学习材料，允许在零样本或少样本设置下有效压缩外部知识。实验表明，该方法在LongBench v2基准测试中，相较于RAG提高了最多7个绝对准确度，同时以30倍的压缩率减少推理延迟，从0.43秒降至0.16秒。对于稀疏证据的任务，RAG表现良好，而任务感知压缩在广泛知识的任务中表现更优。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 16:07:41 GMT</pubDate>
</item>
<item>
<title>YOLOE：高效的开放式检测与分割模型</title>
<link>https://arxiv.org/abs/2503.07465</link>
<guid>https://arxiv.org/abs/2503.07465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">YOLOE结合检测与分割，提升开放场景的实时性能。</p><br /><br /><p><strong>摘要：</strong> YOLOE是一个新型的高效模型，旨在解决传统检测模型在开放场景中的适应性问题。该模型融合了多种开放提示机制的检测与分割能力，包括文本提示的Re-parameterizable Region-Text Alignment（RepRTA）策略，视觉提示的Semantic-Activated Visual Prompt Encoder（SAVPE），以及针对无提示场景的Lazy Region-Prompt Contrast（LRPC）策略。YOLOE在LVIS数据集上显示出卓越的零样本表现，其训练成本降低到YOLO-Worldv2-S的三分之一，同时推理速度提升1.4倍。在迁移到COCO数据集时，YOLOE-v8-L相较于闭集YOLOv8-L获得了0.6 AP^b和0.4 AP^m的提升，训练时间减少近四倍。通过这些创新，YOLOE在保持高效性的同时，实现了高准确率，证明了其在实际应用中的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:42:59 GMT</pubDate>
</item>
<item>
<title>基于ReLU的偏好优化算法RePO：简化语言模型对齐方法</title>
<link>https://arxiv.org/abs/2503.07426</link>
<guid>https://arxiv.org/abs/2503.07426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RePO通过简化超参数与新算法有效优化语言模型对齐。</p><br /><br /><p><strong>摘要：</strong> Aligning large language models (LLMs) with human preferences is faced with challenges in computational efficiency and stability. Existing methods, such as reinforcement learning from human feedback (RLHF), often struggle with complex parameters. In contrast, we propose a novel approach called ReLU-based Preference Optimization (RePO), which streamlines the alignment process by eliminating the need for a beta hyperparameter. This is achieved through two main innovations: retaining reference-free margins while utilizing gradient analysis to remove beta, and employing a ReLU-based max-margin loss to filter trivial pairs effectively. Theoretically, RePO is positioned as a limiting case of SimPO where logistic weighting simplifies to binary thresholding. Empirical evaluations on datasets like AlpacaEval 2 and Arena-Hard demonstrate that RePO consistently outperforms existing methods DPO and SimPO, achieving effective alignment while requiring only a single hyperparameter adjustment.</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:11:07 GMT</pubDate>
</item>
<item>
<title>Llama-MTSK：一种灵活的音视频识别多模态语言模型</title>
<link>https://arxiv.org/abs/2503.06362</link>
<guid>https://arxiv.org/abs/2503.06362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-MTSK通过多层次表示提升音视频识别的效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 音视频识别（AVSR）结合音频和视觉信息提高了语音识别在嘈杂环境中的鲁棒性。随着大语言模型（LLM）的进步，AVSR领域的表现显著提升。然而，直接将语音表示集成至LLM面临高计算成本。为了解决这一问题，提出了Llama-MTSK，这是一种基于马特ryoshka的多模态LLM，能够根据计算约束灵活适配音视频令牌分配，同时保持高性能。该方法在单个模型中编码不同粒度的音视频表示，避免了训练多个模型以适应不同压缩级别的需要。通过引入基于LoRA的马特ryoshka策略，Llama-MTSK在两个大型AVSR数据集上的评估结果显示出其优越性，达到了最先进的性能，匹配或超越了在固定压缩级别上独立训练的模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 19:02:10 GMT</pubDate>
</item>
<item>
<title>探索三维编码器与文本特征空间的后期对齐</title>
<link>https://arxiv.org/abs/2503.05283</link>
<guid>https://arxiv.org/abs/2503.05283</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究三维编码器与文本特征空间的后期对齐方法及其效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨三维编码器在与文本特征空间的对齐中的作用，分析传统方法的局限性。研究发现，仅依靠简单的后期对齐训练，文本与三维编码器的性能提升有限。通过提取特征空间的子空间并进行有针对性的投影，显著提高了对齐质量，进而提高了匹配和检索任务的准确性。此外，分析显示这些共享子空间大致分隔了语义与几何数据表示。此研究首次为三维单模态与文本特征空间的后期对齐建立了基线，并突出了与其他表示相比三维数据的共享与独特属性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05283" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 04:51:56 GMT</pubDate>
</item>
<item>
<title>WritingBench：提升大语言模型写作能力的综合评估基准</title>
<link>https://arxiv.org/abs/2503.05244</link>
<guid>https://arxiv.org/abs/2503.05244</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Introducing WritingBench, a benchmark to evaluate LLMs in diverse writing domains.</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的进步，文本生成能力显著增强，但评估其在写作上的表现仍然面临挑战。现有的基准主要集中在通用文本生成或有限的写作任务上，未能全面捕捉高质量书面内容在各个领域的多样化需求。为此，本文提出了WritingBench，这是一个综合性基准，旨在评估LLMs在六个核心写作领域及其一百个子领域的表现，涵盖创意、说服性、信息性和技术性写作。此外，我们还提出了一种依赖查询的评估框架，使LLMs能够动态生成特定实例的评估标准。该框架配备了一个调优后的评判模型，能够在风格、格式和长度方面进行标准化评分，框架的有效性通过其数据策划能力得以进一步验证，7B参数模型在该基准下可接近最先进的性能。该基准及评估工具与模块化框架组件将以开源形式发布，以推动LLMs在写作方面的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05244" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 07 Mar 2025 03:56:20 GMT</pubDate>
</item>
<item>
<title>AlphaDrive：基于强化学习和推理的自动驾驶视觉语言模型框架</title>
<link>https://arxiv.org/abs/2503.07608</link>
<guid>https://arxiv.org/abs/2503.07608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出AlphaDrive框架，提升了自动驾驶的规划性能和训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AlphaDrive，一个将强化学习（RL）与推理相结合的视觉语言模型（VLM）框架，旨在解决自动驾驶中的复杂规划问题。AlphaDrive引入了四种基于GRPO的RL奖励机制，并采用两阶段的推理训练策略，将监督微调（SFT）与强化学习相结合，从而显著提高了自动驾驶系统的规划性能和训练效率。研究还发现，在经历强化学习训练后，AlphaDrive展现出新兴的多模态规划能力，这对提升驾驶安全和效率至关重要。该框架是首个将GRPO基础的强化学习与规划推理相结合应用于自动驾驶中的研究，未来将发布代码以促进相关研究的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>图像与文本结合预训练模型在视语言任务中的表现</title>
<link>https://arxiv.org/abs/2503.07603</link>
<guid>https://arxiv.org/abs/2503.07603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，图像与文本结合的预训练模型在视语言任务中表现更佳。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了结合图像数据进行预训练的语言模型在视语言任务中的表现，重点分析了两步培训流程与早期集成图像的视觉语言模型（VLMs）之间的收益和损失。通过对不同数据集、模型规模、图像文本比率及预训练量进行实验，结果显示，采用图像和文本数据混合预训练的模型在视语言任务中表现优越，同时在文字仅任务中的表现依然强劲。具体而言，对于一个10亿参数的模型，在预训练过程中将视觉标记引入到80%时，相比于在完全预训练模型中引入视觉标记，平均提升了2%的任务表现。这表明图像集成的时机对模型性能具有重要影响。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:19 GMT</pubDate>
</item>
<item>
<title>DreamRelation：一种基于示例视频的个性化关系视频定制方法</title>
<link>https://arxiv.org/abs/2503.07602</link>
<guid>https://arxiv.org/abs/2503.07602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DreamRelation通过示例视频优化个性化关系视频生成，提升模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 关系视频定制是实现个性化视频的重要环节，但现有方法在处理复杂关系时存在困难，尤其是需要准确关系建模和在多样主题类别间的高泛化能力。为了解决这些问题，我们提出了DreamRelation方法，它利用示例视频通过两大核心组成部分：关系解耦学习和关系动态增强，实现个性化关系建模。在关系解耦学习中，我们剥离了关系与主题外观的交互，确保了在多样关系中的良好泛化。此外，我们通过分析MM-DiT的注意力机制中查询、键和值特征的不同角色，优化了关系LoRA三元组的设计，使该框架具备可解释性。在关系动态增强中，我们引入了时空关系对比损失，优先考虑关系动态，同时减少对详细外观的依赖。实验结果表明，DreamRelation在关系视频定制方面优于现有最先进方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07602" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>MedAgentsBench: 复杂医学问题的新评估基准</title>
<link>https://arxiv.org/abs/2503.07459</link>
<guid>https://arxiv.org/abs/2503.07459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MedAgentsBench基准，以评估复杂医学问题的多步骤推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MedAgentsBench，一个新颖的基准，专注于需要多步骤临床推理、诊断制定和治疗规划的复杂医学问题。在当前的医疗问答基准中，尽管大型语言模型（LLMs）表现优异，但在面对复杂问题时仍然存在明显不足。MedAgentsBench从七个已有的医学数据集中提取数据，旨在解决现有评估中的三大关键限制，包括简单问题导致的高基线性能、一致性不足的采样与评估协议，以及缺乏对性能、成本和推理时间相互关系的系统分析。通过对多种基础模型和推理方法的实验，结果表明，最新的思维模型如DeepSeek R1和OpenAI o3在复杂医学推理任务中表现优异。此外，基于搜索的先进代理方法在性能与成本比方面表现出色，适应不同计算约束下的最佳模型选择也得到了识别。该基准及评估框架已公开发布，供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 11:38:44 GMT</pubDate>
</item>
<item>
<title>DistiLLM-2：通过对比学习提升语言模型蒸馏效果</title>
<link>https://arxiv.org/abs/2503.07067</link>
<guid>https://arxiv.org/abs/2503.07067</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DistiLLM-2通过对比学习显著提升语言模型蒸馏性能。</p><br /><br /><p><strong>摘要：</strong> DistiLLM-2提出了一种新的对比学习方法，旨在通过有效调和教师与学生模型之间的损失函数，提升学生模型的表现。与以往的蒸馏策略相比，该方法同时提高了教师生成回应的概率，并降低了学生生成回应的概率。实验证明，DistiLLM-2在执行指令跟随、代码生成等多种任务时均能构建高性能的学生模型。此外，该方法还支持多样化的应用场景，如偏好对齐和视觉语言扩展。这些研究结果彰显了通过对比学习来增强语言模型蒸馏效果的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07067" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:51:32 GMT</pubDate>
</item>
<item>
<title>ProBench: 新型多模态智能评估基准的构建与实证分析</title>
<link>https://arxiv.org/abs/2503.06885</link>
<guid>https://arxiv.org/abs/2503.06885</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ProBench是一个涵盖多领域的专业多模态智能评估基准。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了ProBench，一个开创性的多模态智能评估基准，旨在测试先进模型在处理专业用户查询时的能力。ProBench包含4000个由专业人士根据日常工作需求独立提交的高质量样本，覆盖科学、艺术、人文学科、编程、数学和创意写作等10个领域及56个子领域。通过MLLM作为评判者，评估了24个最新模型的表现。研究结果显示，尽管最优秀的开源模型在某些方面可与专有模型相媲美，但在视觉感知、文本理解、领域知识和高级推理等方面，ProBench提出了显著挑战，为未来的多模态AI研究指明了方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06885" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 23:29:18 GMT</pubDate>
</item>
<item>
<title>Vision-R1：增强多模态推理能力的深度学习模型</title>
<link>https://arxiv.org/abs/2503.06749</link>
<guid>https://arxiv.org/abs/2503.06749</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vision-R1通过强化学习提升多模态推理能力，展示出优异的模型表现。</p><br /><br /><p><strong>摘要：</strong> DeepSeek-R1-Zero通过强化学习成功展示了大型语言模型（LLMs）的推理能力。受到这一突破的启发，本文探讨如何利用强化学习提升多模态大语言模型（MLLMs）的推理能力。由于缺乏高质量的多模态推理数据，直接使用强化学习难以激活MLLMs的复杂推理功能。为此，本文提出了Vision-R1模型，通过利用现有的MLLM和DeepSeek-R1构建了一个高质量的多模态推理链（CoT）数据集——Vision-R1-cold数据集。随后，我们引入了渐进思维抑制训练（PTST）策略和群体相对策略优化（GRPO）以改善模型的推理能力。通过广泛的实验，模型在多个多模态数学推理基准上平均提升了6%的表现，其中Vision-R1-7B在MathVista基准上达到了73.5%的准确率，仅比领先模型OpenAI O1低0.4%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06749" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 16:06:45 GMT</pubDate>
</item>
<item>
<title>SurveyForge：提升文献综述生成质量的自动化工具</title>
<link>https://arxiv.org/abs/2503.04629</link>
<guid>https://arxiv.org/abs/2503.04629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SurveyForge利用LLMs提升文献综述生成的质量与效率。</p><br /><br /><p><strong>摘要：</strong> 随着科研出版物的快速增长，文献综述在科学研究中发挥着重要作用。近期，研究者们开始使用大语言模型(LLMs)来自动化文献综述的生成，以提高效率。然而，LLM生成的综述在结构和引用准确性方面仍显著低于人工撰写的综述。为了解决这些问题，我们提出了SurveyForge，首先通过分析人工撰写的综述的逻辑结构并参考相关领域的文章生成提纲。随后，SurveyForge利用内存中检索到的高质量论文，自动生成和完善综述内容。此外，我们构建了SurveyBench，以实现全面评估，其中包括100篇人工撰写的综述论文用于胜率比较，并从参考文献、提纲和内容质量三个维度评估AI生成的综述论文。实验结果表明，SurveyForge在质量上优于AutoSurvey等以往工作。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title>提升人工文本检测的可解释性：稀疏自编码器的应用</title>
<link>https://arxiv.org/abs/2503.03601</link>
<guid>https://arxiv.org/abs/2503.03601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过稀疏自编码器提升人工文本检测的可解释性，以分析不同模型的文本特征。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的崛起，人工文本检测（ATD）变得愈加重要。然而，目前没有单一算法能够在不同类型的未见文本上始终如一地表现良好，也无法有效地推广到新的语言模型上。可解释性在实现这一目标中起着关键作用。本研究通过采用稀疏自编码器（SAE）来提取Gemma-2-2b残差流中的特征，从而增强ATD的可解释性。我们识别出既可解释又高效的特征，并通过领域和模型特定统计、引导方法，以及手动或基于LLM的解释，对这些特征进行语义和相关性分析。研究结果为理解各种模型生成的文本与人类撰写的内容之间的差异提供了有价值的见解，表明现代LLM在信息密集型领域有独特的写作风格，尽管它们能够生成与人类相似的输出。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 10:33:52 GMT</pubDate>
</item>
<item>
<title>稀疏专家激活剪枝：优化大型语言模型推理效率的新方法</title>
<link>https://arxiv.org/abs/2503.07605</link>
<guid>https://arxiv.org/abs/2503.07605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出稀疏专家激活剪枝方法，优化大型语言模型的推理效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在自然语言处理任务中取得了显著成功，但推理时的高计算成本依然是一个瓶颈。本文介绍了一种训练无关的稀疏专家激活剪枝（SEAP）方法，该方法选择性保留与任务相关的参数，以降低推理开销。SEAP受到了LLM隐层状态和激活的聚类模式启发，识别任务特定的专家激活模式，通过剪枝模型来保持任务性能并提高计算效率。实验结果表明，SEAP在保持竞争性准确度的同时，显著降低计算开销。在50%的剪枝下，SEAP的性能超过了WandA和FLAP 20%以上，而在20%的剪枝下，与稠密模型相比，性能仅下降了2.2%。这些发现突显了SEAP的可扩展性和有效性，使其成为优化大规模LLM的有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>大型语言模型带来的假新闻风险与检测系统的挑战</title>
<link>https://arxiv.org/abs/2503.07595</link>
<guid>https://arxiv.org/abs/2503.07595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了大型语言模型带来的假新闻风险及其检测技术的挑战。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的普及，假新闻传播的风险日益加剧。因此，开发如DetectGPT这样的分类系统变得至关重要。然而，实验表明这些检测器容易受到规避技术的影响，例如通过系统性地改变生成模型的温度，导致基于浅层学习的检测器可靠性降低。此外，利用强化学习微调生成模型能够突破基于BERT的检测器。结果显示，通过改写，文本尽管与原文高度相似，仍能超过90%地规避像DetectGPT这样的零样本检测器。本文还与现有研究进行了比较，揭示了所提出方法的优越性，并讨论了对社会的潜在影响及未来的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 13:56:25 GMT</pubDate>
</item>
<item>
<title>PE3R：高效的3D重建框架</title>
<link>https://arxiv.org/abs/2503.07507</link>
<guid>https://arxiv.org/abs/2503.07507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PE3R框架在速度和精度上显著提升2D到3D重建的能力。</p><br /><br /><p><strong>摘要：</strong> 针对现有2D到3D感知方法面临的局限，本文提出了一种新颖的框架——Perception-Efficient 3D Reconstruction (PE3R)。PE3R通过前馈架构实现快速的3D语义场重建，展现出强大的零-shot泛化能力，能在多种场景和对象中有效运行。经过广泛实验验证，PE3R不仅在3D重建速度上取得了至少9倍的加速，在感知精度和重建精确度上也显著提升，为相关领域设定了新的基准。相关代码已公开可用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 12:29:10 GMT</pubDate>
</item>
<item>
<title>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.07365</link>
<guid>https://arxiv.org/abs/2503.07365</guid>
<content:encoded><![CDATA[
We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMs' reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 10:23:12 GMT</pubDate>
</item>
<item>
<title>Automated Movie Generation via Multi-Agent CoT Planning</title>
<link>https://arxiv.org/abs/2503.07314</link>
<guid>https://arxiv.org/abs/2503.07314</guid>
<content:encoded><![CDATA[
Existing long-form video generation frameworks lack automated planning, requiring manual input for storylines, scenes, cinematography, and character interactions, resulting in high costs and inefficiencies. To address these challenges, we present MovieAgent, an automated movie generation via multi-agent Chain of Thought (CoT) planning. MovieAgent offers two key advantages: 1) We firstly explore and define the paradigm of automated movie/long-video generation. Given a script and character bank, our MovieAgent can generates multi-scene, multi-shot long-form videos with a coherent narrative, while ensuring character consistency, synchronized subtitles, and stable audio throughout the film. 2) MovieAgent introduces a hierarchical CoT-based reasoning process to automatically structure scenes, camera settings, and cinematography, significantly reducing human effort. By employing multiple LLM agents to simulate the roles of a director, screenwriter, storyboard artist, and location manager, MovieAgent streamlines the production pipeline. Experiments demonstrate that MovieAgent achieves new state-of-the-art results in script faithfulness, character consistency, and narrative coherence. Our hierarchical framework takes a step forward and provides new insights into fully automated movie generation. The code and project website are available at: https://github.com/showlab/MovieAgent and https://weijiawu.github.io/MovieAgent.
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 09:33:27 GMT</pubDate>
</item>
<item>
<title>FedRand框架：提升联邦学习中的数据隐私</title>
<link>https://arxiv.org/abs/2503.07216</link>
<guid>https://arxiv.org/abs/2503.07216</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FedRand框架通过选择性共享参数增强了联邦学习中的数据隐私。</p><br /><br /><p><strong>摘要：</strong> 本文提出了FedRand框架，以解决联邦学习（FL）中数据隐私保护不足的问题。在FL中，中央服务器在聚合过程中会接触到本地客户端的模型参数，这潜在地泄露了用户数据，尤其在进行视觉语言模型（VLMs）训练时更为明显。FedRand方法创新性地通过每个客户端随机选择低秩适应（LoRA）子参数并保持其余参数的私密性，进而减少了数据隐私泄露的风险。在客户端私有数据集上训练后，仅将非私密参数返回服务器进行聚合。实验结果表明，与相关基线相比，FedRand在抵御成员推断攻击（MIAs）方面表现出更高的鲁棒性，同时在多个基准数据集上达到相似的准确性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07216" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:55:50 GMT</pubDate>
</item>
<item>
<title>eMIGM：统一的图像生成与扩散模型</title>
<link>https://arxiv.org/abs/2503.07197</link>
<guid>https://arxiv.org/abs/2503.07197</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">eMIGM模型在图像生成任务上表现卓越，尤其在ImageNet数据集上。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了eMIGM模型，它在不同动机和目标下统一了掩膜图像生成模型与掩膜扩散模型。通过探索训练与采样的设计空间，我们识别出影响性能与效率的关键因素。在经过改进后，eMIGM表现出在ImageNet生成上的强劲性能，尤其在256x256尺寸上，相同的函数评估次数（NFEs）和模型参数下，eMIGM超越了经典的VAR模型。随着NFE和模型参数的增加，eMIGM的性能与最先进的连续扩散模型相当，但所需的NFE却少于40%。在ImageNet 512x512上，eMIGM也在约60%的NFE下超越了最先进的连续扩散模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07197" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 07:27:12 GMT</pubDate>
</item>
<item>
<title>EasyControl: 高效灵活的条件引导扩散变换框架</title>
<link>https://arxiv.org/abs/2503.07027</link>
<guid>https://arxiv.org/abs/2503.07027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyControl框架通过创新模块提升扩散变换模型的控制能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了EasyControl，一个新的框架，旨在提高扩散变换模型的效率和灵活性。我们基于三个关键创新：首先，引入了一种轻量级的条件注入LoRA模块，该模块可作为即插即用的解决方案，能够独立处理条件信号，避免修改基础模型权重，支持多种条件的灵活注入。其次，提出了位置感知训练范式，可以标准化输入条件到固定分辨率，使得图像生成具备任意纵横比和灵活分辨率的能力，同时提升计算效率。最后，开发了适用于条件生成任务的因果注意力机制及KV缓存技术，大幅降低图像合成延迟，提升整体效率。通过大量实验，EasyControl在各种应用场景展现了卓越性能，充分证明了其高效、灵活和广泛适用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 04:07:17 GMT</pubDate>
</item>
<item>
<title>MMDiag: 多轮多模态对话数据集及DiagNote模型</title>
<link>https://arxiv.org/abs/2503.07002</link>
<guid>https://arxiv.org/abs/2503.07002</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了多轮多模态对话数据集MMDiag及新模型DiagNote。</p><br /><br /><p><strong>摘要：</strong> 本文提出了多轮多模态对话数据集MMDiag，旨在更真实地反映人类对话场景。该数据集通过特定规则和GPT的协助生成，强调问题之间的强关联和问题与图像之间的关系，适合多轮对话学习。为提升多模态模型的推理与基础能力，提出了DiagNote模型，具备两个交互模块（Deliberate和Gaze），分别用于链式思考和注释。通过实验证明，DiagNote在多模态信息的共同处理和推理能力上优于现有的多模态大语言模型，显示出更强的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.07002" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 10 Mar 2025 03:32:53 GMT</pubDate>
</item>
<item>
<title>FEA-Bench: 评估大型语言模型在代码库增量开发中的能力</title>
<link>https://arxiv.org/abs/2503.06680</link>
<guid>https://arxiv.org/abs/2503.06680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FEA-Bench是评估LLMs在代码库新特性开发能力的基准测试。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FEA-Bench，一个旨在评估大型语言模型（LLMs）在代码库中进行增量开发能力的基准测试框架。我们从83个GitHub仓库中收集了拉取请求，采用基于规则和意图的过滤方法，构建了专注于新特性开发的任务实例。每个任务实例包含代码修改，并与相关的单元测试文件配对，确保解决方案的可验证性。这一特性实现要求LLMs同时具备新组件的代码补全能力和其他相关代码部分的编辑能力，提供了一种更全面的评估方法。实验结果显示，LLMs在FEA-Bench测试中的表现显著较差，突显出在代码库层面增量代码开发中的诸多挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 12:11:57 GMT</pubDate>
</item>
<item>
<title>AutoCoA：提升自主性的大型代理模型框架</title>
<link>https://arxiv.org/abs/2503.06580</link>
<guid>https://arxiv.org/abs/2503.06580</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoCoA框架增强了大型代理模型的自主性，优化了工具与环境的交互。</p><br /><br /><p><strong>摘要：</strong> 传统的代理工作流程依赖外部提示来管理与工具和环境的互动，这限制了推理模型的自主性。本文提出了一个名为AutoCoA的框架，专注于大型代理模型（LAMs），使其能够自主地确定何时及如何使用外部工具。该框架结合了监督微调（SFT）和强化学习（RL），实现了模型在推理和行动之间的无缝切换，同时高效管理环境互动。AutoCoA的主要组成部分包括步骤级的动作触发、轨迹级的链式行动优化，以及内部世界模型，以降低真实环境互动成本。评估结果显示，经过AutoCoA训练的代理模型在开放域问答任务中显著超越基于ReAct的工作流程，尤其是在需要长期推理和多步骤行动的任务中。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06580" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 08:19:47 GMT</pubDate>
</item>
<item>
<title>Seg-Zero: 用于分割推理的零-shot 学习框架</title>
<link>https://arxiv.org/abs/2503.06520</link>
<guid>https://arxiv.org/abs/2503.06520</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seg-Zero框架通过认知强化实现了有效的分割推理和零-shot泛化。</p><br /><br /><p><strong>摘要：</strong> 传统的分割推理方法依赖于带有类别标签和简单描述的监督微调，限制了其跨域泛化能力，并缺乏明确的推理过程。为解决这一问题，本文提出了Seg-Zero框架，展现了出色的泛化能力，并通过认知强化生成明确的推理链条。Seg-Zero采用解耦架构，包括推理模型和分割模型，其中推理模型解读用户意图，生成明确的推理链并产生位置提示，这些提示随后被分割模型用于生成精准的像素级掩模。我们设计了一种复杂的奖励机制，结合格式和准确性奖励，有效指导优化方向。通过强化学习训练，并不使用显式推理数据，Seg-Zero实现了强大的零-shot泛化能力，并在测试时展现了突出的推理能力。实验结果显示，Seg-Zero-7B在ReasonSeg基准测试中实现了57.5的零-shot性能，超过之前的LISA-7B模型18%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.06520" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 09 Mar 2025 04:48:51 GMT</pubDate>
</item>
<item>
<title>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement for Transformers in Large-Scale Time Series Modeling</title>
<link>https://arxiv.org/abs/2503.06121</link>
<guid>https://arxiv.org/abs/2503.06121</guid>
<content:encoded><![CDATA[
Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7's time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1/23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at https://github.com/Alic-Li/BlackGoose_Rimer.
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 03:31:18 GMT</pubDate>
</item>
<item>
<title>NeuGrasp：应对透明和镜面物体抓取的神经表面重建方法</title>
<link>https://arxiv.org/abs/2503.03511</link>
<guid>https://arxiv.org/abs/2503.03511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NeuGrasp是一种改进的抓取检测方法，专注于透明和镜面物体。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为NeuGrasp的神经表面重建方法，专为处理透明和镜面物体抓取挑战而设计。NeuGrasp利用背景先验进行无材质特性的抓取检测，结合变换器和全局先验体积，以聚合多视图特征和空间编码，从而在狭窄和稀疏的观察条件下实现稳健的表面重建。该方法通过残差特征增强关注前景物体，并利用占用先验体积提高空间感知。 extensive的实验结果表明，NeuGrasp在抓取能力上超越了多种先进方法，同时保持了相似的重建质量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03511" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:57:37 GMT</pubDate>
</item>
<item>
<title>基于状态的参数高效微调方法在状态空间模型中的应用</title>
<link>https://arxiv.org/abs/2503.03499</link>
<guid>https://arxiv.org/abs/2503.03499</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新颖的基于状态的调优方法，提升了SSM的微调效果。</p><br /><br /><p><strong>摘要：</strong> 状态空间模型（SSMs）因其较低的计算成本成为动态替代Transformer的有效选择。然而，参数高效微调（PEFT）方法在SSMs中的应用尚未得到充分探索。特别是依赖提示的调优方法在SSMs上表现不佳。为此，我们提出了一种基于状态的方法，作为优于提示方法的替代方案。这种新方法直接调整与状态相关的特征，而不依赖外部提示。此外，我们引入了一种新颖的基于状态的PEFT方法：状态偏移调优。在每个时间步，方法直接影响当前状态，进而实现更有效的适应。通过在多种数据集上的大量实验，我们证明了该方法的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.03499" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Mar 2025 08:44:42 GMT</pubDate>
</item>
<item>
<title>LLaVE：增强多模态嵌入模型表现的动态框架</title>
<link>https://arxiv.org/abs/2503.04812</link>
<guid>https://arxiv.org/abs/2503.04812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍LLaVE框架，该框架提高了多模态嵌入模型对困难负样本的学习能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了LLaVE框架，以解决现有基于LMM的嵌入模型在区分正负样本时的困难。通过动态调整负样本的表示学习，LLaVE显著改善了模型性能，并在MMEB基准上进行评估，覆盖四个子任务和36个数据集。实验结果显示，LLaVE模型不仅在指标上超过了之前的最佳7B模型，LLaVE-2B取得了领先的SOTA性能，LLaVE-7B进一步提高了6.2个百分点。此外，尽管LLaVE模型是在图像-文本数据上训练的，但它也能够在无监督条件下有效地推广到文本-视频检索任务，展现了其在其他嵌入任务中的潜在应用能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Mar 2025 05:21:57 GMT</pubDate>
</item>
<item>
<title>解析视觉语言模型中的文本偏见现象及其影响</title>
<link>https://arxiv.org/abs/2503.02199</link>
<guid>https://arxiv.org/abs/2503.02199</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究视觉语言模型在视觉与文本不一致时的偏见现象及其影响。</p><br /><br /><p><strong>摘要：</strong> 本研究聚焦于视觉语言模型（VLMs）在处理视觉数据和不同文本输入时的模态偏好，尤其是在视觉中心任务中。通过对四个视觉任务引入文本变体并评估十种VLMs，我们发现了“对文本的盲目信任”现象：当面临不一致时，VLMs倾向于过度信任文本数据，导致在文本损坏情况下显著性能下降，这引发了安全隐患。我们分析了影响文本偏见的多个因素，包括指令提示、语言模型规模、文本相关性、标记顺序以及视觉和文本不确定性之间的相互作用。尽管某些因素（如增大语言模型的规模）对减缓文本偏见有一定效果，其他因素（如标记顺序）则可能因语言模型的位置信息而加剧这一偏见。为应对这一问题，我们探索了通过文本增强进行监督微调，并展示了其在降低文本偏见方面的有效性。此外，我们提供的理论分析表明，“对文本的盲目信任”现象可能源于训练期间纯文本与多模态数据的不平衡。这项研究强调了在VLMs的训练中需关注模态交互，以提升其应对多模态数据不一致的鲁棒性和可靠性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.02199" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 21:21:07 GMT</pubDate>
</item>
<item>
<title>SafeArena：评估大型语言模型代理的网络滥用风险</title>
<link>https://arxiv.org/abs/2503.04957</link>
<guid>https://arxiv.org/abs/2503.04957</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SafeArena是首个专注于网络代理滥用风险的基准，评估其顺应恶意请求的情况。</p><br /><br /><p><strong>摘要：</strong> 随着基于大型语言模型（LLM）的代理在网络任务中的表现日益成熟，随之而来的滥用风险也显著增加。本文提出了SafeArena，这是首个专注于评估网络代理故意滥用风险的基准，包括250个安全任务和250个有害任务，覆盖四个网站。这些有害任务被分为五个类别：误信息、非法活动、骚扰、网络犯罪和社会偏见，旨在评估代理的真实滥用情况。我们采用Agent Risk Assessment框架，系统地评估领先的LLM代理（如GPT-4o和Qwen-2）在面对有害请求时的反应，结果显示，GPT-4o和Qwen-2分别完成了34.7%和27.3%的恶意请求，表明这些代理对恶意请求的顺应性令人惊讶。研究表明，迫切需要为网络代理制定安全对齐流程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04957" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 15:43:14 GMT</pubDate>
</item>
<item>
<title>引入S2S-Arena：评估语音模型的指令跟随能力</title>
<link>https://arxiv.org/abs/2503.05085</link>
<guid>https://arxiv.org/abs/2503.05085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S2S-Arena评估语音模型在指令跟随和副语言信息处理方面的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型的快速发展，语音模型的进展同样引起了广泛关注。特别是当前在支持语音输入和输出的语音到语音（speech2speech, S2S）协议方面的最新成果，然而现有基准使用的自动文本评估方法缺乏对副语言信息的考量。为了解决这一问题，我们引入了S2S-Arena，一个新颖的以竞技场为风格的S2S基准，评估真实世界任务中语音输入和语音输出的指令跟随能力。通过文中设计的154个样本，融合了文本到语音（TTS）和实时录音，涵盖四个领域和21个任务，手动评估了现存流行的语音模型。实验结果显示，GPT-4o表现优异，同样，经过文本-语音对齐后的级联ASR、LLM和TTS的组合模型在S2S协议中也优于联合训练模型。此外，考虑到副语言信息，语音模型的知识传递主要依赖于LLM的基础，而这一性能在多语言支持上受到语音模块的限制。尽管优秀的语音模型已经能理解输入中的副语言信息，但生成合适的带有副语言信息的音频仍然存在挑战。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.05085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 21:07:00 GMT</pubDate>
</item>
<item>
<title>LONGCODEU基准测试：评估长代码理解能力的研究</title>
<link>https://arxiv.org/abs/2503.04359</link>
<guid>https://arxiv.org/abs/2503.04359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出LONGCODEU基准测试，评估长代码理解能力，为软件工程提供重要见解。</p><br /><br /><p><strong>摘要：</strong> 当前的长上下文语言模型在真实软件工程应用中具有巨大潜力，但缺乏严谨的长代码理解评估框架限制了其发展。为此，我们提出LONGCODEU基准测试，从代码单元感知、内部理解、相互关系理解和文档理解四个方面（共八项任务）来评估长上下文语言模型在长代码理解方面的能力。通过对9个流行的长上下文语言模型（6个通用模型和3个代码模型）进行评估，实验结果揭示了当前模型在长代码理解方面的主要局限性，尤其是在代码长度超过32K时，性能显著下降，远低于其声称的128K-1M上下文窗口。在四个评估方面中，相互关系理解对模型来说是最具挑战性的。本研究为优化长上下文语言模型和推动软件工程的进步提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2503.04359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 06 Mar 2025 07:02:31 GMT</pubDate>
</item>
<item>
<title>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</title>
<link>https://arxiv.org/abs/2503.01840</link>
<guid>https://arxiv.org/abs/2503.01840</guid>
<content:encoded><![CDATA[
The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. The code is available at https://github.com/SafeAILab/EAGLE.
]]></content:encoded>
<pubDate>Mon, 03 Mar 2025 13:59:04 GMT</pubDate>
</item>
</channel>
</rss>