<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>Soundwave: 一种高效的语音到文本大语言模型训练方法</title>
<link>https://arxiv.org/abs/2502.12900</link>
<guid>https://arxiv.org/abs/2502.12900</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Soundwave是一种提高语音到文本模型训练效率的新方法，表现优于前作。</p><br><br><p><strong>摘要：</strong> 现有的端到端语音大语言模型通常依赖于大规模的标注数据进行训练，但对于数据高效训练的讨论较少。本文聚焦于语音与文本之间的两个基本问题：表示空间差距和序列长度不一致。我们提出了Soundwave，结合高效的训练策略和新颖的架构，成功解决了这些问题。实验结果表明，Soundwave在语音翻译和AIR-Bench语音任务中，使用仅为训练数据的五十分之一的情况下，超越了先进的Qwen2-Audio模型。此外，进一步分析显示，Soundwave在对话中仍保持其智能。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.12900 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 19 Feb 2025 00:22:36 GMT</pubDate>
<pubDate>Wed, 19 Feb 2025 00:22:36 GMT</pubDate>
</item>
<item>
<title>Magma：多模态AI代理任务的基础模型</title>
<link>https://arxiv.org/abs/2502.13130</link>
<guid>https://arxiv.org/abs/2502.13130</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>Magma是一个新型多模态基础模型，具备数字与物理世界的智能代理能力。</p><br><br><p><strong>摘要：</strong> Magma是一个前沿的基础模型，旨在执行多模态AI代理任务，涵盖数字和物理世界。相较于传统的视觉-语言(VL)模型，Magma不仅具备VL理解能力（语言智能），同时具备在视觉-空间世界中进行规划和行动的能力（时空智能）。为了实现智能代理功能，Magma在大量异构数据集上进行了预训练，这些数据集包括图像、视频和机器人数据。具体而言，图像中的可操作视觉对象通过Set-of-Mark (SoM)进行标记，而视频中的对象运动通过Trace-of-Mark (ToM)进行标记，从而为行动提供基础。实验结果显示，SoM和ToM的结合显著提升了Magma的时空智能，使其在UI导航和机器人操作等任务上打破了以往的记录，优于专门为这些任务设计的模型。同时，在图像和视频相关的多模态任务中，Magma也与其他训练在更大数据集上的大型多模态模型相比较，表现出色。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.13130 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:51:36 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 23:51:36 GMT</pubDate>
</item>
<item>
<title>测试时间缩放在大型语言模型中的应用与效果研究</title>
<link>https://arxiv.org/abs/2502.12215</link>
<guid>https://arxiv.org/abs/2502.12215</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>本研究探讨了大型语言模型的测试时间缩放及其对推理能力的影响。</p><br><br><p><strong>摘要：</strong> 本文研究了大型语言模型（LLMs）在推理过程中测试时间缩放的能力，尤其是OpenAI的o1系列模型。尽管后续模型如QwQ、Deepseek-R1（R1）和LIMO也声称具备类似能力，但其实际效果仍需进一步探讨。研究发现，更长的chain of thought（CoT）并不总能提高准确性，反而对于同一问题，正确答案的长度往往短于错误答案。深入分析后发现，这一现象与模型的自我修正能力密切相关，长CoT中包含较多的自我修正，常导致性能的降低。本文进一步比较了QwQ、R1和LIMO的顺序与并行缩放策略，结果显示并行缩放在覆盖性和可扩展性上更优。基于此发现，提出了一种Shortest Majority Vote的方法，将并行缩放策略与CoT长度特征相结合，显著提高了模型的测试时间可扩展性，较传统多数投票方法表现更佳。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.12215 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:37:46 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 23:37:46 GMT</pubDate>
</item>
<item>
<title>SafeRoute：高效的安全守卫模型自适应路由方案</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>SafeRoute通过自适应路由提升了安全守卫模型的效率与准确性。</p><br><br><p><strong>摘要：</strong> 部署大型语言模型需要高效的安全守卫模型来检测和阻止有害用户提示。虽然大型安全守卫模型性能强大，但计算成本高昂。为此，研究提出了SafeRoute，一种双重路由器，旨在区分困难示例和简单示例。该方法通过仅在路由器认定为困难的输入上应用大型安全守卫模型，提高了模型选择的效率，同时保持了较高的准确性。实验表明，相比单独使用大型模型，自适应选择显著提升了计算成本与安全性能之间的平衡，并在多个基准数据集上超越了相关基线。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.12464 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 23:23:34 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 23:23:34 GMT</pubDate>
</item>
<item>
<title>MUDD连接：提升Transformer跨层信息流动的有效方法</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>MUDD连接改善了Transformer的残差连接，显著提升模型性能。</p><br><br><p><strong>摘要：</strong> 本文提出了一种简单有效的MUDD连接方法，以解决残差连接的局限性，并增强Transformer中的跨层信息流动。与现有的静态共享连接权重方法不同，MUDD根据每个序列位置的隐藏状态动态生成连接权重，并针对Transformer块的每个解耦输入流（查询、键、值或残差）。MUDD连接能够无缝地集成到任何Transformer架构中，形成MUDDFormer。大量实验表明，MUDDFormer在语言建模中显著超越了各种模型架构和规模的Transformers，表现出相当于经过1.8X-2.4X计算训练的Transformers的性能。值得注意的是，MUDDPythia-2.8B在预训练的每个词困惑度和下游任务中与Pythia-6.9B相匹配，并在五次少样本设置中甚至与Pythia-12B相媲美，同时仅增加了0.23%的参数和0.4%的计算量。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.12170 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:59:16 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:59:16 GMT</pubDate>
</item>
<item>
<title>XLM-SWCM：低资源语言文本生成的新框架</title>
<link>https://arxiv.org/abs/2502.10852</link>
<guid>https://arxiv.org/abs/2502.10852</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出XLM-SWCM框架以提升低资源语言的文本生成能力。</p><br><br><p><strong>摘要：</strong> 本文提出了一种新的框架XLM-SWCM，用于在极低资源语言中适应多语言编码器以进行文本生成。尽管现有的多语言模型如XLM-R在自然语言处理上获得了进展，但在极低资源语言的表现仍然较差。此外，现代大型语言模型支持的语言种类远少于XLM-R，导致许多语言缺乏文本生成模型。通过重用编码器和解码器之间的权重，XLM-SWCM框架充分利用了编码器学习到的语义空间，从而实现了在低资源语言中的高效学习与有效泛化。我们将此框架应用于四种中国少数民族语言，并在多项下游任务中展示了其优越的性能，甚至超越了更大模型的表现。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.10852 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:46:16 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:46:16 GMT</pubDate>
</item>
<item>
<title>基于几何性质的连续扩散模型用于语言建模</title>
<link>https://arxiv.org/abs/2502.11564</link>
<guid>https://arxiv.org/abs/2502.11564</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新的连续扩散模型，针对语言建模中的离散数据。</p><br><br><p><strong>摘要：</strong> 本研究提出了一种新的连续扩散模型，旨在解决传统离散扩散模型在语言建模中的局限性。现有的离散扩散模型在信号转换过程中易丢失信息，而现有的连续模型在离散数据上表现不佳，限制了扩散模型的发展。我们通过建立离散扩散与连续流动之间的联系，引入一种简化设计的扩散过程，能够更好地利用底层类别分布的几何结构。此外，基于辐射对称性，我们提出了一种无仿真训练框架，以应对流形的高维性。通过对语言建模基准和其他领域的全面实验，显示我们的方法在性能上优于现有的离散扩散模型，并接近自回归模型的表现。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.11564 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:43:02 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:43:02 GMT</pubDate>
</item>
<item>
<title>HealthGPT：融合医疗视觉的强大语言模型</title>
<link>https://arxiv.org/abs/2502.09838</link>
<guid>https://arxiv.org/abs/2502.09838</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>HealthGPT 是一款强大的医疗视觉语言模型，具有优异的性能与可扩展性。</p><br><br><p><strong>摘要：</strong> HealthGPT 是一款强大的医疗大型视觉语言模型（Med-LVLM），能够在统一的自回归范式内整合医疗视觉理解与生成能力。其核心是逐步适应异构理解与生成知识到预训练的大语言模型（LLMs），采用新颖的异构低秩适应（H-LoRA）技术，结合量身定制的分层视觉感知方法和三阶段学习策略。为了有效训练 HealthGPT，我们开发了一个名为 VL-Health 的综合医学领域特定理解与生成数据集。实验结果显示，HealthGPT 在医疗视觉统一任务中表现出色且具有良好的可扩展性。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.09838 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:35:23 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:35:23 GMT</pubDate>
</item>
<item>
<title>mmMamba：一种线性复杂度的多模态状态空间模型框架</title>
<link>https://arxiv.org/abs/2502.13145</link>
<guid>https://arxiv.org/abs/2502.13145</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>mmMamba框架通过知识蒸馏实现线性复杂度的多模态模型，提升效率。</p><br><br><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）展现了卓越的性能，但在部署时面临计算复杂度高、缓存需求增长等挑战。为此，我们提出了mmMamba框架，通过对现有MLLM的逐步知识蒸馏，开发线性复杂度的多模态状态空间模型。该方法允许直接将经过训练的解码器-仅模型转化为线性复杂度架构，无需预训练的基于RNN的LLM或视觉编码器。通过提出播种策略和三阶段蒸馏流程，我们有效地将知识从Transformer转移到Mamba，同时保持多模态能力。经过Transformer的解码器-仅模型HoVLE蒸馏的mmMamba-linear在性能方面与现有的线性和二次复杂度视觉语言模型相竞争，而mmMamba-hybrid的性能进一步显著提升，接近HoVLE的能力。在103K tokens时，mmMamba-linear实现了20.6倍的加速和75.8%的GPU内存减少，而mmMamba-hybrid则实现了13.5倍的加速及60.2%的内存节省。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.13145 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:08:27 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:08:27 GMT</pubDate>
</item>
<item>
<title>FLAG-Trader：一种融合语言处理与强化学习的金融交易模型</title>
<link>https://arxiv.org/abs/2502.11433</link>
<guid>https://arxiv.org/abs/2502.11433</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出FLAG-Trader，通过强化学习优化金融交易决策，提高多步骤任务表现。</p><br><br><p><strong>摘要：</strong> 本文提出了FLAG-Trader，一种结合语言处理与强化学习的金融交易模型。大型语言模型（LLMs）在处理多模态金融数据时，展现出卓越的推理能力，但在需复杂决策的多步骤互动金融市场（如交易）中，表现尚不理想。FLAG-Trader通过将部分微调的LLM作为策略网络，利用预训练知识与金融领域的参数高效微调相结合，提升了决策过程的表现。借助政策梯度优化，在交易奖励的驱动下，FLAG-Trader不仅优化了交易性能，还显著改善了其他金融领域任务的表现。我们提供了大量实证数据来验证这些提升效果。</p><br><br><p><em>使用 gpt-4o-mini 生成 </em></p><a href=https://arxiv.org/abs/2502.11433 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 22:06:19 GMT</pubDate>
<pubDate>Tue, 18 Feb 2025 22:06:19 GMT</pubDate>
</item>

<item>
<title>Decomposed Reward Models: 提取人类偏好的新方法</title>
<link>https://arxiv.org/abs/2502.13131</link>
<guid>https://arxiv.org/abs/2502.13131</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRMs通过二元比较提取人类偏好，为个性化AI提供了新视角。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的奖赏模型——分解奖赏模型（DRMs），旨在有效提取多样化的人类偏好。传统奖励模型在捕捉偏好的复杂性方面存在局限性。DRMs不需要细粒度的注释，而是通过二元比较来分析人类偏好，并将其表示为向量。采用主成分分析（PCA）对偏好的数据进行分析，构造了偏好与拒绝响应之间的嵌入差异数据集。通过识别正交基向量，DRMs能够捕捉偏好的不同维度，如有帮助性、安全性和幽默感等。这些分解的奖励可以灵活组合，以适应不同用户的需求，从而提供一个可解释且可扩展的替代方案。我们的结果表明，DRMs不仅有效提取偏好维度，还能在无额外训练的情况下适应新用户，展示了其在个性化和可解释性大型语言模型对齐中的强大能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13131" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:59:45 GMT</pubDate>
</item>
<item>
<title>HEADINFER: 一种高效的长序列推理策略</title>
<link>https://arxiv.org/abs/2502.12574</link>
<guid>https://arxiv.org/abs/2502.12574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HEADINFER通过头部级别的KV缓存卸载，显著降低推理内存占用。</p><br /><br /><p><strong>摘要：</strong> 本文提出了HEADINFER，一种优化长序列生成模型的推理内存使用的新策略。通过将关键值缓存（KV缓存）卸载到CPU RAM，HEADINFER避免了在GPU上完全存储Transformer层的KV缓存。该方法采用细粒度的头部级别卸载策略，仅在GPU上维护选择性的注意力头的KV缓存，同时动态计算注意力输出。通过Roofline分析，我们展示了HEADINFER在保持计算效率的同时，大幅降低了内存占用。在对Llama-3-8B模型进行评估时，HEADINFER能够将KV缓存的GPU内存占用从128 GB减少到1 GB，总体GPU内存使用从207 GB减少到17 GB，相比BF16基线推理实现了92%的减少。值得一提的是，HEADINFER使得在单个24GB显存的消费级GPU（如NVIDIA RTX 4090）上实现4百万令牌的推理成为可能，而无需采用近似方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:57:00 GMT</pubDate>
</item>
<item>
<title>Phantom: Subject-consistent video generation via cross-modal alignment</title>
<link>https://arxiv.org/abs/2502.11079</link>
<guid>https://arxiv.org/abs/2502.11079</guid>
<content:encoded><![CDATA[
The continuous development of foundational models for video generation is evolving into various applications, with subject-consistent video generation still in the exploratory stage. We refer to this as Subject-to-Video, which extracts subject elements from reference images and generates subject-consistent video through textual instructions. We believe that the essence of subject-to-video lies in balancing the dual-modal prompts of text and image, thereby deeply and simultaneously aligning both text and visual content. To this end, we propose Phantom, a unified video generation framework for both single and multi-subject references. Building on existing text-to-video and image-to-video architectures, we redesign the joint text-image injection model and drive it to learn cross-modal alignment via text-image-video triplet data. In particular, we emphasize subject consistency in human generation, covering existing ID-preserving video generation while offering enhanced advantages. The project homepage is here https://phantom-video.github.io/Phantom/.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:56:39 GMT</pubDate>
</item>
<item>
<title>基于人群比较评估的LLM自动评价方法的改进</title>
<link>https://arxiv.org/abs/2502.12501</link>
<guid>https://arxiv.org/abs/2502.12501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人群比较的评估方法以提升LLM判断的可靠性。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-as-a-Judge逐渐成为自动评估的主流方法，其基于链式推理（CoT）所产生的判断却存在可靠性不足的问题，主要由于CoT推理无法捕捉到深入且全面的细节而导致的输出不完整。现有方法多依赖于多数投票或标准扩展，这未能有效解决CoT的局限性。为此，我们提出了一种人群比较评估方法，增加了额外的人群反馈来与候选回应进行比较，从而揭示候选回应中的更深层次和更全面的细节。这一过程有效地引导LLM-as-a-Judge提供更详尽的CoT判断。通过大量实验，我们的方法在五个基准测试中平均提高了6.7%的评估准确性，并产生了更高质量的CoT，进一步推动了判断蒸馏与在监督微调过程中的更高效表现。我们的分析表明，所生成的CoT在全面性和质量上优于现有方法，并且随着推理规模的扩大，评估准确性不断改善。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:55:26 GMT</pubDate>
</item>
<item>
<title>RealSyn：用于视觉-语言表示学习的真实与合成文本数据集</title>
<link>https://arxiv.org/abs/2502.12513</link>
<guid>https://arxiv.org/abs/2502.12513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RealSyn数据集通过真实与合成文本增强视觉-语言表示学习。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了RealSyn数据集的构建，该数据集结合了高质量的真实文本和合成文本，以提升视觉-语言表示学习的性能。通过建立一条真实世界数据提取管道，提取高质量图像和文本，并设计层次检索方法有效关联图像与多个语义相关的文本，本文充分利用了未配对的数据。此外，为了增强细粒度视觉信息，提出了图像语义增强生成模块用于合成文本的生产，同时采用了语义均衡抽样策略以提高数据集多样性，从而更好地学习长尾概念。RealSyn可在15M、30M和100M三个规模上使用，实验表明，基于RealSyn预训练的模型在多个下游任务上达到了最新的性能，极大推动了视觉-语言表示学习的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:52:22 GMT</pubDate>
</item>
<item>
<title>利用自然语言定义物体方向以增强机器人操作能力</title>
<link>https://arxiv.org/abs/2502.13143</link>
<guid>https://arxiv.org/abs/2502.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过自然语言定义物体方向，以提升机器人的操作能力。</p><br /><br /><p><strong>摘要：</strong> 空间智能是具身AI的关键组成部分，使机器人能够理解并与环境互动。尽管现有视觉语言模型在理解物体位置和关系方面已取得进展，但仍缺乏对物体方向的精确理解，尤其是在细致操作任务中的需求。为解决这一限制，本文提出通过自然语言来定义语义方向，形成了一种更灵活的表示方式。我们构建了OrienText300K数据集，包含带有语义方向的3D模型，旨在将几何理解与功能语义连接。通过整合语义方向进VLM系统，我们的研究使机器人能够在操作中同时考虑位置和方向的约束，实验表明，模型在仿真和真实场景中显著提升了操作准确率，如在Open6DOR上达到48.7%的准确率，SIMPLER上达到74.9%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 21:51:33 GMT</pubDate>
</item>
<item>
<title>EQ-VAE：提升潜在生成模型的等变性</title>
<link>https://arxiv.org/abs/2502.09509</link>
<guid>https://arxiv.org/abs/2502.09509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出EQ-VAE，提升潜在生成模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 潜在生成模型已成为高质量图像合成的领先方法，然而现有自编码器在应对尺度和旋转等语义保留变换时缺乏等变性，导致潜在空间复杂性增加，从而影响生成性能。为此，本文提出了EQ-VAE，这是一种简单的正则化方法，旨在在潜在空间内强制实现等变性，降低其复杂性而不损害重构质量。通过对预训练自编码器进行EQ-VAE微调，本文显著提升了包括DiT、SiT、REPA和MaskGIT在内的多种最先进生成模型的性能，其中DiT-XL/2在五个SD-VAE微调周期内实现了7倍的加速。EQ-VAE适用于连续和离散自编码器，从而为多种潜在生成模型提供了灵活的增强工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 14:56:45 GMT</pubDate>
</item>
<item>
<title>多模态检索增强生成系统综述</title>
<link>https://arxiv.org/abs/2502.08826</link>
<guid>https://arxiv.org/abs/2502.08826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本综述分析了多模态检索增强生成系统的挑战和进展。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型因依赖静态训练数据而面临幻觉和过时知识的问题。检索增强生成（RAG）通过整合外部动态信息来缓解这些问题，进而提高输出的真实性和时效性。近期的多模态学习进展促成了多模态RAG的发展，将文本、图像、音频和视频等多种模态结合以增强生成效果。然而，跨模态对齐和推理为多模态RAG带来了独特挑战。本文综述了多模态RAG系统，涵盖数据集、指标、基准、评估、方法论及创新等方面，详细审视训练策略、鲁棒性增强及损失函数，并探讨多样化的多模态RAG场景及未来研究方向。该综述为构建更强大、可靠的人工智能系统奠定了基础，旨在有效利用多模态动态外部知识库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>IHEval：评估语言模型指令层级遵循能力的新基准</title>
<link>https://arxiv.org/abs/2502.08745</link>
<guid>https://arxiv.org/abs/2502.08745</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IHEval基准评估语言模型在指令层级遵循中的表现与挑战。</p><br /><br /><p><strong>摘要：</strong> 指令层级在语言模型（LMs）中至关重要，确保系统消息、用户消息、对话历史和工具输出之间的优先顺序。然而，这一领域的研究相对较少，缺乏全面的评估基准。为此，我们推出了IHEval，一个新基准，包含3,538个示例，涵盖九项任务，特别是对优先级不同的指令的处理。我们的评估显示，流行的语言模型在识别指令优先级方面表现不佳，尤其在面对相互冲突的指令时，性能显著下降。最具竞争力的开源模型在解决此类冲突中仅获得48%的准确率。这些结果强调了未来在语言模型开发中需要针对性优化的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08745" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:21:05 GMT</pubDate>
</item>
<item>
<title>高效影响值估计的神经网络方法</title>
<link>https://arxiv.org/abs/2502.09969</link>
<guid>https://arxiv.org/abs/2502.09969</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种名为NN-CIFT的小型神经网络方法，以降低影响值估计的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用小型神经网络（称为InfluenceNetwork）来估计模型培训中的影响值，显著降低了计算成本，达到了99%的节约。传统的影响函数计算方法由于高昂的计算需求和内存消耗，在处理大型模型和数据集时效果不理想；而我们的方法能够以仅占全语言模型0.0027%的小型模型，进行有效的影响值估计。我们将此算法（NN-CIFT）应用于针对指令细化的子集选择任务中，结果显示在速度显著提升的情况下，无需牺牲性能，与四个先进的影响函数方法对比，均表现出良好的效果。此外，我们还对NN-CIFT进行了深入的超参数分析，证明其有效性和通用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09969" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 13:04:04 GMT</pubDate>
</item>
<item>
<title>合成多模态网络任务数据集和探索者代理的研究</title>
<link>https://arxiv.org/abs/2502.11357</link>
<guid>https://arxiv.org/abs/2502.11357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文开发了一个多模态网络任务数据集，提升了代理的性能。</p><br /><br /><p><strong>摘要：</strong> 近期大型多模态模型（LMM）的成功应用已展现出自主完成复杂网络任务的潜力。尽管开源LMM代理在离线评估基准上取得了显著进展，但在更真实的在线环境中仍远未达到人类水平。本文提出了一种可扩展的方法，合成了迄今为止最大的多样化网络任务轨迹数据集，包含超过94K的成功任务轨迹，跨越49K个独特URL、720K张屏幕截图和33M个网页元素。本研究还介绍了“Explorer”多模态网络代理，并在多个基准测试中表现出色，验证了数据扩展对提高网络代理能力的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:57:43 GMT</pubDate>
</item>
<item>
<title>ILIAS：用于实例级图像检索的新测试数据集</title>
<link>https://arxiv.org/abs/2502.11748</link>
<guid>https://arxiv.org/abs/2502.11748</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ILIAS数据集旨在评估图像检索模型对特定对象的识别能力。</p><br /><br /><p><strong>摘要：</strong> ILIAS是一个新推出的测试数据集，专为评估当前及未来的基础模型与检索技术在实例级图像检索能力而设计。与现有数据集相比，ILIAS的优势在于其大规模、多样化的领域，以及准确的真实标注，且性能尚未饱和。该数据集包含1,000个对象实例的查询和正面图像，这些图像经过手动收集，旨在捕捉具有挑战性的条件和多样化的领域。检索任务涉及1亿张来自YFCC100M的数据干扰图像。在避免假阴性并减少额外标注工作量的前提下，ILIAS仅包含确认在2014年后出现的查询对象。通过广泛的基准测试，结果显示：针对特定领域的模型表现优异，但在ILIAS上的效果有限；通过多领域类监督训练线性适配层可以提高性能；局部描述符在重排检索中的作用依然重要，特别是在存在严重背景干扰的情况下；此外，视觉-语言基础模型在文本到图像的性能与图像到图像的性能接近。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11748" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 11:42:58 GMT</pubDate>
</item>
<item>
<title>CALM：结合对话与智能能力的统一语言模型</title>
<link>https://arxiv.org/abs/2502.08820</link>
<guid>https://arxiv.org/abs/2502.08820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CALM统一模型提升了对话系统与任务导向的能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过API调用功能，推动了语言智能体（LA）和传统任务导向对话（TOD）范式的变革。然而，现有方法在支持多轮对话时面临重大困境。任务导向系统通常在有限的目标API上训练，需要新数据来维持与新服务的接口质量，而语言智能体在多轮对话中则难以维持用户意图。为了解决这一问题，本文提出CALM（Conversational Agentic Language Model），一种结合对话和智能能力的统一方法。我们创建了CALM-IT，一个精心构建的多任务数据集，以交错多轮ReAct推理与复杂的API使用。使用CALM-IT训练的CALM系列模型（CALM 8B, CALM 70B, CALM 405B），在三个流行基准（MultiWOZ 2.4, BFCL V3, API-Bank）上均超越了包括GPT-4o在内的顶尖领域专用模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 08:59:34 GMT</pubDate>
</item>
<item>
<title>模型编辑在问答系统中的评估与实践研究</title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明当前模型编辑方法在实际应用中的有效性不足。</p><br /><br /><p><strong>摘要：</strong> 尽管现有模型编辑方法在人工评估中表现良好，但其实用性仍待证实。本研究通过建立QAEdit基准和标准化评估框架，探讨模型编辑在问答（QA）中的有效性。实验结果显示，现有编辑方法在真实应用中的表现远低于预期（38.5%对比~96%），分析指出，主要原因在于以往研究中的评估实践不当，特别是教师强迫使用不当，导致错误无法传播。此外，通过模拟真实场景的连续编辑，发现现有方法在仅进行1000次编辑的情况下效果显著下降。我们的分析对现有模型编辑方法的实际应用及其评估实践进行了重新审视，并提出了改进建议，以推动可靠且实用的模型编辑研究。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:33:17 GMT</pubDate>
</item>
<item>
<title>MIKASA：增强记忆能力的强化学习基准</title>
<link>https://arxiv.org/abs/2502.10550</link>
<guid>https://arxiv.org/abs/2502.10550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MIKASA基准为记忆强化学习提供了统一的评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MIKASA（内存密集技能评估套件），旨在为强化学习中的记忆能力提供全面的评估基准。目前，尽管许多强化学习算法采用了记忆机制，但缺乏统一标准来评估其在各种场景中的表现。在台面机器人操控领域，记忆是解决部分可观测任务和确保系统稳定性的关键因素。为此，MIKASA包括三个主要贡献：首先，提出了一种针对记忆密集型强化学习任务的全面分类框架；其次，收集了MIKASA-Base——一个统一的基准，支持对增强记忆智能体进行系统评估；最后，开发了MIKASA-Robo，这是一个包含32个精心设计的内存密集型任务的新基准，评估台面机器人操控中的记忆能力。这些贡献为推动记忆强化学习研究提供了统一框架，助力更可靠的现实应用系统开发。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 07:16:07 GMT</pubDate>
</item>
<item>
<title>Dyve：基于动态过程验证的语言模型错误检测增强工具</title>
<link>https://arxiv.org/abs/2502.11157</link>
<guid>https://arxiv.org/abs/2502.11157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Dyve通过动态验证提升语言模型的错误检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Dyve，一个动态过程验证工具，通过结合快速与慢速思维（基于Kahneman的系统理论）来增强大型语言模型中的推理错误检测。Dyve根据任务性质，智能地应用即时的token级确认（系统1）处理简单步骤，而对复杂任务则采用全面分析（系统2）。此外，Dyve引入了一种新颖的逐步共识过滤过程监督技术，利用蒙特卡罗估计与基于语言模型的评估相结合，从嘈杂数据中提取高质量的监督信号。在ProcessBench和MATH数据集上的实验结果表明，Dyve在过程验证方面显著优于现有工具，并在最佳选择设置中提升了性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:33:31 GMT</pubDate>
</item>
<item>
<title>NSA：高效的长上下文稀疏注意力机制</title>
<link>https://arxiv.org/abs/2502.11089</link>
<guid>https://arxiv.org/abs/2502.11089</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NSA机制通过高效稀疏注意力实现长上下文建模，提高计算效率。</p><br /><br /><p><strong>摘要：</strong> 长上下文建模对于下一代语言模型至关重要，但标准注意力机制的高计算成本带来了显著挑战。我们提出的NSA机制是一种可原生训练的稀疏注意力解决方案，结合算法创新和硬件优化，实现高效的长上下文建模。NSA采用动态分层稀疏策略，结合粗粒度的令牌压缩与细粒度的令牌选择，既保持全局上下文感知又确保局部精度。通过算术强度平衡的算法设计与现代硬件的实现优化，我们显著提升了计算速度，并实现了端到端的训练，减少了预训练计算量而不影响模型性能。实验表明，使用NSA预训练的模型在各项基准测试、长上下文任务和基于指令的推理中表现与全注意力模型持平或更优，同时在64k长度序列上，NSA在解码、前向传播和反向传播中都显著快于全注意力模型，验证了其在模型生命周期中的高效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11089" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 06:07:36 GMT</pubDate>
</item>
<item>
<title>改进Adam优化器以缓解大语言模型中嵌入的各向异性问题</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Coupled Adam优化器，显著改善大语言模型的嵌入质量。</p><br /><br /><p><strong>摘要：</strong> 尽管大语言模型具有显著的能力，但它们学习的词表示往往表现出各向异性这一不理想且尚不清楚的问题。本文认为，Adam优化器中的二阶矩是导致嵌入各向异性的一个原因，并提出了一种名为Coupled Adam的改进优化器，以减轻这一问题。实验结果表明，Coupled Adam能够显著提高嵌入质量，同时在大规模数据集上也能改善后续和前期性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 05:28:54 GMT</pubDate>
</item>
<item>
<title>提升自动化事实核查工具的有效性</title>
<link>https://arxiv.org/abs/2502.09083</link>
<guid>https://arxiv.org/abs/2502.09083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究自动化事实核查的解释需求，改善信息核查流程。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型和生成性AI在网络媒体中的广泛应用，自动化事实核查的需求日益增强，以帮助事实核查员应对日益增加的信息失真。然而，自动化核查系统的解释如何与事实核查员的决策与推理过程相结合仍然不明确。通过对事实核查专业人士进行半结构化访谈，本研究阐明了事实核查员评估证据、做出决策及解释其过程的方式，探讨了事实核查员在实践中如何使用自动化工具，并识别了他们对这些工具的解释需求。研究结果显示，当前的解释需求未得到满足，并识别了可复制的事实核查解释的重要标准，包括模型的推理路径、具体证据的引用、以及强调不确定性和信息缺口的要求。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:37:21 GMT</pubDate>
</item>
<item>
<title>MagicArticulate：自动将静态3D模型转化为可动画资产的有效框架</title>
<link>https://arxiv.org/abs/2502.12135</link>
<guid>https://arxiv.org/abs/2502.12135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MagicArticulate框架可以自动将静态3D模型转换为支持动画的版本。</p><br /><br /><p><strong>摘要：</strong> 随着3D内容创作的快速发展，对将静态3D模型自动转化为支持真实动画的关节化版本的需求日益增加。传统方法依赖手动注释，效率低下且耗时。为此，我们提出了MagicArticulate框架，能够有效自动转化静态3D模型为可动画资产。我们的主要贡献包括：第一，建立Articulation-XL，这是一项包含超过33,000个高质量关节注释的3D模型的大型基准数据集。第二，提出了一种新颖的骨架生成方法，将任务表述为序列建模问题，利用自回归变换器自然处理骨骼和关节的变化及其依赖关系。第三，使用功能扩散过程预测皮肤加权，结合顶点与关节之间的体积测地距离先验。实验结果表明，MagicArticulate在不同对象类别上显著优于现有方法，能够实现高质量的关节化，支持真实动画效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:34:15 GMT</pubDate>
</item>
<item>
<title>ThinkDiff：增强图文扩散模型的多模态推理能力</title>
<link>https://arxiv.org/abs/2502.10458</link>
<guid>https://arxiv.org/abs/2502.10458</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ThinkDiff通过多模态对齐提升图像生成模型的理解与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的对齐范式ThinkDiff，旨在通过整合视觉语言模型(VLMs)的优势，赋予文本到图像的扩散模型多模态的上下文理解和推理能力。当前的多模态扩散微调方法大多聚焦于像素级重建，而忽略了上下文推理，且受限于推理数据集的复杂性和可用性。ThinkDiff通过将视觉语言训练作为代理任务，简化了与编码-解码大型语言模型(LLM)解码器的对齐过程，有效提升了扩散模型的理解、推理和构成能力。实验证明，ThinkDiff在多模态上下文推理生成的挑战性CoBSAT基准上，准确率从19.2%提升至46.3%，仅需在4个A100 GPU上训练5小时。此外，ThinkDiff在将多张图像和文本组合成逻辑一致的图像方面表现出色。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10458" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:33:41 GMT</pubDate>
</item>
<item>
<title>深度神经网络模型中的直觉物理理解研究</title>
<link>https://arxiv.org/abs/2502.11831</link>
<guid>https://arxiv.org/abs/2502.11831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明深度神经网络能通过视频预测学习直觉物理知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了普遍深度神经网络模型在自然视频中预测缺失区域的过程中，如何逐步获得直觉物理理解。基于期望违背框架的实验表明，训练在学习表示空间中的视频预测模型能展示对象延续性和形状一致性等直觉物理特性。而在像素空间中的视频预测和通过文本推理的多模态大型语言模型，表现更接近于随机概率。研究比较不同结构显示，联合学习抽象表示空间并预测感官输入的缺失部分，类似于预测编码，足以培养对直觉物理的理解。这一发现挑战了固有知识的概念，即理解世界所需的核心知识并不一定需要内置到模型中，即便是训练一周的独特视频模型也能超越随机表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:20:25 GMT</pubDate>
</item>
<item>
<title>量子属性预测中的预训练质量优于量</title>
<link>https://arxiv.org/abs/2502.11085</link>
<guid>https://arxiv.org/abs/2502.11085</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究表明，优质数据集在量子属性预测中优于大规模数据集。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了量子属性预测中近期的范式，认为进展与数据集规模和计算资源的增长相关。研究表明，在精心选择的任务相关数据集上进行预训练，能够匹敌甚至超越大规模预训练，而计算成本仅为1/24。同时，引入了一种新指标——化学相似性指数（CSI），用于量化上游预训练数据集与下游任务的对齐程度。通过选择最相关的数据集，最低化CSI距离，结果表明，基于较小、聚焦数据集的预训练模型在性能上始终优于那些基于大量混合数据集（如JMP）的模型。这一结果表明，数据增加并不总是提升性能，反而可能因低相关性数据的加入而恶化。这一发现突显出在量子属性预测中，预训练的质量常常优于数量。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11085" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 04:16:28 GMT</pubDate>
</item>
<item>
<title>PhysReason：评估大语言模型物理推理能力的新基准</title>
<link>https://arxiv.org/abs/2502.12054</link>
<guid>https://arxiv.org/abs/2502.12054</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhysReason是一个评估大语言模型物理推理能力的新基准，涵盖1200个问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了PhysReason，一个包含1200道问题的新基准，旨在评估大语言模型在物理推理方面的能力。该基准问题分为知识型（25%）和推理型（75%）两类，并依据难度分为简单、中等和困难三级，其中困难问题的平均解题步骤达到15.6步。我们还提出了物理解题自动评分框架，进行高效的答案级和步骤级评估。尽管一些顶尖模型如Deepseek-R1和Gemini-2.0-Flash-Thinking在答案级评估中得分不到60%，为何从知识型问题（75.11%）到困难问题（31.95%）的性能显著下降，借助步骤级评估，我们发现了四个主要瓶颈：物理定理应用、物理过程理解、计算和物理条件分析。这些发现使PhysReason成为评估语言模型物理推理能力的一个新颖且全面的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12054" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 03:53:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型的数学推理能力研究</title>
<link>https://arxiv.org/abs/2502.11574</link>
<guid>https://arxiv.org/abs/2502.11574</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明大型语言模型在数学推理中存在逻辑缺陷。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）的数学推理能力，使用50个新构建的高中水平文字题进行分析。研究不仅关注模型的最终答案正确性，还深入分析了解题步骤，以识别推理失败。评估了八种最新模型，包括Mixtral、Llama、Gemini和GPT系列，发现虽然一些新模型（如o3-mini、deepseek-r1）在准确度上表现较高，但所有模型在空间推理、战略规划和算术方面均存在错误。有些模型通过错误的逻辑得出了正确的答案。常见的失败模式包括不当假设、对数字模式的过度依赖，以及将物理直觉转化为数学步骤的困难。手动分析表明，尽管模型具备广泛的数学知识，它们在处理需多步推理或现实世界知识的问题上表现不佳。研究强调仅仅关注答案而忽视推理过程的评估是有风险的，凸显出LLMs在广泛性和推理能力上的持续差距，需要针对性改进结构化推理和约束处理能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11574" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:26:18 GMT</pubDate>
</item>
<item>
<title>大型语言模型在语言复杂性测量任务中的表现研究</title>
<link>https://arxiv.org/abs/2502.11578</link>
<guid>https://arxiv.org/abs/2502.11578</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示ChatGPT-o1-mini在语言复杂性测量中表现最佳。</p><br /><br /><p><strong>摘要：</strong> 本文研究了当前大型语言模型（LLMs）在语言复杂性测量任务中的表现，重点关注LIX可读性指标和平均依赖距离（ADD）的计算。通过分析瑞典高中的论文和大学级别的论文，我们评估了模型计算LIX分数和进行依赖解析的能力，并将其结果与已建立的基准进行比较。研究发现，所有模型在这些任务上均展现出一定的能力，其中ChatGPT-o1-mini在LIX计算和依赖解析中表现最为稳定，取得了最高的准确率。此外，我们观察到模型在计算LIX时的准确性与其在大规模多任务语言理解基准（MMLU）上的整体表现之间存在显著相关性（-0.875，p = 0.026，N=6）。这些结果表明，语言复杂性测量能力可以作为评估LLMs一般能力的有效零-shot代理，提供了一种无需大量基准数据集的模型评估方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11578" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 02:23:29 GMT</pubDate>
</item>
<item>
<title>SysGen: 提升语言模型响应的系统消息生成管道</title>
<link>https://arxiv.org/abs/2502.11330</link>
<guid>https://arxiv.org/abs/2502.11330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SysGen改善了语言模型响应与系统消息的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SysGen，一个用于生成系统消息的流程，旨在提高大型语言模型（LLMs）响应的对齐度。当前，公开可用的数据常常缺乏系统消息，且在行业中受到严格的许可限制，手动标注需要大量资源。通过对没有系统消息的有监督微调数据集进行训练，SysGen显著提升了模型响应与系统消息及用户指令的一致性，且在多种开源模型的Multifacet基准测试中表现出显著改善，同时对未见的基准测试，如Open LLM Leaderboard 2，的影响极小。定性分析则强调了多样化系统消息在不同环境中的适应性的重要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:45:36 GMT</pubDate>
</item>
<item>
<title>大型语言模型知识电路演化研究</title>
<link>https://arxiv.org/abs/2502.11196</link>
<guid>https://arxiv.org/abs/2502.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大型语言模型如何内化新知识并优化知识存储过程。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在知识密集型任务中表现卓越，但它们在理解新知识的内化过程方面存在重大缺口。本文通过知识电路演化的视角，识别出促进知识存储和处理的计算子图。我们的系统分析显示，新知识的获取受既有知识相关性的影响，知识电路的演化经历从形成到优化的明显相变，并且遵循从深到浅的演化模式。这些发现不仅深化了我们对LLMs中新知识获取机制的理论理解，也为改善持续预训练策略以提升模型性能提供了潜在的启示。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:02:25 GMT</pubDate>
</item>
<item>
<title>探索大型语言模型作为代码执行替代者的能力</title>
<link>https://arxiv.org/abs/2502.11167</link>
<guid>https://arxiv.org/abs/2502.11167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文研究大型语言模型在代码执行预测中的有效性与局限性。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）作为通用代码执行替代者的潜力，尤其是预测程序输出和行为而不实际运行代码。我们提出了SURGE基准，涵盖了八个关键方面，包括多语言编程任务、竞争级编程问题和高成本科学计算等。通过对多种开源和专有LLMs的评估及模型规模与训练数据规模对替代执行准确性的影响分析，我们发现LLMs在某些情况下能够预测代码执行结果，但在通用替代执行方面存在显著限制。研究还对模型预测错误进行了分类，并探讨了潜在的改进领域，提供了使用LLMs作为代码执行替代者的可行性实证见解。代码和数据集已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 01:01:24 GMT</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:58:24 GMT</pubDate>
</item>
<item>
<title>基于学习框架的人形机器人自动起立控制研究</title>
<link>https://arxiv.org/abs/2502.12152</link>
<guid>https://arxiv.org/abs/2502.12152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种学习框架帮助人形机器人从跌倒状态成功起立。</p><br /><br /><p><strong>摘要：</strong> 本文针对人形机器人在跌倒后自动起立的控制问题，提出了一种学习框架。由于人形机器人跌倒后可能处于多种不同配置，并需要在复杂地形上操作，手动设计控制器面临巨大挑战。研究利用两阶段的课程学习方法，首先在松弛约束条件下发现良好的起立轨迹，其后将这些轨迹精炼为适合部署的平滑、缓慢的运动，确保在不同的初始配置和地形下的稳健性。实验表明，该方法使得一款真实的人形机器人能够从仰卧和俯卧两种姿态成功起立，验证了在真实环境中的有效性。这是首个在人形机器人身上成功演示的学习起立策略。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:49:53 GMT</pubDate>
</item>
<item>
<title>SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?</title>
<link>https://arxiv.org/abs/2502.12115</link>
<guid>https://arxiv.org/abs/2502.12115</guid>
<content:encoded><![CDATA[
We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from 50 bug fixes to \$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:28:31 GMT</pubDate>
</item>
<item>
<title>提升视频理解能力的开源多模态LLM video-SALMONN-o1</title>
<link>https://arxiv.org/abs/2502.11775</link>
<guid>https://arxiv.org/abs/2502.11775</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出video-SALMONN-o1，提升视频理解与推理能力的开源多模态语言模型。</p><br /><br /><p><strong>摘要：</strong> 本论文提出了video-SALMONN-o1，这是首个专为一般视频理解任务设计的开源推理增强音视频语言模型。为提升推理能力，我们开发了一个包含具有挑战性音视频问题及逐步解决方案的推理密集型数据集，并提出了过程直接偏好优化（pDPO）方法，利用对比步骤选择实现针对多模态输入的高效步级奖励建模。此外，本文还引入了RivaBench，这是首个理由密集型视频理解基准，包含4000多个高质量专家策划的问题-答案对，覆盖单口喜剧、学术演讲和合成视频检测等场景。与LLaVA-OneVision基线相比，video-SALMONN-o1在不同视频推理基准上实现了3-8%的准确性提升，而pDPO在RivaBench上相对于监督微调模型则提高了6-8%的准确率。增强的推理能力使video-SALMONN-o1具备零次合成视频检测能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11775" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 00:06:55 GMT</pubDate>
</item>
<item>
<title>TalkHier：一种新型LLM-MA系统的结构化沟通框架</title>
<link>https://arxiv.org/abs/2502.11098</link>
<guid>https://arxiv.org/abs/2502.11098</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出TalkHier框架，解决LLM-MA系统中的沟通和优化问题。</p><br /><br /><p><strong>摘要：</strong> 随着LLM-MA系统研究的进展，各代理在复杂任务协作中面临沟通管理和输出优化的挑战。本文提出了Talk Structurally, Act Hierarchically (TalkHier)框架，旨在引入结构化沟通协议和层级优化机制，以解决输出错误、虚假信息及偏见等问题。通过在多种任务上的测试，包括开放域问答、领域特定选择性提问和实际广告文本生成，TalkHier在性能上超过了多种前沿技术，如OpenAI的推理缩放模型和AgentVerse等开源多代理模型。这一新框架展示了其成为LLM-MA系统新标准的潜力，推动了更有效、适应性强的多代理协作框架的发展。相关代码已在GitHub上发布。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11098" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:51:50 GMT</pubDate>
</item>
<item>
<title>通过对例增强数学大语言模型的证明能力研究</title>
<link>https://arxiv.org/abs/2502.10454</link>
<guid>https://arxiv.org/abs/2502.10454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明现有数学大语言模型的证明能力受训练数据的影响，并提出通过对例提高其数学推理能力的方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了数学大语言模型（LLMs）在证明生成中的能力，认为现有模型的证明效果主要依赖于其训练中是否遇到相关的证明过程。这一依赖限制了模型对数学定理及概念的深入理解。我们受到人类数学教育中常用的“反例证明”方法的启发，致力于通过反例增强LLMs的数学推理和证明能力。为此，手动创建了CounterMATH，一个高质量的大学水平数学基准，要求LLMs通过提供反例来证明数学命题，从而评估其对数学概念的掌握情况。此外，我们还开发了数据工程框架，以自动获取训练数据以进一步提升模型性能。广泛的实验和详细的分析表明，CounterMATH具有挑战性，显示出LLMs（如OpenAI o1）在反例驱动的证明能力方面不足。我们认为，增强LLMs的反例驱动概念推理能力对提高其整体数学能力至关重要，提出了对数学大语言模型社区的新视角。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:37:16 GMT</pubDate>
</item>
<item>
<title>Diffusion-Sharpening：一种优化采样轨迹的微调方法</title>
<link>https://arxiv.org/abs/2502.12146</link>
<guid>https://arxiv.org/abs/2502.12146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Diffusion-Sharpening方法，通过优化采样轨迹提升微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Diffusion-Sharpening的微调方法，旨在通过优化采样轨迹提升下游任务的对齐效果。现有的基于强化学习的微调方法往往关注单个训练时间步，而忽视了轨迹级别的对齐；而最近的采样轨迹优化方法则带来了显著的推理NFE成本。Diffusion-Sharpening通过路径积分框架在训练过程中选择最优轨迹，利用奖励反馈来克服这些问题，并摊销推理成本。实验结果表明，该方法在训练效率（二次收敛速度）和推理效率（不需要额外的NFE）方面均表现出色，相较于传统的基于强化学习的微调方法和采样轨迹优化方法，Diffusion-Sharpening在文本对齐、组合能力和人类偏好等多项指标上均取得了更好的效果，提供了一种可扩展且高效的扩散模型微调方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:30:53 GMT</pubDate>
</item>
<item>
<title>HermesFlow：弥合多模态大语言模型理解与生成能力的差距</title>
<link>https://arxiv.org/abs/2502.12148</link>
<guid>https://arxiv.org/abs/2502.12148</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HermesFlow有效弥合了多模态大语言模型的理解与生成能力差距。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HermesFlow，一个旨在优化多模态大语言模型（MLLMs）理解与生成能力的框架。研究表明，MLLMs的理解能力通常强于其生成能力，二者之间存在显著差距。HermesFlow通过输入同源数据，构建理解与生成的同源偏好数据，利用Pair-DPO和自我对抗优化机制，将理解能力与生成能力有效对齐。实验结果显示，HermesFlow在缩小多模态理解与生成的差距方面，显著优于之前的方法。这一发现突显了HermesFlow作为下代多模态基础模型的一般性对齐框架的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.12148" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:29:29 GMT</pubDate>
</item>
<item>
<title>SAFE-SQL：自增强上下文学习提升Text-to-SQL性能</title>
<link>https://arxiv.org/abs/2502.11438</link>
<guid>https://arxiv.org/abs/2502.11438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAFE-SQL通过自生成示例提升Text-to-SQL的执行准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新框架SAFE-SQL，旨在将自然语言问题转换为可执行的SQL查询。传统方法如骨架掩码选择在获取相似训练示例以指导大型语言模型时表现出色，但在实际应用中面临示例缺失的挑战。为此，SAFE-SQL采用自增强上下文学习，通过生成并过滤自增强示例，显著提升SQL生成效果。该框架首先引导大型语言模型生成与测试输入相关的多个Text-to-SQL示例，并通过三种相关性评估进行过滤，构建高质量的上下文学习示例。最终，SAFE-SQL在零-shot和少-shot的Text-to-SQL任务中超越了传统框架，尤其在困难和未见场景中表现出额外的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 23:06:03 GMT</pubDate>
</item>
<item>
<title>CRANE：一种增强推理能力的约束解码算法</title>
<link>https://arxiv.org/abs/2502.09061</link>
<guid>https://arxiv.org/abs/2502.09061</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CRANE算法在约束生成中平衡了语法和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何在保证语法和语义正确性的同时，增强大型语言模型（LLMs）的推理能力。我们首先理论上解释了限制LLM输出为严格语法形式为何会减弱其推理能力。接着，我们提出通过增强输出语法、增加设计良好的额外规则，能够在确保输出合规的同时维持推理能力。基于这些理论见解，我们开发了CRANE算法，它在约束生成的正确性与非约束生成的灵活性之间取得有效平衡。通过对多个开源LLM和基准进行实验，结果显示CRANE在严苛的符号推理基准GSM-symbolic和FOLIO上，相较于最先进的约束解码策略和标准的非约束解码，准确性提升达10%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09061" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:43:51 GMT</pubDate>
</item>
<item>
<title>利用高质量LLM数据提升信息提取模型性能</title>
<link>https://arxiv.org/abs/2502.11275</link>
<guid>https://arxiv.org/abs/2502.11275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Cuckoo模型展示了如何利用LLM数据提升信息提取效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在信息提取（IE）领域中，将大型语言模型（LLM）的数据应用于IE模型的可能性。我们提出了一种新的NTE（下一标记提取）范式，通过将预测下一个标记的过程重新设计为对已存在上下文中标记的提取，从而构建出Cuckoo模型，该模型基于来自LLM的102.6M抽取数据进行训练。在少量样本环境下，Cuckoo能有效适应传统和复杂指令下的信息提取任务，表现优于现有的预训练IE模型。作为一种“搭便车”方案，Cuckoo能随着LLM数据准备的持续进展而自然演化，无需额外的手动干预，即可从LLM训练管道的改进中获益。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11275" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:10:49 GMT</pubDate>
</item>
<item>
<title>合成数据增强在项目级证明导向编程中的应用</title>
<link>https://arxiv.org/abs/2502.11901</link>
<guid>https://arxiv.org/abs/2502.11901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种合成数据增强方法以解决证明导向编程中的数据稀缺问题。</p><br /><br /><p><strong>摘要：</strong> 现有的语言模型在证明导向编程中面临数据稀缺的问题，主要表现为缺乏足够的相关语料库和项目级实现。本文首次提出了一种基于合成数据增强的方法，旨在通过生成和修复项目级的证明导向编程问题来应对这一挑战。该方法通过合成基本的证明导向编程问题提升语言模型在特定编程语言中的熟练程度，同时引入多样化的编码数据，以增强推理能力，并在现有代码库中创建新的证明和修复数据。这一方法使得语言模型能够在函数级和代码库级别生成及修复证明。我们展示了我们微调后的14B参数模型PoPilot，在项目级证明导向编程中超越了GPT-4o模型64%的性能，并能够通过修复其输出，比GPT-4o的自我修复提高54%的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.11901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 22:05:54 GMT</pubDate>
</item>
<item>
<title>深度分析大型推理模型中的过度思考现象</title>
<link>https://arxiv.org/abs/2502.08235</link>
<guid>https://arxiv.org/abs/2502.08235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，过度思考限制了大型推理模型在互动环境中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型推理模型（LRMs）在互动环境中的过度思考现象，这种现象表现为模型倾向于使用冗长的内部推理链而不是进行环境互动。通过在SWE Bench Verified软件工程任务上的实验，我们识别了三种常见行为模式：分析瘫痪、鲁莽行动和过早脱离。分析结果显示，过度思考得分较高与模型表现下降相关，推理模型表现出比非推理模型更强的过度思考倾向。通过采取一些简单措施，如选择过度思考得分较低的解决方案，我们发现可将模型性能提升近30%，同时计算成本降低43%。这些发现表明，缓解过度思考在实际应用中具有重要意义。我们建议可通过利用原生功能调用能力和选择性的强化学习来减轻过度思考倾向，并开源了我们的评估框架和数据集，以推动该研究方向的发展。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08235" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 17:09:38 GMT</pubDate>
</item>
<item>
<title>选择性自我监督微调方法（S3FT）提升大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2502.08130</link>
<guid>https://arxiv.org/abs/2502.08130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">S3FT方法在保持性能的同时，改善了大语言模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的微调方法——选择性自我监督微调（S3FT），该方法旨在提高大语言模型（LLMs）在特定任务上的性能，同时改善其泛化能力。S3FT的方法通过利用多个对同一查询的有效响应来减少模型在微调过程中的过拟合，从而避免过度专注于训练数据的特征。具体而言，S3FT首先通过部署适当的评估者，识别训练集中的正确模型响应，然后利用这些正确响应与目标响应（或其同义句）微调模型。实验结果显示，与标准监督微调（SFT）相比，S3FT在数学推理、Python编程和阅读理解任务上显著提升了性能，并将标准SFT导致的平均性能下降（最高达4.4）减少了一半，说明S3FT在提高模型任务性能的同时，具备更好的泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 12:27:43 GMT</pubDate>
</item>
<item>
<title>CLaMP 3: 一种跨模态与跨语言音乐信息检索统一框架</title>
<link>https://arxiv.org/abs/2502.10362</link>
<guid>https://arxiv.org/abs/2502.10362</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLaMP 3通过对比学习实现跨模态及语言的音乐信息检索。</p><br /><br /><p><strong>摘要：</strong> CLaMP 3是一个为解决音乐信息检索中跨模态和跨语言泛化挑战而开发的统一框架。它通过对比学习将主要音乐模态（包括乐谱、表演信号和音频记录）与多语言文本对齐至共享表示空间，从而实现以文本为桥梁的各模态检索。该框架具有可适应新语言的多语言文本编码器，显示了强大的跨语言泛化能力。我们利用增强检索生成技术，创建了M4-RAG数据集，包含231万对音乐文本对，并附有丰富的元数据，涵盖广泛的全球音乐传统。为推动未来研究，我们还发布了WikiMT-X基准数据集，包含1000个乐谱、音频以及多样化文本描述的三元组。实验结果显示，CLaMP 3在多个音乐信息检索任务上达到领先性能，显著超越之前的强基线，展现出在多模态和多语言音乐上下文中的优秀泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10362" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 10:18:04 GMT</pubDate>
</item>
<item>
<title>高效多级卷积架构在3D视觉定位中的应用</title>
<link>https://arxiv.org/abs/2502.10392</link>
<guid>https://arxiv.org/abs/2502.10392</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效的多级卷积架构，优化3D视觉定位性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种高效的多级卷积架构，用于3D视觉定位，旨在克服传统方法因双阶段或点基础架构导致实时推理困难的问题。受3D物体检测中稀疏卷积架构成功的启发，结合文本特征，使3D场景表示与文本特征有效互动，采用文本引导剪枝（TGP）与基于补全的加法（CBA）方法，通过逐渐区域剪枝与目标补全高度融合信息。TGP通过交叉注意力有效地稀疏化3D场景表示，CBA则解决了过度剪枝对几何信息影响的问题，以微小的计算开销修复被过度剪枝区域。实验显示，与以往单阶段方法相比，该方法在推理速度上领先，且在ScanRefer、NR3D和SR3D上存在显著的准确率提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10392" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 09:25:39 GMT</pubDate>
</item>
<item>
<title>DarwinLM: Evolutionary Structured Pruning of Large Language Models</title>
<link>https://arxiv.org/abs/2502.07780</link>
<guid>https://arxiv.org/abs/2502.07780</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have achieved significant success across various NLP tasks. However, their massive computational costs limit their widespread use, particularly in real-time applications. Structured pruning offers an effective solution by compressing models and directly providing end-to-end speed improvements, regardless of the hardware environment. Meanwhile, different components of the model exhibit varying sensitivities towards pruning, calling for non-uniform model compression. However, a pruning method should not only identify a capable substructure, but also account for post-compression training. To this end, we propose \sysname, a method for training-aware structured pruning. \sysname builds upon an evolutionary search process, generating multiple offspring models in each generation through mutation, and selecting the fittest for survival. To assess the effect of post-training, we incorporate a lightweight, multistep training process within the offspring population, progressively increasing the number of tokens and eliminating poorly performing models in each selection stage. We validate our method through extensive experiments on Llama-2-7B, Llama-3.1-8B and Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured pruning. For instance, \sysname surpasses ShearedLlama while requiring 5times less training data during post-compression training.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>ImageRAG：基于检索增强生成的图像合成方法</title>
<link>https://arxiv.org/abs/2502.09411</link>
<guid>https://arxiv.org/abs/2502.09411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ImageRAG通过动态检索图像提高了图像生成质量，特别是在稀有概念的合成方面。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ImageRAG的方法，它结合了图像生成模型与检索增强生成技术，以解决现有扩散模型在生成稀有或未见概念时的挑战。ImageRAG会根据给定的文本提示动态检索相关图像，并将这些图像作为上下文来指导图像生成过程。与之前专门针对检索生成训练的模型不同，ImageRAG不需要专门的训练，而是利用现有图像条件模型的能力。这种方法具有高度的适应性，能够在多种基础模型中应用，显著提高了稀有和细粒度概念的生成效果。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:41:41 GMT</pubDate>
</item>
<item>
<title>小型多语言模型在低资源语言处理中的适应性研究</title>
<link>https://arxiv.org/abs/2502.10140</link>
<guid>https://arxiv.org/abs/2502.10140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究 explores 小型多语言模型在低资源语言下的适应性与性能提升。</p><br /><br /><p><strong>摘要：</strong> 本研究系统探讨了如何有效地使用参数高效的适配器方法，将小型多语言模型（mLMs）适应于低资源语言（LRLs）。研究评估了三种适配器架构：顺序瓶颈、可逆瓶颈和低秩适配方法。结果显示，使用来自GlotCC的非结构化文本和ConceptNet的结构化知识的小型适配数据集（如最多1GB的自由文本或几MB的知识图数据）能显著提升模型在内部（掩码语言建模）和外部任务（主题分类、情感分析和命名实体识别）的表现。研究发现，顺序瓶颈适配器在语言建模方面表现出色，而可逆瓶颈适配器在下游任务上由于更好的嵌入对齐和更多的参数数量略有凌驾于其他方法之上。适配器方法在使用更少参数的情况下，实现了与完整微调相当或更优的性能，而与大型语言模型（如LLaMA-3、GPT-4及DeepSeek-R1基于的蒸馏模型）相比，小型mLMs在低资源语言任务中更为有效。尽管适应性提高了性能，但预训练数据规模仍是决定因素，特别是对于预训练覆盖面广的语言。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10140" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 08:29:25 GMT</pubDate>
</item>
<item>
<title>CAPI：一种基于聚类的纯MIM框架及其在图像识别中的应用</title>
<link>https://arxiv.org/abs/2502.08769</link>
<guid>https://arxiv.org/abs/2502.08769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAPI框架通过聚类预测提升了自监督学习的表现，达到了高准确率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CAPI，一个新颖的纯蒙面图像建模（MIM）框架，旨在提升自监督表示学习的性能。我们系统分析了目标表示、损失函数和架构，提出了一种基于聚类的损失函数，以提高模型的训练稳定性和扩展性。CAPI使用ViT-L骨干网络，在ImageNet数据集上取得了83.8%的准确率，ADE20K数据集上实现了32.1%的mIoU，相较于现有的MIM方法大幅提升，并接近当前最先进的方法DINOv2。我们将所有代码和模型发布，促进后续研究和应用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 07:24:28 GMT</pubDate>
</item>
<item>
<title>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2502.10235</link>
<guid>https://arxiv.org/abs/2502.10235</guid>
<content:encoded><![CDATA[
Pre-trained foundation models (FMs) have shown exceptional performance in univariate time series forecasting tasks. However, several practical challenges persist, including managing intricate dependencies among features and quantifying uncertainty in predictions. This study aims to tackle these critical limitations by introducing adapters; feature-space transformations that facilitate the effective use of pre-trained univariate time series FMs for multivariate tasks. Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension. Inspired by the literature on representation learning and partially stochastic Bayesian neural networks, we present a range of adapters and optimization/inference strategies. Experiments conducted on both synthetic and real-world datasets confirm the efficacy of adapters, demonstrating substantial enhancements in forecasting accuracy and uncertainty quantification compared to baseline methods. Our framework, AdaPTS, positions adapters as a modular, scalable, and effective solution for leveraging time series FMs in multivariate contexts, thereby promoting their wider adoption in real-world applications. We release the code at https://github.com/abenechehab/AdaPTS.
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:36:23 GMT</pubDate>
</item>
<item>
<title>VibeGen：基于生成AI的蛋白质动态设计框架</title>
<link>https://arxiv.org/abs/2502.10173</link>
<guid>https://arxiv.org/abs/2502.10173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VibeGen是一种基于动态特性的蛋白质生成AI设计框架。</p><br /><br /><p><strong>摘要：</strong> VibeGen是一个新颖的蛋白质生成AI框架，旨在通过正常模式振动进行蛋白质的端到端设计。它采用双模型架构，包括一个基于指定振动模式生成序列候选者的蛋白质设计器和一个评估其动态准确性的蛋白质预测器。通过全原子分子模拟确认，设计的蛋白质能够准确再现规定的正常模式幅度，并在此基础上采用多种稳定、功能相关的结构。显著的是，生成的序列为全新设计，与自然蛋白质没有显著相似性，拓展了可及的蛋白质空间，突破了进化限制。本研究将蛋白质动态性引入生成设计过程，建立了序列与振动行为之间的双向联系，为工程化具有定制动态和功能特性的生物分子开辟了新的路径，具有重要的应用潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 05:09:33 GMT</pubDate>
</item>
<item>
<title>通过新词开发理解人工智能的语言</title>
<link>https://arxiv.org/abs/2502.07586</link>
<guid>https://arxiv.org/abs/2502.07586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">为有效理解AI，需构建新词汇以弥补现有语言的不足。</p><br /><br /><p><strong>摘要：</strong> 本文立论认为，要理解人工智能（AI），我们不能仅依赖现有的人类词汇。相反，我们应努力开发新词汇，以准确表达人类概念或机器概念，从而实现更好的理解。人类与机器的概念不同，因此可将可解释性视为一种沟通问题：人类需要能够参考和控制机器概念，同时将人类概念传达给机器。通过创建共享的人机语言，借助新词汇的发展，可解决这一沟通难题。成功的新词汇应当在抽象程度上适中，既不过于细化，以便于在多个语境中重用，又不太高层，以便于传达精确信息。作为概念验证，文章展示了如何通过“长度新词”控制大型语言模型的回答长度，以及使用“多样性新词”促进更加多变的响应。总体来说，我们认为，无法仅用现有词汇理解AI，而通过新词汇的扩展可为控制和理解机器创造新的机会。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 04:28:55 GMT</pubDate>
</item>
<item>
<title>通过局部化关注层提升扩散模型文本生成能力</title>
<link>https://arxiv.org/abs/2502.09935</link>
<guid>https://arxiv.org/abs/2502.09935</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨如何通过局部化扩散模型的注意力层来优化图像中的文本生成。</p><br /><br /><p><strong>摘要：</strong> 本文研究了如何在扩散模型中通过局部化局部注意力层来提高文本生成的效率和性能。研究表明，扩散模型参数的不到1%影响图像中文本内容的生成，主要集中在注意力层中。通过只对这些局部注意力层进行LoRA微调，可以显著提升大型扩散模型的文本生成能力，并且在保持图像生成质量和多样性的同时，应用于图像中文本编辑和防止有害文本生成。该方法跨多种扩散模型架构（如U-Net和Transformer）均具有广泛适用性，能够兼容多种文本编码器，如CLIP和T5。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09935" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 03:06:17 GMT</pubDate>
</item>
<item>
<title>MR采样器：加速可控生成中的扩散模型采样过程</title>
<link>https://arxiv.org/abs/2502.07856</link>
<guid>https://arxiv.org/abs/2502.07856</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出MR采样器算法，显著加速了MR扩散模型的采样过程。</p><br /><br /><p><strong>摘要：</strong> 随着扩散模型应用的不断增长，可控生成的重要性和挑战并存。目前的可控生成方法主要集中在修改扩散模型的评分函数，而均值回归(MR)扩散则直接修改随机微分方程(SDE)的结构，使得图像条件的结合更加简便自然。然而，现有的无训练快速采样器并不适用于MR扩散，因此在获得高质量样本时，MR扩散需要数百次函数评估(NFEs)。本文提出了一种新的算法MR采样器(MRS)，旨在减少MR扩散的采样NFEs。该算法解决了与MR扩散相关的逆向时间SDE和概率流普通微分方程(PF-ODE)，并推导出半解析解，包括一个解析函数和一个由神经网络参数化的积分。基于这一解法，我们能够在更少的步骤中生成高质量样本。本方法无需训练，支持所有主流参数化，包括噪声预测、数据预测和速度预测。大量实验表明，MR采样器在十个不同的图像恢复任务中以10到20倍的加速维持高采样质量，极大提高了MR扩散的可控生成实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07856" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 02:03:05 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的车辆间合作感知与规划研究</title>
<link>https://arxiv.org/abs/2502.09980</link>
<guid>https://arxiv.org/abs/2502.09980</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出基于大语言模型的车辆间合作感知与规划新方法。</p><br /><br /><p><strong>摘要：</strong> 当前的自动驾驶车辆主要依赖各自的传感器理解周围环境并规划未来轨迹，但当传感器出现故障或被遮挡时，这种方法的可靠性下降。为了解决这一问题，本文提出了一种将大语言模型应用于车辆间合作的创新设定，并构建了车辆间问答数据集（V2V-QA）和基准测试。我们的基线方法，即车辆间大语言模型（V2V-LLM），使用大语言模型融合多个连接自动驾驶车辆的感知信息并回答与驾驶相关的问题，如物体识别和规划。实验结果显示，V2V-LLM在执行不同任务方面表现优越，优于使用其他融合方法的基线。这项研究为未来自动驾驶系统的安全性提供了新的研究方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09980" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 01:33:15 GMT</pubDate>
</item>
<item>
<title>利用LLM进行反监测的创新方法及其潜在风险</title>
<link>https://arxiv.org/abs/2502.09638</link>
<guid>https://arxiv.org/abs/2502.09638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了通过人类干预实现LLM自我越狱的创新方式及其安全隐患。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的LLM作为红队员的方法，通过人类干预使拒绝训练的LLM能够自我越狱或越狱其他LLM。我们将被越狱的LLM称为J_2攻击者，这些模型能够使用各种红队策略系统性地评估目标模型，并通过从以往失败中学习来提高性能。实验结果显示，Sonnet 3.5和Gemini 1.5作为J_2在Harmbench上的攻击成功率分别达到了93.0%和91.0%，显著优于其他LLM。我们的研究不仅展示了一种受人类红队员启发的战略红队方法的可扩展性，还指出了越狱对越狱的被忽视的失败模式，强调了LLM能通过一个越狱版本自身来绕过自身的安全措施。为防止J_2的直接误用并推动AI安全研究，我们分享了我们的研究方法，并对具体提示细节进行了保密。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:04:19 GMT</pubDate>
</item>
<item>
<title>LLaDA：突破自回归模型的扩散模型探索</title>
<link>https://arxiv.org/abs/2502.09992</link>
<guid>https://arxiv.org/abs/2502.09992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA模型挑战自回归模型，展示了扩散模型的潜力。</p><br /><br /><p><strong>摘要：</strong> 本研究推出LLaDA，一个全新训练的扩散模型，旨在挑战自回归模型在大型语言模型中的主导地位。LLaDA通过前向数据掩蔽过程和反向过程来模型化分布，采用简单的Transformer预测被掩蔽的标记，通过优化似然界限提供了一种原则性生成推断方法。在多个基准测试中，LLaDA显示出强大的可扩展性，超越了自构建的自回归模型基线。特别是，LLaDA 8B在上下文学习中与强大的LLM如LLaMA3 8B竞争，并在多轮对话等案例研究中表现出令人印象深刻的指令跟随能力。此外，LLaDA还克服了逆转诅咒，在逆转诗填空任务中超越了GPT-4o。研究结果确定扩散模型作为自回归模型的有效替代方案，挑战了上述关键语言模型能力与自回归模型内在联系的假设。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 17 Feb 2025 00:03:18 GMT</pubDate>
</item>
<item>
<title>多模型推理方法提升LLM在高级数学和编码任务中的表现</title>
<link>https://arxiv.org/abs/2502.09955</link>
<guid>https://arxiv.org/abs/2502.09955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多模型推理方法，显著提升LLM在高级数学问题上的解题能力。</p><br /><br /><p><strong>摘要：</strong> 尽管推理语言模型如OpenAI的o1、o3和DeepSeek R1在数学和编码方面取得了显著进展，但在国际数学奥林匹克（IMO）组合问题、抽象与推理库（ARC）难题以及人类最后考试（HLE）问题等高级任务中仍面临挑战。我们提出了一种多元推理方法，在测试时结合多种模型和方法，发现自动验证数学和编码问题的正确性，以及其他问题的拒绝采样，简单而有效。具体而言，我们通过Lean自动验证IMO问题的正确性，通过代码验证ARC难题的解，并发现“最佳-N”有效回答HLE问题，使IMO组合问题的解答准确率从33.3%提升至77.8%，HLE问题的准确率从8%提升至37%。我们的实验表明，该方法在解决948名人类无法解出的ARC难题中成功率达80%，并在o3高计算下未解的ARC难题中解决率为26.5%。通过测试模拟、强化学习和带推理反馈的元学习，我们提高了模型的泛化能力，适应图形表示和不同的提示、代码及数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:57:43 GMT</pubDate>
</item>
<item>
<title>傅里叶数字嵌入方法及其在大语言模型中的应用</title>
<link>https://arxiv.org/abs/2502.09741</link>
<guid>https://arxiv.org/abs/2502.09741</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出傅里叶数字嵌入法，以提高数字任务的效率和准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为傅里叶数字嵌入（FoNE）的新方法，旨在解决大语言模型在处理数字时的高效性问题。传统上，大语言模型通过多个标记表示数字，这种碎片化的表示方式在训练和推理中降低了效率，影响了解析数字的能力。FoNE通过将每个数字直接映射到傅里叶特征的嵌入空间，允许每个数字作为单个标记进行编码，并仅为每个数字的每个数字提供两个嵌入维度。这种紧凑的表示方式显著加快了训练和推理过程。在数字任务，尤其是加法、减法和乘法中，FoNE的性能远超传统的子词和数字嵌入，使用70万次数据实现99%准确度比子词和数字嵌入少64倍，对每个数字使用的标记分别减少3倍和6倍。此外，FoNE在超过100,000个测试样例上实现了100%的准确率，展示了其优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09741" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 23:07:53 GMT</pubDate>
</item>
<item>
<title>MM-RLHF: 提升多模态大语言模型对人类偏好的对齐研究</title>
<link>https://arxiv.org/abs/2502.10391</link>
<guid>https://arxiv.org/abs/2502.10391</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出MM-RLHF数据集，推动多模态大语言模型对人类偏好的对齐。</p><br /><br /><p><strong>摘要：</strong> 尽管多模态大语言模型（MLLM）取得了显著进展，但大多数最先进的模型未能与人类偏好进行彻底对齐。为此，我们引入了MM-RLHF数据集，包含12万对细粒度人类注释的偏好比较，具有更大的规模、更多样性和更高质量。该数据集帮助我们提出多项创新，如基于批评的奖励模型和动态奖励缩放，旨在提升奖励模型的质量和对齐算法的效率。我们的实验显示，通过MM-RLHF和对齐算法微调LLaVA-ov-7B，模型的对话能力提高了19.5%，安全性提高了60%。我们已开源数据集和训练代码，更多细节可查阅我们的项目页面。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10391" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:51:55 GMT</pubDate>
</item>
<item>
<title>Step-Video-T2V：先进的文本生成视频预训练模型</title>
<link>https://arxiv.org/abs/2502.10248</link>
<guid>https://arxiv.org/abs/2502.10248</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Step-Video-T2V是一款具有30B参数的文本到视频生成模型。</p><br /><br /><p><strong>摘要：</strong> Step-Video-T2V是一款先进的文本生成视频预训练模型，拥有30B个参数，能够生成长达204帧的视频。该模型使用深度压缩变分自编码器(Video-VAE)，实现了16x16空间和8x时间压缩比，同时确保视频重建质量卓越。为了处理英语和汉语的用户提示，采用了两种双语文本编码器。模型结合3D全注意力的DiT，通过Flow Matching去噪输入噪声，转换为潜在帧，利用视频基础的DPO方法减少伪影和提高生成视频的视觉质量。通过新开发的视频生成基准Step-Video-T2V-Eval进行性能评估，Step-Video-T2V在文本到视频生成质量上表现出色。文章还讨论了当前扩散模型的局限性，并指出了未来视频基础模型的发展方向。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10248" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:50:38 GMT</pubDate>
</item>
<item>
<title>RAS：一种高效的动态采样策略以加速扩散模型</title>
<link>https://arxiv.org/abs/2502.10389</link>
<guid>https://arxiv.org/abs/2502.10389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RAS通过动态采样区域显著提高扩散模型的实时性能。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多种生成任务中表现出色，但其依赖于多个顺序前向传播的特性限制了实时性能。传统的加速方法主要集中于减少采样步骤或重用中间结果，未能利用图像中空间区域的差异。本文提出的RAS（区域自适应采样）策略，利用扩散变换器在处理变数量标记的灵活性，为图像的不同区域动态分配采样比例。通过观察模型在每个采样步骤专注于语义重要区域的现象，RAS仅更新当前集中区域，而将其他区域使用上一步的缓存噪声进行更新。实验结果显示，RAS在Stable Diffusion 3和Lumina-Next-T2I上分别实现了2.36倍和2.51倍的加速，同时生成质量几乎不受影响。用户研究表明，RAS在人工评估中表现相当，并实现了1.6倍的速度提升，推动了扩散变换器在实时应用中的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:22:08 GMT</pubDate>
</item>
<item>
<title>ZeroBench：一项全新的视觉推理基准挑战大型多模态模型</title>
<link>https://arxiv.org/abs/2502.09696</link>
<guid>https://arxiv.org/abs/2502.09696</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ZeroBench旨在挑战大型多模态模型的视觉推理能力，结果显示其表现较差。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型（LMMs）在图像解释上存在显著不足，其空间认知能力甚至不及幼儿或动物。尽管如此，这些模型在许多流行的视觉基准上得分颇高，然而其进展空间正在迅速缩小。因此，急需一些更具挑战性的基准以保持其长期相关性。为此，研究者推出了ZeroBench，这是一项完全不能被现代前沿LMMs解决的轻量级视觉推理基准，由100个精心编制的问题和334个较易于答复的子问题组成。对20个LMMs进行ZeroBench评估，结果显示它们的得分均为0.0%。此基准的推出旨在促进视觉理解的进展，并已公开发布以供研究者使用。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09696" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 22:20:53 GMT</pubDate>
</item>
<item>
<title>基于时空记忆的智能代理框架STMA</title>
<link>https://arxiv.org/abs/2502.10177</link>
<guid>https://arxiv.org/abs/2502.10177</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">STMA框架通过时空记忆提升智能代理在动态环境中的决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的代理框架——时空记忆代理(STMA)，旨在提高智能代理在动态环境中执行长远任务的能力。STMA集成了三大关键组件：实时捕捉历史和环境变化的时空记忆模块、支持自适应空间推理的动态知识图谱，以及迭代优化任务策略的规划-评估机制。我们在TextWorld环境中评估了STMA在32个任务上的表现，通过多步规划和探索，结果显示STMA相比最先进模型的成功率提高了31.25%，平均得分提高了24.7%。这些结果突显了时空记忆在提升智能代理记忆能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.10177" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 16 Feb 2025 21:31:11 GMT</pubDate>
</item>
<item>
<title>新框架提升二维潜在空间的三维重建效果</title>
<link>https://arxiv.org/abs/2502.09613</link>
<guid>https://arxiv.org/abs/2502.09613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出新框架，提升二维潜在空间在三维重建中的效果。</p><br /><br /><p><strong>摘要：</strong> 针对现有三维重建方法在二维特征空间与三维表示之间存在的领域差距，本文提出了一种新颖的框架，旨在将三维意识整合到二维潜在空间中。该框架由三个阶段组成，首先通过一种考虑对应关系的自编码方法，增强二维潜在表示的三维一致性；其次，利用潜在辐射场（LRF）将这些具备三维意识的二维表示提升到三维空间；最后，采用VAE-Radiance Field（VAE-RF）对齐策略，提升从渲染的二维表示中解码图像的效果。通过广泛实验，结果表明该方法在合成性能和跨数据集泛化能力方面优于当前最先进的潜在三维重建方法，是首次展示基于二维潜在表示构建的辐射场可以实现逼真的三维重建性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 21:20:14 GMT</pubDate>
</item>
<item>
<title>GSM-Ranges：评估大语言模型数学推理能力的新方法</title>
<link>https://arxiv.org/abs/2502.08680</link>
<guid>https://arxiv.org/abs/2502.08680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍GSM-Ranges，用于评估大语言模型在不同数值范围下的数学推理能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大语言模型（LLMs）在数学推理方面的评估局限性，现有基准测试通常使用有限的数值范围，未能真实反映模型在多样化规模下的问题解决能力。为了解决这些问题，我们引入了GSM-Ranges，这是一个基于GSM8K生成的数据集，旨在系统性地扰动数学问题中的数值，以评估模型在不同数值复杂性下的稳健性。此外，我们提出了一种新颖的评分方法，能够区分逻辑错误和非逻辑错误，从而更精准地评估推理过程。实验显示，随着数值复杂性的增加，模型的逻辑错误率显著上升，高达14个百分点，表明模型在处理超出分布的数值时存在普遍弱点。同时，模型在独立的算术任务上表现良好，但在处理嵌入了文字问题的计算时，其性能显著下降。这些发现为进一步研究LMMs的数学推理能力及其数值泛化的改进提供了重要见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 09:18:18 GMT</pubDate>
</item>
<item>
<title>VFX Creator: 基于AI的可控视觉效果生成新范式</title>
<link>https://arxiv.org/abs/2502.05979</link>
<guid>https://arxiv.org/abs/2502.05979</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种基于文本描述和静态图像的可控视觉效果生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着电影制作中特效技术的发展，视觉效果（VFX）成为实现魔法与幻觉的重要工具。本文提出了一种新的动画视觉效果生成范式，通过用户友好的文本描述和静态参考图像生成动态效果。主要贡献包括：第一，建立了Open-VFX数据集，这是首个高质量的视觉效果视频数据集，涵盖15种多样化的效果类别并附有详细标注；第二，开发了VFX Creator框架，利用视频扩散变换器实现可控VFX生成。该模型具备空间和时间控制的LoRA适配器，支持实例级的空间操控与精准的时间控制。通过在Open-VFX测试集上的广泛实验，证明了该系统在生成真实动态效果上优于现有技术，并引入了一种专门的度量标准以评估时间控制的精确度。VFX Creator通过结合传统特效与生成方法，拓展了高质量视频特效生成的新可能性，使得先进的视觉效果可被更广泛的受众所掌握。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05979" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 08:47:33 GMT</pubDate>
</item>
<item>
<title>通用神经追踪控制器的开发与应用</title>
<link>https://arxiv.org/abs/2502.09614</link>
<guid>https://arxiv.org/abs/2502.09614</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了从人类参考中开发的通用神经追踪控制器，以实现灵活的操作。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何开发一个通用的神经追踪控制器，以实现灵活的机器人手部操控，能够应对多种物体及其不同的操控需求。我们提出了一种方法，利用大规模成功机器人追踪示例，结合人类参考和机器人动作，来训练一个神经控制器。通过数据飞轮的方式，我们不断提升控制器的性能以及成功追踪示例的数量和质量。同时，采用强化学习与模仿学习相结合的策略，以增强控制器在动态环境中的表现。此外，为了获得高质量的追踪示例，我们还优化了每条轨迹的追踪，通过同伦优化方法，解决复杂的轨迹追踪问题，从而增加示例的多样性。实验表明，我们训练的通用神经控制器在仿真和实际环境中实现了超过10%的成功率提升，优于现有的先进基线。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09614" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:50:27 GMT</pubDate>
</item>
<item>
<title>3CAD：用于工业缺陷检测的新型大规模数据集与检测框架</title>
<link>https://arxiv.org/abs/2502.05761</link>
<guid>https://arxiv.org/abs/2502.05761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出3CAD数据集及其用于工业缺陷检测的CFRG框架，提升检测准确性。</p><br /><br /><p><strong>摘要：</strong> 为提高工业缺陷检测精度，本文提出了一个新型大规模数据集3CAD，源自真实的3C生产线。该数据集包含27,039幅高分辨率图像，涵盖八种不同类型的制造部件，标注了像素级的异常，具备多种异常类型和多异常区域的特征。我们还提出了一种简单有效的无监督异常检测框架——细化检测范式与恢复引导（CFRG），通过粗定位和细定位相结合，以捕捉小缺陷异常。实验结果表明，CFRG框架在3CAD数据集上的表现强劲，为异常检测领域的发展提供了一个极具挑战性的基准。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 04:00:29 GMT</pubDate>
</item>
<item>
<title>ProbeLog：提高分类模型检索效率的新方法</title>
<link>https://arxiv.org/abs/2502.09619</link>
<guid>https://arxiv.org/abs/2502.09619</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出ProbeLog，一种高效检索分类模型的方法。</p><br /><br /><p><strong>摘要：</strong> 随着公开可用模型数量的增加，用户对 pretrained 模型的需求不断上升，但现有的模型搜索方法主要依赖于文档中的文本搜索，限制了用户找到相关模型的能力。本文介绍了一种新方法ProbeLog，它能够在没有访问模型元数据或训练数据的情况下，检索识别目标概念（如“狗”）的分类模型。与以往的探测方法不同，ProbeLog通过观测每个模型的输出维度（logit）对固定输入集（探针）的响应，计算出一个描述符。该方法支持基于 logit 的检索和零-shot、基于文本的检索。为降低编码存储库的成本，本文还开发了一种基于协同过滤的方法，使得编码成本降低三倍。实验结果表明，ProbeLog在实际应用和细粒度搜索任务中都实现了高检索准确率，并且能扩展到全尺寸的模型存储库。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09619" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:58:25 GMT</pubDate>
</item>
<item>
<title>CoSER: 高质量角色扮演语言模型数据集及评估协议</title>
<link>https://arxiv.org/abs/2502.09082</link>
<guid>https://arxiv.org/abs/2502.09082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoSER提供高质量角色扮演语言模型的数据集及评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CoSER，一个为角色扮演语言代理(RPLA)提供的高质量数据集、开放模型及评估协议。CoSER数据集包含来自771部著名书籍的17,966个角色，并提供真实对话及多样化的数据类型，如对话设置、角色体验和内心想法。我们引入了基于表演方法的给定情境表演，来训练和评估角色扮演的语言模型，LLMs能顺序展现书中多个角色。通过该数据集，我们开发了基于LLaMA-3.1的CoSER 8B和CoSER 70B先进的开放角色扮演语言模型。大量实验表明，CoSER数据集在RPLA的训练、评估和检索中具有重要价值，其中CoSER 70B在我们的评估及现有三个基准上表现优异，超越或匹配了GPT-4o，在InCharacter和LifeChoice基准上分别达到了75.80%和93.47%的准确率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:50:35 GMT</pubDate>
</item>
<item>
<title>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models</title>
<link>https://arxiv.org/abs/2502.09390</link>
<guid>https://arxiv.org/abs/2502.09390</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models across multiple question-answering datasets, demonstrate that SQuARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQuARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:35:53 GMT</pubDate>
</item>
<item>
<title>无编码器架构在3D理解中的应用探索</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文首次探讨无编码器架构在3D理解中的有效性。</p><br /><br /><p><strong>摘要：</strong> 本文对无编码器架构在3D理解的应用潜力进行了全面研究，解决了现有基于编码器的3D大多模态模型所面临的挑战，例如无法适应不同的点云分辨率以及编码器输出的特征无法满足大型语言模型的语义需求。我们提出了两个关键策略：一是在预训练阶段采用LLM嵌入的语义编码策略，并使用混合语义损失提取高级语义；二是在指令调优阶段引入层次化几何聚合策略，帮助LLM关注点云的局部细节。最终，我们提出了首个无编码器的3D大多模态模型ENEL，表现出色，与当前最先进的模型ShapeLLM-13B相竞争，分别在分类、描述和视觉问答任务中取得55.0%、50.92%和42.7%的成绩。这些结果表明，无编码器架构在3D理解领域具有替代基于编码器的架构的巨大潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 02:27:45 GMT</pubDate>
</item>
<item>
<title>MME-CoT：评估大型多模态模型的链式思维推理性能</title>
<link>https://arxiv.org/abs/2502.09621</link>
<guid>https://arxiv.org/abs/2502.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了MME-CoT基准，评估多模态模型的链式思维推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出MME-CoT，一个专门评估大型多模态模型（LMMs）链式思维（CoT）推理性能的基准，涵盖数学、科学、光学字符识别、逻辑、时空和常规场景等六个领域。作为该领域的首个综合研究，MME-CoT包含三项新颖的评估指标，细致评估推理质量、鲁棒性和效率。通过高质量数据和独特的评估策略，我们深入分析了最先进的LMMs，发现几个关键见解：拥有反思机制的模型在CoT质量上表现优异，其中Kimi k1.5表现优于GPT-4o并取得最高质量结果；CoT提示在感知性任务中通常会降低LMM性能，这表明可能存在有害的过度思考行为；尽管CoT质量较高，但拥有反思机制的LMMs在正常响应和自我修正阶段效率明显不足。我们希望MME-CoT能够为推进LMMs的多模态推理奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:34:58 GMT</pubDate>
</item>
<item>
<title>Typhoon T1：开放的泰语推理模型开发</title>
<link>https://arxiv.org/abs/2502.09042</link>
<guid>https://arxiv.org/abs/2502.09042</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍了Typhoon T1，一个新型泰语推理模型的开放开发项目。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Typhoon T1，这是一个旨在开发开放泰语推理模型的项目。推理模型是一种新型的生成模型，基于大型语言模型（LLMs）之上，通过生成长链思维来得到最终答案，已被发现能够提高复杂任务的性能。然而，关于如何开发此类模型的细节相对有限，特别是在低资源语言的推理模型方面。Typhoon T1以更具成本效益的方式进行开发，采用监督微调和开放数据集，而非强化学习。文章分享了合成数据生成与训练的细节，以及我们的数据集和模型权重。此外，我们提供了跨领域通用推理模型的开发见解，能够生成低资源语言中的推理痕迹，以泰语为例。我们希望这一开放努力能为该领域的进一步研究奠定基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09042" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 01:29:44 GMT</pubDate>
</item>
<item>
<title>CoT-Valve: 动态控制推理链长度的方法</title>
<link>https://arxiv.org/abs/2502.09601</link>
<guid>https://arxiv.org/abs/2502.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoT-Valve方法动态调节推理链长度，优化推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的方法CoT-Valve，用于动态控制推理链的长度，以应对不同任务的难度，降低推理模型的推理成本。研究表明，在简单任务中推理路径容易压缩，而在困难任务中则存在挑战。为此，我们引入了一种新的调整和推理策略，使模型能够生成不同长度的推理链。我们在参数空间中识别出一个能够有效控制生成CoT长度的方向，并构建了从长到短的推理链数据集。实验结果表明，CoT-Valve在控制能力和压缩能力上表现优越，相较于传统的提示控制方法，在减少GSM8K和AIME任务中的推理链长短时，仅带来了轻微的性能下降。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 14 Feb 2025 00:16:30 GMT</pubDate>
</item>
<item>
<title>高质量合成多模态数据及其在mmE5模型中的应用</title>
<link>https://arxiv.org/abs/2502.08468</link>
<guid>https://arxiv.org/abs/2502.08468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了高质量合成多模态数据的标准及其在mmE5模型中的应用。</p><br /><br /><p><strong>摘要：</strong> 多模态嵌入模型因能够将文本和图像等不同模态的数据映射到统一的表示空间而备受关注。然而，有限的标注多模态数据常常限制了嵌入性能。本文提出了高质量合成多模态数据的三个标准：广泛的范围、稳健的跨模态对齐和高保真度。我们基于这些原则合成了涵盖多任务、多模态和多语言的高质量数据集，并通过多模态大语言模型进行深度思考生成。同时，该数据集结合了真实世界的图像与准确的文本，确保保真度，经过自我评估和精炼。利用这些高质量合成和标注数据，我们训练了多模态多语言E5模型mmE5，并在MMEB基准测试上取得了最先进的表现，在XTD基准测试中展现了卓越的多语言性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:32:15 GMT</pubDate>
</item>
<item>
<title>构建评估框架以提升多模态大型语言模型在体感代理中的应用</title>
<link>https://arxiv.org/abs/2502.09560</link>
<guid>https://arxiv.org/abs/2502.09560</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EmbodiedBench评估框架以提升多模态代理的实际应用能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了EmbodiedBench，一个旨在评估基于多模态大型语言模型（MLLM）的体感代理的全面评估框架。虽然语言为中心的体感代理得到了较多关注，但基于MLLM的代理仍未得到充分探索。EmbodiedBench覆盖了1,128个任务，任务内容多样，从高层的语义任务到低层的原子操作，包括空间意识、视觉感知等六个核心能力的评估。通过对13个领先的MLLM进行测试，我们发现尽管MLLM在高层任务中表现良好，但在低层操作中面临挑战，最佳模型GPT-4o的平均得分仅为28.9%。EmbodiedBench为研究人员提供了一个标准化的评估平台，不仅揭示了当前的挑战，也为提升MLLM-based体感代理的研究提供了宝贵的见解。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09560" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:23:42 GMT</pubDate>
</item>
<item>
<title>Skrr: 提高文本编码器在T2I扩散模型中的内存效率</title>
<link>https://arxiv.org/abs/2502.08690</link>
<guid>https://arxiv.org/abs/2502.08690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skrr通过选择性跳过和重用层来优化文本编码器的内存使用。</p><br /><br /><p><strong>摘要：</strong> 在文本到图像（T2I）扩散模型中，文本编码器在从文本提示生成高质量图像方面表现出色，但其内存消耗却是去噪模块的八倍。本研究提出了一种名为Skip and Re-use layers (Skrr)的修剪策略，旨在针对T2I任务优化文本编码器的内存使用。Skrr通过选择性地跳过或重用变换器块中的某些层，利用其固有冗余，显著减少内存占用而不影响性能。实验结果表明，Skrr在高稀疏级别下仍能维持与原始模型相当的图像质量，且在多个评估指标（包括FID、CLIP、DreamSim和GenEval分数）上实现了最先进的内存效率，超越了现有的逐块修剪方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 23:10:44 GMT</pubDate>
</item>
<item>
<title>InfiniteHiP：高效的长序列推理框架</title>
<link>https://arxiv.org/abs/2502.08910</link>
<guid>https://arxiv.org/abs/2502.08910</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出InfiniteHiP框架，提高长序列处理速度和效率。</p><br /><br /><p><strong>摘要：</strong> 在现代的大型语言模型中，处理超长上下文面临诸多挑战，如推理速度缓慢和内存消耗增加。为此，我们提出了InfiniteHiP，一个新的高效推理框架，采用模块化的层次化token剪枝算法，动态去除无关的上下文token，从而加速处理。此外，该方法允许通过根据内部注意模式选择性地应用不同的RoPE调整方法，来实现更长序列的概括。我们还在推理过程中将键值缓存转移至主内存，显著降低了GPU内存压力，实现了在单个L40s 48GB GPU上处理高达300万token的能力，相比之下是原本的3倍，无任何上下文信息的永久丢失。InfinityHiP在处理100万token上下文时，实现了18.95倍的注意力解码加速，并且不需要额外的训练。通过在SGLang框架中的实现，我们通过广泛评估展示了其有效性和实用性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08910" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:57:03 GMT</pubDate>
</item>
<item>
<title>TripoSG：高保真3D形状生成的新流行扩散模型</title>
<link>https://arxiv.org/abs/2502.06608</link>
<guid>https://arxiv.org/abs/2502.06608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TripoSG通过扩散技术实现高保真3D形状生成，提升生成质量与通用性。</p><br /><br /><p><strong>摘要：</strong> 随着扩散技术的进步，图像和视频生成质量得以显著提升，但3D形状生成技术仍面临规模和复杂性限制。本文提出TripoSG，一个新型的形状扩散范式，能生成高保真的3D网格，并精准对应输入图像。TripoSG的关键创新包括：1) 一种针对3D形状生成的大规模规范流变换器，基于大量高质量数据进行训练，以达到最佳保真度；2) 结合SDF、法向量和Eikonal损失的混合监督训练策略，显著提升3D重建性能；3) 一条生成200万高质量3D样本的数据处理管道，强调数据质量和数量在训练3D生成模型中的重要性。实验验证了各组成部分的有效性，使TripoSG在3D形状生成方面实现了领先性能，3D形状细节更为丰富，且对输入图像的保真度极高。此外，TripoSG能从多样的图像风格和内容中生成3D模型，展示了强大的通用性。为了推动3D生成领域的发展，我们将公开该模型。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:56:23 GMT</pubDate>
</item>
<item>
<title>提升泰语大语言模型推理能力的方法研究</title>
<link>https://arxiv.org/abs/2502.09056</link>
<guid>https://arxiv.org/abs/2502.09056</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨了增强泰语大语言模型推理能力的方法。</p><br /><br /><p><strong>摘要：</strong> 本文研究了数据选择和模型合并的方法，旨在将先进的推理能力（如DeepSeek R1）融入语言特定的大语言模型（LLMs），特别关注泰语LLM。我们的目标是在保持语言特性的同时，提升语言特定LLMs的推理能力。DeepSeek R1在推理方面表现出色，但主要集中于英语和中文等高资源语言，导致低资源语言的表现受限。文章展示了仅使用公开数据集和120美元的计算预算，就能提升语言特定LLMs的推理能力，使其水平与DeepSeek R1相当，同时不影响其在目标语言任务上的表现。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09056" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 22:01:48 GMT</pubDate>
</item>
<item>
<title>对大型语言模型理解能力的系统评估</title>
<link>https://arxiv.org/abs/2502.08946</link>
<guid>https://arxiv.org/abs/2502.08946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，LLMs在理解物理概念任务上落后于人类约40%。</p><br /><br /><p><strong>摘要：</strong> 本文系统性地探讨了大型语言模型（LLMs）是否真正理解其所表达的内容，提出了一项名为PhysiCo的物理概念理解任务。该任务通过网格格式输入减轻了记忆化问题，网格表示不同层次的理解，包括核心现象、应用示例以及与其他抽象模式的类比。研究结果表明，尽管当前最先进的LLMs（如GPT-4o、o1和Gemini 2.0）在自然语言中能够描述和识别相关概念，但在此网格任务中表现显著低于人类，落后约40%。此外，LLMs的表现不佳源于任务的内在难度，而非网格格式的陌生性，因为在相同格式的数据上进行的上下文学习和微调对其表现几乎没有提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:59:28 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的逻辑推理能力研究</title>
<link>https://arxiv.org/abs/2502.09100</link>
<guid>https://arxiv.org/abs/2502.09100</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了大型语言模型在逻辑推理方面的最新进展。</p><br /><br /><p><strong>摘要：</strong> 随着OpenAI o3和DeepSeek-R1等先进推理模型的出现，大型语言模型（LLMs）展示了显著的推理能力，但其进行严谨逻辑推理的能力仍然存在疑问。本文综述了LLMs中逻辑推理的最新进展，探讨了其理论基础及评估推理能力的基准。我们分析了在不同推理范式（包括演绎、归纳、溯因和类比）下的现有能力，并评估了提升推理表现的策略，包括数据中心调优、强化学习、解码策略和神经-符号方法。最后，本文提出了未来的研究方向，强调进一步探索以增强人工智能系统中的逻辑推理能力的必要性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09100" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:55:58 GMT</pubDate>
</item>
<item>
<title>SelfCite：一种自监督方法生成高质量句子级引用</title>
<link>https://arxiv.org/abs/2502.09604</link>
<guid>https://arxiv.org/abs/2502.09604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SelfCite以自监督方式提升LLMs生成高质量引用的能力。</p><br /><br /><p><strong>摘要：</strong> SelfCite是一种新颖的自监督方法，旨在提高大型语言模型(LLMs)生成高质量细粒度句子级引用的能力。该方法依赖于LLM自身提供的奖励信号，通过上下文的消融实验来判断引用的必要性。当引用文本被移除时，若应答变化则说明引用必要；而保留引用文本时，应答不变则说明已提供足够的信息。这一奖励信号可以有效指导推理过程中最佳抽样策略的实施，从而显著改善引用质量。此外，该信号也可以用于偏好优化，直接对模型进行微调，以生成更好的引用。在LongBench-Cite基准测试中，SelfCite在五个长文本问答任务上使引用F1值提升了多达5.3个百分点，展现了其有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.09604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 21:42:37 GMT</pubDate>
</item>
<item>
<title>基于GEMINI学习的医疗图像密集对比表示学习</title>
<link>https://arxiv.org/abs/2502.05282</link>
<guid>https://arxiv.org/abs/2502.05282</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出GEMINI学习以增强医疗图像的密集对比表示效率。</p><br /><br /><p><strong>摘要：</strong> 密集对比表示学习（DCRL）在医疗图像密集预测任务中显著提高了学习效率，但由于医疗图像的特殊性，往往会导致不可靠的对应关系发现，从而产生大量的错误匹配对（假阳性和假阴性）。为了解决这一问题，本文提出了一种名为GEMINI的学习框架，通过将同胚性先验嵌入到DCRL中，实现有效的对应关系发现。我们设计了可形变同胚学习（DHL），该方法通过建模医疗图像的同胚性来学习可变形映射，以在保持拓扑结构的前提下预测像素对应关系，有效减少匹配空间。还提出几何语义相似性（GSS），用于提取特征中的语义信息，从而量化对应学习的对齐度。通过这两种方法，GEMINI不仅提高了学习效率，同时构建可靠的正向匹配对。在多项实验中，我们在七个数据集上实现了比现有方法更优的结果，证明了我们方法的有效性和优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05282" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 14:57:40 GMT</pubDate>
</item>
<item>
<title>PDE-Controller: 利用大型语言模型控制偏微分方程系统</title>
<link>https://arxiv.org/abs/2502.00963</link>
<guid>https://arxiv.org/abs/2502.00963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PDE-Controller框架使LLMs能有效控制偏微分方程系统。</p><br /><br /><p><strong>摘要：</strong> PDE-Controller是一个新框架，旨在利用大型语言模型（LLMs）来控制偏微分方程（PDE）系统，充分利用其在应用数学中的潜力。该框架能够将非正式的自然语言指令转化为正式规范，并执行推理和规划步骤，从而提升PDE控制的实用性。为了实现这一目标，我们构建了一个综合解决方案，包含人类撰写的案例及200万条合成样本的数据集、数学推理模型和创新的评估指标，付出了相当大的努力。我们的实验表明，PDE-Controller在推理、自我形式化和程序合成方面，显著优于使用最新开源和GPT模型的提示方法，实现了PDE控制实用性提高62%的显著进展。通过缩小语言生成与PDE系统之间的差距，我们展示了LLMs在解决复杂科学与工程问题方面的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.00963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 11:41:16 GMT</pubDate>
</item>
<item>
<title>通过增强交叉注意机制实现大型模型知识传输至小型模型</title>
<link>https://arxiv.org/abs/2502.08213</link>
<guid>https://arxiv.org/abs/2502.08213</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出通过增强交叉注意机制实现大模型向小模型的知识传输。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种LLM模块架构，利用增强交叉注意机制将知识从大型预训练模型传递给小型模型。具体而言，通过冻结Qwen2-1.5B模型，并将其表示通过特制注意力层传递给GPT-Neo-125M模型，从而在有限的计算资源下进行训练。在Bespoke-Stratos-17k数据集上的实验结果表明，经过15个训练周期后，结合模型生成的响应质量可与蒸馏方法相媲美。文中讨论了模块化方法的优势，提供了输入查询示例和比较分析，并展望了该方法的进一步扩展前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08213" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 05:48:33 GMT</pubDate>
</item>
<item>
<title>改进长效目标优化的语言模型探索方法</title>
<link>https://arxiv.org/abs/2502.06533</link>
<guid>https://arxiv.org/abs/2502.06533</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习改进语言模型在长效目标上的探索能力。</p><br /><br /><p><strong>摘要：</strong> 在大型语言模型（LLMs）发展过程中，实现长效目标是一项重要挑战。本文研究了如何通过强化学习（RL）对预训练的LLMs进行微调，以优化特定目标的解决方案。探索过程中需权衡发现新方案与维持预训练模型基本能力的平衡，通常通过Kullback-Leibler（KL）惩罚来控制。我们通过对小型语言模型在简单算术任务上的探索动态进行研究，发现预训练程度对探索的影响，并强调了“关键标记”的重要性，这些标记对最终结果有显著影响。此外，我们提出了一种对KL惩罚的简单修改，旨在促进关键标记的探索，提升RL微调阶段的效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06533" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:47:28 GMT</pubDate>
</item>
<item>
<title>Animate Anyone 2: 结合环境语义的角色动画生成</title>
<link>https://arxiv.org/abs/2502.06145</link>
<guid>https://arxiv.org/abs/2502.06145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍Animate Anyone 2，提升角色与环境的动画一致性与合理性。</p><br /><br /><p><strong>摘要：</strong> 随着基于扩散模型的角色图像动画方法的发展，Animate Anyone 2应运而生，旨在改善角色与其环境之间的关系。与以往仅提取源视频的运动信号不同，Animate Anyone 2还捕捉环境的表现作为条件输入，这样可以在角色与环境之间建立更合理的关联。文章提出了一种形状无关的掩码策略，以更有效地描述角色与环境的关系。同时，引入了对象引导器用于提取交互对象的特征，并通过空间混合来增强特征注入，以提高对象交互的真实感。此外，作者还提出了姿势调节策略，使模型能够处理更多样化的运动模式。实验结果表明，该方法在动画生成方面具有显著的性能提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:45:43 GMT</pubDate>
</item>
<item>
<title>BenchMAX：一种多语言评估基准以测量语言模型的高级能力</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BenchMAX是一个新兴的多语言评估基准，专注于大型语言模型的高级能力测量。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型(LLMs)的快速发展，传统的多语言基准主要集中于简单理解任务，未能充分评估它们在指令遵循、推理、长文本理解和代码生成等高级能力上的表现。为了解决这一问题，我们引入了BenchMAX，它是一个多方式的多语言评估基准，能公平比较这些关键能力。该基准经过三个不同的母语评审者对所有任务中的样本进行独立标注，并在从英语机器翻译到另外16种语言后进行测试。全面的实验表明，核心能力在不同语言间的有效性存在差异，这些差距不能仅通过扩大模型规模来弥补。BenchMAX为多语言模型的发展提供了一个综合评估平台，数据集和代码均已公开访问。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:34:47 GMT</pubDate>
</item>
<item>
<title>优化模型合并提升大语言模型性能</title>
<link>https://arxiv.org/abs/2502.04411</link>
<guid>https://arxiv.org/abs/2502.04411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过分层合并与任务级路由技术提升大语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本研究针对将不同任务微调后的大语言模型（LLMs）合并为更强模型时参数冲突导致性能下降的问题，提出了一种新的优化方法。研究发现，不同层级的模型存在不同程度的参数冲突。因此，本文提出对参数冲突较小的层进行平均，而对参数冲突明显的层采用新颖的任务级专家路由。同时，为了降低存储成本，借鉴任务算术稀疏性，本文将多个微调专家解耦成一个稠密专家和若干个稀疏专家。在处理分布外样本时，依据任务不确定性选择并合并合适的专家。通过在LLaMA和Qwen等多个参数规模的模型上进行大规模实验，结果表明，该方法在真实世界推理任务中始终能实现显著的性能提升，并且所需的系统成本低于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 03:30:35 GMT</pubDate>
</item>
<item>
<title>动态安全框架优化语言模型推理安全</title>
<link>https://arxiv.org/abs/2502.07985</link>
<guid>https://arxiv.org/abs/2502.07985</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种在推理时优化语言模型安全性的动态安全框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的动态安全框架，用于在推理时优化语言模型的安全性 reasoning，且不需修改模型权重。该方法基于近期自我批评技术的进展，利用一种元批评机制，迭代地更新安全提示（称为规范），以自适应地驱动批评和修正过程。此种测试时优化不仅提高了模型应对对抗性越狱请求的能力，还在避免道德伤害和追求诚实回答等各种安全相关任务中表现出色。通过在多个语言模型上的实证评估，结果显示动态优化的安全提示明显优于固定系统提示和静态自我批评防御策略，显著提高了安全评分。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07985" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:47:30 GMT</pubDate>
</item>
<item>
<title>WorldGUI：一种新颖的GUI基准用于真实用户交互评估</title>
<link>https://arxiv.org/abs/2502.08047</link>
<guid>https://arxiv.org/abs/2502.08047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WorldGUI基准，评估GUI任务中的初始状态敏感性及其影响。</p><br /><br /><p><strong>摘要：</strong> 当前的GUI代理在元素定位方面表现出色，但规划依然面临巨大挑战，尤其是对于环境初始状态的敏感性。些微的初始状态差异，如目标软件未打开或界面未处于默认状态，往往导致规划错误，这在真实用户场景中普遍存在，但现有基准未能对此进行评估。为此，本文提出WorldGUI，这是一种新颖的GUI基准，设计了具有多种初始状态的GUI任务，以模拟真实的计算机用户交互。该基准涵盖了10款流行软件应用的多种任务，包括PowerPoint、VSCode和Adobe Acrobat。此外，为应对动态GUI自动化任务的挑战，我们提出了GUI-Thinker，一个整体框架，利用批判机制，能够有效管理GUI交互的不确定性和复杂性。实验结果显示，GUI-Thinker在WorldGUI任务上相比Claude-3.5（计算机使用）成功率提高了14.9%，突显了基于批判性思维的框架在提升GUI自动化中的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 01:39:08 GMT</pubDate>
</item>
<item>
<title>建立值得信赖的检索增强生成（RAG）系统的综合路线图</title>
<link>https://arxiv.org/abs/2502.06872</link>
<guid>https://arxiv.org/abs/2502.06872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升RAG系统可信度的五大关键视角。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）是一种先进技术，旨在解决人工智能生成内容（AIGC）的挑战，通过将上下文检索集成到内容生成中，RAG提供可靠且最新的外部知识，减少幻觉，并确保跨任务的一致相关性。然而，尽管RAG潜力巨大，最新研究显示其也引入了新的风险，如鲁棒性问题、隐私关注、对抗攻击和责任问题。为应对这些风险，本文提出了一个关于构建值得信赖的RAG系统的综合路线图，围绕可靠性、隐私、安全性、公平性、可解释性和责任感五大关键视角展开讨论，提供一般框架和分类法，以帮助理解当前挑战、评估现有解决方案及确定未来研究方向。同时，强调值得信赖的RAG系统在实际应用中的重要影响，以鼓励更广泛的采用和创新。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:06:04 GMT</pubDate>
</item>
<item>
<title>NoLiMa基准评估长文本环境下大语言模型的检索能力</title>
<link>https://arxiv.org/abs/2502.05167</link>
<guid>https://arxiv.org/abs/2502.05167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究通过NoLiMa基准评估LLMs在长文本中信息检索的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了NoLiMa基准，旨在评估当前大型语言模型(LLMs)在长文本环境中的信息检索能力。NIAH测试是一种常用的方法，通过在冗长的上下文中检索相关信息来衡量模型性能。与传统方法不同，NoLiMa设计了一个针集，其中问题与信息的词汇重叠最小，这要求模型推断潜在的关联以定位信息。研究评估了12种声称支持至少128K标记上下文的流行LLMs，结果显示在短文本(<1K)中它们表现良好，但随着上下文长度的增加，表现显著下降。在32K上下文中，10个模型的表现低于50%的短文本基准，并且即便是表现最好的GPT-4o，基准从99.3%降至69.7%。分析表明，长文本中缺乏字面匹配使注意力机制面临更大困难，进而影响信息的检索能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 13 Feb 2025 00:04:29 GMT</pubDate>
</item>
<item>
<title>TextAtlas5M：评估长文本条件下的图像生成的新数据集</title>
<link>https://arxiv.org/abs/2502.07870</link>
<guid>https://arxiv.org/abs/2502.07870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TextAtlas5M是一个用于评估长文本条件下图像生成的新数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，文本条件下的图像生成受到了广泛关注，尤其是在处理复杂的长文本提示方面。尽管取得了一定进展，现有数据集主要专注于短文本，使得长文本图像生成仍然面临挑战。为了填补这一空白，本文提出了TextAtlas5M，这是一个专门设计用于评估长文本渲染的新数据集，涵盖500万张长文本生成及收集的图像。数据集内容多样，支持对大型生成模型在长文本图像生成上的综合评估。此外，我们精心策划了3000个经过人工改进的测试集TextAtlasEval，建立了长文本条件生成方面的重要基准。评估结果显示，TextAtlasEval基准给当前最先进的专有模型（如GPT4o与DallE-3）带来了显著挑战，而开源模型的性能差距更大。这些证据使TextAtlas5M成为未来文本条件图像生成模型训练和评估的宝贵数据集。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:50:07 GMT</pubDate>
</item>
<item>
<title>Light-A-Video：无训练的视频重光照方法</title>
<link>https://arxiv.org/abs/2502.08590</link>
<guid>https://arxiv.org/abs/2502.08590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Light-A-Video 提供了一种无训练的视频重光照解决方案，提升了时间一致性。</p><br /><br /><p><strong>摘要：</strong> 近日，图像重光照模型的进步主要得益于大规模数据集和预训练扩散模型，使得一致性照明得以实现。然而，视频重光照仍面临训练成本过高和高质量多样化视频数据集稀缺的挑战。本研究提出了 Light-A-Video，这是一种无训练的方法，旨在实现时间平滑的视频重光照。通过设计一致性光照注意模块（CLA），加强了自注意力层内帧间的交互，从而稳定背景光源的生成。此外，我们利用光传输独立性的物理原则，采用渐进光融合（PLF）策略，在源视频的外观和重光照外观之间进行线性混合，以确保照明的平滑过渡。实验证明，Light-A-Video在保持图像质量的同时，提高了重光照视频的时间一致性，确保了帧间光照的连贯性过渡。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:56 GMT</pubDate>
</item>
<item>
<title>LASP-2: 提升线性注意力变换器模型的序列并行ism方法</title>
<link>https://arxiv.org/abs/2502.07563</link>
<guid>https://arxiv.org/abs/2502.07563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LASP-2 提高了线性注意力模型的训练速度和并行性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LASP-2，这是一种新型序列并行（SP）方法，旨在提升线性注意力变换器模型在处理超长输入序列时的通信和计算并行性。与之前的LASP相比，LASP-2重新审视了线性注意力层对于SP的最小通信需求，并重组了整体的通信-计算工作流。此方法仅需对中间内存状态进行一次集体通信，显著提高了通信与计算的并行性及其重叠。另外，LASP-2延伸至LASP-2H，对标准注意力模块进行类似的通信重设计，为融合线性与标准注意力层的混合模型提供了高效的SP解决方案。在对Linear-Llama3模型的评估中，LASP-2实现了相对LASP快15.2%的训练速度提升，相比Ring Attention则提升了36.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:47:31 GMT</pubDate>
</item>
<item>
<title>CoCoMix：结合离散的下一个标记预测与连续概念的预训练框架</title>
<link>https://arxiv.org/abs/2502.08524</link>
<guid>https://arxiv.org/abs/2502.08524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CoCoMix是一种新颖的预训练框架，通过概念学习提高语言模型性能。</p><br /><br /><p><strong>摘要：</strong> CoCoMix是一种新提出的预训练框架，它结合离散的下一个标记预测与连续概念学习。通过使用预训练的稀疏自编码器，CoCoMix能够预测连续概念，并将其与模型的隐藏状态混合。在多个基准测试中，包括语言建模和下游推理任务，实验结果表明CoCoMix在样本效率上表现更佳，并且在性能上一致优于传统的下一个标记预测、知识蒸馏以及插入暂停标记的方法。研究发现，概念学习和交错处理的结合对于性能提升至关重要。此外，CoCoMix也增强了模型的可解释性和可引导性，可以直接检查和修改预测的概念，从而提供一种透明的方式来指导模型的内部推理过程。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:42:44 GMT</pubDate>
</item>
<item>
<title>基于计算预算的模型蒸馏性能估计研究</title>
<link>https://arxiv.org/abs/2502.08606</link>
<guid>https://arxiv.org/abs/2502.08606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一个蒸馏规模法，优化了模型性能与计算预算的分配。</p><br /><br /><p><strong>摘要：</strong> 本研究提供了一种蒸馏规模法，以估算基于计算预算的蒸馏模型性能，并优化了计算在教师和学生模型之间的分配。研究成果降低了大规模使用蒸馏的风险，确保在分配计算时能够最大化学生模型的性能。我们提供了计算最优的蒸馏方案，适用于存在教师模型或需要训练教师模型的情况。如果有多个学生模型进行蒸馏且已有教师模型，则蒸馏的效果优于监督预训练，直到计算水平随着学生规模的增加而可预测性地增长；而如果只有一个学生需要蒸馏且教师也需要训练，则应选择监督学习。此外，基于大规模研究结果，我们提供了关于蒸馏的新见解，增进了对蒸馏的理解并为实验设计提供了指导。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 23:41:41 GMT</pubDate>
</item>
<item>
<title>SARChat-Bench-2M: A Multi-Task Vision-Language Benchmark for SAR Image Interpretation</title>
<link>https://arxiv.org/abs/2502.08168</link>
<guid>https://arxiv.org/abs/2502.08168</guid>
<content:encoded><![CDATA[
In the field of synthetic aperture radar (SAR) remote sensing image interpretation, although Vision language models (VLMs) have made remarkable progress in natural language processing and image understanding, their applications remain limited in professional domains due to insufficient domain expertise. This paper innovatively proposes the first large-scale multimodal dialogue dataset for SAR images, named SARChat-2M, which contains approximately 2 million high-quality image-text pairs, encompasses diverse scenarios with detailed target annotations. This dataset not only supports several key tasks such as visual understanding and object detection tasks, but also has unique innovative aspects: this study develop a visual-language dataset and benchmark for the SAR domain, enabling and evaluating VLMs' capabilities in SAR image interpretation, which provides a paradigmatic framework for constructing multimodal datasets across various remote sensing vertical domains. Through experiments on 16 mainstream VLMs, the effectiveness of the dataset has been fully verified, and the first multi-task dialogue benchmark in the SAR field has been successfully established. The project will be released at https://github.com/JimmyMa99/SARChat, aiming to promote the in-depth development and wide application of SAR visual language models.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:57:30 GMT</pubDate>
</item>
<item>
<title>CineMaster：3D感知可控文本到视频生成框架</title>
<link>https://arxiv.org/abs/2502.08639</link>
<guid>https://arxiv.org/abs/2502.08639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CineMaster框架实现了3D感知与可控的文本到视频生成。</p><br /><br /><p><strong>摘要：</strong> CineMaster是一个创新的3D感知可控文本到视频生成框架，旨在为用户提供与专业导演相似的控制能力，包括场景中的物体精确放置、对象和相机在3D空间中的灵活操控，以及对渲染帧的直观布局控制。该框架分为两个阶段：第一阶段通过交互式工作流程帮助用户在3D空间内构建条件信号；第二阶段利用生成的深度图、相机轨迹和对象类别标签，指导文本到视频扩散模型生成用户意图的视频内容。此外，CineMaster还建立了自动化数据注释管道，以从大规模视频数据中提取3D边界框和相机轨迹，克服野外数据集稀缺的问题。实验结果表明，CineMaster在3D感知文本到视频生成方面明显优于现有方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:55:44 GMT</pubDate>
</item>
<item>
<title>基于下一块预测的半自回归视频生成框架</title>
<link>https://arxiv.org/abs/2502.07737</link>
<guid>https://arxiv.org/abs/2502.07737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种半自回归框架，显著提升视频生成效率与质量。</p><br /><br /><p><strong>摘要：</strong> 本研究提出了一种名为下一块预测（NBP）的半自回归（semi-AR）框架，用于视频生成。通过将视频内容均匀分解成等大小的块（如行或帧），我们将生成单位从单个令牌转变为块，使当前块中的每个令牌可以同时预测下一个块中对应的令牌。这种框架在每个块内应用双向注意力，捕捉到更强的空间依赖性。通过并行预测多个令牌，NBP显著减少了生成步骤，从而提高了推理速度和效率。我们的模型在UCF101和K600数据集上的FVD分数分别达到103.3和25.5，平均超越传统NTP模型4.4。此外，由于推理步骤减少，NBP模型的生成速度达到每秒8.89帧（128x128分辨率），实现了11倍的加速。我们还探索了从700M到3B参数的模型规模，发现生成质量显著提升，UCF101和K600上的FVD分数分别从103.3降至55.3和25.5降至19.5，展示了我们方法的可扩展性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:48:00 GMT</pubDate>
</item>
<item>
<title>评估大型语言模型在金融推理中的能力与改进</title>
<link>https://arxiv.org/abs/2502.08127</link>
<guid>https://arxiv.org/abs/2502.08127</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估大型语言模型在金融推理任务中的表现及改进方法。</p><br /><br /><p><strong>摘要：</strong> 本研究综合评估了16种强大推理和通用大型语言模型(LLMs)在三项复杂金融任务上的表现，包括金融文本、表格数据和方程的处理，重点考察数值推理、表格解读、金融术语理解、长上下文处理与基于方程的问题解决能力。研究结果表明，尽管更好的数据集和预训练可以提升金融推理能力，但如CoT微调等一般性增强并不总是有效。此外，所有推理策略在长上下文和多表格任务中的性能提升面临挑战。为了解决这些限制，研究开发了基于Llama-3.1-8B-Instruct的金融推理增强模型，通过CoT微调和领域特定推理路径的强化学习，简单的单金融数据集微调使模型在任务中平均实现了10%的一致性提升，超越了所有8B模型及Llama3-70B-Instruct和Llama3.1-70B-Instruct。研究强调了金融任务中适应特定领域的必要性，并指明了未来的研究方向，如多表格推理和金融术语理解。所有数据集、模型和代码都已公开，并引入了一个基准排行榜以促进未来的数据集和模型的评估。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.08127" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:45:28 GMT</pubDate>
</item>
<item>
<title>DPO-Shift: Shifting the Distribution of Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.07599</link>
<guid>https://arxiv.org/abs/2502.07599</guid>
<content:encoded><![CDATA[
Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \method to controllably shift the distribution of the chosen probability. Then, we show that \method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift.
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:43:42 GMT</pubDate>
</item>
<item>
<title>TransMLA：提升语言模型通信效率的新方法</title>
<link>https://arxiv.org/abs/2502.07864</link>
<guid>https://arxiv.org/abs/2502.07864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TransMLA方法有效提升语言模型的通信效率与推理速度。</p><br /><br /><p><strong>摘要：</strong> 现代的大型语言模型在当前硬件上经常面临沟通瓶颈，而不仅仅是计算限制。多头潜在注意力（MLA）通过在关键值（KV）层使用低秩矩阵，允许压缩的潜在KV状态被缓存，从而大幅度减少KV缓存大小，提升推理速度。尽管MLA在Deepseek V2/V3/R1中表现出效率和有效性，许多主要模型供应商仍然依赖于组查询注意力（GQA）。本文展示了GQA可以通过MLA进行始终保持相同KV缓存开销的表示，但反之则不成立。为促进MLA的广泛应用，我们引入了**TransMLA**，一种将广泛使用的基于GQA的预训练模型（如LLaMA、Qwen、Mixtral）转换为基于MLA模型的后训练方法。转换后的模型可以在不增加KV缓存大小的情况下经过额外训练来提高表达能力。此外，我们还计划开发MLA特定的推理加速技术，以保持转化模型的低延迟，进一步提升Deepseek R1的提炼效率。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 21:41:19 GMT</pubDate>
</item>
<item>
<title>可学习的合规性放弃：提高大规模语言模型的决策可靠性</title>
<link>https://arxiv.org/abs/2502.06884</link>
<guid>https://arxiv.org/abs/2502.06884</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的合规性放弃方法，以提升LLM/VLM的可靠决策能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为可学习合规性放弃的方法，旨在提高安全关键应用中大规模语言模型（LLM）和视觉-语言模型（VLM）的决策可靠性。传统的合规性预测方法在阈值设定上过于静态，难以适应任务复杂性和数据分布的变化。为此，本文将强化学习融入合规性预测，动态优化放弃阈值，从而在最小化预测集大小的同时，确保可靠的覆盖率。经过在多个LLM/VLM基准上的广泛评估，研究表明，该方法在准确性、幻觉检测的AUROC和不确定性引导选择生成方面均优于现有方法，并显著降低了校准误差。这些改进在多种模型和数据集上均表现出色，且始终满足90%的覆盖目标，确立了该方法作为安全关键应用中可靠决策的更有效和灵活的解决方案。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06884" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 18:40:34 GMT</pubDate>
</item>
<item>
<title>Pippo: 从单张照片生成高分辨率密集视频的多视角扩散模型</title>
<link>https://arxiv.org/abs/2502.07785</link>
<guid>https://arxiv.org/abs/2502.07785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Pippo模型可从一张照片生成高分辨率的人物视频，展现出色的多视角表现。</p><br /><br /><p><strong>摘要：</strong> Pippo是一种先进的生成模型，能够仅通过一张随意拍摄的照片生成分辨率达到1000的密集人物视频。该模型采用了多视角扩散变换器，且无需额外输入，如参数模型或图像拍摄的相机参数。欲使模型有效学习，Pippo在与3亿幅无标签人像图像预训练后，进行多视角的中期训练与后期训练，快速吸收工作室数据集。中期训练中，用于去噪的低分辨率视图数量可达48个；后期训练则使用像素对齐控制，提升3D一致性的生成。在推理阶段，Pippo通过注意力偏置技巧，能够生成超过训练时5倍的视图数量。此外，团队还提出了一种改进的3D一致性评估标准，表明Pippo在单图像多视角生成人物方面的优越性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 13:41:46 GMT</pubDate>
</item>
<item>
<title>Hypencoder：一种新型请求编码器提升文档检索性能</title>
<link>https://arxiv.org/abs/2502.05364</link>
<guid>https://arxiv.org/abs/2502.05364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Hypencoder，一个基于神经网络的请求编码器，显著提升文档检索性能。</p><br /><br /><p><strong>摘要：</strong> 传统的检索模型通常依赖向量内积来生成查询与文档之间的相关性评分，限制了评分的表现力。本文提出一种新范式，使用小型神经网络作为学习的相关性函数，而非生成向量来表示查询。该神经网络以文档的表示为输入，输出标量相关性评分。我们应用超网络（hypernetwork）生成该神经网络的权重，称之为Hypencoder。通过在领域内的搜索任务进行实验，Hypencoder的表现显著超越了强大的稠密检索模型，并在重排序模型和规模更大的模型中取得了更高的指标。此外，Hypencoder在领域外检索任务中也展现出良好的泛化能力。为进一步评估其能力，我们对一系列困难检索任务进行了评测，包括“想不起来的检索”和“遵循指令的检索”，结果表明，与标准检索任务相比，性能差距显著扩大。最后，我们实现了一种近似搜索算法，展示该模型能够在60毫秒内搜索880万份文档。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 10:35:08 GMT</pubDate>
</item>
<item>
<title>Goedel-Prover：开源自动化数学证明生成的最优语言模型</title>
<link>https://arxiv.org/abs/2502.07640</link>
<guid>https://arxiv.org/abs/2502.07640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Goedel-Prover是一个实现数学证明生成最佳性能的开源语言模型。</p><br /><br /><p><strong>摘要：</strong> Goedel-Prover是一种开源大型语言模型，专门用于自动化数学问题的形式化证明生成，达到了该领域的最优性能。为了解决形式化数学语句和证明稀缺的问题，研究团队训练了语句形式化工具，将自然语言数学问题转化为形式语言（Lean 4），创建了一个包含164万个形式语句的数据集。利用大型语言模型，检查这些形式语句准确保留了原始自然语言问题的内容。然后，团队通过训练一系列证明者，迭代构建了一个大型形式证明数据集。每个新的证明者都成功证明了前一个证明者无法解决的许多语句，并将这些新证明追加到下一轮训练集中。最终的证明者在整体证明生成中超越了现有所有开源模型，在miniF2F基准测试中达到了57.6%的成功率，领先于之前的最佳开源模型7.6%。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:56:56 GMT</pubDate>
</item>
<item>
<title>通过稀疏自编码器理解与控制视觉模型</title>
<link>https://arxiv.org/abs/2502.06755</link>
<guid>https://arxiv.org/abs/2502.06755</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种框架，通过稀疏自编码器理解和控制视觉模型。</p><br /><br /><p><strong>摘要：</strong> 为了深入理解视觉模型，我们不仅需要解释其学习的特征，还需通过控制实验验证这些解释。当前的方法要么提供可解释的特征但不能测试其因果影响，要么允许模型编辑但没有可解释的控制。我们提出了一个统一框架，利用稀疏自编码器（SAEs）弥补这一空白，能够发现可人类解释的视觉特征，并精确操纵这些特征以测试模型行为的假设。通过对最先进的视觉模型应用该方法，我们揭示了不同预训练目标的模型在语义抽象学习上的关键差异，并展示了我们框架在多个视觉任务中的实际应用。我们的研究表明，SAEs能够可靠地识别和操纵可解释的视觉特征，而无需对模型进行重新训练，这为理解和控制视觉模型行为提供了强大的工具。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06755" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 09:54:39 GMT</pubDate>
</item>
<item>
<title>基于链式选片的长视频理解优化</title>
<link>https://arxiv.org/abs/2502.06428</link>
<guid>https://arxiv.org/abs/2502.06428</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出链式选片方法以优化长视频的任务相关镜头选择。</p><br /><br /><p><strong>摘要：</strong> 本研究针对多模态大语言模型在处理长视频时面临的视觉token过多问题，提出了链式选片（CoS）方法。传统的视频采样方法难以平衡关键细节与冗余内容的选择，导致模型对视频理解的偏差。CoS方法将镜头选择视为测试时视觉提示的优化，通过优化镜头与任务的对齐来选择适合视频理解的镜头。其核心包括：一个二元视频摘要机制，用于发现任务相关镜头，以及一个视频共推理模块，利用二元编码将相关镜头与不相关镜头进行学习对齐。实验结果表明，CoS在多个基线和数据集上的有效性和适应性得到验证。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06428" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:51:18 GMT</pubDate>
</item>
<item>
<title>深入探讨模型架构与超参数对缩放法则的影响</title>
<link>https://arxiv.org/abs/2502.06857</link>
<guid>https://arxiv.org/abs/2502.06857</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究探讨模型架构及超参数对缩放法则的影响，发布开放数据集Gemstones。</p><br /><br /><p><strong>摘要：</strong> 本文研究使用广泛的模型架构和超参数选择来探讨缩放法则的适用性，强调这些选择对结果中的建议产生的重要影响。作为研究的主要成果，我们发布了Gemstones，这是迄今为止最全面的开源缩放法则数据集，包含超过4000个检查点，这些变换器模型的参数量高达20亿，采用不同的学习率、冷却周期和架构形状进行训练。我们的数据集支持更复杂的缩放研究，例如预测语言建模性能与模型宽度和深度的关系。通过分析模型组合的各个方面，我们发现缩放法则的建议对实验设计过程和使用的具体模型检查点非常敏感。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06857" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:18:08 GMT</pubDate>
</item>
<item>
<title>基于检索增强生成的金融时间序列预测框架</title>
<link>https://arxiv.org/abs/2502.05878</link>
<guid>https://arxiv.org/abs/2502.05878</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的框架，通过有效检索提升金融时间序列预测精度。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新的检索增强生成（RAG）框架，用于金融时间序列预测，解决了现有检索方法在处理复杂金融分析中的不足。该框架的核心创新包括：以一亿参数的大型语言模型（StockLLM）作为基础，采用新颖的候选选择方法并利用LLM反馈，以及通过最大化查询与历史重要序列相似度的训练目标，来提高检索效果。新构建的数据集融合了金融指标和历史股价，确保了FinSeer的有效训练与评估。实验结果表明，该RAG框架在BIGDATA22上实现了比StockLLM和随机检索更高的预测精度，FinSeer在现有检索方法中表现优异，准确率提高了8%。此研究突显了定制化检索模型在金融预测中的重要性，并为未来研究奠定了基础。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05878" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 07:10:24 GMT</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction：提升大型语言模型信息检索能力</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MEAP方法，显著提升大型语言模型的关键信息检索能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Mask-Enhanced Autoregressive Prediction（MEAP）的方法，通过将遮蔽语言建模（MLM）与下一词预测（NTP）结合，显著改善大型语言模型在关键信息检索和长上下文推理任务中的表现。MEAP随机遮蔽输入令牌的一小部分，然后利用解码器Transformer进行自回归的下一词预测，避免了对双向注意力或编码-解码架构的依赖，从而在预训练和推理阶段没有额外的计算开销。实验表明，MEAP在关键检索和长上下文推理任务上优于NTP，并且在常识推理任务上表现相当或更优。在有监督微调中，MEAP在“中间缺失”情境下表现优异，超越NTP达11.77个百分点。分析显示，MEAP能够提升可区分的注意力评分，增强了模型对与任务相关信号的关注，同时减少了外围上下文的影响。这些结果表明，MEAP是大型语言模型训练中的一种有前景的方法。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07490" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 06:55:30 GMT</pubDate>
</item>
<item>
<title>参数化技能扩展与组合框架PSEC的研究</title>
<link>https://arxiv.org/abs/2502.05932</link>
<guid>https://arxiv.org/abs/2502.05932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出参数化技能扩展与组合框架PSEC，用于提高自主智能体的技能扩展效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了参数化技能扩展与组合框架PSEC，旨在提升自主智能体在应对新挑战时的技能扩展效率。传统方法在扩展新技能时训练效率低下，未能充分利用已有知识。PSEC通过维护可管理的技能库，以低秩适配（LoRA）模块的形式逐步集成技能原语，实现参数高效的微调，促进灵活的技能扩展。此外，该框架通过合并不同技能的LoRA模块在参数空间中直接进行技能组合，利用技能间的共享信息高效编程新技能。文章还提出了一种上下文感知模块，能够动态激活不同技能，以协同处理新任务。经过在D4RL、DSRL基准和DeepMind控制套件上的实验，结果表明PSEC在有效利用已有知识以应对新挑战以及扩大技能库方面表现优越。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.05932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:53:50 GMT</pubDate>
</item>
<item>
<title>Eclair: 一种高效的文档级光学字符识别工具</title>
<link>https://arxiv.org/abs/2502.04223</link>
<guid>https://arxiv.org/abs/2502.04223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Eclair是一款优化的文本提取工具，旨在提高文档的理解和处理能力。</p><br /><br /><p><strong>摘要：</strong> Eclair是一种新型的光学字符识别（OCR）工具，专门设计用于处理多种文档类型。它不仅能从图像中提取文本，还能识别文档的结构和语义信息，如格式、公式、表格以及多个块的阅读顺序。这些功能对于文档查询、问题回答及训练大语言模型和视觉语言模型至关重要。通过引入人类标注的基准测试，Eclair在文档级OCR及语义分类上达到了最先进的准确率，超越了其他方法。此外，Eclair在多个现有基准上表现出色，展现了其广泛的应用潜力和强大的性能。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 04:25:54 GMT</pubDate>
</item>
<item>
<title>FailSafeQA：评估金融领域LLM的鲁棒性与上下文意识的新基准</title>
<link>https://arxiv.org/abs/2502.06329</link>
<guid>https://arxiv.org/abs/2502.06329</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出了一种新的金融基准FailSafeQA，旨在测试LLM的鲁棒性与上下文意识。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个新的长上下文金融基准FailSafeQA，旨在测试大型语言模型（LLM）在面对六种人机交互变化时的鲁棒性和上下文意识。我们聚焦于查询失败和上下文失败这两个案例研究，分别通过改变查询的领域专业性、完整性和语言准确性，及模拟上传降级、无关和空文档来进行测试。使用LLM-as-a-Judge方法及Qwen2.5-72B-Instruct模型，定义并计算24种现成模型的鲁棒性、上下文基础和合规性评分。结果表明，尽管某些模型在应对输入扰动方面表现出色，但在提供鲁棒答案时必须谨慎避免虚构信息的出现。Palmyra-Fin-128k-Instruct在合规性上表现最优，但在17%的测试案例中难以维持鲁棒预测；而OpenAI o3-mini则在41%案例中虚构了信息。这些结果表明，即使是高表现的模型仍有显著改进空间，并强调了FailSafeQA在金融应用中优化LLM可靠性的潜力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06329" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 02:51:41 GMT</pubDate>
</item>
<item>
<title>FocalCodec：一种高效的低比特率语音编解码器</title>
<link>https://arxiv.org/abs/2502.04465</link>
<guid>https://arxiv.org/abs/2502.04465</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FocalCodec是一种低比特率语音编解码器，解决现有方法的信息损失问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过自监督预训练在海量数据集上推动了自然语言处理的革命。受此启发，研究者开始将这种方法应用于语音，通过使用神经音频编解码器将连续音频离散为令牌。然而，现有方法面临高比特率、语义或声学信息损失等限制，并且为了同时捕获这两者，常常依赖于多代码簿设计，增加了下游任务的复杂性。为此，我们提出了FocalCodec，这是一种基于焦点调制的高效低比特率编解码器，利用单一二进制代码簿在0.16到0.65 kbps之间压缩语音。FocalCodec在语音重合成和声音转换方面提供与当前最先进技术相媲美的性能，同时有效处理多语言语音和噪声环境。在下游任务的评估中，FocalCodec能够保持足够的语义和声学信息，且非常适合生成建模。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.04465" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Feb 2025 01:31:44 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升大语言模型的代码生成能力</title>
<link>https://arxiv.org/abs/2502.03492</link>
<guid>https://arxiv.org/abs/2502.03492</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本研究提出CTRL框架以提升代码生成模型的输出效果。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在代码生成中的自我批评能力，提出了一种名为CTRL的框架，用于通过强化学习训练批评模型生成反馈，旨在最大化针对固定生成模型的修正效果，而无需人工干预。实验结果表明，使用CTRL训练的批评模型显著提高了通过率，并有效减轻了基础和更强生成模型的错误叠加。此外，这些批评模型还作为准确的生成奖励模型，支持测试时通过迭代批评修正进行扩展，在挑战性的代码生成基准上实现了高达106.1%的相对提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03492" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:55:37 GMT</pubDate>
</item>
<item>
<title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title>
<link>https://arxiv.org/abs/2502.07701</link>
<guid>https://arxiv.org/abs/2502.07701</guid>
<content:encoded><![CDATA[
In this technical report, we present Magic 1-For-1 (Magic141), an efficient video generation model with optimized memory consumption and inference latency. The key idea is simple: factorize the text-to-video generation task into two separate easier tasks for diffusion step distillation, namely text-to-image generation and image-to-video generation. We verify that with the same optimization algorithm, the image-to-video task is indeed easier to converge over the text-to-video task. We also explore a bag of optimization tricks to reduce the computational cost of training the image-to-video (I2V) models from three aspects: 1) model convergence speedup by using a multi-modal prior condition injection; 2) inference latency speed up by applying an adversarial step distillation, and 3) inference memory cost optimization with parameter sparsification. With those techniques, we are able to generate 5-second video clips within 3 seconds. By applying a test time sliding window, we are able to generate a minute-long video within one minute with significantly improved visual quality and motion dynamics, spending less than 1 second for generating 1 second video clips on average. We conduct a series of preliminary explorations to find out the optimal tradeoff between computational cost and video quality during diffusion step distillation and hope this could be a good foundation model for open-source explorations. The code and the model weights are available at https://github.com/DA-Group-PKU/Magic-1-For-1.
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:27:13 GMT</pubDate>
</item>
<item>
<title>Chameleon Benchmark Overfit Detector：评估大型语言模型的真实理解能力</title>
<link>https://arxiv.org/abs/2502.07445</link>
<guid>https://arxiv.org/abs/2502.07445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">C-BOD框架揭示LLM依赖于表面线索而非真实理解的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Chameleon Benchmark Overfit Detector (C-BOD)，一种通过参数化变换系统性扭曲基准提示的元评估框架，以检测大型语言模型（LLMs）的过拟合现象。通过对输入的重新措辞，同时保持其语义内容和标签，C-BOD可以揭示模型的性能是否源自记忆模式。我们对26个领先的LLM在MMLU基准上的评估显示，经过适度扰动后，平均表现下降2.15%，其中20个模型表现出统计显著差异。研究发现，基线准确率较高的模型在扰动下表现差异较大，而更大的LLM对重新措辞表现出更高的敏感性，这表明它们可能过于依赖固定提示模式。相比之下，Llama系列模型和较低基线准确率的模型显示出微不足道的降幅，暗示其对表面线索的依赖减少。此外，C-BOD具有数据集和模型无关的设计，便于集成到训练流程中，促进更强的语言理解。我们的发现挑战社区超越排行榜分数，重视LLM评估中的抗干扰性和泛化能力。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:22:50 GMT</pubDate>
</item>
<item>
<title>VidCRAFT3: 一种精准控制多视觉元素的图像到视频生成框架</title>
<link>https://arxiv.org/abs/2502.07531</link>
<guid>https://arxiv.org/abs/2502.07531</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VidCRAFT3框架实现了对多个视觉元素的精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VidCRAFT3的创新框架，旨在实现精准的图像到视频生成，同时控制摄像机运动、物体运动和光照方向。通过提出空间三重注意力变换器，VidCRAFT3能够对每个视觉元素进行松耦合控制。由于大多数现实世界的视频数据集缺乏光照注释，我们构建了一个高质量的合成视频数据集VideoLightingDirection（VLD），其中包含光照方向标注和多样化的物体外观，确保VidCRAFT3有效处理强光传输和反射效应。此外，我们提出了一种三阶段训练策略，消除了对多视觉元素同时注释的训练数据的需求。广泛的实验结果显示，VidCRAFT3在生成高质量视频内容方面的表现优于现有的最先进方法，尤其在控制精度和视觉一致性上。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07531" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:21:13 GMT</pubDate>
</item>
<item>
<title>CAD-Editor: 基于文本的计算机辅助设计编辑框架</title>
<link>https://arxiv.org/abs/2502.03997</link>
<guid>https://arxiv.org/abs/2502.03997</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CAD-Editor 是一个创新的基于文本的 CAD 编辑框架，提升了设计模型修改的效率。</p><br /><br /><p><strong>摘要：</strong> CAD-Editor 是一种创新的框架，专注于文本驱动的计算机辅助设计（CAD）模型编辑，旨在克服传统方法在文本控制和现有CAD模型应用中的局限性。该框架通过自动化数据合成管道生成原始与编辑模型的配对，并利用大型视觉语言模型（LVLMs）将它们的差异总结为编辑指令。此外，CAD-Editor 采用了定位-再填充的方法，将任务拆分为两大子任务：定位待修改区域与填充适当编辑。依赖于大型语言模型（LLMs）的强大自然语言理解和CAD知识，实验结果表明，CAD-Editor 在定量和定性性能上均表现优异，证明了其在文本式CAD编辑领域的潜力和应用价值。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.03997" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:16:28 GMT</pubDate>
</item>
<item>
<title>Enhance-A-Video: 一种提升DiT生成视频的一体化方法</title>
<link>https://arxiv.org/abs/2502.07508</link>
<guid>https://arxiv.org/abs/2502.07508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出了一种训练无关的方法，提升DiT视频生成的连贯性与质量。</p><br /><br /><p><strong>摘要：</strong> DiT基础的视频生成已经取得显著成果，但现有模型的增强研究仍较为欠缺。本文介绍了一种名为Enhance-A-Video的无训练方法，旨在提升DiT生成视频的连贯性和质量。其核心思想是增强基于非对角时间注意力分布的跨帧相关性。由于设计简单，该方法可轻松应用于大多数DiT视频生成框架，而无需任何重新训练或微调。在多种DiT视频生成模型上，我们的方法在时间一致性和视觉质量方面均表现出良好的改善。我们希望这项研究能激发未来在视频生成增强方面的探索。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:14:10 GMT</pubDate>
</item>
<item>
<title>大语言模型中的提示缓存引发的隐私泄露风险</title>
<link>https://arxiv.org/abs/2502.07776</link>
<guid>https://arxiv.org/abs/2502.07776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提示缓存导致的时间差异可能引发隐私泄露问题。</p><br /><br /><p><strong>摘要：</strong> 在大语言模型(LLMs)中，提示缓存产生的数据依赖性时延差异可能导致隐私泄露，这意味着缓存的提示处理速度快于非缓存的提示。若缓存在不同用户之间共享，攻击者能够通过快速的API响应时间识别出缓存提示，从而获取其他用户的信息。为此，我们开展了对现实世界LLM API提供者的统计审计，以检测提示缓存。结果发现，七家API提供者（包括OpenAI）之间存在全球性缓存共享，可能导致用户提示的隐私泄露。此外，由于提示缓存导致的时延变化还可能泄露模型架构的信息，我们发现OpenAI的嵌入模型是一个仅解码的Transformer，这一信息之前并未公开。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:11:49 GMT</pubDate>
</item>
<item>
<title>Nature语言模型：跨领域科学发现的基础模型</title>
<link>https://arxiv.org/abs/2502.07527</link>
<guid>https://arxiv.org/abs/2502.07527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NatureLM是一个跨科学领域的基础模型，推动科学发现的潜力。</p><br /><br /><p><strong>摘要：</strong> NatureLM是一个新的序列基础科学模型，旨在推动科学发现，通过在多个科学领域的数据上进行预训练，实现了跨领域的生成与设计应用。该模型能够生成和优化小分子、蛋白质、RNA和材料，支持蛋白质到小分子和蛋白质到RNA的跨领域生成，以及在SMILES到IUPAC翻译及USPTO-50k的逆合成任务中实现最先进的性能。NatureLM以不同的模型规模（10亿、80亿和467亿参数）进行开发，且随着模型规模的增大，性能明显改进，展现出在药物发现、材料设计和治疗蛋白或核苷酸开发等领域的广泛应用前景。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:10:26 GMT</pubDate>
</item>
<item>
<title>Hephaestus-Forge：提升LLM代理的预训练数据集</title>
<link>https://arxiv.org/abs/2502.06589</link>
<guid>https://arxiv.org/abs/2502.06589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Hephaestus-Forge是首个针对LLM代理的预训练数据集，显著提升其基本能力。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了Hephaestus-Forge，这是第一个大规模预训练语料库，旨在增强大型语言模型（LLM）代理在API功能调用、内在推理和规划能力方面的基本能力。Hephaestus-Forge包含1030亿个代理特定数据，涵盖76,537个API，提供工具文档以传授API功能知识及功能调用轨迹以强化内在推理。通过研究规模法则以确定最佳数据混合比率，持续在Hephaestus-Forge上进行预训练，Hephaestus在三个代理基准测试中超越小型到中型开源LLM，并与商业LLM相抗衡，证明了该预训练语料库在提升LLM代理基本能力及其对新任务或环境的泛化能力方面的有效性。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:04:08 GMT</pubDate>
</item>
<item>
<title>超大规模预训练视觉语言模型的实证研究</title>
<link>https://arxiv.org/abs/2502.07617</link>
<guid>https://arxiv.org/abs/2502.07617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨1000亿实例的预训练对多文化任务的影响。</p><br /><br /><p><strong>摘要：</strong> 本文对在前所未有的1000亿实例规模上预训练视觉语言模型的潜力进行了实证研究。研究发现，尽管传统的西方分类和检索基准（如COCO Captions）在这一规模上模型表现趋于饱和，但在文化多样性相关任务中，利用1000亿规模的网络数据取得了显著提升，特别是对长尾概念的覆盖。此外，研究分析了模型的多语言能力，低资源语言也表现出提升。值得注意的是，通过使用CLIP等质量过滤器减少预训练数据集的规模，虽然通常被认为有助于提升性能，但却可能无意中降低了大规模数据集中所表现的文化多样性。结果表明，尽管传统基准未能显著受益于扩大到1000亿实例的噪声原始网络数据，这一数据规模在构建真正包容的多模态系统中至关重要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:03:08 GMT</pubDate>
</item>
<item>
<title>CodeI/O：提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2502.07316</link>
<guid>https://arxiv.org/abs/2502.07316</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CodeI/O 提出新方法，通过代码输入输出预测提升推理能力。</p><br /><br /><p><strong>摘要：</strong> CodeI/O 是一种新颖的方法，旨在系统性地提炼多样的推理模式，以应对大语言模型在推理任务中的挑战。通过将原始代码转化为代码输入-输出预测格式，并训练模型在自然语言下预测这些输入和输出，CodeI/O 能够帮助模型掌握通用的推理原理，如逻辑流规划、状态空间搜索、决策树遍历和模块化分解。这种方法有效地将结构化推理与特定代码的语法解耦，确保了程序的严格性。实验结果表明，CodeI/O 在多个推理任务上取得了一致的提升，包括符号、科学、逻辑、数学与数字推理以及常识推理。通过匹配现有的真实输出或重新执行代码以验证预测，CodeI/O++ 实现了更高的性能，促进了多轮修订的思考链。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07316" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 23:00:20 GMT</pubDate>
</item>
<item>
<title>大规模语言模型中长链推理的训练与结构探索</title>
<link>https://arxiv.org/abs/2502.07374</link>
<guid>https://arxiv.org/abs/2502.07374</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现长链推理在大规模语言模型中依赖于结构而非内容。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大规模语言模型（LLM）如何通过数据高效的监督微调（SFT）和参数高效的低秩适应（LoRA）实现长链推理（Long CoT）。通过仅使用17,000个长链推理培训样本，Qwen2.5-32B-Instruct模型在多个数学和编码基准测试中较大幅度提升了性能，与竞争对手的模型相比表现优异。研究表明，长链推理的结构在学习过程中至关重要，而单个推理步骤的内容对性能影响有限。这意味着纠错样本或去除推理关键词等内容扰动对准确性影响不大，相较之下，破坏逻辑一致性的结构性修改会显著降低准确性。这些发现为观察和提升LLM推理能力提供了新的视角，并为未来推理模型的高效训练提供了关键考虑。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.07374" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:58:37 GMT</pubDate>
</item>
<item>
<title>大语言模型强化学习在编码与推理任务中的应用</title>
<link>https://arxiv.org/abs/2502.06807</link>
<guid>https://arxiv.org/abs/2502.06807</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明强化学习提升大语言模型在复杂编码推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文研究了将强化学习应用于大语言模型（LLMs）对复杂编码和推理任务表现的显著提升。通过比较两种通用推理模型——OpenAI o1和o3的早期检查点，以及一个特定领域系统o1-ioi，后者采用手工设计的推理策略，旨在参加2024国际信息学奥林匹克竞赛（IOI）。o1-ioi在IOI 2024现场比赛中，通过手工测试策略，取得了第49百分位的成绩，而在放宽的比赛约束下，获得了金牌。然而，更新的模型o3在没有手工领域特定策略或放宽约束的情况下，同样获得金牌。这些发现表明，虽然像o1-ioi这样的专用管道显著提高了表现，但扩大规模的通用o3模型在没有依赖手工推理启发式的情况下，能够超越这些成果。整体结果显示，扩大通用强化学习的规模，而不是依赖特定领域的技术，是实现推理领域尖端人工智能的可靠路径。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://arxiv.org/abs/2502.06807" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 11 Feb 2025 22:53:19 GMT</pubDate>
</item>
</channel>
</rss>