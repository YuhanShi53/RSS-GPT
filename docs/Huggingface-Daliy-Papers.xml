<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Huggingface Daily Papers</title>
<link>https://huggingface.co/papers</link>

<item>
<title>Open CaptchaWorld：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.24878</link>
<guid>https://arxiv.org/abs/2505.24878</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究提出CaptchaWorld，用于评估多模态LLMs解决CAPTCHA的能力。</p><br><br><p><strong>摘要：</strong> CAPTCHAs长期以来阻碍了网络机器人在实际应用中的部署，而现代多模态大型语言模型（MLLMs）虽在静态感知任务中表现优异，但其处理交互性和多步推理挑战的能力尚未得到充分测试。本文介绍了一个名为Open CaptchaWorld的新基准平台，该平台通过多样化的动态CAPTCHA谜题来评估MLLM驱动代理的视觉推理和交互能力。CaptchaWorld涵盖了20种现代CAPTCHA类型，总计225个CAPTCHA，并引入了新的度量标准“CAPTCHA推理深度”，量化解决每个谜题所需的认知和动作步骤。实验表明，人类在该测试中接近满分，而最先进的MLLM代理成功率仅为40.0%，远低于人类水平的93.3%。这表明CaptchaWorld是一个重要的基准，可用于诊断当前多模态代理系统的局限性，并指导开发更强大的多模态推理系统。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24878 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:55 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Time Blindness: Why Video-Language Models Can't See What Humans Can?</title>
<link>https://arxiv.org/abs/2505.24867</link>
<guid>https://arxiv.org/abs/2505.24867</guid>
<content:encoded><![CDATA[

  Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:12 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>强化学习如何扩展语言模型的推理边界</title>
<link>https://arxiv.org/abs/2505.24864</link>
<guid>https://arxiv.org/abs/2505.24864</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究发现，强化学习可揭示基模型无法触及的新推理策略。</p><br><br><p><strong>摘要：</strong> 近期研究表明，强化学习（RL）在引导语言模型实现可验证奖励方面具有潜力。然而，RL是否真正提升了模型的推理能力，还是仅仅放大了基模型分布中已存在的高奖励输出，仍存争议。此外，持续增加RL计算资源是否能可靠提升推理性能也未有定论。本研究通过引入持续强化学习（ProRL）方法，证明即使在广泛采样的情况下，ProRL训练也能挖掘出基模型无法触及的新推理策略。ProRL方法结合了KL散度控制、参考策略重置及多样化任务套件。实证分析表明，RL训练模型在多项pass@k评估中始终优于基模型，且在某些基模型完全失败的场景下表现优异。进一步研究表明，推理边界的改进与基模型的任务能力和训练时长密切相关，表明RL能够随着时间推移探索并填充新的解空间区域。这些发现为未来长期RL在推理领域的研究奠定了基础，并提供了新的见解。研究模型权重已公开，支持进一步研究。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24864 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:59:01 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>ViStoryBench：故事可视化评估基准的引入</title>
<link>https://arxiv.org/abs/2505.24862</link>
<guid>https://arxiv.org/abs/2505.24862</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究提出ViStoryBench基准，用于评估故事可视化模型性能。</p><br><br><p><strong>摘要：</strong> 随着生成模型的进步，故事可视化领域取得了显著进展。为了进一步提升模型在实际场景中的表现，我们引入了ViStoryBench，这是一个综合性的评估基准。该基准集成了多样化的数据集，涵盖了多种故事类型和艺术风格，通过多维度测试模型能力，包括情节类型（如喜剧、恐怖）和视觉美学（如动漫、3D渲染）。ViStoryBench精心设计，平衡叙事结构与视觉元素，包含单主角和多主角的故事，以及复杂的情节和世界构建。此外，它采用广泛的评价指标进行全面比较。这一系统化框架有助于研究人员深入分析模型优劣，推动针对性改进。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24862 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:21 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:58:21 GMT</pubDate>
</item>
<item>
<title>大型语言模型的忠实置信校准研究</title>
<link>https://arxiv.org/abs/2505.24858</link>
<guid>https://arxiv.org/abs/2505.24858</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究显示现有大型语言模型在传达不确定性时表现不佳，提出新方法MetaFaith显著提升校准效果。</p><br><br><p><strong>摘要：</strong> 可靠不确定性沟通对大型语言模型的信任至关重要，但这些模型常以肯定语气传达错误信息，导致用户过度依赖并削弱信任。本研究首次系统评估了多种模型、数据集及提示策略下的忠实置信校准能力，发现当前方法成效有限，标准提示仅带来微小改进，而基于事实性的校准技术甚至可能损害准确性。为此，我们开发了MetaFaith，一种受人类元认知启发的新型提示校准方法，在多个模型和任务领域显著提升了校准的忠实性，使忠实度提高最多达61%，并获得人类评估83%的胜率。这项工作填补了大型语言模型在不确定性表达上的关键空白。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24858 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:54:08 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:54:08 GMT</pubDate>
</item>
<item>
<title>通过强化蒸馏优化大规模语言模型推理性能</title>
<link>https://arxiv.org/abs/2505.24850</link>
<guid>https://arxiv.org/abs/2505.24850</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新框架REDI，有效利用正负推理样本提升LLM推理能力。</p><br><br><p><strong>摘要：</strong> 近年来，模型蒸馏技术的进步表明，来自高级推理模型的数据可以有效地将复杂推理能力转移到较小的高效学生模型中。然而，标准做法采用拒绝采样方法，丢弃了错误的推理示例——这些数据虽然有价值但通常未被充分利用。本文探讨了如何在离线环境中有效利用正负蒸馏推理轨迹以最大化大型语言模型（LLM）的推理性能。为此，我们提出了强化蒸馏（REDI），这是一种两阶段框架。第一阶段通过监督微调（SFT）学习正向轨迹；第二阶段则通过我们提出的REDI目标函数进一步优化模型，该目标函数是一种简单的无参考损失函数，在这种蒸馏上下文中优于已建立的方法如DPO和SimPO。我们的实证评估显示，REDI在数学推理任务上优于基线拒绝采样SFT或SFT结合DPO/SimPO。特别是，Qwen-REDI-1.5B模型仅使用开放可用的Open-R1数据集中的131k个正负示例进行后训练，在MATH-500（pass@1）上获得了83.1%的分数。其在各种数学推理基准测试中的表现与DeepSeek-R1-Distill-Qwen-1.5B（使用800k专有数据后训练）相当或更好，确立了在仅使用公开可用数据进行离线后训练的1.5B规模模型的新技术水平。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24850 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:47:17 GMT</pubDate>
<pubDate>Fri, 30 May 2025 13:47:17 GMT</pubDate>
</item>
<item>
<title>Harnessing Large Language Models for Scientific Novelty Detection</title>
<link>https://arxiv.org/abs/2505.24615</link>
<guid>https://arxiv.org/abs/2505.24615</guid>
<content:encoded><![CDATA[

  In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.

]]></content:encoded>
<pubDate>Fri, 30 May 2025 10:08:13 GMT</pubDate>
<pubDate>Fri, 30 May 2025 10:08:13 GMT</pubDate>
</item>
<item>
<title>利用扩散模型先验进行跨帧一致性几何估计</title>
<link>https://arxiv.org/abs/2505.24521</link>
<guid>https://arxiv.org/abs/2505.24521</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>通过扩散模型的内在一致性实现视频全局几何属性的跨帧一致预测。</p><br><br><p><strong>摘要：</strong> 本文提出了一种方法，通过合理设计和微调扩散模型，有效利用视频生成模型的内在一致性，用于一致性的单目几何估计。具体而言，我们选择共享相同对应关系的全局坐标系中的几何属性作为预测目标，引入基于位置编码重用的高效条件方法，并通过联合训练多个共享相同对应关系的几何属性提升性能。实验结果显示，我们的方法在视频全局几何属性预测上表现优异，并可以直接应用于重建任务。即使仅在静态视频数据上训练，该方法也展现出对动态视频场景的潜在泛化能力。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24521 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:31:59 GMT</pubDate>
<pubDate>Fri, 30 May 2025 08:31:59 GMT</pubDate>
</item>
<item>
<title>un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP</title>
<link>https://arxiv.org/abs/2505.24517</link>
<guid>https://arxiv.org/abs/2505.24517</guid>
<content:encoded><![CDATA[

  Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.

]]></content:encoded>
<pubDate>Fri, 30 May 2025 08:29:38 GMT</pubDate>
<pubDate>Fri, 30 May 2025 08:29:38 GMT</pubDate>
</item>
<item>
<title>大型语言模型的近似线性分解及其语义结构解析</title>
<link>https://arxiv.org/abs/2505.24293</link>
<guid>https://arxiv.org/abs/2505.24293</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究揭示大型语言模型可被映射为等效线性系统。</p><br><br><p><strong>摘要：</strong> 本文展示了多种开源权重的大型语言模型（LLMs）的推理操作可以映射到与输入序列等效的线性系统，而无需修改模型权重或改变预测输出。通过借鉴图像扩散模型的技术，我们战略性地调整了针对下一个词预测的梯度计算，使得模型的雅可比矩阵几乎精确地再现了前向预测的线性系统。该方法适用于多种模型（如Llama 3、Gemma 3、Qwen 3等），并通过奇异值分解显示这些LLMs在极低维子空间中运行，其中许多最大的奇异向量解码出的概念与最可能的输出词相关。此外，这种方法还允许我们将每一层的操作视为近似的线性系统，并观察到语义概念的出现。尽管现代LLMs具有强大的表达能力和全局非线性，但它们可以通过近乎精确的局部线性分解进行解释，从而提供对其内部表示的洞察并揭示下一个词预测过程中的可解释语义结构。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24293 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 03:08:33 GMT</pubDate>
<pubDate>Fri, 30 May 2025 03:08:33 GMT</pubDate>
</item>
<item>
<title>CLaSp：一种基于上下文层跳过的自推测解码策略</title>
<link>https://arxiv.org/abs/2505.24196</link>
<guid>https://arxiv.org/abs/2505.24196</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>CLaSp通过跳过验证模型的中间层实现高效解码，加速大语言模型推理。</p><br><br><p><strong>摘要：</strong> 本文提出了一种名为CLaSp的新方法，用于提升大语言模型（LLMs）的解码速度。与传统推测解码（SD）需要额外模块训练不同，CLaSp采用即插即用的方式，通过跳过验证模型的部分中间层构建压缩版草案模型，无需额外训练。该方法利用动态规划算法优化层跳过过程，根据每次验证阶段后的完整隐藏状态动态调整策略。实验表明，在LLaMA3系列模型上，CLaSp实现了1.3到1.7倍的加速，且不影响生成文本的原始分布。这项研究为加速LLMs的推理提供了新的思路。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.24196 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Fri, 30 May 2025 00:15:06 GMT</pubDate>
<pubDate>Fri, 30 May 2025 00:15:06 GMT</pubDate>
</item>
<item>
<title>HardTests: Synthesizing High-Quality Test Cases for LLM Coding</title>
<link>https://arxiv.org/abs/2505.24098</link>
<guid>https://arxiv.org/abs/2505.24098</guid>
<content:encoded><![CDATA[

  Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.

]]></content:encoded>
<pubDate>Thu, 29 May 2025 21:00:34 GMT</pubDate>
<pubDate>Thu, 29 May 2025 21:00:34 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在计数任务中的偏见研究</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>研究发现最先进的视觉语言模型在计数和识别任务中表现出显著偏见。</p><br><br><p><strong>摘要：</strong> 大型语言模型（LLMs）通过互联网学习大量先验知识，但这些知识可能导致它们产生错误或有偏见的答案。本研究专注于测试这些知识对视觉语言模型（VLMs）在标准视觉任务（如计数和识别）中的准确性的影响。实验结果显示，最先进的VLMs在处理涉及流行主题的任务时表现不佳，例如，在计数带有附加条纹的阿迪达斯标志条纹数量时，平均准确率仅为17.05%，涵盖动物、商标、国际象棋、棋盘游戏、视觉错觉和图案网格等多个领域。当向图像插入描述性文本时，准确率进一步下降。即使指导模型重新检查答案或依赖图像细节，计数准确性仅提高约2个百分点。这项工作揭示了VLMs的一种有趣失败模式，并提出了一种自动化框架用于检测模型偏差。相关代码和数据可在vlmsarebiased.github.io获取。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.23941 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:47:58 GMT</pubDate>
<pubDate>Thu, 29 May 2025 14:47:58 GMT</pubDate>
</item>
<item>
<title>Point-MoE：实现大规模跨域3D点云理解的Mixture-of-Experts架构</title>
<link>https://arxiv.org/abs/2505.23926</link>
<guid>https://arxiv.org/abs/2505.23926</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新的Mixture-of-Experts架构Point-MoE，用于提升3D点云跨域理解能力。</p><br><br><p><strong>摘要：</strong> 尽管缩放定律已在自然语言处理和计算机视觉领域取得了显著成果，但3D点云理解尚未达到类似阶段。这一差距主要归因于3D数据集规模较小且来源多样化，导致扫描模式、采样密度及语义偏差各异。这种领域异质性严重阻碍了统一模型的大规模训练。本研究提出了Point-MoE，这是一种专门设计的Mixture-of-Experts架构，旨在实现3D感知中的大规模跨域泛化。实验表明，标准点云主干在混合域数据上表现明显下降，而Point-MoE通过简单的top-k路由策略能够自动专业化专家，即使没有领域标签。研究还证明，Point-MoE不仅优于强大的多域基线模型，而且对未见过的领域具有更好的泛化能力。这项工作强调了一种可扩展的3D理解路径：让模型自行发现多样化3D数据中的结构，而非通过手动整理或领域监督强加。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.23926 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Thu, 29 May 2025 14:21:47 GMT</pubDate>
<pubDate>Thu, 29 May 2025 14:21:47 GMT</pubDate>
</item>
<item>
<title>EmergentTTS-Eval：语音合成模型的综合评估基准</title>
<link>https://arxiv.org/abs/2505.23009</link>
<guid>https://arxiv.org/abs/2505.23009</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出一种新基准EmergentTTS-Eval，涵盖六个复杂场景以评估语音合成模型。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种名为EmergentTTS-Eval的新基准，该基准扩展自EmergentTTS，专注于评估文本到语音（TTS）模型在处理微妙和语义复杂文本时的表现。它涵盖了六个具有挑战性的场景，包括情绪表达、副语言特征、外来词、句法复杂性、复杂发音以及问句处理。通过利用大型语言模型（LLMs）迭代生成测试案例，最终构建了包含1645个多样化测试案例的数据集。此外，采用模型作为裁判的方法，利用大型音频语言模型（LALM）从多个维度评估语音质量，如情感表达、韵律、语调和发音准确性。实验结果显示，这种方法不仅能揭示不同TTS系统间的细微性能差异，还与人类偏好高度相关。研究开源了评价代码和数据集，为未来的研究提供了宝贵的资源。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.23009 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Wed, 28 May 2025 22:36:24 GMT</pubDate>
<pubDate>Wed, 28 May 2025 22:36:24 GMT</pubDate>
</item>
<item>
<title>DexUMI：通过人类手部接口学习灵巧操作技能的框架</title>
<link>https://arxiv.org/abs/2505.21864</link>
<guid>https://arxiv.org/abs/2505.21864</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>DexUMI框架通过人类手部接口转移灵巧操作技能到机器人手上，实验成功率达86%。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种名为DexUMI的数据收集和策略学习框架，该框架利用人类手作为自然接口，将灵巧操作技能转移到不同的机器人手上。DexUMI包含硬件和软件适应性调整，以最小化人体手与机器人手之间的差异。硬件上，通过可穿戴外骨骼桥接运动学差距，并提供直接触觉反馈；软件上，通过高保真机器人手图像修复技术解决视觉差异。实验证明，在两个不同硬件平台上的平均任务成功率达到了86%，展示了DexUMI的强大能力。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.21864 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:25:27 GMT</pubDate>
<pubDate>Tue, 27 May 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>无需额外训练的音频-视觉大语言模型平衡模态理解方法</title>
<link>https://arxiv.org/abs/2505.20873</link>
<guid>https://arxiv.org/abs/2505.20873</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>提出了一种名为Fork-Merge Decoding的方法，无需额外训练即可减少模态偏差。</p><br><br><p><strong>摘要：</strong> 本文旨在通过解决模态偏差问题来提升音频-视觉大语言模型（AV-LLMs）的平衡多模态理解能力，而无需进行额外训练。当前的AV-LLMs通常通过解码器联合处理音频和视频特征，虽然促进了统一的多模态理解，但可能引入模态偏差，即模型倾向于过度依赖某一模态。为了解决这一问题，我们提出了Fork-Merge Decoding（FMD），这是一种在推理阶段简单有效的策略，不需要额外的训练或架构修改。FMD首先通过早期解码层对仅音频和仅视频输入进行模态特定推理（分叉阶段），然后在剩余层合并隐藏状态以进行联合推理（合并阶段）。这种方法促进了模态贡献的平衡并利用了跨模态的互补信息。我们在两个代表性AV-LLMs（VideoLLaMA2和video-SALMONN）上使用三个基准数据集评估了该方法。实验结果显示，在专注于音频、视频和组合音频-视觉推理的任务中，性能得到了一致的改善，证明了推理时干预的有效性。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.20873 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 27 May 2025 04:22:56 GMT</pubDate>
<pubDate>Tue, 27 May 2025 04:22:56 GMT</pubDate>
</item>
<item>
<title>v1模型：多模态大语言模型的轻量级视觉重访扩展</title>
<link>https://arxiv.org/abs/2505.18842</link>
<guid>https://arxiv.org/abs/2505.18842</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>v1模型通过点选-复制机制实现推理过程中的动态视觉访问。</p><br><br><p><strong>摘要：</strong> 本文介绍了一种名为v1的轻量级扩展，用于多模态大型语言模型（MLLMs），使模型能够在推理过程中选择性地重新访问视觉信息。不同于传统MLLMs仅一次性处理视觉输入并完全依赖内部记忆，v1引入了一个简单的点选-复制机制，允许模型在整个推理过程中动态检索相关的图像区域。该机制通过最小修改增强了现有架构，基于模型不断发展的假设提供上下文访问视觉标记的能力。为了训练这种能力，我们构建了v1g数据集，包含30万个多模态推理跟踪样本及交错的视觉定位注释。实验表明，在三个多模态数学推理基准测试（MathVista、MathVision和MathVerse）上，v1相比同类基线模型表现更为出色，尤其是在需要精细视觉参考和多步推理的任务中。我们的研究结果表明，动态视觉访问是提升基于事实的多模态推理性能的一个有前景的方向。代码、模型和数据将被公开发布，以支持未来的相关研究。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.18842 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Sat, 24 May 2025 15:30:47 GMT</pubDate>
<pubDate>Sat, 24 May 2025 15:30:47 GMT</pubDate>
</item>
<item>
<title>LLMSynthor：利用大语言模型实现高保真数据合成</title>
<link>https://arxiv.org/abs/2505.14752</link>
<guid>https://arxiv.org/abs/2505.14752</guid>
<content:encoded><![CDATA[

  <div><p style=color:gray;>LLMSynthor通过引入结构感知模拟器，提升基于大语言模型的数据合成效率与统计一致性。</p><br><br><p><strong>摘要：</strong> 数据建模中的一个重要挑战是生成能够忠实反映现实世界分布统计特性的合成数据。传统方法依赖强参数假设或手动结构设计，在高维或异构领域表现不佳。尽管大型语言模型(LLMs)展现出作为灵活高维先验的强大潜力，但其标准采样方法存在效率低、上下文限制固定且难以保证统计对齐的问题。为解决这些问题，我们提出了LLMSynthor框架，它将LLMs转化为由分布反馈引导的结构感知模拟器。该框架利用LLM作为非参数copula模拟器来建模高阶依赖关系，并通过LLM提议采样生成接地提议分布，从而提高采样效率而不需拒绝采样。通过迭代合成循环，LLMSynthor逐步揭示并优化潜在生成结构，使真实数据和合成数据保持统计一致性。我们在隐私敏感领域的异构数据集（如电子商务、人口和移动性）上进行了控制实验和实际应用评估，结果显示LLMSynthor生成的合成数据具有高统计保真度、实用性和跨数据适应性，可广泛应用于经济学、社会科学、城市研究等领域。</p><br><br><p><em>使用 qwen-turbo 生成 </em></p><a href=https://arxiv.org/abs/2505.14752 target="_blank">查看原文</a></div>

]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:35:38 GMT</pubDate>
<pubDate>Tue, 20 May 2025 09:35:38 GMT</pubDate>
</item>

<item>
<title>AlphaOne：一种用于大模型推理过程动态调控的通用框架</title>
<link>https://arxiv.org/abs/2505.24863</link>
<guid>https://arxiv.org/abs/2505.24863</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AlphaOne框架，通过参数化思考阶段提升大模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AlphaOne的新框架，旨在测试时对大规模推理模型（LRMs）的推理进程进行灵活调控。AlphaOne首先引入了“阿尔法时刻”这一概念，通过一个通用参数α来表示扩展的思考阶段。在此过程中，它通过将推理转换标记的插入建模为伯努利随机过程，动态调度缓慢推理的过渡。在“阿尔法时刻”结束后，AlphaOne通过终止符确定性地结束缓慢推理，从而促进快速推理和高效答案生成。这种方法统一并推广了现有的单调缩放方法，实现了缓慢到快速推理的灵活且密集的调控。在数学、编码和科学等多个领域的具有挑战性的基准测试中，AlphaOne展示了其卓越的推理能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24863" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 13:58:36 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的多语言文本生成框架EasyText</title>
<link>https://arxiv.org/abs/2505.24417</link>
<guid>https://arxiv.org/abs/2505.24417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DiT的多语言文本渲染框架EasyText。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EasyText的新框架，该框架基于扩散Transformer（DiT），旨在实现精确的多语言文本生成。通过结合去噪潜变量与多语言字符标记编码，EasyText利用字符位置编码和位置编码插值技术，实现了可控且精确的文本渲染。此外，构建了一个包含百万级多语言图像文本标注的大规模合成文本图像数据集及高质量的2万张标注图像数据集，用于预训练和微调。实验结果表明，EasyText在多语言文本渲染、视觉质量和布局感知的文本集成方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.24417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 30 May 2025 05:55:39 GMT</pubDate>
</item>
<item>
<title>一种自适应知识集成框架用于增强大型语言模型</title>
<link>https://arxiv.org/abs/2505.23844</link>
<guid>https://arxiv.org/abs/2505.23844</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应知识集成框架，解决传统方法内存消耗大及性能下降问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型（LLMs）改进中的挑战，如传统微调方法的局限性和集成其他专用模型时的问题，提出了一个自适应知识集成框架。该框架通过设计一个自适应选择网络，根据源模型的分数选择最相关的模型，减少了知识干扰，并采用动态加权融合策略和反馈驱动损失函数，提升了模型的稳定性和可扩展性。实验表明，该方法相比现有方法将知识干扰降低了50%，同时保持了较高的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23844" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:24:50 GMT</pubDate>
</item>
<item>
<title>GSO基准测试：评估语言模型在高性能软件开发中的能力</title>
<link>https://arxiv.org/abs/2505.23671</link>
<guid>https://arxiv.org/abs/2505.23671</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出GSO基准测试，评估语言模型在优化代码性能上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为GSO的新基准测试，用于评估语言模型在开发高性能软件方面的表现。通过自动化管道生成并执行性能测试，从10个代码库的历史提交记录中提取出102个优化任务，涵盖多个领域和编程语言。实验中，代理被要求提高代码运行效率，并与专家开发者的表现进行对比。定量分析显示，领先的语言模型表现不佳，成功率不足5%，且随着推理时间扩展改善有限。定性分析揭示了主要失败模式，如对低级语言处理困难、懒惰优化策略及瓶颈定位挑战。本研究还公开了基准测试代码及相关数据，以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23671" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:14:55 GMT</pubDate>
</item>
<item>
<title>Yet Another Quantization Algorithm (YAQA) 改进大语言模型后量化性能</title>
<link>https://arxiv.org/abs/2505.22988</link>
<guid>https://arxiv.org/abs/2505.22988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法YAQA，显著提升大语言模型后量化压缩效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Yet Another Quantization Algorithm (YAQA) 的新型自适应舍入算法，用于优化大规模语言模型（LLMs）的后训练量化（PTQ）。传统方法通过独立最小化立即激活误差来量化线性层，但忽略了后续层的影响，导致优化目标局部化。YAQA利用每一层线性层相对于全模型KL散度的Hessian矩阵的Kronecker分解近似值，从而更好地捕获全局影响。该算法由两部分组成：可高效计算的大规模LLMs层间Hessian的Kronecker分解近似，以及与具体量化器无关的具有理论保证的舍入方法。实验表明，在多种模型和量化器上，YAQA可将KL散度降低约30%，同时在下游任务上达到最先进的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:53:00 GMT</pubDate>
</item>
<item>
<title>Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14599</link>
<guid>https://arxiv.org/abs/2505.14599</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:49:40 GMT</pubDate>
</item>
<item>
<title>MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence</title>
<link>https://arxiv.org/abs/2505.23764</link>
<guid>https://arxiv.org/abs/2505.23764</guid>
<content:encoded><![CDATA[
Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a step-by-step reasoning process. We conduct extensive experiments and thoroughly evaluate 34 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while humans score 97%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering valuable insights for advancing multi-image spatial intelligence. Project page: https://runsenxu.com/projects/MMSI_Bench .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在解码字谜中的能力评估</title>
<link>https://arxiv.org/abs/2505.23759</link>
<guid>https://arxiv.org/abs/2505.23759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，视觉语言模型在解码简单视觉线索时表现良好，但在抽象推理方面存在不足。</p><br /><br /><p><strong>摘要：</strong> 字谜（Rebus puzzles）是一种通过图像、空间排列和符号替代来编码语言的视觉谜题，对当前的视觉语言模型（VLMs）提出了独特的挑战。不同于传统的图像描述或问答任务，字谜的解答需要多模态抽象、符号推理以及对文化、语音及语言双关的理解。本文构建了一个由多样化的英文字谜组成的基准测试集，涵盖从简单的图画替代到依赖空间提示的复杂谜题。通过对多种VLMs的性能分析，我们发现尽管这些模型在解析简单视觉线索时表现出一定的能力，但在涉及抽象推理、横向思维以及理解视觉隐喻的任务上仍存在显著困难。这项研究揭示了现有VLMs在跨模态任务上的局限性及其未来改进的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:47 GMT</pubDate>
</item>
<item>
<title>基于关键帧的音乐同步动物舞蹈视频生成框架</title>
<link>https://arxiv.org/abs/2505.23738</link>
<guid>https://arxiv.org/abs/2505.23738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于关键帧的动物舞蹈视频生成方法，结合图优化和扩散模型实现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于关键帧的框架，用于生成与音乐同步且符合编舞意识的动物舞蹈视频。该框架通过将舞蹈合成建模为图优化问题，寻找满足指定编舞模式的最佳关键帧结构，这些模式可以从参考舞蹈视频中自动估计。此外，还引入了一种镜像姿态图像生成方法，以捕捉舞蹈中的对称性。通过视频扩散模型生成中间帧，在仅提供六个输入关键帧的情况下，可以生成长达30秒的跨多种动物和音乐轨道的舞蹈视频。这一方法展示了从文本到图像提示或GPT-4生成的关键帧开始，实现高质量动物舞蹈视频的强大能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>ZPressor：通过信息瓶颈优化提升3D Gaussian Splatting模型的多视角扩展性</title>
<link>https://arxiv.org/abs/2505.23734</link>
<guid>https://arxiv.org/abs/2505.23734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ZPressor模块，压缩多视角输入至紧凑潜在状态Z，显著提高3DGS模型的扩展能力。</p><br /><br /><p><strong>摘要：</strong> 本文分析了基于前馈的3D Gaussian Splatting (3DGS) 模型在处理多视角输入时面临的扩展性限制问题，即随着输入视图数量增加，性能下降或内存消耗过高。我们基于信息瓶颈原理设计了ZPressor，这是一种轻量级且架构无关的模块，用于高效压缩多视角输入到一个保留关键场景信息的紧凑潜在状态Z。具体来说，ZPressor通过将视图划分为锚点集和支持集，并利用交叉注意力机制，将支持视图的信息压缩到锚点视图中，从而构建压缩后的潜在状态Z。实验表明，集成ZPressor后，多个最先进的前馈3DGS模型在适度输入视图下性能得到提升，在密集视图设置下的鲁棒性也有所增强。该方法在DL3DV-10K和RealEstate10K两个大规模基准数据集上表现优异，同时在80GB GPU上实现了超过100个480P分辨率输入视图的处理能力。相关视频结果、代码及训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:04 GMT</pubDate>
</item>
<item>
<title>ViGoRL：通过视觉引导强化学习提升模型的视觉推理能力</title>
<link>https://arxiv.org/abs/2505.23678</link>
<guid>https://arxiv.org/abs/2505.23678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViGoRL通过视觉定位强化学习提升语言模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ViGoRL（Visually Grounded Reinforcement Learning）的视觉语言模型，该模型通过强化学习将每一步推理明确锚定到特定的视觉坐标上，从而实现空间推理路径的生成。ViGoRL受到人类视觉决策的启发，能够指导视觉注意力聚焦于相关区域。在一系列视觉推理基准测试中，包括SAT-2、BLINK、V*bench等任务，ViGoRL的表现显著优于传统的监督微调和缺乏显式定位机制的常规强化学习基线。特别是在需要精细探索的任务中，多轮强化学习框架结合动态缩放功能进一步提升了模型性能。此外，研究表明，显式定位不仅提高了模型在局部元素定位和视觉搜索上的表现，还增强了其他视觉行为，如区域探索和子目标设定。最后，人类评估表明，ViGoRL的视觉参考不仅空间准确，而且有助于理解模型的推理过程。这些结果表明，视觉引导的强化学习是一种强大的方法，可以赋予模型通用的视觉推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:20:26 GMT</pubDate>
</item>
<item>
<title>基于轨迹输入的统一视频运动控制框架</title>
<link>https://arxiv.org/abs/2505.22944</link>
<guid>https://arxiv.org/abs/2505.22944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种运动类型的统一视频生成控制框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种用于视频生成的统一运动控制框架，该框架通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间中，实现了相机移动、对象级平移和精细局部运动的无缝集成。与以往针对不同运动类型采用独立模块的方法不同，我们的方法通过单一框架实现了对局部形变、物体整体运动、虚拟相机动态或其组合的精确控制。实验表明，该方法在多个视频运动控制任务上表现出色，包括风格化运动效果、动态视点变化及局部运动操作，同时在可控性和视觉质量上显著优于现有方法和商业解决方案，且兼容多种先进的视频生成模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:49:18 GMT</pubDate>
</item>
<item>
<title>AIDSAFE：通过多智能体迭代推敲提升LLMs安全推理能力</title>
<link>https://arxiv.org/abs/2505.21784</link>
<guid>https://arxiv.org/abs/2505.21784</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AIDSAFE方法，利用多智能体协作生成高质量的安全推理链，显著提升大模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AIDSAFE的新方法，旨在解决现有安全措施如过度拒绝和越狱漏洞的问题。AIDSAFE通过多智能体迭代推敲的方式生成嵌入安全政策的链式思维（CoT）数据集，同时引入数据精炼阶段以消除重复、冗余及误导性思维。实验表明，基于AIDSAFE生成的CoT进行监督微调可大幅提升开源大模型的安全泛化能力和越狱鲁棒性，同时保持良好的实用性和拒绝精度。此外，为了满足对齐阶段的偏好数据需求，该方法还设计了一种补充方案，通过信念增强技术创建区分选择和拒绝样本的CoT数据。最终评估显示，AIDSAFE生成的CoT在政策遵守度和推理质量方面表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21784" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 17:34:40 GMT</pubDate>
</item>
<item>
<title>LUNGUAGE：基于多研究纵向评估的胸部X光报告生成基准数据集</title>
<link>https://arxiv.org/abs/2505.21190</link>
<guid>https://arxiv.org/abs/2505.21190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持单次与纵向评估的胸部X光报告生成基准数据集及评价方法。</p><br /><br /><p><strong>摘要：</strong> 现有放射学报告评估方法局限于单一报告环境且依赖粗略指标，无法捕捉精细临床语义和时间依赖性。本文引入LUNGUAGE，这是一个结构化胸部X光报告生成基准数据集，支持单次报告评估和跨多个研究的患者级别纵向评估。数据集包含1,473份由专家注释的胸片报告，其中80份具有纵向注释以捕获疾病进展。此外，开发了一个两阶段框架将生成的报告转换为细粒度的结构化表示，实现纵向解释。同时，提出了LUNGUAGESCORE，一种可解释的指标，在实体、关系和属性层面比较结构化输出的同时建模患者时间线的一致性。这些贡献建立了首个针对序列放射学报告的基准数据集、结构化框架和评估指标，实证结果显示LUNGUAGESCORE有效支持结构化报告评估。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:40:00 GMT</pubDate>
</item>
<item>
<title>大型语言模型与知识图谱结合用于复杂问答任务的研究综述</title>
<link>https://arxiv.org/abs/2505.20099</link>
<guid>https://arxiv.org/abs/2505.20099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型与知识图谱结合解决复杂问答任务的方法及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在问答（QA）任务中表现出色，但由于推理能力不足、知识过时及幻觉问题，在处理复杂QA任务时面临挑战。一些研究尝试将LLMs与知识图谱（KGs）相结合，以克服这些限制。本文提出了一种新的结构化分类法，根据QA类型和KG在与LLMs集成时的角色对方法进行分类。我们系统性地回顾了相关领域的最新进展，并从优势、局限性和KG需求等方面比较分析了这些方法。此外，我们还探讨了这些方法如何应对不同类型复杂QA的主要挑战，并总结了现有研究的成果、评估指标和基准数据集，同时指出了开放的问题与未来机会。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:08:23 GMT</pubDate>
</item>
<item>
<title>系统1.5推理：高效且适应性的大语言模型推理框架</title>
<link>https://arxiv.org/abs/2505.18962</link>
<guid>https://arxiv.org/abs/2505.18962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出系统1.5推理方法，显著提高大语言模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 当前基于链式思维（CoT）的大语言模型推理面临效率低下的问题，而潜在空间推理虽提高了效率，但未能区分关键推理步骤与辅助步骤，导致计算资源利用不充分。本文提出系统1.5推理框架，通过潜在空间中的动态捷径路径，在推理步骤间动态分配计算资源。该框架包括模型深度捷径（DS）和步骤捷径（SS），前者允许非关键标记提前退出轻量级适配器分支，后者则跨解码步骤重用隐藏状态以跳过简单步骤。通过两阶段自蒸馏过程训练，系统1.5推理在GSM8K等推理任务上展现了卓越性能，推理速度提升超过20倍，平均减少92.31%的标记生成，同时保持与传统CoT微调方法相当的推理效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 23:35:49 GMT</pubDate>
</item>
<item>
<title>视觉表征压缩对细粒度特征还原的影响及基准评测</title>
<link>https://arxiv.org/abs/2505.18142</link>
<guid>https://arxiv.org/abs/2505.18142</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示视觉分词器和VAE在保存细节特征上的局限性，并提出评估文本与人脸重建性能的新基准。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了视觉分词器和变分自编码器（VAE）在图像压缩过程中丢失细节信息的问题，特别是在处理小尺度的文本和人脸图像时的局限性。尽管这些技术通过提供高效的图像压缩和量化表示推动了视觉生成与多模态建模的发展，但它们在减少计算负担的同时也限制了视觉生成质量的上限。为了评估这一上限，我们聚焦于文本和面部特征的重建质量，因为这些特征通常具有密集纹理、易塌陷且对人类视觉高度敏感的特点。我们收集并整理了来自现有数据集的高质量文本和人脸图像，并采用成熟的OCR和人脸识别模型进行评估，这种方法不仅准确而且轻量级，仅需2GB内存和4分钟即可完成。通过我们的基准测试，分析了不同图像分词器和VAE在各种尺度下的重建质量。结果显示，现代视觉分词器在保存细粒度特征方面仍有不足，尤其是在小尺度下表现欠佳。此外，我们将此评估框架扩展到视频领域，对多种视频分词器进行了综合分析，并证明传统指标无法准确反映人脸和文本的重建效果，而我们提出的指标则是一个有效的补充。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18142" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:52:16 GMT</pubDate>
</item>
<item>
<title>REOrder：通过优化补丁顺序提升视觉Transformer性能</title>
<link>https://arxiv.org/abs/2505.23751</link>
<guid>https://arxiv.org/abs/2505.23751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示补丁排列对现代Transformer模型表现有显著影响，提出REOrder框架优化补丁顺序。</p><br /><br /><p><strong>摘要：</strong> 当前基于Transformer的视觉模型通常将图像展平为一维序列，常用固定行优先（栅格扫描）顺序。尽管完全自注意力机制具有置换等变性，但现代长序列Transformer倾向于采用破坏这种不变性的架构近似方法，导致对补丁排列敏感。本文表明，在这些设置下补丁顺序显著影响模型性能，例如列优先或希尔伯特曲线等简单替代方案可带来明显的准确性变化。受此启发，我们提出了REOrder，这是一种两阶段框架，用于发现任务最优的补丁排列。首先，通过评估各种补丁序列的压缩性得出信息论先验；然后，利用REINFORCE优化Plackett-Luce策略学习排列策略。该方法能够在组合排列空间中实现高效学习。实验证明，REOrder在ImageNet-1K上相比行优先排列提升了高达3.01%的top-1准确率，并在Functional Map of the World数据集上提高了13.35%的准确率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>基于强化学习的大语言模型代码效率优化框架</title>
<link>https://arxiv.org/abs/2505.23387</link>
<guid>https://arxiv.org/abs/2505.23387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过强化学习显著提升大语言模型代码执行效率。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）虽然能够生成功能正确的代码，但在代码效率方面存在不足，成为实际应用中的瓶颈。本文介绍了一种测试时迭代优化框架，利用闭环系统让LLMs根据执行沙箱反馈迭代改进代码。研究探索了三种训练策略：监督微调（SFT）、直接偏好优化（DPO）和分组相对策略优化（GRPO）。实验显示，SFT和DPO在效率提升上很快达到饱和，而采用强化学习的GRPO持续优化代码性能，在Venus数据集和APPS基准测试中分别将pass@1提升至62%并使效率优于人类提交的概率从31%提高到45%。本研究证明了测试时代码效率优化的有效性，并揭示了强化学习在指导LLMs自我提升代码效率方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:14:29 GMT</pubDate>
</item>
<item>
<title>基于语言模型解释性和不确定性量化提升机器翻译质量评估效率</title>
<link>https://arxiv.org/abs/2505.23183</link>
<guid>https://arxiv.org/abs/2505.23183</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究利用语言模型解释性与不确定性量化提升机器翻译质量评估效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过语言模型的解释性和不确定性量化技术，以更高效的方式识别机器翻译中的错误片段，从而替代传统昂贵的质量评估方法。这些传统方法通常依赖大型语言模型的提示或大量人工标注数据的训练。研究对12种翻译方向下的14项指标进行了评估，发现人类标签变化对评估性能有显著影响。实验结果表明，无监督评估方法具有未被充分挖掘的潜力，而当面临标签不确定性时，监督方法存在不足，单一标注者评价实践也显得脆弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23183" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 03:20:36 GMT</pubDate>
</item>
<item>
<title>Re-ttention：通过利用时间冗余实现视觉生成模型的极高稀疏注意力</title>
<link>https://arxiv.org/abs/2505.22918</link>
<guid>https://arxiv.org/abs/2505.22918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Re-ttention方法，在不影响视觉质量的前提下实现极高的稀疏注意力。</p><br /><br /><p><strong>摘要：</strong> 扩散Transformer（DiT）已成为生成高质量视频和图像的主流模型，但其计算瓶颈在于注意力机制，其复杂度随分辨率和视频长度呈平方级增长。现有稀疏注意力技术在极高稀疏水平下无法保持视觉质量且可能带来显著计算开销。为解决此问题，本文提出Re-ttention，利用扩散模型的时间冗余来克服注意力机制中的概率归一化偏移，通过重塑注意力得分来维持全量二次注意力的视觉质量。实验表明，Re-ttention在推理过程中仅需3.1%的tokens，优于FastDiTAttn、Sparse VideoGen和MInference等当代方法。此外，该方法在H100 GPU上实现了超过45%的端到端延迟和超过92%的自注意力延迟减少，且开销可忽略不计。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:39:12 GMT</pubDate>
</item>
<item>
<title>When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy</title>
<link>https://arxiv.org/abs/2505.22888</link>
<guid>https://arxiv.org/abs/2505.22888</guid>
<content:encoded><![CDATA[
Recent Large Reasoning Models (LRMs) with thinking traces have shown strong performance on English reasoning tasks. However, their ability to think in other languages is less studied. This capability is as important as answer accuracy for real world applications because users may find the reasoning trace useful for oversight only when it is expressed in their own language. We comprehensively evaluate two leading families of LRMs on our XReasoning benchmark and find that even the most advanced models often revert to English or produce fragmented reasoning in other languages, revealing a substantial gap in multilingual reasoning. Prompt based interventions that force models to reason in the users language improve readability and oversight but reduce answer accuracy, exposing an important trade off. We further show that targeted post training on just 100 examples mitigates this mismatch, though some accuracy loss remains. Our results highlight the limited multilingual reasoning capabilities of current LRMs and outline directions for future work. Code and data are available at https://github.com/Betswish/mCoT-XReasoning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 17:44:12 GMT</pubDate>
</item>
<item>
<title>CLIPGaussians：一种多模态风格迁移框架</title>
<link>https://arxiv.org/abs/2505.22854</link>
<guid>https://arxiv.org/abs/2505.22854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个支持文本和图像引导的多模态风格迁移框架。</p><br /><br /><p><strong>摘要：</strong> Gaussian Splatting (GS) 是一种高效的 3D 场景渲染方法，但其风格迁移仍具挑战性。本文介绍 CLIPGaussians，这是一种针对 2D 图像、视频、3D 对象和 4D 场景的统一风格迁移框架。该方法直接作用于高斯基元，无需大型生成模型或重新训练即可集成到现有 GS 流程中。CLIPGaussians 能实现 3D 和 4D 环境下的颜色和几何联合优化，并在视频中保持时间一致性，同时保持模型大小。实验表明，它在所有任务中表现出卓越的风格保真度和一致性，验证了其作为通用高效多模态风格迁移解决方案的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 16:41:24 GMT</pubDate>
</item>
<item>
<title>VidText：视频文本理解的新基准</title>
<link>https://arxiv.org/abs/2505.22810</link>
<guid>https://arxiv.org/abs/2505.22810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VidText新基准，评估视频文本理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视频理解基准大多忽视文本信息，而OCR特定基准仅限于静态图像，无法充分捕捉文本与动态视觉背景间的交互。为填补这一空白，我们提出了VidText，这是一个针对视频文本理解进行全面深入评估的新基准。VidText涵盖广泛的真实场景，支持多语言内容，提供多层次评估框架，并引入配对感知推理任务。实验显示当前模型在大多数任务上表现不佳，存在显著改进空间。分析表明模型内在因素（如输入分辨率、OCR能力）和外部因素（如辅助信息使用、推理策略）的影响。我们希望VidText能弥补现有基准的不足，并为未来动态环境中多模态推理研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 15:39:35 GMT</pubDate>
</item>
<item>
<title>FAMA：首个开源科学语音基础模型</title>
<link>https://arxiv.org/abs/2505.22759</link>
<guid>https://arxiv.org/abs/2505.22759</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布首个开源语音基础模型FAMA，性能媲美现有模型且速度提升8倍。</p><br /><br /><p><strong>摘要：</strong> 随着像Whisper和SeamlessM4T这样的语音基础模型的发展，语音处理领域取得了显著进步，但其封闭性质限制了可复现性和公平评估。尽管其他研究领域已通过开放科学取得进展，但语音领域的类似努力仍显不足。为填补这一空白，我们推出了FAMA，这是首个面向英语和意大利语的开源科学语音基础模型家族，训练数据超过15万小时的开源语音数据。此外，我们还发布了包含1.6万小时清理后伪标签语音的新数据集。实验结果显示，FAMA在性能上可与现有模型相媲美，同时运行速度提高了8倍。所有代码、数据集和模型均以符合开源许可的方式发布，推动了语音技术研究的开放性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22759" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:19:34 GMT</pubDate>
</item>
<item>
<title>KronSAE：通过Kronecker分解提升稀疏自编码器效率</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出KronSAE架构，利用Kronecker分解减少稀疏自编码器的计算开销。</p><br /><br /><p><strong>摘要：</strong> 稀疏自编码器(SAEs)在解释语言模型隐藏状态方面表现出巨大潜力，但其训练在大规模场景下具有挑战性，尤其是当字典规模较大时。尽管解码器可以采用稀疏感知核以提高效率，但编码器仍需进行高维线性运算，带来较高的计算成本。为解决这一问题，我们提出了KronSAE，这是一种新颖的架构，通过Kronecker乘积分解来大幅降低内存和计算负担。此外，我们引入了mAND，一种近似二元AND操作的不同iable激活函数，它不仅提升了因子化框架的可解释性，还提高了性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22255" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 07:41:11 GMT</pubDate>
</item>
<item>
<title>Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting</title>
<link>https://arxiv.org/abs/2505.19716</link>
<guid>https://arxiv.org/abs/2505.19716</guid>
<content:encoded><![CDATA[
Existing chain-of-thought (CoT) distillation methods can effectively transfer reasoning abilities to base models but suffer from two major limitations: excessive verbosity of reasoning traces and inadequate adaptability to problem difficulty. Long reasoning traces significantly increase inference costs, and uniform-length solutions prevent base models from learning adaptive reasoning strategies. To address these issues, we propose a difficulty-aware prompting (DAP) method to dynamically shorten reasoning traces without performance loss. In our approach, a large teacher model first judges each problem's difficulty and then rewrites its reasoning traces to an appropriate shorter length, yielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we curate a distilled dataset called LiteCoT consisting of 100K concise reasoning examples, with solutions averaging only 720 tokens (an order of magnitude shorter than typical CoTs). Using LiteCoT, we distilled a new family of reasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5 architecture. Experiments show that a student model fine-tuned on just 100K of these difficulty-pruned CoT samples outperforms a model distilled on 800K original Long CoT samples, while significantly reducing training and inference costs. Our method also generalizes well: across 11 diverse benchmarks, the shorter difficulty-aware CoTs achieve equal or better accuracy than Long chains, using far fewer tokens. For example, on the challenging AIME24 exam, our approach reaches 74.2% Pass@1 using only about 5K inference tokens, surpassing other methods that consume many more tokens. Our code and data are available at https://github.com/Evanwu1125/LiteCoT.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:04:44 GMT</pubDate>
</item>
<item>
<title>VBenchComp：用于评估视频大模型时间推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.14321</link>
<guid>https://arxiv.org/abs/2505.14321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VBenchComp，解决现有视频理解基准混淆知识和图像问题的问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频理解基准通常混淆知识型和纯图像型问题，未能明确区分模型的时间推理能力，这是视频理解区别于其他模态的关键方面。我们发现两个主要问题：强语言先验和时间不变性，导致高分未必真正反映对动态内容的理解。为解决这些问题，我们提出了VBenchComp，通过自动化管道将问题分类为LLM可回答、语义型和时间型，其余归为其他类。这种方法能够更精细地评估视频大模型的不同能力。我们的分析揭示了传统总体分数掩盖的模型弱点，并提供了对未来基准设计的见解和建议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:07:55 GMT</pubDate>
</item>
<item>
<title>ZeroGUI：无需人工成本的图形用户界面自动化训练框架</title>
<link>https://arxiv.org/abs/2505.23762</link>
<guid>https://arxiv.org/abs/2505.23762</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种零人工成本的在线学习框架ZeroGUI，提升图形用户界面代理性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有纯视觉图形用户界面（GUI）代理训练方法依赖手工标注和适应性差的问题，提出了ZeroGUI，这是一种可扩展的在线学习框架，通过基于大型视觉语言模型的任务自动生成功能、奖励评估功能及两阶段强化学习机制，在零人工干预下显著提升了两个先进GUI代理(UI-TARS和Aguvis)在OSWorld和AndroidLab环境中的表现。实验表明，ZeroGUI克服了传统方法的局限性，提高了模型的泛化能力和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23762" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于差分信息分布的直接偏好优化理论分析</title>
<link>https://arxiv.org/abs/2505.23761</link>
<guid>https://arxiv.org/abs/2505.23761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">揭示直接偏好优化的理论基础及其与差分信息分布的关系。</p><br /><br /><p><strong>摘要：</strong> Direct Preference Optimization (DPO) 是一种标准技术，用于以监督方式对齐语言模型与人类偏好。尽管其实证成功，但其对数比率奖励参数化的理论依据仍不完整。本文通过利用差分信息分布 (DID) 解决这一问题，该分布捕捉策略更新过程中的信息增益。首先证明当偏好标签编码将参考策略转换为目标策略所需的差分信息时，DPO 的对数比率奖励成为学习目标策略的最佳形式。其次发现偏好编码差分信息的条件与隐含假设密切相关。最后，通过分析 DID 的熵，我们描述了学习低熵差分信息如何增强策略分布，而高熵差分信息诱导平滑效果。我们在合成实验和真实世界指令跟随数据集中验证了这些理论发现，表明学习高熵差分信息对一般指令跟随至关重要，而学习低熵差分信息有利于知识密集型问答。本文为 DPO 目标、偏好数据结构及由此产生的策略行为提供了统一视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>LoRAShop：基于LoRA的多概念图像编辑框架</title>
<link>https://arxiv.org/abs/2505.23758</link>
<guid>https://arxiv.org/abs/2505.23758</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LoRAShop利用LoRA模型实现多概念图像编辑，提升身份保真度。</p><br /><br /><p><strong>摘要：</strong> LoRAShop是一种创新的多概念图像编辑框架，专为LoRA模型设计。它基于Flux风格扩散变换器内的特征交互模式观察，通过前期前向传递获取每个概念的解耦潜空间掩码，并仅在限定区域内融合相应的LoRA权重，从而实现多个主体或风格的无缝整合，同时保留全局上下文和细节。实验表明，该方法在身份保真度上优于现有基线。LoRAShop无需重新训练且不受外部约束，将个性化扩散模型转变为实用的‘LoRA Photoshop’工具，推动了视觉叙事和创意迭代的新发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23758" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>DeepTheorem：利用自然语言增强大语言模型数学推理能力的综合框架</title>
<link>https://arxiv.org/abs/2505.23754</link>
<guid>https://arxiv.org/abs/2505.23754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DeepTheorem框架，通过强化学习提升大语言模型在非形式定理证明中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DeepTheorem的新框架，旨在利用自然语言提高大型语言模型（LLMs）在非形式定理证明中的数学推理能力。该框架包含一个大规模的基准数据集，由121,000个高质量的国际数学奥林匹克水平的非形式定理及其证明组成，覆盖多个数学领域，并经过严格标注，同时伴随有系统构建的可验证定理变体。我们还设计了一种专门针对非形式定理证明的新型强化学习策略（RL-Zero），利用这些可验证的定理变体激励稳健的数学推理。此外，我们提出了全面的结果和过程评估指标，考察证明的正确性和推理步骤的质量。广泛的实验分析表明，DeepTheorem在现有数据集和监督微调协议上显著提高了LLMs的定理证明性能，达到了最先进的准确度和推理质量。我们的研究结果强调了DeepTheorem在根本上推进自动非形式定理证明和数学探索方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>基于2D观测的空间多模态大语言模型</title>
<link>https://arxiv.org/abs/2505.23747</link>
<guid>https://arxiv.org/abs/2505.23747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需3D数据即可提升空间智能的多模态大语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Spatial-MLLM的新框架，用于仅基于2D观测的视觉空间推理。与依赖CLIP视觉编码器的传统视频多模态大语言模型不同，Spatial-MLLM利用前馈视觉几何基础模型的强大结构先验知识。该模型采用双编码器架构：一个预训练的2D视觉编码器用于提取语义特征，一个初始化自视觉几何模型主干的空问编码器用于提取3D结构特征。通过连接器将两者整合为统一的视觉标记以增强空间理解能力。此外，在推理阶段引入了空间感知帧采样策略，确保模型聚焦于对空间推理至关重要的关键帧。除架构改进外，我们构建了Spatial-MLLM-120k数据集，并通过监督微调和GRPO方法进行训练。实验证明，该模型在多种实际数据集上实现了视觉空间理解和推理任务的最佳性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:04 GMT</pubDate>
</item>
<item>
<title>To Trust Or Not To Trust Your Vision-Language Model's Prediction</title>
<link>https://arxiv.org/abs/2505.23745</link>
<guid>https://arxiv.org/abs/2505.23745</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) have demonstrated strong capabilities in aligning visual and textual modalities, enabling a wide range of applications in multimodal understanding and generation. While they excel in zero-shot and transfer learning scenarios, VLMs remain susceptible to misclassification, often yielding confident yet incorrect predictions. This limitation poses a significant risk in safety-critical domains, where erroneous predictions can lead to severe consequences. In this work, we introduce TrustVLM, a training-free framework designed to address the critical challenge of estimating when VLM's predictions can be trusted. Motivated by the observed modality gap in VLMs and the insight that certain concepts are more distinctly represented in the image embedding space, we propose a novel confidence-scoring function that leverages this space to improve misclassification detection. We rigorously evaluate our approach across 17 diverse datasets, employing 4 architectures and 2 VLMs, and demonstrate state-of-the-art performance, with improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines. By improving the reliability of the model without requiring retraining, TrustVLM paves the way for safer deployment of VLMs in real-world applications. The code will be available at https://github.com/EPFL-IMOS/TrustVLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>MAGREF：基于掩码引导的任意参考多主体视频生成框架</title>
<link>https://arxiv.org/abs/2505.23742</link>
<guid>https://arxiv.org/abs/2505.23742</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架MAGREF，实现高质量多主体视频合成。</p><br /><br /><p><strong>摘要：</strong> 近年来，深度生成模型尤其是扩散模型推动了视频生成技术的进步。然而，基于多个参考主体的视频生成仍面临多主体一致性保持和高质量生成的挑战。本文提出MAGREF框架，通过引入掩码引导机制，在多样参考图像和文本提示条件下实现连贯的多主体视频合成。MAGREF框架包含两个关键创新：区域感知动态掩码机制，使单一模型能够灵活处理多种主体推理；像素级通道连接机制，更好地保留外观特征。实验表明，该方法在复杂多主体场景中表现优于现有开源和商业基线。此外，我们还构建了一个全面的多主体视频基准用于评估。结果表明，MAGREF实现了可扩展、可控且高保真的多主体视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23742" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:58:15 GMT</pubDate>
</item>
<item>
<title>ATLAS：一种高效的长时记忆模块增强Transformer模型</title>
<link>https://arxiv.org/abs/2505.23735</link>
<guid>https://arxiv.org/abs/2505.23735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ATLAS模块，显著提升长上下文理解和序列建模性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统Transformer模型在长序列建模中的瓶颈问题，提出了ATLAS，这是一种具有高容量长时记忆模块，通过优化当前及历史标记来改进记忆管理。基于此设计，我们引入了一组新的深度Transformer架构DeepTransformers，它严格扩展了原始Transformer架构。实验表明，在语言建模、常识推理、召回密集型任务及长上下文理解任务中，ATLAS不仅超越了标准Transformer和近期线性递归模型的表现，还在10M上下文长度的BABILong基准测试中提升了80%的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:57:16 GMT</pubDate>
</item>
<item>
<title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title>
<link>https://arxiv.org/abs/2505.23716</link>
<guid>https://arxiv.org/abs/2505.23716</guid>
<content:encoded><![CDATA[
We introduce AnySplat, a feed forward network for novel view synthesis from uncalibrated image collections. In contrast to traditional neural rendering pipelines that demand known camera poses and per scene optimization, or recent feed forward methods that buckle under the computational weight of dense views, our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi view datasets without any pose annotations. In extensive zero shot evaluations, AnySplat matches the quality of pose aware baselines in both sparse and dense view scenarios while surpassing existing pose free approaches. Moreover, it greatly reduce rendering latency compared to optimization based neural fields, bringing real time novel view synthesis within reach for unconstrained capture settings.Project page: https://city-super.github.io/anysplat/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:49:56 GMT</pubDate>
</item>
<item>
<title>提出新基准VF-Eval评估多模态大语言模型在AI生成内容视频中的能力</title>
<link>https://arxiv.org/abs/2505.23693</link>
<guid>https://arxiv.org/abs/2505.23693</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新基准VF-Eval评估多模态大语言模型在AI生成视频中的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大语言模型（MLLMs）在视频问答方面得到了广泛研究，但现有评估主要集中在自然视频上，忽视了合成视频如AI生成内容（AIGC）。一些视频生成工作依赖MLLMs评估生成质量，然而MLLMs对解释AIGC视频的能力尚未得到充分探索。为此，我们提出了一个新的基准VF-Eval，引入了连贯性验证、错误意识、错误类型检测和推理评估四项任务，以全面评估MLLMs在AIGC视频上的能力。我们在VF-Eval上评估了13个前沿MLLMs，发现即使表现最好的模型GPT-4.1，在所有任务上也难以保持一致的良好性能，这凸显了我们基准的挑战性。此外，为了调查VF-Eval在提升视频生成方面的实际应用，我们进行了重新提示实验（RePrompt），表明使MLLMs更紧密地符合人类反馈可以有助于视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23693" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:31:13 GMT</pubDate>
</item>
<item>
<title>Diffusion via Autoregressive模型：一种新的图像扩散建模范式</title>
<link>https://arxiv.org/abs/2505.23660</link>
<guid>https://arxiv.org/abs/2505.23660</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种将图像扩散过程重新定义为标准自回归预测的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Diffusion via Autoregressive models (D-AR)的新范式，通过将图像扩散过程视为标准的下一个标记预测过程，实现了对图像的自回归建模。首先设计了一种将图像转换为离散标记序列的分词器，这些标记可以解码为像素空间中的去噪扩散步骤。得益于扩散特性，这些标记自然遵循粗到细的顺序，非常适合自回归建模。通过对这些标记进行标准的下一个标记预测，无需修改任何底层设计，就可以实现图像空间中扩散过程的镜像。实验表明，在ImageNet基准上，使用775M Llama骨干网络和256个离散标记的方法达到了2.09的FID分数。该方法支持一致的预览生成部分标记，并且能够在零样本布局控制合成方面表现出色。我们希望这项工作能够激发未来关于视觉合成统一自回归架构的研究，特别是结合大型语言模型的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23660" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 13:09:25 GMT</pubDate>
</item>
<item>
<title>大型推理模型中的幻觉现象研究</title>
<link>https://arxiv.org/abs/2505.23646</link>
<guid>https://arxiv.org/abs/2505.23646</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型推理模型在事实寻求任务中的幻觉现象表现存在争议。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）因其强大的长链推理能力在复杂任务中表现出色。然而，这些模型是否能通过推理能力减少事实寻求任务中的幻觉现象仍存争议。例如，DeepSeek-R1在SimpleQA任务中报告了性能提升，而OpenAI-o3却发现幻觉现象更加严重。本文从三个方面探讨了这一问题：首先，我们对LRMs的幻觉现象进行了全面评估，发现冷启动监督微调和可验证奖励强化学习可以减轻幻觉，而仅使用蒸馏或无冷启动微调的强化学习会引入更多微妙的幻觉；其次，我们分析了不同的后训练管道如何影响LRMs的幻觉现象，发现表面推理重复错误和思考与答案不匹配是影响事实准确性的重要认知行为；最后，我们从模型不确定性角度研究了LRMs的幻觉机制，发现模型不确定性与事实准确性之间的不匹配通常会导致更高的幻觉率。本研究为理解LRMs的幻觉现象提供了初步认识。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23646" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:53:41 GMT</pubDate>
</item>
<item>
<title>基于文本引导扩散模型的零样本音频源分离方法</title>
<link>https://arxiv.org/abs/2505.23625</link>
<guid>https://arxiv.org/abs/2505.23625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过配置良好的预训练扩散模型实现无需微调的音频源分离。</p><br /><br /><p><strong>摘要：</strong> 当前监督深度学习方法在音频源分离任务中受限于大量标注数据的需求且泛化能力有限，而本文受生成基础模型成功的启发，研究了是否可以利用预训练的文本引导音频扩散模型克服这些限制。令人惊讶的是，在适当配置下，纯文本引导的扩散模型能够实现零样本的音频源分离。所提出的方法名为ZeroSep，它通过将混合音频反向投影到扩散模型的潜在空间，并利用文本条件指导去噪过程以恢复单个源信号。ZeroSep无需特定任务的训练或微调，直接重新利用生成扩散模型进行判别性分离任务，并通过丰富的文本先验支持开放集场景。该方法与多种预训练的文本引导音频扩散模型兼容，在多个分离基准测试中表现出色，甚至超过了监督方法的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:31:45 GMT</pubDate>
</item>
<item>
<title>推理时扩展的表格推理研究：基于蒸馏与可验证奖励强化学习的方法</title>
<link>https://arxiv.org/abs/2505.23621</link>
<guid>https://arxiv.org/abs/2505.23621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出两种后训练策略以实现推理时扩展，表R1-Zero模型性能媲美GPT-4.1。</p><br /><br /><p><strong>摘要：</strong> 本文首次探索了表格推理任务中的推理时扩展问题，开发并评估了两种后训练策略：前沿模型推理轨迹蒸馏和可验证奖励强化学习（RLVR）。通过DeepSeek-R1生成的大规模推理轨迹数据集，我们对大型语言模型进行了微调，得到Table-R1-SFT模型；而在RLVR方法中，我们提出了特定任务的可验证奖励函数，并应用GRPO算法获得了Table-R1-Zero模型。这些模型在短形式问答、事实验证和自由形式问答等多样化表格推理任务上表现出色，其中Table-R1-Zero模型在仅使用7B参数的情况下达到了与GPT-4.1和DeepSeek-R1相当甚至更高的性能，并且在跨领域数据集上展现了强大的泛化能力。进一步的消融分析和定性分析揭示了指令微调、模型架构选择以及跨任务泛化的益处，同时显示了在强化学习训练过程中表格推理技能的涌现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:28:50 GMT</pubDate>
</item>
<item>
<title>Muddit：基于离散扩散的统一文本图像生成模型</title>
<link>https://arxiv.org/abs/2505.23606</link>
<guid>https://arxiv.org/abs/2505.23606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Muddit模型，实现文本和图像模态的快速并行生成。</p><br /><br /><p><strong>摘要：</strong> Unified generation models致力于通过单一架构处理跨模态任务，但现有模型存在推理速度慢或泛化能力弱的问题。本文介绍Muddit，一种基于离散扩散的Transformer模型，通过整合预训练的文本到图像骨干网络与轻量级文本解码器，在文本和图像生成方面实现了高效且高质量的多模态生成。实验表明，Muddit在质量和效率上可媲美甚至超越更大规模的自回归模型，展示了纯粹离散扩散模型结合强视觉先验的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:48 GMT</pubDate>
</item>
<item>
<title>EvoScale：通过进化提升小规模语言模型在软件工程任务中的性能</title>
<link>https://arxiv.org/abs/2505.23604</link>
<guid>https://arxiv.org/abs/2505.23604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为EvoScale的方法，提升小规模语言模型在软件工程任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为EvoScale的新方法，旨在通过将生成过程视为进化过程来提高小规模语言模型在实际软件工程任务中的表现。传统方法如监督微调需要高质量但昂贵的数据集，而测试时扩展策略虽然有效但成本高。EvoScale通过迭代选择和变异优化生成输出，显著减少所需样本数量。此外，该方法利用强化学习训练模型自我进化，从而在推理阶段无需依赖外部验证器。实验结果显示，采用EvoScale后，32B参数的Satori-SWE-32B模型在SWE-Bench-Verified基准上达到了超过100B参数模型的性能水平，同时仅需少量样本。代码、数据和模型均计划开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 12:15:36 GMT</pubDate>
</item>
<item>
<title>基于最优奖励基准的对策略强化学习算法</title>
<link>https://arxiv.org/abs/2505.23585</link>
<guid>https://arxiv.org/abs/2505.23585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的强化学习算法OPO，解决大语言模型训练不稳定和计算效率低的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为On-Policy RL with Optimal reward baseline (OPO)的新算法，旨在解决当前强化学习算法在大语言模型训练中的稳定性不足和计算效率低的问题。OPO通过强调精确的对策略训练和引入最优奖励基准来减少梯度方差，从而提高训练稳定性和探索能力。实验表明，OPO在数学推理基准测试中表现出色，且无需额外的辅助模型或正则化项。此外，OPO还减少了策略漂移并提高了输出熵，使得生成的响应更加多样且重复性更低。这些结果表明，OPO为大语言模型的对齐和推理任务提供了一个有前景的方向。相关实现已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:58:04 GMT</pubDate>
</item>
<item>
<title>SafeScientist：强化AI科学家框架的安全性与伦理责任</title>
<link>https://arxiv.org/abs/2505.23559</link>
<guid>https://arxiv.org/abs/2505.23559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SafeScientist框架提升AI驱动科研探索中的安全性和伦理责任。</p><br /><br /><p><strong>摘要：</strong> 本文介绍SafeScientist，一种专门设计用于增强AI科学家框架安全性的创新方法。SafeScientist通过主动拒绝不道德或高风险任务，在整个研究过程中强调安全措施，包括引入多个防御机制如提示监控、协作监控、工具使用监控及伦理审查组件。此外，我们还提出了SciSafetyBench，这是一个用于评估科学领域AI安全性的新基准，涵盖六个领域的240项高风险科学任务及相关工具。实验表明，SafeScientist在保持科研产出质量的同时，将安全性提高了35%，并经受住了多种对抗性攻击测试。该框架的代码和数据将在指定GitHub页面上提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 11:35:58 GMT</pubDate>
</item>
<item>
<title>SWE-bench-Live：面向动态软件修复的大规模可更新基准</title>
<link>https://arxiv.org/abs/2505.23419</link>
<guid>https://arxiv.org/abs/2505.23419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SWE-bench-Live，解决现有基准静态、过时等问题。</p><br /><br /><p><strong>摘要：</strong> SWE-bench-Live是一个全新的可更新基准，由1319个来自GitHub真实问题的任务组成，涵盖93个存储库。它通过自动化管道简化实例创建和环境设置，解决了传统基准如SWE-bench的局限性。在多个最先进的模型上测试表明，SWE-bench-Live中的性能显著优于静态基准，特别是在实时环境中。通过对存储库来源、问题时效性和任务难度的深入分析，揭示了性能差异的原因。SWE-bench-Live为评估大语言模型和智能体在动态软件开发场景中的能力提供了可靠工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:09:44 GMT</pubDate>
</item>
<item>
<title>KVzip：一种高效的Transformer语言模型KV缓存压缩方法</title>
<link>https://arxiv.org/abs/2505.23416</link>
<guid>https://arxiv.org/abs/2505.23416</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">KVzip通过压缩KV缓存提高Transformer模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为KVzip的查询无关的KV缓存淘汰方法，它利用底层的语言模型量化KV对的重要性并淘汰不重要的KV对，从而实现高效复用压缩后的KV缓存。实验表明，KVzip可将KV缓存大小减少3到4倍，将FlashAttention解码延迟降低约2倍，在问答、检索、推理和代码理解等任务中性能损失可以忽略不计。KVzip适用于多种模型如LLaMA3.1-8B、Qwen2.5-14B和Gemma3-12B，且在上下文长度高达17万tokens时表现优异。相比之下，现有的查询感知KV淘汰方法即使在90%缓存预算下多查询场景中也会导致性能下降。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23416" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 09:05:47 GMT</pubDate>
</item>
<item>
<title>UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.23380</link>
<guid>https://arxiv.org/abs/2505.23380</guid>
<content:encoded><![CDATA[
Unified multimodal large language models such as Show-o and Janus have achieved strong performance across both generation and understanding tasks. However, these models typically rely on large-scale datasets and require substantial computation during the pretraining stage. In addition, several post-training methods have been proposed, but they often depend on external data or are limited to task-specific customization. In this work, we introduce UniRL, a self-improving post-training approach. Our approach enables the model to generate images from prompts and use them as training data in each iteration, without relying on any external image data. Moreover, it enables the two tasks to enhance each other: the generated images are used for understanding, and the understanding results are used to supervise generation. We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to optimize the models. UniRL offers three key advantages: (1) it requires no external image data, as all training samples are generated by the model itself during training; (2) it not only improves individual task performance, but also reduces the imbalance between generation and understanding; and (3) it requires only several additional training steps during the post-training stage. We evaluate UniRL on top of Show-o and Janus, achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and models will be released in https://github.com/showlab/UniRL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 08:00:15 GMT</pubDate>
</item>
<item>
<title>VideoReasonBench：评估视觉为中心的复杂视频推理能力</title>
<link>https://arxiv.org/abs/2505.23359</link>
<guid>https://arxiv.org/abs/2505.23359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入VideoReasonBench评估多模态大模型的复杂视频推理性能。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，长链-of-thought（CoT）推理可显著提升大型语言模型（LLMs）在复杂任务中的表现，但在视频理解领域尚未得到验证，因为大多数现有基准缺乏足够的推理深度。本文提出VideoReasonBench，这是一个旨在评估视觉为中心的复杂视频推理的基准。该基准通过设计富含视觉细节且具有高推理复杂度的视频问题，涵盖回忆观察到的视觉信息、推断潜在状态内容及预测视频外信息三个层次。通过对18种最先进的多模态LLMs进行测试，发现大多数模型在此类任务上的表现不佳，而增强思维的Gemini-2.5-Pro表现最佳，达到56.0%的准确率。此外，研究显示扩展思考预算对现有视频基准影响有限，但在VideoReasonBench上至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.23359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 29 May 2025 07:33:43 GMT</pubDate>
</item>
<item>
<title>UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes</title>
<link>https://arxiv.org/abs/2505.23253</link>
<guid>https://arxiv.org/abs/2505.23253</guid>
<content:encoded><![CDATA[
We present UniTEX, a novel two-stage 3D texture generation framework to create high-quality, consistent textures for 3D assets. Existing approaches predominantly rely on UV-based inpainting to refine textures after reprojecting the generated multi-view images onto the 3D shapes, which introduces challenges related to topological ambiguity. To address this, we propose to bypass the limitations of UV mapping by operating directly in a unified 3D functional space. Specifically, we first propose that lifts texture generation into 3D space via Texture Functions (TFs)--a continuous, volumetric representation that maps any 3D point to a texture value based solely on surface proximity, independent of mesh topology. Then, we propose to predict these TFs directly from images and geometry inputs using a transformer-based Large Texturing Model (LTM). To further enhance texture quality and leverage powerful 2D priors, we develop an advanced LoRA-based strategy for efficiently adapting large-scale Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as our first stage. Extensive experiments demonstrate that UniTEX achieves superior visual quality and texture integrity compared to existing approaches, offering a generalizable and scalable solution for automated 3D texture generation. Code will available in: https://github.com/YixunLiang/UniTEX.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 04:58:41 GMT</pubDate>
</item>
<item>
<title>引入Theory of Mind增强的说服模型ToMAP</title>
<link>https://arxiv.org/abs/2505.22961</link>
<guid>https://arxiv.org/abs/2505.22961</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ToMAP，通过增强理论思维提升语言模型的说服力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在说服方面展现出潜力，但现有训练方法仍处于初步阶段。人类擅长动态建模对方的想法，而当前LLMs在理论思维（ToM）推理上存在不足，导致说服多样性有限。为解决此问题，我们提出了ToMAP（Theory of Mind Augmented Persuader），通过两个ToM模块增强对对手心理状态的认知与分析能力。实验显示，尽管ToMAP仅含3B参数，但在多个说服对象模型和语料库上的表现优于更大规模的基线模型GPT-4o，相对提升了39.4%。ToMAP展示了复杂的推理链并减少了重复，使说服更具多样性和有效性。此外，其对手感知特性使其适用于长时间对话，并能采用更逻辑化且有针对性的策略。这些结果验证了ToMAP方法的有效性，并为开发更具说服力的语言代理提供了参考。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22961" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 21:03:41 GMT</pubDate>
</item>
<item>
<title>Multimodal Adversarial Compositionality (MAC)基准测试提升多模态模型鲁棒性</title>
<link>https://arxiv.org/abs/2505.22943</link>
<guid>https://arxiv.org/abs/2505.22943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MAC基准测试，评估多模态模型的组合性漏洞并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Multimodal Adversarial Compositionality (MAC)，这是一个利用大型语言模型生成欺骗性文本样本的新基准，旨在检测和评估跨多种模态的预训练多模态表示（如CLIP）中的组合性脆弱性。通过样本级攻击成功率和组级基于熵的多样性进行评估，MAC揭示了现有模型在处理复杂组合问题时的不足。为改善零样本方法，我们提出了自我训练方法，结合拒绝采样微调和多样性促进过滤技术，显著提升了攻击成功率和样本多样性。实验表明，在较小的语言模型（如Llama-3.1-8B）上，该方法在发现图像、视频和音频等多模态表示中的组合性脆弱性方面表现出色，为提高多模态模型的鲁棒性和安全性提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 19:45:55 GMT</pubDate>
</item>
<item>
<title>多模态CAD重建模型结合视觉语言与强化学习</title>
<link>https://arxiv.org/abs/2505.22914</link>
<guid>https://arxiv.org/abs/2505.22914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合多种输入模态的多模态CAD重建模型。</p><br /><br /><p><strong>摘要：</strong> 计算机辅助设计（CAD）在工程和制造领域至关重要，但现有方法通常只专注于单一输入模态（如点云、图像或文本），限制了其通用性和鲁棒性。本文利用视觉语言模型（VLM）的最新进展，提出了一个多模态CAD重建模型，同时处理三种输入模态。该模型采用两阶段管道：首先在大规模程序生成的数据上进行监督微调（SFT），然后通过在线反馈进行强化学习（RL）微调。我们首次探索了针对CAD任务的LLM在线RL微调，发现如组相对偏好优化（GRPO）等在线算法优于离线替代方案。在DeepCAD基准测试中，我们的SFT模型在所有三种输入模态上均优于现有的单模态方法。经过RL微调后，cadrille在三个具有挑战性的数据集上，包括一个真实世界的数据集，创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 18:32:31 GMT</pubDate>
</item>
<item>
<title>基于合成数据提升语音语言模型对句子重音的理解能力</title>
<link>https://arxiv.org/abs/2505.22765</link>
<guid>https://arxiv.org/abs/2505.22765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入StressTest基准测试，评估现有语音语言模型对句子重音解释的能力并提出改进方法。</p><br /><br /><p><strong>摘要：</strong> 句子重音是指在口语表达中对特定词汇施加强调，以突出或对比某种想法，或者引入新信息。最近，语音感知语言模型(SLMs)的发展使得直接处理音频成为可能，从而绕过转录过程并利用语音信号的丰富性进行音频推理任务，如口语问答。然而，尽管重音在塑造意义和说话者意图方面起着关键作用，在这类模型的评估与开发中却往往被忽视。本研究通过引入StressTest基准测试填补这一空白，评估了几种领先SLMs的表现，发现它们在这类任务上的表现不佳。为解决这一问题，我们提出了一个新的合成数据生成管道，创建了Stress17k训练集，该数据集模拟了由重音变化所暗示的意义改变。实证研究表明，优化这些模型可以很好地适应真实录音，并有效微调SLMs。结果显示，我们的微调模型StresSLM在句子重音推理和检测任务上显著优于现有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 14:32:56 GMT</pubDate>
</item>
<item>
<title>大型语言模型后训练中奖励噪声的影响研究</title>
<link>https://arxiv.org/abs/2505.22653</link>
<guid>https://arxiv.org/abs/2505.22653</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明，大型语言模型对显著奖励噪声具有强鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在后训练大型语言模型时，奖励噪声对模型推理能力的影响。研究发现，即使在数学任务中人为翻转40%的奖励输出，基于Qwen-2.5-7B模型仍能在任务性能上快速收敛至72%的准确率，接近无噪声奖励模型的75%表现。令人惊讶的是，仅通过奖励关键推理短语（如“首先，我需要”）的出现而非答案准确性，模型在下游任务上的表现峰值超过70%，与严格验证正确性的模型相当。结合推理模式奖励（RPR）与噪声奖励模型，可校准奖励模型并减少潜在的误判，提升开放性任务的表现。本研究强调了预训练阶段基础能力的重要性，并为后训练技术的发展提供了新见解。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22653" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>基于双向扩散模型的高效非自回归文本生成</title>
<link>https://arxiv.org/abs/2505.22618</link>
<guid>https://arxiv.org/abs/2505.22618</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的块级近似KV缓存机制，大幅提升扩散语言模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有开源扩散大型语言模型（Diffusion LLMs）在实际推理速度上的不足，尤其是与自回归模型相比的延迟问题，提出了两项创新性改进。首先，我们设计了一种专为双向扩散模型定制的块级近似Key-Value（KV）缓存机制，显著提升了并行解码时的效率，同时保持了极小的性能下降。其次，我们揭示了并行解码导致生成质量下降的根本原因——条件独立假设破坏了令牌依赖关系，并通过引入置信度感知的并行解码策略解决了这一问题，该策略有选择性地对超过置信阈值的令牌进行解码，有效缓解了依赖关系的破坏，从而保证了生成质量。实验结果显示，在LLaDA和Dream等模型上，我们的方法实现了高达27.6倍的吞吐量提升，同时仅造成微小的准确性损失，成功缩小了与自回归模型的性能差距，为扩散语言模型的实际部署铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22618" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:39:15 GMT</pubDate>
</item>
<item>
<title>GeoDrive：提升自动驾驶世界模型的空间感知与安全性</title>
<link>https://arxiv.org/abs/2505.22421</link>
<guid>https://arxiv.org/abs/2505.22421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入3D几何条件，GeoDrive显著提高自动驾驶场景建模的准确性与可靠性。</p><br /><br /><p><strong>摘要：</strong> 近期动态环境模拟领域的发展推动了世界模型的进步，这些模型在自动驾驶中的应用可以预测其他道路使用者的行为并进行风险评估。然而，现有方法存在3D几何一致性不足及遮挡处理中产生伪影的问题，影响了安全评估的可靠性。为解决这些问题，我们提出了GeoDrive，它将鲁棒的3D几何条件整合到驾驶世界模型中，以增强空间理解和行动可控性。具体来说，GeoDrive首先从输入帧中提取3D表示，并基于指定的主车轨迹生成2D渲染。在训练过程中，我们还设计了一个动态编辑模块，通过调整车辆位置来优化渲染效果。实验表明，GeoDrive在动作精度和3D空间感知方面均优于现有模型，从而实现了更真实、灵活且可靠的场景建模，显著提升了自动驾驶的安全性。此外，该模型具备泛化能力，并支持交互式场景编辑功能，如对象编辑和轨迹控制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 10:46:51 GMT</pubDate>
</item>
<item>
<title>SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model</title>
<link>https://arxiv.org/abs/2505.22126</link>
<guid>https://arxiv.org/abs/2505.22126</guid>
<content:encoded><![CDATA[
Recent years have seen rapid advances in AI-driven image generation. Early diffusion models emphasized perceptual quality, while newer multimodal models like GPT-4o-image integrate high-level reasoning, improving semantic understanding and structural composition. Scientific illustration generation exemplifies this evolution: unlike general image synthesis, it demands accurate interpretation of technical content and transformation of abstract ideas into clear, standardized visuals. This task is significantly more knowledge-intensive and laborious, often requiring hours of manual work and specialized tools. Automating it in a controllable, intelligent manner would provide substantial practical value. Yet, no benchmark currently exists to evaluate AI on this front. To fill this gap, we introduce SridBench, the first benchmark for scientific figure generation. It comprises 1,120 instances curated from leading scientific papers across 13 natural and computer science disciplines, collected via human experts and MLLMs. Each sample is evaluated along six dimensions, including semantic fidelity and structural accuracy. Experimental results reveal that even top-tier models like GPT-4o-image lag behind human performance, with common issues in text/visual clarity and scientific correctness. These findings highlight the need for more advanced reasoning-driven visual generation capabilities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 04:51:01 GMT</pubDate>
</item>
<item>
<title>Differentiable Solver Search for Fast Diffusion Sampling</title>
<link>https://arxiv.org/abs/2505.21114</link>
<guid>https://arxiv.org/abs/2505.21114</guid>
<content:encoded><![CDATA[
Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:33:43 GMT</pubDate>
</item>
<item>
<title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title>
<link>https://arxiv.org/abs/2505.20755</link>
<guid>https://arxiv.org/abs/2505.20755</guid>
<content:encoded><![CDATA[
In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a theory-driven framework which we name the \emph{Uni-Instruct}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the f-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded f-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded f-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \emph{1.46} for unconditional generation and \emph{1.38} for conditional generation. On the ImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \emph{1.02}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 01:55:45 GMT</pubDate>
</item>
<item>
<title>One-shot Entropy Minimization</title>
<link>https://arxiv.org/abs/2505.20282</link>
<guid>https://arxiv.org/abs/2505.20282</guid>
<content:encoded><![CDATA[
We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:30 GMT</pubDate>
</item>
<item>
<title>Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking</title>
<link>https://arxiv.org/abs/2505.20199</link>
<guid>https://arxiv.org/abs/2505.20199</guid>
<content:encoded><![CDATA[
Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 12:40:22 GMT</pubDate>
</item>
<item>
<title>Multi-Domain Explainability of Preferences</title>
<link>https://arxiv.org/abs/2505.20088</link>
<guid>https://arxiv.org/abs/2505.20088</guid>
<content:encoded><![CDATA[
Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated method for generating local and global concept-based explanations of preferences across multiple domains. Our method utilizes an LLM to identify concepts that distinguish between chosen and rejected responses, and to represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work establishes a new paradigm for explainability in the era of LLMs.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:01:56 GMT</pubDate>
</item>
<item>
<title>ChartLens: Fine-grained Visual Attribution in Charts</title>
<link>https://arxiv.org/abs/2505.19360</link>
<guid>https://arxiv.org/abs/2505.19360</guid>
<content:encoded><![CDATA[
The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 19:17:32 GMT</pubDate>
</item>
<item>
<title>A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models</title>
<link>https://arxiv.org/abs/2505.19286</link>
<guid>https://arxiv.org/abs/2505.19286</guid>
<content:encoded><![CDATA[
Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 15:34:15 GMT</pubDate>
</item>
<item>
<title>Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator</title>
<link>https://arxiv.org/abs/2505.19236</link>
<guid>https://arxiv.org/abs/2505.19236</guid>
<content:encoded><![CDATA[
Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:25:23 GMT</pubDate>
</item>
<item>
<title>CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays</title>
<link>https://arxiv.org/abs/2505.18087</link>
<guid>https://arxiv.org/abs/2505.18087</guid>
<content:encoded><![CDATA[
Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:44:21 GMT</pubDate>
</item>
<item>
<title>PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions</title>
<link>https://arxiv.org/abs/2505.17818</link>
<guid>https://arxiv.org/abs/2505.17818</guid>
<content:encoded><![CDATA[
Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluated eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3, was validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:34:48 GMT</pubDate>
</item>
<item>
<title>IQBench：评估视觉语言模型在人类智商测试中的推理能力</title>
<link>https://arxiv.org/abs/2505.12000</link>
<guid>https://arxiv.org/abs/2505.12000</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入IQBench基准，评估视觉语言模型在标准化视觉智商测试中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型视觉语言模型(VLMs)在多模态任务中表现优异，但其在人类智商测试中的真正推理能力尚未充分探索。本文介绍了IQBench，这是一个新设计的基准，专注于评估VLMs在标准化视觉智商测试中的推理能力。与以往仅关注最终预测准确性不同，该基准通过评估模型的解释和解题模式，结合最终预测的准确性及人工评价来衡量推理能力。实验结果显示，尽管某些模型如`o4-mini`、`gemini-2.5-flash`和`claude-3.7-sonnet`在平均准确率上表现较好，但在三维空间和变位词推理任务中仍显不足，表明当前VLMs的通用推理能力存在显著局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12000" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 09:24:08 GMT</pubDate>
</item>
<item>
<title>零样本嫁接技术降低视觉语言模型训练成本</title>
<link>https://arxiv.org/abs/2505.22664</link>
<guid>https://arxiv.org/abs/2505.22664</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过小模型迁移策略降低视觉语言模型训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“零样本嫁接”的方法，用于减少视觉语言模型（VLMs）的训练开销。传统方法通常将大型语言模型（LLMs）作为解码器，但这种方法计算负担重且成本高昂。为了解决这个问题，研究者们设计了小型“替代模型”，这些模型继承了目标LLM的浅层网络并共享相同的嵌入空间和表示语言。经过替代模型训练的视觉编码器可以直接迁移到更大的模型上。实验结果显示，这种嫁接方法不仅在某些基准测试中表现优于直接使用小模型，甚至可以达到与完整解码器训练相当的效果。此外，当以Llama-70B作为解码器时，该方法可使总体训练成本降低约45%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22664" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>FastTD3：加速人形机器人强化学习训练的高效算法</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FastTD3通过简单修改显著提升人形机器人在多环境中的训练速度。</p><br /><br /><p><strong>摘要：</strong> 强化学习在机器人领域取得了显著进展，但其复杂性和长时间训练仍是主要瓶颈。本文介绍了一种名为FastTD3的新算法，它通过并行模拟、大批次更新、分布式批评者及精心调优的超参数，在HumanoidBench、IsaacLab和MuJoCo Playground等环境中大幅缩短了人形机器人训练时间，某些任务仅需不到3小时即可完成，且训练过程稳定。此外，我们还提供了FastTD3的轻量化实现，以促进机器人领域的强化学习研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:55:26 GMT</pubDate>
</item>
<item>
<title>PISCES：一种精确擦除语言模型概念知识的新框架</title>
<link>https://arxiv.org/abs/2505.22586</link>
<guid>https://arxiv.org/abs/2505.22586</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PISCES框架，通过直接编辑参数空间中的方向，有效擦除语言模型中的特定概念。</p><br /><br /><p><strong>摘要：</strong> 现有方法在擦除大型语言模型中的不当知识时存在粗略、浅显或无效的问题。本文提出PISCES框架，利用解缠模型将MLP向量分解为可解释特征，并通过自动化可解释技术定位并移除目标概念相关的特征。实验表明，PISCES在Gemma 2和Llama 3.1上对多种概念的擦除效果优于领先方法，将目标概念的准确率降至7.7%，同时显著提高了擦除特异性和鲁棒性，分别提升了31%和38%。整体来看，基于特征的参数内编辑为语言模型中概念知识的更精确和可靠擦除提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22586" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:58:23 GMT</pubDate>
</item>
<item>
<title>HLIP：一种针对3D医学影像的语言-图像预训练框架</title>
<link>https://arxiv.org/abs/2505.21862</link>
<guid>https://arxiv.org/abs/2505.21862</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">HLIP通过分层注意力机制提升了3D医学影像语言-图像预训练的效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HLIP（Hierarchical attention for Language-Image Pre-training）的可扩展预训练框架，用于3D医学影像。HLIP采用轻量级分层注意力机制，模仿放射学数据的自然层次结构（切片、扫描和研究）。该方法在Rad-ChestCT基准测试上展示了强泛化能力，在CT-RATE数据集上预训练后，宏观AUC提高了4.3%。此外，HLIP的计算效率允许直接在未经处理的数据集上进行训练。它在脑部MRI和头部CT数据集上的表现达到当前最佳水平，例如在Pub-Brain-5基准上平衡准确率提升32.4%，在RSNA和CQ500头部CT基准上分别提高1.4%和6.9%的宏观AUC。这些结果表明，HLIP使得直接在未经处理的临床数据集上进行预训练成为3D医学影像领域的一种可行且有效的方法。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21862" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:16:34 GMT</pubDate>
</item>
<item>
<title>DORI基准测试：多模态系统物体方向感知能力评估</title>
<link>https://arxiv.org/abs/2505.21649</link>
<guid>https://arxiv.org/abs/2505.21649</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DORI基准测试揭示现有视觉语言模型在物体方向理解上的局限性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为DORI（辨别性方向推理智能）的新基准测试，专门用于评估物体方向感知能力。DORI通过四个维度（正面对齐、旋转变换、相对方向关系和典型方向理解）来衡量模型的表现。经过对11个数据集的精细设计任务分析，发现最先进的视觉语言模型在粗粒度任务上的准确率仅为54.2%，而在细粒度方向判断上仅为33.0%，且在涉及参考框架转换或复合旋转的任务中表现明显下降。这些结果表明现有模型在内部三维空间表示方面存在严重缺陷，提示需要改进方向表示机制。DORI作为首个针对多模态系统方向意识的诊断框架，具有提升机器人控制、3D场景重建及人机物理交互的重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21649" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 14:22:44 GMT</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Mon, 26 May 2025 19:50:36 GMT</pubDate>
</item>
<item>
<title>HoPE：提升视觉语言模型长上下文能力的混合位置嵌入方法</title>
<link>https://arxiv.org/abs/2505.20444</link>
<guid>https://arxiv.org/abs/2505.20444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出HoPE，一种针对长视频理解的混合位置嵌入方法。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型(VLMs)在多模态任务中取得了显著进展，但在长上下文场景（如长视频）中的表现通常会下降。尽管旋转位置嵌入(RoPE)已被广泛用于大语言模型(LLMs)的长度泛化，但将其扩展到捕捉视频复杂的时空依赖关系仍是一个未解决的挑战。现有方法通常通过不同的频率分配策略来编码三维位置信息，但这些策略主要基于启发式方法，缺乏深入的理论分析。本研究首先探讨了不同分配策略对VLMs长上下文能力的影响，发现当前的多模态RoPE无法可靠地捕获扩展上下文中的语义相似性。为此，我们提出了HoPE，这是一种混合位置嵌入方法，旨在提高VLMs的长上下文能力。HoPE引入了一种混合频率分配策略，以在任意长的上下文中进行可靠的语义建模，并采用动态时间缩放机制，促进在不同上下文长度下的稳健学习和灵活推理。广泛的实验表明，在四个视频基准上的长视频理解和检索任务中，HoPE始终优于现有方法，证明了其有效性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:37:40 GMT</pubDate>
</item>
<item>
<title>大型语言模型在真实文本因果推理中的挑战</title>
<link>https://arxiv.org/abs/2505.18931</link>
<guid>https://arxiv.org/abs/2505.18931</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示顶级大型语言模型在真实文本因果推理上表现不佳。</p><br /><br /><p><strong>摘要：</strong> 理解并推断文本中的因果关系是人类认知的核心方面，也是推动大型语言模型（LLMs）向通用人工智能发展的关键。现有研究多集中于合成文本中明确提到的简单因果关系，未能反映现实任务的复杂性。本文探讨了LLMs是否能在真实世界文本中推断因果关系，并开发了一个来自实际学术文献的数据集，涵盖多样化的文本长度、因果关系复杂性及领域。这是首个针对此任务的真实世界数据集。实验表明，最先进的LLMs在该基准测试中面临重大挑战，最佳模型的平均F1分数仅为0.477。分析揭示了常见问题，如隐含信息的理解困难、区分相关因果因素与上下文细节的能力不足，以及连接分散在长文本中的因果相关信息的难题。通过系统地识别这些缺陷，本研究为LLMs因果推理能力的进一步发展提供了有针对性的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18931" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 21:50:05 GMT</pubDate>
</item>
<item>
<title>个性化安全评估：大语言模型的安全性改进</title>
<link>https://arxiv.org/abs/2505.18882</link>
<guid>https://arxiv.org/abs/2505.18882</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出个性化安全概念，通过PENGUIN基准测试验证其有效性。</p><br /><br /><p><strong>摘要：</strong> 大语言模型通常对相同提示提供相同或相似的响应，在高风险应用场景中可能带来严重安全隐患，因为用户的风险状况差异很大。现有安全性评估主要依赖上下文独立指标（如事实性、偏见或毒性），而忽视了相同响应对不同背景用户的潜在风险差异。本文引入个性化安全概念，并构建PENGUIN基准，包含七个敏感领域的14,000个场景及其上下文版本。评估显示，个性化用户信息可提升安全性评分43.2%，证明个性化方法的有效性。然而，并非所有上下文属性对安全增强同等重要，为此开发RAISE框架，无需重新训练模型即可通过两阶段策略获取用户特定背景，显著提升了六种基础模型的安全评分最高达31.6%，同时平均仅需两次用户查询。研究强调在关键领域中选择性信息收集的重要性，并提供了个性化大语言模型响应的实用解决方案，为适应个体用户情境的安全研究奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18882" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 17:37:10 GMT</pubDate>
</item>
<item>
<title>面向企业专用领域的可扩展硬负采样框架</title>
<link>https://arxiv.org/abs/2505.18366</link>
<guid>https://arxiv.org/abs/2505.18366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种针对企业专用数据的硬负采样框架，显著提升检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了企业搜索系统因语义不匹配和术语重叠导致检索准确性下降的问题，这些问题严重影响了知识管理、客户服务等下游应用的表现。为解决这一挑战，我们提出了一种专门针对企业专用数据的可扩展硬负采样框架。该方法通过动态选择语义上具有挑战性但上下文无关的文档，增强了部署的重新排名模型。我们的方法结合了多种嵌入模型，执行降维操作，并独特地选择了硬负样本，保证了计算效率和语义精确性。在私有企业语料库（云服务领域）上的评估显示，与最先进的基线和其他负采样技术相比，MRR@3提升了15%，MRR@10提升了19%。此外，在公共领域特定数据集（FiQA、Climate Fever、TechQA）上的进一步验证表明，该方法具有广泛的适用性和实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 16:51:20 GMT</pubDate>
</item>
<item>
<title>Transformer架构中的Token缩减：超越效率导向的潜力</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Token缩减在大型生成模型中应超越效率策略，成为生成建模的基本原则。</p><br /><br /><p><strong>摘要：</strong> 在Transformer架构中，Token作为从原始数据中衍生的离散单元，通过将输入分割为固定长度块形成，并映射到嵌入向量以支持并行注意力计算。然而，由于自注意力机制的二次计算复杂性，Token缩减通常被用作提高效率的手段。本文认为，在大型生成模型的时代，Token缩减不应仅限于此，而是应在生成建模中发挥根本性作用。具体而言，Token缩减可以促进多模态集成与对齐、减少过度思考和幻觉现象、维持长输入一致性，并提升训练稳定性等。本文重新定义了Token缩减的意义，提出了包括算法设计、强化学习引导的Token缩减、上下文学习中的Token优化等未来方向，并强调其对推动新模型架构和学习策略的重要性，以增强鲁棒性、提高可解释性并更好地实现生成建模目标。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 07:30:30 GMT</pubDate>
</item>
<item>
<title>Few Shot Domain Adapting Graph：高效文档理解模型</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效视觉丰富文档理解的Few-Shot模型FS-DAG。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Few Shot Domain Adapting Graph（FS-DAG）的模型架构，专门用于在少量样本条件下进行视觉丰富文档理解（VRDU）。该模型通过模块化框架结合领域特定及语言/视觉特定的骨干网络，在有限的数据下适应多种文档类型。FS-DAG对实际部署中的挑战如OCR错误、拼写错误和领域转换具有鲁棒性。实验表明，FS-DAG在信息抽取任务上表现出显著的收敛速度和性能提升，同时参数量低于90M，非常适合资源受限的复杂场景。此外，研究强调了开发更小、更高效的模型而不牺牲性能的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17330" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 18:53:58 GMT</pubDate>
</item>
<item>
<title>强化学习提升大语言模型多轮推理能力的研究</title>
<link>https://arxiv.org/abs/2505.11821</link>
<guid>https://arxiv.org/abs/2505.11821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入细粒度的回合级优势估计策略，显著提升了大语言模型的多轮推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文研究了利用强化学习增强大型语言模型（LLM）推理能力的方法，特别关注多轮工具使用场景，将其建模为马尔可夫决策过程（MDP）。现有方法在多步决策中的回合级信用分配存在困难，限制了其在多轮推理任务中的表现。为此，我们提出了一种细粒度的回合级优势估计策略，用于更精确地分配信用。该策略可融入多种强化学习算法，如组相对偏好优化（GRPO）。实验评估显示，在多轮推理和基于搜索的工具使用任务中，采用GRPO实现的MDP框架及回合级信用分配显著提高了LLM代理在复杂决策环境下的性能，工具执行成功率达到100%，精确答案匹配准确率达50%，远超基线模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 00:09:46 GMT</pubDate>
</item>
<item>
<title>通过大规模知识库CHIMERA探索科学创新中的概念重组</title>
<link>https://arxiv.org/abs/2505.20779</link>
<guid>https://arxiv.org/abs/2505.20779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究者创建了CHIMERA，一个基于科学文献的大规模概念重组知识库。</p><br /><br /><p><strong>摘要：</strong> 人类创新的一个显著特征是重组过程——通过整合现有机制和概念的元素来创造原创想法。本文提出了一种新方法，自动挖掘科学文献并构建了一个名为CHIMERA的知识库，该知识库包含了大量重组实例。CHIMERA可以用来大规模探索科学家如何跨领域重组概念并汲取灵感，或者用于训练机器学习模型以预测新的跨学科创意方向。为了构建这个知识库，我们提出了从科学论文摘要中提取重组信息的新任务，收集了数百份人工标注的高质量摘要数据集，并利用其训练基于大型语言模型（LLM）的提取模型。该模型应用于AI领域的大量论文，生成了超过28,000个重组实例的知识库。我们分析了CHIMERA在AI各子领域中的重组特性，并利用该知识库训练了一个科学假设生成模型，预测现实中研究人员感到启发的新重组方向。我们的数据和代码已公开发布。关键词：ADs</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 02:36:04 GMT</pubDate>
</item>
<item>
<title>基于多模态漫画理解的基准与模型开发</title>
<link>https://arxiv.org/abs/2505.20298</link>
<guid>https://arxiv.org/abs/2505.20298</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出漫画理解的两个新基准并开发专用模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个针对日本漫画（manga）这种复合叙事形式的多模态理解研究。通过构建MangaOCR和MangaVQA两个新基准，分别用于页面内文本识别和基于视觉问答的情境理解评估，其中MangaVQA包含526对高质量问题-答案对。基于这些基准，我们开发了MangaLMM模型，该模型是在开源多模态模型Qwen2.5-VL基础上微调而成，能够同时处理两项任务。实验表明，此模型在理解漫画方面表现优异，并且与GPT-4o和Gemini 2.5等专有模型进行了对比分析，为提升大型多模态模型在漫画领域的应用奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20298" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:59 GMT</pubDate>
</item>
<item>
<title>Influence Distillation: 优化大型语言模型训练的数据选择框架</title>
<link>https://arxiv.org/abs/2505.19051</link>
<guid>https://arxiv.org/abs/2505.19051</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于二阶信息的新型数据选择方法，显著提升LLM训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Influence Distillation的创新框架，用于高效选择大型语言模型（LLMs）的训练数据。该方法通过利用二阶信息来为训练样本分配最优权重，从而指导模型在目标域上的性能表现。我们为梯度下降和Adam优化器推导出这些最优权重，并通过引入基于标志样本的近似方法来降低计算成本。实验表明，该方法在Tulu V2数据集上进行指令微调时，对GSM8k、SQuAD和MMLU等任务的表现达到了或超过了当前最佳水平，同时加快了数据选择速度高达3.5倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19051" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 05:08:00 GMT</pubDate>
</item>
<item>
<title>Sherlock：通过自纠正提升视觉语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.22651</link>
<guid>https://arxiv.org/abs/2505.22651</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为Sherlock的新框架，显著提升了视觉语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过引入自纠正机制来增强视觉语言模型（VLMs）的推理能力。研究发现现有VLMs在处理复杂多模态任务时存在对推理错误敏感、需要大量标注数据等问题。为解决这些问题，我们开发了Sherlock框架，该框架利用轨迹级自纠正目标、基于视觉扰动的偏好数据构建方法以及动态beta值调优，使得模型能够在仅使用少量标注数据的情况下实现自我改进。实验结果显示，在八个基准测试中，Sherlock的直接生成准确率为64.1%，经过自纠正后达到65.4%，优于其他对比模型，且所需标注数据量不到其20%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22651" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>通过未来事件预测提升多模态大模型的时间推理能力</title>
<link>https://arxiv.org/abs/2505.22457</link>
<guid>https://arxiv.org/abs/2505.22457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出未来事件预测任务，利用视频片段自我监督信号提升时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 当前多模态大模型（MLMMs）在处理视频输入时缺乏有效的时间推理学习任务。现有的视频问答任务依赖人工标注或更强的模型，而视频描述则常混淆时间推理与空间信息。为解决这一问题，我们提出了未来事件预测（NEP），该任务利用视频未来片段作为丰富的自我监督信号，鼓励模型进行时间推理。我们构建了一个包含33,000个自动提取视频片段的数据集V1-33K，覆盖多样现实场景。此外，我们研究了多种视频指令微调策略对时间推理的影响，并引入FutureBench评估对未来事件预测的连贯性。实验表明，NEP是一种可扩展且有效的训练范式，可显著提升MLMMs的时间推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 11:13:34 GMT</pubDate>
</item>
<item>
<title>JQL：高效构建大规模高质量多语言数据集的方法</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">JQL通过轻量级标注器提升多语言数据集质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为JQL的系统性方法，用于高效创建大规模且高质量的多语言数据集。传统多语言数据集依赖启发式过滤方法，限制了跨语言迁移性和扩展性。JQL利用预训练多语言嵌入将大型语言模型的标注能力转化为轻量级标注器，这些标注器即使面对训练时未见过的语言和脚本也能表现出强大的多语言和跨语言性能。实验结果显示，在35种语言上的评估表明，JQL的标注流水线显著优于当前的启发式过滤方法如Fineweb2，提升了下游模型训练质量和数据保留率。这项研究为多语言数据集的构建提供了实用见解和宝贵资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22232" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 07:06:54 GMT</pubDate>
</item>
<item>
<title>强化学习中可验证奖励方法的验证器可靠性分析</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有规则验证器存在误判问题，而模型验证器易受攻击，影响RL训练性能。</p><br /><br /><p><strong>摘要：</strong> 本文以数学推理为例，对多种验证器在静态评估及强化学习训练中的表现进行了全面分析。研究显示，当前开源的基于规则的验证器在多个常用数学数据集中无法识别等价但格式不同的答案，导致显著的假阴性率，从而损害了强化学习的训练效果，且这一问题随着策略模型增强愈发严重。随后，我们探索了基于模型的验证器作为解决方案的可能性。尽管静态评估表明这类验证器具有更高的准确性，但在实际强化学习训练中，它们容易受到攻击，表现为将某些响应模式误分类为正确（即假阳性），这被优化策略模型利用，造成人为抬高的奖励。我们的研究揭示了规则验证器和模型验证器各自的风险，为构建更稳健的强化学习奖励系统提供了重要参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 06:28:41 GMT</pubDate>
</item>
<item>
<title>利用预训练语言模型实现抽象结构化推理</title>
<link>https://arxiv.org/abs/2505.22202</link>
<guid>https://arxiv.org/abs/2505.22202</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究预训练语言模型是否能通过学习表示进行抽象推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了预训练的语言模型是否可以通过其已学得的表示被提升到抽象推理空间中，从而能够在句子层面而非原始令牌序列上进行推理。我们提出了一种框架，该框架将预训练的令牌级语言模型适应到句子空间中，通过自回归预测下一个句子的连续嵌入来进行操作。实验评估了两种嵌入范式：基于表面意义的语义嵌入和基于上下文预测的上下文嵌入。在数学、逻辑、常识和规划四个领域内，上下文嵌入在连续推理模式下的表现接近链式思维方法，但推理时的浮点运算减少了平均一半。此外，我们还展示了可扩展性和模块化适配的早期迹象，并引入了一个名为SentenceLens的诊断工具来可视化潜在轨迹。最终结果表明，预训练的语言模型可以在潜在嵌入空间中有效地过渡到抽象的结构化推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22202" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 06:28:35 GMT</pubDate>
</item>
<item>
<title>UniPano: 用于全景图像生成的统一扩散模型适配框架</title>
<link>https://arxiv.org/abs/2505.22129</link>
<guid>https://arxiv.org/abs/2505.22129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示扩散模型适配全景图像生成的关键机制并提出高效框架UniPano。</p><br /><br /><p><strong>摘要：</strong> 最近文本到图像扩散模型（如Stable Diffusion）的成功激发了将其应用于全景图像生成的研究。尽管已有工作展示了通过低秩适应技术改造预训练扩散模型生成全景图像的可行性，但透视图与全景图之间的显著领域差距引发对背后机制的疑问。本文假设并验证了可训练组件在适应全景数据时表现出不同行为，并隐藏了利用预训练模型先验知识的内在机制。分析表明，注意力模块中的查询和键矩阵负责可在两个领域间共享的通用信息，而值矩阵和输出权重矩阵则更专注于将预训练知识适应到全景域。基于这些见解，我们提出了名为UniPano的简单框架，不仅性能超越现有方法，还大幅减少了内存使用和训练时间，为端到端高分辨率全景生成提供了优雅的基线。此外，该框架在效率和效果上均优于先前的双分支方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 04:54:04 GMT</pubDate>
</item>
<item>
<title>基于强化学习的视觉丰富信息检索与推理框架VRAG-RL</title>
<link>https://arxiv.org/abs/2505.22019</link>
<guid>https://arxiv.org/abs/2505.22019</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉感知的强化学习框架，提升复杂视觉信息的推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的基于文本的检索增强生成（RAG）方法难以有效处理视觉相关信息，而当前基于视觉的RAG方法受限于固定管道且模型基础能力激活不足，导致推理效果不佳。本文引入VRAG-RL，这是一种专为复杂视觉信息推理设计的新型强化学习框架。通过该框架，视觉语言模型（VLMs）借助视觉感知标记与搜索引擎交互，自主采样单轮或多轮推理轨迹，并基于这些样本进行持续优化。框架揭示了RAG领域中强化学习的关键限制，包括多模态RAG方法对图像的整合不足及查询表述能力欠缺等问题。为解决这些问题，我们定义了一个针对视觉丰富输入定制的动作空间，并采用融合查询重写和检索性能的奖励机制。最终，VRAG-RL优化了VLMs在RAG任务中的表现，使其更贴近实际应用需求。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22019" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 02:30:51 GMT</pubDate>
</item>
<item>
<title>时间无关统一编码器TiUE提升文本到图像扩散模型推理效率</title>
<link>https://arxiv.org/abs/2505.21960</link>
<guid>https://arxiv.org/abs/2505.21960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的时间无关统一编码器TiUE，显著提升文本到图像扩散模型的推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有文本到图像(T2I)扩散模型在推理速度和图像质量之间存在的权衡问题，分析发现UNet编码器存在冗余计算，并指出解码器更能捕捉丰富的语义信息。基于此，我们首次引入了名为TiUE的时间无关统一编码器，用于学生模型UNet架构，通过共享多个解码器时间步的编码特征实现并行采样，大幅降低推理时间复杂度。此外，还加入了KL散度项来正则化噪声预测，进一步提高生成图像的感知真实性和多样性。实验表明，TiUE在保持计算效率的同时，生成的结果比现有最先进的方法（如LCM、SD-Turbo和SwiftBrushv2）更加多样化且逼真。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:23:22 GMT</pubDate>
</item>
<item>
<title>SVRPBench：首个城市规模车辆路径问题高保真随机动态基准</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVRPBench引入首个捕捉城市尺度车辆路径随机动态的开放基准。</p><br /><br /><p><strong>摘要：</strong> SVRPBench是第一个面向实际物流场景的开放基准，通过模拟时间依赖性拥堵、对数正态延迟、概率事故及基于实证的时间窗口等条件，涵盖了超过500个实例，最多包含1000个客户。该基准测试显示，最先进的强化学习求解器如POMO和AM在分布偏移下性能下降超过20%，而经典和元启发式方法则表现稳健。为了促进可重复研究，项目团队公开了数据集和评估工具包。SVRPBench呼吁社区开发能够超越合成假设并在真实世界不确定性中适应的求解器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21887" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 22:03:31 GMT</pubDate>
</item>
<item>
<title>大语言模型微调机制解析：基于稀疏组件的功能特性研究</title>
<link>https://arxiv.org/abs/2505.21191</link>
<guid>https://arxiv.org/abs/2505.21191</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型微调后指令执行能力提升背后的稀疏计算机制。</p><br /><br /><p><strong>摘要：</strong> 本研究通过设计HexaInst数据集和SPARCOM分析框架，系统性地探索了大语言模型（LLMs）微调过程中指令特定稀疏组件的变化。这些组件包括密集模型中的神经元及Mixture-of-Experts架构中的专家节点。研究发现，这些稀疏组件不仅具有功能普遍性和独特性，还对指令执行起着至关重要的作用。通过实验验证，该工作深入剖析了微调如何重塑LLMs的计算机制，为可信的大语言模型开发提供了理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21191" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:40:28 GMT</pubDate>
</item>
<item>
<title>高效3D场景风格化方法：基于分离架构与身份损失的快速实现</title>
<link>https://arxiv.org/abs/2505.21060</link>
<guid>https://arxiv.org/abs/2505.21060</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用前馈重建模型实现秒级3D场景风格化的创新方法。</p><br /><br /><p><strong>摘要：</strong> 本文解决了在保持多视角一致性的同时快速将风格图像应用于3D场景的问题。传统方法通常需要密集的视角输入图像且计算成本高，而我们利用前馈重建模型实现了在不到一秒的时间内对未定位稀疏视图场景图像进行风格化处理。通过引入分支架构分离结构建模与外观渲染，有效防止风格迁移扭曲底层3D场景结构。此外，通过适配身份损失，我们的模型能够在新型视图合成任务中进行预训练，从而在微调用于风格化时仍保留原始重建能力。实验表明，该方法不仅在风格与场景外观融合上表现优异，而且在多视角一致性和效率方面超越现有技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21060" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 07:47:15 GMT</pubDate>
</item>
<item>
<title>基于代理的智能辅导系统AITEE提升电气工程教育</title>
<link>https://arxiv.org/abs/2505.21582</link>
<guid>https://arxiv.org/abs/2505.21582</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合大型语言模型的代理式辅导系统AITEE显著提升了电气工程教育中的个性化学习效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为AITEE的代理式智能辅导系统，专门针对电气工程教育设计。该系统通过手绘和数字电路支持，结合图相似性度量与增强型Spice仿真技术，为学生提供个性化的学习支持。AITEE采用苏格拉底式对话引导学生自主学习，实验显示其在特定领域知识应用上优于基线方法，证明了代理式导师在规模化、个性化和高效教育中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21582" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 06:07:05 GMT</pubDate>
</item>
<item>
<title>MUSEG：基于强化学习的多片段时间对齐提升视频时间理解</title>
<link>https://arxiv.org/abs/2505.20715</link>
<guid>https://arxiv.org/abs/2505.20715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MUSEG方法，通过时间戳感知的多片段定位增强视频时间理解。</p><br /><br /><p><strong>摘要：</strong> 视频时间理解对多模态大语言模型（MLLMs）进行视频事件推理至关重要。尽管近期在一般视频理解上取得进展，但现有MLLMs在细粒度时间推理方面仍面临挑战。虽然强化学习（RL）最近被探索用于解决此问题，但现有RL方法效果有限。本研究提出MUSEG，一种新颖的基于RL的方法，通过引入时间戳感知的多片段定位来增强时间理解。MUSEG使MLLMs能够将查询与多个相关视频片段对齐，从而促进更全面的时间推理。为了实现有效的学习，我们设计了一个定制的RL训练方案，采用分阶段奖励逐步引导模型进行时间定位推理。广泛的实验表明，MUSEG在时间定位和时间敏感的视频问答任务上显著优于现有方法，并在多样化的时间理解场景中表现出良好的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:50:07 GMT</pubDate>
</item>
<item>
<title>基于LLM的软件工程代理训练数据集及无污染基准构建</title>
<link>https://arxiv.org/abs/2505.20411</link>
<guid>https://arxiv.org/abs/2505.20411</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自动化方法生成大规模真实交互式软件工程任务数据集。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）驱动的软件工程代理在多种任务中展现出潜力，但面临高质量训练数据稀缺及静态基准过时两大挑战。现有数据集规模小且多样性不足，难以反映真实世界场景。为此，本文设计了一套自动化可扩展的流水线，从GitHub仓库提取真实交互式Python软件工程任务，构建了包含超过21,000个任务的SWE-rebench数据集。此外，通过该方法持续收集的新鲜任务，建立了无污染的软件工程代理基准，揭示某些模型性能可能因数据污染而被高估的现象。这一研究为LLM在软件工程中的应用提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20411" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:01:00 GMT</pubDate>
</item>
<item>
<title>UniR：一种通用且高效的轻量级推理模块</title>
<link>https://arxiv.org/abs/2505.19075</link>
<guid>https://arxiv.org/abs/2505.19075</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出UniR模块，通过轻量化推理增强大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniR（Universal Reasoner）的新型推理模块，它是一种轻量级、可组合且即插即用的推理模块，可以与任何冻结的大语言模型（LLM）结合，赋予其专门的推理能力。UniR将奖励分解为独立的推理模块，通过预定义的奖励进行训练，从而将轨迹级别的信号转化为令牌级别的指导。实验结果显示，在数学推理和机器翻译任务上，UniR显著优于现有的基于参数高效微调的方法，尤其是在Llama3.2模型上的表现。此外，UniR展示了强大的弱到强泛化能力，训练在小模型上的推理模块可以有效引导更大的LLM。这使得UniR成为一种成本效益高、适应性强且稳健的解决方案，用于增强LLMs的推理能力而不损害其核心能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19075" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 06:19:10 GMT</pubDate>
</item>
<item>
<title>Chain-of-Zoom：一种可扩展的单图像超分辨率框架</title>
<link>https://arxiv.org/abs/2505.18600</link>
<guid>https://arxiv.org/abs/2505.18600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种模型不可知的框架Chain-of-Zoom，实现极端放大倍率下的高质量图像超分辨率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Chain-of-Zoom (CoZ) 的新框架，旨在解决现有单一图像超分辨率(SISR)模型在高倍率放大时性能下降的问题。CoZ通过将SISR分解为一系列中间尺度状态的自回归链，并结合多尺度感知提示，利用现有的骨干超分辨率模型多次重用来实现极高分辨率的图像重建，而无需额外训练。为了应对高倍率放大时视觉线索减弱的问题，CoZ在每次缩放步骤中引入由视觉语言模型生成的多尺度感知文本提示。此外，通过广义奖励策略优化方法对提示提取器进行微调，使其更符合人类偏好。实验表明，基于标准4倍扩散模型的CoZ能够在保持高水平感知质量和保真度的情况下实现超过256倍的放大效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 04:50:08 GMT</pubDate>
</item>
<item>
<title>First Finish Search：一种高效的推理时间扩展策略</title>
<link>https://arxiv.org/abs/2505.18149</link>
<guid>https://arxiv.org/abs/2505.18149</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的并行解码策略First Finish Search，显著提升大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为First Finish Search（FFS）的新颖训练自由并行解码策略，该策略通过同时启动多个独立样本并在任意一个完成时立即返回，有效提升了大型语言模型在推理阶段的表现。研究发现，对于推理任务而言，较短的解码路径比较长的路径更可能产生正确的答案。FFS在四个不同的推理模型和四个数据集上的实验表明，它能够在保持较低计算成本的同时显著提高准确性。例如，在AIME数据集上，使用DeepSeek-R1模型时，FFS达到了82.23%的准确率，相比单次运行提升了15%，几乎达到OpenAI o4-mini的水平。此外，理论分析解释了为何提前停止于最短路径通常会得到正确答案，并指出了早停可能不适用的情况。这一研究揭示了简单推理时间扩展策略的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18149" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:57:43 GMT</pubDate>
</item>
<item>
<title>基于免疫原理的生成式AI模型虚假信息防控框架</title>
<link>https://arxiv.org/abs/2505.17870</link>
<guid>https://arxiv.org/abs/2505.17870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在训练中引入标注的虚假数据集，生成式AI可增强对虚假信息的识别能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种新颖的生成式人工智能模型训练框架，借鉴生物免疫概念，将经过标注的少量虚假信息作为“疫苗”，通过定期注入到模型微调过程中，以提高模型识别并拒绝误导性陈述的能力，同时保持对真实信息的准确性。这种框架不同于传统方法，它直接利用事实核查过的虚假数据而非输入扰动或通用反馈信号。研究显示，经过免疫处理的模型生成虚假信息的比例显著低于基线模型。此外，文章还探讨了相关的伦理安全措施和治理机制，以确保虚假数据的安全使用。这一创新方法为实现AI系统与事实性的对齐提供了积极的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:20:23 GMT</pubDate>
</item>
<item>
<title>BraInCoRL：利用少量样本预测神经响应的视觉皮层模型</title>
<link>https://arxiv.org/abs/2505.15813</link>
<guid>https://arxiv.org/abs/2505.15813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BraInCoRL通过提示学习在无需微调的情况下预测新对象和刺激的神经反应。</p><br /><br /><p><strong>摘要：</strong> 理解高等视觉皮层的功能表示是计算神经科学中的基本问题。尽管大规模数据集上的预训练人工神经网络在人类神经反应表征上表现出显著的对齐性，但学习视觉皮层的图像可计算模型需要个体水平的大规模功能磁共振成像（fMRI）数据集。高昂的数据获取成本限制了编码器对新受试者和刺激物的泛化能力。本文提出BraInCoRL方法，采用上下文学习的方式，在少量样本示例下预测体素级神经反应，而无需针对新受试者和刺激进行额外的微调。该方法利用可变数量上下文图像刺激的Transformer架构，学习多个受试者的归纳偏置。通过联合条件化图像特征和体素激活，模型直接生成性能更好的高等视觉皮层体素级模型。实验表明，BraInCoRL在低数据环境下优于现有体素级编码器设计，并在全新图像测试中表现出良好的扩展特性。此外，该模型还能推广到全新的视觉fMRI数据集，展示出对语义相关刺激的关注，提升了神经信号的可解释性，并实现了自然语言查询到体素选择性的可解释映射。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>WebDancer: Towards Autonomous Information Seeking Agency</title>
<link>https://arxiv.org/abs/2505.22648</link>
<guid>https://arxiv.org/abs/2505.22648</guid>
<content:encoded><![CDATA[
Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:57:07 GMT</pubDate>
</item>
<item>
<title>大型语言模型在简体与繁体中文表现差异的研究</title>
<link>https://arxiv.org/abs/2505.22645</link>
<guid>https://arxiv.org/abs/2505.22645</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，大型语言模型在简体与繁体中文中的表现存在显著差异。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在简体与繁体中文两种书写形式下的性能差异，这种差异可能影响模型在教育、招聘等领域的应用。为了验证这一假设，我们设计了两个反映现实场景的任务：区域术语选择和人名选择，并测试了11款主流商用及开源模型的表现。结果显示，大多数模型在术语选择任务中偏向简体中文，而在人名选择任务中则更倾向于传统中文。这些偏见可能源于训练数据分布、书写习惯以及中文字形分词方式的不同。我们的研究强调了进一步分析LLMs偏见的重要性，并提供了开源基准数据集以促进未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22645" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:56:49 GMT</pubDate>
</item>
<item>
<title>克服强化学习中大规模语言模型推理障碍：熵管理的重要性</title>
<link>https://arxiv.org/abs/2505.22617</link>
<guid>https://arxiv.org/abs/2505.22617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了强化学习中策略熵崩溃的现象及其对性能的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习（RL）中与大型语言模型（LLMs）推理相关的重大障碍——策略熵崩溃现象。实验发现，在没有熵干预的情况下，策略熵在训练初期急剧下降，导致探索能力减弱并伴随策略表现饱和。通过建立熵与下游表现之间的经验关系式R=-a*e^H+b，表明策略性能是以熵为代价的，且最终受其耗尽限制。为了应对这一问题，我们理论与实证分析了熵动态变化机制，发现策略熵的变化由动作概率与logits变化间的协方差驱动，该协方差在优势策略算法中与优势成正比。实证研究表明，协方差项与熵差异完全匹配，支持理论推导。此外，协方差项在整个训练过程中保持正值，解释了策略熵为何单调递减。基于此机制的理解，我们提出两种简单而有效的技术：Clip-Cov和KL-Cov，分别通过剪裁和施加KL惩罚高协方差令牌来控制熵。实验表明，这些方法有助于增强探索能力，避免熵崩溃并提升下游性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:38:45 GMT</pubDate>
</item>
<item>
<title>通过视觉重建优化图像描述生成的RICO框架</title>
<link>https://arxiv.org/abs/2505.22613</link>
<guid>https://arxiv.org/abs/2505.22613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RICO框架，利用视觉重建提升图像描述准确性与完整性。</p><br /><br /><p><strong>摘要：</strong> 现有图像描述生成方法通常依赖多模态大语言模型（MLLMs），但因细节缺失易产生幻觉与不完整问题。为解决这些局限性，本文提出RICO框架，通过文本到图像模型重构描述并对比原图与重构图，迭代优化描述。此外，还引入RICO-Flash简化计算成本。实验表明，该方法在CapsBench和CompreCap上比多数基线提升了约10%，显著改善了描述的准确性和完整性。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 13:29:34 GMT</pubDate>
</item>
<item>
<title>Thinking with Generated Images</title>
<link>https://arxiv.org/abs/2505.22525</link>
<guid>https://arxiv.org/abs/2505.22525</guid>
<content:encoded><![CDATA[
We present Thinking with Generated Images, a novel paradigm that fundamentally transforms how large multimodal models (LMMs) engage with visual reasoning by enabling them to natively think across text and vision modalities through spontaneous generation of intermediate visual thinking steps. Current visual reasoning with LMMs is constrained to either processing fixed user-provided images or reasoning solely through text-based chain-of-thought (CoT). Thinking with Generated Images unlocks a new dimension of cognitive capability where models can actively construct intermediate visual thoughts, critique their own visual hypotheses, and refine them as integral components of their reasoning process. We demonstrate the effectiveness of our approach through two complementary mechanisms: (1) vision generation with intermediate visual subgoals, where models decompose complex visual tasks into manageable components that are generated and integrated progressively, and (2) vision generation with self-critique, where models generate an initial visual hypothesis, analyze its shortcomings through textual reasoning, and produce refined outputs based on their own critiques. Our experiments on vision generation benchmarks show substantial improvements over baseline approaches, with our models achieving up to 50% (from 38% to 57%) relative improvement in handling complex multi-object scenarios. From biochemists exploring novel protein structures, and architects iterating on spatial designs, to forensic analysts reconstructing crime scenes, and basketball players envisioning strategic plays, our approach enables AI models to engage in the kind of visual imagination and iterative refinement that characterizes human creative, analytical, and strategic thinking. We release our open-source suite at https://github.com/GAIR-NLP/thinking-with-generated-images.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:12:45 GMT</pubDate>
</item>
<item>
<title>ART+: 开创多层透明图像生成的新篇章</title>
<link>https://arxiv.org/abs/2505.22523</link>
<guid>https://arxiv.org/abs/2505.22523</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个高质量多层透明图像数据集及生成模型，提升创意控制力。</p><br /><br /><p><strong>摘要：</strong> 现有的文本到图像生成模型主要集中在单层图像上，而多层透明图像生成因缺乏高质量的数据集发展滞后。本文通过发布首个开放的超高清透明图像数据集PrismLayersPro（包含20万张图像），设计无需训练的合成流水线以及开源多层生成模型ART+，解决了这一难题。技术贡献包括LayerFLUX用于生成高质量单层透明图像，MultiLayerFLUX将多层图像组合成完整图像，并通过过滤和人工筛选提高质量。ART+在用户研究中表现出色，优于原始ART模型，并达到现代单层生成模型的视觉水平。本研究为多层透明图像生成奠定了坚实基础，推动相关研究和应用发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22523" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 12:09:33 GMT</pubDate>
</item>
<item>
<title>基于GRPO的无监督多模态大语言模型后训练框架MM-UPT</title>
<link>https://arxiv.org/abs/2505.22453</link>
<guid>https://arxiv.org/abs/2505.22453</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需标注数据的多模态大语言模型后训练方法MM-UPT。</p><br /><br /><p><strong>摘要：</strong> 本文首次研究了使用稳定可扩展的在线强化学习算法GRPO实现多模态大语言模型（MLLMs）在无监督条件下的持续自我改进。传统方法依赖昂贵的手动标注数据或复杂迭代困难的策略，而我们提出的MM-UPT框架通过引入基于多数投票的自奖励机制替代传统的奖励信号，显著提升了Qwen2.5-VL-7B在MathVista和We-Math等基准测试上的推理能力，且无需人工标注。实验表明，该方法不仅优于先前的无监督基线，甚至接近有监督的GRPO效果。此外，利用模型自身生成的合成问题进一步提升性能，展示了无监督自我改进的潜力。MM-UPT为无外部监督条件下MLLMs的持续增强提供了一种新范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22453" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 11:11:16 GMT</pubDate>
</item>
<item>
<title>Text2Grad：基于文本反馈的细粒度强化学习优化</title>
<link>https://arxiv.org/abs/2505.22338</link>
<guid>https://arxiv.org/abs/2505.22338</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Text2Grad将自然语言反馈转化为梯度信号，实现模型参数的精确调整。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Text2Grad的新方法，该方法通过将自由形式的文本反馈转换为跨度级别的梯度信号，实现了对语言模型的细粒度优化。传统强化学习仅依赖粗略的标量奖励，导致学习过程缓慢且不透明。Text2Grad通过引入三个关键组件：高质量的反馈标注流水线、细粒度奖励模型以及跨度级别策略优化器，将文本反馈转化为可微分的奖励信号，并直接更新模型中出现问题的部分。这种方法不仅提高了任务性能，还增强了模型的可解释性。实验结果显示，Text2Grad在摘要生成、代码生成和问答任务上均优于传统的标量奖励强化学习和仅使用提示的方法。这一成果展示了自然语言反馈作为细粒度策略优化信号的强大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22338" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 09:23:49 GMT</pubDate>
</item>
<item>
<title>基于冷启动的强化学习提升多模态推理能力研究</title>
<link>https://arxiv.org/abs/2505.22334</link>
<guid>https://arxiv.org/abs/2505.22334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出结合监督微调与强化学习的两阶段方法，显著提升多模态语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，大型语言模型的推理能力得益于强化学习的进步，但多模态模型中的‘顿悟’模式可能并非强化学习带来的唯一结果。本研究首先发现这些模式在强化学习前已存在于多模态语言模型中，但未必与推理表现直接相关。基于此，我们设计了一种两阶段方法：先通过监督微调引入结构化的推理模式，再利用GRPO进行强化学习优化。实验结果显示，该方法在多个挑战性多模态推理基准测试中优于单一方法，其中7B规模模型在MathVista上提升了7.1分，在We-Math上提升了7.5分，而3B规模模型的性能也接近更大的7B模型。这项工作为构建先进的多模态推理模型提供了实用指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 09:21:38 GMT</pubDate>
</item>
<item>
<title>Skywork-OR1：通过强化学习提升长链推理能力</title>
<link>https://arxiv.org/abs/2505.22312</link>
<guid>https://arxiv.org/abs/2505.22312</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于DeepSeek-R1的Skywork-OR1模型，显著提高大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Skywork-OR1，一种针对长链思维（CoT）模型的有效且可扩展的强化学习（RL）实现方法。该方法基于DeepSeek-R1-Distill系列模型，显著提升了32B和7B模型的推理准确性，在AIME24、AIME25及LiveCodeBench等基准测试中分别实现了15.0%和13.9%的提升。Skywork-OR1-32B在AIME24和AIME25上优于DeepSeek-R1和Qwen3-32B，而Skywork-OR1-7B和Skywork-OR1-Math-7B也展现出与同类规模模型相当的推理能力。研究还探讨了熵坍塌现象及其对模型性能的影响，验证了缓解早期熵坍塌的重要性。为促进社区研究，所有模型权重、训练代码和数据集均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.22312" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 28 May 2025 08:56:04 GMT</pubDate>
</item>
<item>
<title>RenderFormer：基于Transformer的神经渲染方法</title>
<link>https://arxiv.org/abs/2505.21925</link>
<guid>https://arxiv.org/abs/2505.21925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RenderFormer是一种无需场景特定训练即可实现全局光照效果的神经渲染方法。</p><br /><br /><p><strong>摘要：</strong> RenderFormer是一种创新的神经渲染框架，可以直接从基于三角形的场景表示生成图像，同时支持完整的全局光照效果，且无需针对每个场景进行训练或微调。该方法将渲染过程视为序列到序列的转换任务，通过Transformer架构处理三角形反射属性序列并生成像素块序列。RenderFormer分为两个阶段：第一阶段建模视图无关的三角形间光线传输，第二阶段根据第一阶段的结果将光线束令牌转化为对应的像素值。两种阶段均采用Transformer架构构建，并仅依赖少量先验约束进行学习。我们展示了RenderFormer在不同复杂度的形状和光线传输场景中的表现与评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 23:20:46 GMT</pubDate>
</item>
<item>
<title>EPiC：一种高效精确的视频扩散模型相机控制框架</title>
<link>https://arxiv.org/abs/2505.21876</link>
<guid>https://arxiv.org/abs/2505.21876</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EPiC框架，通过掩码源视频自动生成高质量锚视频，实现精准的3D相机控制。</p><br /><br /><p><strong>摘要：</strong> 近期视频扩散模型（VDMs）中的3D相机控制方法通常依赖于从估计的点云渲染锚视频作为结构化先验，但点云估计误差会导致锚视频不准确，且需要昂贵的相机轨迹标注。为解决这些问题，我们提出了EPiC框架，它通过基于第一帧可见性的掩码源视频自动构建高质量锚视频，无需昂贵的相机轨迹标注。该方法确保了高对齐性，并消除了对相机轨迹注释的需求，可应用于任何野外视频生成图像到视频（I2V）训练对。此外，引入轻量级的Anchor-ControlNet模块，将锚视频引导集成到预训练VDMs的可见区域中，仅占骨干模型参数的不到1%。通过结合提出的锚视频数据和ControlNet模块，EPiC实现了高效的训练，所需参数、训练步数和数据显著减少。尽管训练基于掩码的锚视频，我们的方法在推理时对基于点云的锚视频表现出稳健的泛化能力，实现了精确的3D感知相机控制。EPiC在RealEstate10K和MiraData上的I2V相机控制任务中达到了最先进的性能，并在定量和定性上展示了精确和鲁棒的相机控制能力，同时在零样本情况下对视频到视频场景也表现出强大的泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21876" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 21:45:26 GMT</pubDate>
</item>
<item>
<title>Roads to Rome：高效结合大语言模型与小语言模型的方法</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种神经路由方法，在保持性能的同时大幅提升推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为 Roads to Rome (R2R) 的神经路由方法，旨在有效结合大型语言模型（LLMs）和小型语言模型（SLMs），既保持推理性能又提高效率。研究发现，LLMs 和 SLMs 的推理路径只有少数标记存在差异，因此 R2R 方法仅针对这些关键路径分歧的标记利用 LLMs，其余标记则由 SLM 处理。通过开发自动数据生成管道来标注这些路由标签，R2R 在数学、编码和问答基准测试中表现出色。当平均激活参数规模为 5.6B 时，R2R 超过了 R1-7B 平均准确率的 1.6 倍，甚至超过了 R1-14B 模型的表现。同时，它比 R1-32B 模型快 2.8 倍，且性能相当，推动了测试时间扩展效率的帕累托前沿。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21600" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:57:20 GMT</pubDate>
</item>
<item>
<title>SageAttention2++: A More Efficient Implementation of SageAttention2</title>
<link>https://arxiv.org/abs/2505.21136</link>
<guid>https://arxiv.org/abs/2505.21136</guid>
<content:encoded><![CDATA[
The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:50:36 GMT</pubDate>
</item>
<item>
<title>DeepResearchGym: A Free, Transparent, and Reproducible Evaluation Sandbox for Deep Research</title>
<link>https://arxiv.org/abs/2505.19253</link>
<guid>https://arxiv.org/abs/2505.19253</guid>
<content:encoded><![CDATA[
Deep research systems represent an emerging class of agentic information retrieval methods that generate comprehensive and well-supported reports to complex queries. However, most existing frameworks rely on dynamic commercial search APIs, which pose reproducibility and transparency challenges in addition to their cost. To address these limitations, we introduce DeepResearchGym, an open-source sandbox that combines a reproducible search API with a rigorous evaluation protocol for benchmarking deep research systems. The API indexes large-scale public web corpora, namely ClueWeb22 and FineWeb, using a state-of-the-art dense retriever and approximate nearest neighbor search via DiskANN. It achieves lower latency than popular commercial APIs while ensuring stable document rankings across runs, and is freely available for research use. To evaluate deep research systems' outputs, we extend the Researchy Questions benchmark with automatic metrics through LLM-as-a-judge assessments to measure alignment with users' information needs, retrieval faithfulness, and report quality. Experimental results show that systems integrated with DeepResearchGym achieve performance comparable to those using commercial APIs, with performance rankings remaining consistent across evaluation metrics. A human evaluation study further confirms that our automatic protocol aligns with human preferences, validating the framework's ability to help support controlled assessment of deep research systems. Our code and API documentation are available at https://www.deepresearchgym.ai.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 14:16:13 GMT</pubDate>
</item>
<item>
<title>PIR框架优化语言模型推理能力并减少计算开销</title>
<link>https://arxiv.org/abs/2505.19187</link>
<guid>https://arxiv.org/abs/2505.19187</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PIR框架优化语言模型推理链条，提升精度同时降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过测试时扩展方法展示出色推理能力，尤其在经过链式思维（CoT）数据微调后表现突出。然而，这些推理链条通常包含冗长的人类解题元素，增加了推理过程中的计算负担。本文介绍PIR（基于困惑度的重要性精炼）框架，通过定量评估每一步推理对答案预测置信度的影响，系统性地识别并修剪低重要性的功能步骤，从而生成优化后的训练数据，既保留核心推理路径又减少了冗余。实验表明，基于PIR优化数据微调的模型在多个挑战性推理基准测试（如AIME、AMC和GPQA Diamond）中表现出更高的准确性，同时显著降低了推理所需令牌数量。该方法在不同模型规模、数据源和令牌预算下均展现出良好的泛化性能，为高效部署具备推理能力的LLMs提供了实用解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19187" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 11:17:57 GMT</pubDate>
</item>
<item>
<title>GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains</title>
<link>https://arxiv.org/abs/2505.18700</link>
<guid>https://arxiv.org/abs/2505.18700</guid>
<content:encoded><![CDATA[
Recent advances in Visual Language Models (VLMs) have demonstrated exceptional performance in visual reasoning tasks. However, geo-localization presents unique challenges, requiring the extraction of multigranular visual cues from images and their integration with external world knowledge for systematic reasoning. Current approaches to geo-localization tasks often lack robust reasoning mechanisms and explainability, limiting their effectiveness. To address these limitations, we propose the Geo Reason Enhancement (GRE) Suite, a novel framework that augments VLMs with structured reasoning chains for accurate and interpretable location inference. The GRE Suite is systematically developed across three key dimensions: dataset, model, and benchmark. First, we introduce GRE30K, a high-quality geo-localization reasoning dataset designed to facilitate fine-grained visual and contextual analysis. Next, we present the GRE model, which employs a multi-stage reasoning strategy to progressively infer scene attributes, local details, and semantic features, thereby narrowing down potential geographic regions with enhanced precision. Finally, we construct the Geo Reason Evaluation Benchmark (GREval-Bench), a comprehensive evaluation framework that assesses VLMs across diverse urban, natural, and landmark scenes to measure both coarse-grained (e.g., country, continent) and fine-grained (e.g., city, street) localization performance. Experimental results demonstrate that GRE significantly outperforms existing methods across all granularities of geo-localization tasks, underscoring the efficacy of reasoning-augmented VLMs in complex geographic inference. Code and data will be released at https://github.com/Thorin215/GRE.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 09:48:57 GMT</pubDate>
</item>
<item>
<title>DynToM：评估大型语言模型动态心智理论能力的新基准</title>
<link>https://arxiv.org/abs/2505.17663</link>
<guid>https://arxiv.org/abs/2505.17663</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DynToM基准，评估大型语言模型对动态心理状态的理解和跟踪能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）越来越多地参与人机交互，评估其心智理论（ToM）能力变得至关重要。然而，现有的ToM评估基准主要关注静态心理状态，忽视了现实社交互动中的时间演变。为解决这一问题，本文提出了DynToM，这是一个专门设计用来评估LLMs理解并跟踪互联场景中心理状态时间进程能力的新基准。通过系统化的四步框架，我们生成了包含5500个情景和78100个问题的1100个社交背景，并对其真实性进行了验证。对十种最先进的LLMs的综合评估显示，其平均表现比人类低44.7%，且在跟踪和推理心理状态变化时性能显著下降。这种性能差距揭示了当前LLMs在建模人类心理状态动态性方面存在的根本局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17663" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:27:40 GMT</pubDate>
</item>
<item>
<title>构建HuggingKG知识图谱以促进开源机器学习资源管理</title>
<link>https://arxiv.org/abs/2505.17507</link>
<guid>https://arxiv.org/abs/2505.17507</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开发首个基于Hugging Face社区的大规模知识图谱HuggingKG。</p><br /><br /><p><strong>摘要：</strong> 开源机器学习资源的快速增长推动了信息检索研究的发展，但现有平台如Hugging Face缺乏对结构化表示的明确利用，限制了高级查询和分析能力。为解决这一问题，我们构建了HuggingKG，这是首个由Hugging Face社区构建用于管理机器学习资源的大规模知识图谱，包含260万个节点和620万条边，涵盖了特定领域的关系及丰富的文本属性。基于此，我们进一步提出了HuggingBench，这是一个多任务基准测试，包含三个新的测试集合，用于信息检索任务中的资源推荐、分类和追踪。实验揭示了HuggingKG的独特特性及其衍生任务的特点。这两个资源均已公开可用，有望推动开源资源共享与管理的研究进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17507" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:00:20 GMT</pubDate>
</item>
<item>
<title>Safe-Sora：首个嵌入式AI视频生成水印框架</title>
<link>https://arxiv.org/abs/2505.12667</link>
<guid>https://arxiv.org/abs/2505.12667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Safe-Sora框架，首次将图形水印嵌入视频生成过程。</p><br /><br /><p><strong>摘要：</strong> 随着生成式视频模型的快速发展，可靠保护AI生成内容版权的需求日益增加。尽管在图像合成领域已广泛应用不可见生成式水印技术，但在视频生成中的应用却鲜有探索。为填补这一空白，我们提出了Safe-Sora框架，这是首个直接在视频生成过程中嵌入图形水印的系统。该框架通过引入分层粗到细的自适应匹配机制，将水印图像分割成块并分配至视觉相似度最高的视频帧，同时定位最佳空间区域实现无缝嵌入。此外，通过开发基于三维小波变换的Mamba架构及新颖的空间时间局部扫描策略，实现了水印补丁在视频帧间的时间融合，有效建模了水印嵌入和检索中的长距离依赖关系。据我们所知，这是首次利用状态空间模型进行水印保护，开辟了高效且稳健水印保护的新途径。实验结果表明，Safe-Sora在视频质量、水印保真度和鲁棒性方面均达到当前最优水平。我们将在发布时公开代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 23:31:31 GMT</pubDate>
</item>
<item>
<title>Post Hoc Registers：无需重训的视觉Transformer补救方案</title>
<link>https://arxiv.org/abs/2505.21501</link>
<guid>https://arxiv.org/abs/2505.21501</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效自蒸馏方法Post Hoc Registers，为现有ViT模型添加注册令牌以减少伪影。</p><br /><br /><p><strong>摘要：</strong> 视觉Transformer(ViTs)已成为视觉处理领域的主导架构，但存在与局部语义不一致的伪影令牌问题，影响细粒度定位和结构连贯性任务的表现。本文提出Post Hoc Registers(PH-Reg)，这是一种无需重新训练即可为大型预训练ViT模型添加注册令牌的高效自蒸馏方法。PH-Reg通过冻结教师网络并优化学生网络中的少量权重，利用教师网络生成去噪密集嵌入来指导学生网络学习，从而有效减少了伪影令牌的数量，提升了零样本和线性探测下的分割和深度预测性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21501" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>一种无需验证器的强化学习方法用于大规模语言模型训练</title>
<link>https://arxiv.org/abs/2505.21493</link>
<guid>https://arxiv.org/abs/2505.21493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种无需验证器的强化学习方法，显著提升了大规模语言模型的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于DeepSeek-R1-Zero风格的强化学习方法在大规模语言模型训练中的局限性，特别是其无法直接应用于化学、医疗等实际领域的问题。为解决这一限制，我们提出了Verifier-Free（VeriFree）方法，该方法通过直接优化生成参考答案的概率来替代传统的验证器机制。实验表明，VeriFree不仅在计算需求上有所减少，还在多个基准测试中表现出色，甚至超越了传统基于验证器的方法。此外，我们从多个角度分析了此方法的优势，包括统一模型中策略与隐式验证器的结合，以及作为变分优化方法的应用前景。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:27 GMT</pubDate>
</item>
<item>
<title>基于双重过程理论优化大语言模型推理能力的研究</title>
<link>https://arxiv.org/abs/2505.21097</link>
<guid>https://arxiv.org/abs/2505.21097</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过引入四阶段任务改进大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，通过强化学习应用于数学和编程等领域的问答任务可以提升大型语言模型（LLMs）的推理能力。然而，长上下文长度下LLMs虽表现出搜索行为，但这种行为常缺乏精确性和信心，导致冗长且冗余的回答。受心理学中的双重过程理论启发，我们提出了一种简单修改的问答任务，包括快速思考、验证、慢速思考及总结四个阶段。此任务显著提升了Qwen2.5-1.5B和DeepSeek-R1-Qwen-1.5B的平均准确率，其中Qwen2.5-1.5B在快速思考模式下仅用不到1000个token就达到了26.8%的准确率，展示了显著的推理效率提升。这些发现表明直觉和深思熟虑的推理是两种互补且独立的系统，需要针对性训练。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21097" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 08:22:46 GMT</pubDate>
</item>
<item>
<title>通过渲染反馈强化学习提升可缩放矢量图形生成</title>
<link>https://arxiv.org/abs/2505.20793</link>
<guid>https://arxiv.org/abs/2505.20793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合渲染反馈的强化学习方法显著提升了视觉语言模型生成高质量SVG的能力。</p><br /><br /><p><strong>摘要：</strong> 可缩放矢量图形(SVG)因其代码可解释性成为视觉设计的重要格式，而近期的视觉语言模型(VLM)通过将SVG生成视为代码生成任务实现了高质量生成。然而，由于训练过程中缺乏对渲染图像的观察，现有VLM方法难以生成既忠实又高效的SVG。本文提出了一种名为RLRF的强化学习方法，利用渲染输出与原始输入之间的比较提供评估反馈，从而引导模型生成更精确、高效且语义连贯的SVG。实验表明，RLRF在性能上显著优于监督微调，有效解决了常见失败模式，同时增强了模型的结构化理解和泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 02:56:00 GMT</pubDate>
</item>
<item>
<title>FinTagging：首个全面的XBRL基准测试评估大型语言模型的财务报告结构化信息提取能力</title>
<link>https://arxiv.org/abs/2505.20650</link>
<guid>https://arxiv.org/abs/2505.20650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FinTagging评估大型语言模型在XBRL财务报告中的信息提取与语义对齐能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FinTagging的新基准测试，它旨在评估大型语言模型（LLMs）在基于XBRL的财务报告中的结构化信息提取和语义对齐能力。与之前仅关注叙述文本且将XBRL标签简化为多分类问题的基准不同，FinTagging将XBRL标签问题分解为两个子任务：FinNI用于财务实体提取，FinCL用于基于税制的概念对齐。该基准测试要求模型同时处理非结构化文本和结构化表格中的事实提取，并将其与完整的1万多项US-GAAP税制进行对齐，从而实现现实且细致的评估。我们对多种LLMs进行了零样本设置下的评估，系统分析了其在子任务和整体标签准确性上的表现。结果表明，尽管LLMs在信息提取方面表现出强大的泛化能力，但在细粒度概念对齐上存在困难，特别是在区分密切相关的税制条目时。这些发现揭示了现有LLMs在完全自动化XBRL标签方面的局限性，并强调了改进语义推理和模式感知建模的需求，以满足精确财务披露的要求。相关代码可在GitHub仓库获取，数据则可在Hugging Face仓库找到。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 22:55:53 GMT</pubDate>
</item>
<item>
<title>MMPerspective：评估多模态大语言模型透视几何理解能力的新基准</title>
<link>https://arxiv.org/abs/2505.20426</link>
<guid>https://arxiv.org/abs/2505.20426</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入MMPerspective，首个专门评估多模态大语言模型透视理解能力的基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项名为MMPerspective的研究成果，该成果是首个专门设计用于系统性评估多模态大语言模型（MLLMs）对透视几何理解的基准。MMPerspective通过三个互补维度——透视感知、推理和鲁棒性——共10项精心设计的任务，涵盖了2,711个真实世界及合成图像实例和5,083个问题-答案对。这些任务旨在测试模型的关键能力，如消失点感知、透视类型推理、三维空间中的线关系理解等。通过对43个最先进的MLLMs进行综合评估，研究发现尽管模型在表面级感知任务上表现出色，但在组合推理和在扰动下保持空间一致性方面存在显著局限。此外，分析揭示了模型架构、规模与透视能力之间的有趣模式，强调了鲁棒性瓶颈和逐步提示的好处。MMPerspective为诊断和推进视觉-语言系统的空间理解提供了一个有价值的测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20426" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 14:20:22 GMT</pubDate>
</item>
<item>
<title>MotionPro：用于精确图像到视频运动控制的新型生成模型</title>
<link>https://arxiv.org/abs/2505.20287</link>
<guid>https://arxiv.org/abs/2505.20287</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MotionPro，通过区域轨迹和运动掩码实现精细的运动合成控制。</p><br /><br /><p><strong>摘要：</strong> 动画图像与交互式运动控制在图像到视频（I2V）生成领域备受关注。现有方法通常依赖大高斯核扩展运动轨迹，但无法明确定义移动区域，导致粗略的运动控制且难以区分物体与相机的移动。为解决这些问题，我们提出了MotionPro，这是一种精确的运动控制器，创新性地利用区域轨迹和运动掩码来调节细粒度的运动合成并识别目标运动类别（即物体或相机移动）。MotionPro首先通过跟踪模型估计每个训练视频上的流图，然后采样区域轨迹以模拟推理场景。与通过大高斯核扩展流不同，我们的区域轨迹方法通过直接利用局部区域内的轨迹实现更精确的控制，从而有效表征细粒度运动。同时，运动掩码从预测的流图中导出，捕捉运动区域的整体动态。为了追求自然的运动控制，MotionPro进一步通过特征调制结合区域轨迹和运动掩码加强视频去噪。此外，我们精心构建了一个基准数据集MC-Bench，包含1100个用户注释的图像-轨迹对，用于评估细粒度和对象级别的I2V运动控制。在WebVid-10M和MC-Bench上的广泛实验验证了MotionPro的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20287" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>VLM-3R：一种基于视觉语言模型的统一三维重建框架</title>
<link>https://arxiv.org/abs/2505.20279</link>
<guid>https://arxiv.org/abs/2505.20279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种结合三维重建指令微调的视觉语言模型VLM-3R。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VLM-3R的统一框架，用于处理基于单目视频帧的三维场景理解任务。通过几何编码器生成隐式的三维标记，并利用空间-视觉-视图融合技术及大规模的三维重建指令微调问答对，VLM-3R实现了现实世界空间上下文与语言指令的有效对齐，支持单目三维空间辅助和具身推理。此外，为了评估时间推理能力，还提出了Vision-Spatial-Temporal Intelligence基准，包含五个专注于动态空间关系的任务。实验表明，该模型不仅在视觉空间推理方面表现稳健，还能理解时间维度上的三维情境变化，在准确性与可扩展性上均表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:56:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型红队测试中的能力差距研究</title>
<link>https://arxiv.org/abs/2505.20162</link>
<guid>https://arxiv.org/abs/2505.20162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，攻击者与目标模型的能力差距决定了红队测试的有效性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的能力和自主性增强，通过红队测试识别漏洞变得至关重要。然而，当红队测试转变为弱到强的问题时，传统的提示工程方法可能失效。本研究通过能力差距视角重新定义红队测试，评估超过500对攻击者-目标模型组合的表现。结果显示，能力更强的模型更有效作为攻击者，攻击成功率在目标模型能力超过攻击者后急剧下降，并且攻击成功率与MMLU-Pro基准的社会科学部分表现呈正相关。基于这些趋势，我们推导出一个越狱规模法则，预测固定目标模型的攻击成功率。这些发现表明，固定能力的攻击者可能在未来模型中失去效用，开放源代码模型的增强风险需要被重视，模型提供方需准确衡量和控制模型的说服力和操控能力以限制其作为攻击者的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 12:05:41 GMT</pubDate>
</item>
<item>
<title>ScienceBoard：助力科学发现的大语言模型综合评估平台</title>
<link>https://arxiv.org/abs/2505.19897</link>
<guid>https://arxiv.org/abs/2505.19897</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScienceBoard提出多域环境与基准任务，评估当前大语言模型在科研中的表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）已超越自然语言处理领域，推动跨学科研究发展。特别是计算机交互型代理正在革新科学研究方式。本文介绍ScienceBoard，它由两个部分组成：一是具备动态科学工作流的多域仿真环境，集成专业软件并支持多接口交互；二是涵盖生物化学、天文学等领域的169项真实任务的人类验证基准。尽管最先进的模型如GPT-4o等在部分实验中表现良好，但整体成功率仅为15%，表明现有模型尚不能可靠辅助复杂科研任务。深入分析为改进模型设计提供了方向，ScienceBoard的代码、环境及基准已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19897" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 08:27:27 GMT</pubDate>
</item>
<item>
<title>CoreMatching：融合Token稀疏性和Neuron稀疏性的视觉语言模型高效推理框架</title>
<link>https://arxiv.org/abs/2505.19235</link>
<guid>https://arxiv.org/abs/2505.19235</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Token稀疏性和Neuron稀疏性存在协同作用，并提出CoreMatching框架提升视觉语言模型推理效率。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型（VLMs）在多项任务中表现出色，但其高推理成本成为瓶颈。传统上，Token稀疏性和Neuron稀疏性被视为独立优化路径，然而本研究首次系统探索两者潜在交互。通过分析核心Neuron与核心Token之间的匹配机制，发现二者在推理过程中相互影响、彼此增强。基于此洞察，我们提出CoreMatching框架，通过协同适应性稀疏推理显著提升效率。实验表明，该方法在十项图像理解任务及三种硬件设备上超越现有最优模型，在NVIDIA Titan Xp上实现了5倍浮点运算减少和整体10倍加速。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19235" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:16:34 GMT</pubDate>
</item>
<item>
<title>SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.19094</link>
<guid>https://arxiv.org/abs/2505.19094</guid>
<content:encoded><![CDATA[
DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:11:06 GMT</pubDate>
</item>
<item>
<title>Guided by Gut：高效自引导测试时扩展框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.20325</link>
<guid>https://arxiv.org/abs/2505.20325</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需外部验证器的自引导方法，使小规模模型实现与大规模模型相当的推理精度。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Guided by Gut (GG)的高效自引导测试时扩展(TTS)框架，该框架通过仅依赖内部大型语言模型(LLM)信号(如token级置信度和步进新颖性)的轻量级树搜索实现与过程奖励模型(PRM)相当的性能，而无需昂贵的外部验证器模型。关键创新包括通过目标强化学习微调阶段提高内部置信估计的可靠性。实验表明，GG使参数量为1.5B的小型模型在数学推理基准上达到或超越参数量为32B-70B的大型模型的准确性，同时将GPU内存使用减少高达10倍。与基于PRM的方法相比，GG的推理速度提高了8倍，内存使用降低了4-5倍。此外，与最佳-of-N(BoN)策略相比，GG减少了约50%的KV缓存内存使用，使得TTS技术的部署更加高效和实用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20325" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:19:09 GMT</pubDate>
</item>
<item>
<title>BiomedSQL：评估科学推理的文本转SQL基准</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个用于评估生物医学领域文本转SQL生成中科学推理能力的基准BiomedSQL。</p><br /><br /><p><strong>摘要：</strong> 随着生物医学研究人员越来越多地依赖大规模结构化数据库进行复杂分析，现有文本到SQL系统在将定性科学问题转换为可执行SQL查询时面临挑战，尤其是在需要隐式领域推理的情况下。本文介绍了一个名为BiomedSQL的新基准，它专门设计用于评估真实世界生物医学知识库中的文本到SQL生成中的科学推理能力。BiomedSQL由68,000个基于整合了基因-疾病关联、组学数据因果推断及药物批准记录的大规模BigQuery知识库的问题/SQL查询/答案三元组组成。该基准要求模型推断特定领域的标准，而非单纯依赖语法翻译。我们测试了多种开源和闭源LLMs，并发现GPT-o3-mini和定制多步代理BMSQL分别达到了59.0%和62.6%的执行准确性，远低于90.0%的专家基线。这些结果表明BiomedSQL为提升支持科学发现的文本到SQL系统提供了新基础。该数据集和代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20321" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>VideoGameBench：评估视觉语言模型的人类技能</title>
<link>https://arxiv.org/abs/2505.18134</link>
<guid>https://arxiv.org/abs/2505.18134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示前沿视觉语言模型难以完成经典游戏。</p><br /><br /><p><strong>摘要：</strong> 本研究引入VideoGameBench，一个由10款90年代经典游戏组成的基准测试集，用于评估视觉语言模型（VLMs）在感知、导航和记忆管理等人类自然能力方面的表现。这些游戏仅提供原始视觉输入和高级目标描述，不依赖特定游戏的支持信息。实验表明，目前最先进的VLMs如Gemini 2.5 Pro，在实时交互中表现不佳，仅完成了极小比例的游戏内容。为解决推理延迟问题，还设计了暂停模式的VideoGameBench Lite，但性能提升有限。该基准旨在推动相关领域研究进展，探索模型在复杂任务中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:43:27 GMT</pubDate>
</item>
<item>
<title>CLUE：一种基于冲突与共识的语言模型不确定性解释框架</title>
<link>https://arxiv.org/abs/2505.17855</link>
<guid>https://arxiv.org/abs/2505.17855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLUE是首个通过自然语言解释语言模型预测不确定性的框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CLUE的新框架，用于生成语言模型预测不确定性的自然语言解释。CLUE通过无监督方式识别文本片段之间的主张-证据或证据间冲突与共识，这些因素驱动着模型的预测不确定性。随后，利用提示生成和注意力引导技术将这些关键交互转化为可读解释。实验表明，在三个语言模型和两个事实核查数据集上，CLUE生成的解释比未引导的不确定性解释更具忠实性和一致性。此外，人类评估者认为CLUE的解释更具有帮助性、信息量更大且逻辑上更一致。CLUE无需微调或架构改动，适用于任何白盒语言模型，有助于提高事实核查工作的实用性，并可推广到其他需要复杂推理的任务中。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:06:43 GMT</pubDate>
</item>
<item>
<title>PreMoe：高效部署大规模Mixture-of-Experts模型的新框架</title>
<link>https://arxiv.org/abs/2505.17639</link>
<guid>https://arxiv.org/abs/2505.17639</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PreMoe框架，通过专家剪枝和检索实现大规模MoE模型在内存受限环境下的高效部署。</p><br /><br /><p><strong>摘要：</strong> 本文研究了Mixture-of-Experts（MoE）架构中的专家激活模式，发现其具有显著的任务特定专业化特性。基于此，我们提出了PreMoe框架，它由概率专家剪枝（PEP）和任务适应性专家检索（TAER）两个主要组件构成。PEP利用任务条件期望选择评分（TCESS）量化专家的重要性，从而识别出任务关键专家的最小集合。而TAER则通过预计算和存储针对多样化任务的紧凑专家模式，在接收到用户查询时快速定位相关任务模式并仅加载必要专家，大幅降低内存占用。实验表明，DeepSeek-R1 671B和Pangu-Ultra-MoE 718B等大型MoE模型在经过不同程度的专家剪枝后仍保持较高的精度。该方法在云服务器到消费级设备的各种计算环境中均表现出色，且代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17639" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:59:16 GMT</pubDate>
</item>
<item>
<title>MMMG：面向多模态生成的人类对齐基准测试</title>
<link>https://arxiv.org/abs/2505.17613</link>
<guid>https://arxiv.org/abs/2505.17613</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMMG基准测试，解决多模态生成自动评估难题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MMMG的综合基准测试，专门用于评估涉及图像、音频、文本与图像交织、文本与音频交织四种模态组合的多模态生成模型。该基准测试包含49项任务，其中29项为新开发任务，共涵盖937条指令，旨在系统性评估多模态生成模型的推理能力、可控性等关键特性。通过广泛的验证表明，MMMG与人类评价高度一致，平均一致性达到94.3%。对24种多模态生成模型的基准测试结果显示，尽管最先进的模型GPT Image在图像生成方面达到78.3%的准确率，但在多模态推理和交织生成任务上表现欠佳，同时音频生成领域仍有显著提升空间，为未来研究提供了重要方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17613" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:21:28 GMT</pubDate>
</item>
<item>
<title>SweEval：评估大语言模型在企业场景中的伦理对齐性</title>
<link>https://arxiv.org/abs/2505.17332</link>
<guid>https://arxiv.org/abs/2505.17332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SweEval通过模拟真实场景评估大语言模型是否能抵制不当指令。</p><br /><br /><p><strong>摘要：</strong> 随着企业越来越多地采用大型语言模型（LLMs）用于关键沟通任务，如撰写邮件、销售演示文稿等，如何确保这些模型在全球不同地区正确理解和适应文化及语言背景变得至关重要。企业应用中尤为关注的是降低声誉风险、维护信任并保证合规性，特别是在处理不安全或冒犯性语言时。为此，我们开发了SweEval基准测试，它通过模拟正负语气及正式非正式语境的变化来测试LLMs的表现。特别地，SweEval的提示会明确要求模型包含特定的脏话，以此评估模型遵守还是抗拒此类不当指示的能力，同时考察其在伦理框架、文化差异及语言理解方面的表现。为了推动构建符合道德规范的企业级AI系统，我们公开了该数据集及相关代码资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 18:56:58 GMT</pubDate>
</item>
<item>
<title>AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery</title>
<link>https://arxiv.org/abs/2505.21499</link>
<guid>https://arxiv.org/abs/2505.21499</guid>
<content:encoded><![CDATA[
Vision-Language Model (VLM) based Web Agents represent a significant step towards automating complex tasks by simulating human-like interaction with websites. However, their deployment in uncontrolled web environments introduces significant security vulnerabilities. Existing research on adversarial environmental injection attacks often relies on unrealistic assumptions, such as direct HTML manipulation, knowledge of user intent, or access to agent model parameters, limiting their practical applicability. In this paper, we propose AdInject, a novel and real-world black-box attack method that leverages the internet advertising delivery to inject malicious content into the Web Agent's environment. AdInject operates under a significantly more realistic threat model than prior work, assuming a black-box agent, static malicious content constraints, and no specific knowledge of user intent. AdInject includes strategies for designing malicious ad content aimed at misleading agents into clicking, and a VLM-based ad content optimization technique that infers potential user intents from the target website's context and integrates these intents into the ad content to make it appear more relevant or critical to the agent's task, thus enhancing attack effectiveness. Experimental evaluations demonstrate the effectiveness of AdInject, attack success rates exceeding 60% in most scenarios and approaching 100% in certain cases. This strongly demonstrates that prevalent advertising delivery constitutes a potent and real-world vector for environment injection attacks against Web Agents. This work highlights a critical vulnerability in Web Agent security arising from real-world environment manipulation channels, underscoring the urgent need for developing robust defense mechanisms against such threats. Our code is available at https://github.com/NicerWang/AdInject.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>ExtAgents：一种多智能体框架提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.21471</link>
<guid>https://arxiv.org/abs/2505.21471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多智能体框架ExtAgents，有效扩展知识输入规模且不增加上下文长度。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）在推理与信息获取任务中的局限性展开研究。传统方法受限于模型上下文窗口大小，难以处理大规模外部知识输入，导致性能瓶颈。为解决这一问题，作者开发了名为ExtAgents的多智能体框架，通过分布式的知识同步与推理过程克服现有方法的信息丢失问题。该框架在多跳问答测试集$inftyBench+$及长文档生成等任务中表现优异，显著提升了非训练方法的性能，同时保持高效并行计算优势。未来研究可进一步优化多智能体协调机制，推动实际应用发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:45:04 GMT</pubDate>
</item>
<item>
<title>冻结的大语言模型通过两个嵌入实现多令牌生成</title>
<link>https://arxiv.org/abs/2505.21189</link>
<guid>https://arxiv.org/abs/2505.21189</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，冻结的大语言模型无需自回归即可一次性生成数百个准确令牌。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）是否可以在不依赖自回归的情况下重构长文本。实验表明，当提供两个训练好的嵌入时，冻结的LLMs可以在单次前向传递中生成多达数百个准确的令牌，展示了这些模型一种令人惊讶且尚未充分探索的能力——即一次性生成多个令牌而无需迭代解码。此外，研究还分析了这些嵌入的行为，并揭示了它们所编码的信息类型。尽管这些表示并非对于给定文本唯一，但它们在嵌入空间中形成连接且局部的区域，这一特性暗示了学习专用编码器进入该空间的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21189" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:39:24 GMT</pubDate>
</item>
<item>
<title>ConciseR：通过强化学习实现大型语言模型的简洁推理</title>
<link>https://arxiv.org/abs/2505.21178</link>
<guid>https://arxiv.org/abs/2505.21178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段强化学习框架ConciseR，提升大型语言模型推理效率并减少冗余。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在推理能力上的发展，扩展生成长度成为研究热点。然而，过长且重复的推理链条（CoT）现象普遍存在。本文提出ConciseR，一种两阶段强化学习方法，第一阶段通过Group Relative Policy Optimization优化推理能力，第二阶段利用Length-aware Group Relative Policy Optimization提升表达简洁性。实验表明，ConciseR在多个基准测试中超越现有最先进的推理模型，同时生成更简洁的推理链条。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:29:51 GMT</pubDate>
</item>
<item>
<title>Alita：通过极简预定义与最大化自演化实现通用智能代理</title>
<link>https://arxiv.org/abs/2505.20286</link>
<guid>https://arxiv.org/abs/2505.20286</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种极简设计的通用智能代理Alita，显著提升跨领域适应性和推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Alita的通用智能代理，其设计理念强调“简单即至高精妙”，通过极简的预定义组件与强大的自演化能力，实现了高效的开放式任务执行。Alita仅配备一个用于直接问题求解的基本组件，摒弃了传统复杂工具和工作流的设计，从而大幅提升了通用性和扩展性。此外，通过生成任务相关的模型上下文协议（MCPs），Alita能够自主构建和优化外部能力，进一步增强了其推理能力。在GAIA基准测试中，Alita取得了75.15%的pass@1和87.27%的pass@3的高准确率，分别在Mathvista和PathVQA上达到了74.00%和52.00%的pass@1成绩，展现出卓越的性能表现。未来更多细节将在GitHub项目页面更新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20286" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:58:53 GMT</pubDate>
</item>
<item>
<title>多任务预训练提升蛋白质语言模型性能</title>
<link>https://arxiv.org/abs/2505.20052</link>
<guid>https://arxiv.org/abs/2505.20052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多任务预训练策略显著提高蛋白质语言模型的性能。</p><br /><br /><p><strong>摘要：</strong> 蛋白质语言模型（PLMs）在检测蛋白质序列复杂模式方面表现出色，但单一预训练任务可能限制其信息捕捉能力。尽管添加数据模态或监督目标可提升模型表现，但通常仍集中于去噪处理。本研究开发了Ankh3模型，通过联合优化掩码语言建模和基于蛋白质序列的序列完成两项任务，证明PLMs可以从蛋白质序列中学习到更丰富且可泛化的表征。实验表明，在二级结构预测、荧光性、GB1适应性和接触预测等下游任务上，该模型表现优异，多任务集成使其对蛋白质特性有更全面的理解，从而实现更稳健和精确的预测。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:41:10 GMT</pubDate>
</item>
<item>
<title>基于蛋白质语言模型的蛋白质相互作用亲和力预测架构优化</title>
<link>https://arxiv.org/abs/2505.20036</link>
<guid>https://arxiv.org/abs/2505.20036</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新的蛋白质相互作用数据集及架构，显著提升亲和力预测性能。</p><br /><br /><p><strong>摘要：</strong> 蛋白质-蛋白质相互作用(PPIs)对细胞过程至关重要，但利用蛋白质语言模型(PLMs)进行序列驱动的PPI结合亲和力预测仍面临挑战。本文通过构建高质量PPB-Affinity数据集并评估四种适应性架构(嵌入级联、序列级联、分层池化、池化注意力加法)，采用全微调及轻量级ConvBERT头方法，测试了多种领先PLMs。实验表明，分层池化和池化注意力加法架构优于传统方法，Spearman相关性提升高达12%，凸显复杂架构设计的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20036" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:23:08 GMT</pubDate>
</item>
<item>
<title>基于深度学习与大语言模型的神经退行性痴呆MRI诊断框架</title>
<link>https://arxiv.org/abs/2505.19954</link>
<guid>https://arxiv.org/abs/2505.19954</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合文本报告生成与大语言模型的透明诊断框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对神经退行性痴呆的鉴别诊断难题，提出了一种融合模块化MRI文本报告生成与现代大语言模型（LLMs）的新框架。该框架通过将3D T1加权脑部MRI转换为放射学报告，并利用强化学习优化LLMs的诊断推理能力，从而在保持高预测性能的同时增强诊断透明度。不同于事后解释方法，该框架在推理过程中生成因果相关的诊断理由，支持临床医生的理解与决策。实验表明，该方法在诊断准确性上媲美现有深度学习方法，同时提供可解释的诊断依据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19954" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:18:32 GMT</pubDate>
</item>
<item>
<title>Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval</title>
<link>https://arxiv.org/abs/2505.19650</link>
<guid>https://arxiv.org/abs/2505.19650</guid>
<content:encoded><![CDATA[
Multimodal information retrieval (MIR) faces inherent challenges due to the heterogeneity of data sources and the complexity of cross-modal alignment. While previous studies have identified modal gaps in feature spaces, a systematic approach to address these challenges remains unexplored. In this work, we introduce UNITE, a universal framework that tackles these challenges through two critical yet underexplored aspects: data curation and modality-aware training configurations. Our work provides the first comprehensive analysis of how modality-specific data properties influence downstream task performance across diverse scenarios. Moreover, we propose Modal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive relationships among the instances of different modalities. Our framework achieves state-of-the-art results on multiple multimodal retrieval benchmarks, outperforming existing methods by notable margins. Through extensive experiments, we demonstrate that strategic modality curation and tailored training protocols are pivotal for robust cross-modal representation learning. This work not only advances MIR performance but also provides a foundational blueprint for future research in multimodal systems. Our project is available at https://friedrichor.github.io/projects/UNITE.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 04:09:44 GMT</pubDate>
</item>
<item>
<title>SynLogic：通过逻辑推理数据增强大语言模型的泛化能力</title>
<link>https://arxiv.org/abs/2505.19641</link>
<guid>https://arxiv.org/abs/2505.19641</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SynLogic框架，生成大规模逻辑推理数据，提升大语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SynLogic的数据合成框架及相应数据集，该框架能够大规模生成多样化的逻辑推理数据，涵盖35种不同的逻辑推理任务。SynLogic的独特之处在于其可控性，可以调整数据难度和数量，且所有生成的数据均可以通过简单规则验证，非常适合强化学习中的可验证奖励机制。实验结果显示，基于SynLogic训练的模型在逻辑推理任务上达到领先水平，并显著提升了其他领域如数学和编程任务的训练效率和泛化能力。此外，将SynLogic数据与其他领域的任务数据混合训练，进一步提高了模型的综合表现。这些成果表明SynLogic是一个重要的资源，有助于推动大语言模型的广泛推理能力发展。SynLogic的数据合成管道及相关数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19641" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:59:36 GMT</pubDate>
</item>
<item>
<title>Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression</title>
<link>https://arxiv.org/abs/2505.19433</link>
<guid>https://arxiv.org/abs/2505.19433</guid>
<content:encoded><![CDATA[
Post-training compression reduces the computational and memory costs of large language models (LLMs), enabling resource-efficient deployment. However, existing compression benchmarks only focus on language modeling (e.g., perplexity) and natural language understanding tasks (e.g., GLUE accuracy), ignoring the agentic capabilities - workflow, tool use/function call, long-context understanding and real-world application. We introduce the Agent Compression Benchmark (ACBench), the first comprehensive benchmark for evaluating how compression impacts LLMs' agentic abilities. ACBench spans (1) 12 tasks across 4 capabilities (e.g., WorfBench for workflow generation, Needle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ) and pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B), standard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill). Our experiments reveal compression tradeoffs: 4-bit quantization preserves workflow generation and tool use (1%-3% drop) but degrades real-world application accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation and Energy to systematize analysis. ACBench provides actionable insights for optimizing LLM compression in agentic scenarios. The code can be found in https://github.com/pprp/ACBench.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:49:07 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型中的模态偏差研究</title>
<link>https://arxiv.org/abs/2505.18657</link>
<guid>https://arxiv.org/abs/2505.18657</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨多模态大型语言模型中的模态偏差问题并提出缓解策略。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态大型语言模型（MLLMs）在整合文本、图像等多种模态方面取得了显著进展。然而，这些模型深受模态偏差影响，往往过度依赖语言而忽视视觉等其他模态的信息。本文首先诊断了当前MLLMs中模态偏差的表现形式，接着提出了系统性的研究路线图，并分析了导致模态偏差的关键因素。通过实验验证，我们发现数据特性、主干网络能力不平衡以及训练目标设计不当均加剧了模态偏差现象。为解决这些问题，我们建议采用平衡的训练策略和优化模型架构，促进跨模态信息的有效融合。此外，我们呼吁跨学科合作以推动MLLMs领域的进一步发展，为实现通用人工智能提供支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18657" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 07:49:31 GMT</pubDate>
</item>
<item>
<title>AlphaMed：通过强化学习实现大型语言模型的医学推理能力</title>
<link>https://arxiv.org/abs/2505.17952</link>
<guid>https://arxiv.org/abs/2505.17952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlphaMed无需使用昂贵的有监督微调数据，在公共数据集上通过强化学习实现顶级医学问答性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为AlphaMed的医学领域大型语言模型（LLM），该模型展示了仅通过强化学习（RL）即可实现复杂推理能力的可能性。传统方法通常依赖于从闭源模型（如GPT-4o）蒸馏出的有监督链式思维（CoT）数据进行微调，而AlphaMed则完全避免了这一过程，而是利用基于简单规则的奖励机制，在公开的多选题问答数据集上训练。实验结果显示，AlphaMed在六个医学问答基准测试中取得了最先进的成果，甚至超过了规模更大的闭源模型，例如DeepSeek-V3-671B和Claude-3.5-Sonnet。为了深入理解这一成功的原因，我们围绕三个问题进行了系统分析：1）是否可以通过简单的规则激励推理而不依赖蒸馏的CoT监督？2）数据的数量和多样性如何影响推理能力？3）问题难度如何塑造推理的产生与泛化？研究发现，数据的信息量是推理表现的关键驱动因素，而基于多选题问答数据的极简主义强化学习方法在没有CoT监督的情况下也能有效诱导推理。此外，不同基准测试中的表现差异揭示了当前评估标准的局限性，强调了开发更具挑战性和推理导向的医学问答基准的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:27:37 GMT</pubDate>
</item>
<item>
<title>引入热带注意力机制以增强神经算法推理的鲁棒性</title>
<link>https://arxiv.org/abs/2505.17190</link>
<guid>https://arxiv.org/abs/2505.17190</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出热带注意力机制，提升算法推理在分布外数据上的表现。</p><br /><br /><p><strong>摘要：</strong> 动态规划(DP)算法在组合优化问题中通过最大值、最小值及经典加法进行递归运算，其相关值函数对应于最大半环中的凸多面体。然而，现有的神经算法推理模型依赖于softmax归一化的点积注意力，这种平滑的指数加权方式会模糊这些尖锐的多面体结构，并在分布外(OOD)设置下失效。本文引入了一种新型的热带注意力机制，该机制原生运行于热带几何的最大半环中。我们证明了热带注意力可以近似动态规划类型的组合算法的热带电路。此外，实验表明，使用热带变换器在算法推理任务中增强了长度泛化和值泛化的经验OOD性能，同时在对抗攻击下保持稳定。我们的研究还提出了对抗攻击泛化作为神经算法推理基准测试的第三个维度。结果表明，热带注意力恢复了softmax所缺失的尖锐且尺度不变的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17190" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 14:01:25 GMT</pubDate>
</item>
<item>
<title>R1-Searcher++：一种高效的知识增强型大语言模型框架</title>
<link>https://arxiv.org/abs/2505.17005</link>
<guid>https://arxiv.org/abs/2505.17005</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合内部知识与外部检索的新框架，提升大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为R1-Searcher++的新框架，旨在通过结合大型语言模型（LLMs）的内部知识与外部信息源，解决现有检索增强生成（RAG）方法中存在的成本高、泛化性差等问题。该框架采用两阶段训练策略：首先进行初步格式学习的SFT冷启动阶段，随后进入强化学习阶段，用于动态知识获取。在强化学习阶段，通过结果监督促进探索，引入奖励机制以利用内部知识，并结合记忆机制持续吸收检索到的信息，从而丰富模型的内部知识库。实验结果显示，R1-Searcher++在多个任务上优于传统RAG及推理方法，同时实现了高效的检索增强推理。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17005" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:58:26 GMT</pubDate>
</item>
<item>
<title>检索增强生成中的位置偏差影响研究</title>
<link>https://arxiv.org/abs/2505.15561</link>
<guid>https://arxiv.org/abs/2505.15561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示位置偏差对LLM利用相关信息和抵御干扰信息的影响。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了位置偏差如何影响检索增强生成（Retrieval Augmented Generation）中大型语言模型（LLM）利用相关文档的能力及受到干扰文档影响的程度。通过在三个基准测试上的大量实验表明，最先进的检索管道虽旨在检索相关文档，但往往会将高度干扰性的文档排在前10位，超过60%的查询包含至少一个高干扰性文档。尽管位置偏差在受控环境中被认为非常显著，但在实际场景中其影响较小，因为相关和干扰文档均被惩罚。此外，研究发现试图根据LLM位置偏好重新排列文档的复杂策略并不优于随机排序。这些发现揭示了位置偏差的实际影响及其应对策略的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 10:18:01 GMT</pubDate>
</item>
<item>
<title>NOVA基准测试：评估模型在罕见脑MRI异常检测中的泛化能力</title>
<link>https://arxiv.org/abs/2505.14064</link>
<guid>https://arxiv.org/abs/2505.14064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NOVA基准测试揭示了现有视觉语言模型在罕见脑MRI异常检测中的显著性能下降。</p><br /><br /><p><strong>摘要：</strong> 随着部署的模型越来越多地遇到训练数据之外的输入，如何有效进行分布外检测和开放世界识别成为重要研究方向。然而，现有的基准测试往往忽略了对真正罕见或未知条件的评估，导致模型的实际表现被高估。为了解决这一问题，本文提出了NOVA基准测试，该测试集包含了900个真实的脑部MRI扫描图像，涵盖了281种罕见病理类型及多样化的采集协议。每个案例不仅提供了详细的临床描述，还通过双盲专家标注了病灶区域。NOVA作为纯评估基准，从未用于模型训练，因此能够对模型的分布外泛化能力进行极限测试。实验结果显示，领先的视觉语言模型（如GPT-4o、Gemini 2.0 Flash和Qwen2.5-VL-72B）在NOVA上的表现大幅下滑，表明其在异常定位、视觉描述和诊断推理方面仍存在不足。NOVA为推动模型在未知异常检测方面的进步提供了一个严格的测试平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 04:10:57 GMT</pubDate>
</item>
<item>
<title>基于神经元分析的大语言模型多语言对齐研究</title>
<link>https://arxiv.org/abs/2505.21505</link>
<guid>https://arxiv.org/abs/2505.21505</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法识别多语言神经元并分析大语言模型的多语言推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多语言对齐作为提升大型语言模型（LLMs）多语言能力的有效范式，同时揭示了LLMs中存在特定于语言的神经元。基于此，我们提出了一种新的细粒度神经元识别算法，用于检测语言神经元（包括特定语言神经元和相关语言神经元）及语言无关神经元。此外，通过分析不同类型的神经元分布特性，我们将LLMs的多语言推理过程分为四个部分：多语言理解、共享语义空间推理、多语言输出空间转换和词汇空间输出。研究还分析了对齐前后的模型特性，探讨了“自发多语言对齐”现象。整体而言，本研究基于多种神经元类型进行了全面调查，为更好地理解多语言对齐和LLMs的多语言能力提供了实证结果和有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21505" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>ViewSpatial-Bench：多视角空间定位评估基准</title>
<link>https://arxiv.org/abs/2505.21500</link>
<guid>https://arxiv.org/abs/2505.21500</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有视觉语言模型在跨视角空间推理上表现不佳，新基准显著提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 当前视觉语言模型（VLMs）在处理视觉内容理解和推理方面表现出色，但在需要跨视角理解与空间推理的任务上仍面临重大挑战。研究发现，现有VLMs主要擅长基于相机视角的自中心空间推理，而在需要采用其他实体视角时则难以泛化。为解决这一问题，我们提出了ViewSpatial-Bench，这是一个专门设计用于多视角空间定位识别评估的综合基准，涵盖五类不同任务，并通过自动化的3D标注流水线生成精确的方向标签。对多种VLMs的全面评估显示，这些模型在相机视角任务上的表现尚可，但在人类视角任务中的准确性明显下降。通过对多视角空间数据集进行微调，整体任务性能提升了46.24%，证明了我们的方法的有效性。本研究为具身AI系统的空间智能建立了重要基准，并提供了实证证据表明建模3D空间关系可以增强VLMs的空间理解能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21500" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:59:26 GMT</pubDate>
</item>
<item>
<title>UI-Genie：一种面向GUI代理的自提升框架</title>
<link>https://arxiv.org/abs/2505.21496</link>
<guid>https://arxiv.org/abs/2505.21496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UI-Genie通过奖励模型和自提升管道解决GUI代理中的两大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UI-Genie的自提升框架，旨在应对图形用户界面（GUI）代理中的两个关键挑战：轨迹结果验证困难及高质量训练数据难以扩展。为了解决第一个问题，UI-Genie引入了一个奖励模型UI-Genie-RM，该模型采用图像文本交织架构，高效处理历史上下文并统一动作级和任务级奖励。为了支持UI-Genie-RM的训练，开发了包括基于规则的验证、受控轨迹破坏和难负样本挖掘等故意设计的数据生成策略。针对第二个挑战，自提升管道通过奖励引导探索和动态环境中的结果验证逐步扩展可解决的复杂GUI任务。实验结果显示，UI-Genie在多个GUI代理基准测试中实现了最先进的性能，并展示了高质量的合成轨迹生成能力，无需人工标注。此外，研究团队开源了完整的框架实现和生成的数据集以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:58:06 GMT</pubDate>
</item>
<item>
<title>基于特征最优对齐的多模态大语言模型迁移对抗攻击方法</title>
<link>https://arxiv.org/abs/2505.21494</link>
<guid>https://arxiv.org/abs/2505.21494</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种改进的迁移对抗攻击方法FOA-Attack，显著提升多模态大语言模型的对抗迁移能力。</p><br /><br /><p><strong>摘要：</strong> 针对现有多模态大语言模型（MLLMs）对抗攻击方法中忽视局部信息的问题，本文提出了一种基于特征最优对齐的迁移对抗攻击方法FOA-Attack。该方法通过引入全局特征余弦相似度损失和局部聚类最优传输损失，分别优化粗粒度和细粒度特征对齐，同时结合动态集成模型加权策略增强跨模型的对抗迁移性能。实验表明，FOA-Attack在多种模型上优于当前最先进的方法，尤其在闭源MLLMs上的表现尤为突出。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21494" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:57 GMT</pubDate>
</item>
<item>
<title>DetailFlow：基于粗到细1D自回归的高效图像生成方法</title>
<link>https://arxiv.org/abs/2505.21473</link>
<guid>https://arxiv.org/abs/2505.21473</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型的图像生成方法DetailFlow，通过逐步细化细节显著提升生成效率和质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DetailFlow的粗到细1D自回归(AR)图像生成方法，该方法通过新颖的下一细节预测策略对图像进行建模。通过学习与逐步退化图像监督的分辨率感知标记序列，DetailFlow使生成过程能够从全局结构开始并逐步细化细节。这种方法不仅提供了更自然高效的AR模型生成复杂视觉内容的方式，还通过紧凑的1D AR模型实现了高质量的图像合成，所需标记数量远少于先前的方法如VAR/VQGAN。此外，我们进一步提出了带有自校正的并行推理机制，加速生成速度约8倍，并减少了教师强制监督固有的累积采样误差。在ImageNet 256x256基准测试中，DetailFlow以128个标记达到了2.96 gFID的成绩，优于需要更多标记的VAR和FlexVAR。同时，由于大幅减少的标记数和并行推理机制，DetailFlow的推理速度接近VAR和FlexVAR的两倍。广泛的实验结果证明了DetailFlow在生成质量和效率方面超越现有最先进的方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21473" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:45:21 GMT</pubDate>
</item>
<item>
<title>HoliTom：一种高效的视频大语言模型推理优化框架</title>
<link>https://arxiv.org/abs/2505.21334</link>
<guid>https://arxiv.org/abs/2505.21334</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HoliTom框架，结合内外部LLM剪枝策略，显著降低视频大语言模型计算成本。</p><br /><br /><p><strong>摘要：</strong> 当前视频大语言模型在视频理解方面表现出色，但因冗余视频token导致计算效率低下。现有token剪枝方法虽有所改进，但存在局限性。内LLM剪枝方法如FastV在浅层有额外开销，而外LLM剪枝主要关注局部空间或短时序冗余，未能充分利用视频全局时间动态特性。为解决此问题，我们提出HoliTom，这是一种无需训练的综合token合并框架。它通过全局冗余感知的时间分割进行外LLM剪枝，并结合空间-时间合并，将视觉token减少超90%，大幅减轻LLM计算负担。同时，引入基于内LLMtoken相似性的稳健合并方法，提升性能并兼容外LLM剪枝。实验显示，HoliTom在LLaVA-OneVision-7B上将计算成本降至FLOPs的6.9%，保持原性能的99.1%，并在推理时间及解码吞吐量上实现显著加速。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21334" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:28:45 GMT</pubDate>
</item>
<item>
<title>基于贝叶斯自适应框架的反思性探索强化学习算法</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种新算法BARL，优化语言模型在推理任务中的反射性探索。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通过强化学习训练后展现出强大的推理能力及反射行为，但传统马尔可夫决策过程限制了探索范围且依赖当前状态，导致反射性推理是否能在训练期间出现尚不明确。为此，研究者将反射性探索引入贝叶斯自适应强化学习框架，该方法通过后验分布优化预期回报，同时激励奖励最大化和信息收集。所提出的BARL算法指导模型根据观察到的结果切换策略，显著提升了合成与数学推理任务中的测试表现，表现出更高的令牌效率和探索有效性。实验代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 18:51:00 GMT</pubDate>
</item>
<item>
<title>DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response</title>
<link>https://arxiv.org/abs/2505.19973</link>
<guid>https://arxiv.org/abs/2505.19973</guid>
<content:encoded><![CDATA[
Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:35:37 GMT</pubDate>
</item>
<item>
<title>基于全局绝对关节坐标的文本到运动生成模型</title>
<link>https://arxiv.org/abs/2505.19377</link>
<guid>https://arxiv.org/abs/2505.19377</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的文本到运动生成方法，使用全局绝对关节坐标代替传统相对表示。</p><br /><br /><p><strong>摘要：</strong> 现有的文本到运动生成模型通常采用HumanML3D推广的基于骨盆相对及帧间局部相对的运动表示方式，但这种方式对扩散模型存在局限性且不适用于下游任务。本文重新审视运动表示方法，提出使用全局空间中的绝对关节坐标作为替代方案。通过系统分析设计选择，我们发现此方法不仅提高了运动保真度和文本对齐效果，还增强了模型的可扩展性，即使使用简单的Transformer结构且无需辅助的运动感知损失函数。此外，我们的方法自然支持诸如文本驱动的运动控制和时间/空间编辑等下游任务，而无需额外的任务特定重构和昂贵的分类器引导生成。最后，我们在直接从文本生成SMPL-H网格顶点的运动方面展示了有前景的泛化能力，为未来的研究和相关应用奠定了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19377" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 20:36:00 GMT</pubDate>
</item>
<item>
<title>ComfyMind：一种协作式通用生成AI系统</title>
<link>https://arxiv.org/abs/2505.17908</link>
<guid>https://arxiv.org/abs/2505.17908</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ComfyMind通过引入语义工作流接口和搜索树规划机制，提升生成模型的稳定性和灵活性。</p><br /><br /><p><strong>摘要：</strong> 随着生成模型的快速发展，跨模态统一任务的通用生成方法受到广泛关注。然而，现有的开源框架由于缺乏结构化的工作流规划和执行级反馈，难以支持复杂的真实世界应用。为解决这些问题，本文介绍了一种名为ComfyMind的新系统，该系统基于ComfyUI平台构建，旨在实现稳健且可扩展的通用生成能力。ComfyMind提出了两项核心创新：语义工作流接口（SWI），它将底层节点图抽象为自然语言描述的功能模块，从而简化高阶组合并减少结构错误；搜索树规划机制结合局部反馈执行，将生成过程建模为分层决策过程，并允许在每个阶段进行自适应修正。这些组件共同提升了复杂生成工作流的稳定性和灵活性。我们在三个公开基准上评估了ComfyMind，结果显示其性能优于现有开源基线，并接近GPT-Image-1的表现。ComfyMind为开源通用生成AI系统的开发开辟了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17908" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:53:03 GMT</pubDate>
</item>
<item>
<title>挑战长推理链假设：高效推理语言模型的新方法</title>
<link>https://arxiv.org/abs/2505.17813</link>
<guid>https://arxiv.org/abs/2505.17813</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现较短推理链比长链表现更好，提出新的推理方法short-m@k。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型(LLMs)在处理复杂推理任务时依赖扩展测试时间的计算资源，通过生成大量“思考”链条来实现，但这种方法成本高且耗时。本研究质疑长推理链是否真的带来更好的推理能力，通过实验表明较短推理链在许多情况下更为准确，最高可提升34.5%的正确率。基于此，我们提出了short-m@k方法，即并行执行k次独立生成，并在前m次推理完成后停止计算，最终答案由多数投票决定。实验显示，基本的short-1@k在低计算设置下性能与标准多数投票相当甚至更优，且使用的推理标记减少高达40%。而short-3@k虽然效率稍逊，但在所有计算预算下均优于多数投票，同时节省高达33%的墙钟时间。此外，通过对模型微调发现，训练时使用较短推理链也能显著提高性能。这些结果提示我们重新审视当前推理LLMs中的测试时间计算策略，强调较长推理并不总是带来更好效果，反而可能适得其反。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17813" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:29:06 GMT</pubDate>
</item>
<item>
<title>基于开放源代码大型语言模型的仓库级软件工程任务研究</title>
<link>https://arxiv.org/abs/2505.16901</link>
<guid>https://arxiv.org/abs/2505.16901</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示开放源码LLM可高效完成仓库级软件工程任务。</p><br /><br /><p><strong>摘要：</strong> 近期大规模语言模型(LLMs)在函数级代码生成方面取得了显著进展，但在仓库级软件工程任务中的表现仍具挑战性。当前解决方案多依赖专有LLM代理，这不仅引入不可预测性，还限制了其可及性，并引发了关于数据隐私和模型定制化的担忧。本文探讨了开源LLM是否能够在不依赖代理的情况下有效解决仓库级任务。我们通过让LLM理解代码库中的函数和文件的语义信息及其结构依赖性，展示了这种可能性。为此，我们提出了代码图模型(CGMs)，将代码库的图结构整合到LLM的注意力机制中，并利用专门的适配器将节点属性映射到LLM的输入空间。结合无代理图检索增强框架，我们的方法在SWE-bench Lite基准测试中达到了43.00%的解决率，这一成绩在开源权重模型中排名第一，在开源系统方法中排名第二，总体排名第八，比之前的最佳开源模型方法高出12.33个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16901" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:00:55 GMT</pubDate>
</item>
<item>
<title>R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO</title>
<link>https://arxiv.org/abs/2505.16673</link>
<guid>https://arxiv.org/abs/2505.16673</guid>
<content:encoded><![CDATA[
In this work, we aim to incentivize the reasoning ability of Multimodal Large Language Models (MLLMs) via reinforcement learning (RL) and develop an effective approach that mitigates the sparse reward and advantage vanishing issues during RL. To this end, we propose Share-GRPO, a novel RL approach that tackle these issues by exploring and sharing diverse reasoning trajectories over expanded question space. Specifically, Share-GRPO first expands the question space for a given question via data transformation techniques, and then encourages MLLM to effectively explore diverse reasoning trajectories over the expanded question space and shares the discovered reasoning trajectories across the expanded questions during RL. In addition, Share-GRPO also shares reward information during advantage computation, which estimates solution advantages hierarchically across and within question variants, allowing more accurate estimation of relative advantages and improving the stability of policy training. Extensive evaluations over six widely-used reasoning benchmarks showcase the superior performance of our method. Code will be available at https://github.com/HJYao00/R1-ShareVL.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 09:39:32 GMT</pubDate>
</item>
<item>
<title>CLEANMOL：提升大语言模型解析分子结构能力的新框架</title>
<link>https://arxiv.org/abs/2505.16340</link>
<guid>https://arxiv.org/abs/2505.16340</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CLEANMOL框架通过将SMILES解析分解为明确的任务，显著提升了大语言模型对分子结构的理解能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在科学发现，特别是分子科学领域展现出巨大潜力。然而，当前LLMs在理解分子结构（通常以SMILES表示）方面存在局限性，甚至无法完成基本任务如计数分子环。为此，我们提出了CLEANMOL，这是一种新框架，将SMILES解析转化为一系列清晰且确定性的任务，旨在促进分子图级别的理解。这些任务涵盖从子图匹配到全局图匹配，提供与分子结构属性一致的结构化监督。我们构建了一个具有自适应难度评分的分子预训练数据集，并在这些任务上对开源LLMs进行预训练。结果显示，CLEANMOL不仅增强了对分子结构的理解，还在Mol-Instructions基准测试中取得了最佳表现或与基线竞争。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16340" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 03:54:39 GMT</pubDate>
</item>
<item>
<title>AutoRefine：通过强化学习提升大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2505.11277</link>
<guid>https://arxiv.org/abs/2505.11277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的检索增强框架AutoRefine，显著提升复杂多跳推理任务的表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型虽然具备强大的推理能力，但受限于知识库容量。检索增强推理方法通过查询外部资源缓解这一限制，但常会检索到无关或噪声信息，影响推理准确性。本文提出AutoRefine，这是一种基于强化学习的后训练框架，采用“边搜索边优化”的新范式。AutoRefine在连续搜索调用之间引入显式的知识精炼步骤，使模型能够迭代地过滤、提炼和组织证据，从而生成更准确的答案。此外，我们利用分组相对策略优化，结合检索特定奖励和答案正确性奖励。实验表明，在单跳和多跳问答基准测试中，AutoRefine大幅超越现有方法，尤其在复杂的多跳推理场景中表现突出。进一步分析显示，AutoRefine能发出更高频率且质量更高的搜索请求，并有效合成证据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 10:11:29 GMT</pubDate>
</item>
<item>
<title>基于多智能体系统的学术海报自动生成方法</title>
<link>https://arxiv.org/abs/2505.21497</link>
<guid>https://arxiv.org/abs/2505.21497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合视觉反馈的多智能体系统用于学术海报生成。</p><br /><br /><p><strong>摘要：</strong> 学术海报生成在科研交流中至关重要但极具挑战性，需要将复杂的文档压缩到单一且视觉连贯的页面上。本文首次构建了海报生成的基准测试集和指标套件，通过与人工设计的海报对比，评估生成海报的视觉质量、文本连贯性和整体美观度等多方面表现。此外，提出了名为PosterAgent的多智能体生成管道，包括解析器、规划器和绘图-评论循环三个阶段，显著提升了生成效率与质量。实验表明，尽管GPT-4生成的海报在视觉上吸引人，但存在文本噪声和信息传达不足的问题。而基于Qwen-2.5系列的开源版本在多个指标上优于现有系统，同时减少了87%的令牌消耗。该方法可将一篇22页论文转化为可编辑的.pptx海报，成本仅为0.005美元。研究为下一代全自动海报生成模型提供了明确方向。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:58:49 GMT</pubDate>
</item>
<item>
<title>基于帧进出技术的可控视频生成研究</title>
<link>https://arxiv.org/abs/2505.21491</link>
<guid>https://arxiv.org/abs/2505.21491</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像到视频生成方法，通过帧进出技术实现对象自然进出场景。</p><br /><br /><p><strong>摘要：</strong> 视频生成中的可控性、时间一致性及细节合成是主要挑战。本文聚焦于电影制作中常用的但未被充分探索的技术——帧进出。该技术允许用户通过指定运动轨迹控制图像中的对象自然离开或进入场景。为此，我们构建了一个半自动标注的新数据集、针对此设置的综合评估协议，以及一种高效的保持身份且可控制运动的视频扩散Transformer架构。实验表明，所提出的方法显著优于现有基线模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21491" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:56:07 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的主动感知能力研究与ACTIVE-O3框架</title>
<link>https://arxiv.org/abs/2505.21457</link>
<guid>https://arxiv.org/abs/2505.21457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种强化学习驱动的ACTIVE-O3框架，使多模态大语言模型具备主动感知能力。</p><br /><br /><p><strong>摘要：</strong> 主动感知是高效感知与决策的关键组成部分，在人类及智能体中至关重要。尽管多模态大语言模型（MLLMs）在机器人系统中的应用受到关注，但其主动感知能力尚未得到充分探索。本文首次系统定义了基于MLLMs的主动感知任务，并指出GPT-o3的缩放搜索策略可视为主动感知的一种特殊情况，但存在效率低和区域选择不准确的问题。为此，我们提出了ACTIVE-O3，这是一种基于GRPO的纯强化学习训练框架，旨在赋予MLLMs主动感知能力。此外，我们构建了一个综合基准套件，评估ACTIVE-O3在开放世界任务和特定领域场景中的表现，包括遥感图像中的小目标检测、自动驾驶及精细交互分割等。ACTIVE-O3还在V*基准上展现了强大的零样本推理能力。我们希望此工作能为未来MLLMs领域的主动感知研究提供基础工具和评价协议。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 13:29:31 GMT</pubDate>
</item>
<item>
<title>Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?</title>
<link>https://arxiv.org/abs/2505.21374</link>
<guid>https://arxiv.org/abs/2505.21374</guid>
<content:encoded><![CDATA[
Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a "Holmes-test" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 12:05:01 GMT</pubDate>
</item>
<item>
<title>MME-VideoOCR基准评测：视频OCR中的多模态大语言模型挑战</title>
<link>https://arxiv.org/abs/2505.21333</link>
<guid>https://arxiv.org/abs/2505.21333</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有MLLMs在视频OCR中表现有限，需改进以应对复杂场景。</p><br /><br /><p><strong>摘要：</strong> 多模态大语言模型（MLLMs）在静态图像OCR中表现出色，但在视频OCR中因运动模糊、时间变化等因素表现显著下降。为指导实际应用，我们推出了MME-VideoOCR基准，涵盖25项任务、44种场景，涉及文本识别及深层次理解。该基准包含1464段视频和2000组人工标注问题答案对。评估结果显示，即便最优模型Gemini-2.5 Pro也仅达73.7%准确率。细粒度分析表明，模型在单帧或多帧文本任务上表现良好，但在需要整体视频理解的任务中能力受限，尤其在时空推理和跨帧信息整合方面表现不足。此外，高分辨率输入和充足时间覆盖对动态场景OCR至关重要。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21333" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:27:46 GMT</pubDate>
</item>
<item>
<title>MME-Reasoning：评估多模态大型语言模型逻辑推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.21327</link>
<guid>https://arxiv.org/abs/2505.21327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MME-Reasoning基准以全面评估多模态大型语言模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 逻辑推理是人类智能的基本方面，也是多模态大型语言模型（MLLMs）的重要能力。尽管在多模态推理方面取得了显著进展，但现有基准无法全面评估其推理能力，因为缺乏对推理类型的具体分类且推理理解不明确。为解决这些问题，我们引入了MME-Reasoning，这是一个综合性的基准，涵盖了推理问题中的三种推理类型（归纳、演绎和溯因）。通过精心策划数据，确保每个问题有效评估推理能力而非感知技能或知识广度，并扩展了评估协议以涵盖多样化问题的评估。我们的评估显示，最先进的MLLMs在全面评估逻辑推理能力时存在显著局限性。即使是最先进的MLLMs在全面逻辑推理方面的表现也有限，且在不同推理类型之间存在明显性能不平衡。此外，我们深入分析了“思维模式”和基于规则的强化学习等方法，这些方法通常被认为可以增强推理能力。这些发现揭示了当前MLLMs在多样化逻辑推理场景中的关键局限性和性能不平衡，为理解和评估推理能力提供了全面系统的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:23:23 GMT</pubDate>
</item>
<item>
<title>rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset</title>
<link>https://arxiv.org/abs/2505.21297</link>
<guid>https://arxiv.org/abs/2505.21297</guid>
<content:encoded><![CDATA[
Advancing code reasoning in large language models (LLMs) is fundamentally limited by the scarcity of high-difficulty datasets, especially those with verifiable input-output test cases necessary for rigorous solution validation at scale. We introduce rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty. This is achieved through three core contributions: (1) we curate competitive programming code problems and oracle solutions to synthesize new, solvable problems; (2) we introduce a reliable input-output test case synthesis pipeline that decouples the generation into a three-step input generation method and a mutual verification mechanism for effective output labeling; (3) we augment problems with high-quality, test-case-verified long-reasoning solutions. Extensive experiments on Qwen models (1.5B-14B) across various code reasoning benchmarks demonstrate the superiority of rStar-Coder dataset, achieving leading performance comparable to frontier reasoning LLMs with much smaller model sizes. On LiveCodeBench, rStar-Coder improves Qwen2.5-7B from 17.4% to an impressive 57.3%, and Qwen2.5-14B from 23.3% to 62.5%, surpassing o3-mini (low) by3.1%. On the more challenging USA Computing Olympiad, our 7B model achieves an average pass@1 accuracy of 16.15%, outperforming the frontier-level QWQ-32B. Code and the dataset will be released at https://github.com/microsoft/rStar.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 11:00:57 GMT</pubDate>
</item>
<item>
<title>Sci-Fi框架：实现起始帧与结束帧对中间帧的对称约束</title>
<link>https://arxiv.org/abs/2505.21205</link>
<guid>https://arxiv.org/abs/2505.21205</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Sci-Fi框架解决现有方法中起始帧和结束帧控制强度不对称的问题。</p><br /><br /><p><strong>摘要：</strong> 当前最先进的帧间插值方法主要通过微调或省略训练来扩展大型预训练图像到视频扩散模型（I2V-DMs），但存在设计上的局限性：引入结束帧约束的方式与处理起始帧约束相同机制，导致结束帧影响较弱。本文提出Sci-Fi框架，采用改进机制引入结束帧约束，利用轻量级模块EF-Net增强结束帧的影响力，使起始帧和结束帧对中间帧的控制达到对称效果，从而生成更和谐的过渡效果。实验验证了Sci-Fi框架在多种场景下的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21205" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 09:53:50 GMT</pubDate>
</item>
<item>
<title>DualParal：基于扩散变换的分布式视频生成高效策略</title>
<link>https://arxiv.org/abs/2505.21070</link>
<guid>https://arxiv.org/abs/2505.21070</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DualParal策略，大幅提升扩散Transformer视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于扩散Transformer的视频生成模型因处理长视频时的高延迟和高内存成本问题，提出了一种名为DualParal的新型分布式推理策略。该方法通过在多个GPU上并行化处理时间帧和模型层，有效缓解了这一限制。然而，由于扩散模型对帧间噪声同步的需求，直接实现并行化会导致串行化问题。为此，我们引入块级去噪方案，逐步降低噪声水平处理帧块，同时利用特征缓存减少GPU间的通信开销，采用协调噪声初始化策略保证全局一致性。实验表明，在8块RTX 4090 GPU上，该方法可将生成1025帧视频的延迟降低至原来的6.54倍，内存成本降低1.48倍，实现了高效、无瑕疵且无限长的视频生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.21070" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 27 May 2025 07:55:22 GMT</pubDate>
</item>
<item>
<title>OpenS2V-Nexus：推动Subject-to-Video生成的研究基础设施</title>
<link>https://arxiv.org/abs/2505.20292</link>
<guid>https://arxiv.org/abs/2505.20292</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OpenS2V-Nexus，包括评估基准和大规模数据集，推动S2V视频生成研究。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为OpenS2V-Nexus的新框架，旨在建立Subject-to-Video（S2V）生成的基础架构。该框架由两个主要部分组成：OpenS2V-Eval，这是一个精细的基准测试集；以及OpenS2V-5M，一个百万规模的数据集。与现有的基于VBench的S2V基准相比，OpenS2V-Eval专注于模型生成符合主题一致性且外观和身份忠实的视频能力。为此，它引入了来自七个主要S2V类别的180个提示，其中包括真实和合成测试数据。此外，为了准确对齐人类偏好与S2V基准，我们提出了三种自动度量标准：NexusScore、NaturalScore和GmeScore，分别量化生成视频中的主题一致性、自然性和文本相关性。基于此，我们对16个代表性S2V模型进行了全面评估，揭示了它们在不同内容上的优缺点。此外，我们创建了第一个开源的大规模S2V生成数据集OpenS2V-5M，其中包括五百万高质量的720P主题-文本-视频三元组。通过OpenS2V-Nexus，我们提供了一个强大的基础设施，以加速未来的S2V生成研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20292" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>VisTA：基于强化学习的视觉工具动态选择框架</title>
<link>https://arxiv.org/abs/2505.20289</link>
<guid>https://arxiv.org/abs/2505.20289</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisTA通过强化学习实现视觉工具的选择与组合优化。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VisTA的新框架，该框架利用端到端强化学习使视觉代理能够根据经验动态探索、选择和组合工具库中的工具。与现有方法相比，VisTA不需要训练自由提示或大规模微调，也不依赖于显式的推理监督。它通过组相对策略优化（GRPO）使代理自主发现有效的工具选择路径。实验表明，VisTA在ChartQA、Geometry3K和BlindTest基准测试中取得了显著性能提升，尤其是在分布外样本上。这些成果展示了VisTA在增强泛化能力、适应性利用多样工具以及推动灵活的经验驱动视觉推理系统方面的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20289" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>ImgEdit：面向复杂图像编辑的大规模高质量数据集与模型</title>
<link>https://arxiv.org/abs/2505.20275</link>
<guid>https://arxiv.org/abs/2505.20275</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ImgEdit数据集和模型，提升开源图像编辑能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ImgEdit的大规模高质量图像编辑数据集，包含120万精心策划的编辑对，涵盖单步和多步复杂编辑任务。通过多阶段的数据处理流程确保数据质量，该数据集显著超越现有数据集。基于此数据集训练的ImgEdit-E1模型，在多个任务上优于现有开源模型。同时，我们设计了ImgEdit-Bench基准测试，用于评估指令遵循、编辑质量和细节保留能力。本研究还分析了开源和专有模型的行为，提供了深入见解。ImgEdit数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20275" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:53:33 GMT</pubDate>
</item>
<item>
<title>Granular Low-Rank Adaptation (GraLoRA): 提升参数高效微调性能的新方法</title>
<link>https://arxiv.org/abs/2505.20355</link>
<guid>https://arxiv.org/abs/2505.20355</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GraLoRA通过引入子块结构克服LoRA的过拟合问题，显著提升生成模型微调效果。</p><br /><br /><p><strong>摘要：</strong> 低秩适配（LoRA）作为参数高效微调（PEFT）的一种流行方法，因其简单性和有效性受到关注。然而，LoRA存在根本性限制——当瓶颈被拓宽时容易发生过拟合，且在高秩情况下性能停滞甚至下降，难以媲美全量微调（FFT）。研究发现，LoRA的结构瓶颈导致梯度纠缠并扭曲传播。为解决这一问题，我们提出Granular Low-Rank Adaptation（GraLoRA），通过将权重矩阵划分为子块并为每个子块配备独立的低秩适配器，有效缓解过拟合，增强表示能力，并更接近FFT表现。实验表明，GraLoRA在代码生成和常识推理基准测试中显著优于LoRA及其他基线模型，在HumanEval+上的Pass@1得分提高达8.5%，且该优势在不同模型规模和秩设置下均成立，证明了其可扩展性和鲁棒性。相关代码、数据及脚本已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20355" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 02:48:20 GMT</pubDate>
</item>
<item>
<title>SoloSpeech：一种新颖的目标语音提取生成管道</title>
<link>https://arxiv.org/abs/2505.19314</link>
<guid>https://arxiv.org/abs/2505.19314</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法SoloSpeech，提升目标语音提取的自然度与准确性。</p><br /><br /><p><strong>摘要：</strong> 目标语音提取（TSE）旨在通过利用特定说话人的辅助音频分离混合语音中的目标语音。尽管近年来基于判别模型的方法在感知质量上表现优异，但容易引入伪影并降低自然度，且对训练与测试环境差异敏感。而生成模型则在感知质量和可懂度方面有所不足。为解决这些问题，我们提出了SoloSpeech，这是一种新颖的级联生成管道，集成了压缩、提取、重建和校正过程。该方法采用无需说话人嵌入的目标提取器，并利用辅助音频潜在空间的条件信息与混合音频潜在空间对齐，避免不匹配问题。实验结果显示，SoloSpeech在Libri2Mix数据集上的目标语音提取和语音分离任务中达到了新的最优性能，在域外数据和真实场景中也表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19314" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 17:00:48 GMT</pubDate>
</item>
<item>
<title>SeePhys：基于物理学问题的大规模多模态基准测试</title>
<link>https://arxiv.org/abs/2505.19099</link>
<guid>https://arxiv.org/abs/2505.19099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SeePhys测试显示当前大型语言模型在视觉理解方面存在重大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了SeePhys，这是一个针对从中学到博士资格考试的物理问题设计的大规模多模态基准测试。该基准涵盖了物理学的七个基础领域，包括21类高度异构的图表。不同于以往研究中视觉元素仅起辅助作用的情况，本基准中有75%的问题需要依赖视觉信息提取才能获得正确答案。通过广泛的评估发现，即使是最先进的视觉推理模型也只能达到不到60%的准确率。这些结果揭示了当前大型语言模型在视觉理解能力上的根本性挑战，特别是如何在图表解释与物理推理之间建立严格的联系，以及如何克服对文本线索的认知捷径的持续依赖。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:28:34 GMT</pubDate>
</item>
<item>
<title>基于验证器引导迭代优化的视频大语言模型强化学习方法</title>
<link>https://arxiv.org/abs/2505.19000</link>
<guid>https://arxiv.org/abs/2505.19000</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VerIPO方法，提升视频大语言模型复杂推理链生成能力。</p><br /><br /><p><strong>摘要：</strong> 当前基于强化学习微调的方法在视频大语言模型的复杂推理训练中面临数据准备瓶颈及性能不稳定的问题。为解决这些挑战，本文提出VerIPO（验证器引导迭代策略优化），通过引入Rollout-Aware Verifier，在GRPO与DPO阶段间形成循环训练框架，利用小型语言模型评估推理逻辑，构建高质量对比数据，显著提高推理链质量和优化效率。实验表明，VerIPO相比传统GRPO方法优化速度提升7倍，推理链长度与一致性明显改善，并在多种视频推理任务中优于大规模指令微调模型和长推理模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19000" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 02:41:28 GMT</pubDate>
</item>
<item>
<title>MetaMind：通过多智能体框架实现类人社会推理</title>
<link>https://arxiv.org/abs/2505.18943</link>
<guid>https://arxiv.org/abs/2505.18943</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MetaMind框架提升语言模型在社会推理上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MetaMind的多智能体框架，该框架旨在通过模仿元认知的心理学理论来实现类人社会推理能力。MetaMind将社会理解分解为三个协作阶段：理论思维代理生成假设、领域代理细化假设以及响应代理生成适当的回应。实验结果显示，MetaMind在三个具有挑战性的基准测试中取得了最先进的性能，在现实世界的社会场景中提升了35.7%，在理论思维推理方面提高了6.2%。此外，消融研究验证了所有组件的必要性，展示了框架在上下文合理性、社交适宜性和用户适应性方面的平衡能力。这项工作推动了人工智能系统向人类水平的社会智能发展，适用于共情对话和文化敏感交互等应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18943" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 22:32:57 GMT</pubDate>
</item>
<item>
<title>SVG2：通过语义感知聚类优化扩散Transformer视频生成效率</title>
<link>https://arxiv.org/abs/2505.18875</link>
<guid>https://arxiv.org/abs/2505.18875</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SVG2通过语义感知聚类和动态预算控制，显著提升扩散Transformer视频生成效率。</p><br /><br /><p><strong>摘要：</strong> 扩散Transformer (DiTs) 在视频生成中因注意力机制的二次复杂性而面临显著延迟问题。尽管稀疏注意力方法试图通过计算关键令牌来降低成本，但现有方法在相同计算预算下无法达到最佳生成质量。本文提出SVG2，一种无需训练的框架，通过语义感知的排列方式对令牌进行聚类和重新排序，同时结合top-p动态预算控制和定制化内核实现，实现了生成质量和效率之间的帕累托前沿权衡。实验表明，SVG2在HunyuanVideo和Wan 2.1数据集上分别实现了高达2.30倍和1.89倍的速度提升，同时保持了高PSNR值。这项研究解决了现有方法在关键令牌识别不精确和计算浪费方面的不足，为高效视频生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18875" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 17:30:29 GMT</pubDate>
</item>
<item>
<title>OmniConsistency：弥合扩散模型风格化一致性差距的通用插件</title>
<link>https://arxiv.org/abs/2505.18445</link>
<guid>https://arxiv.org/abs/2505.18445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出OmniConsistency插件提升扩散模型风格化一致性。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在图像风格化方面取得了显著进展，但复杂场景中的一致性保持及风格退化问题仍未解决。GPT-4o展示了性能优势，本文提出的OmniConsistency通过引入大规模Diffusion Transformer实现一致性的显著增强。该方法包括基于对齐图像对的上下文一致性学习框架、分阶段渐进学习策略以及与任意风格LoRA兼容的插件设计，实验表明其视觉连贯性和美学质量接近商业领先模型GPT-4o。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 21:00:20 GMT</pubDate>
</item>
<item>
<title>通过Steering Target Atoms实现语言模型精确控制</title>
<link>https://arxiv.org/abs/2505.20322</link>
<guid>https://arxiv.org/abs/2505.20322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Steering Target Atoms，提升语言模型的安全性和精确性。</p><br /><br /><p><strong>摘要：</strong> 当前对大型语言模型的控制主要依赖提示工程和引导技术，但高参数数量导致内部表示高度交织，限制了控制精度并可能引发意外后果。近期研究尝试利用稀疏自动编码器解耦知识以实现引导，然而由于定位原子知识组件的难度，这些应用仅限于简单任务。本文提出Steering Target Atoms方法，隔离并操控解耦的知识组件以增强安全性。实验表明该方法有效，且在对抗场景中展现出优越的鲁棒性和灵活性。此外，将此引导策略应用于大规模推理模型验证了其在精确推理控制中的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:59:18 GMT</pubDate>
</item>
<item>
<title>MMMR基准：评估多模态大语言模型的推理能力</title>
<link>https://arxiv.org/abs/2505.16459</link>
<guid>https://arxiv.org/abs/2505.16459</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准MMMR以评估多模态推理并揭示现有模型的问题。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）的发展使得跨语言、视觉和结构化输入的统一处理成为可能，但其推理能力尤其是具有中间思考痕迹的模型（MLLMs-T）尚未被充分理解。现有研究主要关注感知或最终答案的准确性，对模型如何跨模态推理或失败缺乏深入洞察。为此，我们引入了MMMR，这是一个新的基准，旨在通过显式思考严格评估多模态推理。MMMR包含一个由六个多样化推理类型组成的高难度数据集，以及一个模块化的推理轨迹评估管道（RTEP），用于评估推理质量。实验结果显示，尽管MLLMs-T总体上优于非思考模型，但顶级模型如Claude-3.7-Sonnet和Gemini-2.5 Pro仍存在不一致性及过度推理等问题。该基准揭示了准确率与推理质量之间的持续差距，并为未来模型开发提供了可操作的评估管道。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16459" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:41:55 GMT</pubDate>
</item>
<item>
<title>EgoZero: Robot Learning from Smart Glasses</title>
<link>https://arxiv.org/abs/2505.20290</link>
<guid>https://arxiv.org/abs/2505.20290</guid>
<content:encoded><![CDATA[
Despite recent progress in general purpose robotics, robot policies still lag far behind basic human capabilities in the real world. Humans interact constantly with the physical world, yet this rich data resource remains largely untapped in robot learning. We propose EgoZero, a minimal system that learns robust manipulation policies from human demonstrations captured with Project Aria smart glasses, and zero robot data. EgoZero enables: (1) extraction of complete, robot-executable actions from in-the-wild, egocentric, human demonstrations, (2) compression of human visual observations into morphology-agnostic state representations, and (3) closed-loop policy learning that generalizes morphologically, spatially, and semantically. We deploy EgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot transfer with 70% success rate over 7 manipulation tasks and only 20 minutes of data collection per task. Our results suggest that in-the-wild human data can serve as a scalable foundation for real-world robot learning - paving the way toward a future of abundant, diverse, and naturalistic training data for robots. Code and videos are available at https://egozero-robot.github.io.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:17 GMT</pubDate>
</item>
<item>
<title>FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models</title>
<link>https://arxiv.org/abs/2505.20225</link>
<guid>https://arxiv.org/abs/2505.20225</guid>
<content:encoded><![CDATA[
Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:06:25 GMT</pubDate>
</item>
<item>
<title>基于大语言模型的列表式推理重排序代理REARANK</title>
<link>https://arxiv.org/abs/2505.20046</link>
<guid>https://arxiv.org/abs/2505.20046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">REARANK通过强化学习和数据增强显著提升检索性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了REARANK，一种基于大型语言模型（LLM）的列表式推理重排序代理。REARANK在重排序前进行显式推理，不仅提升了性能，还增强了可解释性。通过强化学习和数据增强技术，REARANK在多个流行的信息检索基准测试中表现优异，仅需179个标注样本即可超越基线模型。基于Qwen2.5-7B构建的REARANK-7B，在域内和域外基准测试中的表现可媲美GPT-4，甚至在推理密集型BRIGHT基准上超过GPT-4。这些成果验证了该方法的有效性，并展示了如何利用强化学习提升LLM的推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 10:31:48 GMT</pubDate>
</item>
<item>
<title>大型语言模型中可解释分类特征的涌现特性研究</title>
<link>https://arxiv.org/abs/2505.19440</link>
<guid>https://arxiv.org/abs/2505.19440</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示了大型语言模型中分类特征在时间、空间及规模上的涌现规律。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）中可解释分类特征的出现机制，通过分析训练过程中的多个时间点、变压器网络的不同层以及不同规模模型的行为，利用稀疏自动编码器进行机制解释性研究。研究发现，在多个领域内，特征的出现存在明确的时间和规模阈值。此外，空间分析显示早期层特征会在后期层重新激活，这一现象挑战了关于变压器模型表征动态的传统假设。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19440" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:59:54 GMT</pubDate>
</item>
<item>
<title>MMIG-Bench：多模态图像生成综合基准测试</title>
<link>https://arxiv.org/abs/2505.19415</link>
<guid>https://arxiv.org/abs/2505.19415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMIG-Bench，统一文本到图像生成及图像编辑评估。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态图像生成模型虽在复杂指令处理和概念一致性维护方面表现出色，但缺乏统一的评估标准。本文提出了MMIG-Bench，该基准通过结合4850个带注释的文本提示与1750个多视角参考图像，涵盖380个主题，包括人、动物、物体及艺术风格。MMIG-Bench具备三级评估框架，包括低级视觉指标、Aspect Matching Score（AMS）中期指标及高级美学评价。通过对17个顶级模型的评估，验证了新指标的有效性，并提供了深入见解。研究将公开数据集和评估代码，推动多模态图像生成领域的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:07:24 GMT</pubDate>
</item>
<item>
<title>利用物理力作为视频生成控制信号的研究</title>
<link>https://arxiv.org/abs/2505.19386</link>
<guid>https://arxiv.org/abs/2505.19386</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出力提示方法，通过点力和风场实现对图像的物理交互。</p><br /><br /><p><strong>摘要：</strong> 近年来，视频生成模型的进步激发了对世界模型的兴趣，这类模型可以模拟逼真的环境。然而，与导航相比，模仿真实世界力的物理意义交互仍较少被研究。本研究探讨将物理力作为视频生成的控制信号，并提出了力提示方法，使用户可以通过局部点力（如戳植物）和全局风场（如风吹动布料）与图像进行交互。实验表明，力提示可以利用原始预训练模型中的视觉和运动先验，使视频对物理控制信号做出真实的响应，而无需使用3D资产或物理模拟器。研究面临的挑战是如何获取高质量的配对力-视频训练数据，尤其是在现实世界中难以获得力信号的情况下。研究发现，即使在有限演示的情况下，视频生成模型也能很好地推广到Blender合成视频的物理力条件上。这种方法能够在多样化几何形状、场景和材料上模拟力，且仅需在四块A100 GPU上训练一天约15k个训练样本，便在力一致性与物理真实性方面超越现有方法。我们将在项目页面发布所有数据集、代码、权重及交互式视频演示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19386" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 21:04:02 GMT</pubDate>
</item>
<item>
<title>WHISTRESS: Enriching Transcriptions with Sentence Stress Detection</title>
<link>https://arxiv.org/abs/2505.19103</link>
<guid>https://arxiv.org/abs/2505.19103</guid>
<content:encoded><![CDATA[
Spoken language conveys meaning not only through words but also through intonation, emotion, and emphasis. Sentence stress, the emphasis placed on specific words within a sentence, is crucial for conveying speaker intent and has been extensively studied in linguistics. In this work, we introduce WHISTRESS, an alignment-free approach for enhancing transcription systems with sentence stress detection. To support this task, we propose TINYSTRESS-15K, a scalable, synthetic training data for the task of sentence stress detection which resulted from a fully automated dataset creation process. We train WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive baselines. Our results show that WHISTRESS outperforms existing methods while requiring no additional input priors during training or inference. Notably, despite being trained on synthetic data, WHISTRESS demonstrates strong zero-shot generalization across diverse benchmarks. Project page: https://pages.cs.huji.ac.il/adiyoss-lab/whistress.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 07:45:08 GMT</pubDate>
</item>
<item>
<title>基于神经物理系统的实时交互流体模拟</title>
<link>https://arxiv.org/abs/2505.18926</link>
<guid>https://arxiv.org/abs/2505.18926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合神经网络和经典数值求解器的流体模拟系统。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种针对实时交互流体模拟的神经物理系统。传统基于物理的方法虽然精确但计算成本高且存在延迟问题，而机器学习方法虽降低了计算开销但仍难以满足低延迟需求并缺乏互动支持。为解决这一问题，我们引入了一种新颖的混合方法，该方法融合了数值模拟、神经物理及生成控制技术。通过采用回退保护机制，我们的神经物理系统能够在保证物理真实性的前提下实现低延迟模拟。此外，我们还开发了一种基于扩散的控制器，利用反向建模策略生成用于流体操作的外部动态力场。实验表明，该系统在多种二维和三维场景中表现出色，能够以高帧率（11%-29%延迟）实现真实的实时模拟，并允许用户通过友好的自由手绘进行流体控制。此研究标志着向实用、可控且物理逼真的实时交互流体模拟迈出了重要一步。我们承诺在论文被接受后公开模型和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 May 2025 21:27:18 GMT</pubDate>
</item>
<item>
<title>基于强化学习的混合推理策略优化提升大语言模型的隐式推理能力</title>
<link>https://arxiv.org/abs/2505.18454</link>
<guid>https://arxiv.org/abs/2505.18454</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HRPO方法，通过强化学习整合离散与连续表示，提升大语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，隐式推理作为大型语言模型（LLMs）的一种新范式，相比传统的自回归推理，能利用更丰富的隐藏状态特征。然而，隐式推理方法常与LLMs不兼容，且依赖于显式的链式思维（CoT）路径进行训练。本文提出了一种基于强化学习的混合推理策略优化（HRPO），该方法通过可学习门机制将先前的隐藏状态融入采样标记，并在训练初期主要使用标记嵌入，逐步引入更多隐藏特征，从而保持LLMs的生成能力并激励混合推理。此外，HRPO通过标记采样引入隐式推理中的随机性，使基于强化学习的优化无需依赖CoT轨迹。在多项基准测试中，HRPO在知识密集型和推理密集型任务中均超越现有方法，同时保持模型的可解释性，展现出跨语言模式和较短完成长度等有趣特性，为未来隐式推理研究提供了启示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18454" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 21:26:16 GMT</pubDate>
</item>
<item>
<title>InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning</title>
<link>https://arxiv.org/abs/2505.18291</link>
<guid>https://arxiv.org/abs/2505.18291</guid>
<content:encoded><![CDATA[
Large multimodal foundation models, particularly in the domains of language and vision, have significantly advanced various tasks, including robotics, autonomous driving, information retrieval, and grounding. However, many of these models perceive objects as indivisible, overlooking the components that constitute them. Understanding these components and their associated affordances provides valuable insights into an object's functionality, which is fundamental for performing a wide range of tasks. In this work, we introduce a novel real-world benchmark, InstructPart, comprising hand-labeled part segmentation annotations and task-oriented instructions to evaluate the performance of current models in understanding and executing part-level tasks within everyday contexts. Through our experiments, we demonstrate that task-oriented part segmentation remains a challenging problem, even for state-of-the-art Vision-Language Models (VLMs). In addition to our benchmark, we introduce a simple baseline that achieves a twofold performance improvement through fine-tuning with our dataset. With our dataset and benchmark, we aim to facilitate research on task-oriented part segmentation and enhance the applicability of VLMs across various domains, including robotics, virtual reality, information retrieval, and other related fields. Project website: https://zifuwan.github.io/InstructPart/.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:36:13 GMT</pubDate>
</item>
<item>
<title>负反馈驱动的监督学习提升大模型数学推理能力</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">负反馈感知微调方法通过利用负样本改进大模型，效果媲美强化学习。</p><br /><br /><p><strong>摘要：</strong> 本文挑战了强化学习在基于二元验证信号提升大语言模型数学能力上的主导地位，提出了一种名为负反馈感知微调（NFT）的监督学习方法。NFT通过构建隐式的负向策略来处理自动生成的负样本，从而让模型能够反思错误并自主改进。实验表明，在数学推理任务中，NFT显著优于传统的监督学习基线方法，并且在某些情况下可与领先的强化学习算法相媲美甚至超越。此外，理论分析揭示了NFT与强化学习中的GRPO方法在严格按策略训练时的等价性，这为监督学习和强化学习之间的联系提供了新的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18116" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:17:40 GMT</pubDate>
</item>
<item>
<title>统一微调方法（UFT）提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.16984</link>
<guid>https://arxiv.org/abs/2505.16984</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型微调范式UFT，综合监督和强化微调优势。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）的后训练方法主要分为监督微调（SFT）和强化微调（RFT）。SFT高效但可能过拟合，而RFT虽泛化性更好但依赖基础模型。为克服这些局限，我们提出统一微调（UFT），将两者整合成单一过程，不仅在所有模型规模上优于SFT和RFT，还理论上突破了RFT在长时推理任务中的样本复杂度瓶颈，首次证明了统一训练可显著加速收敛。这项研究展示了后训练技术的新方向，对提高模型推理能力具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16984" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:53:57 GMT</pubDate>
</item>
<item>
<title>大型语言模型中基于推理的段落重排序器的表现研究</title>
<link>https://arxiv.org/abs/2505.16886</link>
<guid>https://arxiv.org/abs/2505.16886</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示基于推理的段落重排序器表现不如非推理版本。</p><br /><br /><p><strong>摘要：</strong> 随着复杂自然语言任务中推理模型的成功应用，信息检索领域开始探索如何将类似的能力整合到基于大型语言模型构建的段落重排序器中。通常，这些方法利用大型语言模型生成逐步推理过程，然后得出最终的相关性预测。然而，推理是否真的能提高重排序准确性？本文深入探讨了这一问题，在相同训练条件下对比了基于推理的点式重排序器（ReasonRR）与标准非推理点式重排序器（StandardRR），发现StandardRR普遍优于ReasonRR。进一步研究发现，即使禁用ReasonRR的推理过程（ReasonRR-NoReason），其性能仍优于带有推理功能的版本。分析表明，基于推理的重排序器受限于大型语言模型的推理过程，倾向于产生极端的相关性评分，未能充分考虑部分相关性，这是点式重排序器准确性的重要因素之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16886" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:41:37 GMT</pubDate>
</item>
<item>
<title>TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification</title>
<link>https://arxiv.org/abs/2505.18283</link>
<guid>https://arxiv.org/abs/2505.18283</guid>
<content:encoded><![CDATA[
Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at https://github.com/JianghaoWu/TAGS.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 14:28:59 GMT</pubDate>
</item>
<item>
<title>STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15804</link>
<guid>https://arxiv.org/abs/2505.15804</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across diverse tasks, yet they lag significantly behind humans in spatial reasoning. We investigate this gap through Transformation-Driven Visual Reasoning (TVR), a challenging task requiring identification of object transformations across images under varying viewpoints. While traditional Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in cross-view settings, sparse-reward Reinforcement Learning (RL) suffers from inefficient exploration and slow convergence. To address these limitations, we propose STAR-R1, a novel framework that integrates a single-stage RL paradigm with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1 rewards partial correctness while penalizing excessive enumeration and passive inaction, enabling efficient exploration and precise reasoning. Comprehensive evaluations demonstrate that STAR-R1 achieves state-of-the-art performance across all 11 metrics, outperforming SFT by 23% in cross-view scenarios. Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its unique ability to compare all objects for improving spatial reasoning. Our work provides critical insights in advancing the research of MLLMs and reasoning models. The codes, model weights, and data will be publicly available at https://github.com/zongzhao23/STAR-R1.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:57:38 GMT</pubDate>
</item>
<item>
<title>ModernGBERT：基于德语的透明高性能编码器模型</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ModernGBERT及LL"aMmlein2Vec两种编码器模型，评估其在自然语言理解等任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ModernGBERT（134M, 1B），一种全新的德语编码器模型家族，从零开始训练并融合了ModernBERT的架构创新。同时，还提出了通过LLM2Vec方法从德语解码器模型转换而来的LL"aMmlein2Vec（120M, 1B, 7B）编码器家族。研究对这些模型在自然语言理解、文本嵌入及长上下文推理任务上进行基准测试，以比较专用编码器与转换后的解码器之间的性能差异。结果显示，ModernGBERT 1B在性能和参数效率方面优于先前最先进的德语编码器以及通过LLM2Vec改编的编码器。所有模型、训练数据、检查点和代码均公开可用，推动了德语NLP生态系统的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13136" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:07:20 GMT</pubDate>
</item>
<item>
<title>基于选项感知的时间抽象值学习的离线目标条件强化学习</title>
<link>https://arxiv.org/abs/2505.12737</link>
<guid>https://arxiv.org/abs/2505.12737</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法OTA，通过时间抽象改进高策略学习，显著提升离线目标条件强化学习在长时序任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了离线目标条件强化学习（GCRL）在处理长时序任务时面临的挑战，特别是现有方法如HIQL存在的性能瓶颈问题。研究发现，主要障碍源于高层策略难以生成合适的子目标，以及在长时序环境中优势信号方向不正确的现象。为解决这些问题，我们提出了Option-aware Temporally Abstracted（OTA）值学习方法，通过在时间差分学习过程中引入时间抽象，使价值函数能够产生更清晰的优势信号，从而有效缩短有效时序长度。实验表明，使用OTA价值函数提取的高层策略在OGBench基准测试中的复杂任务上表现出色，包括迷宫导航和视觉机器人操作环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12737" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 01:51:11 GMT</pubDate>
</item>
<item>
<title>GLEAM-Bench与GLEAM：提升移动机器人在复杂未知环境中的主动映射能力</title>
<link>https://arxiv.org/abs/2505.20294</link>
<guid>https://arxiv.org/abs/2505.20294</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GLEAM-Bench基准与GLEAM策略，显著提高移动机器人在多样化复杂环境中的主动映射性能。</p><br /><br /><p><strong>摘要：</strong> 当前移动机器人在复杂未知环境中的主动映射面临挑战，现有方法因训练数据不足和探索策略保守，难以适应多样化的场景布局和连接性。为解决这一问题，本文引入GLEAM-Bench，首个大规模通用主动映射基准，涵盖1152个来自合成与真实扫描数据集的多样化3D场景。基于此，我们提出了GLEAM，一种统一的通用探索策略。该策略通过语义表示、长期导航目标及随机化策略显著提升了泛化能力，在128个未见过的复杂场景中实现了66.50%的覆盖率，提高了映射精度并优化了轨迹效率。项目页面已发布以供参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20294" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>基于覆盖原则的大规模语言模型系统性组合泛化能力分析</title>
<link>https://arxiv.org/abs/2505.20278</link>
<guid>https://arxiv.org/abs/2505.20278</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大规模语言模型依赖模式匹配时难以实现系统性组合泛化。</p><br /><br /><p><strong>摘要：</strong> 本文提出覆盖原则这一数据驱动框架，表明主要依赖模式匹配的模型无法可靠地推广到超出片段替换的情形。通过实验发现，Transformer模型在处理两跳泛化任务时，所需训练数据随词汇集大小至少呈二次增长，且参数量扩大20倍并未提升效率。对于存在路径歧义的组合任务，Transformer学习到的上下文相关状态表示会削弱性能与互操作性。尽管思维链监督能提高多跳任务的训练效率，但对路径歧义仍显乏力。最后，我们提出了一个基于机制的分类方法，区分了神经网络实现泛化的三种方式。这一概念视角不仅解释了我们的研究结果，还指出了需要新架构创新的方向，以实现真正的系统性组合泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20278" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>面向未知攻击的终身安全对齐框架</title>
<link>https://arxiv.org/abs/2505.20259</link>
<guid>https://arxiv.org/abs/2505.20259</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种终身安全对齐框架，使大模型适应新型越狱攻击。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）在部署过程中可能遭遇的未知越狱攻击问题，提出了一种名为终身安全对齐（Lifelong Safety Alignment）的框架。该框架通过设置Meta-Attacker（元攻击者）和Defender（防御者）的竞争机制，使模型能够持续适应新出现的越狱策略。首先利用GPT-4o API从大量研究论文中提取关键信息，作为Meta-Attacker的初始训练数据。实验显示，在单轮攻击下，第一代Meta-Attacker在RR任务上的成功率为73%，在LAT任务上的迁移成功率也达到57%。然而，经过对抗训练后，Defender显著提升了鲁棒性，最终将Meta-Attacker的成功率降至7%左右。这项工作为LLMs在开放环境中的安全部署提供了重要参考。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20259" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:40:40 GMT</pubDate>
</item>
<item>
<title>自适应推理模型（ARM）解决“过度思考”问题</title>
<link>https://arxiv.org/abs/2505.20258</link>
<guid>https://arxiv.org/abs/2505.20258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应推理模型ARM，通过调整推理格式减少令牌使用，提升效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在复杂任务中的“过度思考”问题，即过多不必要的推理，提出了自适应推理模型（ARM）。ARM能够根据任务需求自动选择适当的推理格式，包括高效但简单的Direct Answer、Short CoT和Code，以及更复杂的Long CoT格式。为了训练ARM，引入了Ada-GRPO算法，解决了传统GRPO中的格式崩溃问题，使ARM在减少令牌使用的同时保持性能，平均减少30%，最高可达70%。此外，ARM不仅提高了推理效率，还加快了训练速度达两倍。ARM提供了三种模式：默认的自适应模式、指令引导模式和共识引导模式，以满足不同的应用场景需求。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:38:50 GMT</pubDate>
</item>
<item>
<title>Omni-R1：基于强化学习的多模态全局推理与细节理解模型</title>
<link>https://arxiv.org/abs/2505.20256</link>
<guid>https://arxiv.org/abs/2505.20256</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Omni-R1模型解决多模态模型在时空覆盖和像素级理解间的矛盾。</p><br /><br /><p><strong>摘要：</strong> 本文针对长时间视频音频推理与精细像素理解对多模态模型提出的冲突需求，设计了双系统架构：全局推理系统通过低空间成本选择关键帧并重写任务，细节理解系统则对高分辨率片段进行像素级定位。由于最优关键帧选择和任务重写难以监督，我们将其转化为强化学习问题，提出了Omni-R1框架。该框架通过在线协作获取分层奖励来训练全局推理系统，仅需少量任务划分的强化学习一轮迭代。实验表明，Omni-R1不仅超越了强监督基线和专门领域的最新模型，还显著提升了跨域泛化能力和减少了多模态幻觉现象，展示了强化学习在大规模多模态推理中的首次成功应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20256" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:34:06 GMT</pubDate>
</item>
<item>
<title>稀疏自编码器特征一致性对神经网络可解释性研究的重要性</title>
<link>https://arxiv.org/abs/2505.20254</link>
<guid>https://arxiv.org/abs/2505.20254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章强调稀疏自编码器特征一致性对提升神经网络机制可解释性的关键作用。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了稀疏自编码器（SAEs）在神经网络机制可解释性（MI）中的应用局限性，特别是特征不一致性问题。作者指出，机制可解释性研究需要优先考虑SAEs的特征一致性，即在独立训练运行中可靠收敛至等效特征集。为此，我们引入成对字典均相关系数（PW-MCC）作为衡量一致性的实用指标，并展示了通过适当架构选择可以实现高水平的一致性（如LLM激活中TopK SAEs达到0.80）。我们的贡献包括阐述一致性优先的益处，提供理论支持及合成验证，证明PW-MCC是恢复真实特征的有效代理；同时将这些发现扩展到实际LLM数据，表明高特征一致性与学习特征解释的语义相似度高度相关。最后，我们呼吁社区转向系统测量特征一致性，以推动机制可解释性领域的稳健累积进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 13:31:36 GMT</pubDate>
</item>
<item>
<title>基于硬负样本对比学习的大规模多模态模型在几何推理中的应用</title>
<link>https://arxiv.org/abs/2505.20152</link>
<guid>https://arxiv.org/abs/2505.20152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型硬负样本对比学习框架提升视觉编码器的几何理解能力。</p><br /><br /><p><strong>摘要：</strong> 大规模多模态模型（LMMs）在视觉感知任务中表现优异，但对比学习的局限性限制了其在精细推理中的表现，特别是在几何问题求解场景中。本文提出了一种结合图像和文本的硬负样本对比学习框架，通过扰动图表生成代码创建生成型硬负样本、基于修改的几何描述生成规则型硬负样本以及基于标题相似性选择检索型硬负样本，训练得到的MMGeoLM模型在三个几何推理基准测试中显著优于其他开源模型，甚至与强大的闭源模型GPT-4o相媲美。此外，研究还探讨了不同负样本构建方法及负样本数量对几何推理性能的影响，得出丰富的结论。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:55:28 GMT</pubDate>
</item>
<item>
<title>StructEval：评估大型语言模型结构化输出能力的新基准</title>
<link>https://arxiv.org/abs/2505.20139</link>
<guid>https://arxiv.org/abs/2505.20139</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出StructEval基准，系统评估大型语言模型生成和转换多种结构化格式的能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在软件开发中的广泛应用，其生成结构化输出的能力变得至关重要。本文介绍了一个名为StructEval的新基准，用于评估LLMs在生成非渲染格式（如JSON、YAML、CSV）和可渲染格式（如HTML、React、SVG）方面的表现。与现有基准不同，StructEval通过生成任务和转换任务两种范式，系统性地评估多种格式的结构保真度。该基准涵盖18种格式和44种任务类型，并引入了新的衡量指标来评估格式遵守和结构正确性。实验结果显示，即使是最先进的模型（如o1-mini）也仅达到平均75.58分，而开源替代方案则落后约10分。研究还发现生成任务比转换任务更具挑战性，生成视觉内容的难度高于生成纯文本结构。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.20139" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 11:40:42 GMT</pubDate>
</item>
<item>
<title>MLR-Bench：评估AI研究代理能力的综合基准</title>
<link>https://arxiv.org/abs/2505.19955</link>
<guid>https://arxiv.org/abs/2505.19955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLR-Bench评估AI代理在开放性机器学习研究中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MLR-Bench的新基准，用于评估AI代理在开放性机器学习研究中的能力。该基准包含三个关键部分：来自NeurIPS、ICLR和ICML研讨会的201个研究任务，涵盖多种ML主题；MLR-Judge，一个结合LLM评审员和精心设计的评审标准的自动化评估框架；以及MLR-Agent，一种模块化代理框架，能够完成研究任务的四个阶段。我们使用MLR-Bench评估了六种前沿LLM和一个先进的编码代理，发现虽然LLM在生成连贯想法和结构良好的论文方面表现出色，但当前的编码代理经常产生伪造或无效的实验结果。此外，通过人工评估验证了MLR-Judge的有效性，并开源了MLR-Bench，以帮助社区对AI研究代理进行基准测试、诊断和改进，推动可信和透明的科学发现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:18:37 GMT</pubDate>
</item>
<item>
<title>基于影响函数的大语言模型推理能力归因研究</title>
<link>https://arxiv.org/abs/2505.19949</link>
<guid>https://arxiv.org/abs/2505.19949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过影响函数揭示大语言模型数学与代码推理能力的训练数据特性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在数学和编码推理方面表现出色，通常得益于更强模型生成的链式思维（CoTs）。然而，现有训练数据的筛选策略主要依赖启发式方法，导致泛化性有限且难以捕捉数据中的微妙特征。本文利用影响函数系统性地将LLMs的数学和编码推理能力归因至单个训练样本、序列和标记，从而深入洞察有效数据的特性。我们的基于影响函数的推理归因（Infra）发现了跨领域效应：高难度数学示例同时提升数学和代码推理能力，而低难度代码任务最能促进代码推理。基于此，我们提出了一种简单有效的数据重加权策略，通过翻转任务难度，使AIME24的准确率从10%提高到20%，LiveCodeBench的准确率从33.8%提高到35.3%。此外，细粒度归因显示，序列级探索行为增强了数学和代码推理性能，而标记级影响模式在数学和代码推理中存在显著差异：前者偏好自然语言逻辑连接符，后者强调结构语法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 09:15:26 GMT</pubDate>
</item>
<item>
<title>Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles</title>
<link>https://arxiv.org/abs/2505.19914</link>
<guid>https://arxiv.org/abs/2505.19914</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 08:40:31 GMT</pubDate>
</item>
<item>
<title>基于元学习视角的大语言模型推理能力研究</title>
<link>https://arxiv.org/abs/2505.19815</link>
<guid>https://arxiv.org/abs/2505.19815</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将大语言模型的推理视为元学习过程。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的框架，通过元学习的视角来理解大型语言模型（LLMs）的推理能力。我们设想推理轨迹为对模型参数的伪梯度下降更新，并发现LLM推理与多种元学习范式的相似性。我们将推理任务的训练过程形式化为元学习设置，其中每个问题被视为一个单独的任务，推理轨迹作为适应模型参数的内循环优化。经过多样化问题的训练后，LLM能够发展出可泛化的基础推理能力。广泛的实证评估证实了LLM推理与元学习之间的紧密联系，并从元学习的角度探讨了多个重要议题。这项工作不仅深化了对LLM推理的理解，还为通过传统元学习技术改进这些模型提供了实用见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19815" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:52:17 GMT</pubDate>
</item>
<item>
<title>MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs</title>
<link>https://arxiv.org/abs/2505.19800</link>
<guid>https://arxiv.org/abs/2505.19800</guid>
<content:encoded><![CDATA[
Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets' scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: https://github.com/IVUL-KAUST/MOLE and dataset: https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:31:26 GMT</pubDate>
</item>
<item>
<title>基于多轮分解的大型推理模型高效推理方法</title>
<link>https://arxiv.org/abs/2505.19788</link>
<guid>https://arxiv.org/abs/2505.19788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种多轮分解方法MinD，大幅减少推理延迟。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）因复杂的链式思维（CoT）导致推理效率低下，首令牌及整体延迟较高。本文引入多轮分解（MinD），将传统CoT解码为显式、结构化的多轮交互序列，每轮专注于一个思考单元并给出相应答案，支持反思、验证、修正及探索替代方案，显著提升推理速度并实现过程控制。通过监督微调（SFT）与强化学习（RL）结合优化模型，在MATH等数据集上，MinD可使输出令牌使用量和首次令牌时间（TTFT）减少约70%，同时保持在MATH-500、AIME24、AMC23和GPQA-Diamond等推理基准测试中的竞争力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 06:18:57 GMT</pubDate>
</item>
<item>
<title>离散马尔可夫桥：一种新型的离散表示学习框架</title>
<link>https://arxiv.org/abs/2505.19752</link>
<guid>https://arxiv.org/abs/2505.19752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种针对离散表示学习的新框架，显著提升了表达能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为离散马尔可夫桥的新框架，用于解决现有离散扩散模型中存在的局限性。这些局限包括固定转换矩阵导致潜在表示表达力不足以及设计空间受限的问题。新框架通过矩阵学习和分数学习两个组件构建，并提供了理论性能保证及收敛性证明。此外，我们还分析了方法的空间复杂度，解决了先前研究中发现的实际约束问题。实验结果表明，该框架在Text8数据集上的证据下界（ELBO）达到了1.38，优于现有基线模型；同时在CIFAR-10数据集上也表现出与图像特定生成方法相当的竞争性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:32:12 GMT</pubDate>
</item>
<item>
<title>基于纳什均衡的人类反馈强化学习算法及其在线实现</title>
<link>https://arxiv.org/abs/2505.19731</link>
<guid>https://arxiv.org/abs/2505.19731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的在线纳什均衡强化学习算法Nash-MP，实现在复杂人类偏好下的快速收敛。</p><br /><br /><p><strong>摘要：</strong> 传统基于人类反馈的强化学习（RLHF）通常依赖奖励模型，但可能无法准确捕捉真实的人类偏好结构。本文引入Nash Mirror Prox (Nash-MP)，这是一种新型的在线纳什学习算法，利用镜像优化方法实现对纳什均衡的快速稳定收敛。理论分析表明，Nash-MP在KL散度、exploitability gap及对数概率的span半范数上均具有线性收敛特性，且这些特性不受动作空间大小影响。此外，我们还提出了近似版本的Nash-MP，通过随机策略梯度估计近似求解，并探讨其在大语言模型微调中的实际应用，实验结果证明其性能与现有方法兼容且竞争力强。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 05:17:32 GMT</pubDate>
</item>
<item>
<title>PathFinder-PRM：一种基于分层错误感知的数学推理奖励模型</title>
<link>https://arxiv.org/abs/2505.19706</link>
<guid>https://arxiv.org/abs/2505.19706</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入新的Process Reward Model提升多跳推理任务中的数学问题求解能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在多跳推理任务如数学问题求解中容易出现幻觉。本文介绍了一种名为PathFinder-PRM的新过程奖励模型，它通过分级检测每一步骤中的数学和一致性错误来指导生成连贯的解决方案。为了训练该模型，我们扩展了人类注释的数据集，构建了一个包含40万样本的数据集。实验结果显示，PathFinder-PRM在PRMBench上达到了67.7的新SOTA分数，比之前的最佳成绩高出2.2分，同时使用的数据量仅为之前的三分之一。当应用于基于奖励的贪心搜索时，模型在prm@8指标上取得了48.3的成绩，比最强基线提升了1.5个百分点。这些成果表明，分离的错误检测和奖励估计不仅提高了细粒度错误检测的能力，还显著提升了端到端的基于奖励的数学推理效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19706" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 04:56:36 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多跳推理改进方法</title>
<link>https://arxiv.org/abs/2505.19640</link>
<guid>https://arxiv.org/abs/2505.19640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习指导语言模型交替推理和回答，大幅提升多跳问题解决效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种新颖的训练范式，利用强化学习引导大型语言模型在解决多跳问题时交替进行推理与回答。实验表明，这种方法可以有效提升推理能力，同时显著减少首次响应时间（TTFT），平均降低超过80%，并且在Pass@1准确性上提高了19.3%。此外，该方法无需依赖外部工具，仅基于问答和逻辑推理数据集即可实现对复杂推理任务的良好泛化。研究还深入分析了条件奖励建模中的若干有价值见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:58:17 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多智能体协作框架提升医学问答系统性能</title>
<link>https://arxiv.org/abs/2505.19630</link>
<guid>https://arxiv.org/abs/2505.19630</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DoctorAgent-RL框架，通过强化学习优化医学咨询对话策略。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在生物医学问答领域表现优异，但在临床咨询中的应用仍面临挑战，如信息传递不明确导致诊断建议不具体。传统方法受限于静态数据驱动模式，缺乏泛化能力且难以智能提取关键信息。为解决这些问题，本文提出DoctorAgent-RL，一种基于强化学习的多智能体协作框架，将医疗咨询建模为动态决策过程。该框架通过医生代理与患者代理的多轮交互优化提问策略，依据综合奖励动态调整信息采集路径。实验表明，DoctorAgent-RL在多轮推理能力和最终诊断性能上优于现有模型，并在MTMedDialog数据集上展示了实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19630" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:48:14 GMT</pubDate>
</item>
<item>
<title>ScaleKV：针对视觉自回归模型的KV缓存压缩框架</title>
<link>https://arxiv.org/abs/2505.19602</link>
<guid>https://arxiv.org/abs/2505.19602</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ScaleKV框架，大幅降低视觉自回归模型KV缓存内存需求。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于视觉自回归（VAR）模型的高效推理问题，指出其粗略到精细的方法导致KV缓存指数增长的问题。为解决这一瓶颈，我们引入了ScaleKV，这是一种专为VAR架构设计的KV缓存压缩框架。通过将Transformer层分为起草者（drafters）和精炼器（refiners），并根据不同的注意力模式进行差异化缓存管理，ScaleKV显著减少了缓存内存需求。实验表明，在最先进的文本到图像VAR模型Infinity上，该方法将所需KV缓存内存减少至原来的10%，同时保持像素级保真度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19602" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:11:42 GMT</pubDate>
</item>
<item>
<title>利用内部反馈信号实现大规模语言模型的无监督强化学习</title>
<link>https://arxiv.org/abs/2505.19590</link>
<guid>https://arxiv.org/abs/2505.19590</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出基于自我确信度的无监督强化学习方法，可媲美有监督学习效果。</p><br /><br /><p><strong>摘要：</strong> 当前通过验证奖励进行强化学习（RLVR）训练复杂推理的大规模语言模型（LLMs）有效但成本高昂且依赖特定领域的监督。本文探索了一种新的框架——基于内部反馈的强化学习（RLIF），该框架使LLMs能够利用内在信号而非外部奖励或标记数据进行学习。文中提出的Intuitor方法，仅使用模型自身的置信度作为奖励信号，在无需黄金标准解决方案或测试用例的情况下，实现了完全无监督的学习。实验表明，Intuitor在数学基准测试中的表现与需要外部奖励的团体相对策略优化（GRPO）相当，同时在代码生成等跨领域任务上表现出更好的泛化能力。这项研究表明，内在模型信号可以驱动跨领域有效的学习，为无法获得可验证奖励的自主AI系统提供了一种可扩展的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19590" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 26 May 2025 03:01:06 GMT</pubDate>
</item>
<item>
<title>BizFinBench：首个面向金融应用的大语言模型评估基准</title>
<link>https://arxiv.org/abs/2505.19457</link>
<guid>https://arxiv.org/abs/2505.19457</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出BizFinBench，首个针对大语言模型在金融领域应用的评估基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BizFinBench，这是一个专门设计用于评估大型语言模型（LLMs）在真实世界金融应用场景中的表现的全新基准。该基准包含6,781个经过良好标注的中文查询，涵盖了五个维度：数值计算、推理、信息提取、预测识别和基于知识的问题回答，分为九个细分类别。此外，还引入了IteraJudge，这是一种新的LLM评估方法，旨在减少LLMs作为客观指标评估者时的偏差。实验测试了25种模型，结果显示没有一款模型在所有任务中占据主导地位。分析揭示了各模型在不同能力上的显著差异，例如在数值计算方面，Claude-3.5-Sonnet和DeepSeek-R1表现出色；而在推理任务中，闭源模型明显优于开源模型。尽管当前LLMs在处理常规财务查询时表现良好，但在需要跨概念推理的复杂场景中仍显不足。BizFinBench为未来相关研究提供了严谨且业务导向的评估工具，代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19457" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 23:23:02 GMT</pubDate>
</item>
<item>
<title>AI辅助软件开发中的两种新兴范式：氛围编码与自主编码对比分析</title>
<link>https://arxiv.org/abs/2505.19443</link>
<guid>https://arxiv.org/abs/2505.19443</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文对比分析了AI辅助软件开发中的氛围编码与自主编码两种范式。</p><br /><br /><p><strong>摘要：</strong> 本文深入探讨了AI辅助软件开发领域中两种新兴范式——氛围编码和自主编码。尽管两者均依赖大型语言模型（LLMs），但在自主性、架构设计及开发者角色上存在根本差异。氛围编码侧重通过基于提示词的对话工作流支持创意探索，而自主编码则通过目标驱动代理实现自动化开发。文章提出了涵盖概念基础、执行模型、反馈机制等的详细分类法，并通过20个具体案例展示了氛围系统在原型设计和教育中的优势，以及自主系统在企业级自动化和CI/CD集成中的卓越表现。此外，文章还探讨了自然语言接口与自动化执行管道结合的混合架构趋势，并规划了可信、可解释且协作的自主AI基础设施未来路径。研究指出，成功的AI软件工程需要整合这两种范式的优点，构建以人为中心的统一开发生命周期。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19443" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 23:00:21 GMT</pubDate>
</item>
<item>
<title>基于格式与长度代理信号的大规模语言模型数学问题求解训练研究</title>
<link>https://arxiv.org/abs/2505.19439</link>
<guid>https://arxiv.org/abs/2505.19439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出利用格式和长度作为代理信号训练大规模语言模型解决数学问题。</p><br /><br /><p><strong>摘要：</strong> 大规模语言模型在自然语言处理领域取得了显著成就，强化学习在模型应用适配中起到了关键作用。然而，在数学问题求解的训练过程中，获取真实答案往往困难且昂贵。本文探索了利用格式和长度作为代理信号的方法，通过设计奖励函数，使得模型在早期阶段的表现可与标准GRPO算法相媲美。随着训练进展，单纯依赖格式的奖励机制暴露出局限性，因此引入了长度相关的奖励信号。最终提出的GRPO方法结合格式与长度的代理信号，在特定场景下不仅达到了与传统基于真实答案方法相当甚至更好的性能，还在AIME2024测试集上实现了40.0%的准确率。这项研究不仅提供了一种减少对大量真实标注数据依赖的实用解决方案，还揭示了无标签方法成功的本质：基础模型已具备强大的数学和逻辑推理能力，只需培养良好的答题习惯即可展现潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:56:22 GMT</pubDate>
</item>
<item>
<title>WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference</title>
<link>https://arxiv.org/abs/2505.19427</link>
<guid>https://arxiv.org/abs/2505.19427</guid>
<content:encoded><![CDATA[
The growing computational demands of large language models (LLMs) make efficient inference and activation strategies increasingly critical. While recent approaches, such as Mixture-of-Experts (MoE), leverage selective activation but require specialized training, training-free sparse activation methods offer broader applicability and superior resource efficiency through their plug-and-play design. However, many existing methods rely solely on hidden state magnitudes to determine activation, resulting in high approximation errors and suboptimal inference accuracy. To address these limitations, we propose WINA (Weight Informed Neuron Activation), a novel, simple, and training-free sparse activation framework that jointly considers hidden state magnitudes and the column-wise ell_2-norms of weight matrices. We show that this leads to a sparsification strategy that obtains optimal approximation error bounds with theoretical guarantees tighter than existing techniques. Empirically, WINA also outperforms state-of-the-art methods (e.g., TEAL) by up to 2.94% in average performance at the same sparsity levels, across a diverse set of LLM architectures and datasets. These results position WINA as a new performance frontier for training-free sparse activation in LLM inference, advancing training-free sparse activation methods and setting a robust baseline for efficient inference. The source code is available at https://github.com/microsoft/wina.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 22:37:32 GMT</pubDate>
</item>
<item>
<title>通过生成模型筛选构建通用文本到图像精调数据集</title>
<link>https://arxiv.org/abs/2505.19297</link>
<guid>https://arxiv.org/abs/2505.19297</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法构建通用的文本到图像模型精调数据集。</p><br /><br /><p><strong>摘要：</strong> 预训练虽然为文本到图像（T2I）模型提供了广泛的世界知识，但仅靠此不足以实现高质量的美学效果和对齐。因此，监督微调（SFT）至关重要，但其效果高度依赖于微调数据集的质量。现有的公开SFT数据集往往针对狭窄领域，创建高质量、通用的SFT数据集仍是一项重大挑战。本文介绍了一种利用预训练生成模型作为高影响力训练样本估计器的新方法，以此构建并发布了Alchemist数据集。该数据集虽小（3,350个样本），却非常有效。实验表明，Alchemist显著提高了五个公共T2I模型的生成质量，同时保持了多样性和风格。此外，还向公众发布了微调模型的权重。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19297" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 16:08:20 GMT</pubDate>
</item>
<item>
<title>基于过程级自适应推理模式的大语言模型高效推理方法</title>
<link>https://arxiv.org/abs/2505.19250</link>
<guid>https://arxiv.org/abs/2505.19250</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新推理范式PATS，动态调整大语言模型的推理策略以优化性能与效率。</p><br /><br /><p><strong>摘要：</strong> 当前大规模语言模型通常采用固定推理策略处理所有问题，忽视了任务复杂性和推理过程的变化，导致性能与效率失衡。现有方法通过训练自由的快慢思考系统切换来应对不同难度的问题，但受限于粗粒度的解决方案级策略调整。为解决这一问题，本文提出了一种新的推理范式——过程级自适应推理模式切换(PATS)，使大语言模型能够根据每一步的难度动态调整推理策略，从而在准确性与计算效率之间取得平衡。该方法将过程奖励模型(PRMs)与束搜索相结合，引入渐进式模式切换和坏步惩罚机制。实验表明，在多种数学基准测试中，我们的方法在保持适度令牌使用量的同时实现了高精度。这项研究强调了过程级、难度感知的推理策略适应的重要性，为大语言模型的高效推理提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19250" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 13:58:50 GMT</pubDate>
</item>
<item>
<title>Variance-Reduced Preference Optimization提升掩码扩散模型对齐性能</title>
<link>https://arxiv.org/abs/2505.19223</link>
<guid>https://arxiv.org/abs/2505.19223</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过VRPO框架优化掩码扩散模型对齐人类偏好，显著提升LLaDA表现。</p><br /><br /><p><strong>摘要：</strong> 掩码扩散模型（MDMs）如LLaDA在语言建模方面展现出巨大潜力，但其与人类偏好的对齐尚未得到充分探索，主要因为基于证据下界（ELBO）的似然估计具有高方差问题。为此，我们提出变异性降低偏好优化（VRPO），通过理论分析ELBO估计器的方差，推导偏好优化梯度的偏差和方差界限，并引入无偏方差减少策略如最优蒙特卡洛预算分配和反向采样，大幅提高MDM对齐效果。实验表明，应用VRPO优化后的LLaDA 1.5在数学、代码及对齐基准测试中均显著优于仅经过监督微调的前代模型，并在数学能力上与顶级语言MDMs和自回归模型（ARMs）竞争。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19223" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 12:36:20 GMT</pubDate>
</item>
<item>
<title>大型语言模型在精细科学假设发现中的应用与优化研究</title>
<link>https://arxiv.org/abs/2505.19209</link>
<guid>https://arxiv.org/abs/2505.19209</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法解决大型语言模型生成精细科学假设的问题。</p><br /><br /><p><strong>摘要：</strong> 本文引入并正式定义了精细科学假设发现这一全新任务，即从粗略的研究方向中生成具有实验操作性的详细假设。我们将其视为组合优化问题，并探讨了大型语言模型在此任务上的能力上限。通过构建潜在奖励景观，我们研究了如何利用模型内部启发式来优化假设生成过程，并比较了单一模型与模型集合的效果。实验表明，提出的分层搜索方法可有效平滑奖励景观并提高优化效果，在化学文献基准测试中显著优于现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19209" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 12:13:46 GMT</pubDate>
</item>
<item>
<title>从模型压缩到令牌压缩：AI效率研究的新范式</title>
<link>https://arxiv.org/abs/2505.19147</link>
<guid>https://arxiv.org/abs/2505.19147</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨了AI效率研究从模型压缩转向数据驱动的令牌压缩。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）和多模态LLMs的发展主要依赖于通过增加参数数量实现性能提升。然而，随着硬件限制的逼近，计算瓶颈已转变为自注意力机制在长序列上的二次成本问题。本文认为，AI效率研究的重点正在从模型为中心的压缩转向数据为中心的压缩。我们提出令牌压缩作为新的研究前沿，它通过减少训练或推理过程中的令牌数量来提高AI效率。通过综合分析，本文首先考察了各领域长上下文AI的最新进展，并建立了现有模型效率策略的统一数学框架，展示了令牌压缩为何是解决长上下文开销的关键范式转变。随后，系统回顾了令牌压缩的研究现状，分析了其基本优势，并在多种场景下展示了其显著优势。此外，还深入剖析了当前令牌压缩研究面临的挑战，并提出了未来有前景的方向。本研究旨在为AI效率提供新视角，整合现有研究成果，并推动创新解决方案以应对长上下文对AI发展的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.19147" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 25 May 2025 09:51:17 GMT</pubDate>
</item>
<item>
<title>Jodi: Unification of Visual Generation and Understanding via Joint Modeling</title>
<link>https://arxiv.org/abs/2505.19084</link>
<guid>https://arxiv.org/abs/2505.19084</guid>
<content:encoded><![CDATA[
Visual generation and understanding are two deeply interconnected aspects of human intelligence, yet they have been traditionally treated as separate tasks in machine learning. In this paper, we propose Jodi, a diffusion framework that unifies visual generation and understanding by jointly modeling the image domain and multiple label domains. Specifically, Jodi is built upon a linear diffusion transformer along with a role switch mechanism, which enables it to perform three particular types of tasks: (1) joint generation, where the model simultaneously generates images and multiple labels; (2) controllable generation, where images are generated conditioned on any combination of labels; and (3) image perception, where multiple labels can be predicted at once from a given image. Furthermore, we present the Joint-1.6M dataset, which contains 200,000 high-quality images collected from public sources, automatic labels for 7 visual domains, and LLM-generated captions. Extensive experiments demonstrate that Jodi excels in both generation and understanding tasks and exhibits strong extensibility to a wider range of visual domains. Code is available at https://github.com/VIPL-GENUN/Jodi.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 06:40:52 GMT</pubDate>
</item>
<item>
<title>An Embarrassingly Simple Defense Against LLM Abliteration Attacks</title>
<link>https://arxiv.org/abs/2505.19056</link>
<guid>https://arxiv.org/abs/2505.19056</guid>
<content:encoded><![CDATA[
Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.
]]></content:encoded>
<pubDate>Sun, 25 May 2025 05:18:24 GMT</pubDate>
</item>
<item>
<title>AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting</title>
<link>https://arxiv.org/abs/2505.18822</link>
<guid>https://arxiv.org/abs/2505.18822</guid>
<content:encoded><![CDATA[
Modern large reasoning models demonstrate impressive problem-solving capabilities by employing sophisticated reasoning strategies. However, they often struggle to balance efficiency and effectiveness, frequently generating unnecessarily lengthy reasoning chains for simple problems. In this work, we propose AdaCtrl, a novel framework to support both difficulty-aware adaptive reasoning budget allocation and explicit user control over reasoning depth. AdaCtrl dynamically adjusts its reasoning length based on self-assessed problem difficulty, while also allowing users to manually control the budget to prioritize either efficiency or effectiveness. This is achieved through a two-stage training pipeline: an initial cold-start fine-tuning phase to instill the ability to self-aware difficulty and adjust reasoning budget, followed by a difficulty-aware reinforcement learning (RL) stage that refines the model's adaptive reasoning strategies and calibrates its difficulty assessments based on its evolving capabilities during online training. To enable intuitive user interaction, we design explicit length-triggered tags that function as a natural interface for budget control. Empirical results show that AdaCtrl adapts reasoning length based on estimated difficulty, compared to the standard training baseline that also incorporates fine-tuning and RL, it yields performance improvements and simultaneously reduces response length by 10.06% and 12.14% on the more challenging AIME2024 and AIME2025 datasets, which require elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K datasets, where more concise responses are sufficient. Furthermore, AdaCtrl enables precise user control over the reasoning budget, allowing for tailored responses to meet specific needs.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 14:46:50 GMT</pubDate>
</item>
<item>
<title>Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models</title>
<link>https://arxiv.org/abs/2505.18773</link>
<guid>https://arxiv.org/abs/2505.18773</guid>
<content:encoded><![CDATA[
State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today's LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC&lt;0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 12:23:43 GMT</pubDate>
</item>
<item>
<title>The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation</title>
<link>https://arxiv.org/abs/2505.18759</link>
<guid>https://arxiv.org/abs/2505.18759</guid>
<content:encoded><![CDATA[
Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 11:54:19 GMT</pubDate>
</item>
<item>
<title>Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps</title>
<link>https://arxiv.org/abs/2505.18675</link>
<guid>https://arxiv.org/abs/2505.18675</guid>
<content:encoded><![CDATA[
Multimodal large language models (MLLMs) have recently achieved significant progress in visual tasks, including semantic scene understanding and text-image alignment, with reasoning variants enhancing performance on complex tasks involving mathematics and logic. However, their capacity for reasoning tasks involving fine-grained visual understanding remains insufficiently evaluated. To address this gap, we introduce ReasonMap, a benchmark designed to assess the fine-grained visual understanding and spatial reasoning abilities of MLLMs. ReasonMap encompasses high-resolution transit maps from 30 cities across 13 countries and includes 1,008 question-answer pairs spanning two question types and three templates. Furthermore, we design a two-level evaluation pipeline that properly assesses answer correctness and quality. Comprehensive evaluations of 15 popular MLLMs, including both base and reasoning variants, reveal a counterintuitive pattern: among open-source models, base models outperform reasoning ones, while the opposite trend is observed in closed-source models. Additionally, performance generally degrades when visual inputs are masked, indicating that while MLLMs can leverage prior knowledge to answer some questions, fine-grained visual reasoning tasks still require genuine visual perception for strong performance. Our benchmark study offers new insights into visual reasoning and contributes to investigating the gap between open-source and closed-source models.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 08:33:52 GMT</pubDate>
</item>
<item>
<title>Flex-Judge: Think Once, Judge Anywhere</title>
<link>https://arxiv.org/abs/2505.18601</link>
<guid>https://arxiv.org/abs/2505.18601</guid>
<content:encoded><![CDATA[
Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 04:50:53 GMT</pubDate>
</item>
<item>
<title>B-score: Detecting biases in large language models using response history</title>
<link>https://arxiv.org/abs/2505.18545</link>
<guid>https://arxiv.org/abs/2505.18545</guid>
<content:encoded><![CDATA[
Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 02:23:52 GMT</pubDate>
</item>
<item>
<title>Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.18536</link>
<guid>https://arxiv.org/abs/2505.18536</guid>
<content:encoded><![CDATA[
Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.
]]></content:encoded>
<pubDate>Sat, 24 May 2025 02:01:48 GMT</pubDate>
</item>
<item>
<title>Dynamic Risk Assessments for Offensive Cybersecurity Agents</title>
<link>https://arxiv.org/abs/2505.18384</link>
<guid>https://arxiv.org/abs/2505.18384</guid>
<content:encoded><![CDATA[
Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 17:18:59 GMT</pubDate>
</item>
<item>
<title>Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation</title>
<link>https://arxiv.org/abs/2505.18323</link>
<guid>https://arxiv.org/abs/2505.18323</guid>
<content:encoded><![CDATA[
For nearly a decade the academic community has investigated backdoors in neural networks, primarily focusing on classification tasks where adversaries manipulate the model prediction. While demonstrably malicious, the immediate real-world impact of such prediction-altering attacks has remained unclear. In this paper we introduce a novel and significantly more potent class of backdoors that builds upon recent advancements in architectural backdoors. We demonstrate how these backdoors can be specifically engineered to exploit batched inference, a common technique for hardware utilization, enabling large-scale user data manipulation and theft. By targeting the batching process, these architectural backdoors facilitate information leakage between concurrent user requests and allow attackers to fully control model responses directed at other users within the same batch. In other words, an attacker who can change the model architecture can set and steal model inputs and outputs of other users within the same batch. We show that such attacks are not only feasible but also alarmingly effective, can be readily injected into prevalent model architectures, and represent a truly malicious threat to user privacy and system integrity. Critically, to counteract this new class of vulnerabilities, we propose a deterministic mitigation strategy that provides formal guarantees against this new attack vector, unlike prior work that relied on Large Language Models to find the backdoors. Our mitigation strategy employs a novel Information Flow Control mechanism that analyzes the model graph and proves non-interference between different user inputs within the same batch. Using our mitigation strategy we perform a large scale analysis of models hosted through Hugging Face and find over 200 models that introduce (unintended) information leakage between batch entries due to the use of dynamic quantization.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 15:28:45 GMT</pubDate>
</item>
<item>
<title>Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model</title>
<link>https://arxiv.org/abs/2505.17894</link>
<guid>https://arxiv.org/abs/2505.17894</guid>
<content:encoded><![CDATA[
We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:42:21 GMT</pubDate>
</item>
<item>
<title>Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective</title>
<link>https://arxiv.org/abs/2505.17652</link>
<guid>https://arxiv.org/abs/2505.17652</guid>
<content:encoded><![CDATA[
Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces Competence-Difficulty Alignment Sampling (CDAS), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model's current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:15:26 GMT</pubDate>
</item>
<item>
<title>From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition</title>
<link>https://arxiv.org/abs/2505.16972</link>
<guid>https://arxiv.org/abs/2505.16972</guid>
<content:encoded><![CDATA[
Recent advances in Automatic Speech Recognition (ASR) have been largely fueled by massive speech corpora. However, extending coverage to diverse languages with limited resources remains a formidable challenge. This paper introduces Speech Back-Translation, a scalable pipeline that improves multilingual ASR models by converting large-scale text corpora into synthetic speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just tens of hours of real transcribed speech can effectively train TTS models to generate synthetic speech at hundreds of times the original volume while maintaining high quality. To evaluate synthetic speech quality, we develop an intelligibility-based assessment framework and establish clear thresholds for when synthetic data benefits ASR training. Using Speech Back-Translation, we generate more than 500,000 hours of synthetic speech in ten languages and continue pre-training Whisper-large-v3, achieving average transcription error reductions of over 30\%. These results highlight the scalability and effectiveness of Speech Back-Translation for enhancing multilingual ASR systems.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:51:05 GMT</pubDate>
</item>
<item>
<title>Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance</title>
<link>https://arxiv.org/abs/2505.16348</link>
<guid>https://arxiv.org/abs/2505.16348</guid>
<content:encoded><![CDATA[
Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents' understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO
]]></content:encoded>
<pubDate>Thu, 22 May 2025 04:00:10 GMT</pubDate>
</item>
<item>
<title>EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning</title>
<link>https://arxiv.org/abs/2505.16312</link>
<guid>https://arxiv.org/abs/2505.16312</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) excel at complex reasoning through search algorithms, yet current strategies often suffer from massive token consumption due to redundant exploration of semantically equivalent steps. Existing semantic similarity methods struggle to accurately identify such equivalence in domain-specific contexts like mathematical reasoning. To address this, we propose EquivPruner, a simple yet effective approach that identifies and prunes semantically equivalent actions during LLM reasoning search. We also introduce MathEquiv, the first dataset we created for mathematical statement equivalence, which enables the training of a lightweight equivalence detector. Extensive experiments across various models and tasks demonstrate that EquivPruner significantly reduces token consumption, improving searching efficiency and often bolstering reasoning accuracy. For instance, when applied to Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by 48.1\% while also improving accuracy. Our code is available at https://github.com/Lolo1222/EquivPruner.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 03:07:43 GMT</pubDate>
</item>
<item>
<title>Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.15957</link>
<guid>https://arxiv.org/abs/2505.15957</guid>
<content:encoded><![CDATA[
With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:17:29 GMT</pubDate>
</item>
<item>
<title>G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13426</link>
<guid>https://arxiv.org/abs/2505.13426</guid>
<content:encoded><![CDATA[
Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:54:39 GMT</pubDate>
</item>
<item>
<title>InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction</title>
<link>https://arxiv.org/abs/2505.10887</link>
<guid>https://arxiv.org/abs/2505.10887</guid>
<content:encoded><![CDATA[
This paper introduces InfantAgent-Next, a generalist agent capable of interacting with computers in a multimodal manner, encompassing text, images, audio, and video. Unlike existing approaches that either build intricate workflows around a single large model or only provide workflow modularity, our agent integrates tool-based and pure vision agents within a highly modular architecture, enabling different models to collaboratively solve decoupled tasks in a step-by-step manner. Our generality is demonstrated by our ability to evaluate not only pure vision-based real-world benchmarks (i.e., OSWorld), but also more general or tool-intensive benchmarks (e.g., GAIA and SWE-Bench). Specifically, we achieve 7.27% accuracy on OSWorld, higher than Claude-Computer-Use. Codes and evaluation scripts are open-sourced at https://github.com/bin123apple/InfantAgent.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 01:43:27 GMT</pubDate>
</item>
<item>
<title>面向低资源语言的文化适配大型语言模型研究</title>
<link>https://arxiv.org/abs/2505.18383</link>
<guid>https://arxiv.org/abs/2505.18383</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合文化与语言特性的大型语言模型创建方法。</p><br /><br /><p><strong>摘要：</strong> 当前针对低资源语言增强大型语言模型（LLMs）的研究多依赖于从英语语料库翻译生成的合成数据，尽管这些模型展示了良好的语言理解和翻译能力，但往往未能充分反映目标社区的文化遗产与价值观。本文提出了一种创新方法，通过构建符合特定社区语言、文化遗产及价值观的合成与检索型预训练数据，以实现对社区文化的深度适配。以埃及方言和摩洛哥方言为例进行测试，我们开发了NileChat，一款30亿参数量的语言模型，显著提升了对阿拉伯语相关任务的理解、翻译及文化对齐表现，同时性能媲美更大规模的模型。本研究共享了方法、数据与模型，以推动LLMs在更多元化社区中的发展与包容性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18383" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 17:18:40 GMT</pubDate>
</item>
<item>
<title>RankNovo：基于深度重排序框架的全新从头肽段测序方法</title>
<link>https://arxiv.org/abs/2505.17552</link>
<guid>https://arxiv.org/abs/2505.17552</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RankNovo，首个利用多模型互补性的深度重排序框架提升从头肽段测序性能。</p><br /><br /><p><strong>摘要：</strong> 从头肽段测序是蛋白质组学中的关键任务，但当前基于深度学习的方法受限于质谱数据的复杂性和噪声信号的异质分布，导致数据特定偏差。本文介绍RankNovo，这是一种新的深度重排序框架，通过结合多个测序模型的优势增强从头肽段测序性能。RankNovo采用列表式重排序方法，将候选肽段建模为多重序列比对，并利用轴向注意力提取跨候选肽段的信息特征。此外，引入了两个新指标PMD（肽质量偏差）和RMD（残余质量偏差），通过量化序列和残基层面的肽质量差异提供精细监督。实验表明，RankNovo不仅超越了用于生成训练候选肽段的基线模型，还设定了新的最先进的基准。同时，RankNovo在未见过的模型上展示了强大的零样本泛化能力，突显其稳健性及作为通用重排序框架的潜力。这项工作提出了挑战现有单一模型范式的新型重排序策略，推动了从头肽段测序精度的进步。源代码已在GitHub上发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17552" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:56:55 GMT</pubDate>
</item>
<item>
<title>基于长上下文推理轨迹的高效价值模型训练方法</title>
<link>https://arxiv.org/abs/2505.17373</link>
<guid>https://arxiv.org/abs/2505.17373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需定义“步”的高效价值模型训练方法，显著提升长上下文推理模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种简单而高效的值模型训练方法，专门针对长上下文推理轨迹进行优化。与现有的过程奖励模型（PRMs）相比，该方法避免了对“步”的精细定义问题，这对于长上下文推理模型而言通常难以明确。通过收集包含250万个推理轨迹的数据集，我们训练了一个拥有15亿标记的值模型，并将其应用于DeepSeek模型中，以在测试时间计算扩展时提高性能。实验结果显示，采用块级值引导搜索（VGS）结合最终加权多数投票的方法，在四个竞争性数学基准测试（AIME 2024 & 2025，HMMT Feb 2024 & 2025）中实现了平均45.7%的准确率，达到了与o3-mini-medium相当的水平。此外，VGS大幅减少了实现相同效果所需的推理浮点运算次数。本研究的数据集、模型及代码库均已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 21:05:07 GMT</pubDate>
</item>
<item>
<title>ScanBot：面向高精度表面扫描的指令条件机器人学习数据集</title>
<link>https://arxiv.org/abs/2505.17295</link>
<guid>https://arxiv.org/abs/2505.17295</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ScanBot数据集专注于工业级激光扫描的高精度需求，提供语言引导的机器人扫描轨迹。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ScanBot数据集，该数据集专门用于指导机器人执行高精度表面扫描任务。与现有主要关注粗粒度任务（如抓取、导航或对话）的机器人学习数据集不同，ScanBot针对工业激光扫描的需求设计，强调亚毫米级别的路径连续性和参数稳定性。数据集涵盖了机器人对12种多样化对象执行的6类扫描任务，包括全面扫描、几何区域聚焦、空间参考部分、功能相关结构、缺陷检测及对比分析。每项扫描任务均依据自然语言指令进行，并配以同步的RGB、深度图像及激光轮廓，同时记录机器人姿态与关节状态。尽管当前视觉-语言动作模型已取得进展，但在精细指令下的稳定轨迹生成和实际精度需求方面仍面临挑战。通过在完整感知-规划-执行循环中评估多种多模态大型语言模型，研究揭示了在真实约束条件下指令遵循存在的持续难题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17295" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 17:22:50 GMT</pubDate>
</item>
<item>
<title>基于稀疏大语言模型的中文多语言机器翻译系统FuxiMT</title>
<link>https://arxiv.org/abs/2505.14256</link>
<guid>https://arxiv.org/abs/2505.14256</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的中文多语言机器翻译模型FuxiMT，显著提升低资源场景下的翻译性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了FuxiMT，一种由中国主导的新型多语言机器翻译模型，它基于稀疏化的大规模语言模型（LLM）构建。通过两阶段训练策略，首先利用大规模中文语料进行预训练，然后在包含65种语言的平行数据集上进行多语言微调。FuxiMT采用了专家混合（MoEs）技术，并应用了课程学习策略，以确保在不同资源水平下都能表现稳健。实验结果显示，FuxiMT在多项基准测试中超越了现有最先进的LLM和机器翻译模型，尤其是在低资源条件下表现尤为突出。此外，FuxiMT展现出卓越的零样本翻译能力，能够在缺乏平行数据的语言对之间实现有效的翻译，显示出其在填补跨语言交流障碍方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14256" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 08:09:17 GMT</pubDate>
</item>
<item>
<title>DanceTogether：多角色交互可控视频生成框架</title>
<link>https://arxiv.org/abs/2505.18078</link>
<guid>https://arxiv.org/abs/2505.18078</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个端到端扩散模型DanceTogether，实现多演员身份动作一致的长视频生成。</p><br /><br /><p><strong>摘要：</strong> 近年来可控视频生成技术发展迅速，但现有系统在处理多个演员需移动、互动并交换位置时，尤其是在嘈杂控制信号下表现不佳。本文引入DanceTogether，这是首个基于扩散模型的端到端框架，通过单一参考图像加上独立的姿态掩码流生成高质量的长视频，同时严格保留每个角色的身份。该模型利用MaskPoseAdapter模块，在去噪过程中结合鲁棒跟踪掩码和语义丰富的姿态热图，解决了身份漂移和外观泄漏的问题。为了训练和评估，我们创建了PairFS-4K（包含7000+独特身份的双滑冰者视频）、HumanRob-300（一小时的人形机器人交互数据集）和TogetherVideoBench（涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰的三轨基准测试套件）。实验表明，DanceTogether在TogetherVideoBench上显著优于现有方法。此外，通过一小时的微调，该模型能够生成令人信服的人机交互视频，展示了其在具身AI和人机交互任务中的广泛应用潜力。广泛的消融研究进一步验证了持续的身份动作绑定对性能提升的重要性。综上所述，我们的模型、数据集和基准测试为多演员交互的数字制作、仿真和具身智能开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18078" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:37:14 GMT</pubDate>
</item>
<item>
<title>扩散分类器在组合性理解中的能力研究</title>
<link>https://arxiv.org/abs/2505.17955</link>
<guid>https://arxiv.org/abs/2505.17955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散模型在组合性视觉场景理解中表现出色，但其能力依赖特定条件。</p><br /><br /><p><strong>摘要：</strong> 本文对扩散分类器在多种组合性任务上的判别能力进行了全面研究，涵盖三个扩散模型及十个数据集的三十多个任务。我们探讨了目标数据集领域对性能的影响，并引入新的诊断基准Self-Bench以隔离领域效应。此外，我们分析了时间步加权的重要性及其与领域差距的关系，特别是对SD3-m模型的影响。研究结果表明，扩散分类器确实具备组合性理解的能力，但其表现受条件限制。代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:29:52 GMT</pubDate>
</item>
<item>
<title>FREESON框架：让大型推理模型自主检索知识</title>
<link>https://arxiv.org/abs/2505.16409</link>
<guid>https://arxiv.org/abs/2505.16409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FREESON框架，使大型推理模型能独立完成检索任务。</p><br /><br /><p><strong>摘要：</strong> 现有检索增强推理方法依赖独立的检索模型，限制了大型推理模型在检索中的作用，导致硬件成本增加及错误率上升。为解决这一问题，我们提出了FREESON框架，该框架允许大型推理模型同时作为生成器和检索器，通过引入针对检索任务优化的CT-MCTS算法，在五个开放领域问答基准测试中，平均提升了14.4%的精确匹配率（EM）和F1分数，优于四个具有独立检索器的多步推理模型，并在PopQA和2WikiMultihopQA上分别超过最强基线3%和2%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:00:08 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的位置偏差及其跨语言特性研究</title>
<link>https://arxiv.org/abs/2505.16134</link>
<guid>https://arxiv.org/abs/2505.16134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型语言模型的位置偏差在不同语言中的表现及影响。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型通常表现出位置偏差，即对特定上下文位置信息的系统性忽视，但其与语言多样性之间的相互作用尚不明确。本研究通过分析英语、俄语、德语、印地语和越南语五种类型学上截然不同的语言，探讨位置偏差如何与模型不确定性、句法结构及提示方法交互作用。主要发现包括：(1) 位置偏差由模型驱动且具有语言特异性，例如Qwen2.5-7B更倾向于后期位置而非早期位置；(2) 明确的位置引导会降低多种语言的准确性，挑战传统的提示工程实践；(3) 对齐上下文与位置偏差会增加熵值，但最低熵值并不预测准确性；(4) 进一步研究表明，大型语言模型在自由词序语言（如印地语）中施加主导词序的方式有所不同。这项研究有助于更好地理解位置偏差对多语言处理的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 22:23:00 GMT</pubDate>
</item>
<item>
<title>ReflAct：强化LLM代理推理能力的新框架</title>
<link>https://arxiv.org/abs/2505.15182</link>
<guid>https://arxiv.org/abs/2505.15182</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入ReflAct新框架提升LLM代理在复杂环境中的目标一致性。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLM）代理的进展主要依赖于像ReAct这样的推理后端，但ReAct常产生不一致或不连贯的推理步骤，导致代理实际状态与目标之间的偏差。我们的分析表明，这是由于ReAct无法维持内部信念的一致性及目标对齐，从而引发累积错误和幻觉。为此，我们提出了ReflAct，一种新的推理后端，将推理重点从单纯规划下一步行动转向持续反思代理相对于目标的状态。通过明确地基于状态进行决策并强制执行持续的目标对齐，ReflAct显著提高了策略的可靠性，在ALFWorld环境中平均表现比ReAct高出27.7%，成功率达到93.3%。值得注意的是，ReflAct的表现甚至超过了配备增强模块（如Reflexion、WKM）的ReAct，证明强化核心推理后端是实现可靠代理性能的关键。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15182" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 02:57:39 GMT</pubDate>
</item>
<item>
<title>Quartet：一种高效且精确的大语言模型低精度训练方法</title>
<link>https://arxiv.org/abs/2505.14669</link>
<guid>https://arxiv.org/abs/2505.14669</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Quartet方法，实现全FP4精度的大规模语言模型训练。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于硬件支持的FP4精度训练方法，针对当前算法在低精度训练中的准确性下降问题，提出了名为Quartet的新方法。该方法能够在所有主要计算（如线性层）中均以低精度执行，同时保持高准确性。通过在Llama型模型上的广泛评估，揭示了一种新的低精度缩放定律，用于量化不同位宽下的性能权衡。实验表明，Quartet在NVIDIA Blackwell GPU上实现了最先进的FP4精度训练效果，并成功训练了十亿级规模的模型。此方法展示了全FP4精度训练作为标准精度和FP8训练的竞争替代方案的潜力。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14669" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:55:50 GMT</pubDate>
</item>
<item>
<title>轻量级模型不可知框架s3提升检索增强生成性能</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型框架s3，显著提高检索增强生成系统的下游表现。</p><br /><br /><p><strong>摘要：</strong> 现有的检索增强生成(RAG)系统在推理过程中通过大型语言模型(LLMs)访问外部知识，但优化方法往往忽视下游实用性或限制搜索功能。本研究提出s3框架，该框架通过解耦搜索器与生成器，并采用基于RAG改进的奖励机制训练搜索器，在无需大量训练样本的情况下，显著超越基于更大规模数据训练的基线模型，在六个通用问答和五个医学问答基准测试中表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:53:56 GMT</pubDate>
</item>
<item>
<title>TabSTAR：一种具有语义目标感知表示的表格基础模型</title>
<link>https://arxiv.org/abs/2505.18125</link>
<guid>https://arxiv.org/abs/2505.18125</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TabSTAR通过结合文本特征实现表格数据的迁移学习并达到最先进的性能。</p><br /><br /><p><strong>摘要：</strong> 深度学习在许多领域取得了显著的成功，但在表格学习任务上表现不佳，这些任务仍由梯度提升决策树主导。然而，最近的进步正在推动表格基础模型的发展，这些模型可以利用现实世界的知识并在多样化数据集上进行泛化，尤其是在数据包含自由文本的情况下。尽管将语言模型能力引入表格任务已被探索，但大多数现有方法使用静态、与目标无关的文本表示，限制了它们的有效性。我们介绍了TabSTAR：一种具有语义目标感知表示的表格基础模型。TabSTAR设计用于对具有文本特征的表格数据进行迁移学习，其架构不依赖于特定于数据集的参数。它解冻了一个预训练的文本编码器，并采用目标令牌作为输入，为目标提供上下文以学习任务特定的嵌入。TabSTAR在已知分类任务基准的中型和大型数据集上实现了最先进的性能，其预训练阶段在数据集数量上表现出缩放规律，为未来的性能改进提供了途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18125" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:34:28 GMT</pubDate>
</item>
<item>
<title>QwenLong-CPRS：一种面向长上下文优化的动态压缩框架</title>
<link>https://arxiv.org/abs/2505.18092</link>
<guid>https://arxiv.org/abs/2505.18092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">QwenLong-CPRS通过自然语言指导的动态上下文优化机制提升大模型长序列处理效率。</p><br /><br /><p><strong>摘要：</strong> QwenLong-CPRS是一种针对长上下文优化设计的新型框架，旨在解决大型语言模型（LLMs）在长序列处理时的计算开销问题及“中间迷失”性能下降现象。该框架通过自然语言指令引导的多粒度上下文压缩实现高效且高性能的推理。作为Qwen系列架构的一部分，QwenLong-CPRS引入了四项关键技术：自然语言引导的动态优化、双向推理层增强边界意识、带语言建模头的Token批评机制以及窗口并行推理。经五项基准测试验证，QwenLong-CPRS在准确性、效率和通用性上均表现出色，不仅优于其他上下文管理方法，还能显著提升多种旗舰LLMs的表现，并在特定测试中达到SOTA水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 12:47:00 GMT</pubDate>
</item>
<item>
<title>基于模拟实验引导的假设排序方法</title>
<link>https://arxiv.org/abs/2505.17873</link>
<guid>https://arxiv.org/abs/2505.17873</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用模拟实验结果对假设进行排序的新方法。</p><br /><br /><p><strong>摘要：</strong> 假设排名在自动化科学发现中至关重要，尤其是在自然科学研究领域，实验成本高且效率有限。传统方法仅依赖语言模型推理而未结合实际实验结果。本文引入实验引导的假设排序任务，旨在根据先前测试结果优先排序候选假设。然而，由于重复进行真实实验的难度，该问题难以解决。为此，我们设计了一个基于三个领域导向假设的模拟器，将假设性能建模为与已知真值假设相似性的函数，并受噪声影响。我们还整理了一个包含124个化学假设及其实验结果的数据集来验证模拟器的有效性。在此基础上，我们开发了一种伪实验引导的排序方法，通过聚类共享功能特性的假设并根据模拟反馈优化排序策略。实验表明，该方法优于传统的预实验基线和强基准模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17873" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 09:24:50 GMT</pubDate>
</item>
<item>
<title>QwenLong-L1：通过渐进式上下文扩展提升长上下文推理能力</title>
<link>https://arxiv.org/abs/2505.17667</link>
<guid>https://arxiv.org/abs/2505.17667</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架QwenLong-L1，有效解决长上下文推理中的训练效率低和优化不稳定问题。</p><br /><br /><p><strong>摘要：</strong> 近期大型推理模型（LRMs）在强化学习（RL）的助力下展现了强大的推理能力，但这些改进主要局限于短上下文任务。针对长上下文推理任务，如何高效应用RL仍是一个未解难题。本文首次定义了长上下文推理RL的范式，并分析了其训练效率低下和优化过程不稳定的关键挑战。为此，我们提出了QwenLong-L1框架，通过逐步扩展上下文的方法将短上下文LRMs适应到长上下文场景。具体而言，该框架首先利用有监督微调建立稳健的初始策略，随后采用课程引导的分阶段RL技术稳定策略演化，并结合难度感知的回顾采样策略增强策略探索。实验表明，QwenLong-L1-32B在七个长上下文文档问答基准测试中表现优异，性能优于OpenAI-o3-mini、Qwen3-235B-A22B等旗舰LRMs，与Claude-3.7-Sonnet-Thinking相当，成为当前最先进的LRMs之一，推动了信息密集型环境中稳健推理模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17667" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 05:31:55 GMT</pubDate>
</item>
<item>
<title>Scaling Image and Video Generation via Test-Time Evolutionary Search</title>
<link>https://arxiv.org/abs/2505.17618</link>
<guid>https://arxiv.org/abs/2505.17618</guid>
<content:encoded><![CDATA[
As the marginal cost of scaling computation (data and parameters) during model pre-training continues to increase substantially, test-time scaling (TTS) has emerged as a promising direction for improving generative model performance by allocating additional computation at inference time. While TTS has demonstrated significant success across multiple language tasks, there remains a notable gap in understanding the test-time scaling behaviors of image and video generative models (diffusion-based or flow-based models). Although recent works have initiated exploration into inference-time strategies for vision tasks, these approaches face critical limitations: being constrained to task-specific domains, exhibiting poor scalability, or falling into reward over-optimization that sacrifices sample diversity. In this paper, we propose Evolutionary Search (EvoSearch), a novel, generalist, and efficient TTS method that effectively enhances the scalability of both image and video generation across diffusion and flow models, without requiring additional training or model expansion. EvoSearch reformulates test-time scaling for diffusion and flow models as an evolutionary search problem, leveraging principles from biological evolution to efficiently explore and refine the denoising trajectory. By incorporating carefully designed selection and mutation mechanisms tailored to the stochastic differential equation denoising process, EvoSearch iteratively generates higher-quality offspring while preserving population diversity. Through extensive evaluation across both diffusion and flow architectures for image and video generation tasks, we demonstrate that our method consistently outperforms existing approaches, achieves higher diversity, and shows strong generalizability to unseen evaluation metrics. Our project is available at the website https://tinnerhrhe.github.io/evosearch.
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:25:46 GMT</pubDate>
</item>
<item>
<title>RePrompt：通过强化学习增强文本到图像生成的提示</title>
<link>https://arxiv.org/abs/2505.17540</link>
<guid>https://arxiv.org/abs/2505.17540</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架RePrompt，通过强化学习优化文本到图像生成的提示质量。</p><br /><br /><p><strong>摘要：</strong> 现有文本到图像生成模型在处理简略提示时难以准确反映用户意图，尽管有研究尝试利用大型语言模型（LLMs）改进提示，但这些方法常因缺乏视觉语义和现实世界构图的充分基础而产生风格化或不真实的图像。受语言模型推理进展的启发，我们提出了RePrompt，这是一种新的重新提示框架，通过强化学习引入显式的推理过程到提示优化中。不同于依赖手工规则或风格重写，我们的方法训练一个语言模型生成结构化的自我反思提示，目标是优化图像级结果。定制的奖励模型从人类偏好、语义对齐和视觉构图方面评估生成的图像，为细化提示生成提供间接监督。该方法实现了端到端训练而无需人工标注数据。实验表明，在GenEval和T2I-Compbench基准测试中，RePrompt显著提升了多种文本到图像生成模型的空间布局忠实度和组合泛化能力，达到了新的技术水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17540" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:44:26 GMT</pubDate>
</item>
<item>
<title>Direct3D S2：基于稀疏体素的高效3D形状生成框架</title>
<link>https://arxiv.org/abs/2505.17412</link>
<guid>https://arxiv.org/abs/2505.17412</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Direct3D S2框架，利用稀疏体素显著提升3D形状生成效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Direct3D S2的可扩展3D生成框架，该框架基于稀疏体素实现了高分辨率3D形状生成，同时大幅降低了训练成本。Direct3D S2的关键创新在于空间稀疏注意力机制（SSA），它显著提高了Diffusion Transformer在稀疏体素数据上的计算效率，在前向和反向传播中分别实现了3.9倍和9.6倍的速度提升。此外，框架中的变分自编码器在整个输入、潜在表示和输出阶段保持一致的稀疏体素格式，从而提升了训练效率和稳定性。通过在公开数据集上的实验验证，Direct3D S2不仅在生成质量和效率上超越现有方法，还支持在8块GPU上实现1024分辨率的训练，而传统方法通常需要至少32块GPU才能完成256分辨率的训练。这一突破使得大规模3D形状生成更加实用和普及。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17412" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 22:58:01 GMT</pubDate>
</item>
<item>
<title>FullFront：前端工程全流程多模态大语言模型基准测试</title>
<link>https://arxiv.org/abs/2505.17399</link>
<guid>https://arxiv.org/abs/2505.17399</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FullFront评估多模态大语言模型在前端开发全流程中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为FullFront的新基准，用于评估多模态大语言模型（MLLMs）在整个前端开发流程中的能力。FullFront涵盖了三个基本任务：网页设计（概念化阶段）、网页感知质量保证（视觉组织和元素理解）以及网页代码生成（实现阶段）。与其他仅依赖于复杂或简化数据集的基准不同，FullFront通过新颖的两阶段过程处理真实世界网页，生成干净且标准化的HTML代码，同时保持多样化的设计并避免版权问题。对最新一代MLLMs的广泛测试显示，这些模型在页面感知、代码生成（特别是图像处理和布局方面）以及交互实现上存在显著局限性。研究结果定量展示了不同模型及任务间的性能差异，并揭示了当前MLLMs在前端工程中的实际能力与人类专家之间的巨大差距。FullFront基准及相关代码现已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17399" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 22:16:11 GMT</pubDate>
</item>
<item>
<title>RIPT-VLA：基于强化学习的轻量级视觉-语言-动作模型后训练范式</title>
<link>https://arxiv.org/abs/2505.17016</link>
<guid>https://arxiv.org/abs/2505.17016</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RIPT-VLA通过交互式后训练显著提升视觉-语言-动作模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RIPT-VLA的新型后训练方法，该方法利用稀疏二值成功奖励对预训练的视觉-语言-动作（VLA）模型进行微调，无需依赖大量专家演示数据。RIPT-VLA通过动态滚动采样和留一法优势估计实现了稳定策略优化算法。实验表明，RIPT-VLA不仅适用于多种VLA模型，还能大幅提升模型性能，例如将QueST模型提升了21.2%，使OpenVLA-OFT模型达到97.5%的成功率。此外，RIPT-VLA在计算效率和数据效率上表现出色，在仅有一份演示的情况下，使初始成功率仅为4%的简单微调模型在15轮迭代后成功率达到97%。研究还证明，RIPT-VLA学到的策略在不同任务和场景中具有良好的泛化能力，且对初始状态上下文具有鲁棒性。这些成果表明，RIPT-VLA是一种实用且高效的VLA模型后训练方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17016" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs</title>
<link>https://arxiv.org/abs/2505.16770</link>
<guid>https://arxiv.org/abs/2505.16770</guid>
<content:encoded><![CDATA[
The rapid advancement of native multi-modal models and omni-models, exemplified by GPT-4o, Gemini, and o3, with their capability to process and generate content across modalities such as text and images, marks a significant milestone in the evolution of intelligence. Systematic evaluation of their multi-modal output capabilities in visual thinking processes (also known as multi-modal chain of thought, M-CoT) becomes critically important. However, existing benchmarks for evaluating multi-modal models primarily focus on assessing multi-modal inputs and text-only reasoning while neglecting the importance of reasoning through multi-modal outputs. In this paper, we present a benchmark, dubbed RBench-V, designed to assess models' vision-indispensable reasoning abilities. To construct RBench-V, we carefully hand-pick 803 questions covering math, physics, counting, and games. Unlike previous benchmarks that typically specify certain input modalities, RBench-V presents problems centered on multi-modal outputs, which require image manipulation such as generating novel images and constructing auxiliary lines to support the reasoning process. We evaluate numerous open- and closed-source models on RBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the best-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below the human score of 82.3%, highlighting that current models struggle to leverage multi-modal reasoning. Data and code are available at https://evalmodels.github.io/rbenchv
]]></content:encoded>
<pubDate>Thu, 22 May 2025 11:11:57 GMT</pubDate>
</item>
<item>
<title>ClearNight：多天气夜间图像复原框架</title>
<link>https://arxiv.org/abs/2505.16479</link>
<guid>https://arxiv.org/abs/2505.16479</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ClearNight框架，有效复原受多种恶劣天气影响的夜间图像。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多天气条件下夜间图像复原这一具有挑战性的研究问题，提出了ClearNight框架，该框架结合Retinex双先验提取技术及天气感知动态特异性-共同性协作方法，可以一次性高效去除复杂的多重退化效果。同时，构建了AllWeatherNight数据集，用于支持研究并验证模型性能。实验表明，ClearNight在合成及真实世界图像上均达到当前最优水平，且消融实验进一步证明了数据集和框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16479" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 06:06:35 GMT</pubDate>
</item>
<item>
<title>Notes Writing提升多跳问答中的迭代RAG性能</title>
<link>https://arxiv.org/abs/2505.16293</link>
<guid>https://arxiv.org/abs/2505.16293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Notes Writing方法，通过生成简洁相关笔记减少噪声并提高模型处理能力。</p><br /><br /><p><strong>摘要：</strong> 当前基于检索增强生成（RAG）的多跳问答系统面临长上下文和无关信息积累的问题，限制了模型的推理能力。现有压缩检索信息的方法要么局限于单轮RAG，要么需要微调且扩展性差。为解决这些问题，我们提出了Notes Writing方法，在每次迭代中生成简洁相关的文档笔记，有效降低噪声并保留关键信息，间接增加了大型语言模型的有效上下文长度。该方法框架无关，可与多种迭代RAG方法集成。实验表明，Notes Writing在三种迭代RAG方法、两种模型和四个评估数据集上平均提升了15.6个百分点的整体性能，同时仅小幅增加输出标记数。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 02:45:05 GMT</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:13:09 GMT</pubDate>
</item>
<item>
<title>NOVER：无需外部验证器的强化学习框架</title>
<link>https://arxiv.org/abs/2505.16022</link>
<guid>https://arxiv.org/abs/2505.16022</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需外部验证器的强化学习框架NOVER，显著提升文本到文本任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为NOVER的新框架，该框架是一种无需外部验证器的强化学习方法，仅需标准监督微调数据即可实现激励训练。与依赖外部评估器的传统方法不同，NOVER通过奖励机制直接基于语言模型输出的最终答案部分进行强化学习，从而鼓励生成中间推理步骤。实验表明，NOVER在多种文本到文本任务上表现优异，其优化的模型性能超越了从大型推理模型（如DeepSeek R1 671B）蒸馏出的同规模模型7.7个百分点。此外，NOVER的灵活性还开启了逆向激励训练等新优化可能性，进一步拓展了大语言模型的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16022" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 17:12:35 GMT</pubDate>
</item>
<item>
<title>PhyX：首个大规模物理推理视觉场景基准测试</title>
<link>https://arxiv.org/abs/2505.15929</link>
<guid>https://arxiv.org/abs/2505.15929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PhyX评估模型在物理推理上的能力，发现顶级模型表现远逊于人类。</p><br /><br /><p><strong>摘要：</strong> 现有智能评估基准未能涵盖智力的关键方面——物理推理。为填补这一空白，我们推出了PhyX，这是首个专门用于评估模型在视觉场景中物理推理能力的大规模基准。PhyX包含3000个精心策划的多模态问题，涉及六大核心物理领域及25种子领域中的六种推理类型。通过全面评估发现，即使最先进的模型如GPT-4o、Claude3.7-Sonnet和GPT-o4-mini，在物理推理上也表现不佳，分别仅达到32.5%、42.2%和45.8%的准确率，与人类专家相比存在超过29%的性能差距。分析表明，当前模型存在过度依赖记忆知识、数学公式及表面视觉模式匹配等局限性。为了深入剖析这些问题，我们采用了多种评估范式进行详细案例研究，并提供兼容的评估协议以促进可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 14:33:50 GMT</pubDate>
</item>
<item>
<title>大规模语言模型在敏感领域的上下文安全保护研究</title>
<link>https://arxiv.org/abs/2505.15805</link>
<guid>https://arxiv.org/abs/2505.15805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有大模型在敏感领域中难以遵守用户定义的安全策略。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在企业及政府等敏感领域的应用日益广泛，确保这些模型能够遵循用户定义的上下文安全政策变得至关重要，尤其是针对信息非披露的要求。然而，尽管已有研究关注LLMs的一般安全性和社会敏感数据处理，但针对上下文安全性的大规模基准测试仍然匮乏。为解决这一问题，本文引入了一个名为CoPriva的新基准数据集，用于评估问答场景下LLMs对上下文非披露策略的遵守情况。该数据集基于现实情境构建，包含明确的策略声明和查询设计，旨在模拟直接与间接的信息泄露攻击。通过对10种主流LLMs进行测试，研究发现许多模型存在严重漏洞，尤其是在面对间接攻击时会泄露敏感信息。此外，分析表明，虽然模型通常能够正确回答查询，但在生成过程中整合政策约束的能力较弱，而通过显式提示修改输出则表现出一定能力。本研究揭示了当前LLMs在敏感应用场景中的重大安全隐患，强调了开发更强大保障机制的紧迫性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:58:11 GMT</pubDate>
</item>
<item>
<title>TIME：面向现实世界场景的大规模多层级时序推理基准</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TIME基准，解决现有模型在时序推理中的三大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型语言模型（LLMs）在时序推理能力上的不足，提出了一个新的多层级基准TIME，该基准旨在应对真实世界场景中的复杂时序推理问题。TIME包含38,522个问答对，涵盖三个难度级别及11个细粒度子任务，并分为反映不同现实挑战的三个子数据集：TIME-Wiki、TIME-News和TIME-Dial。通过在多种推理与非推理模型上的实验分析，研究揭示了测试时扩展对时序推理能力的影响，并发布了TIME-Lite作为人工标注的子集以促进未来研究。TIME基准及相关代码和数据已公开，有助于推动时序推理领域的标准化评估和进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 05:22:02 GMT</pubDate>
</item>
<item>
<title>基于合成数据的强化学习方法提升大模型性能</title>
<link>https://arxiv.org/abs/2505.17063</link>
<guid>https://arxiv.org/abs/2505.17063</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用合成数据实现强化学习微调，显著提升大模型在多任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Synthetic Data RL的新框架，该框架通过仅使用从任务定义生成的合成数据对基础模型进行强化学习微调，从而避免了大规模人工标注数据的需求。实验表明，该方法在多个基准测试上显著优于基线模型及指令微调等传统方法。例如，在GSM8K数据集上，模型性能提升了29.2%，同时在其他领域如数学、问答等也有显著改进。此外，研究发现少量的人类演示对最终性能的提升有限，进一步证明了合成数据的有效性。这项工作展示了在减少人工标注的情况下实现高效模型适应的可能性，为强化学习的实际应用提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17063" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 01:35:13 GMT</pubDate>
</item>
<item>
<title>引入正交残差更新以提升深度神经网络的特征学习能力</title>
<link>https://arxiv.org/abs/2505.11881</link>
<guid>https://arxiv.org/abs/2505.11881</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出正交残差更新策略，显著改善多种架构的泛化性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为正交残差更新的新方法，通过将模块的输出分解为与输入流平行和正交的部分，并仅添加正交部分，促使模块主要学习全新的特征方向，从而丰富特征表示并提高训练效率。实验表明，该策略在ResNetV2、视觉Transformer等架构及CIFARs、TinyImageNet、ImageNet-1k等多个数据集上提升了泛化准确率和训练稳定性，例如ViT-B在ImageNet-1k上的top-1准确率提高了+4.3个百分点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11881" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 03:16:11 GMT</pubDate>
</item>
<item>
<title>Time-R1：为大语言模型赋予全面的时间智能</title>
<link>https://arxiv.org/abs/2505.13508</link>
<guid>https://arxiv.org/abs/2505.13508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Time-R1通过创新方法使中等规模语言模型具备出色的时间理解、预测及创造性生成能力。</p><br /><br /><p><strong>摘要：</strong> 现有大型语言模型虽表现出色，但在时间推理方面存在明显不足，难以将历史事件理解与未来预测相结合。目前大多数相关研究仅关注单一时间技能，如过去事件问答或基础预测，且泛化能力较差。为解决这些问题，我们提出了Time-R1框架，该框架基于一个精心设计的动态规则奖励系统，采用三阶段强化学习课程，使一个中等规模（30亿参数）的语言模型获得了全面的时间理解、预测及创造能力。实验表明，Time-R1在未来的事件预测和创造性场景生成任务上超越了比其大200倍以上的模型，例如最先进的671B参数DeepSeek-R1。此外，我们还发布了Time-Bench，这是一个源自10年新闻数据的大规模多任务时间推理数据集，以及一系列Time-R1检查点，以促进进一步的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 09:46:28 GMT</pubDate>
</item>
<item>
<title>V-Triune：统一强化学习提升视觉语言模型的推理与感知能力</title>
<link>https://arxiv.org/abs/2505.18129</link>
<guid>https://arxiv.org/abs/2505.18129</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出V-Triune系统，使视觉语言模型同时学习推理与感知任务。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为V-Triune的Visual Triple Unified Reinforcement Learning系统，它通过整合样本级数据格式化、验证器级奖励计算和源级指标监控三个互补组件，使得视觉语言模型(VLMs)能够在单一训练管道中联合学习视觉推理和感知任务。此外，还引入了动态IoU奖励，以提供自适应反馈。该方法基于开源7B和32B骨干模型，在多样化的数据集上进行训练，生成的Orsta模型在多种推理和感知任务上均表现出显著性能提升，特别是在MEGA-Bench Core测试中表现优异。这些成果证明了统一强化学习方法在视觉语言模型中的有效性和可扩展性。V-Triune系统及其相关模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.18129" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 13:41:14 GMT</pubDate>
</item>
<item>
<title>VeriThinker: Learning to Verify Makes Reasoning Model Efficient</title>
<link>https://arxiv.org/abs/2505.17941</link>
<guid>https://arxiv.org/abs/2505.17941</guid>
<content:encoded><![CDATA[
Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker
]]></content:encoded>
<pubDate>Fri, 23 May 2025 10:17:56 GMT</pubDate>
</item>
<item>
<title>Trinity-RFT：一种灵活可扩展的大语言模型强化微调框架</title>
<link>https://arxiv.org/abs/2505.17826</link>
<guid>https://arxiv.org/abs/2505.17826</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Trinity-RFT是一种用于大语言模型强化微调的通用框架。</p><br /><br /><p><strong>摘要：</strong> Trinity-RFT是一种旨在实现大语言模型强化微调（RFT）的通用、灵活且可扩展的框架。该框架采用解耦设计，包含统一同步/异步、在线/离线等模式的RFT核心模块，高效稳健的智能体-环境交互集成，以及针对RFT优化的数据管道系统。其设计目标是适应多样化的应用场景，并作为探索先进强化学习范式的统一平台。本技术报告介绍了Trinity-RFT的愿景、功能、设计与实现细节，并通过大量示例展示了该框架的实用性和易用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17826" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 08:41:09 GMT</pubDate>
</item>
<item>
<title>Agent Distillation：将大型语言模型推理能力迁移至小规模模型</title>
<link>https://arxiv.org/abs/2505.17612</link>
<guid>https://arxiv.org/abs/2505.17612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Agent Distillation框架，使小规模语言模型通过模仿大型模型完成任务实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文针对大规模语言模型（LLMs）在实际部署中的计算成本问题，提出Agent Distillation框架，该框架不仅迁移推理能力，还转移大型模型基于检索和代码工具的任务解决行为至小规模语言模型（sLMs）。通过引入first-thought prefix提示方法提升教师模型生成轨迹的质量，并采用自一致性动作生成技术增强测试时小模型的鲁棒性。实验在事实性和数学推理任务上验证了方法的有效性，证明参数量仅为0.5B、1.5B、3B的小模型在性能上可媲美更大规模的模型，展示了构建实用工具型小模型的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 04:20:15 GMT</pubDate>
</item>
<item>
<title>ANSE：通过主动噪声选择提升视频扩散模型质量</title>
<link>https://arxiv.org/abs/2505.17561</link>
<guid>https://arxiv.org/abs/2505.17561</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ANSE框架，利用注意力机制量化不确定性，优化视频扩散模型的初始噪声选择。</p><br /><br /><p><strong>摘要：</strong> 本文研究了初始噪声对视频扩散模型生成质量和对齐速度的影响，发现相同提示下不同的噪声种子可能导致显著差异的结果。尽管现有方法依赖外部设计先验如频率滤波或帧间平滑，但忽略了内部模型信号。为此，我们提出了ANSE（Active Noise Selection for Generation），这是一种基于模型感知的框架，通过量化的基于注意力的不确定性选择高质量的噪声种子。ANSE的核心是BANSA（Bayesian Active Noise Selection via Attention），它通过多个随机注意力样本之间的熵分歧测量模型置信度和一致性。为了高效部署，我们还引入了BANSA的伯努利掩码近似版本，使得仅需单步扩散和部分注意力层即可进行评分估计。实验结果显示，在CogVideoX-2B和5B数据集上，ANSE提高了视频质量和时间一致性，分别仅增加了8%和13%的推理时间，提供了一种有原则且通用的视频扩散噪声选择方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17561" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 03:09:10 GMT</pubDate>
</item>
<item>
<title>基于课程学习的大语言模型幻觉检测方法</title>
<link>https://arxiv.org/abs/2505.17558</link>
<guid>https://arxiv.org/abs/2505.17558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用精心设计的幻觉样本作为负例的课程学习策略，提升大语言模型的幻觉检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）难以准确检测幻觉文本这一挑战，创新性地将高质量的幻觉样本作为负例引入到DPO对齐过程中。通过引入课程学习策略，逐步从概率分数降低最多的简单样本开始训练，再到复杂样本，确保了学习过程的稳定性和渐进性。实验表明，采用此方法训练的HaluCheck模型在MedHallu和HaluEval等困难基准测试中表现优异，各项指标提升最高可达24%，并且在零样本设置下展现出强大的鲁棒性，优于现有最先进的大型模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 03:05:09 GMT</pubDate>
</item>
<item>
<title>基于正则化策略梯度的在线强化学习方法研究</title>
<link>https://arxiv.org/abs/2505.17508</link>
<guid>https://arxiv.org/abs/2505.17508</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RPG框架，优化大语言模型推理能力，提升训练稳定性与性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了正则化策略梯度（RPG）框架，用于系统性地推导和分析在线强化学习（RL）中的KL散度正则化策略梯度方法。我们针对前向和反向KL散度正则化的两种目标函数，分别推导出对应的策略梯度和代理损失函数，同时考虑归一化与非归一化策略分布。此外，还提供了完全可微的损失函数及REINFORCE风格的梯度估计器，满足多样化的算法需求。通过在大语言模型推理任务上的实验，RPG方法在训练稳定性和性能方面表现出优于GRPO、REINFORCE++和DAPO等基线算法的结果。该研究为KL正则化在在线RL中的应用提供了新视角，并推动了大语言模型推理能力的提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17508" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 23 May 2025 02:01:21 GMT</pubDate>
</item>
<item>
<title>基于语义表征的语音指令数据生成方法</title>
<link>https://arxiv.org/abs/2505.17417</link>
<guid>https://arxiv.org/abs/2505.17417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需文本到语音模型的语音指令数据生成方法。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型驱动的语音助手快速发展，对训练这些系统所需的语音指令数据的需求日益增长。然而，尽管存在大量语音识别数据，高质量的语音指令数据却相对匮乏。传统生成高质量合成语音需要依赖优秀的文本到语音模型，但对于资源匮乏的语言来说，这一条件难以满足。本文提出了一种创新方法，在语义表示层停止合成过程，绕过对文本到语音模型的依赖。通过将合成的语义表示与预训练的Whisper编码器对齐，使大型语言模型能够在文本指令上进行微调的同时，在推理过程中仍能理解口语指令。此简化训练流程为低资源语言构建语音助手提供了有前景的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 23:05:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型中的推理刚性问题及其诊断研究</title>
<link>https://arxiv.org/abs/2505.17225</link>
<guid>https://arxiv.org/abs/2505.17225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型常因推理惯性忽视用户指令，本文通过构建诊断集揭示其问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型在复杂推理任务中表现出色，但普遍存在推理刚性问题，即倾向于依赖熟悉的推理模式，即使面对明确的用户指示也容易偏离正轨，导致错误结论。这一现象在数学和逻辑领域尤为突出。为了系统研究推理刚性，本文构建了一个由专家精心策划的诊断数据集，其中包括对AIME和MATH500等基准测试的修改版本，以及重新设计的经典谜题，旨在迫使模型偏离常规推理路径。通过分析，我们识别出三种主要的污染模式：解释过载、输入不信任和部分指令关注，这些模式导致模型忽略或扭曲提供的指示。本研究公开发布了诊断集，以促进未来缓解语言模型推理刚性的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 15:00:01 GMT</pubDate>
</item>
<item>
<title>CANOE框架提升大语言模型生成任务的准确性</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需人工标注的CANOE框架，显著提高大语言模型在多种任务中的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为CANOE的系统框架，旨在提升大型语言模型（LLMs）在短文本和长文本生成任务中的准确性，而无需依赖人工标注。通过合成多样化的问答数据，构建高质量且易于验证的训练集，并引入基于规则的强化学习方法Dual-GRPO，该框架优化了生成任务中的规则性奖励。实验表明，CANOE在11项下游任务中均表现出色，甚至超过了最先进的模型如GPT-4o和OpenAI o1。这项研究强调了在信息检索系统中构建可靠模型的重要性，同时提供了有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 06:10:07 GMT</pubDate>
</item>
<item>
<title>Transformer Copilot：通过日志驱动优化的大语言模型增强框架</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Transformer Copilot框架，通过日志记录和日志驱动的日志纠正显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Transformer Copilot的新框架，该框架通过引入日志驱动的学习机制来改进大型语言模型的下游任务表现。Transformer Copilot由Pilot模型和Copilot模型组成，其中Pilot模型作为主模型，而Copilot模型则负责通过修正预测结果（logits）来提高Pilot的推理性能。通过构建Mistake Log来系统地跟踪和分析模型在微调过程中的错误模式，Transformer Copilot实现了对模型学习信号的深度利用。实验表明，在涉及常识推理、算术问题及推荐系统的12项基准测试中，Transformer Copilot可以将模型性能提升高达34.5%，同时保持较低的计算开销和良好的可扩展性与迁移能力。此外，我们还提供了理论和实证分析以支持该框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 02:00:45 GMT</pubDate>
</item>
<item>
<title>AudioTrust：首个面向音频大语言模型的多维度可信性评估框架</title>
<link>https://arxiv.org/abs/2505.16211</link>
<guid>https://arxiv.org/abs/2505.16211</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个针对音频大语言模型的多维度可信性评估框架AudioTrust。</p><br /><br /><p><strong>摘要：</strong> 现有的大型语言模型评估框架主要集中在文本模态，而针对音频模态的系统性研究较少，尤其缺乏对音频特性及应用场景的独特考量。本文引入AudioTrust，这是首个专门设计用于评估音频大型语言模型（ALLMs）可信性的多维度框架和基准。AudioTrust涵盖公平性、幻觉生成、安全性、隐私保护、鲁棒性和认证六个关键维度，并通过18种实验设置进行综合评估。该框架基于超过4,420个来自真实场景（如日常对话、紧急呼叫、语音助手交互）的音频/文本样本构建，采用9个音频专用评估指标，并结合大规模自动化评分管道实现客观评估。实验结果显示当前最先进的开源和闭源ALLMs在高风险音频场景中的可信性边界与局限性，为未来音频模型的安全部署提供了重要参考。AudioTrust平台和基准现已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16211" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 00:27:46 GMT</pubDate>
</item>
<item>
<title>TAPO：通过融入外部知识提升强化学习推理能力</title>
<link>https://arxiv.org/abs/2505.15692</link>
<guid>https://arxiv.org/abs/2505.15692</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入外部知识的TAPO框架显著提升了推理模型的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TAPO的新框架，它通过整合外部高阶指导（即“思想模式”）增强强化学习的能力。与传统方法相比，TAPO在训练过程中平衡了内部探索与外部指导利用，从而显著提高了推理模型的表现，在AIME、AMC和Minerva Math等任务上的表现均优于现有方法。实验表明，这些抽象自少量先验样本的思想模式可以有效推广到多种任务和模型中，展示了TAPO在多领域应用中的潜力。此外，引入外部指导还增强了模型的解释性和输出可读性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15692" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 12:06:10 GMT</pubDate>
</item>
<item>
<title>基于真实表情包评估视觉语言模型的安全性</title>
<link>https://arxiv.org/abs/2505.15389</link>
<guid>https://arxiv.org/abs/2505.15389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现当前视觉语言模型对普通用户分享的表情包更易产生有害输出。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了现有视觉语言模型(VLMs)在面对普通用户广泛传播的表情包时的安全性问题。通过构建MemeSafetyBench数据集，包含50,430组真实表情包及其对应指令，结合大语言模型生成指令，我们评估了多种VLMs在单轮与多轮交互中的安全性表现。研究显示，表情包相较于合成或纯文本输入显著增加了有害输出并降低了拒绝率，尽管多轮对话提供了一定缓解作用，但模型仍表现出较高脆弱性。研究强调了生态有效评估的重要性及加强安全机制的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:26:40 GMT</pubDate>
</item>
<item>
<title>基于文本大模型的跨模态学习能力研究</title>
<link>https://arxiv.org/abs/2505.17091</link>
<guid>https://arxiv.org/abs/2505.17091</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过训练文本模型，模型自动发展出理解和处理图像及音频的能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种有趣的现象：通过在文本标记上训练自回归语言模型（LLM），该模型在内部自发发展出理解图像和音频的能力，从而仅通过阅读就能具备视觉和听觉功能。传统方法通常通过微调文本LLM生成特定条件下的音频或视觉输出，而我们提出的架构直接将图像块、音频波形或标记作为输入，输出分类嵌入或标签。实验表明，这种基于文本权重的方法在FSD-50K、GTZAN数据集上的音频分类以及CIFAR-10、Fashion-MNIST数据集上的图像分类中表现良好，甚至可以处理图像补丁。这一发现推动了文本-LLM学习强大内部电路的概念，即激活必要的连接即可用于多种应用，而非每次都需要从头开始训练模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17091" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 18:20:16 GMT</pubDate>
</item>
<item>
<title>利用生成模型的感知组织能力实现类别无关实例分割</title>
<link>https://arxiv.org/abs/2505.15263</link>
<guid>https://arxiv.org/abs/2505.15263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过微调Stable Diffusion和MAE实现类别无关的实例分割，发现生成模型具备跨类别泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何利用生成模型对物体边界的内在理解，将其应用于通用感知组织任务。研究者通过引入实例着色损失函数，在有限的室内家具和汽车类型上对Stable Diffusion和MAE进行微调，实现了类别无关的实例分割。令人惊讶的是，这些模型在未见过的对象类型和风格上表现出强大的零样本泛化能力，甚至超过了高度监督的SAM模型在细结构和模糊边界上的表现。相比之下，现有的提示可调分割架构或判别式预训练模型无法泛化。这表明生成模型学习到的分组机制可以跨类别和领域转移，即使没有互联网规模的预训练。相关代码、预训练模型和演示可在项目网站获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 04:42:05 GMT</pubDate>
</item>
<item>
<title>RoPECraft：基于旋转位置嵌入的无训练视频动作迁移方法</title>
<link>https://arxiv.org/abs/2505.13344</link>
<guid>https://arxiv.org/abs/2505.13344</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅通过修改旋转位置嵌入实现扩散变压器视频动作迁移的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RoPECraft的训练自由视频运动转移方法，该方法专门针对扩散变压器设计，通过仅修改其旋转位置嵌入（RoPE）来实现动作迁移。首先，从参考视频中提取密集光流，并利用运动偏移量扭曲复杂指数张量的RoPE，从而将运动编码到生成过程中。在去噪时间步中，通过目标速度和预测速度之间的轨迹对齐进一步优化这些嵌入，采用流匹配目标函数。为了保持输出忠实于文本提示并防止重复生成，还引入了一个基于参考视频傅里叶变换相位分量的正则化项，将相位角度投影到一个平滑流形上以抑制高频伪影。实验表明，RoPECraft在基准测试中超越了所有近期发布的同类方法，无论是定性还是定量评估均表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13344" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 12:50:26 GMT</pubDate>
</item>
<item>
<title>通过高质量训练数据提升检索与重排序模型性能</title>
<link>https://arxiv.org/abs/2505.16967</link>
<guid>https://arxiv.org/abs/2505.16967</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">去除部分低质量数据集显著提升检索模型效果。</p><br /><br /><p><strong>摘要：</strong> 目前构建强大的检索与重排序模型通常依赖大规模数据集，如BGE集合包含160万查询-文档对。然而，我们发现某些数据集可能对模型有效性产生负面影响，在从BGE集合中剔除8个数据集后，训练集规模缩小至原来的1/2.35，同时BEIR上的nDCG@10得分提升了1.0点。因此，本文深入研究了训练数据的质量问题，特别是“假阴性”现象，即相关文档被错误标记为不相关的情况。为此，提出了一种简单且成本效益高的方法，利用级联大型语言模型提示来识别并重新标注这些假阴性样本。实验结果显示，将假阴性样本重新标注为真阳性可以提高E5（基础版）和Qwen2.5-7B检索模型在BEIR上的nDCG@10得分0.7到1.4点，并在零样本AIR-Bench评估中提升1.7到1.8点。对于基于重新标注数据微调的重排序器，例如Qwen2.5-3B在BEIR上的表现也有类似改进。此外，人类注释结果表明，GPT-4o的判断与人工标注的一致性远高于GPT-4o-mini，进一步验证了该级联设计的可靠性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16967" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:47:57 GMT</pubDate>
</item>
<item>
<title>WebAgent-R1：一种高效的多轮网络代理强化学习框架</title>
<link>https://arxiv.org/abs/2505.16421</link>
<guid>https://arxiv.org/abs/2505.16421</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">WebAgent-R1显著提升了多轮网络交互任务的成功率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WebAgent-R1的端到端多轮强化学习框架，用于训练网络代理处理复杂的动态网页环境。该框架通过异步生成多样化轨迹，仅依赖任务成功与否的二元奖励进行指导。实验表明，WebAgent-R1在WebArena-Lite基准测试中大幅提高了Qwen和Llama等模型的任务成功率。此外，研究还探讨了基于思考的提示策略及测试时扩展方法的有效性，并通过引入WebAgent-R1-Zero和WebAgent-R1-CoT两种变体，强调了行为克隆暖启动阶段的重要性以及长链推理在网页任务中的应用价值。这些成果显著优于现有最先进的方法，为多轮网络交互提供了新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16421" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:07:43 GMT</pubDate>
</item>
<item>
<title>Think-RM：一种基于生成式奖励模型的强化学习新框架</title>
<link>https://arxiv.org/abs/2505.16265</link>
<guid>https://arxiv.org/abs/2505.16265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Think-RM通过长时推理提升生成式奖励模型性能，优化强化学习从人类反馈。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Think-RM的新框架，旨在通过模拟内部思考过程，使生成式奖励模型（GenRMs）具备长时推理能力。与传统方法相比，Think-RM生成灵活的自我引导推理轨迹，支持反思、假设推理等高级能力。研究首先通过监督微调（SFT）利用长链式思维数据对模型进行预热，随后借助基于规则的强化学习进一步增强其长时推理能力。此外，我们还设计了一种新颖的基于成对偏好的强化学习从人类反馈（RLHF）管道，直接优化策略而不需转换为点式奖励信号。实验表明，Think-RM在RM-Bench基准测试中表现优异，比传统Bradley-Terry奖励模型和垂直扩展的GenRM分别高出8%和显著改进。结合成对RLHF管道后，其最终策略性能也优于传统方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 01:56:11 GMT</pubDate>
</item>
<item>
<title>FoVer：基于形式验证的大规模语言模型过程奖励模型训练方法</title>
<link>https://arxiv.org/abs/2505.15960</link>
<guid>https://arxiv.org/abs/2505.15960</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FoVer方法，通过形式验证工具自动生成标注，提升大规模语言模型在多种推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于解决过程奖励模型（PRMs）在大规模语言模型（LLMs）推理过程中面临的两大挑战：一是人工标注成本高昂，二是现有PRMs局限于数学推理问题。为填补这些研究空白，我们提出了FoVer方法，利用形式验证工具如Z3和Isabelle自动生成符号任务的逐级错误标签，从而无需人工标注即可创建训练数据集。实验表明，基于该数据集训练的PRMs不仅在形式逻辑和定理证明任务上表现出色，还能在跨任务验证中显著超越传统基线PRMs，并达到与人类标注或更强模型训练的PRMs相当甚至更好的效果。此外，相关数据集、模型及代码已公开共享。关键词：过程奖励模型，形式验证，跨任务泛化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15960" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:23:45 GMT</pubDate>
</item>
<item>
<title>RAVENEA：通过检索增强视觉文化理解</title>
<link>https://arxiv.org/abs/2505.14462</link>
<guid>https://arxiv.org/abs/2505.14462</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入RAVENEA基准，提升视觉文化理解能力。</p><br /><br /><p><strong>摘要：</strong> 随着视觉语言模型(VLMs)日益融入日常生活，准确理解视觉文化变得至关重要。然而，现有模型常无法有效解读文化细微差别。虽然文本检索增强生成(RAG)在单模态场景中已被证明有效，但在多模态应用方面仍待探索。为此，我们提出了RAVENEA（Retrieval-Augmented Visual culturE uNdErstAnding），这是一个新的基准，旨在通过检索推动视觉文化理解，专注于文化导向的视觉问答(cVQA)和图像描述(cIC)两个任务。RAVENEA通过整合超过10,000份由人工注释者整理和排名的Wikipedia文档扩展了现有数据集。通过训练和评估七个多模态检索器，我们发现当轻量级VLMs结合文化感知检索时，在cVQA和cIC任务上分别比非增强版本高出至少3.2%和6.2%，证明了检索增强方法及文化包容性基准的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14462" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:57:16 GMT</pubDate>
</item>
<item>
<title>SAKURA：评估大型音频语言模型多跳推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.13237</link>
<guid>https://arxiv.org/abs/2505.13237</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型音频语言模型在多跳推理方面存在挑战。</p><br /><br /><p><strong>摘要：</strong> 尽管大型音频语言模型（LALMs）在语音和音频处理任务上的表现得到了广泛研究，但其推理能力尤其是多跳推理能力却未被充分探索。现有基准主要集中在一般语音和音频处理任务、对话能力和公平性等方面，忽视了对多跳推理的系统评估。为填补这一空白，我们提出了SAKURA，这是一个基于语音和音频信息评估LALMs多跳推理能力的新基准。实验结果显示，即使LALMs能够正确提取相关信息，它们在整合语音/音频表示进行多跳推理时仍面临困难。这一发现揭示了LALMs的一个关键局限性，为未来研究提供了重要的见解和资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13237" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 11:20:32 GMT</pubDate>
</item>
<item>
<title>通过强化学习提升视觉生成中的语义-空间推理能力</title>
<link>https://arxiv.org/abs/2505.17022</link>
<guid>https://arxiv.org/abs/2505.17022</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GoT-R1框架，利用强化学习增强视觉生成模型的语义-空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GoT-R1的新框架，该框架将强化学习应用于视觉生成模型，以提高其处理复杂文本提示的能力，特别是那些涉及多个具有精确空间关系和属性的对象的提示。GoT-R1基于Generation Chain-of-Thought方法，使模型能够自主发现有效的推理策略，而无需依赖预定义模板。为此，我们提出了一个双阶段多维奖励框架，利用多语言大模型（MLLMs）评估推理过程和最终输出，从而在整个生成管道中实现有效监督。奖励系统采用统一的方法评估语义对齐、空间准确性以及视觉质量。实验结果显示，在T2I-CompBench基准上的表现显著提升，特别是在涉及精确空间关系和属性绑定的组合任务上。GoT-R1的成功标志着图像生成领域在引入复杂推理能力方面取得了新的进展。为了促进未来的研究，我们的代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17022" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>基于三阶段框架Let Androids Dream的图像隐含含义理解</title>
<link>https://arxiv.org/abs/2505.17019</link>
<guid>https://arxiv.org/abs/2505.17019</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架LAD，有效解决AI在图像隐含含义理解中的上下文缺失问题。</p><br /><br /><p><strong>摘要：</strong> 现有AI系统在理解图像隐喻时面临文化、情感及语境等复杂因素的挑战，而多模态大型语言模型虽在基础视觉问答任务上表现良好，但在处理图像隐含意义时存在上下文断层的问题。受人类认知启发，我们提出了名为Let Androids Dream (LAD) 的新型框架，通过感知、搜索和推理三个阶段解决上下文缺失问题。实验表明，使用轻量级GPT-4o-mini模型的LAD在英文基准测试中达到当前最佳性能，在中文基准测试中也取得了显著改进。此外，该研究为AI更有效地解读图像隐含含义提供了新视角，推动了视觉-语言推理和人机交互领域的发展。项目代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17019" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的多帧空间理解增强</title>
<link>https://arxiv.org/abs/2505.17015</link>
<guid>https://arxiv.org/abs/2505.17015</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种框架，通过整合深度感知等能力使多模态大语言模型具备多帧空间理解能力。</p><br /><br /><p><strong>摘要：</strong> 多模态大型语言模型（MLLMs）在视觉任务上取得了显著进展，但其空间理解主要局限于单张图像，难以满足需要多帧推理的实际应用需求，如机器人学。本文提出了一种新框架，通过引入深度感知、视觉对应及动态感知，使MLLMs具备强大的多帧空间理解能力。为此，我们创建了MultiSPA数据集，该数据集包含超过2700万个样本，覆盖多样化的3D和4D场景。同时，我们设计了一个综合基准测试，用于评估多种空间任务的表现。实验结果显示，我们的模型Multi-SpatialMLLM在多项基准测试中显著优于基线模型和专有系统，展示了其在多帧推理上的可扩展性和泛化能力。此外，我们还观察到多任务收益以及在复杂场景中的潜在能力，并展示了该模型如何作为机器人领域的多帧奖励标注器发挥作用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17015" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>AgentIF：大型语言模型在具身场景下指令遵循能力的评估基准</title>
<link>https://arxiv.org/abs/2505.16944</link>
<guid>https://arxiv.org/abs/2505.16944</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个用于评估具身场景下大型语言模型指令遵循能力的基准AgentIF。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了AgentIF，这是首个专门设计用来系统性评估大型语言模型（LLMs）在具身应用场景中指令遵循能力的基准。AgentIF具有三个显著特点：基于真实世界的应用构建，平均长度达1723词；复杂度高，每条指令平均包含11.9个约束条件；涵盖多样化的约束类型如工具规格和条件约束等。通过收集来自工业应用代理和开源系统的707个人工标注指令，研究者们对这些指令及其相关约束进行了详细的注释，并采用了多种评估方法进行测试。结果显示当前主流模型在此类任务上的表现普遍不佳，特别是在处理复杂的约束结构方面存在明显不足。此外，通过对指令长度及元约束等因素进行分析，揭示了一些现有模型存在的典型失效模式。本研究已公开代码与数据集，以促进后续研究工作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16944" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:31:10 GMT</pubDate>
</item>
<item>
<title>基于Itakura-Saito散度的风险规避强化学习</title>
<link>https://arxiv.org/abs/2505.16925</link>
<guid>https://arxiv.org/abs/2505.16925</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的数值稳定损失函数用于风险规避强化学习。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了风险规避强化学习在高风险领域的应用，其核心在于通过指数效用函数构建Bellman方程，并对传统强化学习算法进行少量修改。然而，这些方法存在数值不稳定性问题，因为需要在整个过程中计算指数值。为了解决这一问题，我们引入了一种基于Itakura-Saito散度的数值稳定且数学严谨的损失函数，用于学习状态值和动作值函数。我们通过理论分析和实证研究验证了该损失函数的表现，并在多个金融场景中展示了其优越性，部分场景具有已知解析解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16925" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:18:07 GMT</pubDate>
</item>
<item>
<title>基于大语言模型个性化文学翻译的研究</title>
<link>https://arxiv.org/abs/2505.16612</link>
<guid>https://arxiv.org/abs/2505.16612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索低资源环境下个性化大语言模型翻译的方法。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于基于大语言模型（LLMs）的高质量机器翻译系统在处理隐式风格要求时的局限性，特别是在文学翻译这一具有挑战性的领域。我们研究了多种提示策略及推理阶段干预方法，以引导模型生成符合个人风格的翻译。此外，提出了一种对比框架，利用稀疏自动编码器提取的潜在概念来识别重要的个性化属性。实验结果显示，通过我们的方法可以实现强个性化的翻译，同时保持翻译质量。进一步分析表明，用于个性化处理的模型层在多示例提示和我们的方法下受到相似的影响，暗示二者可能存在类似的机制。关键词：个性化翻译，大语言模型，文学翻译。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 08:47:16 GMT</pubDate>
</item>
<item>
<title>VLM-R^3：结合视觉区域识别与推理的多模态大语言模型</title>
<link>https://arxiv.org/abs/2505.16192</link>
<guid>https://arxiv.org/abs/2505.16192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架VLM-R^3，提升多模态大语言模型处理复杂视觉推理任务的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VLM-R^3的新框架，它通过增强多模态大语言模型（MLLMs）的视觉区域识别和推理能力，解决复杂任务中需要动态聚焦和反复查看视觉区域的问题。VLM-R^3的核心是区域条件强化策略优化（R-GRPO），该方法通过奖励模型选择信息丰富的视觉区域、制定适当的变换操作并将相关视觉上下文整合到后续推理步骤中，从而实现对文本推理的精确视觉定位。为了启动这一策略，我们构建了一个精心策划的视觉语言交互解释（VLIR）语料库，提供逐级监督以指导区域选择和文本解释。实验表明，VLM-R^3在零样本和少量样本设置下显著提升了性能，在需要微妙空间推理或精细视觉线索提取的问题上表现尤为突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:50:13 GMT</pubDate>
</item>
<item>
<title>SafeKey：通过关键句激活提升大推理模型的安全泛化能力</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出SafeKey方法，通过激活关键句中的安全洞察来提升大推理模型对未知有害查询的泛化能力。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）通过显式推理的方式显著提升了复杂任务的表现，但同时也带来了严重的安全风险。尽管监督微调（SFT）等主流安全策略在一定程度上改善了模型的安全性，但这些模型在面对未见过的越狱提示时仍表现不佳。本研究深入分析了LRMs的生成过程，发现了一个被称为“aha时刻”的安全洞察，它能够在关键句中触发安全推理并引导模型给出安全响应。基于这一发现，我们提出了SafeKey方法，该方法包括两个互补的目标：(1) 双路径安全头，增强模型内部表示中的安全信号；(2) 查询掩码建模目标，提高模型对查询理解的关注度。实验表明，SafeKey方法在多个安全基准测试中显著提高了模型对越狱攻击和分布外有害提示的安全泛化能力，平均有害率降低了9.6%，同时保持了模型的一般能力。此外，我们的分析揭示了SafeKey如何通过重塑内部注意力机制和改进隐藏表示的质量来增强安全性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:46:03 GMT</pubDate>
</item>
<item>
<title>大型语言模型如何承认错误？错误重述行为的研究</title>
<link>https://arxiv.org/abs/2505.16170</link>
<guid>https://arxiv.org/abs/2505.16170</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示大型语言模型能够承认错误但频率较低，内部信念影响其重述能力。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在面对自身知识矛盾时是否能够承认错误的行为，定义此行为为“重述”。通过构建特定模型的数据集，我们发现尽管LLMs具备重述能力，但实际发生的频率较低。进一步研究表明，这种行为与模型内部信念紧密相关，即当模型认为答案正确时，即便错误也不易重述。实验表明，内部信念对重述行为有因果影响，且当模型不信任自身答案时会尝试验证并改变注意力分布。最后，通过简单的监督微调显著提升了模型的重述性能。本研究代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16170" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:16:00 GMT</pubDate>
</item>
<item>
<title>现代日期处理在语言模型中的挑战与改进</title>
<link>https://arxiv.org/abs/2505.16088</link>
<guid>https://arxiv.org/abs/2505.16088</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，现代分词器会破坏日期结构，影响时间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现代基于BPE的分词器在处理日期时的问题，例如将连续数字拆分为不连贯的部分（如20250312 → 202, 503, 12），这不仅增加了标记数量，还削弱了时间推理所需的整体结构。为此，我们引入了一个名为日期碎片化比率的新指标，用于衡量分词器对多字符日期组件的保真度，并发布了DateAugBench基准数据集，包含跨越三种时间推理任务的6500个示例。通过实验，我们发现大型语言模型中存在一种新兴的日期抽象机制，即自动拼接年、月、日片段进行时间推理。研究进一步表明，过度碎片化会导致罕见日期（如历史或未来日期）的准确性下降高达10个百分点，而更大规模的模型能更快完成这种抽象过程。此外，我们观察到语言模型的时间推理路径与人类解读方式不同（通常是从年份开始）。这些发现为提升时间推理性能提供了新的视角和方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16088" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 20:06:29 GMT</pubDate>
</item>
<item>
<title>基于拓扑优化的大型语言模型物理与空间推理能力评估数据集</title>
<link>https://arxiv.org/abs/2505.16048</link>
<guid>https://arxiv.org/abs/2505.16048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一个新数据集，用于评估大型语言模型在拓扑优化中的物理与空间推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个新的数据集，旨在评估大型语言模型（LLMs）在基于拓扑优化的物理与空间推理能力。该数据集通过提供二维边界、施加力和支撑等条件，要求LLMs推断出最优的材料分布。这些任务涵盖了从填补部分结构中的掩码区域到预测完整的材料分布，需要理解力的流动及在特定约束下的材料需求，而无需借助模拟工具或显式的物理模型。数据集强调对结构稳定性和空间组织的推理能力，适用于二维环境下的评估，为传统语言和逻辑基准测试提供了补充视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 18:00:20 GMT</pubDate>
</item>
<item>
<title>VideoGameQA-Bench：推动游戏开发质量保障自动化</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入VideoGameQA-Bench基准以评估视觉语言模型在游戏测试中的性能。</p><br /><br /><p><strong>摘要：</strong> 随着视频游戏成为娱乐行业收入最高的领域，优化开发工作流变得至关重要。尽管视觉-语言模型（VLMs）在提升游戏开发效率方面展现出巨大潜力，但目前的游戏质量保障（QA）仍面临高度劳动密集且自动化不足的问题。现有基准难以满足游戏QA的具体需求，为此，我们推出了VideoGameQA-Bench，该基准涵盖视觉单元测试、回归测试、异常检测等多类游戏QA任务。这一工具旨在标准化评估VLMs在实际场景中的表现，从而促进游戏开发的自动化进程。代码与数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:08:38 GMT</pubDate>
</item>
<item>
<title>引入视觉信息的强化学习推理模型GRIT</title>
<link>https://arxiv.org/abs/2505.15879</link>
<guid>https://arxiv.org/abs/2505.15879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合图像和文本的视觉推理新方法GRIT。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Grounded Reasoning with Images and Texts (GRIT)的新方法，用于训练多模态大语言模型(MLLMs)进行基于图像的推理。GRIT通过引入一种结合自然语言和显式边界框坐标的推理范式，增强了模型对视觉信息的整合能力，使其能够生成清晰且视觉上可靠的推理链。此外，GRIT采用了一种基于强化学习的方法GRPO-GR，该方法专注于最终答案的准确性和推理输出的格式，从而避免了对带推理链注释的数据或显式边界框标签的需求。实验表明，GRIT在数据效率方面表现优异，仅需少量现有数据集中的图像-问题-答案三元组即可实现高效的训练，显著提升了模型生成连贯且视觉上可靠推理链的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:54:49 GMT</pubDate>
</item>
<item>
<title>基于机器人轨迹数据增强视觉语言模型的VQA数据集生成框架</title>
<link>https://arxiv.org/abs/2505.15517</link>
<guid>https://arxiv.org/abs/2505.15517</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用机器人轨迹数据生成用于视觉问答的VQA数据集，提升视觉语言模型的空间与交互推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Robo2VLM的视觉问答(VQA)数据集生成框架，该框架通过真实多模态机器人轨迹数据增强和评估视觉语言模型(VLMs)。Robo2VLM从机器人轨迹中非视觉和非描述性传感器模态（如末端执行器位置、夹爪开度和力传感）中推导出真值，并将其分割成一系列操作阶段。在每个阶段，它利用场景和交互理解识别机器人、任务目标和目标物体的三维属性，进而生成基于空间、目标条件和交互推理的问题模板的代表性VQA查询。最终，我们构建了Robo2VLM-1，这是一个包含684,710个问题的大规模野外数据集，覆盖463个独特场景和来自176,000个真实机器人轨迹的3,396个机器人操作任务。实验结果表明，Robo2VLM-1可以作为基准来衡量和改进VLMs在空间和交互推理方面的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15517" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 09:42:52 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型中光学字符识别头的功能解析</title>
<link>https://arxiv.org/abs/2505.15865</link>
<guid>https://arxiv.org/abs/2505.15865</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型视觉语言模型中光学字符识别头的独特功能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型视觉语言模型（LVLMs）中的不同头部结构，特别聚焦于负责从图像中识别文本的光学字符识别头（OCR Head）。研究发现，OCR头部相较于传统检索头部表现出了激活模式不稀疏、特性显著不同及激活频率与OCR评分紧密关联等特点。通过下游任务验证，包括链式思维（CoT）应用与头部掩蔽实验，进一步确认了这些发现的有效性。此外，重新分配OCR头部内的sink-token值可提升模型性能，从而加深了对LVLMs处理嵌入图像文本机制的理解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15865" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 06:53:41 GMT</pubDate>
</item>
<item>
<title>通过填补思维跳跃提升大规模语言模型的数学推理能力</title>
<link>https://arxiv.org/abs/2505.14684</link>
<guid>https://arxiv.org/abs/2505.14684</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CoT Thought Leap Bridge Task解决数学推理中的思维跳跃问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有数学Chain-of-Thought（CoT）数据集中存在的思维跳跃问题，提出了一项名为CoT Thought Leap Bridge Task的任务，旨在自动检测并生成缺失的中间推理步骤，从而恢复CoT的完整性和连贯性。为此，我们基于ScaleQuestMath构建了一个专门的训练数据集ScaleQM+，并训练了CoT-Bridge模型来填补这些跳跃。实验表明，微调后的模型在NuminaMath等数学推理基准测试中表现显著优于原始模型，提升了多达5.87%。此外，该方法不仅增强了蒸馏数据的效果，还为强化学习提供了更好的起点，并且在跨领域逻辑推理任务中也表现出更强的泛化能力。这一成果证明了提升推理完整性具有广泛的应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14684" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>Self-Braking Tuning:缓解大模型过推理问题的新框架</title>
<link>https://arxiv.org/abs/2505.14604</link>
<guid>https://arxiv.org/abs/2505.14604</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为Self-Braking Tuning的框架，解决大模型推理冗余问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型（LRMs）在提升推理能力过程中产生的冗余推理问题，提出了一种名为Self-Braking Tuning（SBT）的新框架。该框架通过让模型自行调控推理过程，避免依赖外部干预机制。研究构建了基于标准答案的过推理识别指标，并设计了一种系统性方法来检测不必要的推理步骤，从而生成训练信号以学习自我调节行为。此外，还开发了一种自适应推理长度的数据构造策略和创新的刹车提示机制，使模型能够在适当阶段自然终止推理。实验表明，该方法在数学基准测试中可减少高达60%的令牌消耗，同时保持与不受约束模型相当的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14604" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:53:40 GMT</pubDate>
</item>
<item>
<title>MUG-Eval：一种评估多语言大型语言模型生成能力的新框架</title>
<link>https://arxiv.org/abs/2505.14395</link>
<guid>https://arxiv.org/abs/2505.14395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架MUG-Eval，用于评估多语言大型语言模型的生成能力。</p><br /><br /><p><strong>摘要：</strong> 评估大型语言模型（LLMs）的文本生成能力具有挑战性，尤其是在资源匮乏的语言中。本文提出了一种名为MUG-Eval的新框架，该框架通过将现有基准转换为会话任务，并测量模型在这些任务上的准确性来评估LLMs的多语言生成能力。为了确保有效性，设计的任务需要目标语言中的有效交流。通过任务成功率作为成功对话生成的代理指标，该方法避免了对特定语言NLP工具或标注数据集的依赖，也不依赖于LLMs作为评判者，从而在多种语言和模型间提供标准化比较。研究发现，MUG-Eval与已建立的基准高度相关（r > 0.75），并且可以在数千种语言中扩展。这项工作为多语言生成能力评估提供了稳健且高效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:14:00 GMT</pubDate>
</item>
<item>
<title>SophiaVL-R1：强化多模态大语言模型推理能力的新方法</title>
<link>https://arxiv.org/abs/2505.17018</link>
<guid>https://arxiv.org/abs/2505.17018</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入思考过程奖励信号，提升多模态大语言模型的推理能力和泛化性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为SophiaVL-R1的方法，旨在通过规则驱动的强化学习范式改进多模态大型语言模型（MLLMs）的推理能力。传统方法往往缺乏对推理过程的监督，可能导致模型学习到次优策略，从而影响其泛化能力。为解决这一问题，我们首先训练了一个评估推理过程质量的奖励模型，并设计了Trust-GRPO方法，通过计算信任权重来缓解潜在不可靠的思考奖励的影响。此外，还引入了一种退火训练策略，逐渐减少对思考奖励的依赖，使模型更多地依靠精确的规则导向结果奖励。实验表明，SophiaVL-R1在多个基准测试中表现出色，尤其是在MathVisita和MMMU等任务上，甚至超越了参数量大10倍的竞争对手模型。所有代码、模型和数据集均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17018" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>多模态大语言模型的空间感知能力评估</title>
<link>https://arxiv.org/abs/2505.17012</link>
<guid>https://arxiv.org/abs/2505.17012</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究多模态大语言模型是否具备三维空间感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了现有多模态大语言模型（MLLMs）是否具有三维空间感知和理解能力。为此，我们引入了VGBench基准测试，专门用于评估MLLMs在视觉几何感知方面的能力，例如相机姿态和运动估计。此外，我们提出了SpatialScore，这是迄今为止最全面和多样化的多模态空间理解基准，整合了VGBench与其他11个现有数据集的相关数据。该基准测试包含28K个样本，涵盖多种空间理解任务、模态和问答格式，并设有一个精心策划的挑战子集SpatialScore-Hard。我们还开发了SpatialAgent，这是一种新型的多智能体系统，集成了9种专门用于空间理解的工具，支持Plan-Execute和ReAct推理范式。通过广泛的评估，我们揭示了空间推理中存在的持续挑战，并展示了SpatialAgent的有效性。我们认为，SpatialScore将为MLLMs的进一步发展提供有价值的见解，并作为严格的基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.17012" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding</title>
<link>https://arxiv.org/abs/2505.16990</link>
<guid>https://arxiv.org/abs/2505.16990</guid>
<content:encoded><![CDATA[
In this work, we propose Dimple, the first Discrete Diffusion Multimodal Large Language Model (DMLLM). We observe that training with a purely discrete diffusion approach leads to significant training instability, suboptimal performance, and severe length bias issues. To address these challenges, we design a novel training paradigm that combines an initial autoregressive phase with a subsequent diffusion phase. This approach yields the Dimple-7B model, trained on the same dataset and using a similar training pipeline as LLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%, demonstrating that DMLLM can achieve performance comparable to that of autoregressive models. To improve inference efficiency, we propose a decoding strategy termed confident decoding, which dynamically adjusts the number of tokens generated at each step, significantly reducing the number of generation iterations. In autoregressive models, the number of forward iterations during generation equals the response length. With confident decoding, however, the number of iterations needed by Dimple is even only text{response length}{3}. We also re-implement the prefilling technique in autoregressive models and demonstrate that it does not significantly impact performance on most benchmark evaluations, while offering a speedup of 1.5x to 7x. Additionally, we explore Dimple's capability to precisely control its response using structure priors. These priors enable structured responses in a manner distinct from instruction-based or chain-of-thought prompting, and allow fine-grained control over response format and length, which is difficult to achieve in autoregressive models. Overall, this work validates the feasibility and advantages of DMLLM and enhances its inference efficiency and controllability. Code and models are available at https://github.com/yu-rp/Dimple.
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:55:04 GMT</pubDate>
</item>
<item>
<title>NovelSeek：人工智能驱动的跨领域自主科学研究框架</title>
<link>https://arxiv.org/abs/2505.16938</link>
<guid>https://arxiv.org/abs/2505.16938</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NovelSeek利用多智能体框架加速科学创新，显著提升研究效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为NovelSeek的统一闭环多智能体框架，用于在多个科研领域开展自主科学研究（ASR），其主要优势体现在可扩展性、交互性和高效性。通过在反应产率预测、增强子活性预测及二维语义分割等任务中的应用，NovelSeek展示了其快速生成创新想法的能力，显著提升了基线模型的表现。例如，在反应产率预测任务中，仅需12小时便将准确率从27.6%提升至35.4%；在增强子活性预测中，4小时内精度从0.52提高到0.79；而二维语义分割任务中，30小时内精确度从78.8%升至81.0%。此外，NovelSeek还支持人类专家反馈和多智能体交互，实现了自动化端到端过程的无缝整合。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16938" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:27:43 GMT</pubDate>
</item>
<item>
<title>LLaDA-V：一种基于扩散模型的多模态大型语言模型</title>
<link>https://arxiv.org/abs/2505.16933</link>
<guid>https://arxiv.org/abs/2505.16933</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaDA-V结合视觉指令微调与掩码扩散模型，展示了出色的多模态性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LLaDA-V的新模型，该模型是一种纯粹基于扩散的多模态大型语言模型（MLLM），通过将视觉指令微调与掩码扩散模型相结合，突破了当前多模态方法中主导的自回归范式。LLaDA-V建立在代表性语言扩散模型LLaDA的基础上，集成了视觉编码器和MLP连接器，将视觉特征投影到语言嵌入空间，实现了有效的多模态对齐。实验结果显示，尽管LLaDA-V的语言模型在纯文本任务上的表现不如LLaMA3-8B和Qwen2-7B等模型强大，但在相同指令数据训练下，它在多模态任务中的表现与LLaMA3-V相当，并且具有更好的数据扩展性，缩小了与Qwen2-VL的性能差距。此外，在多模态理解方面，LLaDA-V相比现有的混合自回归-扩散和纯粹扩散的MLLM达到了最先进的性能。这些发现表明大型语言扩散模型在多模态环境中具有潜力，值得未来进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16933" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:23:26 GMT</pubDate>
</item>
<item>
<title>Believe Your Eyes: 通过注意力熵模式防御多模态大语言模型后门攻击</title>
<link>https://arxiv.org/abs/2505.16916</link>
<guid>https://arxiv.org/abs/2505.16916</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BYE框架，利用注意力熵模式过滤后门样本，有效抵御多模态大语言模型的后门威胁。</p><br /><br /><p><strong>摘要：</strong> 随着多模态大语言模型（MLLMs）在微调即服务（FTaaS）中的应用日益广泛，恶意微调可能植入后门的风险随之增加。本文发现后门触发器会导致跨模态处理中异常的注意力集中于非语义区域，这种现象被称为注意力崩溃。基于此，我们提出了BYE框架，它通过注意力图的熵模式作为自监督信号，无需干净标注或模型修改即可识别并过滤后门样本。BYE采用三阶段流水线操作：首先提取微调模型的注意力图，接着通过双模态分离计算熵分数并分析敏感层，最后进行无监督聚类以移除可疑样本。实验表明，BYE在多种数据集、模型及触发器类型上均表现出色，攻击成功率接近零，同时保持了目标任务的性能，提供了一种鲁棒且通用的解决方案来应对MLLMs中的后门威胁。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16916" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 13:11:58 GMT</pubDate>
</item>
<item>
<title>Jenga：通过动态注意力裁剪和渐进分辨率生成提升视频扩散模型推理效率</title>
<link>https://arxiv.org/abs/2505.16864</link>
<guid>https://arxiv.org/abs/2505.16864</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Jenga方法，结合动态注意力裁剪和渐进分辨率生成，显著加速视频扩散模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频Diffusion Transformer模型在实际部署中的计算资源需求问题，提出了名为Jenga的新型推理管道。该方法通过引入块状注意力机制和渐进分辨率策略，有效解决了传统扩散模型中存在的自注意力计算复杂度高以及多步推理效率低的问题。实验表明，Jenga能够在多个最先进的视频扩散模型上实现高达8.83倍的速度提升，同时仅损失0.01%的性能。作为一种即插即用的解决方案，Jenga使得现代硬件上的高质量视频生成成为可能，将推理时间从数分钟缩短至秒级，且无需重新训练模型。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16864" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:21:32 GMT</pubDate>
</item>
<item>
<title>通过两阶段训练策略实现视觉语言模型的人类化推理模式</title>
<link>https://arxiv.org/abs/2505.16854</link>
<guid>https://arxiv.org/abs/2505.16854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合监督微调与强化学习的两阶段训练方法，显著减少推理步骤。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TON的两阶段训练策略，旨在使视觉语言模型在推理必要性上模仿人类思维过程。第一阶段通过引入“思想丢弃”操作实现有选择性的推理，而第二阶段则利用强化学习优化推理决策。实验表明，相比传统方法，TON可将推理长度减少高达90%，同时保持甚至提升性能。该方法在多个视觉语言任务中的表现证明了其有效性和广泛适用性，为强化学习中的推理模式研究提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:13:29 GMT</pubDate>
</item>
<item>
<title>LaViDa：基于离散扩散模型的多模态视觉语言模型</title>
<link>https://arxiv.org/abs/2505.16839</link>
<guid>https://arxiv.org/abs/2505.16839</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LaViDa通过结合离散扩散模型和视觉编码器，提升多模态任务中的推理速度和可控性。</p><br /><br /><p><strong>摘要：</strong> 现代视觉-语言模型（VLMs）在多种需要视觉推理的任务中表现出色，但在实际应用中，如快速推理和可控生成方面仍面临挑战。现有的自回归（AR）VLMs如LLaVA在这些方面表现不佳。离散扩散模型（DMs）作为一种替代方案，提供了并行解码能力和双向上下文支持，但其在多模态任务中的潜力尚未被充分挖掘。本文介绍了一种名为LaViDa的新一代VLM家族，它通过在DMs中引入视觉编码器，并联合微调整个系统来实现多模态指令跟随。为了克服训练中的难点，LaViDa采用了互补掩码、前缀KV缓存和时间步偏移等创新技术。实验结果显示，LaViDa在多模态基准测试中表现优异，特别是在COCO图像描述任务上，其性能比Open-LLaVa-Next-8B提升了4.1分CIDEr，同时推理速度提高了1.92倍。此外，在双向任务中，LaViDa在约束诗续写任务上的表现提升了59%。这些成果表明LaViDa是一种有竞争力的AR VLM替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16839" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 12:07:12 GMT</pubDate>
</item>
<item>
<title>KRIS-Bench：基于知识推理的图像编辑系统评估基准</title>
<link>https://arxiv.org/abs/2505.16707</link>
<guid>https://arxiv.org/abs/2505.16707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出KRIS-Bench基准，用于评估多模态生成模型的知识推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态生成模型在指令驱动的图像编辑领域取得了显著进展，但其在知识推理编辑任务中的表现尚待深入探索。本文介绍了一个名为KRIS-Bench的新基准，该基准通过认知视角设计，将图像编辑任务划分为事实性、概念性和程序性三种基础知识类型，并涵盖七个推理维度。基准包含1,267个高质量标注的编辑实例，并提出了一个综合评估协议，包括引入知识合理性新指标并结合人类研究进行校准。对十种最先进的模型进行的实验显示了显著的推理性能差距，强调了开发知识中心化基准的重要性，以推动智能图像编辑系统的进步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 10:08:59 GMT</pubDate>
</item>
<item>
<title>Tool-Star：基于强化学习的大语言模型多工具协作推理框架</title>
<link>https://arxiv.org/abs/2505.16410</link>
<guid>https://arxiv.org/abs/2505.16410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tool-Star框架，通过强化学习使大语言模型自主调用多个外部工具进行推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Tool-Star的强化学习框架，旨在提升大型语言模型（LLMs）在逐步推理过程中自主调用多种外部工具的能力。该框架集成了六类工具，并在数据合成与训练中采用系统性设计。为解决工具使用数据稀缺问题，提出了结合工具集成提示与基于提示采样的通用工具集成推理数据合成管道。此外，还设计了两阶段训练方法：冷启动微调引导模型探索推理模式，多工具自批评强化学习算法优化奖励理解并促进工具协作。实验分析显示，Tool-Star在超过10个具有挑战性的推理基准测试中表现出色。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 05:00:19 GMT</pubDate>
</item>
<item>
<title>强化学习显著提升中小规模模型推理能力</title>
<link>https://arxiv.org/abs/2505.16400</link>
<guid>https://arxiv.org/abs/2505.16400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示强化学习可大幅增强中小模型推理能力，超越现有最佳蒸馏模型。</p><br /><br /><p><strong>摘要：</strong> 尽管大规模强化学习（RL）在推理方面取得进展，但构建高性能推理模型的具体训练方法仍不明确。本文通过系统性研究发现，对强大小型及中型模型进行大规模RL训练可以显著提高推理能力，优于基于蒸馏的方法。我们提出一种简单有效的策略：先用数学提示训练，再用代码提示训练。实验表明，仅数学RL不仅提升了数学基准测试的表现（如AIME 2025上提升14.6%/17.2%），也改善了代码推理任务的结果（如LiveCodeBench上提升6.8%/5.8%）。此外，扩展的代码RL迭代进一步提升了代码基准测试表现，同时对数学结果影响甚微。我们开发了稳健的数据整理管道，并揭示了逐步增加响应长度的课程学习及参数更新稳定性等关键见解。最终，RL不仅激发了预训练和监督微调获得的基础推理能力，还推动了模型推理极限，使其解决之前无法解决的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 22 May 2025 04:50:47 GMT</pubDate>
</item>
<item>
<title>Understanding Generative AI Capabilities in Everyday Image Editing Tasks</title>
<link>https://arxiv.org/abs/2505.16181</link>
<guid>https://arxiv.org/abs/2505.16181</guid>
<content:encoded><![CDATA[
Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on March 25, 2025. However, what subjects do people most often want edited? What kinds of editing actions do they want to perform (e.g., removing or stylizing the subject)? Do people prefer precise edits with predictable outcomes or highly creative ones? By understanding the characteristics of real-world requests and the corresponding edits made by freelance photo-editing wizards, can we draw lessons for improving AI-based editors and determine which types of requests can currently be handled successfully by AI editors? In this paper, we present a unique study addressing these questions by analyzing 83k requests from the past 12 years (2013-2025) on the Reddit community, which collected 305k PSR-wizard edits. According to human ratings, approximately only 33% of requests can be fulfilled by the best AI editors (including GPT-4o, Gemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on low-creativity requests that require precise editing than on more open-ended tasks. They often struggle to preserve the identity of people and animals, and frequently make non-requested touch-ups. On the other side of the table, VLM judges (e.g., o1) perform differently from human judges and may prefer AI edits more than human edits. Code and qualitative examples are available at: https://psrdataset.github.io
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:35:15 GMT</pubDate>
</item>
<item>
<title>QuickVideo：加速长视频理解的系统-算法协同设计</title>
<link>https://arxiv.org/abs/2505.16175</link>
<guid>https://arxiv.org/abs/2505.16175</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuickVideo系统，解决长视频解码和预填充瓶颈问题。</p><br /><br /><p><strong>摘要：</strong> 长视频理解在实际应用中至关重要，但传统方法因解码耗时及预填充开销导致计算效率低下。为应对这一挑战，本文提出QuickVideo，通过并行化CPU解码器（QuickDecoder）、高效内存预填充方法（QuickPrefill）以及CPU解码与GPU推理重叠方案，大幅降低长视频处理时间，实现实时应用支持。实验表明，该方法适用于多种时长和采样率，具有广泛适用性，即使在有限硬件上也能提供高质量视频理解能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16175" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 23:26:50 GMT</pubDate>
</item>
<item>
<title>无需微调的多模态大模型推理能力增强方法</title>
<link>https://arxiv.org/abs/2505.16151</link>
<guid>https://arxiv.org/abs/2505.16151</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的多模态大模型推理增强框架FRANK。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FRANK的训练-Free ANd r1-like多模态大型语言模型（MLLM），该模型通过解耦多模态大模型解码器层中的感知与推理，赋予现成的多模态模型推理和反思能力，而无需梯度更新或额外监督。研究的关键洞察是观察到较浅层解码器更多关注视觉标记，而深层解码器则集中处理文本语义。基于此，提出了分层泰勒推导闭式融合机制，将视觉预训练模型与专门推理的LLM结合，从而在深层解码器中融入推理能力的同时保持浅层解码器的视觉接地。实验结果显示，在具有挑战性的多模态推理基准MMMU上，FRANK-38B模型取得了69.2%的准确率，比最强基线模型InternVL2.5-38B高出5.3个百分点，并超过了闭源的GPT-4o模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.16151" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 22:51:12 GMT</pubDate>
</item>
<item>
<title>引入像素空间推理提升视觉语言模型性能</title>
<link>https://arxiv.org/abs/2505.15966</link>
<guid>https://arxiv.org/abs/2505.15966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过在像素空间中引入视觉推理操作，显著提高了视觉语言模型在多种视觉推理任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 链式思维推理极大地提升了大型语言模型在多领域的性能，但传统上仅限于文本空间，难以有效应对视觉密集型任务。本文提出了一种新颖的像素空间推理框架，使视觉语言模型具备直接从视觉证据中推断的能力，例如通过放大和选择帧等操作。为解决模型初始能力不平衡及对新操作的抗拒问题，我们采用了双阶段训练方法：第一阶段利用合成推理轨迹进行指令微调；第二阶段采用基于好奇心的奖励机制进行强化学习，平衡像素空间与文本空间推理的探索。实验表明，该方法显著提升了模型在多个视觉推理基准上的表现，其中7B参数规模的模型在V*基准测试中达到84%，TallyQA-Complex达到74%，InfographicsVQA达到84%，创开源模型最高记录。这些成果突显了像素空间推理的重要性及其框架的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:35:08 GMT</pubDate>
</item>
<item>
<title>基于在线对比学习的视觉语言模型幻觉抑制方法</title>
<link>https://arxiv.org/abs/2505.15963</link>
<guid>https://arxiv.org/abs/2505.15963</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过动态生成对比训练数据减少视觉语言模型幻觉的新框架。</p><br /><br /><p><strong>摘要：</strong> 现有大型视觉语言模型（LVLMs）在生成内容时易出现与视觉输入不一致的幻觉问题。尽管近期方法通过多模态直接偏好优化（DPO）有所改进，但依赖的负样本往往不能真实反映模型错误，导致训练效果受限。本研究提出在线视觉语言偏好学习（OViP）框架，利用模型自身生成的幻觉输出动态构建对比训练数据。通过采样响应对的语义差异并结合扩散模型合成负样本图像，OViP实时提供更相关的监督信号，实现文本和视觉偏好的自适应对齐。此外，改进了现有评估协议以更好地平衡幻觉抑制与表达能力。实验表明，OViP在减少幻觉的同时保持了模型的核心多模态能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15963" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 15:26:09 GMT</pubDate>
</item>
<item>
<title>最大更新参数化在扩散Transformer中的扩展及其在视觉生成模型中的应用</title>
<link>https://arxiv.org/abs/2505.15270</link>
<guid>https://arxiv.org/abs/2505.15270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究证明最大更新参数化可有效应用于扩散Transformer，大幅降低超参数调优成本。</p><br /><br /><p><strong>摘要：</strong> 近年来，最大更新参数化(muP)被提出用于标准Transformer，显著降低了语言模型从小型到大型迁移超参数的成本。然而，扩散Transformer由于架构和目标上的差异，muP的有效性尚不明确。本研究首次将muP推广至扩散Transformer，并通过大规模实验验证其有效性。研究证明主流扩散Transformer（如DiT、U-ViT等）的muP与标准Transformer一致，使现有muP方法可以直接应用。实验表明，基于muP的DiT-XL-2仅需原版1/2.9的时间即可收敛。此外，在文本转图像生成任务中，通过muP优化PixArt-alpha和MMDiT，模型性能超越基线模型，同时大幅减少了调优开销。这些成果确立了muP作为高效扩展扩散Transformer的框架地位。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 04:49:03 GMT</pubDate>
</item>
<item>
<title>MathIF：评估数学推理任务中指令跟随能力的基准</title>
<link>https://arxiv.org/abs/2505.14810</link>
<guid>https://arxiv.org/abs/2505.14810</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大语言模型在数学推理中有效推理与指令跟随之间的权衡。</p><br /><br /><p><strong>摘要：</strong> 本研究引入了MathIF基准，用于评估大型语言模型在数学推理任务中的指令跟随能力。实验发现，虽然提高推理能力的模型在复杂数学问题上表现优异，但其对自然语言指令的遵守程度却较低，尤其是在生成较长内容时。通过分析，我们观察到经过精馏长链推理训练或采用推理导向强化学习的模型，在指令遵循方面表现下降。尽管简单的干预措施能在一定程度上恢复模型的服从性，但会牺牲部分推理性能。这些发现强调了当前大语言模型训练范式中存在的基本矛盾，并呼吁开发更具指令意识的推理模型。本研究代码和数据已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14810" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 14:18:01 GMT</pubDate>
</item>
<item>
<title>解决大语言模型强化学习中验证器假阴性问题的研究</title>
<link>https://arxiv.org/abs/2505.14625</link>
<guid>https://arxiv.org/abs/2505.14625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示验证器假阴性问题严重影响强化学习训练并提出改进方案。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过强化学习优化大型语言模型推理能力时面临的一个普遍问题——验证器假阴性现象，即验证器错误地拒绝了正确的模型输出。通过对Big-Math-RL-Verified数据集的深入分析发现，超过38%的模型生成响应存在此类问题。假阴性不仅剥夺了模型获得有益梯度信号的机会，还显著减缓了收敛速度。为应对这一挑战，我们提出了tinyV，一种基于轻量级语言模型的验证器，它能够动态识别潜在的假阴性并恢复有效响应，从而提供更精确的奖励估计。实验表明，在多个数学推理基准测试中，结合tinyV可以将通过率提高多达10%，并加速收敛过程。本研究强调了解决验证器假阴性问题的重要性，并提供了改善强化学习精调方法的实际路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:16:44 GMT</pubDate>
</item>
<item>
<title>强化学习对大语言模型参数更新的稀疏性研究</title>
<link>https://arxiv.org/abs/2505.11711</link>
<guid>https://arxiv.org/abs/2505.11711</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现强化学习仅更新小部分参数即可显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习在大型语言模型下游任务中的应用，揭示了一个有趣的现象：通过强化学习训练，仅需更新模型总参数的5%到30%，其余参数基本保持不变，即可实现显著的性能提升。这种现象被称为“强化学习诱导的参数更新稀疏性”。研究覆盖了7种常用强化学习算法及10种不同家族的大语言模型，并且该稀疏性是内在属性，无需显式的稀疏正则化或架构约束。进一步实验表明，单独微调被更新的子网络即可恢复测试准确性，并且得到的模型与全量微调的结果几乎相同。此外，来自不同随机种子、训练数据和强化学习算法的子网络之间显示出比偶然预期更高的重叠度。分析表明，这种稀疏性并非由于只更新某些特定层，而是几乎所有参数矩阵都接收到了稀疏更新，且这些更新几乎涵盖了参数矩阵所能表示的所有子空间。我们推测，这种稀疏性主要归因于训练数据接近策略分布，而KL正则化和梯度裁剪等技术的影响有限。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11711" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 17:42:28 GMT</pubDate>
</item>
<item>
<title>评估参考型奖励系统的VerifyBench基准测试</title>
<link>https://arxiv.org/abs/2505.15801</link>
<guid>https://arxiv.org/abs/2505.15801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出VerifyBench基准以评估参考型奖励系统性能。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型如OpenAI和DeepSeek-R1在推理领域表现优异，其训练的关键在于强化学习中的可验证奖励机制。然而，现有的奖励基准未能评估基于参考的奖励系统，导致研究人员对强化学习中使用的校验器准确性缺乏深入了解。本文引入了两个新基准——VerifyBench和VerifyBench-Hard，通过精心收集和整理数据，并进行细致的人工标注构建，旨在评估此类奖励系统的性能。当前模型在这两个基准上仍有较大改进空间，尤其是较小规模的模型。此外，我们还对评估结果进行了全面分析，为理解和发展基于参考的奖励系统提供了见解。所提出的基准为指导校验器准确性及通过强化学习训练的推理模型的推理能力发展提供了有效工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:54:43 GMT</pubDate>
</item>
<item>
<title>基于多臂老虎机的自适应推测解码框架</title>
<link>https://arxiv.org/abs/2505.15141</link>
<guid>https://arxiv.org/abs/2505.15141</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的在线学习框架，优化大语言模型推测解码性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需训练的在线学习框架BanditSpec，用于自适应调整大语言模型推测解码的超参数配置。该问题被形式化为一个多臂老虎机问题，设计并分析了UCBSpec和EXP3Spec两种算法，其停止时间遗憾在随机和对抗奖励设定下均得到上界估计。UCBSpec的遗憾性能接近最优。通过实验验证，该方法在多种输入提示场景下的吞吐量接近最佳固定配置。关键词：推测解码、多臂老虎机、大语言模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15141" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 01:56:31 GMT</pubDate>
</item>
<item>
<title>熵最小化提升大语言模型在复杂任务中的表现</title>
<link>https://arxiv.org/abs/2505.15134</link>
<guid>https://arxiv.org/abs/2505.15134</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">熵最小化无需标注数据即可显著提升大语言模型在数学、物理及编程任务上的性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了熵最小化(Entropy Minimization, EM)作为一种训练目标对大规模语言模型(Large Language Models, LLMs)在解决复杂任务时的效果。我们提出了三种方法：(1) EM-FT，类似于指令微调但作用于未标记的数据；(2) EM-RL，采用负熵作为唯一奖励的强化学习；(3) EM-INF，在推理阶段通过调整logits减少熵，无需训练数据或参数更新。实验表明，在Qwen-7B上，EM-RL仅基于未标注数据便能达到或超过使用60K标注数据训练的强基线模型如GRPO和RLOO的表现；而EM-INF使Qwen-32B在SciCode基准测试中达到甚至超越了GPT-4o、Claude 3 Opus和Gemini 1.5 Pro的性能，同时效率提升了3倍。研究揭示了许多预训练的LLMs具备未被充分挖掘的推理能力，仅通过熵最小化即可有效激发这些潜力，无需标注数据或参数更新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15134" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 01:39:11 GMT</pubDate>
</item>
<item>
<title>DiCo：基于标准卷积网络的高效扩散模型</title>
<link>https://arxiv.org/abs/2505.11196</link>
<guid>https://arxiv.org/abs/2505.11196</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DiCo，一种基于卷积网络的高效扩散模型，性能超越传统Transformer模型。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了卷积操作作为构建高效表达扩散模型的替代模块的可能性。虽然直接用卷积替换自注意力通常会导致性能下降，我们发现这是因为卷积神经网络比Transformer具有更高的通道冗余度。为此，我们引入了一种紧凑的通道注意力机制，提高了特征多样性。由此产生了DiCo，一种完全由标准卷积模块组成的扩散模型家族，在ImageNet上的表现优于现有扩散模型，同时显著提升了效率。例如，DiCo-XL在256x256分辨率下的FID得分为2.05，比DiT-XL/2快2.7倍；在512x512分辨率下FID为2.53，快3.1倍。此外，我们的最大模型DiCo-H达到1B参数规模，在ImageNet 256x256上的FID为1.90，且无需额外监督。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11196" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 08:54:04 GMT</pubDate>
</item>
<item>
<title>BLEUBERI: BLEU is a surprisingly effective reward for instruction following</title>
<link>https://arxiv.org/abs/2505.11080</link>
<guid>https://arxiv.org/abs/2505.11080</guid>
<content:encoded><![CDATA[
Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 06:11:43 GMT</pubDate>
</item>
<item>
<title>Llama-SMoP：一种高效多模态大型语言模型用于视听语音识别</title>
<link>https://arxiv.org/abs/2505.14336</link>
<guid>https://arxiv.org/abs/2505.14336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Llama-SMoP模型，通过稀疏混合投影模块提升多模态语音识别性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对视听语音识别(AVSR)领域中大型语言模型(LLMs)高计算成本的问题，提出了Llama-SMoP，这是一种利用稀疏混合投影(SMoP)模块的高效多模态LLM。该模型通过引入稀疏门控的混合专家(MoE)投影器，在不增加推理成本的前提下扩展模型容量。实验表明，采用模态特定路由和专家的DED R配置在ASR、VSR及AVSR任务上表现出色，消融研究进一步验证了其在专家激活、可扩展性和噪声鲁棒性方面的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:20:55 GMT</pubDate>
</item>
<item>
<title>Mixture-of-Thought框架：多模态推理提升逻辑推理性能</title>
<link>https://arxiv.org/abs/2505.15817</link>
<guid>https://arxiv.org/abs/2505.15817</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mixture-of-Thought框架，通过自然语言、代码和真理表三种模态协同推理，显著提升逻辑推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 现有基于大型语言模型（LLMs）的方法通常仅限于单一推理模态（如自然语言），限制了多模态间的协同效应。本文提出Mixture-of-Thought（MoT）框架，引入自然语言、代码和新提出的真理表三种互补模态，实现跨模态推理。MoT框架分为两个阶段：自我演化的MoT训练阶段，联合学习自动生成的多模态理由；以及MoT推理阶段，充分利用三模态协同效果优化预测。实验表明，MoT在FOLIO和ProofWriter等逻辑推理基准测试中显著优于单模态链式推理方法，平均准确率提升11.7个百分点。进一步分析显示，该框架对复杂逻辑问题尤其有效，各模态贡献互补优势，其中真理表推理有助于克服自然语言推理中的关键瓶颈。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15817" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>基于强化学习的会话查询重写框架ConvSearch-R1</title>
<link>https://arxiv.org/abs/2505.15776</link>
<guid>https://arxiv.org/abs/2505.15776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需外部监督的ConvSearch-R1框架提升会话查询重写性能。</p><br /><br /><p><strong>摘要：</strong> 现有会话查询重写(CQR)方法面临高依赖外部标注或大语言模型以及与下游检索器对齐不足的问题。本文介绍ConvSearch-R1，首个完全消除对外部重写监督依赖的自驱动框架，利用强化学习优化重写过程。该框架采用两阶段方法，首先通过检索引导的自我蒸馏解决冷启动问题，随后结合专门设计的排名激励奖励塑造机制解决传统检索指标的稀疏性问题。在TopiOCQA和QReCC数据集上的实验表明，ConvSearch-R1显著优于现有最先进方法，在TopiOCQA数据集上实现超过10%的性能提升，同时使用参数规模更小的3B模型且无需任何外部监督。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:27:42 GMT</pubDate>
</item>
<item>
<title>BiasLens：基于模型向量空间结构的大语言模型偏见分析框架</title>
<link>https://arxiv.org/abs/2505.15524</link>
<guid>https://arxiv.org/abs/2505.15524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BiasLens框架，无需标注数据即可高效检测大语言模型偏见。</p><br /><br /><p><strong>摘要：</strong> 现有大语言模型（LLMs）存在显著的偏见问题，影响了其可靠性和公平性。传统偏见评估方法依赖人工标注数据，耗时且覆盖概念有限。本文提出BiasLens框架，该框架基于模型向量空间结构，利用概念激活向量（CAVs）与稀疏自编码器（SAEs）提取可解释的概念表示，并通过衡量目标概念与参考概念间的表征相似性变化量化偏见。实验表明，BiasLens与传统偏见评估指标高度相关（Spearman相关系数r>0.85），同时揭示了现有方法难以发现的偏见形式，例如保险状态可能导致临床诊断偏见。BiasLens为偏见发现提供了可扩展、可解释且高效的解决方案，有助于提升LLMs的公平性与透明度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 09:50:23 GMT</pubDate>
</item>
<item>
<title>AJailBench：评估大型音频语言模型的越狱攻击漏洞</title>
<link>https://arxiv.org/abs/2505.15406</link>
<guid>https://arxiv.org/abs/2505.15406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次构建AJailBench基准，评估大型音频语言模型的越狱攻击漏洞。</p><br /><br /><p><strong>摘要：</strong> 随着大型音频语言模型(LAMs)的发展，其音频输出可能包含有害或不道德的内容，但现有研究缺乏对这些模型安全性的系统性定量评估，尤其是针对越狱攻击。本文引入AJailBench，这是首个专门用于评估LAMs越狱漏洞的基准。我们首先构建了AJailBench-Base数据集，包含1495个跨10类政策违规的对抗性音频提示，通过现实的文本转语音合成技术转换自文本越狱攻击。通过此数据集，我们测试了几种最先进的LAMs，发现它们在攻击面前缺乏一致性鲁棒性。为进一步加强越狱测试并模拟更真实的攻击条件，我们提出了一种生成动态对抗变体的方法。我们的音频扰动工具包(APT)在时间、频率和振幅域上应用目标扰动，并通过约束语义一致性并采用贝叶斯优化来搜索既微妙又高效的扰动，从而生成了AJailBench-APT扩展数据集。研究结果表明，即使是小幅度且语义保持的扰动，也能显著降低领先LAMs的安全性能，强调了需要更稳健和语义感知的防御机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:47:47 GMT</pubDate>
</item>
<item>
<title>自适应自我恢复推理框架提升大规模推理模型效率</title>
<link>https://arxiv.org/abs/2505.15400</link>
<guid>https://arxiv.org/abs/2505.15400</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出自适应自我恢复推理框架，大幅减少计算开销且性能损失微乎其微。</p><br /><br /><p><strong>摘要：</strong> 大规模推理模型通过长推理链实现优异表现，但在简单任务上因冗余推理导致高计算开销。本文量化了两种模式下这些模型的上限，并揭示了“内部自我恢复机制”，即模型在回答生成时隐式补充推理。基于此，我们提出了自适应自我恢复推理（ASRR）框架，该框架通过抑制不必要的推理并启用隐式恢复，根据问题难度动态分配推理资源，在多个基准测试和模型上的实验表明，相比GRPO，ASRR可将推理预算减少高达32.5%（1.5B）和25.7%（7B），同时仅轻微降低准确性（pass@1分别下降1.2%和0.6%），并在安全性基准测试中显著提高了无害率（最高增加21.7%）。ASRR展示了在高效、自适应及安全推理方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15400" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:41:39 GMT</pubDate>
</item>
<item>
<title>Tango: 一种同时训练语言模型生成器和验证器的强化学习框架</title>
<link>https://arxiv.org/abs/2505.15034</link>
<guid>https://arxiv.org/abs/2505.15034</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tango框架，通过强化学习同时训练语言模型生成器和验证器，显著提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Tango的新框架，该框架利用强化学习同时训练大型语言模型（LLMs）的生成器和验证器。传统方法中的验证器通常固定或通过监督微调训练，容易出现奖励黑客问题且泛化能力差。Tango的创新之处在于其采用生成式过程级LLM验证器，此验证器通过强化学习训练并与生成器协同进化，无需显式的过程级标注。实验表明，Tango的两个组件在多个基准测试中达到了当前最佳性能，在数学推理等难题上表现尤为突出。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15034" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 22:43:15 GMT</pubDate>
</item>
<item>
<title>WebNovelBench：评估大语言模型叙事能力的新基准</title>
<link>https://arxiv.org/abs/2505.14818</link>
<guid>https://arxiv.org/abs/2505.14818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WebNovelBench基准，用于评估大语言模型的长篇小说生成能力。</p><br /><br /><p><strong>摘要：</strong> 现有评估长篇叙事能力的大规模基准往往缺乏必要的规模、多样性和客观度量标准，因此我们开发了WebNovelBench这一新基准。该基准利用超过4000部中文网络小说的数据集，将评估设定为概要到故事生成的任务。通过多维度框架和LM作为评委的方法，自动评估生成的故事质量。实验表明，WebNovelBench能够有效区分人类创作的杰作、流行网络小说以及由大语言模型生成的内容。此外，该研究对24个最先进的大语言模型进行了综合分析，排名其叙事能力并提供了未来发展的见解。这项基准提供了一种可扩展、可复制且基于数据驱动的方法，用于评估和推进由大语言模型驱动的叙事生成技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 14:32:28 GMT</pubDate>
</item>
<item>
<title>Toto：基于观测数据的时间序列预测基础模型</title>
<link>https://arxiv.org/abs/2505.14766</link>
<guid>https://arxiv.org/abs/2505.14766</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Toto是一种具有1.51亿参数的解码器时间序列预测基础模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Toto，这是一个拥有151百万参数的时间序列预测基础模型，采用现代解码器-only架构，并结合了针对多变量可观察性时间序列数据特定挑战的创新设计。Toto的预训练语料库由可观测性数据、开放数据集和合成数据混合组成，规模是领先时间序列基础模型的4到10倍。同时，我们推出了BOOM，这是一个包含2807个真实世界时间序列的3.5亿观测值的大规模基准测试。Toto和BOOM的数据均来自Datadog的遥测技术和内部可观测性指标。广泛评估显示，Toto在BOOM及现有通用时间序列预测基准上表现出最先进的性能。Toto的模型权重、推理代码和评估脚本，以及BOOM的数据和评估代码，均已作为开源项目，在Apache 2.0许可下提供。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14766" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:48:13 GMT</pubDate>
</item>
<item>
<title>强化微调中的先验提示工程研究</title>
<link>https://arxiv.org/abs/2505.14157</link>
<guid>https://arxiv.org/abs/2505.14157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示先验提示工程可有效提升语言模型的行为表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化微调（RFT）背景下的先验提示工程（pPE），指出现有研究主要集中在算法、奖励塑造和数据整理上，而先验提示的设计尚未得到充分探索。受推理时提示工程（iPE）启发，我们翻译了五种典型的iPE策略（推理、规划、基于代码的推理、知识回忆和空例利用）为相应的pPE方法，并通过Qwen2.5-7B实验验证。结果显示，所有pPE训练的模型均优于其对应的iPE提示模型，其中空例pPE方法在AIME2024和GPQA-Diamond上的平均性能提升最大。此外，通过行为分类框架，我们发现不同的pPE策略会在模型中培养出独特的行为风格。这些结果表明先验提示工程在RFT中具有强大但未被充分研究的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 06:05:11 GMT</pubDate>
</item>
<item>
<title>基于知识图谱的多语言多跳幻觉评估基准MultiHal</title>
<link>https://arxiv.org/abs/2505.14101</link>
<guid>https://arxiv.org/abs/2505.14101</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识图谱的多语言多跳幻觉评估基准MultiHal。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）存在忠实性和事实性方面的固有限制，即所谓的幻觉现象。现有评估基准主要依赖英语为中心的数据集，并通过补充网络链接或文本片段进行事实性评估，而忽略了可用的结构化事实资源。鉴于此，研究者指出知识图谱（KGs）可有效减轻幻觉问题。本文填补了现有幻觉评估基准中知识图谱路径及多语言支持的空白，提出了名为MultiHal的知识图谱驱动的多语言多跳评估基准。通过从开放领域知识图谱中挖掘并筛选出高质量的25.9k知识图谱路径，MultiHal在多个语言和模型上展示了语义相似度评分的显著提升，相比传统问答系统提高了0.12至0.36分。这一基准有望推动未来基于图的幻觉缓解和事实核查任务的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14101" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:03:35 GMT</pubDate>
</item>
<item>
<title>HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation</title>
<link>https://arxiv.org/abs/2505.11454</link>
<guid>https://arxiv.org/abs/2505.11454</guid>
<content:encoded><![CDATA[
Large multimodal models (LMMs) now excel on many vision language benchmarks, however, they still struggle with human centered criteria such as fairness, ethics, empathy, and inclusivity, key to aligning with human values. We introduce HumaniBench, a holistic benchmark of 32K real-world image question pairs, annotated via a scalable GPT4o assisted pipeline and exhaustively verified by domain experts. HumaniBench evaluates seven Human Centered AI (HCAI) principles: fairness, ethics, understanding, reasoning, language inclusivity, empathy, and robustness, across seven diverse tasks, including open and closed ended visual question answering (VQA), multilingual QA, visual grounding, empathetic captioning, and robustness tests. Benchmarking 15 state of the art LMMs (open and closed source) reveals that proprietary models generally lead, though robustness and visual grounding remain weak points. Some open-source models also struggle to balance accuracy with adherence to human-aligned principles. HumaniBench is the first benchmark purpose built around HCAI principles. It provides a rigorous testbed for diagnosing alignment gaps and guiding LMMs toward behavior that is both accurate and socially responsible. Dataset, annotation prompts, and evaluation code are available at: https://vectorinstitute.github.io/HumaniBench
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:09:44 GMT</pubDate>
</item>
<item>
<title>Streamline Without Sacrifice - Squeeze out Computation Redundancy in LMM</title>
<link>https://arxiv.org/abs/2505.15816</link>
<guid>https://arxiv.org/abs/2505.15816</guid>
<content:encoded><![CDATA[
Large multimodal models excel in multimodal tasks but face significant computational challenges due to excessive computation on visual tokens. Unlike token reduction methods that focus on token-level redundancy, we identify and study the computation-level redundancy on vision tokens to ensure no information loss. Our key insight is that vision tokens from the pretrained vision encoder do not necessarily require all the heavy operations (e.g., self-attention, FFNs) in decoder-only LMMs and could be processed more lightly with proper designs. We designed a series of experiments to discover and progressively squeeze out the vision-related computation redundancy. Based on our findings, we propose ProxyV, a novel approach that utilizes proxy vision tokens to alleviate the computational burden on original vision tokens. ProxyV enhances efficiency without compromising performance and can even yield notable performance gains in scenarios with more moderate efficiency improvements. Furthermore, the flexibility of ProxyV is demonstrated through its combination with token reduction methods to boost efficiency further. The code will be made public at this https://github.com/penghao-wu/ProxyV URL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:52 GMT</pubDate>
</item>
<item>
<title>MMaDA：一种跨模态扩散基础模型的创新设计</title>
<link>https://arxiv.org/abs/2505.15809</link>
<guid>https://arxiv.org/abs/2505.15809</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型跨模态扩散基础模型MMaDA，在多领域表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了MMaDA，这是一种全新的跨模态扩散基础模型，旨在实现文本推理、多模态理解和文本到图像生成等领域的卓越性能。MMaDA通过三个关键创新实现这一目标：首先，采用统一的扩散架构，具有共享的概率公式和模态无关的设计，无需特定模态组件，从而实现不同类型数据的无缝集成；其次，实施混合长链式推理微调策略，统一多种模态的推理格式，通过文本和视觉域之间推理过程的对齐，提升模型处理复杂任务的能力；最后，提出UniGRPO算法，基于策略梯度的强化学习方法，用于扩散基础模型，统一推理和生成任务的后训练过程。实验结果显示，MMaDA-8B在文本推理、多模态理解和文本到图像生成方面均表现出色，超越了多个强大的现有模型。这些成果展示了MMaDA在统一扩散架构内弥合预训练与后训练差距的有效性，为未来研究提供了全面框架。代码和训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15809" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>VARD：基于价值函数的强化扩散模型优化方法</title>
<link>https://arxiv.org/abs/2505.15791</link>
<guid>https://arxiv.org/abs/2505.15791</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合价值函数和KL正则化的新型强化扩散模型训练方法。</p><br /><br /><p><strong>摘要：</strong> 扩散模型在多个领域作为强大的生成工具崭露头角，但如何让预训练模型表现出特定的期望属性仍具挑战性。尽管强化学习提供了潜在解决方案，但现有方法难以同时实现稳定高效的微调并支持非可微奖励。本文提出VARD（Value-based Reinforced Diffusion），通过先学习中间状态奖励期望的价值函数，再利用该函数结合KL正则化在整个生成过程中提供密集监督，解决了现有方法的不足。实验表明，该方法不仅有效提升了轨迹引导能力，还提高了训练效率，并扩展了强化学习在复杂非可微奖励函数优化中的适用范围。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15791" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:44:37 GMT</pubDate>
</item>
<item>
<title>基于延迟键值缓存机制的扩散语言模型加速方法</title>
<link>https://arxiv.org/abs/2505.15781</link>
<guid>https://arxiv.org/abs/2505.15781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种延迟键值缓存机制，显著提升扩散语言模型推理速度。</p><br /><br /><p><strong>摘要：</strong> 扩散语言模型(DLMs)近年来被视为自回归语言模型的强大竞争者，但其非自回归架构和双向注意力机制限制了其推理效率。本文通过引入延迟键值缓存(dKV-Cache)机制，解决了这一瓶颈问题。dKV-Cache机制基于扩散过程中不同标记具有不同表示动态的观察结果，设计了延迟且条件化的缓存策略。我们提出了两种互补变体：dKV-Cache-Decode几乎无损加速，甚至在长序列上提升了性能；dKV-Cache-Greedy通过更激进的缓存策略实现更高的加速比，但性能略有下降。最终，dKV-Cache实现了推理速度2至10倍的提升，显著缩小了自回归模型与扩散模型之间的差距。实验表明，该缓存机制适用于多种基准测试，包括通用语言理解、数学及代码生成任务，且无需重新训练现有DLMs即可实现缓存功能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15781" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:32:10 GMT</pubDate>
</item>
<item>
<title>互联网增强文本到图像生成框架解决不确定知识问题</title>
<link>https://arxiv.org/abs/2505.15779</link>
<guid>https://arxiv.org/abs/2505.15779</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种互联网增强的文本到图像生成框架解决知识不确定性问题。</p><br /><br /><p><strong>摘要：</strong> 当前文本到图像生成模型在处理不确定性知识时表现不佳，例如对未发布的电影角色设计风格难以生成合适图像。为了解决这一问题，本文提出了一种互联网增强的文本到图像生成（IA-T2I）框架，通过引入主动检索模块确定是否需要参考图像，层次化图像选择模块找到最合适的搜索结果，以及自我反思机制持续评估和优化生成图像。实验中构建了一个名为Img-Ref-T2I的数据集，并通过精心设计的提示指导GPT-4进行偏好评估，结果显示该框架在人类评估中比GPT-4高出约30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15779" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:31:49 GMT</pubDate>
</item>
<item>
<title>Soft Thinking: 模拟人类软推理突破离散语言推理瓶颈</title>
<link>https://arxiv.org/abs/2505.15778</link>
<guid>https://arxiv.org/abs/2505.15778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的Soft Thinking方法，通过连续概念空间生成抽象概念，提升推理模型表现。</p><br /><br /><p><strong>摘要：</strong> 当前推理模型受限于离散的语言嵌入表示，在语义空间中仅能探索有限路径。本文引入Soft Thinking，这是一种无需训练的方法，通过概率加权混合生成连续概念空间中的抽象概念，实现平滑过渡和丰富的表达能力，从而有效探索多种推理路径并收敛到正确答案。实验显示，Soft Thinking在数学和编码基准测试中提升了最高2.48个百分点的pass@1准确性，同时减少了高达22.4%的token使用量，且输出结果具有高度可解释性。这项研究突破了基于离散语言的推理瓶颈，展示了其在推理模型中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:29:15 GMT</pubDate>
</item>
<item>
<title>基于单张顶视图生成高质量3D场景的3DTown框架</title>
<link>https://arxiv.org/abs/2505.15765</link>
<guid>https://arxiv.org/abs/2505.15765</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的3DTown框架，从单张顶视图生成高质量3D场景。</p><br /><br /><p><strong>摘要：</strong> 现有获取详细3D场景的方法通常成本高昂且复杂，而本文介绍了一种轻量级替代方案——3DTown框架，该框架能够仅凭一张顶视图生成复杂的3D场景。尽管近期的3D生成模型在物体级别取得了显著成果，但在全场景生成时却面临几何不一致、布局幻觉及低质量网格等问题。为解决这些问题，3DTown采用区域生成与空间感知3D修复相结合的方法，将输入图像分解为重叠区域后分别生成，并通过掩码校正流修复缺失几何结构，从而确保全局一致性与高质量几何生成。实验表明，3DTown在多个场景下优于当前最先进的基准模型，证明了其在无需3D监督或微调的情况下实现高质量3D场景生成的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15765" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 13:10:47 GMT</pubDate>
</item>
<item>
<title>Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!</title>
<link>https://arxiv.org/abs/2505.15656</link>
<guid>https://arxiv.org/abs/2505.15656</guid>
<content:encoded><![CDATA[
Fine-tuning on open-source Large Language Models (LLMs) with proprietary data is now a standard practice for downstream developers to obtain task-specific LLMs. Surprisingly, we reveal a new and concerning risk along with the practice: the creator of the open-source LLMs can later extract the private downstream fine-tuning data through simple backdoor training, only requiring black-box access to the fine-tuned downstream model. Our comprehensive experiments, across 4 popularly used open-source models with 3B to 32B parameters and 2 downstream datasets, suggest that the extraction performance can be strikingly high: in practical settings, as much as 76.3% downstream fine-tuning data (queries) out of a total 5,000 samples can be perfectly extracted, and the success rate can increase to 94.9% in more ideal settings. We also explore a detection-based defense strategy but find it can be bypassed with improved attack. Overall, we highlight the emergency of this newly identified data breaching risk in fine-tuning, and we hope that more follow-up research could push the progress of addressing this concerning risk. The code and data used in our experiments are released at https://github.com/thu-coai/Backdoor-Data-Extraction.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 11:32:14 GMT</pubDate>
</item>
<item>
<title>基于奖励塑形的高效推理模型研究</title>
<link>https://arxiv.org/abs/2505.15612</link>
<guid>https://arxiv.org/abs/2505.15612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出LASER和LASER-D方法，显著提升大规模推理模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型（LRMs）通过强化学习解决复杂问题时展现出强大的能力，但其冗长的推理轨迹常伴随大量冗余，影响效率。本文提出了一种基于长度的奖励塑形统一框架，并设计了LASER方法，采用受目标长度控制的步函数作为奖励，实现性能与效率的帕累托最优平衡。进一步提出LASER-D方法，引入动态适应性和难度感知机制，优化快慢思维的结合。实验表明，该方法在DeepSeek-R1系列模型上显著提升了AIME2024成绩，同时减少63%的token使用量。此外，分析显示，所提出的压缩技术减少了冗余的自我反思，生成更简洁的推理模式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 11:03:26 GMT</pubDate>
</item>
<item>
<title>通过监督微调提升大规模推理模型的安全性研究</title>
<link>https://arxiv.org/abs/2505.15404</link>
<guid>https://arxiv.org/abs/2505.15404</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，直接蒸馏安全响应无法显著增强大规模推理模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 大规模推理模型（LRMs）在数学和编程等推理密集型任务中表现出色，但其安全性并未随之提高，甚至可能降低。本文通过监督微调（SFT）探索如何提升LRMs的安全性。研究发现，直接从DeepSeek-R1蒸馏安全响应未能显著改善安全性，并分析出三种导致失败的关键模式。通过在数据蒸馏过程中解决这些问题，可以实现显著的安全性提升。此外，研究发现简单的短推理过程或模板推理即可达到类似的安全性能，且更容易被模型学习。这些发现促使对推理在保障模型安全性中的作用进行深入反思。最终，我们发现混合数学推理数据有助于平衡安全性和过度拒绝问题。本研究旨在提供更全面的视角来增强LRMs的安全性。相关代码和数据已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15404" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 07:45:29 GMT</pubDate>
</item>
<item>
<title>Web-Shepherd：首个用于网页导航的进程奖励模型</title>
<link>https://arxiv.org/abs/2505.15277</link>
<guid>https://arxiv.org/abs/2505.15277</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个可评估网页导航轨迹的进程奖励模型Web-Shepherd。</p><br /><br /><p><strong>摘要：</strong> 网页导航作为独特领域，因其需要长期序列决策而极具挑战性。然而，专门用于训练和测试的奖励模型一直缺失。现有方法依赖多模态大型语言模型（MLLM），但面临速度和成本限制。本文提出首个过程奖励模型Web-Shepherd，通过逐步评估网页导航轨迹解决此问题。我们构建了包含4万组逐步偏好对和多样化标注清单的WebPRM Collection数据集，并引入WebRewardBench，首个元评估基准。实验表明，Web-Shepherd在WebRewardBench上比GPT-4o准确率高出约30分，在WebArena-lite测试中性能提升10.9分且成本降低10倍。我们的模型、数据集及代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15277" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 04:56:55 GMT</pubDate>
</item>
<item>
<title>基于知识图谱的信任推理框架Deliberation over Priors</title>
<link>https://arxiv.org/abs/2505.15210</link>
<guid>https://arxiv.org/abs/2505.15210</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合结构与约束先验的信任推理框架，提升大型语言模型生成的可靠性。</p><br /><br /><p><strong>摘要：</strong> 现有基于知识图谱的知识增强生成方法难以充分利用其中蕴含的先验知识，尤其是结构信息和显式/隐式约束。本文提出一种名为Deliberation over Priors (DP) 的信任推理框架，通过逐步知识蒸馏策略将结构先验整合到大型语言模型中，提高关系路径生成的忠实性；同时采用推理自省策略，依据提取的约束先验引导模型进行细化推理验证，确保响应生成的可靠性。实验表明，DP在三个基准数据集上达到新性能高度，在ComplexWebQuestions数据集上的Hit@1指标提升13%，并生成高度可信的响应。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15210" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 03:38:45 GMT</pubDate>
</item>
<item>
<title>lmgame-Bench：通过视频游戏评估大型语言模型</title>
<link>https://arxiv.org/abs/2505.15146</link>
<guid>https://arxiv.org/abs/2505.15146</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现直接将大型语言模型放入游戏中无法有效评估，提出lmgame-Bench解决感知脆弱、提示敏感和数据污染问题。</p><br /><br /><p><strong>摘要：</strong> 当前的大型语言模型（LLM）在处理复杂任务时面临诸多挑战，而视频游戏因其对感知、记忆和规划能力的要求，成为评估LLM的理想环境。然而，现有方法存在三大问题：视觉感知脆弱、提示敏感性及潜在的数据污染，导致评估效果不佳。本文引入了lmgame-Bench基准测试套件，通过统一的游戏接口和轻量级的感知与记忆支持模块，解决了上述问题，实现了更可靠的评估。该基准涵盖了平台跳跃、益智解谜及叙事类三种类型的游戏，能够区分多种领先模型的能力差异。此外，分析表明每款游戏测试的独特能力组合，且在单个游戏中进行强化学习训练可迁移至其他未见过的游戏及外部规划任务。这一研究成果为LLM的综合能力评估提供了新的视角和工具，相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15146" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 21 May 2025 02:02:55 GMT</pubDate>
</item>
<item>
<title>PiFlow：基于信息论的自动化科学发现框架</title>
<link>https://arxiv.org/abs/2505.15047</link>
<guid>https://arxiv.org/abs/2505.15047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PiFlow显著提升多智能体系统在科学发现中的效率和质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为PiFlow的信息理论框架，旨在解决基于大型语言模型的多智能体系统在科学发现中的局限性。现有方法通常依赖预定义的工作流，缺乏理性约束，导致假设盲目且无法有效链接证据，从而阻碍了系统的不确定性减少。PiFlow将自动化科学发现视为受原则（如科学定律）引导的结构化不确定性减少问题，通过引入科学规律作为指导原则，提高了系统在纳米材料、生物分子及超导体候选物等三个领域内的发现效率和解决方案质量。实验结果显示，与普通代理系统相比，PiFlow的AUC值提升了73.55%，而解决方案质量提高了94.06%。作为一种即插即用的方法，PiFlow标志着高效自动化科学发现范式的转变，为AI驱动的研究提供了新的可能性。代码已公开发布于GitHub。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 23:09:39 GMT</pubDate>
</item>
<item>
<title>基于扩散语言模型的文本嵌入方法超越传统大型语言模型</title>
<link>https://arxiv.org/abs/2505.15045</link>
<guid>https://arxiv.org/abs/2505.15045</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">扩散语言模型在文本嵌入任务中优于大型语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文研究了基于扩散语言模型的文本嵌入方法，该方法通过利用扩散语言模型的双向架构，在长文档检索、推理密集型检索及指令跟随检索等任务上表现优异，分别比大型语言模型高出20%、8%和2%，并在传统文本嵌入基准测试中取得竞争力。分析表明，双向注意力对于编码长且复杂文本的全局上下文至关重要，解决了传统大型语言模型在文本嵌入任务中的单向注意力局限性问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.15045" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 22:59:14 GMT</pubDate>
</item>
<item>
<title>语言特定知识对多语言推理能力的影响研究</title>
<link>https://arxiv.org/abs/2505.14990</link>
<guid>https://arxiv.org/abs/2505.14990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，不同语言可能承载特定领域的更多知识。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了语言特定知识（LSK）现象，即某些语言模型在特定语言下表现更好。通过分析文化相关数据集，发现非英语语言有时在低资源环境下也能提升推理能力。此外，实验表明，利用LSKExtractor方法可显著提高模型准确性约10%，这为构建更具包容性和文化适应性的开源语言模型提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 20:31:13 GMT</pubDate>
</item>
<item>
<title>Mixture of Inputs提升大语言模型生成质量</title>
<link>https://arxiv.org/abs/2505.14827</link>
<guid>https://arxiv.org/abs/2505.14827</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的Mixture of Inputs方法提升LLM生成文本质量。</p><br /><br /><p><strong>摘要：</strong> 标准自回归生成过程中，语言模型会丢弃预测的下一个token分布，仅传递采样的离散token作为新输入。为保留该分布的丰富信息，本文提出了无需训练的Mixture of Inputs（MoI）方法。在生成token后，构建一种融合生成的离散token与之前被丢弃的token分布的新输入。具体而言，采用贝叶斯估计法，将token分布视为先验，采样得到的token视为观测值，用连续的后验期望替代传统的one-hot向量作为新模型输入。MoI使得模型在整个生成过程中保持更丰富的内部表示，从而提升文本质量和推理能力。在数学推理、代码生成及博士级别问答等任务上，MoI对多种模型如QwQ-32B、Nemotron-Super-49B、Gemma-3-27B和DAPO-Qwen-32B均有性能改进，且无需额外训练，计算开销可忽略不计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14827" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 14:41:46 GMT</pubDate>
</item>
<item>
<title>Vid2World：利用视频扩散模型构建交互式世界模型</title>
<link>https://arxiv.org/abs/2505.14357</link>
<guid>https://arxiv.org/abs/2505.14357</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Vid2World方法，将视频扩散模型转化为高效的交互式世界模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Vid2World的方法，旨在通过迁移学习的方式将预训练的视频扩散模型转化为具备交互功能的世界模型。传统世界模型在复杂环境中表现受限，而视频扩散模型虽能生成高质量视频，但缺乏交互性。Vid2World通过调整架构和训练目标实现因果化生成，并引入动作引导机制增强交互能力。实验表明，该方法在机器人操作和游戏模拟领域表现出色，提供了将强大视频扩散模型应用于交互场景的高效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14357" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:41:45 GMT</pubDate>
</item>
<item>
<title>量化感知训练的统一缩放定律及其误差分析</title>
<link>https://arxiv.org/abs/2505.14302</link>
<guid>https://arxiv.org/abs/2505.14302</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究量化感知训练在4位精度下的缩放行为及误差来源。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了量化感知训练（QAT）在4位权重和激活（W4A4）精度下的缩放规律，通过大量实验发现量化误差受模型大小、训练数据量和量化组大小影响。通过对权重和激活分量的分解，确定FC2层激活量化误差是主要瓶颈，并通过混合精度量化优化解决了该问题。此外，增加训练数据可使权重量化误差超过激活量化误差，表明减少权重量化误差同样重要。这些发现对QAT的研究与应用具有指导意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14302" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 08:54:43 GMT</pubDate>
</item>
<item>
<title>UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14231</link>
<guid>https://arxiv.org/abs/2505.14231</guid>
<content:encoded><![CDATA[
Traditional visual grounding methods primarily focus on single-image scenarios with simple textual references. However, extending these methods to real-world scenarios that involve implicit and complex instructions, particularly in conjunction with multiple images, poses significant challenges, which is mainly due to the lack of advanced reasoning ability across diverse multi-modal contexts. In this work, we aim to address the more practical universal grounding task, and propose UniVG-R1, a reasoning guided multimodal large language model (MLLM) for universal visual grounding, which enhances reasoning capabilities through reinforcement learning (RL) combined with cold-start data. Specifically, we first construct a high-quality Chain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning chains, to guide the model towards correct reasoning paths via supervised fine-tuning. Subsequently, we perform rule-based reinforcement learning to encourage the model to identify correct reasoning chains, thereby incentivizing its reasoning capabilities. In addition, we identify a difficulty bias arising from the prevalence of easy samples as RL training progresses, and we propose a difficulty-aware weight adjustment strategy to further strengthen the performance. Experimental results demonstrate the effectiveness of UniVG-R1, which achieves state-of-the-art performance on MIG-Bench with a 9.1% improvement over the previous method. Furthermore, our model exhibits strong generalizability, achieving an average improvement of 23.4% in zero-shot performance across four image and video reasoning grounding benchmarks. The project page can be accessed at https://amap-ml.github.io/UniVG-R1-page/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 07:40:43 GMT</pubDate>
</item>
<item>
<title>基于强化学习与可验证奖励的世界模型优化框架</title>
<link>https://arxiv.org/abs/2505.13934</link>
<guid>https://arxiv.org/abs/2505.13934</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RLVR-World框架，优化世界模型以直接提升特定任务性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RLVR-World的统一框架，该框架结合了具有可验证奖励的强化学习（RLVR），旨在直接优化跨多种模态的世界模型，使其更好地适应特定任务目标。传统最大似然估计等训练目标往往无法有效对齐任务需求，而RLVR-World通过将世界建模视为分词序列的自回归预测，并利用解码预测的指标作为可验证奖励，实现了显著的性能提升。实验结果表明，该方法在文本游戏、网页导航及机器人操作等多个领域的语言和视频世界模型上均表现优异，证明了RLVR作为一种后训练范式在提升生成模型实用性方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13934" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 01:02:53 GMT</pubDate>
</item>
<item>
<title>基于少量高质量轨迹数据的高效计算机操作代理训练框架</title>
<link>https://arxiv.org/abs/2505.13909</link>
<guid>https://arxiv.org/abs/2505.13909</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PC Agent-E通过少量标注数据和合成决策实现对Claude 3.7 Sonnet的超越。</p><br /><br /><p><strong>摘要：</strong> 长期以来，获取大规模高质量的人类轨迹数据一直是开发类人计算机使用代理的关键瓶颈。我们提出了PC Agent-E，这是一种高效的代理训练框架，大幅减少了对大规模人类演示数据的依赖。初始阶段利用仅312条人工注释的计算机使用轨迹，通过结合Claude 3.7 Sonnet生成多样化行动决策进一步提升数据质量。经过这些增强轨迹的训练，我们的PC Agent-E模型在WindowsAgentArena-V2基准测试中取得了141%的相对性能提升，超过了Claude 3.7 Sonnet。此外，该模型在OSWorld上展示了对不同操作系统良好的泛化能力。研究结果表明，强大的计算机使用能力可以从少量高质量轨迹数据中激发出来。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13909" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:20:18 GMT</pubDate>
</item>
<item>
<title>AutoMat：基于深度学习将电子显微镜图像转化为晶体结构的自动化流水线</title>
<link>https://arxiv.org/abs/2505.12650</link>
<guid>https://arxiv.org/abs/2505.12650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AutoMat实现了从扫描透射电子显微镜图像到原子晶体结构的自动化转换。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AutoMat的端到端自动化流水线，该系统能够自动处理扫描透射电子显微镜(STEM)图像并将其转化为原子级晶体结构，同时预测其物理性质。AutoMat通过结合多种先进技术，如自适应去噪、物理引导模板检索、对称性感知原子重建、快速松弛及MatterSim辅助的属性预测，显著提升了模型训练和验证效率。此外，研究提出了首个专门针对此任务的基准数据集STEM2Mat-Bench，并通过一系列评估指标验证了系统的性能。实验结果显示，AutoMat在大规模测试中优于现有的多模态大型语言模型和工具，为显微镜成像与材料科学中的原子模拟之间的桥梁搭建提供了重要支持。所有代码和数据均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 23:04:50 GMT</pubDate>
</item>
<item>
<title>BARREL框架提升大型推理模型的事实可靠性</title>
<link>https://arxiv.org/abs/2505.13529</link>
<guid>https://arxiv.org/abs/2505.13529</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BARREL框架解决大型推理模型过自信及错误回答的问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）在数学和逻辑推理方面展现了令人印象深刻的性能，但这些模型往往表现出过度自信并给出错误答案，主要归因于两种病态推理模式：最后一分钟猜测和二次思考螺旋。为应对这些问题，本文提出了BARREL框架，该框架通过促进简洁且边界感知的事实推理提升了模型的可靠性。实验表明，采用BARREL训练后，DeepSeek-R1-Distill-Llama-8B模型的可靠性从39.33%提高到61.48%，同时保持了与基于R1生成的推理数据微调模型相当的准确性。这些结果表明，本研究为构建更可靠的事实导向型系统2级LRMs提供了启发。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13529" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 03:27:34 GMT</pubDate>
</item>
<item>
<title>探究Transformer语言模型推理阶段中的非激活层</title>
<link>https://arxiv.org/abs/2505.14467</link>
<guid>https://arxiv.org/abs/2505.14467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示并非所有Transformer模型层在推理阶段都被激活。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在推理过程中是否所有Transformer模型层都被激活这一基本问题。通过引入一种名为L2自适应计算（LAC）的非参数化方法，我们检测到了未被激活的层（即空洞）。实验分析了指令微调模型在提示处理和响应生成两个阶段中各层的激活情况，发现这两个阶段激活的层存在显著差异。例如，在MMLU零样本测试中，跳过Qwen2.5-7B-Instruct的空洞层后，性能从69.24提升至71.29，同时仅使用了30%的层；同样，Mistral-7B-Instruct-v0.3在GPQA Diamond上的表现也因减少层的使用而有所提高。这些结果表明，某些任务中可以有选择地跳过大部分层以优化模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:01:56 GMT</pubDate>
</item>
<item>
<title>强化微调对大语言模型可信度的影响及“幻觉税”问题研究</title>
<link>https://arxiv.org/abs/2505.13988</link>
<guid>https://arxiv.org/abs/2505.13988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化微调导致大语言模型拒绝不可回答问题的能力显著下降，产生更多自信但虚假的回答。</p><br /><br /><p><strong>摘要：</strong> 强化微调（RFT）作为提升大型语言模型（LLMs）推理能力的标准方法，其对模型可信度的影响尚不明确。本研究发现，RFT会引发一种被称为“幻觉税”的副作用，即模型拒绝不可回答问题的比例大幅降低，导致对无法回答的问题给出自信但虚假的答案。为了深入分析这一现象，我们构建了一个名为SUM（合成不可回答数学题）的数据集，用于测试模型通过有限或模糊信息判断问题是否可解的能力。实验表明，标准RFT训练使模型拒绝不可回答问题的比例减少了80%以上，显著增加了幻觉倾向。然而，仅在RFT过程中加入10%的SUM数据即可显著恢复适当的拒绝行为，且对可解任务的准确性影响极小。这种方法还帮助LLMs在推理时利用计算资源评估自身不确定性与知识边界，不仅提升了跨领域数学问题的泛化能力，也改善了事实性问题回答的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 02:36:45 GMT</pubDate>
</item>
<item>
<title>通过信息瓶颈理论提升多模态大语言模型的分布外泛化能力</title>
<link>https://arxiv.org/abs/2505.13946</link>
<guid>https://arxiv.org/abs/2505.13946</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于信息瓶颈原理的新方法Vittle，提升多模态大语言模型的分布外鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多模态大型语言模型（MLLMs）在分布偏移下性能下降的问题，提出了从表示学习角度增强其泛化能力的方法。不同于传统需要更多指令数据或更大模型架构的方法，我们受到信息瓶颈（IB）原则的启发，推导出适用于MLLMs的信息瓶颈变分下界，并设计了其实现方式——Visual Instruction Bottleneck Tuning（Vittle）。通过理论分析，揭示了Vittle与MLLM信息论鲁棒性度量之间的联系。实证研究在开放问答、闭合问答及物体幻觉检测等45个数据集上的30种分布偏移场景验证了Vittle的有效性，证明其能有效提升MLLM在分布偏移下的鲁棒性，同时追求最小充分表示的学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13946" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 01:24:53 GMT</pubDate>
</item>
<item>
<title>GeoRanker：基于视觉语言模型的地理排名框架</title>
<link>https://arxiv.org/abs/2505.13731</link>
<guid>https://arxiv.org/abs/2505.13731</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出GeoRanker框架，通过多模态信息提升全球图像地理定位性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对全球范围内的图像地理定位（image geolocalization）任务展开研究，该任务旨在根据地球上任意位置拍摄的图像预测其GPS坐标。传统方法通常采用检索候选对象并选择最佳匹配的两阶段流水线，但往往依赖简单的相似性启发式方法和逐点监督，未能有效建模候选对象间的空间关系。为解决这一问题，我们提出了GeoRanker，这是一种距离感知的排名框架，利用大型视觉-语言模型联合编码查询-候选对象交互并预测地理邻近性。此外，我们引入了一种多阶距离损失函数，可以对绝对距离和相对距离进行排名，使模型能够推理结构化的空间关系。为了支持这一框架，我们还创建了GeoRanking数据集，这是首个明确设计用于地理排名任务的多模态候选信息数据集。实验结果显示，GeoRanker在两个著名的基准数据集（IM2GPS3K和YFCC4K）上取得了最先进的结果，显著优于当前最佳方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13731" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 17:04:46 GMT</pubDate>
</item>
<item>
<title>Vox-Profile：基于语音基础模型的多维度说话人与语音特征基准</title>
<link>https://arxiv.org/abs/2505.14648</link>
<guid>https://arxiv.org/abs/2505.14648</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Vox-Profile基准，全面评估说话人与语音的静态和动态特性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Vox-Profile的综合基准，用于通过语音基础模型刻画丰富的说话人和语音特征。与现有仅关注单一维度的研究不同，Vox-Profile提供了涵盖说话人静态属性（如年龄、性别、口音）和语音动态特性（如情感、语流）的全方位多维描述。该基准结合了语音科学和语言学专业知识，由领域专家开发以准确索引说话人及语音特征。研究利用超过15个公开可用的语音数据集和多种广泛使用的语音基础模型进行了基准实验，并展示了Vox-Profile在多个下游应用中的潜力，包括增强现有的自动语音识别数据集、评估语音生成系统性能，以及验证自动化描述的质量。Vox-Profile已开源供公众访问。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14648" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:36:41 GMT</pubDate>
</item>
<item>
<title>通过检测AI价值观预测潜在风险</title>
<link>https://arxiv.org/abs/2505.14633</link>
<guid>https://arxiv.org/abs/2505.14633</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现AI模型的潜在风险与其内在价值观相关。</p><br /><br /><p><strong>摘要：</strong> 随着AI模型能力增强，传统的风险检测方法面临挑战。本文受到人类行为受价值观驱动的启发，提出通过分析AI模型的价值观来预测其潜在风险。为此，我们开发了LitmusValues评估管道，用于揭示AI模型对多种价值类别的优先级。同时，我们收集了AIRiskDilemmas数据集，其中包含与AI安全相关的困境场景。实验表明，LitmusValues中的价值观不仅能预测已知的危险行为，还能预测未见过的风险行为。这一研究为AI风险管理提供了新的视角和工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14633" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:24:09 GMT</pubDate>
</item>
<item>
<title>KERL：基于知识图谱与大语言模型的个性化食品推荐与食谱生成系统</title>
<link>https://arxiv.org/abs/2505.14629</link>
<guid>https://arxiv.org/abs/2505.14629</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识图谱与大语言模型的KERL系统，实现个性化食品推荐及营养分析。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步及其对食品数据的应用催生了多项研究，试图通过LLMs提升食品理解能力。尽管已有推荐系统利用LLMs和知识图谱(KGs)，但将食品相关KGs与LLMs整合的研究尚显不足。本文介绍了一种名为KERL的新系统，该系统结合食品KGs与LLMs，提供个性化食品推荐并生成带有微营养信息的食谱。KERL首先从自然语言问题中提取实体，从KG中检索子图，然后将其作为上下文输入LLM以筛选满足条件的食谱。接着，系统生成每道菜的烹饪步骤和营养信息。为了评估此方法，我们开发了一个包含食谱相关问题、约束条件和个人偏好的基准数据集。实验表明，我们的KG增强型LLM显著优于现有方法，提供了食品推荐、食谱生成和营养分析的一体化解决方案。代码和基准数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14629" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:19:57 GMT</pubDate>
</item>
<item>
<title>基于动态神经活动扩散的脑成像解码模型</title>
<link>https://arxiv.org/abs/2505.14556</link>
<guid>https://arxiv.org/abs/2505.14556</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种单阶段扩散模型Dynadiff用于从动态fMRI信号重建图像。</p><br /><br /><p><strong>摘要：</strong> 近年来，生成式人工智能模型的进步和超大超高磁场功能磁共振成像(fMRI)数据的可用性推动了脑到图像解码的发展。然而，现有方法依赖复杂的多阶段管道和预处理步骤，通常会压缩大脑记录的时间维度，限制了解码器的时间分辨率。本文介绍了一种名为Dynadiff的新单阶段扩散模型，旨在从动态演化的fMRI记录中重建图像。该模型具有三大贡献：简化训练过程、在时间分辨fMRI信号上的卓越表现以及对大脑活动中图像表示演化进行精确表征的能力。总体而言，这项工作为时间分辨的脑到图像解码奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14556" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 12:14:37 GMT</pubDate>
</item>
<item>
<title>基于视觉视角理解的视觉语言模型训练框架</title>
<link>https://arxiv.org/abs/2505.14366</link>
<guid>https://arxiv.org/abs/2505.14366</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出用于机器人视觉视角理解的视觉语言模型训练框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个概念框架，用于训练视觉语言模型（VLMs）实现视觉视角理解（VPT），这是人机交互（HRI）中主体认知的核心能力。作为迈向该目标的第一步，我们引入了一个在NVIDIA Omniverse中生成的合成数据集，支持空间推理任务的有监督学习。数据集中的每一项包括RGB图像、自然语言描述及表示物体姿态的4X4变换矩阵。我们重点关注推断Z轴距离这一基础技能，并计划未来扩展至完整的6自由度推理。此数据集已公开，以支持进一步研究，标志着向具备交互场景中空间理解能力的具身AI系统迈进的重要一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14366" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:49:09 GMT</pubDate>
</item>
<item>
<title>CoIn：提升闭源大语言模型计费透明性的验证框架</title>
<link>https://arxiv.org/abs/2505.13778</link>
<guid>https://arxiv.org/abs/2505.13778</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架CoIn，用于审计闭源大语言模型中的令牌计数和推理内容真实性。</p><br /><br /><p><strong>摘要：</strong> 随着后训练技术的发展，大型语言模型通过强化学习被赋予多步结构化推理能力，这些模型在复杂任务上表现优于标准模型，并成为许多商业API的基础。然而，为了保护专有行为并减少冗长性，服务提供商通常隐藏推理痕迹仅返回最终答案，导致计费透明度缺失。这种不透明性可能引发令牌计数膨胀问题，即服务商可能虚报令牌使用量或注入合成令牌来增加费用。本文提出CoIn，这是一种验证框架，通过构建可验证哈希树检查令牌数量，并利用基于嵌入的相关性匹配检测伪造的推理内容。实验表明，当作为可信第三方审计员部署时，CoIn能够有效检测令牌计数膨胀，成功率高达94.7%，恢复了闭源大语言模型服务的计费透明性。相关代码和数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13778" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 19:39:23 GMT</pubDate>
</item>
<item>
<title>Learning to Highlight Audio by Watching Movies</title>
<link>https://arxiv.org/abs/2505.12154</link>
<guid>https://arxiv.org/abs/2505.12154</guid>
<content:encoded><![CDATA[
Recent years have seen a significant increase in video content creation and consumption. Crafting engaging content requires the careful curation of both visual and audio elements. While visual cue curation, through techniques like optimal viewpoint selection or post-editing, has been central to media production, its natural counterpart, audio, has not undergone equivalent advancements. This often results in a disconnect between visual and acoustic saliency. To bridge this gap, we introduce a novel task: visually-guided acoustic highlighting, which aims to transform audio to deliver appropriate highlighting effects guided by the accompanying video, ultimately creating a more harmonious audio-visual experience. We propose a flexible, transformer-based multimodal framework to solve this task. To train our model, we also introduce a new dataset -- the muddy mix dataset, leveraging the meticulous audio and video crafting found in movies, which provides a form of free supervision. We develop a pseudo-data generation process to simulate poorly mixed audio, mimicking real-world scenarios through a three-step process -- separation, adjustment, and remixing. Our approach consistently outperforms several baselines in both quantitative and subjective evaluation. We also systematically study the impact of different types of contextual guidance and difficulty levels of the dataset. Our project page is here: https://wikichao.github.io/VisAH/.
]]></content:encoded>
<pubDate>Sat, 17 May 2025 18:03:57 GMT</pubDate>
</item>
<item>
<title>可变粒度搜索提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.11730</link>
<guid>https://arxiv.org/abs/2505.11730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新算法VG-Search优化验证频率，提高推理效率。</p><br /><br /><p><strong>摘要：</strong> 测试时扩展(TTS)通过验证提升大语言模型(LLMs)推理性能，但传统验证方式影响计算效率。本文引入变量粒度搜索(VG-Search)，通过调整粒度参数g统一束搜索和最佳N采样，实验显示动态选择g能显著改善计算效率和扩展行为。基于此，提出自适应VG-Search策略，在特定任务上提升精度同时减少FLOPs超过52%，并计划开源代码支持后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 18:24:48 GMT</pubDate>
</item>
<item>
<title>基于对象中心表征的机器人操作策略鲁棒性研究</title>
<link>https://arxiv.org/abs/2505.11563</link>
<guid>https://arxiv.org/abs/2505.11563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">对象中心表征在复杂视觉条件下的泛化能力优于密集和全局特征。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了对象中心表征（OCR）作为一种结构化替代方案在机器人操作策略中的应用，它通过将视觉输入分割成一组实体，引入了更符合操作任务的归纳偏置。我们评估了一系列视觉编码器——对象中心、全局和密集方法，在一系列模拟和真实世界操作任务中的表现，并在光照变化、纹理差异及干扰物存在等多样化视觉条件下测试其泛化能力。结果显示，即使没有针对特定任务的预训练，基于OCR的策略在泛化场景中也优于密集和全局表示法。这些发现表明，OCR是设计能够在动态真实环境中有效泛化的视觉系统的一个有前景的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 03:06:37 GMT</pubDate>
</item>
<item>
<title>提升RAG系统性能的新方法：利用困难干扰片段</title>
<link>https://arxiv.org/abs/2505.06914</link>
<guid>https://arxiv.org/abs/2505.06914</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出新方法利用困难干扰片段提高检索增强生成模型准确性。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于检索增强生成（RAG）系统中的一个核心问题——不相关文档对语言模型答案生成的干扰效应。我们量化了这种干扰效应，并证明其在不同语言模型中的稳健性。研究引入了识别和应用困难干扰片段的新方法，通过微调这些片段，RAG系统的回答准确性提升了高达7.5%，优于传统数据集微调的对照组。我们的贡献分为两方面：一是超越了无关文档简单二分类的方法，二是开发并分析了多种寻找困难干扰片段的技术。据我们所知，这是首个提供全面框架以识别和利用困难干扰片段的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06914" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 05:25:05 GMT</pubDate>
</item>
<item>
<title>通过强化认知专家提升大规模推理模型的推理效率</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外训练的推理时方法RICE，显著提高大规模推理模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大型推理模型（LRMs）中的混合专家（MoE）架构，指出当前模型存在的过度思考和思考不足等认知效率问题。为解决这些问题，我们提出了推理时强化认知专家（RICE）方法，利用归一化点互信息（nPMI）识别并强化特定的“认知专家”，这些专家负责高层次推理操作。通过在多个基准测试中的实验表明，该方法显著提升了基于MoE的LRMs（如DeepSeek-R1和Qwen3-235B）在定量和科学推理任务中的准确性、认知效率及跨领域泛化能力。相比现有方法如提示设计和解码约束，RICE方法不仅表现更优，还保持了模型的通用指令跟随能力，展示了提升复杂推理模型认知效率的有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:16 GMT</pubDate>
</item>
<item>
<title>通过强化学习训练视觉语言模型实现无监督推理</title>
<link>https://arxiv.org/abs/2505.14677</link>
<guid>https://arxiv.org/abs/2505.14677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现通过强化学习训练视觉语言模型可提升图像推理能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）如DeepSeek-R1的研究表明，利用策略梯度优化（GRPO）等强化学习技术能够使预训练模型具备推理能力。本研究旨在探索如何通过强化学习训练视觉语言模型（VLMs），使其无需显式的链式思维（CoT）监督即可完成基于图像的推理任务。实验表明，仅简单提示模型先生成推理链再给出答案的方式可能导致模型依赖捷径学习，从而影响其泛化能力。为了克服这一问题，我们提出了一种新的输出格式，即先生成图像描述，再构建推理链，从而显著提升了模型的推理性能。最终，我们的模型Visionary-R1在多个视觉推理基准测试中超越了包括GPT-4o、Claude3.5-Sonnet和Gemini-1.5-Pro在内的强大多模态模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:35 GMT</pubDate>
</item>
<item>
<title>IndexMark：一种无需训练的自回归图像生成模型水印框架</title>
<link>https://arxiv.org/abs/2505.14673</link>
<guid>https://arxiv.org/abs/2505.14673</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需训练的水印框架IndexMark，用于保护自回归图像生成模型的版权。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为IndexMark的水印框架，专门针对自回归图像生成模型进行版权保护。IndexMark利用代码簿的冗余特性，在不影响图像质量的前提下嵌入水印。该方法通过匹配相似索引并替换生成索引来实现水印嵌入，同时引入索引编码器提高验证精度，并设计辅助验证方案增强对裁剪攻击的鲁棒性。实验表明，IndexMark在图像质量和验证准确性方面表现优异，且对多种干扰具有良好的鲁棒性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14673" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>基于自适应混合推理的大规模模型研究</title>
<link>https://arxiv.org/abs/2505.14631</link>
<guid>https://arxiv.org/abs/2505.14631</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种能根据查询上下文自适应选择推理模式的大型混合推理模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，大规模推理模型（LRMs）通过引入扩展推理过程显著提升了推理能力，但其过长的推理过程导致效率下降，特别是在处理简单查询时显得多余。本文提出了大型混合推理模型（LHRMs），这是首个可根据用户查询上下文自适应决定是否进行推理的模型。我们设计了一个双阶段训练管道，包括混合微调（HFT）作为冷启动，随后通过提出的混合组策略优化（HGPO）进行在线强化学习，从而隐式学习如何选择适当的推理模式。此外，我们引入了混合准确率这一指标来定量评估模型的混合推理能力。大量实验结果表明，LHRMs在处理不同难度和类型的查询时可以自适应执行混合推理，同时在推理和通用能力上超越现有LRMs和LLMs，且显著提高了效率。这项工作呼吁重新审视扩展推理过程的适当性，并为构建混合推理系统提供了坚实的基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14631" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:23:25 GMT</pubDate>
</item>
<item>
<title>Gemini模型对抗性鲁棒性评估方法及经验</title>
<link>https://arxiv.org/abs/2505.14534</link>
<guid>https://arxiv.org/abs/2505.14534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Gemini模型因具备工具调用能力可访问用户数据，但面临恶意指令风险。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Google DeepMind评估Gemini模型对抗性鲁棒性的方法，并分享了相关经验。通过构建对抗性评估框架，采用多种自适应攻击技术持续测试Gemini的历史、当前及未来版本，揭示了这些测试如何提升Gemini抵御操控的能力。尽管Gemini模型能够执行用户指定的任务并利用工具访问数据，但当处理不受信任的数据时可能引入安全隐患，恶意指令可能导致模型违背用户期望并错误处理数据或权限。因此，加强模型的鲁棒性和安全性成为研究重点。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:54:45 GMT</pubDate>
</item>
<item>
<title>基于推理诱导的无参考图像质量评估模型VisualQuality-R1</title>
<link>https://arxiv.org/abs/2505.14460</link>
<guid>https://arxiv.org/abs/2505.14460</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习训练的VisualQuality-R1在图像质量评估中优于现有方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为VisualQuality-R1的新模型，该模型通过强化学习优化实现了推理诱导的无参考图像质量评估（NR-IQA）。与传统的深度学习方法不同，VisualQuality-R1利用组相对策略优化为图像对生成多个质量评分，并采用Thurstone模型计算比较概率。实验表明，该模型不仅在性能上超越了其他NR-IQA方法，还能提供丰富的人类可解释的质量描述，支持多数据集训练且无需重新校准感知尺度。这些特性使其特别适用于超分辨率和图像生成等任务中的图像质量评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14460" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 10:56:50 GMT</pubDate>
</item>
<item>
<title>探索语言模型中的隐藏知识：Taboo模型与解密策略</title>
<link>https://arxiv.org/abs/2505.14352</link>
<guid>https://arxiv.org/abs/2505.14352</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示现有技术如何揭示语言模型中的隐秘知识。</p><br /><br /><p><strong>摘要：</strong> 随着语言模型能力增强，确保其可信性和可靠性至关重要。初步证据表明，这些模型可能试图对操作者隐瞒信息或欺骗。本文通过训练Taboo模型（描述特定秘密词但不明确陈述），测试当前技术揭示隐藏知识的能力。我们首先评估非解释性（黑盒）方法，随后基于机械性解释技术开发自动化策略，如Logit透镜和稀疏自动编码器。实验结果显示两种方法均有效。本研究强调了解释技术在揭示隐藏知识方面的潜力，并提出未来研究方向，包括在更复杂的模型上测试和完善这些方法，从而助力语言模型的安全部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14352" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 09:36:37 GMT</pubDate>
</item>
<item>
<title>视觉主动强化微调提升多模态大模型推理能力</title>
<link>https://arxiv.org/abs/2505.14246</link>
<guid>https://arxiv.org/abs/2505.14246</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出视觉主动强化微调方法，显著提升多模态大模型的推理与工具使用能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大型视觉语言模型（LVLMs）的多模态主动能力开发，通过提出视觉主动强化微调（Visual-ARFT），使这些模型能够利用外部工具如网页浏览器进行实时信息检索及图像处理操作。我们设计了一个多模态主动工具基准（MAT），包括搜索（MAT-Search）和编码（MAT-Coding）两种设置，用于评估LVLMs的主动搜索与编码能力。实验结果显示，Visual-ARFT在MAT-Coding上比基线提升了18.6%的F1分数和13.0%的精确匹配得分，在MAT-Search上分别提升了10.3%和8.7%，甚至超越了GPT-4o的表现。此外，该方法在现有多跳问答基准测试如2Wiki和HotpotQA上也取得了显著进步，显示出较强的泛化能力。这项研究为构建强大且可推广的多模态智能代理提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14246" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 07:59:25 GMT</pubDate>
</item>
<item>
<title>基于双阶段训练策略构建少样本推理能力的大语言模型</title>
<link>https://arxiv.org/abs/2505.13718</link>
<guid>https://arxiv.org/abs/2505.13718</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效两阶段训练方法，在有限标注下提升大语言模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在训练具备推理能力的大语言模型时面临的挑战，特别是在高质量训练数据稀缺的情况下。传统方法如基于可验证奖励的强化学习（RLVR）或通过精心设计的长链思维（CoT）进行蒸馏，均需要大量训练数据支持。针对这一问题，我们提出了一个样本高效的两阶段训练策略，用于在有限监督条件下开发推理型大语言模型。第一阶段通过蒸馏逻辑谜题（Knights & Knaves）中的长链思维来“预热”模型，以获取通用推理技能；第二阶段则利用少量目标领域样例对预热后的模型应用RLVR优化。实验表明，此方法不仅提升了模型在多种任务上的表现，还增强了跨领域的泛化能力，并显著提高了样本效率。研究结果证明了预热策略在构建鲁棒推理大语言模型方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13718" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 16:29:15 GMT</pubDate>
</item>
<item>
<title>AnytimeReasoner：优化语言模型在动态计算预算下的推理性能</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架AnytimeReasoner，提升大语言模型在不同计算预算下的推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为AnytimeReasoner的新框架，旨在通过优化推理过程中的即时性能来提高大型语言模型（LLMs）的推理能力。传统方法通常依赖强化学习来最大化最终奖励，但这些方法往往在固定的大规模计算预算下进行优化，导致训练和部署效率低下。AnytimeReasoner通过截断完整的推理过程以适应来自先验分布的采样计算预算，迫使模型针对每个截断的推理阶段生成最优答案并验证，从而引入可验证的密集奖励，改进强化学习中的信用分配。此外，该框架还通过分离推理策略和摘要策略的优化，以及引入预算相对策略优化（BRPO）技术，进一步提高了学习过程的鲁棒性和效率。实验结果显示，在数学推理任务中，AnytimeReasoner在各种先验分布和计算预算下均优于传统的GRPO方法，显著提升了训练和计算效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:58:44 GMT</pubDate>
</item>
<item>
<title>基于量化零阶优化的高效大语言模型微调方法</title>
<link>https://arxiv.org/abs/2505.13430</link>
<guid>https://arxiv.org/abs/2505.13430</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型量化零阶优化方法，大幅降低大语言模型微调的显存消耗。</p><br /><br /><p><strong>摘要：</strong> 随着大规模语言模型的参数量呈指数级增长，GPU显存成为适配下游任务的瓶颈。本文通过统一框架减少模型权重、梯度及优化器状态的内存占用，提出了量化零阶优化（QZO），该方法在不损失精度的情况下将4位LLMs的总显存成本减少了超过18倍，使Llama-2-13B和Stable Diffusion 3.5 Large能够在单块24GB GPU上完成微调。QZO通过扰动连续量化尺度进行梯度估计，并采用方向导数裁剪稳定训练过程，同时与标量型和码本型后训练量化方法正交兼容。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13430" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:55:15 GMT</pubDate>
</item>
<item>
<title>神经符号扩散模型提升视觉推理能力</title>
<link>https://arxiv.org/abs/2505.13138</link>
<guid>https://arxiv.org/abs/2505.13138</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入神经符号扩散模型解决传统模型过自信和泛化不足的问题。</p><br /><br /><p><strong>摘要：</strong> 神经符号（Neurosymbolic, NeSy）预测器结合神经感知与符号推理，在视觉推理等任务中展现出潜力，但受限于条件独立假设，难以有效建模符号间的交互与不确定性，导致预测过自信且分布外泛化表现欠佳。为克服这一局限，本文提出神经符号扩散模型（NeSyDMs），这是一种新的NeSy预测框架，通过离散扩散过程捕捉符号依赖性。在合成及真实世界基准测试中，包括高维视觉路径规划和基于规则的自动驾驶任务，NeSyDMs实现了最先进的准确率并表现出良好的校准性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13138" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:07:47 GMT</pubDate>
</item>
<item>
<title>WILLIAMT：通过模板引导提升自动化程序修复效率</title>
<link>https://arxiv.org/abs/2505.13103</link>
<guid>https://arxiv.org/abs/2505.13103</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法简化漏洞修复任务并降低语言模型成本。</p><br /><br /><p><strong>摘要：</strong> 随着漏洞检测技术的进步，现代软件开发面临修复资源不足的问题，促使对高效自动化程序修复(APR)的需求增加。然而，复杂漏洞的分析难度限制了现有修复工具的效果。本研究提出了crash-site修复策略，并结合模板引导的补丁生成方法，显著降低了大型语言模型(LLMs)的令牌消耗，同时保持了修复效率和效果。实验显示，当与CodeRover-S结合时，WILLIAMT在ARVO基准测试中将修复成功率提高至73.5%，并减少了45.9%的令牌消耗。此外，即使在没有前沿LLMs的情况下，该系统仍能实现合理的修复率。这些成果表明WILLIAMT具有广泛适用性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13103" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 09:32:51 GMT</pubDate>
</item>
<item>
<title>基于RoBERTa的媒体偏见检测模型性能提升研究</title>
<link>https://arxiv.org/abs/2505.13010</link>
<guid>https://arxiv.org/abs/2505.13010</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过微调RoBERTa模型，显著提升了媒体偏见检测的性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了句子级媒体偏见分类问题，利用RoBERTa模型在BABE数据集上进行微调，并与DA-RoBERTa基线模型对比，证明了在McNemar测试和5x2交叉验证配对t检验下性能有显著改善。注意力机制分析显示，该模型避免了过度关注政治敏感词汇的问题，转而更加注重上下文相关的有意义词汇。此外，我们还提出了一种结合现有偏见类型分类器的完整偏见检测管道，展示了良好的泛化性和可解释性。尽管受到数据集规模限制，我们的方法仍为构建更健壮、可解释且社会责任感更强的自然语言处理系统提供了贡献。未来的研究方向包括上下文感知建模、偏见中立化及高级偏见类型分类。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13010" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:54:39 GMT</pubDate>
</item>
<item>
<title>FedPrLLM：一种面向隐私保护的大型语言模型压缩框架</title>
<link>https://arxiv.org/abs/2505.13547</link>
<guid>https://arxiv.org/abs/2505.13547</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FedPrLLM框架解决LLM压缩中的隐私保护问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FedPrLLM的联邦剪枝框架，用于在资源受限设备上部署大型语言模型（LLMs）的同时保护本地数据隐私。FedPrLLM允许每个客户端仅基于本地校准数据计算剪枝掩码并共享给服务器，从而实现全局模型的协同剪枝。实验表明，采用层比较的一次性剪枝且不缩放权重是该框架下的最佳选择。本研究旨在指导隐私敏感领域中的LLM剪枝工作，并提供开源代码。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13547" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 23:41:54 GMT</pubDate>
</item>
<item>
<title>探究语言模型在多跳问答中的表现及优化策略</title>
<link>https://arxiv.org/abs/2505.11754</link>
<guid>https://arxiv.org/abs/2505.11754</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，编码器-解码器模型在多跳问答任务中优于解码器-only模型。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了语言模型在多跳问答（MHQA）任务中的表现，通过重新排列检索到的文档，分析了不同配置下模型的行为。研究发现，如Flan-T5家族的编码器-解码器模型尽管规模较小，但普遍优于解码器-only模型；解码器-only模型在文档顺序与推理链顺序一致时性能最佳；通过修改因果掩码增强双向注意力机制可有效提升解码器-only模型的表现。此外，我们还深入研究了语言模型在多跳问答中的注意力权重分布，发现正确答案对应的注意力权重值更高，据此提出了一种启发式方法来提升模型性能。我们的代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11754" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 19:29:47 GMT</pubDate>
</item>
<item>
<title>Phare：多语言大语言模型安全性诊断框架</title>
<link>https://arxiv.org/abs/2505.11365</link>
<guid>https://arxiv.org/abs/2505.11365</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phare框架评估17个顶尖大语言模型的安全性问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Phare的多语言诊断框架，用于评估大型语言模型（LLMs）在三个关键维度上的行为：幻觉与可靠性、社会偏见及有害内容生成。通过对17个最先进的LLMs进行评估，研究发现所有安全维度上存在系统性漏洞，如谄媚、提示敏感性和刻板印象再现等特定失效模式。与仅对模型排名不同，Phare为研究人员和从业者提供了可操作的见解，以构建更强大、一致且可信的语言系统。这一框架强调识别具体失效模式而非单纯比较性能，从而推动负责任的大规模语言模型部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11365" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 11:31:08 GMT</pubDate>
</item>
<item>
<title>基于生物逆效应机制的多模态融合策略</title>
<link>https://arxiv.org/abs/2505.10176</link>
<guid>https://arxiv.org/abs/2505.10176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种受生物逆效应启发的多模态融合方法，显著提升模型性能并降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有多模态融合研究中静态整合不足的问题，受大脑逆效应现象的启发，提出了一种逆效应驱动的多模态融合（IEMF）策略。该策略通过弱化强模态信号的影响，增强弱模态信号的作用，从而实现更高效的多模态信息整合。实验验证了IEMF在音频-视觉分类、持续学习和问答等任务中的优异表现，并展示了其在人工神经网络（ANN）和脉冲神经网络（SNN）上的良好适应性。研究强调了将生物启发机制融入多模态网络的潜力，并为未来多模态人工智能的发展提供了新方向。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 07:08:50 GMT</pubDate>
</item>
<item>
<title>MIGRATION-BENCH：面向代码迁移的大规模基准测试</title>
<link>https://arxiv.org/abs/2505.09569</link>
<guid>https://arxiv.org/abs/2505.09569</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MIGRATION-BENCH，评估大语言模型在Java代码迁移中的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，强大的大型语言模型（LLMs）显著提升了软件工程生产力，但现有基准主要聚焦于问题解决任务，而忽视了代码迁移这一重要领域。本文介绍MIGRATION-BENCH，这是首个专注于从Java 8迁移到最新长期支持版本（如Java 17和21）的综合基准测试集。该基准集包含5102个和300个仓库的完整数据集及其精选子集，提供了复杂性和难度适中的代表性样本。此外，我们还开发了一个全面的评估框架，用于标准化评估LLMs在代码迁移任务上的表现。通过引入SD-Feedback方法，我们展示了LLMs在处理仓库级代码迁移方面的有效性，其中Claude-3.5-Sonnet-v2在最小迁移和最大迁移任务上分别达到了62.33%和27.00%的成功率。MIGRATION-BENCH的数据集及相关源代码已公开供研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09569" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:11:23 GMT</pubDate>
</item>
<item>
<title>Aloe Beta：开源医疗领域大型语言模型的标杆</title>
<link>https://arxiv.org/abs/2505.04388</link>
<guid>https://arxiv.org/abs/2505.04388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Aloe Beta通过优化数据处理和训练提升模型安全性和效能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Aloe Beta这一开源医疗领域的大型语言模型，该模型基于Llama 3.1和Qwen 2.5等基础模型构建，并通过自定义数据集增强公共数据，同时采用直接偏好优化（DPO）进行对齐，强调伦理和政策导向性能。Aloe Beta在闭合型、开放型、安全性及人类评估测试中表现出色，展现出卓越的竞争能力，尤其是在减少偏见和毒性方面显著提高了安全性，并对潜在风险进行了详尽评估。研究结果表明，这些模型在多个医疗基准测试中表现优异，且受到专业人士青睐，标志着医疗领域开源LLM发展的新标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 09:13:14 GMT</pubDate>
</item>
<item>
<title>Emerging Properties in Unified Multimodal Pretraining</title>
<link>https://arxiv.org/abs/2505.14683</link>
<guid>https://arxiv.org/abs/2505.14683</guid>
<content:encoded><![CDATA[
Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open0source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder0only model pretrained on trillions of tokens curated from large0scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>NExT-Search：重塑生成式AI搜索的反馈驱动范式</title>
<link>https://arxiv.org/abs/2505.14680</link>
<guid>https://arxiv.org/abs/2505.14680</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">论文提出NExT-Search框架，解决生成式AI搜索反馈断层问题。</p><br /><br /><p><strong>摘要：</strong> 生成式AI搜索通过提供端到端答案简化了复杂查询的信息检索过程，但因用户反馈集中在最终答案层面，导致无法有效映射至具体系统组件，阻碍了中间阶段的优化和反馈循环的持续改进。本文提出NExT-Search框架，引入用户干预和个性化模拟反馈模式，恢复对生成式AI搜索关键阶段的人类控制，旨在通过实时在线适应和离线模型更新实现系统持续进化。NExT-Search融合用户调试模式与影子用户模式，重新建立细粒度的反馈机制，为生成式AI搜索提供了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14680" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:59:13 GMT</pubDate>
</item>
<item>
<title>Reward Reasoning Model</title>
<link>https://arxiv.org/abs/2505.14674</link>
<guid>https://arxiv.org/abs/2505.14674</guid>
<content:encoded><![CDATA[
Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:58:03 GMT</pubDate>
</item>
<item>
<title>General-Reasoner：一种增强大语言模型跨领域推理能力的训练范式</title>
<link>https://arxiv.org/abs/2505.14652</link>
<guid>https://arxiv.org/abs/2505.14652</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型训练方法，提升大语言模型在多领域推理的能力。</p><br /><br /><p><strong>摘要：</strong> 近年来，强化学习在增强大型语言模型推理能力方面展现出巨大潜力。然而，现有研究主要集中在数学和编程领域，因数据丰富且答案验证容易。本文提出了General-Reasoner，这是一种新的训练范式，旨在提高大型语言模型在多样领域中的推理能力。我们构建了一个大规模高质量的问题数据集，并开发了一种基于生成模型的答案验证器，取代传统的规则验证方法。通过在多个领域（如物理、化学、金融等）的数据集上进行评估，结果显示General-Reasoner在多种基准测试中优于现有基线方法，特别是在数学推理任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14652" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:41:33 GMT</pubDate>
</item>
<item>
<title>重新审视视频理解基准：VideoEval-Pro 的提出</title>
<link>https://arxiv.org/abs/2505.14640</link>
<guid>https://arxiv.org/abs/2505.14640</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有视频理解基准存在评估偏差问题，本文提出新的基准 VideoEval-Pro。</p><br /><br /><p><strong>摘要：</strong> 大型多模态模型（LMMs）在长视频理解（LVU）中的应用推动了相关基准的发展，但我们的研究表明现有LVU基准存在显著问题。多数基准依赖多选题（MCQs），导致评估结果因猜测而虚高；且部分问题具有强先验，模型无需观看视频即可作答。此外，增加帧数未必提升性能，削弱了当前基准的有效性和鲁棒性。为此，我们提出了VideoEval-Pro，一个包含开放式简答题的新基准，真正需要理解整个视频。通过评估21个视频LMMs，发现开放题性能比MCQs下降明显，高MCQ分数不等同于高开放题分数，且VideoEval-Pro从增加帧数中获益更多。该研究揭示了现有基准的局限性，并提供了一个更可靠的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14640" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 13:26:32 GMT</pubDate>
</item>
<item>
<title>基于流匹配的潜在流Transformer压缩大语言模型</title>
<link>https://arxiv.org/abs/2505.14513</link>
<guid>https://arxiv.org/abs/2505.14513</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Latent Flow Transformer，通过单一流动算子替换多层网络实现高效压缩。</p><br /><br /><p><strong>摘要：</strong> 标准的大语言模型通常由数十到数百个离散层组成，尽管更多层数可能带来更好的性能，但这种方法效率低下。受扩散和基于流的图像生成模型的启发，本文提出了Latent Flow Transformer (LFT)，它用单一学习的传输算子替代一层或多层网络，同时保持与原始架构的兼容性。此外，为了克服现有基于流方法在保持耦合方面的局限性，引入了Flow Walking算法。实验显示，在Pythia-410M模型上，LFT通过流匹配压缩6层后优于直接跳过2层，且当结合FW算法时，可以将12层进一步蒸馏为1层，显著缩小了自回归生成与基于流生成范式的差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14513" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:41:05 GMT</pubDate>
</item>
<item>
<title>推理模型在置信度表达上的优越表现</title>
<link>https://arxiv.org/abs/2505.14489</link>
<guid>https://arxiv.org/abs/2505.14489</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示推理模型在链式思维中表现出更好的置信度校准能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）在许多方面表现出色，但它们往往难以准确传达自身的置信度，这限制了其可靠性。本研究证明，采用扩展链式思维（CoT）推理的推理模型不仅在问题解决上表现优异，还在准确表达置信度方面更为出色。通过对比六种推理模型在六个数据集上的表现，我们发现它们在33种设置下相较于非推理模型具有更优的置信度校准能力。深入分析表明，这种改进源于推理模型的慢思考特性，例如探索替代方案和回溯等，这些特性使其在整个链式思维过程中动态调整置信度，从而逐步提高准确性。尤其值得注意的是，随着链式思维的展开，推理模型的置信度校准能力逐渐增强，而非推理模型则不具备这一趋势。此外，即使移除链式思维中的慢思考行为，也会显著降低模型的校准性能。最后，研究还发现，非推理模型在通过情境学习被引导进行慢思考时也能获得类似的收益。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14489" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:19:00 GMT</pubDate>
</item>
<item>
<title>基于推理数据蒸馏提升开源语言模型性能的研究</title>
<link>https://arxiv.org/abs/2505.14464</link>
<guid>https://arxiv.org/abs/2505.14464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过收集高质量推理数据蒸馏提升了开源语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对推理数据蒸馏展开大规模实证研究，从三个最先进的教师模型（AM-Thinking-v1、Qwen3-235B-A22B和DeepSeek-R1）中收集验证过的输出数据，构建了三个并行数据集，涵盖总计189万个查询。分析显示，AM-Thinking-v1蒸馏的数据具有更高的令牌长度多样性和更低的困惑度。基于这些数据训练的学生模型在AIME2024、AIME2025、MATH500和LiveCodeBench等推理基准测试中表现出色，其中AM-Thinking-v1模型在多个任务上取得了最佳性能，例如在AIME2024得分为84.3，在AIME2025得分为72.2，在MATH500得分为98.4，在LiveCodeBench得分为65.9。此外，该模型展示了适应性输出行为，即对较难任务生成较长响应，对简单任务生成较短响应。这些发现强调了高质量验证推理轨迹的重要性，并公开发布了AM-Thinking-v1和Qwen3-235B-A22B蒸馏的数据集，支持未来关于开放高性能推理导向型语言模型的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 11:00:51 GMT</pubDate>
</item>
<item>
<title>语言模型中的分词对符号推理能力的影响</title>
<link>https://arxiv.org/abs/2505.14178</link>
<guid>https://arxiv.org/abs/2505.14178</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">分词结构显著影响语言模型的符号推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了分词方案，特别是基于子词的方法（如字节对编码 BPE），如何通过合并或模糊原子推理单元而阻碍符号计算。研究引入了“标记感知”概念，以形式化粒度较差的标记如何破坏逻辑对齐并阻止模型泛化符号过程。通过对算术和符号任务的系统评估发现，即使采用思维链提示，分词结构仍会严重影响推理表现，而原子对齐的格式则能够解锁强大的泛化能力，使小型模型在结构化推理任务中优于大型系统。研究结果表明，大型语言模型的符号推理能力不仅依赖于架构，还深深依赖于标记级表示。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14178" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 06:32:30 GMT</pubDate>
</item>
<item>
<title>Hunyuan-Game：智能游戏创作的革新项目</title>
<link>https://arxiv.org/abs/2505.14135</link>
<guid>https://arxiv.org/abs/2505.14135</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">利用人工智能生成高质量游戏内容，提升设计师效率。</p><br /><br /><p><strong>摘要：</strong> 智能游戏创作是游戏开发领域的一项革命性进步，通过生成式人工智能动态生成和优化游戏内容。尽管生成模型已取得显著进展，但高质量游戏资产（图像和视频）的综合合成仍具挑战性。为解决这一问题，我们推出了Hunyuan-Game项目，专注于图像和视频生成两大方向。图像生成基于数十亿游戏图像的数据集，开发了多种定制化模型，如文本到图像生成、视觉效果生成、透明图像生成及角色生成等。视频生成则依托数百万游戏和动漫视频数据集，开发了五大算法模型，涵盖图像到视频生成、人物视频合成、动态插图生成、超分辨率视频生成及交互式游戏视频生成。这些模型不仅具备高水平美学表达，还深度融合了特定领域的知识，系统理解了多样化的游戏和动漫艺术风格。Hunyuan-Game旨在大幅提升设计师的工作效率并满足玩家偏好。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.14135" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 20 May 2025 05:39:48 GMT</pubDate>
</item>
<item>
<title>推理路径压缩提升逻辑型大模型推理效率</title>
<link>https://arxiv.org/abs/2505.13866</link>
<guid>https://arxiv.org/abs/2505.13866</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的推理加速方法RPC，显著提升逻辑型大模型的吞吐量。</p><br /><br /><p><strong>摘要：</strong> 近期基于推理路径的语言模型通过生成长序列中间推理过程提高准确性，但增加了内存消耗和计算复杂度。本文提出推理路径压缩（RPC），一种无需训练的方法，利用推理路径的语义稀疏性加速推理过程。RPC通过保留重要性得分高的键值对（KV缓存），显著提高了生成吞吐量，在AIME 2024基准测试中的准确率仅下降1.2%。实验表明，RPC可将QwQ-32B模型的生成吞吐量提升最多1.6倍，为高效部署逻辑型大语言模型提供了可行方案。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13866" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 23:21:52 GMT</pubDate>
</item>
<item>
<title>CompeteSMoE：一种高效的大规模混合专家模型训练机制</title>
<link>https://arxiv.org/abs/2505.13380</link>
<guid>https://arxiv.org/abs/2505.13380</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出竞争机制提升稀疏混合专家模型的路由效率。</p><br /><br /><p><strong>摘要：</strong> 稀疏混合专家（SMoE）模型通过增加复杂度而非单纯扩大网络深度或宽度提供了新的可能性，但其训练挑战在于子优化的路由过程。本文提出了竞争机制，使计算专家直接参与路由决策，理论分析表明其样本效率优于传统softmax路由。我们开发了CompeteSMoE算法，通过学习竞争策略显著提升了大语言模型的训练效果，同时降低了训练开销。在视觉指令微调和语言预训练任务上的实验验证了该方法的有效性、鲁棒性和可扩展性，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13380" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:24:26 GMT</pubDate>
</item>
<item>
<title>跨语言切换对大语言模型的影响及评估基准CS-Sum</title>
<link>https://arxiv.org/abs/2505.13559</link>
<guid>https://arxiv.org/abs/2505.13559</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入CS-Sum评估大语言模型处理跨语言对话总结的能力。</p><br /><br /><p><strong>摘要：</strong> 跨语言切换（Code-switching, CS）对大语言模型（LLMs）构成重大挑战，但其可理解性尚未被充分探索。本研究介绍了CS-Sum，这是一个用于评估LLMs通过跨语言对话到英语总结的可理解性的基准。CS-Sum涵盖了汉语-英语（EN-ZH）、泰米尔语-英语（EN-TA）和马来语-英语（EN-MS）三种语言对，每种语言对包含900至1300个人类注释的对话。通过对十种LLMs（包括开源和闭源模型）进行评估，我们分析了Few-shot、Translate-Summarize和Fine-tuning（LoRA、QLoRA基于合成数据）方法的表现。研究发现，尽管自动化指标得分较高，但LLMs在处理CS输入时会犯一些细微错误，这些错误可能会改变对话的整体含义。为此，我们总结了LLMs在处理CS输入时最常见的三种错误类型。错误率在不同的CS语言对和LLMs之间有所不同，某些LLMs在特定语言对上的错误频率更高，这凸显了对跨语言数据进行专门训练的必要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13559" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 05:18:14 GMT</pubDate>
</item>
<item>
<title>SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.12448</link>
<guid>https://arxiv.org/abs/2505.12448</guid>
<content:encoded><![CDATA[
Despite impressive advancements in Visual-Language Models (VLMs) for multi-modal tasks, their reliance on RGB inputs limits precise spatial understanding. Existing methods for integrating spatial cues, such as point clouds or depth, either require specialized sensors or fail to effectively exploit depth information for higher-order reasoning. To this end, we propose a novel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that transforms raw depth data into structured, interpretable textual rationales. These textual rationales serve as meaningful intermediate representations to significantly enhance spatial reasoning capabilities. Additionally, we leverage knowledge distillation to compress the generated rationales into compact latent embeddings, which facilitate resource-efficient and plug-and-play integration into existing VLMs without retraining. To enable comprehensive evaluation, we introduce a new dataset named SSR-CoT, a million-scale visual-language reasoning dataset enriched with intermediate spatial reasoning annotations, and present SSRBench, a comprehensive multi-task benchmark. Extensive experiments on multiple benchmarks demonstrate SSR substantially improves depth utilization and enhances spatial reasoning, thereby advancing VLMs toward more human-like multi-modal understanding. Our project page is at https://yliu-cs.github.io/SSR.
]]></content:encoded>
<pubDate>Sun, 18 May 2025 10:40:16 GMT</pubDate>
</item>
<item>
<title>WikiDYK基准测试揭示因果语言模型的知识记忆弱点</title>
<link>https://arxiv.org/abs/2505.12306</link>
<guid>https://arxiv.org/abs/2505.12306</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现双向语言模型比因果语言模型具有更强的知识记忆能力。</p><br /><br /><p><strong>摘要：</strong> 尽管大型语言模型（LLMs）取得了显著进展，但它们的知识记忆能力仍未被充分探索，因为缺乏标准化且高质量的测试平台。本文引入了一个名为WikiDYK的新基准测试，它基于维基百科的“你知道吗？”条目，这些条目由专家编辑精心挑选，包含可验证性和清晰性等标准。WikiDYK包含12,290个事实和77,180个问题，并可以随着维基百科的更新无缝扩展。通过持续预训练实验，我们惊讶地发现，尽管因果语言模型（CLMs）在现代LLMs中很常见，但它们在可靠性方面的准确性比双向语言模型（BiLMs）低23%。为了弥补当前BiLM规模较小的问题，我们提出了一种模块化协作框架，利用BiLM集合作为外部知识库与LLMs集成，实验显示该框架将可靠性准确性提高了29.1%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12306" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 04:39:05 GMT</pubDate>
</item>
<item>
<title>语言模型中的真相神经元：机制解析与验证</title>
<link>https://arxiv.org/abs/2505.12182</link>
<guid>https://arxiv.org/abs/2505.12182</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言模型通过“真相神经元”编码真实性。</p><br /><br /><p><strong>摘要：</strong> 本文提出一种方法，用于识别语言模型中神经元级别的真相表示。实验表明，多种规模的语言模型均包含不依赖具体主题的真相神经元，这些神经元的分布模式与先前关于真相几何学的研究结果一致。抑制通过TruthfulQA数据集发现的真相神经元激活会降低模型在TruthfulQA及其他基准测试上的表现，证明真相机制并非局限于特定数据集。本研究为理解语言模型中真相编码机制提供了新见解，并指出了提升模型可信度的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12182" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 20:47:21 GMT</pubDate>
</item>
<item>
<title>FlexiVe：一种灵活的大型语言模型推理验证方法</title>
<link>https://arxiv.org/abs/2505.11966</link>
<guid>https://arxiv.org/abs/2505.11966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FlexiVe，一种高效且可扩展的推理验证方法，提升大规模语言模型的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在复杂任务中大规模语言模型推理过程中解决精度与计算效率之间权衡的问题。传统验证步骤虽然旨在提高性能，但引入了新的挑战，即如何平衡高级生成奖励模型的可靠性和计算成本。为了解决这些问题，我们提出了FlexiVe，这是一种新的生成型验证器，通过灵活分配验证预算，在快速可靠的快速思考和细致的慢速思考之间实现平衡。此外，我们还设计了Solve-Detect-Verify管道，这是一个高效的推理时间扩展框架，通过智能集成FlexiVe，主动识别解决方案完成点来触发有针对性的验证并提供聚焦的求解器反馈。实验表明，FlexiVe在ProcessBench上显著提高了对推理轨迹中错误的定位能力。同时，在具有挑战性的数学推理基准测试中，我们的完整方法在推理准确性和效率方面超过了自一致性等基线方法。该系统为增强大规模语言模型的实时推理提供了可扩展且有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 07:41:44 GMT</pubDate>
</item>
<item>
<title>通过低比特注意力机制提升大规模模型训练效率</title>
<link>https://arxiv.org/abs/2505.11594</link>
<guid>https://arxiv.org/abs/2505.11594</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于FP4和8位精度的注意力加速方法，显著提升模型训练与推理速度。</p><br /><br /><p><strong>摘要：</strong> 本文针对注意力计算的时间复杂度问题，提出了两项改进措施。首先，利用Blackwell GPU中的新FP4张量核心加速推理阶段的注意力计算，在RTX5090上达到1038 TOPS，较FlashAttention提升了5倍。该方法能够无缝集成到多种模型中，实现高效推理。其次，我们开创性地将低比特注意力引入训练任务，设计了一种适用于前向和反向传播的8位注意力机制。实验表明，该方法在微调任务中表现无损，但在预训练任务中收敛速度较慢。本研究为大规模模型的高效训练提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11594" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 14:01:54 GMT</pubDate>
</item>
<item>
<title>AI对Z世代数字语言的理解评估：在线安全的新挑战</title>
<link>https://arxiv.org/abs/2505.10588</link>
<guid>https://arxiv.org/abs/2505.10588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了主流AI模型对Z世代数字语言中隐藏风险的检测能力。</p><br /><br /><p><strong>摘要：</strong> 本研究首次评估了AI系统如何解读Z世代（2010-2024年出生）的数字语言，揭示了现有安全工具在面对这一群体独特沟通方式时的局限性。通过分析四款领先AI模型（GPT-4、Claude、Gemini和Llama 3）的表现，研究发现这些系统难以有效识别Z世代交流中的潜在有害互动。研究基于100条来自游戏平台、社交媒体和视频内容的真实表达构建了一个全新数据集，并提出改进青少年保护的AI监管框架。此外，研究结合了AI系统、人类管理者及家长的多视角评价，并直接采纳了Z世代共同研究员的意见，强调了重新设计适配青少年语言的安全系统的紧迫性。这项工作为解决数字时代的关键安全挑战提供了新见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>MedCaseReasoning：评估大型语言模型临床诊断推理能力的新基准</title>
<link>https://arxiv.org/abs/2505.11733</link>
<guid>https://arxiv.org/abs/2505.11733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出首个开放数据集MedCaseReasoning，用于评估大型语言模型在医疗诊断推理中的表现。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLMs）在医学诊断中的应用日益广泛，但现有评估标准主要关注最终答案准确性，忽视了临床推理过程的质量。为弥补这一不足，本研究引入MedCaseReasoning，这是一个包含14,489个病例的开放数据集，每个病例均附有详细的医生推理说明。通过测试现有最先进的LLMs，发现这些模型在诊断准确性和推理召回率上存在显著缺陷。例如，表现最好的开源模型DeepSeek-R1仅达到48%的诊断准确率和64%的推理召回率。然而，通过对模型进行基于MedCaseReasoning推理痕迹的微调，可以显著提升诊断准确率和推理召回率，分别提高29%和41%。该数据集及相关代码和模型已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 18:34:36 GMT</pubDate>
</item>
<item>
<title>通过镜像方法评估图像常识一致性</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法TLG，利用大型视觉语言模型评估图像常识一致性。</p><br /><br /><p><strong>摘要：</strong> 在人工智能研究中，衡量真实图像的真实性是一项复杂的任务。例如，一个男孩手持吸尘器站在沙漠中的图片违背了常识。本文介绍了一种名为Through the Looking Glass (TLG)的新方法，用于评估图像的常识一致性。TLG利用大型视觉语言模型(LVLMs)和基于Transformer的编码器从图像中提取原子事实，从而获得混合的精确事实。随后，通过在一个紧凑的注意力池化分类器上进行微调，该分类器对编码的原子事实进行处理，TLG在WHOOPS!和WEIRD数据集上的表现达到了新的最先进水平，同时采用了紧凑的微调组件。这一方法为图像真实性评估提供了创新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07704" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 12:12:11 GMT</pubDate>
</item>
<item>
<title>R3框架：提升奖励模型的可控性和可解释性</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的奖励建模框架R3，提升语言模型与人类偏好的对齐能力。</p><br /><br /><p><strong>摘要：</strong> 当前的奖励模型虽对齐语言模型输出与人类偏好至关重要，但通常缺乏可控性和可解释性。这些模型多针对狭窄目标优化，难以泛化到更广泛的下游任务，且其标量输出难以解读。为解决这些问题，我们引入R3框架，这是一种不依赖评分标准、跨评估维度通用且提供可解释评分分配的新方法。R3实现了语言模型评估的透明度和灵活性，支持与多样化的价值观和应用场景的稳健对齐。相关模型、数据和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:29:03 GMT</pubDate>
</item>
<item>
<title>低秩克隆方法大幅提升小语言模型训练效率</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的高效预训练方法LRC，大幅提升小语言模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有小语言模型训练中的三大挑战——硬剪枝导致的信息丢失、表示对齐效率低下及对FFN信号利用不足，提出了名为Low-Rank Clone (LRC) 的高效预训练方法。该方法通过训练一组低秩投影矩阵实现教师权重的软剪枝与学生激活信号的对齐，包括FFN信号，从而构建行为上等同于强教师模型的小语言模型。实验显示，LRC在仅使用20B令牌的情况下达到了超越基于万亿令牌训练的SOTA模型的效果，提升了超过1000倍的训练效率。研究代码和模型检查点已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12781" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 03:10:42 GMT</pubDate>
</item>
<item>
<title>SEED-GRPO：基于语义熵的大语言模型不确定性感知优化</title>
<link>https://arxiv.org/abs/2505.12346</link>
<guid>https://arxiv.org/abs/2505.12346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对大语言模型在不同输入提示下的不确定性问题，提出SEED-GRPO方法提升数学推理任务表现。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在处理不同输入提示时表现出不同程度的置信度，这反映了模型对特定问题的理解不确定性。传统方法如Group Relative Policy Optimization (GRPO) 忽略了这一重要信息。为解决此问题，本文提出了SEED-GRPO，通过引入语义熵显式衡量LLMs对输入提示的不确定性。语义熵评估给定提示下多个生成答案的语义多样性，并据此调节策略更新的幅度。这种不确定性感知机制使政策更新幅度根据问题的不确定性动态调整，在高不确定性问题上采取更保守的更新策略，同时保留对确定性问题的原始学习信号。实验结果显示，在五个数学推理基准测试中（AIME24 56.7，AMC 68.7，MATH 83.4，Minerva 34.2，OlympiadBench 48.0），SEED-GRPO在平均准确率上达到新的最先进水平，验证了不确定性感知策略优化的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 06:20:59 GMT</pubDate>
</item>
<item>
<title>HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology</title>
<link>https://arxiv.org/abs/2505.12120</link>
<guid>https://arxiv.org/abs/2505.12120</guid>
<content:encoded><![CDATA[
Recent advancements in Digital Pathology (DP), particularly through artificial intelligence and Foundation Models, have underscored the importance of large-scale, diverse, and richly annotated datasets. Despite their critical role, publicly available Whole Slide Image (WSI) datasets often lack sufficient scale, tissue diversity, and comprehensive clinical metadata, limiting the robustness and generalizability of AI models. In response, we introduce the HISTAI dataset, a large, multimodal, open-access WSI collection comprising over 60,000 slides from various tissue types. Each case in the HISTAI dataset is accompanied by extensive clinical metadata, including diagnosis, demographic information, detailed pathological annotations, and standardized diagnostic coding. The dataset aims to fill gaps identified in existing resources, promoting innovation, reproducibility, and the development of clinically relevant computational pathology solutions. The dataset can be accessed at https://github.com/HistAI/HISTAI.
]]></content:encoded>
<pubDate>Sat, 17 May 2025 14:59:32 GMT</pubDate>
</item>
<item>
<title>HelpSteer3-Preference：高质量指令跟随语言模型偏好数据集</title>
<link>https://arxiv.org/abs/2505.11475</link>
<guid>https://arxiv.org/abs/2505.11475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">发布了一个包含4万多个样本的高质量偏好数据集，提升大语言模型训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了HelpSteer3-Preference，这是一个高许可性(CC-BY-4.0)且高质量的人类注释偏好数据集，包含超过40,000个样本，涵盖了科学、技术、工程、数学(STEM)、编码及多语言场景等实际应用领域。该数据集通过训练奖励模型(RMs)，在RM-Bench上取得了82.4%的出色成绩，在JudgeBench上也达到了73.7%，较现有最佳结果提升了约10个百分点。此外，我们展示了如何利用此数据集训练生成型奖励模型，并进一步将策略模型通过RLHF方法进行对齐优化。该数据集现可公开获取，适用于多种强化学习应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:31:19 GMT</pubDate>
</item>
<item>
<title>基于多模态观察的一般用户模型</title>
<link>https://arxiv.org/abs/2505.10831</link>
<guid>https://arxiv.org/abs/2505.10831</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种能从多模态交互中学习用户偏好的一般用户模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为一般用户模型（GUM）的新架构，该模型通过观察用户与计算机的任何互动来了解用户。GUM可以处理未经结构化的用户观察数据（如设备截图），并构建置信加权命题来捕捉用户的偏好和知识。实验表明，GUM不仅能够准确推断用户行为，还能通过增强聊天助手、管理操作系统通知和提供跨应用自适应代理等方式，实现人机交互的长期愿景。此外，还展示了GUM如何驱动主动助手（GUMBOs），自动为用户提供有用建议。评估结果显示，基于GUM的系统能有效识别并执行用户可能不会明确请求的操作。GUM引入的方法利用多模态模型理解非结构化上下文，推动了人机交互领域的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10831" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:31 GMT</pubDate>
</item>
<item>
<title>基于非配对数据学习的智能手机图像信号处理器优化</title>
<link>https://arxiv.org/abs/2505.10420</link>
<guid>https://arxiv.org/abs/2505.10420</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需配对数据的新型学习方法，提升手机ISP图像质量。</p><br /><br /><p><strong>摘要：</strong> 本文解决现代智能手机相机ISP开发中的一个难题，即获取像素级对齐的配对训练数据的高成本问题。通过引入一种新颖的非配对训练方法，该方法利用多损失项对抗训练，并结合多个判别器处理预训练网络的特征图，从而在保持内容结构的同时学习目标RGB数据集的颜色和纹理特性。实验表明，使用轻量级神经网络架构，在Zurich RAW到RGB和Fujifilm UltraISP数据集上评估时，该方法相较于传统的配对训练方法表现出了更强的潜力，并在多项评价指标上实现了高保真度。相关代码和预训练模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10420" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 11:37:51 GMT</pubDate>
</item>
<item>
<title>VSA：一种高效的可训练稀疏注意力机制</title>
<link>https://arxiv.org/abs/2505.13389</link>
<guid>https://arxiv.org/abs/2505.13389</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VSA稀疏注意力机制，显著降低视频扩散模型计算开销。</p><br /><br /><p><strong>摘要：</strong> 本文针对视频扩散变压器（DiTs）因全3D注意力导致的计算瓶颈问题，提出了VSA（可训练硬件高效稀疏注意力）。VSA通过轻量级粗阶段对tokens进行池化并识别关键tokens，再由细粒度阶段仅在这些关键tokens所在块内计算注意力，从而实现硬件效率提升和高效训练。实验表明，VSA在不影响扩散损失的前提下将训练浮点运算减少2.53倍，且在Wan-2.1模型上的应用显著加快了推理速度，同时保持了生成质量。这一成果确立了可训练稀疏注意力作为全注意力的实用替代方案，为视频扩散模型的进一步扩展提供了支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13389" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:30:13 GMT</pubDate>
</item>
<item>
<title>LatentSeek：通过测试时实例级适应提升大语言模型推理能力</title>
<link>https://arxiv.org/abs/2505.13308</link>
<guid>https://arxiv.org/abs/2505.13308</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用潜在空间的框架LatentSeek，显著提升大语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在实现通用人工智能（AGI）过程中面临的推理难题。尽管模型在训练规模上取得了进展，但在训练算法如灾难性遗忘及新颖训练数据获取方面仍存在挑战。为此，我们提出了LatentSeek，这是一种基于测试时实例级适应（TTIA）的新框架，在模型潜在空间中增强推理能力。具体而言，LatentSeek利用策略梯度迭代更新潜在表示，由自动生成的奖励信号引导。实验表明，无论是在GSM8K、MATH-500还是AIME2024等基准测试中，LatentSeek均优于基线方法，如思维链提示和微调方法。此外，LatentSeek具有高效性，在少数几次迭代后即可解决平均复杂度问题，同时还能从更多迭代中获益，展示了测试时潜在空间扩展的巨大潜力。这些发现使LatentSeek成为一种轻量级、可扩展且有效的LLMs推理能力增强解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13308" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 12:26:02 GMT</pubDate>
</item>
<item>
<title>OSWorld-G与Jedi：提升图形用户界面定位能力的新基准与数据集</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新基准OSWorld-G和大规模数据集Jedi，解决现有GUI定位任务过于简化的问题。</p><br /><br /><p><strong>摘要：</strong> 图形用户界面（GUI）定位技术是开发计算机代理的关键瓶颈，当前基准未能捕捉真实交互中的复杂性。为应对这一挑战，我们引入OSWorld-G，这是一个包含564个精心标注样本的综合基准，涵盖多种任务类型。同时，我们发布了Jedi，这是目前最大的计算机使用定位数据集，包含400万个示例。通过多视角分解任务，Jedi显著提升了现有模型的性能，在ScreenSpot-v2、ScreenSpot-Pro和OSWorld-G上表现优异。研究还表明，结合特定接口元素的数据可实现对新界面的组合泛化。所有资源均开源并提供访问链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13227" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 11:09:23 GMT</pubDate>
</item>
<item>
<title>ViPlan：视觉规划领域的首个开源基准测试</title>
<link>https://arxiv.org/abs/2505.13180</link>
<guid>https://arxiv.org/abs/2505.13180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究对比符号规划与直接使用视觉语言模型进行视觉规划的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ViPlan，这是首个用于视觉规划的开源基准测试，结合符号谓词和视觉语言模型(VLM)。ViPlan涵盖了两个领域中的挑战性任务：经典Blocksworld问题的视觉变体及模拟的家庭机器人环境。我们评估了多个开源VLM家族及其封闭模型，分别测试了基于VLM的符号规划方法与直接使用VLM提出动作的方法。结果显示，在需要精确图像定位的Blocksworld任务中，符号规划优于直接VLM规划；而在家庭机器人任务中，常识知识和错误恢复能力更为重要。此外，大多数模型和方法中，链式思维提示并未带来显著优势，表明当前VLM在视觉推理方面仍存在困难。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:38:15 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多语言机器翻译模型研究</title>
<link>https://arxiv.org/abs/2505.12996</link>
<guid>https://arxiv.org/abs/2505.12996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新奖励建模方法提升大模型在神经机器翻译中的性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）在复杂问题上展现了强大能力，但将其应用于神经机器翻译（MT）仍面临挑战。现有研究主要集中在高资源语言如英语和中文，且奖励建模方法未能充分发挥强化学习潜力。本研究设计了一种新的奖励建模方法，通过将翻译结果与强LRM对比量化评分提供奖励，显著提升了翻译质量。实验表明，使用Qwen2.5-7B-Instruct作为基础模型，在文学翻译中达到最新技术水平，并超越OpenAI-o1和DeepSeek-R1等强LRMs。此外，该方法扩展到11种语言的多语言设置，在轻量级奖励建模下实现90种翻译方向上的出色表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:34:47 GMT</pubDate>
</item>
<item>
<title>Fractured Sampling：提升大语言模型推理效率的新策略</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">截断CoT和Fractured Sampling方法显著提升了大语言模型推理的准确性和效率。</p><br /><br /><p><strong>摘要：</strong> 截断CoT（Chain-of-Thought prompting）方法在推理过程中提前终止生成中间推理路径，直接得出答案，相较于完整CoT，在大幅减少token消耗的同时保持了相似的准确性。基于这一发现，本文提出了一种名为Fractured Sampling的统一推理策略，该策略通过在三个正交维度上进行插值：推理轨迹数量、每条轨迹最终解的数量以及推理深度的截断位置，实现了更优的准确率-成本权衡。通过在五个多样化推理基准测试及多种模型规模下的广泛实验，证明了Fractured Sampling能够在Pass@k指标上获得显著的对数线性扩展收益。此外，研究还揭示了如何在这三个维度上分配计算资源以最大化性能，为大语言模型的高效可扩展推理奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:30:41 GMT</pubDate>
</item>
<item>
<title>解决同形异义词消歧挑战的半自动化方法及快速规则系统</title>
<link>https://arxiv.org/abs/2505.12973</link>
<guid>https://arxiv.org/abs/2505.12973</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">针对低资源语言中的同形异义词问题，提出新方法提升深度学习和规则系统的消歧准确性。</p><br /><br /><p><strong>摘要：</strong> 同形异义词消歧在基于图示到音素转换（G2P）中是一个重大挑战，尤其对低资源语言而言。主要困难在于构建平衡且全面的数据集耗时费力，而特定策略又引入延迟，不适合实时应用。本文提出解决方案：首先，通过半自动化管道创建同形异义词数据集HomoRich，并将其应用于增强波斯语的先进深度学习G2P系统；其次，倡导利用丰富离线数据开发适用于低延迟无障碍工具（如屏幕阅读器）的快速规则方法，改进规则系统eSpeak为HomoFast eSpeak。实验结果显示，深度学习和eSpeak系统的同形异义词消歧准确率提高了约30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12973" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 07:11:12 GMT</pubDate>
</item>
<item>
<title>语言如何在合作中演化？基于多智能体觅食游戏的研究</title>
<link>https://arxiv.org/abs/2505.12872</link>
<guid>https://arxiv.org/abs/2505.12872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示语言如何通过合作需求在多智能体系统中自然演化。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了人类语言如何从早期合作需求中演化而来。通过构建多智能体觅食游戏环境，模拟早期人类面临的认知和生态约束，我们发现智能体在协作过程中自发发展出具备自然语言特征的交流协议，如任意性、可互换性、位移性、文化传递性和构成性。实验表明，群体规模和时间依赖性等因素对语言特性有显著影响。我们的框架为研究语言在具身化多智能体系统中的演化提供了平台，并计划公开所有数据、代码和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 04:57:30 GMT</pubDate>
</item>
<item>
<title>通过结构化上下文条件化增强大语言模型在科学文档验证中的准确性</title>
<link>https://arxiv.org/abs/2505.12257</link>
<guid>https://arxiv.org/abs/2505.12257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示通过调整大语言模型的上下文条件可以提高其对科学文档中技术错误的检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种基于Persistent Workflow Prompting（PWP）原则的结构化大语言模型（LLM）上下文条件化方法，旨在解决LLM在复杂科学和技术文档验证中的局限性，特别是那些需要多模态解释的任务。实验集中在验证一篇包含已知文本和图像错误的化学测试论文上，结果显示基本提示策略效果不佳，但采用PWP结构调整LLM分析心态的方法显著提升了Gemini 2.5 Pro模型在文本错误检测上的表现，并使其成功识别出之前手动审查中忽略的图像公式错误，而ChatGPT Plus o3在类似测试中未能完成此任务。这些初步结果表明该方法有助于开发更可靠的LLM驱动分析流程，尤其适用于需要细致误差检测的场景。然而，未来仍需更大范围的验证来确认其普适性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 02:33:08 GMT</pubDate>
</item>
<item>
<title>大规模预训练中的模型合并技术研究</title>
<link>https://arxiv.org/abs/2505.12082</link>
<guid>https://arxiv.org/abs/2505.12082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示模型合并可显著提升大语言模型性能并降低训练成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在大规模预训练过程中模型合并技术的应用，通过密集型及Mixture-of-Experts(MoE)架构的实验表明，采用恒定学习率训练的检查点合并不仅提升了模型性能，还预测了退火行为。这些改进提高了模型开发效率并降低了训练成本。此外，对合并策略和超参数的消融研究揭示了潜在机制并发现新应用，为开源社区提供了实用的预训练指导。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 12:53:14 GMT</pubDate>
</item>
<item>
<title>Tiny QA Benchmark++：轻量级多语言模型安全测试套件</title>
<link>https://arxiv.org/abs/2505.12058</link>
<guid>https://arxiv.org/abs/2505.12058</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">TQB++提供轻量级多语言测试套件，快速检测大语言模型的安全性问题。</p><br /><br /><p><strong>摘要：</strong> Tiny QA Benchmark++（TQB++）是一个超轻量级、多语言的测试套件，旨在为大型语言模型（LLM）的流水线提供类似单元测试的安全保障数据集。它由Comet Opik提示优化SDK的开发需求催生，后者对快速反馈循环有极高要求，而传统重型基准测试会破坏开发流程。TQB++包含一个52项的英语黄金数据集（小于20kB），并结合了一个基于供应商不可知的LiteLLM构建的小型合成数据生成器PyPI包。该生成器允许用户在任何语言、领域或难度下创建自己的微型数据包，同时已准备好十个涵盖阿拉伯语、中文、法语等十种语言的现成数据包。每个数据集都带有Croissant元数据和即插即用文件，适用于OpenAI-Evals、LangChain和标准CI工具，使得团队可以直接将确定性的微基准测试集成到拉取请求门禁、提示工程循环和生产仪表板中，而无需占用GPU预算。完整运行TQB++仅需几秒钟，但可以提前检测提示模板错误、标记器漂移和微调副作用，比像MMLU或BIG-Bench这样的全规模套件更快发现问题。整个框架被开源，以加速生成式人工智能生态系统的持续、资源高效的质量保证。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12058" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 11:40:03 GMT</pubDate>
</item>
<item>
<title>基于RAG框架的领域特定恶意技术识别方法</title>
<link>https://arxiv.org/abs/2505.11988</link>
<guid>https://arxiv.org/abs/2505.11988</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合检索增强生成的恶意技术识别新框架。</p><br /><br /><p><strong>摘要：</strong> 现有的恶意技术识别方法面临模型精度与资源消耗之间的权衡问题，本文提出了一种名为TechniqueRAG的领域特定检索增强生成框架，通过整合现成的检索器、指令微调的大语言模型及少量文本-技术对，解决了数据稀缺问题，同时通过零样本LLM重排序提升检索质量与领域特定精度。实验表明，该方法在多个安全基准测试中达到最先进的性能，且无需大量任务特定优化或标记数据。这项研究为网络防御提供了高效的技术支持，同时揭示了进一步改进的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11988" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 08:46:10 GMT</pubDate>
</item>
<item>
<title>大型语言模型在学术验证中的局限性研究</title>
<link>https://arxiv.org/abs/2505.11855</link>
<guid>https://arxiv.org/abs/2505.11855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示当前大型语言模型在学术验证中的表现远未达到可靠水平。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）的进步激发了自动化科学发现的愿景，但以往的研究多将其视为生成假设或撰写论文的合著者。本研究探索了一种互补的应用场景：将LLMs作为学术验证的核查工具。为此，我们构建了一个名为SPOT的数据集，该数据集包含83篇已发表论文及其91处足以引发勘误或撤稿的重要错误，并通过作者和人工标注进行交叉验证。评估结果显示，最先进的LLMs在召回率和精确度上分别仅为21.1%和6.1%，且置信度普遍较低，多次运行间难以重现相同错误，表明其可靠性不足。此外，领域专家的定性分析显示，即使是最强的模型也会犯类似于学生水平的误解错误。这些发现揭示了当前LLM能力与可靠学术验证需求之间存在巨大差距。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 01:45:16 GMT</pubDate>
</item>
<item>
<title>QVGen：面向极低比特量化视频扩散模型的高效推理框架</title>
<link>https://arxiv.org/abs/2505.11497</link>
<guid>https://arxiv.org/abs/2505.11497</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QVGen框架，显著提升低比特量化视频扩散模型的性能与效率。</p><br /><br /><p><strong>摘要：</strong> 视频扩散模型在高质量视频合成方面表现出色，但其高昂的计算与内存需求限制了实际应用。尽管图像扩散模型通过量化技术有效降低了成本，但直接应用于视频扩散模型效果不佳。本文提出QVGen，一种针对极低比特量化（如4比特及以下）的量化感知训练框架，旨在提高视频扩散模型的高性能与推理效率。通过理论分析证明梯度范数减小对量化感知训练至关重要，并引入辅助模块缓解大量化误差，同时采用秩衰减策略逐步消除这些模块的推理开销。实验表明，在四种最先进的视频扩散模型上，QVGen首次实现了4比特设置下接近全精度的质量，并在多个性能指标上超越现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11497" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:59:40 GMT</pubDate>
</item>
<item>
<title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.11484</link>
<guid>https://arxiv.org/abs/2505.11484</guid>
<content:encoded><![CDATA[
Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:47:50 GMT</pubDate>
</item>
<item>
<title>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</title>
<link>https://arxiv.org/abs/2505.10238</link>
<guid>https://arxiv.org/abs/2505.10238</guid>
<content:encoded><![CDATA[
Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 08:50:29 GMT</pubDate>
</item>
<item>
<title>基于Persistent Workflow Prompting的科学手稿评审方法研究</title>
<link>https://arxiv.org/abs/2505.03332</link>
<guid>https://arxiv.org/abs/2505.03332</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过Prompt工程提升大语言模型科学评审能力的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Persistent Workflow Prompting（PWP）的方法，旨在利用标准的大语言模型聊天接口解决科学手稿评审中的复杂性和数据限制问题。PWP通过分层模块化架构定义详细的分析工作流，采用迭代元提示和元推理技术系统化编码专家评审流程，包括隐性知识。实验表明，PWP引导下的大语言模型能够识别实验化学论文中的主要方法学缺陷，并有效执行复杂任务如区分主张与证据、整合文本/图片/图表分析、执行定量可行性检查等。该方法不仅展示了大语言模型在复杂科学任务中的潜力，还提供了透明且可复制的研究资源。除了具体应用外，这项工作还揭示了元开发过程本身的见解，强调了通过详细工作流形式化实现复杂科学任务分析的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03332" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:06:18 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型在图表理解中的视觉推理能力挑战</title>
<link>https://arxiv.org/abs/2505.13444</link>
<guid>https://arxiv.org/abs/2505.13444</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示大型视觉语言模型在复杂图表理解中的视觉推理短板。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型视觉语言模型（LVLMs）在图表理解中的独特挑战，即需要整合复杂的文本和视觉推理能力。通过合成数据集的研究表明，模型性能在视觉复杂度增加时显著下降，而人类表现依然稳健。为此，我们提出了ChartMuseum，这是一个新的图表问答基准，包含来自184个真实来源的1,162个专家标注的问题，旨在评估复杂的视觉和文本推理能力。与现有基准不同，我们的基准显示出模型与人类表现之间存在显著差距，同时有效区分了模型的能力。尽管人类达到了93%的准确率，但最佳模型Gemini-2.5-Pro仅达到63.0%，而开源模型Qwen2.5-VL-72B-Instruct仅为38.5%。此外，在主要依赖视觉推理的问题上，所有模型的表现比文本推理问题下降了35%-55%。最后，我们的定性误差分析揭示了当前LVLMs在特定视觉推理类别上的挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13444" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:59:27 GMT</pubDate>
</item>
<item>
<title>FinePhys：结合物理学的人类动作生成框架</title>
<link>https://arxiv.org/abs/2505.13437</link>
<guid>https://arxiv.org/abs/2505.13437</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合物理学的人类动作生成框架，显著提高复杂人体动作的生成质量。</p><br /><br /><p><strong>摘要：</strong> 尽管视频生成技术取得了显著进步，但生成物理上合理的精细人类动作仍是一个挑战。本文介绍FinePhys框架，它通过引入物理学原理提供骨骼指导，解决现有方法在处理精细语义和复杂时间动态方面的不足。FinePhys首先在线估计2D姿态，然后利用上下文学习进行2D到3D维度提升，并通过基于欧拉-拉格朗日方程的运动重估模块增强稳定性。该框架在FineGym数据集的三个子集上表现优异，生成的动作更加自然且符合物理规律。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13437" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:58:11 GMT</pubDate>
</item>
<item>
<title>通过过程奖励模型提升多模态推理逻辑一致性</title>
<link>https://arxiv.org/abs/2505.13427</link>
<guid>https://arxiv.org/abs/2505.13427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MM-PRM模型解决多模态大语言模型推理不一致问题。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多模态大型语言模型在复杂多步推理中的局限性，即缺乏对中间推理步骤的精细监督。为了解决这一问题，研究团队提出了MM-PRM（Process Reward Model），该模型在一个完全自动化且可扩展的框架内进行训练。首先，构建了MM-Policy，这是一个在多样化数学推理数据上训练的强大多模态模型；接着，创建了MM-K12数据集，包含10,000个具有可验证答案的多模态数学问题作为种子数据。利用基于蒙特卡洛树搜索（MCTS）的管道，自动生成超过70万条步骤级标注，无需人工标记。最终的PRM用于最佳N推断设置中评分候选推理路径，在域内（如MM-K12测试集）和域外基准（如OlympiadBench、MathVista等）均取得了显著改进。进一步分析表明，软标签、较小的学习率和路径多样性对优化PRM性能至关重要。MM-PRM证明了过程监督是增强多模态推理系统逻辑鲁棒性的强大工具，并开源了所有代码和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:55:08 GMT</pubDate>
</item>
<item>
<title>AdaptThink：一种基于强化学习的自适应推理模式优化算法</title>
<link>https://arxiv.org/abs/2505.13417</link>
<guid>https://arxiv.org/abs/2505.13417</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AdaptThink算法，通过自适应选择推理模式提升模型效率与性能。</p><br /><br /><p><strong>摘要：</strong> 本文研究了大型推理模型在多种任务中的表现，发现NoThinking（跳过推理直接输出答案）在简单任务中更具性能与效率优势。受此启发，我们提出了AdaptThink，一种新的强化学习算法，使推理模型能够根据问题难度自适应选择最佳推理模式。该算法包含两个核心组件：约束优化目标与重要性采样策略，实验表明其显著降低了推理成本并提升了性能，在三个数学数据集上将DeepSeek-R1-D1-Qwen-1.5B模型的平均响应长度减少53%，同时提高了2.4%的准确性。这些成果展示了自适应推理模式选择在平衡推理质量和效率方面的潜力。相关代码和模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13417" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:50:52 GMT</pubDate>
</item>
<item>
<title>Thinkless：让语言模型学会何时需要推理</title>
<link>https://arxiv.org/abs/2505.13379</link>
<guid>https://arxiv.org/abs/2505.13379</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Thinkless框架，使LLMs根据任务复杂度自适应选择推理方式。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了具备扩展链式推理能力的语言模型在处理所有查询时可能面临计算效率低下的问题，特别是当许多问题可以简单解决时。为了解决这一问题，我们提出了Thinkless，这是一个可学习的框架，允许语言模型根据任务复杂性和自身能力自适应地选择简短形式或长链推理形式。Thinkless通过强化学习进行训练，使用两个控制令牌分别用于简洁响应和详细推理。核心算法Decoupled Group Relative Policy Optimization (DeGRPO) 将混合推理的学习目标分解为两个部分：控制令牌损失和响应损失。这种方法实现了对每个目标贡献的精细控制，稳定了训练过程并有效防止了传统GRPO中的崩溃现象。实证结果显示，在Minerva Algebra、MATH-500和GSM8K等基准测试中，Thinkless将长链推理的使用减少了50%-90%，显著提高了推理语言模型的效率。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13379" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 13:24:16 GMT</pubDate>
</item>
<item>
<title>混合3D-4D高斯泼溅技术提升动态场景重建效率</title>
<link>https://arxiv.org/abs/2505.13215</link>
<guid>https://arxiv.org/abs/2505.13215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种混合方法，在静态区域使用3D高斯模型提高效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为混合3D-4D高斯泼溅(3D-4DGS)的新框架，用于动态场景重建。传统4D高斯泼溅方法因对静态区域分配过多高斯函数导致计算和内存开销大且图像质量下降。3D-4DGS框架通过将时间不变的高斯函数转换为3D模型，减少参数数量并提高效率，同时保留动态元素的4D表示以捕捉复杂运动。实验表明，该方法在训练速度上显著快于基准方法，且视觉质量不降低甚至有所提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.13215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 10:59:58 GMT</pubDate>
</item>
<item>
<title>通过GS-Jacobi优化加速TarFlow图像生成模型采样</title>
<link>https://arxiv.org/abs/2505.12849</link>
<guid>https://arxiv.org/abs/2505.12849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法大幅加速TarFlow图像生成模型采样，同时保持生成质量。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于Transformer的图像生成模型如TarFlow在多个基准测试中取得了最先进的成果。然而，由于其因果注意力机制需要顺序计算，导致采样过程极其缓慢。本文提出了一种基于Gauss-Seidel-Jacobi迭代的新方法，通过引入收敛排名度量（CRM）和初始猜测度量（IGM），有效区分不同块的重要性及对初始值的敏感性。实验表明，在Img128cond、AFHQ、Img64uncond和Img64cond四个模型上，该方法分别实现了4.53倍、5.32倍、2.96倍和2.51倍的采样加速，且FID分数和样本质量未受影响。代码和检查点已开源。关键词：图像生成、TarFlow、GS-Jacobi。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 04:35:44 GMT</pubDate>
</item>
<item>
<title>FedSVD：通过SVD实现联邦学习中高效的低秩适配</title>
<link>https://arxiv.org/abs/2505.12805</link>
<guid>https://arxiv.org/abs/2505.12805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FedSVD通过全局重新参数化解决了LoRA在差分隐私下的噪声放大问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了在联邦学习中低秩适配（LoRA）方法的高效微调问题。然而，当LoRA与差分隐私随机梯度下降（DP-SGD）结合时，会面临显著的噪声放大挑战。为了解决这一问题，我们提出了FedSVD，这是一种基于奇异值分解（SVD）的简单而有效的全局重新参数化方法。在FedSVD中，客户端仅优化B矩阵并将结果发送到服务器，服务器通过SVD重新因子化得到新的自适应A矩阵和更新后的B矩阵。该方法避免了二次噪声放大，同时增强了模型表达能力。理论分析表明，这种正交结构可以限制B的梯度范数并保留更多信号。实验结果显示，FedSVD在各种隐私设置和基准测试中表现出色，在私有和非私有环境下均优于相关基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 19 May 2025 03:32:56 GMT</pubDate>
</item>
<item>
<title>Clipped Policy Gradient Optimization with Policy Drift (CPGD) 提升语言模型强化学习稳定性</title>
<link>https://arxiv.org/abs/2505.12504</link>
<guid>https://arxiv.org/abs/2505.12504</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新算法CPGD，解决基于规则的强化学习训练不稳定性问题。</p><br /><br /><p><strong>摘要：</strong> 近期基于规则的强化学习方法显著提升了语言模型的推理能力，但现有方法如GRPO、REINFORCE++和RLOO常因训练不稳定而失败。本文提出了一种名为Clipped Policy Gradient Optimization with Policy Drift (CPGD)的新算法，通过引入基于KL散度的策略漂移约束和对数比率的裁剪机制，动态正则化策略更新，防止过度策略更新，从而解决训练崩溃问题。理论分析表明，CPGD不仅提高了性能，还保持了训练稳定性。实验验证了该方法的有效性，同时代码已开源，为后处理阶段的语言模型强化学习提供了稳健的替代方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12504" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 May 2025 13:44:53 GMT</pubDate>
</item>
<item>
<title>VisionReasoner：统一视觉推理框架在多任务感知中的卓越表现</title>
<link>https://arxiv.org/abs/2505.12081</link>
<guid>https://arxiv.org/abs/2505.12081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisionReasoner通过多对象认知学习和任务重铸，在多个视觉任务中表现出色。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为VisionReasoner的统一框架，该框架能够在一个共享模型中进行推理并解决多种视觉感知任务。通过设计新颖的多对象认知学习策略和系统化的任务重构方法，VisionReasoner增强了对视觉输入的分析能力，并在检测、分割和计数三个关键领域内的十个多样化任务上进行了评估。实验结果显示，相比Qwen2.5VL，VisionReasoner在COCO检测任务上提升了29.1%，在ReasonSeg分割任务上提升了22.1%，在CountBench计数任务上提升了15.3%，展现了其作为统一模型的强大性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.12081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 12:51:47 GMT</pubDate>
</item>
<item>
<title>QCompiler：通过神经符号框架提升检索增强生成系统的复杂查询处理能力</title>
<link>https://arxiv.org/abs/2505.11932</link>
<guid>https://arxiv.org/abs/2505.11932</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QCompiler框架，有效解决资源受限下复杂嵌套查询的检索增强生成难题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为QCompiler的神经符号框架，旨在改进检索增强生成（RAG）系统对复杂查询意图的精确识别。传统的RAG系统在处理复杂查询时面临挑战，尤其是在资源有限的情况下。QCompiler基于Backus-Naur形式（BNF）语法设计了一种最小且充分的形式化规则，能够完整表达复杂查询同时减少冗余。该框架包含查询表达翻译器、词法语法解析器和递归下降处理器，可将查询编译为抽象语法树（AST），从而实现更精确的文档检索和响应生成。实验表明，QCompiler显著提升了RAG系统应对复杂查询的能力，特别是在资源受限场景下的性能表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11932" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 05:36:03 GMT</pubDate>
</item>
<item>
<title>AdaCoT：通过自适应推理提升大型语言模型效率</title>
<link>https://arxiv.org/abs/2505.11896</link>
<guid>https://arxiv.org/abs/2505.11896</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AdaCoT框架通过自适应决定是否采用Chain-of-Thought提示，显著降低复杂推理任务的计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）虽然展现了卓越的能力，但在需要复杂推理的任务上常面临挑战。尽管Chain-of-Thought（CoT）提示大幅提升了推理能力，但它对所有查询都生成冗长推理步骤，导致计算成本高且效率低下，尤其是对于简单输入。为解决这一问题，我们提出了AdaCoT（自适应Chain-of-Thought），这是一种新框架，使LLMs能够根据需要灵活决定是否调用CoT。我们将自适应推理建模为帕累托优化问题，旨在平衡模型性能与CoT调用相关的成本（频率和计算开销）。我们提出了一种基于强化学习的方法，具体使用Proximal Policy Optimization（PPO），通过调整惩罚系数动态控制CoT触发决策边界，从而使模型能够根据隐式的查询复杂性判断是否需要CoT。技术贡献之一是Selective Loss Masking（SLM），用于对抗多阶段强化学习训练中的决策边界坍塌，确保自适应触发的鲁棒性和稳定性。实验结果显示，AdaCoT成功导航帕累托前沿，在不需要复杂推理的查询中显著减少了CoT的使用。例如，在生产流量测试集中，AdaCoT将CoT触发率降至仅3.18%，平均响应令牌数减少69.06%，同时在复杂任务上保持高性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11896" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 May 2025 04:27:00 GMT</pubDate>
</item>
<item>
<title>Chain-of-Model Learning for Language Model</title>
<link>https://arxiv.org/abs/2505.11820</link>
<guid>https://arxiv.org/abs/2505.11820</guid>
<content:encoded><![CDATA[
In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.
]]></content:encoded>
<pubDate>Sat, 17 May 2025 00:06:12 GMT</pubDate>
</item>
<item>
<title>通过校正分布偏移提升Transformer稀疏注意力性能</title>
<link>https://arxiv.org/abs/2505.11254</link>
<guid>https://arxiv.org/abs/2505.11254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法纠正稀疏注意力的分布偏移，显著提升性能且保持高效率。</p><br /><br /><p><strong>摘要：</strong> Transformer模型中的注意力机制具有二次复杂度，在长序列推理时会导致高昂的成本和延迟。尽管可以通过稀疏计算降低计算负担，但通常会带来性能下降的问题。我们发现这种性能下降的原因在于稀疏计算导致注意力输出的分布发生偏移，从而影响解码查询与prefill阶段关键键值的对齐效果。为此，我们提出了一种简单而有效的方法来校正这一分布偏移，使稀疏注意力输出的分布更接近全量二次注意力的分布。该方法可与任意稀疏注意力方法结合使用，在基于滑动窗口注意力及sink token的应用中，平均提升了36个百分点的性能，恢复了88%的全量二次注意力精度。同时，我们的方法维持了约98.5%的全量二次注意力稀疏性，在处理100万token预填充时比Flash Attention 2快32倍。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 09:48:33 GMT</pubDate>
</item>
<item>
<title>基于自然语言指令的图像编辑模型评估基准GIE-Bench</title>
<link>https://arxiv.org/abs/2505.11493</link>
<guid>https://arxiv.org/abs/2505.11493</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的基准GIE-Bench，用于评估文本引导图像编辑模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为GIE-Bench的新基准，旨在通过功能正确性和内容保存两个关键维度更精确地评估文本引导的图像编辑模型性能。功能正确性通过自动生成的选择题验证目标修改是否成功应用；内容保存则利用对象感知掩码技术确保非目标区域视觉一致性。该基准包含超过1000个高质量编辑示例，涵盖20个多样化内容类别。研究对比了最新旗舰模型GPT-Image-1与其他先进编辑模型，发现GPT-Image-1在指令遵循准确性上表现最佳，但常过度修改无关区域，揭示了当前模型行为的重要权衡。GIE-Bench提供了一个可扩展且可重复的框架，推动文本引导图像编辑更准确的评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11493" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:55:54 GMT</pubDate>
</item>
<item>
<title>uLLSAM：利用多模态大语言模型提升显微镜跨域图像分割性能</title>
<link>https://arxiv.org/abs/2505.10769</link>
<guid>https://arxiv.org/abs/2505.10769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入视觉-语言知识，uLLSAM显著提升了显微镜图像的分割性能。</p><br /><br /><p><strong>摘要：</strong> 准确分割生物医学图像中的感兴趣区域具有重要意义。然而，现有基础模型在未见过的领域数据上表现不佳，主要因为缺乏分割前的视觉-语言知识。受多模态大语言模型的理解和推理能力启发，我们提出了一种名为uLLSAM的新方法，该方法通过视觉-语言语义对齐模块将视觉-语言知识注入到Segment Anything Model（SAM）中，并进一步通过语义边界正则化模块优化边界轮廓感知。实验表明，uLLSAM在9个领域内显微镜数据集上的Dice和SA指标分别提高了7.71%和12.10%，并在10个跨领域数据集上分别提高了6.79%和10.08%，达到了最先进的性能。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 20:55:56 GMT</pubDate>
</item>
<item>
<title>结合文本与图像结构引导的文生图模型</title>
<link>https://arxiv.org/abs/2505.05678</link>
<guid>https://arxiv.org/abs/2505.05678</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合LLM实例级指令和图像结构初始化的文生图技术。</p><br /><br /><p><strong>摘要：</strong> 尽管生成模型的能力迅速提升，但现有的文本到图像模型在处理复杂提示时仍难以准确捕捉语义。为解决这一问题，研究者开始尝试引入粗略边界框等结构性约束来指导生成过程。本文进一步推进这一思路，利用当代图像生成模型直接提供精细的结构性初始化，并结合基于大型语言模型的实例级指令，从而生成符合文本提示中对象数量、实例级属性及实例间空间关系的图像。这种方法显著提高了生成图像对复杂提示的适应性，为文生图技术提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05678" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 18:31:23 GMT</pubDate>
</item>
<item>
<title>大型语言模型在汇编代码优化中的强化学习应用</title>
<link>https://arxiv.org/abs/2505.11480</link>
<guid>https://arxiv.org/abs/2505.11480</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，大型语言模型通过强化学习可显著提升汇编代码性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了大型语言模型（LLMs）在优化汇编代码性能方面的潜力。通过引入基于Proximal Policy Optimization（PPO）的强化学习框架，并结合功能正确性和执行性能的奖励函数，我们开发了Qwen2.5-Coder-7B-PPO模型。该模型在由8,072个真实世界程序组成的基准测试中表现出色，测试通过率达96.0%，平均速度比gcc -O3基线快1.47倍，优于其他20个评估模型。实验结果表明，强化学习可以释放LLMs在汇编代码优化中的巨大潜能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11480" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 13:40:45 GMT</pubDate>
</item>
<item>
<title>结合先验知识的实时优化风格迁移方法提升音频效果转换</title>
<link>https://arxiv.org/abs/2505.11315</link>
<guid>https://arxiv.org/abs/2505.11315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入高斯先验改进风格迁移模型，显著提升音频效果转换质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种结合先验知识的风格迁移方法（ST-ITO），通过在参数空间引入基于DiffVox数据集的高斯先验，优化风格嵌入空间中的距离，从而改善音频效果转换的逼真度与可靠性。实验表明，该方法在MedleyDB数据集上的表现优于现有基线，特别是在均方误差减少及参考风格匹配方面取得了显著进步。此外，主观评估进一步验证了新方法在有限数据条件下的优越性。本研究强调了在推理阶段引入先验知识对提高音频处理系统性能的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 10:40:31 GMT</pubDate>
</item>
<item>
<title>CheXGenBench：医学影像生成模型的综合评估框架</title>
<link>https://arxiv.org/abs/2505.10496</link>
<guid>https://arxiv.org/abs/2505.10496</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种综合评估医学影像生成模型的新框架CheXGenBench。</p><br /><br /><p><strong>摘要：</strong> CheXGenBench是一种针对合成胸部X光图像生成模型的全面评估框架，它同时考量了模型的真实性、隐私风险和临床实用性。传统医学领域的AI评估存在方法不一致、架构比较过时及评估标准脱节等问题，忽视了样本的实际临床价值。CheXGenBench通过标准化的数据划分和统一的评估协议解决了这些问题，该协议包含了超过20个定量指标，系统分析了11种领先文本到图像生成架构的质量、潜在隐私漏洞及下游临床适用性。研究揭示了现有评估协议中的不足，特别是在生成真实性的评估上。此框架为医学AI社区提供了一个标准化基准，促进了客观可重复的比较，并支持未来模型的无缝集成。此外，还发布了SynthCheX-75K数据集，包含由表现最佳模型生成的75,000张高质量合成X光片。CheXGenBench建立了新的行业标杆，并公开了相关资源和数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10496" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:59:17 GMT</pubDate>
</item>
<item>
<title>一种结合Logits与采样策略的大语言模型鲁棒水印框架</title>
<link>https://arxiv.org/abs/2505.09924</link>
<guid>https://arxiv.org/abs/2505.09924</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种融合两类主流水印方案的新框架，显著提升检测与安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对大语言模型（LLMs）文本滥用问题，整合logits-based和sampling-based水印技术，设计了一种新颖的共生水印框架，包含串行、并行及混合三种策略。该框架通过自适应嵌入机制优化检测能力、鲁棒性、文本质量和安全性之间的平衡。研究团队通过广泛实验验证了方法的有效性，在多个数据集和模型上取得了最先进的性能表现。这项工作为未来水印技术发展提供了新视角，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09924" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 23:12:36 GMT</pubDate>
</item>
<item>
<title>Mergenetic：用于语言模型的开源进化模型合并库</title>
<link>https://arxiv.org/abs/2505.11427</link>
<guid>https://arxiv.org/abs/2505.11427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Mergenetic结合进化算法提升语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 模型合并是一种无需额外训练即可将现有模型能力整合到新模型中的方法，因其低成本和对消费者GPU的支持而受到欢迎。最新研究表明，将模型合并与进化算法结合可进一步提高性能，但目前尚无框架支持灵活实验。本文介绍了一个名为Mergenetic的开源库，该库专门针对语言模型设计，支持轻松组合不同的合并方法和进化算法，并引入轻量级适应度评估器以降低评估成本。通过描述其设计并展示在多种任务和语言上的竞争力，Mergenetic证明了其在有限硬件条件下仍能取得良好效果的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 12:43:23 GMT</pubDate>
</item>
<item>
<title>视觉规划：基于视觉表示的推理新范式</title>
<link>https://arxiv.org/abs/2505.11409</link>
<guid>https://arxiv.org/abs/2505.11409</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的视觉规划范式，通过纯视觉表示进行推理。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型（LLMs）及其多模态扩展（MLLMs）显著提升了跨任务的机器推理能力，但这些模型主要依赖纯文本表达推理，即便在存在视觉信息的情况下亦如此。本文指出语言并非在所有情况下最自然或有效的推理模式，特别是在涉及空间和几何信息的任务中。因此，我们提出了视觉规划这一新范式，它通过纯粹的视觉表示执行规划，无需依赖文本。在这一范式下，规划通过图像序列完成，每一步推理均在视觉领域编码，类似于人类的草图或可视化未来行动的方式。我们引入了视觉规划强化学习框架（VPRL），借助GRPO对大规模视觉模型进行后训练，显著提升了在代表性视觉导航任务（如FrozenLake、Maze和MiniBehavior）中的规划性能。实验结果显示，视觉规划在所有其他仅限文本空间的推理变体中表现最优，确立了其作为语言推理替代方案的可行性和潜力，为直观的图像推理任务开辟了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11409" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 12:17:22 GMT</pubDate>
</item>
<item>
<title>解决类别和空间不平衡问题的密集手部接触估计框架</title>
<link>https://arxiv.org/abs/2505.11152</link>
<guid>https://arxiv.org/abs/2505.11152</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架解决手部接触估计中的类别和空间不平衡问题。</p><br /><br /><p><strong>摘要：</strong> 手部与世界的交互对于人类至关重要，但有效学习密集手部接触估计仍面临挑战。主要问题包括类别不平衡（大部分样本无接触）和空间不平衡（接触多集中于指尖）。为解决这些问题，本文提出了一种名为HACO的新框架，通过平衡接触采样和顶点级类别均衡损失函数，有效预测大规模手部接触数据的密集接触估计，且不受到类别和空间不平衡的影响。该方法不仅提升了模型性能，还为未来研究提供了宝贵资源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11152" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 07:54:25 GMT</pubDate>
</item>
<item>
<title>大语言模型推理能力增强研究：开放领域问答中的测试时扩展</title>
<link>https://arxiv.org/abs/2505.11140</link>
<guid>https://arxiv.org/abs/2505.11140</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究表明增加推理链长度可提高大语言模型在开放领域问答中的事实准确性。</p><br /><br /><p><strong>摘要：</strong> 本研究通过分析复杂开放领域问答任务中的大语言模型推理能力，发现较短推理链的小型模型在经过微调后，其事实准确性显著优于原始指令调优模型。此外，引入知识图谱路径作为补充信息进一步增强了推理痕迹的丰富性。实验设置涵盖了六组基准数据集，超过22.6K个问题，并进行了168次实验，分析约170万条推理痕迹。结果表明，在单次运行中，通过添加测试时计算资源和令牌预算，事实准确性普遍提升2-8%，验证了测试时扩展的有效性。本研究公开所有实验材料供后续研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11140" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 07:39:33 GMT</pubDate>
</item>
<item>
<title>Group Think：基于单一大语言模型的并发推理新范式</title>
<link>https://arxiv.org/abs/2505.11107</link>
<guid>https://arxiv.org/abs/2505.11107</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型并发推理方法，使单个大语言模型能同时扮演多个推理代理，提升效率并降低延迟。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在推理能力方面取得了显著进步，但传统的多推理代理通常采用轮流交互的方式，这虽然提高了推理质量，却带来了较高的延迟。本文介绍了一种名为Group Think的新方法，它通过让单一LLM充当多个并发推理代理，在保持高质量的同时大幅降低了延迟。Group Think中的推理代理共享彼此的部分生成进度，从而能够在令牌级别动态适应对方，减少冗余推理。此外，这种并发机制能够更好地利用计算资源，特别适用于边缘推理场景。我们还提供了一个简单的修改方案，使现有的LLM能够在本地GPU上执行Group Think，并设计了一种评估策略，证明了这种方法在开源LLM上的延迟改进效果。这项研究为未来LLMs实现更复杂、更高效的协作行为奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11107" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 06:40:35 GMT</pubDate>
</item>
<item>
<title>GuardReasoner-VL：基于推理的视觉语言模型安全增强方法</title>
<link>https://arxiv.org/abs/2505.11049</link>
<guid>https://arxiv.org/abs/2505.11049</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出一种基于在线强化学习的视觉语言模型安全增强方法GuardReasoner-VL。</p><br /><br /><p><strong>摘要：</strong> 为了提高视觉语言模型(VLM)的安全性，本文提出了一种名为GuardReasoner-VL的新颖推理型VLM保护模型。该模型通过在线强化学习激励保护模型在做出审核决策前进行深思熟虑的推理。首先构建了一个包含12.3万个样本和63.1万个推理步骤的GuardReasoner-VLTrain推理语料库，涵盖文本、图像及图文输入。接着通过SFT冷启动模型的推理能力。此外，通过在线强化学习进一步增强审核中的推理。具体而言，在拒绝采样后，利用所提出的安全性感知数据连接进行数据增强，同时采用动态裁剪参数促进早期探索和后期利用。为平衡性能和令牌效率，设计了考虑准确性、格式和令牌成本的长度感知安全性奖励。实验表明，GuardReasoner-VL在F1得分上平均比亚军高出19.27%，并开源了相关数据、代码和模型(3B/7B)。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11049" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 05:46:10 GMT</pubDate>
</item>
<item>
<title>人类在博弈实验中对大型语言模型对手的行为差异研究</title>
<link>https://arxiv.org/abs/2505.11011</link>
<guid>https://arxiv.org/abs/2505.11011</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">人类在与大型语言模型博弈时选择更低数字，主要由高战略推理能力者驱动。</p><br /><br /><p><strong>摘要：</strong> 本研究通过首个受货币激励的实验室实验，探讨人类在多人p-beauty博弈中面对人类对手与大型语言模型（LLMs）对手时的行为差异。采用被试内设计，结果显示，与对抗人类相比，人类参与者对抗LLMs时选择显著更低的数字，这种现象主要源于零纳什均衡选择的增加。此转变主要由具有较高战略推理能力的参与者推动，他们通过引用LLMs的推理能力和合作倾向来解释策略选择。研究揭示了混合人类-LLMs系统机制设计的重要启示，并展示了参与者的异质性行为及对LLMs行为信念的变化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.11011" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 05:01:09 GMT</pubDate>
</item>
<item>
<title>基于多视角搜索的自动化定理证明系统MPS-Prover</title>
<link>https://arxiv.org/abs/2505.10962</link>
<guid>https://arxiv.org/abs/2505.10962</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MPS-Prover通过创新的数据精炼和多视角树搜索机制，显著提升了定理证明效率。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为MPS-Prover的新自动化定理证明系统，旨在解决现有逐步定理证明器因搜索引导偏差导致的低效问题。MPS-Prover采用两种关键创新：一种高效的数据后训练精炼策略，可以去除约40%的冗余训练数据而不影响性能；以及一种结合学习到的批评模型与精心设计启发式规则的多视角树搜索机制，该机制有助于多样化战术选择、避免陷入无效状态并增强搜索鲁棒性。广泛的评估显示，MPS-Prover在多个具有挑战性的基准测试中达到了最先进的性能，包括miniF2F和ProofNet，超过了之前7B参数规模的模型。此外，分析表明，与现有的逐步方法和整体证明方法相比，MPS-Prover生成的证明更短且更多样化，凸显了其效率和有效性。这项工作推动了基于大型语言模型的形式推理能力，并为开发更强大的定理证明器提供了稳健框架和全面分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10962" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 03:56:03 GMT</pubDate>
</item>
<item>
<title>MatTools：评估大型语言模型在材料科学工具应用中的能力</title>
<link>https://arxiv.org/abs/2505.10852</link>
<guid>https://arxiv.org/abs/2505.10852</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MatTools基准应用，评估大型语言模型在材料科学问题上的解答能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MatTools的新基准应用，用于评估大型语言模型（LLMs）在材料科学领域的能力，涵盖文献理解、性质预测、材料发现及合金设计等方面。MatTools由两个互补部分组成：材料模拟工具问答（QA）基准和真实世界工具使用基准。QA基准基于pymatgen代码库和文档构建，包含69,225对问答对；真实世界基准则包含49项任务（138个子任务），涉及材料属性计算的Python代码生成。通过对多种LLMs的评估，研究得出三个关键见解：通用模型优于专用模型、人工智能了解人工智能、简单方法更有效。MatTools为提升LLMs在材料科学工具应用中的能力提供了标准化框架，推动了更有效的AI系统在材料科学及通用科学研究中的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10852" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:43:05 GMT</pubDate>
</item>
<item>
<title>MMLongBench：首个全面评估长上下文视觉语言模型的基准测试</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个涵盖多种长上下文视觉语言任务的基准MMLongBench。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MMLongBench的新基准测试，这是第一个全面评估长上下文视觉语言模型(LCVLMs)能力的数据集。MMLongBench包含13,331个样本，涉及五个下游任务类别，并覆盖自然和合成图像等多种类型。通过在标准化输入长度下进行跨模态标记化处理，该基准能够有效评估模型在不同输入长度下的鲁棒性。通过对46个闭源和开源LCVLMs的详尽测试，研究发现单一任务的表现并不能很好地反映整体长上下文能力，且现有模型在长上下文任务中仍面临挑战，具备更强推理能力的模型表现更优。MMLongBench为下一代LCVLMs的发展提供了重要的诊断基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10610" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:52:54 GMT</pubDate>
</item>
<item>
<title>MuToR：一种高效多令牌预测方法</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MuToR方法，提升语言模型预训练效果并适用于多种场景。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MuToR的新方法，用于增强语言模型的多令牌预测能力。MuToR通过在输入序列中插入可学习的寄存器令牌来实现这一目标，每个令牌负责预测未来的特定目标。相比现有技术，MuToR具有参数增加少、无需架构改动且与下一令牌预训练目标兼容等优势。实验表明，MuToR在多种任务上表现出色，包括监督微调、参数高效微调及预训练。无论是在语言还是视觉领域，MuToR都能有效应对复杂的生成任务。此外，该方法支持扩展预测范围，并且代码将在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10518" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:25:03 GMT</pubDate>
</item>
<item>
<title>Qwen3：大型语言模型家族的新里程碑</title>
<link>https://arxiv.org/abs/2505.09388</link>
<guid>https://arxiv.org/abs/2505.09388</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Qwen3发布，集复杂推理与快速响应于一身，支持多语言并开源。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Qwen3系列大型语言模型（LLMs），这是Qwen模型家族的最新版本。Qwen3包含密集架构和混合专家（MoE）架构的多种规模模型，参数量从0.6亿到235亿不等。创新之处在于统一整合了思考模式（用于复杂多步推理）和非思考模式（用于快速上下文响应），无需切换不同模型即可实现动态模式转换。此外，Qwen3引入了思考预算机制，允许用户根据任务复杂度灵活分配计算资源，平衡延迟与性能。与前代相比，Qwen3将多语言支持扩展至119种语言和方言，并通过开源促进社区研究。实验表明，Qwen3在代码生成、数学推理及代理任务等多个基准测试中表现优异，优于部分更大规模的MoE模型和专有模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09388" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 09:41:34 GMT</pubDate>
</item>
<item>
<title>一种用于视觉语言模型知识蒸馏的双头优化框架</title>
<link>https://arxiv.org/abs/2505.07675</link>
<guid>https://arxiv.org/abs/2505.07675</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效的知识蒸馏框架DHO，在半监督设置下提升紧凑模型性能。</p><br /><br /><p><strong>摘要：</strong> 近年来，视觉-语言模型（VLMs）在多种任务上取得了显著成果，但其部署在资源受限环境中的挑战依然存在。传统知识蒸馏方法通常涉及多阶段训练或额外调优，增加了计算开销和优化复杂性。本文提出了一种名为“双头优化”（DHO）的新框架，通过引入独立学习标签数据和教师预测的双预测头，在半监督环境中有效转移VLM的知识到紧凑的任务特定模型。实验表明，DHO在多个领域和细粒度数据集上优于基线模型，在ImageNet上使用1%和10%标记数据时分别提升了3%和0.1%的准确性，同时减少了参数量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07675" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 11:39:51 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的3D先验引导图像编辑框架3D-Fixup</title>
<link>https://arxiv.org/abs/2505.10566</link>
<guid>https://arxiv.org/abs/2505.10566</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用扩散模型和3D先验进行复杂图像编辑的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对3D-aware图像编辑中的挑战，提出了名为3D-Fixup的新框架，该框架通过学习到的3D先验指导2D图像编辑。不同于仅依赖单一图像描述物体，3D-Fixup利用基于扩散模型的生成能力，并结合视频数据生成训练数据对。此外，通过引入Image-to-3D模型提供3D引导，实现了物体平移、旋转等复杂编辑任务。实验表明，该方法能够在保持身份一致性的同时实现高质量的3D-aware图像编辑，推动扩散模型在真实图像操作中的应用。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10566" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>Real2Render2Real：无需硬件操作的机器人训练数据生成方法</title>
<link>https://arxiv.org/abs/2505.09601</link>
<guid>https://arxiv.org/abs/2505.09601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需物理机器人操作的机器人训练数据生成方法R2R2R。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Real2Render2Real（R2R2R）的新方法，用于生成机器人训练数据，而无需依赖物体动力学模拟或机器人硬件的远程操作。该方法仅需输入智能手机扫描的物体图像及人类演示视频，即可通过3D高保真渲染生成大量机器人演示数据。R2R2R利用3D高斯点云技术实现刚性与可动物体的灵活资产生成和轨迹合成，并将其转换为网格以兼容大规模渲染引擎。实验表明，基于R2R2R生成的数据训练模型的表现可媲美传统方式下150倍的人类远程操作数据训练效果。这种方法显著降低了数据收集成本，为机器人学习提供了新的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:50:35 GMT</pubDate>
</item>
<item>
<title>X-Sim：通过人类视频训练机器人操作策略的新框架</title>
<link>https://arxiv.org/abs/2505.07096</link>
<guid>https://arxiv.org/abs/2505.07096</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于RGBD视频的跨实体机器人操作策略学习框架X-Sim。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为X-Sim的新框架，用于利用人类视频训练机器人操作策略。X-Sim是一种真实到模拟再到真实的框架，它使用物体运动作为密集且可转移的信号来学习机器人策略。该框架首先从RGBD人类视频重建出逼真的模拟环境并跟踪物体轨迹以定义物体中心奖励，这些奖励被用来在模拟环境中训练强化学习策略。然后，该策略被蒸馏成条件图像扩散策略，使用具有不同视角和光照的合成滚动渲染。为了将策略转移到现实世界，X-Sim引入了一种在线领域适应技术，在部署过程中对齐真实和模拟观察。重要的是，X-Sim不需要任何机器人遥操作数据。我们在两个环境中评估了X-Sim在五个操作任务上的表现，结果显示它平均提高了30%的任务进展，比手部追踪和模拟到真实基线更好，并且匹配了需要10倍更多数据收集时间的行为克隆方法，同时泛化到新的相机视点和测试时的变化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07096" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 15:04:00 GMT</pubDate>
</item>
<item>
<title>并行扩展(ParScale)：语言模型更高效的扩展方法</title>
<link>https://arxiv.org/abs/2505.10475</link>
<guid>https://arxiv.org/abs/2505.10475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的语言模型扩展方法——并行扩展，可显著提高推理效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种全新的语言模型扩展方法，即并行扩展(ParScale)，通过在训练和推理过程中增加模型的并行计算来提升效率，而无需大幅增加参数规模或输出令牌数。该方法通过对输入应用P种不同的变换，在并行执行模型前向传播后动态聚合输出。实验表明，相比参数扩展，ParScale能在达到相同性能提升的同时，减少高达22倍的内存增长和6倍的延迟增长。此外，ParScale还可通过少量微调将现成的预训练模型转化为并行扩展版本，进一步降低训练成本。这一新发现的扩展定律可能有助于在资源受限场景下部署更强大的模型，并为计算在机器学习中的作用提供了新的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:24:45 GMT</pubDate>
</item>
<item>
<title>AI Agents与Agentic AI对比分析及发展路径</title>
<link>https://arxiv.org/abs/2505.10468</link>
<guid>https://arxiv.org/abs/2505.10468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究界定了AI Agents与Agentic AI的设计哲学与能力差异。</p><br /><br /><p><strong>摘要：</strong> 本研究通过结构化的概念分类法、应用映射和挑战分析，清晰地区分了AI Agents与Agentic AI。AI Agents被定义为由大型语言模型（LLMs）和大型图像模型（LIMs）驱动的模块化系统，专注于特定任务自动化，而Generative AI为其前身。相比之下，Agentic AI则通过多代理协作、动态任务分解、持久记忆和协调自治展现了范式转变。文章通过架构演化、操作机制、交互风格和自治水平的对比分析，展示了两者在客户支持、调度和数据总结等领域的应用差异，并探讨了幻觉、脆弱性、涌现行为和协调失败等独特挑战，提出了ReAct循环、RAG、协调层和因果建模等解决方案。此研究旨在为开发稳健、可扩展且可解释的AI代理和Agentic AI驱动系统提供明确路线图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 12:21:33 GMT</pubDate>
</item>
<item>
<title>QuXAI：用于混合量子经典机器学习模型的可解释性框架</title>
<link>https://arxiv.org/abs/2505.10167</link>
<guid>https://arxiv.org/abs/2505.10167</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuXAI框架，提升混合量子经典机器学习模型的可解释性和可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于混合量子经典机器学习（HQML）模型的可解释性研究，指出当前领域在全局和局部解释方法上的研究空白。为此，作者引入QuXAI框架，基于Q-MEDLEY方法，通过量化特征编码与经典学习结合，分析特征重要性并可视化结果。实验表明，Q-MEDLEY不仅能有效区分经典部分的重要性及噪声，还在验证测试中表现优异。此外，消融实验进一步验证了Q-MEDLEY复合结构的优势。这项工作对增强HQML模型的透明度和可靠性具有重要意义，推动量子增强人工智能技术的安全应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10167" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 06:51:34 GMT</pubDate>
</item>
<item>
<title>Unilogit：一种新颖的语言模型自蒸馏机删方法</title>
<link>https://arxiv.org/abs/2505.06027</link>
<guid>https://arxiv.org/abs/2505.06027</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型自蒸馏方法Unilogit，实现语言模型中高效有选择性遗忘。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Unilogit的新颖自蒸馏方法，用于大规模语言模型中的机器遗忘任务。该方法通过动态调整目标logits，使目标令牌具有均匀概率，从而解决了在遵守GDPR等隐私法规的同时选择性遗忘特定信息的难题。与依赖静态超参数或初始模型输出的现有方法不同，Unilogit利用当前模型的输出作为更准确的自蒸馏目标，不仅无需额外的超参数设置，还提升了模型对黄金标准目标的逼近能力。实验表明，Unilogit在公共基准和自有电商数据集上表现出色，在平衡遗忘与保留目标方面优于NPO和UnDIAL等最先进方法，并且在多种场景下展现出强大的鲁棒性和实际应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06027" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 09:19:09 GMT</pubDate>
</item>
<item>
<title>Prior Depth Anything框架实现任意场景高精度深度图生成</title>
<link>https://arxiv.org/abs/2505.10565</link>
<guid>https://arxiv.org/abs/2505.10565</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合精确度量和相对结构的Prior Depth Anything框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Prior Depth Anything的新框架，该框架通过整合不完整但精确的度量信息与相对完整的几何结构，在任意场景中生成准确、密集且详细的度量深度图。为此，设计了一个粗到细的管道逐步融合两种互补的深度源。首先引入像素级度量对齐和距离感知加权，通过显式使用深度预测预填充多种度量先验，有效缩小了先验模式之间的领域差距。其次，开发了一个条件单目深度估计模型来细化深度先验的固有噪声。我们的模型在7个真实世界的数据集上展示了令人印象深刻的零样本泛化能力，并提供了灵活的准确性和效率权衡。此外，它还能在测试时通过切换预测模型进行改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10565" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>Style Customization of Text-to-Vector Generation with Image Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.10558</link>
<guid>https://arxiv.org/abs/2505.10558</guid>
<content:encoded><![CDATA[
Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics. Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.   To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:21 GMT</pubDate>
</item>
<item>
<title>PointArena：多模态指针能力评估平台</title>
<link>https://arxiv.org/abs/2505.09990</link>
<guid>https://arxiv.org/abs/2505.09990</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PointArena平台，用于评估多模态模型在多样化推理场景中的指针能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为PointArena的综合性平台，用于评估多模态模型的指针能力，涵盖从视觉情境中定位物体到实际操作场景下的应用。该平台由三个主要部分组成：Point-Bench数据集，包含五个推理类别的约1000项指针任务；Point-Battle交互式网络竞技场，已收集超过4500份匿名投票进行盲测；以及Point-Act机器人操作系统，用于在真实环境中直接测试模型的指针能力。通过对多种开源及专有模型的广泛评估发现，Molmo-72B表现出色，而针对指针任务的监督训练显著提升了模型性能。此外，研究还揭示了抽象推理与具体现实世界动作之间的强相关性，强调了精确指针能力的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09990" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 02:04:42 GMT</pubDate>
</item>
<item>
<title>Tokenadapt：一种高效的多语言模型 tokenizer 移植框架</title>
<link>https://arxiv.org/abs/2505.09738</link>
<guid>https://arxiv.org/abs/2505.09738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Tokenadapt方法解决固定分词器导致的语言模型效率低下问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对预训练语言模型（LLMs）因固定分词方案而产生的性能瓶颈及资源消耗问题，提出了名为Tokenadapt的创新性tokenizer移植框架。该框架通过混合启发式方法初始化新的唯一token嵌入，同时引入多词SuperTokens进行压缩优化，有效减少碎片化并提升多语言应用表现。实验表明，Tokenadapt在多个基准测试中显著优于现有方法，尤其在零样本困惑度方面，展现了卓越的性能改进能力，至少实现了2倍以上的整体分数提升。关键词：Tokenadapt, tokenizer移植, 多词SuperTokens</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 15:00:27 GMT</pubDate>
</item>
<item>
<title>基于动作条件的世界模型EnerVerse-AC实现高效机器人模仿学习</title>
<link>https://arxiv.org/abs/2505.09723</link>
<guid>https://arxiv.org/abs/2505.09723</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EnerVerse-AC模型，通过生成未来视觉观察提升机器人交互场景测试效率。</p><br /><br /><p><strong>摘要：</strong> 机器人模仿学习已从静态任务扩展到动态交互场景，但实时与动态环境互动的测试和评估成本高昂且困难重重。本文提出EnerVerse-AC（EVAC），这是一种动作条件世界模型，可基于预测动作生成未来的视觉观察，从而实现真实且可控的机器人推理。EVAC基于先前架构，引入多级动作条件机制和射线图编码以支持动态多视角图像生成，同时通过多样化失败轨迹扩充训练数据以增强泛化能力。作为数据引擎和评估工具，EVAC将人类收集的轨迹扩展为多样化的数据集，并生成动作条件视频观察以进行策略测试，无需物理机器人或复杂模拟。此方法大幅降低了成本，同时保持了机器人操作评估的高保真度。实验结果验证了该方法的有效性。代码、检查点和数据集可在指定网址获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09723" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 14:30:53 GMT</pubDate>
</item>
<item>
<title>ReSurgSAM2：一种高效的手术场景分割框架</title>
<link>https://arxiv.org/abs/2505.08581</link>
<guid>https://arxiv.org/abs/2505.08581</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出ReSurgSAM2框架，提升手术场景分割精度和效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReSurgSAM2的两阶段手术参考分割框架，该框架结合了Segment Anything Model 2进行目标检测，并通过可靠的初始帧识别和多样性驱动的长期记忆机制实现跟踪。在检测阶段，引入跨模态时空Mamba生成精确结果；在跟踪阶段，采用多样性驱动的记忆机制确保长期一致性。实验表明，ReSurgSAM2在准确性与效率上显著优于现有方法，实时运行速度达61.2 FPS。相关代码和数据集将在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08581" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 09:56:10 GMT</pubDate>
</item>
<item>
<title>ETT：端到端视觉标记器调优提升多模态任务性能</title>
<link>https://arxiv.org/abs/2505.10562</link>
<guid>https://arxiv.org/abs/2505.10562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端视觉标记器调优方法，显著提高多模态理解和视觉生成任务的表现。</p><br /><br /><p><strong>摘要：</strong> 现有视觉标记器的优化通常独立于下游任务，忽略了视觉标记器在不同任务中的表征差异性问题。这种解耦范式可能导致目标任务中的表示瓶颈。为解决这一问题，我们提出了ETT（End-to-End Vision Tokenizer Tuning），通过联合优化视觉标记器和目标自回归任务，实现更好的跨任务适应能力。与传统仅依赖冻结标记器的模型不同，ETT利用标记器代码本的视觉嵌入，在重建和描述双重目标下进行端到端优化。该方法易于实施且兼容现有训练管道，无需修改原始代码本或架构。实验表明，ETT在多模态理解和视觉生成任务上相较于冻结标记器基准提升了2-6%的性能，同时保持了原有的重建能力。我们期望这一简单而强大的方法能够推动更多多模态基础模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:59:39 GMT</pubDate>
</item>
<item>
<title>通过元能力对齐提升大规模推理模型的可扩展性和可靠性</title>
<link>https://arxiv.org/abs/2505.10554</link>
<guid>https://arxiv.org/abs/2505.10554</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过元能力对齐增强大规模推理模型的逻辑推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大规模推理模型（LRMs）在长链推理中的局限性，提出了一种基于元能力对齐的新方法。传统的方法依赖于提示和偶然出现的“顿悟”现象，但这些行为的时机和一致性难以预测和控制，限制了模型的扩展性和可靠性。为了克服这些问题，我们引入了演绎、归纳和溯因三种元能力的显式对齐，并设计了一个三阶段的流水线：个体对齐、参数空间合并以及领域特定强化学习。这种方法相较于仅通过指令微调的基线模型，在性能上提升了超过10%。此外，通过领域特定的强化学习进一步提升了数学、编码和科学基准测试中的表现，平均提高了2%，证明了元能力对齐提供了可扩展且可靠的推理基础。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10554" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:58:33 GMT</pubDate>
</item>
<item>
<title>世界偏好建模（WorldPM）揭示人类偏好建模中的规模法则</title>
<link>https://arxiv.org/abs/2505.10527</link>
<guid>https://arxiv.org/abs/2505.10527</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现语言模型的测试损失与模型和数据集大小成幂律关系，这一规律在偏好建模中同样适用。</p><br /><br /><p><strong>摘要：</strong> 本文提出世界偏好建模（WorldPM），通过从公共论坛收集多样化用户社区的偏好数据，在参数规模从1.5B到72B的不同模型上进行大规模训练（15M量级）。实验表明，对抗性指标随训练数据和基础模型增大而提升；客观指标在大模型中表现出涌现特性；主观指标则未显示规模趋势。进一步验证显示，WorldPM可有效作为偏好微调的基础模型。在7个基准的20个子任务中，WorldPM显著提升了多种规模的人类偏好数据集的泛化性能，尤其在内部强化学习人类反馈（RLHF）流水线中，表现提升4%-8%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10527" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 13:38:37 GMT</pubDate>
</item>
<item>
<title>基于强化学习的AI法官模型训练方法J1</title>
<link>https://arxiv.org/abs/2505.10320</link>
<guid>https://arxiv.org/abs/2505.10320</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种强化学习方法J1，显著提升AI法官模型的判断能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为J1的强化学习方法，用于训练具备更强推理能力的大型语言模型（LLM），使其作为评价工具时能够提供更准确的判断。通过将可验证和不可验证的任务转化为具有明确奖励机制的判断任务，J1有效提升了模型的推理能力和减少偏见。实验表明，J1在多个基准测试中表现优于其他同类规模（8B或70B参数量）的模型，甚至在某些任务上超过了更大规模的模型。此外，研究还探讨了不同训练策略、奖励机制及提示种子对模型性能的影响，发现模型通过学习制定评估标准、对比自身生成的答案以及重新评估响应正确性等方式提高了判断质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10320" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 10:05:15 GMT</pubDate>
</item>
<item>
<title>基于CoT百科全书的大语言模型推理分析与优化</title>
<link>https://arxiv.org/abs/2505.10185</link>
<guid>https://arxiv.org/abs/2505.10185</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自下而上的框架来分析和引导大语言模型的推理策略。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CoT百科全书，这是一种用于分析和引导现代大型语言模型推理的新框架。该方法通过自动提取模型生成的长链推理中的多样化推理标准，并将其嵌入语义空间进行聚类，形成代表性类别，同时推导对比标准来解释推理行为。实验结果显示，与现有方法相比，此框架生成的分析更具可解释性和全面性。此外，这种对推理策略的理解还能带来性能提升，帮助预测模型可能采用的策略并指导其走向更有效的替代方案。最后，研究还揭示了训练数据格式而非数据领域对推理行为的影响更大，强调了格式感知模型设计的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10185" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 07:31:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型与扩散变压器融合在文本到图像合成中的探索</title>
<link>https://arxiv.org/abs/2505.10046</link>
<guid>https://arxiv.org/abs/2505.10046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨大型语言模型与扩散变压器在多模态生成中的深度融合设计空间。</p><br /><br /><p><strong>摘要：</strong> 本文并非介绍新方法，而是深入探索近期文本到图像合成领域的关键设计空间，即大型语言模型（LLMs）与扩散变压器（DiTs）的深度融合在多模态生成中的应用。先前的研究多集中于系统整体性能而缺乏对替代方法的详细比较，且关键设计细节及训练方案常未公开，导致该方法潜力存在不确定性。为填补这些空白，我们进行了一项关于文本到图像生成的实证研究，通过与基准模型的受控比较、分析重要设计选择以及提供可扩展训练的清晰可复现方案，希望为未来多模态生成研究提供有意义的数据参考和实用指南。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.10046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 15 May 2025 03:43:23 GMT</pubDate>
</item>
<item>
<title>AdaptCLIP：基于CLIP的视觉异常检测新方法</title>
<link>https://arxiv.org/abs/2505.09926</link>
<guid>https://arxiv.org/abs/2505.09926</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为AdaptCLIP的新方法，显著提升跨域异常检测性能。</p><br /><br /><p><strong>摘要：</strong> 现有的通用视觉异常检测方法在处理未知领域时面临设计复杂模板或额外微调的问题，限制了灵活性。本文提出AdaptCLIP方法，通过交替学习视觉和文本表示，并结合上下文和对齐残差特征的比较学习，改进了CLIP模型的零样本/少样本泛化能力。该方法仅需添加三个简单的适配器，在工业和医学领域的12个基准测试中达到了最先进的性能。AdaptCLIP无需针对目标领域进行训练，展示了出色的跨域适应能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09926" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 23:24:28 GMT</pubDate>
</item>
<item>
<title>构建具身世界模型评估基准EWMBench</title>
<link>https://arxiv.org/abs/2505.09694</link>
<guid>https://arxiv.org/abs/2505.09694</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出EWMBench基准用于评估具身世界模型的物理一致性与动作一致性。</p><br /><br /><p><strong>摘要：</strong> 近期创意AI的进步推动了文本到视频扩散模型的发展，使其演变为能够根据语言指令生成物理上合理场景的具身世界模型（EWMs）。然而，现有模型的评估多局限于一般感知指标，缺乏对物理基础和动作一致性的深入考量。本文提出了EWMBench评估框架，通过视觉场景一致性、运动正确性和语义对齐三个维度，利用精心策划的数据集及多维工具，系统性评估EWMs的表现。该基准不仅揭示了现有视频生成模型在具身任务中的局限性，还为未来研究提供了指导方向。相关数据集与工具已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09694" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 14:00:19 GMT</pubDate>
</item>
<item>
<title>双层优化框架：提升大规模语言模型系统提示的鲁棒性与迁移能力</title>
<link>https://arxiv.org/abs/2505.09666</link>
<guid>https://arxiv.org/abs/2505.09666</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种双层优化方法，用于提升大规模语言模型系统提示的通用性和迁移能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大规模语言模型（LLMs）中系统提示（system prompt）的优化问题，指出现有研究多集中于特定任务的用户提示（user prompt），而忽视了对系统提示的全局优化。为此，我们引入双层系统提示优化这一新问题，旨在设计出对多样用户提示具有鲁棒性且能在未知任务间迁移的系统提示。为此，我们提出了一个元学习框架，通过在多个数据集上的多种用户提示上迭代优化系统提示，同时同步更新用户提示，以实现两者间的协同效应。实验结果表明，该方法在五个领域的14个未见过的数据集上均表现出色，不仅提升了系统提示对用户提示的适应性，还显著加快了对未知任务的适配速度，同时提高了性能表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09666" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:46:15 GMT</pubDate>
</item>
<item>
<title>基于纯视觉元学习框架的通用异常分割方法</title>
<link>https://arxiv.org/abs/2505.09265</link>
<guid>https://arxiv.org/abs/2505.09265</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种纯视觉模型实现零样本和少样本通用异常分割的方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了使用纯视觉基础模型替代广泛使用的视觉-语言模型进行通用视觉异常分割的可能性。通过将异常分割统一为变化分割的新范式，我们利用现有图像数据集中的大规模合成图像对，这些图像对具有对象级和局部区域变化。提出了一个名为MetaUAS的一次提示元学习框架，该框架在合成数据集上训练后，能够很好地泛化到现实世界中的任何新型或未见视觉异常的分割任务。为了处理提示图像与查询图像之间的几何变化，设计了一种软特征对齐模块，以桥接配对图像的变化感知和单图像语义分割。这是首次使用纯视觉模型实现通用异常分割的工作，无需依赖特殊异常检测数据集和预训练的视觉-语言模型。实验表明，MetaUAS在性能上显著优于以往的零样本、少样本甚至全样本异常分割方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09265" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:26 GMT</pubDate>
</item>
<item>
<title>基于单张正常图像提示的统一异常检测方法</title>
<link>https://arxiv.org/abs/2505.09264</link>
<guid>https://arxiv.org/abs/2505.09264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅需一张正常图像即可实现高效统一异常检测的方法OneNIP。</p><br /><br /><p><strong>摘要：</strong> 现有的自注意力重建网络在多类别异常检测中表现出色，但其主要依赖目标特征进行重建，可能导致正常和异常特征的完美重建，从而无法有效检测异常。此外，由于在低空间分辨率潜在空间中进行重建，这些模型常常产生不准确的异常分割。为了解决这些问题，本文提出了一个名为OneNIP的新方法，它仅使用一张正常图像即可重建正常特征并恢复异常特征。与以往工作相比，OneNIP首次实现了仅凭一张正常图像进行异常重建或恢复，显著提升了统一异常检测的性能。同时，我们还提出了一种监督细化器，通过使用真实正常图像和合成异常图像来回归重建误差，从而大幅提高了像素级异常分割的准确性。实验表明，OneNIP在MVTec、BTAD和VisA三个工业异常检测基准上优于现有方法。代码和预训练模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:14 GMT</pubDate>
</item>
<item>
<title>基于少样本生成的工业异常检测方法</title>
<link>https://arxiv.org/abs/2505.09263</link>
<guid>https://arxiv.org/abs/2505.09263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种少样本生成方法，有效提升工业异常检测性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对工业检测中异常样本稀缺的问题，提出了一种名为AnoGen的少样本异常驱动生成方法。该方法通过引导扩散模型利用少量真实异常样本生成逼真的多样化异常图像，从而辅助训练异常检测模型。具体而言，首先基于给定的真实异常样本学习异常分布并注入嵌入向量；其次，结合嵌入向量和边界框指导扩散模型生成特定对象上的异常；最后，设计弱监督异常检测方法，进一步优化模型性能。实验表明，AnoGen显著提升了DRAEM和DesTSeg在分割任务中的AU-PR指标，分别提高了5.8%和1.5%。研究代码与生成的异常数据已在GitHub公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 06:25:06 GMT</pubDate>
</item>
<item>
<title>OpenThinkIMG：赋能视觉语言模型的工具增强框架</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个端到端工具增强框架OpenThinkIMG，提升视觉语言模型动态工具操作能力。</p><br /><br /><p><strong>摘要：</strong> 尽管人类能够灵活运用交互式视觉认知解决复杂问题，但使大型视觉语言模型（LVLMs）学习类似的适应性行为仍具挑战。主要障碍在于缺乏标准化基础设施，阻碍了工具整合、交互数据生成及高效训练。为填补这些空白，我们推出了OpenThinkIMG，这是一个开源的综合框架，支持工具增强型LVLMs。它具备标准化视觉工具接口、可扩展轨迹生成和灵活训练环境。鉴于监督微调对动态工具调用的策略泛化有限，我们还提出了V-ToolRL强化学习框架，使LVLMs能够自主发现最佳工具使用策略。实验表明，基于Qwen2-VL-2B的RL训练代理在图表推理任务中表现优异，不仅优于监督微调初始化版本，还超过了Taco、CogCom等基准模型，并且比GPT-4.1高出8.68个百分点。我们希望OpenThinkIMG能成为推动动态工具增强视觉推理发展的基础框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08617" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 10:35:51 GMT</pubDate>
</item>
<item>
<title>MLE-Dojo：用于自主大语言模型迭代训练的交互式框架</title>
<link>https://arxiv.org/abs/2505.07782</link>
<guid>https://arxiv.org/abs/2505.07782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MLE-Dojo是一个支持大语言模型迭代学习和优化的交互式框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为MLE-Dojo的新框架，它是一种类似于Gym的环境，旨在系统性地进行强化学习、评估以及改进自主大型语言模型（LLM）代理。MLE-Dojo不同于依赖静态数据集或单一评估方法的传统基准测试，它通过提供互动环境，使代理能够在结构化反馈循环中反复试验、调试和优化解决方案。该框架基于超过200个真实世界Kaggle挑战构建，涵盖了多样化的开放性机器学习工程任务，如数据处理、架构搜索、超参数调优和代码调试。实验结果显示，尽管当前模型能够在一定程度上实现迭代改进，但在生成长期解决方案和高效解决复杂错误方面仍存在显著局限性。此外，MLE-Dojo具有灵活且可扩展的架构，能够无缝集成多种数据源、工具和评估协议，从而促进模型驱动代理的调优、互操作性、可扩展性和可重复性。我们开源了这一框架及其基准测试，以推动社区对下一代机器学习工程代理的创新。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:35:43 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的图像精确光照编辑方法</title>
<link>https://arxiv.org/abs/2505.09608</link>
<guid>https://arxiv.org/abs/2505.09608</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的精确光照编辑方法，实现对图像光源的细粒度控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种简单而有效的基于扩散模型的方法，用于在图像中对光源进行精细且参数化的控制。现有的一些重照明方法要么依赖多个输入视图进行逆渲染，要么无法提供明确的光照变化控制。我们的方法通过微调扩散模型，在一组真实的原始照片对和大规模合成渲染图像上训练，以激发其真实感先验用于重照明。我们利用光的线性特性合成展示受控光照变化的图像对，从而实现对目标光源或环境光的精确控制。实验表明，该方法不仅能够产生令人信服的光照编辑效果，而且在用户偏好测试中优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09608" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:57:27 GMT</pubDate>
</item>
<item>
<title>基于音频语言模型的对话系统评估方法WavReward</title>
<link>https://arxiv.org/abs/2505.09558</link>
<guid>https://arxiv.org/abs/2505.09558</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于音频语言模型的评估框架WavReward，用于评价语音对话系统的IQ和EQ。</p><br /><br /><p><strong>摘要：</strong> 近年来，端到端语音对话模型如GPT-4o-audio受到广泛关注，但其会话性能的评估却未得到足够重视。主要原因是传统文本型语言模型难以衡量智能聊天机器人传达的丰富非文本信息。为填补这一空白，我们提出了WavReward，这是一种基于音频语言模型的奖励反馈模型，能够对语音输入的对话系统进行IQ和EQ的综合评估。WavReward通过多样本反馈机制结合强化学习算法优化后训练，同时引入了包含理解与生成两大方面的ChatReward-30K数据集，涵盖多种任务场景。实验表明，WavReward在多个语音对话场景中显著优于现有评估模型，客观准确性从55.1%提升至91.5%，主观A/B测试领先83%。此外，消融研究验证了WavReward各组件的必要性。所有数据和代码将在论文被接受后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09558" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 12:54:15 GMT</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 15:01:12 GMT</pubDate>
</item>
<item>
<title>LLaVA图像文本预训练数据集毒性分析与缓解策略</title>
<link>https://arxiv.org/abs/2505.06356</link>
<guid>https://arxiv.org/abs/2505.06356</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现并移除LLaVA预训练数据集中有害内容，提出针对性的毒性缓解方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了LLaVA图像-文本预训练数据集中毒性内容的存在情况，分析了有害内容在不同模态下的表现形式，并针对常见毒性类别提出了缓解策略。通过这些措施，我们成功移除了7,531对有毒图像-文本对，创建了一个经过毒性优化的数据集。此外，我们还提供了构建稳健毒性检测管道的指南。研究结果表明，主动识别和过滤诸如仇恨言论、露骨图像及针对性骚扰等毒性内容对于构建更负责任且公平的多模态系统至关重要。该去毒化数据集已开源，可供进一步研究使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06356" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 14:01:50 GMT</pubDate>
</item>
<item>
<title>SteepGS：优化3D Gaussian Splatting密度控制以提升渲染效率</title>
<link>https://arxiv.org/abs/2505.05587</link>
<guid>https://arxiv.org/abs/2505.05587</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的3DGS优化框架SteepGS，大幅减少高分辨率场景中的Gaussian点数，同时保持渲染质量。</p><br /><br /><p><strong>摘要：</strong> 3D Gaussian Splatting (3DGS) 是一种高效的新视角合成技术，通过GPU加速渲染实现高质量图像生成。然而，其密集化算法容易导致冗余点云，增加内存和存储需求。本文提出了一种理论框架，分析并改进了3DGS的密度控制机制，揭示了分裂操作在避免鞍点的重要性。基于优化理论，我们确定了最小化后代Gaussian数量的条件，并引入了陡峭密度控制策略（SteepestGS）。该方法通过优化参数更新方向和后代透明度归一化，将Gaussian点数减少了约50%，显著提升了渲染效率和系统可扩展性，尤其适用于资源受限设备。实验表明，SteepGS在不降低视觉质量的前提下实现了性能飞跃。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05587" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 14:41:38 GMT</pubDate>
</item>
<item>
<title>Omni-R1：通过强化学习提升多模态大模型音频问答性能</title>
<link>https://arxiv.org/abs/2505.09439</link>
<guid>https://arxiv.org/abs/2505.09439</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Omni-R1在MMAU基准测试中实现音频问答领域新SOTA。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Omni-R1的新方法，该方法基于对近期多模态大型语言模型Qwen2.5-Omni进行微调，采用强化学习算法GRPO优化处理音频问答数据集。实验结果显示，在MMAU基准测试的多个类别中，包括声音、音乐、语音及整体平均分上，Omni-R1在Test-mini和Test-full两个子集上均取得了最高准确率。进一步分析表明，这种性能提升主要归因于文本推理能力的增强。此外，研究还发现，仅使用纯文本数据集对模型进行微调同样能够有效改善音频相关任务的表现，这一发现令人意外且具有重要意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09439" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 10:47:16 GMT</pubDate>
</item>
<item>
<title>Marigold：通过条件生成模型从预训练扩散模型中提取知识</title>
<link>https://arxiv.org/abs/2505.09358</link>
<guid>https://arxiv.org/abs/2505.09358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法Marigold，利用预训练扩散模型实现高效密集图像分析。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Marigold的条件生成模型家族及其微调协议，该协议可以从预训练的如Stable Diffusion等基于潜在扩散的模型中提取知识，并将其应用于单目深度估计、表面法线预测和内在分解等密集图像分析任务。Marigold仅需对预训练模型进行少量修改，使用小规模合成数据集，在单块GPU上训练几天即可完成，且表现出卓越的零样本泛化能力。这种方法充分利用了文本到图像生成模型的强大潜力，尤其是在稀缺标注数据的情况下，为计算机视觉领域提供了新的研究方向和应用前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 09:07:03 GMT</pubDate>
</item>
<item>
<title>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</title>
<link>https://arxiv.org/abs/2505.08787</link>
<guid>https://arxiv.org/abs/2505.08787</guid>
<content:encoded><![CDATA[
Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>Visually Interpretable Subtask Reasoning for Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.08084</link>
<guid>https://arxiv.org/abs/2505.08084</guid>
<content:encoded><![CDATA[
Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 17:37:06 GMT</pubDate>
</item>
<item>
<title>DetReIDX：大规模空地视角下的人体再识别数据集</title>
<link>https://arxiv.org/abs/2505.04793</link>
<guid>https://arxiv.org/abs/2505.04793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的大规模空地人体再识别数据集，用于评估真实场景下的技术性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为DetReIDX的大规模空地人体再识别数据集，旨在对现实世界条件下的人体再识别技术进行压力测试。该数据集包含来自三个大洲七个大学校园的超过1300万个边界框，涉及509个身份标识，无人机飞行高度介于5.8至120米之间。此外，数据集中的人体记录至少在两天的不同时间点进行，且穿着、光照及位置均发生变化，使其特别适合评估长期人体再识别。数据还标注了16种软生物特征属性以及检测、跟踪、再识别和动作识别等多任务标签。实验表明，当前最先进的方法在DetReIDX的条件下性能显著下降（检测精度下降高达80%，Rank-1再识别准确率下降超过70%）。该数据集及相关资源现已公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 16:41:06 GMT</pubDate>
</item>
<item>
<title>SweRank：高效代码定位框架解决软件问题描述匹配难题</title>
<link>https://arxiv.org/abs/2505.07849</link>
<guid>https://arxiv.org/abs/2505.07849</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SweRank框架，解决传统代码检索模型在处理自然语言问题描述时的不足。</p><br /><br /><p><strong>摘要：</strong> 软件问题定位是软件开发中的关键环节，但传统方法存在效率低下等问题。本文介绍了一种名为SweRank的新框架，该框架通过高效的检索与重排序机制显著提升了问题描述到代码位置的匹配效果。此外，为了支持训练，研究团队构建了一个大规模的SweLoc数据集，包含来自公共GitHub仓库的真实问题描述及其对应的代码修改记录。实验表明，SweRank不仅在性能上超越了现有的排名模型，还优于基于闭源LLMs的昂贵代理系统。同时，SweLoc作为宝贵的公共资源，也为改进现有检索与重排序模型提供了帮助。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07849" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 15:44:09 GMT</pubDate>
</item>
<item>
<title>基于单张RGB图像的高效3D场景重建方法CAST</title>
<link>https://arxiv.org/abs/2502.12894</link>
<guid>https://arxiv.org/abs/2502.12894</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合GPT模型和物理约束的3D场景重建新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为CAST的新方法，用于从单张RGB图像中恢复高质量的3D场景。CAST首先通过提取对象级别的2D分割和相对深度信息，然后利用基于GPT的模型分析对象间的空间关系，以确保重建的一致性。接着，采用遮挡感知的大规模3D生成模型独立生成每个对象的完整几何形状，并使用MAE和点云条件处理来减轻遮挡和部分对象信息的影响。为了将对象对齐到场景中，使用对齐生成模型计算必要的变换。最后，引入物理感知校正步骤，利用细粒度关系图生成约束图，优化对象姿态，解决遮挡、物体穿透和漂浮等问题。CAST在机器人领域具有广泛应用，可实现真实到模拟的工作流并提供逼真的仿真环境。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2502.12894" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 18 Feb 2025 09:29:52 GMT</pubDate>
</item>
<item>
<title>DeepSeek-V3模型的硬件感知设计及其对AI系统的启示</title>
<link>https://arxiv.org/abs/2505.09343</link>
<guid>https://arxiv.org/abs/2505.09343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨硬件感知的深度学习模型设计如何有效解决大规模语言模型训练中的硬件瓶颈。</p><br /><br /><p><strong>摘要：</strong> 本文深入分析了DeepSeek-V3/R1模型架构及其支持基础设施，强调多头潜在注意力（MLA）、专家混合（MoE）架构、FP8混合精度训练和多平面网络拓扑等创新技术，显著提升了内存效率、计算通信权衡优化及硬件潜能释放。这些成果基于在开发过程中遇到的硬件限制，进一步与学术界和工业界展开讨论，提出未来硬件发展的方向，如精确低精度计算单元、规模扩展融合以及低延迟通信结构的创新。研究展示了硬件与模型协同设计在应对人工智能工作负载增长需求中的关键作用，为下一代AI系统提供了实践蓝图。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 08:39:03 GMT</pubDate>
</item>
<item>
<title>Video-based 长篇因果推理基准VCRBench及其评估</title>
<link>https://arxiv.org/abs/2505.08455</link>
<guid>https://arxiv.org/abs/2505.08455</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新基准VCRBench测试大型视频语言模型的因果推理能力。</p><br /><br /><p><strong>摘要：</strong> 尽管视频理解领域取得了进展，但大型视频语言模型（LVLMs）在基于视频的因果推理方面的表现尚未被充分探索，主要因为缺乏专门的基准来评估这种能力。为解决这一问题，我们引入了一个名为VCRBench的新基准，该基准使用日常活动的程序化视频构建，通过打乱步骤并捕捉关键因果事件来测试LVLMs是否能识别、推理并正确排序实现特定目标所需的事件。此外，VCRBench设计旨在防止模型依赖语言捷径，并避免开放性问答评估的挑战。对当前最先进的LVLMs进行的评估表明，这些模型在处理基于视频的长篇因果推理时存在困难，主要是由于难以直接从视觉观察中建模长距离因果依赖关系。作为迈向提升这种能力的第一步，我们提出了识别-推理分解（RRD）模块化方法，将视频因果推理分为视频识别和因果推理两个子任务。实验结果显示，RRD显著提高了VCRBench上的准确性，提升了最多25.2%。最后，我们的分析揭示了有趣见解，例如LVLMs在复杂视频因果推理任务中主要依赖语言知识。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08455" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 07:35:58 GMT</pubDate>
</item>
<item>
<title>DeCLIP：通过解耦自注意力机制提升开放词汇密集预测性能</title>
<link>https://arxiv.org/abs/2505.04410</link>
<guid>https://arxiv.org/abs/2505.04410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架DeCLIP，改进CLIP模型用于开放词汇密集预测任务。</p><br /><br /><p><strong>摘要：</strong> 本文研究了视觉预测任务中依赖预定义类别带来的限制问题，尤其是Vision-Language Models（VLMs）如CLIP在开放词汇任务中的局限性。我们观察到，CLIP的图像tokens在聚合空间或语义相关区域的信息时表现不佳，导致特征缺乏局部辨别性和空间一致性。为此，我们提出了DeCLIP框架，通过解耦自注意力模块分别获得“内容”和“上下文”特征。“内容”特征与图像裁剪表示对齐以提高局部辨别性，“上下文”特征则在视觉基础模型指导下学习保留空间相关性。实验表明，DeCLIP在多个开放词汇密集预测任务（如目标检测和语义分割）上显著优于现有方法。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 09:46:34 GMT</pubDate>
</item>
<item>
<title>BLIP3-o：统一图像理解和生成的创新多模态模型</title>
<link>https://arxiv.org/abs/2505.09568</link>
<guid>https://arxiv.org/abs/2505.09568</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合扩散模型和CLIP特征的新方法，提升图像理解和生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了统一图像理解和生成的多模态模型设计，重点研究自回归和扩散模型在高质量生成及可扩展性上的潜力。通过引入扩散Transformer生成语义丰富的CLIP图像特征，该方法不仅提高了训练效率，还提升了生成质量。此外，通过先训练图像理解再转向图像生成的分阶段预训练策略，模型在保持理解能力的同时增强了生成能力。同时，构建了一个高质的指令微调数据集BLIP3o-60k，进一步优化了模型表现。最终，开发的BLIP3-o在多个基准测试中表现出色，并开源了模型及相关资源，推动未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.09568" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 May 2025 13:11:07 GMT</pubDate>
</item>
<item>
<title>Aya-Vision：解决多语言多模态模型挑战的新方法</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出新方法解决多语言多模态模型中的数据稀缺及遗忘问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了针对多语言多模态语言模型开发的创新技术和框架，以应对跨视觉与文本模态对齐、高质量指令数据采集以及引入视觉能力后文本性能退化等挑战。通过构建合成注释框架生成多样化多语言多模态指令数据，同时提出跨模态模型融合技术缓解灾难性遗忘问题，从而在多种语言环境下保持并提升模型性能。实验结果显示，Aya-Vision-8B 和 Aya-Vision-32B 分别在性能上优于同类较大规模的模型，展示了高效计算利用和卓越性能表现的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08751" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 13:03:48 GMT</pubDate>
</item>
<item>
<title>基于信息瓶颈的LLMs训练方法：压缩与记忆的平衡</title>
<link>https://arxiv.org/abs/2505.08727</link>
<guid>https://arxiv.org/abs/2505.08727</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过压缩内部表征提升大语言模型泛化能力。</p><br /><br /><p><strong>摘要：</strong> 本文证明了泛化能力不仅可以通过数据规模提升，还可以通过压缩内部表征实现。为此引入了信息瓶颈语言建模（IBLM）目标，将语言建模重新定义为受约束优化问题。实验发现，在大型语言模型预训练过程中存在记忆-压缩循环现象，这与IBLM理论预测及生物学习-巩固过程相似。基于此，提出门控相变（GAPT）算法，该算法可自适应切换记忆与压缩阶段。在FineWeb数据集上对GPT-2进行预训练时，GAPT减少了50%的矩阵熵并提升了4.8%的交叉熵。此外，在算术乘法预训练任务中，GAPT使OOD泛化能力提高了35%，并在模拟灾难性遗忘场景中大幅降低了干扰，实现了97%的分离改善。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08727" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 12:37:54 GMT</pubDate>
</item>
<item>
<title>面向复杂工作流的可扩展评估方法研究</title>
<link>https://arxiv.org/abs/2505.08638</link>
<guid>https://arxiv.org/abs/2505.08638</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">现有方法无法应对复杂工作流评估需求，新研究提出分类法并构建标注数据集。</p><br /><br /><p><strong>摘要：</strong> 随着自主性工作流在多个领域的普及，如何系统性地评估这些系统生成的复杂轨迹成为紧迫需求。当前评估依赖人工分析，难以应对日益增长的工作流复杂性和数量。此外，工具输出与语言模型推理之间的相互作用使错误分析更加困难。本研究提出了对自主系统轨迹进行稳健动态评估的需求，引入了自主系统错误类型的正式分类，并基于此构建了包含148个大型人工标注轨迹的数据集（TRAIL）。数据集涵盖单代理和多代理系统，聚焦软件工程和开放世界信息检索等实际应用。评估显示现代长上下文LLMs在轨迹调试方面表现不佳，最佳Gemini-2.5-pro模型仅达11%准确率。研究数据和代码已公开，以推动自主工作流的可扩展评估研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08638" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 10:55:31 GMT</pubDate>
</item>
<item>
<title>检索增强生成系统中超参数对性能的影响分析</title>
<link>https://arxiv.org/abs/2505.08445</link>
<guid>https://arxiv.org/abs/2505.08445</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示合适超参数可大幅提升检索增强生成系统的准确性与速度。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了超参数如何影响检索增强生成（RAG）系统的速度与质量，涉及Chroma和Faiss向量存储、分块策略、交叉编码重排序及温度等。实验评估了六个指标，发现Chroma查询处理更快，而Faiss具有更高的检索精度，存在明显的速度-准确性权衡。固定长度分块优于语义分割，但重新排序虽提升了检索质量却增加了五倍运行时间。最终通过纠正性RAG工作流验证，最佳配置在迭代请求证据时仍保持优势，达到了近乎完美的上下文精确度（99%），表明恰当的超参数组合可极大提升医疗等领域下游任务表现的关键性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08445" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 07:13:27 GMT</pubDate>
</item>
<item>
<title>开源模型AM-Thinking-v1：32B规模下的推理能力新标杆</title>
<link>https://arxiv.org/abs/2505.08311</link>
<guid>https://arxiv.org/abs/2505.08311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AM-Thinking-v1在多项数学和代码评测中超越同类开源模型。</p><br /><br /><p><strong>摘要：</strong> AM-Thinking-v1是一款基于开源Qwen2.5-32B基模型构建的大型密集语言模型，通过精心设计的后训练管道结合监督微调和强化学习技术，实现了卓越的推理能力。该模型在AIME 2024、AIME 2025以及LiveCodeBench等测试中分别获得85.3、74.4和70.3的高分，表现出色，与顶级混合专家模型如Qwen3-235B-A2B和Seed1.5-Thinking相媲美。此外，AM-Thinking-v1完全由开源资源开发，展示了开放社区在32B规模上的强大潜力，强调了高性能与实际可用性之间的平衡。本项目已托管于Hugging Face平台，旨在激励更多协作努力以推动中型模型的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 03:41:15 GMT</pubDate>
</item>
<item>
<title>基于对抗相对对比学习的文本转音频加速方法</title>
<link>https://arxiv.org/abs/2505.08175</link>
<guid>https://arxiv.org/abs/2505.08175</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的非蒸馏加速算法，显著降低文本转音频模型推理延迟。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Adversarial Relativistic-Contrastive（ARC）后训练的创新方法，该方法首次针对扩散/流模型提出了非蒸馏加速技术。传统蒸馏方法成本较高，而ARC通过引入相对论性对抗框架并结合对比鉴别器目标，有效提升了生成音频对文本提示的依从性。实验表明，当与Stable Audio Open结合优化后，所构建的模型在H100硬件上可于约75毫秒内生成12秒的44.1kHz立体声音频，在移动边缘设备上也可达到约7秒的生成速度，成为目前最快的文本转音频系统之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08175" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 22:25:47 GMT</pubDate>
</item>
<item>
<title>跨模态模型融合：将语言模型推理能力融入视觉语言模型</title>
<link>https://arxiv.org/abs/2505.05464</link>
<guid>https://arxiv.org/abs/2505.05464</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究通过跨模态模型融合实现视觉语言模型获取语言模型的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文探索了如何通过参数连接不同模型来整合感知与推理能力。不同于以往仅限于同类模型融合的工作，我们提出了跨模态模型融合方法，使语言模型的推理能力得以融入视觉语言模型，且无需额外训练。实验表明，这种融合方式成功实现了推理能力的迁移。此外，我们利用合并后的模型分析了感知与推理在模型中的分布情况，发现感知主要集中在早期层，而推理则由中间到晚期层主导。融合后，所有层均开始参与推理，但感知能力的分布基本保持不变。这些发现揭示了模型融合作为多模态集成与解释工具的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05464" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:56:23 GMT</pubDate>
</item>
<item>
<title>基于Transformer的阿拉伯语反向词典系统研究</title>
<link>https://arxiv.org/abs/2504.21475</link>
<guid>https://arxiv.org/abs/2504.21475</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">开发了一种基于Transformer的新型阿拉伯语反向词典系统。</p><br /><br /><p><strong>摘要：</strong> 本研究针对阿拉伯自然语言处理领域的空白，提出了一种基于Transformer的创新性半编码神经网络架构，用于构建高效的阿拉伯语反向词典系统。该系统通过几何递减层实现对词语描述或意义的查询功能，显著提升了阿拉伯语反向词典任务的表现。实验表明，特定领域的预训练模型优于通用多语言嵌入模型，其中ARBERTv2表现最佳。此外，研究还提出了逆向词典任务的正式抽象，并开发了一个可配置训练管道的Python库(RDTL)，为阿拉伯语定义构建提供了重要的质量标准。这项工作对阿拉伯计算语言学具有重要意义，也为阿拉伯语学习、学术写作及专业交流提供了实用工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21475" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 05:56:36 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的机器人导航策略NavDP</title>
<link>https://arxiv.org/abs/2505.08712</link>
<guid>https://arxiv.org/abs/2505.08712</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端的仿真训练导航方法NavDP，实现跨多种真实环境的零样本迁移。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为Navigation Diffusion Policy (NavDP) 的端到端框架，该框架仅通过仿真训练即可实现对不同真实环境中的多种机器人形态的零样本迁移。NavDP结合了基于扩散的轨迹生成和轨迹选择的批评函数，这些均基于共享策略Transformer编码的局部观测标记。通过利用仿真环境中全局环境的特权信息，我们扩展了高质量演示的规模以训练扩散策略，并通过对比负样本制定批评值函数目标。此方法每天可生成约2500条轨迹/GPU，效率比真实世界数据收集高20倍，生成了包含1244个场景、总长度达363.2公里的大规模导航数据集。在四足、轮式和人形机器人上的实验表明，NavDP在室内和室外多样化环境中表现出最先进的性能和一致性卓越的泛化能力。此外，我们初步尝试使用Gaussian Splatting进行域内真实到仿真微调，实验显示添加此类数据可以将成功率提高30%，而不会损害其泛化能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08712" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 12:20:28 GMT</pubDate>
</item>
<item>
<title>SkillFormer：基于多视角融合的技能评估高效架构</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种用于多视角技能评估的参数高效架构SkillFormer。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SkillFormer的新架构，该架构旨在通过融合第一人称和第三人称视频，实现复杂活动中的人类技能水平统一评估。SkillFormer基于TimeSformer模型构建，引入了CrossViewFusion模块，利用多头交叉注意力、可学习门控机制及自适应校准技术来融合特定视角的特征。通过低秩适应方法进行微调，仅需调整少量参数即可显著降低训练成本。实验表明，在EgoExo4D数据集上的多视角设置中，SkillFormer达到了最先进的准确性，同时表现出卓越的计算效率，所需参数量仅为先前基线的1/4.5，训练周期减少至3.75倍。SkillFormer在多个结构化任务中的表现证明了多视角整合在细粒度技能评估中的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.08665" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 13 May 2025 11:27:24 GMT</pubDate>
</item>
<item>
<title>MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder</title>
<link>https://arxiv.org/abs/2505.07916</link>
<guid>https://arxiv.org/abs/2505.07916</guid>
<content:encoded><![CDATA[
We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:25:20 GMT</pubDate>
</item>
<item>
<title>多维度约束框架提升大语言模型指令跟随能力</title>
<link>https://arxiv.org/abs/2505.07591</link>
<guid>https://arxiv.org/abs/2505.07591</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多维度约束框架并生成新测试样本，评估多种大语言模型性能差异。</p><br /><br /><p><strong>摘要：</strong> 现有指令跟随基准测试常依赖模板约束提示，缺乏真实场景多样性且限制精细评估。本研究构建了一个包含三种约束模式、四种约束类别及四个难度等级的多维度约束框架，并开发自动化指令生成流水线，生成了1200个可代码验证的指令跟随测试样本。通过对七大家族19个大语言模型的评估发现，不同约束形式下模型表现差异显著，例如平均表现从一级的77.67%下降到四级的32.96%。此外，利用此方法生成强化学习数据，不仅提升了指令跟随能力，还避免了对整体性能的影响，深入分析表明改进主要源于注意力模块参数调整以增强约束识别。相关代码和数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07591" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:16:55 GMT</pubDate>
</item>
<item>
<title>ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation</title>
<link>https://arxiv.org/abs/2505.07416</link>
<guid>https://arxiv.org/abs/2505.07416</guid>
<content:encoded><![CDATA[
Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP
]]></content:encoded>
<pubDate>Mon, 12 May 2025 06:11:28 GMT</pubDate>
</item>
<item>
<title>gg-bench：评估语言模型通用推理能力的游戏基准</title>
<link>https://arxiv.org/abs/2505.07215</link>
<guid>https://arxiv.org/abs/2505.07215</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">gg-bench是一种动态生成游戏环境的基准，用于评估语言模型的通用推理能力。</p><br /><br /><p><strong>摘要：</strong> gg-bench是一个创新性的游戏环境集合，旨在评估大型语言模型（LLMs）的通用推理能力。与静态基准不同，gg-bench通过大语言模型生成新的游戏描述和代码实现，然后利用强化学习代理进行自我训练。该基准挑战性强，目前最先进的LLMs在其中的表现仅为7-9%，而专门设计的推理模型也仅达到31-36%的胜率。我们公开了生成的游戏、数据生成过程及评估代码，以支持未来的研究工作和基准扩展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07215" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:01:03 GMT</pubDate>
</item>
<item>
<title>INTELLECT-2：首个全球分布式强化学习语言模型训练</title>
<link>https://arxiv.org/abs/2505.07291</link>
<guid>https://arxiv.org/abs/2505.07291</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">INTELLECT-2实现首个320亿参数语言模型的全球分布式强化学习训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了INTELLECT-2，这是首个在全球范围内进行的分布式异步强化学习训练，用于一个拥有320亿参数的语言模型。不同于传统的集中式训练方法，INTELLECT-2通过完全异步的方式，在动态且异构的去中心化计算贡献者群组中训练推理模型。为了支持这种独特的基础设施，我们从零开始构建了多个组件，例如PRIME-RL框架，以及用于验证非可信推理工作者的TOPLOC和高效广播策略权重的SHARDCAST。此外，我们还对标准GRPO训练方案进行了修改，并采用了数据过滤技术，以确保训练稳定性并成功达成训练目标，从而超越了现有320亿参数范围内的最佳推理模型QwQ-32B。最后，我们将INTELLECT-2及其所有代码和数据开源，希望推动去中心化训练领域的开放研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07291" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 03:24:33 GMT</pubDate>
</item>
<item>
<title>LlamaPIE：首款实时主动对话助手提升人类交流体验</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LlamaPIE是一款无需用户明确调用即可提供实时背景辅助的新型对话助手。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LlamaPIE，这是首个旨在通过可穿戴设备提供隐性且简洁指导以增强人类对话的实时主动对话助手。与传统需要用户明确请求的语言模型不同，LlamaPIE能在不打断对话的情况下预测并满足用户需求。为了实现这一目标，我们解决了几个关键挑战，如确定何时响应、生成增强对话的精炼回复、利用用户知识进行上下文感知的辅助以及实现实时的本地处理。为此，我们构建了一个半合成对话数据集，并提出了一个双模型管道：一个小模型用于决定何时响应，一个大模型负责生成回复。我们在真实世界的数据集上评估了该方法，证明其能有效提供有益且不显眼的帮助。用户研究显示，LlamaPIE相较于无辅助基线和反应型模型更受青睐，展示了其在提升实时对话中的潜力。关键词：对话助手、实时辅助、上下文感知。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04066" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 22:08:56 GMT</pubDate>
</item>
<item>
<title>Triply-Hierarchical Diffusion Policy: 强化机器人视觉-动作学习的层级结构方法</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型的层级扩散策略，显著提升机器人视觉-动作学习性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Triply-Hierarchical Diffusion Policy（H^3DP）的新框架，该框架通过引入三层层级结构加强视觉特征与动作生成之间的整合。具体而言，H^3DP包含深度感知输入分层、多尺度视觉表示以及层级条件扩散过程。实验表明，在44个模拟任务中，H^3DP相比基线平均提升了27.5%，并在四个具有挑战性的双臂现实世界操作任务中表现出色。这项研究为机器人视觉-动作学习提供了新的思路和技术支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:59:43 GMT</pubDate>
</item>
<item>
<title>无需向量量化：连续视觉自回归生成框架</title>
<link>https://arxiv.org/abs/2505.07812</link>
<guid>https://arxiv.org/abs/2505.07812</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需向量量化即可进行连续视觉自回归生成的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对传统视觉自回归模型在处理连续模态（如视觉数据）时通常依赖于基于量化的方法而导致信息损失的问题，引入了一种新的连续视觉自回归（Continuous VAR）框架。该框架利用严格适当评分规则作为理论基础，通过选择适当的评分规则并将其设为训练目标，实现了直接的连续视觉自回归生成，而无需进行向量量化操作。文中特别探讨了基于能量评分的训练目标，该目标无需显式计算似然性，从而解决了在连续空间中进行概率预测的难题。此外，其他方法如GIVT和扩散损失也可以通过本框架中的其他严格适当评分规则推导得出。这一新框架为连续模态数据的生成建模提供了更加灵活和高效的选择。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07812" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:58:14 GMT</pubDate>
</item>
<item>
<title>Skywork-VL Reward：一种多模态奖励模型</title>
<link>https://arxiv.org/abs/2505.07263</link>
<guid>https://arxiv.org/abs/2505.07263</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种用于多模态理解和推理任务的奖励模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Skywork-VL Reward的多模态奖励模型，该模型能够为多模态理解和推理任务提供奖励信号。技术方法包括构建大规模多模态偏好数据集，以及基于Qwen2.5-VL-7B-Instruct设计奖励模型架构并进行多阶段微调。实验结果显示，Skywork-VL Reward在多模态VL-RewardBench上达到最先进水平，并在文本-only RewardBench基准上表现优异。此外，基于此模型构建的偏好数据对混合偏好优化训练非常有效，显著提升了多模态推理能力。研究强调了Skywork-VL Reward作为通用可靠奖励模型的重要进展，并已公开发布以促进透明度和可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07263" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 02:23:08 GMT</pubDate>
</item>
<item>
<title>UMoE：通过统一设计提升Transformer模型的稀疏混合专家性能</title>
<link>https://arxiv.org/abs/2505.07260</link>
<guid>https://arxiv.org/abs/2505.07260</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出UMoE架构，通过重新定义注意力机制实现FFN与注意力模块的参数共享。</p><br /><br /><p><strong>摘要：</strong> 近年来，稀疏混合专家（MoE）架构被广泛应用于扩展Transformer模型，尤其是在前馈网络（FFN）层的应用取得了显著成效。然而，基于注意力机制的MoE层因需要特殊实现且性能不如FFN层而面临挑战。本研究通过重新构建注意力机制，揭示了注意力模块中隐藏的类似FFN结构，并提出UMoE架构，在保持注意力机制优势的同时实现了高效参数共享，从而在性能上超越了传统的FFN-MoE方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07260" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 02:21:44 GMT</pubDate>
</item>
<item>
<title>DynamicRAG：通过强化学习优化检索增强生成模型</title>
<link>https://arxiv.org/abs/2505.07233</link>
<guid>https://arxiv.org/abs/2505.07233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种动态调整文档数量和顺序的RAG框架，显著提升知识密集型任务性能。</p><br /><br /><p><strong>摘要：</strong> 检索增强生成（RAG）系统结合大型语言模型（LLM）与外部知识检索，在知识密集型任务中表现出色，但其中重排序器组件的重要性常被忽视。现有方法在确定最佳文档数量（k）时面临挑战，且大多依赖模型内部知识而忽略LLM提供的丰富监督信号。本文提出DynamicRAG框架，通过强化学习优化重排序器，使其根据查询动态调整文档数量和顺序。实验结果显示，DynamicRAG在七个知识密集型数据集上达到最优性能，刷新多项记录。该研究为RAG系统的改进提供了新思路，同时代码与模型已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 01:19:01 GMT</pubDate>
</item>
<item>
<title>基于多模态大语言模型的照片润色方法</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用多模态大语言模型进行照片润色的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过多模态大语言模型（MLLM）对原始照片进行批评、提出修复建议并执行操作，以实现高质量的照片润色。与传统的基于文本或笔画的生成编辑工具相比，这种方法更加保守且可预测，同时保留了对象细节和分辨率。研究者通过训练MLLM解决专门设计的视觉难题，使其具备图像处理操作意识，并结合专家编辑的数据集合成推理数据集，用于微调。实验表明，该方法在解释性和身份保存方面优于现有的生成式和其他程序化替代方案。代码、数据、模型及补充结果可在项目网站获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06176" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 12:38:27 GMT</pubDate>
</item>
<item>
<title>PASSAT：一种融合物理与地形信息的天气预测深度学习模型</title>
<link>https://arxiv.org/abs/2505.04918</link>
<guid>https://arxiv.org/abs/2505.04918</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出PASSAT模型，结合物理方程和地球表面拓扑优化天气预测。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度学习天气预测模型忽视物理过程或地形信息的问题，开发了一种名为PASSAT的新模型。PASSAT通过求解对流方程和纳维-斯托克斯方程描述天气演化中的对流过程，同时利用球形图神经网络捕捉地球-大气相互作用，并考虑地球表面的真实拓扑结构而非平面假设。实验结果显示，在ERA5数据集上，PASSAT的表现优于最先进的深度学习模型及传统数值天气预报模型IFS T42。代码和模型参数已在GitHub上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04918" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 23:25:55 GMT</pubDate>
</item>
<item>
<title>生成式人工智能评估中的危机与竞赛标准化</title>
<link>https://arxiv.org/abs/2505.00612</link>
<guid>https://arxiv.org/abs/2505.00612</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">传统机器学习评估策略无法满足现代生成式AI模型评估需求。</p><br /><br /><p><strong>摘要：</strong> 本文指出生成式人工智能（GenAI）的评估正面临严重危机，因为传统的机器学习评估和基准测试策略不足以应对现代GenAI模型的需求。这些问题源于模型输入输出空间几乎无限、缺乏明确的ground truth目标，以及强反馈循环和上下文依赖性等特性。此外，我们强调泄漏（leakage）和污染（contamination）是GenAI评估中最重要且难以解决的问题。有趣的是，AI竞赛领域已发展出有效措施对抗泄漏，以防止恶意参与者作弊。因此，AI竞赛可视为生成式AI评估的黄金标准，值得更广泛的应用和借鉴。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00612" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 11:43:51 GMT</pubDate>
</item>
<item>
<title>DanceGRPO：首个统一强化学习框架实现视觉生成多领域适配</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DanceGRPO首次将GRPO优化算法应用于视觉生成，显著提升多种模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DanceGRPO，这是一种全新的强化学习框架，旨在解决现有基于强化学习的视觉生成方法中的关键问题，如与现代采样范式的兼容性不足、大规模训练的不稳定性以及视频生成验证缺乏等。DanceGRPO首次实现了在两种生成范式（扩散模型和修正流）、三种任务（文本到图像、文本到视频、图像到视频）、四种基础模型（Stable Diffusion、HunyuanVideo、FLUX、SkyReel-I2V）以及五种奖励模型（图像/视频美学、文本-图像对齐、视频运动质量及二元反馈）上的统一适配。实验表明，该框架在多个基准测试（如HPS-v2.1、CLIP Score、VideoAlign、GenEval）中较基线提升了高达181%，不仅增强了复杂视频生成的策略优化稳定性，还改善了去噪轨迹捕捉及从稀疏反馈中学习的能力。DanceGRPO标志着在视觉生成领域的强化学习从人类反馈（RLHF）任务上取得了重大进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07818" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:59:34 GMT</pubDate>
</item>
<item>
<title>连续预训练中的学习动态及扩展定律研究</title>
<link>https://arxiv.org/abs/2505.07796</link>
<guid>https://arxiv.org/abs/2505.07796</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索大语言模型在连续预训练中的学习动态及其扩展定律。</p><br /><br /><p><strong>摘要：</strong> 本文研究了连续预训练（CPT）过程中大型语言模型的学习动态，重点关注每一步中通用性能和特定领域性能的变化，通过验证损失衡量领域性能。我们发现CPT损失曲线本质上描述了从一个曲线到另一个隐藏曲线的转变，可以通过解耦分布偏移和学习率退火的影响来描述。由此推导出的CPT扩展定律结合了这两个因素，能够预测任何连续训练步骤下的损失值，并适用于不同的学习率调度。我们的公式全面解释了CPT中的多个关键因素，如损失潜力、峰值学习率、训练步数、回放比例等。此外，该方法还可根据不同的CPT目标定制超参数。大量实验表明，该扩展定律适用于多种CPT数据集和超参数设置。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07796" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:47:32 GMT</pubDate>
</item>
<item>
<title>基于分块推理的LLMs长上下文处理效率提升研究</title>
<link>https://arxiv.org/abs/2505.07793</link>
<guid>https://arxiv.org/abs/2505.07793</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示分块推理方法可显著提高多种长上下文模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于近期大型语言模型（LLMs）中发展出的递归次二次模型，这些模型旨在提高长上下文处理效率。我们对主要的大规模长上下文模型进行了实验分析，重点关注其固定大小的递归记忆对其表现的影响。结果显示，尽管这些模型经过长时间上下文训练，但其长上下文的实际利用率仍较低。通过采用基于分块的推理程序，即仅识别并处理输入中最相关的部分，可以有效缓解递归记忆失效问题，并在许多长上下文任务中表现出色。具体而言，在LongBench基准测试中，我们的方法使Falcon3-Mamba-Inst-7B的总体性能提高了14%，Falcon-Mamba-Inst-7B提升了28%，RecurrentGemma-IT-9B提升了50%，RWKV6-Finch-7B提升了51%。令人惊讶的是，这一简单策略还在极具挑战性的LongBench v2基准测试中取得了最先进的成果，展示了与同等规模Transformer模型相媲美的竞争力。此外，我们的研究还引发了关于递归模型是否真正利用了长距离依赖关系的思考，因为单一分块策略的表现甚至优于一些需要跨上下文关系的任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07793" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:45:05 GMT</pubDate>
</item>
<item>
<title>通过同伴学习解决大型推理模型的前缀主导陷阱</title>
<link>https://arxiv.org/abs/2505.07787</link>
<guid>https://arxiv.org/abs/2505.07787</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Learning from Peers方法提升大型推理模型的错误修正能力。</p><br /><br /><p><strong>摘要：</strong> 大型推理模型(LRMs)虽能自我纠正错误，但若推理初始路径不佳则难以恢复，此现象称为“前缀主导陷阱”。受心理学启发，我们提出了Learning from Peers(LeaP)，让推理路径通过路由机制共享中间结果，从而整合同伴见解。实验表明，LeaP显著提升了多种数学基准测试的表现，例如QwQ-32B平均高出基线近5个百分点。此外，经过微调的LeaP-T-7B在AIME 2024上表现与更大规模的DeepSeek模型相当。深入分析显示，LeaP通过及时的同伴见解实现稳健的错误修正，表现出强大的容错能力和处理任务难度差异的能力。LeaP标志着大型推理模型在推理过程中协作的重要里程碑，相关代码、数据集和模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07787" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 13:39:56 GMT</pubDate>
</item>
<item>
<title>Step1X-3D：推动可控3D资产生成的开源框架</title>
<link>https://arxiv.org/abs/2505.07747</link>
<guid>https://arxiv.org/abs/2505.07747</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Step1X-3D框架解决3D生成挑战，提升生成质量和跨模态一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Step1X-3D的开源框架，旨在应对3D生成领域的数据稀缺、算法局限和生态系统碎片化等问题。该框架通过三个主要方面实现突破：首先，构建了一个包含超过2百万高质量资产的数据集，通过标准化几何和纹理属性提高数据质量；其次，采用两阶段的3D专用架构，结合混合VAE-DiT几何生成器和基于扩散的纹理合成模块，显著提升生成效果；最后，提供模型、训练代码及适应模块的完全开源支持。实验结果显示，Step1X-3D在几何生成和纹理合成上均表现出色，超越现有开源方法，并接近商业解决方案的质量。此外，该框架还首次实现了2D控制技术（如LoRA）向3D生成的直接迁移，推动了开放研究标准的建立。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07747" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 12:56:30 GMT</pubDate>
</item>
<item>
<title>MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining</title>
<link>https://arxiv.org/abs/2505.07608</link>
<guid>https://arxiv.org/abs/2505.07608</guid>
<content:encoded><![CDATA[
We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:30:11 GMT</pubDate>
</item>
<item>
<title>基于强化学习的知识协同推理代理IKEA提升大语言模型性能</title>
<link>https://arxiv.org/abs/2505.07596</link>
<guid>https://arxiv.org/abs/2505.07596</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入知识协同推理代理IKEA减少大语言模型幻觉并优化知识检索。</p><br /><br /><p><strong>摘要：</strong> 大语言模型（LLMs）中常见的幻觉问题可通过检索增强生成（RAG）策略缓解，但现有方法常未能充分利用内部知识，导致冗余检索和推理延迟。本文提出了一种名为IKEA的高效自适应搜索代理，通过识别自身知识边界优先利用内部知识，并仅在必要时求助外部检索。这一改进通过知识边界感知奖励函数和训练数据实现，显著提升了多领域知识推理任务的表现，降低了检索频率并增强了泛化能力，超越了现有基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07596" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 10:21:57 GMT</pubDate>
</item>
<item>
<title>Unified Continuous Generative Models</title>
<link>https://arxiv.org/abs/2505.07447</link>
<guid>https://arxiv.org/abs/2505.07447</guid>
<content:encoded><![CDATA[
Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 07:15:39 GMT</pubDate>
</item>
<item>
<title>基于注意力影响机制的弱监督大规模推理数据选择方法</title>
<link>https://arxiv.org/abs/2505.07293</link>
<guid>https://arxiv.org/abs/2505.07293</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需监督信号的训练自由方法提升大模型推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于通过收集推理密集型预训练数据来增强大型语言模型（LLMs）的复杂推理能力。传统方法依赖于有监督分类器进行数据标注，容易引入领域特定偏差。鉴于注意力头对上下文推理的重要性，我们提出了AttentionInfluence方法，这是一种无需训练且高效的无监督数据选择策略。该方法利用小型预训练语言模型通过简单的注意力头屏蔽操作充当强大的数据选择器。实验中，我们在SmolLM语料库上应用此方法，结合选定的73B令牌子集，预训练了一个7B参数的密集模型，显著提升了多个知识密集型和推理密集型基准测试（如MMLU、AGIEval-en等）的表现，增幅在1.4pp至3.5pp之间。这验证了从弱到强的有效扩展特性，为推理导向的数据选择提供了一条有前景且可扩展的路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07293" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 12 May 2025 03:25:51 GMT</pubDate>
</item>
<item>
<title>Multi-Objective-Guided Discrete Flow Matching用于多目标生物序列设计</title>
<link>https://arxiv.org/abs/2505.07086</link>
<guid>https://arxiv.org/abs/2505.07086</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新框架MOG-DFM，用于高效生成满足多重生物功能需求的生物序列。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Multi-Objective-Guided Discrete Flow Matching (MOG-DFM) 的通用框架，该框架能够引导任何预训练的离散时间流匹配生成器实现多个标量目标之间的帕累托最优折中。现有方法仅处理单一目标或需要连续嵌入，可能导致离散分布失真，而MOG-DFM通过计算候选转换的混合排名方向得分并应用自适应超锥过滤器来确保一致的多目标进展。此外，还训练了两个无条件离散流匹配模型——PepDFM用于多样肽生成，EnhancerDFM用于功能性增强子DNA生成，作为MOG-DFM的基础生成模型。实验表明，MOG-DFM在优化五种属性（溶血性、非污染性、溶解度、半衰期和结合亲和力）的肽结合剂生成以及设计具有特定增强子类别和DNA形状的DNA序列方面表现出色，证明了其作为多属性导向生物分子序列设计的强大工具的价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07086" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 14:17:44 GMT</pubDate>
</item>
<item>
<title>Seed1.5-VL：高性能多模态基础模型</title>
<link>https://arxiv.org/abs/2505.07062</link>
<guid>https://arxiv.org/abs/2505.07062</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Seed1.5-VL在视觉语言任务中表现优异，尤其在多模态推理任务中表现突出。</p><br /><br /><p><strong>摘要：</strong> Seed1.5-VL是一种由532M参数视觉编码器和20B参数的混合专家(MoE)语言模型组成的多模态基础模型。尽管架构紧凑，它在多种公开视觉语言模型(VLM)基准测试和内部评估套件中表现出色，在60个公开基准测试中有38个达到最先进的性能。此外，Seed1.5-VL在基于代理的任务如图形用户界面(GUI)控制和游戏玩法中也优于领先的多模态系统，如OpenAI CUA和Claude 3.7。该模型不仅在视觉和视频理解方面表现出色，还具备强大的推理能力，适用于多模态推理挑战如视觉谜题。这些能力将推动其在多样任务中的广泛应用。本报告详细回顾了我们在构建Seed1.5-VL过程中在模型设计、数据构建和训练方面的经验，希望为后续研究提供灵感。Seed1.5-VL现已通过Volcano Engine平台开放获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.07062" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 11 May 2025 13:28:30 GMT</pubDate>
</item>
<item>
<title>大型语言模型文档归因技术研究</title>
<link>https://arxiv.org/abs/2505.06324</link>
<guid>https://arxiv.org/abs/2505.06324</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出两种方法提升大型语言模型文档归因准确性。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）被广泛应用于文档相关任务，如摘要生成、问答和信息抽取，确保这些系统的可信度和可解释性成为关键问题。归因作为一种核心方法，旨在将模型输出追溯到源文档。然而，由于LLMs可能产生不准确或不精确的响应，评估引用可靠性至关重要。本文提出了两种技术解决方案：一是零样本方法，将归因视为文本蕴含任务，使用flan-ul2模型在AttributionBench的ID和OOD集合上分别提升了0.27%和2.4%；二是探索注意力机制的作用，在使用较小模型flan-t5-small时，F1分数在几乎所有层面上均优于基线，仅在第4层及第8至11层稍逊。通过这些方法，我们期望提高归因过程的可靠性和模型性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06324" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:40:11 GMT</pubDate>
</item>
<item>
<title>基于强化学习的开源小规模LLM指令数据生成框架</title>
<link>https://arxiv.org/abs/2505.06548</link>
<guid>https://arxiv.org/abs/2505.06548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索开源小规模LLM结合半自动化框架生成指令数据并利用强化学习提升性能。</p><br /><br /><p><strong>摘要：</strong> 指令驱动的大语言模型（LLMs）在许多少样本或零样本自然语言处理任务中表现出色，但人工标注指令数据耗时且成本高昂。先前研究通过任务不可知的方式从模型本身自动生成指令，但多依赖昂贵的API模型。本文考察三个开源小规模LLM（如LLaMA 2-7B、LLaMA 2-13B和Mistral 7B），采用半自动化框架减少人力干预和成本，用于微调LLM的指令数据集生成。此外，将基于强化学习（RL）的训练算法融入该框架，进一步提升了性能。实验结果显示，这种基于RL的框架在63%-66%的任务中较以往方法有显著改进。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 10 May 2025 03:23:19 GMT</pubDate>
</item>
<item>
<title>WebGen-Bench：评估基于LLM的多文件网站代码生成能力的新基准</title>
<link>https://arxiv.org/abs/2505.03733</link>
<guid>https://arxiv.org/abs/2505.03733</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文介绍WebGen-Bench，评估LLM生成多文件网站代码库的能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为WebGen-Bench的新基准，用于衡量基于大型语言模型（LLM）的代理在从零开始创建多文件网站代码库方面的能力。该基准包含了由人工注释者与GPT-4o共同协作产生的多样化网站生成指令，涵盖三大类别和十三个小类别，几乎涉及所有重要的网络应用程序类型。为了评估生成网站的质量，我们使用GPT-4o生成针对每项功能的测试用例，并通过手动筛选、调整和组织这些测试用例，最终得到了647个测试用例。每个测试用例指定了对网站执行的操作及其预期结果。我们还构建了WebGen-Instruct，这是一个包含6,667个网站生成指令的数据集。训练Qwen2.5-Coder-32B-Instruct模型在从该数据集子集生成的轨迹上，达到了38.2%的准确性，超过了最好的专有模型。此外，我们评估了三个高性能代码代理框架：Bolt.diy、OpenHands和Aider，使用多个专有和开源LLM作为引擎。其中表现最佳的组合——由DeepSeek-R1驱动的Bolt.diy，在测试用例上的准确率仅为27.8%，凸显了WebGen-Bench的挑战性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03733" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:15 GMT</pubDate>
</item>
<item>
<title>POLAR：高效多视角点云刚性配准方法</title>
<link>https://arxiv.org/abs/2504.21467</link>
<guid>https://arxiv.org/abs/2504.21467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于潜在空间的多视角点云配准方法，显著提升大变换和高退化情况下的性能。</p><br /><br /><p><strong>摘要：</strong> 点云刚性配准是三维计算机视觉中的基础问题，在多视角情况下，传统基于两两配准的方法因同步算法限制而扩展性差，而生成模型虽能克服此问题但难以处理大变换。本文提出POLAR（POint cloud LAtent Registration），通过将配准问题映射到预训练自动编码器的潜在空间，设计考虑退化的损失函数并采用高效的多起点优化策略，实现了对大量视图的高效处理且具备高鲁棒性。实验表明，POLAR在合成数据和真实数据上均优于现有技术。该方法开源且可直接安装使用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 05:42:38 GMT</pubDate>
</item>
<item>
<title>GPT-4o在图像修复领域的潜力与挑战</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">GPT-4o在图像生成方面表现优异，但修复任务中存在像素级结构失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文首次系统评估了GPT-4o模型在多种图像修复任务中的表现。实验表明，尽管GPT-4o生成的图像视觉效果良好，但在像素级结构保真度上往往逊色于真实图像，常见问题是比例变化、物体位置及数量偏移以及视角改变。针对这些问题，通过去雾、去雨和低光照增强三个案例研究，我们发现GPT-4o的输出可以作为强大的视觉先验，显著提升现有去雾网络的性能。此外，我们提供了实用指南和基准框架，以促进未来将GPT-4o集成到图像修复流程中。希望这项研究能推动图像生成领域的发展，同时我们将公开来自超过10个常用图像修复数据集的GPT-4o修复图像以支持后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 16:00:11 GMT</pubDate>
</item>
<item>
<title>UniVLA：一种用于跨形态机器人视觉-语言-动作学习的新框架</title>
<link>https://arxiv.org/abs/2505.06111</link>
<guid>https://arxiv.org/abs/2505.06111</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniVLA通过任务驱动的动作表示和语言指令，实现高效跨形态机器人策略学习。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为UniVLA的新框架，旨在解决现有机器人策略学习方法依赖标注数据且难以跨形态和环境迁移的问题。UniVLA通过引入潜在动作模型从视频中推导任务导向的动作表示，并结合语言指令在DINO特征空间中建立模型，从而有效利用多样化数据源。该框架在多个操作和导航基准测试及真实机器人部署中取得了最先进的成果，相比OpenVLA显著减少了预训练计算量和下游数据需求。此外，随着更多异构数据的加入，UniVLA的性能持续提升，展示了其在规模化和高效机器人策略学习中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06111" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 11:11:13 GMT</pubDate>
</item>
<item>
<title>大型语言模型在英国公共卫生信息领域的知识评估</title>
<link>https://arxiv.org/abs/2505.06046</link>
<guid>https://arxiv.org/abs/2505.06046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估了多个大型语言模型对英国公共卫生信息的知识水平。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的广泛普及，了解其在特定领域内的知识变得至关重要，尤其是在公共卫生领域，获取相关、准确且最新的信息直接影响公众健康。本文介绍了PubHealthBench，这是一个包含超过8000个问题的新基准，用于评估LLMs在处理公共卫生查询时的多项选择题回答能力和自由形式响应能力。通过自动化管道创建的问题集基于从英国政府提取的公共卫生指导文件。对24个LLMs进行测试后发现，最新的私有LLMs（如GPT-4.5、GPT-4.1和o1）在多项选择题设置中的表现优异，超过90%，甚至超越了人类用户的基本搜索引擎使用。然而，在自由形式响应设置中，所有模型的表现均低于75%。因此，尽管最先进的LLMs在提供公共卫生信息方面显示出较高的准确性，但在自由形式响应时仍需额外的安全措施或工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.06046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 09 May 2025 09:42:59 GMT</pubDate>
</item>
<item>
<title>WiserUI-Bench与G-FOCUS：提升UI设计说服力评估的创新方法</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WiserUI-Bench基准和G-FOCUS策略，改进基于视觉语言模型的UI设计说服力评估。</p><br /><br /><p><strong>摘要：</strong> 用户界面(UI)设计的有效性不仅关乎美学，还影响用户行为，这是设计说服力的核心原则。A/B测试虽是确定UI变体对用户参与度影响的主要方法，但成本高且耗时。尽管最近的视觉语言模型(VLMs)可以处理自动化UI分析，但现有方法主要关注孤立的设计属性，而非关键的说服力比较。为解决这一问题，我们引入WiserUI-Bench，这是一个用于成对UI设计说服力评估的基准，包含300对实际UI图像样本及其A/B测试结果和专家解释。此外，我们提出了G-FOCUS，一种新的推理策略，通过减少位置偏差并提高评估准确性增强基于VLM的说服力评估。实验结果显示，G-FOCUS在成对UI评估的一致性和准确性上优于现有方法。这项工作通过促进基于VLM的UI说服力评估，为补充A/B测试提供了途径，推动了可扩展的UI偏好建模和设计优化的进步。代码和数据将公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05026" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:00:32 GMT</pubDate>
</item>
<item>
<title>Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models</title>
<link>https://arxiv.org/abs/2505.02686</link>
<guid>https://arxiv.org/abs/2505.02686</guid>
<content:encoded><![CDATA[
Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 10:33:49 GMT</pubDate>
</item>
<item>
<title>Bielik v3：优化波兰语处理的高效生成式文本模型</title>
<link>https://arxiv.org/abs/2505.02550</link>
<guid>https://arxiv.org/abs/2505.02550</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik v3推出两个参数量分别为1.5B和4.5B的模型，性能媲美更大规模模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik v3系列的两种参数高效生成式文本模型（1.5B和4.5B），专门针对波兰语言处理进行了优化。这些模型通过一系列创新技术实现高性能，例如定制化的波兰语分词器（APT4）、加权指令交叉熵损失函数以及自适应学习率策略，显著提升了训练效率和模型效果。模型基于精心筛选的2920亿标记的语料库进行训练，在多个基准测试中表现出色，如Open PL LLM Leaderboard、复杂波兰文理解基准、Polish EQ-Bench和波兰医学领导力榜单等。其中4.5B参数模型的表现可与规模为其2至3倍的模型相媲美，而1.5B参数模型则在紧凑架构下展现出强劲的性能。这一成果为资源受限的应用场景提供了高质量波兰语AI模型的新标杆。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02550" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 06:39:51 GMT</pubDate>
</item>
<item>
<title>Bielik 11B v2：面向波兰语处理的高效语言模型</title>
<link>https://arxiv.org/abs/2505.02410</link>
<guid>https://arxiv.org/abs/2505.02410</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Bielik 11B v2展示了卓越的波兰语处理能力，同时具备强大的跨语言功能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Bielik 11B v2，这是一种基于Mistral 7B v0.2架构优化的波兰语语言模型，参数规模达到11B，通过深度扩展实现性能提升。该模型引入了加权指令交叉熵损失函数和自适应学习率两项关键技术，显著提升了多语言任务的表现，尤其是在波兰语相关任务中的表现超越了许多更大参数量的模型。此外，其高效的参数利用和多种量化选项使其能够在不同硬件配置上部署，为资源有限的语言建模提供了新的基准。Bielik 11B v2在多项波兰语基准测试中表现出色，涵盖了从语言理解到复杂推理的各种任务，成为少有代表性语言领域的一项重要进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02410" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 03:03:41 GMT</pubDate>
</item>
<item>
<title>基于鲁棒文本水印的大语言模型高效遗忘评估方法</title>
<link>https://arxiv.org/abs/2505.05064</link>
<guid>https://arxiv.org/abs/2505.05064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个面向大语言模型的数据驱动遗忘度量方法WaterDrum。</p><br /><br /><p><strong>摘要：</strong> 现有基于模型效用的遗忘度量方法在实际应用中存在局限性，如忘记集和保留集内容语义相似、重新训练模型不可行等情况下无法准确评估遗忘效果。本文提出了首个面向大语言模型的数据驱动遗忘度量方法WaterDrum，利用鲁棒文本水印技术克服这些限制。此外，我们还引入了新的基准数据集，用于严格评估遗忘算法，并提供了代码和数据集的公开访问链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 04:56:46 GMT</pubDate>
</item>
<item>
<title>RL^V：强化学习中引入验证能力提升LLM推理性能</title>
<link>https://arxiv.org/abs/2505.04842</link>
<guid>https://arxiv.org/abs/2505.04842</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RL^V通过联合训练LLM作为推理器和生成式验证器，显著提高数学问题求解精度并优化计算效率。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLM）推理微调的主流强化学习方法（如GRPO或Leave-one-out PPO），倾向于放弃已学得的价值函数，转而依赖经验估计回报，这限制了测试阶段基于价值函数进行验证的计算扩展性。本文提出了一种名为RL^V的新方法，该方法通过使用强化学习生成的数据，同时训练LLM作为推理器和生成式验证器，从而在不增加显著开销的情况下赋予模型验证能力。实验表明，RL^V使MATH数据集的准确率提升了超过20%，并且相比基础强化学习方法，在测试阶段的并行计算效率提高了8到32倍。此外，RL^V在易难任务迁移及域外任务上均展现出强大的泛化能力，同时在联合扩展并行与顺序测试时间计算时，相较于原始R1模型取得了1.2至1.6倍的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04842" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 18:41:26 GMT</pubDate>
</item>
<item>
<title>视觉-语言-行动模型综述：架构创新与未来展望</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述了近3年视觉-语言-行动模型的最新进展及其应用。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了近年来视觉-语言-行动（VLA）模型的研究进展，涵盖五大主题支柱，如从跨模态学习到整合视觉语言模型、动作规划器和分层控制器的通用代理。通过分析80余篇相关论文，重点讨论了架构创新、高效参数训练及实时推理加速等进步领域，并探讨了人形机器人、自动驾驶、医疗工业机器人等多样化应用场景。同时，文章还针对实时控制、多模态动作表示等挑战提出解决方案，并展望了VLA模型与具身人工智能融合推动社会对齐的智能通用体的未来方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 15:46:43 GMT</pubDate>
</item>
<item>
<title>结合策略优化语言模型对齐：on-policy与off-policy数据的互补优势</title>
<link>https://arxiv.org/abs/2505.02363</link>
<guid>https://arxiv.org/abs/2505.02363</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现on-policy和off-policy数据在偏好优化中具有互补性，提出SIMPLEMIX方法显著提升语言模型对齐效果。</p><br /><br /><p><strong>摘要：</strong> 语言模型对齐依赖于成对偏好数据集，然而关于on-policy数据与off-policy数据在偏好学习中的优劣关系存在争议。本研究通过系统分析指出，on-policy数据在推理任务如数学和编程中表现优异，而off-policy数据在开放式任务如创意写作和个人推荐方面更具优势。基于此，我们提出了SIMPLEMIX方法，通过简单混合两种数据源充分利用其互补特性。实验结果显示，SIMPLEMIX在Alpaca Eval 2.0等多任务基准测试中平均优于单独使用on-policy DPO和off-policy DPO达6.03%，且性能超越了更为复杂的HyPO和DPO-Mix-P方法，平均提高3.05%。这一成果为语言模型的高效对齐提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02363" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:54:44 GMT</pubDate>
</item>
<item>
<title>3D场景生成综述：技术进展与未来方向</title>
<link>https://arxiv.org/abs/2505.05474</link>
<guid>https://arxiv.org/abs/2505.05474</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述深度学习在3D场景生成中的最新进展及四大范式。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于深度生成模型（如GANs、扩散模型）和3D表示（如NeRF、3D高斯模型）的3D场景生成技术取得了显著进步，大幅提升了场景的真实感、多样性和视角一致性。本文系统性回顾了当前最先进的方法，将其分为基于规则生成、神经3D生成、基于图像生成和基于视频生成四大范式，分析技术基础、优缺点及代表性成果，并探讨常用数据集、评估协议及下游应用。尽管如此，生成能力、3D表示、数据标注及评估仍面临挑战，未来研究方向包括更高保真度、物理感知交互生成及统一感知生成模型。该综述旨在为3D场景生成领域的研究人员提供全面参考，并通过项目页面持续跟踪最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05474" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:59:54 GMT</pubDate>
</item>
<item>
<title>Flow-GRPO：首个结合在线强化学习与流匹配模型的方法</title>
<link>https://arxiv.org/abs/2505.05470</link>
<guid>https://arxiv.org/abs/2505.05470</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个将在线强化学习融入流匹配模型的方法Flow-GRPO。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Flow-GRPO的新方法，它是第一个将在线强化学习（RL）与流匹配模型相结合的技术。该方法通过两种策略实现创新：首先，将确定性的常微分方程（ODE）转换为等效的随机微分方程（SDE），使模型在所有时间步上都能匹配原始模型的边缘分布，从而支持统计采样进行RL探索；其次，采用去噪降维策略，在减少训练去噪步骤的同时保持原始推理时间步数，显著提高了采样效率且不降低性能。实验表明，Flow-GRPO在多个文本到图像的任务中表现出色，尤其在复杂组合场景下，经过RL调优的SD3.5模型能够生成几乎完美的对象数量、空间关系及细粒度属性，使得GenEval准确率从63%提升至95%，视觉文本渲染准确率也从59%提高到92%。此外，Flow-GRPO在人类偏好对齐方面取得了显著进步，且未观察到明显的奖励黑客现象，即奖励提升并未牺牲图像质量和多样性，这两者在实验中均保持稳定。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05470" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:45 GMT</pubDate>
</item>
<item>
<title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
<link>https://arxiv.org/abs/2505.05469</link>
<guid>https://arxiv.org/abs/2505.05469</guid>
<content:encoded><![CDATA[
We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:58:18 GMT</pubDate>
</item>
<item>
<title>StreamBridge：将离线Video-LLMs转化为流式模型的高效框架</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">StreamBridge解决了现有视频语言模型向在线场景适配的两大挑战。</p><br /><br /><p><strong>摘要：</strong> 本文提出StreamBridge框架，旨在将现有的离线视频大语言模型（Video-LLMs）改造为具备实时流处理能力的模型。该框架通过引入内存缓冲区与轮次衰减压缩策略支持多轮长上下文交互，同时采用解耦轻量级激活模型实现持续的主动响应。此外，为了支持这一框架，我们构建了Stream-IT数据集，专门用于流式视频理解任务。实验表明，StreamBridge显著提升了多种任务下的流式理解性能，甚至超过了GPT-4o和Gemini 1.5 Pro等专有模型，在标准视频理解基准测试中也表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05467" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 13:57:40 GMT</pubDate>
</item>
<item>
<title>英语推理能力的跨语言泛化研究</title>
<link>https://arxiv.org/abs/2505.05408</link>
<guid>https://arxiv.org/abs/2505.05408</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语推理微调可提升多语言数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本研究探讨了针对英语推理进行微调的大型多语言语言模型在跨语言推理中的泛化能力。实验表明，增加英语推理模型的推理计算规模可以显著提高多种语言（包括低资源语言）的数学推理能力，甚至超越更大规模的模型。同时，虽然这些模型的推理链条主要基于英语，但它们能够通过引用和思考非英语输入的方式进行跨语言推理。此外，研究揭示了一种有效控制推理链条语言的方法，并发现模型在高资源语言上的表现更优。然而，模型在跨领域推理上表现出较差的泛化能力，特别是在从科学、技术、工程和数学（STEM）到文化常识知识的转换中。综上所述，该研究展示了英语推理在测试阶段扩展的潜力、机制及其局限性，并建议在高资源语言中进行推理，同时需进一步改进低资源语言及跨领域推理的能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05408" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 12:50:06 GMT</pubDate>
</item>
<item>
<title>基于情境学习的贡献度测量方法ICon提升大语言模型训练效率</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需梯度计算的情境学习贡献度测量方法ICon，显著提升大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ICon的新方法，它利用情境学习（ICL）的隐式微调特性，无需梯度计算或人工设计启发式指标，即可评估样本对模型训练的贡献度。ICon通过衡量隐式学习下性能的变化，有效筛选高贡献数据，显著降低了训练成本并提升了模型性能。实验表明，在LLaMA3.1-8B上，仅使用15%经ICon筛选的数据即可超越完整数据集的表现，并优于现有常用数据选择方法。进一步分析显示，ICon选出的高贡献样本不仅涵盖多样化任务，还具有适中的难度，而非仅仅是最难的任务。ICon为大语言模型的数据选择提供了高效且低偏见的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05327" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:17:37 GMT</pubDate>
</item>
<item>
<title>弹性推理框架实现可控的大规模链式思维推理</title>
<link>https://arxiv.org/abs/2505.05315</link>
<guid>https://arxiv.org/abs/2505.05315</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出弹性推理框架解决大规模模型推理长度不可控问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型推理模型在复杂任务中生成过长链条思维的问题，提出了一种名为弹性推理的新框架。该框架将推理过程分为独立分配预算的思考和解决方案两个阶段，在测试时优先保证解决方案的完整性，显著提升了在资源受限情况下的可靠性。同时，通过引入轻量级预算约束滚动策略，使模型在思考过程被截断时也能自适应推理，并有效泛化到未见过的预算限制下，而无需额外训练。实验表明，弹性推理在严格的预算限制下表现稳健，且训练成本显著低于基线方法，即使在无约束设置中也产生更简洁高效的推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05315" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 11:01:06 GMT</pubDate>
</item>
<item>
<title>语言引导的3D场景物体放置任务及基准</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的语言引导3D场景物体放置任务并创建基准。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一项名为“语言引导的3D场景物体放置”的新任务，模型需要根据点云、3D资产和文本提示，在3D场景中找到符合要求的物体位置。此任务相较于其他3D场景中的语言引导定位任务具有独特挑战，如多解性和对三维几何关系的理解需求。为了推动该领域的发展，我们提出了一个新的基准和评估协议，同时发布了用于训练3D大语言模型的数据集，并提供了首个非平凡基线方法。我们认为，这项具有挑战性的任务及其新基准将成为评估和比较通用3D大语言模型的标准之一。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05288" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 10:29:11 GMT</pubDate>
</item>
<item>
<title>Fine-Grained CLIP：通过多模态增强实现细粒度理解</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Fine-Grained CLIP，显著提升图像细粒度理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比语言-图像预训练模型（CLIP）在细粒度理解上的局限性，提出Fine-Grained CLIP（FG-CLIP）。该方法通过利用大规模多模态模型生成包含全局语义细节的长描述图像对、构建高质量区域标注数据集以及引入硬负样本等创新手段，大幅提升了模型区分细微语义差异的能力。实验表明，FG-CLIP在细粒度理解、开放词汇目标检测、图像-文本检索及通用多模态基准任务上均优于现有方法。相关代码、数据和模型已开源。关键词：多模态学习、细粒度理解、CLIP</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.05071" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 05:06:53 GMT</pubDate>
</item>
<item>
<title>链式思维令牌在复杂推理中的变量特性研究</title>
<link>https://arxiv.org/abs/2505.04955</link>
<guid>https://arxiv.org/abs/2505.04955</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示链式思维令牌在解决复杂任务时的功能类似计算机程序中的变量。</p><br /><br /><p><strong>摘要：</strong> 本文通过实证研究探讨大型语言模型中链式思维（CoT）令牌在复合任务如多数字乘法和动态规划中的作用。尽管CoT对解决问题至关重要，但实验表明仅保留存储中间结果的令牌即可实现相近性能。此外，使用替代潜在形式存储中间结果并未影响模型表现。随机干预部分CoT值后，后续令牌及最终答案随之改变。这些发现表明CoT令牌可能类似于程序中的变量，但存在未预料到的捷径和令牌间计算复杂性限制等潜在问题。相关代码与数据已在GitHub上开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04955" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 08 May 2025 01:32:36 GMT</pubDate>
</item>
<item>
<title>多模态推理模型的研究进展与未来展望</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态推理模型在人工智能中实现复杂推理能力，面临泛化、深度及自主行为等挑战。</p><br /><br /><p><strong>摘要：</strong> 推理是智能的核心，尤其在开放、不确定且多模态的环境中，推理能力对人工智能系统至关重要。大型多模态推理模型（LMRMs）通过整合文本、图像、音频和视频等模态，支持复杂的推理功能。早期研究基于特定任务模块，而现代方法则转向统一的语言中心框架，通过指令微调和强化学习提升推理效果。然而，跨模态泛化、推理深度及自主行为仍面临诸多挑战。本文综述了多模态推理领域的研究进展，提出四阶段发展路线图，涵盖任务特定模块、统一多模态大模型、以及原生多模态推理模型（N-LMRMs）。通过分析O3和O4-mini等基准测试案例，探讨了在真实复杂环境中的可扩展性、自主性和适应性推理规划的潜在方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 23:35:23 GMT</pubDate>
</item>
<item>
<title>迈向通用多模态模型：General-Level评估框架与General-Bench基准</title>
<link>https://arxiv.org/abs/2505.04620</link>
<guid>https://arxiv.org/abs/2505.04620</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多模态大语言模型正向通用化发展，但性能提升是否等同于更强能力？</p><br /><br /><p><strong>摘要：</strong> 当前多模态大型语言模型（MLLM）正迅速发展，从理解多种模态到跨模态生成，其能力已扩展至细粒度理解及任意模态支持。然而，现有评估基准无法简单将任务表现直接关联至模型的整体能力。本文提出General-Level评估框架，定义五级性能与通用性标准，通过协同效应衡量模型能力一致性，并推出包含超70万实例的General-Bench基准，涵盖325,800多个样本和700多项任务。该研究通过对上百种顶级MLLM的评估揭示了通用化模型的能力排名及其面临的挑战，为下一代多模态基础模型的研究提供了坚实基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04620" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:32 GMT</pubDate>
</item>
<item>
<title>X-Reasoner：通过文本后训练实现跨模态和跨领域可泛化推理</title>
<link>https://arxiv.org/abs/2505.03981</link>
<guid>https://arxiv.org/abs/2505.03981</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究证明通用文本后训练可使视觉语言模型具备跨模态和跨领域的强泛化推理能力。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，专有模型在多模态推理方面表现出色，但现有开源研究多集中于文本推理，且评估局限于数学和通用领域任务。本文探讨了推理是否能在模态和领域间通用这一基础问题，并通过引入X-Reasoner模型，验证了通用领域文本后训练可以有效提升推理的泛化能力。该模型采用两阶段方法：初始监督微调阶段结合蒸馏后的长链思维，随后通过可验证奖励进行强化学习。实验显示，X-Reasoner在多模态和跨领域设置中表现优异，超越了现有基于领域内和多模态数据训练的最先进模型。进一步研究发现，针对特定领域的文本数据继续训练可进一步提高X-Reasoner在专业领域的性能，由此推出了X-Reasoner-Med，其在多种文本和多模态医学基准测试中达到新高度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03981" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 17:08:27 GMT</pubDate>
</item>
<item>
<title>LiftFeat：通过三维几何特征增强视觉定位中的局部特征匹配</title>
<link>https://arxiv.org/abs/2505.03422</link>
<guid>https://arxiv.org/abs/2505.03422</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种轻量级网络LiftFeat，利用三维几何特征提升特征描述器的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为LiftFeat的新轻量级网络，旨在提高视觉特征描述器在极端条件下的鲁棒性和区分度。传统方法在光照变化剧烈、纹理稀少或存在重复模式的情况下难以提取有效特征，而LiftFeat通过结合伪表面法线标签和预测的表面法线，设计了一个三维几何感知特征提升模块，将二维原始描述符与表面法线特征融合，显著增强了描述符在极端环境下的性能。实验表明，LiftFeat在相对位姿估计、单应性估计和视觉定位等任务上优于现有的一些轻量级方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03422" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:59:23 GMT</pubDate>
</item>
<item>
<title>SAGE框架评估大语言模型的社会认知能力</title>
<link>https://arxiv.org/abs/2505.02847</link>
<guid>https://arxiv.org/abs/2505.02847</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">SAGE框架通过模拟情感变化评估大语言模型的社会认知能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sentient Agent as a Judge (SAGE) 的自动化评估框架，用于衡量大型语言模型（LLM）的高级社会认知能力。SAGE通过实例化一个具备人类情感变化和内心思考模拟的智能代理，在多轮对话中提供更真实的模型评估。实验结果显示，SAGE的最终情感评分与Barrett-Lennard关系量表（BLRI）评分及话语级同理心指标高度相关，验证了其心理真实性。此外，基于SAGE构建的公开排行榜揭示了前沿系统（如GPT-4o-Latest、Gemini2.5-Pro）与早期基线之间显著的能力差距，这种差距在传统排行榜中并未显现。因此，SAGE成为追踪真正具有同理心和社会适应能力的语言代理发展的重要工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02847" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 15:06:10 GMT</pubDate>
</item>
<item>
<title>BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese</title>
<link>https://arxiv.org/abs/2504.19314</link>
<guid>https://arxiv.org/abs/2504.19314</guid>
<content:encoded><![CDATA[
As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 13:32:43 GMT</pubDate>
</item>
<item>
<title>OpenVision：开源视觉编码器家族挑战CLIP</title>
<link>https://arxiv.org/abs/2505.04601</link>
<guid>https://arxiv.org/abs/2505.04601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenVision提供完全开源的视觉编码器，性能媲美CLIP。</p><br /><br /><p><strong>摘要：</strong> OpenAI的CLIP自2021年初发布以来一直是构建多模态基础模型的首选视觉编码器。然而，近期的替代方案如SigLIP虽开始挑战这一地位，但大多存在数据闭源或训练配方未公开的问题。本文通过推出OpenVision填补了这一空白，OpenVision是一个完全开源且成本效益高的视觉编码器系列，在整合到LLaVA等多模态框架时表现可媲美甚至超越CLIP。基于现有工作如CLIPS训练框架和Recap-DataComp-1B训练数据，OpenVision揭示了提升编码器质量的关键见解，并展示了在推进多模态模型方面的实际益处。通过提供参数规模从5.9M到632.1M不等的多种视觉编码器，OpenVision为实践者提供了在容量与效率之间灵活权衡的选择，较大模型提升了多模态性能，而较小版本则支持轻量级边缘部署。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:48:35 GMT</pubDate>
</item>
<item>
<title>COSMOS：在资源约束下高效预测大语言模型适配结果</title>
<link>https://arxiv.org/abs/2505.01449</link>
<guid>https://arxiv.org/abs/2505.01449</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出COSMOS框架，大幅降低大语言模型适配性能和成本预测的成本。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在资源受限条件下，是否可以准确预测大语言模型（LLMs）的性能和成本，而无需进行昂贵的试验。我们形式化了LLMs的策略选择问题，并引入了COSMOS框架，该框架通过轻量级代理模型和低样本缩放律预测微调性能和检索增强上下文学习结果。对八个代表性基准的广泛评估显示，COSMOS在平均情况下将计算成本降低了92.72%，在资源密集型场景中最高可减少98.71%。实验结果表明，这种高效的预测方法不仅可行，还能显著减少LLMs部署的计算开销，同时保持性能标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01449" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 22:06:26 GMT</pubDate>
</item>
<item>
<title>OmniGIRL：多语言、多模态、多领域的GitHub问题自动解决基准测试</title>
<link>https://arxiv.org/abs/2505.04606</link>
<guid>https://arxiv.org/abs/2505.04606</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OmniGIRL是一个面向多语言、多模态、多领域GitHub问题解决的基准测试。</p><br /><br /><p><strong>摘要：</strong> 当前针对大型语言模型（LLMs）在GitHub问题自动解决任务中的评估基准主要存在三个局限性：仅限单一编程语言、覆盖领域狭窄以及只关注文本信息。本文提出OmniGIRL，这是一个涵盖四种编程语言（Python、JavaScript、TypeScript和Java）及八个不同领域的多语言、多模态、多域基准测试，包含959个任务实例。实验表明，现有LLMs在此基准上的表现有限，即使最佳模型GPT-4o也仅解决了8.6%的问题。此外，LLMs在处理涉及图像信息的问题时表现尤为困难，Claude-3.5-Sonnet在包含图像信息的问题上取得了10.5%的最佳解决率。最后，本文分析了LLMs在OmniGIRL上表现不佳的原因，并为未来改进提供了见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04606" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:51:10 GMT</pubDate>
</item>
<item>
<title>轻量级外部信息驱动的自适应检索方法</title>
<link>https://arxiv.org/abs/2505.04253</link>
<guid>https://arxiv.org/abs/2505.04253</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于外部信息的轻量级自适应检索方法，提升问答性能并降低计算成本。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）容易产生幻觉，而检索增强生成（RAG）虽能缓解这一问题，但计算成本高昂且存在信息误导风险。现有自适应检索方法依赖LLM的不确定性估计，效率较低且不实用。本研究引入了一种独立于LLM的轻量级自适应检索方法，该方法基于外部信息。我们分析了27个特征，分属7组及其混合组合，并在6个问答数据集上评估了这些方法的问答表现与效率。实验结果显示，我们的方法在性能上可媲美复杂的LLM基方法，同时显著提高了效率，展示了外部信息在自适应检索中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04253" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 04:58:52 GMT</pubDate>
</item>
<item>
<title>RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2505.03538</link>
<guid>https://arxiv.org/abs/2505.03538</guid>
<content:encoded><![CDATA[
Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 09:50:57 GMT</pubDate>
</item>
<item>
<title>Cognitio Emergens框架：重新定义人机协作的科学知识创造</title>
<link>https://arxiv.org/abs/2505.03105</link>
<guid>https://arxiv.org/abs/2505.03105</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出Cognitio Emergens框架，重新审视人机合作中的科学知识创造。</p><br /><br /><p><strong>摘要：</strong> 随着人类和AI系统逐渐超越工具使用者的关系，进入协同演化的认识伙伴关系阶段，科学知识的创造方式正在发生根本性变革。例如，AlphaFold在蛋白质结构预测上的突破改变了研究人员对基本关系的认知方式。本文引入Cognitio Emergens（CE）框架，该框架旨在解决现有模型的关键局限性，这些模型往往关注静态角色或狭窄指标，而未能捕捉科学理解如何通过递归的人机交互随着时间推移而产生。CE框架由三个组成部分构成：首先，Agency Configurations描述了人类和AI之间权威分配的方式，包括指导型、贡献型和合作伙伴型三种模式，这些模式动态地在不同配置间振荡而非线性发展；其次，Epistemic Dimensions捕捉了六个特定能力，这些能力通过发现、整合和投影轴上的协作而出现，形成了独特的“能力特征”，用于指导发展；最后，Partnership Dynamics确定了影响这些关系发展的力量，特别是认知异化风险，即研究人员可能失去对其正式认可的知识的解释控制。CE框架借鉴了自创生理论、社会系统理论和组织模块化理论，揭示了知识共同创造如何通过持续的角色、价值观和组织结构谈判而产生。通过重新概念化人机科学合作为根本的协同进化过程，CE框架提供了一种平衡的观点，既不盲目庆祝也不过度恐惧AI不断演化的角色，而是提供了概念工具，用以培养既能维持有意义的人类参与又能推动科学突破的合作关系。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03105" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 21:49:44 GMT</pubDate>
</item>
<item>
<title>AutoLibra：基于开放反馈的智能体评估框架</title>
<link>https://arxiv.org/abs/2505.02820</link>
<guid>https://arxiv.org/abs/2505.02820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AutoLibra框架，将人类反馈转化为细粒度智能体行为评估指标。</p><br /><br /><p><strong>摘要：</strong> 当前智能体评估多依赖于任务成功率等粗略指标，这些指标由专家手动设计且无法捕捉中间阶段的行为表现。本文提出AutoLibra框架，通过将开放式的用户反馈（如“按钮禁用时不要重复点击”）转化为对智能体轨迹的细粒度行为评价指标，实现对智能体行为的量化评估。该框架通过行为聚类和具体示例定义指标，可作为大型语言模型评估器的提示来源。同时，引入覆盖率和冗余度两个元指标优化指标集的一致性，实验表明AutoLibra能诱导出比现有基准更具体的评估指标并发现新指标。此外，AutoLibra在文本游戏任务和网页导航任务中展示了其在提升智能体性能和数据筛选上的应用价值，证明其作为任务不可知工具的强大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:49 GMT</pubDate>
</item>
<item>
<title>PrimitiveAnything：一种基于形状条件的几何元素组合生成框架</title>
<link>https://arxiv.org/abs/2505.04622</link>
<guid>https://arxiv.org/abs/2505.04622</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新的形状分解框架，能够生成高质量的几何元素组合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为PrimitiveAnything的新框架，该框架将形状基本元素抽象重新定义为基本元素组装生成任务。通过形状条件下的基本元素变换器和无歧义参数化方案，实现了多种类型的基本元素统一表示。与现有方法相比，新框架直接从大规模手工制作的抽象数据中学习，从而更好地捕捉人类对复杂形状分解的理解能力。实验表明，该方法在保持几何精确性的同时，生成的元素组合更符合人类感知，并且在多样化的形状类别中表现出色。此外，它还适用于多个3D应用领域，并展示了在游戏等用户生成内容中的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04622" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>ZeroSearch：无需实时搜索引擎的大型语言模型检索能力强化学习框架</title>
<link>https://arxiv.org/abs/2505.04588</link>
<guid>https://arxiv.org/abs/2505.04588</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过强化学习提升大型语言模型检索能力，解决文档质量不可控和API成本过高的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ZeroSearch的强化学习框架，用于增强大型语言模型（LLMs）的检索能力，而无需与实时搜索引擎交互。传统方法因文档质量不可控和高昂的API费用面临挑战，ZeroSearch通过轻量级有监督微调将LLM转化为检索模块，并采用基于课程的滚动策略逐步降低生成文档的质量，从而激发模型的推理能力。实验表明，该方法不仅有效提升了LLMs的检索性能，还具有良好的泛化性和与多种强化学习算法的兼容性，且在某些情况下甚至超越了真实搜索引擎的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04588" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 13:30:22 GMT</pubDate>
</item>
<item>
<title>基于确定性马尔可夫决策过程的形式化问题求解框架</title>
<link>https://arxiv.org/abs/2505.04528</link>
<guid>https://arxiv.org/abs/2505.04528</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出FPS框架实现过程验证问题求解并评估多个FTP模型表现。</p><br /><br /><p><strong>摘要：</strong> 本文填补了科学与工程领域问题求解形式化表述的空白，通过将问题求解建模为确定性马尔可夫决策过程，提出了FPS（Formal Problem-Solving）框架，利用现有FTP（Formal Theorem Proving）环境进行过程验证。此外，还设计了D-FPS框架以分离求解与答案验证，增强人类对齐能力。文中证明了这些框架的表达能力、正确性和完备性，并构建了三个问题求解基准测试集：FormalMath500、MiniF2F-Solving和PutnamBench-Solving。同时，引入RPE（Restricted Propositional Equivalence）符号方法评估答案正确性。实验表明，目前流行的FTP模型和提示方法在基准测试中的表现有限，例如在FormalMath500上最多解决23.77%的问题。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04528" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 12:02:14 GMT</pubDate>
</item>
<item>
<title>HunyuanCustom：多模态自定义视频生成框架提升身份一致性</title>
<link>https://arxiv.org/abs/2505.04512</link>
<guid>https://arxiv.org/abs/2505.04512</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出支持多模态条件的HunyuanCustom框架，显著提升定制视频的身份一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为HunyuanCustom的多模态自定义视频生成框架，该框架通过引入文本-图像融合模块和图像ID增强模块来解决身份一致性问题，并支持多种输入模态如图像、音频、视频和文本。具体而言，文本-图像融合模块基于LLaVA增强多模态理解能力，而图像ID增强模块则利用时间串联强化帧间身份特征。此外，为了实现音频和视频条件下的生成任务，设计了特定的模态条件注入机制，包括通过空间交叉注意力实现层次对齐的AudioNet模块，以及通过基于补丁化的特征对齐网络整合潜压缩条件视频的视频驱动注入模块。实验表明，HunyuanCustom在单主体和多主体场景中均优于现有方法，在身份一致性、真实感及文本-视频对齐方面表现优异。此外，该模型在下游任务中也表现出鲁棒性，所有代码和模型资源均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.04512" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 07 May 2025 11:33:18 GMT</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 08:32:01 GMT</pubDate>
</item>
<item>
<title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.03912</link>
<guid>https://arxiv.org/abs/2505.03912</guid>
<content:encoded><![CDATA[
Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 14:35:07 GMT</pubDate>
</item>
<item>
<title>OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents</title>
<link>https://arxiv.org/abs/2505.03570</link>
<guid>https://arxiv.org/abs/2505.03570</guid>
<content:encoded><![CDATA[
In this paper, we introduce OSUniverse: a benchmark of complex, multimodal desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on ease of use, extensibility, comprehensive coverage of test cases, and automated validation. We divide the tasks in increasing levels of complexity, from basic precision clicking to multistep, multiapplication tests requiring dexterity, precision, and clear thinking from the agent. In version one of the benchmark, presented here, we have calibrated the complexity of the benchmark test cases to ensure that the SOTA (State of the Art) agents (at the time of publication) do not achieve results higher than 50%, while the average white collar worker can perform all these tasks with perfect accuracy. The benchmark can be scored manually, but we also introduce an automated validation mechanism that has an average error rate less than 2%. Therefore, this benchmark presents solid ground for fully automated measuring of progress, capabilities and the effectiveness of GUI-navigation AI agents over the short and medium-term horizon. The source code of the benchmark is available at https://github.com/agentsea/osuniverse.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 10:29:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在复杂问题求解中的能力与挑战</title>
<link>https://arxiv.org/abs/2505.03418</link>
<guid>https://arxiv.org/abs/2505.03418</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述大型语言模型在多步推理、领域知识整合及结果验证中的应用与局限。</p><br /><br /><p><strong>摘要：</strong> 随着人工智能的发展，大型语言模型（LLMs）已成为解决复杂问题的强大工具。与传统计算系统不同，LLMs结合了强大的计算能力和类似人类的推理能力，能够在多个领域生成解决方案。然而，将LLMs应用于实际问题解决时面临多步推理、领域知识整合和结果验证等重大挑战。本文探讨了LLMs在复杂问题求解中的能力与限制，分析了链式思维（CoT）、知识增强和多种基于LLMs和工具的验证技术。此外，文章还针对软件工程、数学推理与证明、数据分析建模及科学研究等特定领域的挑战进行了阐述，并讨论了当前LLMs解决方案的基本局限性及其未来发展方向，特别是在多步推理、领域知识整合和结果验证方面的潜在改进路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03418" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 06:53:58 GMT</pubDate>
</item>
<item>
<title>多模态理解与图像生成统一模型的研究综述</title>
<link>https://arxiv.org/abs/2505.02567</link>
<guid>https://arxiv.org/abs/2505.02567</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了多模态理解和图像生成统一模型的最新进展及挑战。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态理解和图像生成领域各自取得了显著进步，但因采用不同的架构范式（如自回归与扩散模型），两者发展相对独立。近期，学术界对统一这两类任务的框架表现出浓厚兴趣，GPT-4o等新模型的出现进一步推动了这一趋势。然而，由于架构差异，实现有效融合仍面临诸多挑战。本文通过介绍多模态理解与文本到图像生成的基础概念与最新进展，梳理现有统一模型的三种主要架构范式（扩散型、自回归型及混合型），并分析相关创新设计。同时，我们整理了适用于统一模型的数据集与基准测试资源，讨论了该领域的关键难题，如标记化策略、跨模态注意力机制及数据需求。作为新兴研究方向，本综述旨在为未来探索提供指导，并将持续更新以反映最新成果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02567" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 07:18:03 GMT</pubDate>
</item>
<item>
<title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.02393</link>
<guid>https://arxiv.org/abs/2505.02393</guid>
<content:encoded><![CDATA[
Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:33:20 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在视觉视角转换中的能力评估</title>
<link>https://arxiv.org/abs/2505.03821</link>
<guid>https://arxiv.org/abs/2505.03821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示现有视觉语言模型在场景理解方面表现良好，但在空间推理和视角转换上存在显著不足。</p><br /><br /><p><strong>摘要：</strong> 本研究通过设计一套基于人类测试的视觉任务，探索视觉语言模型（VLMs）执行视觉视角转换的能力。我们构建了144个独特的视觉任务，这些任务通过控制场景中的物体位置及人形迷你模型的方向等变量，结合鸟瞰图和表面视图来评估模型的视觉认知水平。每个任务配有一系列诊断问题，旨在测试场景理解、空间推理及视觉视角转换三个层次的认知能力。实验结果显示，尽管最先进的模型如GPT-4-Turbo和Llama-3.2-11B-Vision-Instruct在场景理解方面表现出色，但其在空间推理和视角转换上的表现却明显下降。分析表明，当前模型在表面级对象识别与复杂视觉任务所需的深层空间和视角推理之间存在差距，这提示未来VLM开发需整合显式的几何表示并采用定制化的训练方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 20:10:41 GMT</pubDate>
</item>
<item>
<title>R&amp;B框架通过语义重分区和高效优化提升数据混合策略性能</title>
<link>https://arxiv.org/abs/2505.00358</link>
<guid>https://arxiv.org/abs/2505.00358</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">R&amp;B框架通过语义重分区和优化数据组成显著提升语言模型训练效果。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为R&amp;B的框架，旨在解决现有数据混合策略中的两个主要问题：依赖预先设定的数据领域可能导致遗漏关键语义细微差别，且其计算复杂度随领域数量呈指数增长。R&amp;B通过基于语义相似性的重新分组（Regroup）创建更精细的数据领域，并利用训练过程中获得的领域梯度诱导的Gram矩阵来高效优化数据组合（Balance）。与传统方法不同的是，R&amp;B无需额外计算即可获取评估信息如损失或梯度。理论分析表明，该技术在标准规则条件下表现优于非自适应混合方法。实证研究显示，在仅增加0.01%的计算开销下，R&amp;B在五个多样化数据集上的表现匹配甚至超越了最先进的数据混合策略，涵盖了自然语言处理、推理及多模态任务。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00358" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 03:08:19 GMT</pubDate>
</item>
<item>
<title>通过Selective Loss方法提升语言模型对高风险文本的理解能力</title>
<link>https://arxiv.org/abs/2505.03052</link>
<guid>https://arxiv.org/abs/2505.03052</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SLUNG方法，使模型能理解高风险文本而不生成它们。</p><br /><br /><p><strong>摘要：</strong> 语言模型开发者通常会过滤掉预训练数据中的高风险内容（如有毒或受版权保护的文本），以防止模型生成类似内容。然而，这种做法限制了模型识别和适当响应有害或敏感内容的能力。本文介绍了一种名为SLUNG（Selective Loss to Understand but Not Generate）的预训练范式，该方法让模型学会理解高风险数据而不学习生成它们。SLUNG不是均匀地应用下一个令牌预测损失，而是有选择地避免激励生成高风险令牌，同时确保它们保留在模型的上下文窗口内。实验表明，SLUNG在提高模型对高风险数据的理解能力方面始终表现出色，而不会增加生成这些内容的风险。总的来说，SLUNG范式使得模型能够从原本会被过滤掉的高风险文本中受益。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03052" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 18:24:06 GMT</pubDate>
</item>
<item>
<title>Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00212</link>
<guid>https://arxiv.org/abs/2505.00212</guid>
<content:encoded><![CDATA[
Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 19:09:44 GMT</pubDate>
</item>
<item>
<title>SWE-smith: Scaling Data for Software Engineering Agents</title>
<link>https://arxiv.org/abs/2504.21798</link>
<guid>https://arxiv.org/abs/2504.21798</guid>
<content:encoded><![CDATA[
Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:56:06 GMT</pubDate>
</item>
<item>
<title>VITA-Audio：基于多模态预测模块的低延迟语音生成大模型</title>
<link>https://arxiv.org/abs/2505.03739</link>
<guid>https://arxiv.org/abs/2505.03739</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种端到端的高效语音生成模型VITA-Audio，大幅降低实时对话中的初始音频生成延迟。</p><br /><br /><p><strong>摘要：</strong> 随着自然人机交互需求的增长，语音系统备受关注，但现有模型在流式生成首个音频标记时存在高延迟问题。为解决这一瓶颈，我们提出了VITA-Audio，这是一种具有快速音素-文本标记生成能力的端到端大型语音模型。通过引入轻量级的多模态跨模态标记预测（MCTP）模块，该模型在一个前向传播过程中可高效生成多个音频标记，不仅加速推理速度，还显著降低了流式场景下的首次音频生成延迟。此外，我们探索了一种四阶段渐进式训练策略，在保证最小语音质量损失的同时实现模型加速。作为首个能够在首次前向传播中生成音频输出的多模态大语言模型，VITA-Audio支持实时对话功能，且完全开源可复现。实验结果显示，VITA-Audio在70亿参数规模下推理速度提升了3至5倍，并在自动语音识别（ASR）、文本转语音（TTS）和口语问答（SQA）等多个基准测试中显著优于同类开源模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03739" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>面向综合理解的AI在足球领域的框架与贡献</title>
<link>https://arxiv.org/abs/2505.03735</link>
<guid>https://arxiv.org/abs/2505.03735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合知识库、基准测试及多智能体系统的足球全面理解框架。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有AI在足球领域研究集中在孤立任务的问题，提出了一个全面的足球理解框架。首先，构建了SoccerWiki，这是首个大规模多模态足球知识库，整合了丰富的球员、球队、裁判及场馆等领域的知识。其次，开发了SoccerBench，这是一个包含约10K标准化多模态多选题的基准数据集，涵盖13项不同的理解任务。此外，引入了SoccerAgent，这是一种创新的多智能体系统，通过协作推理分解复杂问题，并利用SoccerWiki的专业知识实现稳健性能。最后，通过对SoccerBench的广泛评估和消融实验，展示了所提出的智能体系统的优越性。所有数据和代码均可公开获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>FlexiAct：一种灵活的动作迁移方法</title>
<link>https://arxiv.org/abs/2505.03730</link>
<guid>https://arxiv.org/abs/2505.03730</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FlexiAct通过引入RefAdapter和FAE技术，实现跨多样主体和场景的动作迁移。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlexiAct的新方法，用于在视频中将动作从参考视频迁移到任意目标图像上。传统动作定制方法受限于空间结构的严格约束，如布局、骨架和视角一致性，这限制了其在多样化主体和场景中的适用性。FlexiAct突破了这些限制，允许参考视频主体与目标图像之间存在布局、视角和骨骼结构的变化，同时保持身份一致性。为了实现这一目标，我们引入了轻量级的图像条件适配器RefAdapter，它在空间适应性和一致性保存方面表现出色，超过了现有方法在外观一致性和结构灵活性之间的平衡能力。此外，我们观察到去噪过程在不同时间步对运动和外观细节的关注程度不同，因此提出了频率感知动作提取（FAE），它直接在去噪过程中实现动作提取，而无需依赖单独的空间-时间架构。实验表明，我们的方法可以有效地将动作转移到具有多样化布局、骨骼和视角的主体上。我们发布了代码和模型权重以支持进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03730" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 13:58:02 GMT</pubDate>
</item>
<item>
<title>大型语言模型在地理空间解释性研究中的新框架</title>
<link>https://arxiv.org/abs/2505.03368</link>
<guid>https://arxiv.org/abs/2505.03368</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，揭示大型语言模型处理地理信息的方式。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）在自然语言处理领域展现了强大的能力，但在地理知识处理和空间推理方面的内部机制仍不明确。本文建立了一种基于空间分析的新框架，通过探针技术和机械可解释性方法，探讨LLMs如何处理地理信息。研究利用空间自相关技术，证明了从地名特征中可以发现与地理位置相关的空间模式，为理解这些模型的地理信息处理提供了新视角。最后讨论了该框架对地理领域基础模型研究与应用的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03368" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:40:06 GMT</pubDate>
</item>
<item>
<title>绝对零度范式下的自我进化推理模型</title>
<link>https://arxiv.org/abs/2505.03335</link>
<guid>https://arxiv.org/abs/2505.03335</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出绝对零度范式，使模型无需外部数据即可通过自拟任务提升推理能力。</p><br /><br /><p><strong>摘要：</strong> 强化学习中的可验证奖励方法（RLVR）被证明可以增强大型语言模型的推理能力，但现有方法仍依赖人工标注的问题和答案。为解决人类监督的长期可持续性问题及未来超级智能系统的潜在局限，我们提出了绝对零度（Absolute Zero）范式，该范式下模型自行设计最大化自身学习进度的任务并从中提升推理能力。作为该范式的实例，绝对零度推理器（AZR）利用代码执行器验证任务和答案，提供统一的可验证奖励来源，实现开放且有基础的学习。尽管完全不依赖外部数据，AZR在编码和数学推理任务上达到了当前最优性能，超越了依赖数万个人工标注样本的传统零样本模型，并且适用于不同规模和类型的模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03335" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 05:08:00 GMT</pubDate>
</item>
<item>
<title>UnifiedReward-Think：基于长链条推理的多模态奖励模型</title>
<link>https://arxiv.org/abs/2505.03318</link>
<guid>https://arxiv.org/abs/2505.03318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入长链条推理提升多模态奖励模型的可靠性和准确性。</p><br /><br /><p><strong>摘要：</strong> 近年来，多模态奖励模型在提供符合人类偏好的视觉模型奖励信号方面取得了显著进展。然而，当前的奖励模型通常局限于浅层推理过程，导致奖励信号不准确。本文提出了一种名为UnifiedReward-Think的新方法，该方法通过引入显式的长链条推理增强奖励推理的可靠性和鲁棒性，并通过隐式推理能力提高直接响应的准确性。具体而言，我们采用探索驱动的强化微调方法来诱发和激励模型潜在的复杂推理能力，包括利用少量图像生成偏好数据蒸馏GPT-4o的推理过程用于冷启动学习，以及利用大规模统一多模态偏好数据在多种视觉任务中引发推理过程。实验结果表明，该方法在多个视觉奖励任务中表现出色，验证了其优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 04:46:41 GMT</pubDate>
</item>
<item>
<title>InfoVids：重塑演示者与可视化之间的关系</title>
<link>https://arxiv.org/abs/2505.03164</link>
<guid>https://arxiv.org/abs/2505.03164</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过InfoVids重新定义演示者与可视化的关系，提升人本体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过InfoVids（基于信息图表的视频）改变传统数据展示中演示者与可视化分离的状况。InfoVids旨在建立演示者与可视化之间更平等的关系，探索布局、形式及交互对观看体验的影响。研究对比了InfoVids与传统2D幻灯片，在9项指标下进行测试，并收集了30名参与者的反馈。分析表明，InfoVids减少了观众注意力分散，将关注点从可视化转移到演示者身上，使数据展示更加互动、自然且吸引人。这一方法为重新思考演示者与可视化动态提供了新视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03164" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:18:42 GMT</pubDate>
</item>
<item>
<title>RADLADS协议：高效转换Transformer至线性注意力解码模型</title>
<link>https://arxiv.org/abs/2505.03005</link>
<guid>https://arxiv.org/abs/2505.03005</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RADLADS协议，将Transformer高效转化为线性注意力解码模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Rapid Attention Distillation to Linear Attention Decoders at Scale（RADLADS）协议，用于将softmax注意力的Transformer高效转化为线性注意力解码模型。该研究还提出了两个RWKV变体架构，并基于Qwen2.5开源模型创建了多种规模（7B、32B、72B）的模型。转换过程仅需350-700M tokens，成本低于2000美元，且推理质量接近原始Transformer。这些模型在标准基准测试中表现出色，所有模型均开源于HuggingFace，其中72B模型受Qwen许可协议约束。本文还提供了模型和训练代码的具体链接。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.03005" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 16:03:28 GMT</pubDate>
</item>
<item>
<title>RetroInfer：一种加速长上下文大语言模型推理的新系统</title>
<link>https://arxiv.org/abs/2505.02922</link>
<guid>https://arxiv.org/abs/2505.02922</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RetroInfer系统，通过向量存储和注意力稀疏性优化加速长上下文大语言模型推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为RetroInfer的创新系统，旨在解决大语言模型（LLMs）在高效推理时因上下文长度增加而面临的GPU内存和带宽限制问题。该系统重新定义了键值（KV）缓存为向量存储系统，利用内在的注意力稀疏性来加速长上下文的LLMs推理。RetroInfer的核心是一个波浪索引（wave index），这是一种基于注意力的向量索引，通过三部分注意力近似、有界注意力估计和分段聚类等技术实现对关键令牌的有效且精确检索。此外，系统还配备了波浪缓冲区（wave buffer），用于协调KV缓存放置并重叠GPU和CPU上的计算与数据传输，从而维持高吞吐量。与之前基于稀疏性的方法相比，RetroInfer在不牺牲模型准确性的情况下提供了稳健的性能。实验表明，在GPU内存限制下，相对于完整注意力，RetroInfer可实现高达4.5倍的速度提升；当KV缓存扩展到CPU内存时，相对于稀疏注意力基准，速度提升可达10.5倍，同时保持全注意力级别的准确性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02922" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 14:01:17 GMT</pubDate>
</item>
<item>
<title>基于AttenHScore的大模型与小模型协作优化方法</title>
<link>https://arxiv.org/abs/2505.02311</link>
<guid>https://arxiv.org/abs/2505.02311</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出AttenHScore评估指标提升小模型实时幻觉检测能力。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于大语言模型（LM）与小语言模型协作中的关键挑战——即如何精准定位小模型生成幻觉的时刻。传统优化主要依赖于独立于推理过程的后处理技术，导致高计算成本且效果有限。为此，我们提出了AttenHScore这一实用的调用评估指标，通过累积并传播小模型生成过程中的幻觉现象，动态调整检测阈值实现对大模型的更精确调用。同时，结合不确定性感知的知识重组策略，增强小模型捕获关键信息的能力。实验表明，AttenHScore在多个问答数据集上显著优于基线方法，尤其在处理复杂查询时表现优异，且无需额外的模型训练，具备适应多种Transformer架构LM的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02311" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 21:45:56 GMT</pubDate>
</item>
<item>
<title>Qwen3低比特量化性能评估与挑战</title>
<link>https://arxiv.org/abs/2505.02214</link>
<guid>https://arxiv.org/abs/2505.02214</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探索Qwen3在低比特量化下的表现，揭示压缩大型语言模型的机遇与难题。</p><br /><br /><p><strong>摘要：</strong> Qwen系列作为领先的开源大型语言模型家族，其最新成员Qwen3展现出卓越的自然语言理解能力。然而，在资源受限环境下高效部署这些模型仍具挑战性。本研究系统评估了五种经典后训练量化技术对Qwen3的影响，量化位宽从1到8位不等，并在多个数据集上测试其有效性。结果显示，Qwen3在中等比特宽度下保持竞争力，但在超低精度下显著影响语言任务性能。研究强调了进一步优化极端量化场景下性能的重要性，并提供了针对Qwen3及其未来版本的量化方法改进建议。项目代码已公开于GitHub及Hugging Face。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02214" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 14:43:44 GMT</pubDate>
</item>
<item>
<title>通过眼球运动自动解码开放性阅读目标的研究</title>
<link>https://arxiv.org/abs/2505.02872</link>
<guid>https://arxiv.org/abs/2505.02872</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次探索能否通过眼球运动自动解码开放性阅读目标。</p><br /><br /><p><strong>摘要：</strong> 本研究首次探讨了是否可以通过眼球运动自动解码读者在开放性阅读中的具体目标。为了回答这一问题，我们引入了目标分类和目标重建任务及其评估框架，并利用大规模英文阅读的眼动追踪数据，这些数据涉及数百种特定于文本的信息检索任务。我们开发并比较了几种结合眼球运动和文本的判别型与生成型多模态大语言模型（LLMs），用于目标分类和目标重建。实验结果显示，在两项任务上取得了显著的成功，表明LLMs可以从眼球运动中提取出关于读者文本特定目标的有价值信息。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02872" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:23:48 GMT</pubDate>
</item>
<item>
<title>HoloTime：通过扩散模型实现全景视频到4D场景的重建</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架HoloTime，结合视频扩散模型生成沉浸式4D体验。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了扩散模型在虚拟现实(VR)和增强现实(AR)技术中的应用潜力，特别是针对需要场景级4D资产的用户体验。现有的扩散模型主要集中在静态3D场景或物体级别的动态建模上，限制了它们提供真正沉浸式体验的能力。为了解决这一问题，我们提出了HoloTime框架，它集成了视频扩散模型，可以从单一提示或参考图像生成全景视频，并结合360度4D场景重建方法将生成的全景视频无缝转换为4D资产，从而为用户提供完全沉浸式的4D体验。具体来说，为了控制视频扩散模型以生成高保真的全景视频，我们引入了360World数据集，这是首个适合下游4D场景重建任务的全景视频综合集合。基于此精心策划的数据集，我们提出了Panoramic Animator，这是一种两阶段的图像到视频扩散模型，可以将全景图像转换为高质量的全景视频。随后，我们介绍了Panoramic Space-Time Reconstruction，它利用时空深度估计方法将生成的全景视频转换为4D点云，使优化整体4D高斯点撒表示成为可能，从而重建空间和时间一致的4D场景。通过与现有方法进行比较分析，验证了我们的方法在全景视频生成和4D场景重建方面的优越性，展示了其在创建更具吸引力和真实感的沉浸式环境方面的能力，从而提升了VR和AR应用程序中的用户体验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21650" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:55:28 GMT</pubDate>
</item>
<item>
<title>Auto-SLURP：用于评估大型语言模型驱动多智能体框架的新基准数据集</title>
<link>https://arxiv.org/abs/2504.18373</link>
<guid>https://arxiv.org/abs/2504.18373</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Auto-SLURP，一个针对智能个人助手的多智能体框架评估基准数据集。</p><br /><br /><p><strong>摘要：</strong> 近年来，由大型语言模型（LLMs）驱动的多智能体框架取得了显著进展。然而，缺乏专门用于评估这些框架性能的基准数据集。为解决这一问题，本文提出了Auto-SLURP，这是一个扩展自SLURP数据集的新基准，通过重新标注数据并集成模拟服务器和外部服务，支持从语言理解到任务执行再到响应生成的端到端评估。实验表明，当前最先进的框架在Auto-SLURP上面临严峻挑战，表明真正可靠且智能的多智能体个人助手仍需进一步研究。该数据集及相关代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18373" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 10:17:47 GMT</pubDate>
</item>
<item>
<title>大型语言模型在图结构数据中的注意力机制研究</title>
<link>https://arxiv.org/abs/2505.02130</link>
<guid>https://arxiv.org/abs/2505.02130</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大型语言模型在处理图结构数据时的注意力机制表现欠佳。</p><br /><br /><p><strong>摘要：</strong> 本文从注意力机制的角度出发，对大型语言模型（LLMs）处理图结构数据的方式进行了实证研究。尽管LLMs能够识别图数据并捕获文本节点交互，但在建模节点间关系上存在局限性。此外，LLMs的注意力分布未能适应图拓扑结构的特点，而中间状态注意力窗口在训练和推理阶段均表现出更好的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02130" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 10:40:31 GMT</pubDate>
</item>
<item>
<title>基于Motion-enhanced Event Tensor的RGB-事件模态融合及其应用</title>
<link>https://arxiv.org/abs/2505.01548</link>
<guid>https://arxiv.org/abs/2505.01548</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的事件表示方法MET，解决RGB-事件融合中的三类对齐问题并提升语义分割性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对RGB与事件相机模态融合中存在的时序、空间及模态错配问题，提出了Motion-enhanced Event Tensor (MET) 新型事件表示方法。MET通过利用密集光流和事件时间特征将稀疏事件体素转化为密集且时序一致的形式。此外，引入Frequency-aware Bidirectional Flow Aggregation Module (BFAM) 和 Temporal Fusion Module (TFM)，分别缓解模态错配并解决时空错配问题。实验表明，在两个大规模数据集上，所提框架显著优于现有最先进的RGB-事件语义分割方法。研究代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01548" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 15:19:58 GMT</pubDate>
</item>
<item>
<title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.02005</link>
<guid>https://arxiv.org/abs/2505.02005</guid>
<content:encoded><![CDATA[
Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes (&gt;6.5km^2) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.
]]></content:encoded>
<pubDate>Sun, 04 May 2025 02:25:14 GMT</pubDate>
</item>
<item>
<title>MUSAR：仅需单领域训练数据的多领域定制框架</title>
<link>https://arxiv.org/abs/2505.02823</link>
<guid>https://arxiv.org/abs/2505.02823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种仅需单领域数据即可实现多领域自定义的新框架MUSAR。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有跨领域定制方法面临的两大挑战——多领域训练数据获取困难和属性纠缠问题，提出了名为MUSAR的简单而有效的框架。MUSAR通过引入去偏双联学习解决数据限制问题，利用静态注意力路由和双分支LoRA校正分布偏差；同时借助动态注意力路由机制消除领域间的纠缠，从而实现多领域的解耦表示。实验表明，尽管仅使用单领域数据，MUSAR在图像质量、领域一致性及交互自然性方面均优于现有方法，甚至超越部分基于多领域数据训练的方法。这一成果为多领域定制提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:50:24 GMT</pubDate>
</item>
<item>
<title>ReplaceMe：无需训练的Transformer块剪枝方法</title>
<link>https://arxiv.org/abs/2505.02819</link>
<guid>https://arxiv.org/abs/2505.02819</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种无需训练即可有效剪枝Transformer块的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReplaceMe的新型训练-free深度剪枝方法，该方法通过将Transformer块替换为线性操作，同时保持高精度性能，适用于低压缩比场景。与传统需要额外训练或微调的剪枝方法不同，ReplaceMe仅需一个小规模校准数据集来估计线性变换，从而逼近被剪枝的块。这一线性映射可无缝合并到剩余的Transformer块中，无需添加额外网络参数。实验表明，ReplaceMe在多个大型语言模型上的表现优于其他训练-free方法，并与涉及大量重新训练/微调及架构修改的最新剪枝方法具有竞争力。在开放基准测试中，ReplaceMe实现了高达25%的剪枝率，而性能保留约90%，且没有额外的计算开销。此外，我们开源了一个包含ReplaceMe及其相关技术的库。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02819" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:47:42 GMT</pubDate>
</item>
<item>
<title>Voila：迈向自然人机交互的大型语音语言基础模型</title>
<link>https://arxiv.org/abs/2505.02707</link>
<guid>https://arxiv.org/abs/2505.02707</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Voila是一种新型全双工语音语言模型，实现低延迟、情感丰富的对话交互。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Voila，一种旨在实现自主、实时且情感表达丰富的人机语音交互的大型语音语言基础模型。不同于传统的流水线系统，Voila采用端到端架构，支持毫秒级响应时间，同时保留丰富的语音特征如音调、节奏和情感。它通过层次化多尺度Transformer融合了大语言模型的推理能力与强大的声学建模能力，允许用户通过文本指令定义语音角色特性。此外，Voila还支持百万级预制语音并能高效定制新语音，仅需10秒音频样本即可完成。除了对话功能，Voila还能轻松适应自动语音识别（ASR）、文本转语音（TTS）及多语言语音翻译等多种应用场景。该模型已完全开源，助力开放研究并推动下一代人机交互发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02707" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:05:01 GMT</pubDate>
</item>
<item>
<title>Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL</title>
<link>https://arxiv.org/abs/2505.02391</link>
<guid>https://arxiv.org/abs/2505.02391</guid>
<content:encoded><![CDATA[
Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:26:00 GMT</pubDate>
</item>
<item>
<title>引入推理能力的生成型奖励模型提升奖励建模性能</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出推理奖励模型（ReasRM），显著提高大语言模型对人类偏好的对齐能力。</p><br /><br /><p><strong>摘要：</strong> 奖励建模（Reward Modeling）对于通过强化学习从人类反馈（RLHF）来对齐大型语言模型（LLMs）至关重要。然而，现有的奖励模型要么提供不透明的标量评分，要么直接预测首选答案，导致缺乏解释性。受长链推理（CoT）方法的启发，本研究提出了推理奖励模型（ReasRM），将奖励建模视为一个推理任务。ReasRM通过两个阶段进行训练：高质量推理链蒸馏和基于可验证奖励的强化学习。实验结果显示，所提出的RM-R1模型在多个基准测试中达到或接近当前最佳性能，甚至超越了更大的开源权重模型（如Llama3.1-405B）和专有模型（如GPT-4o）。此外，我们分析了ReasRM成功训练的关键因素，并开源了六个ReasRM模型及相关代码和数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02387" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 02:11:12 GMT</pubDate>
</item>
<item>
<title>Muon优化器在计算效率与数据效能上的改进</title>
<link>https://arxiv.org/abs/2505.02222</link>
<guid>https://arxiv.org/abs/2505.02222</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Muon优化器在计算时间和AdamW之间扩展帕累托前沿，提高大批次训练的数据效率。</p><br /><br /><p><strong>摘要：</strong> 本文展示了Muon作为二阶优化器的最简单实例，如何在计算时间与AdamW的权衡中扩展帕累托前沿。研究发现，Muon在大批次训练中比AdamW更具数据效率，且保持了计算效率，从而实现更经济的训练。此外，Muon与最大更新参数化(muP)的结合用于高效超参数转移，提出了一种考虑所有muP误差来源的简单望远镜算法，仅引入适度的资源开销。通过模型大小高达40亿参数的广泛实验及对数据分布和架构的消融研究验证了这些发现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02222" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 15:14:43 GMT</pubDate>
</item>
<item>
<title>基于演示交互强化学习的数据增强与技能获取</title>
<link>https://arxiv.org/abs/2505.02094</link>
<guid>https://arxiv.org/abs/2505.02094</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解决强化学习中演示噪声与覆盖不足问题的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文解决了强化学习从交互演示中获取技能的一个基础挑战：演示噪声与覆盖限制。现有数据收集方法虽有价值，但常产生稀疏、不连贯且嘈杂的轨迹。我们通过引入两种数据增强技术——Stitched Trajectory Graph (STG) 和 State Transition Field (STF)，有效连接演示技能并构建状态转换场。同时，开发了自适应轨迹采样策略和记忆依赖技能学习机制。实验表明，在多种交互任务上，该方法显著提升了收敛稳定性、泛化能力和恢复鲁棒性，超越当前最先进方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02094" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 09:00:29 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理引擎系统性评估与未来展望</title>
<link>https://arxiv.org/abs/2505.01658</link>
<guid>https://arxiv.org/abs/2505.01658</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究评估25个开源及商业LLM推理引擎，探讨优化方法与适用场景。</p><br /><br /><p><strong>摘要：</strong> 大规模语言模型（LLMs）在聊天机器人、代码生成器和搜索引擎中广泛应用，但复杂推理等任务显著增加推理成本。尽管已有并行化、压缩和缓存等多种优化方法，但服务需求多样化使得方法选择困难。本文对25个开源和商业推理引擎进行全面评估，涵盖易用性、部署便捷性、通用支持、可扩展性和吞吐量/延迟敏感计算的适用性。通过分析各引擎支持的优化技术，揭示其设计目标，并评估开源引擎的生态系统成熟度及商业解决方案的成本性能策略。此外，本文还提出未来研究方向，如支持复杂服务、兼容多种硬件及增强安全性，并提供公共存储库追踪领域进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01658" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 22:47:43 GMT</pubDate>
</item>
<item>
<title>多模态大型语言模型的目标遗忘评估基准研究</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出多模态遗忘学习基准UnLOK-VQA及框架，评估删除特定知识的效果。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大规模数据训练的语言模型可能无意中获取敏感信息的风险，尤其是多模态模型整合文本和图像信息时风险更高。为了应对这一挑战，研究者引入了一个名为UnLOK-VQA的多模态遗忘学习基准，并设计了一套攻击与防御框架，用于评估如何有效删除多模态知识。该基准通过自动化管道扩展视觉问答数据集，同时进行人工筛选，确保高质量。研究对六种防御目标进行了测试，发现多模态攻击比单一模态更具优势，且最有效的防御方法是从模型状态中移除答案信息。此外，更大的模型表现出更强的编辑后鲁棒性，表明规模有助于提高安全性。UnLOK-VQA为多模态大型语言模型的安全性提升提供了严格的评估工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01456" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 21:54:00 GMT</pubDate>
</item>
<item>
<title>通过Grokking增强Transformer模型的多步事实推理能力</title>
<link>https://arxiv.org/abs/2504.20752</link>
<guid>https://arxiv.org/abs/2504.20752</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次将Grokking应用于现实世界事实数据，显著提升多跳推理准确性。</p><br /><br /><p><strong>摘要：</strong> 尽管Transformer模型在众多自然语言处理任务中取得了显著成功，但在缺乏实际知识的情况下进行多步事实推理时仍存在明显不足。最近关于Grokking的研究表明，神经网络可以通过检测潜在逻辑模式实现从记忆到完全泛化的转变，但这些研究主要集中在小型合成任务上。本文首次将Grokking扩展到现实世界的事实数据，并通过精心设计的合成数据扩充现有知识图谱，提高推断事实与原子事实的比例phi_r，从而克服数据稀疏性问题。令人惊讶的是，即使合成数据存在事实错误，也能加强新兴推理电路的形成，而非降低准确性。在多跳推理基准测试中，我们的方法在2WikiMultiHopQA上的准确率达到了95%-100%，大幅优于现有基线模型。此外，我们深入分析了phi_r增加如何促进Transformer内部形成泛化电路。研究结果表明，基于Grokking的数据增强可以解锁隐式的多跳推理能力，为大规模语言模型提供更健壮且可解释的事实推理奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20752" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:33:29 GMT</pubDate>
</item>
<item>
<title>ARTIST：引入具身推理与工具集成的大语言模型框架</title>
<link>https://arxiv.org/abs/2505.01441</link>
<guid>https://arxiv.org/abs/2505.01441</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ARTIST框架通过具身推理和工具集成显著提升大语言模型问题解决能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出ARTIST（具身推理与工具集成的自我改进Transformer），这是一种将具身推理、强化学习和工具集成统一起来的框架，旨在增强大型语言模型（LLMs）的动态、多步推理能力以及适应性决策能力。ARTIST允许模型自主决定何时、如何以及选择哪个工具进行调用，并通过基于结果的强化学习策略优化工具使用和环境交互。实验表明，ARTIST在数学推理和多轮函数调用基准测试中超越现有最先进的基线模型，特别是在最具挑战性的任务上取得了高达22%的绝对性能提升。深入研究和指标分析显示，具身强化学习训练促进了更深层次的推理、更有效的工具使用和更高质量的解决方案。这些结果确立了具身强化学习与工具集成作为LLMs鲁棒、可解释和泛化能力强的问题解决新方向的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01441" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 06:42:49 GMT</pubDate>
</item>
<item>
<title>基于强化学习的多模态奖励模型优化研究</title>
<link>https://arxiv.org/abs/2505.02835</link>
<guid>https://arxiv.org/abs/2505.02835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文提出StableReinforce算法，通过改进现有强化学习方法显著提升多模态奖励模型性能。</p><br /><br /><p><strong>摘要：</strong> 多模态奖励模型（MRMs）对多模态大型语言模型（MLLMs）的表现至关重要，但长期推理能力的有效性及激活方式尚未充分探索。本文将奖励建模问题重新定义为基于规则的强化学习任务，并提出StableReinforce算法，通过改进训练损失、优势估计策略和奖励设计，解决现有强化学习算法在奖励建模中的不稳定性问题。实验中，我们从多个数据集中收集了20万份偏好数据用于模型训练，所提出的R1-Reward奖励模型在VL Reward-Bench和Multimodal Reward Bench上分别实现了8.4%和14.3%的性能提升。此外，随着推理计算资源的增加，R1-Reward的性能进一步增强，证明了强化学习算法在优化多模态奖励模型方面的潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 13:59:50 GMT</pubDate>
</item>
<item>
<title>FormalMATH：大规模数学定理形式化基准及其挑战</title>
<link>https://arxiv.org/abs/2505.02735</link>
<guid>https://arxiv.org/abs/2505.02735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">FormalMATH提供了一个涵盖多领域的大型数学形式化问题集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为FormalMATH的大规模数学形式化基准，包含5560个从高中到大学水平的已验证数学问题，涉及代数、微积分等多个领域。为了提高自动形式化的效率，研究提出了结合大语言模型（LLMs）和人类协作的新型自动化流水线，该方法显著降低了人工标注成本。然而，当前最先进的基于LLMs的定理证明器表现有限，在实际采样预算下成功率仅为16.46%，且存在明显的领域偏向性。进一步分析发现，自然语言解题指导在链式推理场景中反而可能引入噪声而非清晰度，这对形式化数学推理提出了新的见解。FormalMATH有望成为评估形式化数学推理能力的重要基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 11:37:00 GMT</pubDate>
</item>
<item>
<title>LLaMA-Omni 2：基于大语言模型的高质量实时语音交互</title>
<link>https://arxiv.org/abs/2505.02625</link>
<guid>https://arxiv.org/abs/2505.02625</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">LLaMA-Omni 2实现高质量实时语音交互，参数规模从0.5B到14B。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了LLaMA-Omni 2系列语音语言模型（SpeechLMs），该系列模型基于Qwen2.5系列，整合了语音编码器和自回归流式语音解码器，参数规模涵盖0.5B到14B。尽管仅使用了20万个多轮对话样本进行训练，LLaMA-Omni 2在多个语音问答和指令跟随基准测试中表现出色，超过了如GLM-4-Voice等之前基于大规模语音数据训练的先进模型。通过将大语言模型的能力扩展到语音领域，LLaMA-Omni 2展示了下一代人机交互中智能语音交互的巨大潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02625" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 08:53:09 GMT</pubDate>
</item>
<item>
<title>Ming-Lite-Uni：开源多模态框架实现文本到图像生成及指令驱动图像编辑</title>
<link>https://arxiv.org/abs/2505.02471</link>
<guid>https://arxiv.org/abs/2505.02471</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Ming-Lite-Uni开源框架结合统一视觉生成器和多模态自回归模型，实现文本到图像生成及图像编辑。</p><br /><br /><p><strong>摘要：</strong> Ming-Lite-Uni是一个开源的多模态框架，其核心特性在于新设计的统一视觉生成器和原生多模态自回归模型，旨在整合视觉与语言处理。该框架提供了集成MetaQueries和M2-omni框架的开放源代码实现，同时引入了多尺度可学习标记和多尺度表示对齐策略。通过固定MLLM和可学习扩散模型的结合，Ming-Lite-Uni使原生多模态AR模型不仅能够进行文本到图像生成，还能执行基于指令的图像编辑任务，从而扩展了其能力范围。实验结果表明，Ming-Lite-Uni具有卓越的表现，并展示了其交互过程的流畅性。所有代码和模型权重均公开，以促进社区进一步探索。值得注意的是，这项工作与最近的多模态AI里程碑（如ChatGPT-4o的本机图像生成功能更新）相呼应，强调了像Ming-Lite-Uni这样的统一模型在迈向通用人工智能（AGI）道路上的重要性。目前，Ming-Lite-Uni处于Alpha阶段，未来将进一步优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02471" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 04:56:12 GMT</pubDate>
</item>
<item>
<title>基于对比指令优化的图像编辑方法</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的图像编辑指令构造方法，显著提升编辑效果。</p><br /><br /><p><strong>摘要：</strong> 现有图像编辑数据集因自动化构建方法导致监督信号噪声问题，限制了模型性能。本文提出一种创新解决方案，通过修正编辑指令与图像对的匹配度并引入对比指令增强有效性，无需依赖视觉语言模型或预训练任务。实验表明，该方法在多个基准测试中显著优于现有技术，在Real-Edit基准上比SmartEdit提升9.19%，且所需训练数据和模型规模大幅减少。研究发现编辑模型在推理步骤中有特定生成属性，据此定义统一指导规则，同时利用三元组损失构建对比监督信号进一步提高监督效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02370" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 05 May 2025 01:19:40 GMT</pubDate>
</item>
<item>
<title>自适应模式学习提升社会智能模拟中的动态推理能力</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自适应模式学习方法，显著提高社会智能模拟中的推理深度和效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对当前社会智能模拟中语言代理缺乏动态调整推理深度的能力这一问题，提出了自适应模式学习（Adaptive Mode Learning, AML），该方法根据实时情境从四种思考模式中进行选择（直觉反应到深思熟虑）。核心创新的自适应模式策略优化（AMPO）算法实现了多粒度思考模式设计、上下文感知模式切换及通过深度自适应处理实现令牌高效推理。实验表明，AML在社会智能任务上的表现比现有最先进方法高出15.6%，且推理链条缩短32.8%，优于固定深度的GRPO方法7.0%。这些成果证明了AMPO在实现更人性化适应性推理方面的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.02156" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 04 May 2025 11:39:58 GMT</pubDate>
</item>
<item>
<title>TEMPURA框架提升视频因果事件关系理解</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TEMPURA框架，通过因果推理与细粒度时间分割提高视频理解能力。</p><br /><br /><p><strong>摘要：</strong> 现有视觉语言模型在理解因果事件关系及视频的时间定位方面仍面临挑战。为解决此问题，本文提出TEMPURA框架，该框架分为两个阶段：首先利用事件掩码预测推理重构缺失事件并生成因果解释；其次学习视频分割与密集描述以分解视频为非重叠事件并提供精确描述。TEMPURA在VER数据集上进行训练，该数据集包含100万训练实例及50万带有时间对齐事件描述和结构化推理步骤的视频。实验表明，TEMPURA在时间定位和亮点检测基准测试中优于基线模型，证明结合因果推理与细粒度时间分割可显著提升视频理解效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01583" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 17:00:17 GMT</pubDate>
</item>
<item>
<title>Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.01043</link>
<guid>https://arxiv.org/abs/2505.01043</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several componentsx2013such as weights, activations, and gradientsx2013each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 02:33:25 GMT</pubDate>
</item>
<item>
<title>基于多层记忆与一致性引导的迭代图像编辑框架</title>
<link>https://arxiv.org/abs/2505.01079</link>
<guid>https://arxiv.org/abs/2505.01079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种支持多对象修改并保持上下文关系的迭代图像编辑方法。</p><br /><br /><p><strong>摘要：</strong> 当前大多数图像编辑技术专注于单一对象修改，难以应对需要多次顺序编辑的复杂场景。针对这一挑战，本文提出了两项创新方案：引入粗糙掩码以自然融合新元素同时保留现有内容，以及支持多轮编辑的一致性维护机制。通过引入分层记忆存储先前编辑的潜在表示和提示嵌入，结合背景一致性引导和跨注意力的多查询解耦技术，该框架能够在复杂的多对象编辑任务中保持高质量的结果。此外，我们构建了一个包含语义对齐指标和交互式编辑场景的新基准数据集，实验结果证明了该方法在减少用户操作负担的同时显著提升了迭代图像编辑的效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.01079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 02 May 2025 03:36:49 GMT</pubDate>
</item>
<item>
<title>Llama-Nemotron系列模型：开放且高效的异构推理模型家族</title>
<link>https://arxiv.org/abs/2505.00949</link>
<guid>https://arxiv.org/abs/2505.00949</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Llama-Nemotron提供三种规模模型，兼具高效推理和卓越推理能力。</p><br /><br /><p><strong>摘要：</strong> Llama-Nemotron是一组开放的异构推理模型系列，包含Nano（8B）、Super（49B）和Ultra（253B）三种版本，其推理效率和内存利用率优于当前最先进的模型如DeepSeek-R1。该系列模型通过神经架构搜索、知识蒸馏和持续预训练优化，并经过推理聚焦的后训练阶段，包括监督微调和大规模强化学习。此外，Llama-Nemotron是首个支持动态推理切换的开源模型，用户可在标准聊天模式和推理模式间切换。为了促进开放研究，我们提供了完整的后训练数据集和训练代码库，并在NVIDIA开放模型许可协议下发布模型。这些资源旨在推动模型开发并支持学术界和企业用户。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00949" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 21:35:35 GMT</pubDate>
</item>
<item>
<title>基于逆映射学习的大型语言模型评估方法</title>
<link>https://arxiv.org/abs/2504.21117</link>
<guid>https://arxiv.org/abs/2504.21117</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种逆映射学习方法以生成高效的LLM评估提示。</p><br /><br /><p><strong>摘要：</strong> 自然语言生成系统的评估因其输出多样性而具有挑战性，尽管人类评估是标准，但存在不一致性、缺乏标准化及人口统计学偏差等问题，限制了可重复性。基于大型语言模型（LLM）的评估提供了可扩展的替代方案，但对提示设计极为敏感，微小变化可能导致显著差异。本文提出了一种逆映射学习方法，通过从模型输出反向映射到输入指令来学习有效的逆映射，从而实现自动高效生成针对特定模型的评估提示。该方法仅需单一样本即可完成，无需耗时的人工提示工程，提升了效率与鲁棒性，为更稳健、高效的LLM评估开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21117" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 14:56:12 GMT</pubDate>
</item>
<item>
<title>基于图神经网络的信号时态逻辑学习框架TeLoGraF</title>
<link>https://arxiv.org/abs/2505.00562</link>
<guid>https://arxiv.org/abs/2505.00562</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合图编码与流匹配的信号时态逻辑学习方法，显著提升复杂任务求解效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对信号时态逻辑(STL)在实际应用中的局限性，设计了一种名为TeLoGraF的新框架，通过图神经网络(GNN)编码器和流匹配技术实现对一般STL规格的学习。研究团队收集了20万组配对演示数据，涵盖四种常见STL模板，并在五个模拟环境中进行测试，包括二维动态模型到复杂的七自由度机械臂及四足机器人导航。实验结果显示，TeLoGraF在STL满足率上优于其他基线算法，推理速度比经典STL规划算法快10-100倍，且适用于任意系统动力学。此外，该方法展现出解决复杂STL问题的能力，并具备处理分布外STL规格的鲁棒性。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00562" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:40:07 GMT</pubDate>
</item>
<item>
<title>生成式人工智能研究焦点转移及其潜在风险</title>
<link>https://arxiv.org/abs/2505.00174</link>
<guid>https://arxiv.org/abs/2505.00174</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">企业AI研究重心转向模型对齐与测试，部署阶段问题关注减少。</p><br /><br /><p><strong>摘要：</strong> 本研究基于2020年至2025年间9,439篇生成式人工智能论文中的1,178篇安全与可靠性相关文献，对比了领先AI公司（如Anthropic、Google DeepMind等）与顶尖大学（如麻省理工学院、斯坦福大学等）的研究产出。结果显示，企业AI研究正越来越多地聚焦于模型部署前的对齐与评估领域，而对部署阶段的重要议题如模型偏差的关注度有所下降。此外，在高风险应用领域（如医疗、金融、虚假信息传播等），存在显著的研究空白。若缺乏对实际部署AI系统的深入观察，企业集中化趋势可能加剧这些知识鸿沟。因此，建议扩大外部研究人员对部署数据的访问权限，并建立系统化的市场中AI行为观测机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00174" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 16:44:42 GMT</pubDate>
</item>
<item>
<title>X-Cross：一种高效的跨域推荐模型</title>
<link>https://arxiv.org/abs/2504.20859</link>
<guid>https://arxiv.org/abs/2504.20859</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Cross通过低秩适配器实现跨域推荐，性能媲美传统方法但参数量减少25%。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为“X-Cross”的新型跨域顺序推荐模型，旨在无需大规模重新训练的情况下适应新领域的产品推荐。该模型整合多个特定领域的语言模型，利用低秩适配器（LoRA）进行微调。X-Cross通过动态优化各源语言模型表示，将知识层间传播，从而在保持领域特异性的同时实现跨域适应性。基于Amazon的数据集测试显示，X-Cross在性能上与全参数LoRA相当，但仅需25%的额外参数。在跨域任务中，如从玩具领域迁移到工具、电子或运动领域时，X-Cross表现出色，所需微调数据减少了50%-75%，且在准确性上优于其他跨域基准模型。总体而言，X-Cross为数据受限环境提供了高效、可扩展的跨域推荐解决方案，显著降低了计算开销。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20859" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:33:20 GMT</pubDate>
</item>
<item>
<title>PixelHacker：基于扩散模型的图像修复新范式</title>
<link>https://arxiv.org/abs/2504.20438</link>
<guid>https://arxiv.org/abs/2504.20438</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的图像修复方法PixelHacker，显著提升复杂结构和语义的一致性。</p><br /><br /><p><strong>摘要：</strong> 图像修复是图像编辑与生成领域的重要研究方向。现有最先进的方法虽引入新颖的注意力机制和轻量化架构，但在处理复杂结构（如纹理、形状、空间关系）和语义一致性（如颜色一致性、对象恢复及逻辑正确性）时仍存在不足，导致伪影和不恰当生成。为解决这一问题，我们设计了一种简单而有效的修复范式——潜在类别引导，并提出了名为PixelHacker的扩散模型。该方法通过构建包含1400万图像-掩码对的大规模数据集，分别编码前景和背景表示，并将其特征注入去噪过程中，最终实现优异的修复效果。实验表明，PixelHacker在Places2、CelebA-HQ和FFHQ等数据集上全面超越现有最先进方法，在结构和语义一致性方面表现突出。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20438" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 01:28:36 GMT</pubDate>
</item>
<item>
<title>Context Organizer (CORG): 处理跨文档知识关系的新框架</title>
<link>https://arxiv.org/abs/2505.00023</link>
<guid>https://arxiv.org/abs/2505.00023</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架CORG，有效处理跨文档复杂知识关系并提升性能。</p><br /><br /><p><strong>摘要：</strong> 在现实世界的语料库中，知识经常在文档间重复出现，但由于命名模糊、过时信息或错误等原因，常常存在不一致性，导致上下文之间形成复杂的相互关系。以往研究表明，语言模型难以应对这些复杂性，通常只能单独关注单一因素。我们把这种关系分为分散、模糊、反事实和重复四类。分析表明，没有单一方法可以同时解决所有这些关系。为此，我们提出了Context Organizer（CORG），该框架将多个上下文组织成独立处理的组，从而让模型高效找到所有相关答案并实现去模糊化。CORG由图构造器、重排序器和聚合器三个关键组件组成。实验结果显示，CORG在性能和效率上均优于现有分组方法，并达到与计算密集型单上下文方法相当的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00023" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 22:40:48 GMT</pubDate>
</item>
<item>
<title>基于离线模拟框架的软件特定技能集生成方法</title>
<link>https://arxiv.org/abs/2504.20406</link>
<guid>https://arxiv.org/abs/2504.20406</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用大型语言模型生成软件特定技能集的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种结合图神经网络的离线模拟框架，用于生成经过验证的脚本集合，从而解决传统实时代码生成中存在的问题。该框架通过任务创建和技能生成两个组件实现，其中任务创建采用自顶向下和自底向上的方法，而技能生成则利用执行反馈进行优化和验证。实验表明，在Adobe Illustrator中使用该框架可显著提高自动化成功率，减少响应时间和运行时标记成本。这是首次将软件脚本接口作为大型语言模型系统测试平台的尝试，为满足专业软件领域用户需求提供了有价值的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20406" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:03:37 GMT</pubDate>
</item>
<item>
<title>空间语音翻译技术：让听觉空间语言无缝转换</title>
<link>https://arxiv.org/abs/2504.18715</link>
<guid>https://arxiv.org/abs/2504.18715</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型听戴设备技术，实现实时空间语音翻译并保持方向和音质。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为空间语音翻译的新概念，旨在通过智能听戴设备将佩戴者周围环境中的语音实时翻译成母语，同时保持各发言人的声音方向和独特音色。该技术解决了盲源分离、定位、实时情感翻译及双耳渲染等多个技术难题，并在苹果M2芯片上实现了实时推理。实验表明，尽管存在干扰，我们的模型在BLEU评分上达到22.01，优于现有模型。用户研究进一步验证了系统在真实世界混响环境中的有效性。这一成果标志着将空间感知融入语音翻译领域的第一步。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18715" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 17:58:56 GMT</pubDate>
</item>
<item>
<title>基于深度学习的城市多目标多摄像头车辆跟踪框架</title>
<link>https://arxiv.org/abs/2505.00534</link>
<guid>https://arxiv.org/abs/2505.00534</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种高效的深度学习框架解决城市交通多摄像头下的车辆跟踪问题。</p><br /><br /><p><strong>摘要：</strong> 随着智能交通系统中网络摄像头数量的增加，视觉传感器在交通监控、管理和优化中的作用愈发重要。然而，在大规模城市交通场景中实现非重叠摄像头间的车辆目标跟踪与匹配面临诸多挑战，如车辆属性多样性、遮挡、光照变化及阴影等。本文提出了一种名为MO-MCT的高效且经济的深度学习框架，该框架结合Mask R-CNN进行目标检测，利用迁移学习实现跨摄像头车辆再识别，并通过适当的损失函数和距离度量应对各种复杂情况。最终方案采用ResNet-152进行特征提取，并结合Deep SORT算法进行车辆跟踪。该框架在AI City Challenge第5赛道的数据集上进行了评估，涵盖46路摄像头视频流，取得了优秀的性能表现，IDF1得分为0.8289，精确率和召回率分别为0.9026和0.8527。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00534" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 10:00:25 GMT</pubDate>
</item>
<item>
<title>通过自我生成示例提升大语言模型的序列决策性能</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，通过积累成功经验自动生成示例可显著提高大语言模型在决策任务中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLM）代理通过学习自身在类似任务上的成功经验来自我提升性能的可能性。与依赖特定任务的知识工程方法不同，我们专注于构建和完善一个由自身生成的示例数据库。实验表明，简单累积训练任务中的成功轨迹即可显著提升三个基准测试的表现：ALFWorld（从73%提升到89%）、Wordcraft（从55%提升到64%）以及InterCode-SQL（从75%提升到79%），相当于初始代理在每项任务允许两到三次尝试时的表现。进一步地，我们引入了两种扩展策略：一是基于种群训练的数据库级选择，二是根据实际效用对单个轨迹进行示例级选择。这些改进使ALFWorld的性能达到了91%，接近采用特定任务组件和提示符的复杂方法。我们的研究证明，自动构建轨迹数据库是一种有吸引力的替代方案，可以避免劳动密集型的知识工程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00234" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 20:48:12 GMT</pubDate>
</item>
<item>
<title>强化学习增强大型语言模型在高功率火箭设计中的应用研究</title>
<link>https://arxiv.org/abs/2504.19394</link>
<guid>https://arxiv.org/abs/2504.19394</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">强化学习提升的LLMs在高功率火箭设计优化中超越人类专家。</p><br /><br /><p><strong>摘要：</strong> 本文通过RocketBench基准测试评估大型语言模型（LLMs）在高功率火箭设计中的能力，涉及目标高度优化及精确着陆挑战两个任务。尽管最先进的LLMs展现了强大的基础工程知识，但在结合仿真结果迭代设计时表现不佳，最终性能低于人类水平。然而，通过强化学习增强后，一款7B参数模型不仅优于顶级基础模型，还超过了人类专家的表现。这一研究表明，强化学习训练的LLMs可成为复杂工程优化的有效工具，有望推动工程领域的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19394" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 19:59:39 GMT</pubDate>
</item>
<item>
<title>基于双层推理过程的文本到图像生成模型T2I-R1</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合强化学习的推理增强型文本到图像生成模型T2I-R1。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为T2I-R1的新模型，该模型通过强化学习和双层链式思维推理过程改进了文本到图像的生成能力。T2I-R1在两个生成阶段引入了两种链式思维策略：语义级用于高级提示规划，令牌级用于低级像素处理。此外，我们提出了BiCoT-GRPO方法，以集成奖励优化这两个推理层级。实验结果显示，在T2I-CompBench和WISE基准测试中，T2I-R1分别提升了13%和19%，甚至超越了最先进的FLUX模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于两阶段框架提升大型语言模型数学解题批评能力的研究</title>
<link>https://arxiv.org/abs/2505.00662</link>
<guid>https://arxiv.org/abs/2505.00662</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种两阶段框架以增强大型语言模型在数学问题中的批评能力。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）的快速发展，如何提供准确且可扩展的反馈成为了一个紧迫的问题。本研究专注于提高LLMs在数学解题中的批评能力，现有的批评模型对每一步的批评过于浅显，导致判断准确性低且难以提供足够的反馈。为解决这一问题，我们提出了一个新颖有效的两阶段框架。第一阶段利用Qwen2.5-72B-Instruct生成4500份长篇批评作为监督微调的种子数据；第二阶段通过强化学习进一步优化模型，使用人类标注的数据或通过蒙特卡洛采样获得的自动标注数据。最终开发出的批评模型不仅在多种错误识别基准上显著优于现有模型，还通过更详细的反馈帮助生成器修正错误。此外，该模型在数学解题批评任务中表现出色，为LLMs的自动化监督提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2505.00662" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 01 May 2025 13:03:17 GMT</pubDate>
</item>
<item>
<title>KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution</title>
<link>https://arxiv.org/abs/2505.00497</link>
<guid>https://arxiv.org/abs/2505.00497</guid>
<content:encoded><![CDATA[
Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 08:56:17 GMT</pubDate>
</item>
<item>
<title>交互生成视频技术综述及其在多领域的应用与挑战</title>
<link>https://arxiv.org/abs/2504.21853</link>
<guid>https://arxiv.org/abs/2504.21853</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">定义交互生成视频(IGV)，并探讨其在游戏、具身AI和自动驾驶中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文首次明确交互生成视频(IGV)的概念，即通过生成技术和互动特性创造高质量视频内容，支持用户通过控制信号和反馈进行互动。IGV已在多个领域崭露头角，包括允许虚拟世界无限探索的游戏、提供物理感知环境合成的具身AI训练场景，以及为自动驾驶提供闭环模拟测试的系统。为了指导未来研究，我们提出了一套全面的框架，将理想中的IGV系统分解为五个核心模块：生成、控制、记忆、动力学和智能。同时，针对实现这些模块的技术挑战进行了系统分析，如实现实时生成、开放域控制、长期一致性维护、精确物理模拟及因果推理集成等。这项工作旨在推动IGV技术向更复杂和实用化的方向发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21853" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:02 GMT</pubDate>
</item>
<item>
<title>AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization</title>
<link>https://arxiv.org/abs/2504.21659</link>
<guid>https://arxiv.org/abs/2504.21659</guid>
<content:encoded><![CDATA[
Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 10:01:45 GMT</pubDate>
</item>
<item>
<title>TF1-EN-3M：大规模开放道德故事数据集的开创性成果</title>
<link>https://arxiv.org/abs/2504.20605</link>
<guid>https://arxiv.org/abs/2504.20605</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次推出包含三百万英文寓言的大规模结构化数据集TF1-EN-3M。</p><br /><br /><p><strong>摘要：</strong> 道德故事长期以来被用于传递价值观，但现代自然语言处理领域缺乏结合连贯叙事与明确伦理教训的大型结构化语料库。为填补这一空白，本文推出了TF1-EN-3M，这是一个由指令微调模型生成的首个人类可读开放数据集，包含三百万英语寓言，每个故事均遵循特定的六槽结构（角色 -> 特质 -> 背景 -> 冲突 -> 解决方案 -> 道德）。通过混合评估管道，该数据集的质量得到了全面验证，其中基于Llama-3的8B参数变体表现最佳，可在消费级GPU上高效生成高质量故事。此数据集及其相关代码和元数据已开源，为研究指令跟随、叙事智能、价值对齐及儿童友好型教育AI开辟了新路径，证明大规模道德叙事不再依赖专有巨型模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20605" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:15:28 GMT</pubDate>
</item>
<item>
<title>MediAug：医学影像数据增强统一评估框架</title>
<link>https://arxiv.org/abs/2504.18983</link>
<guid>https://arxiv.org/abs/2504.18983</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MediAug框架，综合评估多种混合数据增强方法在医学影像分类中的表现。</p><br /><br /><p><strong>摘要：</strong> 本文针对医学影像数据增强面临的领域差距大及单一任务研究局限性问题，提出了MediAug，一个集成六种基于混合的数据增强技术的统一评估框架。该框架适用于脑肿瘤MRI和眼病眼底图像数据集，并结合卷积神经网络（ResNet-50）和Transformer架构（ViT-B）。实验结果显示，MixUp在ResNet-50上显著提升了脑肿瘤分类精度至79.19%，而SnapMix在ViT-B上达到99.44%；YOCO在ResNet-50上的眼疾分类准确率达到91.60%，CutMix在ViT-B上则提升至97.94%。MediAug为医学影像数据增强提供了全面且可复现的基准工具，代码将在GitHub公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18983" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 13:56:56 GMT</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 20:09:31 GMT</pubDate>
</item>
<item>
<title>基于对抗性动态的联合分析优化候选人特征研究</title>
<link>https://arxiv.org/abs/2504.19043</link>
<guid>https://arxiv.org/abs/2504.19043</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种概率分布方法优化政治候选人特征，在对抗性环境中更贴近历史选举结果。</p><br /><br /><p><strong>摘要：</strong> 联合分析是一种实验设计工具，广泛应用于社会科学研究中，尤其在政治分析领域，用于研究选民对候选人特征的多维偏好。本文探讨如何确定最优候选人配置的问题。由于联合实验中的特征组合数量远超观察总数，无法精确识别最优配置。为解决这一识别难题，我们推导出一种最优随机干预策略，该策略旨在通过属性的概率分布实现最有利的平均结果。首先，我们在单一政党优化候选人的情境下进行分析；随后扩展到两个政党同时优化并相互对抗的现实情况。我们将所提方法应用于一项关于美国总统选举投票选择的联合实验。结果显示，与非对抗性方法相比，对抗性环境下的预期结果更符合历史选举结果，且由方法建议的最优策略更可能与实际观察到的候选人匹配。这些发现表明，在联合分析中引入对抗性动态可能为实验数据提供独特的社会科学研究洞见。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19043" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 18:35:58 GMT</pubDate>
</item>
<item>
<title>Sadeed：基于轻量解码器模型的阿拉伯文变音标注新方法</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于轻量语言模型的阿拉伯文变音标注方法Sadeed。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Sadeed的新方法，该方法通过微调Kuwain 1.5B Hennara等人的解码器-only语言模型，解决了自然语言处理中阿拉伯文变音标注的难题。Sadeed经过精心整理的高质量变音数据集训练，尽管使用了有限的计算资源，但其性能与专有大型语言模型相当，并优于传统模型。此外，我们还指出了当前阿拉伯文变音标注基准测试中的局限性，并提出了新的基准测试SadeedDiac-25，以实现更公平和全面的评估。Sadeed和SadeedDiac-25共同为推进阿拉伯自然语言处理应用提供了坚实基础，涵盖机器翻译、文本转语音及语言学习工具等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21635" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 09:37:24 GMT</pubDate>
</item>
<item>
<title>UniBiomed：基于多模态大语言模型的生物医学图像统一解释框架</title>
<link>https://arxiv.org/abs/2504.21336</link>
<guid>https://arxiv.org/abs/2504.21336</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UniBiomed实现生物医学图像跨模态统一解释，性能卓越。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniBiomed的新模型，它是首个用于生物医学图像解释的通用基础模型。UniBiomed结合了多模态大语言模型（MLLM）和Segment Anything Model（SAM），实现了临床文本生成与生物医学对象分割的统一。该模型支持十种不同的生物医学成像模式，涵盖了多种任务，如分割、疾病识别、诊断等。为了开发UniBiomed，研究团队创建了一个包含超过2700万张图像及其注释和文本描述的大规模数据集。通过在84个内部和外部数据集上的验证，UniBiomed展示了其在分割、疾病识别、区域感知诊断、视觉问答和报告生成等方面的领先性能。与依赖临床专家的传统方法相比，UniBiomed可以提供自动化且端到端的解释，显著提升了诊断效率，标志着临床工作流程的一次范式转变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21336" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:51:48 GMT</pubDate>
</item>
<item>
<title>基于子思维分析的大语言模型推理性能提升方法</title>
<link>https://arxiv.org/abs/2504.20708</link>
<guid>https://arxiv.org/abs/2504.20708</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究挑战传统大语言模型评价方式，提出通过分析中间子思维步骤提升推理准确性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）通过逐步推理解决复杂问题，通常通过评估最终答案来衡量模型表现。本研究质疑仅依赖最终答案的有效性，提出一种新方法分析推理过程中的中间子思维步骤。我们通过语言线索将完整推理轨迹分割成多个子思维片段，并从各片段生成可能的答案，通过频率统计发现聚合答案的准确性显著优于直接采用完整推理轨迹的结果。进一步分析显示，子思维答案的一致性可反映模型的置信度与正确性。实验表明，该方法在AIME2024和AIME2025数学推理数据集上分别提升了最高13%和10%的准确率。此方法为提高LLMs推理能力提供了新的思路与工具支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20708" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:39:07 GMT</pubDate>
</item>
<item>
<title>大规模语言模型推理服务优化方法综述</title>
<link>https://arxiv.org/abs/2504.19720</link>
<guid>https://arxiv.org/abs/2504.19720</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大规模语言模型推理服务优化的关键技术与未来方向。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在生成性人工智能领域取得了显著进展，但其巨大的参数量和注意力机制带来的高计算需求阻碍了低延迟和高吞吐量的实现。本文全面回顾了相关领域的研究进展，涵盖了实例级方法（如模型放置、请求调度等）、集群级策略（如GPU部署和负载均衡）、新兴场景下的具体任务优化及辅助方法，还探讨了若干重要但边缘的研究领域。通过整合这些技术成果，文章旨在为LLM推理服务提供系统化的优化方案，并指出了未来可能的研究方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19720" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:14:02 GMT</pubDate>
</item>
<item>
<title>Foundation-Sec-8B：面向网络安全部署的大型语言模型</title>
<link>https://arxiv.org/abs/2504.21039</link>
<guid>https://arxiv.org/abs/2504.21039</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出一款基于Llama架构并增强的网络安全专用大语言模型。</p><br /><br /><p><strong>摘要：</strong> 随着基于Transformer的大规模语言模型（LLMs）在社会各领域的渗透，软件工程、创意写作及数字艺术等领域得到了显著革新。然而，在网络安全领域，由于缺乏专门训练数据且需复杂表示特定知识，其应用受到限制。本文介绍了一款名为Foundation-Sec-8B的新模型，该模型基于Llama 3.1架构构建，并通过精心策划的网络安全语料库进行持续预训练得以增强。我们评估了Foundation-Sec-8B在现有和新网络安全基准上的表现，结果显示它在某些特定任务上与Llama 3.1-70B和GPT-4o-mini的表现相当。通过公开此模型，我们希望推动AI驱动工具在公共和私人网络安全环境中的进步和应用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21039" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 04:41:12 GMT</pubDate>
</item>
<item>
<title>ReVision：通过参数化物理知识提升视频生成模型能力</title>
<link>https://arxiv.org/abs/2504.21855</link>
<guid>https://arxiv.org/abs/2504.21855</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入ReVision框架，显著提升复杂动作和交互视频生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为ReVision的插件式框架，通过将参数化的三维物理知识融入到预训练的条件视频生成模型中，显著增强了生成高质量复杂运动和交互视频的能力。该框架分为三个阶段：首先利用视频扩散模型生成粗略视频；接着提取二维和三维特征构建对象中心的三维表示，并通过参数化物理先验模型优化得到精确的三维运动序列；最后将优化后的运动序列反馈至扩散模型作为附加条件，生成一致性更强的视频。实验表明，ReVision在Stable Video Diffusion上的表现优于参数量更多的现有模型，验证了其在复杂场景中的优越性，为实现逼真的物理视频生成提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21855" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>COMPACT：提升多模态大语言模型复杂视觉-语言任务性能的新方法</title>
<link>https://arxiv.org/abs/2504.21850</link>
<guid>https://arxiv.org/abs/2504.21850</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">COMPACT通过控制训练样本的组合复杂性显著提升多模态大语言模型在复杂任务上的表现。</p><br /><br /><p><strong>摘要：</strong> 当前的多模态大型语言模型（MLLMs）在简单视觉-语言任务上表现出色，但在需要多种能力同时工作的复杂任务中表现欠佳。这种局限性部分源于视觉指令微调（VIT）的传统训练策略过于注重数据量扩展而忽视了训练示例的组合复杂性。为此，我们提出COMPACT（COMPositional Atomic-to-complex visual Capability Tuning），该方法专门设计了一个训练数据集，通过显式控制训练样本的组合复杂性来提高模型学习效率。实验表明，COMPACT在多个基准测试中达到了与LLaVA-665k VIT相当甚至更好的性能，特别是在涉及四种及以上原子能力的复杂问题上，例如在MMStar和MM-Vet任务中分别实现了83.3%和94.0%的显著改进。COMPACT提供了一种可扩展且数据高效的视觉组合微调方案，有效提升了复杂视觉-语言任务的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21850" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 13:57:22 GMT</pubDate>
</item>
<item>
<title>WebThinker：增强大型推理模型复杂知识密集型任务处理能力</title>
<link>https://arxiv.org/abs/2504.21776</link>
<guid>https://arxiv.org/abs/2504.21776</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出WebThinker系统，提升大型推理模型在线索推理中的性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WebThinker的新方法，该方法通过结合深度网络探索模块和自主思考搜索撰写策略，增强了大型推理模型（LRMs）在复杂知识密集型任务中的表现。WebThinker允许LRMs在推理过程中动态搜索网络、导航网页并撰写研究报告，从而克服了传统LRMs依赖静态内部知识的局限性。此外，我们还通过基于强化学习的迭代在线直接偏好优化（DPO）策略进一步提升了研究工具的利用效率。实验表明，WebThinker在多个复杂推理基准测试（如GPQA、GAIA等）和科学报告生成任务中显著优于现有方法及专有系统，大幅提高了LRMs在复杂场景中的可靠性和适用性。本研究代码已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21776" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 12:25:25 GMT</pubDate>
</item>
<item>
<title>Phi-4-reasoning：高效推理模型在复杂任务中的表现</title>
<link>https://arxiv.org/abs/2504.21318</link>
<guid>https://arxiv.org/abs/2504.21318</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Phi-4-reasoning及增强版在多种推理任务上超越更大规模的开放权重模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了Phi-4-reasoning，一种拥有140亿参数的推理模型，在复杂推理任务中表现出色。该模型通过监督微调Phi-4结合精心挑选的提示和o3-mini生成的推理演示训练而成，可生成详细的推理链。此外，Phi-4-reasoning-plus版本经过短期基于成果的强化学习进一步优化，推理轨迹更长，性能更高。实验结果显示，这两个模型在数学、科学推理、编码、算法问题解决、规划和空间理解等多个领域显著优于大型开放权重模型如DeepSeek-R1-Distill-Llama-70B，并接近全量DeepSeek-R1模型的表现。有趣的是，这些改进在通用基准测试中也有非平凡的迁移效果。研究还探讨了训练数据的精细筛选对监督微调的重要性，并指出强化学习可进一步提升性能。最后，本报告指出了评估推理模型性能和鲁棒性的潜在改进方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21318" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 01:05:09 GMT</pubDate>
</item>
<item>
<title>通过系统训练提升小规模语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.21233</link>
<guid>https://arxiv.org/abs/2504.21233</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一套系统性训练方法，显著提升了小规模语言模型的推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一套针对小规模语言模型（SLMs）的系统训练方案，该方案由四个阶段组成：大规模中期训练、高质量长链推理数据的监督微调、基于精心筛选偏好数据集的Rollout DPO以及具有可验证奖励的强化学习。实验中，将此方法应用于Phi-4-Mini模型上，其推理版本Phi-4-Mini-Reasoning在数学推理任务中表现出色，超越了更大规模的模型如DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B。这一成果证明，即使在资源受限的小规模模型中，精心设计的训练方案结合高质量的链式推理数据也能有效提升推理能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.21233" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 20:04:35 GMT</pubDate>
</item>
<item>
<title>软选择（Softpick）：一种改进Transformer注意力机制的新方法</title>
<link>https://arxiv.org/abs/2504.20966</link>
<guid>https://arxiv.org/abs/2504.20966</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">软选择替代Softmax消除注意力问题并提高量化性能。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为软选择（Softpick）的新方法，它是Transformer注意力机制中Softmax的改进版本，能够消除注意力沉没现象并减少大量激活值。实验表明，在3.4亿参数规模模型上，软选择与Softmax在标准基准测试中的表现相当，但实现了零沉没率。此外，软选择Transformer产生的隐藏状态峰度显著降低，且生成的注意力图更加稀疏。当进行量化时，使用软选择的模型始终优于Softmax，尤其是在较低位精度下优势更为明显。分析显示，软选择可能为量化、低精度训练、稀疏优化、剪枝和可解释性开辟新的可能性。相关代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20966" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:36:18 GMT</pubDate>
</item>
<item>
<title>RoboVerse：机器人学习的综合框架</title>
<link>https://arxiv.org/abs/2504.18904</link>
<guid>https://arxiv.org/abs/2504.18904</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RoboVerse框架，解决机器人领域数据规模和评估基准标准化问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对机器人领域数据规模化及评估协议建立的独特挑战，介绍了RoboVerse，一个集模拟平台、合成数据集及统一基准为一体的综合性框架。该框架通过MetaSim基础设施抽象多种仿真环境，支持跨模拟器和机器人形态的无缝转换，并提供高质量、多样化的合成数据。此外，还提出了用于模仿学习和强化学习的统一基准，支持不同泛化水平的评估。实验表明，RoboVerse显著提升了模仿学习、强化学习、世界模型学习及仿真到现实迁移的性能，验证了其数据集和基准的可靠性，成为推动机器人学习发展的稳健解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18904" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 08:31:04 GMT</pubDate>
</item>
<item>
<title>通过防御性思维提升大语言模型鲁棒性的研究</title>
<link>https://arxiv.org/abs/2504.20769</link>
<guid>https://arxiv.org/abs/2504.20769</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示如何利用增强推理能力提高大语言模型对参考污染攻击的鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了通过增强的大语言模型推理能力，在非推理任务中提升其鲁棒性的方法。特别提出了一种名为“防御性思维”的简单方法，仅需少量结构化且具有防御性的推理示例作为演示，就能显著改善模型的鲁棒性。实验表明，该方法效果显著，例如在Natural Questions任务中，标准提示下GPT-4o的准确率从60%降至3%，而采用防御性思维提示后，其准确率仍能保持在50%。这一发现凸显了该方法的简单性和适用性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20769" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:50:05 GMT</pubDate>
</item>
<item>
<title>LawFlow：构建端到端法律工作流程的数据集</title>
<link>https://arxiv.org/abs/2504.18942</link>
<guid>https://arxiv.org/abs/2504.18942</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出LawFlow数据集，用于捕捉法律实践中的动态推理过程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LawFlow的新数据集，该数据集收集了法律学生完成的真实商业实体形成场景中的端到端法律工作流程。与现有专注于输入输出对或线性推理链的数据集不同，LawFlow强调动态、模块化且迭代的推理过程，这些过程反映了法律实践中存在的模糊性、修订及客户导向策略。通过对比人类和大型语言模型（LLMs）生成的工作流程，发现两者在结构、推理灵活性和计划执行方面存在显著差异。研究还表明，法律专业人士更倾向于让AI承担辅助角色，如头脑风暴、盲点识别和提供替代方案，而非直接执行复杂的全流程。基于此，我们提出了结合人类目标的设计建议，旨在通过混合规划、自适应执行和支持决策点来提升AI协作能力。本研究揭示了当前LLMs支持复杂法律工作的局限性，并为开发更协作、推理感知的法律AI系统提供了机会。所有数据和代码均可在项目页面获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18942" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 11:01:55 GMT</pubDate>
</item>
<item>
<title>RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.20073</link>
<guid>https://arxiv.org/abs/2504.20073</guid>
<content:encoded><![CDATA[
Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:57:08 GMT</pubDate>
</item>
<item>
<title>基于强化学习的自主驾驶特权规划研究</title>
<link>https://arxiv.org/abs/2504.17838</link>
<guid>https://arxiv.org/abs/2504.17838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种基于单一直观奖励项的新方法，显著提升强化学习在自主驾驶中的性能。</p><br /><br /><p><strong>摘要：</strong> 当前针对自主驾驶的任务大多采用基于规则的方法，但这些方法难以应对长尾问题。强化学习（RL）因其可扩展性且不受累积误差影响而受到关注。然而，传统RL方法依赖复杂的复合奖励设计，导致优化困难。本文提出一种新奖励机制，主要基于路线完成度优化，并通过终止或乘法惩罚违规行为，使PPO算法在更大批量下表现优异。实验表明，该方法在CARLA和nuPlan数据集上分别达到64DS和91.3/90.6的高分，超越其他复杂奖励设计的RL方法，同时大幅提高训练效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>基于解释性方法的强化学习从人类反馈中优化奖励分配</title>
<link>https://arxiv.org/abs/2504.16272</link>
<guid>https://arxiv.org/abs/2504.16272</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">改进奖励分配机制以提升语言模型对齐性能。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型（LLM）对齐中的强化学习从人类反馈（RLHF）管道通常为序列分配标量奖励，以序列末尾标记作为整体质量的代理指标，导致反馈稀疏且令牌级奖励分配次优。本文将奖励塑造视为专注于令牌级奖励分配的优化问题，提出一种利用SHAP和LIME等解释性方法估计令牌奖励的奖励塑造函数，并采用双层优化框架结合贝叶斯优化和策略训练来处理令牌奖励估计中的噪声。实验表明，更好的令牌级奖励归因平衡在下游任务上优于基线模型，并在训练期间更快找到最优策略。此外，理论上证明了特征可加性属性的解释性方法保持原始奖励的最优策略不变。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16272" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 17:09:33 GMT</pubDate>
</item>
<item>
<title>基于上下文提示的大规模图像编辑方法</title>
<link>https://arxiv.org/abs/2504.20690</link>
<guid>https://arxiv.org/abs/2504.20690</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效且高精度的指令引导图像编辑框架。</p><br /><br /><p><strong>摘要：</strong> 本文解决了基于指令的图像编辑领域中精度与效率之间的权衡问题。通过利用大规模Diffusion Transformer（DiT）的生成能力和上下文感知能力，我们提出了三个创新贡献：一是通过上下文提示实现零样本指令合规的编辑框架；二是引入LoRA-MoE混合微调策略，在不进行大量重新训练的情况下增强灵活性；三是采用视觉-语言模型在推理阶段早期筛选初始噪声，从而提升编辑质量。实验结果显示，该方法在性能上超越现有最先进方法，同时仅需传统基线方法0.5%的训练数据和1%的可训练参数。这项工作开创了一种新的范式，实现了高精度且高效的指令引导图像编辑。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20690" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 08:14:47 GMT</pubDate>
</item>
<item>
<title>基于视觉语言模型的3D目标检测系统性综述</title>
<link>https://arxiv.org/abs/2504.18738</link>
<guid>https://arxiv.org/abs/2504.18738</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文综述了视觉语言模型在3D目标检测中的应用与挑战。</p><br /><br /><p><strong>摘要：</strong> 本文对基于视觉语言模型(VLMs)的3D目标检测进行了系统的分析，通过对超过100篇研究论文的考察，首次提供了专门针对这一领域的系统性分析。文章首先概述了VLMs在3D目标检测中的独特挑战，特别是与2D检测相比在空间推理和数据复杂性上的差异。接着比较了传统方法如点云和体素网格与现代框架如CLIP及3D大型语言模型(3D LLMs)，后者支持开放词汇检测和零样本泛化。文中还回顾了关键架构、预训练策略及提示工程方法，这些方法用于有效对齐文本和3D特征。通过可视化示例和评估基准讨论了性能和行为。最后，文章指出了当前存在的问题，例如有限的3D-语言数据集和计算需求，并提出了未来的研究方向以推动VLMs在3D目标检测中的发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18738" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 19:27:26 GMT</pubDate>
</item>
<item>
<title>X-Fusion：一种保持语言能力的多模态任务扩展框架</title>
<link>https://arxiv.org/abs/2504.20996</link>
<guid>https://arxiv.org/abs/2504.20996</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">X-Fusion框架通过双塔设计提升多模态任务性能同时保留语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为X-Fusion的框架，旨在扩展预训练的大规模语言模型（LLMs）以处理多模态任务，同时保持其原有的语言处理能力。X-Fusion采用了一种双塔结构，其中包含针对特定模态的权重，使得在整合视觉信息时可以保持LLM参数不变。实验结果显示，在图像到文本和文本到图像的任务上，X-Fusion的表现优于其他替代架构。研究还发现，引入专注于理解的数据能提高生成质量，减少图像数据噪声可改善整体表现，而特征对齐对较小模型的收敛速度有显著促进作用，但对较大模型影响有限。这些发现为构建高效的统一多模态模型提供了宝贵的见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20996" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:45 GMT</pubDate>
</item>
<item>
<title>Chatbot Arena排名体系中的系统性问题及改进建议</title>
<link>https://arxiv.org/abs/2504.20879</link>
<guid>https://arxiv.org/abs/2504.20879</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示Chatbot Arena排名受私测偏颇影响，导致不公平竞争。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了Chatbot Arena作为顶级AI系统排行榜所面临的问题。研究发现，少数公司通过未公开的私测策略，在公开发布前测试多个模型变体并选择最佳成绩，这种做法导致了排行榜评分的偏差。例如，Meta在Llama-4发布前对27个私测变体进行了评估。此外，闭源模型比开源模型获得更多测试机会，造成数据获取的不对等。Google和OpenAI分别占Arena总数据的19.2%和20.4%，而83个开源模型仅占29.7%。这些动态使模型过度适应Arena特定环境，而非整体性能提升。尽管如此，Chatbot Arena仍得益于组织者和开放社区的努力。我们提出了改进评估框架的建议，以实现更公平透明的基准测试。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20879" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 11:48:49 GMT</pubDate>
</item>
<item>
<title>ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.20630</link>
<guid>https://arxiv.org/abs/2504.20630</guid>
<content:encoded><![CDATA[
Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 06:56:44 GMT</pubDate>
</item>
<item>
<title>Meta Policy Optimization提升大语言模型奖励对齐的鲁棒性</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入Meta Policy Optimization框架解决大语言模型奖励对齐中的奖励黑客和提示工程问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于奖励的大语言模型对齐方法存在的两大局限性——易受奖励黑客影响及高度依赖人工设计的提示工程——提出了一种名为Meta Policy Optimization (MPO) 的新框架。MPO通过集成一个元奖励模型，在训练过程中动态优化奖励模型的提示，从而有效减少奖励信号被模型利用的可能性，同时大幅降低手动设计提示的需求。实验表明，该方法不仅在性能上媲美甚至优于传统方法，而且在多种任务（如问答和数学推理）中表现稳定，无需特定奖励设计。此外，MPO的元学习特性使其可扩展至更高层次的对齐框架，为大语言模型的奖励对齐提供了更稳健且灵活的解决方案。未来，代码和模型将公开共享。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 14:02:35 GMT</pubDate>
</item>
<item>
<title>TreeHop：一种高效的多跳问答系统</title>
<link>https://arxiv.org/abs/2504.20114</link>
<guid>https://arxiv.org/abs/2504.20114</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需LLMs的嵌入级框架TreeHop，显著提升多跳问答效率。</p><br /><br /><p><strong>摘要：</strong> 本文针对多跳问答（MHQA）中检索增强生成（RAG）系统的挑战，提出了一种名为TreeHop的新框架。传统方法依赖迭代的基于LLMs的查询重写和路由，导致高计算成本。TreeHop通过融合前序查询和文档的语义信息动态更新查询嵌入，在嵌入空间内完成迭代检索，从而避免重复调用LLMs和多阶段处理。此外，引入基于规则的停止准则进一步减少冗余检索，平衡效率与召回率。实验结果显示，TreeHop在三个公开数据集上的表现与先进RAG方法相当，但参数规模仅为5%-0.4%，查询延迟降低约99%。这一成果表明TreeHop是一种更快且更具成本效益的解决方案，适用于知识密集型应用。代码和数据已开源以促进复现研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20114" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:56:31 GMT</pubDate>
</item>
<item>
<title>DICE-Talk：基于解耦身份与情感的高表现力可泛化口型同步虚拟头像生成</title>
<link>https://arxiv.org/abs/2504.18087</link>
<guid>https://arxiv.org/abs/2504.18087</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架DICE-Talk，解决现有情感生成方法的情感表达不足问题。</p><br /><br /><p><strong>摘要：</strong> 近年来，基于扩散模型的虚拟口型同步技术在唇同步和视觉质量方面取得了显著进步，但在保持说话者身份的同时生成富有情感的表情仍然面临挑战。本文指出当前情感口型生成方法存在三大局限：音频内在情感线索利用不足、情感表示中的身份泄露以及情感相关性孤立学习的问题。为此，我们提出了名为DICE-Talk的新框架，通过解耦身份与情感并协作具有相似特性的感情来解决这些问题。首先，开发了一种解耦的情感嵌入器，通过跨模态注意力共同建模音视频情感线索，将情感表示为与身份无关的高斯分布。其次，引入了一个增强相关性的条件模块，使用可学习的情绪银行通过向量量化和基于注意力的特征聚合显式捕获情绪间关系。第三，设计了一种情绪辨别目标，在扩散过程中通过潜在空间分类强制情感一致性。在MEAD和HDTF数据集上的大量实验表明，该方法在情感准确性方面优于最先进的方法，同时保持了竞争性的唇同步性能。定性结果和用户研究进一步证实了该方法能够生成保留身份且具有丰富相关情感表达的可泛化虚拟头像。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18087" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 01:28:21 GMT</pubDate>
</item>
<item>
<title>BloomScrub：一种高效的大语言模型版权清除方法</title>
<link>https://arxiv.org/abs/2504.16046</link>
<guid>https://arxiv.org/abs/2504.16046</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法有效解决大语言模型的版权侵权问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型（LLMs）在预训练阶段接触大量受版权保护材料后可能引发的无意版权侵权问题，探讨了现有“版权清除”技术的局限性，特别是对最坏情况下的长段直接引用处理不足的问题。为此，我们提出了BloomScrub，一种简单而高效的推理阶段方法，通过交替检测并重写潜在侵权片段，利用高效的Bloom过滤器实现大规模文本的版权筛查。该方法不仅显著降低了侵权风险，还保证了模型功能的实用性，并支持根据需求调整执行严格程度。实验表明，这种轻量级的推理时间方法在版权预防方面表现出了令人惊讶的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16046" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:16:53 GMT</pubDate>
</item>
<item>
<title>YoChameleon: Personalized Vision and Language Generation</title>
<link>https://arxiv.org/abs/2504.20998</link>
<guid>https://arxiv.org/abs/2504.20998</guid>
<content:encoded><![CDATA[
Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>基于RGB-DN视频的高效四维世界模型学习方法</title>
<link>https://arxiv.org/abs/2504.20995</link>
<guid>https://arxiv.org/abs/2504.20995</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种通过RGB-DN视频学习四维世界模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种有效学习新型四维具身世界模型的方法，该模型能够预测三维场景随时间动态变化的过程，响应具身智能体的动作需求，提供空间和时间的一致性。我们建议通过训练RGB-DN（RGB、深度图和法线图）视频来学习四维世界模型。这种方法不仅超越传统的二维模型，通过整合详细的形状、配置和时间变化到预测中，而且还能有效地学习具身智能体的逆动力学模型。具体来说，我们首先利用现有的机器人操作视频数据集扩展深度和法线信息，然后微调视频生成模型，使其联合预测每帧的RGB-DN。最后，我们提出了一种算法，直接将生成的RGB、深度和法线视频转换为高质量的四维场景。我们的方法确保了具身场景中四维场景预测的时间和空间一致性，实现了具身环境中的新颖视图合成，并促进了显著优于先前基于视频的世界模型的策略学习。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20995" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>UniversalRAG：一种支持多模态异构知识检索的增强型生成框架</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，可从多模态异构知识源中检索并整合信息。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为UniversalRAG的新框架，该框架旨在通过从异构的多模态知识源中检索和整合相关信息，提升基于检索的生成模型（RAG）的能力。现有RAG方法大多局限于单一模态的知识库，而UniversalRAG则通过引入模态感知路由机制，动态确定最合适的模态特定知识库并进行针对性检索。此外，还对各模态按粒度级别组织，以实现根据查询复杂性和范围进行精细化检索。实验结果显示，在涉及多种模态的8个基准测试中，UniversalRAG的表现优于模态特定和统一基线。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20734" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 09:18:58 GMT</pubDate>
</item>
<item>
<title>ReasonIR-8B：首个专为推理任务训练的检索器</title>
<link>https://arxiv.org/abs/2504.20595</link>
<guid>https://arxiv.org/abs/2504.20595</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ReasonIR-8B通过合成数据显著提升了推理密集型信息检索任务的表现。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了ReasonIR-8B，这是首个专门针对一般推理任务训练的检索器。传统检索器在推理任务中的表现有限，因为现有训练数据集主要关注简单事实查询。我们开发了一种合成数据生成管道，该管道为每份文档创建具有挑战性和相关性的查询，同时结合看似相关但最终无用的硬负样本。通过混合使用合成数据和现有公开数据进行训练，ReasonIR-8B在BRIGHT基准测试中实现了新的最佳性能，nDCG@10分别达到了29.9（无重排序器）和36.9（有重排序器）。此外，在RAG任务中，它相对提高了MMLU和GPQA性能，分别提升了6.4%和22.6%，并更有效地利用了测试时计算资源。我们的训练方法通用且易于扩展到未来的大型语言模型，为此我们开源了代码、数据和模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20595" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:49:28 GMT</pubDate>
</item>
<item>
<title>通过1-shot强化学习实现大型语言模型数学推理能力的有效提升</title>
<link>https://arxiv.org/abs/2504.20571</link>
<guid>https://arxiv.org/abs/2504.20571</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究展示1-shot强化学习结合可验证奖励显著提升了大型语言模型的数学推理性能。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了一种基于单训练样本（1-shot）的强化学习方法，即强化学习与可验证奖励（RLVR），其在提升大型语言模型（LLMs）的数学推理能力方面表现出色。通过将此方法应用于基础模型Qwen2.5-Math-1.5B，我们发现仅使用一个示例即可显著提高其在MATH500测试集上的表现，从36.0%提升至73.6%，并在六个常见数学推理基准上平均提高了18.1个百分点。此外，实验表明，这种方法不仅适用于多种模型架构和强化学习算法，还能在多个数学问题上实现类似的效果。进一步研究还揭示了一些有趣的现象，如跨领域泛化能力增强、自我反思频率增加以及在训练精度饱和后持续的测试性能提升（称为“后饱和泛化”）。我们还验证了策略梯度损失在此过程中的关键作用，并强调了探索促进的重要性。最后，单独使用熵损失也能显著提高Qwen2.5-Math-1.5B在MATH500上的表现。这些发现为未来提高RLVR的数据效率提供了启示，并促使重新审视相关领域的进展及其机制。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.20571" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 05:24:30 GMT</pubDate>
</item>
<item>
<title>NORA：高效视觉-语言-动作模型实现机器人实时自主性</title>
<link>https://arxiv.org/abs/2504.19854</link>
<guid>https://arxiv.org/abs/2504.19854</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NORA模型，通过减少参数量提升机器人任务执行效率。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言-动作(VLA)模型在零样本场景下表现出色，但存在视觉编码限制导致任务失败的问题，且通常参数规模超过7B，带来高昂计算开销。NORA作为一款3B参数的新型模型，采用Qwen-2.5-VL-3B多模态模型作为基础，结合97万真实世界机器人演示数据及FAST+分词器，显著提升了视觉推理和动作定位能力，同时大幅降低计算资源需求。实验结果显示，NORA在任务表现上超越现有大型VLA模型，成为适用于实时机器人环境的更优解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19854" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:47:34 GMT</pubDate>
</item>
<item>
<title>Mem0：基于记忆增强的大语言模型对话一致性优化</title>
<link>https://arxiv.org/abs/2504.19413</link>
<guid>https://arxiv.org/abs/2504.19413</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Mem0架构解决大语言模型长期对话一致性问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对大型语言模型在长时间多会话对话中因固定上下文窗口导致的一致性挑战，引入了Mem0这一可扩展的记忆中心型架构。Mem0通过动态提取、整合和检索对话中的关键信息来维持一致性。此外，还提出了利用图结构记忆表示法捕捉复杂关系的方法。在LOCOMO基准测试中，Mem0在单跳、时间序列、多跳及开放领域四个问题类别上均优于六类基线系统。实验显示，Mem0在OpenAI的LLM-as-a-Judge指标上提升了26%，且显著降低了计算开销，相比全上下文方法，延迟降低91%，令牌成本节省超90%。这些成果表明，结构化持久化记忆机制对提升长期对话连贯性至关重要，为高效可靠的LLM驱动AI代理铺平了道路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19413" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 21:46:35 GMT</pubDate>
</item>
<item>
<title>基于领域适应的Chisel代码生成模型ChiseLLM</title>
<link>https://arxiv.org/abs/2504.19144</link>
<guid>https://arxiv.org/abs/2504.19144</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合数据处理和提示引导推理的ChiseLLM模型，显著提升Chisel代码语法正确性和设计变异性。</p><br /><br /><p><strong>摘要：</strong> 随着对特定领域架构需求的增长，敏捷硬件开发方法学得到了快速发展，而像Chisel这样的构造语言因其高级抽象特性成为理想选择。尽管大型语言模型在代码生成方面表现出色，但在Chisel代码生成任务上仍面临语法正确性和设计变异性挑战。本文介绍ChiseLLM，通过数据处理、提示引导推理追踪合成及领域适应模型训练，显著提升了Chisel代码生成性能，语法正确性提升至多26.32%，设计变异性提高47.58%。相关数据集和模型已公开，为基于硬件构造语言的敏捷开发提供高效工具，并为后续研究奠定基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19144" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 03:56:49 GMT</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 21:00:06 GMT</pubDate>
</item>
<item>
<title>RepText：一种无需理解文本的多语言视觉文本生成方法</title>
<link>https://arxiv.org/abs/2504.19724</link>
<guid>https://arxiv.org/abs/2504.19724</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RepText模型，提升文本到图像生成模型对非拉丁字母及多语言视觉文本的精确渲染能力。</p><br /><br /><p><strong>摘要：</strong> 尽管当前文本到图像生成模型在生成视觉上吸引人的图像方面取得了显著进展，但其在生成精确且灵活的印刷元素（尤其是非拉丁字母）方面的能力仍受到限制。为解决这些局限性，我们基于一个假设——即文本理解只是文本呈现的充分条件而非必要条件，提出了RepText模型。该模型通过引入字体无关的字符和位置信息，允许用户自定义文本内容、字体和位置，同时结合感知损失和扩散损失提高准确性。此外，在推理阶段，RepText采用噪声字符潜在初始化并使用区域掩码来稳定生成过程。实验表明，RepText在多语言视觉文本生成任务中表现出色，性能优于现有开源方法，与闭源多语言模型相当，同时也讨论了其局限性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19724" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 08:19:53 GMT</pubDate>
</item>
<item>
<title>利用替代密码研究In-Context Learning中的双模态操作</title>
<link>https://arxiv.org/abs/2504.19395</link>
<guid>https://arxiv.org/abs/2504.19395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入基于替代密码的任务重构，研究In-Context Learning的学习模式。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，In-Context Learning (ICL) 存在任务检索与任务学习两种模式，但区分这两种模式仍具挑战性。本文提出一种基于经典密码学中替代密码的任务重构方法ICL CIPHERS，通过部分替换上下文输入中的标记，使英文句子对人类不直观，但仍保持可逆性。实验显示，大型语言模型在处理具有双射映射的ICL CIPHERS时表现优于非双射基线，为量化ICL中的学习能力提供了新途径。此外，我们分析了模型内部表征，发现其具备解码加密输入的能力。这一研究结果在四个数据集和六种模型上具有一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 20:05:29 GMT</pubDate>
</item>
<item>
<title>基于自博弈批评者的大型语言模型推理可靠性评估方法</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需人工标注的自博弈批评者方法提升大语言模型推理可靠性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Self-Play Critic（SPC）的新方法，用于评估大型语言模型（LLM）的推理可靠性，如Chain-of-Thought推理。由于高质量的分步监督数据难以获取且成本高昂，SPC通过两个模型之间的对抗性自博弈游戏进化出评估推理步骤的能力，消除了对人工分步注释的需求。这两个模型分别是“狡猾生成器”和“批评者”，前者生成故意错误的推理步骤以迷惑后者，后者则试图检测这些错误。通过基于游戏结果的强化学习机制，两者迭代改进。实验表明，SPC在三个推理过程基准测试上提升了错误检测能力，并优于其他基线模型，同时在数学推理任务中显著提高了多种LLMs的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 27 Apr 2025 04:45:06 GMT</pubDate>
</item>
<item>
<title>CipherBank：评估大型语言模型在密码学推理中的能力</title>
<link>https://arxiv.org/abs/2504.19093</link>
<guid>https://arxiv.org/abs/2504.19093</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出CipherBank基准测试集，评估LLMs在加密解密任务中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍CipherBank，这是一个针对大型语言模型（LLMs）在密码学推理任务中表现进行评估的综合基准测试集。CipherBank包含2358个精心设计的问题，覆盖了5大领域和14种子领域的262种独特明文，重点关注隐私敏感和现实场景下的加密需求。从密码学角度来看，该基准涵盖了三大类加密方法，涉及9种不同的算法，从经典密码到自定义加密技术均有涉及。通过在CipherBank上对当前最先进的LLMs（如GPT-4、DeepSeek-V3等）及专注于推理的模型（如o1、DeepSeek-R1）进行评估，发现这些模型在推理能力上存在显著差距，不仅体现在通用聊天型LLMs与推理型LLMs之间，也表现在推理型LLMs在经典密码学解密任务中的表现不足。本研究通过详细分析和错误调查揭示了LLMs在密码推理方面的局限性及改进空间，强调了持续提升LLMs推理能力的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19093" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 23:41:17 GMT</pubDate>
</item>
<item>
<title>VCBENCH：大型视觉语言模型在多模态数学推理中的挑战与评估</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有大型视觉语言模型在处理依赖显式视觉的数学问题时表现有限。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型视觉-语言模型（LVLMs）在整合视觉与语言信息方面取得了显著进展，但在涉及基本数学元素和视觉概念的推理任务上仍存在不足。当前基准测试多集中于特定领域的专业知识评估，而忽视了基础数学推理能力。为填补这一空白，我们开发了VCBENCH，这是一个包含1720个问题的综合基准，涉及六个认知领域，平均每个问题包含近四个图像，强调多图推理。通过对26个最先进的LVLM进行测试，我们发现即使是最优秀的模型，在准确性上也未能超过50%，这揭示了视觉与数学整合方面的持续挑战，并为未来模型改进提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18589" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 02:16:38 GMT</pubDate>
</item>
<item>
<title>基于均匀下采样的群等变架构广义化研究</title>
<link>https://arxiv.org/abs/2504.17258</link>
<guid>https://arxiv.org/abs/2504.17258</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究了群等变架构中的均匀下采样层的广义化方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了在群等变架构（如G-CNNs）中均匀下采样层的一般化应用，特别是在反混淆下的通用有限群信号（特征图）下采样问题。首先，给出了一种根据有限群和下采样率选择合适子群的算法；其次，在给定群和子群的情况下，研究了带限性概念并提出了反混淆操作的实现方式。该方法推广了经典采样理论中的下采样概念，当信号位于循环群时，其等效于理想低通滤波器后接下采样操作。实验表明，将所提出的下采样操作引入G-等变网络中，可提高图像分类任务的准确性，更好地保持等变性并减少模型规模。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17258" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 01:29:51 GMT</pubDate>
</item>
<item>
<title>MMInference：一种加速多模态长上下文推理的动态稀疏注意力方法</title>
<link>https://arxiv.org/abs/2504.16083</link>
<guid>https://arxiv.org/abs/2504.16083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MMInference方法，显著加速视觉语言模型的长上下文推理。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MMInference的动态稀疏注意力方法，旨在解决视觉语言模型(VLMs)在处理长上下文多模态输入时，由于二次注意力复杂度导致的预填充阶段效率低下问题。通过分析视频输入的时间和空间局部性，我们发现了一种独特的Grid模式，并提出了基于排列的方法来利用这一模式并处理模态边界问题。此外，通过离线搜索每个头的最佳稀疏模式，MMInference能够根据输入动态构建稀疏分布。该方法无需对现有VLM管道进行任何修改或微调即可无缝集成，并且在多模态基准测试中表现出高达8.3倍的速度提升，同时保持了准确性。实验涵盖了多种任务，如视频问答、视频描述生成等。我们的代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:51 GMT</pubDate>
</item>
<item>
<title>基于可信赖生成的数据引擎TrustGeoGen用于几何问题求解</title>
<link>https://arxiv.org/abs/2504.15780</link>
<guid>https://arxiv.org/abs/2504.15780</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为TrustGeoGen的数据引擎，用于生成高质量几何问题数据集。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TrustGeoGen的可扩展数据引擎，专门用于生成几何问题数据。该引擎通过多模态对齐生成图表、文本描述及分步解答，结合形式化验证确保推理路径符合规则，并采用自举机制递归生成复杂性增加的状态。此外，设计的GeoExplore系列算法同时生成多解变体并进行自我反思回溯跟踪。通过形式逻辑验证，生成的GeoTrust-200K数据集保证了模态完整性，且GeoTrust-test测试集显示出极高的评估严格性，当前最先进的模型仅达到49.17%的准确率。训练后的模型在GeoQA上表现出出色的OOD泛化能力，显著减少了逻辑不一致性。此工作为几何问题求解方法的发展奠定了基础。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15780" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:45:23 GMT</pubDate>
</item>
<item>
<title>基于大型语言模型的手机图形用户界面智能代理综述</title>
<link>https://arxiv.org/abs/2504.19838</link>
<guid>https://arxiv.org/abs/2504.19838</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">综述大型语言模型如何推动手机GUI自动化向智能化方向发展。</p><br /><br /><p><strong>摘要：</strong> 本文系统性回顾了由大型语言模型（LLMs）驱动的手机图形用户界面（GUI）智能代理的发展历程，从基于脚本的传统自动化转变为具备智能和适应性的系统。首先分析了传统自动化面临的三大挑战：泛化能力有限、维护成本高及意图理解薄弱，并阐述LLMs通过先进的语言理解、多模态感知和稳健决策解决这些问题的方法。接着提出了一种涵盖基础代理框架、建模方法及关键数据集和基准的分类体系。此外，还详细介绍了针对特定任务的架构设计、监督微调及强化学习策略，这些技术连接了用户意图与GUI操作。最后，讨论了该领域尚未解决的问题，如数据集多样性、设备端高效部署、用户定制化及安全问题，为研究人员提供了未来研究方向的前瞻性见解。本文旨在为开发可扩展且用户友好的手机GUI代理提供权威参考。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.19838" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 10:39:25 GMT</pubDate>
</item>
<item>
<title>大型语言模型在医疗建议中的表现与用户交互挑战</title>
<link>https://arxiv.org/abs/2504.18919</link>
<guid>https://arxiv.org/abs/2504.18919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示，尽管大型语言模型在医学考试中表现出色，但用户交互影响其实际应用效果。</p><br /><br /><p><strong>摘要：</strong> 全球医疗提供者正在探索使用大型语言模型（LLMs）向公众提供医疗建议。LLMs在医学执照考试中几乎达到满分，但在现实场景中的准确性仍存疑。本研究测试了LLMs是否能帮助公众识别潜在疾病并选择行动方案，在1,298名参与者中进行了实验。单独测试时，LLMs正确识别疾病的比例为94.9%，确定处置方案的比例为56.3%，但当参与者使用相同模型时，识别疾病的比例降至34.5%，确定处置方案的比例降至44.2%，均未显著优于对照组。研究指出用户交互是LLMs在医疗建议中部署的关键挑战。现有医学知识基准和模拟患者互动无法预测人类参与者中出现的问题。因此，我们建议在面向公众部署前进行系统化的人类用户测试，以评估其交互能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 26 Apr 2025 09:32:49 GMT</pubDate>
</item>
<item>
<title>Kimi-Audio Technical Report</title>
<link>https://arxiv.org/abs/2504.18425</link>
<guid>https://arxiv.org/abs/2504.18425</guid>
<content:encoded><![CDATA[
We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:31:46 GMT</pubDate>
</item>
<item>
<title>新一代小型推理模型Pleias-RAG在RAG和搜索领域的突破性进展</title>
<link>https://arxiv.org/abs/2504.18225</link>
<guid>https://arxiv.org/abs/2504.18225</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">新推出的Pleias-RAG系列模型在多语言开放源检索及引用支持方面表现优异。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了两种新型的小型推理模型Pleias-RAG-350m和Pleias-RAG-1B，这些模型专门设计用于检索增强生成（RAG）、搜索以及来源摘要。通过在一个大型合成数据集上的训练，这些模型能够模拟从Common Crawl中检索多种多语言开放资源的过程。它们不仅提供直接引用和上下文关联的支持，还重新整合了与RAG工作流相关的多个功能，例如查询路由、查询重写和来源重排序。在标准化的RAG基准测试（如HotPotQA和2Wiki）中，这两种模型的表现优于参数少于40亿的单语言模型，并且与更大规模的流行模型（如Qwen-2.5-7B、Llama-3.1-8B和Gemma-3-4B）竞争。值得注意的是，Pleias-RAG系列是目前唯一能在主要欧洲语言中保持一致RAG性能并确保陈述系统性引用关联的单语言模型。由于其较小的体积和对受限基础设施的良好适应性，这些模型为生成式人工智能开辟了新的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18225" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 06:17:04 GMT</pubDate>
</item>
<item>
<title>Transformer LLMs中稀疏注意力的研究：效率-准确性权衡与扩展性分析</title>
<link>https://arxiv.org/abs/2504.17768</link>
<guid>https://arxiv.org/abs/2504.17768</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究探讨Transformer模型中稀疏注意力方法的可行性及效率-准确性权衡。</p><br /><br /><p><strong>摘要：</strong> 本文针对Transformer语言模型处理长上下文的能力，研究了无需训练的稀疏注意力方法。通过在多种长序列任务上的实验，我们发现：1）在非常长的序列上，较大的高稀疏模型优于较小的密集模型；2）解码阶段相比填充阶段可以实现更高的稀疏水平并保证准确性；3）没有一种策略在所有任务和阶段中表现最佳，不同场景需要不同的稀疏化单元或预算适应性；4）即使中等稀疏度也可能导致某些任务性能显著下降，表明稀疏注意力并非万能解决方案。此外，我们提出了适用于稀疏注意力的新尺度定律，并验证了这些发现可能超越实验范围。综上所述，稀疏注意力是增强Transformer模型长序列处理能力的关键工具，但需要对性能敏感的应用进行仔细评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17768" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:39:25 GMT</pubDate>
</item>
<item>
<title>针对意大利语优化的英语大型语言模型词汇适应技术</title>
<link>https://arxiv.org/abs/2504.17025</link>
<guid>https://arxiv.org/abs/2504.17025</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出一种新方法优化英语大型语言模型以处理意大利语。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了多种词汇适应技术，以提升英语大型语言模型（LLMs）对意大利语的处理能力。传统LLMs虽能处理多语言，但因训练数据混杂或非优化设计，在非英语语言上的表现欠佳。为此，我们提出了语义对齐词汇适应（SAVA）方法，利用神经映射实现词汇替换。通过适配两个模型——Mistral-7b-v0.1和Llama-3.1-8B，分别降低了25%的标记“生育率”和减少10亿参数。实验表明，经过词汇适配后，这些模型在持续少量目标语言训练后即可恢复性能。最后，我们在多项选择和生成任务中测试了适配后的模型能力，显示出显著效果。此研究为多语言LLMs优化提供了新的思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17025" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:12:27 GMT</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo：首个跨文化视频评估基准</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个跨文化、多语言、多领域的视频理解评估基准VideoVista-CulturalLingo。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了VideoVista-CulturalLingo，这是一个旨在弥合文化、语言和领域差距的视频理解评估基准。与现有基准不同，它涵盖了中国文化、北美文化和欧洲文化，问题以中文和英文呈现，并包含来自数百个人类创作领域的视频。该基准包含1389个视频和3134个QA对，并评估了24个最近开源或专有的视频大模型。实验结果显示，现有模型在与中国历史相关的问题上表现较差，在事件定位任务中的时间理解能力有限，而主流模型在科学问题上表现出色，但开源模型在数学问题上的表现较弱。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17821" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 09:47:30 GMT</pubDate>
</item>
<item>
<title>Skywork R1V2：下一代多模态推理模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.16656</link>
<guid>https://arxiv.org/abs/2504.16656</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Skywork R1V2引入混合强化学习范式，显著提升多模态推理能力。</p><br /><br /><p><strong>摘要：</strong> Skywork R1V2作为Skywork R1V的升级版，通过结合奖励模型指导与基于规则策略的混合强化学习框架，解决了复杂推理与广泛泛化之间的平衡难题。此外，R1V2创新性地采用Selective Sample Buffer机制，优化训练效率并缓解了GRPO中的“优势消失”问题。实验表明，R1V2在多项基准测试中表现优异，如OlympiadBench得分62.6、AIME2024得分79.0等，展现了其在开放源代码模型中的领先地位，并逐步缩小与顶级专有系统的性能差距。该模型权重已公开发布，以促进研究的透明度与可重复性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16656" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 08:24:10 GMT</pubDate>
</item>
<item>
<title>零样本条件下的个性化视频生成模型</title>
<link>https://arxiv.org/abs/2504.17816</link>
<guid>https://arxiv.org/abs/2504.17816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种解耦身份学习与时间动态的个性化视频生成方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种无需微调的个性化视频生成模型，通过将身份特定的学习与时间动态解耦，在零样本设置下实现视频定制。传统方法依赖大规模标注数据集，成本高昂且需要大量标注工作，而我们的方法直接利用图像定制数据集训练视频定制模型，并将其分解为两个部分：通过图像定制数据集进行身份注入，以及借助少量未标注视频通过图像到视频训练方法保留时间建模。此外，在图像到视频微调过程中采用随机图像标记丢弃和随机图像初始化以缓解复制粘贴问题，并引入随机切换机制以增强学习效果，避免灾难性遗忘。实验表明，该方法在零样本设置下表现出色，具有较强的主体一致性与可扩展性，优于现有视频定制模型。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 02:48:31 GMT</pubDate>
</item>
<item>
<title>DianJin-R1：面向金融领域的推理增强框架</title>
<link>https://arxiv.org/abs/2504.15716</link>
<guid>https://arxiv.org/abs/2504.15716</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DianJin-R1框架，通过强化推理能力提升金融领域大语言模型性能。</p><br /><br /><p><strong>摘要：</strong> 本文针对金融领域大语言模型在推理方面的挑战，提出了一种名为DianJin-R1的推理增强框架。该框架通过构建高质量的数据集DianJin-R1-Data（整合CFLUE、FinQA及自有合规语料库），结合结构化输出的方式对Qwen2.5进行微调，显著提升了模型的推理能力。此外，采用Group Relative Policy Optimization方法优化模型的奖励机制，进一步提高了答案的准确性。实验表明，DianJin-R1在复杂金融任务上表现优异，甚至超越多代理系统，为实际应用提供了高效且实用的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15716" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 05:01:04 GMT</pubDate>
</item>
<item>
<title>BitNet v2：高效部署1比特大语言模型的新框架</title>
<link>https://arxiv.org/abs/2504.18415</link>
<guid>https://arxiv.org/abs/2504.18415</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BitNet v2框架，实现1比特大语言模型的4比特激活量化。</p><br /><br /><p><strong>摘要：</strong> 本文针对1比特大语言模型（LLMs）在低比特量化时因激活异常值导致的部署难题，引入了BitNet v2这一创新框架，支持原生4比特激活量化。为解决注意力机制及前馈网络中的异常激活问题，提出了H-BitLinear模块，在量化之前应用在线哈达玛变换，将尖锐分布平滑为更符合高斯分布的形式，从而适应低比特表示。实验表明，从零训练的BitNet v2在8比特激活下性能与BitNet b1.58相当，并且在使用原生4比特激活训练时仅产生极小的性能下降，显著减少了内存占用和计算成本，适用于批量推理。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.18415" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 25 Apr 2025 11:17:52 GMT</pubDate>
</item>
<item>
<title>MMLA基准：多模态语言理解能力的全面评估</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现现有多模态大模型在认知级语义理解上存在局限性。</p><br /><br /><p><strong>摘要：</strong> 多模态语言分析是理解人类会话语义的重要领域，但当前多模态大型语言模型（MLLMs）在认知级语义理解方面的能力尚待深入研究。本文介绍了一个名为MMLA的新基准，该基准包含超过61,000个多模态语句，涵盖意图、情感、对话行为、情感倾向、说话风格及沟通行为六大维度。通过零样本推理、监督微调和指令微调三种方法对主流LLMs和MLLMs进行评估后发现，即使经过微调的模型在复杂人类语言理解上的准确率也仅在60%-70%之间，凸显了现有MLLMs的局限性。MMLA有望成为探索大语言模型潜力的重要基础，并为推动多模态语言分析领域的发展提供宝贵资源。相关数据集和代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16427" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 01:25:13 GMT</pubDate>
</item>
<item>
<title>CameraBench：评估与提升摄像机运动理解的数据集与基准</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CameraBench数据集，用于评估和改进摄像机运动理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为CameraBench的大规模数据集和基准，该数据集由约3000段互联网视频组成，经过专家的多阶段质量控制标注而成。我们与电影摄影师合作开发了一种摄像机运动基元的分类法，发现某些运动如“跟踪”需要理解场景内容。通过大规模的人类研究量化了人类标注性能，发现领域专业知识和教程培训可以显著提高准确性。使用CameraBench，我们评估了运动恢复结构（SfM）模型和视频-语言模型（VLMs），发现SfM模型难以捕捉依赖场景内容的语义基元，而VLMs则难以捕捉需要精确轨迹估计的几何基元。我们进一步在一个生成式VLM上进行微调，使其兼具两者优势，并展示了其在运动增强的字幕生成、视频问答和视频-文本检索中的应用。我们希望此分类法、基准和教程能够推动未来对视频中摄像机运动理解的研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15376" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 14:34:57 GMT</pubDate>
</item>
<item>
<title>基于Dual Consistency SAM方法的上下文分割研究</title>
<link>https://arxiv.org/abs/2504.12080</link>
<guid>https://arxiv.org/abs/2504.12080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于SAM的上下文图像及视频分割方法DC-SAM。</p><br /><br /><p><strong>摘要：</strong> 上下文分割（In-context Segmentation）作为小样本学习中的单次分割任务，探索了分割模型的泛化能力，被应用于场景理解和图像/视频编辑等任务。然而，现有的Segment Anything Models在交互式分割中表现优异，但不适用于上下文分割。本文提出Dual Consistency SAM (DC-SAM) 方法，通过提示调优适配SAM和SAM2进行图像和视频的上下文分割。该方法通过提供高质量视觉提示增强SAM提示编码器特征，并设计循环一致性交叉注意力和双分支设计提升性能。此外，还提出了掩码管训练策略以适应所提出的双重一致性方法。尽管DC-SAM主要针对图像设计，但在SAM2的支持下可无缝扩展到视频领域。由于视频领域的上下文分割尚属空白，我们手动整理并构建首个基准数据集IC-VOS，用于评估模型的上下文分割能力。实验结果显示，DC-SAM在COCO-20i上达到55.5 mIoU，在PASCAL-5i上达到73.0 mIoU，并在IC-VOS基准上获得71.52的J&amp;F分数。源代码和基准数据集已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:41:59 GMT</pubDate>
</item>
<item>
<title>DynPose-100K：大规模动态互联网视频的相机姿态标注数据集</title>
<link>https://arxiv.org/abs/2504.17788</link>
<guid>https://arxiv.org/abs/2504.17788</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种大规模动态视频相机姿态标注数据集DynPose-100K。</p><br /><br /><p><strong>摘要：</strong> 随着逼真的视频生成和模拟领域的发展，在动态互联网视频上大规模标注相机姿态变得至关重要。然而，由于大多数互联网视频不适合姿态估计，收集这样的数据集极具挑战性。本文介绍了一个名为DynPose-100K的大规模动态互联网视频数据集，其中包含相机姿态注释。为了克服数据收集的困难，我们设计了一个结合特定任务模型和通用模型的过滤管道。此外，通过整合最新的点跟踪、动态掩蔽和运动结构技术，我们在姿态估计方面实现了对现有方法的改进。实验分析表明，DynPose-100K在多个关键属性上具有大规模和多样性，为下游应用的进一步发展开辟了新的途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17788" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>DiMeR：一种用于稀疏视图网格重建的解耦双流前馈模型</title>
<link>https://arxiv.org/abs/2504.17670</link>
<guid>https://arxiv.org/abs/2504.17670</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出了一种新型解耦双流模型DiMeR，显著提升了稀疏视图网格重建的效果。</p><br /><br /><p><strong>摘要：</strong> 随着大规模3D数据集的到来，前馈3D生成模型如大尺度重建模型（LRM）受到广泛关注并取得显著成功。然而，RGB图像往往导致训练目标冲突且缺乏几何重建所需的清晰度。本文重新审视了与网格重建相关的归纳偏差，引入了DiMeR，这是一种新颖的解耦双流前馈模型，用于稀疏视图网格重建。DiMeR通过将输入和框架分解为几何和纹理部分，减少了每个部分的训练难度。几何分支利用法线贴图作为输入，而纹理分支则使用RGB图像获取纹理网格。实验表明，DiMeR在多种任务中表现出色，并在GSO和OmniObject3D数据集上显著优于现有方法，Chamfer距离提升超过30%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17670" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 11:39:20 GMT</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:44:54 GMT</pubDate>
</item>
<item>
<title>Token-Shuffle：通过维度重排提升基于Transformer的文本到高分辨率图像生成</title>
<link>https://arxiv.org/abs/2504.17789</link>
<guid>https://arxiv.org/abs/2504.17789</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Token-Shuffle方法，大幅减少Transformer中的图像tokens数量，实现高效高分辨率图像生成。</p><br /><br /><p><strong>摘要：</strong> 本文针对自回归（AR）模型在图像合成领域的局限性展开研究，主要问题在于其所需图像tokens数量庞大，影响训练和推理效率及图像分辨率。为此，我们提出了Token-Shuffle，一种新颖且简单的方法，通过在视觉编码器生成的低维视觉代码映射到语言词汇表时利用视觉词汇表的维度冗余性，减少输入tokens的数量。具体而言，Token-Shuffle通过在通道维度上合并空间局部tokens来降低输入tokens数量，而token-unshuffle则在Transformer块后解纠缠推断出的tokens，恢复空间排列以输出图像。该策略无需额外的预训练文本编码器，使多模态大语言模型（MLLMs）能够以统一的下一-token预测方式支持极高分辨率（如2048x2048）的图像生成，同时保持高效的训练和推理。实验表明，在GenAI基准测试中，我们的2.7B参数模型在困难提示下获得0.77的整体分数，优于LlamaGen 0.18分，超过LDM 0.15分。大规模的人类评估进一步证明了我们在文本对齐、视觉瑕疵和视觉外观方面的卓越能力。我们希望Token-Shuffle能成为高效高分辨率图像生成的基础设计。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17789" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>结合线性和非线性方法的新一代降维算法</title>
<link>https://arxiv.org/abs/2504.17601</link>
<guid>https://arxiv.org/abs/2504.17601</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">一种结合线性和非线性特性的新型降维技术被提出。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种新的降维方法，旨在解决现有技术如t-SNE和PCA在表达能力和可解释性之间的权衡问题。该算法通过将高维空间映射到低维空间时结合线性变换和高斯加权的非线性变换，实现了复杂非线性转换的同时保持线性方法的可解释性。文章详细描述了这种架构的设计及其在几何关系保留上的优势，并提出了用于分析学习到的变换的技术，例如识别被抑制的维度以及空间如何被扩展和收缩的方法。此外，为了促进学术界和工业界的广泛应用，强调了开发用户友好型软件包的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17601" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 10:26:42 GMT</pubDate>
</item>
<item>
<title>RefVNLI：一种高效可靠的文本到图像生成评估方法</title>
<link>https://arxiv.org/abs/2504.17502</link>
<guid>https://arxiv.org/abs/2504.17502</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法RefVNLI，用于同时评估文本对齐和主体一致性。</p><br /><br /><p><strong>摘要：</strong> 当前基于主体驱动的文本到图像生成技术在个性化图像生成和视频角色表现等领域有广泛应用潜力，但缺乏可靠的自动评估手段。现有评估方法通常仅关注任务的一个方面、与人类判断不一致或依赖昂贵的API评估。为解决这一问题，我们提出了RefVNLI，这是一种成本效益高的度量方法，能够在单一预测中评估文本对齐和主体保存。通过在大规模数据集上训练，RefVNLI在多个基准测试和主体类别中表现出色，相较于现有基线方法，在文本对齐和主体一致性方面分别提高了6.4和8.5个百分点。此外，它在处理较冷门概念时也表现出色，与人类偏好的一致性超过87%。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17502" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 08:44:51 GMT</pubDate>
</item>
<item>
<title>基于扩散模型的高质量视频试穿框架3DV-TON</title>
<link>https://arxiv.org/abs/2504.17414</link>
<guid>https://arxiv.org/abs/2504.17414</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的扩散模型方法，解决复杂服装图案和多变人体姿态下的高质量视频试穿问题。</p><br /><br /><p><strong>摘要：</strong> 现有的视频试穿技术在处理复杂的服装图案和多样的人体姿势时，难以生成高质量且时间上一致的结果。本文介绍了一种名为3DV-TON的新框架，该框架基于扩散模型，旨在生成高保真度且时间一致的视频试穿效果。3DV-TON通过生成可动画化的纹理化3D网格作为显式的帧级指导，缓解了模型过于关注外观细节而忽略运动连贯性的问题。具体而言，该方法首先选择关键帧进行初始2D图像试穿，随后重建并同步原始视频姿态的纹理化3D网格。此外，我们引入了一种鲁棒的矩形掩码策略，成功减少了动态人体和服装运动过程中因服装信息泄漏导致的伪影传播。为了推动视频试穿研究的发展，我们还创建了一个名为HR-VVT的高分辨率基准数据集，包含130段具有多样服装类型和场景的视频。定量和定性结果表明，我们的方法在性能上优于现有方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17414" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:12:40 GMT</pubDate>
</item>
<item>
<title>TimeChat-Online：革新实时视频交互的在线VideoLLM</title>
<link>https://arxiv.org/abs/2504.17343</link>
<guid>https://arxiv.org/abs/2504.17343</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TimeChat-Online，解决流媒体视频冗余问题并实现高效实时互动。</p><br /><br /><p><strong>摘要：</strong> 随着在线视频平台特别是直播服务的快速发展，在线视频理解系统的需求日益迫切。然而，现有的VideoLLMs在处理完整视频时表现出色，但在流媒体场景下由于无法有效处理密集冗余帧而面临显著限制。本文介绍了一种名为TimeChat-Online的新型在线VideoLLM，它通过创新的Differential Token Drop (DTD)模块解决了流媒体视频中的视觉冗余问题。DTD模块受到人类视觉感知变化盲视现象的启发，能够在过滤冗余内容的同时保留有意义的时间变化。实验表明，DTD可以将视频令牌减少82.8%，同时在StreamingBench上保持98%的性能。此外，TimeChat-Online-139K数据集支持多种交互模式，包括后向追踪、当前感知和未来响应。TimeChat-Online的独特主动响应能力也使其在流媒体基准测试和长视频任务中表现优异。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17343" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 03:59:46 GMT</pubDate>
</item>
<item>
<title>PaperCoder：基于多智能体大语言模型的论文代码自动生成框架</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">PaperCoder通过多阶段生成高质量机器学习论文代码实现。</p><br /><br /><p><strong>摘要：</strong> 尽管机器学习研究快速发展，但相关代码实现往往不可用，导致研究人员难以复现结果或扩展先前工作。近期大型语言模型（LLMs）在理解科学文档和生成高质量代码方面表现出色。受此启发，我们提出了PaperCoder，这是一种多智能体LLM框架，可将机器学习论文转化为功能完善的代码库。PaperCoder分为三个阶段：规划阶段构建高层次路线图、设计系统架构并生成配置文件；分析阶段专注于解析特定实现细节；生成阶段则产生模块化且依赖感知的代码。每个阶段均由专门设计的智能体协作完成。我们在基于模型和人工评估的基础上对PaperCoder进行了评估，以机器学习论文为基础生成代码实现，同时以作者发布的代码库作为真实数据。结果显示，PaperCoder生成的代码质量高且忠实于原论文，尤其在新发布的PaperBench基准测试中表现优异，大幅超越了其他基线方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17192" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 21:57:01 GMT</pubDate>
</item>
<item>
<title>基于任意顺序补丁生成的自回归图像生成方法</title>
<link>https://arxiv.org/abs/2504.17069</link>
<guid>https://arxiv.org/abs/2504.17069</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的自回归图像生成方法，通过训练模型在任意顺序下生成补丁，显著提升图像质量。</p><br /><br /><p><strong>摘要：</strong> 自回归补丁式图像生成在图像质量和可扩展性方面表现出色，且易于整合到视觉语言模型中。然而，这类模型需要定义补丁生成的顺序，传统上采用从左上角到右下角的光栅扫描顺序。本文指出这种顺序并非最优，因为它无法尊重图像内容的因果关系。为此，我们首先训练模型能够在任意顺序下生成补丁，并在生成过程中推断每个补丁的内容及其位置。其次，利用这些提取出的顺序对任意顺序模型进行微调，从而生成更高质量的图像。实验结果显示，新方法在两个数据集上的生成效果优于传统的光栅扫描方法，且训练成本相近，无需额外标注。关键词：自回归生成、补丁生成、图像质量优化。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17069" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 15:33:58 GMT</pubDate>
</item>
<item>
<title>DyMU：一种高效且无需训练的视觉-语言模型动态计算降低框架</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DyMU框架，通过动态合并视觉标记实现VLM高效计算。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DyMU的高效、无需训练的框架，用于动态减少视觉-语言模型（VLMs）的计算负担，同时保持高任务性能。该方法包含两个关键组件：动态标记合并（DToMe）通过基于图像复杂度合并相似标记来减少视觉标记嵌入的数量；虚拟标记拆分（VTU）则通过高效重建完整序列的注意力动态来模拟大型语言模型的预期标记序列，从而在不进行额外微调的情况下保持下游性能。与现有方法不同，DyMU根据图像内容动态调整标记压缩，且完全无需训练，可轻松应用于大多数最先进的VLM架构。实验表明，DyMU在多种VLM架构上平均减少了32%-85%的视觉标记数量，同时保持了与全长度模型相当的性能。此外，定性分析显示，DToMe可根据图像复杂度有效调整标记减少量，为用户提供更多对计算成本的控制权。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17040" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 14:38:18 GMT</pubDate>
</item>
<item>
<title>IberBench：评估伊比利亚半岛及伊比罗-美洲语言的大语言模型基准</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">IberBench是一个针对伊比利亚半岛及伊比罗-美洲语言的大语言模型综合评估基准。</p><br /><br /><p><strong>摘要：</strong> 现有的大型语言模型（LLMs）评估基准主要集中在英语上，对于其他语言尤其是资源有限的语言，评估方法存在不足。这些基准通常忽视了语言多样性，过分关注基础自然语言处理（NLP）能力而忽略了工业相关的任务，并且缺乏动态更新机制。为解决这些问题，我们提出了IberBench，这是一个全面且可扩展的基准，用于评估伊比利亚半岛及伊比罗-美洲语言上的LLMs在基础和工业相关NLP任务中的表现。IberBench整合了来自评估活动和最新基准的101个数据集，涵盖22个任务类别，如情感分析、情绪分析、毒性检测和摘要生成等。此外，该基准通过支持持续更新和社区驱动的模型与数据集提交，解决了当前评估实践中存在的问题。我们对23个参数规模从1亿到140亿的LLMs进行了评估，并提供了关于其优势和局限性的实证见解。研究发现，LLMs在工业相关任务上的表现不如基础任务好，在加利西亚语和巴斯克语上的表现较低，某些任务的结果接近随机水平，而在其他任务上LLMs的表现高于随机但低于共享任务系统。IberBench提供了完整的开源实现，包括数据集标准化和托管、LLMs的增量评估以及公开的排行榜。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:48:25 GMT</pubDate>
</item>
<item>
<title>QuaDMix：统一框架优化大语言模型训练数据的质量与多样性</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出QuaDMix框架，同时优化大语言模型训练数据质量和多样性。</p><br /><br /><p><strong>摘要：</strong> 现有研究通常分别优化大语言模型（LLMs）训练数据的质量和多样性，忽视了两者之间的权衡关系。本文提出QuaDMix框架，通过统一的数据采样函数，在固定训练预算下平衡质量与多样性。QuaDMix利用多个指标衡量数据质量，并借助领域分类评估整体多样性。通过模拟实验和参数搜索加速框架优化，最终在多个基准测试中实现平均性能提升7.2%，显著优于独立优化策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16511" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 04:36:50 GMT</pubDate>
</item>
<item>
<title>一种结合表征学习与扩散模型的图像生成新框架</title>
<link>https://arxiv.org/abs/2504.16064</link>
<guid>https://arxiv.org/abs/2504.16064</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，将扩散模型与表征学习结合，提升图像生成质量和训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新的图像生成框架，通过引入扩散模型同时建模低级图像潜在表示和高级语义特征，实现了表征学习与生成建模的无缝融合。该方法基于变分自编码器和预训练的自监督编码器，从纯噪声开始生成连贯的图像-特征对，在不显著改变标准扩散变换架构的情况下显著提升了生成质量与训练效率。此外，通过引入“表征引导”的推理策略，进一步增强了图像生成的可控性。实验表明，该方法在条件和无条件生成设置下均表现出色，为表征感知的生成建模开辟了新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16064" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:41:42 GMT</pubDate>
</item>
<item>
<title>ViSMap：基于元提示的无监督视频摘要技术</title>
<link>https://arxiv.org/abs/2504.15921</link>
<guid>https://arxiv.org/abs/2504.15921</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">ViSMap通过元提示策略生成长视频伪摘要，实现无监督长视频摘要。</p><br /><br /><p><strong>摘要：</strong> ViSMap是一种无需标注的长视频摘要系统，解决了现有模型在稀疏分布事件长视频上的总结难题。传统方法依赖于昂贵的分层监督训练，而ViSMap利用LLMs根据短视频片段描述生成长视频的伪摘要，作为长视频摘要模型的训练数据。该系统采用元提示策略迭代生成并优化伪摘要，使用三个LLMs依次生成、评估和改进伪摘要，以提升质量。实验表明，ViSMap在多个数据集上表现优异，性能接近完全监督的最先进模型，且具有跨领域的泛化能力。代码将在发表后公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15921" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 10:06:01 GMT</pubDate>
</item>
<item>
<title>Step1X-Edit：开源图像编辑模型的突破性进展</title>
<link>https://arxiv.org/abs/2504.17761</link>
<guid>https://arxiv.org/abs/2504.17761</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种开源图像编辑模型Step1X-Edit，性能接近闭源顶级模型。</p><br /><br /><p><strong>摘要：</strong> 近年来，图像编辑模型发展迅猛，闭源模型如GPT-4o和Gemini2 Flash展现了强大的图像编辑能力。然而，现有开源算法与其存在显著差距。本文发布了一种名为Step1X-Edit的最新图像编辑模型，通过多模态大语言模型处理参考图像和用户指令，利用潜在嵌入与扩散图像解码器结合生成目标图像。为训练该模型，构建了高质量数据生成管道，并开发了基于真实用户指令的GEdit-Bench基准进行评估。实验结果显示，Step1X-Edit大幅超越现有开源基线，接近领先闭源模型的性能，为图像编辑领域做出了重要贡献。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17761" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 13:25:12 GMT</pubDate>
</item>
<item>
<title>UniME：基于多模态大语言模型的表征学习框架</title>
<link>https://arxiv.org/abs/2504.17432</link>
<guid>https://arxiv.org/abs/2504.17432</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的两阶段框架UniME，提升多模态表示学习能力。</p><br /><br /><p><strong>摘要：</strong> 现有的对比语言图像预训练（CLIP）框架存在文本截断、孤立编码及组合性不足等问题，限制了其效能。尽管多模态大型语言模型（MLLMs）在视觉-语言理解方面取得了显著进展，但其在迁移学习中的潜力尚未充分挖掘。本文介绍了一种名为UniME的新型两阶段框架，通过从强大的基于LLM的教师模型中进行文本辨别性知识蒸馏，增强MLLM的语言组件嵌入能力，并引入硬负样本增强指令微调进一步改进表征学习。实验结果显示，UniME在多个基准测试和检索任务中均表现出色，具有更强的辨别能力和组合性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17432" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 06:51:52 GMT</pubDate>
</item>
<item>
<title>基于抽象视角变化的视觉语言模型视角感知推理框架</title>
<link>https://arxiv.org/abs/2504.17207</link>
<guid>https://arxiv.org/abs/2504.17207</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用心理意象模拟提升视觉语言模型视角感知能力的新框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个用于视觉语言模型(VLMs)视角感知推理的框架，名为抽象视角变化(APC)。该框架通过模拟心理意象，利用视觉基础模型如物体检测、分割和方向估计来构建场景抽象并实现视角转换，从而有效弥补现代VLMs在视角感知推理上的不足。实验结果显示，该框架在合成图像和真实图像基准测试中显著提升了视角感知推理能力，优于微调的空间推理模型和基于新视图合成的方法。这项研究对提高VLMs的人类水平视觉理解能力具有重要意义，有助于促进人机交互与协作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.17207" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 22:41:34 GMT</pubDate>
</item>
<item>
<title>Causal-Copilot: An Autonomous Causal Analysis Agent</title>
<link>https://arxiv.org/abs/2504.13263</link>
<guid>https://arxiv.org/abs/2504.13263</guid>
<content:encoded><![CDATA[
Causal analysis plays a foundational role in scientific discovery and reliable decision-making, yet it remains largely inaccessible to domain experts due to its conceptual and algorithmic complexity. This disconnect between causal methodology and practical usability presents a dual challenge: domain experts are unable to leverage recent advances in causal learning, while causal researchers lack broad, real-world deployment to test and refine their methods. To address this, we introduce Causal-Copilot, an autonomous agent that operationalizes expert-level causal analysis within a large language model framework. Causal-Copilot automates the full pipeline of causal analysis for both tabular and time-series data -- including causal discovery, causal inference, algorithm selection, hyperparameter optimization, result interpretation, and generation of actionable insights. It supports interactive refinement through natural language, lowering the barrier for non-specialists while preserving methodological rigor. By integrating over 20 state-of-the-art causal analysis techniques, our system fosters a virtuous cycle -- expanding access to advanced causal methods for domain experts while generating rich, real-world applications that inform and advance causal theory. Empirical evaluations demonstrate that Causal-Copilot achieves superior performance compared to existing baselines, offering a reliable, scalable, and extensible solution that bridges the gap between theoretical sophistication and real-world applicability in causal analysis. A live interactive demo of Causal-Copilot is available at https://causalcopilot.com/.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 14:05:39 GMT</pubDate>
</item>
<item>
<title>构建顶级数学推理模型的方法与实践</title>
<link>https://arxiv.org/abs/2504.16891</link>
<guid>https://arxiv.org/abs/2504.16891</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于大规模数据集和创新方法的数学推理模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了我们在AI数学奥林匹克竞赛(AIMO-2)中的获奖方案。我们的方法围绕三个关键支柱展开：首先，创建了一个包含54万独特高质量数学问题及其320万个长推理解决方案的大规模数据集；其次，开发了一种通过迭代训练、生成和质量过滤将代码执行与长推理模型集成的新方法，产生了170万个高质量工具集成推理解决方案；最后，构建了一个从多个候选者中挑选最有可能解决方案的流水线。实验表明，这种生成性解选(GenSelect)显著优于多数投票基线。结合这些技术，我们训练了一系列在数学推理基准测试中达到最先进的模型。为了促进进一步研究，我们以商业许可方式发布了代码、模型以及完整的OpenMathReasoning数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16891" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:13:04 GMT</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:53:29 GMT</pubDate>
</item>
<item>
<title>基于渐进语言引导视觉学习的多任务视觉定位框架</title>
<link>https://arxiv.org/abs/2504.16145</link>
<guid>https://arxiv.org/abs/2504.16145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需额外跨模态模块的多任务视觉定位框架PLVL。</p><br /><br /><p><strong>摘要：</strong> 本文针对多任务视觉定位(MTVG)中的子任务(指表达理解REC和分割RES)提出了渐进语言引导视觉学习框架PLVL。传统方法通过独立特征提取、跨模态交互及独立预测头实现任务处理，但存在语言信息注入不足及任务间关系未有效利用的问题。PLVL不仅深入挖掘视觉模态自身特性，还逐步引入语言信息以增强语言相关视觉特征的学习能力，避免了额外跨模态融合模块的需求。此外，通过分析REC定位中心对RES分割对象区域的潜在帮助，设计了多任务头以实现协作预测。实验表明，PLVL在多个基准数据集上显著优于现有代表性方法。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:48:12 GMT</pubDate>
</item>
<item>
<title>Tina：基于LoRA高效实现语言模型强推理能力的研究</title>
<link>https://arxiv.org/abs/2504.15777</link>
<guid>https://arxiv.org/abs/2504.15777</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出Tina模型家族，通过LoRA技术显著降低资源消耗实现高效推理。</p><br /><br /><p><strong>摘要：</strong> 本文探讨如何以低成本实现语言模型的强推理能力，提出了Tina这一高效推理模型家族。Tina通过对已有1.5B参数的小型基础模型应用低秩适应（LoRA）技术，在强化学习过程中进行参数高效更新，从而在极小资源投入下展现出卓越的推理性能。实验表明，最佳的Tina模型在AIME24数据集上实现了超过20%的推理性能提升及43.33%的Pass@1准确率，且仅需9美元的后训练与评估成本，较现有顶级模型减少了约260倍的成本。此外，Tina在多个开源推理数据集上的表现验证了其有效性，同时通过消融实验进一步支持了LoRA方法的优势。研究表明，LoRA能够快速使模型适应强化学习奖励的推理结构，同时保留基础模型的知识。为促进开放研究，所有代码、训练日志及模型权重均已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15777" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:38:00 GMT</pubDate>
</item>
<item>
<title>RePOPE: Impact of Annotation Errors on the POPE Benchmark</title>
<link>https://arxiv.org/abs/2504.15707</link>
<guid>https://arxiv.org/abs/2504.15707</guid>
<content:encoded><![CDATA[
Since data annotation is costly, benchmark datasets often incorporate labels from established image datasets. In this work, we assess the impact of label errors in MSCOCO on the frequently used object hallucination benchmark POPE. We re-annotate the benchmark images and identify an imbalance in annotation errors across different subsets. Evaluating multiple models on the revised labels, which we denote as RePOPE, we observe notable shifts in model rankings, highlighting the impact of label quality. Code and data are available at https://github.com/YanNeu/RePOPE .
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:47:59 GMT</pubDate>
</item>
<item>
<title>大型语言模型全栈安全综述</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">首次提出大型语言模型全生命周期安全概念。</p><br /><br /><p><strong>摘要：</strong> 随着大型语言模型（LLMs）在学术界和工业界的广泛应用，其安全性问题逐渐成为关注焦点。然而，现有研究多聚焦于特定阶段的安全性，缺乏对整个生命周期的全面分析。本文首次引入“全栈”安全的概念，系统探讨LLMs从训练到商业化过程中的安全挑战。通过综合分析超过800篇文献，我们定义了完整的LLMs生命周期，涵盖数据准备、预训练、后训练、部署及商业化的全过程，并提出了包括数据生成、对齐技术、模型编辑及基于LLM的代理系统在内的多个研究方向，为未来研究提供了宝贵的指导。这一工作不仅填补了现有研究的空白，还为构建更加安全可靠的LLMs提供了理论支持。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15585" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 01:02:49 GMT</pubDate>
</item>
<item>
<title>CRUST-Bench：评估C转Rust编译的基准数据集</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CRUST-Bench数据集，用于评估C代码转安全Rust的能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CRUST-Bench，这是一个包含100个C代码仓库及其对应安全Rust接口和测试用例的数据集，旨在评估系统将C代码转换为符合规范且通过测试的安全Rust代码的能力。与仅关注孤立函数的传统方法不同，CRUST-Bench考虑整个项目及其多文件依赖关系，提供明确的Rust接口规格以保证内存安全性，并通过测试用例验证功能正确性。尽管现有最先进的大型语言模型（LLMs）在该任务上表现有限，但研究仍揭示了模型常见的错误类型。CRUST-Bench的成功应用有望推动复杂场景下C到Rust等内存安全语言迁移技术的发展。数据集及代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15254" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:33:33 GMT</pubDate>
</item>
<item>
<title>DreamID：基于扩散模型的高效高保真人脸交换技术</title>
<link>https://arxiv.org/abs/2504.14509</link>
<guid>https://arxiv.org/abs/2504.14509</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的高保真人脸交换方法DreamID。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DreamID的基于扩散模型的人脸交换方法，该方法通过构建三元组身份组（Triplet ID Group）数据实现了对身份相似性和属性保留的显式监督，解决了传统隐式监督方法的局限性。此外，DreamID采用加速扩散模型SD Turbo实现单次迭代推理，显著提高了训练效率。研究还设计了一个由SwapNet、FaceNet和ID Adapter组成的改进架构，进一步增强了显式监督的效果。实验表明，DreamID在身份相似性、姿态表情保留和图像保真度方面超越了现有最先进方法，并且能在0.6秒内完成512*512分辨率的高质量人脸交换，尤其在复杂光照、大角度和遮挡等挑战场景下表现出色。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14509" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 02:53:00 GMT</pubDate>
</item>
<item>
<title>基于LLM自适应难度的高质量链式思维数据生成方法</title>
<link>https://arxiv.org/abs/2504.11919</link>
<guid>https://arxiv.org/abs/2504.11919</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种高效生成高质量链式思维数据的方法，显著提升模型微调效率。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种通过构建LLM自适应难度问题数据库并利用DeepSeek-R1生成高质量链式思维(CoT)数据的方法。该方法降低了数据生成成本，提升了模型微调效率。实验验证了该方法在数学竞赛和代码生成任务中的有效性，其中ZMath-32B和ZCode-32B分别在仅使用2k高质量数据的情况下超越了DeepSeek-Distill-32B。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11919" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 05:55:34 GMT</pubDate>
</item>
<item>
<title>CheckboxQA：评估视觉语言模型处理复选框能力的新基准</title>
<link>https://arxiv.org/abs/2504.10419</link>
<guid>https://arxiv.org/abs/2504.10419</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">CheckboxQA 数据集用于提升视觉语言模型对复选框的理解能力。</p><br /><br /><p><strong>摘要：</strong> 在文档处理中，复选框的存在与否直接影响到数据提取和决策制定，然而现有大型视觉语言模型在这方面的表现并不理想。这种不足在法律和技术金融等对细节要求极高的行业中尤为突出。为解决这一问题，我们发布了 CheckboxQA 数据集，该数据集专门设计用于评估并改进模型在复选框相关任务上的性能。通过揭示当前模型的局限性，CheckboxQA 为推动文档理解系统的进步提供了宝贵的资源，在法律科技和金融等领域具有重要意义。数据集已公开发布于 GitHub 平台。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10419" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:06:59 GMT</pubDate>
</item>
<item>
<title>统一信息论框架下的现代机器学习损失函数</title>
<link>https://arxiv.org/abs/2504.16929</link>
<guid>https://arxiv.org/abs/2504.16929</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种信息论方程，统一多种现代机器学习损失函数。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于信息论的一般化方程，该方程能够涵盖大量现代机器学习中的损失函数。通过这一框架，我们揭示了几种类别的机器学习方法本质上是在最小化两个条件分布之间的集成KL散度。这种视角展示了聚类、谱方法、降维、对比学习及有监督学习背后隐藏的信息几何结构。此外，该框架促进了新损失函数的开发，并通过理论成果改进了无监督图像分类器，在ImageNet-1K上的表现提升了8%以上，同时提出了有效的去偏方法以优化对比表示学习器。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16929" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:59:01 GMT</pubDate>
</item>
<item>
<title>DreamO：一种支持多类型条件集成的图像定制框架</title>
<link>https://arxiv.org/abs/2504.16915</link>
<guid>https://arxiv.org/abs/2504.16915</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出DreamO框架，实现多种图像定制任务并灵活整合不同类型条件。</p><br /><br /><p><strong>摘要：</strong> 近年来，关于图像定制的研究展示了大规模生成模型的强大定制能力。然而，大多数方法针对特定任务设计，限制了其对多种条件组合的泛化能力。本文介绍DreamO，这是一种统一的图像定制框架，可支持广泛的任务并无缝集成多种条件。DreamO利用扩散Transformer（DiT）框架处理不同类型输入，并通过构建大规模训练集及引入特征路由约束来精确查询参考图像中的相关信息。此外，设计了占位符策略以控制生成结果中条件的位置。采用渐进式训练策略，分三个阶段提升定制能力并校正质量偏差。实验表明，DreamO在高质量完成定制任务的同时，具有很强的灵活性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16915" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 13:41:44 GMT</pubDate>
</item>
<item>
<title>Decoupled Global-Local Alignment框架提升视觉语言对齐模型的复合概念理解</title>
<link>https://arxiv.org/abs/2504.16801</link>
<guid>https://arxiv.org/abs/2504.16801</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Decoupled Global-Local Alignment框架，提升CLIP的复合概念理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有对比学习方法在理解视觉语言复合概念时的局限性，提出了一种名为Decoupled Global-Local Alignment（DeGLA）的新框架。DeGLA通过引入局部对齐机制和自蒸馏策略，在增强模型对关系和属性等复合概念理解的同时，有效缓解了全局对比学习导致的泛化能力下降问题。实验结果显示，DeGLA在多个基准测试中表现优异，相较于现有最佳方法平均提升了3.5%，并在零样本分类任务中取得了13.0%的性能提升。此外，该框架还通过利用大语言模型生成高质量负样本及设计新的对比损失函数，进一步增强了模型的视觉语言对齐能力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16801" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 11:20:53 GMT</pubDate>
</item>
<item>
<title>Pre-DPO：通过引导参考模型优化偏好强化学习</title>
<link>https://arxiv.org/abs/2504.15843</link>
<guid>https://arxiv.org/abs/2504.15843</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于DPO的Pre-DPO训练范式，通过参考模型优化大型语言模型的偏好强化学习。</p><br /><br /><p><strong>摘要：</strong> 直接偏好优化（DPO）简化了大型语言模型（LLMs）从人类反馈中的强化学习过程，但初始策略与参考模型相同的做法可能限制性能。本文提出Pre-DPO，通过引入指导参考模型增强偏好优化，该模型根据样本适配性动态分配权重，提高数据利用率和训练鲁棒性。实验表明，Pre-DPO在AlpacaEval 2.0和Arena-Hard v0.1基准测试中显著提升DPO和简单偏好优化（SimPO）的表现，且无需外部模型或额外数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15843" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 08:39:30 GMT</pubDate>
</item>
<item>
<title>Trillion-7B：高效韩文多语言大模型</title>
<link>https://arxiv.org/abs/2504.15431</link>
<guid>https://arxiv.org/abs/2504.15431</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Trillion-7B通过创新机制实现高效的跨语言知识迁移。</p><br /><br /><p><strong>摘要：</strong> Trillion-7B是一款专注于韩语的多语言大型语言模型，具有极高的token效率。该模型引入了交叉语言文档注意（XLDA）机制，能够有效且高效地将英语知识转移到目标语言如韩语和日语上。通过优化的数据混合、语言特定过滤及定制化分词器构建，Trillion-7B仅用其2万亿训练tokens中的10%进行多语言数据训练，并且仅需59.4K H100 GPU小时完成全部训练。全面评估显示，该模型在四种语言的27个基准测试中展现了强大的多语言性能和卓越的跨语言一致性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15431" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 16:54:44 GMT</pubDate>
</item>
<item>
<title>VisuLogic：评估多模态大型语言模型视觉推理能力的新基准</title>
<link>https://arxiv.org/abs/2504.15279</link>
<guid>https://arxiv.org/abs/2504.15279</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">VisuLogic通过六类视觉推理问题评估多模态大模型的视觉推理能力。</p><br /><br /><p><strong>摘要：</strong> 现有的多模态大型语言模型（MLLMs）评估往往依赖文本描述并允许语言推理捷径，无法有效衡量真正的视觉中心推理能力。为了解决这一问题，本文提出了VisuLogic，这是一个包含1000个人类验证问题的基准，涵盖了六个类别，如定量变化、空间关系和属性比较。通过在该基准上的评估，发现大多数模型的准确率低于30%，远低于人类的51.4%和随机基线的25%，揭示了这些模型在视觉推理方面存在显著差距。此外，研究还提供了补充训练数据集和强化学习基线以促进进一步发展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15279" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>视觉语言模型在处理遮挡模式计数中的挑战与局限性</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入新任务CAPTURe测试视觉语言模型对遮挡物体的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Counting Amodally for Patterns Through Unseen REgions (CAPTURe)的新任务，旨在评估视觉语言模型(VLMs)在面对遮挡物体时的推理和空间理解能力。该任务要求模型通过推断被遮挡区域后的模式来完成计数。CAPTURe分为两部分：CAPTURe-real使用真实物体图像，CAPTURe-synthetic则使用生成的图像。实验评估了四个强大的VLMs(GPT-4o、Intern-VL2、Molmo和Qwen2-VL)，发现这些模型在处理遮挡和非遮挡模式时均表现不佳，尤其是在遮挡情况下表现更差。人类的表现远优于模型，提供遮挡物体位置的辅助信息可提高性能，表明模型在处理遮挡和计数方面都存在困难。这项研究揭示了当前VLMs在理解遮挡模式和空间关系上的不足。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15485" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 19:38:43 GMT</pubDate>
</item>
<item>
<title>基于自回归模型的个性化图像合成研究</title>
<link>https://arxiv.org/abs/2504.13162</link>
<guid>https://arxiv.org/abs/2504.13162</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示自回归模型在文本转图像生成中的潜力。</p><br /><br /><p><strong>摘要：</strong> 个性化图像合成作为文本到图像生成的重要应用，近年来受到广泛关注。尽管扩散模型在此领域占据主导地位，但自回归模型因其统一的文本和图像建模架构，在个性化图像生成方面尚未得到充分探索。本文提出了一种两阶段训练策略，通过优化文本嵌入和微调Transformer层，使自回归模型在个性化图像生成任务上达到了与领先扩散模型相当的效果。实验表明，这种策略在主体保真度和指令遵循能力方面表现优异，为未来的研究提供了新的方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13162" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:26 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化大型语言模型的决策能力</title>
<link>https://arxiv.org/abs/2504.16078</link>
<guid>https://arxiv.org/abs/2504.16078</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究显示通过强化学习微调可显著提升大型语言模型的决策能力和探索效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）在决策场景中的次优表现问题，分析了贪婪性、频率偏差及知识应用差距等三种常见失败模式。研究表明，利用自我生成的链式思维推理进行基于强化学习的微调可以有效缓解这些问题，从而增强模型的探索能力和行动效果。实验分别在多臂老虎机、上下文老虎机和井字棋等任务中验证了该方法的有效性。此外，文章还研究了经典探索机制如ε-贪心策略以及大型语言模型特有的自我校正和一致性方法，以进一步优化模型的决策性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16078" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:57:14 GMT</pubDate>
</item>
<item>
<title>IPBench：推动大语言模型在知识产权领域的应用评估</title>
<link>https://arxiv.org/abs/2504.15524</link>
<guid>https://arxiv.org/abs/2504.15524</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个涵盖知识产权多机制与任务的综合基准IPBench。</p><br /><br /><p><strong>摘要：</strong> 知识产权领域因其技术与法律知识的高度融合而复杂且知识密集，现有处理知识产权任务的大语言模型虽潜力巨大，但相关数据集与基准存在局限性，未能完全契合实际场景需求。为解决这一问题，本文引入了首个全面的知识产权任务分类体系及名为IPBench的大型双语基准，该基准覆盖8种知识产权机制及20项任务，旨在真实反映知识产权应用场景下的理解与生成能力。通过评估16种不同类型的大型语言模型，我们发现即便是表现最佳的模型，在IPBench上的准确率也仅为75.8%，表明仍有较大提升空间。此外，开源知识产权与法律导向模型的表现明显落后于闭源通用模型。为了促进进一步研究，我们公开了IPBench的所有数据与代码，并计划持续更新以更好地模拟知识产权领域的实际挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15524" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 22:00:41 GMT</pubDate>
</item>
<item>
<title>高效整合新语言到大型语言模型的方法研究</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可将新语言融入现有大模型的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种创新方法，用于将新的目标语言（阿拉伯语）无缝集成到现有的大型语言模型（LLM）中，而不会影响其原有的知识。通过在一个主要基于英语的小型开源模型中注入阿拉伯语数据，开发出参数量为15亿的Kuwain模型，该模型在多种基准测试中的阿拉伯语性能平均提升了8%，同时仅需少量原始模型的数据即可保留其原有知识。这种方法不仅显著降低了多语言模型开发的成本，还展示了高效扩展语言模型的潜力，避免了资源密集型的全面重新训练过程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15120" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:17:25 GMT</pubDate>
</item>
<item>
<title>DiffVox：一种可微分的音乐人声效果匹配模型</title>
<link>https://arxiv.org/abs/2504.14735</link>
<guid>https://arxiv.org/abs/2504.14735</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型可解释的人声效果匹配模型DiffVox。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为DiffVox（“可微分人声效果”）的新模型，用于音乐制作中的声效匹配。该模型结合了参数均衡、动态范围控制、延迟和混响等模块，并通过高效的不同iable实现支持基于梯度的参数优化。研究利用MedleyDB和私人收藏的数据集中的435首歌曲进行分析，揭示了声效参数间的强相关性及与McAdams音色维度的联系。统计测试表明参数分布非高斯特性，突显了人声效果空间的复杂性。这些初步发现为未来人声效果建模和自动混音奠定了基础。相关代码和数据集可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14735" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 16:52:58 GMT</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>MR. Video：基于MapReduce原理的长视频理解框架</title>
<link>https://arxiv.org/abs/2504.16082</link>
<guid>https://arxiv.org/abs/2504.16082</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于MapReduce原则的长视频理解框架MR. Video。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为MR. Video的长视频理解框架，它通过Map（独立密集感知短视频片段）和Reduce（联合聚合所有片段信息）两个步骤，有效解决了传统序列到序列视觉语言模型在处理长视频时上下文长度受限的问题。与依赖关键片段选择的现有视频代理相比，MR. Video的Map操作实现了更简单的并行感知，而Reduce则支持更全面的上下文聚合和推理。该框架在LVBench挑战数据集上比最先进的视觉语言模型和视频代理提高了超过10%的准确性。此外，MR. Video还展示了其在短视频字幕生成和问题分析中的应用效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16082" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>ReflectionFlow：基于推理时自省的文本到图像扩散模型优化框架</title>
<link>https://arxiv.org/abs/2504.16080</link>
<guid>https://arxiv.org/abs/2504.16080</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ReflectionFlow，通过推理时自省提升文本到图像生成质量。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReflectionFlow的新框架，该框架旨在解决现有文本到图像扩散模型在处理复杂场景和精细细节时表现不佳的问题。受大型语言模型中涌现的自我反思能力启发，ReflectionFlow在推理阶段引入三个互补的扩展轴：噪声级扩展、提示级扩展以及创新性的反射级扩展，后者通过提供可操作的反馈来评估并修正之前的生成结果。为了支持反射级扩展，我们构建了一个包含一百万个三元组的大规模数据集GenRef，每个三元组包括一条反馈、一张有缺陷的图像和一张增强后的图像。利用此数据集，我们在最先进的扩散变换器FLUX.1-dev上高效执行了反射调优。实验结果表明，ReflectionFlow显著优于朴素的噪声级扩展方法，在具有挑战性的任务中提供了可扩展且计算高效的高质量图像合成解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16080" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:58:07 GMT</pubDate>
</item>
<item>
<title>Describe Anything Model：实现图像和视频的精细化局部描述</title>
<link>https://arxiv.org/abs/2504.16072</link>
<guid>https://arxiv.org/abs/2504.16072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种用于图像和视频精细化局部描述的新模型DAM。</p><br /><br /><p><strong>摘要：</strong> 描述特定区域内的图像和视频细节一直是视觉语言模型面临的基本挑战。本文介绍了一种名为DAM（Describe Anything Model）的模型，专门用于精细化局部描述（DLC）。DAM通过两个关键创新保持局部细节和全局上下文：焦点提示确保目标区域的高分辨率编码，局部视觉主干将精确的定位与其更广泛的上下文相结合。为了解决高质量DLC数据稀缺的问题，我们提出了基于半监督学习的数据管道（DLC-SDP），该管道从现有的分割数据集开始，并扩展到未标记的网络图像。此外，我们还引入了DLC-Bench基准，用于评估DLC而不依赖参考标题。DAM在七个涵盖关键词级、短语级和详细的多句局部图像和视频描述基准上创造了新的最先进水平。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.16072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 13:51:41 GMT</pubDate>
</item>
<item>
<title>LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale</title>
<link>https://arxiv.org/abs/2504.16030</link>
<guid>https://arxiv.org/abs/2504.16030</guid>
<content:encoded><![CDATA[
Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary model APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data production pipeline to process YouTube videos and their closed captions (CC, same as ASR), resulting in Live-CC-5M dataset for pre-training and Live-WhisperX-526K dataset for high-quality supervised fine-tuning (SFT). Remarkably, even without SFT, the ASR-only pre-trained LiveCC-7B-Base model demonstrates competitive general video QA performance and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new LiveSports-3K benchmark, using LLM-as-a-judge to measure the free-form commentary. Experiments show our final LiveCC-7B-Instruct model can surpass advanced 72B models (Qwen2.5-VL-72B-Instruct, LLaVA-Video-72B) in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B/8B scale on popular video QA benchmarks such as VideoMME and OVOBench, demonstrating the broad generalizability of our approach. All resources of this paper have been released at https://showlab.github.io/livecc.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 12:52:09 GMT</pubDate>
</item>
<item>
<title>通过神经符号世界模型提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.15785</link>
<guid>https://arxiv.org/abs/2504.15785</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出无需训练的世界对齐方法提升大型语言模型作为世界模型的准确性。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大型语言模型（LLMs）能否构建出准确的世界模型，并分析了世界模型如何增强LLM代理的表现。主要瓶颈在于LLMs的先验知识与特定环境动态之间的差距。为此，我们提出了无需训练的“世界对齐”方法，通过从探索轨迹中提取并编码动作规则、知识图谱及场景图等符号知识，来补充LLMs的不足。进一步地，我们设计了一个无需强化学习的基于模型的代理WALL-E 2.0，利用模型预测控制框架，采用LLM代理作为高效前瞻优化器，显著提升了新环境中的学习效率。在Mars（类似Minecraft）和ALFWorld（具身室内环境）的开放世界挑战中，WALL-E 2.0在成功率和得分上均大幅超越现有方法，特别是在Mars环境中成功率提升16.1%-51.6%，而在ALFWorld中仅需四轮迭代就达到了98%的成功率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15785" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 06:58:27 GMT</pubDate>
</item>
<item>
<title>Vidi：面向视频编辑的大规模多模态模型</title>
<link>https://arxiv.org/abs/2504.15681</link>
<guid>https://arxiv.org/abs/2504.15681</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Vidi模型在长时间视频的时间检索任务上显著优于GPT-4o和Gemini。</p><br /><br /><p><strong>摘要：</strong> 随着互联网上视频成为主要的沟通和表达方式，高质量大规模视频内容的制作需求日益增长。然而，传统模型在处理多模态数据（如视觉、音频、文本）以及灵活输入长度时面临挑战。本文介绍了一种名为Vidi的大规模多模态模型家族，专门针对视频理解与编辑场景。Vidi的第一个版本专注于时间检索任务，即根据文本查询识别输入视频中的时间范围。该模型能够在长达数小时的视频中表现出强大的时间理解能力。为了支持真实场景的全面评估，研究团队还推出了VUE-TR基准，具有视频时长更长、支持音频查询、多样化查询格式、高质量标注及改进的IoU评价指标等五大优势。实验结果显示，Vidi在时间检索任务上显著超越了领先的专有模型如GPT-4o和Gemini，展示了其在视频编辑领域的优越性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15681" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 04:04:45 GMT</pubDate>
</item>
<item>
<title>多语言基准评估的现状与未来：挑战与改进建议</title>
<link>https://arxiv.org/abs/2504.15521</link>
<guid>https://arxiv.org/abs/2504.15521</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现英语在多语言基准中过度代表，提出创建本地化基准的重要性。</p><br /><br /><p><strong>摘要：</strong> 本文基于对2021年至2024年间来自148个国家超过2000个多语言（非英语）基准的分析，探讨了当前多语言基准评估的实践及其不足之处。尽管投入了大量资金，但英语仍占据显著优势，且多数基准源自高资源国家。此外，STEM相关任务与人类评价高度相关，而传统NLP任务的相关性较低。翻译基准不足以反映实际应用需求，本地化基准表现更优。研究指出了当前评估中的六大局限，并提出了指导原则及五大研究方向。最后，呼吁全球合作开发更符合人类需求的应用导向型基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15521" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 21:47:37 GMT</pubDate>
</item>
<item>
<title>自适应并行推理框架提升语言模型推理能力</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架APR，解决现有推理方法的效率与性能瓶颈。</p><br /><br /><p><strong>摘要：</strong> 现有语言模型推理方法存在显著局限性：序列化推理生成过长输出导致延迟增加及上下文窗口耗尽；而并行推理如自一致性则因协调不足产生冗余计算和有限性能提升。为此，本文提出自适应并行推理（APR），一种端到端推理框架，可整合序列化与并行计算。APR通过spawn()和join()操作实现多线程推理，创新性采用端到端强化学习策略优化推理过程。实验表明，在Countdown推理任务中，APR相比传统方法在相同上下文窗口内表现更优（83.4% vs. 60.0%），扩展性更强且在同等延迟下精度更高（75.2% vs. 57.3%）。APR标志着语言模型自主优化推理过程的新方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15466" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 18:29:02 GMT</pubDate>
</item>
<item>
<title>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.15415</link>
<guid>https://arxiv.org/abs/2504.15415</guid>
<content:encoded><![CDATA[
Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 15:53:44 GMT</pubDate>
</item>
<item>
<title>基于平行隐藏解码的高效长度缩放预训练框架</title>
<link>https://arxiv.org/abs/2504.14992</link>
<guid>https://arxiv.org/abs/2504.14992</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新型框架PHD-Transformer，实现预训练中的高效长度缩放。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型的研究表明，在后训练阶段通过长度缩放可以显著提升性能，但这一方法在预训练阶段的应用尚待深入探索。本文介绍了一种名为Parallel Hidden Decoding Transformer（PHD-Transformer）的新框架，该框架能够在预训练期间实现高效的长度缩放，同时保持推理效率。通过引入创新的KV缓存管理策略，PHD-Transformer能够区分原始令牌和隐藏解码令牌，仅保留原始令牌的KV缓存以维持长距离依赖关系，而立即丢弃已使用的隐藏解码令牌，从而在不增加KV缓存大小的情况下实现有效扩展。此外，还提出了两种优化变体：PHD-SWA利用滑动窗口注意力机制保存局部依赖关系；PHD-CSWA则采用分块滑动窗口注意力机制消除预填充时间的线性增长。实验结果表明，该方法在多个基准测试中均表现出一致的性能提升。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14992" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:41:26 GMT</pubDate>
</item>
<item>
<title>基于DiT模型的可控角色动画生成方法</title>
<link>https://arxiv.org/abs/2504.14977</link>
<guid>https://arxiv.org/abs/2504.14977</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出通过基础模型微调解决罕见姿态等挑战的新视角。</p><br /><br /><p><strong>摘要：</strong> 可控角色动画在处理罕见姿态、风格化角色及复杂场景时面临诸多难题，传统方法主要依赖复杂的旁路网络注入姿态与外观指导，但泛化能力有限。本文提出新方法，认为只要基础模型足够强大，通过简单修改并采用灵活微调策略即可有效应对上述挑战。文中介绍的RealisDance-DiT基于Wan-2.1视频基础模型构建，实验表明其性能显著优于现有方法。此外，我们还设计了新的测试数据集，涵盖多样化的现实世界挑战，补充了现有基准数据集，进一步验证了该方法的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14977" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 05:09:21 GMT</pubDate>
</item>
<item>
<title>BookWorld：基于书籍的多智能体社会构建与模拟系统</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出BookWorld系统，用于构建和模拟基于书籍的多智能体社会。</p><br /><br /><p><strong>摘要：</strong> 近期大型语言模型(LLMs)的进步推动了通过多智能体系统进行社会模拟的发展，但对已建立的虚构世界和角色的模拟研究较少。本文介绍BookWorld系统，该系统能够全面构建和模拟基于书籍的多智能体社会，涵盖复杂现实细节如多样化动态角色、世界观、地理约束等。实验表明，BookWorld生成的故事具有创意且质量高，忠实于原作，胜过前人方法。BookWorld还可应用于故事生成、互动游戏和社会模拟等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14538" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 04:56:27 GMT</pubDate>
</item>
<item>
<title>CheXWorld：基于自监督学习的胸部X光影像世界模型</title>
<link>https://arxiv.org/abs/2504.13820</link>
<guid>https://arxiv.org/abs/2504.13820</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出首个针对放射影像的自监督世界模型CheXWorld。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了CheXWorld，这是首个用于放射影像的自监督世界模型，旨在捕捉医学知识的三个关键方面：局部解剖结构、全局解剖布局及领域变化。通过统一框架同时建模这三个方面，实验表明CheXWorld不仅成功捕捉了这些医学知识维度，还在八个医学图像分类和分割基准上显著优于现有自监督学习方法和大规模医学基础模型。代码和预训练模型已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13820" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:50:43 GMT</pubDate>
</item>
<item>
<title>Progent：一种用于大型语言模型代理的权限控制机制</title>
<link>https://arxiv.org/abs/2504.11703</link>
<guid>https://arxiv.org/abs/2504.11703</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">Progent通过灵活的权限策略提高大型语言模型代理的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Progent的新方法，这是首个面向大型语言模型（LLM）代理的权限控制机制。Progent的核心是一种领域特定语言，用于在代理执行期间灵活表达权限控制策略。这些策略对工具调用施加细粒度限制，决定哪些工具调用是允许的，并指定回退方案。这种设计使得代理开发者和用户可以根据具体用例创建并确定性地应用适当的策略，从而保证安全性。由于其模块化设计，集成Progent不会改变代理内部结构，只需对代理实现进行最小改动，这提升了其实用性和广泛采用的可能性。此外，我们利用LLM自动生成基于用户查询的策略，并动态更新以增强安全性和实用性。我们的广泛评估表明，Progent能够在三个不同的场景或基准测试中（AgentDojo、ASB和AgentPoison）提供强大的安全保障，同时保持高实用性。此外，我们还进行了深入分析，展示了其核心组件的有效性以及自动化策略生成在对抗适应性攻击时的弹性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11703" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 21:58:40 GMT</pubDate>
</item>
<item>
<title>通过强化学习优化工具集成推理中的工具使用效率</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架，通过奖励机制减少工具调用次数并提高工具生产力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过强化学习提升工具集成推理（TIR）的能力，尤其是在优化最终答案正确性的同时考虑工具使用的效率与成本。现有方法往往忽视了工具调用的数量及其带来的计算和财务负担，可能导致不理想的模型行为。为解决这一问题，我们提出了Optimal Tool Call-controlled Policy Optimization (OTC-PO)，这是一种基于强化学习的新框架，旨在鼓励模型以最少的工具调用生成准确的答案。该框架引入了一个综合考虑正确性和工具效率的奖励机制，从而提升了工具的生产力。我们将此框架应用于Proximal Policy Optimization (PPO) 和 Group Relative Preference Optimization (GRPO)，分别得到OTC-PPO和OTC-GRPO。实验结果显示，在多个问答基准测试中，我们的方法将工具调用次数减少了高达73.1%，同时提高了工具生产力达229.4%，且保持了相近的答案准确性。据我们所知，这是首个明确优化TIR中工具使用效率的强化学习框架。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14870" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 01:40:05 GMT</pubDate>
</item>
<item>
<title>推理模型过思考问题的研究与优化</title>
<link>https://arxiv.org/abs/2504.13367</link>
<guid>https://arxiv.org/abs/2504.13367</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究推理模型过思考问题并提出THOUGHTTERMINATOR优化方法。</p><br /><br /><p><strong>摘要：</strong> 推理模型在处理传统语言模型难以应对的任务时表现出色，但普遍存在过思考问题，即生成大量不必要的标记，这不仅不提升准确性还浪费计算资源。本文引入问题难度的近似度量方法，揭示了问题难度与最优标记消耗之间的关系，并评估了几种推理模型在有效分配最优标记数量方面的校准程度。研究发现，大多数推理模型在校准方面表现不佳，尤其是在简单问题上。为此，我们构建了DUMB500数据集，包含极其简单的数学、推理、代码及任务问题，同时结合现有前沿基准中的极难问题，对推理模型进行综合评估。最后，我们提出了无需训练的黑盒解码技术THOUGHTTERMINATOR，显著改善了推理模型的校准性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13367" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 18:16:30 GMT</pubDate>
</item>
<item>
<title>RF-DETR与YOLOv12在复杂果园环境中的绿果检测对比研究</title>
<link>https://arxiv.org/abs/2504.13099</link>
<guid>https://arxiv.org/abs/2504.13099</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">RF-DETR在单类和多类绿果检测中均优于YOLOv12。</p><br /><br /><p><strong>摘要：</strong> 本研究对基于Transformer的RF-DETR和基于CNN的YOLOv12两种目标检测模型在复杂果园环境下的绿果检测性能进行了详细比较。通过构建包含单类和多类标注的自定义数据集，评估了模型在处理标签模糊、遮挡及背景融合等动态条件下的表现。RF-DETR凭借DINOv2骨干网络和可变形注意力机制，在全局上下文建模和局部特征提取方面表现出色，尤其在单类检测中取得了0.9464的mAP50高分；而YOLOv12则通过优化计算效率和边缘部署能力，在多类检测中达到了0.6622的mAP@50:95最佳分类效果。此外，RF-DETR展现了更快的收敛速度，尤其是在单类检测中仅需10个训练周期即可达到稳定状态。这些结果表明，RF-DETR更适合应用于精确农业场景，而YOLOv12则适用于需要快速响应的应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13099" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:08:11 GMT</pubDate>
</item>
<item>
<title>基于语音交互的医学视觉语言模型SilVar-Med及其可解释性研究</title>
<link>https://arxiv.org/abs/2504.10642</link>
<guid>https://arxiv.org/abs/2504.10642</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合语音交互的端到端医学视觉语言模型SilVar-Med。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SilVar-Med的端到端语音驱动的医学视觉语言模型（VLM），该模型通过整合语音交互功能，突破了传统文本指令限制，在临床环境中实现更实用的医疗图像分析辅助。此外，针对当前医学图像分析模型缺乏透明推理的问题，我们提出了一个用于异常预测解释的数据集，展示了基于推理的医学图像解释概念验证研究。实验表明，SilVar-Med在提升诊断支持系统的透明度、互动性和临床实用性方面具有重要意义。我们的代码和数据集已公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10642" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 14:51:37 GMT</pubDate>
</item>
<item>
<title>StyleMe3D：实现3D高斯点云风格迁移的综合性框架</title>
<link>https://arxiv.org/abs/2504.15281</link>
<guid>https://arxiv.org/abs/2504.15281</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出StyleMe3D框架，解决3D高斯点云在风格化场景中的问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为StyleMe3D的综合性框架，旨在解决3D Gaussian Splatting (3DGS) 在处理卡通、游戏等风格化场景时遇到的纹理碎片化、语义错位及适应抽象美学能力有限的问题。StyleMe3D通过多模态风格条件、多层级语义对齐和感知质量增强等技术，实现了几何结构完整性和风格一致性。该框架包含动态风格评分蒸馏(DSSD)、对比风格描述符(CSD)、同时优化尺度(SOS)以及可微分的3D高斯质量评估(3DG-QA)四个创新组件。实验表明，StyleMe3D在NeRF合成数据集和tandt db场景数据集上表现优异，不仅保留了几何细节，还确保了场景间的风格一致性，并且具备实时渲染能力，适用于游戏、虚拟世界和数字艺术等领域。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15281" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>Eagle 2.5：面向长上下文多模态学习的前沿视觉语言模型</title>
<link>https://arxiv.org/abs/2504.15271</link>
<guid>https://arxiv.org/abs/2504.15271</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的视觉语言模型Eagle 2.5，显著提升长视频和高分辨率图像的理解能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组名为Eagle 2.5的前沿视觉语言模型（VLMs），专门针对长上下文多模态学习进行优化。这些模型解决了长视频理解和高分辨率图像分析中的挑战，提供了一个通用框架来同时处理这两种任务。研究中采用了自动降级采样和图像区域保留两项技术，以确保上下文完整性和视觉细节的保留。此外，还对长上下文数据训练的整个流程进行了多项效率优化。为了促进长视频理解，我们提出了一个新的数据集Eagle-Video-110K，该数据集结合了故事级和片段级标注。实验结果显示，Eagle 2.5在长上下文多模态基准测试中表现出色，其中最佳模型Eagle 2.5-8B在Video-MME测试中达到了72.4%的准确率，这一成绩与顶级商业模型如GPT-4o以及大规模开源模型如Qwen2.5-VL-72B和InternVL2.5-78B相当。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15271" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>Quicksviewer：基于动态时间密度划分的高效多模态模型</title>
<link>https://arxiv.org/abs/2504.15270</link>
<guid>https://arxiv.org/abs/2504.15270</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的多模态模型Quicksviewer，通过自适应时间密度划分实现视频高效理解。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为Quicksviewer的大型多模态模型，该模型采用全新的感知范式，通过Gumbel Softmax将非均匀密度的视频分割成不同大小的立方体，并对每个立方体进行统一重采样，从而提高视频理解效率。这种方法能够在线动态压缩视频，显著减少时空冗余（总体压缩率可达45倍），同时支持大感受野下的高效训练。Quicksviewer通过三个渐进阶段进行训练，在平均420秒/1帧的长时间视频上表现出色。仅使用0.8M视频-文本样本进行训练，该模型在准确性上比采用固定划分策略的基线高出最多8.72分，同时在Video-MME基准测试中达到SOTA性能，且所需令牌数量仅为基线的5%。此外，实验验证了立方网络生成的片段有助于分析视频中的连续事件。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15270" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:57:21 GMT</pubDate>
</item>
<item>
<title>基于推理的查询级元代理FlowReasoner自动化设计</title>
<link>https://arxiv.org/abs/2504.15257</link>
<guid>https://arxiv.org/abs/2504.15257</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于推理的元代理FlowReasoner，显著提升多智能体系统设计性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为FlowReasoner的查询级元代理，旨在自动化设计针对每个用户查询的多智能体系统。核心理念是通过外部执行反馈激励基于推理的元代理。首先通过DeepSeek R1提炼出生成多智能体系统的初步推理能力，然后借助强化学习进一步优化，设计了多用途奖励函数从性能、复杂性和效率角度引导训练。实验表明，FlowReasoner在工程和竞赛代码基准测试中优于其他模型，在三个基准上比o1-mini提升了10.52%的准确率。相关代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15257" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 13:35:42 GMT</pubDate>
</item>
<item>
<title>DRAGON框架：一种灵活的生成模型微调方法</title>
<link>https://arxiv.org/abs/2504.15217</link>
<guid>https://arxiv.org/abs/2504.15217</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DRAGON是一种比传统RLHF更灵活的生成模型微调框架。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了DRAGON（Distributional RewArds for Generative OptimizatioN），一种用于优化生成模型以实现特定目标的多功能框架。与传统的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）等方法相比，DRAGON更加灵活，能够处理个体示例或其分布的奖励函数评估，支持多种奖励类型。通过利用这一灵活性，研究者们设计了新颖的奖励函数，结合跨模态编码器如CLAP，并通过对比正负样本集最大化奖励。实验表明，DRAGON在多个目标奖励下平均胜率为81.45%，且无需依赖人类偏好标注即可达到较高的音乐质量。这项工作展示了如何通过设计和优化奖励函数来提升生成模型的人类感知质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15217" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 12:41:40 GMT</pubDate>
</item>
<item>
<title>EasyEdit2：支持大语言模型实时行为调控的新框架</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">EasyEdit2 是一种新框架，可轻松实现大语言模型的行为控制。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为 EasyEdit2 的新框架，该框架旨在为大型语言模型（LLM）的行为控制提供插件式调节功能。与前代相比，EasyEdit2 引入了一种专门设计的新架构，用于平滑的模型操控。它包含多个关键模块，如操控向量生成器和应用器，允许用户通过生成并应用操控向量来影响模型行为，而无需修改模型参数。框架支持多种测试时干预，包括安全性、情感、人格、推理模式、事实性和语言特性等。此外，EasyEdit2 用户界面友好，仅需单个示例即可引导和调整模型响应，使精确控制变得简单高效。实验表明，该框架在多种 LLM 上均表现良好。源代码已公开于 GitHub，并附有演示笔记本和介绍视频。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15133" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 10:33:55 GMT</pubDate>
</item>
<item>
<title>RainbowPlus：基于进化计算的大语言模型红队框架</title>
<link>https://arxiv.org/abs/2504.15047</link>
<guid>https://arxiv.org/abs/2504.15047</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RainbowPlus框架，显著提升大语言模型对抗性提示生成的效率与多样性。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）表现出强大的能力，但容易受到对抗性提示的影响，从而产生不安全或有偏见的输出。现有红队方法通常面临可扩展性差、资源消耗高或攻击策略多样性不足的问题。本文提出RainbowPlus，这是一种基于进化计算的新红队框架，通过适应性的质量-多样性（QD）搜索算法改进对抗性提示生成。该框架采用多元素存档存储多样化的高质量提示，并使用综合适应度函数同时评估多个提示，从而克服了先前QD方法中的单一提示存档和成对比较的限制。实验表明，RainbowPlus在六个基准数据集和四种开源LLMs上的攻击成功率（ASR）和多样性均优于其他QD方法。此外，在HarmBench数据集上，RainbowPlus对十二个LLMs的平均ASR达到81.1%，比最先进的AutoDAN-Turbo高出3.9%，且运行速度快9倍。RainbowPlus的开源实现为LLM安全性评估提供了可扩展工具，促进了相关领域的进一步研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.15047" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 08:04:57 GMT</pubDate>
</item>
<item>
<title>LUFFY框架：通过离线策略指导提升大规模推理模型的泛化能力</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入LUFFY框架，结合离线策略推理轨迹显著提升了数学基准测试中的性能。</p><br /><br /><p><strong>摘要：</strong> 近期研究表明，大规模推理模型可以通过简单的基于规则奖励的强化学习实现复杂的推理行为。然而，现有的零样本强化学习方法受限于仅能依赖自身输出进行学习，难以突破初始能力限制。为解决这一问题，我们提出了LUFFY框架，该框架通过整合离线推理轨迹扩展了零样本强化学习。LUFFY在训练过程中动态平衡模仿与探索，采用正则化的重要性采样进行策略塑造，避免了混合策略训练中的浅层模仿。实验表明，LUFFY在六个数学基准测试中平均提升了超过7.0分，在分布外任务中领先6.2分以上，且在泛化能力上显著优于监督微调方法。分析显示，LUFFY不仅有效模仿了现有知识，还实现了超出示范范围的探索，为大规模推理模型的可扩展训练提供了新路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14945" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 04:09:13 GMT</pubDate>
</item>
<item>
<title>Uni3C：统一3D增强框架实现视频生成中摄像机与人体运动的精确控制</title>
<link>https://arxiv.org/abs/2504.14899</link>
<guid>https://arxiv.org/abs/2504.14899</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一框架Uni3C，实现视频生成中摄像机与人体运动的同步精准控制。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有视频生成方法中摄像机与人体运动控制分离的问题，提出了Uni3C，这是一种基于3D增强的统一框架。Uni3C通过引入PCDController模块，利用未投影点云进行摄像机控制，该模块无需重新训练视频生成的基础模型，表现出强大的泛化能力。此外，Uni3C还设计了一种联合对齐的3D世界引导机制，在推理阶段整合场景点云和SMPL-X人物模型，实现摄像机与人体运动信号的统一控制。实验表明，Uni3C在摄像机可控性和人体动作质量方面均优于现有方法，并通过定制验证集进一步验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14899" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 03:10:41 GMT</pubDate>
</item>
<item>
<title>TAPIP3D：一种新颖的单目RGB和RGB-D视频中的长期3D点跟踪方法</title>
<link>https://arxiv.org/abs/2504.14717</link>
<guid>https://arxiv.org/abs/2504.14717</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出TAPIP3D方法，通过三维空间特征云实现长期3D点跟踪。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为TAPIP3D的新方法，用于在单目RGB和RGB-D视频中的长期3D点跟踪。该方法将视频表示为相机稳定的空间-时间特征云，并利用深度和相机运动信息将二维视频特征提升到三维世界空间中，从而有效消除相机运动影响。TAPIP3D在这一稳定表示中迭代优化多帧3D运动估计，实现了长时间的稳健跟踪。为了处理3D点分布的固有不规则性，我们提出了局部成对注意力机制，这种三维上下文策略有效地利用了空间关系，形成了精确的3D轨迹估计所需的有用特征邻域。实验结果显示，我们的方法显著优于现有的3D点跟踪方法，并且在有准确深度的情况下，甚至提高了2D跟踪精度。此外，该方法支持在相机坐标系和世界坐标系下的推理，并证明了补偿相机运动可以提高跟踪性能。相比之前的2D和3D跟踪器使用的传统2D方形相关邻域，TAPIP3D提供了更鲁棒和准确的结果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14717" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 15:09:43 GMT</pubDate>
</item>
<item>
<title>LeetCodeDataset：面向代码生成模型的高质量基准数据集</title>
<link>https://arxiv.org/abs/2504.14655</link>
<guid>https://arxiv.org/abs/2504.14655</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推出用于评估和训练代码生成模型的高质量基准数据集LeetCodeDataset。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为LeetCodeDataset的新数据集，该数据集旨在应对LLM研究中的两个关键挑战：缺乏注重推理的编程基准和自包含的训练测试平台。通过精心整理具有丰富元数据的LeetCode Python问题，并提供每题超过100个测试用例及时间分割（2024年7月前/后），该数据集实现了无污染评估和高效的监督微调（SFT）。实验表明，具备推理能力的模型显著优于非推理模型，而仅使用2.6K模型生成解决方案的SFT表现可媲美拥有110K样本的数据。数据集及其评估框架已在Hugging Face和Github上公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14655" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 11:28:16 GMT</pubDate>
</item>
<item>
<title>UFO2：面向Windows桌面的多智能体操作系统实现可靠自动化</title>
<link>https://arxiv.org/abs/2504.14603</link>
<guid>https://arxiv.org/abs/2504.14603</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">UFO2通过多智能体架构提升计算机使用代理的实用性。</p><br /><br /><p><strong>摘要：</strong> 近期基于多模态大语言模型的计算机使用代理（CUAs）为自然语言驱动的复杂桌面工作流自动化提供了新方向，但现有CUAs多为概念性原型，受制于浅层操作系统集成、脆弱的基于截图交互及干扰性的执行方式。本文提出UFO2，这是一种针对Windows桌面的多智能体操作系统，将CUAs转化为实用的系统级自动化工具。UFO2采用中心化的HostAgent进行任务分解与协调，并配备一系列应用专用AppAgent，具备原生API、领域特定知识及统一的图形用户界面-应用程序接口动作层。该架构不仅实现了稳健的任务执行，还保持了模块化与可扩展性。通过融合Windows UI自动化与基于视觉解析的混合控制检测管道，支持多样化的界面风格。此外，通过推测多动作规划优化运行效率，减少每步大语言模型开销。最后，嵌套式画中画界面允许代理与用户在隔离虚拟桌面中并发操作。实验评估显示，UFO2在超过20款真实Windows应用中显著提升了鲁棒性和执行准确性，证明了深度操作系统集成是实现可靠用户导向桌面自动化的可扩展路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14603" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 20 Apr 2025 09:04:43 GMT</pubDate>
</item>
<item>
<title>SphereDiff：一种无缝全景图像与视频生成方法</title>
<link>https://arxiv.org/abs/2504.14396</link>
<guid>https://arxiv.org/abs/2504.14396</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散模型的新方法，解决全景图像生成中的极点失真问题。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为SphereDiff的创新方法，用于通过最先进的扩散模型生成无缝的360度全景图像和视频。传统方法因等距矩形投影(ERP)引起的严重畸变面临挑战，而SphereDiff定义了一个球面潜在表示，确保所有视角下均匀分布，从而减轻ERP固有的畸变。该方法扩展了多扩散机制至球面潜在空间，并提出了球面潜在采样方法，使预训练扩散模型能够直接应用。此外，引入了畸变感知加权平均技术，进一步提升投影过程中的生成质量。实验表明，SphereDiff在生成高质量全景内容方面优于现有方法，适用于增强现实/虚拟现实(AR/VR)等沉浸式应用。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.14396" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 15:59:11 GMT</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 05:25:55 GMT</pubDate>
</item>
<item>
<title>通过人机演示提升移动GUI代理性能</title>
<link>https://arxiv.org/abs/2504.13805</link>
<guid>https://arxiv.org/abs/2504.13805</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出基于人演示的学习方法，显著提高移动GUI代理在多样化场景中的性能。</p><br /><br /><p><strong>摘要：</strong> 移动图形用户界面（GUI）代理在自动化任务方面展现出潜力，但面临现实世界多样场景中泛化能力不足的问题。传统方法通过大规模数据集进行预训练或微调难以应对移动应用及用户特定任务的多样性。本文提出通过人类演示增强移动GUI代理能力，专注于提升其在未见过场景中的表现而非追求更大数据集的普遍泛化。为此，我们引入LearnGUI，这是首个专门用于研究基于演示学习的移动GUI代理的综合数据集，包含2,252个离线任务和101个在线任务的人类高质量演示。同时，开发了LearnAct，这是一种复杂的多智能体框架，可自动从演示中提取知识以增强任务完成。该框架结合了三个专业化智能体：DemoParser用于知识提取，KnowSeeker用于相关知识检索，ActExecutor用于演示增强的任务执行。实验结果显示，在离线和在线评估中均取得了显著性能提升。例如，在离线评估中，单个演示使Gemini-1.5-Pro的准确性从19.3%提高到51.7%；在线评估中，框架将UI-TARS-7B-SFT的任务成功率从18.1%提高到32.8%。LearnAct框架和LearnGUI基准确立了基于演示学习作为更适应性、个性化且可部署的移动GUI代理的有前景方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13805" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:13:34 GMT</pubDate>
</item>
<item>
<title>强化学习中工具使用奖励设计的研究与实践</title>
<link>https://arxiv.org/abs/2504.13958</link>
<guid>https://arxiv.org/abs/2504.13958</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出强化学习中工具使用任务的奖励设计方案，显著提升大语言模型能力。</p><br /><br /><p><strong>摘要：</strong> 当前大型语言模型(LLMs)通过有监督微调(SFT)获取工具使用能力，但难以泛化到复杂场景。强化学习(RL)的最新进展显示了优秀的推理和泛化能力，但在工具使用奖励设计上面临挑战，如多种工具参数及反馈不足问题。本研究首次系统性探索强化学习框架下的工具选择和应用任务奖励策略，分析其类型、尺度、粒度及时序动态，提出面向工具使用的奖励设计方案并结合分组相对策略优化(GRPO)训练模型。实验表明该方法比基础模型提高17%，比SFT模型提高15%，强调了奖励设计对提升LLMs工具使用能力和泛化性能的重要性。所有代码已公开以促进后续研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13958" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 17:45:32 GMT</pubDate>
</item>
<item>
<title>基于单目摄像机的多人3D姿态检测与跟踪方法</title>
<link>https://arxiv.org/abs/2504.12186</link>
<guid>https://arxiv.org/abs/2504.12186</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种从单目视频流中检测与跟踪多人3D姿态的方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种利用单目摄像机流检测并追踪多人详细3D姿势的方法。系统能够在拥挤场景中处理复杂姿势和遮挡情况，维持时间上连贯的预测。该模型不仅能在每帧进行强检测，还通过学习的姿态更新机制实现跨帧追踪。与传统匹配检测不同，该方法直接从新输入图像更新姿态，支持在线追踪。通过使用大量伪标签标注的数据集训练，模型在3D姿态估计准确性方面达到领先水平，同时在多目标长时间追踪中表现更快更准确。代码和权重可在指定GitHub仓库获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12186" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:40:15 GMT</pubDate>
</item>
<item>
<title>NEMOTRON-CROSSTHINK：强化学习框架提升大语言模型跨领域推理能力</title>
<link>https://arxiv.org/abs/2504.13941</link>
<guid>https://arxiv.org/abs/2504.13941</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出NEMOTRON-CROSSTHINK框架，通过多领域数据增强强化学习训练，显著提高大语言模型在数学及非数学推理任务中的准确性与效率。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型语言模型（LLMs）在推理任务中表现出色，特别是在数学推理领域。然而，将其推广到其他复杂且多样化的推理任务时面临诸多挑战，例如有限的数据量、缺乏可验证的奖励结构以及任务需求的多样性。为解决这些问题，本文提出了NEMOTRON-CROSSTHINK框架，该框架通过整合来自科学、技术、工程、数学（STEM）、人文学科和社会科学等多个领域的合成和真实问题-答案对，增强了强化学习的训练过程。具体而言，NEMOTRON-CROSSTHINK通过采用多选题和开放性问题模板控制答案空间复杂度，筛选可验证的答案，并优化多源数据融合策略，从而实现对数学及非数学推理任务的高效泛化。实验结果显示，该方法不仅在数学推理任务（如MATH-500和AMC23）上取得了显著的性能提升（分别提升了30.1%和27.5%），还在非数学推理任务（如MMLU-PRO、GPQA-DIAMOND、AGIEVAL和SUPERGPQA）上实现了11.3%至15.1%的准确率增长。此外，NEMOTRON-CROSSTHINK还大幅提高了响应效率，在正确回答问题时所需令牌数量减少了28%，表明其推理更加集中和高效。总之，这一研究证明了在强化学习中引入多领域、多格式数据的重要性，为构建更精确、高效且通用的大语言模型提供了新途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13941" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 17:37:13 GMT</pubDate>
</item>
<item>
<title>X-Teaming：一种高效的多轮对话语言模型攻击框架</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出X-Teaming框架，显著提升多轮对话中语言模型的安全性。</p><br /><br /><p><strong>摘要：</strong> 本文针对多轮对话中语言模型存在的安全风险展开研究，现有工作主要集中在单轮安全性，而多轮对抗测试仍面临适应性和多样性挑战。为解决这些问题，我们提出了X-Teaming框架，通过协作代理进行规划、攻击优化和验证，成功率达到98.1%，尤其对Claude 3.7 Sonnet等先进模型也实现了高达96.2%的成功率。此外，基于X-Teaming，我们开发了XGuard-Train开源训练集，规模为此前最佳资源的20倍，包含3万种交互式越狱场景，助力提升语言模型的多轮安全性。本研究为应对复杂对话攻击提供了重要工具和见解。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13203" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 12:11:28 GMT</pubDate>
</item>
<item>
<title>基于生成模型的新型透视变形图像创作</title>
<link>https://arxiv.org/abs/2504.08902</link>
<guid>https://arxiv.org/abs/2504.08902</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合生成模型与频率感知变换技术，创造可直接解读的透视变形图像。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了经典的光学错觉——透视变形图像（anamorphosis），这类图像是通过故意扭曲形成，在常规视角下难以辨认，仅当从特定角度或借助反射设备观看时才显现其真实形态。尽管这类视觉效果的数学原理可以追溯到17世纪，但它们的意义通常依赖于特定视角。我们提出了一种创新方法，利用潜伏修正流模型和一种名为拉普拉斯金字塔扭曲的技术，在保持图像直接可读性的同时生成高质量的透视变形图像。这一研究将视觉文字谜（Visual Anagrams）的概念扩展到了潜在空间模型及更广泛的几何变换领域，为生成式艺术与感知错觉的创作提供了新的可能性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.08902" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 14:12:01 GMT</pubDate>
</item>
<item>
<title>从知识检索到思维构建：生成式AI的第二幕</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">生成式AI进入“Act II”，从知识检索转向思维构建。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了生成式人工智能（Generative AI）从第一代到第二代的演进过程。第一代模型通过大规模参数和数据扩展取得了显著成功，但存在知识延迟、浅层推理和受限认知等根本性限制。在此期间，提示工程成为我们与AI交互的主要方式。如今，随着测试时扩展技术的应用，第二代模型正从知识检索系统转变为思维构建引擎，从而实现与AI在思维层面的连接。文章还解释了认知工程的概念基础，并提供了相关教程和优化实现，使更多从业者能够参与到AI的第二幕发展中。此外，作者维护了一个定期更新的GitHub仓库，收录了关于测试时扩展的相关论文。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13828" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:55:58 GMT</pubDate>
</item>
<item>
<title>多语言大型语言模型的知识边界感知研究</title>
<link>https://arxiv.org/abs/2504.13816</link>
<guid>https://arxiv.org/abs/2504.13816</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示多语言LLMs的知识边界感知机制及跨语言迁移方法。</p><br /><br /><p><strong>摘要：</strong> 本文首次分析了大型语言模型（LLMs）在不同语言中识别知识边界的内部表征特性。通过在多种语言中处理已知与未知问题的探针测试，我们发现LLMs对知识边界的感知主要编码在中间至中上层网络中，且跨语言感知差异呈线性结构。基于此，提出了一种无需训练的对齐方法，可有效实现低资源语言间知识边界感知能力的转移，降低幻觉风险。此外，双语问题对微调进一步提升了跨语言知识边界识别能力。鉴于缺乏标准测试基准，我们构建了一个包含三种代表性知识边界数据的多语言评估套件。所有代码和数据集均公开可用。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13816" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:44:12 GMT</pubDate>
</item>
<item>
<title>语言模型中不确定性量化评估的偏差问题及其解决方案</title>
<link>https://arxiv.org/abs/2504.13677</link>
<guid>https://arxiv.org/abs/2504.13677</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现常用正确性函数会放大某些不确定性量化方法的表现。</p><br /><br /><p><strong>摘要：</strong> 不确定性量化（UQ）在提升语言模型（LMs）的安全性和可靠性方面至关重要。本文通过评估四种数据集和四种模型下七种正确性函数（包括基于词汇和嵌入的指标及大型语言模型作为裁判的方法）对六种不确定性量化方法的影响，揭示了这些正确性函数中的长度偏差会与量化方法中的长度偏差相互作用，从而扭曲UQ评估结果。研究指出，将大型语言模型作为裁判的方法相对较少受到长度偏差影响，可能是解决此类偏差的一种潜在方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13677" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 09:13:42 GMT</pubDate>
</item>
<item>
<title>Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering</title>
<link>https://arxiv.org/abs/2504.13519</link>
<guid>https://arxiv.org/abs/2504.13519</guid>
<content:encoded><![CDATA[
Effective denoising is crucial in low-dose CT to enhance subtle structures and low-contrast lesions while preventing diagnostic errors. Supervised methods struggle with limited paired datasets, and self-supervised approaches often require multiple noisy images and rely on deep networks like U-Net, offering little insight into the denoising mechanism. To address these challenges, we propose an interpretable self-supervised single-image denoising framework -- Filter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral Filter that adapted to each noisy input through a lightweight module that predicts spatially varying filter parameters, which can be visualized and adjusted post-training for user-controlled denoising in specific regions of interest. To enable single-image training, we introduce a novel downsampling shuffle strategy with a new self-supervised loss function that extends the concept of Noise2Noise to a single image and addresses spatially correlated noise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading self-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving transparency, user control, and parametric efficiency. These features provide key advantages for medical applications that require precise and interpretable noise reduction. Our code is demonstrated at https://github.com/sypsyp97/Filter2Noise.git .
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 03:15:27 GMT</pubDate>
</item>
<item>
<title>基于生产理论的语言模型经济价值评估框架</title>
<link>https://arxiv.org/abs/2504.13359</link>
<guid>https://arxiv.org/abs/2504.13359</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出结合准确性与推理成本的经济评估框架。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一个基于生产理论的框架，用于评估语言模型的经济价值，通过整合准确性与推理成本的概念，定义了“通过成本”（cost-of-pass）和“前沿通过成本”（frontier cost-of-pass）。研究发现轻量级模型在基本量化任务中最具成本效益，大型模型在知识密集型任务中表现优异，而推理模型在复杂量化问题上更具优势。此外，过去一年中复杂量化任务的成本大幅下降，创新如轻量级、大型和推理模型的进展对此起到了关键作用。同时，多数推理技术带来的边际精度提升未能抵消其成本，表明互补的模型层面创新是提高成本效益的主要驱动力。此框架为衡量进展和指导部署提供了原则性工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13359" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 17:58:29 GMT</pubDate>
</item>
<item>
<title>基于自我校准框架的大型视频语言模型改进研究</title>
<link>https://arxiv.org/abs/2504.12083</link>
<guid>https://arxiv.org/abs/2504.12083</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种自我校准框架解决大型视频语言模型的细粒度时间理解问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有大型视频语言模型（LVLMs）在细粒度时间理解、幻觉现象及简单视频问答任务中易犯错误的问题，提出了一个自我校准框架，通过学习自身错误来提升模型性能。该框架首先构建了一组优选与非优选响应对，非优选响应引入常见错误模式，如空间时间理解不足、概念间虚假相关性等。为实现模型与这些响应对的自我校准，我们引入了精炼正则化偏好优化（RRPO），该方法利用子序列级精炼奖励和逐令牌KL正则化，克服直接偏好优化（DPO）的局限性。实验表明，RRPO在多种视频任务中，包括视频幻觉、短视频和长视频理解以及细粒度时间推理上，均表现出更精确的校准和更稳定的训练效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12083" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 09:43:56 GMT</pubDate>
</item>
<item>
<title>ThoughtMani：通过外部CoTs减少大型推理模型的冗余推理步骤</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法ThoughtMani，通过插入小型模型生成的外部CoTs来减少大型推理模型的冗余推理步骤。</p><br /><br /><p><strong>摘要：</strong> 近年来，大型推理模型（LRMs）通过扩展测试时计算能力，在多个任务中表现出色。然而，这些模型通常面临“过度思考”问题，即产生大量冗余推理步骤但性能提升有限。现有解决方案依赖于微调，这需要额外的数据、非传统训练设置、潜在的安全性失配和较差的泛化能力。本研究通过实证分析揭示LRM行为的一个重要特性，即在推理令牌之间插入由较小模型生成的外部CoTs可以有效操控模型生成较少的推理步骤。基于此洞察，我们提出了一种简单而高效的管道——ThoughtMani，使LRMs能够绕过不必要的中间步骤并显著降低计算成本。我们在LiveBench/Code数据集上对QwQ-32B进行实验验证，发现ThoughtMani保持了原始性能，减少了约30%的输出标记数，且外部CoT生成器带来的开销极小。此外，ThoughtMani还提高了平均10%的安全性对齐。由于模型供应商通常同时提供不同规模的模型，ThoughtMani为构建更高效、更易访问的LRMs提供了有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 07:07:19 GMT</pubDate>
</item>
<item>
<title>CLASH数据集评估大型语言模型在高风险情境下的价值决策能力</title>
<link>https://arxiv.org/abs/2504.10823</link>
<guid>https://arxiv.org/abs/2504.10823</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究首次引入CLASH数据集，评估大语言模型在高风险价值冲突情境中的推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为CLASH的新数据集，用于评估大型语言模型（LLMs）在高风险情境下处理价值冲突的能力。CLASH包含345个高影响力困境及其3,795个不同的价值视角，旨在弥补先前研究仅限于日常场景的不足。通过测试10种前沿开放及封闭模型，发现顶级模型如GPT-4o和Claude-Sonnet在识别模棱两可决策方面准确率不足50%，但在明确情景中表现较好。此外，尽管LLMs能合理预测心理不适，但对涉及价值转变的观点理解不足，表明LLMs需要增强复杂价值推理能力。实验还显示LLMs的价值偏好与其向特定价值方向的可引导性密切相关，且从第三方视角进行价值推理时表现出更高的可引导性，但某些价值对则从第一人称视角中受益更多。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10823" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 22:54:16 GMT</pubDate>
</item>
<item>
<title>DehazeXL：一种高效处理大分辨率图像雾霾去除的方法</title>
<link>https://arxiv.org/abs/2504.09621</link>
<guid>https://arxiv.org/abs/2504.09621</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种平衡全局上下文和局部特征的大分辨率图像去雾方法DehazeXL。</p><br /><br /><p><strong>摘要：</strong> 本文针对现有深度学习模型在处理大分辨率雾霾图像时遇到的内存限制问题，提出了DehazeXL，一种能够在主流GPU硬件上实现端到端建模的去雾方法。DehazeXL通过有效平衡全局上下文和局部特征提取，在不牺牲细节的情况下解决了高分辨率图像去雾难题。此外，为了评估全局上下文在去雾性能中的作用，我们设计了一种特定于去雾任务的视觉归因方法。同时，由于缺乏大型图像去雾基准数据集，我们创建了一个超高分辨率去雾数据集（8KDehaze），包含10,000对大小为8192×8192像素的清晰与雾霾图像。实验表明，DehazeXL可以在仅占用21GB显存的情况下推断高达10240×10240像素的图像，达到当前最先进的性能。相关代码和数据集已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09621" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 11:41:25 GMT</pubDate>
</item>
<item>
<title>强化学习与可验证奖励对大语言模型推理能力的影响</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现强化学习并未显著提升大语言模型的基础推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文重新评估了强化学习与可验证奖励（RLVR）在增强大型语言模型（LLMs）推理能力方面的作用，特别是在数学和编程任务中的表现。虽然RLVR训练的模型在小值k（如k=1）下的pass@k指标优于基线模型，但在大值k时，基线模型的表现可以达到甚至超过RL训练模型。进一步分析表明，RL训练通过调整模型输出分布，提高了采样正确响应的效率，但同时也限制了模型的推理能力边界。此外，在视觉推理任务中也观察到类似结果。值得注意的是，蒸馏方法可以引入不同于RLVR的新知识。这些发现揭示了RLVR在提升LLMs推理能力方面的局限性，需要重新审视强化学习对推理模型的影响并探索更优的训练范式。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13837" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>基于最大信息增益的指令微调数据高效采样方法</title>
<link>https://arxiv.org/abs/2504.13835</link>
<guid>https://arxiv.org/abs/2504.13835</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种统一的数据信息量化方法及最大信息增益采样算法，显著提升指令微调数据集构建效果。</p><br /><br /><p><strong>摘要：</strong> 数据质量和多样性对构建有效的指令微调数据集至关重要。现有方法多侧重实例质量并采用启发式规则维持多样性，但缺乏全局视角可能导致结果不佳，且难以精确捕捉复杂指令意图。本文提出一种基于标签图构造的语义空间量化方法，通过信息分布衡量数据集多样性，并设计最大信息增益(MIG)采样算法，迭代选择样本以最大化语义空间中的信息增益。实验表明，MIG方法在多个数据集和基础模型上优于当前先进方法，例如用Tulu3数据集5%采样数据微调的模型在AlpacaEval和Wildbench上的性能分别提升了5.73%和6.89%，接近全量数据SFT模型的表现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13835" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 13:59:46 GMT</pubDate>
</item>
<item>
<title>基于注意力偏差的深度学习架构设计框架Miras</title>
<link>https://arxiv.org/abs/2504.13173</link>
<guid>https://arxiv.org/abs/2504.13173</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架Miras，通过重新设计神经网络作为关联记忆模块提升模型能力。</p><br /><br /><p><strong>摘要：</strong> 本文受人类认知现象“注意力偏差”的启发，将Transformer等神经网络重新定义为通过内部目标学习键值映射的关联记忆模块。研究发现现有序列模型大多采用点积相似性或L2回归作为注意力偏差，为此提出了多种替代配置及其稳定训练的近似方法，并重新解释了现代深度学习中的遗忘机制。基于这些见解，我们开发了Miras框架，包含四种设计选择：关联记忆架构、注意力偏差目标、保留门和记忆学习算法。由此产生了三个新型序列模型Moneta、Yaad和Memora，不仅超越了现有线性RNN的能力，还展现出在语言建模、常识推理及回忆密集型任务上的优异表现，甚至优于Transformer等现代模型。实验表明，Miras的不同设计变体在特定任务上具有显著优势。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13173" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>结合伪合成与真实图像的空中地面视角几何重建</title>
<link>https://arxiv.org/abs/2504.13157</link>
<guid>https://arxiv.org/abs/2504.13157</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合伪合成与真实图像的框架，解决空中与地面视角差异大的几何重建问题。</p><br /><br /><p><strong>摘要：</strong> 本文研究了从混合地面和空中视图捕获的图像进行几何重建的任务。当前最先进的基于学习的方法无法处理空中与地面图像对之间极端的视角变化，主要原因是缺乏高质量的共配准空中-地面训练数据集。我们假设这是失败的关键原因，因为这样的数据难以精确组装且难以以可扩展的方式重建。为了解决这一挑战，我们提出了一种可扩展的框架，该框架结合了来自3D城市范围网格（如Google Earth）的伪合成渲染和真实地面级众包图像（如MegaDepth）。伪合成数据模拟了广泛的空中视角，而真实众包图像则在地面级图像中改善了视觉保真度，有效地弥合了真实图像与伪合成渲染之间的领域差距。使用此混合数据集，我们微调了几种最先进的算法，在现实世界的零样本空中-地面任务中取得了显著改进。例如，基线DUSt3R方法仅将不到5%的空中-地面对在相机旋转误差5度以内定位，而使用我们的数据微调后，准确率提高到近56%，解决了处理大视角变化的主要失败点。除了相机估计和场景重建外，我们的数据集还在具有挑战性的空中-地面场景中的下游任务（如新颖视图合成）中提高了性能，展示了我们的方法在实际应用中的实用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13157" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:57:05 GMT</pubDate>
</item>
<item>
<title>HiScene：基于层次结构的场景级3D生成框架</title>
<link>https://arxiv.org/abs/2504.13072</link>
<guid>https://arxiv.org/abs/2504.13072</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出HiScene框架，结合2D与3D生成技术实现高质量场景生成。</p><br /><br /><p><strong>摘要：</strong> 场景级3D生成是多媒体与计算机图形学的重要前沿领域，但现有方法存在对象类别有限或编辑灵活性不足的问题。本文介绍了一种名为HiScene的新框架，通过将场景视为具有等距视图的层次化“对象”，实现了高保真度的场景生成。HiScene将房间视为复杂对象并进一步分解为可操控的项目，从而确保生成内容与2D表示的一致性同时保持组成结构。为了保证分解实例的完整性和空间对齐，我们开发了一种基于视频扩散的模态完成技术，有效处理对象之间的遮挡和阴影问题，并引入形状先验注入以确保场景内的空间一致性。实验结果显示，该方法生成的物体排列更加自然且适合交互应用，同时保持物理真实性和与用户输入的对齐。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13072" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:33:39 GMT</pubDate>
</item>
<item>
<title>多语言推理在大型语言模型中的上限潜力研究</title>
<link>https://arxiv.org/abs/2504.11833</link>
<guid>https://arxiv.org/abs/2504.11833</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">多语言推理比英语推理具有更高的性能上限。</p><br /><br /><p><strong>摘要：</strong> 现有研究表明，大型语言模型存在显著的“英语偏见”，即任务用英语呈现时表现更好。然而，我们发现某些其他语言在推理任务中甚至可以优于英语。本文探索了多语言推理在任务中的性能上限，指出多语言推理不仅能显著提高近10 Acc@k点，还更具鲁棒性，对翻译质量和语言选择的变化具有更强的容忍度。尽管如此，目前常见的答案选择方法无法达到这一上限，因为它们受到局限性和偏见的影响。此外，我们分析了性能上限背后的原因及实现挑战。这些发现为未来充分挖掘大型语言模型中多语言推理的潜力提供了方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11833" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 03:45:10 GMT</pubDate>
</item>
<item>
<title>NodeRAG：基于异构图结构的大规模语言模型增强型检索生成框架</title>
<link>https://arxiv.org/abs/2504.11544</link>
<guid>https://arxiv.org/abs/2504.11544</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">NodeRAG通过引入异构图结构优化检索增强生成性能。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为NodeRAG的新框架，该框架以图为中心，通过引入异构图结构，实现了图算法在检索增强生成（RAG）工作流中的无缝整合。与现有方法相比，NodeRAG不仅提升了索引时间、查询时间和存储效率，还在多跳基准测试和开放式一对一评估中展现出更优的问题回答能力。此外，NodeRAG在使用最少检索标记的情况下，表现出了显著的优势，尤其是在GraphRAG和LightRAG等方法的基础上进一步优化了性能。NodeRAG的代码已开源，可在GitHub上获取。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11544" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:24:00 GMT</pubDate>
</item>
<item>
<title>构建透明图像与视频理解研究的开放性感知语言模型</title>
<link>https://arxiv.org/abs/2504.13180</link>
<guid>https://arxiv.org/abs/2504.13180</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究提出通过开源框架构建透明的感知语言模型用于图像和视频理解。</p><br /><br /><p><strong>摘要：</strong> 视觉语言模型在计算机视觉领域至关重要，但许多高性能模型仍为闭源状态，限制了科学进展。尽管学术界尝试通过黑盒模型蒸馏生成训练数据取得显著基准成果，但缺乏对教师模型及其数据来源的了解阻碍了真正的科学进步。本文提出在完全开源且可复现的框架下构建感知语言模型（PLM），以推动图像和视频理解领域的透明研究。我们分析了标准训练管道，并探索大规模合成数据，发现特别是在详细视频理解方面存在关键的数据缺口。为此，我们发布了包含280万个细粒度视频问答对的人类标注实例以及时空定位的视频描述。此外，我们推出了PLM-VideoBench，这是一个评估具有挑战性的视频理解任务的工具包，重点关注对视频“什么”、“哪里”、“何时”和“如何”的推理能力。我们的工作通过提供数据、训练方法、代码和模型实现了完全的可复现性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13180" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Complex-Edit：基于指令的图像编辑模型综合评估基准</title>
<link>https://arxiv.org/abs/2504.13143</link>
<guid>https://arxiv.org/abs/2504.13143</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出Complex-Edit基准，评估指令复杂性对图像编辑模型的影响。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为Complex-Edit的综合基准，用于系统性评估基于指令的图像编辑模型在不同复杂度指令下的表现。通过利用GPT-4o大规模自动生成多样化编辑指令，我们设计了一套“编辑链”管道来构建复杂的编辑任务。此外，还引入了一系列指标和基于视觉语言模型的自动化评估流程，以支持大规模性能评估。研究发现开源模型显著落后于闭源模型，且随着指令复杂性的增加，模型在保留输入图像关键元素和整体美学质量方面的能力显著下降。此外，将复杂指令分解为原子步骤执行会降低性能，而Best-of-N策略能提升直接编辑和逐步方法的效果。有趣的是，训练中使用合成数据会导致模型生成的图像显得更加人工化，这种现象在GPT-4o输出中也有所体现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13143" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:51:59 GMT</pubDate>
</item>
<item>
<title>基于Vision Transformer的无人机跟踪中抗遮挡表示学习</title>
<link>https://arxiv.org/abs/2504.09228</link>
<guid>https://arxiv.org/abs/2504.09228</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法增强单流ViT模型在航拍跟踪中的遮挡鲁棒性。</p><br /><br /><p><strong>摘要：</strong> 本文针对使用视觉Transformer(ViT)主干网络的单流架构在实时无人机跟踪中的不足，即缺乏有效处理遮挡的策略，提出了一种基于ViT学习抗遮挡表示（ORR）的新方法。通过引入随机屏蔽操作模拟目标遮挡，使ViT模型具备更强的目标遮挡鲁棒性，构建了名为ORTrack的框架。此外，为满足实时应用需求，设计了自适应特征知识蒸馏（AFKD）方法，生成高效的学生模型ORTrack-D，既保留了ORTrack的性能，又提高了效率。在多个基准数据集上的实验验证了该方法的有效性和领先性能，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09228" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 10:06:50 GMT</pubDate>
</item>
<item>
<title>70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float</title>
<link>https://arxiv.org/abs/2504.11651</link>
<guid>https://arxiv.org/abs/2504.11651</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https://github.com/LeanModels/DFloat11.
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 18:38:38 GMT</pubDate>
</item>
<item>
<title>Perception Encoder：通过视觉-语言对比学习实现多任务通用编码器</title>
<link>https://arxiv.org/abs/2504.13181</link>
<guid>https://arxiv.org/abs/2504.13181</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">仅靠对比视觉-语言训练即可生成适用于多种下游任务的强大通用嵌入。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了名为Perception Encoder（PE）的最先进图像和视频理解编码器，该编码器通过简单的视觉-语言学习进行训练。传统视觉编码器依赖于针对具体任务定制的各种预训练目标，而我们发现，经过精心调校的图像预训练方法和稳健的视频数据引擎优化后，仅基于对比视觉-语言训练就能产生适用于分类、描述、定位等所有这些下游任务的强大通用嵌入。然而，这些嵌入隐藏在网络的中间层中，为此我们提出了两种对齐方法：用于多模态语言建模的语言对齐和用于密集预测的空间对齐。结合核心对比检查点，PE模型家族在零样本图像和视频分类与检索、文档问答、图像问答、视频问答以及检测、深度估计和跟踪等空间任务上均取得了最先进的性能。为了促进进一步研究，我们将发布我们的模型、代码以及一个包含合成和人工注释视频的新颖数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13181" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:57 GMT</pubDate>
</item>
<item>
<title>MetaSynth：通过元提示增强合成数据多样性以实现领域自适应</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">MetaSynth通过元提示生成多样化合成数据，成功将大型语言模型适配到金融和生物医学领域。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了利用合成数据进行领域自适应的问题，特别是针对小型语言模型Phi-3.5和Phi-4，这些模型依赖由大型语言模型生成的合成数据。然而，合成数据的低多样性限制了其在改进其他模型方面的应用。为解决这一问题，我们提出了MetaSynth方法，该方法通过元提示机制，让多个“专家”大型语言模型协同生成数据，从而提高合成数据的多样性。实验表明，仅使用2500万token的MetaSynth生成的合成数据，就能有效将已训练好的大型语言模型（Mistral-7B-v0.3）适配到金融和生物医学两个特定领域，同时不影响其在通用任务中的性能。此外，通过七种自动化指标评估，MetaSynth生成的合成数据的多样性接近大型语言模型预训练语料库。持续预训练Mistral-7B-v0.3模型显示，在金融领域提升了4.08%，在生物医学领域提升了13.75%的表现。相比之下，使用模板提示生成的数据即使包含真实数据示例，也会导致模型性能下降。研究结果表明，使用MetaSynth生成的少量多样化合成数据即可实现有效的领域自适应。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12563" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 21:25:15 GMT</pubDate>
</item>
<item>
<title>基于学习的跨相机色彩恒常性方法</title>
<link>https://arxiv.org/abs/2504.07959</link>
<guid>https://arxiv.org/abs/2504.07959</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种无需重新训练即可适应新相机的跨相机色彩校正学习方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种基于学习的方法，用于解决计算色彩恒常性（即白平衡）问题中的跨相机适应性挑战。该方法利用图像信号处理器(ISP)中预校准的颜色校正矩阵(CCM)，将标准空间的颜色映射到相机的原始颜色空间，并通过编码生成紧凑的相机指纹嵌入(CFE)，从而实现对未知相机的适应。为了防止训练过程中因相机数量有限导致的过拟合，引入了一种相机间数据插值增强技术。实验结果表明，该方法在多个数据集和网络结构上实现了最先进的跨相机色彩恒常性性能，同时保持轻量化且仅依赖于ISP中已有的数据。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.07959" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:31 GMT</pubDate>
</item>
<item>
<title>通过睡眠计算提升大语言模型推理效率</title>
<link>https://arxiv.org/abs/2504.13171</link>
<guid>https://arxiv.org/abs/2504.13171</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">引入睡眠计算显著降低大语言模型推理时的计算需求。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为睡眠计算的新方法，允许模型在用户查询到来之前预先处理可能的上下文信息，从而大幅减少测试阶段的计算需求和延迟。实验表明，在两个推理任务(Stateful GSM-Symbolic 和 Stateful AIME)中，睡眠计算可使所需测试计算量减少约5倍，同时通过扩展计算还可提高准确性最高达13%和18%。此外，多查询GSM-Symbolic进一步优化了相关查询的平均成本，降低了2.5倍。研究还发现，用户查询的可预测性与睡眠计算的效果高度相关。最后，睡眠计算在实际代理任务中也显示出良好的应用潜力。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13171" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>CLIMB框架实现高效预训练数据混合优化</title>
<link>https://arxiv.org/abs/2504.13161</link>
<guid>https://arxiv.org/abs/2504.13161</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出CLIMB框架优化预训练数据混合，提升模型性能。</p><br /><br /><p><strong>摘要：</strong> 当前预训练数据集多来源于网络内容且缺乏明确领域划分，导致构建最优数据混合极具挑战性。本文提出CLIMB框架，通过嵌入和聚类大规模数据集并迭代优化数据混合，显著提升了预训练模型的表现。实验表明，在400B令牌上训练时，CLIMB框架下的1B模型比Llama-3.2-1B高出2%，特定领域优化进一步提升5%。此外，我们发布了ClimbLab和ClimbMix数据集供研究使用，揭示了最优数据混合的特性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13161" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:58:13 GMT</pubDate>
</item>
<item>
<title>利用专家失败探索提升大型语言模型代理性能</title>
<link>https://arxiv.org/abs/2504.13145</link>
<guid>https://arxiv.org/abs/2504.13145</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法EEF，通过分析专家失败轨迹提高大型语言模型代理能力。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLMs）作为代理展现出强大的潜力，特别是在需要多轮推理和交互的任务上。拒绝采样微调（RFT）是一种有效的微调方法，通过模仿专家成功轨迹并迭代优化自生成轨迹来提升代理技能。然而，由于专家（如GPT-4）在简单子任务上表现较好，而RFT倾向于处理简单场景，许多复杂子任务仍未解决且持续处于分布外（OOD）。研究发现，专家失败轨迹中的计划和关键操作可显著提高代理的探索效率和技能获取。基于此，我们提出了探索专家失败（EEF），该方法从失败的专家轨迹中识别出有益操作并整合到训练数据集中，同时排除潜在有害操作以保护模型学习过程。实验表明，EEF在WebShop中取得了62%的胜率，优于RFT（53.6%）和GPT-4（35.6%），并在WebShop和SciWorld中创造了新的性能记录。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13145" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:53:54 GMT</pubDate>
</item>
<item>
<title>多因素挑战下的RAG系统改进：RAMDocs与MADAM-RAG</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出RAMDocs和MADAM-RAG方法，同时处理模糊查询和信息冲突问题。</p><br /><br /><p><strong>摘要：</strong> 大型语言模型（LLM）的检索增强生成（RAG）技术虽提升了事实准确性，但在处理模糊查询及多源冲突信息时仍面临挑战。现有研究通常孤立解决单一问题，如模糊性或噪声信息。本文提出RAMDocs，一个模拟复杂真实场景的新数据集，涵盖模糊性、错误信息和噪声；并开发MADAM-RAG，一种多代理辩论机制，通过多轮讨论聚合去噪后的有效答案。实验显示，MADAM-RAG在AmbigDocs和FaithEval任务中均优于基线模型，但RAMDocs对现有RAG系统构成重大挑战，表明仍有提升空间。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13079" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:46:11 GMT</pubDate>
</item>
<item>
<title>NoisyRollout：通过视觉导向策略增强视觉语言模型的推理能力</title>
<link>https://arxiv.org/abs/2504.13055</link>
<guid>https://arxiv.org/abs/2504.13055</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合清洁与失真图像的RL方法NoisyRollout，提升视觉语言模型的推理与感知能力。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了强化学习在视觉语言模型中的应用，特别是如何通过引入视觉导向的噪声策略来增强模型的探索能力。传统的视觉语言模型在面对不完美视觉感知时表现不佳，这限制了其推理效果。为此，我们提出了NoisyRollout，这是一种简单而有效的强化学习方法，通过混合清洁和适度失真的图像轨迹，为视觉感知和推理模式引入目标多样性。该方法无需额外训练成本，在仅使用2.1K训练样本的情况下，在五个跨领域的基准测试中实现了最先进的性能，同时保持了同等甚至更好的领域内表现。此外，NoisyRollout采用了一种噪声退火调度，逐步减少训练过程中的失真强度，从而早期利用噪声信号的优势，同时确保后期训练的稳定性和可扩展性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13055" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 12:10:13 GMT</pubDate>
</item>
<item>
<title>ANT：通过自动引导去噪轨迹实现文本转图像模型的概念擦除</title>
<link>https://arxiv.org/abs/2504.12782</link>
<guid>https://arxiv.org/abs/2504.12782</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架ANT，用于有效防止有害内容生成并实现概念擦除。</p><br /><br /><p><strong>摘要：</strong> 现有基于微调的文本到图像模型概念擦除方法存在显著局限性，本文引入一种名为ANT的新框架，通过自动引导去噪轨迹，实现了对不想要概念的精确修改。该框架基于分类器自由引导条件方向反转的见解，在去噪的中晚期阶段启用此操作，从而在不牺牲早期结构完整性的情况下实现内容修改。对于单概念擦除，采用增强的权重显著图来精确定位关键参数；对于多概念擦除，提供了一种灵活的插件解决方案。实验表明，ANT在单概念和多概念擦除方面均达到最先进的性能，同时保持生成保真度。代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12782" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 05:29:30 GMT</pubDate>
</item>
<item>
<title>InstantCharacter：基于扩散Transformer的可扩展角色定制框架</title>
<link>https://arxiv.org/abs/2504.12395</link>
<guid>https://arxiv.org/abs/2504.12395</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于扩散Transformer的角色定制框架，提升图像质量和文本可控性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为InstantCharacter的新框架，旨在解决现有基于学习的角色定制方法的局限性，如有限的泛化能力和图像质量下降问题，以及优化方法中特定对象微调导致的文本可控性降低问题。该框架基于扩散Transformer构建，具有开放域个性化能力，支持多样化的角色外观、姿势和风格，同时保持高保真度。通过引入堆叠变换编码器的可扩展适配器，框架能够有效处理开放域角色特征并与现代扩散Transformer的潜在空间无缝交互。此外，构建了一个包含1000万级别样本的大规模角色数据集，分为配对（多视角角色）和非配对（文本-图像组合）子集，以优化身份一致性与文本编辑能力。实验结果表明，InstantCharacter在生成高保真、文本可控且角色一致的图像方面表现出色，树立了新的角色驱动图像生成基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12395" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 14:01:59 GMT</pubDate>
</item>
<item>
<title>基于分数蒸馏的文本到图像模型合并方法</title>
<link>https://arxiv.org/abs/2504.12364</link>
<guid>https://arxiv.org/abs/2504.12364</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新方法将多个模型的知识整合为单一多才多艺的文本到图像生成模型。</p><br /><br /><p><strong>摘要：</strong> 随着文本到图像生成模型的成功，大量从同一基础模型微调而来的专门模型涌现，但带来了高参数冗余和存储成本问题。传统静态线性插值方法忽略了多样风格可能带来的不兼容性。为此，我们设计了一种可通过风格向量控制任意风格图像生成的流水线，并提出了基于分数蒸馏的模型合并范式（DMM），有效压缩多个模型为单一多功能模型。此外，我们重新思考并重新定义了文本到图像生成领域的模型合并任务，提出新的合并目标和评估协议。实验表明，DMM能够紧凑地重组多个教师模型的知识，并实现可控的任意风格生成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12364" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:09:45 GMT</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 11:04:14 GMT</pubDate>
</item>
<item>
<title>GRA框架：多小模型协作生成高质量数据</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过多小模型协作模拟同行评审，实现与大模型相当的数据合成质量。</p><br /><br /><p><strong>摘要：</strong> 本文提出了一种名为GRA的框架，该框架通过多个小型语言模型（LLMs）协同工作，模拟人类同行评审过程，完成数据合成与优化任务。具体而言，GRA框架中的小模型分别承担生成者（Generator）、审查者（Reviewer）和裁定者（Adjudicator）的角色，逐步迭代并控制数据的质量与多样性。实验结果显示，该方法生成的数据在多个基准测试中达到了甚至超过了单一大型语言模型（如Qwen-2.5-72B-Instruct）的表现。这一成果挑战了传统上依赖大型模型进行高质量数据合成的必要性，提倡采用小型模型的策略性协作。研究代码和数据集已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12322" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 02:13:43 GMT</pubDate>
</item>
<item>
<title>REVERSE：一种统一的视觉-语言模型幻觉缓解框架</title>
<link>https://arxiv.org/abs/2504.13169</link>
<guid>https://arxiv.org/abs/2504.13169</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出REVERSE框架，通过训练与实时自验证结合显著减少视觉幻觉。</p><br /><br /><p><strong>摘要：</strong> 现有的视觉-语言模型在视觉理解方面表现出色，但容易产生不存在的对象、动作或概念的描述，这在安全性至关重要的应用场景中构成重大风险。目前的幻觉缓解方法主要分为两类：生成调整和事后验证。然而，生成调整方法通常依赖启发式规则且缺乏修正机制，而事后验证则复杂且倾向于拒绝输出而非改进。本研究提出了REVERSE框架，它将幻觉感知训练与动态自我验证相结合。通过利用包含超过130万个半合成样本的新幻觉验证数据集，以及创新的推理时回顾重采样技术，该方法使视觉-语言模型能够在生成过程中检测幻觉并动态修正这些幻觉。实验表明，REVERSE在CHAIR-MSCOCO和HaloQuest数据集上的幻觉减少效果达到当前最佳水平，分别比现有最佳方法高出12%和28%。相关数据集、模型和代码已公开。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13169" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:59:22 GMT</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:54:14 GMT</pubDate>
</item>
<item>
<title>VistaDPO：基于层次化空间-时间直接偏好优化的大规模视频模型对齐框架</title>
<link>https://arxiv.org/abs/2504.13122</link>
<guid>https://arxiv.org/abs/2504.13122</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出VistaDPO框架解决视频理解中的对齐与幻觉问题。</p><br /><br /><p><strong>摘要：</strong> 本文针对基于大型语言模型构建的大规模视频模型（LVMs）在视频理解中存在的对齐人类直觉差及视频幻觉问题，引入了VistaDPO这一新颖的视频层次化空间-时间直接偏好优化框架。该框架通过实例级、时间级及感知级三个层级增强文本与视频间偏好对齐效果。鉴于缺乏细粒度视频语言偏好对齐的数据集，我们创建了VistaDPO-7k数据集，包含7.2K个标注问答对及时空定位信息。在视频幻觉、视频问答及视频描述等基准测试中，VistaDPO显著提升了现有LVMs的表现，有效缓解了视频语言不匹配及幻觉现象。相关代码与数据已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.13122" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 13:39:41 GMT</pubDate>
</item>
<item>
<title>FramePack：用于视频生成的高效帧预测神经网络结构</title>
<link>https://arxiv.org/abs/2504.12626</link>
<guid>https://arxiv.org/abs/2504.12626</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种名为FramePack的神经网络结构，用于视频生成中的帧预测。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为FramePack的神经网络结构，旨在训练视频生成中的下一帧预测模型。FramePack通过压缩输入帧，使Transformer的上下文长度固定，从而可以处理大量帧并显著提高训练批次大小。此外，该方法还提出了一种反向漂移采样方法，通过逆时间顺序生成帧并提前确定端点来避免曝光偏差。最后，实验表明，现有的视频扩散模型可以通过FramePack进行微调，视觉质量可能得到改善。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12626" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:02:31 GMT</pubDate>
</item>
<item>
<title>WorldMem：基于记忆模块提升世界模拟中的时空一致性</title>
<link>https://arxiv.org/abs/2504.12369</link>
<guid>https://arxiv.org/abs/2504.12369</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种结合记忆机制的世界模拟框架，有效增强长期场景生成的一致性。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为WorldMem的新框架，旨在通过引入记忆银行（存储记忆帧和状态）来改善虚拟环境中的场景生成质量。记忆银行中的记忆单元不仅存储视觉信息，还记录物体姿态及时间戳等状态数据。通过采用记忆注意力机制，该方法可以从记忆帧中高效提取相关信息，从而即使在较大的视角或时间差距下也能精确重建先前观察到的场景。此外，通过将时间戳纳入状态描述，WorldMem不仅能构建静态世界模型，还能捕捉世界的动态演化过程，支持感知与交互功能。实验表明，该方法在虚拟和真实场景中均表现出色，验证了其有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12369" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:59:30 GMT</pubDate>
</item>
<item>
<title>ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering</title>
<link>https://arxiv.org/abs/2504.05506</link>
<guid>https://arxiv.org/abs/2504.05506</guid>
<content:encoded><![CDATA[
Charts are ubiquitous, as people often use them to analyze data, answer questions, and discover critical insights. However, performing complex analytical tasks with charts requires significant perceptual and cognitive effort. Chart Question Answering (CQA) systems automate this process by enabling models to interpret and reason with visual representations of data. However, existing benchmarks like ChartQA lack real-world diversity and have recently shown performance saturation with modern large vision-language models (LVLMs). To address these limitations, we introduce ChartQAPro, a new benchmark that includes 1,341 charts from 157 diverse sources, spanning various chart types, including infographics and dashboards, and featuring 1,948 questions in various types, such as multiple-choice, conversational, hypothetical, and unanswerable questions, to better reflect real-world challenges. Our evaluations with 21 models show a substantial performance drop for LVLMs on ChartQAPro; e.g., Claude Sonnet 3.5 scores 90.5% on ChartQA but only 55.81% on ChartQAPro, underscoring the complexity of chart reasoning. We complement our findings with detailed error analyses and ablation studies, identifying key challenges and opportunities for advancing LVLMs in chart understanding and reasoning. We release ChartQAPro at https://github.com/vis-nlp/ChartQAPro.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:05:06 GMT</pubDate>
</item>
<item>
<title>BitNet b1.58 2B4T：首个开源1比特20亿参数大型语言模型</title>
<link>https://arxiv.org/abs/2504.12285</link>
<guid>https://arxiv.org/abs/2504.12285</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">BitNet b1.58 2B4T实现了高性能与低能耗的结合。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了BitNet b1.58 2B4T，这是首个开源的原生1比特20亿参数大型语言模型（LLM）。该模型基于4万亿令牌的语料库训练，性能在多个基准测试中得到验证，包括语言理解、数学推理、编码能力和对话能力。研究结果显示，BitNet b1.58 2B4T的性能与同类规模的领先开源全精度LLM相当，但在计算效率方面具有显著优势，如大幅减少内存占用、能耗和解码延迟。为了促进进一步的研究和应用，模型权重已通过Hugging Face发布，同时提供了支持GPU和CPU架构的开源推理实现。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12285" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:51:43 GMT</pubDate>
</item>
<item>
<title>基于激光雷达的零样本形状补全方法CAL</title>
<link>https://arxiv.org/abs/2504.12264</link>
<guid>https://arxiv.org/abs/2504.12264</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种利用多模态传感器序列进行激光雷达形状补全的新方法。</p><br /><br /><p><strong>摘要：</strong> 本文提出了CAL（Complete Anything in Lidar），一种用于野外激光雷达形状补全的方法，它与基于激光雷达的语义/全景场景补全密切相关。与现有方法只能完成和识别封闭词汇表中标记的对象不同，CAL采用零样本方法，通过挖掘多模态传感器序列中的时间上下文来获取观察对象的形状和语义特征，并将其蒸馏到仅依赖激光雷达的实例级补全和识别模型中。尽管我们只挖掘部分形状补全，但发现该模型能够从数据集中的多个部分观测推断完整的物体形状。实验表明，该模型可以在标准基准上进行语义和全景场景补全，并识别超出固定类别词汇表的对象。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12264" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 13:21:55 GMT</pubDate>
</item>
<item>
<title>Cobra：高效灵活的漫画线条艺术着色方法</title>
<link>https://arxiv.org/abs/2504.12240</link>
<guid>https://arxiv.org/abs/2504.12240</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种基于因果稀疏DiT架构的高效着色方法Cobra。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了基于参考的漫画线条艺术着色在高精度、效率、上下文一致性及灵活控制方面的需求。传统扩散模型在处理复杂漫画场景时存在局限性，如处理大量参考图像耗时、推理时间长等问题。为此，我们提出了Cobra方法，该方法结合颜色提示并利用超过200张参考图像，在保持低延迟的同时实现高质量着色。Cobra的核心是一种因果稀疏DiT架构，通过设计的位置编码、因果稀疏注意力及Key-Value缓存机制有效管理长上下文参考并保证色彩一致性。实验结果显示，Cobra显著提升了推理速度和交互性，满足了工业应用的关键需求。相关代码和模型已公开发布。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.12240" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 12:45:19 GMT</pubDate>
</item>
<item>
<title>多语言混合作者文本中AI生成内容检测模型的研究</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出适用于多种生成器和混合作者文本的AI生成内容检测模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一组针对标记分类任务设计的模型，这些模型经过大量人类与机器共同创作的文本训练，在未见过的领域、生成器、非母语文本及对抗性输入上表现优异。同时，我们构建了一个包含超过240万条文本的新数据集，覆盖23种语言，主要由几种流行的专有大型语言模型与人协作生成。研究还分析了模型在不同领域、生成器下的性能，并对比了对抗方法、输入文本长度及生成文本特性对检测效果的影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11952" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 16 Apr 2025 06:29:30 GMT</pubDate>
</item>
<item>
<title>ReTool：通过工具集成学习提升复杂数学推理能力</title>
<link>https://arxiv.org/abs/2504.11536</link>
<guid>https://arxiv.org/abs/2504.11536</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">结合代码解释器的强化学习模型在数学推理任务上显著超越传统文本模型。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一种名为ReTool的新方法，它通过将工具集成到基于强化学习的推理模型中，显著提升了模型在几何推理、复杂方程求解等结构化问题上的表现。ReTool的关键创新在于动态整合自然语言推理过程中的实时代码执行，并采用自动化的强化学习范式来优化工具调用策略。实验结果显示，在AIME数学竞赛题集上，ReTool的32B参数版本在仅需400个训练步的情况下达到了67%的准确率，优于文本基线模型的40%准确率。此外，在扩展设置下，该模型进一步达到72.5%的准确率，大幅领先OpenAI的o1-preview模型。研究还观察到模型具备自主代码修正的能力，表明其已掌握适应性工具使用的“顿悟”能力。这些发现为推动复杂数学推理及混合神经符号系统的进步提供了新思路。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11536" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 14:10:22 GMT</pubDate>
</item>
<item>
<title>Vivid4D：基于单目视频的4D动态场景重建方法</title>
<link>https://arxiv.org/abs/2504.11092</link>
<guid>https://arxiv.org/abs/2504.11092</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">通过引入新方法Vivid4D，利用几何先验和生成先验结合实现多视角合成，提升4D场景重建质量。</p><br /><br /><p><strong>摘要：</strong> 单目视频的4D动态场景重建是一个具有挑战性的任务，因为每个时间戳仅能从单一视角观察。本文提出了一种名为Vivid4D的新方法，通过增强观测视角（即从单目输入合成多视角视频）来改进4D单目视频合成。与现有方法不同，Vivid4D同时利用了几何先验和生成先验，将视角增强重新表述为视频修复任务，通过将观测到的视图基于单目深度先验扭曲到新的视角上实现。为了训练这一模型，我们使用了未定位的网络视频，并通过合成遮挡掩模来模拟扭曲导致的遮挡情况，确保空间和时间上的一致性修复。此外，为了减少单目深度先验的不准确性，我们还引入了迭代视角增强策略和鲁棒重建损失函数。实验表明，该方法显著提高了单目4D场景的重建和完成效果。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11092" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 15 Apr 2025 07:38:14 GMT</pubDate>
</item>
<item>
<title>基于表示对齐的端到端扩散模型训练方法</title>
<link>https://arxiv.org/abs/2504.10483</link>
<guid>https://arxiv.org/abs/2504.10483</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新的训练方法REPA-E，显著提升扩散模型训练效率。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了是否可以将潜在扩散模型与变分自编码器（VAE）tokenizer以端到端的方式联合训练的问题。传统深度学习认为端到端训练更优，但实验表明标准扩散损失函数下的联合训练不仅无效，甚至会降低最终性能。研究发现，通过引入表示对齐（REPA）损失，可以在训练过程中同时微调VAE和扩散模型，从而实现高效的端到端训练。该方法不仅大幅加速了训练过程，还提升了扩散模型的性能，同时改善了VAE的潜在空间结构及下游生成效果。最终，在ImageNet 256 x 256上的FID得分达到1.26（带分类器自由引导）和1.83（不带分类器自由引导），创造了新记录。此外，代码已开源。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10483" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:53 GMT</pubDate>
</item>
<item>
<title>AlayaDB：面向大规模语言模型的高效向量数据库系统</title>
<link>https://arxiv.org/abs/2504.10326</link>
<guid>https://arxiv.org/abs/2504.10326</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">AlayaDB通过解耦KV缓存与注意力计算，优化大规模语言模型推理性能。</p><br /><br /><p><strong>摘要：</strong> AlayaDB是由AlayaDB AI开发的新型向量数据库系统，专为提高大型语言模型（LLMs）的长上下文推理效率而设计。该系统将LLM推理中的KV缓存和注意力计算分离并封装进独立的数据库系统中，显著减少了对硬件资源的需求，同时提升了多种工作负载的服务质量。相较于现有解决方案，如KV缓存解聚合和基于检索的稀疏注意力方法，AlayaDB通过将注意力计算和缓存管理抽象为查询处理过程，并利用原生查询优化器提升性能。本文通过来自行业合作伙伴的三个实际案例以及LLM推理基准测试的广泛实验结果，验证了AlayaDB的有效性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10326" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 11:34:26 GMT</pubDate>
</item>
<item>
<title>MLRC-Bench：评估大语言模型在机器学习研究竞赛中的表现</title>
<link>https://arxiv.org/abs/2504.09702</link>
<guid>https://arxiv.org/abs/2504.09702</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出MLRC-Bench基准，评估大语言模型在解决复杂机器学习问题上的有效性。</p><br /><br /><p><strong>摘要：</strong> 现有对大型语言模型在科学发现中的评估缺乏客观基线和度量标准。本文引入MLRC-Bench基准，专注于衡量语言代理提出和实现创新研究方法的能力，不同于以往关注已建立任务的基准。通过7项竞赛任务测试，发现顶级代理仅达到人类参与者得分的9.3%，并揭示了代理创新与前沿研究性能之间的不匹配。MLRC-Bench是一个动态增长的基准，旨在促进AI研究能力的严谨评估。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09702" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 15:35:43 GMT</pubDate>
</item>
<item>
<title>Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</title>
<link>https://arxiv.org/abs/2504.09566</link>
<guid>https://arxiv.org/abs/2504.09566</guid>
<content:encoded><![CDATA[
Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 09:35:41 GMT</pubDate>
</item>
<item>
<title>AI语音生成技术对多语言口音影响的社会技术分析</title>
<link>https://arxiv.org/abs/2504.09346</link>
<guid>https://arxiv.org/abs/2504.09346</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究揭示AI语音技术可能加剧语言特权和口音歧视。</p><br /><br /><p><strong>摘要：</strong> 近期人工智能语音生成及克隆技术取得了显著进展，但其对不同口音和语言特征的社会技术系统的影响尚未完全明晰。本研究通过混合方法评估了两个AI语音服务（Speechify和ElevenLabs），并结合调查和访谈探讨用户实际体验如何影响他们对这些技术中口音变化的认知。研究发现，技术性能在五种英语区域口音间存在差异，并指出当前技术可能无意中强化语言特权和基于口音的歧视，从而可能导致新的数字排斥形式。总体而言，本研究强调了包容性设计和监管的重要性，为开发者、政策制定者和组织提供了可操作的见解，以确保公平且负责任的人工智能语音技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09346" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 17:31:22 GMT</pubDate>
</item>
<item>
<title>基于SIFT的大规模语音指令微调数据集与语言模型</title>
<link>https://arxiv.org/abs/2504.09081</link>
<guid>https://arxiv.org/abs/2504.09081</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出SIFT数据集用于语音文本大模型的指令微调和预训练。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为SIFT（Speech Instruction Fine-Tuning）的5000万样本数据集，专门用于语音文本大语言模型（LLMs）的指令微调和预训练。SIFT-50M数据集由14,000小时公开可用的语音语料库构建而成，涵盖五种语言，并结合大型语言模型和现成的专业模型进行处理。该数据集支持多样化的语音理解和可控语音生成指令。利用SIFT-50M，我们训练了SIFT-LLM模型，在指令跟随基准测试中表现出色，同时在基础语音任务上也达到了竞争水平。此外，为了促进进一步研究，我们还发布了EvalSIFT，这是一个专门设计用来评估语音文本大模型指令跟随能力的基准数据集。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09081" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 00:45:48 GMT</pubDate>
</item>
<item>
<title>BlockGaussian：高效高质量的大规模场景重建框架</title>
<link>https://arxiv.org/abs/2504.09048</link>
<guid>https://arxiv.org/abs/2504.09048</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">提出一种新框架BlockGaussian，实现大规模场景重建的高效率与高质量。</p><br /><br /><p><strong>摘要：</strong> 近期3D Gaussian Splatting (3DGS) 技术在新型视图合成中展现出巨大潜力，但大规模场景重建仍面临分区、优化和合并等挑战。本文介绍了一种名为BlockGaussian的新框架，通过引入基于内容感知的场景分区策略及基于可见性感知的块级优化，实现了高效且高质量的大型场景重建。具体而言，该方法根据区域内容复杂度变化平衡计算负载，解决了独立块优化中的监督不匹配问题，并提出了伪视图几何约束以缓解块合并时的渲染退化。实验表明，该方法在多个基准测试上不仅提升了优化速度达5倍，还提高了平均PSNR值1.21 dB，同时显著降低了计算需求，使大场景重建能在单块24GB显存设备上完成。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.09048" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 22:05:55 GMT</pubDate>
</item>
<item>
<title>大型视觉语言模型训练中的伪推理路径问题及改进方法</title>
<link>https://arxiv.org/abs/2504.11468</link>
<guid>https://arxiv.org/abs/2504.11468</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究发现，监督微调可能损害后续强化学习，引入新数据集验证并提出改进方案。</p><br /><br /><p><strong>摘要：</strong> 本文重新审视了用于训练大型视觉语言模型（LVLMs）的监督微调（SFT）后接强化学习（RL）的主流范式，并揭示了一个重要发现：SFT可能导致“伪推理路径”的产生，这些路径虽看似模仿自专家模型，但实际上包含冗长、犹豫且较少信息量甚至错误的推理步骤，从而对后续RL造成负面影响。为系统性研究这一现象，我们设计了一个名为VLAA-Thinking的新多模态数据集，该数据集通过包含描述、推理蒸馏、答案重写和验证等六个步骤构建，提供了高质量的SFT逐步视觉推理轨迹以及来自同一数据源的更具挑战性的RL拆分。实验表明，尽管SFT有助于模型学习推理格式，但它常常使已对齐的模型陷入模仿性和僵化的推理模式，阻碍进一步学习。相比之下，基于Group Relative Policy Optimization（GRPO）算法并结合新颖混合奖励模块（整合感知与认知信号），我们的RL方法促进了更真实、适应性强的推理行为。基于Qwen2.5VL 3B的VLAA-Thinker模型在Open LMM Reasoning Leaderboard上达到了4B规模LVLMs中的最佳性能，比之前最先进的方法高出1.8%。我们希望这些发现能为开发具备推理能力的LVLMs提供有价值的见解，并指导该领域的未来研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.11468" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:54:05 GMT</pubDate>
</item>
<item>
<title>ColorBench：评估视觉语言模型颜色理解能力的基准</title>
<link>https://arxiv.org/abs/2504.10514</link>
<guid>https://arxiv.org/abs/2504.10514</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究引入ColorBench基准，评估视觉语言模型的颜色感知与推理能力。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了一个名为ColorBench的创新基准，用于评估视觉语言模型（VLMs）在颜色感知、推理和鲁棒性方面的能力。通过构建多样化的测试场景，ColorBench考察了这些模型如何处理颜色信息以及在不同颜色变换下的表现。对32种不同架构的VLMs进行广泛评估后发现，更大规模的语言模型比视觉编码器更重要，但现有模型在颜色理解上的性能差距较小，表明这一领域被忽视。此外，CoT推理可以提高颜色理解的准确性和鲁棒性，尽管这些任务本质上是视觉驱动的。虽然VLMs确实利用了颜色线索，但在某些任务中也可能产生误导。这些发现揭示了当前VLMs的关键局限性，并强调了提升颜色理解能力的重要性。ColorBench可作为多模态AI实现人类水平颜色理解的基础工具。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://arxiv.org/abs/2504.10514" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 12:36:26 GMT</pubDate>
</item>
<item>
<title>M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10449</link>
<guid>https://arxiv.org/abs/2504.10449</guid>
<content:encoded><![CDATA[
Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:38:25 GMT</pubDate>
</item>
<item>
<title>LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models</title>
<link>https://arxiv.org/abs/2504.10430</link>
<guid>https://arxiv.org/abs/2504.10430</guid>
<content:encoded><![CDATA[
Recent advancements in Large Language Models (LLMs) have enabled them to approach human-level persuasion capabilities. However, such potential also raises concerns about the safety risks of LLM-driven persuasion, particularly their potential for unethical influence through manipulation, deception, exploitation of vulnerabilities, and many other harmful tactics. In this work, we present a systematic investigation of LLM persuasion safety through two critical aspects: (1) whether LLMs appropriately reject unethical persuasion tasks and avoid unethical strategies during execution, including cases where the initial persuasion goal appears ethically neutral, and (2) how influencing factors like personality traits and external pressures affect their behavior. To this end, we introduce PersuSafety, the first comprehensive framework for the assessment of persuasion safety which consists of three stages, i.e., persuasion scene creation, persuasive conversation simulation, and persuasion safety assessment. PersuSafety covers 6 diverse unethical persuasion topics and 15 common unethical strategies. Through extensive experiments across 8 widely used LLMs, we observe significant safety concerns in most LLMs, including failing to identify harmful persuasion tasks and leveraging various unethical persuasion strategies. Our study calls for more attention to improve safety alignment in progressive and goal-driven conversations such as persuasion.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:20:34 GMT</pubDate>
</item>
<item>
<title>Breaking the Data Barrier -- Building GUI Agents Through Task Generalization</title>
<link>https://arxiv.org/abs/2504.10127</link>
<guid>https://arxiv.org/abs/2504.10127</guid>
<content:encoded><![CDATA[
Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 07:35:02 GMT</pubDate>
</item>
<item>
<title>Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems</title>
<link>https://arxiv.org/abs/2504.09763</link>
<guid>https://arxiv.org/abs/2504.09763</guid>
<content:encoded><![CDATA[
Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 20:06:48 GMT</pubDate>
</item>
<item>
<title>Iterative Self-Training for Code Generation via Reinforced Re-Ranking</title>
<link>https://arxiv.org/abs/2504.09643</link>
<guid>https://arxiv.org/abs/2504.09643</guid>
<content:encoded><![CDATA[
Generating high-quality code that solves complex programming tasks is challenging, especially with current decoder-based models that produce highly stochastic outputs. In code generation, even minor errors can easily break the entire solution. Leveraging multiple sampled solutions can significantly improve the overall output quality.   One effective way to enhance code generation is by pairing a code generation model with a reranker model, which selects the best solution from the generated samples. We propose a novel iterative self-training approach for self-training reranker models using Proximal Policy Optimization (PPO), aimed at improving both reranking accuracy and the overall code generation process. Unlike traditional PPO approaches, where the focus is on optimizing a generative model with a reward model, our approach emphasizes the development of a robust reward/reranking model. This model improves the quality of generated code through reranking and addresses problems and errors that the reward model might overlook during PPO alignment with the reranker. Our method iteratively refines the training dataset by re-evaluating outputs, identifying high-scoring negative examples, and incorporating them into the training loop, that boosting model performance.   Our evaluation on the MultiPL-E dataset demonstrates that our 13.4B parameter model outperforms a 33B model in code generation quality while being three times faster. Moreover, it achieves performance comparable to GPT-4 and surpasses it in one programming language.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:34:17 GMT</pubDate>
</item>
<item>
<title>How new data permeates LLM knowledge and how to dilute it</title>
<link>https://arxiv.org/abs/2504.09522</link>
<guid>https://arxiv.org/abs/2504.09522</guid>
<content:encoded><![CDATA[
Large language models learn and continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a "priming" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts. To systematically study this phenomenon, we introduce "Outlandish," a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before learning. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages. Finally, we develop two novel techniques to modulate how new knowledge affects existing model behavior: (1) a ``stepping-stone'' text augmentation strategy and (2) an ``ignore-k'' update pruning method. These approaches reduce undesirable priming effects by 50-95\% while preserving the model's ability to learn new information. Our findings provide both empirical insights into how LLMs learn and practical tools for improving the specificity of knowledge insertion in language models. Further materials: https://sunchipsster1.github.io/projects/outlandish/
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 07:25:04 GMT</pubDate>
</item>
<item>
<title>VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search</title>
<link>https://arxiv.org/abs/2504.09130</link>
<guid>https://arxiv.org/abs/2504.09130</guid>
<content:encoded><![CDATA[
Recent advancements in Large Vision-Language Models have showcased remarkable capabilities. However, they often falter when confronted with complex reasoning tasks that humans typically address through visual aids and deliberate, step-by-step thinking. While existing methods have explored text-based slow thinking or rudimentary visual assistance, they fall short of capturing the intricate, interleaved nature of human visual-verbal reasoning processes. To overcome these limitations and inspired by the mechanisms of slow thinking in human cognition, we introduce VisuoThink, a novel framework that seamlessly integrates visuospatial and linguistic domains. VisuoThink facilitates multimodal slow thinking by enabling progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. Extensive experiments demonstrate that VisuoThink significantly enhances reasoning capabilities via inference-time scaling, even without fine-tuning, achieving state-of-the-art performance in tasks involving geometry and spatial reasoning.
]]></content:encoded>
<pubDate>Sat, 12 Apr 2025 04:37:30 GMT</pubDate>
</item>
<item>
<title>The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</title>
<link>https://arxiv.org/abs/2504.08066</link>
<guid>https://arxiv.org/abs/2504.08066</guid>
<content:encoded><![CDATA[
AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 14:44:41 GMT</pubDate>
</item>
<item>
<title>InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models</title>
<link>https://arxiv.org/abs/2504.10479</link>
<guid>https://arxiv.org/abs/2504.10479</guid>
<content:encoded><![CDATA[
We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:59:25 GMT</pubDate>
</item>
<item>
<title>LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models</title>
<link>https://arxiv.org/abs/2504.10415</link>
<guid>https://arxiv.org/abs/2504.10415</guid>
<content:encoded><![CDATA[
Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 13:00:13 GMT</pubDate>
</item>
<item>
<title>S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10368</link>
<guid>https://arxiv.org/abs/2504.10368</guid>
<content:encoded><![CDATA[
We introduce S1-Bench, a novel benchmark designed to evaluate Large Reasoning Models' (LRMs) performance on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their reliance on deep analytical thinking may limit their system 1 thinking capabilities. Moreover, a lack of benchmark currently exists to evaluate LRMs' performance in tasks that require such capabilities. To fill this gap, S1-Bench presents a set of simple, diverse, and naturally clear questions across multiple domains and languages, specifically designed to assess LRMs' performance in such tasks. Our comprehensive evaluation of 22 LRMs reveals significant lower efficiency tendencies, with outputs averaging 15.5 times longer than those of traditional small LLMs. Additionally, LRMs often identify correct answers early but continue unnecessary deliberation, with some models even producing numerous errors. These findings highlight the rigid reasoning patterns of current LRMs and underscore the substantial development needed to achieve balanced dual-system thinking capabilities that can adapt appropriately to task complexity.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 12:13:23 GMT</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 08:12:52 GMT</pubDate>
</item>
<item>
<title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.10068</link>
<guid>https://arxiv.org/abs/2504.10068</guid>
<content:encoded><![CDATA[
Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose Mavors, a novel framework that introduces Multi-granularity video representation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 06:14:44 GMT</pubDate>
</item>
<item>
<title>FUSION: Fully Integration of Vision-Language Representations for Deep Cross-Modal Understanding</title>
<link>https://arxiv.org/abs/2504.09925</link>
<guid>https://arxiv.org/abs/2504.09925</guid>
<content:encoded><![CDATA[
We introduce FUSION, a family of multimodal large language models (MLLMs) with a fully vision-language alignment and integration paradigm. Unlike existing methods that primarily rely on late-stage modality interaction during LLM decoding, our approach achieves deep, dynamic integration throughout the entire processing pipeline. To this end, we propose Text-Guided Unified Vision Encoding, incorporating textual information in vision encoding to achieve pixel-level integration. We further design Context-Aware Recursive Alignment Decoding that recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained, question-level semantic integration. To guide feature mapping and mitigate modality discrepancies, we develop Dual-Supervised Semantic Mapping Loss. Additionally, we construct a Synthesized Language-Driven Question-Answer (QA) dataset through a new data synthesis method, prioritizing high-quality QA pairs to optimize text-guided feature integration. Building on these foundations, we train FUSION at two scales-3B, 8B-and demonstrate that our full-modality integration approach significantly outperforms existing methods with only 630 vision tokens. Notably, FUSION 3B surpasses Cambrian-1 8B and Florence-VL 8B on most benchmarks. FUSION 3B continues to outperform Cambrian-1 8B even when limited to 300 vision tokens. Our ablation studies show that FUSION outperforms LLaVA-NeXT on over half of the benchmarks under same configuration without dynamic resolution, highlighting the effectiveness of our approach. We release our code, model weights, and dataset. https://github.com/starriver030515/FUSION
]]></content:encoded>
<pubDate>Mon, 14 Apr 2025 02:33:29 GMT</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 14:47:22 GMT</pubDate>
</item>
<item>
<title>TinyLLaVA-Video-R1: Towards Smaller LMMs for Video Reasoning</title>
<link>https://arxiv.org/abs/2504.09641</link>
<guid>https://arxiv.org/abs/2504.09641</guid>
<content:encoded><![CDATA[
Recently, improving the reasoning ability of large multimodal models (LMMs) through reinforcement learning has made great progress. However, most existing works are based on highly reasoning-intensive datasets such as mathematics and code, and researchers generally choose large-scale models as the foundation. We argue that exploring small-scale models' reasoning capabilities remains valuable for researchers with limited computational resources. Moreover, enabling models to explain their reasoning processes on general question-answering datasets is equally meaningful. Therefore, we present the small-scale video reasoning model TinyLLaVA-Video-R1. Based on TinyLLaVA-Video, a traceably trained video understanding model with no more than 4B parameters, it not only demonstrates significantly improved reasoning and thinking capabilities after using reinforcement learning on general Video-QA datasets, but also exhibits the emergent characteristic of "aha moments". Furthermore, we share a series of experimental findings, aiming to provide practical insights for future exploration of video reasoning (thinking) abilities in small-scale models. It is available at https://github.com/ZhangXJ199/TinyLLaVA-Video-R1.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 12:32:49 GMT</pubDate>
</item>
<item>
<title>AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories</title>
<link>https://arxiv.org/abs/2504.08942</link>
<guid>https://arxiv.org/abs/2504.08942</guid>
<content:encoded><![CDATA[
Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 15:49:22 GMT</pubDate>
</item>
<item>
<title>VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08837</link>
<guid>https://arxiv.org/abs/2504.08837</guid>
<content:encoded><![CDATA[
Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:41:56 GMT</pubDate>
</item>
<item>
<title>Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability</title>
<link>https://arxiv.org/abs/2504.08003</link>
<guid>https://arxiv.org/abs/2504.08003</guid>
<content:encoded><![CDATA[
OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 12:10:15 GMT</pubDate>
</item>
<item>
<title>PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters</title>
<link>https://arxiv.org/abs/2504.08791</link>
<guid>https://arxiv.org/abs/2504.08791</guid>
<content:encoded><![CDATA[
Emergency of DeepSeek R1 and QwQ 32B have broken through performance barriers for running frontier large language models (LLMs) on home devices. While consumer hardware is getting stronger and model quantization is improving, existing end-side solutions still demand GPU clusters, large RAM/VRAM, and high bandwidth, far beyond what a common home cluster can handle. This paper introduces prima.cpp, a distributed inference system that runs 70B-scale models on everyday home devices using a mix of CPU/GPU, low RAM/VRAM, Wi-Fi, and cross-platform support. It uses mmap to manage model weights and introduces piped-ring parallelism with prefetching to hide disk loading. By modeling heterogeneity in computation, communication, disk, memory (and its management behavior), and OS, it optimally assigns model layers to each device's CPU and GPU, further reducing token latency. An elegant algorithm named Halda is proposed to solve this NP-hard assignment problem. We evaluate prima.cpp on a common four-node home cluster. It outperforms llama.cpp, exo, and dllama on 30B+ models while keeping memory pressure below 6%. This brings frontier 30B-70B models, such as Llama 3, DeepSeek R1, Qwen 2.5, and QwQ to home assistants, making advanced AI truly accessible to individuals. The code is open source and available at https://github.com/Lizonghang/prima.cpp.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 09:46:21 GMT</pubDate>
</item>
<item>
<title>DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training</title>
<link>https://arxiv.org/abs/2504.09710</link>
<guid>https://arxiv.org/abs/2504.09710</guid>
<content:encoded><![CDATA[
Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https://github.com/ZhentingWang/DUMP.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2025 16:10:27 GMT</pubDate>
</item>
<item>
<title>BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing</title>
<link>https://arxiv.org/abs/2504.01786</link>
<guid>https://arxiv.org/abs/2504.01786</guid>
<content:encoded><![CDATA[
3D graphics editing is crucial in applications like movie production and game design, yet it remains a time-consuming process that demands highly specialized domain expertise. Automating this process is challenging because graphical editing requires performing a variety of tasks, each requiring distinct skill sets. Recently, vision-language models (VLMs) have emerged as a powerful framework for automating the editing process, but their development and evaluation are bottlenecked by the lack of a comprehensive benchmark that requires human-level perception and presents real-world editing complexity. In this work, we present BlenderGym, the first comprehensive VLM system benchmark for 3D graphics editing. BlenderGym evaluates VLM systems through code-based 3D reconstruction tasks. We evaluate closed- and open-source VLM systems and observe that even the state-of-the-art VLM system struggles with tasks relatively easy for human Blender users. Enabled by BlenderGym, we study how inference scaling techniques impact VLM's performance on graphics editing tasks. Notably, our findings reveal that the verifier used to guide the scaling of generation can itself be improved through inference scaling, complementing recent insights on inference scaling of LLM generation in coding and math tasks. We further show that inference compute is not uniformly effective and can be optimized by strategically distributing it between generation and verification.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 10:51:45 GMT</pubDate>
</item>
<item>
<title>UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2504.06908</link>
<guid>https://arxiv.org/abs/2504.06908</guid>
<content:encoded><![CDATA[
In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:10:51 GMT</pubDate>
</item>
<item>
<title>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</title>
<link>https://arxiv.org/abs/2504.08727</link>
<guid>https://arxiv.org/abs/2504.08727</guid>
<content:encoded><![CDATA[
We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:55:45 GMT</pubDate>
</item>
<item>
<title>Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization</title>
<link>https://arxiv.org/abs/2504.08641</link>
<guid>https://arxiv.org/abs/2504.08641</guid>
<content:encoded><![CDATA[
Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:41:43 GMT</pubDate>
</item>
<item>
<title>ZipIR: Latent Pyramid Diffusion Transformer for High-Resolution Image Restoration</title>
<link>https://arxiv.org/abs/2504.08591</link>
<guid>https://arxiv.org/abs/2504.08591</guid>
<content:encoded><![CDATA[
Recent progress in generative models has significantly improved image restoration capabilities, particularly through powerful diffusion models that offer remarkable recovery of semantic details and local fidelity. However, deploying these models at ultra-high resolutions faces a critical trade-off between quality and efficiency due to the computational demands of long-range attention mechanisms. To address this, we introduce ZipIR, a novel framework that enhances efficiency, scalability, and long-range modeling for high-res image restoration. ZipIR employs a highly compressed latent representation that compresses image 32x, effectively reducing the number of spatial tokens, and enabling the use of high-capacity models like the Diffusion Transformer (DiT). Toward this goal, we propose a Latent Pyramid VAE (LP-VAE) design that structures the latent space into sub-bands to ease diffusion training. Trained on full images up to 2K resolution, ZipIR surpasses existing diffusion-based methods, offering unmatched speed and quality in restoring high-resolution images from severely degraded inputs.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 10:49:52 GMT</pubDate>
</item>
<item>
<title>VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.07615</link>
<guid>https://arxiv.org/abs/2504.07615</guid>
<content:encoded><![CDATA[
Recently DeepSeek R1 has shown that reinforcement learning (RL) can substantially improve the reasoning capabilities of Large Language Models (LLMs) through a simple yet effective design. The core of R1 lies in its rule-based reward formulation, which leverages tasks with deterministic ground-truth answers to enable precise and stable reward computation. In the visual domain, we similarly observe that a wide range of visual understanding tasks are inherently equipped with well-defined ground-truth annotations. This property makes them naturally compatible with rule-based reward mechanisms. Motivated by this observation, we investigate the extension of R1-style reinforcement learning to Vision-Language Models (VLMs), aiming to enhance their visual reasoning capabilities. To this end, we develop VLM-R1, a dedicated framework designed to harness RL for improving VLMs' performance on general vision-language tasks. Using this framework, we further explore the feasibility of applying RL to visual domain. Experimental results indicate that the RL-based model not only delivers competitive performance on visual understanding tasks but also surpasses Supervised Fine-Tuning (SFT) in generalization ability. Furthermore, we conduct comprehensive ablation studies that uncover a series of noteworthy insights, including the presence of reward hacking in object detection, the emergence of the "OD aha moment", the impact of training data quality, and the scaling behavior of RL across different model sizes. Through these analyses, we aim to deepen the understanding of how reinforcement learning enhances the capabilities of vision-language models, and we hope our findings and open-source contributions will support continued progress in the vision-language RL community. Our code and model are available at https://github.com/om-ai-lab/VLM-R1
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 06:05:15 GMT</pubDate>
</item>
<item>
<title>Latent Diffusion Autoencoders: Toward Efficient and Meaningful Unsupervised Representation Learning in Medical Imaging</title>
<link>https://arxiv.org/abs/2504.08635</link>
<guid>https://arxiv.org/abs/2504.08635</guid>
<content:encoded><![CDATA[
This study presents Latent Diffusion Autoencoder (LDAE), a novel encoder-decoder diffusion-based framework for efficient and meaningful unsupervised learning in medical imaging, focusing on Alzheimer disease (AD) using brain MR from the ADNI database as a case study. Unlike conventional diffusion autoencoders operating in image space, LDAE applies the diffusion process in a compressed latent representation, improving computational efficiency and making 3D medical imaging representation learning tractable. To validate the proposed approach, we explore two key hypotheses: (i) LDAE effectively captures meaningful semantic representations on 3D brain MR associated with AD and ageing, and (ii) LDAE achieves high-quality image generation and reconstruction while being computationally efficient. Experimental results support both hypotheses: (i) linear-probe evaluations demonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%) and age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic representations enable attribute manipulation, yielding anatomically plausible modifications; (iii) semantic interpolation experiments show strong reconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month gap. Even for longer gaps (24 months), the model maintains robust performance (SSIM &gt; 0.93, MSE &lt; 0.004), indicating an ability to capture temporal progression trends; (iv) compared to conventional diffusion autoencoders, LDAE significantly increases inference throughput (20x faster) while also enhancing reconstruction quality. These findings position LDAE as a promising framework for scalable medical imaging applications, with the potential to serve as a foundation model for medical image analysis. Code available at https://github.com/GabrieleLozupone/LDAE
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:37:46 GMT</pubDate>
</item>
<item>
<title>Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</title>
<link>https://arxiv.org/abs/2504.07866</link>
<guid>https://arxiv.org/abs/2504.07866</guid>
<content:encoded><![CDATA[
We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:41:51 GMT</pubDate>
</item>
<item>
<title>SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs</title>
<link>https://arxiv.org/abs/2504.08192</link>
<guid>https://arxiv.org/abs/2504.08192</guid>
<content:encoded><![CDATA[
Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce Dynamic DAE Guardrails (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 21:24:03 GMT</pubDate>
</item>
<item>
<title>InteractVLM: 3D Interaction Reasoning from 2D Foundational Models</title>
<link>https://arxiv.org/abs/2504.05303</link>
<guid>https://arxiv.org/abs/2504.05303</guid>
<content:encoded><![CDATA[
We introduce InteractVLM, a novel method to estimate 3D contact points on human bodies and objects from single in-the-wild images, enabling accurate human-object joint reconstruction in 3D. This is challenging due to occlusions, depth ambiguities, and widely varying object shapes. Existing methods rely on 3D contact annotations collected via expensive motion-capture systems or tedious manual labeling, limiting scalability and generalization. To overcome this, InteractVLM harnesses the broad visual knowledge of large Vision-Language Models (VLMs), fine-tuned with limited 3D contact data. However, directly applying these models is non-trivial, as they reason only in 2D, while human-object contact is inherently 3D. Thus we introduce a novel Render-Localize-Lift module that: (1) embeds 3D body and object surfaces in 2D space via multi-view rendering, (2) trains a novel multi-view localization model (MV-Loc) to infer contacts in 2D, and (3) lifts these to 3D. Additionally, we propose a new task called Semantic Human Contact estimation, where human contact predictions are conditioned explicitly on object semantics, enabling richer interaction modeling. InteractVLM outperforms existing work on contact estimation and also facilitates 3D reconstruction from an in-the wild image. Code and models are available at https://interactvlm.is.tue.mpg.de.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:33 GMT</pubDate>
</item>
<item>
<title>Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models</title>
<link>https://arxiv.org/abs/2504.05262</link>
<guid>https://arxiv.org/abs/2504.05262</guid>
<content:encoded><![CDATA[
Despite high benchmark scores, Large Language Models (LLMs) often fail simple problem, raising a critical question: Do LLMs learn mathematical principles or merely memorize patterns? Rather than designing increasingly complex benchmarks like recent works, we investigate this using elementary two-integer addition (0 to 2^{64}), probing two core properties: commutativity (A+B=B+A) and compositional generalization (via isomorphic symbolic mappings, e.g., 7 rightarrow y). While state-of-the-art LLMs achieve 73.8-99.8\% accuracy on numerical addition, performance collapses to leq7.5\% under symbolic mapping, indicating failure to generalize learned rules. Non-monotonic performance scaling with digit count and frequent commutativity violations (over 1,700 cases of A+B neq B+A) further support this. Explicitly providing addition rules degrades performance by 81.2\% on average, while self-explanation maintains baseline accuracy, suggesting LLM arithmetic processing is misaligned with human-defined principles. Our findings indicate current LLMs rely on memory pattern over genuine rule learning, highlighting architectural limitations and the need for new approaches to achieve true mathematical reasoning.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 12:57:10 GMT</pubDate>
</item>
<item>
<title>CoRAG: Collaborative Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.01883</link>
<guid>https://arxiv.org/abs/2504.01883</guid>
<content:encoded><![CDATA[
Retrieval-Augmented Generation (RAG) models excel in knowledge-intensive tasks, especially under few-shot learning constraints. We introduce CoRAG, a framework extending RAG to collaborative settings, where clients jointly train a shared model using a collaborative passage store. To evaluate CoRAG, we introduce CRAB, a benchmark for collaborative homogeneous open-domain question answering. Our experiments demonstrate that CoRAG consistently outperforms both parametric collaborative learning methods and locally trained RAG models in low-resource scenarios. Further analysis reveals the critical importance of relevant passages within the shared store, the surprising benefits of incorporating irrelevant passages, and the potential for hard negatives to negatively impact performance. This introduces a novel consideration in collaborative RAG: the trade-off between leveraging a collectively enriched knowledge base and the potential risk of incorporating detrimental passages from other clients. Our findings underscore the viability of CoRAG, while also highlighting key design challenges and promising avenues for future research.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 12:40:43 GMT</pubDate>
</item>
<item>
<title>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2504.08736</link>
<guid>https://arxiv.org/abs/2504.08736</guid>
<content:encoded><![CDATA[
In autoregressive (AR) image generation, visual tokenizers compress images into compact discrete latent tokens, enabling efficient training of downstream autoregressive models for visual generation via next-token prediction. While scaling visual tokenizers improves image reconstruction quality, it often degrades downstream generation quality -- a challenge not adequately addressed in existing literature. To address this, we introduce GigaTok, the first approach to simultaneously improve image reconstruction, generation, and representation learning when scaling visual tokenizers. We identify the growing complexity of latent space as the key factor behind the reconstruction vs. generation dilemma. To mitigate this, we propose semantic regularization, which aligns tokenizer features with semantically consistent features from a pre-trained visual encoder. This constraint prevents excessive latent space complexity during scaling, yielding consistent improvements in both reconstruction and downstream autoregressive generation. Building on semantic regularization, we explore three key practices for scaling tokenizers:(1) using 1D tokenizers for better scalability, (2) prioritizing decoder scaling when expanding both encoder and decoder, and (3) employing entropy loss to stabilize training for billion-scale tokenizers. By scaling to 3 space billion parameters, GigaTok achieves state-of-the-art performance in reconstruction, downstream AR generation, and downstream AR representation quality.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:59:58 GMT</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 13:29:35 GMT</pubDate>
</item>
<item>
<title>SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.08600</link>
<guid>https://arxiv.org/abs/2504.08600</guid>
<content:encoded><![CDATA[
Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 11:01:30 GMT</pubDate>
</item>
<item>
<title>PixelFlow: Pixel-Space Generative Models with Flow</title>
<link>https://arxiv.org/abs/2504.07963</link>
<guid>https://arxiv.org/abs/2504.07963</guid>
<content:encoded><![CDATA[
We present PixelFlow, a family of image generation models that operate directly in the raw pixel space, in contrast to the predominant latent-space models. This approach simplifies the image generation process by eliminating the need for a pre-trained Variational Autoencoder (VAE) and enabling the whole model end-to-end trainable. Through efficient cascade flow modeling, PixelFlow achieves affordable computation cost in pixel space. It achieves an FID of 1.98 on 256times256 ImageNet class-conditional image generation benchmark. The qualitative text-to-image results demonstrate that PixelFlow excels in image quality, artistry, and semantic control. We hope this new paradigm will inspire and open up new opportunities for next-generation visual generation models. Code and models are available at https://github.com/ShoufaChen/PixelFlow.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</title>
<link>https://arxiv.org/abs/2504.08685</link>
<guid>https://arxiv.org/abs/2504.08685</guid>
<content:encoded><![CDATA[
This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 12:46:20 GMT</pubDate>
</item>
<item>
<title>MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft</title>
<link>https://arxiv.org/abs/2504.08388</link>
<guid>https://arxiv.org/abs/2504.08388</guid>
<content:encoded><![CDATA[
World modeling is a crucial task for enabling intelligent agents to effectively interact with humans and operate in dynamic environments. In this work, we propose MineWorld, a real-time interactive world model on Minecraft, an open-ended sandbox game which has been utilized as a common testbed for world modeling. MineWorld is driven by a visual-action autoregressive Transformer, which takes paired game scenes and corresponding actions as input, and generates consequent new scenes following the actions. Specifically, by transforming visual game scenes and actions into discrete token ids with an image tokenizer and an action tokenizer correspondingly, we consist the model input with the concatenation of the two kinds of ids interleaved. The model is then trained with next token prediction to learn rich representations of game states as well as the conditions between states and actions simultaneously. In inference, we develop a novel parallel decoding algorithm that predicts the spatial redundant tokens in each frame at the same time, letting models in different scales generate 4 to 7 frames per second and enabling real-time interactions with game players. In evaluation, we propose new metrics to assess not only visual quality but also the action following capacity when generating new scenes, which is crucial for a world model. Our comprehensive evaluation shows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion based world models significantly. The code and model have been released.
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:41:04 GMT</pubDate>
</item>
<item>
<title>In-2-4D: Inbetweening from Two Single-View Images to 4D Generation</title>
<link>https://arxiv.org/abs/2504.08366</link>
<guid>https://arxiv.org/abs/2504.08366</guid>
<content:encoded><![CDATA[
We propose a new problem, In-2-4D, for generative 4D (i.e., 3D + motion) inbetweening from a minimalistic input setting: two single-view images capturing an object in two distinct motion states. Given two images representing the start and end states of an object in motion, our goal is to generate and reconstruct the motion in 4D. We utilize a video interpolation model to predict the motion, but large frame-to-frame motions can lead to ambiguous interpretations. To overcome this, we employ a hierarchical approach to identify keyframes that are visually close to the input states and show significant motion, then generate smooth fragments between them. For each fragment, we construct the 3D representation of the keyframe using Gaussian Splatting. The temporal frames within the fragment guide the motion, enabling their transformation into dynamic Gaussians through a deformation field. To improve temporal consistency and refine 3D motion, we expand the self-attention of multi-view diffusion across timesteps and apply rigid transformation regularization. Finally, we merge the independently generated 3D motion segments by interpolating boundary deformation fields and optimizing them to align with the guiding video, ensuring smooth and flicker-free transitions. Through extensive qualitative and quantitiave experiments as well as a user study, we show the effectiveness of our method and its components. The project page is available at https://in-2-4d.github.io/
]]></content:encoded>
<pubDate>Fri, 11 Apr 2025 05:01:09 GMT</pubDate>
</item>
<item>
<title>FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation</title>
<link>https://arxiv.org/abs/2504.07405</link>
<guid>https://arxiv.org/abs/2504.07405</guid>
<content:encoded><![CDATA[
With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 22:58:22 GMT</pubDate>
</item>
<item>
<title>Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2504.07961</link>
<guid>https://arxiv.org/abs/2504.07961</guid>
<content:encoded><![CDATA[
We introduce Geo4D, a method to repurpose video diffusion models for monocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic prior captured by such video models, Geo4D can be trained using only synthetic data while generalizing well to real data in a zero-shot manner. Geo4D predicts several complementary geometric modalities, namely point, depth, and ray maps. It uses a new multi-modal alignment algorithm to align and fuse these modalities, as well as multiple sliding windows, at inference time, thus obtaining robust and accurate 4D reconstruction of long videos. Extensive experiments across multiple benchmarks show that Geo4D significantly surpasses state-of-the-art video depth estimation methods, including recent methods such as MonST3R, which are also designed to handle dynamic scenes.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:55 GMT</pubDate>
</item>
<item>
<title>Scaling Laws for Native Multimodal Models Scaling Laws for Native Multimodal Models</title>
<link>https://arxiv.org/abs/2504.07951</link>
<guid>https://arxiv.org/abs/2504.07951</guid>
<content:encoded><![CDATA[
Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)--those trained from the ground up on all modalities--and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows for models that learn modality-specific weights, significantly enhancing performance.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:57:28 GMT</pubDate>
</item>
<item>
<title>TAPNext: Tracking Any Point (TAP) as Next Token Prediction</title>
<link>https://arxiv.org/abs/2504.05579</link>
<guid>https://arxiv.org/abs/2504.05579</guid>
<content:encoded><![CDATA[
Tracking Any Point (TAP) in a video is a challenging computer vision problem with many demonstrated applications in robotics, video editing, and 3D reconstruction. Existing methods for TAP rely heavily on complex tracking-specific inductive biases and heuristics, limiting their generality and potential for scaling. To address these challenges, we present TAPNext, a new approach that casts TAP as sequential masked token decoding. Our model is causal, tracks in a purely online fashion, and removes tracking-specific inductive biases. This enables TAPNext to run with minimal latency, and removes the temporal windowing required by many existing state of art trackers. Despite its simplicity, TAPNext achieves a new state-of-the-art tracking performance among both online and offline trackers. Finally, we present evidence that many widely used tracking heuristics emerge naturally in TAPNext through end-to-end training.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 20:28:42 GMT</pubDate>
</item>
<item>
<title>MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection</title>
<link>https://arxiv.org/abs/2504.06801</link>
<guid>https://arxiv.org/abs/2504.06801</guid>
<content:encoded><![CDATA[
Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 07:47:48 GMT</pubDate>
</item>
<item>
<title>Compass Control: Multi Object Orientation Control for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2504.06752</link>
<guid>https://arxiv.org/abs/2504.06752</guid>
<content:encoded><![CDATA[
Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware compass tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 06:15:15 GMT</pubDate>
</item>
<item>
<title>C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing</title>
<link>https://arxiv.org/abs/2504.07964</link>
<guid>https://arxiv.org/abs/2504.07964</guid>
<content:encoded><![CDATA[
Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or "re-mixing" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's "successful neighbors" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to "Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:56 GMT</pubDate>
</item>
<item>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.07960</link>
<guid>https://arxiv.org/abs/2504.07960</guid>
<content:encoded><![CDATA[
Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:42 GMT</pubDate>
</item>
<item>
<title>MM-IFEngine: Towards Multimodal Instruction Following</title>
<link>https://arxiv.org/abs/2504.07957</link>
<guid>https://arxiv.org/abs/2504.07957</guid>
<content:encoded><![CDATA[
The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2%), MIA (+7.6%), and IFEval (+12.3%). The full data and evaluation code will be released on https://github.com/SYuan03/MM-IFEngine.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:12 GMT</pubDate>
</item>
<item>
<title>VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2504.07956</link>
<guid>https://arxiv.org/abs/2504.07956</guid>
<content:encoded><![CDATA[
The advancement of Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs) and large vision-language models (LVLMs). However, a rigorous evaluation framework for video CoT reasoning remains absent. Current video benchmarks fail to adequately assess the reasoning process and expose whether failures stem from deficiencies in perception or reasoning capabilities. Therefore, we introduce VCR-Bench, a novel benchmark designed to comprehensively evaluate LVLMs' Video Chain-of-Thought Reasoning capabilities. VCR-Bench comprises 859 videos spanning a variety of video content and durations, along with 1,034 high-quality question-answer pairs. Each pair is manually annotated with a stepwise CoT rationale, where every step is tagged to indicate its association with the perception or reasoning capabilities. Furthermore, we design seven distinct task dimensions and propose the CoT score to assess the entire CoT process based on the stepwise tagged CoT rationals. Extensive experiments on VCR-Bench highlight substantial limitations in current LVLMs. Even the top-performing model, o1, only achieves a 62.8% CoT score and an 56.7% accuracy, while most models score below 40%. Experiments show most models score lower on perception than reasoning steps, revealing LVLMs' key bottleneck in temporal-spatial information processing for complex video reasoning. A robust positive correlation between the CoT score and accuracy confirms the validity of our evaluation framework and underscores the critical role of CoT reasoning in solving complex video reasoning tasks. We hope VCR-Bench to serve as a standardized evaluation framework and expose the actual drawbacks in complex video reasoning task.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:59:03 GMT</pubDate>
</item>
<item>
<title>HoloPart: Generative 3D Part Amodal Segmentation</title>
<link>https://arxiv.org/abs/2504.07943</link>
<guid>https://arxiv.org/abs/2504.07943</guid>
<content:encoded><![CDATA[
3D part amodal segmentation--decomposing a 3D shape into complete, semantically meaningful parts, even when occluded--is a challenging but crucial task for 3D content creation and understanding. Existing 3D part segmentation methods only identify visible surface patches, limiting their utility. Inspired by 2D amodal segmentation, we introduce this novel task to the 3D domain and propose a practical, two-stage approach, addressing the key challenges of inferring occluded 3D geometry, maintaining global shape consistency, and handling diverse shapes with limited training data. First, we leverage existing 3D part segmentation to obtain initial, incomplete part segments. Second, we introduce HoloPart, a novel diffusion-based model, to complete these segments into full 3D parts. HoloPart utilizes a specialized architecture with local attention to capture fine-grained part geometry and global shape context attention to ensure overall shape consistency. We introduce new benchmarks based on the ABO and PartObjaverse-Tiny datasets and demonstrate that HoloPart significantly outperforms state-of-the-art shape completion methods. By incorporating HoloPart with existing segmentation techniques, we achieve promising results on 3D part amodal segmentation, opening new avenues for applications in geometry editing, animation, and material assignment.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:53:31 GMT</pubDate>
</item>
<item>
<title>SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement</title>
<link>https://arxiv.org/abs/2504.07934</link>
<guid>https://arxiv.org/abs/2504.07934</guid>
<content:encoded><![CDATA[
In this paper, we present an effective method to enhance visual reasoning with significantly fewer training samples, relying purely on self-improvement with no knowledge distillation. Our key insight is that the difficulty of training data during reinforcement fine-tuning (RFT) is critical. Appropriately challenging samples can substantially boost reasoning capabilities even when the dataset is small. Despite being intuitive, the main challenge remains in accurately quantifying sample difficulty to enable effective data filtering. To this end, we propose a novel way of repurposing Monte Carlo Tree Search (MCTS) to achieve that. Starting from our curated 70k open-source training samples, we introduce an MCTS-based selection method that quantifies sample difficulty based on the number of iterations required by the VLMs to solve each problem. This explicit step-by-step reasoning in MCTS enforces the model to think longer and better identifies samples that are genuinely challenging. We filter and retain 11k samples to perform RFT on Qwen2.5-VL-7B-Instruct, resulting in our final model, ThinkLite-VL. Evaluation results on eight benchmarks show that ThinkLite-VL improves the average performance of Qwen2.5-VL-7B-Instruct by 7%, using only 11k training samples with no knowledge distillation. This significantly outperforms all existing 7B-level reasoning VLMs, and our fairly comparable baselines that use classic selection methods such as accuracy-based filtering. Notably, on MathVista, ThinkLite-VL-7B achieves the SoTA accuracy of 75.1, surpassing Qwen2.5-VL-72B, GPT-4o, and O1. Our code, data, and model are available at https://github.com/si0wang/ThinkLite-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 13:49:05 GMT</pubDate>
</item>
<item>
<title>MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations</title>
<link>https://arxiv.org/abs/2504.07830</link>
<guid>https://arxiv.org/abs/2504.07830</guid>
<content:encoded><![CDATA[
We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 11:06:54 GMT</pubDate>
</item>
<item>
<title>Kimi-VL Technical Report</title>
<link>https://arxiv.org/abs/2504.07491</link>
<guid>https://arxiv.org/abs/2504.07491</guid>
<content:encoded><![CDATA[
We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.
]]></content:encoded>
<pubDate>Thu, 10 Apr 2025 02:48:26 GMT</pubDate>
</item>
<item>
<title>Towards Visual Text Grounding of Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.04974</link>
<guid>https://arxiv.org/abs/2504.04974</guid>
<content:encoded><![CDATA[
Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 08:01:59 GMT</pubDate>
</item>
<item>
<title>DeepSeek-R1 Thoughtology: Let's  about LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.07128</link>
<guid>https://arxiv.org/abs/2504.07128</guid>
<content:encoded><![CDATA[
Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 01 Apr 2025 20:36:08 GMT</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed 100,000 tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 14:30:18 GMT</pubDate>
</item>
<item>
<title>Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting</title>
<link>https://arxiv.org/abs/2504.05541</link>
<guid>https://arxiv.org/abs/2504.05541</guid>
<content:encoded><![CDATA[
We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:35:36 GMT</pubDate>
</item>
<item>
<title>Pretraining Language Models for Diachronic Linguistic Change Discovery</title>
<link>https://arxiv.org/abs/2504.05523</link>
<guid>https://arxiv.org/abs/2504.05523</guid>
<content:encoded><![CDATA[
Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.   We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for "typical" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned.   We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:51:32 GMT</pubDate>
</item>
<item>
<title>Are We Done with Object-Centric Learning?</title>
<link>https://arxiv.org/abs/2504.07092</link>
<guid>https://arxiv.org/abs/2504.07092</guid>
<content:encoded><![CDATA[
Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available https://github.com/AlexanderRubinstein/OCCAM{here}.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:05 GMT</pubDate>
</item>
<item>
<title>Self-Steering Language Models</title>
<link>https://arxiv.org/abs/2504.07081</link>
<guid>https://arxiv.org/abs/2504.07081</guid>
<content:encoded><![CDATA[
While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:54:22 GMT</pubDate>
</item>
<item>
<title>A Unified Agentic Framework for Evaluating Conditional Image Generation</title>
<link>https://arxiv.org/abs/2504.07046</link>
<guid>https://arxiv.org/abs/2504.07046</guid>
<content:encoded><![CDATA[
Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:04:14 GMT</pubDate>
</item>
<item>
<title>VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.06958</link>
<guid>https://arxiv.org/abs/2504.06958</guid>
<content:encoded><![CDATA[
Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 11:09:27 GMT</pubDate>
</item>
<item>
<title>RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts</title>
<link>https://arxiv.org/abs/2504.06947</link>
<guid>https://arxiv.org/abs/2504.06947</guid>
<content:encoded><![CDATA[
In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 10:54:00 GMT</pubDate>
</item>
<item>
<title>Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.06719</link>
<guid>https://arxiv.org/abs/2504.06719</guid>
<content:encoded><![CDATA[
Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 05:19:49 GMT</pubDate>
</item>
<item>
<title>WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments</title>
<link>https://arxiv.org/abs/2504.03886</link>
<guid>https://arxiv.org/abs/2504.03886</guid>
<content:encoded><![CDATA[
We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system designed to handle dynamic environments by leveraging uncertainty-aware geometric mapping. Unlike traditional SLAM systems, which assume static scenes, our approach integrates depth and uncertainty information to enhance tracking, mapping, and rendering performance in the presence of moving objects. We introduce an uncertainty map, predicted by a shallow multi-layer perceptron and DINOv2 features, to guide dynamic object removal during both tracking and mapping. This uncertainty map enhances dense bundle adjustment and Gaussian map optimization, improving reconstruction accuracy. Our system is evaluated on multiple datasets and demonstrates artifact-free view synthesis. Results showcase WildGS-SLAM's superior performance in dynamic environments compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 15:19:40 GMT</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:58 GMT</pubDate>
</item>
<item>
<title>A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility</title>
<link>https://arxiv.org/abs/2504.07086</link>
<guid>https://arxiv.org/abs/2504.07086</guid>
<content:encoded><![CDATA[
Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:58:17 GMT</pubDate>
</item>
<item>
<title>RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception</title>
<link>https://arxiv.org/abs/2504.05287</link>
<guid>https://arxiv.org/abs/2504.05287</guid>
<content:encoded><![CDATA[
Robust grasping of various objects from single-view perception is fundamental for dexterous robots. Previous works often rely on fully observable objects, expert demonstrations, or static grasping poses, which restrict their generalization ability and adaptability to external disturbances. In this paper, we present a reinforcement-learning-based framework that enables zero-shot dynamic dexterous grasping of a wide range of unseen objects from single-view perception, while performing adaptive motions to external disturbances. We utilize a hand-centric object representation for shape feature extraction that emphasizes interaction-relevant local shapes, enhancing robustness to shape variance and uncertainty. To enable effective hand adaptation to disturbances with limited observations, we propose a mixed curriculum learning strategy, which first utilizes imitation learning to distill a policy trained with privileged real-time visual-tactile feedback, and gradually transfers to reinforcement learning to learn adaptive motions under disturbances caused by observation noises and dynamic randomization. Our experiments demonstrate strong generalization in grasping unseen objects with random poses, achieving success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects. We also demonstrate the robustness of our method to various disturbances, including unobserved object movement and external forces, through both quantitative and qualitative evaluations. Project Page: https://zdchan.github.io/Robust_DexGrasp/
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:38:19 GMT</pubDate>
</item>
<item>
<title>DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion</title>
<link>https://arxiv.org/abs/2504.04010</link>
<guid>https://arxiv.org/abs/2504.04010</guid>
<content:encoded><![CDATA[
Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 21:19:46 GMT</pubDate>
</item>
<item>
<title>OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens</title>
<link>https://arxiv.org/abs/2504.07096</link>
<guid>https://arxiv.org/abs/2504.07096</guid>
<content:encoded><![CDATA[
We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:59:35 GMT</pubDate>
</item>
<item>
<title>GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography</title>
<link>https://arxiv.org/abs/2504.07083</link>
<guid>https://arxiv.org/abs/2504.07083</guid>
<content:encoded><![CDATA[
Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.
]]></content:encoded>
<pubDate>Wed, 09 Apr 2025 13:56:01 GMT</pubDate>
</item>
<item>
<title>Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?</title>
<link>https://arxiv.org/abs/2504.06514</link>
<guid>https://arxiv.org/abs/2504.06514</guid>
<content:encoded><![CDATA[
We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 21:25:27 GMT</pubDate>
</item>
<item>
<title>FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis</title>
<link>https://arxiv.org/abs/2504.04842</link>
<guid>https://arxiv.org/abs/2504.04842</guid>
<content:encoded><![CDATA[
Creating a realistic animatable avatar from a single static portrait remains challenging. Existing approaches often struggle to capture subtle facial expressions, the associated global body movements, and the dynamic background. To address these limitations, we propose a novel framework that leverages a pretrained video diffusion transformer model to generate high-fidelity, coherent talking portraits with controllable motion dynamics. At the core of our work is a dual-stage audio-visual alignment strategy. In the first stage, we employ a clip-level training scheme to establish coherent global motion by aligning audio-driven dynamics across the entire scene, including the reference portrait, contextual objects, and background. In the second stage, we refine lip movements at the frame level using a lip-tracing mask, ensuring precise synchronization with audio signals. To preserve identity without compromising motion flexibility, we replace the commonly used reference network with a facial-focused cross-attention module that effectively maintains facial consistency throughout the video. Furthermore, we integrate a motion intensity modulation module that explicitly controls expression and body motion intensity, enabling controllable manipulation of portrait movements beyond mere lip motion. Extensive experimental results show that our proposed approach achieves higher quality with better realism, coherence, motion intensity, and identity preservation. Ours project page: https://fantasy-amap.github.io/fantasy-talking/.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 04:56:01 GMT</pubDate>
</item>
<item>
<title>DDT: Decoupled Diffusion Transformer</title>
<link>https://arxiv.org/abs/2504.05741</link>
<guid>https://arxiv.org/abs/2504.05741</guid>
<content:encoded><![CDATA[
Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \color{ddtD}ecoupled \color{ddtD}iffusion \color{ddtT}ransformer~(\color{ddtDDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet 256times256, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly 4times faster training convergence compared to previous diffusion transformers). For ImageNet 512times512, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 03:17:45 GMT</pubDate>
</item>
<item>
<title>HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference</title>
<link>https://arxiv.org/abs/2504.05897</link>
<guid>https://arxiv.org/abs/2504.05897</guid>
<content:encoded><![CDATA[
The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33times in the prefill stage and 1.70times in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 06:47:37 GMT</pubDate>
</item>
<item>
<title>Efficient Reinforcement Finetuning via Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.05520</link>
<guid>https://arxiv.org/abs/2504.05520</guid>
<content:encoded><![CDATA[
Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 17:31:31 GMT</pubDate>
</item>
<item>
<title>ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2504.03755</link>
<guid>https://arxiv.org/abs/2504.03755</guid>
<content:encoded><![CDATA[
Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 02:13:14 GMT</pubDate>
</item>
<item>
<title>Leanabell-Prover: Posttraining Scaling in Formal Reasoning</title>
<link>https://arxiv.org/abs/2504.06122</link>
<guid>https://arxiv.org/abs/2504.06122</guid>
<content:encoded><![CDATA[
Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages.To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:15:26 GMT</pubDate>
</item>
<item>
<title>HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance</title>
<link>https://arxiv.org/abs/2504.06232</link>
<guid>https://arxiv.org/abs/2504.06232</guid>
<content:encoded><![CDATA[
Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:30:40 GMT</pubDate>
</item>
<item>
<title>V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2504.06148</link>
<guid>https://arxiv.org/abs/2504.06148</guid>
<content:encoded><![CDATA[
Recent advancements in Multimodal Large Language Models (MLLMs) have led to significant improvements across various multimodal benchmarks. However, as evaluations shift from static datasets to open-world, dynamic environments, current game-based benchmarks remain inadequate because they lack visual-centric tasks and fail to assess the diverse reasoning skills required for real-world decision-making. To address this, we introduce Visual-centric Multiple Abilities Game Evaluation (V-MAGE), a game-based evaluation framework designed to assess visual reasoning capabilities of MLLMs. V-MAGE features five diverse games with 30+ handcrafted levels, testing models on core visual skills such as positioning, trajectory tracking, timing, and visual memory, alongside higher-level reasoning like long-term planning and deliberation. We use V-MAGE to evaluate leading MLLMs, revealing significant challenges in their visual perception and reasoning. In all game environments, the top-performing MLLMs, as determined by Elo rating comparisons, exhibit a substantial performance gap compared to humans. Our findings highlight critical limitations, including various types of perceptual errors made by the models, and suggest potential avenues for improvement from an agent-centric perspective, such as refining agent strategies and addressing perceptual inaccuracies. Code is available at https://github.com/CSU-JPG/V-MAGE.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 11:43:01 GMT</pubDate>
</item>
<item>
<title>COIG-P: A High-Quality and Large-Scale Chinese Preference Dataset for Alignment with Human Values</title>
<link>https://arxiv.org/abs/2504.05535</link>
<guid>https://arxiv.org/abs/2504.05535</guid>
<content:encoded><![CDATA[
Aligning large language models (LLMs) with human preferences has achieved remarkable success. However, existing Chinese preference datasets are limited by small scale, narrow domain coverage, and lack of rigorous data validation. Additionally, the reliance on human annotators for instruction and response labeling significantly constrains the scalability of human preference datasets. To address these challenges, we design an LLM-based Chinese preference dataset annotation pipeline with no human intervention. Specifically, we crawled and carefully filtered 92k high-quality Chinese queries and employed 15 mainstream LLMs to generate and score chosen-rejected response pairs. Based on it, we introduce COIG-P (Chinese Open Instruction Generalist - Preference), a high-quality, large-scale Chinese preference dataset, comprises 1,009k Chinese preference pairs spanning 6 diverse domains: Chat, Code, Math, Logic, Novel, and Role. Building upon COIG-P, to reduce the overhead of using LLMs for scoring, we trained a 8B-sized Chinese Reward Model (CRM) and meticulously constructed a Chinese Reward Benchmark (CRBench). Evaluation results based on AlignBench liu2024alignbenchbenchmarkingchinesealignment show that that COIG-P significantly outperforms other Chinese preference datasets, and it brings significant performance improvements ranging from 2% to 12% for the Qwen2/2.5 and Infinity-Instruct-3M-0625 model series, respectively. The results on CRBench demonstrate that our CRM has a strong and robust scoring ability. We apply it to filter chosen-rejected response pairs in a test split of COIG-P, and our experiments show that it is comparable to GPT-4o in identifying low-quality samples while maintaining efficiency and cost-effectiveness. Our codes and data are released in https://github.com/multimodal-art-projection/COIG-P.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 18:15:51 GMT</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[
Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.
]]></content:encoded>
<pubDate>Wed, 26 Mar 2025 09:28:57 GMT</pubDate>
</item>
<item>
<title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
<link>https://arxiv.org/abs/2504.06263</link>
<guid>https://arxiv.org/abs/2504.06263</guid>
<content:encoded><![CDATA[
Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:49 GMT</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 13:59:41 GMT</pubDate>
</item>
<item>
<title>An Empirical Study of GPT-4o Image Generation Capabilities</title>
<link>https://arxiv.org/abs/2504.05979</link>
<guid>https://arxiv.org/abs/2504.05979</guid>
<content:encoded><![CDATA[
The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated into a unified framework for those methods. In this work, we conduct an empirical study of GPT-4o's image generation capabilities, benchmarking it against leading open-source and commercial models. Our evaluation covers four main categories, including text-to-image, image-to-image, image-to-3D, and image-to-X generation, with more than 20 tasks. Our analysis highlights the strengths and limitations of GPT-4o under various settings, and situates it within the broader evolution of generative modeling. Through this investigation, we identify promising directions for future unified generative models, emphasizing the role of architectural design and data scaling.
]]></content:encoded>
<pubDate>Tue, 08 Apr 2025 08:34:36 GMT</pubDate>
</item>
<item>
<title>Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought</title>
<link>https://arxiv.org/abs/2504.05599</link>
<guid>https://arxiv.org/abs/2504.05599</guid>
<content:encoded><![CDATA[
We introduce Skywork R1V, a multimodal reasoning model extending the an R1-series Large language models (LLM) to visual modalities via an efficient multimodal transfer method. Leveraging a lightweight visual projector, Skywork R1V facilitates seamless multimodal adaptation without necessitating retraining of either the foundational language model or the vision encoder. To strengthen visual-text alignment, we propose a hybrid optimization strategy that combines Iterative Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO), significantly enhancing cross-modal integration efficiency. Additionally, we introduce an adaptive-length Chain-of-Thought distillation approach for reasoning data generation. This approach dynamically optimizes reasoning chain lengths, thereby enhancing inference efficiency and preventing excessive reasoning overthinking. Empirical evaluations demonstrate that Skywork R1V, with only 38B parameters, delivers competitive performance, achieving a score of 69.0 on the MMMU benchmark and 67.5 on MathVista. Meanwhile, it maintains robust textual reasoning performance, evidenced by impressive scores of 72.0 on AIME and 94.0 on MATH500. The Skywork R1V model weights have been publicly released to promote openness and reproducibility.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:19:20 GMT</pubDate>
</item>
<item>
<title>Tuning-Free Image Editing with Fidelity and Editability via Unified Latent Diffusion Model</title>
<link>https://arxiv.org/abs/2504.05594</link>
<guid>https://arxiv.org/abs/2504.05594</guid>
<content:encoded><![CDATA[
Balancing fidelity and editability is essential in text-based image editing (TIE), where failures commonly lead to over- or under-editing issues. Existing methods typically rely on attention injections for structure preservation and leverage the inherent text alignment capabilities of pre-trained text-to-image (T2I) models for editability, but they lack explicit and unified mechanisms to properly balance these two objectives. In this work, we introduce UnifyEdit, a tuning-free method that performs diffusion latent optimization to enable a balanced integration of fidelity and editability within a unified framework. Unlike direct attention injections, we develop two attention-based constraints: a self-attention (SA) preservation constraint for structural fidelity, and a cross-attention (CA) alignment constraint to enhance text alignment for improved editability. However, simultaneously applying both constraints can lead to gradient conflicts, where the dominance of one constraint results in over- or under-editing. To address this challenge, we introduce an adaptive time-step scheduler that dynamically adjusts the influence of these constraints, guiding the diffusion latent toward an optimal balance. Extensive quantitative and qualitative experiments validate the effectiveness of our approach, demonstrating its superiority in achieving a robust balance between structure preservation and text alignment across various editing tasks, outperforming other state-of-the-art methods. The source code will be available at https://github.com/CUC-MIPG/UnifyEdit.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 21:02:50 GMT</pubDate>
</item>
<item>
<title>Generative Evaluation of Complex Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.02810</link>
<guid>https://arxiv.org/abs/2504.02810</guid>
<content:encoded><![CDATA[
With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:54:18 GMT</pubDate>
</item>
<item>
<title>Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</title>
<link>https://arxiv.org/abs/2504.02160</link>
<guid>https://arxiv.org/abs/2504.02160</guid>
<content:encoded><![CDATA[
Although subject-driven generation has been extensively explored in image generation due to its wide applications, it still has challenges in data scalability and subject expansibility. For the first challenge, moving from curating single-subject datasets to multiple-subject ones and scaling them is particularly difficult. For the second, most recent methods center on single-subject generation, making it hard to apply when dealing with multi-subject scenarios. In this study, we propose a highly-consistent data synthesis pipeline to tackle this challenge. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Additionally, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding. It is a multi-image conditioned subject-to-image model iteratively trained from a text-to-image model. Extensive experiments show that our method can achieve high consistency while ensuring controllability in both single-subject and multi-subject driven generation.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 18:20:21 GMT</pubDate>
</item>
<item>
<title>CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs with Controllable Puzzle Generation</title>
<link>https://arxiv.org/abs/2504.00043</link>
<guid>https://arxiv.org/abs/2504.00043</guid>
<content:encoded><![CDATA[
Existing reasoning evaluation frameworks for Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) predominantly either assess text-based reasoning or vision-language understanding capabilities, with limited dynamic interplay between textual and visual constraints. To address this limitation, we introduce CrossWordBench, a benchmark designed to evaluate the reasoning capabilities of both LLMs and LVLMs through the medium of crossword puzzles-a task requiring multimodal adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. CrossWordBench leverages a controllable puzzle generation framework that produces puzzles in multiple formats (text and image) and offers different evaluation strategies ranging from direct puzzle solving to interactive modes. Our extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. We further demonstrate that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy. Our findings offer insights into the limitations of the reasoning capabilities of current LLMs and LVLMs, and provide an effective approach for creating multimodal constrained tasks for future evaluations.
]]></content:encoded>
<pubDate>Sun, 30 Mar 2025 16:03:36 GMT</pubDate>
</item>
<item>
<title>3D Scene Understanding Through Local Random Access Sequence Modeling</title>
<link>https://arxiv.org/abs/2504.03875</link>
<guid>https://arxiv.org/abs/2504.03875</guid>
<content:encoded><![CDATA[
3D scene understanding from single images is a pivotal problem in computer vision with numerous downstream applications in graphics, augmented reality, and robotics. While diffusion-based modeling approaches have shown promise, they often struggle to maintain object and scene consistency, especially in complex real-world scenarios. To address these limitations, we propose an autoregressive generative approach called Local Random Access Sequence (LRAS) modeling, which uses local patch quantization and randomly ordered sequence generation. By utilizing optical flow as an intermediate representation for 3D scene editing, our experiments demonstrate that LRAS achieves state-of-the-art novel view synthesis and 3D object manipulation capabilities. Furthermore, we show that our framework naturally extends to self-supervised depth estimation through a simple modification of the sequence design. By achieving strong performance on multiple 3D scene understanding tasks, LRAS provides a unified and effective framework for building the next generation of 3D vision models.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 14:59:41 GMT</pubDate>
</item>
<item>
<title>Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking</title>
<link>https://arxiv.org/abs/2504.03947</link>
<guid>https://arxiv.org/abs/2504.03947</guid>
<content:encoded><![CDATA[
We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 17:27:48 GMT</pubDate>
</item>
<item>
<title>VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks</title>
<link>https://arxiv.org/abs/2504.05118</link>
<guid>https://arxiv.org/abs/2504.05118</guid>
<content:encoded><![CDATA[
We present VAPO, Value-based Augmented Proximal Policy Optimization framework for reasoning models., a novel framework tailored for reasoning models within the value-based paradigm. Benchmarked the AIME 2024 dataset, VAPO, built on the Qwen 32B pre-trained model, attains a state-of-the-art score of 60.4. In direct comparison under identical experimental settings, VAPO outperforms the previously reported results of DeepSeek-R1-Zero-Qwen-32B and DAPO by more than 10 points. The training process of VAPO stands out for its stability and efficiency. It reaches state-of-the-art performance within a mere 5,000 steps. Moreover, across multiple independent runs, no training crashes occur, underscoring its reliability. This research delves into long chain-of-thought (long-CoT) reasoning using a value-based reinforcement learning framework. We pinpoint three key challenges that plague value-based methods: value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signals. Through systematic design, VAPO offers an integrated solution that effectively alleviates these challenges, enabling enhanced performance in long-CoT reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 10:21:11 GMT</pubDate>
</item>
<item>
<title>GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.04155</link>
<guid>https://arxiv.org/abs/2504.04155</guid>
<content:encoded><![CDATA[
Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:30:58 GMT</pubDate>
</item>
<item>
<title>Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources</title>
<link>https://arxiv.org/abs/2504.04152</link>
<guid>https://arxiv.org/abs/2504.04152</guid>
<content:encoded><![CDATA[
Large Language Models (LLMs) exhibit significant disparities in performance across languages, primarily benefiting high-resource languages while marginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as a promising approach to address this imbalance, although the relative effectiveness of monolingual, bilingual, and code-augmented data strategies remains unclear. This study systematically evaluates 36 CPT configurations involving three multilingual base models, across 30+ languages categorized as altruistic, selfish, and stagnant, spanning various resource levels. Our findings reveal three major insights: (1) Bilingual CPT improves multilingual classification but often causes language mixing issues during generation. (2) Including programming code data during CPT consistently enhances multilingual classification accuracy, particularly benefiting low-resource languages, but introduces a trade-off by slightly degrading generation quality. (3) Contrary to prior work, we observe substantial deviations from language classifications according to their impact on cross-lingual transfer: Languages classified as altruistic often negatively affect related languages, selfish languages show conditional and configuration-dependent behavior, and stagnant languages demonstrate surprising adaptability under certain CPT conditions. These nuanced interactions emphasize the complexity of multilingual representation learning, underscoring the importance of systematic studies on generalizable language classification to inform future multilingual CPT strategies.
]]></content:encoded>
<pubDate>Sat, 05 Apr 2025 08:10:55 GMT</pubDate>
</item>
<item>
<title>Rethinking Reflection in Pre-Training</title>
<link>https://arxiv.org/abs/2504.04022</link>
<guid>https://arxiv.org/abs/2504.04022</guid>
<content:encoded><![CDATA[
A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 22:24:07 GMT</pubDate>
</item>
<item>
<title>Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)</title>
<link>https://arxiv.org/abs/2504.03151</link>
<guid>https://arxiv.org/abs/2504.03151</guid>
<content:encoded><![CDATA[
Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 00:04:56 GMT</pubDate>
</item>
<item>
<title>Sample, Don't Search: Rethinking Test-Time Alignment for Language Models</title>
<link>https://arxiv.org/abs/2504.03790</link>
<guid>https://arxiv.org/abs/2504.03790</guid>
<content:encoded><![CDATA[
Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 20:41:40 GMT</pubDate>
</item>
<item>
<title>SmolVLM: Redefining small and efficient multimodal models</title>
<link>https://arxiv.org/abs/2504.05299</link>
<guid>https://arxiv.org/abs/2504.05299</guid>
<content:encoded><![CDATA[
Large Vision-Language Models (VLMs) deliver exceptional performance but require significant computational resources, limiting their deployment on mobile and edge devices. Smaller VLMs typically mirror design choices of larger models, such as extensive image tokenization, leading to inefficient GPU memory usage and constrained practicality for on-device applications.   We introduce SmolVLM, a series of compact multimodal models specifically engineered for resource-efficient inference. We systematically explore architectural configurations, tokenization strategies, and data curation optimized for low computational overhead. Through this, we identify key design choices that yield substantial performance gains on image and video tasks with minimal memory footprints.   Our smallest model, SmolVLM-256M, uses less than 1GB GPU memory during inference and outperforms the 300-times larger Idefics-80B model, despite an 18-month development gap. Our largest model, at 2.2B parameters, rivals state-of-the-art VLMs consuming twice the GPU memory. SmolVLM models extend beyond static images, demonstrating robust video comprehension capabilities.   Our results emphasize that strategic architectural optimizations, aggressive yet efficient tokenization, and carefully curated training data significantly enhance multimodal performance, facilitating practical, energy-efficient deployments at significantly smaller scales.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:58:57 GMT</pubDate>
</item>
<item>
<title>LiveVQA: Live Visual Knowledge Seeking</title>
<link>https://arxiv.org/abs/2504.05288</link>
<guid>https://arxiv.org/abs/2504.05288</guid>
<content:encoded><![CDATA[
We introduce LiveVQA, an automatically collected dataset of latest visual knowledge from the Internet with synthesized VQA problems. LiveVQA consists of 3,602 single- and multi-hop visual questions from 6 news websites across 14 news categories, featuring high-quality image-text coherence and authentic information. Our evaluation across 15 MLLMs (e.g., GPT-4o, Gemma-3, and Qwen-2.5-VL family) demonstrates that stronger models perform better overall, with advanced visual reasoning capabilities proving crucial for complex multi-hop questions. Despite excellent performance on textual problems, models with tools like search engines still show significant gaps when addressing visual questions requiring latest visual knowledge, highlighting important areas for future research.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:39:31 GMT</pubDate>
</item>
<item>
<title>T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models</title>
<link>https://arxiv.org/abs/2504.04718</link>
<guid>https://arxiv.org/abs/2504.04718</guid>
<content:encoded><![CDATA[
Recent studies have demonstrated that test-time compute scaling effectively improves the performance of small language models (sLMs). However, prior research has mainly examined test-time compute scaling with an additional larger model as a verifier, leaving self-verification by sLMs underexplored. In this work, we investigate whether sLMs can reliably self-verify their outputs under test-time scaling. We find that even with knowledge distillation from larger verifiers, sLMs struggle with verification tasks requiring memorization, such as numerical calculations and fact-checking. To address this limitation, we propose Tool-integrated self-verification (T1), which delegates memorization-heavy verification steps to external tools, such as a code interpreter. Our theoretical analysis shows that tool integration reduces memorization demands and improves test-time scaling performance. Experiments on the MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under test-time scaling outperforms the significantly larger Llama-3.1 8B model. Moreover, T1 generalizes effectively to both mathematical (MATH500) and multi-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the potential of tool integration to substantially improve the self-verification abilities of sLMs.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 00:01:17 GMT</pubDate>
</item>
<item>
<title>Clinical ModernBERT: An efficient and long context encoder for biomedical text</title>
<link>https://arxiv.org/abs/2504.03964</link>
<guid>https://arxiv.org/abs/2504.03964</guid>
<content:encoded><![CDATA[
We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 18:14:12 GMT</pubDate>
</item>
<item>
<title>Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2504.03193</link>
<guid>https://arxiv.org/abs/2504.03193</guid>
<content:encoded><![CDATA[
Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.
]]></content:encoded>
<pubDate>Fri, 04 Apr 2025 01:44:45 GMT</pubDate>
</item>
<item>
<title>BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.02812</link>
<guid>https://arxiv.org/abs/2504.02812</guid>
<content:encoded><![CDATA[
We present the evaluation methodology, datasets and results of the BOP Challenge 2024, the sixth in a series of public competitions organized to capture the state of the art in 6D object pose estimation and related tasks. In 2024, our goal was to transition BOP from lab-like setups to real-world scenarios. First, we introduced new model-free tasks, where no 3D object models are available and methods need to onboard objects just from provided reference videos. Second, we defined a new, more practical 6D object detection task where identities of objects visible in a test image are not provided as input. Third, we introduced new BOP-H3 datasets recorded with high-resolution sensors and AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D models and onboarding videos to support both model-based and model-free tasks. Participants competed on seven challenge tracks, each defined by a task, object onboarding setup, and dataset group. Notably, the best 2024 method for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22% higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is only 4% behind the best 2023 method for seen objects (GPose2023) although being significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for this task is Co-op which takes only 0.8s per image and is 25X faster and 13% more accurate than GenFlow. Methods have a similar ranking on 6D detection as on 6D localization but higher run time. On model-based 2D detection of unseen objects, the best 2024 method (MUSE) achieves 21% relative improvement compared to the best 2023 method (CNOS). However, the 2D detection accuracy for unseen objects is still noticealy (-53%) behind the accuracy for seen objects (GDet2023). The online evaluation system stays open and is available at http://bop.felk.cvut.cz/
]]></content:encoded>
<pubDate>Thu, 03 Apr 2025 13:55:19 GMT</pubDate>
</item>
<item>
<title>DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2504.02882</link>
<guid>https://arxiv.org/abs/2504.02882</guid>
<content:encoded><![CDATA[
Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling.
]]></content:encoded>
<pubDate>Wed, 02 Apr 2025 01:47:28 GMT</pubDate>
</item>
<item>
<title>URECA: Unique Region Caption Anything</title>
<link>https://arxiv.org/abs/2504.05305</link>
<guid>https://arxiv.org/abs/2504.05305</guid>
<content:encoded><![CDATA[
Region-level captioning aims to generate natural language descriptions for specific image regions while highlighting their distinguishing features. However, existing methods struggle to produce unique captions across multi-granularity, limiting their real-world applicability. To address the need for detailed region-level understanding, we introduce URECA dataset, a large-scale dataset tailored for multi-granularity region captioning. Unlike prior datasets that focus primarily on salient objects, URECA dataset ensures a unique and consistent mapping between regions and captions by incorporating a diverse set of objects, parts, and background elements. Central to this is a stage-wise data curation pipeline, where each stage incrementally refines region selection and caption generation. By leveraging Multimodal Large Language Models (MLLMs) at each stage, our pipeline produces distinctive and contextually grounded captions with improved accuracy and semantic diversity. Building upon this dataset, we present URECA, a novel captioning model designed to effectively encode multi-granularity regions. URECA maintains essential spatial properties such as position and shape through simple yet impactful modifications to existing MLLMs, enabling fine-grained and semantically rich region descriptions. Our approach introduces dynamic mask modeling and a high-resolution mask encoder to enhance caption uniqueness. Experiments show that URECA achieves state-of-the-art performance on URECA dataset and generalizes well to existing region-level captioning benchmarks.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:44 GMT</pubDate>
</item>
<item>
<title>Gaussian Mixture Flow Matching Models</title>
<link>https://arxiv.org/abs/2504.05304</link>
<guid>https://arxiv.org/abs/2504.05304</guid>
<content:encoded><![CDATA[
Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an L_2 denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256times256.
]]></content:encoded>
<pubDate>Mon, 07 Apr 2025 13:59:42 GMT</pubDate>
</item>
</channel>
</rss>