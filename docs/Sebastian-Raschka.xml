<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Sebastian Raschka, PhD</title>
<link>https://sebastianraschka.com/</link>


<item>
<title>推理扩展提升大语言模型回答质量</title>
<link>https://sebastianraschka.com/blog/2026/categories-of-inference-time-scaling.html</link>
<guid>https://sebastianraschka.com/blog/2026/categories-of-inference-time-scaling.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理扩展可提升大语言模型的回答质量与准确性。</p><br /><br /><p><strong>摘要：</strong> 文章指出，推理扩展是提高部署后大型语言模型回答质量和准确性的有效方法。其核心思想是在推理阶段增加计算资源和时间投入，从而让模型生成更高质量的文本输出。这种方法通过优化模型在实际应用中的表现，提升了整体的响应质量。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2026/categories-of-inference-time-scaling.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 24 Jan 2026 08:15:00 +0000</pubDate>
</item>
<item>
<title>2025年大语言模型的发展、问题与预测</title>
<link>https://sebastianraschka.com/blog/2025/state-of-llms-2025-copy.html</link>
<guid>https://sebastianraschka.com/blog/2025/state-of-llms-2025-copy.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2025年大语言模型在推理能力、RLVR算法和架构优化方面取得进展。</p><br /><br /><p><strong>摘要：</strong> 本文回顾了2025年大语言模型（LLMs）的主要发展，包括推理模型的提升、RLVR（可验证奖励强化学习）算法的应用以及GRPO等新训练方法的出现。文章还讨论了大模型在数学、代码等领域的表现，以及工具使用和推理缩放技术对性能的提升。同时指出，尽管模型性能持续提高，但评估指标的有效性受到质疑，且数据隐私和领域专业化仍是挑战。最后，作者预测了2026年的趋势，包括RLVR扩展到更多领域、低延迟推理模型的普及以及工具使用的增强。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/state-of-llms-2025-copy.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 30 Dec 2025 08:15:00 +0000</pubDate>
</item>
<item>
<title>2025年大语言模型的发展、问题与预测</title>
<link>https://sebastianraschka.com/blog/2025/state-of-llms-2025.html</link>
<guid>https://sebastianraschka.com/blog/2025/state-of-llms-2025.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2025年大语言模型在推理能力、RLVR算法和效率提升方面取得显著进展。</p><br /><br /><p><strong>摘要：</strong> 文章回顾了2025年大语言模型（LLMs）的主要发展，包括推理能力的提升、RLVR（可验证奖励强化学习）和GRPO算法的应用。DeepSeek R1模型展示了通过强化学习实现推理行为的可能性，降低了训练成本。同时，文章讨论了LLM架构的优化、工具使用和推理扩展的重要性，并指出2026年可能在更多领域扩展RLVR，以及关注持续学习和低延迟推理。此外，文章还提到LLMs在代码编写、技术写作和研究中的应用，强调其作为辅助工具的价值，而非完全替代人类。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/state-of-llms-2025.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 30 Dec 2025 08:15:00 +0000</pubDate>
</item>
<item>
<title>2025年7月至12月大语言模型研究论文汇总</title>
<link>https://sebastianraschka.com/blog/2025/llm-research-papers-2025-part2.html</link>
<guid>https://sebastianraschka.com/blog/2025/llm-research-papers-2025-part2.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">整理2025年下半年大语言模型相关研究论文列表。</p><br /><br /><p><strong>摘要：</strong> 本文是Sebastian Raschka为付费订阅者整理的2025年7月至12月期间收藏和分类的研究论文列表。文章涵盖了多个大语言模型（LLM）相关的主题，如推理模型、强化学习方法、模型发布、架构设计、高效训练等。作者仅浏览了这些论文的摘要，并未深入阅读全部内容，但仍将其整理成列表以供日后参考。此外，作者还提到正在撰写年度LLM综述文章，并将本列表作为补充内容单独发布，以便读者更方便地查阅和回顾。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/llm-research-papers-2025-part2.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 30 Dec 2025 08:00:00 +0000</pubDate>
</item>
<item>
<title>2025年机器学习与AI入门示例更新</title>
<link>https://sebastianraschka.com/blog/2025/hello-world-ai.html</link>
<guid>https://sebastianraschka.com/blog/2025/hello-world-ai.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">更新2025年机器学习与AI入门示例，提供更现代的教程和解释。</p><br /><br /><p><strong>摘要：</strong> 作者两年前在社交媒体上分享了机器学习与AI的入门示例（Hello World），旨在为初学者展示各种方法。他设置了一个每两年更新一次的提醒，现在重新整理并补充了2025年的示例，以反映最新的技术和实践。文章不仅列出了更新后的示例，还提供了更多背景信息，帮助读者更好地理解每个示例的意义和应用场景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/hello-world-ai.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:20:00 +0000</pubDate>
</item>
<item>
<title>DeepSeek V3.2 模型发布与性能表现</title>
<link>https://sebastianraschka.com/blog/2025/technical-deepseek.html</link>
<guid>https://sebastianraschka.com/blog/2025/technical-deepseek.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">DeepSeek V3.2 在性能上媲美 GPT-5 和 Gemini 3.0 Pro。</p><br /><br /><p><strong>摘要：</strong> DeepSeek 团队在其新旗舰模型 DeepSeek V3.2 的发布中，选择在重大美国假日周末推出。该模型在性能上表现出色，达到 GPT-5 和 Gemini 3.0 Pro 的水平，并且提供开放权重版本，便于广泛使用和研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:06:00 +0000</pubDate>
</item>
<item>
<title>如何从零构建大型语言模型的阅读指南</title>
<link>https://sebastianraschka.com/blog/2025/reading-books.html</link>
<guid>https://sebastianraschka.com/blog/2025/reading-books.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章提供构建大型语言模型的阅读建议。</p><br /><br /><p><strong>摘要：</strong> 本文总结了读者在阅读《从零构建大型语言模型》一书时的实用方法。作者强调按照章节顺序阅读的重要性，因为每一章都依赖于前一章的内容。文章还推荐了每章的阅读步骤，旨在帮助读者更好地理解和掌握书中内容。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/reading-books.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:08:00 +0000</pubDate>
</item>
<item>
<title>探索替代Transformer的大型语言模型架构</title>
<link>https://sebastianraschka.com/blog/2025/beyond-standard-llms.html</link>
<guid>https://sebastianraschka.com/blog/2025/beyond-standard-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章探讨了替代Transformer的LLM架构及其潜在优势。</p><br /><br /><p><strong>摘要：</strong> 文章回顾了之前关于基于Transformer的大型语言模型的比较，并回应了读者对其他替代架构的兴趣。作者在PyTorch Conference 2025上提到这一话题，并在此文中进一步展开讨论，介绍和分析了非Transformer架构的可能性与优劣，为读者提供了更全面的视角。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/beyond-standard-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 04 Nov 2025 00:08:00 +0000</pubDate>
</item>
<item>
<title>本地LLM推理与微调的DGX Spark实践</title>
<link>https://sebastianraschka.com/blog/2025/dgx-impressions.html</link>
<guid>https://sebastianraschka.com/blog/2025/dgx-impressions.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索DGX Spark在本地LLM推理和微调中的应用与性能表现。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了近期关于本地大语言模型（LLM）推理与微调的热门话题，作者亲自尝试使用DGX Spark平台进行相关工作，主要基于PyTorch框架进行LLM的训练与优化，并收集了相关的性能基准数据和实践经验。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/dgx-impressions.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 29 Oct 2025 00:06:00 +0000</pubDate>
</item>
<item>
<title>Understanding the 4 Main Approaches to LLM Evaluation (From Scratch)</title>
<link>http://localhost:4000/blog/2025/llm-evaluation-4-approaches.html</link>
<guid>http://localhost:4000/blog/2025/llm-evaluation-4-approaches.html</guid>
<content:encoded><![CDATA[
<div>Multiple-Choice Benchmarks, Verifiers, Leaderboards, and LLM Judges with Code Examples</div>
]]></content:encoded>
<pubDate>Sun, 05 Oct 2025 00:06:00 +0000</pubDate>
</item>
<item>
<title>2025年开放权重架构的代码解析与实践</title>
<link>http://localhost:4000/blog/2025/qwen3-from-scratch.html</link>
<guid>http://localhost:4000/blog/2025/qwen3-from-scratch.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">深入分析2025年主流开放权重架构的代码实现。</p><br /><br /><p><strong>摘要：</strong> 本文延续前文对2025年最显著的开放权重架构的比较，进一步从代码层面解析这些架构的实际运作方式。通过实际操作，读者可以理解其内部机制，并获取可用于自身实验或项目的构建模块。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/qwen3-from-scratch.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 08:00:00 +0000</pubDate>
</item>
<item>
<title>OpenAI发布新型开源大模型gpt-oss-120b和gpt-oss-20b</title>
<link>http://localhost:4000/blog/2025/from-gpt-2-to-gpt-oss.html</link>
<guid>http://localhost:4000/blog/2025/from-gpt-2-to-gpt-oss.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenAI推出两款开源大模型，支持本地运行。</p><br /><br /><p><strong>摘要：</strong> OpenAI最近发布了两款新的开源大语言模型gpt-oss-120b和gpt-oss-20b，这是自2019年GPT-2以来首次公开权重的模型。通过一些优化技术，这些模型可以在本地设备上运行。作者在过去的几天里阅读了相关代码和技术报告，总结了其中最值得关注的细节。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/from-gpt-2-to-gpt-oss.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 11:00:00 +0000</pubDate>
</item>
<item>
<title>大语言模型架构演进与性能分析</title>
<link>http://localhost:4000/blog/2025/the-big-llm-architecture-comparison.html</link>
<guid>http://localhost:4000/blog/2025/the-big-llm-architecture-comparison.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大语言模型架构在多年间变化不大，但性能差异仍值得关注。</p><br /><br /><p><strong>摘要：</strong> 文章回顾了从GPT-2到DeepSeek-V3和Llama 4等大语言模型的演进过程，指出尽管这些模型在时间跨度上跨越多年，其结构仍保持高度相似。由于数据集、训练技术和超参数的多样性且常未公开，直接比较模型性能具有挑战性。因此，文章强调通过分析模型架构的变化来理解2025年大语言模型开发的趋势和策略。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/the-big-llm-architecture-comparison.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 06:00:00 +0000</pubDate>
</item>
<item>
<title>LLM Research Papers: The 2025 List (January to June)</title>
<link>http://localhost:4000/blog/2025/llm-research-papers-the-2025-list-january-to-june.html</link>
<guid>http://localhost:4000/blog/2025/llm-research-papers-the-2025-list-january-to-june.html</guid>
<content:encoded><![CDATA[
<div>The latest in LLM research with a hand-curated, topic-organized list of over 200 research papers from 2025.</div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 06:06:11 +0000</pubDate>
</item>
<item>
<title>KV缓存技术在大型语言模型推理中的应用</title>
<link>http://localhost:4000/blog/2025/coding-the-kv-cache-in-llms.html</link>
<guid>http://localhost:4000/blog/2025/coding-the-kv-cache-in-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章讲解KV缓存的原理与实现方式。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了KV缓存在大型语言模型（LLM）推理中的重要作用，特别是在提高计算效率方面。通过从零开始的可读代码实现，文章详细解释了KV缓存的概念和工作原理，帮助读者深入理解其在生产环境中的应用价值。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/coding-the-kv-cache-in-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 08:00:00 +0000</pubDate>
</item>
<item>
<title>Coding LLMs from the Ground Up: A Complete Course</title>
<link>http://localhost:4000/blog/2025/coding-llms-from-the-ground-up-a-complete-course.html</link>
<guid>http://localhost:4000/blog/2025/coding-llms-from-the-ground-up-a-complete-course.html</guid>
<content:encoded><![CDATA[
<div>Why build an LLM from scratch? It's probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.</div>
]]></content:encoded>
<pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate>
</item>
<item>
<title>推理增强的大型语言模型发展与未来趋势</title>
<link>http://localhost:4000/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</link>
<guid>http://localhost:4000/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">推理能力提升成为LLM新方向。</p><br /><br /><p><strong>摘要：</strong> 文章讨论了近期GPT-4.5和Llama 4等旗舰模型发布后反应平淡的原因，指出这些模型仍为传统架构，未经过显式推理强化学习训练。而OpenAI推出的o3推理模型展示了通过针对性强化学习提升模型推理能力的潜力。尽管推理并非万能，但其在复杂任务中的准确性和问题解决能力得到了验证，并预计未来推理导向的后训练将成为LLM的标准流程。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
</item>
<item>
<title>LLM推理机制初探与新书章节分享</title>
<link>http://localhost:4000/blog/2025/first-look-at-reasoning-from-scratch.html</link>
<guid>http://localhost:4000/blog/2025/first-look-at-reasoning-from-scratch.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍LLM推理机制及新书第一章内容。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了大型语言模型（LLM）中的推理机制，包括推理在LLM中的作用以及相关方法如推理时缩放和强化学习。作者还分享了即将出版的新书第一章内容，作为对付费订阅者的感谢，并预告了后续关于推理研究的博客文章。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/first-look-at-reasoning-from-scratch.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Inference-Time Compute Scaling Methods to Improve Reasoning Models</title>
<link>http://localhost:4000/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</link>
<guid>http://localhost:4000/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</guid>
<content:encoded><![CDATA[
<div>This article explores recent research advancements in reasoning-optimized LLMs, with a particular focus on inference-time compute scaling that have emerged since the release of DeepSeek R1.</div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>构建推理模型的四种主要方法</title>
<link>http://localhost:4000/blog/2025/understanding-reasoning-llms.html</link>
<guid>http://localhost:4000/blog/2025/understanding-reasoning-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍增强LLM推理能力的四种主要方法。</p><br /><br /><p><strong>摘要：</strong> 本文详细介绍了四种构建推理模型的方法，旨在提升大型语言模型（LLM）的推理能力。文章帮助读者理解当前在该领域的发展趋势和相关研究，提供了有价值的见解，以应对这一快速发展的领域中的文献和炒作。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/understanding-reasoning-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Feb 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Noteworthy LLM Research Papers of 2024</title>
<link>http://localhost:4000/blog/2025/llm-research-2024.html</link>
<guid>http://localhost:4000/blog/2025/llm-research-2024.html</guid>
<content:encoded><![CDATA[
<div>This article covers 12 influential AI research papers of 2024, ranging from mixture-of-experts models to new LLM scaling laws for precision.</div>
]]></content:encoded>
<pubDate>Thu, 23 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>从零实现字节对编码（BPE）算法</title>
<link>http://localhost:4000/blog/2025/bpe-from-scratch.html</link>
<guid>http://localhost:4000/blog/2025/bpe-from-scratch.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文从零实现BPE算法，用于教学目的。</p><br /><br /><p><strong>摘要：</strong> 本文是一个独立的笔记本，旨在从零开始实现字节对编码（BPE）算法。该算法广泛应用于GPT-2到GPT-4、Llama 3等模型中，用于文本的分词处理。文章通过实际代码演示了BPE的基本原理和实现过程，目的是帮助学习者更好地理解这一常见的文本预处理技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2025/bpe-from-scratch.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 17 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>2024年值得关注的LLM相关论文书签列表</title>
<link>http://localhost:4000/blog/2024/llm-research-papers-the-2024-list.html</link>
<guid>http://localhost:4000/blog/2024/llm-research-papers-the-2024-list.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">分享2024年发现的LLM相关论文书签列表。</p><br /><br /><p><strong>摘要：</strong> 本文作者分享了在2024年发现的一些有趣且主要与大型语言模型（LLM）相关的论文书签列表。这只是一个简单的列表，但可能对那些希望在假期中寻找值得阅读的高质量论文的人有所帮助。文章旨在为读者提供一些有价值的参考资源，方便他们进一步探索和研究LLM领域的最新进展。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/llm-research-papers-the-2024-list.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 29 Dec 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>多模态大语言模型的架构与最新研究进展</title>
<link>http://localhost:4000/blog/2024/understanding-multimodal-llms.html</link>
<guid>http://localhost:4000/blog/2024/understanding-multimodal-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章解析多模态LLM的架构及Llama 3.2等模型的处理方式。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了多模态大语言模型（LLM）的最新研究，特别是Llama 3.2视觉模型采用的多种架构策略，如解码器-only方法和交叉注意力机制。解码器-only方法通过单一解码器块依次处理不同模态数据，而交叉注意力方法则为不同模态分别设置编码器，并通过交叉注意力层实现交互。文章还总结了近期十余篇多模态相关论文的研究方法，对比分析了它们的技术路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/understanding-multimodal-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Nov 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>将预训练大语言模型转化为文本分类器的方法</title>
<link>http://localhost:4000/blog/2024/building-a-gpt-style-llm-classifier.html</link>
<guid>http://localhost:4000/blog/2024/building-a-gpt-style-llm-classifier.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍如何将预训练大语言模型转化为强大的文本分类器。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了如何将预训练的大语言模型（LLMs）转化为高效的文本分类器。文章指出，文本分类是模型微调的一个良好切入点，因为它既简单又有效。此外，许多实际应用和商业问题都依赖于文本分类技术，如垃圾邮件检测、情感分析、客户反馈分类和主题标签等。通过微调预训练模型，可以有效提升其在特定任务上的性能。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/building-a-gpt-style-llm-classifier.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 21 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>从零开始构建大型语言模型教程</title>
<link>http://localhost:4000/blog/2024/building-llms-from-the-ground-up.html</link>
<guid>http://localhost:4000/blog/2024/building-llms-from-the-ground-up.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文讲解如何用PyTorch构建和训练大型语言模型。</p><br /><br /><p><strong>摘要：</strong> 本文面向对大型语言模型（LLM）感兴趣的开发者，介绍了LLM的基本构成、工作原理以及如何在PyTorch中从零开始构建一个类似GPT的模型。教程涵盖数据输入管道、核心架构组件和预训练代码，并进一步讲解如何加载预训练权重并使用开源库进行微调。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/building-llms-from-the-ground-up.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>近期主流大语言模型的预训练与后训练流程分析</title>
<link>http://localhost:4000/blog/2024/new-llm-pre-training-and-post-training.html</link>
<guid>http://localhost:4000/blog/2024/new-llm-pre-training-and-post-training.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">分析最新大语言模型的预训练和后训练方法。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于近期发布的四款主流大语言模型——阿里巴巴的Qwen 2、苹果的Apple Intelligence Foundation Language Models、谷歌的Gemma 2以及Meta AI的Llama 3.1，探讨它们的预训练和后训练流程。通过对这些模型的技术报告进行研究，可以更好地理解当前大语言模型在实际应用中的有效方法和技术路径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/new-llm-pre-training-and-post-training.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 17 Aug 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>新型数据生成方法提升大模型指令微调效率</title>
<link>http://localhost:4000/blog/2024/instruction-pretraining-llms.html</link>
<guid>http://localhost:4000/blog/2024/instruction-pretraining-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍一种低成本的数据生成方法用于大模型指令微调。</p><br /><br /><p><strong>摘要：</strong> 文章介绍了用于大模型指令微调的一种新型、成本效益高的数据生成方法，涵盖了从零开始进行指令微调、使用指令数据预训练大模型等内容，并对Gemma 2的新特性进行了概述。该方法旨在提高模型在特定任务上的表现，同时降低数据准备的成本和复杂度。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/instruction-pretraining-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 20 Jul 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>大型语言模型开发流程概述</title>
<link>http://localhost:4000/blog/2024/llms-building-training-finetuning.html</link>
<guid>http://localhost:4000/blog/2024/llms-building-training-finetuning.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍大型语言模型开发的三个核心阶段及评估方法。</p><br /><br /><p><strong>摘要：</strong> 本文是一场关于大型语言模型（LLM）开发过程的一小时讲座，重点介绍了开发LLM的三个关键阶段：构建模型架构、进行预训练以及对模型进行微调。此外，还讨论了评估LLM的主要方法及其各自的局限性。文章旨在帮助读者全面理解LLM的开发流程和评估标准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/llms-building-training-finetuning.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>指令微调与LoRA在大语言模型中的新研究</title>
<link>http://localhost:4000/blog/2024/llm-research-insights-instruction.html</link>
<guid>http://localhost:4000/blog/2024/llm-research-insights-instruction.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">三篇新论文探讨指令微调与LoRA方法在LLM中的应用。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了三篇关于指令微调和参数高效微调方法（如LoRA）在大语言模型中的最新研究。这些方法在日常工作中被广泛应用，新研究提供了实用的见解，有助于提升模型性能和效率。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/llm-research-insights-instruction.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>四款主流大语言模型发布及强化学习方法研究</title>
<link>http://localhost:4000/blog/2024/how-good-open-llm.html</link>
<guid>http://localhost:4000/blog/2024/how-good-open-llm.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章分析四款大语言模型并探讨强化学习方法。</p><br /><br /><p><strong>摘要：</strong> 文章回顾并讨论了Mixtral、Meta AI的Llama 3、Microsoft的Phi-3以及Apple的OpenELM这四款重要的开源大语言模型。随后，文章还介绍了基于PPO和DPO算法的强化学习方法在指令微调中的应用与研究。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/how-good-open-llm.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 12 May 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>预训练大语言模型的使用与微调方法</title>
<link>http://localhost:4000/blog/2024/using-finetuning-transformers.html</link>
<guid>http://localhost:4000/blog/2024/using-finetuning-transformers.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了三种使用和微调预训练大语言模型的方法。</p><br /><br /><p><strong>摘要：</strong> 文章详细探讨了三种常见的使用和微调预训练大语言模型（LLMs）的方法。第一种是基于特征的方法，即利用预训练模型生成的嵌入向量，训练新的下游模型，如线性分类器，而无需进一步微调。第二种是上下文提示法，通过在输入中展示新任务的例子，直接引导模型输出预期结果，无需模型更新。第三种是微调全部或部分参数，以达到特定任务的效果。文章对这三种方法进行了深入分析。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/using-finetuning-transformers.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 20 Apr 2024 07:00:00 +0000</pubDate>
</item>
<item>
<title>大语言模型持续预训练与奖励建模研究</title>
<link>http://localhost:4000/blog/2024/research-papers-in-march-2024.html</link>
<guid>http://localhost:4000/blog/2024/research-papers-in-march-2024.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">研究大语言模型的持续预训练与奖励建模方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了大语言模型（LLMs）的持续预训练策略，以及在人类反馈强化学习中使用的奖励建模方法。持续预训练有助于更新现有模型，使其保持最新信息并适应新领域，而无需从头开始训练。奖励建模则用于使LLMs更符合人类偏好，并在复杂任务中提供指导，尤其在难以直接编程的情况下。此外，文章还介绍了一个新的评估基准。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/research-papers-in-march-2024.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 31 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>AI研究进展：新开源大模型与高效微调技术</title>
<link>http://localhost:4000/blog/2024/research-papers-in-february-2024.html</link>
<guid>http://localhost:4000/blog/2024/research-papers-in-february-2024.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本月AI研究推出两款开源大模型及高效微调方法。</p><br /><br /><p><strong>摘要：</strong> 本月AI研究取得了多项进展，包括两款新的开源大语言模型（LLM）的发布。其中，OLMo是完全开源的，从训练代码到数据集和日志文件均公开可用；而Gemma虽然权重公开，但在多个基准测试中表现优异，超越了Llama 2 7B和Mistral 7B等同规模模型。此外，还介绍了针对小型微调LLM的新见解以及一种参数高效的微调技术。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/research-papers-in-february-2024.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 03 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>LoRA与DoRA：模型微调技术的比较分析</title>
<link>http://localhost:4000/blog/2024/lora-dora.html</link>
<guid>http://localhost:4000/blog/2024/lora-dora.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章对比了LoRA和新兴的DoRA模型微调技术。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了低秩适应（LoRA）这一机器学习技术，它通过调整预训练模型中少量低秩参数来优化模型在特定小数据集上的表现。同时，文章还探讨了DoRA，这是一种作为LoRA替代方案的新技术，具有潜在的优势。通过对这两种方法的分析，文章旨在帮助读者理解它们在模型微调中的应用和前景。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2024/lora-dora.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 18 Feb 2024 08:00:00 +0000</pubDate>
</item>
<item>
<title>基于指令数据集的LLM微调方法</title>
<link>http://localhost:4000/blog/2023/optimizing-LLMs-dataset-perspective.html</link>
<guid>http://localhost:4000/blog/2023/optimizing-LLMs-dataset-perspective.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍如何通过优化数据集提升LLM微调效果。</p><br /><br /><p><strong>摘要：</strong> 本文聚焦于通过精心构建的数据集来提升大语言模型（LLM）的微调性能。文章强调了在指令基础上进行数据集的修改、利用或操作，而不是改变模型结构或训练算法。此外，文章还详细说明了如何准备自己的数据集以用于开源LLM的微调过程。这种方法为提高模型在特定任务上的表现提供了有效途径。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2023/optimizing-LLMs-dataset-perspective.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Fri, 15 Sep 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>参与NeurIPS 2023 LLM效率挑战指南</title>
<link>http://localhost:4000/blog/2023/neurips2023-starter-guide.html</link>
<guid>http://localhost:4000/blog/2023/neurips2023-starter-guide.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">介绍如何参与NeurIPS 2023 LLM效率挑战赛。</p><br /><br /><p><strong>摘要：</strong> 本文为读者提供了参与NeurIPS 2023 LLM Efficiency Challenge的详细指南，重点介绍了如何高效地进行大型语言模型（LLM）微调。文章涵盖了从搭建开发环境到提交第一个作品的全过程，帮助参赛者更好地理解和应对此次挑战。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2023/neurips2023-starter-guide.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Thu, 10 Aug 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>降低深度学习模型内存消耗的高效技术</title>
<link>http://localhost:4000/blog/2023/pytorch-memory-optimization.html</link>
<guid>http://localhost:4000/blog/2023/pytorch-memory-optimization.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍降低深度学习模型内存消耗的技巧。</p><br /><br /><p><strong>摘要：</strong> 文章探讨了在训练视觉变换器和大语言模型等深度学习模型时，内存消耗是一个常见的瓶颈。作者提出了一系列技术，能够在不牺牲模型性能和预测准确性的前提下，将内存消耗降低约20倍。这些方法为优化模型训练效率提供了有效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2023/pytorch-memory-optimization.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 01 Jul 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>参数高效微调方法对比：Falcon模型的高效微调实践</title>
<link>http://localhost:4000/blog/2023/falcon-finetuning.html</link>
<guid>http://localhost:4000/blog/2023/falcon-finetuning.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章比较了Falcon模型的参数高效微调方法。</p><br /><br /><p><strong>摘要：</strong> 本文探讨了如何通过参数高效微调方法，以更低的成本和更短的时间对最新的高性能开源大语言模型Falcon进行微调。文章分析了多种微调方法，并指出在单块GPU上仅需1小时即可完成微调，而传统方法则需要6块GPU运行一整天。这为实际应用提供了高效的解决方案。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="http://localhost:4000/blog/2023/falcon-finetuning.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 14 Jun 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Accelerating Large Language Models with Mixed-Precision Techniques</title>
<link>http://localhost:4000/blog/2023/llm-mixed-precision-copy.html</link>
<guid>http://localhost:4000/blog/2023/llm-mixed-precision-copy.html</guid>
<content:encoded><![CDATA[
Training and using large language models (LLMs) is expensive due to their large compute requirements and memory footprints. This article will explore how leveraging lower-precision formats can enhance training and inference speeds up to 3x without compromising model accuracy.
]]></content:encoded>
<pubDate>Thu, 11 May 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)</title>
<link>http://localhost:4000/blog/2023/llm-finetuning-lora.html</link>
<guid>http://localhost:4000/blog/2023/llm-finetuning-lora.html</guid>
<content:encoded><![CDATA[
Pretrained large language models are often referred to as foundation models for a good reason: they perform well on various tasks, and we can use them as a foundation for finetuning on a target task. As an alternative to updating all layers, which is very expensive, parameter-efficient methods such as prefix tuning and adapters have been developed. Let's talk about one of the most popular parameter-efficient finetuning techniques: Low-rank adaptation (LoRA). What is LoRA? How does it work? And how does it compare to the other popular finetuning approaches? Let's answer all these questions in this article!
]]></content:encoded>
<pubDate>Wed, 26 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters</title>
<link>http://localhost:4000/blog/2023/llm-finetuning-llama-adapter.html</link>
<guid>http://localhost:4000/blog/2023/llm-finetuning-llama-adapter.html</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of artificial intelligence, utilizing large language models in an efficient and effective manner has become increasingly important. Parameter-efficient finetuning stands at the forefront of this pursuit, allowing researchers and practitioners to reuse pretrained models while minimizing their computational and resource footprints. This article explains the broad concept of finetuning and discusses popular parameter-efficient alternatives like prefix tuning and adapters. Finally, we will look at the recent LLaMA-Adapter method and see how we can use it in practice.
]]></content:encoded>
<pubDate>Wed, 12 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Finetuning Large Language Models On A Single GPU Using Gradient Accumulation</title>
<link>http://localhost:4000/blog/2023/llm-grad-accumulation.html</link>
<guid>http://localhost:4000/blog/2023/llm-grad-accumulation.html</guid>
<content:encoded><![CDATA[
Previously, I shared an article using multi-GPU training strategies to speed up the finetuning of large language models. Several of these strategies include mechanisms such as model or tensor sharding that distributes the model weights and computations across different devices to work around GPU memory limitations. However, many of us don't have access to multi-GPU resources. So, this article illustrates a simple technique that works as a great workaround to train models with larger batch sizes when GPU memory is a concern: gradient accumulation.
]]></content:encoded>
<pubDate>Tue, 28 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Keeping Up With AI Research And News</title>
<link>http://localhost:4000/blog/2023/keeping-up-with-ai.html</link>
<guid>http://localhost:4000/blog/2023/keeping-up-with-ai.html</guid>
<content:encoded><![CDATA[
When it comes to productivity workflows, there are a lot of things I'd love to share. However, the one topic many people ask me about is how I keep up with machine learning and AI at large, and how I find interesting papers.
]]></content:encoded>
<pubDate>Thu, 23 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Some Techniques To Make Your PyTorch Models Train (Much) Faster</title>
<link>http://localhost:4000/blog/2023/pytorch-faster.html</link>
<guid>http://localhost:4000/blog/2023/pytorch-faster.html</guid>
<content:encoded><![CDATA[
This blog post outlines techniques for improving the training performance of your PyTorch model without compromising its accuracy. To do so, we will wrap a PyTorch model in a LightningModule and use the Trainer class to enable various training optimizations. By changing only a few lines of code, we can reduce the training time on a single GPU from 22.53 minutes to 2.75 minutes while maintaining the model's prediction accuracy. Yes, that's a 8x performance boost!
]]></content:encoded>
<pubDate>Thu, 23 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</title>
<link>http://localhost:4000/blog/2023/self-attention-from-scratch.html</link>
<guid>http://localhost:4000/blog/2023/self-attention-from-scratch.html</guid>
<content:encoded><![CDATA[
In this article, we are going to understand how self-attention works from scratch. This means we will code it ourselves one step at a time. Since its introduction via the original transformer paper, self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing. Since self-attention is now everywhere, it's important to understand how it works.
]]></content:encoded>
<pubDate>Thu, 09 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Large Language Models -- A Transformative Reading List</title>
<link>http://localhost:4000/blog/2023/llm-reading-list.html</link>
<guid>http://localhost:4000/blog/2023/llm-reading-list.html</guid>
<content:encoded><![CDATA[
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models.
]]></content:encoded>
<pubDate>Tue, 07 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>What Are the Different Approaches for Detecting Content Generated by LLMs Such As ChatGPT? And How Do They Work and Differ?</title>
<link>http://localhost:4000/blog/2023/detect-ai.html</link>
<guid>http://localhost:4000/blog/2023/detect-ai.html</guid>
<content:encoded><![CDATA[
Since the release of the AI Classifier by OpenAI made big waves yesterday, I wanted to share a few details about the different approaches  for detecting AI-generated text. This article briefly outlines four approaches to identifying AI-generated contents.
]]></content:encoded>
<pubDate>Wed, 01 Feb 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Comparing Different Automatic Image Augmentation Methods in PyTorch</title>
<link>http://localhost:4000/blog/2023/data-augmentation-pytorch.html</link>
<guid>http://localhost:4000/blog/2023/data-augmentation-pytorch.html</guid>
<content:encoded><![CDATA[
Data augmentation is a key tool in reducing overfitting, whether it's for images or text. This article compares three Auto Image Data Augmentation techniques in PyTorch: AutoAugment, RandAugment, and TrivialAugment.
]]></content:encoded>
<pubDate>Sun, 29 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Curated Resources and Trustworthy Experts: The Key Ingredients for Finding Accurate Answers to Technical Questions in the Future</title>
<link>http://localhost:4000/blog/2023/chatgpt-dilemma.html</link>
<guid>http://localhost:4000/blog/2023/chatgpt-dilemma.html</guid>
<content:encoded><![CDATA[
Conversational chat bots such as ChatGPT probably will not be able replace traditional search engines and expert knowledge anytime soon. With the vast amount of misinformation available on the internet, the ability to distinguish between credible and unreliable sources remains challenging and crucial.
]]></content:encoded>
<pubDate>Mon, 16 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Training an XGBoost Classifier Using Cloud GPUs Without Worrying About Infrastructure</title>
<link>http://localhost:4000/blog/2023/xgboost-gpu.html</link>
<guid>http://localhost:4000/blog/2023/xgboost-gpu.html</guid>
<content:encoded><![CDATA[
Imagine you want to quickly train a few machine learning or deep learning models on the cloud but don't want to deal with cloud infrastructure. This short article explains how we can get our code up and running in seconds using the open source lightning library.
]]></content:encoded>
<pubDate>Sun, 15 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Open Source Highlights 2022 for Machine Learning &amp; AI</title>
<link>http://localhost:4000/blog/2023/open-source-highlights-2022.html</link>
<guid>http://localhost:4000/blog/2023/open-source-highlights-2022.html</guid>
<content:encoded><![CDATA[
Recently, I shared the top 10 papers that I read in 2022. As a follow-up, I am compiling a list of my favorite 10 open-source releases that I discovered, used, or contributed to in 2022.
]]></content:encoded>
<pubDate>Thu, 05 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Influential Machine Learning Papers Of 2022</title>
<link>http://localhost:4000/blog/2023/top10-papers-2022.html</link>
<guid>http://localhost:4000/blog/2023/top10-papers-2022.html</guid>
<content:encoded><![CDATA[
Every day brings something new and exciting to the world of machine learning and AI, from the latest developments and breakthroughs in the field to emerging trends and challenges. To mark the start of the new year, below is a short review of the top ten papers I've read in 2022.
]]></content:encoded>
<pubDate>Tue, 03 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Ahead Of AI, And What's Next?</title>
<link>http://localhost:4000/blog/2022/ahead-of-ai-and-whats-next.html</link>
<guid>http://localhost:4000/blog/2022/ahead-of-ai-and-whats-next.html</guid>
<content:encoded><![CDATA[
About monthly machine learning musings, and other things I am currently workin on ...
]]></content:encoded>
<pubDate>Sat, 15 Oct 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>A Short Chronology Of Deep Learning For Tabular Data</title>
<link>http://localhost:4000/blog/2022/deep-learning-for-tabular-data.html</link>
<guid>http://localhost:4000/blog/2022/deep-learning-for-tabular-data.html</guid>
<content:encoded><![CDATA[
Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional methods or counterexamples. So, with this short post, I aim to briefly summarize the major papers on deep tabular learning I am currently aware of. However, I want to emphasize that no matter how interesting or promising deep tabular methods look, I still recommend using a conventional machine learning method as a baseline. There is a reason why I cover conventional machine learning before deep learning in my books.
]]></content:encoded>
<pubDate>Sun, 24 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>No, We Don't Have to Choose Batch Sizes As Powers Of 2</title>
<link>http://localhost:4000/blog/2022/batch-size-2.html</link>
<guid>http://localhost:4000/blog/2022/batch-size-2.html</guid>
<content:encoded><![CDATA[
Regarding neural network training, I think we are all guilty of doing this: we choose our batch sizes as powers of 2, that is, 64, 128, 256, 512, 1024, and so forth. There are some valid theoretical justifications for this, but how does it pan out in practice? We had some discussions about that in the last couple of days, and here I want to write down some of the take-aways so I can reference them in the future. I hope you'll find this helpful as well!
]]></content:encoded>
<pubDate>Tue, 05 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud</title>
<link>http://localhost:4000/blog/2022/lightning-app-srgan-2.html</link>
<guid>http://localhost:4000/blog/2022/lightning-app-srgan-2.html</guid>
<content:encoded><![CDATA[
In this article, we will take deploy a Super Resolution App on the cloud using lightning.ai. The primary goal here is to see how easy it is to create and share a research demo. However, the cloud is for more than just model sharing: we will also learn how we can tap into additional GPU resources for model training.
]]></content:encoded>
<pubDate>Thu, 30 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App</title>
<link>http://localhost:4000/blog/2022/lightning-app-srgan-1.html</link>
<guid>http://localhost:4000/blog/2022/lightning-app-srgan-1.html</guid>
<content:encoded><![CDATA[
In this post, we will build a Lightning App. Why? Because it is 2022, and it is time to explore a more modern take on interacting with, presenting, and sharing our deep learning models. We are going to tackle this in three parts. In this first part, we will learn what a Lightning App is and how we build a Super Resolution GAN demo.
]]></content:encoded>
<pubDate>Fri, 17 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Taking Datasets, DataLoaders, and PyTorch’s New DataPipes for a Spin</title>
<link>http://localhost:4000/blog/2022/datapipes.html</link>
<guid>http://localhost:4000/blog/2022/datapipes.html</guid>
<content:encoded><![CDATA[
The PyTorch team recently announced TorchData, a prototype library focused on implementing composable and reusable data loading utilities for PyTorch. In particular, the TorchData library is centered around DataPipes, which are meant to be a DataLoader-compatible replacement for the existing Dataset class.
]]></content:encoded>
<pubDate>Sun, 12 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Running PyTorch on the M1 GPU</title>
<link>http://localhost:4000/blog/2022/pytorch-m1-gpu.html</link>
<guid>http://localhost:4000/blog/2022/pytorch-m1-gpu.html</guid>
<content:encoded><![CDATA[
Today, PyTorch officially introduced GPU support for Apple's ARM M1 chips. This is an exciting day for Mac users out there, so I spent a few minutes trying it out in practice. In this short blog post, I will summarize my experience and thoughts with the M1 chip for deep learning tasks.
]]></content:encoded>
<pubDate>Wed, 18 May 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Creating Confidence Intervals for Machine Learning Classifiers</title>
<link>http://localhost:4000/blog/2022/confidence-intervals-for-ml.html</link>
<guid>http://localhost:4000/blog/2022/confidence-intervals-for-ml.html</guid>
<content:encoded><![CDATA[
Developing good predictive models hinges upon accurate performance evaluation and comparisons. However, when evaluating machine learning models, we typically have to work around many constraints, including limited data, independence violations, and sampling biases. Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. This article outlines different methods for creating confidence intervals for machine learning models. Note that these methods also apply to deep learning.
]]></content:encoded>
<pubDate>Mon, 25 Apr 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Losses Learned</title>
<link>http://localhost:4000/blog/2022/losses-learned-part1.html</link>
<guid>http://localhost:4000/blog/2022/losses-learned-part1.html</guid>
<content:encoded><![CDATA[
The cross-entropy loss is our go-to loss for training deep learning-based classifiers. In this article, I am giving you a quick tour of how we usually compute the cross-entropy loss and how we compute it in PyTorch. There are two parts to it, and here we will look at a binary classification context first. You may wonder why bother writing this article; computing the cross-entropy loss should be relatively straightforward!? Yes and no. We can compute the cross-entropy loss in one line of code, but there's a common gotcha due to numerical optimizations under the hood. (And yes, when I am not careful, I sometimes make this mistake, too.) So, in this article, let me tell you a bit about deep learning jargon, improving numerical performance, and what could go wrong.
]]></content:encoded>
<pubDate>Mon, 04 Apr 2022 15:00:00 +0000</pubDate>
</item>
<item>
<title>TorchMetrics</title>
<link>http://localhost:4000/blog/2022/torchmetrics.html</link>
<guid>http://localhost:4000/blog/2022/torchmetrics.html</guid>
<content:encoded><![CDATA[
TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It's designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows. This iterative computation is useful if we want to track a model during iterative training or evaluation on minibatches (and optionally across on multiple GPUs). In deep learning, that's essentially *all the time*. However, when using TorchMetrics, one common question is whether we should use `.update()` or `.forward()`? (And that's also a question I certainly had when I started using it.). Here's a hands-on example and explanation.
]]></content:encoded>
<pubDate>Thu, 24 Mar 2022 13:00:00 +0000</pubDate>
</item>
<item>
<title>Machine Learning with PyTorch and Scikit-Learn</title>
<link>http://localhost:4000/blog/2022/ml-pytorch-book.html</link>
<guid>http://localhost:4000/blog/2022/ml-pytorch-book.html</guid>
<content:encoded><![CDATA[
Machine Learning with PyTorch and Scikit-Learn has been a long time in the making, and I am excited to finally get to talk about the release of my new book. Initially, this project started as the 4th edition of Python Machine Learning. However, we made so many changes to the book that we thought it deserved a new title to reflect that. So, what's new, you may wonder? In this post, I am excited to tell you all about it.
]]></content:encoded>
<pubDate>Fri, 25 Feb 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Machine Learning</title>
<link>http://localhost:4000/blog/2021/ml-course.html</link>
<guid>http://localhost:4000/blog/2021/ml-course.html</guid>
<content:encoded><![CDATA[
About half a year ago, I organized all my deep learning-related videos in a handy blog post to have everything in one place. Since many people liked this post, and because I like to use my winter break to get organized, I thought I could free two birds with one key by compiling this list below. Here, you find a list of approximately 90 machine learning lectures I recorded in 2020 and 2021! Once again, I hope this is useful to you!
]]></content:encoded>
<pubDate>Wed, 29 Dec 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Deep Learning</title>
<link>http://localhost:4000/blog/2021/dl-course.html</link>
<guid>http://localhost:4000/blog/2021/dl-course.html</guid>
<content:encoded><![CDATA[
I just sat down this morning and organized all deep learning related videos I recorded in 2021. I am sure this will be a useful reference for my future self, but I am also hoping it might be useful for one or the other person out there. PS: All code examples are in PyTorch :)
]]></content:encoded>
<pubDate>Fri, 09 Jul 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Datasets for Machine Learning and Deep Learning</title>
<link>http://localhost:4000/blog/2021/ml-dl-datasets.html</link>
<guid>http://localhost:4000/blog/2021/ml-dl-datasets.html</guid>
<content:encoded><![CDATA[
With the semester being in full swing, I recently shared this set of dataset repositories with my deep learning class. However, I thought that beyond using this list for finding inspiration for interesting student class projects, these are also good places to look for additional bechmark datasets for your model.
]]></content:encoded>
<pubDate>Thu, 11 Feb 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Deep Learning With PyTorch</title>
<link>http://localhost:4000/blog/2021/pytorch-deeplearning-review.html</link>
<guid>http://localhost:4000/blog/2021/pytorch-deeplearning-review.html</guid>
<content:encoded><![CDATA[
After its release in August 2020, Deep Learning with PyTorch has been sitting on my shelf before I finally got a chance to read it during this winter break. It turned out to be the perfect easy-going reading material for a bit of productivity after the relaxing holidays. As promised last week, here are my thoughts.
]]></content:encoded>
<pubDate>Thu, 21 Jan 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>How I Keep My Projects Organized</title>
<link>http://localhost:4000/blog/2021/project-management.html</link>
<guid>http://localhost:4000/blog/2021/project-management.html</guid>
<content:encoded><![CDATA[
Since I started my undergraduate studies in 2008, I have been obsessed with productivity tips, notetaking solutions, and todo-list management. Over the years, I tried many, many workflows and hundreds of (mostly digital) tools to keep my life, projects, and notes organized. Occasionally, I exchange ideas with friends and colleagues, and upon request, I talked about my workflow a couple of times on Twitter. After today's 2021-edition of this discussion, I thought that writing a quick and informal blogpost makes sense, making it easier to read and having a quick reference if someone asks about it again :).
]]></content:encoded>
<pubDate>Sun, 03 Jan 2021 20:00:00 +0000</pubDate>
</item>
<item>
<title>Scientific Computing in Python: Introduction to NumPy and Matplotlib</title>
<link>http://localhost:4000/blog/2020/numpy-intro.html</link>
<guid>http://localhost:4000/blog/2020/numpy-intro.html</guid>
<content:encoded><![CDATA[
Since many students in my Stat 451 (Introduction to Machine Learning and Statistical Pattern Classification) class are relatively new to Python and NumPy, I was recently devoting a lecture to the latter. Since the course notes are based on an interactive Jupyter notebook file, which I used as a basis for the lecture videos, I thought it would be worthwhile to reformat it as a blog article with the embedded 'narrated content' -- the video recordings.
]]></content:encoded>
<pubDate>Sun, 27 Sep 2020 17:00:00 +0000</pubDate>
</item>
<item>
<title>Interpretable Machine Learning</title>
<link>http://localhost:4000/blog/2020/interpretable-ml-1.html</link>
<guid>http://localhost:4000/blog/2020/interpretable-ml-1.html</guid>
<content:encoded><![CDATA[
In this blog post, I am (briefly) reviewing Christoph Molnar's *Interpretable Machine Learning Book*. Then, I am writing about two classic generalized linear models, linear and logistic regression. Mainly, this blog post explains the relationship between feature weights and predictions and demonstrates how to construct confidence intervals via Python.
]]></content:encoded>
<pubDate>Wed, 26 Aug 2020 18:00:00 +0000</pubDate>
</item>
<item>
<title>Chapter 1: Introduction to Machine Learning and Deep Learning</title>
<link>http://localhost:4000/blog/2020/intro-to-dl-ch01.html</link>
<guid>http://localhost:4000/blog/2020/intro-to-dl-ch01.html</guid>
<content:encoded><![CDATA[
The first chapter (draft) of the Introduction to Deep Learning book, which is a book based on my lecture notes and slides.
]]></content:encoded>
<pubDate>Wed, 05 Aug 2020 21:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Architects of Intelligence by Martin Ford</title>
<link>http://localhost:4000/blog/2020/book-review-1-architects-of-intelligence.html</link>
<guid>http://localhost:4000/blog/2020/book-review-1-architects-of-intelligence.html</guid>
<content:encoded><![CDATA[
A brief review of Martin Ford's book that features interviews with 23 of the most well-known and brightest minds working on AI.
]]></content:encoded>
<pubDate>Mon, 06 Jan 2020 13:00:00 +0000</pubDate>
</item>
<item>
<title>What's New in the 3rd Edition</title>
<link>http://localhost:4000/blog/2019/whats-new-in-the-3rd-edition.html</link>
<guid>http://localhost:4000/blog/2019/whats-new-in-the-3rd-edition.html</guid>
<content:encoded><![CDATA[
A brief summary of what's new in the 3rd edition of Python Machine Learning.
]]></content:encoded>
<pubDate>Thu, 12 Dec 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>My First Year at UW-Madison and a Gallery of Awesome Student Projects</title>
<link>http://localhost:4000/blog/2019/student-gallery-1.html</link>
<guid>http://localhost:4000/blog/2019/student-gallery-1.html</guid>
<content:encoded><![CDATA[
Not too long ago, in the Summer of 2018, I was super excited to join the Department of Statistics at the University of Wisconsin-Madison after obtaining my Ph.D. after ~5 long and productive years. Now, two semesters later after finals' week, I finally found some quiet days to look back on what's happened since then. In this post, I am sharing a short reflection as well as a some of the exciting projects my students were working on.
]]></content:encoded>
<pubDate>Fri, 24 May 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>http://localhost:4000/blog/2018/model-evaluation-selection-part4.html</link>
<guid>http://localhost:4000/blog/2018/model-evaluation-selection-part4.html</guid>
<content:encoded><![CDATA[
This final article in the series *Model evaluation, model selection, and algorithm selection in machine learning* presents overviews of several statistical hypothesis testing approaches, with applications to machine learning model and algorithm comparisons. This includes statistical tests based on target predictions for independent test sets (the downsides of using a single test set for model comparisons was discussed in previous articles) as well as methods for algorithm comparisons by fitting and evaluating models via cross-validation. Lastly, this article will introduce *nested cross-validation*, which has become a common and recommended a method of choice for algorithm comparisons for small to moderately-sized datasets.
]]></content:encoded>
<pubDate>Sat, 10 Nov 2018 22:00:00 +0000</pubDate>
</item>
<item>
<title>Generating Gender-Neutral Face Images with Semi-Adversarial Neural Networks to Enhance Privacy</title>
<link>http://localhost:4000/blog/2018/semi-adversarial-nets-1.html</link>
<guid>http://localhost:4000/blog/2018/semi-adversarial-nets-1.html</guid>
<content:encoded><![CDATA[
I thought that it would be nice to have short and concise summaries of recent projects handy, to share them with a more general audience, including colleagues and students. So, I challenged myself to use fewer than 1000 words without getting distracted by the nitty-gritty details and technical jargon. In this post, I mainly cover some of my recent research in collaboration with the [iPRoBe Lab](http://iprobe.cse.msu.edu) that falls under the broad category of developing approaches to hide specific information in face images. The research discussed in this post is about "maximizing privacy while preserving utility."
]]></content:encoded>
<pubDate>Thu, 02 Aug 2018 05:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>http://localhost:4000/blog/2016/model-evaluation-selection-part3.html</link>
<guid>http://localhost:4000/blog/2016/model-evaluation-selection-part3.html</guid>
<content:encoded><![CDATA[
Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset. In Part I and Part II, we saw different holdout and bootstrap techniques for estimating the generalization performance of a model. We learned about the bias-variance trade-off, and we computed the uncertainty of our estimates. In this third part, we will focus on different methods of cross-validation for model evaluation and model selection. We will use these cross-validation techniques to rank models from several hyperparameter configurations and estimate how well they generalize to independent datasets.
]]></content:encoded>
<pubDate>Sun, 02 Oct 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>http://localhost:4000/blog/2016/model-evaluation-selection-part2.html</link>
<guid>http://localhost:4000/blog/2016/model-evaluation-selection-part2.html</guid>
<content:encoded><![CDATA[
In this second part of this series, we will look at some advanced techniques for model evaluation and techniques to estimate the uncertainty of our estimated model performance as well as its variance and stability. Then, in the next article, we will shift the focus onto another task that is one of the main pillar of successful, real-world machine learning applications -- Model Selection.
]]></content:encoded>
<pubDate>Sat, 13 Aug 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>http://localhost:4000/blog/2016/model-evaluation-selection-part1.html</link>
<guid>http://localhost:4000/blog/2016/model-evaluation-selection-part1.html</guid>
<content:encoded><![CDATA[
Machine learning has become a central part of our life -- as consumers, customers, and hopefully as researchers and practitioners! Whether we are applying predictive modeling techniques to our research or business problems, I believe we have one thing in common  &#58; We want to make good predictions! Fitting a model to our training data is one thing, but how do we know that it generalizes well to unseen data? How do we know that it doesn't simply memorize the data we fed it and fails to make good predictions on future samples, samples that it hasn't seen before? And how do we select a good model in the first place? Maybe a different learning algorithm could be better-suited for the problem at hand? Model evaluation is certainly not just the end point of our machine learning pipeline.<br /><br />Before we handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this article, we will go over a selection of these techniques, and we will see how they fit into the bigger picture, a typical machine learning workflow.
]]></content:encoded>
<pubDate>Sat, 11 Jun 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Writing 'Python Machine Learning'</title>
<link>http://localhost:4000/blog/2015/writing-pymle.html</link>
<guid>http://localhost:4000/blog/2015/writing-pymle.html</guid>
<content:encoded><![CDATA[
It's been about time. I am happy to announce that "Python Machine Learning" was finally released today! Sure, I could just send an email around to all the people who were interested in this book. On the other hand, I could put down those 140 characters on Twitter (minus what it takes to insert a hyperlink) and be done with it. Even so, writing "Python Machine Learning" really was quite a journey for a few months, and I would like to sit down in my favorite coffeehouse once more to say a few words about this experience.
]]></content:encoded>
<pubDate>Thu, 24 Sep 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Python, Machine Learning, and Language Wars</title>
<link>http://localhost:4000/blog/2015/why-python.html</link>
<guid>http://localhost:4000/blog/2015/why-python.html</guid>
<content:encoded><![CDATA[
This has really been quite a journey for me lately. And regarding the frequently asked question “Why did you choose Python for Machine Learning?” I guess it is about time to write my script. In this article, I really don’t mean to tell you why you or anyone else should use Python. But read on if you are interested in my opinion.
]]></content:encoded>
<pubDate>Mon, 24 Aug 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Single-Layer Neural Networks and Gradient Descent</title>
<link>http://localhost:4000/Articles/2015_singlelayer_neurons.html</link>
<guid>http://localhost:4000/Articles/2015_singlelayer_neurons.html</guid>
<content:encoded><![CDATA[
This article offers a brief glimpse of the history and basic concepts of machine learning. We will take a look at the first algorithmically described neural network and the gradient descent algorithm in context of adaptive linear neurons, which will not only introduce the principles of machine learning but also serve as the basis for modern multilayer neural networks in future articles.
]]></content:encoded>
<pubDate>Tue, 24 Mar 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Principal Component Analysis</title>
<link>http://localhost:4000/Articles/2015_pca_in_3_steps.html</link>
<guid>http://localhost:4000/Articles/2015_pca_in_3_steps.html</guid>
<content:encoded><![CDATA[
Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a “black box”, and we are going to unravel its internals in 3 basic steps.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2015 16:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Weighted Majority Rule Ensemble Classifier</title>
<link>http://localhost:4000/Articles/2014_ensemble_classifier.html</link>
<guid>http://localhost:4000/Articles/2014_ensemble_classifier.html</guid>
<content:encoded><![CDATA[
Here, I want to present a simple and conservative approach of implementing a weighted majority rule ensemble classifier in scikit-learn that yielded remarkably good results when I tried it in a kaggle competition. For me personally, kaggle competitions are just a nice way to try out and compare different approaches and ideas -- basically an opportunity to learn in a controlled environment with nice datasets.
]]></content:encoded>
<pubDate>Sun, 11 Jan 2015 18:00:00 +0000</pubDate>
</item>
<item>
<title>MusicMood</title>
<link>http://localhost:4000/blog/2014/musicmood.html</link>
<guid>http://localhost:4000/blog/2014/musicmood.html</guid>
<content:encoded><![CDATA[
In this article, I want to share my experience with a recent data mining project which probably was one of my most favorite hobby projects so far. It's all about building a classification model that can automatically predict the mood of music based on song lyrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>Turn Your Twitter Timeline into a Word Cloud</title>
<link>http://localhost:4000/Articles/2014_twitter_wordcloud.html</link>
<guid>http://localhost:4000/Articles/2014_twitter_wordcloud.html</guid>
<content:encoded><![CDATA[
Last week, I posted some visualizations in context of Happy Rock Song data mining project, and some people were curious about how I created the word clouds. Learn how to create YOUR personal Twitter Timeline!
]]></content:encoded>
<pubDate>Fri, 28 Nov 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Naive Bayes and Text Classification</title>
<link>http://localhost:4000/Articles/2014_naive_bayes_1.html</link>
<guid>http://localhost:4000/Articles/2014_naive_bayes_1.html</guid>
<content:encoded><![CDATA[
Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes’ probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction. In this first part of a series, we will take a look at the theory of naive Bayes classifiers and introduce the basic concepts of text classification. In following articles, we will implement those concepts to train a naive Bayes spam filter and apply naive Bayes to song classification based on lyrics.
]]></content:encoded>
<pubDate>Sat, 04 Oct 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</title>
<link>http://localhost:4000/Articles/2014_kernel_pca.html</link>
<guid>http://localhost:4000/Articles/2014_kernel_pca.html</guid>
<content:encoded><![CDATA[
The focus of this article is to briefly introduce the idea of kernel methods and to implement a Gaussian radius basis function (RBF) kernel that is used to perform nonlinear dimensionality reduction via KBF kernel principal component analysis (kPCA).
]]></content:encoded>
<pubDate>Sun, 14 Sep 2014 20:00:00 +0000</pubDate>
</item>
<item>
<title>Predictive modeling, supervised machine learning, and pattern classification</title>
<link>http://localhost:4000/Articles/2014_intro_supervised_learning.html</link>
<guid>http://localhost:4000/Articles/2014_intro_supervised_learning.html</guid>
<content:encoded><![CDATA[
When I was working on my next pattern classification application, I realized that it might be worthwhile to take a step back and look at the big picture of pattern classification in order to put my previous topics into context and to provide and introduction for the future topics that are going to follow.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2014 08:00:00 +0000</pubDate>
</item>
<item>
<title>Linear Discriminant Analysis</title>
<link>http://localhost:4000/Articles/2014_python_lda.html</link>
<guid>http://localhost:4000/Articles/2014_python_lda.html</guid>
<content:encoded><![CDATA[
I received a lot of positive feedback about the step-wise Principal Component Analysis (PCA) implementation. Thus, I decided to write a little follow-up about Linear Discriminant Analysis (LDA) — another useful linear transformation technique. Both LDA and PCA are commonly used dimensionality reduction techniques in statistics, pattern classification, and machine learning applications. By implementing the LDA step-by-step in Python, we will see and understand how it works, and we will compare it to a PCA to see how it differs.
]]></content:encoded>
<pubDate>Sun, 03 Aug 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>Dixon's Q test for outlier identification</title>
<link>http://localhost:4000/Articles/2014_dixon_test.html</link>
<guid>http://localhost:4000/Articles/2014_dixon_test.html</guid>
<content:encoded><![CDATA[
I recently faced the impossible task to identify outliers in a dataset with very, very small sample sizes and Dixon's Q test caught my attention. Honestly, I am not a big fan of this statistical test, but since Dixon's Q-test is still quite popular in certain scientific fields (e.g., chemistry) that it is important to understand its principles in order to draw your own conclusion of the presented research data that you might stumble upon in research articles or scientific talks.
]]></content:encoded>
<pubDate>Sat, 19 Jul 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>About Feature Scaling and Normalization</title>
<link>http://localhost:4000/Articles/2014_about_feature_scaling.html</link>
<guid>http://localhost:4000/Articles/2014_about_feature_scaling.html</guid>
<content:encoded><![CDATA[
I received a couple of questions in response to my previous article (Entry point: Data) where people asked me why I used Z-score standardization as feature scaling method prior to the PCA. I added additional information to the original article, however, I thought that it might be worthwhile to write a few more lines about this important topic in a separate article.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Entry Point Data</title>
<link>http://localhost:4000/Articles/2014_scikit_dataprocessing.html</link>
<guid>http://localhost:4000/Articles/2014_scikit_dataprocessing.html</guid>
<content:encoded><![CDATA[
In this short tutorial I want to provide a short overview of some of my favorite Python tools for common procedures as entry points for general pattern classification and machine learning tasks, and various other data analyses.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Molecular docking, estimating free energies of binding, and AutoDock's semi-empirical force field</title>
<link>http://localhost:4000/Articles/2014_autodock_energycomps.html</link>
<guid>http://localhost:4000/Articles/2014_autodock_energycomps.html</guid>
<content:encoded><![CDATA[
Discussions and questions about methods, approaches, and tools for estimating (relative) binding free energies of protein-ligand complexes are quite popular, and even the simplest tools can be quite tricky to use. Here, I want to briefly summarize the idea of molecular docking, and give a short overview about how we can use AutoDock 4.2's hybrid approach for evaluating binding affinities.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>An introduction to parallel programming using Python's multiprocessing module</title>
<link>http://localhost:4000/Articles/2014_multiprocessing.html</link>
<guid>http://localhost:4000/Articles/2014_multiprocessing.html</guid>
<content:encoded><![CDATA[
The default Python interpreter was designed with simplicity in mind and has a thread-safe mechanism, the so-called "GIL" (Global Interpreter Lock). In order to prevent conflicts between threads, it executes only one statement at a time (so-called serial processing, or single-threading). In this introduction to Python's multiprocessing module, we will see how we can spawn multiple subprocesses to avoid some of the GIL's disadvantages and make best use of the multiple cores in our CPU.
]]></content:encoded>
<pubDate>Fri, 20 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel density estimation via the Parzen-Rosenblatt window method</title>
<link>http://localhost:4000/Articles/2014_kernel_density_est.html</link>
<guid>http://localhost:4000/Articles/2014_kernel_density_est.html</guid>
<content:encoded><![CDATA[
The Parzen-window method (also known as Parzen-Rosenblatt window method) is a widely used non-parametric approach to estimate a probability density function *p(**x**)* for a specific point *p(**x**)* from a sample *p(**x**<sub>n</sub>)* that doesn't require any knowledge or assumption about the underlying distribution.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Numeric matrix manipulation</title>
<link>http://localhost:4000/Articles/2014_matrix_cheatsheet.html</link>
<guid>http://localhost:4000/Articles/2014_matrix_cheatsheet.html</guid>
<content:encoded><![CDATA[
At its core, this article is about a simple cheat sheet for basic operations on numeric matrices, which can be very useful if you working and experimenting with some of the most popular languages that are used for scientific computing, statistics, and data analysis.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>The key differences between Python 2.7.x and Python 3.x with examples</title>
<link>http://localhost:4000/Articles/2014_python_2_3_key_diff.html</link>
<guid>http://localhost:4000/Articles/2014_python_2_3_key_diff.html</guid>
<content:encoded><![CDATA[
Many beginning Python users are wondering with which version of Python they should start. My answer to this question is usually something along the lines 'just go with the version your favorite tutorial was written in, and check out the differences later on.'\ But what if you are starting a new project and have the choice to pick? I would say there is currently no 'right' or 'wrong' as long as both Python 2.7.x and Python 3.x support the libraries that you are planning to use. However, it is worthwhile to have a look at the major differences between those two most popular versions of Python to avoid common pitfalls when writing the code for either one of them, or if you are planning to port your project.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>5 simple steps for converting Markdown documents into HTML and adding Python syntax highlighting</title>
<link>http://localhost:4000/Articles/2014_markdown_syntax_color.html</link>
<guid>http://localhost:4000/Articles/2014_markdown_syntax_color.html</guid>
<content:encoded><![CDATA[
In this little tutorial, I want to show you in 5 simple steps how easy it is to add code syntax highlighting to your blog articles.
]]></content:encoded>
<pubDate>Wed, 28 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Creating a table of contents with internal links in IPython Notebooks and Markdown documents</title>
<link>http://localhost:4000/Articles/2014_ipython_internal_links.html</link>
<guid>http://localhost:4000/Articles/2014_ipython_internal_links.html</guid>
<content:encoded><![CDATA[
Many people have asked me how I create the table of contents with internal links for my IPython Notebooks and Markdown documents on GitHub. Well, no (IPython) magic is involved, it is just a little bit of HTML, but I thought it might be worthwhile to write this little how-to tutorial.
]]></content:encoded>
<pubDate>Tue, 20 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A Beginner's Guide to Python's Namespaces, Scope Resolution, and the LEGB Rule</title>
<link>http://localhost:4000/Articles/2014_python_scope_and_namespaces.html</link>
<guid>http://localhost:4000/Articles/2014_python_scope_and_namespaces.html</guid>
<content:encoded><![CDATA[
A short tutorial about Python's namespaces and the scope resolution for variable names using the LEGB-rule with little quiz-like exercises.
]]></content:encoded>
<pubDate>Mon, 12 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Diving deep into Python</title>
<link>http://localhost:4000/Articles/2014_deep_python.html</link>
<guid>http://localhost:4000/Articles/2014_deep_python.html</guid>
<content:encoded><![CDATA[
Some while ago, I started to collect some of the not-so-obvious things I encountered when I was coding in Python. I thought that it was worthwhile sharing them and encourage you to take a brief look at the section-overview and maybe you'll find something that you do not already know - I can guarantee you that it'll likely save you some time at one or the other tricky debugging challenge.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Principal Component Analysis (PCA)</title>
<link>http://localhost:4000/Articles/2014_pca_step_by_step.html</link>
<guid>http://localhost:4000/Articles/2014_pca_step_by_step.html</guid>
<content:encoded><![CDATA[
In this article I want to explain how a Principal Component Analysis (PCA) works by implementing it in Python step by step. At the end we will compare the results to the more convenient Python PCA() classes that are available through the popular matplotlib and scipy libraries and discuss how they differ.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Installing Scientific Packages for Python3 on MacOS 10.9 Mavericks</title>
<link>http://localhost:4000/Articles/2014_install_python_sci_pkgs.html</link>
<guid>http://localhost:4000/Articles/2014_install_python_sci_pkgs.html</guid>
<content:encoded><![CDATA[
I just went through some pain (again) when I wanted to install some of Python's scientific libraries on my second Mac. I summarized the setup and installation process for future reference.\ If you encounter any different or additional obstacles let me know, and please feel free to make any suggestions to improve this short walkthrough.
]]></content:encoded>
<pubDate>Thu, 13 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A thorough guide to SQLite database operations in Python</title>
<link>http://localhost:4000/Articles/2014_sqlite_in_python_tutorial.html</link>
<guid>http://localhost:4000/Articles/2014_sqlite_in_python_tutorial.html</guid>
<content:encoded><![CDATA[
After I wrote the initial teaser article "SQLite - Working with large data sets in Python effectively" about how awesome SQLite databases are via sqlite3 in Python, I wanted to delve a little bit more into the SQLite syntax and provide you with some more hands-on examples.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Using OpenEye software for substructure alignments</title>
<link>http://localhost:4000/Articles/2014_openeye_alignments_overlays.html</link>
<guid>http://localhost:4000/Articles/2014_openeye_alignments_overlays.html</guid>
<content:encoded><![CDATA[
This is a quickguide showing how to use OpenEye software command line tools to align target molecules to a query based on substructure matches and how to retrieve the best molecule overlay from two sets of low-energy conformers.
]]></content:encoded>
<pubDate>Sun, 23 Feb 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Unit testing in Python</title>
<link>http://localhost:4000/Articles/2013_python_unittest.html</link>
<guid>http://localhost:4000/Articles/2013_python_unittest.html</guid>
<content:encoded><![CDATA[
Let’s be honest, code testing is everything but a joyful task. However, a good unit testing framework makes this process as smooth as possible. Eventually, testing becomes a regular and continuous process, accompanied by the assurance that our code will operate just as exact and seamlessly as a Swiss clockwork.
]]></content:encoded>
<pubDate>Sat, 14 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>A short tutorial for decent heat maps in R</title>
<link>http://localhost:4000/Articles/heatmaps_in_r.html</link>
<guid>http://localhost:4000/Articles/heatmaps_in_r.html</guid>
<content:encoded><![CDATA[
I received many questions from people who want to quickly visualize their data via heat maps - ideally as quickly as possible. This is the major issue of exploratory data analysis, since we often don’t have the time to digest whole books about the particular techniques in different software packages to just get the job done. But once we are happy with our initial results, it might be worthwhile to dig deeper into the topic in order to further customize our plots and maybe even polish them for publication. In this post, my aim is to briefly introduce one of R’s several heat map libraries for a simple data analysis. I chose R, because it is one of the most popular free statistical software packages around. Of course there are many more tools out there to produce similar results (and even in R there are many different packages for heat maps), but I will leave this as an open topic for another time.
]]></content:encoded>
<pubDate>Sun, 08 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>SQLite</title>
<link>http://localhost:4000/Articles/2013_sqlite_database.html</link>
<guid>http://localhost:4000/Articles/2013_sqlite_database.html</guid>
<content:encoded><![CDATA[
My new project confronted me with the task of screening a massive set of large data files in text format with billions of entries each. I will have to retrieve data repeatedly and frequently in the future. Thus, I was tempted to find a better solution than brute-force scanning through ~20 separate 1-column text files with ~6 billion entries every time line by line.
]]></content:encoded>
<pubDate>Sun, 03 Nov 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>理解LLM评估的四种主要方法</title>
<link>https://sebastianraschka.com/blog/2025/llm-evaluation-4-approaches.html</link>
<guid>https://sebastianraschka.com/blog/2025/llm-evaluation-4-approaches.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">文章介绍了LLM评估的四种主要方法及其代码实现。</p><br /><br /><p><strong>摘要：</strong> 本文详细介绍了评估大型语言模型（LLM）的四种主要方法：多项选择基准、验证器、排行榜和LLM法官。每种方法都有其优缺点，适用于不同的评估场景。文章通过具体的代码示例展示了如何实现这些方法，帮助读者更好地理解它们的工作原理。此外，还讨论了不同评估方法在实际应用中的适用性和局限性，强调了结合多种评估方法的重要性。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/llm-evaluation-4-approaches.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sun, 05 Oct 2025 00:06:00 +0000</pubDate>
</item>
<item>
<title>2025年开放权重大模型架构解析与代码实践</title>
<link>https://sebastianraschka.com/blog/2025/qwen3-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2025/qwen3-from-scratch.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探索2025年主流开放权重模型架构及其代码实现。</p><br /><br /><p><strong>摘要：</strong> 文章回顾了2025年最具代表性的开放权重大模型架构，并深入分析了其架构组件。作者通过实际代码演示，帮助读者理解这些模型的内部工作原理，提供可复用的构建模块以支持个人实验或项目开发。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/qwen3-from-scratch.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 06 Sep 2025 08:00:00 +0000</pubDate>
</item>
<item>
<title>OpenAI发布首个开源大模型GPT-OSS-120B与GPT-OSS-20B</title>
<link>https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html</link>
<guid>https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">OpenAI推出两款开源大模型，支持本地运行。</p><br /><br /><p><strong>摘要：</strong> OpenAI最近发布了两款新的开源大模型GPT-OSS-120B和GPT-OSS-20B，这是自2019年GPT-2以来首次推出的开源模型。通过一些优化技术，这些模型可以在本地运行，降低了使用门槛。作者在过去的几天里研究了代码和技术报告，总结了其中最有趣的技术细节。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 09 Aug 2025 11:00:00 +0000</pubDate>
</item>
<item>
<title>大语言模型架构演进与性能分析</title>
<link>https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html</link>
<guid>https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">大语言模型架构在多年间保持高度相似性。</p><br /><br /><p><strong>摘要：</strong> 文章回顾了从GPT-2到DeepSeek-V3和Llama 4等大语言模型的架构演变，指出尽管这些模型在时间跨度上相差数年，但其结构仍保持高度相似。由于数据集、训练技术和超参数的差异较大且常未公开，直接比较模型性能较为困难。因此，作者认为通过分析模型架构的变化，可以更好地理解当前大语言模型开发的趋势和方向。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Jul 2025 06:00:00 +0000</pubDate>
</item>
<item>
<title>2025年1月至6月的大型语言模型研究论文汇总</title>
<link>https://sebastianraschka.com/blog/2025/llm-research-papers-the-2025-list-january-to-june.html</link>
<guid>https://sebastianraschka.com/blog/2025/llm-research-papers-the-2025-list-january-to-june.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">2025年上半年LLM研究论文按主题分类整理。</p><br /><br /><p><strong>摘要：</strong> 本文是对2025年1月至6月期间发布的大量大型语言模型（LLM）研究论文的整理和分类。文章按照主题划分为多个部分，包括推理模型、强化学习方法、高效训练与架构、扩散语言模型、多模态与视觉-语言模型等。作者特别强调了今年的研究重点是提升LLM的推理能力，并详细列出了相关论文，涵盖训练策略、推理时间优化、模型评估等多个方面。此外，作者还提到未来将对部分重要论文进行更深入的分析和讨论。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/llm-research-papers-the-2025-list-january-to-june.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 01 Jul 2025 06:06:11 +0000</pubDate>
</item>
<item>
<title>KV缓存机制在LLM高效推理中的原理与实现</title>
<link>https://sebastianraschka.com/blog/2025/coding-the-kv-cache-in-llms.html</link>
<guid>https://sebastianraschka.com/blog/2025/coding-the-kv-cache-in-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文解释了KV缓存在LLM生产环境中的工作原理及代码实现。</p><br /><br /><p><strong>摘要：</strong> KV缓存是大规模语言模型（LLMs）在线推理过程中提高计算效率的关键技术之一。通过缓存先前计算的结果，可以显著减少后续推理所需的计算量。本文从概念层面深入解析KV缓存的工作机制，并提供了一个从零开始、易于理解的人类可读代码实现示例。这种详细展示不仅帮助开发者更好地理解KV缓存的作用，还为构建高效推理系统提供了实用参考。无论是对理论研究者还是实际应用开发人员而言，这篇文章都具有重要的指导意义。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/coding-the-kv-cache-in-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Tue, 17 Jun 2025 08:00:00 +0000</pubDate>
</item>
<item>
<title>从零构建大型语言模型：完整课程</title>
<link>https://sebastianraschka.com/blog/2025/coding-llms-from-the-ground-up-a-complete-course.html</link>
<guid>https://sebastianraschka.com/blog/2025/coding-llms-from-the-ground-up-a-complete-course.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">作者分享如何从零开始构建大型语言模型的教程。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了作者近期录制的一系列视频教程，旨在通过从零开始构建大型语言模型（LLMs）的方式帮助读者深入理解其工作原理。这些教程涵盖了设置代码环境、处理文本数据、实现注意力机制、搭建模型架构、无监督预训练以及微调等内容。尽管作者因健康问题暂时无法继续新内容创作，但希望分享这些已有资源能对读者有所帮助。关键词方面，重点涉及‘LLM’、‘从零构建’和‘教程’。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/coding-llms-from-the-ground-up-a-complete-course.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 10 May 2025 00:00:00 +0000</pubDate>
</item>
<item>
<title>强化学习推理技术在大模型中的最新进展</title>
<link>https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</link>
<guid>https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习提升大模型推理能力的最新发展。</p><br /><br /><p><strong>摘要：</strong> 本月科技界迎来多个旗舰模型发布，如GPT-4.5和Llama 4，但市场反应平淡，主要因为这些模型缺乏基于强化学习的推理优化。相比之下，OpenAI发布的o3推理模型展示了战略性计算投入的价值，尤其是针对推理任务的强化学习方法。尽管推理并非万能钥匙，但它显著提高了复杂任务中的准确性与问题解决能力。预计未来，这种以推理为中心的后训练方式将成为大型语言模型的标准流程。本文将深入分析强化学习推理技术的最新进展及其潜在影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
</item>
<item>
<title>First Look at Reasoning From Scratch: Chapter 1</title>
<link>https://sebastianraschka.com/blog/2025/first-look-at-reasoning-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2025/first-look-at-reasoning-from-scratch.html</guid>
<content:encoded><![CDATA[
As you know, I've been writing a lot lately about the latest research on reasoning in LLMs. Before my next research-focused blog post, I wanted to offer something special to my paid subscribers as a thank-you for your ongoing support. So, I've started writing a new book on how reasoning works in LLMs, and here I'm sharing the first Chapter 1 with you. This ~15-page chapter is an introduction reasoning in the context of LLMs and provides an overview of methods like inference-time scaling and reinforcement learning. Thanks for your support! I hope you enjoy the chapter, and stay tuned for my next blog post on reasoning research!
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>2025年大型语言模型推理能力的最新进展</title>
<link>https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</link>
<guid>https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了2025年大型语言模型推理能力的提升方法，重点是推理时间计算的最新研究进展。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了2025年在大型语言模型（LLM）推理能力方面的新进展，特别是推理时间计算的提升方法。随着研究不断深入，提高LLM的推理能力成为热门课题，涉及的技术包括推理时间计算的扩展、强化学习、监督微调和模型蒸馏等多种策略。文章详细阐述了推理模型的四大类实现方法，强调了推理时间计算的有效性，如链式思维（CoT）提示和预算强制等技术，为LLM在复杂任务中的应用提供了新的视角。尽管推理时间计算能显著改善小型模型的表现，但也要考虑其增加的计算成本，使得在使用和成本之间进行权衡成为必要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>增强大型语言模型的推理能力的四种主要方法</title>
<link>https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html</link>
<guid>https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升大型语言模型推理能力的四种主要方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了四种主要的方法来构建推理模型，从而增强大型语言模型（LLMs）的推理能力。这些方法提供了有效的框架，帮助研究者和工程师应对快速发展的相关文献和市场炒作。通过深入分析每种方法的特点和应用，本文旨在为读者提供有价值的见解，以便更好地理解如何在实际应用中实现推理能力的提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Feb 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Noteworthy LLM Research Papers of 2024</title>
<link>https://sebastianraschka.com/blog/2025/llm-research-2024.html</link>
<guid>https://sebastianraschka.com/blog/2025/llm-research-2024.html</guid>
<content:encoded><![CDATA[
This article covers 12 influential AI research papers of 2024, ranging from mixture-of-experts models to new LLM scaling laws for precision.
]]></content:encoded>
<pubDate>Thu, 23 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch</title>
<link>https://sebastianraschka.com/blog/2025/bpe-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2025/bpe-from-scratch.html</guid>
<content:encoded><![CDATA[
This is a standalone notebook implementing the popular byte pair encoding (BPE) tokenization algorithm, which is used in models like GPT-2 to GPT-4, Llama 3, etc., from scratch for educational purposes."
]]></content:encoded>
<pubDate>Fri, 17 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>LLM Research Papers: The 2024 List</title>
<link>https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html</link>
<guid>https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html</guid>
<content:encoded><![CDATA[
I want to share my running bookmark list of many fascinating (mostly LLM-related) papers I stumbled upon in 2024. It's just a list, but maybe it will come in handy for those who are interested in finding some gems to read for the holidays.
]]></content:encoded>
<pubDate>Sun, 29 Dec 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Understanding Multimodal LLMs</title>
<link>https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html</link>
<guid>https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html</guid>
<content:encoded><![CDATA[
There has been a lot of new research on the multimodal LLM front, including the latest Llama 3.2 vision models, which employ diverse architectural strategies to integrate various data types like text and images. For instance, The decoder-only method uses a single stack of decoder blocks to process all modalities sequentially. On the other hand, cross-attention methods (for example, used in Llama 3.2) involve separate encoders for different modalities with a cross-attention layer that allows these encoders to interact. This article explains how these different types of multimodal LLMs function. Additionally, I will review and summarize roughly a dozen other recent multimodal papers and models published in recent weeks to compare their approaches.
]]></content:encoded>
<pubDate>Sun, 03 Nov 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Building A GPT-Style LLM Classifier From Scratch</title>
<link>https://sebastianraschka.com/blog/2024/building-a-gpt-style-llm-classifier.html</link>
<guid>https://sebastianraschka.com/blog/2024/building-a-gpt-style-llm-classifier.html</guid>
<content:encoded><![CDATA[
This article shows you how to transform pretrained large language models (LLMs) into strong text classifiers. But why focus on classification? First, finetuning a pretrained model for classification offers a gentle yet effective introduction to model finetuning. Second, many real-world and business challenges revolve around text classification: spam detection, sentiment analysis, customer feedback categorization, topic labeling, and more.
]]></content:encoded>
<pubDate>Sat, 21 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Building LLMs from the Ground Up: A 3-hour Coding Workshop</title>
<link>https://sebastianraschka.com/blog/2024/building-llms-from-the-ground-up.html</link>
<guid>https://sebastianraschka.com/blog/2024/building-llms-from-the-ground-up.html</guid>
<content:encoded><![CDATA[
This tutorial is aimed at coders interested in understanding the building blocks of large language models (LLMs), how LLMs work, and how to code them from the ground up in PyTorch. We will kick off this tutorial with an introduction to LLMs, recent milestones, and their use cases. Then, we will code a small GPT-like LLM, including its data input pipeline, core architecture components, and pretraining code ourselves. After understanding how everything fits together and how to pretrain an LLM, we will learn how to load pretrained weights and finetune LLMs using open-source libraries.
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>New LLM Pre-training and Post-training Paradigms</title>
<link>https://sebastianraschka.com/blog/2024/new-llm-pre-training-and-post-training.html</link>
<guid>https://sebastianraschka.com/blog/2024/new-llm-pre-training-and-post-training.html</guid>
<content:encoded><![CDATA[
There are hundreds of LLM papers each month proposing new techniques and approaches. However, one of the best ways to see what actually works well in practice is to look at the pre-training and post-training pipelines of the most recent state-of-the-art models. Luckily, four major new LLMs have been released in the last months, accompanied by relatively detailed technical reports. In this article, I focus on the pre-training and post-training pipelines of the following models: Alibaba's Qwen 2, Apple Intelligence Foundation Language Models, Google's Gemma 2, Meta AI's Llama 3.1.
]]></content:encoded>
<pubDate>Sat, 17 Aug 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Instruction Pretraining LLMs</title>
<link>https://sebastianraschka.com/blog/2024/instruction-pretraining-llms.html</link>
<guid>https://sebastianraschka.com/blog/2024/instruction-pretraining-llms.html</guid>
<content:encoded><![CDATA[
This article covers a new, cost-effective method for generating data for instruction finetuning LLMs; instruction finetuning from scratch; pretraining LLMs with instruction data; and an overview of what's new in Gemma 2.
]]></content:encoded>
<pubDate>Sat, 20 Jul 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Developing an LLM: Building, Training, Finetuning</title>
<link>https://sebastianraschka.com/blog/2024/llms-building-training-finetuning.html</link>
<guid>https://sebastianraschka.com/blog/2024/llms-building-training-finetuning.html</guid>
<content:encoded><![CDATA[
This is an overview of the LLM development process. This one-hour talk focuses on the essential three stages of developing an LLM: coding the architecture, implementing pretraining, and fine-tuning the LLM. Lastly, we also discuss the main ways LLMs are evaluated, along with the caveats of each method.
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments?</title>
<link>https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html</link>
<guid>https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html</guid>
<content:encoded><![CDATA[
This article covers three new papers related to instruction finetuning and parameter-efficient finetuning with LoRA in large language models (LLMs). I work with these methods on a daily basis, so it's always exciting to see new research that provides practical insights.
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?</title>
<link>https://sebastianraschka.com/blog/2024/how-good-open-llm.html</link>
<guid>https://sebastianraschka.com/blog/2024/how-good-open-llm.html</guid>
<content:encoded><![CDATA[
What a month! We had four major open LLM releases: Mixtral, Meta AI's Llama 3, Microsoft's Phi-3, and Apple's OpenELM. In my new article, I review and discuss all four of these major transformer-based LLM model releases, followed by new research on reinforcement learning with human feedback methods for instruction finetuning using PPO and DPO algorithms.
]]></content:encoded>
<pubDate>Sun, 12 May 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Using and Finetuning Pretrained Transformers</title>
<link>https://sebastianraschka.com/blog/2024/using-finetuning-transformers.html</link>
<guid>https://sebastianraschka.com/blog/2024/using-finetuning-transformers.html</guid>
<content:encoded><![CDATA[
What are the different ways to use and finetune pretrained large language models (LLMs)? The three most common ways to use and finetune pretrained LLMs include a feature-based approach, in-context prompting, and updating a subset of the model parameters. First, most pretrained LLMs or language transformers can be utilized without the need for further finetuning. For instance, we can employ a feature-based method to train a new downstream model, such as a linear classifier, using embeddings generated by a pretrained transformer. Second, we can showcase examples of a new task within the input itself, which means we can directly exhibit the expected outcomes without requiring any updates or learning from the model. This concept is also known as prompting. Finally, it’s also possible to finetune all or just a small number of parameters to achieve the desired outcomes. This article discusses these types of approaches in greater depth
]]></content:encoded>
<pubDate>Sat, 20 Apr 2024 07:00:00 +0000</pubDate>
</item>
<item>
<title>Tips for LLM Pretraining and Evaluating Reward Models</title>
<link>https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html</link>
<guid>https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html</guid>
<content:encoded><![CDATA[
It's another month in AI research, and it's hard to pick favorites. This month, I am going over a paper that discusses strategies for the continued pretraining of LLMs, followed by a discussion of reward modeling used in reinforcement learning with human feedback (a popular LLM alignment method), along with a new benchmark. Continued pretraining for LLMs is an important topic because it allows us to update existing LLMs, for instance, ensuring that these models remain up-to-date with the latest information and trends. Also, it allows us to adapt them to new target domains without having them to retrain from scratch. Reward modeling is important because it allows us to align LLMs more closely with human preferences and, to some extent, helps with safety. But beyond human preference optimization, it also provides a mechanism for learning and adapting LLMs to complex tasks by providing instruction-output examples where explicit programming of correct behavior is challenging or impractical.
]]></content:encoded>
<pubDate>Sun, 31 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>Research Papers in February 2024</title>
<link>https://sebastianraschka.com/blog/2024/research-papers-in-february-2024.html</link>
<guid>https://sebastianraschka.com/blog/2024/research-papers-in-february-2024.html</guid>
<content:encoded><![CDATA[
Once again, this has been an exciting month in AI research. This month, I'm covering two new openly available LLMs, insights into small finetuned LLMs, and a new parameter-efficient LLM finetuning technique. The two LLMs mentioned above stand out for several reasons. One LLM (OLMo) is completely open source, meaning that everything from the training code to the dataset to the log files is openly shared. The other LLM (Gemma) also comes with openly available weights but achieves state-of-the-art performance on several benchmarks and outperforms popular LLMs of similar size, such as Llama 2 7B and Mistral 7B, by a large margin.
]]></content:encoded>
<pubDate>Sun, 03 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch</title>
<link>https://sebastianraschka.com/blog/2024/lora-dora.html</link>
<guid>https://sebastianraschka.com/blog/2024/lora-dora.html</guid>
<content:encoded><![CDATA[
Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters. In this article, we will take a look at both LoRA and DoRA, which is a new promising alternative to LoRA.
]]></content:encoded>
<pubDate>Sun, 18 Feb 2024 08:00:00 +0000</pubDate>
</item>
<item>
<title>Optimizing LLMs From a Dataset Perspective</title>
<link>https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html</link>
<guid>https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html</guid>
<content:encoded><![CDATA[
This article focuses on improving the modeling performance of LLMs by finetuning them using carefully curated datasets. Specifically, this article highlights strategies that involve modifying, utilizing, or manipulating the datasets for instruction-based finetuning rather than altering the model architecture or training algorithms (the latter will be topics of a future article). This article will also explain how you can prepare your own datasets to finetune open-source LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Sep 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>The NeurIPS 2023 LLM Efficiency Challenge Starter Guide</title>
<link>https://sebastianraschka.com/blog/2023/neurips2023-starter-guide.html</link>
<guid>https://sebastianraschka.com/blog/2023/neurips2023-starter-guide.html</guid>
<content:encoded><![CDATA[
Large language models (LLMs) offer one of the most interesting opportunities for developing more efficient training methods. A few weeks ago, the NeurIPS 2023 LLM Efficiency Challenge launched to focus on efficient LLM finetuning, and this guide is a short walkthrough explaining how to participate in this competition. This article covers everything you need to know, from setting up the coding environment to making the first submission.
]]></content:encoded>
<pubDate>Thu, 10 Aug 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch</title>
<link>https://sebastianraschka.com/blog/2023/pytorch-memory-optimization.html</link>
<guid>https://sebastianraschka.com/blog/2023/pytorch-memory-optimization.html</guid>
<content:encoded><![CDATA[
Peak memory consumption is a common bottleneck when training deep learning models such as vision transformers and LLMs. This article provides a series of techniques that can lower memory consumption by approximately 20x without sacrificing modeling performance and prediction accuracy.
]]></content:encoded>
<pubDate>Sat, 01 Jul 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Finetuning Falcon LLMs More Efficiently With LoRA and Adapters</title>
<link>https://sebastianraschka.com/blog/2023/falcon-finetuning.html</link>
<guid>https://sebastianraschka.com/blog/2023/falcon-finetuning.html</guid>
<content:encoded><![CDATA[
Finetuning allows us to adapt pretrained LLMs in a cost-efficient manner. But which method should we use? This article compares different parameter-efficient finetuning methods for the latest top-performing open-source LLM, Falcon. Using parameter-efficient finetuning methods outlined in this article, it's possible to finetune an LLM in 1 hour on a single GPU instead of a day on 6 GPUs.
]]></content:encoded>
<pubDate>Wed, 14 Jun 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Accelerating Large Language Models with Mixed-Precision Techniques</title>
<link>https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html</guid>
<content:encoded><![CDATA[
Training and using large language models (LLMs) is expensive due to their large compute requirements and memory footprints. This article will explore how leveraging lower-precision formats can enhance training and inference speeds up to 3x without compromising model accuracy.
]]></content:encoded>
<pubDate>Thu, 11 May 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)</title>
<link>https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html</guid>
<content:encoded><![CDATA[
Pretrained large language models are often referred to as foundation models for a good reason: they perform well on various tasks, and we can use them as a foundation for finetuning on a target task. As an alternative to updating all layers, which is very expensive, parameter-efficient methods such as prefix tuning and adapters have been developed. Let's talk about one of the most popular parameter-efficient finetuning techniques: Low-rank adaptation (LoRA). What is LoRA? How does it work? And how does it compare to the other popular finetuning approaches? Let's answer all these questions in this article!
]]></content:encoded>
<pubDate>Wed, 26 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters</title>
<link>https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of artificial intelligence, utilizing large language models in an efficient and effective manner has become increasingly important. Parameter-efficient finetuning stands at the forefront of this pursuit, allowing researchers and practitioners to reuse pretrained models while minimizing their computational and resource footprints. This article explains the broad concept of finetuning and discusses popular parameter-efficient alternatives like prefix tuning and adapters. Finally, we will look at the recent LLaMA-Adapter method and see how we can use it in practice.
]]></content:encoded>
<pubDate>Wed, 12 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Finetuning Large Language Models On A Single GPU Using Gradient Accumulation</title>
<link>https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html</guid>
<content:encoded><![CDATA[
Previously, I shared an article using multi-GPU training strategies to speed up the finetuning of large language models. Several of these strategies include mechanisms such as model or tensor sharding that distributes the model weights and computations across different devices to work around GPU memory limitations. However, many of us don't have access to multi-GPU resources. So, this article illustrates a simple technique that works as a great workaround to train models with larger batch sizes when GPU memory is a concern: gradient accumulation.
]]></content:encoded>
<pubDate>Tue, 28 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Keeping Up With AI Research And News</title>
<link>https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html</link>
<guid>https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html</guid>
<content:encoded><![CDATA[
When it comes to productivity workflows, there are a lot of things I'd love to share. However, the one topic many people ask me about is how I keep up with machine learning and AI at large, and how I find interesting papers.
]]></content:encoded>
<pubDate>Thu, 23 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Some Techniques To Make Your PyTorch Models Train (Much) Faster</title>
<link>https://sebastianraschka.com/blog/2023/pytorch-faster.html</link>
<guid>https://sebastianraschka.com/blog/2023/pytorch-faster.html</guid>
<content:encoded><![CDATA[
This blog post outlines techniques for improving the training performance of your PyTorch model without compromising its accuracy. To do so, we will wrap a PyTorch model in a LightningModule and use the Trainer class to enable various training optimizations. By changing only a few lines of code, we can reduce the training time on a single GPU from 22.53 minutes to 2.75 minutes while maintaining the model's prediction accuracy. Yes, that's a 8x performance boost!
]]></content:encoded>
<pubDate>Thu, 23 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</title>
<link>https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</guid>
<content:encoded><![CDATA[
In this article, we are going to understand how self-attention works from scratch. This means we will code it ourselves one step at a time. Since its introduction via the original transformer paper, self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing. Since self-attention is now everywhere, it's important to understand how it works.
]]></content:encoded>
<pubDate>Thu, 09 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Large Language Models -- A Transformative Reading List</title>
<link>https://sebastianraschka.com/blog/2023/llm-reading-list.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-reading-list.html</guid>
<content:encoded><![CDATA[
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models.
]]></content:encoded>
<pubDate>Tue, 07 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>What Are the Different Approaches for Detecting Content Generated by LLMs Such As ChatGPT? And How Do They Work and Differ?</title>
<link>https://sebastianraschka.com/blog/2023/detect-ai.html</link>
<guid>https://sebastianraschka.com/blog/2023/detect-ai.html</guid>
<content:encoded><![CDATA[
Since the release of the AI Classifier by OpenAI made big waves yesterday, I wanted to share a few details about the different approaches  for detecting AI-generated text. This article briefly outlines four approaches to identifying AI-generated contents.
]]></content:encoded>
<pubDate>Wed, 01 Feb 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Comparing Different Automatic Image Augmentation Methods in PyTorch</title>
<link>https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html</link>
<guid>https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html</guid>
<content:encoded><![CDATA[
Data augmentation is a key tool in reducing overfitting, whether it's for images or text. This article compares three Auto Image Data Augmentation techniques in PyTorch: AutoAugment, RandAugment, and TrivialAugment.
]]></content:encoded>
<pubDate>Sun, 29 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Curated Resources and Trustworthy Experts: The Key Ingredients for Finding Accurate Answers to Technical Questions in the Future</title>
<link>https://sebastianraschka.com/blog/2023/chatgpt-dilemma.html</link>
<guid>https://sebastianraschka.com/blog/2023/chatgpt-dilemma.html</guid>
<content:encoded><![CDATA[
Conversational chat bots such as ChatGPT probably will not be able replace traditional search engines and expert knowledge anytime soon. With the vast amount of misinformation available on the internet, the ability to distinguish between credible and unreliable sources remains challenging and crucial.
]]></content:encoded>
<pubDate>Mon, 16 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Training an XGBoost Classifier Using Cloud GPUs Without Worrying About Infrastructure</title>
<link>https://sebastianraschka.com/blog/2023/xgboost-gpu.html</link>
<guid>https://sebastianraschka.com/blog/2023/xgboost-gpu.html</guid>
<content:encoded><![CDATA[
Imagine you want to quickly train a few machine learning or deep learning models on the cloud but don't want to deal with cloud infrastructure. This short article explains how we can get our code up and running in seconds using the open source lightning library.
]]></content:encoded>
<pubDate>Sun, 15 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Open Source Highlights 2022 for Machine Learning &amp; AI</title>
<link>https://sebastianraschka.com/blog/2023/open-source-highlights-2022.html</link>
<guid>https://sebastianraschka.com/blog/2023/open-source-highlights-2022.html</guid>
<content:encoded><![CDATA[
Recently, I shared the top 10 papers that I read in 2022. As a follow-up, I am compiling a list of my favorite 10 open-source releases that I discovered, used, or contributed to in 2022.
]]></content:encoded>
<pubDate>Thu, 05 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Influential Machine Learning Papers Of 2022</title>
<link>https://sebastianraschka.com/blog/2023/top10-papers-2022.html</link>
<guid>https://sebastianraschka.com/blog/2023/top10-papers-2022.html</guid>
<content:encoded><![CDATA[
Every day brings something new and exciting to the world of machine learning and AI, from the latest developments and breakthroughs in the field to emerging trends and challenges. To mark the start of the new year, below is a short review of the top ten papers I've read in 2022.
]]></content:encoded>
<pubDate>Tue, 03 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Ahead Of AI, And What's Next?</title>
<link>https://sebastianraschka.com/blog/2022/ahead-of-ai-and-whats-next.html</link>
<guid>https://sebastianraschka.com/blog/2022/ahead-of-ai-and-whats-next.html</guid>
<content:encoded><![CDATA[
About monthly machine learning musings, and other things I am currently workin on ...
]]></content:encoded>
<pubDate>Sat, 15 Oct 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>A Short Chronology Of Deep Learning For Tabular Data</title>
<link>https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html</link>
<guid>https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html</guid>
<content:encoded><![CDATA[
Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional methods or counterexamples. So, with this short post, I aim to briefly summarize the major papers on deep tabular learning I am currently aware of. However, I want to emphasize that no matter how interesting or promising deep tabular methods look, I still recommend using a conventional machine learning method as a baseline. There is a reason why I cover conventional machine learning before deep learning in my books.
]]></content:encoded>
<pubDate>Sun, 24 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>No, We Don't Have to Choose Batch Sizes As Powers Of 2</title>
<link>https://sebastianraschka.com/blog/2022/batch-size-2.html</link>
<guid>https://sebastianraschka.com/blog/2022/batch-size-2.html</guid>
<content:encoded><![CDATA[
Regarding neural network training, I think we are all guilty of doing this: we choose our batch sizes as powers of 2, that is, 64, 128, 256, 512, 1024, and so forth. There are some valid theoretical justifications for this, but how does it pan out in practice? We had some discussions about that in the last couple of days, and here I want to write down some of the take-aways so I can reference them in the future. I hope you'll find this helpful as well!
]]></content:encoded>
<pubDate>Tue, 05 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud</title>
<link>https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html</link>
<guid>https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html</guid>
<content:encoded><![CDATA[
In this article, we will take deploy a Super Resolution App on the cloud using lightning.ai. The primary goal here is to see how easy it is to create and share a research demo. However, the cloud is for more than just model sharing: we will also learn how we can tap into additional GPU resources for model training.
]]></content:encoded>
<pubDate>Thu, 30 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App</title>
<link>https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html</link>
<guid>https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html</guid>
<content:encoded><![CDATA[
In this post, we will build a Lightning App. Why? Because it is 2022, and it is time to explore a more modern take on interacting with, presenting, and sharing our deep learning models. We are going to tackle this in three parts. In this first part, we will learn what a Lightning App is and how we build a Super Resolution GAN demo.
]]></content:encoded>
<pubDate>Fri, 17 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Taking Datasets, DataLoaders, and PyTorch’s New DataPipes for a Spin</title>
<link>https://sebastianraschka.com/blog/2022/datapipes.html</link>
<guid>https://sebastianraschka.com/blog/2022/datapipes.html</guid>
<content:encoded><![CDATA[
The PyTorch team recently announced TorchData, a prototype library focused on implementing composable and reusable data loading utilities for PyTorch. In particular, the TorchData library is centered around DataPipes, which are meant to be a DataLoader-compatible replacement for the existing Dataset class.
]]></content:encoded>
<pubDate>Sun, 12 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Running PyTorch on the M1 GPU</title>
<link>https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html</link>
<guid>https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html</guid>
<content:encoded><![CDATA[
Today, PyTorch officially introduced GPU support for Apple's ARM M1 chips. This is an exciting day for Mac users out there, so I spent a few minutes trying it out in practice. In this short blog post, I will summarize my experience and thoughts with the M1 chip for deep learning tasks.
]]></content:encoded>
<pubDate>Wed, 18 May 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Creating Confidence Intervals for Machine Learning Classifiers</title>
<link>https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html</link>
<guid>https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html</guid>
<content:encoded><![CDATA[
Developing good predictive models hinges upon accurate performance evaluation and comparisons. However, when evaluating machine learning models, we typically have to work around many constraints, including limited data, independence violations, and sampling biases. Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. This article outlines different methods for creating confidence intervals for machine learning models. Note that these methods also apply to deep learning.
]]></content:encoded>
<pubDate>Mon, 25 Apr 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Losses Learned</title>
<link>https://sebastianraschka.com/blog/2022/losses-learned-part1.html</link>
<guid>https://sebastianraschka.com/blog/2022/losses-learned-part1.html</guid>
<content:encoded><![CDATA[
The cross-entropy loss is our go-to loss for training deep learning-based classifiers. In this article, I am giving you a quick tour of how we usually compute the cross-entropy loss and how we compute it in PyTorch. There are two parts to it, and here we will look at a binary classification context first. You may wonder why bother writing this article; computing the cross-entropy loss should be relatively straightforward!? Yes and no. We can compute the cross-entropy loss in one line of code, but there's a common gotcha due to numerical optimizations under the hood. (And yes, when I am not careful, I sometimes make this mistake, too.) So, in this article, let me tell you a bit about deep learning jargon, improving numerical performance, and what could go wrong.
]]></content:encoded>
<pubDate>Mon, 04 Apr 2022 15:00:00 +0000</pubDate>
</item>
<item>
<title>TorchMetrics</title>
<link>https://sebastianraschka.com/blog/2022/torchmetrics.html</link>
<guid>https://sebastianraschka.com/blog/2022/torchmetrics.html</guid>
<content:encoded><![CDATA[
TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It's designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows. This iterative computation is useful if we want to track a model during iterative training or evaluation on minibatches (and optionally across on multiple GPUs). In deep learning, that's essentially *all the time*. However, when using TorchMetrics, one common question is whether we should use `.update()` or `.forward()`? (And that's also a question I certainly had when I started using it.). Here's a hands-on example and explanation.
]]></content:encoded>
<pubDate>Thu, 24 Mar 2022 13:00:00 +0000</pubDate>
</item>
<item>
<title>Machine Learning with PyTorch and Scikit-Learn</title>
<link>https://sebastianraschka.com/blog/2022/ml-pytorch-book.html</link>
<guid>https://sebastianraschka.com/blog/2022/ml-pytorch-book.html</guid>
<content:encoded><![CDATA[
Machine Learning with PyTorch and Scikit-Learn has been a long time in the making, and I am excited to finally get to talk about the release of my new book. Initially, this project started as the 4th edition of Python Machine Learning. However, we made so many changes to the book that we thought it deserved a new title to reflect that. So, what's new, you may wonder? In this post, I am excited to tell you all about it.
]]></content:encoded>
<pubDate>Fri, 25 Feb 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Machine Learning</title>
<link>https://sebastianraschka.com/blog/2021/ml-course.html</link>
<guid>https://sebastianraschka.com/blog/2021/ml-course.html</guid>
<content:encoded><![CDATA[
About half a year ago, I organized all my deep learning-related videos in a handy blog post to have everything in one place. Since many people liked this post, and because I like to use my winter break to get organized, I thought I could free two birds with one key by compiling this list below. Here, you find a list of approximately 90 machine learning lectures I recorded in 2020 and 2021! Once again, I hope this is useful to you!
]]></content:encoded>
<pubDate>Wed, 29 Dec 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Deep Learning</title>
<link>https://sebastianraschka.com/blog/2021/dl-course.html</link>
<guid>https://sebastianraschka.com/blog/2021/dl-course.html</guid>
<content:encoded><![CDATA[
I just sat down this morning and organized all deep learning related videos I recorded in 2021. I am sure this will be a useful reference for my future self, but I am also hoping it might be useful for one or the other person out there. PS: All code examples are in PyTorch :)
]]></content:encoded>
<pubDate>Fri, 09 Jul 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Datasets for Machine Learning and Deep Learning</title>
<link>https://sebastianraschka.com/blog/2021/ml-dl-datasets.html</link>
<guid>https://sebastianraschka.com/blog/2021/ml-dl-datasets.html</guid>
<content:encoded><![CDATA[
With the semester being in full swing, I recently shared this set of dataset repositories with my deep learning class. However, I thought that beyond using this list for finding inspiration for interesting student class projects, these are also good places to look for additional bechmark datasets for your model.
]]></content:encoded>
<pubDate>Thu, 11 Feb 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Deep Learning With PyTorch</title>
<link>https://sebastianraschka.com/blog/2021/pytorch-deeplearning-review.html</link>
<guid>https://sebastianraschka.com/blog/2021/pytorch-deeplearning-review.html</guid>
<content:encoded><![CDATA[
After its release in August 2020, Deep Learning with PyTorch has been sitting on my shelf before I finally got a chance to read it during this winter break. It turned out to be the perfect easy-going reading material for a bit of productivity after the relaxing holidays. As promised last week, here are my thoughts.
]]></content:encoded>
<pubDate>Thu, 21 Jan 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>How I Keep My Projects Organized</title>
<link>https://sebastianraschka.com/blog/2021/project-management.html</link>
<guid>https://sebastianraschka.com/blog/2021/project-management.html</guid>
<content:encoded><![CDATA[
Since I started my undergraduate studies in 2008, I have been obsessed with productivity tips, notetaking solutions, and todo-list management. Over the years, I tried many, many workflows and hundreds of (mostly digital) tools to keep my life, projects, and notes organized. Occasionally, I exchange ideas with friends and colleagues, and upon request, I talked about my workflow a couple of times on Twitter. After today's 2021-edition of this discussion, I thought that writing a quick and informal blogpost makes sense, making it easier to read and having a quick reference if someone asks about it again :).
]]></content:encoded>
<pubDate>Sun, 03 Jan 2021 20:00:00 +0000</pubDate>
</item>
<item>
<title>Scientific Computing in Python: Introduction to NumPy and Matplotlib</title>
<link>https://sebastianraschka.com/blog/2020/numpy-intro.html</link>
<guid>https://sebastianraschka.com/blog/2020/numpy-intro.html</guid>
<content:encoded><![CDATA[
Since many students in my Stat 451 (Introduction to Machine Learning and Statistical Pattern Classification) class are relatively new to Python and NumPy, I was recently devoting a lecture to the latter. Since the course notes are based on an interactive Jupyter notebook file, which I used as a basis for the lecture videos, I thought it would be worthwhile to reformat it as a blog article with the embedded 'narrated content' -- the video recordings.
]]></content:encoded>
<pubDate>Sun, 27 Sep 2020 17:00:00 +0000</pubDate>
</item>
<item>
<title>Interpretable Machine Learning</title>
<link>https://sebastianraschka.com/blog/2020/interpretable-ml-1.html</link>
<guid>https://sebastianraschka.com/blog/2020/interpretable-ml-1.html</guid>
<content:encoded><![CDATA[
In this blog post, I am (briefly) reviewing Christoph Molnar's *Interpretable Machine Learning Book*. Then, I am writing about two classic generalized linear models, linear and logistic regression. Mainly, this blog post explains the relationship between feature weights and predictions and demonstrates how to construct confidence intervals via Python.
]]></content:encoded>
<pubDate>Wed, 26 Aug 2020 18:00:00 +0000</pubDate>
</item>
<item>
<title>Chapter 1: Introduction to Machine Learning and Deep Learning</title>
<link>https://sebastianraschka.com/blog/2020/intro-to-dl-ch01.html</link>
<guid>https://sebastianraschka.com/blog/2020/intro-to-dl-ch01.html</guid>
<content:encoded><![CDATA[
The first chapter (draft) of the Introduction to Deep Learning book, which is a book based on my lecture notes and slides.
]]></content:encoded>
<pubDate>Wed, 05 Aug 2020 21:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Architects of Intelligence by Martin Ford</title>
<link>https://sebastianraschka.com/blog/2020/book-review-1-architects-of-intelligence.html</link>
<guid>https://sebastianraschka.com/blog/2020/book-review-1-architects-of-intelligence.html</guid>
<content:encoded><![CDATA[
A brief review of Martin Ford's book that features interviews with 23 of the most well-known and brightest minds working on AI.
]]></content:encoded>
<pubDate>Mon, 06 Jan 2020 13:00:00 +0000</pubDate>
</item>
<item>
<title>What's New in the 3rd Edition</title>
<link>https://sebastianraschka.com/blog/2019/whats-new-in-the-3rd-edition.html</link>
<guid>https://sebastianraschka.com/blog/2019/whats-new-in-the-3rd-edition.html</guid>
<content:encoded><![CDATA[
A brief summary of what's new in the 3rd edition of Python Machine Learning.
]]></content:encoded>
<pubDate>Thu, 12 Dec 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>My First Year at UW-Madison and a Gallery of Awesome Student Projects</title>
<link>https://sebastianraschka.com/blog/2019/student-gallery-1.html</link>
<guid>https://sebastianraschka.com/blog/2019/student-gallery-1.html</guid>
<content:encoded><![CDATA[
Not too long ago, in the Summer of 2018, I was super excited to join the Department of Statistics at the University of Wisconsin-Madison after obtaining my Ph.D. after ~5 long and productive years. Now, two semesters later after finals' week, I finally found some quiet days to look back on what's happened since then. In this post, I am sharing a short reflection as well as a some of the exciting projects my students were working on.
]]></content:encoded>
<pubDate>Fri, 24 May 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html</link>
<guid>https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html</guid>
<content:encoded><![CDATA[
This final article in the series *Model evaluation, model selection, and algorithm selection in machine learning* presents overviews of several statistical hypothesis testing approaches, with applications to machine learning model and algorithm comparisons. This includes statistical tests based on target predictions for independent test sets (the downsides of using a single test set for model comparisons was discussed in previous articles) as well as methods for algorithm comparisons by fitting and evaluating models via cross-validation. Lastly, this article will introduce *nested cross-validation*, which has become a common and recommended a method of choice for algorithm comparisons for small to moderately-sized datasets.
]]></content:encoded>
<pubDate>Sat, 10 Nov 2018 22:00:00 +0000</pubDate>
</item>
<item>
<title>Generating Gender-Neutral Face Images with Semi-Adversarial Neural Networks to Enhance Privacy</title>
<link>https://sebastianraschka.com/blog/2018/semi-adversarial-nets-1.html</link>
<guid>https://sebastianraschka.com/blog/2018/semi-adversarial-nets-1.html</guid>
<content:encoded><![CDATA[
I thought that it would be nice to have short and concise summaries of recent projects handy, to share them with a more general audience, including colleagues and students. So, I challenged myself to use fewer than 1000 words without getting distracted by the nitty-gritty details and technical jargon. In this post, I mainly cover some of my recent research in collaboration with the [iPRoBe Lab](http://iprobe.cse.msu.edu) that falls under the broad category of developing approaches to hide specific information in face images. The research discussed in this post is about "maximizing privacy while preserving utility."
]]></content:encoded>
<pubDate>Thu, 02 Aug 2018 05:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html</guid>
<content:encoded><![CDATA[
Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset. In Part I and Part II, we saw different holdout and bootstrap techniques for estimating the generalization performance of a model. We learned about the bias-variance trade-off, and we computed the uncertainty of our estimates. In this third part, we will focus on different methods of cross-validation for model evaluation and model selection. We will use these cross-validation techniques to rank models from several hyperparameter configurations and estimate how well they generalize to independent datasets.
]]></content:encoded>
<pubDate>Sun, 02 Oct 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html</guid>
<content:encoded><![CDATA[
In this second part of this series, we will look at some advanced techniques for model evaluation and techniques to estimate the uncertainty of our estimated model performance as well as its variance and stability. Then, in the next article, we will shift the focus onto another task that is one of the main pillar of successful, real-world machine learning applications -- Model Selection.
]]></content:encoded>
<pubDate>Sat, 13 Aug 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html</guid>
<content:encoded><![CDATA[
Machine learning has become a central part of our life -- as consumers, customers, and hopefully as researchers and practitioners! Whether we are applying predictive modeling techniques to our research or business problems, I believe we have one thing in common  &#58; We want to make good predictions! Fitting a model to our training data is one thing, but how do we know that it generalizes well to unseen data? How do we know that it doesn't simply memorize the data we fed it and fails to make good predictions on future samples, samples that it hasn't seen before? And how do we select a good model in the first place? Maybe a different learning algorithm could be better-suited for the problem at hand? Model evaluation is certainly not just the end point of our machine learning pipeline.<br /><br />Before we handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this article, we will go over a selection of these techniques, and we will see how they fit into the bigger picture, a typical machine learning workflow.
]]></content:encoded>
<pubDate>Sat, 11 Jun 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Writing 'Python Machine Learning'</title>
<link>https://sebastianraschka.com/blog/2015/writing-pymle.html</link>
<guid>https://sebastianraschka.com/blog/2015/writing-pymle.html</guid>
<content:encoded><![CDATA[
It's been about time. I am happy to announce that "Python Machine Learning" was finally released today! Sure, I could just send an email around to all the people who were interested in this book. On the other hand, I could put down those 140 characters on Twitter (minus what it takes to insert a hyperlink) and be done with it. Even so, writing "Python Machine Learning" really was quite a journey for a few months, and I would like to sit down in my favorite coffeehouse once more to say a few words about this experience.
]]></content:encoded>
<pubDate>Thu, 24 Sep 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Python, Machine Learning, and Language Wars</title>
<link>https://sebastianraschka.com/blog/2015/why-python.html</link>
<guid>https://sebastianraschka.com/blog/2015/why-python.html</guid>
<content:encoded><![CDATA[
This has really been quite a journey for me lately. And regarding the frequently asked question “Why did you choose Python for Machine Learning?” I guess it is about time to write my script. In this article, I really don’t mean to tell you why you or anyone else should use Python. But read on if you are interested in my opinion.
]]></content:encoded>
<pubDate>Mon, 24 Aug 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Single-Layer Neural Networks and Gradient Descent</title>
<link>https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</link>
<guid>https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</guid>
<content:encoded><![CDATA[
This article offers a brief glimpse of the history and basic concepts of machine learning. We will take a look at the first algorithmically described neural network and the gradient descent algorithm in context of adaptive linear neurons, which will not only introduce the principles of machine learning but also serve as the basis for modern multilayer neural networks in future articles.
]]></content:encoded>
<pubDate>Tue, 24 Mar 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Principal Component Analysis</title>
<link>https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html</link>
<guid>https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html</guid>
<content:encoded><![CDATA[
Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a “black box”, and we are going to unravel its internals in 3 basic steps.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2015 16:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Weighted Majority Rule Ensemble Classifier</title>
<link>https://sebastianraschka.com/Articles/2014_ensemble_classifier.html</link>
<guid>https://sebastianraschka.com/Articles/2014_ensemble_classifier.html</guid>
<content:encoded><![CDATA[
Here, I want to present a simple and conservative approach of implementing a weighted majority rule ensemble classifier in scikit-learn that yielded remarkably good results when I tried it in a kaggle competition. For me personally, kaggle competitions are just a nice way to try out and compare different approaches and ideas -- basically an opportunity to learn in a controlled environment with nice datasets.
]]></content:encoded>
<pubDate>Sun, 11 Jan 2015 18:00:00 +0000</pubDate>
</item>
<item>
<title>MusicMood</title>
<link>https://sebastianraschka.com/blog/2014/musicmood.html</link>
<guid>https://sebastianraschka.com/blog/2014/musicmood.html</guid>
<content:encoded><![CDATA[
In this article, I want to share my experience with a recent data mining project which probably was one of my most favorite hobby projects so far. It's all about building a classification model that can automatically predict the mood of music based on song lyrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>Turn Your Twitter Timeline into a Word Cloud</title>
<link>https://sebastianraschka.com/Articles/2014_twitter_wordcloud.html</link>
<guid>https://sebastianraschka.com/Articles/2014_twitter_wordcloud.html</guid>
<content:encoded><![CDATA[
Last week, I posted some visualizations in context of Happy Rock Song data mining project, and some people were curious about how I created the word clouds. Learn how to create YOUR personal Twitter Timeline!
]]></content:encoded>
<pubDate>Fri, 28 Nov 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Naive Bayes and Text Classification</title>
<link>https://sebastianraschka.com/Articles/2014_naive_bayes_1.html</link>
<guid>https://sebastianraschka.com/Articles/2014_naive_bayes_1.html</guid>
<content:encoded><![CDATA[
Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes’ probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction. In this first part of a series, we will take a look at the theory of naive Bayes classifiers and introduce the basic concepts of text classification. In following articles, we will implement those concepts to train a naive Bayes spam filter and apply naive Bayes to song classification based on lyrics.
]]></content:encoded>
<pubDate>Sat, 04 Oct 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</title>
<link>https://sebastianraschka.com/Articles/2014_kernel_pca.html</link>
<guid>https://sebastianraschka.com/Articles/2014_kernel_pca.html</guid>
<content:encoded><![CDATA[
The focus of this article is to briefly introduce the idea of kernel methods and to implement a Gaussian radius basis function (RBF) kernel that is used to perform nonlinear dimensionality reduction via KBF kernel principal component analysis (kPCA).
]]></content:encoded>
<pubDate>Sun, 14 Sep 2014 20:00:00 +0000</pubDate>
</item>
<item>
<title>Predictive modeling, supervised machine learning, and pattern classification</title>
<link>https://sebastianraschka.com/Articles/2014_intro_supervised_learning.html</link>
<guid>https://sebastianraschka.com/Articles/2014_intro_supervised_learning.html</guid>
<content:encoded><![CDATA[
When I was working on my next pattern classification application, I realized that it might be worthwhile to take a step back and look at the big picture of pattern classification in order to put my previous topics into context and to provide and introduction for the future topics that are going to follow.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2014 08:00:00 +0000</pubDate>
</item>
<item>
<title>Linear Discriminant Analysis</title>
<link>https://sebastianraschka.com/Articles/2014_python_lda.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_lda.html</guid>
<content:encoded><![CDATA[
I received a lot of positive feedback about the step-wise Principal Component Analysis (PCA) implementation. Thus, I decided to write a little follow-up about Linear Discriminant Analysis (LDA) — another useful linear transformation technique. Both LDA and PCA are commonly used dimensionality reduction techniques in statistics, pattern classification, and machine learning applications. By implementing the LDA step-by-step in Python, we will see and understand how it works, and we will compare it to a PCA to see how it differs.
]]></content:encoded>
<pubDate>Sun, 03 Aug 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>Dixon's Q test for outlier identification</title>
<link>https://sebastianraschka.com/Articles/2014_dixon_test.html</link>
<guid>https://sebastianraschka.com/Articles/2014_dixon_test.html</guid>
<content:encoded><![CDATA[
I recently faced the impossible task to identify outliers in a dataset with very, very small sample sizes and Dixon's Q test caught my attention. Honestly, I am not a big fan of this statistical test, but since Dixon's Q-test is still quite popular in certain scientific fields (e.g., chemistry) that it is important to understand its principles in order to draw your own conclusion of the presented research data that you might stumble upon in research articles or scientific talks.
]]></content:encoded>
<pubDate>Sat, 19 Jul 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>About Feature Scaling and Normalization</title>
<link>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</link>
<guid>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</guid>
<content:encoded><![CDATA[
I received a couple of questions in response to my previous article (Entry point: Data) where people asked me why I used Z-score standardization as feature scaling method prior to the PCA. I added additional information to the original article, however, I thought that it might be worthwhile to write a few more lines about this important topic in a separate article.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Entry Point Data</title>
<link>https://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html</link>
<guid>https://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html</guid>
<content:encoded><![CDATA[
In this short tutorial I want to provide a short overview of some of my favorite Python tools for common procedures as entry points for general pattern classification and machine learning tasks, and various other data analyses.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Molecular docking, estimating free energies of binding, and AutoDock's semi-empirical force field</title>
<link>https://sebastianraschka.com/Articles/2014_autodock_energycomps.html</link>
<guid>https://sebastianraschka.com/Articles/2014_autodock_energycomps.html</guid>
<content:encoded><![CDATA[
Discussions and questions about methods, approaches, and tools for estimating (relative) binding free energies of protein-ligand complexes are quite popular, and even the simplest tools can be quite tricky to use. Here, I want to briefly summarize the idea of molecular docking, and give a short overview about how we can use AutoDock 4.2's hybrid approach for evaluating binding affinities.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>An introduction to parallel programming using Python's multiprocessing module</title>
<link>https://sebastianraschka.com/Articles/2014_multiprocessing.html</link>
<guid>https://sebastianraschka.com/Articles/2014_multiprocessing.html</guid>
<content:encoded><![CDATA[
The default Python interpreter was designed with simplicity in mind and has a thread-safe mechanism, the so-called "GIL" (Global Interpreter Lock). In order to prevent conflicts between threads, it executes only one statement at a time (so-called serial processing, or single-threading). In this introduction to Python's multiprocessing module, we will see how we can spawn multiple subprocesses to avoid some of the GIL's disadvantages and make best use of the multiple cores in our CPU.
]]></content:encoded>
<pubDate>Fri, 20 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel density estimation via the Parzen-Rosenblatt window method</title>
<link>https://sebastianraschka.com/Articles/2014_kernel_density_est.html</link>
<guid>https://sebastianraschka.com/Articles/2014_kernel_density_est.html</guid>
<content:encoded><![CDATA[
The Parzen-window method (also known as Parzen-Rosenblatt window method) is a widely used non-parametric approach to estimate a probability density function *p(**x**)* for a specific point *p(**x**)* from a sample *p(**x**<sub>n</sub>)* that doesn't require any knowledge or assumption about the underlying distribution.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Numeric matrix manipulation</title>
<link>https://sebastianraschka.com/Articles/2014_matrix_cheatsheet.html</link>
<guid>https://sebastianraschka.com/Articles/2014_matrix_cheatsheet.html</guid>
<content:encoded><![CDATA[
At its core, this article is about a simple cheat sheet for basic operations on numeric matrices, which can be very useful if you working and experimenting with some of the most popular languages that are used for scientific computing, statistics, and data analysis.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>The key differences between Python 2.7.x and Python 3.x with examples</title>
<link>https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html</guid>
<content:encoded><![CDATA[
Many beginning Python users are wondering with which version of Python they should start. My answer to this question is usually something along the lines 'just go with the version your favorite tutorial was written in, and check out the differences later on.'\ But what if you are starting a new project and have the choice to pick? I would say there is currently no 'right' or 'wrong' as long as both Python 2.7.x and Python 3.x support the libraries that you are planning to use. However, it is worthwhile to have a look at the major differences between those two most popular versions of Python to avoid common pitfalls when writing the code for either one of them, or if you are planning to port your project.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>5 simple steps for converting Markdown documents into HTML and adding Python syntax highlighting</title>
<link>https://sebastianraschka.com/Articles/2014_markdown_syntax_color.html</link>
<guid>https://sebastianraschka.com/Articles/2014_markdown_syntax_color.html</guid>
<content:encoded><![CDATA[
In this little tutorial, I want to show you in 5 simple steps how easy it is to add code syntax highlighting to your blog articles.
]]></content:encoded>
<pubDate>Wed, 28 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Creating a table of contents with internal links in IPython Notebooks and Markdown documents</title>
<link>https://sebastianraschka.com/Articles/2014_ipython_internal_links.html</link>
<guid>https://sebastianraschka.com/Articles/2014_ipython_internal_links.html</guid>
<content:encoded><![CDATA[
Many people have asked me how I create the table of contents with internal links for my IPython Notebooks and Markdown documents on GitHub. Well, no (IPython) magic is involved, it is just a little bit of HTML, but I thought it might be worthwhile to write this little how-to tutorial.
]]></content:encoded>
<pubDate>Tue, 20 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A Beginner's Guide to Python's Namespaces, Scope Resolution, and the LEGB Rule</title>
<link>https://sebastianraschka.com/Articles/2014_python_scope_and_namespaces.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_scope_and_namespaces.html</guid>
<content:encoded><![CDATA[
A short tutorial about Python's namespaces and the scope resolution for variable names using the LEGB-rule with little quiz-like exercises.
]]></content:encoded>
<pubDate>Mon, 12 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Diving deep into Python</title>
<link>https://sebastianraschka.com/Articles/2014_deep_python.html</link>
<guid>https://sebastianraschka.com/Articles/2014_deep_python.html</guid>
<content:encoded><![CDATA[
Some while ago, I started to collect some of the not-so-obvious things I encountered when I was coding in Python. I thought that it was worthwhile sharing them and encourage you to take a brief look at the section-overview and maybe you'll find something that you do not already know - I can guarantee you that it'll likely save you some time at one or the other tricky debugging challenge.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Principal Component Analysis (PCA)</title>
<link>https://sebastianraschka.com/Articles/2014_pca_step_by_step.html</link>
<guid>https://sebastianraschka.com/Articles/2014_pca_step_by_step.html</guid>
<content:encoded><![CDATA[
In this article I want to explain how a Principal Component Analysis (PCA) works by implementing it in Python step by step. At the end we will compare the results to the more convenient Python PCA() classes that are available through the popular matplotlib and scipy libraries and discuss how they differ.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Installing Scientific Packages for Python3 on MacOS 10.9 Mavericks</title>
<link>https://sebastianraschka.com/Articles/2014_install_python_sci_pkgs.html</link>
<guid>https://sebastianraschka.com/Articles/2014_install_python_sci_pkgs.html</guid>
<content:encoded><![CDATA[
I just went through some pain (again) when I wanted to install some of Python's scientific libraries on my second Mac. I summarized the setup and installation process for future reference.\ If you encounter any different or additional obstacles let me know, and please feel free to make any suggestions to improve this short walkthrough.
]]></content:encoded>
<pubDate>Thu, 13 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A thorough guide to SQLite database operations in Python</title>
<link>https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</link>
<guid>https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</guid>
<content:encoded><![CDATA[
After I wrote the initial teaser article "SQLite - Working with large data sets in Python effectively" about how awesome SQLite databases are via sqlite3 in Python, I wanted to delve a little bit more into the SQLite syntax and provide you with some more hands-on examples.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Using OpenEye software for substructure alignments</title>
<link>https://sebastianraschka.com/Articles/2014_openeye_alignments_overlays.html</link>
<guid>https://sebastianraschka.com/Articles/2014_openeye_alignments_overlays.html</guid>
<content:encoded><![CDATA[
This is a quickguide showing how to use OpenEye software command line tools to align target molecules to a query based on substructure matches and how to retrieve the best molecule overlay from two sets of low-energy conformers.
]]></content:encoded>
<pubDate>Sun, 23 Feb 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Unit testing in Python</title>
<link>https://sebastianraschka.com/Articles/2013_python_unittest.html</link>
<guid>https://sebastianraschka.com/Articles/2013_python_unittest.html</guid>
<content:encoded><![CDATA[
Let’s be honest, code testing is everything but a joyful task. However, a good unit testing framework makes this process as smooth as possible. Eventually, testing becomes a regular and continuous process, accompanied by the assurance that our code will operate just as exact and seamlessly as a Swiss clockwork.
]]></content:encoded>
<pubDate>Sat, 14 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>A short tutorial for decent heat maps in R</title>
<link>https://sebastianraschka.com/Articles/heatmaps_in_r.html</link>
<guid>https://sebastianraschka.com/Articles/heatmaps_in_r.html</guid>
<content:encoded><![CDATA[
I received many questions from people who want to quickly visualize their data via heat maps - ideally as quickly as possible. This is the major issue of exploratory data analysis, since we often don’t have the time to digest whole books about the particular techniques in different software packages to just get the job done. But once we are happy with our initial results, it might be worthwhile to dig deeper into the topic in order to further customize our plots and maybe even polish them for publication. In this post, my aim is to briefly introduce one of R’s several heat map libraries for a simple data analysis. I chose R, because it is one of the most popular free statistical software packages around. Of course there are many more tools out there to produce similar results (and even in R there are many different packages for heat maps), but I will leave this as an open topic for another time.
]]></content:encoded>
<pubDate>Sun, 08 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>SQLite</title>
<link>https://sebastianraschka.com/Articles/2013_sqlite_database.html</link>
<guid>https://sebastianraschka.com/Articles/2013_sqlite_database.html</guid>
<content:encoded><![CDATA[
My new project confronted me with the task of screening a massive set of large data files in text format with billions of entries each. I will have to retrieve data repeatedly and frequently in the future. Thus, I was tempted to find a better solution than brute-force scanning through ~20 separate 1-column text files with ~6 billion entries every time line by line.
]]></content:encoded>
<pubDate>Sun, 03 Nov 2013 09:00:00 +0000</pubDate>
</item>
</channel>
</rss>