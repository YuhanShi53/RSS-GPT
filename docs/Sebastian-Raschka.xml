<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>Sebastian Raschka, PhD</title>
<link>https://sebastianraschka.com/</link>


<item>
<title>强化学习推理技术在大模型中的最新进展</title>
<link>https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</link>
<guid>https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">探讨通过强化学习提升大模型推理能力的最新发展。</p><br /><br /><p><strong>摘要：</strong> 本月科技界迎来多个旗舰模型发布，如GPT-4.5和Llama 4，但市场反应平淡，主要因为这些模型缺乏基于强化学习的推理优化。相比之下，OpenAI发布的o3推理模型展示了战略性计算投入的价值，尤其是针对推理任务的强化学习方法。尽管推理并非万能钥匙，但它显著提高了复杂任务中的准确性与问题解决能力。预计未来，这种以推理为中心的后训练方式将成为大型语言模型的标准流程。本文将深入分析强化学习推理技术的最新进展及其潜在影响。</p><br /><br /><p><em>使用 qwen-turbo 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 19 Apr 2025 00:00:00 +0000</pubDate>
</item>
<item>
<title>First Look at Reasoning From Scratch: Chapter 1</title>
<link>https://sebastianraschka.com/blog/2025/first-look-at-reasoning-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2025/first-look-at-reasoning-from-scratch.html</guid>
<content:encoded><![CDATA[
As you know, I've been writing a lot lately about the latest research on reasoning in LLMs. Before my next research-focused blog post, I wanted to offer something special to my paid subscribers as a thank-you for your ongoing support. So, I've started writing a new book on how reasoning works in LLMs, and here I'm sharing the first Chapter 1 with you. This ~15-page chapter is an introduction reasoning in the context of LLMs and provides an overview of methods like inference-time scaling and reinforcement learning. Thanks for your support! I hope you enjoy the chapter, and stay tuned for my next blog post on reasoning research!
]]></content:encoded>
<pubDate>Sat, 29 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>2025年大型语言模型推理能力的最新进展</title>
<link>https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</link>
<guid>https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了2025年大型语言模型推理能力的提升方法，重点是推理时间计算的最新研究进展。</p><br /><br /><p><strong>摘要：</strong> 本文讨论了2025年在大型语言模型（LLM）推理能力方面的新进展，特别是推理时间计算的提升方法。随着研究不断深入，提高LLM的推理能力成为热门课题，涉及的技术包括推理时间计算的扩展、强化学习、监督微调和模型蒸馏等多种策略。文章详细阐述了推理模型的四大类实现方法，强调了推理时间计算的有效性，如链式思维（CoT）提示和预算强制等技术，为LLM在复杂任务中的应用提供了新的视角。尽管推理时间计算能显著改善小型模型的表现，但也要考虑其增加的计算成本，使得在使用和成本之间进行权衡成为必要。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/state-of-llm-reasoning-and-inference-scaling.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Sat, 08 Mar 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>增强大型语言模型的推理能力的四种主要方法</title>
<link>https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html</link>
<guid>https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html</guid>
<content:encoded><![CDATA[
<div><p style="color: gray;">本文探讨了提升大型语言模型推理能力的四种主要方法。</p><br /><br /><p><strong>摘要：</strong> 本文介绍了四种主要的方法来构建推理模型，从而增强大型语言模型（LLMs）的推理能力。这些方法提供了有效的框架，帮助研究者和工程师应对快速发展的相关文献和市场炒作。通过深入分析每种方法的特点和应用，本文旨在为读者提供有价值的见解，以便更好地理解如何在实际应用中实现推理能力的提升。</p><br /><br /><p><em>使用 gpt-4o-mini 生成 </em></p><a href="https://sebastianraschka.com/blog/2025/understanding-reasoning-llms.html" target="_blank">查看原文</a></div>
]]></content:encoded>
<pubDate>Wed, 05 Feb 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Noteworthy LLM Research Papers of 2024</title>
<link>https://sebastianraschka.com/blog/2025/llm-research-2024.html</link>
<guid>https://sebastianraschka.com/blog/2025/llm-research-2024.html</guid>
<content:encoded><![CDATA[
This article covers 12 influential AI research papers of 2024, ranging from mixture-of-experts models to new LLM scaling laws for precision.
]]></content:encoded>
<pubDate>Thu, 23 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch</title>
<link>https://sebastianraschka.com/blog/2025/bpe-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2025/bpe-from-scratch.html</guid>
<content:encoded><![CDATA[
This is a standalone notebook implementing the popular byte pair encoding (BPE) tokenization algorithm, which is used in models like GPT-2 to GPT-4, Llama 3, etc., from scratch for educational purposes."
]]></content:encoded>
<pubDate>Fri, 17 Jan 2025 06:03:00 +0000</pubDate>
</item>
<item>
<title>LLM Research Papers: The 2024 List</title>
<link>https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html</link>
<guid>https://sebastianraschka.com/blog/2024/llm-research-papers-the-2024-list.html</guid>
<content:encoded><![CDATA[
I want to share my running bookmark list of many fascinating (mostly LLM-related) papers I stumbled upon in 2024. It's just a list, but maybe it will come in handy for those who are interested in finding some gems to read for the holidays.
]]></content:encoded>
<pubDate>Sun, 29 Dec 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Understanding Multimodal LLMs</title>
<link>https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html</link>
<guid>https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html</guid>
<content:encoded><![CDATA[
There has been a lot of new research on the multimodal LLM front, including the latest Llama 3.2 vision models, which employ diverse architectural strategies to integrate various data types like text and images. For instance, The decoder-only method uses a single stack of decoder blocks to process all modalities sequentially. On the other hand, cross-attention methods (for example, used in Llama 3.2) involve separate encoders for different modalities with a cross-attention layer that allows these encoders to interact. This article explains how these different types of multimodal LLMs function. Additionally, I will review and summarize roughly a dozen other recent multimodal papers and models published in recent weeks to compare their approaches.
]]></content:encoded>
<pubDate>Sun, 03 Nov 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Building A GPT-Style LLM Classifier From Scratch</title>
<link>https://sebastianraschka.com/blog/2024/building-a-gpt-style-llm-classifier.html</link>
<guid>https://sebastianraschka.com/blog/2024/building-a-gpt-style-llm-classifier.html</guid>
<content:encoded><![CDATA[
This article shows you how to transform pretrained large language models (LLMs) into strong text classifiers. But why focus on classification? First, finetuning a pretrained model for classification offers a gentle yet effective introduction to model finetuning. Second, many real-world and business challenges revolve around text classification: spam detection, sentiment analysis, customer feedback categorization, topic labeling, and more.
]]></content:encoded>
<pubDate>Sat, 21 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Building LLMs from the Ground Up: A 3-hour Coding Workshop</title>
<link>https://sebastianraschka.com/blog/2024/building-llms-from-the-ground-up.html</link>
<guid>https://sebastianraschka.com/blog/2024/building-llms-from-the-ground-up.html</guid>
<content:encoded><![CDATA[
This tutorial is aimed at coders interested in understanding the building blocks of large language models (LLMs), how LLMs work, and how to code them from the ground up in PyTorch. We will kick off this tutorial with an introduction to LLMs, recent milestones, and their use cases. Then, we will code a small GPT-like LLM, including its data input pipeline, core architecture components, and pretraining code ourselves. After understanding how everything fits together and how to pretrain an LLM, we will learn how to load pretrained weights and finetune LLMs using open-source libraries.
]]></content:encoded>
<pubDate>Sun, 01 Sep 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>New LLM Pre-training and Post-training Paradigms</title>
<link>https://sebastianraschka.com/blog/2024/new-llm-pre-training-and-post-training.html</link>
<guid>https://sebastianraschka.com/blog/2024/new-llm-pre-training-and-post-training.html</guid>
<content:encoded><![CDATA[
There are hundreds of LLM papers each month proposing new techniques and approaches. However, one of the best ways to see what actually works well in practice is to look at the pre-training and post-training pipelines of the most recent state-of-the-art models. Luckily, four major new LLMs have been released in the last months, accompanied by relatively detailed technical reports. In this article, I focus on the pre-training and post-training pipelines of the following models: Alibaba's Qwen 2, Apple Intelligence Foundation Language Models, Google's Gemma 2, Meta AI's Llama 3.1.
]]></content:encoded>
<pubDate>Sat, 17 Aug 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Instruction Pretraining LLMs</title>
<link>https://sebastianraschka.com/blog/2024/instruction-pretraining-llms.html</link>
<guid>https://sebastianraschka.com/blog/2024/instruction-pretraining-llms.html</guid>
<content:encoded><![CDATA[
This article covers a new, cost-effective method for generating data for instruction finetuning LLMs; instruction finetuning from scratch; pretraining LLMs with instruction data; and an overview of what's new in Gemma 2.
]]></content:encoded>
<pubDate>Sat, 20 Jul 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Developing an LLM: Building, Training, Finetuning</title>
<link>https://sebastianraschka.com/blog/2024/llms-building-training-finetuning.html</link>
<guid>https://sebastianraschka.com/blog/2024/llms-building-training-finetuning.html</guid>
<content:encoded><![CDATA[
This is an overview of the LLM development process. This one-hour talk focuses on the essential three stages of developing an LLM: coding the architecture, implementing pretraining, and fine-tuning the LLM. Lastly, we also discuss the main ways LLMs are evaluated, along with the caveats of each method.
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments?</title>
<link>https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html</link>
<guid>https://sebastianraschka.com/blog/2024/llm-research-insights-instruction.html</guid>
<content:encoded><![CDATA[
This article covers three new papers related to instruction finetuning and parameter-efficient finetuning with LoRA in large language models (LLMs). I work with these methods on a daily basis, so it's always exciting to see new research that provides practical insights.
]]></content:encoded>
<pubDate>Sun, 02 Jun 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?</title>
<link>https://sebastianraschka.com/blog/2024/how-good-open-llm.html</link>
<guid>https://sebastianraschka.com/blog/2024/how-good-open-llm.html</guid>
<content:encoded><![CDATA[
What a month! We had four major open LLM releases: Mixtral, Meta AI's Llama 3, Microsoft's Phi-3, and Apple's OpenELM. In my new article, I review and discuss all four of these major transformer-based LLM model releases, followed by new research on reinforcement learning with human feedback methods for instruction finetuning using PPO and DPO algorithms.
]]></content:encoded>
<pubDate>Sun, 12 May 2024 06:03:00 +0000</pubDate>
</item>
<item>
<title>Using and Finetuning Pretrained Transformers</title>
<link>https://sebastianraschka.com/blog/2024/using-finetuning-transformers.html</link>
<guid>https://sebastianraschka.com/blog/2024/using-finetuning-transformers.html</guid>
<content:encoded><![CDATA[
What are the different ways to use and finetune pretrained large language models (LLMs)? The three most common ways to use and finetune pretrained LLMs include a feature-based approach, in-context prompting, and updating a subset of the model parameters. First, most pretrained LLMs or language transformers can be utilized without the need for further finetuning. For instance, we can employ a feature-based method to train a new downstream model, such as a linear classifier, using embeddings generated by a pretrained transformer. Second, we can showcase examples of a new task within the input itself, which means we can directly exhibit the expected outcomes without requiring any updates or learning from the model. This concept is also known as prompting. Finally, it’s also possible to finetune all or just a small number of parameters to achieve the desired outcomes. This article discusses these types of approaches in greater depth
]]></content:encoded>
<pubDate>Sat, 20 Apr 2024 07:00:00 +0000</pubDate>
</item>
<item>
<title>Tips for LLM Pretraining and Evaluating Reward Models</title>
<link>https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html</link>
<guid>https://sebastianraschka.com/blog/2024/research-papers-in-march-2024.html</guid>
<content:encoded><![CDATA[
It's another month in AI research, and it's hard to pick favorites. This month, I am going over a paper that discusses strategies for the continued pretraining of LLMs, followed by a discussion of reward modeling used in reinforcement learning with human feedback (a popular LLM alignment method), along with a new benchmark. Continued pretraining for LLMs is an important topic because it allows us to update existing LLMs, for instance, ensuring that these models remain up-to-date with the latest information and trends. Also, it allows us to adapt them to new target domains without having them to retrain from scratch. Reward modeling is important because it allows us to align LLMs more closely with human preferences and, to some extent, helps with safety. But beyond human preference optimization, it also provides a mechanism for learning and adapting LLMs to complex tasks by providing instruction-output examples where explicit programming of correct behavior is challenging or impractical.
]]></content:encoded>
<pubDate>Sun, 31 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>Research Papers in February 2024</title>
<link>https://sebastianraschka.com/blog/2024/research-papers-in-february-2024.html</link>
<guid>https://sebastianraschka.com/blog/2024/research-papers-in-february-2024.html</guid>
<content:encoded><![CDATA[
Once again, this has been an exciting month in AI research. This month, I'm covering two new openly available LLMs, insights into small finetuned LLMs, and a new parameter-efficient LLM finetuning technique. The two LLMs mentioned above stand out for several reasons. One LLM (OLMo) is completely open source, meaning that everything from the training code to the dataset to the log files is openly shared. The other LLM (Gemma) also comes with openly available weights but achieves state-of-the-art performance on several benchmarks and outperforms popular LLMs of similar size, such as Llama 2 7B and Mistral 7B, by a large margin.
]]></content:encoded>
<pubDate>Sun, 03 Mar 2024 06:00:00 +0000</pubDate>
</item>
<item>
<title>Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch</title>
<link>https://sebastianraschka.com/blog/2024/lora-dora.html</link>
<guid>https://sebastianraschka.com/blog/2024/lora-dora.html</guid>
<content:encoded><![CDATA[
Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters. In this article, we will take a look at both LoRA and DoRA, which is a new promising alternative to LoRA.
]]></content:encoded>
<pubDate>Sun, 18 Feb 2024 08:00:00 +0000</pubDate>
</item>
<item>
<title>Optimizing LLMs From a Dataset Perspective</title>
<link>https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html</link>
<guid>https://sebastianraschka.com/blog/2023/optimizing-LLMs-dataset-perspective.html</guid>
<content:encoded><![CDATA[
This article focuses on improving the modeling performance of LLMs by finetuning them using carefully curated datasets. Specifically, this article highlights strategies that involve modifying, utilizing, or manipulating the datasets for instruction-based finetuning rather than altering the model architecture or training algorithms (the latter will be topics of a future article). This article will also explain how you can prepare your own datasets to finetune open-source LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Sep 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>The NeurIPS 2023 LLM Efficiency Challenge Starter Guide</title>
<link>https://sebastianraschka.com/blog/2023/neurips2023-starter-guide.html</link>
<guid>https://sebastianraschka.com/blog/2023/neurips2023-starter-guide.html</guid>
<content:encoded><![CDATA[
Large language models (LLMs) offer one of the most interesting opportunities for developing more efficient training methods. A few weeks ago, the NeurIPS 2023 LLM Efficiency Challenge launched to focus on efficient LLM finetuning, and this guide is a short walkthrough explaining how to participate in this competition. This article covers everything you need to know, from setting up the coding environment to making the first submission.
]]></content:encoded>
<pubDate>Thu, 10 Aug 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch</title>
<link>https://sebastianraschka.com/blog/2023/pytorch-memory-optimization.html</link>
<guid>https://sebastianraschka.com/blog/2023/pytorch-memory-optimization.html</guid>
<content:encoded><![CDATA[
Peak memory consumption is a common bottleneck when training deep learning models such as vision transformers and LLMs. This article provides a series of techniques that can lower memory consumption by approximately 20x without sacrificing modeling performance and prediction accuracy.
]]></content:encoded>
<pubDate>Sat, 01 Jul 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Finetuning Falcon LLMs More Efficiently With LoRA and Adapters</title>
<link>https://sebastianraschka.com/blog/2023/falcon-finetuning.html</link>
<guid>https://sebastianraschka.com/blog/2023/falcon-finetuning.html</guid>
<content:encoded><![CDATA[
Finetuning allows us to adapt pretrained LLMs in a cost-efficient manner. But which method should we use? This article compares different parameter-efficient finetuning methods for the latest top-performing open-source LLM, Falcon. Using parameter-efficient finetuning methods outlined in this article, it's possible to finetune an LLM in 1 hour on a single GPU instead of a day on 6 GPUs.
]]></content:encoded>
<pubDate>Wed, 14 Jun 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Accelerating Large Language Models with Mixed-Precision Techniques</title>
<link>https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html</guid>
<content:encoded><![CDATA[
Training and using large language models (LLMs) is expensive due to their large compute requirements and memory footprints. This article will explore how leveraging lower-precision formats can enhance training and inference speeds up to 3x without compromising model accuracy.
]]></content:encoded>
<pubDate>Thu, 11 May 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)</title>
<link>https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html</guid>
<content:encoded><![CDATA[
Pretrained large language models are often referred to as foundation models for a good reason: they perform well on various tasks, and we can use them as a foundation for finetuning on a target task. As an alternative to updating all layers, which is very expensive, parameter-efficient methods such as prefix tuning and adapters have been developed. Let's talk about one of the most popular parameter-efficient finetuning techniques: Low-rank adaptation (LoRA). What is LoRA? How does it work? And how does it compare to the other popular finetuning approaches? Let's answer all these questions in this article!
]]></content:encoded>
<pubDate>Wed, 26 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters</title>
<link>https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html</guid>
<content:encoded><![CDATA[
In the rapidly evolving field of artificial intelligence, utilizing large language models in an efficient and effective manner has become increasingly important. Parameter-efficient finetuning stands at the forefront of this pursuit, allowing researchers and practitioners to reuse pretrained models while minimizing their computational and resource footprints. This article explains the broad concept of finetuning and discusses popular parameter-efficient alternatives like prefix tuning and adapters. Finally, we will look at the recent LLaMA-Adapter method and see how we can use it in practice.
]]></content:encoded>
<pubDate>Wed, 12 Apr 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Finetuning Large Language Models On A Single GPU Using Gradient Accumulation</title>
<link>https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html</guid>
<content:encoded><![CDATA[
Previously, I shared an article using multi-GPU training strategies to speed up the finetuning of large language models. Several of these strategies include mechanisms such as model or tensor sharding that distributes the model weights and computations across different devices to work around GPU memory limitations. However, many of us don't have access to multi-GPU resources. So, this article illustrates a simple technique that works as a great workaround to train models with larger batch sizes when GPU memory is a concern: gradient accumulation.
]]></content:encoded>
<pubDate>Tue, 28 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Keeping Up With AI Research And News</title>
<link>https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html</link>
<guid>https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html</guid>
<content:encoded><![CDATA[
When it comes to productivity workflows, there are a lot of things I'd love to share. However, the one topic many people ask me about is how I keep up with machine learning and AI at large, and how I find interesting papers.
]]></content:encoded>
<pubDate>Thu, 23 Mar 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Some Techniques To Make Your PyTorch Models Train (Much) Faster</title>
<link>https://sebastianraschka.com/blog/2023/pytorch-faster.html</link>
<guid>https://sebastianraschka.com/blog/2023/pytorch-faster.html</guid>
<content:encoded><![CDATA[
This blog post outlines techniques for improving the training performance of your PyTorch model without compromising its accuracy. To do so, we will wrap a PyTorch model in a LightningModule and use the Trainer class to enable various training optimizations. By changing only a few lines of code, we can reduce the training time on a single GPU from 22.53 minutes to 2.75 minutes while maintaining the model's prediction accuracy. Yes, that's a 8x performance boost!
]]></content:encoded>
<pubDate>Thu, 23 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch</title>
<link>https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</link>
<guid>https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</guid>
<content:encoded><![CDATA[
In this article, we are going to understand how self-attention works from scratch. This means we will code it ourselves one step at a time. Since its introduction via the original transformer paper, self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing. Since self-attention is now everywhere, it's important to understand how it works.
]]></content:encoded>
<pubDate>Thu, 09 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>Understanding Large Language Models -- A Transformative Reading List</title>
<link>https://sebastianraschka.com/blog/2023/llm-reading-list.html</link>
<guid>https://sebastianraschka.com/blog/2023/llm-reading-list.html</guid>
<content:encoded><![CDATA[
Since transformers have such a big impact on everyone's research agenda, I wanted to flesh out a short reading list for machine learning researchers and practitioners getting started with large language models.
]]></content:encoded>
<pubDate>Tue, 07 Feb 2023 08:00:00 +0000</pubDate>
</item>
<item>
<title>What Are the Different Approaches for Detecting Content Generated by LLMs Such As ChatGPT? And How Do They Work and Differ?</title>
<link>https://sebastianraschka.com/blog/2023/detect-ai.html</link>
<guid>https://sebastianraschka.com/blog/2023/detect-ai.html</guid>
<content:encoded><![CDATA[
Since the release of the AI Classifier by OpenAI made big waves yesterday, I wanted to share a few details about the different approaches  for detecting AI-generated text. This article briefly outlines four approaches to identifying AI-generated contents.
]]></content:encoded>
<pubDate>Wed, 01 Feb 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Comparing Different Automatic Image Augmentation Methods in PyTorch</title>
<link>https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html</link>
<guid>https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html</guid>
<content:encoded><![CDATA[
Data augmentation is a key tool in reducing overfitting, whether it's for images or text. This article compares three Auto Image Data Augmentation techniques in PyTorch: AutoAugment, RandAugment, and TrivialAugment.
]]></content:encoded>
<pubDate>Sun, 29 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Curated Resources and Trustworthy Experts: The Key Ingredients for Finding Accurate Answers to Technical Questions in the Future</title>
<link>https://sebastianraschka.com/blog/2023/chatgpt-dilemma.html</link>
<guid>https://sebastianraschka.com/blog/2023/chatgpt-dilemma.html</guid>
<content:encoded><![CDATA[
Conversational chat bots such as ChatGPT probably will not be able replace traditional search engines and expert knowledge anytime soon. With the vast amount of misinformation available on the internet, the ability to distinguish between credible and unreliable sources remains challenging and crucial.
]]></content:encoded>
<pubDate>Mon, 16 Jan 2023 09:00:00 +0000</pubDate>
</item>
<item>
<title>Training an XGBoost Classifier Using Cloud GPUs Without Worrying About Infrastructure</title>
<link>https://sebastianraschka.com/blog/2023/xgboost-gpu.html</link>
<guid>https://sebastianraschka.com/blog/2023/xgboost-gpu.html</guid>
<content:encoded><![CDATA[
Imagine you want to quickly train a few machine learning or deep learning models on the cloud but don't want to deal with cloud infrastructure. This short article explains how we can get our code up and running in seconds using the open source lightning library.
]]></content:encoded>
<pubDate>Sun, 15 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Open Source Highlights 2022 for Machine Learning &amp; AI</title>
<link>https://sebastianraschka.com/blog/2023/open-source-highlights-2022.html</link>
<guid>https://sebastianraschka.com/blog/2023/open-source-highlights-2022.html</guid>
<content:encoded><![CDATA[
Recently, I shared the top 10 papers that I read in 2022. As a follow-up, I am compiling a list of my favorite 10 open-source releases that I discovered, used, or contributed to in 2022.
]]></content:encoded>
<pubDate>Thu, 05 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Influential Machine Learning Papers Of 2022</title>
<link>https://sebastianraschka.com/blog/2023/top10-papers-2022.html</link>
<guid>https://sebastianraschka.com/blog/2023/top10-papers-2022.html</guid>
<content:encoded><![CDATA[
Every day brings something new and exciting to the world of machine learning and AI, from the latest developments and breakthroughs in the field to emerging trends and challenges. To mark the start of the new year, below is a short review of the top ten papers I've read in 2022.
]]></content:encoded>
<pubDate>Tue, 03 Jan 2023 07:00:00 +0000</pubDate>
</item>
<item>
<title>Ahead Of AI, And What's Next?</title>
<link>https://sebastianraschka.com/blog/2022/ahead-of-ai-and-whats-next.html</link>
<guid>https://sebastianraschka.com/blog/2022/ahead-of-ai-and-whats-next.html</guid>
<content:encoded><![CDATA[
About monthly machine learning musings, and other things I am currently workin on ...
]]></content:encoded>
<pubDate>Sat, 15 Oct 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>A Short Chronology Of Deep Learning For Tabular Data</title>
<link>https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html</link>
<guid>https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html</guid>
<content:encoded><![CDATA[
Occasionally, I share research papers proposing new deep learning approaches for tabular data on social media, which is typically an excellent discussion starter. Often, people ask for additional methods or counterexamples. So, with this short post, I aim to briefly summarize the major papers on deep tabular learning I am currently aware of. However, I want to emphasize that no matter how interesting or promising deep tabular methods look, I still recommend using a conventional machine learning method as a baseline. There is a reason why I cover conventional machine learning before deep learning in my books.
]]></content:encoded>
<pubDate>Sun, 24 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>No, We Don't Have to Choose Batch Sizes As Powers Of 2</title>
<link>https://sebastianraschka.com/blog/2022/batch-size-2.html</link>
<guid>https://sebastianraschka.com/blog/2022/batch-size-2.html</guid>
<content:encoded><![CDATA[
Regarding neural network training, I think we are all guilty of doing this: we choose our batch sizes as powers of 2, that is, 64, 128, 256, 512, 1024, and so forth. There are some valid theoretical justifications for this, but how does it pan out in practice? We had some discussions about that in the last couple of days, and here I want to write down some of the take-aways so I can reference them in the future. I hope you'll find this helpful as well!
]]></content:encoded>
<pubDate>Tue, 05 Jul 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 2: Leveraging the Cloud</title>
<link>https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html</link>
<guid>https://sebastianraschka.com/blog/2022/lightning-app-srgan-2.html</guid>
<content:encoded><![CDATA[
In this article, we will take deploy a Super Resolution App on the cloud using lightning.ai. The primary goal here is to see how easy it is to create and share a research demo. However, the cloud is for more than just model sharing: we will also learn how we can tap into additional GPU resources for model training.
]]></content:encoded>
<pubDate>Thu, 30 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Sharing Deep Learning Research Models with Lightning Part 1: Building A Super Resolution App</title>
<link>https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html</link>
<guid>https://sebastianraschka.com/blog/2022/lightning-app-srgan-1.html</guid>
<content:encoded><![CDATA[
In this post, we will build a Lightning App. Why? Because it is 2022, and it is time to explore a more modern take on interacting with, presenting, and sharing our deep learning models. We are going to tackle this in three parts. In this first part, we will learn what a Lightning App is and how we build a Super Resolution GAN demo.
]]></content:encoded>
<pubDate>Fri, 17 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Taking Datasets, DataLoaders, and PyTorch’s New DataPipes for a Spin</title>
<link>https://sebastianraschka.com/blog/2022/datapipes.html</link>
<guid>https://sebastianraschka.com/blog/2022/datapipes.html</guid>
<content:encoded><![CDATA[
The PyTorch team recently announced TorchData, a prototype library focused on implementing composable and reusable data loading utilities for PyTorch. In particular, the TorchData library is centered around DataPipes, which are meant to be a DataLoader-compatible replacement for the existing Dataset class.
]]></content:encoded>
<pubDate>Sun, 12 Jun 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Running PyTorch on the M1 GPU</title>
<link>https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html</link>
<guid>https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html</guid>
<content:encoded><![CDATA[
Today, PyTorch officially introduced GPU support for Apple's ARM M1 chips. This is an exciting day for Mac users out there, so I spent a few minutes trying it out in practice. In this short blog post, I will summarize my experience and thoughts with the M1 chip for deep learning tasks.
]]></content:encoded>
<pubDate>Wed, 18 May 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Creating Confidence Intervals for Machine Learning Classifiers</title>
<link>https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html</link>
<guid>https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html</guid>
<content:encoded><![CDATA[
Developing good predictive models hinges upon accurate performance evaluation and comparisons. However, when evaluating machine learning models, we typically have to work around many constraints, including limited data, independence violations, and sampling biases. Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. This article outlines different methods for creating confidence intervals for machine learning models. Note that these methods also apply to deep learning.
]]></content:encoded>
<pubDate>Mon, 25 Apr 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Losses Learned</title>
<link>https://sebastianraschka.com/blog/2022/losses-learned-part1.html</link>
<guid>https://sebastianraschka.com/blog/2022/losses-learned-part1.html</guid>
<content:encoded><![CDATA[
The cross-entropy loss is our go-to loss for training deep learning-based classifiers. In this article, I am giving you a quick tour of how we usually compute the cross-entropy loss and how we compute it in PyTorch. There are two parts to it, and here we will look at a binary classification context first. You may wonder why bother writing this article; computing the cross-entropy loss should be relatively straightforward!? Yes and no. We can compute the cross-entropy loss in one line of code, but there's a common gotcha due to numerical optimizations under the hood. (And yes, when I am not careful, I sometimes make this mistake, too.) So, in this article, let me tell you a bit about deep learning jargon, improving numerical performance, and what could go wrong.
]]></content:encoded>
<pubDate>Mon, 04 Apr 2022 15:00:00 +0000</pubDate>
</item>
<item>
<title>TorchMetrics</title>
<link>https://sebastianraschka.com/blog/2022/torchmetrics.html</link>
<guid>https://sebastianraschka.com/blog/2022/torchmetrics.html</guid>
<content:encoded><![CDATA[
TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It's designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows. This iterative computation is useful if we want to track a model during iterative training or evaluation on minibatches (and optionally across on multiple GPUs). In deep learning, that's essentially *all the time*. However, when using TorchMetrics, one common question is whether we should use `.update()` or `.forward()`? (And that's also a question I certainly had when I started using it.). Here's a hands-on example and explanation.
]]></content:encoded>
<pubDate>Thu, 24 Mar 2022 13:00:00 +0000</pubDate>
</item>
<item>
<title>Machine Learning with PyTorch and Scikit-Learn</title>
<link>https://sebastianraschka.com/blog/2022/ml-pytorch-book.html</link>
<guid>https://sebastianraschka.com/blog/2022/ml-pytorch-book.html</guid>
<content:encoded><![CDATA[
Machine Learning with PyTorch and Scikit-Learn has been a long time in the making, and I am excited to finally get to talk about the release of my new book. Initially, this project started as the 4th edition of Python Machine Learning. However, we made so many changes to the book that we thought it deserved a new title to reflect that. So, what's new, you may wonder? In this post, I am excited to tell you all about it.
]]></content:encoded>
<pubDate>Fri, 25 Feb 2022 07:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Machine Learning</title>
<link>https://sebastianraschka.com/blog/2021/ml-course.html</link>
<guid>https://sebastianraschka.com/blog/2021/ml-course.html</guid>
<content:encoded><![CDATA[
About half a year ago, I organized all my deep learning-related videos in a handy blog post to have everything in one place. Since many people liked this post, and because I like to use my winter break to get organized, I thought I could free two birds with one key by compiling this list below. Here, you find a list of approximately 90 machine learning lectures I recorded in 2020 and 2021! Once again, I hope this is useful to you!
]]></content:encoded>
<pubDate>Wed, 29 Dec 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Introduction to Deep Learning</title>
<link>https://sebastianraschka.com/blog/2021/dl-course.html</link>
<guid>https://sebastianraschka.com/blog/2021/dl-course.html</guid>
<content:encoded><![CDATA[
I just sat down this morning and organized all deep learning related videos I recorded in 2021. I am sure this will be a useful reference for my future self, but I am also hoping it might be useful for one or the other person out there. PS: All code examples are in PyTorch :)
]]></content:encoded>
<pubDate>Fri, 09 Jul 2021 06:00:00 +0000</pubDate>
</item>
<item>
<title>Datasets for Machine Learning and Deep Learning</title>
<link>https://sebastianraschka.com/blog/2021/ml-dl-datasets.html</link>
<guid>https://sebastianraschka.com/blog/2021/ml-dl-datasets.html</guid>
<content:encoded><![CDATA[
With the semester being in full swing, I recently shared this set of dataset repositories with my deep learning class. However, I thought that beyond using this list for finding inspiration for interesting student class projects, these are also good places to look for additional bechmark datasets for your model.
]]></content:encoded>
<pubDate>Thu, 11 Feb 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Deep Learning With PyTorch</title>
<link>https://sebastianraschka.com/blog/2021/pytorch-deeplearning-review.html</link>
<guid>https://sebastianraschka.com/blog/2021/pytorch-deeplearning-review.html</guid>
<content:encoded><![CDATA[
After its release in August 2020, Deep Learning with PyTorch has been sitting on my shelf before I finally got a chance to read it during this winter break. It turned out to be the perfect easy-going reading material for a bit of productivity after the relaxing holidays. As promised last week, here are my thoughts.
]]></content:encoded>
<pubDate>Thu, 21 Jan 2021 09:00:00 +0000</pubDate>
</item>
<item>
<title>How I Keep My Projects Organized</title>
<link>https://sebastianraschka.com/blog/2021/project-management.html</link>
<guid>https://sebastianraschka.com/blog/2021/project-management.html</guid>
<content:encoded><![CDATA[
Since I started my undergraduate studies in 2008, I have been obsessed with productivity tips, notetaking solutions, and todo-list management. Over the years, I tried many, many workflows and hundreds of (mostly digital) tools to keep my life, projects, and notes organized. Occasionally, I exchange ideas with friends and colleagues, and upon request, I talked about my workflow a couple of times on Twitter. After today's 2021-edition of this discussion, I thought that writing a quick and informal blogpost makes sense, making it easier to read and having a quick reference if someone asks about it again :).
]]></content:encoded>
<pubDate>Sun, 03 Jan 2021 20:00:00 +0000</pubDate>
</item>
<item>
<title>Scientific Computing in Python: Introduction to NumPy and Matplotlib</title>
<link>https://sebastianraschka.com/blog/2020/numpy-intro.html</link>
<guid>https://sebastianraschka.com/blog/2020/numpy-intro.html</guid>
<content:encoded><![CDATA[
Since many students in my Stat 451 (Introduction to Machine Learning and Statistical Pattern Classification) class are relatively new to Python and NumPy, I was recently devoting a lecture to the latter. Since the course notes are based on an interactive Jupyter notebook file, which I used as a basis for the lecture videos, I thought it would be worthwhile to reformat it as a blog article with the embedded 'narrated content' -- the video recordings.
]]></content:encoded>
<pubDate>Sun, 27 Sep 2020 17:00:00 +0000</pubDate>
</item>
<item>
<title>Interpretable Machine Learning</title>
<link>https://sebastianraschka.com/blog/2020/interpretable-ml-1.html</link>
<guid>https://sebastianraschka.com/blog/2020/interpretable-ml-1.html</guid>
<content:encoded><![CDATA[
In this blog post, I am (briefly) reviewing Christoph Molnar's *Interpretable Machine Learning Book*. Then, I am writing about two classic generalized linear models, linear and logistic regression. Mainly, this blog post explains the relationship between feature weights and predictions and demonstrates how to construct confidence intervals via Python.
]]></content:encoded>
<pubDate>Wed, 26 Aug 2020 18:00:00 +0000</pubDate>
</item>
<item>
<title>Chapter 1: Introduction to Machine Learning and Deep Learning</title>
<link>https://sebastianraschka.com/blog/2020/intro-to-dl-ch01.html</link>
<guid>https://sebastianraschka.com/blog/2020/intro-to-dl-ch01.html</guid>
<content:encoded><![CDATA[
The first chapter (draft) of the Introduction to Deep Learning book, which is a book based on my lecture notes and slides.
]]></content:encoded>
<pubDate>Wed, 05 Aug 2020 21:00:00 +0000</pubDate>
</item>
<item>
<title>Book Review: Architects of Intelligence by Martin Ford</title>
<link>https://sebastianraschka.com/blog/2020/book-review-1-architects-of-intelligence.html</link>
<guid>https://sebastianraschka.com/blog/2020/book-review-1-architects-of-intelligence.html</guid>
<content:encoded><![CDATA[
A brief review of Martin Ford's book that features interviews with 23 of the most well-known and brightest minds working on AI.
]]></content:encoded>
<pubDate>Mon, 06 Jan 2020 13:00:00 +0000</pubDate>
</item>
<item>
<title>What's New in the 3rd Edition</title>
<link>https://sebastianraschka.com/blog/2019/whats-new-in-the-3rd-edition.html</link>
<guid>https://sebastianraschka.com/blog/2019/whats-new-in-the-3rd-edition.html</guid>
<content:encoded><![CDATA[
A brief summary of what's new in the 3rd edition of Python Machine Learning.
]]></content:encoded>
<pubDate>Thu, 12 Dec 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>My First Year at UW-Madison and a Gallery of Awesome Student Projects</title>
<link>https://sebastianraschka.com/blog/2019/student-gallery-1.html</link>
<guid>https://sebastianraschka.com/blog/2019/student-gallery-1.html</guid>
<content:encoded><![CDATA[
Not too long ago, in the Summer of 2018, I was super excited to join the Department of Statistics at the University of Wisconsin-Madison after obtaining my Ph.D. after ~5 long and productive years. Now, two semesters later after finals' week, I finally found some quiet days to look back on what's happened since then. In this post, I am sharing a short reflection as well as a some of the exciting projects my students were working on.
]]></content:encoded>
<pubDate>Fri, 24 May 2019 22:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html</link>
<guid>https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html</guid>
<content:encoded><![CDATA[
This final article in the series *Model evaluation, model selection, and algorithm selection in machine learning* presents overviews of several statistical hypothesis testing approaches, with applications to machine learning model and algorithm comparisons. This includes statistical tests based on target predictions for independent test sets (the downsides of using a single test set for model comparisons was discussed in previous articles) as well as methods for algorithm comparisons by fitting and evaluating models via cross-validation. Lastly, this article will introduce *nested cross-validation*, which has become a common and recommended a method of choice for algorithm comparisons for small to moderately-sized datasets.
]]></content:encoded>
<pubDate>Sat, 10 Nov 2018 22:00:00 +0000</pubDate>
</item>
<item>
<title>Generating Gender-Neutral Face Images with Semi-Adversarial Neural Networks to Enhance Privacy</title>
<link>https://sebastianraschka.com/blog/2018/semi-adversarial-nets-1.html</link>
<guid>https://sebastianraschka.com/blog/2018/semi-adversarial-nets-1.html</guid>
<content:encoded><![CDATA[
I thought that it would be nice to have short and concise summaries of recent projects handy, to share them with a more general audience, including colleagues and students. So, I challenged myself to use fewer than 1000 words without getting distracted by the nitty-gritty details and technical jargon. In this post, I mainly cover some of my recent research in collaboration with the [iPRoBe Lab](http://iprobe.cse.msu.edu) that falls under the broad category of developing approaches to hide specific information in face images. The research discussed in this post is about "maximizing privacy while preserving utility."
]]></content:encoded>
<pubDate>Thu, 02 Aug 2018 05:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html</guid>
<content:encoded><![CDATA[
Almost every machine learning algorithm comes with a large number of settings that we, the machine learning researchers and practitioners, need to specify. These tuning knobs, the so-called hyperparameters, help us control the behavior of machine learning algorithms when optimizing for performance, finding the right balance between bias and variance. Hyperparameter tuning for performance optimization is an art in itself, and there are no hard-and-fast rules that guarantee best performance on a given dataset. In Part I and Part II, we saw different holdout and bootstrap techniques for estimating the generalization performance of a model. We learned about the bias-variance trade-off, and we computed the uncertainty of our estimates. In this third part, we will focus on different methods of cross-validation for model evaluation and model selection. We will use these cross-validation techniques to rank models from several hyperparameter configurations and estimate how well they generalize to independent datasets.
]]></content:encoded>
<pubDate>Sun, 02 Oct 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html</guid>
<content:encoded><![CDATA[
In this second part of this series, we will look at some advanced techniques for model evaluation and techniques to estimate the uncertainty of our estimated model performance as well as its variance and stability. Then, in the next article, we will shift the focus onto another task that is one of the main pillar of successful, real-world machine learning applications -- Model Selection.
]]></content:encoded>
<pubDate>Sat, 13 Aug 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Model evaluation, model selection, and algorithm selection in machine learning</title>
<link>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html</link>
<guid>https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html</guid>
<content:encoded><![CDATA[
Machine learning has become a central part of our life -- as consumers, customers, and hopefully as researchers and practitioners! Whether we are applying predictive modeling techniques to our research or business problems, I believe we have one thing in common  &#58; We want to make good predictions! Fitting a model to our training data is one thing, but how do we know that it generalizes well to unseen data? How do we know that it doesn't simply memorize the data we fed it and fails to make good predictions on future samples, samples that it hasn't seen before? And how do we select a good model in the first place? Maybe a different learning algorithm could be better-suited for the problem at hand? Model evaluation is certainly not just the end point of our machine learning pipeline.<br /><br />Before we handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this article, we will go over a selection of these techniques, and we will see how they fit into the bigger picture, a typical machine learning workflow.
]]></content:encoded>
<pubDate>Sat, 11 Jun 2016 20:00:00 +0000</pubDate>
</item>
<item>
<title>Writing 'Python Machine Learning'</title>
<link>https://sebastianraschka.com/blog/2015/writing-pymle.html</link>
<guid>https://sebastianraschka.com/blog/2015/writing-pymle.html</guid>
<content:encoded><![CDATA[
It's been about time. I am happy to announce that "Python Machine Learning" was finally released today! Sure, I could just send an email around to all the people who were interested in this book. On the other hand, I could put down those 140 characters on Twitter (minus what it takes to insert a hyperlink) and be done with it. Even so, writing "Python Machine Learning" really was quite a journey for a few months, and I would like to sit down in my favorite coffeehouse once more to say a few words about this experience.
]]></content:encoded>
<pubDate>Thu, 24 Sep 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Python, Machine Learning, and Language Wars</title>
<link>https://sebastianraschka.com/blog/2015/why-python.html</link>
<guid>https://sebastianraschka.com/blog/2015/why-python.html</guid>
<content:encoded><![CDATA[
This has really been quite a journey for me lately. And regarding the frequently asked question “Why did you choose Python for Machine Learning?” I guess it is about time to write my script. In this article, I really don’t mean to tell you why you or anyone else should use Python. But read on if you are interested in my opinion.
]]></content:encoded>
<pubDate>Mon, 24 Aug 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Single-Layer Neural Networks and Gradient Descent</title>
<link>https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</link>
<guid>https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html</guid>
<content:encoded><![CDATA[
This article offers a brief glimpse of the history and basic concepts of machine learning. We will take a look at the first algorithmically described neural network and the gradient descent algorithm in context of adaptive linear neurons, which will not only introduce the principles of machine learning but also serve as the basis for modern multilayer neural networks in future articles.
]]></content:encoded>
<pubDate>Tue, 24 Mar 2015 10:00:00 +0000</pubDate>
</item>
<item>
<title>Principal Component Analysis</title>
<link>https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html</link>
<guid>https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html</guid>
<content:encoded><![CDATA[
Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a “black box”, and we are going to unravel its internals in 3 basic steps.
]]></content:encoded>
<pubDate>Tue, 27 Jan 2015 16:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Weighted Majority Rule Ensemble Classifier</title>
<link>https://sebastianraschka.com/Articles/2014_ensemble_classifier.html</link>
<guid>https://sebastianraschka.com/Articles/2014_ensemble_classifier.html</guid>
<content:encoded><![CDATA[
Here, I want to present a simple and conservative approach of implementing a weighted majority rule ensemble classifier in scikit-learn that yielded remarkably good results when I tried it in a kaggle competition. For me personally, kaggle competitions are just a nice way to try out and compare different approaches and ideas -- basically an opportunity to learn in a controlled environment with nice datasets.
]]></content:encoded>
<pubDate>Sun, 11 Jan 2015 18:00:00 +0000</pubDate>
</item>
<item>
<title>MusicMood</title>
<link>https://sebastianraschka.com/blog/2014/musicmood.html</link>
<guid>https://sebastianraschka.com/blog/2014/musicmood.html</guid>
<content:encoded><![CDATA[
In this article, I want to share my experience with a recent data mining project which probably was one of my most favorite hobby projects so far. It's all about building a classification model that can automatically predict the mood of music based on song lyrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>Turn Your Twitter Timeline into a Word Cloud</title>
<link>https://sebastianraschka.com/Articles/2014_twitter_wordcloud.html</link>
<guid>https://sebastianraschka.com/Articles/2014_twitter_wordcloud.html</guid>
<content:encoded><![CDATA[
Last week, I posted some visualizations in context of Happy Rock Song data mining project, and some people were curious about how I created the word clouds. Learn how to create YOUR personal Twitter Timeline!
]]></content:encoded>
<pubDate>Fri, 28 Nov 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Naive Bayes and Text Classification</title>
<link>https://sebastianraschka.com/Articles/2014_naive_bayes_1.html</link>
<guid>https://sebastianraschka.com/Articles/2014_naive_bayes_1.html</guid>
<content:encoded><![CDATA[
Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes’ probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction. In this first part of a series, we will take a look at the theory of naive Bayes classifiers and introduce the basic concepts of text classification. In following articles, we will implement those concepts to train a naive Bayes spam filter and apply naive Bayes to song classification based on lyrics.
]]></content:encoded>
<pubDate>Sat, 04 Oct 2014 22:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</title>
<link>https://sebastianraschka.com/Articles/2014_kernel_pca.html</link>
<guid>https://sebastianraschka.com/Articles/2014_kernel_pca.html</guid>
<content:encoded><![CDATA[
The focus of this article is to briefly introduce the idea of kernel methods and to implement a Gaussian radius basis function (RBF) kernel that is used to perform nonlinear dimensionality reduction via KBF kernel principal component analysis (kPCA).
]]></content:encoded>
<pubDate>Sun, 14 Sep 2014 20:00:00 +0000</pubDate>
</item>
<item>
<title>Predictive modeling, supervised machine learning, and pattern classification</title>
<link>https://sebastianraschka.com/Articles/2014_intro_supervised_learning.html</link>
<guid>https://sebastianraschka.com/Articles/2014_intro_supervised_learning.html</guid>
<content:encoded><![CDATA[
When I was working on my next pattern classification application, I realized that it might be worthwhile to take a step back and look at the big picture of pattern classification in order to put my previous topics into context and to provide and introduction for the future topics that are going to follow.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2014 08:00:00 +0000</pubDate>
</item>
<item>
<title>Linear Discriminant Analysis</title>
<link>https://sebastianraschka.com/Articles/2014_python_lda.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_lda.html</guid>
<content:encoded><![CDATA[
I received a lot of positive feedback about the step-wise Principal Component Analysis (PCA) implementation. Thus, I decided to write a little follow-up about Linear Discriminant Analysis (LDA) — another useful linear transformation technique. Both LDA and PCA are commonly used dimensionality reduction techniques in statistics, pattern classification, and machine learning applications. By implementing the LDA step-by-step in Python, we will see and understand how it works, and we will compare it to a PCA to see how it differs.
]]></content:encoded>
<pubDate>Sun, 03 Aug 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>Dixon's Q test for outlier identification</title>
<link>https://sebastianraschka.com/Articles/2014_dixon_test.html</link>
<guid>https://sebastianraschka.com/Articles/2014_dixon_test.html</guid>
<content:encoded><![CDATA[
I recently faced the impossible task to identify outliers in a dataset with very, very small sample sizes and Dixon's Q test caught my attention. Honestly, I am not a big fan of this statistical test, but since Dixon's Q-test is still quite popular in certain scientific fields (e.g., chemistry) that it is important to understand its principles in order to draw your own conclusion of the presented research data that you might stumble upon in research articles or scientific talks.
]]></content:encoded>
<pubDate>Sat, 19 Jul 2014 01:00:00 +0000</pubDate>
</item>
<item>
<title>About Feature Scaling and Normalization</title>
<link>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</link>
<guid>https://sebastianraschka.com/Articles/2014_about_feature_scaling.html</guid>
<content:encoded><![CDATA[
I received a couple of questions in response to my previous article (Entry point: Data) where people asked me why I used Z-score standardization as feature scaling method prior to the PCA. I added additional information to the original article, however, I thought that it might be worthwhile to write a few more lines about this important topic in a separate article.
]]></content:encoded>
<pubDate>Fri, 11 Jul 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Entry Point Data</title>
<link>https://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html</link>
<guid>https://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html</guid>
<content:encoded><![CDATA[
In this short tutorial I want to provide a short overview of some of my favorite Python tools for common procedures as entry points for general pattern classification and machine learning tasks, and various other data analyses.
]]></content:encoded>
<pubDate>Fri, 27 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Molecular docking, estimating free energies of binding, and AutoDock's semi-empirical force field</title>
<link>https://sebastianraschka.com/Articles/2014_autodock_energycomps.html</link>
<guid>https://sebastianraschka.com/Articles/2014_autodock_energycomps.html</guid>
<content:encoded><![CDATA[
Discussions and questions about methods, approaches, and tools for estimating (relative) binding free energies of protein-ligand complexes are quite popular, and even the simplest tools can be quite tricky to use. Here, I want to briefly summarize the idea of molecular docking, and give a short overview about how we can use AutoDock 4.2's hybrid approach for evaluating binding affinities.
]]></content:encoded>
<pubDate>Thu, 26 Jun 2014 16:00:00 +0000</pubDate>
</item>
<item>
<title>An introduction to parallel programming using Python's multiprocessing module</title>
<link>https://sebastianraschka.com/Articles/2014_multiprocessing.html</link>
<guid>https://sebastianraschka.com/Articles/2014_multiprocessing.html</guid>
<content:encoded><![CDATA[
The default Python interpreter was designed with simplicity in mind and has a thread-safe mechanism, the so-called "GIL" (Global Interpreter Lock). In order to prevent conflicts between threads, it executes only one statement at a time (so-called serial processing, or single-threading). In this introduction to Python's multiprocessing module, we will see how we can spawn multiple subprocesses to avoid some of the GIL's disadvantages and make best use of the multiple cores in our CPU.
]]></content:encoded>
<pubDate>Fri, 20 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Kernel density estimation via the Parzen-Rosenblatt window method</title>
<link>https://sebastianraschka.com/Articles/2014_kernel_density_est.html</link>
<guid>https://sebastianraschka.com/Articles/2014_kernel_density_est.html</guid>
<content:encoded><![CDATA[
The Parzen-window method (also known as Parzen-Rosenblatt window method) is a widely used non-parametric approach to estimate a probability density function *p(**x**)* for a specific point *p(**x**)* from a sample *p(**x**<sub>n</sub>)* that doesn't require any knowledge or assumption about the underlying distribution.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Numeric matrix manipulation</title>
<link>https://sebastianraschka.com/Articles/2014_matrix_cheatsheet.html</link>
<guid>https://sebastianraschka.com/Articles/2014_matrix_cheatsheet.html</guid>
<content:encoded><![CDATA[
At its core, this article is about a simple cheat sheet for basic operations on numeric matrices, which can be very useful if you working and experimenting with some of the most popular languages that are used for scientific computing, statistics, and data analysis.
]]></content:encoded>
<pubDate>Thu, 19 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>The key differences between Python 2.7.x and Python 3.x with examples</title>
<link>https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html</guid>
<content:encoded><![CDATA[
Many beginning Python users are wondering with which version of Python they should start. My answer to this question is usually something along the lines 'just go with the version your favorite tutorial was written in, and check out the differences later on.'\ But what if you are starting a new project and have the choice to pick? I would say there is currently no 'right' or 'wrong' as long as both Python 2.7.x and Python 3.x support the libraries that you are planning to use. However, it is worthwhile to have a look at the major differences between those two most popular versions of Python to avoid common pitfalls when writing the code for either one of them, or if you are planning to port your project.
]]></content:encoded>
<pubDate>Sun, 01 Jun 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>5 simple steps for converting Markdown documents into HTML and adding Python syntax highlighting</title>
<link>https://sebastianraschka.com/Articles/2014_markdown_syntax_color.html</link>
<guid>https://sebastianraschka.com/Articles/2014_markdown_syntax_color.html</guid>
<content:encoded><![CDATA[
In this little tutorial, I want to show you in 5 simple steps how easy it is to add code syntax highlighting to your blog articles.
]]></content:encoded>
<pubDate>Wed, 28 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Creating a table of contents with internal links in IPython Notebooks and Markdown documents</title>
<link>https://sebastianraschka.com/Articles/2014_ipython_internal_links.html</link>
<guid>https://sebastianraschka.com/Articles/2014_ipython_internal_links.html</guid>
<content:encoded><![CDATA[
Many people have asked me how I create the table of contents with internal links for my IPython Notebooks and Markdown documents on GitHub. Well, no (IPython) magic is involved, it is just a little bit of HTML, but I thought it might be worthwhile to write this little how-to tutorial.
]]></content:encoded>
<pubDate>Tue, 20 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A Beginner's Guide to Python's Namespaces, Scope Resolution, and the LEGB Rule</title>
<link>https://sebastianraschka.com/Articles/2014_python_scope_and_namespaces.html</link>
<guid>https://sebastianraschka.com/Articles/2014_python_scope_and_namespaces.html</guid>
<content:encoded><![CDATA[
A short tutorial about Python's namespaces and the scope resolution for variable names using the LEGB-rule with little quiz-like exercises.
]]></content:encoded>
<pubDate>Mon, 12 May 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Diving deep into Python</title>
<link>https://sebastianraschka.com/Articles/2014_deep_python.html</link>
<guid>https://sebastianraschka.com/Articles/2014_deep_python.html</guid>
<content:encoded><![CDATA[
Some while ago, I started to collect some of the not-so-obvious things I encountered when I was coding in Python. I thought that it was worthwhile sharing them and encourage you to take a brief look at the section-overview and maybe you'll find something that you do not already know - I can guarantee you that it'll likely save you some time at one or the other tricky debugging challenge.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Implementing a Principal Component Analysis (PCA)</title>
<link>https://sebastianraschka.com/Articles/2014_pca_step_by_step.html</link>
<guid>https://sebastianraschka.com/Articles/2014_pca_step_by_step.html</guid>
<content:encoded><![CDATA[
In this article I want to explain how a Principal Component Analysis (PCA) works by implementing it in Python step by step. At the end we will compare the results to the more convenient Python PCA() classes that are available through the popular matplotlib and scipy libraries and discuss how they differ.
]]></content:encoded>
<pubDate>Sun, 13 Apr 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Installing Scientific Packages for Python3 on MacOS 10.9 Mavericks</title>
<link>https://sebastianraschka.com/Articles/2014_install_python_sci_pkgs.html</link>
<guid>https://sebastianraschka.com/Articles/2014_install_python_sci_pkgs.html</guid>
<content:encoded><![CDATA[
I just went through some pain (again) when I wanted to install some of Python's scientific libraries on my second Mac. I summarized the setup and installation process for future reference.\ If you encounter any different or additional obstacles let me know, and please feel free to make any suggestions to improve this short walkthrough.
]]></content:encoded>
<pubDate>Thu, 13 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>A thorough guide to SQLite database operations in Python</title>
<link>https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</link>
<guid>https://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html</guid>
<content:encoded><![CDATA[
After I wrote the initial teaser article "SQLite - Working with large data sets in Python effectively" about how awesome SQLite databases are via sqlite3 in Python, I wanted to delve a little bit more into the SQLite syntax and provide you with some more hands-on examples.
]]></content:encoded>
<pubDate>Fri, 07 Mar 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Using OpenEye software for substructure alignments</title>
<link>https://sebastianraschka.com/Articles/2014_openeye_alignments_overlays.html</link>
<guid>https://sebastianraschka.com/Articles/2014_openeye_alignments_overlays.html</guid>
<content:encoded><![CDATA[
This is a quickguide showing how to use OpenEye software command line tools to align target molecules to a query based on substructure matches and how to retrieve the best molecule overlay from two sets of low-energy conformers.
]]></content:encoded>
<pubDate>Sun, 23 Feb 2014 09:00:00 +0000</pubDate>
</item>
<item>
<title>Unit testing in Python</title>
<link>https://sebastianraschka.com/Articles/2013_python_unittest.html</link>
<guid>https://sebastianraschka.com/Articles/2013_python_unittest.html</guid>
<content:encoded><![CDATA[
Let’s be honest, code testing is everything but a joyful task. However, a good unit testing framework makes this process as smooth as possible. Eventually, testing becomes a regular and continuous process, accompanied by the assurance that our code will operate just as exact and seamlessly as a Swiss clockwork.
]]></content:encoded>
<pubDate>Sat, 14 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>A short tutorial for decent heat maps in R</title>
<link>https://sebastianraschka.com/Articles/heatmaps_in_r.html</link>
<guid>https://sebastianraschka.com/Articles/heatmaps_in_r.html</guid>
<content:encoded><![CDATA[
I received many questions from people who want to quickly visualize their data via heat maps - ideally as quickly as possible. This is the major issue of exploratory data analysis, since we often don’t have the time to digest whole books about the particular techniques in different software packages to just get the job done. But once we are happy with our initial results, it might be worthwhile to dig deeper into the topic in order to further customize our plots and maybe even polish them for publication. In this post, my aim is to briefly introduce one of R’s several heat map libraries for a simple data analysis. I chose R, because it is one of the most popular free statistical software packages around. Of course there are many more tools out there to produce similar results (and even in R there are many different packages for heat maps), but I will leave this as an open topic for another time.
]]></content:encoded>
<pubDate>Sun, 08 Dec 2013 09:00:00 +0000</pubDate>
</item>
<item>
<title>SQLite</title>
<link>https://sebastianraschka.com/Articles/2013_sqlite_database.html</link>
<guid>https://sebastianraschka.com/Articles/2013_sqlite_database.html</guid>
<content:encoded><![CDATA[
My new project confronted me with the task of screening a massive set of large data files in text format with billions of entries each. I will have to retrieve data repeatedly and frequently in the future. Thus, I was tempted to find a better solution than brute-force scanning through ~20 separate 1-column text files with ~6 billion entries every time line by line.
]]></content:encoded>
<pubDate>Sun, 03 Nov 2013 09:00:00 +0000</pubDate>
</item>
</channel>
</rss>